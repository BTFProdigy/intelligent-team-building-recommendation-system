Coling 2008: Proceedings of the workshop on Multi-source Multilingual Information Extraction and Summarization, pages 10?16
Manchester, August 2008
Abstract 
Hall? Norden is a web site with information 
regarding mobility between the Nordic coun-
tries in five different languages; Swedish, 
Danish, Norwegian, Icelandic and Finnish.  
We wanted to create a Nordic cross-language 
dictionary for the use in a cross-language 
search engine for Hall? Norden. The entire 
set of texts on the web site was treated as one 
multilingual parallel corpus. From this we 
extracted parallel corpora for each language 
pair. The corpora were very sparse, contain-
ing on average less than 80 000 words per 
language pair. We have used the Uplug word 
alignment system (Tiedemann 2003a), for the 
creation of the dictionaries. The results gave 
on average 213 new dictionary words (fre-
quency > 3) per language pair. The average 
error rate was 16 percent. Different combina-
tions with Finnish had a higher error rate, 33 
percent, whereas the error rate for the re-
maining language pairs only yielded on aver-
age 9 percent errors. The high error rate for 
Finnish is possibly due to the fact that the 
Finnish language belongs to a different lan-
guage family. Although the corpora were 
very sparse the word alignment results for the 
combinations of Swedish, Danish, Norwe-
gian and Icelandic were surprisingly good 
compared to other experiments with larger 
corpora.   
                                                 
 ? 2008. Licensed under the Creative Commons Attri-
bution-Noncommercial-Share Alike 3.0 Unported 
license (http://creativecommons.org/licenses/by-nc-
sa/3.0/). Some rights reserved. 
1 Introduction 
Hall? Norden (Hello Scandinavia) is a web site 
with information regarding mobility between the 
Nordic countries and is maintained by the Nordic 
Council.  Mobility information concerns issues 
such as how employment services, social ser-
vices, educational systems etc. work in the dif-
ferent countries. The web site has information in 
five different languages; Swedish, Danish, Nor-
wegian, Icelandic and Finnish.  In this paper 
Nordic languages are defined as Swedish, Danish, 
Norwegian, Icelandic and Finnish. Scandinavian 
languages are defined as the Nordic languages 
excluding Finnish. 
The texts on the web site were almost parallel 
and there were also ten minimal dictionaries with 
on average 165 words available for the different 
languages. The dictionaries consisted of domain-
specific words regarding mobility information in 
the Nordic countries. The Nordic Council wanted 
to extend the dictionaries so they would cover a 
larger part of the specific vocabulary, in order to 
help the people in the Nordic countries to find 
and learn the concepts in their neighboring coun-
tries. 
The entire set of texts on the web site was 
treated as one multilingual parallel corpus. From 
this we extracted parallel corpora for each lan-
guage pair.  We discovered, as expected, that the 
corpora were very sparse, containing on average 
less than 80 000 words per language pair. We 
needed to construct 10 different dictionaries and 
therefore we processed 10 pairs of parallel text 
sets. We have used the Uplug word alignment 
system (Tiedemann 2003a), for the creation of 
the dictionaries. The system and motivation for 
the choice of system is further discussed in Sec-
tion 2.1. 
Automatic Construction of Domain-specific Dictionaries on  
Sparse Parallel Corpora in the Nordic Languages 
Sumithra Velupillai 
DSV/KTH-Stockholm University 
SE-164 40 Kista 
Sweden 
sumithra@dsv.su.se 
 
Hercules Dalianis 1, 2 
1) DSV/KTH-Stockholm University 
SE-164 40 Kista 
Sweden 
2) Euroling AB 
Igeldammsgatan 22c 
112 49 Stockholm, Sweden 
hercules@dsv.su.se 
10
We also discovered that the texts were not 
completely parallel. Therefore, we made a small 
experiment on attempting to enhance the results 
by deleting texts that were not parallel. Multilin-
gual parallel corpora covering all Nordic lan-
guages are very rare. Although the corpora cre-
ated in this work are domain-specific, they are an 
important contribution for further research on 
Nordic multilingual issues. Moreover, many 
large governmental, industrial or similar web 
sites that contain information in several lan-
guages may profit from compiling multilingual 
dictionaries automatically in order to enhance 
their search engines and search results. 
In this project, our two main goals were to 
compile parallel corpora covering the Nordic 
languages, and to evaluate the results of auto-
matically creating dictionaries using an existing 
tool with basic settings, in order to find out 
where more work would need to be done and 
where performance is actually acceptable. We 
have limited the work by only testing one system 
(Uplug) with basic settings. Our experiments and 
results are described in further detail in the fol-
lowing sections. Conclusions and future work are 
discussed in the final section. 
2 Related Work 
Word alignment systems have been used in pre-
vious research projects for automatically creating 
dictionaries. In Charitakis (2007) Uplug was 
used for aligning words in a Greek-English paral-
lel corpus. The corpus was relatively sparse, con-
taining around 200 000 words for each language, 
downloaded from two different bilingual web 
sites. A sample of 498 word pairs from Uplug 
were evaluated by expert evaluators and the re-
sult was 51 percent correctly translated words 
(frequency > 3). When studying high frequent 
word pairs (>11), there were 67 percent correctly 
translated words. In Megyesi & Dahlqvist (2007) 
an experiment is described where they had 150 
000 words in Swedish and 126 000 words in 
Turkish that gave 69 percent correct translations 
(Uplug being one of the main tools used). In this 
work the need for parallel corpora in different 
language combinations is also discussed. 
The ITools? suite for word alignment that was 
used in Nystr?m et al(2006) on a medical paral-
lel corpus, containing 174 000 Swedish words 
and 153 000 English words, created 31 000 word 
pairs with 76 percent precision and 77 percent 
recall. In this work the word alignment was pro-
duced interactively.  
A shared task on languages with sparse re-
sources is described in Martin et al(2005). The 
language pairs processed were English-Inuktitut, 
Romanian-English and English-Hindi, where the 
English-Inuktitut parallel corpus contained 
around 4 million words for English and 2 mil-
lions words for Inuktitut. English-Hindi had less 
words, 60 000 words and 70 000 words respec-
tively. The languages with the largest corpora 
obtained best word alignment results, for Eng-
lish-Inuktitut over 90 percent precision and recall 
and for English-Hindi 77 percent precision and 
68 percent recall. One conclusion from the 
shared task was that it is worth using additional 
resources for languages with very sparse corpora 
improving results with up to 20 percent but not 
for the languages with more abundant corpora 
such as for instance English-Inuktitut.  
2.1 Word Alignment: Uplug 
We have chosen to use the Uplug word align-
ment system since it is a non-commercial system 
which does not need a pre-trained model and is 
easy to use. It is also updated continuously and 
incorporates other alignment models, such as 
GIZA++ (Och & Ney 2003). We did not want to 
evaluate the performance of different systems in 
the work presented here, but rather evaluate the 
performance of only one system applied on dif-
ferent language combinations and on sparse cor-
pora. Evaluating the performance of different 
systems is an important and interesting research 
problem, but is left for future work. An evalua-
tion of two word alignment systems Plug (Uplug) 
and Arcade is described in Ahrenberg et al
(2000). 
The Uplug system implements a word align-
ment process that combines different statistical 
measures for finding word alignment candidates 
and is fully automatic. It is also possible to com-
bine statistical measures with linguistic informa-
tion, such as part-of-speech tags. In the preproc-
essing steps the corpora are converted to an xml-
format and they are also sentence aligned. 
We have chosen to use basic settings for all 
corpora in the different language pairs, in order 
to evaluate the effect of this. The default word 
alignment settings in Uplug works in the follow-
ing way:  
 
? create basic clues (Dice and LCSR) 
? run GIZA++ with standard settings 
(trained on plain text) 
11
? learn clues from GIZA's Viterbi align-
ments 
? "radical stemming" (take only the 3 initial 
characters of each token) and run GIZA++ 
again 
? align words with existing clues 
? learn clues from previous alignment 
? align words again with all existing clues1 
 
This approach is called the clue alignment ap-
proach and is described further in Tiedemann 
(2003b). In the work presented here, we have not 
included any linguistic information, as we 
wanted to evaluate the performance of applying 
the system on sparse, raw, unprocessed corpora 
for different (Nordic) language pairs, using de-
fault settings. 
 
3 Experiments and Results 
For the project presented in this paper we wanted 
to see if it was possible to create domain-specific 
dictionaries on even smaller corpora. (compared 
to the ones described in Section 2) for all the 
Nordic language pairs. We did not have the pos-
sibility to evaluate the results for Icelandic-
Finnish, since we did not find any evaluator hav-
ing knowledge in both Icelandic and Finnish. 
Therefore we present the results for the remain-
ing nine language pairs. In total we had four 
evaluators for the other language combinations. 
Each evaluator evaluated those language pairs 
                                                 
                                                
1 Steps taken from the Quickstart guidelines for the Uplug 
system, which can be downloaded here: 
http://uplug.sourceforge.net/ 
she or he had fluent or near-fluent knowledge in. 
The domain was very restricted containing only 
words about mobility between the Nordic coun-
tries. 
The Scandinavian languages are closely re-
lated. Swedish, Danish, and Norwegian are com-
prehensible for Scandinavians. A typical Swede 
will for instance understand written and to a cer-
tain degree spoken Danish, but is not able to 
speak Danish. Typical Swedes will, for instance, 
have a passive understanding of Danish (and vice 
versa for the other languages). Finnish on the 
other hand belongs to the Finno-Ugric group of 
the Uralic languages, while the Scandinavian 
languages are North-Germanic Indo-European 
languages. We wanted to investigate if, and how, 
these differences affect the word alignment re-
sults. We also wanted to experiment with differ-
ent frequency thresholds, in order to see if this 
would influence the results. 
The first step was to extract the web pages 
from the web site and obtain the web pages in 
plain text format. We obtained help for that work 
from Euroling AB,2 our contractor.  
In Table 1 we show general information about 
the corpora. We see that the distribution of words 
is even for the Scandinavian languages, but not 
for the combinations with Finnish. It is interest-
ing to observe that Finnish has fewer word to-
kens than the Scandinavian languages.  
All Nordic languages, both Scandinavian and 
Finnish, have very productive word compound-
ing. In Finnish word length is longer, on average, 
 
2 See: http://www.euroling.se/ 
Language pair No. texts No. words Word distribution, first language in language pair, %
sw-da 191 83871 49.2
sw-no 133 62554 49.7
sw-fi 196 73933 57.6
sw-ice 187 82711 48.5
da-no 156 68777 50.2
da-fi 239 84194 58.4
da-ice 232 97411 49.5
no-fi 156 58901 58.2
no-ice 145 64931 49.6
Average 182 75254 52.3
Table 1: General corpora information, initial corpora 
12
and the number of words per clause lower, on 
average, due to its extensive morphology. 
In Dalianis et al(2007) lemmatizing the text 
set before the alignment process did not improve 
results. In the work presented here, we have also 
made some experiments on lemmatizing the cor-
pora before the alignment process. We have used 
the CST lemmatizer3 for the Scandinavian lan- 
guages and Fintwol4 for Finnish. Unfortunately, 
the results were not improved. The main reason 
for the decrease in performance is probably due 
to the loss of sentence formatting during the 
lemmatization process. The sentence alignment 
is a crucial preprocessing step for the word 
alignment process, and a lot of the sentence 
boundaries were lost in the lemmatization proc-
ess. However, the resulting word lists from 
Uplug have been lemmatized using the same 
lemmatizers, in order to obtain normalized dic-
tionaries. 
The corpora were to some extent non-parallel 
containing some extra non-parallel paragraphs. 
We found that around five percent of the corpora 
were non-parallel. In order to detect non-parallel 
sections we have used a simpler algorithm than 
in for instance Munteanu & Marcu (2006). The 
total number of paragraphs and sentences in each 
                                                 
                                                
3 See: http://cst.dk/download/cstlemma/current/doc/ 
4 See: http://www2.lingsoft.fi/cgi-bin/fintwol 
parallel text pair were counted. If the total num-
ber for each language in some language pair dif-
fered more than 20 percent these files were de-
leted. The refined corpora have been re-aligned 
with Uplug and evaluated. In Table 2 we show 
the general information for the refined corpora. 
3.1 Evaluation 
Our initial plan was to use the manually con-
structed dictionaries from the web site as an 
evaluation resource, but the words in these dic-
tionaries were rare in the corpus. Therefore we 
used human evaluators to evaluate the results 
from Uplug.  
The results from the Uplug execution gave on 
average 213 new dictionary words (frequency > 
3) per language, see Table 3. The average error 
rate 5  was 16 percent. We delimited the word 
amount by removing words shorter than six char-
acters, and also multiword expressions6 from the 
resulting word lists. The six character strategy is 
efficient for the Scandinavian languages as an 
alternative to stop word removal (Dalianis et al
2003) since the Scandinavian languages, as well 
 
5 The error rate is in this paper defined as the percentage of 
wrongly generated entries compared to the total number of 
generated entries. 
6 A multiword expression is in this paper defined as words 
(sequences of characters, letters or digits) separated by a 
blank or a hyphen. 
Language pair No. parallel texts Deleted files, % No. words, parallel 
Word distribution, 
first language in 
language pair, %
sw-da 179 6.3 78356 49.7
sw-no 128 3.8 59161 49.8
sw-fi 189 3.6 69525 58.1
sw-ice 175 5.9 76056 48.3
da-no 147 5.8 64946 50.2
da-fi 222 7.1 77849 58.6
da-ice 210 3.4 89093 49.0
no-fi 145 7.1 55409 58.3
no-ice 130 2.1 59622 49.0
Average 169 5.0 70002 52.3
Table 2: General corpora information, refined parallel corpora (non-parallel texts deleted) 
 
13
as Finnish, mostly produce compounds that are 
formed into one word (i.e. without blanks or hy-
phens). In Tiedemann (2008), a similar strategy 
of removing words with a word length shorter 
than five characters was carried out but in that 
case for English, Dutch and German. 
Different combinations with Finnish had a 
higher error rate, 30 percent, whereas the error 
rate for the combinations of the Scandinavian 
languages only yielded on average 9 percent  
errors. 
The high error rate for Finnish is possibly due 
to the fact that the Finnish language belongs to a 
different language family. We can see the same 
phenomena for Greek (Charitakis, 2007) and 
Turkish (Megyesi & Dahlqvist, 2007) combined 
with English and Swedish respectively, with 33 
and 31 percent erroneously translated words. 
However, one might expect even higher error 
rates due to the differences in the different lan-
guage pairs (and the sparseness of the data). Fin-
nish has free word order and is typologically 
very different from the Scandinavian languages, 
and the use of form words differs between the 
languages. On the other hand, both Finnish and 
the Scandinavian languages produce long, com-
plex compounds somewhat similarly, and the 
word order in Finnish share many features with 
the word order in the Scandinavian languages. 
One important aspect is the cultural similarities 
that the languages share.  
The main errors that were produced for the 
combinations of Finnish and the Scandinavian 
languages consisted of either errors with particles 
or compounds where the head word or attribute 
were missing in the Finnish alignment. For in-
stance, the Swedish word inv?nare (inhabitant) 
was aligned with the Finnish word asukasluku 
(number of inhabitants). Another error which 
was produced for all combinations with Finnish 
was lis?tieto (more information) which was 
aligned with ytterligere (additional, more) in 
Norwegian (and equivalent words in Swedish 
and Danish), an example of an error where the 
head word is missing. Many texts had sentences 
pointing to further information, which might ex-
plain this type of error. 
The lemmatizers produced some erroneous 
word forms. In Dalianis & Jongejan (2006) the 
CST lemmatizer was evaluated and reported an 
average error rate of nine percent. Moreover, 
since the lemmatization process is performed on 
the resulting word lists, and not within the origi-
nal context in which the words occur, the auto-
matic lemmatization is more difficult for the two 
lemmatizers used in this project. These errors 
have not been included in our evaluation since 
they are not produced by the Uplug alignment 
procedure. 
We can also see in Table 3 that deleting non-
parallel texts using our simple algorithm did not 
improve the overall results significantly. Perhaps 
our simple algorithm was too coarse for these 
corpora. The texts were in general very short and 
simple frequency information on paragraph and 
sentence amounts might not have captured non-
parallel fragments on such texts. 
 Initial   Deleting non-parallel 
Language 
pair 
No. dictionary 
words  
Erroneous 
translations, %
No. dictionary 
words  Erroneous translations, % 
sw-da 322 7.1 305 7.2
sw-no 269 6.3 235 9.4
sw-fi 138 29.0 133  34.6
sw-ice 151 18.5 173 16.2
da-no 322 3.7 304 4.3
da-fi 169 34.3 244  33.2
da-ice 206 6.8 226 10.2
no-fi 185 27.6 174  30.0
no-ice 159 14.5 181 14.4
Average  213 16.4  219  16.1
Table 3: Produced dictionary words and error rate 
14
The produced dictionary words were of high 
domain-specific quality. The majority of the cor-
rect and erroneous word pairs were covered by 
both the initial and the refined corpus. Deleting 
non-parallel texts produced some new, valuable 
words that were not included in the initial results. 
However, since these dictionaries were generally 
smaller, this did not improve the overall results, 
and the error rate was somewhat higher for most 
language pairs. Improved dictionary in this work 
means as many word pairs as possible with do-
main-specific significance. 
Since the texts were about different country-
specific issues they could contain sections in an-
other language (names of ministries, offices etc). 
This produced some errors in the alignment re-
sults. These errors might have been avoided by 
applying a language checker while processing 
the texts. 
The errors for the Scandinavian languages 
were also mainly of the same type, and mostly 
due to the fact that the texts were not completely 
parallel, or due to form words or compounds. For 
instance, the Swedish word exempelvis (for ex-
ample) was aligned with the Norwegian word 
eksempel (example), which was counted as an 
error, but which, in its context, is not completely 
erroneous. 
Even at a relatively low frequency threshold 
the results were very good for the Scandinavian 
languages. We tried to increase the frequency 
threshold in order to see if this would improve 
the results for Finnish, which it unfortunately did 
not. However, as stated above, the errors were 
mainly of the same type, and probably constant 
over different frequencies. We also see that for 
Icelandic, unlike the other languages, deleting 
non-parallel fragments yielded larger dictionar-
ies. Uplug produced more multiword units for 
the initial corpora containing Icelandic, single 
word pairs were more frequent in the refined 
corpus. However, the overall results were not 
improved. 
4 Conclusions and Future Work 
Although the corpora were very sparse the word 
alignment results for Swedish-Danish, Swedish-
Norwegian and Danish-Norwegian were surpris-
ingly good with on average 93.1 percent correct 
results. The results for Finnish were worse with 
on average only 67.4 percent correct results. 
However, as discussed above, the main errors 
were of the same type. Creating dictionaries for 
non-related languages might need more elaborate 
alignment approaches. In the special case of Fin-
nish combined with one (or several) of the Scan-
dinavian languages, simple preprocessing steps 
might improve the results. For instance, remov-
ing stop words before running the corpora 
through a word alignment system might handle 
the errors where particles and form words are 
included. Also, tagging the corpora with part-of-
speech tags and lemmatizing as a preprocessing 
step might improve results. 
An important aspect of automatically creating 
multilingual dictionaries is the need for preproc-
essing tools covering all languages. This is often 
difficult to obtain, and different tools use differ-
ent formatting and tagging schemes. Moreover, 
they might differ in robustness, which also af-
fects the end results. In this project, we encoun-
tered such problems during the lemmatization 
process for instance, but we did not have the op-
portunity to explore and evaluate alternative 
tools. In the future, evaluating the performance 
of the preprocessing steps might be desirable. 
Evaluating translated words is not easy. Many 
words may be related without being direct trans-
lations. Manual evaluation has the advantage of 
taking such issues into account, but this also 
means that the results might differ depending on 
the evaluator. Furthermore, evaluating transla-
tions without contextual information is problem-
atic. Also, the criteria for judging a translation as 
correct or not depend on the goal for the use of 
the word lists. For instance, the errors for the 
combinations with Finnish might not be prob-
lematic in a real-world search engine setting, de-
pending on which demands there are on the 
search results. The errors produced in the work 
presented here would probably yield acceptable 
search results. Such user and search engine result 
aspects have not been evaluated here, but are 
interesting research questions for future work. 
The Nordic languages are highly inflectional. 
Combining compound splitting and lemmatizing 
before the alignment process might improve the 
results. Especially compound splitting could 
probably handle the errors produced for the com-
binations of Finnish with the Scandinavian lan-
guages. Cross-combining the different language 
pairs might enhance the results and create more 
specific and errorless dictionaries. Other word 
alignment systems should also be tested, in order 
to compare different approaches and their results. 
Perhaps results from different systems could also 
be combined, in order to produce more extensive 
dictionaries. Furthermore, other approaches to 
15
detect non-parallel fragments should be investi-
gated. 
Finding the boundary for the minimum size of 
parallel corpora in order to obtain acceptable dic-
tionaries is also an interesting research issue 
which should be explored. 
Automatically creating multilingual dictionar-
ies is not trivial. Many aspects need to be consid-
ered. Especially, the final use of the produced 
results influences both the preprocessing steps 
required and the evaluation of the results. Also, 
the languages in consideration affect the steps 
that need to be made. However, in this paper we 
have shown that using state-of-the-art tools on 
sparse, raw, unprocessed domain-specific cor-
pora in both related and non-related languages 
yield acceptable and even commendable results. 
Depending on the purposes for the use of the dic-
tionaries, simple adjustments would probably 
yield even better results. 
In a real-world setting, parallel (or near-
parallel) corpora covering several (small) lan-
guages are difficult to obtain and compile. Most 
resources are found on the Internet, and the qual-
ity of the corpora may vary depending on many 
aspects. Formatting, translations, text length and 
style may differ considerably depending on the 
type of texts. Freely available text sets for small 
languages are often sparse. Despite this, we have 
shown that it is possible to compile valuable re-
sources from available data.   
There are very few sources of dictionaries 
covering the Nordic language pairs. The created 
corpora will be made publicly available for fur-
ther research and evaluation. 
References 
Ahrenberg, L., M. Merkel, A. S?gvall Hein and J. 
Tiedemann 2000. Evaluation of word alignment 
systems. Lars Ahrenberg, Magnus Merkel, Anna 
S?gvall Hein and J?rg Tiedemann. Proceedings of 
the Second International Conference on Linguistic 
Resources and Evaluation (LREC-2000), Athens, 
Greece, 31 May - 2 June, 2000, Volume III: 1255-
1261. 
Charitakis, K. 2007. Using parallel corpora to create a 
Greek-English dictionary with Uplug, in Proc. 16th 
Nordic Conference on Computational Linguistics - 
NODALIDA ?07. 
Dalianis, H. and B. Jongejan 2006. Hand-crafted ver-
sus Machine-learned Inflectional Rules: the Eurol-
ing-SiteSeeker Stemmer and CST's Lemmatiser, in 
Proc. of the International Conference on Language 
Resources and Evaluation, LREC 2006. 
Dalianis, H., M. Rimka and V. Kann 2007. Using 
Uplug and SiteSeeker to construct a cross language 
search engine for Scandinavian. Workshop: The 
Automatic Treatment of Multilinguality in Re-
trieval, Search and Lexicography, Copenhagen, 
April 2007. 
Dalianis, H., M. Hassel, J. Wedekind, D. Haltrup, K. 
de Smedt and T.C. Lech. 2003. Automatic text 
summarization for the Scandinavian languages. In 
Holmboe, H. (ed.) Nordisk Sprogteknologi 2002: 
?rbog for Nordisk Spr?kteknologisk Forsknings-
program 2000-2004, pp. 153-163. Museum Tuscu-
lanums Forlag. 
Martin, J and R. Mihalcea and T. Pedersen. 2005. 
Word Alignment for Languages with Scarce Re-
sources. Proceedings of the ACL 2005 Workshop 
on Building and Using Parallel Texts: Data Driven 
Machine Translation and Beyond, Ann Arbor, MI, 
June 2005.  
Megyesi, B. and B. Dahlqvist, 2007. The Swedish-
Turkish Parallel Corpus and Tools for its Creation, 
in Proc. 16th Nordic Conference on Computational 
Linguistics - NODALIDA ?07. 
Munteanu, D.S. and D. Marcu 2006. Extracting Paral-
lel Sub-sentential Fragments from Non-parallel 
Corpora. ACL ?06: Proceedings of the 21st Interna-
tional Conference on Computational Linguistics, 
pp. 81-88, Sydney, Australia. 
Nystr?m, M., M. Merkel, L. Ahrenberg, P. Zweigen-
baum, H. Petersson and H. ?hlfeldt. 2006. Creat-
ing a Medical English-Swedish Dictionary using 
Interactive Word Alignment, in BMC medical in-
formatics and decision making, 6:35.  
Franz Josef Och, Hermann Ney. A Systematic Com-
parison of Various Statistical Alignment Models, 
Computational Linguistics, volume 29, number 1, 
pp. 19-51 March 2003. 
Tiedemann, J. 2003a. Recycling Translations: Extrac-
tion of Lexical Data from Parallel Corpora and 
their Application in Natural Language Processing. 
Acta Universitatis Upsaliensis: Studia linguistica 
upsaliensia, ISSN 1652-1366, ISBN 91-554-5815-
7. 
Tiedemann, J. 2003b. Combining clues for word 
alignment. In Proceedings of the Tenth Conference 
on European Chapter of the Association For Com-
putational Linguistics - Volume 1 (Budapest, Hun-
gary, April 12 - 17, 2003). European Chapter Meet-
ing of the ACL. Association for Computational 
Linguistics, Morristown, NJ, 339-346. DOI= 
http://dx.doi.org/10.3115/1067807.1067852. 
Tiedemann, J. 2008. Synchronizing Translated Movie 
Subtitles. In the Proceedings of the Sixth Interna-
tional Conference on Language Resources and 
Evaluation, LREC 2008, Marrakech, Morocco, 
May 28-30, 2008. 
16
CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 248?252
Manchester, August 2008
Mixing and Blending Syntactic and Semantic Dependencies
Yvonne Samuelsson
Dept. of Linguistics
Stockholm University
yvonne.samuelsson@ling.su.se
Johan Eklund
SSLIS
University College of Bor?as
johan.eklund@hb.se
Oscar T
?
ackstr
?
om
Dept. of Linguistics and Philology
SICS / Uppsala University
oscar@sics.se
Mark Fi
?
sel
Dept. of Computer Science
University of Tartu
fishel@ut.ee
Sumithra Velupillai
Dept. of Computer and Systems Sciences
Stockholm University / KTH
sumithra@dsv.su.se
Markus Saers
Dept. of Linguistics and Philology
Uppsala University
markus.saers@lingfil.uu.se
Abstract
Our system for the CoNLL 2008 shared
task uses a set of individual parsers, a set of
stand-alone semantic role labellers, and a
joint system for parsing and semantic role
labelling, all blended together. The system
achieved a macro averaged labelled F
1
-
score of 79.79 (WSJ 80.92, Brown 70.49)
for the overall task. The labelled attach-
ment score for syntactic dependencies was
86.63 (WSJ 87.36, Brown 80.77) and the
labelled F
1
-score for semantic dependen-
cies was 72.94 (WSJ 74.47, Brown 60.18).
1 Introduction
This paper presents a system for the CoNLL 2008
shared task on joint learning of syntactic and se-
mantic dependencies (Surdeanu et al, 2008), com-
bining a two-step pipelined approach with a joint
approach.
In the pipelined system, eight different syntac-
tic parses were blended, yielding the input for two
variants of a semantic role labelling (SRL) system.
Furthermore, one of the syntactic parses was used
with an early version of the SRL system, to pro-
vide predicate predictions for a joint syntactic and
semantic parser. For the final submission, all nine
syntactic parses and all three semantic parses were
blended.
The system is outlined in Figure 1; the dashed
arrow indicates the potential for using the predi-
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
Process
Parse + SRL
Process
8 MaltParsers
Parser Blender
Process
2 Pipelined SRLs
SRL Blender
Joint Parser/SRL
Possible
Iteration
Figure 1: Overview of the submitted system.
cate prediction to improve the joint syntactic and
semantic system.
2 Dependency Parsing
The initial parsing system was created using Malt-
Parser (Nivre et al, 2007) by blending eight dif-
ferent parsers. To further advance the syntactic ac-
curacy, we added the syntactic structure predicted
by a joint system for syntactic and semantic depen-
dencies (see Section 3.4) in the blending process.
2.1 Parsers
The MaltParser is a dependency parser genera-
tor, with three parsing algorithms: Nivre?s arc
standard, Nivre?s arc eager (see Nivre (2004)
for a comparison between the two Nivre algo-
rithms), and Covington?s (Covington, 2001). Both
of Nivre?s algorithms assume projectivity, but
the MaltParser supports pseudo-projective parsing
(Nilsson et al, 2007), for projectivization and de-
projectivization.
248
WSJ Brown
Best single parse 85.22% 78.37%
LAS weights 87.00% 80.60%
Learned weights 87.36% 80.77%
Table 1: Labelled attachment score on the two test
sets of the best single parse, blended with weights
set to PoS labelled attachment score (LAS) and
blended with learned weights.
Four parsing algorithms (the two Nivre al-
gorithms, and Covington?s projective and non-
projective version) were used, creating eight
parsers by varying the parsing direction, left-to-
right and right-to-left. The latter was achieved by
reversing the word order in a pre-processing step
and then restoring it in post-processing. For the fi-
nal system, feature models and training parameters
were adapted from Hall et al (2007).
2.2 Blender
The single parses were blended following the pro-
cedure of Hall et al (2007). The parses of each
sentence were combined into a weighted directed
graph. The Chu-Liu-Edmonds algorithm (Chu and
Liu, 1965; Edmonds, 1967) was then used to find
the maximum spanning tree (MST) of the graph,
which was considered the final parse of the sen-
tence. The weight of each graph edge was calcu-
lated as the sum of the weights of the correspond-
ing edges in each single parse tree.
We used a simple iterative weight updating algo-
rithm to learn the individual weights of each single
parser output and part-of-speech (PoS) using the
development set. To construct an initial MST, the
labelled attachment score was used. Each single
weight, corresponding to an edge of the hypoth-
esis tree, was then iteratively updated by slightly
increasing or decreasing the weight, depending on
whether it belonged to a correct or incorrect edge
as compared to the reference tree.
2.3 Results
The results are summarized in Table 1; the parse
with LAS weights and the best single parse
(Nivre?s arc eager algorithm with left-to-right pars-
ing direction) are also included for comparison.
3 Semantic Role Labelling
The SRL system is a pipeline with three chained
stages: predicate identification, argument identifi-
cation, and argument classification. Predicate and
argument identification are treated as binary clas-
sification problems. In a simple post-processing
predicate classification step, a predicted predicate
is assigned the most frequent sense from the train-
ing data. Argument classification is treated as a
multi-class learning problem, where the classes
correspond to the argument types.
3.1 Learning and Parameter Optimization
For learning and prediction we used the freely
available support vector machine (SVM) imple-
mentation LIBSVM (version 2.86) (Chang and
Lin, 2001). The choice of cost and kernel parame-
ter values will often significantly influence the per-
formance of the SVM classifier. We therefore im-
plemented a parameter optimizer based on the DI-
RECT optimization algorithm (Gablonsky, 2001).
It iteratively divides the search space into smaller
hyperrectangles, sampling the objective function
in the centroid of each hyperrectangle, and select-
ing those hyperrectangles that are potentially opti-
mal for further processing. The search space con-
sisted of the SVM parameters to optimize and the
objective function was the cross-validation accu-
racy reported by LIBSVM.
Tests performed during training for predicate
identification showed that the use of runtime opti-
mization of the SVM parameters for nonlinear ker-
nels yielded a higher average F
1
-score effective-
ness. Surprisingly, the best nonlinear kernels were
always outperformed by the linear kernel with de-
fault settings, which indicates that the data is ap-
proximately linearly separable.
3.2 Filtering and Data Set Splitting
To decrease the number of instances during train-
ing, all predicate and argument candidates with
PoS-tags that occur very infrequently in the
training set were filtered out. Some PoS-tags
were filtered out for all three stages, e.g. non-
alphanumerics, HYPH, SYM, and LS. This ap-
proach was effective, e.g. removing more than half
of the total number of instances for predicate pre-
diction.
To speed up the SVM training and allow for
parallelization, each data set was split into several
bins. However, there is a trade-off between speed
and accuracy. Performance consistently deterio-
rated when splitting into smaller bins. The final
system contained two variants, one with more bins
based on a combination of PoS-tags and lemma
frequency information, and one with fewer bins
249
based only on PoS-tag information. The three
learning tasks used different splits. In general, the
argument identification step was the most difficult
and therefore required a larger number of bins.
3.3 Features
We implemented a large number of features (over
50)
1
for the SRL system. Many of them can be
found in the literature, starting from Gildea and
Jurafsky (2002) and onward. All features, except
bag-of-words, take nominal values, which are bi-
narized for the vectors used as input to the SVM
classifier. Low-frequency feature values (except
for Voice, Initial Letter, Number of Words, Rela-
tive Position, and the Distance features), below a
threshold of 20 occurrences, were given a default
value.
We distinguish between single node and node
pair features. The following single node features
were used for all three learning tasks and for both
the predicate and argument node:
2
? Lemma, PoS, and Dependency relation (DepRel) for
the node itself, the parent, and the left and right sibling
? Initial Letter (upper-case/lower-case), Number of
Words, and Voice (based on simple heuristics, only for
the predicate node during argument classification)
? PoS Sequence and PoS bag-of-words (BoW) for the
node itself with children and for the parent with chil-
dren
? Lemma and PoS for the first and last child of the node
? Sequence and BoW of Lemma and PoS for content
words
? Sequence and BoW of PoS for the immediate children?s
content words
? Sequence and BoW of PoS for the parent?s content
words and for the parent?s immediate children
? Sequence and BoW of DepRels for the node itself, for
the immediate children, and for the parent?s immediate
children
All extractors of node pair features, where the pair
consists of the predicate and the argument node,
can be used both for argument identification and
argument classification. We used the following
node pair features:
? Relative Position (the argument is before/after the pred-
icate), Distance in Words, Middle Distance in DepRels
? PoS Full Path, PoS Middle Path, PoS Short Path
1
Some features were discarded for the final system based
on Information Gain, calculated using Weka (Witten and
Frank, 2005).
2
For all features using lemma or PoS the (predicted) split
value is used.
The full path feature contains the PoS-tag of the ar-
gument node, all dependency relations between the
argument node and the predicate node and finally
the PoS-tag of the predicate node. The middle path
goes to the lowest common ancestor for argument
and predicate (this is also the distance calculated
by Middle Distance in DepRels) and the short path
only contains the dependency relation of the argu-
ment and predicate nodes.
3.4 Joint Syntactic and Semantic Parsing
When considering one predicate at a time, SRL be-
comes a regular labelling problem. Given a pre-
dicted predicate, joint learning of syntactic and se-
mantic dependencies can be carried out by simulta-
neously assigning an argument label and a depen-
dency relation. This is possible because we know
a priori where to attach the argument, since there
is only one predicate candidate
3
. The MaltParser
system for English described in Hall et al (2007)
was used as a baseline, and then optimized for this
new task, focusing on feature selection.
A large feature model was constructed, and
backward selection was carried out until no fur-
ther gain could be observed. The feature model of
MaltParser consists of a number of feature types,
each describing a starting point, a path through the
structure so far, and a column of the node arrived
at. The number of feature types was reduced from
37 to 35 based on the labelled F
1
-score.
As parsing is done at the same time as argu-
ment labelling, different syntactic structures risk
being assigned to the same sentence, depending
on which predicate is currently processed. This
means that several, possibly different, parses have
to be combined into one. In this experiment, the
head and the dependency label were concatenated,
and the most frequent one was used. In case of
a tie, the first one to appear was used. The like-
lihood of the chosen labelling was also used as a
confidence measure for the syntactic blender.
3.5 Blending and Post-Processing
Combining the output from several different sys-
tems has been shown to be beneficial (Koomen
et al, 2005). For the final submission, we com-
bined the output of two variants of the pipelined
SRL system, each using different data splits, with
3
The version of the joint system used in the submission
was based on an early predicate prediction. More accurate
predicates would give a major improvement for the results.
250
Test set Pred PoS Labelled F
1
Unlabelled F
1
WSJ All 82.90 90.90
NN* 81.12 86.39
VB* 85.52 96.49
Brown All 67.48 85.49
NN* 58.34 75.35
VB* 73.24 91.97
Table 2: Semantic predicate results on the test sets.
the SRL output of the joint system. A simple uni-
form weight majority vote heuristic was used, with
no combinatorial constraints on the selected argu-
ments. For each sentence, all predicates that were
identified by a majority of the systems were se-
lected. Then, for each selected predicate, its ar-
guments were picked by majority vote (ignoring
the systems not voting for the predicate). The best
single SRL system achieved a labelled F
1
-score
of 71.34 on the WSJ test set and 57.73 on the
Brown test set, compared to 74.47 and 60.18 for
the blended system.
As a final step, we filtered out all verbal and
nominal predicates not in PropBank or NomBank,
respectively, based on the predicted PoS-tag and
lemma. Each lexicon was expanded with lemmas
from the training set, due to predicted lemma er-
rors in the training data. This turned out to be a
successful strategy for the individual systems, but
slightly detrimental for the blended system.
3.6 Results
Semantic predicate results for WSJ and Brown can
be found in Table 2. Table 4 shows the results for
identification and classification of arguments.
4 Analysis and Conclusions
In general, the mixed and blended system performs
well on all tasks, rendering a sixth place in the
CoNLL 2008 shared task. The overall scores for
the submitted system can be seen in Table 3.
4.1 Parsing
For the blended parsing system, the labelled at-
tachment score drops from 87.36 for the WSJ test
set to 80.77 for the Brown test set, while the unla-
belled attachment score only drops from 89.88 to
86.28. This shows that the system is robust with
regards to the overall syntactic structure, even if
picking the correct label is more difficult for the
out-of-domain text.
The parser has difficulties finding the right head
for punctuation and symbols. Apart from errors re-
WSJ + Brown WSJ Brown
Syn + Sem 79.79 80.92 70.49
Syn 86.63 87.36 80.77
Sem 72.94 74.47 60.18
Table 3: Syntactic and semantic scores on the test
sets for the submitted system. The scores, from top
to bottom, are labelled macro F
1
, labelled attach-
ment score and labelled F
1
.
garding punctuation, most errors occur for IN and
TO. A majority of these problems are related to as-
signing the correct dependency. This is not surpris-
ing, since these are categories that focus on form
rather than function.
There is no significant difference in score for left
and right dependencies, presumably because of the
bi-directional parsing. However, the system over-
predicts dependencies to the root. This is mainly
due to the way MaltParser handles tokens not be-
ing attached anywhere during parsing. These to-
kens are by default assigned to the root.
4.2 SRL
Similarly to the parsing results, the blended SRL
system is less robust with respect to labelled F
1
-
score, dropping from 74.47 on the WSJ test set to
60.18 on the Brown test set. The corresponding
drop in unlabelled F
1
-score is from 82.90 to 75.49.
The simple method of picking the most com-
mon sense from the training data works quite well,
but the difference in domain makes it more diffi-
cult to find the correct sense for the Brown corpus.
In the future, a predicate classification module is
needed. For the WSJ corpus, assigning the most
common predicate sense works better with nomi-
nal than with verbal predicates, while verbal pred-
icates are handled better for the Brown corpus.
In general, verbal predicate-argument structures
are handled better than nominal ones, for both
test sets. This is not surprising, since nominal
predicate-argument structures tend to vary more in
their composition.
Since we do not use global constraints for the
argument labelling (looking at the whole argument
structure for each predicate), the system can out-
put the same argument label for a predicate several
times. For the WSJ test set, for instance, the ra-
tio of repeated argument labels is 5.4% in the sys-
tem output, compared to 0.3% in the gold standard.
However, since there are no confidence scores for
predictions it is difficult to handle this in the cur-
rent system.
251
PPOSS(pred) + ARG WSJ F
1
Brown F
1
NN* + A0 61.42 38.99
NN* + A1 67.07 53.10
NN* + A2 57.02 26.19
NN* + A3 63.08 (16.67)
NN* + AM-ADV 4.65 (-)
NN* + AM-EXT 44.78 (40.00)
NN* + AM-LOC 49.45 (-)
NN* + AM-MNR 53.51 21.82
NN* + AM-NEG 79.37 (46.15)
NN* + AM-TMP 67.23 (25.00)
VB* + A0 81.72 73.58
VB* + A1 81.77 67.99
VB* + A2 60.91 50.67
VB* + A3 61.49 (14.28)
VB* + A4 77.84 (40.00)
VB* + AM-ADV 47.49 30.33
VB* + AM-CAU 55.12 (35.29)
VB* + AM-DIR 41.86 37.14
VB* + AM-DIS 71.91 37.04
VB* + AM-EXT 60.38 (-)
VB* + AM-LOC 55.69 37.50
VB* + AM-MNR 49.54 36.25
VB* + AM-MOD 94.85 82.42
VB* + AM-NEG 93.45 77.08
VB* + AM-PNC 50.00 (62.50)
VB* + AM-TMP 69.59 49.07
VB* + C-A1 70.76 55.32
VB* + R-A0 83.68 70.83
VB* + R-A1 68.87 51.43
VB* + R-AM-LOC 38.46 (25.00)
VB* + R-AM-TMP 56.82 (58.82)
Table 4: Semantic argument results on the two
test sets, showing arguments with more than 20
instances in the gold test set (fewer instances for
Brown are given in parentheses).
Acknowledgements
This project was carried out within the course Ma-
chine Learning 2, organized by GSLT (Swedish
National Graduate School of Language Tech-
nology), with additional support from NGSLT
(Nordic Graduate School of Language Technol-
ogy). We thank our supervisors Joakim Nivre,
Bj?orn Gamb?ack and Pierre Nugues for advice and
support. Computations were performed on the
BalticGrid and UPPMAX (projects p2005008 and
p2005028) resources. We thank Tore Sundqvist at
UPPMAX for technical assistance.
References
Chang, Chih-Chung and Chih-Jen Lin, 2001. LIBSVM:
A library for support vector machines.
Chu, Y. J. and T. H. Liu. 1965. On the shortest arbores-
cence of a directed graph. Science Sinica, 14:1396?
1400.
Covington, Michael A. 2001. A fundamental algo-
rithm for dependency parsing. In Proceedings of the
39th Annual Association for Computing Machinery
Southeast Conference, Athens, Georgia.
Edmonds, Jack. 1967. Optimum branchings. Jour-
nal of Research of the National Bureau of Standards,
71(B):233?240.
Gablonsky, J?org M. 2001. Modifications of the DI-
RECT algorithm. Ph.D. thesis, North Carolina State
University, Raleigh, North Carolina.
Gildea, Daniel and Daniel Jurafsky. 2002. Automatic
labeling of semantic roles. Computational Linguis-
tics, 28(3):245?288.
Hall, Johan, Jens Nilsson, Joakim Nivre, G?uls?en
Eryi?git, Be?ata Megyesi, Mattias Nilsson, and
Markus Saers. 2007. Single malt or blended?
A study in multilingual parser optimization. In
Proceedings of the CoNLL Shared Task Session of
EMNLP-CoNLL 2007, Prague, Czech Republic.
Koomen, Peter, Vasin Punyakanok, Dan Roth, and
Wen-tau Yih. 2005. Generalized inference with
multiple semantic role labeling systems. In Proceed-
ings of the Ninth Conference on Computational Nat-
ural Language Learning (CoNLL-2005), Ann Arbor,
Michigan.
Nilsson, Jens, Joakim Nivre, and Johan Hall. 2007.
Generalizing tree transformations for inductive de-
pendency parsing. In Proceedings of the 45th An-
nual Meeting of the Association for Computational
Linguistics, Prague, Czech Republic.
Nivre, Joakim, Johan Hall, Jens Nilsson, Atanas
Chanev, G?uls?en Eryi?git, Sandra K?ubler, Svetoslav
Marinov, and Erwin Marsi. 2007. Maltparser:
A language-independent system for data-driven de-
pendency parsing. Natural Language Engineering,
2(13):95?135.
Nivre, Joakim. 2004. Incrementality in deterministic
dependency parsing. In Proceedings of the ACL?04
Workshop on Incremental Parsing: Bringing Engi-
neering and Cognition Together, Barcelona, Spain.
Surdeanu, Mihai, Richard Johansson, Adam Meyers,
Llu??s M`arquez, and Joakim Nivre. 2008. The
CoNLL-2008 shared task on joint parsing of syntac-
tic and semantic dependencies. In Proceedings of
the 12th Conference on Computational Natural Lan-
guage Learning (CoNLL-2008), Manchester, Great
Britain.
Witten, Ian H. and Eibe Frank. 2005. Data mining:
Practical machine learning tools and techniques.
Morgan Kaufmann, Amsterdam, 2nd edition.
252
Proceedings of the NAACL HLT 2010 Second Louhi Workshop on Text and Data Mining of Health Documents, pages 53?60,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Characteristics and Analysis of Finnish and Swedish  Clinical Intensive Care Nursing Narratives   Helen Allvinf, Elin Carlssonf, Hercules Dalianisf, Riitta Danielsson-Ojalaa, Vidas Daudaravi?iusb, Martin Hasself, Dimitrios Kokkinakisc, Helj? Lund-gren-Lainea, Gunnar Nilssonf, ?ystein Nytr?d, Sanna Salanter?a, Maria Skeppstedtf, Hanna Suominene, Sumithra Velupillaif aDepartment of Nursing Science, University of Turku, VSSHP, Turku, Finland, bVytautas Magnus University, Lithuania, cDepartment of Swedish, University of Gothenburg, Sweden, dIDI, The Norwegian University of Science and Technology, Norway, eNICTA Canberra Research Laboratory and Australian National University, Australia fDepartment of Computer and Systems Sciences/Stockholm University Forum 100 SE-164 40 Kista, Sweden http://www.dsv.su.se/hexanord 
Abstract 
We present a comparative study of Finnish and Swedish free-text nursing narratives from intensive care. Although the two languages are linguistically very dissimilar, our hypothe-sis is that there are similarities that are impor-tant and interesting from a language technology point of view. This may have im-plications when building tools to support pro-ducing and using health care documentation. We perform a comparative qualitative analysis based on structure and content, as well as a comparative quantitative analysis on Finnish and Swedish Intensive Care Unit (ICU) nurs-ing narratives. Our findings are that ICU nurs-ing narratives in Finland and Sweden have many properties in common, but that many of these are challenging when it comes to devel-oping language technology tools. 
1 Introduction The purpose of this study1 is to do content and lexical analysis of nursing narratives written in an                                                            1 Our research on the Stockholm EPR Corpus (Dalianis et al, 2009) has been approved by Etikpr?vningsn?mnden i Stock-holm, (the Regional Vetting Board), reference number 
Intensive Care Unit (ICU). The ultimate goal of our research is to define linguistic similarities and language-specific aspects that differentiate clinical narratives in Finnish and Swedish in order to lay groundwork for developing internationally appli-cable language technology solutions and create a framework for characterising and comparing clini-cal narratives. Free text is handy for information entry but a challenge for information extraction, care handover and other uses of gathered informa-tion. Language technology can alleviate some of these problems in retrospective analysis by offer-ing a more semantically informed interpretation and abstraction. However, the most promising po-tential of language technology is to interactively improve, interpret and code during text entry so that the resulting structured, coded, free text can be validated immediately. The critical bottleneck to-day is namely information handover and reuse, and extensive text is simply not used nor is useful. In-teractively validated, semantically processed text could be more usable and support abstraction, visualization and query tools for the benefit of cli-nicians, patients, researchers and quality adminis-trators.                                                                                               2009/1742-31/5. Our research on the Finnish Corpus (Salan-ter? et al, 2009) has been approved by the Ethical committee of the Hospital District of South West Finland, reference number 2/2009, ?66. 
53
In this paper, we analyze Finnish and Swedish ICU nursing narratives from both qualitative and quantitative perspectives. Our data includes textual nursing documentation of adult patients with a protracted inpatient period. We have chosen ICUs because of their international similarity in decision making (Lauri & Salanter? 2002) and nursing documentation because it covers the entire inpa-tient period. 2 Background 
2.1 Clinical text Clinical text covers the text documents produced for clinical work by clinicians and occurs in clini-cal information systems. It is written by clinicians, that is, professionals (physicians, nurses, therapists and other specialist) responsible for patient care. Its primary purpose is to serve patient care as a summary or hand-over note. However, clinical text is also written for legal requirements, care continu-ity and purposes of reimbursement, management and research. Clinical text covers every care phase and, depending on the purpose, documents differ. Documents that describe the patient?s state, current health problems and socio-medical history are very different from those describing a care plan, its ac-tualization and evaluation of care outcomes. Again, these differ from diagnostic notes, lab results, ra-diography readings, pathology reports and dis-charge documents that plan further care at discharge.  Finally, clinical text may have been entered in ?real time?, in retrospect, or as a sum-mary, by the bedside or elsewhere. The enterer can be a clinician, secretary who transcribes a dictate, speech recognition software or another system that generates or synthesizes text, (McDonald 1997, Thoroddsen et al, 2009.).  2.2 Legal requirements for clinical documen-tation in different countries In several countries clinical documentation is based on law. In Finland, the Ministry of Social Affairs and Health (Statutes of Finland, 298/2009) defines that to ensure good care, all necessary and wide-ranging information has to be registered in patient records. In Sweden, the National Board of Health and Welfare has a similar approach (Pa-tientdatalagen 2008:355). Clinical text should be 
explicit and intelligible, and only generally well-known, accepted concepts and abbreviations are allowed to be used. It should detail adequately the patient?s conditions, care and recovery.  2.3 Special features of ICU and nursing An ICU is an essential component of most large hospitals with high quality care.  ICUs provide care for critically ill patients and focus on condi-tions that are life-threatening and require compre-hensive care and constant monitoring (Webster's 2010). This task is fairly similar universally. It is based on optional, international guidelines focus-ing on triage, admission, discharge and education. This international similarity was evident when nurses? decision making was studied in Canada, Finland, Northern Ireland, Switzerland, Norway and the USA (Lauri & Salanter? 2002); the study showed that decision making of ICU nurses was the most uniform in different countries when com-pared with nurses working in public health care, psychiatric care, and short and long term care. Clinical text written by nurses, that is, nursing narratives, both in Finland and in Sweden is based on the care process which stands for gathering in-formation from the patient, setting goals for care, implementing nursing interventions, and evaluat-ing the results of given care. In Finland, the na-tional standardized documentation model has been implemented with the Finnish care classification (assessment, interventions and outcomes of care) (Tanttu & Ikonen 2007). The Swedish VIPS model provides a structure for the documentation process with key words that reflect the nursing process (Ehrenberg et al, 1996).  ICU nursing narratives can be lengthy, espe-cially when the patient stay in the ICU is pro-longed. As much as 60 A4 pages equivalents of written text may be gathered during one period of care. However, clinicians have somewhat different opinion on how to organize the information they write. For example, headings are often inconsistent and text under headings can cover a lot of other issues than those directly concerning the given heading. (Suominen et al, 2009.)  2.4 Related studies Since most of the available clinical documents are in free-text form, a number of stylistically oriented efforts to characterize the data from various angles 
54
have taken place. This may include various topics, from viewing detailed information about specific items (e.g. readability, Kim et al, 2007) to identi-fying patterns and structures in order to provide better technology to automatically process the sublanguage (Pakhomov et al, 2006). The majority of such efforts investigate different aspects of lin-guistic features at a monolingual level, for in-stance, Hahn & Wermter (2004); Tomanek et al, (2007); Chung (2009); Harkema et al, (2009); while for a thorough review of various related is-sues see Meystre et al, (2008). In the Nordic con-text, Josefsson (1999) discusses Swedish clinical language and shows examples on how verb con-structions in a clinical setting differ from a non clinical setting.  One claim is that the physician unmarks the verb forms for agentivity when writ-ing about the patient and what actions she takes, for example, Patienten hallucinerar [The patient hallucinates] instead of the normal form  Patienten f?r hallucinationer [The patient experiences hallu-cinations].   Helles? (2005) describes nurses' general use of the language function in the nursing discharge notes. She finds that the text in the nursing dis-charge notes is information-dense and character-ized by technical terms, and that the use of standardised templates helped nurses improve the completeness, structure and content of the informa-tion. Comparisons at a monolingual level between written clinical text and lay text has been carried out by Dalianis et al, (2009). A contrastive com-putational linguistics study was carried out be-tween the Stockholm EPR Corpus (SEPR) and a general language corpus, both written Swedish text. The findings showed that SEPR contained longer words and that the vocabulary was highly domain-specific. Other work is described in Ownby, (2005). Comparing clinical text at a crosslingual level has, to our knowledge, only been done by Borin et al, (2007).  3 Analysis of Finnish and Swedish ICU nursing narratives  The analyzed nursing narratives origin from one ICU in a university-affiliated hospital both in Fin-land and Sweden. Our inclusion criterion was an ICU inpatient period of at least 5 days and patient's age of at least 16 years. The Finnish data includes nursing narratives from 514 patient records (496 
unique patients, 18 rebounds, a patient record is defined as each inpatient period of at least 5 days per patient) between January 2005 and August 2006. The Swedish data includes nursing narra-tives from 379 patient records (333 unique pa-tients, 46 rebounds) between January 2006 and May 2008. Since we did not have complete admis-sion and discharge documents from both countries, our analysis is performed on daily nursing narra-tives. These documents are written by ICU nurses during the actual inpatient period from the patient admission to the discharge.    3.1 Qualitative analysis A manual content analysis was performed by four health care professionals (i.e., three native Finnish speakers knowing Swedish, one Swedish native speaker) and one native Swedish speaking lan-guage consultant. Three average-sized patient re-cords each from Finland and Sweden were chosen for our analysis (average size 2,389 words for Fin-land and 5,169 words for Sweden). In the analysis, we considered special features (Table 1) of daily notes both from the structural and content related points of view.  The style and context of both Finnish and Swed-ish text is very similar. For health care profession-als, and especially with an ICU background, all the texts are intelligible and the meaning of a writer becomes evident from the context even in the pres-ence of numerous linguistic and grammatical mis-takes; almost all the sentences are lacking both grammatical subjects and objects. It is evident that in both countries, the narratives are written from a professional to a professional in order to support information transfer, remind about important facts, and supplement numerical data.  A feature common for all the six records is that they rarely contain any subjects or objects when nurses are writing about patients. However, in the Swedish nursing narratives the word patient is used as a subject or object much more often than in the Finnish narratives. The abbreviation pat. is mostly used for this reference and she/he is never used for this purpose. In the whole data, pat. is 40 percent more common than she/he, which is the most common personal pronoun. It seems that the word patient or pat. is used more when the profes-sionals are writing about relatives. In general, pro-nouns are used infrequently in the narratives, and  
55
  Table 1. Special structural and contextual features of Finnish and Swedish daily ICU nursing narratives. The original examples are added in ().  I very rarely. If the reader is not a health care pro-fessional, a risk for confusing the subject (i.e., the patient or nurse) arises. However, the context makes it almost always clear who is referred to.  Approximately half of the narratives do not contain any verb. The most common tense is perfect, but 
without the auxiliary has. When the meaning does not contain a subject it becomes ?unnatural? to use has. Instead, the supine form is used, for example slept, lain, and eaten. Both present and past parti- ciples without be-verb are common, for example, Breathing: Ventilator parameters unchanged. 
Special features of Finnish narratives   Special features of Swedish narratives   Structure Examples Structure Examples 
Headings are used in 2 out of 3 patient records. Headings are typi-cally used as subjects or subjects are partially used.  
Diuresis: occasionally profuse. (Diureesi: ajoittain runsasta.)  Pupils move under eyelids but does not open eyes. (Pupillit liikkuvat luomien alla, mutta ei avaa silmi??n.)  
Headings are used in all daily narra-tives. In Swedish daily narratives, the structure of headings seems to be obligatory. The headings are used typically as subjects.  
Circulation: Stable with ino-trop. (Cirkulation: Stabil med ino-tropi.)  Reacts only for pain stimula-tion during the suction of intu-bation tube. (Reagerar enbart vid sm?rt-stimuli vid sugning i tuben.) 
Present and past participles are typical but verbs of be, is and are are not used.  
Consciousness remained un-changed. (Tajunta pysynyt ennallaan.)  Blood pressure low. (Verenpaine matala.) 
Present and past participles are typical but verbs of be, is and are are not used.  
Breathing: Ventilator parame-ters unchanged. (Andning: Ventilator paramet-rarna of?r?ndrade.) 
Complete sentences are rarely used.  
No spontaneous movements, rigidifies. (Ei spontaania liikett?, j?ykiste-lee.)  Husband and daughter have been staying a long time beside the patient. (Mies ja tyt?r olleet pitk??n potilaan vierell?.) 
Complete sentences are rarely used.  
Light sedation, looks up now and then. (L?tt sederad, tittar upp ibl-and.)  She took the wedding ring and the watch home. (Hon tog med sig vigselring och klocka hem.) 
Misspellings are found but the content or meaning is still clear.  Hemodynamic ? hemodynamic (Hemodynamiikka ? henody-namiikka) Misspellings are found but the content or meaning is still clear.  
The motther is informed. (Mammman ?r informered.)  Magnesium is addded. (Magnesium har tilllsatts.) Content Examples Content  Examples 
The word patient as a subject or object is infrequently mentioned. If this word is mentioned it is not abbreviated.  
Oxidates well or ventilates well. (Happeutuu hyvin tai ventiloituu hyvin.)  
The word patient is used more often than in Finnish narratives as a subject or object. It is also replaced with abbreviations of Pat or Pt.  
Patient got a percutanous tracheostomy today. (Patienten har f?tt en perkutan trakeostomi idag.)  Very worried about patient?s condition. (Mycket oroliga ?ver patien-tens tillst?nd.)  Pt. wakes up for talking and appears to be adequate. (Pt. vakner p? tilltal och up-plevs som adekvat.) 
Signs are typically used: e.g., >, <, -->, +, -.  
The height for the drain raised from 10 --> 20 mmHg. (Dreneerausrajaa nostettu 10  --> 20 mmHg.)  Got medicine --> good response. (Sai l??kett? -->hyv? vaste.) 
Many different abbreviations are used. The origin of entire word is Swedish, English, Latin, professional or ICU typical.  
em. [eftermiddag, afternoon], HR [heart rate], VF [Ven-tricula/Fibrillation, Ventricular Fibrillation]  
56
The use of headings is frequent and good ? most of the time the content matches the headings (Tables 1 and 3). In addition, headings are used similarly in the Swedish and Finnish documents. Most of the time the headings are considered as subjects of the sentence, for example, Consciousness: Unchanged. Liquor brighter than yesterday. However, in the use of headings there are two interesting findings: If the headings are to be cho-sen freely, as in the Finnish narratives, nurses tend to use their own headings and hence many syno-nyms or closely related concepts are used; for ex-ample, hemodynamics versus blood pressure and pulse or breathing versus oxidation. If the headings are obligatory, as in the Swedish narratives, nurses tend to write their observations under the heading which is somehow closest to the subject; for exam-ple, body temperature under circulation or level of sedation under sleep. For both languages the use of different abbrevia-tions is very common. Almost every daily nursing narrative included several abbreviations. Most of the abbreviations are typical for an ICU domain: CVP [central venous pressure], PEEP [Positive End-Expiratory Pressure], EN [Enteral Nutrition], TPN [Total Parenteral Nutrition], pO2 [partial pressure of oxygen], pCO2 [partial pressure of carbon dioxide], MV [Minute Ventilation] and MAP [Mean Arterial Pressure]. From a language technology point of view this means that ICU nurs-ing narratives contain language-independent vo-cabulary. However, nurses in both countries also use many language dependent abbreviations. 
3.2 Quantitative analysis The Finnish data set (n=514) was quantitatively analyzed using the morphological analyser FinT-WOL and the disambiguator FinCG, (Lingsoft 2010), and the Swedish data set (n=379) using the GTA, Granska Text Analyzer (Knutsson et al, 2003). Both data sets are rich in terms of amount of text and vocabulary (Table 2). It is also clear that the amount of text written per day and patient varies a lot in both data sets. More complex words were spelled in numerous ways. For example, the pharmacological substance Noradrenalin had ap-proximately 350 and 60 different spellings in the Finnish and Swedish data sets, respectively. This problem is part of a more general issue of refer-
ence resolution e.g. when mapping different lexical terms referring to the same concept. In our quantitative analysis, we have included punctuation characters. In the Swedish data there was a large amount of html-tags and other format-ting characters, which has a high impact on the total number of tokens (see Table 2). Moreover, as Finnish is highly inflective, FinCG produces alter-native lemmas, hence it is possible to reduce the sparseness of the data by processing the output by choosing only one alternative lemma (see total number of types in Table 2).  To further illustrate the richness of ICU nursing language, the number of unique bigrams (e.g., ?is not?, ?oxidate well? and ?night time? (note: a mis-spelled compound) are the most common ones for Finnish) and trigrams (e.g., ?oxygeneated and ven-tilated?, ?and ventilate well? are among the most common ones for Finnish) were 368,166 (275,205 after FinCG) and 745,407 (356,307 after FinCG) for Finnish patient records. For the Swedish data, the number of unique bigrams was 469,455 (344,127 after GTA) and 1,064,944 (905,539 after GTA). Examples of common Swedish ICU bi-grams and trigrams include ?circulation stabile?, ?during night?, ?in connection with?, and ?with good effect?. Of the content of Finnish nursing narratives, 11% are verbs, 7% nouns and less than 1% pronouns. For Swedish nursing narratives, the respective percentages are 11%, 27% and 2%. One reason for the high numbers for nouns in the Swed-ish data might be due to the large amount of (obligatory) headings relative to the Finnish data (see Table 3).  To support fluent information flow, language technology is needed to strengthen referential con-gruence. Much of this richness of vocabulary is explained by abbreviations and personal differ-ences in professional jargon. In particular, abbre-viations were common. Based on the analysis of the most common words, abbreviations were rela-tively established in Swedish data. For the Finnish data, abbreviations were less standard but RR, SR, CVP, h, ad, ml, ok, vas. [vasen, left] and oik. [oikea, right] were extremely common. Thus, ref-erential congruence can be strengthened by spell-ing out the most common abbreviations automatically. Adding topical content headings is another way to support information flow. Topical content head-ings were mandatory for Swedish data, but no de-
57
fault headings for Finnish existed. However, the headings for Finnish were established in terms of content. In Table 3, we see that the headings for both languages cover similar topics, which indi-cates that the clinical information need is similar for professionals in both countries (and languages). Thus, we recommend forming a standardized set of headings from which the user can voluntarily se-lect the ones to be used. This does not exclude add-ing other headings. Another alternative is to develop language technology for topic segmenta-tion and labeling. We have promising results from this approach (see, e.g., Suominen 2009).  Temporal expressions (e.g., time, evening, night) were often used in both data sets. This poses the question of tense analysis of verbs being unneces-sary and the time-related words being enough to imply the needed temporal information. It is also interesting to note that the negations inte [not, Swe], ingen [none, Swe], ej [not, Swe] and ei [no/not, Fin] are all among the most common types, which is an important property to take into account in information extraction applications. Furthermore, words regarding the oral cavity, such as breathing and mucus, as well as relations, such as daughter, son, wife, and husband are very com-mon in both data sets.  Inspired by the tf?idf-measure from information retrieval, we also analyzed the most common words in terms of a) the number of patients in whose documents the word was used and b) the number of daily nursing narratives in which the word was used. Here, we found, in both data sets, that those words that were used for all patients as well as all daily narratives, were very similar in both data sets, and were related to the most com-mon headings, temporal expressions, negations and monitoring (e.g., increase, continue, begin).  The amount of Protected Health Information (PHI) in form of person names was equal in both of the data sets: 1.5 person names per thousand tokens. This is notable, since this has implications when it comes to integrity issues and reuse of data for research purposes. FinCG did not recognize 36% of the content of Finnish nursing narratives. However, words marked as unrecognized by FinCG also included punctuation marks. In our previous study (see Suominen 2009 and references therein), we tai-lored FinCG by extending approximately 35,000 clinical terms. The extension not only substantially 
improved the applicability of FinCG to the health domain but also initiated piloting of our language technology components in an authentic healthcare environment in the fall 2008. This lead to the re-lease of commercial language technology for Fin-nish health records (Lingsoft 2010).   Data Finnish  Swedish  Total number of patients  514  379  Total number of  tokens,  types (unique tokens) and types after processing 
 1,227,909 63,328 38,649 
 1,959,271 - 41,883 Number of tokens per patient: Minimum Maximum Average Standard deviation 
 540 14,118 2,389 1,635   
 92 36,830 5,169 5,271 Total number of  daily documents and shifts  5,915 17,103  4,700 ? Number of tokens per daily document: Minimum Maximum Average Standard deviation 
  0 915 208 87 
  5 9,389 417 239  Table 2. Comparison of Finnish and Swedish ICU data sets: total amount of text per patient. A daily document, i.e. nursing narrative, contains all text written about a given patient during a calendar day.  Finnish n ? Swedish  n = Hemodynamics  7,800  Respiratory  11,301  Consciousness  6,900  Circulation 10,630  Relatives  5,700  Elimination  10,041  Diuresis  5,400  Nutrition  8,258  Breathing  4,500  Communication  5,880  Oxygenation  3,600  Event Time  5,681  Other  3,200  Pain  4,732  Excretion  590  Psychosocial  4,682  Hemodialysis  370  Sleep  4,438  Pulse  160  Skin  4,402  Skin  160  Activity  3,794   Table 3. Comparison of Finnish and Swedish ICU data sets: the most common headings. For the Finnish data, where default headings were not given, we approxi-mated the amount of heading by using an automated heuristics followed by manual combination of headings with the same meaning. 
58
For Swedish, GTA handles unknown words dif-ferently than FinCG. However, by comparing the ICU words with a Swedish general language cor-pus (PAROLE, Gellerstam et al (2000)), we found that 69% of the types are not included in PAROLE, which indicates a need for tailoring GTA (or simi-lar tools for Swedish) with domain-specific ICU terms.  4 Conclusions The purpose of this study was to do content and lexical analysis of nursing narratives written in an ICU. Our findings are that, even though the Fin-nish and Swedish languages are not linguistically closely related, the way of writing clinical nursing ICU narratives in both countries is very similar. Moreover, the written context made sentences clear for content experts, even though the texts were full of specialized jargon, misspellings, ab-breviations, and missing subjects and objects. However, these characteristics make clinical text challenging for language technology. For example coreference resolution as in the case of noradrena-lin. We have also shown that the content characteris-tics of Finnish and Swedish ICU nursing narratives are very similar. This implies that developing tools for documentation support in ICUs is not country or language dependant in that respect. Developing such tools may improve possibilities for informa-tion extraction and text mining, enabling the possi-bilities to reuse the vast amounts of important practice-based information and evidence captured in clinical narratives. The framework we have in-troduced here could easily be employed in other studies of clinical texts. 6 Future work In the future, we will use the results of this study in developing language technology for Finnish, Swe-dish and other Nordic ICU narratives. We will study how to identify abbreviations, misspellings and normalize and correct them, by using various distance measures and concept management tech-niques. We will also study how to automatically identify important parts of text and highlight them. Furthermore, we are interested in studying text provenance and pragmatics in this particular set-ting. In addition, we will evaluate the influence of 
these technology components in clinical practice. We will also address similarities and differences in clinical text written by various professional groups or at other hospital wards and health care units. Finally, we are eager to seek possibilities to incor-porate laymen's information needs and their inter-action with health care providers into our study. Acknowledgments We would like to thank Nordforsk and the Nordic Council of Ministers for the funding of our research network HEXAnord ? HEalth teXt Analysis network in the Nordic and Baltic countries and NICTA, funded by the Australian Government as represented by the De-partment of Broadband, Communications and the Digi-tal Economy and the Australian Research Council through the ICT Centre of Excellence program. We would also like to thank the Department of Information Technology and TUCS, University of Turku, Finland. References Lars Borin, Natalia Grabar, Catalina Hallett, Davis Hardcastle, Maria Toporowska Gronostaj, Dimitrios Kokkinakis, Sandra Williams and Alistair Willis. 2007. Empowering the patient with language tech-nology. SemanticMining NoE 507505: Deliverable D27.2. <http://gup.ub.gu.se/gup/record/index.xsql?pubid=53590>  Grace Yuet-Chee Chung. 2009. Towards identifying intervention arms in randomized controlled trials: ex-tracting coordinating constructions.  Journal of Bio-medical Informatics. 42(5):790?800  Hercules Dalianis, Martin Hassel and Sumithra Velupil-lai. 2009. The Stockholm EPR Corpus - Characteris-tics and Some Initial Findings. Proceedings of ISHIMR 2009, Evaluation and implementation of e-health and health information initiatives: interna-tional perspectives. 14th International Symposium for Health Information Management Research, Kal-mar, Sweden, 14-16 October, 2009, pp 243-249, pdf. Awarded best paper.  Anna Ehrenberg, Margareta Ehnfors and Ingrid Thorell-Ekstrand. 1996. Nursing documentation in patient re-cords: experience of the use of the VIPS model. Journal of Advanced Nursing 24, 853?867.  Martin Gellerstam, Yvonne Cederholm, and Torgny Rasmark. The bank of Swedish. In: Proceedings of LREC 2000 -- The 2nd International Conference on Language Resources and Evaluation, pages 329?333, Athens, Greece.  Udo Hahn and Joachim Wermter. 2004. High-performance tagging on medical texts. Proceedings of the 20th international conference on Computa-tional Linguistics. Geneva, Switzerland.  
59
Henk Harkema, Dowling JN, Thornblade T, Chapman WW. 2009. ConText: an algorithm for determining negation, experiencer, and temporal status from clin-ical reports. Journal of Biomedical Informatics 2009;42(5):839?51.  Ragnhild Helles?. 2005. Information handling in the nursing discharge notes, Journal of Clinical Nursing, Volume 15 Issue 1, 11 - 21. Blackwell publishing  Gunl?g Josefsson. 1999. F? feber eller tempa? N?gra tankar om agentivitet i medicinskt fackspr?k, Alla tiders spr?k: en v?nskrift till Gertrud Pettersson. Pages 127. Institutionen f?r nordiska spr?k. Lund. (In Swedish)  Hyeoneui Kim, Sergey Goryachev, Craciela Rosemblat, Allen Browne, Alla Keselman and Qing Zeng-Treitler. 2007. Beyond surface characteristics: a new health text-specific readability measurement. AMIA Annual Symp. 11:418-22.  Ola Knutsson, Johnny Bigert, and Vigg Kann. 2003. A robust shallow parser for Swedish. In Proceedings 14th Nordic Conf. on Comp. Ling. NODALIDA. Sirkka Lauri and Sanna Salanter?;. 2002. Developing an instrument to measure and describe clinical decision making in different nursing fields. Journal of Profes-sional Nursing. Mar-Apr;18(2), 93-100.  Lingsoft. 2010, Lingsoft Oy, http://www.lingsoft.fi/  Clement J. McDonald. 1997. The Barriers to Electronic Medical Record Systems and How to Overcome Them. JAMIA. 1997;4:213?221.   St?phane M. Meystre, Guergana K. Savova, Karin C. Kipper-Schuler and John E. Hurdle. 2008. Extracting Information from Textual Documents in the Elec-tronic Health Record: a Review of Recent Research. Yearbook Med Inform. 2008:128-44.  Raymond L. Ownby 2005. Influence of Vocabulary and Sentence Complexity and Passive Voice on the Readability of Consumer-Oriented Mental Health In-formation on the Internet. AMIA Annual Symposium Proceedings. 2005: 585?588. Serguei V. S. Pakhomov, Anni Coden and Christopher G. Chute. 2006. Developing a corpus of clinical notes manually annotated for part-of-speech. International Journal of Medical Informatics. 2006 Jun;75(6):418-29. Epub 2005 Sep 19. Patientdatalagen (2008:355) Svensk f?rfattnings-samling, Socialdepartementet, 2008, Stockholm. (In Swedish)  Hanna Suominen. 2009. Machine Learning and Clinical Text: Supporting Health Information Flow. TUCS Dissertations No 125, Turku Centre for Computer Science, 2009, Turku, Finland.  Hanna Suominen, Helj? Lundgr?n-Laine, Sanna Salan-ter?, Helena Karsten, and Tapio Salakoski. 2009. In-formation flow in intensive care narratives. In Chen J, Chen C, Ely J, Hakkani-Tr D, He J, Hsu H.-H, Liao L, Liu C, Pop  M, Ranganathan S, Reddy C.K, 
Ruan J, Song Y, Tseng V.S, Ungar L, Wu D, Wu Z, Xu K, Yu H, Zelikovsky A, editors. Proceedings IEEE International Conference on Bioinformatics and Biomedicine Workshops, BIBM 2009, pages 325?330. Institute of Electrical and Electronics Engi-neers, Los Alamitos, California, USA.  Kaarina Tanttu and Helena Ikonen. 2007. Nationally standardized electronic nursing documentation in Finland by the year 2007. Stud Health Technol In-form.122:540-1.  Asta Thoroddsen, Kaija Saranto, Anna Ehrenberg, Wal-ter Sermeus. 2009. Models, standards and structures of nursing documentation in European countries. Stud Health Technol Inform.146:327-31.  Katrin Tomanek, Joachim Wermter and Udo Hahn. 2007. A Reappraisal of sentence and token splitting for life sciences documents. Stud Health Technol In-form. 129 (Pt 1):524-8. Webster?s 2010. Webster?s New World Medical Dic-tionary. http://www.medterms.com/script/main/hp.asp,last visited February 2, 2010. 
60
Proceedings of the Fourteenth Conference on Computational Natural Language Learning: Shared Task, pages 84?91,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Uncertainty Detection as Approximate Max-Margin Sequence Labelling
Oscar Ta?ckstro?m
SICS / Uppsala University
Kista / Uppsala, Sweden
oscar@sics.se
Gunnar Eriksson
SICS
Kista, Sweden
guer@sics.se
Sumithra Velupillai
DSV, Stockholm University
Kista, Sweden
sumithra@dsv.su.se
Hercules Dalianis
DSV, Stockholm University
Kista, Sweden
hercules@dsv.su.se
Martin Hassel
DSV, Stockholm University
Kista, Sweden
xmartin@dsv.su.se
Jussi Karlgren
SICS
Kista, Sweden
jussi@sics.se
Abstract
This paper reports experiments for the
CoNLL-2010 shared task on learning to
detect hedges and their scope in natu-
ral language text. We have addressed
the experimental tasks as supervised lin-
ear maximum margin prediction prob-
lems. For sentence level hedge detection
in the biological domain we use an L1-
regularised binary support vector machine,
while for sentence level weasel detection
in the Wikipedia domain, we use an L2-
regularised approach. We model the in-
sentence uncertainty cue and scope de-
tection task as an L2-regularised approxi-
mate maximum margin sequence labelling
problem, using the BIO-encoding. In ad-
dition to surface level features, we use a
variety of linguistic features based on a
functional dependency analysis. A greedy
forward selection strategy is used in ex-
ploring the large set of potential features.
Our official results for Task 1 for the bio-
logical domain are 85.2 F1-score, for the
Wikipedia set 55.4 F1-score. For Task 2,
our official results are 2.1 for the entire
task with a score of 62.5 for cue detec-
tion. After resolving errors and final bugs,
our final results are for Task 1, biologi-
cal: 86.0, Wikipedia: 58.2; Task 2, scopes:
39.6 and cues: 78.5.
1 Introduction
This paper reports experiments to detect uncer-
tainty in text. The experiments are part of the two
shared tasks given by CoNLL-2010 (Farkas et al,
2010). The first task is to identify uncertain sen-
tences; the second task is to detect the cue phrase
which makes the sentence uncertain and to mark
its scope or span in the sentence.
Uncertainty as a target category needs to be ad-
dressed with some care. Sentences, utterances,
statements are not uncertain ? their producer, the
speaker or author, is. Statements may explicitly
indicate this uncertainty, employing several differ-
ent linguistic and textual mechanisms to encode
the speaker?s attitude with respect to the verac-
ity of an utterance. The absence of such markers
does not necessarily indicate certainty ? the oppo-
sition between certain and uncertain is not clearly
demarkable, but more of a dimensional measure.
Uncertainty on the part of the speaker may be dif-
ficult to differentiate from a certain assessment of
an uncertain situation, It is unclear whether this
specimen is an X or a Y vs. The difference between
X and Y is unclear.
In this task, the basis for identifying uncertainty
in utterances is almost entirely lexical. Hedges,
the main target of this experiment, are an estab-
lished category in lexical grammar analyses - see
e.g. Quirk et al (1985), for examples of English
language constructions. Most languages use vari-
ous verbal markers or modifiers for indicating the
speaker?s beliefs in what is being said, most proto-
typically using conditional or optative verb forms,
Six Parisiens seraient morts, or auxiliaries, This
mushroom may be edible, but aspectual markers
may also be recruited for this purpose, more indi-
rectly, I?m hoping you will help vs. I hope you will
help; Do you want to see me now vs. Did you want
to see me now. Besides verbs, there are classes
of terms that through their presence, typically in
an adverbial role, in an utterance make explicit
its tentativeness: possibly, perhaps... and more
complex constructions with some reservation, es-
pecially such that explicitly mention the speaker
and the speaker?s beliefs or doubts, I suspect that
X.
Weasels, the other target of this experiment,
on the other hand, do not indicate uncertainty.
84
Weasels are employed when speakers attempt to
convince the listener of something they most likely
are certain of themselves, by anchoring the truth-
fulness of the utterance to some outside fact or au-
thority (Most linguists believe in the existence of
an autonomous linguistic processing component),
but where the authority in question is so unspecific
as not to be verifiable when scrutinised.
We address both CoNLL-2010 shared tasks
(Farkas et al, 2010). The first, detecting uncer-
tain information on a sentence level, we solve by
using an L1-regularised support vector machine
with hinge loss for the biological domain, and
an L2-regularised maximum margin model for the
Wikipedia domain. The second task, resolution of
in-sentence scopes of hedge cues, we approach as
an approximate L2-regularized maximum margin
structured prediction problem. Our official results
for Task 1 for the biological domain are 85.2 F1-
score, for the Wikipedia set 55.4 F1-score. For
Task 2, our official results were 2.1 for the entire
task with a score of 62.5 for cue detection. After
resolving errors and unfortunate bugs, our final re-
sults are for Task 1, biological: 86.0, Wikipedia:
58.2; Task 2: 39.6 and 78.5 for cues.
2 Detecting Sentence Level Uncertainty
On the sentence level, word- and lemma-based
features have been shown to be useful for uncer-
tainty detection (see e.g. Light et al (2004), Med-
lock and Briscoe (2007), Medlock (2008), and
Szarvas (2008)). Medlock (2008) and Szarvas
(2008) employ probabilistic, weakly supervised
methods, where in the former, a stemmed single
term and bigram representation achieved best re-
sults (0.82 BEP), and in the latter, a more complex
n-gram feature selection procedure was applied
using a Maximum Entropy classifier, achieving
best results when adding reliable keywords from
an external hedge keyword dictionary (0.85 BEP,
85.08 F1-score on biomedical articles). More lin-
guistically motivated features are used by Kil-
icoglu and Bergler (2008), such as negated ?un-
hedging? verbs and nouns and that preceded by
epistemic verbs and nouns. On the fruit-fly dataset
(Medlock and Briscoe, 2007) they achieve 0.85
BEP, and on the BMC dataset (Szarvas, 2008) they
achieve 0.82 BEP. Light et al (2004) also found
that most of the uncertain sentences appeared to-
wards the end of the abstract, indicating that the
position of an uncertain sentence might be a use-
ful feature.
Ganter and Strube (2009) consider weasel tags
in Wikipedia articles as hedge cues, and achieve
results of 0.70 BEP using word- and distance
based features on a test set automatically derived
from Wikipedia, and 0.69 BEP on a manually an-
notated test set using syntactic patterns as fea-
tures. These results suggest that syntactic features
are useful for identifying weasels that ought to be
tagged. However, evaluation is performed on bal-
anced test sets, which gives a higher baseline.
2.1 Learning and Optimization Framework
A guiding principle in our approach to this shared
task has been to focus on highly computationally
efficient models, both in terms of training and pre-
diction times. Although kernel based non-linear
separators may sometimes obtain better predic-
tion performance, compared to linear models, the
speed penalty at prediction time is often substan-
tial, since the number of support patterns often
grows linearly with the size of the training set. We
therefore restrict ourselves to linear models, but
allow for a restricted family of explicit non-linear
mappings by feature combinations.
For sentence level hedge detection in the bio-
logical domain, we employ an L1-regularised sup-
port vector machine with hinge loss, as provided
by the library implemented by Fan et al (2008),
while for weasel detection in the Wikipedia do-
main, we instead use the L2-regularised maximum
margin model described in more detail in section
3.1. In both cases, we approximately optimise the
F1-measure by weighting each class by the inverse
of its proportion in the training data.
The reason for using L1-regularisation in the bi-
ological domain is that the annotation is heavily
biased towards a rather small number of lexical
cues, making most of the potential surface features
irrelevant. The Wikipedia weasel annotation, on
the other hand, is much more noisy and less de-
termined by specific lexical markers. Regularising
with respect to the L1-norm is known to give pref-
erence to sparse models and for the special case
of logistic regression, Ng (2004) proved that the
sample complexity grows only logarithmically in
the number of irrelevant features, instead of lin-
early as when regularising with respect to the L2-
norm. Our preliminary experiments indicated that
L1-regularisation is superior to L2-regularisation
in the biological domain, while slightly inferior in
85
the Wikipedia domain.
2.2 Feature Definitions
The asymmetric relationship between certain and
uncertain sentences becomes evident when one
tries to learn this distinction based on surface level
cues. While the UNCERTAIN category is to a large
extent explicitly anchored in lexical markers, the
CERTAIN category is more or less defined implic-
itly as the complement of the UNCERTAIN cate-
gory. To handle this situation, we use a bias fea-
ture to model the weight of the CERTAIN category,
while explicit features are used to model the UN-
CERTAIN category.
The following list describes the feature tem-
plates explored for sentence level uncertainty de-
tection. Some features are based on a linguistic
analysis by the Connexor Functional Dependency
(FDG) parser (Tapanainen and Ja?rvinen, 1997).
SENLEN Preliminary experiments indicated that taking sen-
tence length into account is beneficial. We incorporate
this by using three different bias terms, according to the
length (in tokens) of the sentences. This feature takes
the following values: S < 18 ? M ? 32 < L.
DOCPT Document part, e.g., TITLE, ABSTRACT and BODY
TEXT, allowing for different models for different docu-
ment parts.
TOKEN, LEMMA Tokens in most cases equals words, but
may in some special cases also be multiword units, e.g.
of course, as defined by the FDG tokenisation. Lemmas
are base forms of words, with some special features
introduced for numeric tokens, e.g., year, short number,
and long number.
QUANT Syntactic function of a noun phrase with a quanti-
fier head (at least some of the isoforms are conserved
between mouse and humans), or a modifying quantifier
(Recently, many investigators have been interested in
the study on eosinophil biology).
HEAD, DEPREL Functional dependency head of the token,
and the type of dependency relation between the head
and the token, respectively.
SYN Phrase-level and clause-level syntactic functions of a
word.
MORPH Part-of-speech and morphological traits of a word.
Each feature template defines a set of features
when applied to data. The TOKEN, LEMMA,
QUANT, HEAD, DEPREL templates yield single-
ton sets of features for each token, while the SYN
and MORPH templates extends to sets consisting
of several features for each token. A sentence is
represented as the union of all active token level
features and the SENLEN and DOCPT, if active.
In addition to the linear combination of concrete
features, we allow combined features by the Carte-
sian product of the feature set extensions of two or
more feature templates.
2.3 Feature Template Selection
Although regularised maximum margin models
often cope well even in the presence of irrelevant
features, it is a good idea to search the large set of
potential features for an optimal subset.
In order to make this search feasible we make
two simplifications. First, we do not explore the
full set of individual features, but instead the set of
feature templates, as defined above. Second, we
perform a greedy search in which we iteratively
add the feature template that gives the largest per-
formance improvement, when added to the cur-
rent optimal set of templates. The performance of
a feature set for sentence level detection is mea-
sured as the mean F1-score, with respect to the
UNCERTAIN class, minus one standard deviation
? the mean and standard deviation are computed
by three fold cross-validation on the training set.
We subtract one standard deviation from the mean
in order to promote stable solutions over unstable
ones.
Of course, these simplifications do not come for
free. The solution of the optimisation problem
might be quite unstable with respect to the optimal
hyper-parameters of the learning algorithm, which
in turn may depend on the feature set used. This
risk could be reduced by conducting a more thor-
ough parameter search for each candidate feature
set, however, this was simply too time consuming
for the present work. A further risk of using for-
ward selection is that feature interactions are ig-
nored. This issue is handled better with backward
elimination, but that is also more time consuming.
The full set of explored feature templates is too
large to be listed here; instead we list the features
selected in each iteration of the search, together
with their corresponding scores, in Table 1.
3 Detecting In-sentence Uncertainty
When it comes to the automatic identification of
hedge cues and their linguistic scopes, Morante
and Daelemans (2009) and O?zgu?r and Radev
(2009) report experiments on the BioScope cor-
pus (Vincze et al, 2008), achieving best results
(10-fold cross evaluation) on the identification of
hedge cues of 71.59 F-score (using IGTree with
current, preceding and subsequent word and cur-
86
Task Template set Dev F1 Test F1
Bio
SENLEN - -
? LEMMA 88.9 (.25) 78.79
? LEMMABI 90.3 (.19) 85.86
? LEMMA?QUANT 90.3 (.07) 85.97
Wiki
SENLEN - -
? TOKEN?DOCPT 59.0 (.76) 60.12
? TOKENBI?SENLEN 59.9 (.09) 58.26
Table 1: Top feature templates for sentence level
hedge and weasel detection.
rent lemma as features) and 82.82 F-score (using a
Support Vector Machine classifier and a complex
feature set including keyword and dependency re-
lation information), respectively. On the task of
automatic scope resolution, best results are re-
ported as 59.66 (F-score) and 61.13 (accuracy),
respectively, on the full paper subset. O?zgu?r and
Radev (2009) use a rule-based method for this sub-
task, while Morante and Daelemans (2009) use
three different classifiers as input to a CRF-based
meta-learner, with a complex set of features, in-
cluding hedge cue information, current and sur-
rounding token information, distance information
and location information.
3.1 Learning and Optimisation Framework
In recent years, a wide range of different ap-
proaches to general structured prediction prob-
lems, of which sequence labelling is a special
case, have been suggested. Among others, Con-
ditional Random Fields (Lafferty et al, 2001),
Max-Margin Markov Networks (Taskar et al,
2003), and Structured Support Vector Machines
(Tsochantaridis et al, 2005). A drawback of
these approaches is that they are all quite com-
putationally demanding. As an alternative, we
propose a much more computationally lenient ap-
proach based on the regularised margin-rescaling
formulation of Taskar et al (2003), which we in-
stead optimise by stochastic subgradient descent
as suggested by Ratliff et al (2007). In addi-
tion we only perform approximate decoding, us-
ing beam search, which allows arbitrary complex
joint feature maps to be employed, without sacri-
ficing speed.
3.1.1 Technical Details
Let X denote the pattern set and let Y denote the
set of structured labels. Let A denote the set of
atomic labels and let each label y ? Y consist of
an indexed sequence of atomic labels yi ? A. De-
note by Yx ? Y the set of possible label assign-
ments to pattern x ? X and by yx ? Yx its cor-
rect label. In the specific case of BIO-sequence
labelling, A = {BEGIN, INSIDE, OUTSIDE} and
Yx = A|x|, where |x| is the length of the sequence
x ? X .
A structured classification problem amounts
to learning a mapping from patterns to labels,
f : X 7? Y , such that the expected loss
EX?Y [?(yx, f(x))] is minimised. The prediction
loss, ? : Y ? Y 7? <+, measures the loss of
predicting label y = f(x) when the correct la-
bel is yx, with ?(yx, yx) = 0. Here we assume
the Hamming loss, ?H(y, y?) = ?|y|i=1 ?(yi, y?i),
where ?(yi, y?i) = 1 if yi 6= y?i and 0 otherwise.
The idea of the margin-rescaling approach is to
let the structured margin between the correct label
yx and a hypothesis y ? Yx scale linearly with the
prediction loss ?(yx, y) (Taskar et al, 2003). The
structured margin is defined in terms of a score
function S : X ? Y 7? <, in our case the linear
score function S(x, y) = wT?(x, y), where w ?
<m is a vector of parameters and? : X?Y 7? <m
is a joint feature function. The learning problem
then amounts to finding parameters w such that
S(x, yx) ? S(x, y) + ?(yx, y) for all y ? Yx \
{yx} over the training data D. In other words, we
want the score of the correct label to be higher than
the score plus the loss, of all other labels, for each
instance. In order to balance margin maximisation
and margin violation, we add theL2-regularisation
term ?w?2.
By making use of the loss augmented decoding
function
f?(x, yx) = argmax
y?Yx
[S(x, y) + ?(yx, y)] , (1)
we get the following regularised risk functional:
Q?,D(w) =
|D|?
i=1
S?(x(i), yx(i)) + ?2 ?w?
2, (2)
where
S?(x, yx) = maxy?Yx [S(x, y) + ?(yx, y)]?S(x, yx)
(3)
We optimise (2) by stochastic approximate subgra-
dient descent with step size sequence [?0/?t]?t=1
(Ratliff et al, 2007). The initial step size ?0
and the regularisation factor ? are data depen-
dent hyper-parameters, which we tune by cross-
validation.
87
This framework is highly efficient both at learn-
ing and prediction time. Training cues and scopes
on the biological data, takes about a minute, while
prediction times are in the order of seconds, using
a Java based implementation on a standard laptop;
the absolute majority of that time is spent on read-
ing and extracting features from an inefficient in-
ternal JSON-based format.
3.1.2 Hashed Feature Functions
Joint feature functions enable encoding of depen-
dencies between labels and relations between pat-
tern and label. Most feature templates are de-
fined based on input only, while some are de-
fined with respect to output features as well. Let
?(x, y1:i?1, i) ? <m denote the joint feature func-
tion corresponding to the application of all active
feature templates to pattern x ? X and partially
decoded label y1:i?1 ? Ai?1 when decoding at
position i. The feature mapping used in scoring
candidate label yi ? A is then computed as the
Cartesian product ?(x, y, i) = ?(x, y1:i?1, i) ?
?(yi), where ?(yi) ? <m is a unique unitary fea-
ture vector representation of label yi. The feature
representation for a complete sequence x and its
associated label y is then computed as
?(x, y) =
|x|?
i=1
?(x, y, i)
When employing joint feature functions and com-
bined features, the number of unique features may
grow very large. This is a problem when the
amount of internal memory is limited. Feature
hashing, as described by Weinberger et al (2009),
is a simple trick to circumvent this problem. As-
sume that we have an original feature function
? : X ? Y 7? <m, where m might be arbitrar-
ily large. Let h : N+ 7? [1, n] be a hash function
and let h?1(i) ? [1,m] be the set of integers such
that j ? h?1(i) iff h(j) = i. We now use this
hash function to map the index of each feature in
?(x, y) to its corresponding index in ?(x, y), as
?i(x, y) =?j?h?1(i) ?j(x, y). The features in ?
are thus unions of multisets of features in ?. Given
a hash function with good collision properties, we
can expect that the subset of features mapped to
any index in?(x, y) is small and composed of ele-
ments drawn at random from ?(x, y). Weinberger
et al (2009) contains proofs of bounds on these
distributions. Furthermore, by using a k-valued
hash function h : Nk 7? [1, n], the Cartesian prod-
uct of k feature sets can be computed much more
efficiently, compared to using a dictionary.
3.2 Position Based Feature Definitions
For in-sentence cue and scope prediction we make
use of the same token level feature templates as
for sentence level detection. An additional level
of expressivity is added in that each token level
template is associated with a token position. A
template is addressed either relative to the token
currently being decoded, or by the dependency arc
of a token, which in turn is addressed by a relative
position. The addressing can be either to a single
position, or a range of positions. Feature templates
may further be defined with respect to features of
the input pattern, the token level labels predicted
so far, or with respect to combinations of input
and label features. Joint features, just as complex
feature combinations, are created by forming the
Cartesian product of an input feature set and a la-
bel feature set.
The feature templates are instantiated by pre-
fixing the template name to each member of the
feature set. To exemplify, the single position tem-
plate TOKENi, given that the token currently be-
ing decoded at position i is suggests, is instanti-
ated as the singleton set {TOKENi = suggests}.
The range template TOKENi,i+1, given that the
current token is suggests and the next token is
that, is instantiated as the set {TOKENi,i+1 =
suggests, TOKENi,i+1 = that}; i.e. each member
of the set is prefixed by the range template name.
In addition to the token level templates used for
sentence level prediction, the following templates
were explored:
LABEL Label predicted so far at the addressed position(s).
HEAD.X An arbitrary feature, X, addressed by follow-
ing the dependency arc(s) from the addressed posi-
tion(s). For example, HEAD.LEMMAi corresponds to
the lemma found by looking at the dependency head of
the current token.
CUE, CUESCOPE Whether the token(s) addressed is re-
spectively, a cue marker, or within the syntactic scope
of the current cue, following the definition of scope
provided by Vincze et al (2008).
3.3 Feature Template Selection
Just as with sentence level detection, we used a
greedy forward selection strategy when searching
for the optimal subset of feature templates. The
cue and scope detection subtasks were optimised
separately.
88
The scoring measures used in the search for
cue and scope detection features differ. In order
to match the official scoring measure for cue de-
tection, we optimise the F1-score of labels cor-
responding to cue tags, i.e. we treat the BEGIN
and INSIDE cue tags as an equivalence class. The
official scoring measure for scope prediction, on
the other hand, corresponds to the exact match
of scope boundaries. Unfortunately using exact
match performance turned out to be not very well
suited for use in greedy forward selection. This
is because before a sufficient per token accuracy
has been reached, and even when it has, the ex-
act match score may fluctuate wildly. Therefore,
as a substitute, we instead guide the search by to-
ken level accuracy. This discrepancy between the
search criterion and the official scoring metric is
unfortunate.
Again, when taking into account position ad-
dressing, joint features and combined features, the
complete set of explored templates is too large to
fit in the current experiment. The selected features
together with their corresponding scores are found
in Table 2.
Task Template set Dev F1 Test F1
Cue
TOKENi 74.0 (1.5) -
? TOKENi?1 81.0 (.30) 68.78
? MORPHi 83.6 (.10) 74.06
? LEMMAi ? LEMMAi+1 85.6 (.20) 78.41
? SYNi 86.5 (.41) 78.28
? LEMMAi?1 ? LEMMAi 86.7 (.42) 78.52
Scope
CueScopei 66.9 (.92) -
? LABELi?2,i?1 79.5 (.67) 34.80
? LEMMAi 82.4 (1.1) 33.18
? MORPHi 83.1 (.35) 35.70
? CUEi?2,i?1 83.4 (.13) 40.14
? CUEi,i+1,i+2 83.6 (.11) 41.15
? LEMMAi?1 84.1 (.16) 40.04
? MORPHi 84.4 (.33) 40.04
? TOKENi+1 84.5 (.09) 39.64
Table 2: Top feature templates for in-sentence de-
tection of hedge cues and scopes.
4 Discussion
Our final F1-score results for the corrected system
are, in Task 1 for the biological domain 85.97, for
the Wikipedia domain 58.25; for Task 2, our re-
sults are 39.64 for the entire task with a score of
78.52 for cue detection.
Any gold standard-based shared experiment un-
avoidably invites discussion on the reliability of
the gold standard. It is easy to find borderline ex-
amples in the evaluation corpus, e.g. sentences
that may just as well be labeled ?certain? rather
than ?uncertain?. This gives an indication of the
true complexity of assessing the hidden variable of
uncertainty and coercing it to a binary judgment
rather than a dimensional one. It is unlikely that
everyone will agree on a binary judgment every
time.
To improve experimental results and the gen-
eralisability of the results for the task of detect-
ing uncertain information on a sentence level, we
would need to break reliance on the purely lexical
cues. For instance, we now have identified possi-
ble and putative as markers for uncertainty, but in
many instances they are not (Finally, we wish to
ensure that others can use and evaluate the GREC
as simply as possible). This would be avoidable
through either a deeper analysis of the sentence
to note that possible in this case does not modify
anything of substance in the sentence, or alterna-
tively through a multi-word term preprocessor to
identify as simply as possible as an analysis unit.
In the Wikipedia experiment, where the objec-
tive is to identify weasel phrases, the judicious en-
coding of quantifiers such as ?some of the most
well-known researchers say that X? would be
likely to identify the sought-for sentences when
the quantified NP is in subject position. In our
experiment we find that our dependency analysis
did not distinguish between the various syntactic
roles of quantified NPs. As a result, we marked
several sentences with a quantifier as a ?weasel?
sentence, even where the quantified NP was in a
non-subject role ? leading to overly many weasel
sentences. An example is given in Table 3.
If certainty can be identified separately, not as
absence of overt uncertainty, identifying uncer-
tainty can potentially be aided through the iden-
tification of explicit certainty together with nega-
tion, as found by Kilicoglu and Bergler (2008). In
keeping with their results, we found negations in a
sizeable proportion of the annotated training mate-
rial. Currently we capture negation as a lexical cue
in immediate bigrams, but with longer range nega-
tions, we will miss some clear cases: Table 3 gives
two examples. To avoid these misses, we will both
need to identify overt expressions of certainty and
to identify and track the scope of negation ? the
first challenge is unexplored but would not seem
to be overly complex; the second is a well-known
89
and established challenge for NLP systems in gen-
eral.
In the task of detecting in-sentence uncertainty
? identification of hedge cues and their scopes ?
we find that an evaluation method based on ex-
act match of a token sequence is overly unforgiv-
ing. There are many cases where the marginal to-
kens of a sequence are less than central or irrele-
vant for the understanding of the hedge cue and its
scope: moving the boundary by one position over
an uninteresting token may completely invalidate
an otherwise arguably correct analysis. A token-
by-token scoring would be a more functional eval-
uation criterion, or perhaps a fuzzy match, allow-
ing for a certain amount of erroneous characters.
For our experiments, this has posed some chal-
lenges. While we model the in-sentence un-
certainty detection as a sequence labelling prob-
lem in the BIO-representation (BEGIN, INSIDE,
OUTSIDE), the provided corpus uses an XML-
representation. Moreover, the official scoring tool
requires that the predictions are well formed XML,
necessitating a conversion from XML to BIO prior
to training and from BIO to XML after prediction.
Consistent tokenisation is important, but the syn-
tactic analysis components used by us distorted the
original tokenisation and restoring the exact same
token sequence proved problematic.
Conversion from BIO to XML is straightforward
for cues, while some care must be taken when an-
notating scopes, since erroneous scope predictions
may result in malformed XML. When adding the
scope annotation, we use a stack based algorithm.
For each sentence, we simultaneously traverse the
scope-sequence corresponding to each cue, left to
right, token by token. The stack is used to en-
sure that scopes are either separated or nested and
an additional restriction ensures that scopes may
never start or end inside a cue. In case the al-
gorithm fails to place a scope according to these
restrictions, we fall back and let the scope cover
the whole sentence. Several of the more frequent
errors in our analyses are scoping errors, many
likely to do with the fallback solution. Our analy-
sis quite frequently fails also to assign the subject
of a sentence to the scope of a hedging verb. Ta-
ble 3 shows one example each of these errors ?
overextended scope and missing subject.
Unfortunately, the tokenisation output by our
analysis components is not always consistent with
the tokenisation assumed by the BioScope annota-
tion. A post-processing step was therefore added
in which each, possibly complex, token in the pre-
dicted BIO-sequence is heuristically mapped to its
corresponding position in the XML structure. This
post-processing is not perfect and scopes and cues
at non-word token boundaries, such as parenthe-
ses, are quite often misplaced with respect to the
BioScope annotation. Table 3 gives one example
which is scored ?erroneous? since the token ?(63)?
is in scope, where the ?correct? solution has it out-
side the scope. These errors are not important to
address, but are quite frequent in our results ? ap-
proximately 80 errors are of this type.
To achieve more general and effective methods
to detect uncertainty in an argument, we should
note that uncertainty is signalled in a text through
many mechanisms, and that the purely lexical and
explicit signal found through the present experi-
ments in hedge identification is effective and use-
ful, but will not catch everything we might want to
find. Lexical approaches are also domain depen-
dent. For instance, Szarvas (2008) and Morante
and Daelemans (2009) report loss in performance,
when applying the same methods developed on bi-
ological data, on clinical text. Using the systems
developed for scientific text elsewhere poses a mi-
gration challenge. It would be desirable both to
automatically learn a hedging lexicon from a gen-
eral seed set and to have features on a higher level
of abstraction.
Our main result is that casting this task as a se-
quence labelling problem affords us the possibility
to combine linguistic analyses with a highly effi-
cient implementation of a max-margin prediction
algorithm. Our framework processes the data sets
in minutes for training and seconds for prediction
on a standard personal computer.
5 Acknowledgements
The authors would like to thank Joakim Nivre
for feedback in earlier stages of this work. This
work was funded by The Swedish National Grad-
uate School of Language Technology and by the
Swedish Research Council.
References
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui
Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A library
for large linear classification. Journal of Machine Learn-
ing Research, 9:1871?1874.
Richa?rd Farkas, Veronika Vincze, Gyo?rgy Mo?ra, Ja?nos
90
Neg + certain However, how IFN-? and IL-4 inhibit IL-17 production is not yet known.
Neg + certain The mechanism by which Tregs preserve peripheral tolerance is still not entirely clear.
?some?: not weasel Tourist folks usually visit this peaceful paradise to enjoy some leisurenonsubj .
?some?: weasel Somesubj suggest that the origin of music likely stems from naturally occurring sounds and rhythms.
Prediction dRas85DV12 <xcope .1><cue .1>may</cue> be more potent than dEGFR? because
dRas85DV12 can activate endogenous PI3K signaling [16]</xcope>.
Gold standard dRas85DV12 <xcope .1><cue .1>may</cue> be more potent than dEGFR?</xcope> because
dRas85DV12 can activate endogenous PI3K signaling [16].
Prediction However, the precise molecular mechanisms of Stat3-mediated expression of ROR?t
<xcope .1>are still <cue .1>unclear</cue></xcope>.
Gold standard However, <xcope .1>the precise molecular mechanisms of Stat3-mediated expression of ROR?t
are still <cue .1>unclear</cue></xcope>.
Prediction Interestingly, Foxp3 <xcope .1><cue .1>may</cue> inhibit ROR?t
activity on its target genes, at least in par,t through direct interaction with ROR?t (63)</xcope>.
Gold standard Interestingly, Foxp3 <xcope .1><cue .1>may</cue> inhibit RORt
activity on its target genes, at least in par,t through direct interaction with RORt</xcope> (63).
Table 3: Examples of erroneous analyses.
Csirik, and Gyo?rgy Szarvas. 2010. The CoNLL-2010
Shared Task: Learning to Detect Hedges and their Scope
in Natural Language Text. In Proceedings of the 14th
Conference on Computational Natural Language Learn-
ing (CoNLL-2010): Shared Task, pages 1?12, Uppsala,
Sweden, July. Association for Computational Linguistics.
Viola Ganter and Michael Strube. 2009. Finding hedges
by chasing weasels: hedge detection using Wikipedia tags
and shallow linguistic features. In ACL-IJCNLP ?09: Pro-
ceedings of the ACL-IJCNLP 2009 Conference Short Pa-
pers, Morristown, NJ, USA. Association for Computa-
tional Linguistics.
Halil Kilicoglu and Sabine Bergler. 2008. Recognizing spec-
ulative language in biomedical research articles: a linguis-
tically motivated perspective. BMC Bioinformatics, 9.
John D. Lafferty, Andrew McCallum, and Fernando C. N.
Pereira. 2001. Conditional random fields: Probabilis-
tic models for segmenting and labeling sequence data. In
Proc. 18th Int. Conf. on Machine Learning. Morgan Kauf-
mann Publishers.
Marc Light, Xin Ying Qiu, and Padmini Srinivasan. 2004.
The language of bioscience: Facts, speculations, and state-
ments in between. In Lynette Hirschman and James
Pustejovsky, editors, HLT-NAACL 2004 Workshop: Bi-
oLINK 2004, Linking Biological Literature, Ontologies
and Databases, Boston, USA. ACL.
Ben Medlock and Ted Briscoe. 2007. Weakly supervised
learning for hedge classification in scientific literature. In
Proceedings of the 45th Annual Meeting of the Associa-
tion of Computational Linguistics, Prague, Czech Repub-
lic. Association for Computational Linguistics.
Ben Medlock. 2008. Exploring hedge identification in
biomedical literature. Journal of Biomedical Informatics,
41(4):636?654.
Roser Morante and Walter Daelemans. 2009. Learning the
scope of hedge cues in biomedical texts. In BioNLP ?09:
Proceedings of Workshop on BioNLP, Morristown, NJ,
USA. ACL.
Andrew Y. Ng. 2004. Feature selection, l1 vs. l2 regulariza-
tion, and rotational invariance. In ICML ?04: Proceedings
of the 21st International Conference on Machine learning,
page 78, New York, NY, USA. ACM.
Arzucan O?zgu?r and Dragomir R. Radev. 2009. Detecting
speculations and their scopes in scientific text. In Pro-
ceedings of 2009 Conference on Empirical Methods in
Natural Language Processing, Singapore. ACL.
Randolph Quirk, Sidney Greenbaum, Geoffrey Leech, and
Jan Svartvik. 1985. A comprehensive grammar of the
English language. Longman.
Nathan D. Ratliff, Andrew J. Bagnell, and Martin A. Zinke-
vich. 2007. (Online) subgradient methods for structured
prediction. In Eleventh International Conference on Arti-
ficial Intelligence and Statistics (AIStats).
Gyo?rgy Szarvas. 2008. Hedge classification in biomedical
texts with a weakly supervised selection of keywords. In
Proceedings of ACL-08: HLT, Columbus, Ohio. ACL.
Pasi Tapanainen and Timo Ja?rvinen. 1997. A non-projective
dependency parser. In Proceedings of the 5th Conference
on Applied Natural Language Processing.
Benjamin Taskar, Carlos Guestrin, and Daphne Koller. 2003.
Max-margin Markov networks. In Sebastian Thrun,
Lawrence K. Saul, and Bernhard Scho?lkopf, editors,
NIPS. MIT Press.
Ioannis Tsochantaridis, Thorsten Joachims, Thomas Hof-
mann, and Yasemin Altun. 2005. Large margin methods
for structured and interdependent output variables. Jour-
nal of Machine Learning Research, 6:1453?1484.
Veronika Vincze, Gyo?rgy Szarvas, Richa?rd Farkas, Gyo?rgy
Mo?ra, and Ja?nos Csirik. 2008. The BioScope corpus:
biomedical texts annotated for uncertainty, negation and
their scopes. BMC Bioinformatics, 9(S-11).
Kilian Weinberger, Anirban Dasgupta, John Langford, Alex
Smola, and Josh Attenberg. 2009. Feature hashing for
large scale multitask learning. In ICML ?09: Proceedings
of the 26th Annual International Conference on Machine
Learning, New York, NY, USA. ACM.
91
Proceedings of the Workshop on Negation and Speculation in Natural Language Processing, pages 14?22,
Uppsala, July 2010.
Towards A Better Understanding of
Uncertainties and Speculations in Swedish Clinical Text
? Analysis of an Initial Annotation Trial
Sumithra Velupillai
Department of Computer and Systems Sciences (DSV)
Stockholm University
Forum 100
SE-164 40 Kista, Sweden
sumithra@dsv.su.se
Abstract
Electronic Health Records (EHRs) contain
a large amount of free text documentation
which is potentially very useful for Infor-
mation Retrieval and Text Mining appli-
cations. We have, in an initial annotation
trial, annotated 6 739 sentences randomly
extracted from a corpus of Swedish EHRs
for sentence level (un)certainty, and token
level speculative keywords and negations.
This set is split into different clinical prac-
tices and analyzed by means of descrip-
tive statistics and pairwise Inter-Annotator
Agreement (IAA) measured by F
1
-score.
We identify geriatrics as a clinical prac-
tice with a low average amount of uncer-
tain sentences and a high average IAA,
and neurology with a high average amount
of uncertain sentences. Speculative words
are often n-grams, and uncertain sentences
longer than average. The results of this
analysis is to be used in the creation of a
new annotated corpus where we will refine
and further develop the initial annotation
guidelines and introduce more levels of di-
mensionality. Once we have finalized our
guidelines and refined the annotations we
plan to release the corpus for further re-
search, after ensuring that no identifiable
information is included.
1 Introduction
Electronic Health Records (EHRs) contain a large
amount of free text documentation which is po-
tentially very useful for Information Retrieval and
Text Mining applications. Clinical documentation
is specific in many ways; there are many authors
in a document (e.g. physicians, nurses), there are
different situations that are documented (e.g. ad-
mission, current status). Moreover, they may often
be written under time pressure, resulting in frag-
mented, brief texts often containing spelling errors
and abbreviations. With access to EHR data, many
possibilities to exploit documented clinical knowl-
edge and experience arise.
One of the properties of EHRs is that they con-
tain reasoning about the status and diagnoses of
patients. Gathering such information for the use
in e.g. medical research in order to find rela-
tionships between diagnoses, treatments etc. has
great potential. However, in many situations, clin-
icians might describe uncertain or negated find-
ings, which is crucial to distinguish from positive
or asserted findings. Potential future applications
include search engines where medical researchers
can search for particular diseases where negated
or speculative contexts are separated from asserted
contexts, or text mining systems where e.g. dis-
eases that seem to occur often in speculative con-
texts are presented to the user, indicating that more
research is needed. Moreover, laymen may also
benefit from information retrieval systems that dis-
tinguish diseases or symptoms that are more or
less certain given current medical expertise and
knowledge.
We have, in an initial annotation trial, annotated
6 739 sentences randomly extracted from a corpus
of Swedish EHRs for sentence level (un)certainty,
and token level speculative keywords and nega-
tions
1
. In this paper, a deeper analysis of the re-
sulting annotations is performed. The aims are
to analyze the results split into different clinical
practices by means of descriptive statistics and
pairwise Inter-Annotator Agreement (IAA) mea-
sured by F
1
-score, with the goal of identifying a)
whether specific clinical practices contain higher
or lower amounts of uncertain expressions, b)
1
This research has been carried out after approval
from the Regional Ethical Review Board in Stockholm
(Etikpr?ovningsn?amnden i Stockholm), permission number
2009/1742-31/5
14
whether specific clinical practices result in higher
or lower IAA - indicating a less or more difficult
clinical practice for judging uncertainties, and c)
identifying the characteristics of the entities anno-
tated as speculative words, are they highly lexi-
cal or is a deeper syntactic and/or semantic anal-
ysis required for modeling? From this analysis,
we plan to conduct a new annotation trial where
we will refine and further develop the annotation
guidelines and use domain experts for annotations
in order to be able to create a useful annotated cor-
pus modeling uncertainties, negations and specu-
lations in Swedish clinical text, which can be used
to develop tools for the automatic identification of
these phenomena in, for instance, Text Mining ap-
plications.
2 Related Research
In recent years, the interest for identifying and
modeling speculative language in natural language
text has grown. In particular, biomedical scien-
tific articles and abstracts have been the object of
several experiments. In Light et al (2004), four
annotators annotated 891 sentences each as either
highly speculative, low speculative, or definite,
in biomedical scientific abstracts extracted from
Medline. In total, they found 11 percent specula-
tive sentences, resulting in IAA results, measured
with kappa, between 0.54 and 0.68. One of their
main findings was that the majority of the specu-
lative sentences appeared towards the end of the
abstract.
Vincze et al (2008) describe the creation of the
BioScope corpus, where more than 20 000 sen-
tences from both medical (clinical) free texts (ra-
diology reports), biological full papers and biolog-
ical scientific abstracts have been annotated with
speculative and negation keywords along with
their scope. Over 10 percent of the sentences
were either speculative or negated. In the clinical
sub-corpus, 14 percent contained speculative key-
words. Three annotators annotated the corpus, and
the guidelines were modified several times during
the annotation process, in order to resolve prob-
lematic issues and refine definitions. The IAA
results, measured with F
1
-score, in the clinical
sub-corpus for negation keywords ranged between
0.91 and 0.96, and for speculative keywords be-
tween 0.84 and 0.92. The BioScope corpus has
been used to train and evaluate automatic classi-
fiers (e.g.
?
Ozg?ur and Radev (2009) and Morante
and Daelemans (2009)) with promising results.
Five qualitative dimensions for characterizing
scientific sentences are defined in Wilbur et al
(2006), including levels of certainty. Here, guide-
lines are also developed over a long period of time
(more than a year), testing and revising the guide-
lines consecutively. Their final IAA results, mea-
sured with F
1
-score, range between 0.70 and 0.80.
Different levels of dimensionality for categorizing
certainty (in newspaper articles) is also presented
in Rubin et al (2006).
Expressions for communicating probabilities or
levels of certainty in clinical care may be inher-
ently difficult to judge. Eleven observers were
asked to indicate the level of probability of a dis-
ease implied by eighteen expressions in the work
presented by Hobby et al (2000). They found
that expressions indicating intermediate probabili-
ties were much less consistently rated than those
indicating very high or low probabilities. Sim-
ilarly, Khorasani et al (2003) performed a sur-
vey analyzing agreement between radiologists and
non-radiologists regarding phrases used to convey
degrees of certainty. In this study, they found lit-
tle or no agreement among the survey participants
regarding the diagnostic certainty associated with
these phrases. Although we do not have access to
radiology reports in our corpus, these findings in-
dicate that it is not trivial to classify uncertain lan-
guage in clinical documentation, even for domain
experts.
3 Method
The annotation trial is based on sentences ran-
domly extracted from a corpus of Swedish EHRs
(see Dalianis and Velupillai (2010) for an initial
description and analysis). These records contain
both structured (e.g. measure values, gender in-
formation) and unstructured information (i.e. free
text). Each free text entry is written under a spe-
cific heading, e.g. Status, Current medication, So-
cial Background. For this corpus, sentences were
extracted only from the free text entry Assessment
(Bed?omning), with the assumption that these en-
tries contain a substantial amount of reasoning re-
garding a patient?s diagnosis and situation. A sim-
ple sentence tokenizing strategy was employed,
based on heuristic regular expressions
2
. We have
used Knowtator (Ogren, 2006) for the annotation
2
The performance of the sentence tokenizer has not been
evaluated in this work.
15
work.
One senior level student (SLS), one undergrad-
uate computer scientist (UCS), and one undergrad-
uate language consultant (ULC) annotated the sen-
tences into the following classes; on a sentence
level: certain, uncertain or undefined, and on a
token level: speculative words, negations, and un-
defined words.
The annotators are to be considered naive
coders, as they had no prior knowledge of the
task, nor any clinical background. The annota-
tion guidelines were inspired by those created for
the BioScope corpus (Vincze et al, 2008), with
some modifications (see Dalianis and Velupillai
(2010)). The annotators were allowed to break a
sentence into subclauses if they found that a sen-
tence contained conflicting levels of certainty, and
they were allowed to mark question marks as spec-
ulative words. They did not annotate the linguis-
tic scopes of each token level instance. The anno-
tators worked independently, and met for discus-
sions in even intervals (in total seven), in order to
resolve problematic issues. No information about
the clinic, patient gender, etc. was shown. The
annotation trial is considered as a first step in fur-
ther work of annotating Swedish clinical text for
speculative language.
Clinical practice # sentences # tokens
hematology 140 1 494
surgery 295 3 269
neurology 351 4 098
geriatrics 142 1 568
orthopaedics 245 2 541
rheumatology 384 3 348
urology 120 1 393
cardiology 128 1 242
oncology 550 5 262
ENT 224 2 120
infection 107 1 228
emergency 717 6 755
paediatrics 935 8 926
total, clinical practice 4 338 43 244
total, full corpus 6 739 69 495
Table 1: Number of sentences and tokens per clin-
ical practice (#sentences > 100), and in total. ENT
= Ear, Nose and Throat.
3.1 Annotations and clinical practices
The resulting corpus consists of 6 739 sentences,
extracted from 485 unique clinics. In order to
be able to analyze possible similarities and dif-
ferences across clinical practices, sentences from
clinics belonging to a specific practice type were
grouped together. In Table 1, the resulting groups,
along with the total amount of sentences and to-
kens, are presented
3
. Only groups with a total
amount of sentences > 100 were used in the anal-
ysis, resulting in 13 groups. A clinic was included
in a clinical practice group based on a priority
heuristics, e.g. the clinic ?Barnakuten-kir? (Pae-
diatric emergency surgery) was grouped into pae-
diatrics.
The average length (in tokens) per clinical prac-
tice and in total are given in Table 2. Clinical
documentation is often very brief and fragmented,
for most clinical practices (except urology and
cardiology) the minimum sentence length (in to-
kens) was one, e.g. ?basal?, ?terapisvikt? (ther-
apy failure), ?lymf?odem? (lymphedema), ?viros?
(virosis), ?opanm?ales? (reported to surgery, com-
pound with abbreviation). We see that the aver-
age sentence length is around ten for all practices,
where the shortest are found in rheumatology and
the longest in infection.
As the annotators were allowed to break up sen-
tences into subclauses, but not required to, this led
to a considerable difference in the total amount of
annotations per annotator. In order to be able to
analyze similarities and differences between the
resulting annotations, all sentence level annota-
tions were converted into one sentence class only,
the primary class (defined as the first sentence
level annotation class, i.e. if a sentence was bro-
ken into two clauses by an annotator, the first be-
ing certain and the second being uncertain, the
final sentence level annotation class will be cer-
tain). The sentence level annotation class certain
was in clear majority among all three annotators.
On both sentence and token level, the class unde-
fined (a sentence that could not be classified as
certain or uncertain, or a token which was not
clearly speculative) was rarely used. Therefore,
all sentence level annotations marked as undefined
are converted to the majority class, certain, result-
ing in two sentence level annotation classes (cer-
tain and uncertain) and two token level annotation
classes (speculative words and negations, i.e. to-
3
White space tokenization.
16
kens annotated as undefined are ignored).
For the remaining analysis, we focus on the
distributions of the annotation classes uncertain
and speculative words, per annotator and annota-
tor pair, and per clinical practice.
Clinical practice Max Avg Stddev
hematology 40 10.67 7.97
surgery 57 11.08 8.29
neurology 105 11.67 10.30
geriatrics 58 11.04 9.29
orthopaedics 40 10.37 6.88
rheumatology 59 8.72 7.99
urology 46 11.61 7.86
cardiology 50 9.70 7.46
oncology 54 9.57 7.75
ENT 54 9.46 7.53
infection 37 11.48 7.76
emergency 55 9.42 6.88
paediatrics 68 9.55 7.24
total, full corpus 120 10.31 8.53
Table 2: Token statistics per sentence and clinical
practice. All clinic groups except urology (min =
2) and cardiology (min = 2) have a minimum sen-
tence length of one token.
Figure 1: Sentence level annotation: uncertain,
percentage per annotator and clinical practice.
4 Results
We have measured the proportions (in percent) per
annotator for each clinical practice and in total.
This enables an analysis of whether there are sub-
stantial individual differences in the distributions,
indicating that this annotation task is highly sub-
jective and/or difficult. Moreover, we measure
IAA by pairwise F
1
-score. From this, we may
Figure 2: Pairwise F
1
-score, sentence level anno-
tation class uncertain.
draw conclusions whether specific clinical prac-
tices are harder or easier to judge reliably (i.e. by
high IAA results).
Figure 3: Average length in tokens, per annotator
and sentence class.
In Figure 1, we see that the average amount of
uncertain sentences lies between 9 and 12 percent
for each annotator in the full corpus. In general,
UCS has annotated a larger proportion of uncer-
tain sentences compared to ULC and SLS.
The clinical discipline with the highest average
amount of uncertain sentences is neurology (13.7
percent), the lowest average amount is found in
cardiology (4.7 percent). Surgery and cardiology
show the largest individual differences in propor-
tions (from 9 percent (ULC) to 15 percent (UCS),
and from 2 percent (ULC) to 7 percent (UCS), re-
spectively).
However, in Figure 2, we see that the pairwise
IAA, measured by F
1
-score, is relatively low, with
an average IAA of 0.58, ranging between 0.54
(UCS/SLS) and 0.65 (UCS/ULC), for the entire
corpus. In general, the annotator pair UCS/ULC
have higher IAA results, with the highest for geri-
atrics (0.78). The individual proportions for un-
17
certain sentences in geriatrics is also lower for
all annotators (see Figure 1), indicating a clinical
practice with a low amount of uncertain sentences,
and a slightly higher average IAA (0.64 F
1
-score).
4.1 Sentence lengths
As the focus lies on analyzing sentences annotated
as uncertain, one interesting property is to look at
sentence lengths (measured in tokens). One hy-
pothesis is that uncertain sentences are in general
longer. In Figure 3 we see that in general, for
all three annotators, uncertain sentences are longer
than certain sentences. This result is, of course,
highly influenced by the skewness of the data (i.e.
uncertain sentences are in minority), but it is clear
that uncertain sentences, in general, are longer on
average. It is interesting to note that the annota-
tor SLS has, in most cases, annotated longer sen-
tences as uncertain, compared to UCS and ULC.
Moreover, geriatrics, with relatively high IAA but
relatively low amounts of uncertain sentences, has
well above average sentence lengths in the uncer-
tain class.
4.2 Token level annotations
When it comes to the token level annotations,
speculative words and negations, we observed
very high IAA for negations (0.95 F
1
-score (exact
match) on average in the full corpus, the lowest for
neurology, 0.94). These annotations were highly
lexical (13 unique tokens) and unambiguous, and
spread evenly across the two sentence level anno-
tation classes (ranging between 1 and 3 percent of
the total amount of tokens per class). Moreover,
all negations were unigrams.
On the other hand, we observed large variations
in IAA results for speculative words. In Figure
4, we see that there are considerable differences
between exact and partial matches
4
between all
annotator pairs, indicating individual differences
in the interpretations of what constitutes a spec-
ulative word and how many tokens they cover,
and the lexicality is not as evident as for nega-
tions. The highest level of agreement we find be-
tween UCS/ULC in orthopaedics (0.65 F
1
-score,
partial match) and neurology (0.64 F
1
-score, par-
tial match), and the lowest in infection (UCS/SLS,
0.31 F
1
-score).
4
Partial matches are measured on a character level.
Figure 4: F
1
-score, speculative words, exact and
partial match.
4.2.1 Speculative words ? most common
The low IAA results for speculative words invites
a deeper analysis for this class. How is this inter-
preted by the individual annotators? First, we look
at the most common tokens annotated as specu-
lative words, shared by the three annotators: ???,
?sannolikt? (likely), ?ev? (possibly, abbreviated),
?om? (if). The most common speculative words
are all unigrams, for all three annotators. These
tokens are similar to the most common specu-
lative words in the clinical BioScope subcorpus,
where if, may and likely are among the top five
most common. Those tokens that are most com-
mon per annotator and not shared by the other two
(among the five most frequent) include ?bed?oms?
(judged), ?kan? (could), ?helt? (completely) and
?st?allningstagande? (standpoint).
Looking at neurology and urology, with a higher
overall average amount of uncertain sentences, we
find that the most common words for neurology
are similar to those most common in total, while
for urology we find more n-grams. In Table 3, the
five most common speculative words per annotator
for neurology and urology are presented.
When it comes to the unigrams, many of these
are also not annotated as speculative words. For
instance, ?om? (if), is annotated as speculative in
only 9 percent on average of its occurrence in the
neurological data (the same distribution holds, on
average, in the total set). In Morante and Daele-
mans (2009), if is also one of the words that are
subject to the majority of false positives in their
automatic classifier. On the other hand, ?sanno-
likt? (likely) is almost always annotated as a spec-
ulative word (over 90 percent of the time).
18
UCS ULC SLS
neurology ? ? ?
sannolikt (likely) kan (could) sannolikt (likely)
kan (could) sannolikt (likely) ev (possibly, abbr)
om (if) om (if) om (if)
pr?ova (try) verkar (seems) st?allningstagande (standpoint)
ter (seem) ev (possibly, abbr) m?ojligen (possibly)
urology kan vara (could be) mycket (very) tyder p?a(indicates)
tyder p?a(indicates) inga tecken (no signs) i f?orsta hand (primarily)
ev (possibly, abbr) kan vara (could be) misst?ankt (suspected)
misst?ankt (suspected) kan (could) kanske (perhaps)
kanske (perhaps) tyder (indicates) skall vi f?ors?oka (should we try)
planeras tydligen (apparently planned) misst?ankt (suspected) kan vara (could be)
Table 3: Most common speculative words per annotator for neurology and urology.
4.2.2 Speculative words ? n-grams
Speculative words are, in Swedish clinical text,
clearly not simple lexical unigrams. In Figure 5
we see that the average length of tokens anno-
tated as speculative words is, on average, 1.34,
with the longest in orthopaedics (1.49) and urol-
ogy (1.46). We also see that SLS has, on aver-
age, annotated longer sequences of tokens as spec-
ulative words compared to UCS and ULC. The
longest n-grams range between three and six to-
kens, e.g. ?kan inte se n?agra tydliga? (can?t see
any clear), ?kan r?ora sig om? (could be about),
?inte helt har kunnat uteslutas? (has not been able
to completely exclude), ?i f?orsta hand? (primarily).
In many of these cases, the strongest indicator is
actually a unigram (?kan? (could)), within a verb
phrase. Moreover, negations inside a speculative
word annotation, such as ?inga tecken? (no signs)
are annotated differently among the individual an-
notators.
Figure 5: Average length, speculative words.
4.3 Examples
We have observed low average pairwise IAA for
sentence level annotations in the uncertain class,
with more or less large differences between the an-
notator pairs. Moreover, at the token level and for
the class speculative words, we also see low av-
erage agreement, and indications that speculative
words often are n-grams. We focus on the clinical
practices neurology, because of its average large
proportion of uncertain sentences, geriatrics for
its high IAA results for UCS/ULC and low aver-
age proportion of uncertain sentences, and finally
surgery, for its large discrepancy in proportions
and low average IAA results.
In Example 1 we see a sentence where two an-
notators (ULC, SLS) have marked the sentence
as uncertain, also marking a unigram (?ospecifik?
(unspecific) as a speculative word. This example
is interesting since the utterance is ambiguous, it
can be judged as certain as in the dizziness is con-
firmed to be of an unspecific type or uncertain as
in the type of dizziness is unclear, a type of ut-
terance which should be clearly addressed in the
guidelines.
<C> Yrsel av ospecifik typ. </C>
<U> Yrsel av <S> ospecifik </S> typ.
</U>
<U> Yrsel av <S> ospecifik </S> typ.
</U>
Dizziness of unspecific type
Example 1: Annotation example, neurology. Am-
biguous sentence, unspecific as a possible specu-
lation cue. C = Certain, U = Uncertain, S = Spec-
ulative words.
An example of different interpretations of the
minimum span a speculative word covers is given
in Example 2. Here, we see that ?inga egentliga
m?arkbara? (no real apparent) has been annotated
in three different ways. It is also interesting to
19
note the role of the negation as part of ampli-
fying speculation. Several such instances were
marked by the annotators (for further examples,
see Dalianis and Velupillai (2010)), which con-
forms well with the findings reported in Kilicoglu
and Bergler (2008), where it is showed that ex-
plicit certainty markers together with negation are
indicators of speculative language. In the Bio-
Scope corpus (Vincze et al, 2008), such instances
are marked as speculation cues. This example, as
well as Example 1, is also interesting as they both
clearly are part of a longer passage of reasoning of
a patient, with no particular diagnosis mentioned
in the current sentence. Instead of randomly ex-
tracting sentences from the free text entry Assess-
ment, one possibility would be to let the annotators
judge all sentences in an entry (or a full EHR). Do-
ing this, differences in where speculative language
often occur in an EHR (entry) might become ev-
ident, as for scientific writings, where it has been
showed that speculative sentences occur towards
the end of abstracts (Light et al, 2004).
<U> <S><N> Inga </N> egentliga </S>
<S> m?arkbara</S> minnessv?arigheter under
samtal. </U>.
<U> <N> Inga </N> <S> egentliga </S>
m?arkbara minnessv?arigheter under samtal. </U>.
<U> <S><N> Inga </N> egentliga m?arkbara
</S> minnessv?arigheter under samtal. </U>.
No real apparent memory difficulties during
conversation
Example 2: Annotation example, neurology. Dif-
ferent annotation coverage over negation and spec-
ulation. C = Certain, U = Uncertain, S = Specula-
tive words, N = Negation
In geriatrics, we have observed a lower than
average amount of uncertain sentences, and high
IAA between UCS and ULC. In Example 3 we see
a sentence where UCS and ULC have matching
annotations, whereas SLS has judged this sentence
as certain. This example shows the difficulty of
interpreting expressions indicating possible spec-
ulation ? is ?ganska? (relatively) used here as a
marker of certainty (as certain as one gets when
diagnosing this type of illness)?
The word ?sannolikt? (likely) is one of the most
common words annotated as a speculative word
in the total corpus. In Example 4, we see a sen-
<U> B?ade anamnestiskt och testm?assigt <S>
ganska </S> stabil vad det g?aller Alzheimer
sjukdom. </U>.
<U> B?ade anamnestiskt och testm?assigt <S>
ganska </S> stabil vad det g
?
ller Alzheimer
sjukdom. </U>.
<C> B?ade anamnestiskt och testm?assigt ganska
stabil vad det g?aller Alzheimer sjukdom. </C>.
Both anamnesis and tests relatively stabile
when it comes to Alzheimer?s disease.
Example 3: Annotation example, geriatrics. Dif-
ferent judgements for the word ?ganska? (rela-
tively). C = Certain, U = Uncertain, S = Specu-
lative words.
tence where the annotators UCS and SLS have
judged it to be uncertain, while UCS and ULC
have marked the word ?sannolikt? (likely) as a
speculative word. This is an interesting exam-
ple, through informal discussions with clinicians
we were informed that this word might as well be
used as a marker of high certainty. Such instances
show the need for using domain experts in future
annotations of similar corpora.
<C>En 66-?arig kvinna med <S>sannolikt</S>
2 synkrona tum?orer v?anster colon/sigmoideum och
d?ar till levermetastaser.</C>.
<U>En 66-?arig kvinna med <S>sannolikt</S>
2 synkrona tum?orer v?anster colon/sigmoideum och
d?ar till levermetastaser.</U>.
<C>En 66-?arig kvinna med sannolikt 2 synkrona
tum?orer v?anster colon/sigmoideum och d?ar till
levermetastaser.</C>.
A 66 year old woman likely with 2 synchronous
tumours left colon/sigmoideum in addition to liver
metastasis.
Example 4: Annotation example, surgery. Differ-
ent judgements for the word ?sannolikt? (likely). C
= Certain, U = Uncertain, S = Speculative words.
5 Discussion
We have presented an analysis of an initial anno-
tation trial for the identification of uncertain sen-
tences as well as for token level cues (specula-
tive words) across different clinical practices. Our
main findings are that IAA results for both sen-
tence level annotations of uncertainty and token
level annotations for speculative words are, on av-
20
erage, fairly low, with higher average agreement
in geriatrics and rheumatology (see Figures 1 and
2). Moreover, by analyzing the individual distri-
butions for the classes uncertain and speculative
words, we find that neurology has the highest aver-
age amount of uncertain sentences, and cardiology
the lowest. On average, the amount of uncertain
sentences ranges between 9 and 12 percent, which
is in line with previous work on sentence level an-
notations of uncertainty (see Section 2).
We have also showed that the most common
speculative words are unigrams, but that a substan-
tial amount are n-grams. The n-grams are, how-
ever, often part of verb phrases, where the head is
often the speculation cue. However, it is evident
that speculative words are not always simple lex-
ical units, i.e. syntactic information is potentially
very useful. Question marks are the most common
entities annotated as speculative words. Although
these are not interesting indicators in themselves,
it is interesting to note that they are very common
in clinical documentation.
From the relatively low IAA results we draw the
conclusion that this task is difficult and requires
more clearly defined guidelines. Moreover, using
naive coders on clinical documentation is possibly
not very useful if the resulting annotations are to
be used in, e.g. a Text Mining application for med-
ical researchers. Clinical documentation is highly
domain-specific and contains a large amount of
internal jargon, which requires judgements from
clinicians. However, we find it interesting to note
that we have identified differences between dif-
ferent clinical practices. A consensus corpus has
been created from the resulting annotations, which
has been used in an experiment for automatic clas-
sification, see Dalianis and Skeppstedt (2010) for
initial results and evaluation.
During discussions among the annotators, some
specific problems were noted. For instance, the
extracted sentences were not always about the pa-
tient or the current status or diagnosis, and in many
cases an expression could describe (un)certainty of
someone other than the author (e.g. another physi-
cian or a family member), introducing aspects of
perspective. The sentences annotated as certain,
are difficult to interpret, as they are simply not un-
certain. We believe that it is important to intro-
duce further dimensions, e.g. explicit certainty,
and focus (what is (un)certain?), as well as time
(e.g. current or past).
6 Conclusions
To our knowledge, there is no previous research on
annotating Swedish clinical text for sentence and
token level uncertainty together with an analysis
of the differences between different clinical prac-
tices. Although the initial IAA results are in gen-
eral relatively low for all clinical practice groups,
we have identified indications that neurology is a
practice which has an above average amount of
uncertain elements, and that geriatrics has a be-
low average amount, as well as higher IAA. Both
these disciplines would be interesting to continue
the work on identifying speculative language.
It is evident that clinical language contains a rel-
atively high amount of uncertain elements, but it
is also clear that naive coders are not optimal to
use for interpreting the contents of EHRs. More-
over, more care needs to be taken in the extrac-
tion of sentences to be annotated, in order to en-
sure that the sentences actually describe reason-
ing about the patient status and diagnosis. For in-
stance, instead of randomly extracting sentences
from within a free text entry, it might be better to
let the annotators judge all sentences within an en-
try. This would also enable an analysis of whether
speculative language is more or less frequent in
specific parts of EHRs.
From our findings, we plan to further develop
the guidelines and particularly focus on specify-
ing the minimal entities that should be annotated
as speculative words (e.g. ?kan? (could)). We
also plan to introduce further levels of dimension-
ality in the annotation task, e.g. cues that indi-
cate a high level of certainty, and to use domain
experts as annotators. Although there are prob-
lematic issues regarding the use of naive coders
for this task, we believe that our analysis has re-
vealed some properties of speculative language in
clinical text which enables us to develop a useful
resource for further research in the area of specula-
tive language. Judging an instance as being certain
or uncertain is, perhaps, a task which can never
exclude subjective interpretations. One interesting
way of exploiting this fact would be to exploit in-
dividual annotations similar to the work presented
in Reidsma and op den Akker (2008). Once we
have finalized the annotated set, and ensured that
no identifiable information is included, we plan to
make this resource available for further research.
21
References
Hercules Dalianis and Maria Skeppstedt. 2010. Cre-
ating and Evaluating a Consensus for Negated and
Speculative Words in a Swedish Clinical Corpus. To
be published in the proceedings of the Negation and
Speculation in Natural Language Processing Work-
shop, July 10, Uppsala, Sweden.
Hercules Dalianis and Sumithra Velupillai. 2010.
How Certain are Clinical Assessments? Annotat-
ing Swedish Clinical Text for (Un)certainties, Spec-
ulations and Negations. In Proceedings of the of
the Seventh International Conference on Language
Resources and Evaluation, LREC 2010, Valletta,
Malta, May 19-21.
J. L. Hobby, B. D. M. Tom, C. Todd, P. W. P. Bearcroft,
and A. K. Dixon. 2000. Communication of doubt
and certainty in radiological reports. The British
Journal of Radiology, 73:999?1001, September.
R. Khorasani, D. W. Bates, S. Teeger, J. M. Rotschild,
D. F. Adams, and S. E. Seltzer. 2003. Is terminol-
ogy used effectively to convey diagnostic certainty
in radiology reports? Academic Radiology, 10:685?
688.
Halil Kilicoglu and Sabine Bergler. 2008. Recogniz-
ing speculative language in biomedical research ar-
ticles: a linguistically motivated perspective. BMC
Bioinformatics, 9(S-11).
Marc Light, Xin Ying Qiu, and Padmini Srinivasan.
2004. The language of bioscience: Facts, spec-
ulations, and statements in between. In Lynette
Hirschman and James Pustejovsky, editors, HLT-
NAACL 2004 Workshop: BioLINK 2004, Linking
Biological Literature, Ontologies and Databases,
pages 17?24, Boston, Massachusetts, USA, May 6.
Association for Computational Linguistics.
Roser Morante and Walter Daelemans. 2009. Learn-
ing the scope of hedge cues in biomedical texts.
In BioNLP ?09: Proceedings of the Workshop on
BioNLP, pages 28?36, Morristown, NJ, USA. As-
sociation for Computational Linguistics.
Philip V. Ogren. 2006. Knowtator: a prot?eg?e plug-in
for annotated corpus construction. In Proceedings of
the 2006 Conference of the North American Chapter
of the Association for Computational Linguistics on
Human Language Technology, pages 273?275, Mor-
ristown, NJ, USA. Association for Computational
Linguistics.
Arzucan
?
Ozg?ur and Dragomir R. Radev. 2009. De-
tecting speculations and their scopes in scientific
text. In Proceedings of the 2009 Conference on Em-
pirical Methods in Natural Language Processing,
pages 1398?1407, Singapore, August. Association
for Computational Linguistics.
Dennis Reidsma and Rieks op den Akker. 2008. Ex-
ploiting ?subjective? annotations. In HumanJudge
?08: Proceedings of the Workshop on Human Judge-
ments in Computational Linguistics, pages 8?16,
Morristown, NJ, USA. Association for Computa-
tional Linguistics.
Victoria L. Rubin, Elizabeth D. Liddy, and Noriko
Kando. 2006. Certainty identification in texts: Cat-
egorization model and manual tagging results. In
Computing Affect and Attitutde in Text: Theory and
Applications. Springer.
Veronika Vincze, Gy?orgy Szarvas, Rich?ard Farkas,
Gy?orgy M?ora, and J?anos Csirik. 2008. The bio-
scope corpus: biomedical texts annotated for uncer-
tainty, negation and their scopes. BMC Bioinformat-
ics, 9(S-11).
J. W. Wilbur, A. Rzhetsky, and H. Shatkay. 2006. New
directions in biomedical text annotation: definitions,
guidelines and corpus construction. BMC Bioinfor-
matics, 7:356+, July.
22
Proceedings of the Workshop on Negation and Speculation in Natural Language Processing, pages 41?45,
Uppsala, July 2010.
Levels of Certainty in Knowledge-Intensive Corpora:
An Initial Annotation Study
Aron Henriksson
DSV/KTH-Stockholm University
Sweden
aronhen@dsv.su.se
Sumithra Velupillai
DSV/KTH-Stockholm University
Sweden
sumithra@dsv.su.se
Abstract
In this initial annotation study, we sug-
gest an appropriate approach for determin-
ing the level of certainty in text, including
classification into multiple levels of cer-
tainty, types of statement and indicators of
amplified certainty. A primary evaluation,
based on pairwise inter-annotator agree-
ment (IAA) using F
1
-score, is performed
on a small corpus comprising documents
from the World Bank. While IAA results
are low, the analysis will allow further re-
finement of the created guidelines.
1 Introduction
Despite ongoing efforts to codify knowledge, it is
often communicated in an informal manner. In
our choice of words and expressions, we implicitly
or explicitly judge the certainty of the knowledge
we wish to convey. This fact makes it possible
to gauge the reliability of knowledge based on the
subjective perspective of the author.
As knowledge is often difficult to ascertain, it
seems reasonable to regard knowledge on a contin-
uum of varying degrees of certainty, as opposed to
a binary (mis)conception. This corresponds to the
notion of epistemic modality: the degree of confi-
dence in, or commitment to, the truth of proposi-
tions (Hyland, 1998). Hedging is a means of af-
fecting epistemic modality by qualifying proposi-
tions, realized through tentative words and expres-
sions such as possibly and tends to.
A holistic perspective on certainty?in which
not only speculation is considered, but also signs
of increased certainty?requires a classification
into various levels. Applying such an approach
to knowledge-intensive corpora, it may in due
course be possible to evaluate unstructured, infor-
mal knowledge. This would not least be valuable
to organizational knowledge management prac-
tices, where it could provide a rough indicator of
reliability in internal knowledge audits.
2 Related Research
The hedging concept was first introduced by
Lakoff (1973) but has only really come into the
spotlight in more recent years. Studies have
mainly taken place in the biomedical domain, with
Hyland?s (1998) influential work investigating the
phenomenon in scientific research articles. Spec-
ulative keywords and negations, along with their
linguistic scopes, are annotated in the BioScope
corpus by Vincze et al (2008), which contains a
large collection of medical and biological text (sci-
entific articles and abstracts, as well as radiology
reports). After several iterations of refining their
guidelines, they report IAA values ranging from
77.6 to 92.37 F
1
-score for speculative keywords
(62.5 and 95.54 F
1
-score for full scope). This cor-
pus is freely available and has been used for train-
ing and evaluation of automatic classifiers, see e.g.
Morante and Daelemans (2009). One of the main
findings is that hedge cues are highly domain-
dependent. Automatic identification of other pri-
vate states, including opinions, represents a sim-
ilar task, see e.g. Wiebe et al (2005). Diab et
al. (2009) study annotation of committed and non-
committed belief and show that automatic tagging
of such classes is feasible. A different annotation
approach is proposed by Rubin et al (2006), in
which certainty in newspaper articles is catego-
rized along four dimensions: level, perspective, fo-
cus and time. Similarly, five dimensions are used
in Wilbur et al (2006) for the creation of an an-
notated corpus of biomedical text: focus, polarity,
certainty, evidence and directionality.
3 Method
Based on previous approaches and an extensive lit-
erature review, we propose a set of guidelines that
41
(1) incorporates some new features and (2) shifts
the perspective to suit knowledge-intensive cor-
pora, e.g. comprising organizational knowledge
documents. Besides categorization into levels of
certainty, this approach distinguishes between two
types of statement and underscores the need to
take into account words and expressions that add
certainty to a proposition.
A small corpus of 10 World Bank documents?
a publicly available resource known as Viewpoints
(The World Bank Group, 2010)?is subsequently
annotated in two sets by different annotators. The
corpus is from a slightly different domain to those
previously targeted and represents an adequate al-
ternative to knowledge documents internal to an
organization by fulfilling the criterion of knowl-
edge intensity. The process is carried out in a
Prote?ge? plugin: Knowtator (Ogren, 2006). Pair-
wise IAA, measured as F
1
-score, is calculated to
evaluate the feasibility of the approach.
Statements are annotated at the clause level, as
sentences often contain subparts subject to differ-
ent levels of certainty. These are not predefined
and the span of classes is determined by the an-
notator. Furthermore, a distinction is made be-
tween different types of statement: statements that
give an account of something, typically a report
of past events, and statements that express con-
crete knowledge claims. The rationale behind this
distinction is that text comprises statements that
make more or less claims of constituting knowl-
edge. Thus, knowledge claims?often less preva-
lent than accounts?should be given more weight
in the overall assessment, as the application lies
in automatically evaluating the reliability of infor-
mal knowledge. Assuming the view of knowledge
and certainty as continuous, it is necessary to dis-
cretize that into a number of intervals, albeit more
than two. Hence, accounts and claims are cate-
gorized according to four levels of certainty: very
certain, quite certain, quite uncertain and very un-
certain. In addition to the statement classes, four
indicators make up the total of twelve. We in-
troduce certainty amplifiers, which have received
little attention in previous work. These are lin-
guistic features that add certainty to a statement,
e.g. words like definitely and expressions like
without a shadow of a doubt. Hedging indica-
tors, on the other hand, have gained much atten-
tion recently and signify uncertainty. The source
hedge class is applicable to instances where the
source of epistemic judgement is stated explicitly,
yet only when it provides a hedging function (e.g.
some say). Modality strengtheners are features
that strengthen the effect of epistemic modality
when used in conjunction with other (un)certainty
indicators?but alone do not signify any polarity
orientation?and may be in the form of vagueness
(e.g. <could be> around that number) or quantity
gradations (e.g. very <sure>).
4 Results
The corpus contains a total of 772 sentences,
which are annotated twice: set #1 by one anno-
tator and set #2 by five annotators, annotating two
documents each. The statistics in Table 1 show
a discrepancy over the two sets in the number of
classified statements, which is likely due to diffi-
culties in determining the scope of clauses. There
are likewise significant differences in the propor-
tion between accounts and claims, as had been an-
ticipated.
Accounts Claims
Set #1 Set #2 Set #1 Set #2
726 574 395 393
Table 1: Frequencies of accounts and claims.
Despite the problem of discriminating between ac-
counts and claims, they seem to be susceptible to
varying levels of certainty. The average distribu-
tion of certainty for account statements is depicted
in Figure 1. As expected, an overwhelming ma-
jority (87%) of such statements are quite certain,
merely relating past events and established facts.
Figure 1: Average distribution of certainty in ac-
count statements.
By comparison, knowledge claims are more
commonly hedged (23%), although the majority
is still quite certain. Interestingly, claims are also
expressed with added confidence more often than
accounts?around one in every ten claims.
42
Figure 2: Average distribution of certainty in
knowledge claims.
As expected, the most common indicator is of
hedging. Common cues include may, can, might,
could, indicate(s), generally and typically. Many
of these cues are also among the most common in
the biomedical sub-corpus of BioScope (Vincze et
al., 2008). It is interesting to note the fairly com-
mon phenomenon of certainty amplifiers. These
are especially interesting, as they have not been
studied much before, although Wiebe et al (2005)
incorporate intensity ratings in their annotation
scheme. There is agreement on words like clearly,
strongly and especially.
Indicator Set #1 Set #2
Certainty amplifier 61 29
Hedging indicator 151 133
Source hedge 0 40
Modality strengthener 9 122
Table 2: Frequency of indicators
To evaluate the approach, we calculate IAA by
pairwise F
1
-score, considering set #1 as the gold
standard, i.e. as correctly classified, in relation to
which the other subsets are evaluated. We do this
for exact matches and partial matches1. For exact
matches in a single document, the F
1
-score val-
ues range from an extremely low 0.09 to a some-
what higher?although still poor?0.52, yielding
an overall average of 0.28. These results clearly
reflect the difficulty of the task, although one has
to keep in mind the impact of the discrepancy in
the number of annotations. This is partly reflected
in the higher overall average for partial matches:
0.41.
Certainty amplifiers and hedging indicators
have F
1
-scores that range up to 0.53 and 0.55 re-
spectively (ditto for partial matches) in a single
document. Over the entire corpus, however, the
1Partial matches are calculated on a character level while
exact matches are calculated on a token level.
averages come down to 0.27 for certainty ampli-
fiers (0.30 for partial matches) and 0.33 for hedg-
ing indicators (0.35 for partial matches).
Given the poor results, we want to find out
whether the main difficulty is presented by having
to judge certainty according to four levels of cer-
tainty, or whether it lies in having to distinguish
between types of statement. We therefore general-
ize the eight statement-related classes into a single
division between accounts and claims. Naturally,
the agreement is higher than for any single class,
with 0.44 for the former and 0.41 for the latter.
A substantial increase is seen in partial matches,
with 0.70 for accounts and 0.55 for claims. The
results are, however, sufficiently low to conclude
that there were real difficulties in distinguishing
between the two.
Statement Type Exact F
1
Partial F
1
Account 0.44 0.70
Claim 0.41 0.55
Table 3: Pairwise IAA per statement type, F
1
-
scores for exact and partial matches.
We subsequently generalize the eight classes into
four, according to their level of certainty alone.
The results are again low: quite certain yields
the highest agreement at 0.47 (0.76 for partial
matches), followed by quite uncertain at 0.24
(0.35 for partial matches). These numbers suggest
that this part of the task is likewise difficult. The
rise in F
1
-scores for partial matches is noteworthy,
as it highlights the problem of different interpreta-
tions of clause spans.
Certainty Level Exact F
1
Partial F
1
Very certain 0.15 0.15
Quite certain 0.47 0.76
Quite uncertain 0.24 0.35
Very uncertain 0.08 0.08
Table 4: Pairwise IAA per certainty level, F
1
-
scores for exact and partial matches
5 Discussion
In the guidelines, it is suggested that the level of
certainty can typically be gauged by identifying
the number of indicators. There is, however, a se-
rious drawback to this approach. Hedging indica-
tors, in particular, are inherently uncertain to dif-
ferent degrees. Consider the words possibly and
43
probably. According to the guidelines, a single
occurrence of either of these hedging indicators
would normally render a statement quite uncer-
tain. Giving freer hands to the annotator might
be a way to evade this problem; however, it is not
likely to lead to any more consistent annotations.
Kilicoglu and Bergler (2008) address this by as-
signing weights to hedging cues.
A constantly recurring bone of contention is
presented by the relationship between certainty
and precision. One of the hardest judgements to
make is whether imprecision, or vagueness, is a
sign of uncertainty. Consider the following exam-
ple from the corpus:
Cape Verde had virtually no private sec-
tor.
Clearly, this statement would be more certain if it
had said: Cape Verde had no private sector. How-
ever, virtually no could be substituted with, say,
a very small, in which case the statement would
surely not be deemed uncertain. Perhaps precision
is a dimension of knowledge that should be ana-
lyzed in conjunction with certainty, but be anno-
tated separately.
6 Conclusion
There are, of course, a number of ways one can
go about annotating the level of certainty from
a knowledge perspective. Some modifications to
the approach described here are essential?which
the low IAA values are testament to?while oth-
ers may be worth exploring. Below is a selection
of five key changes to the approach that may lead
to improved results:
1. Explicate statement types. Although there
seems to be a useful difference between the
two types, the distinction needs to be further
explicated in the guidelines.
2. Focus on indicators. It is clear that indicators
cannot be judged in an identical fashion only
because they have been identified as signify-
ing either certainty or uncertainty. It is not
simply the number of occurrences of indica-
tors that determines the level of certainty but
rather how strong those indicators are. A pos-
sible solution is to classify indicators accord-
ing to the level of certainty they affect.
3. Discard rare classes. Very rare phenomena
that do not have a significant impact on the
overall assessment can be sacrificed without
affecting the results negatively, which may
also make the task a little less difficult.
4. Clarify guidelines. A more general remedy
is to clarify further the guidelines, including
instructions on how to determine the scope of
clauses; alternatively, predefine them.
5. Instruct annotators. Exposing annotators
to the task would surely result in increased
agreement, in particular if they agree be-
forehand on the distinctions described in the
guidelines. At the same time, you do not
want to steer the process too much. Perhaps
the task is inherently difficult to define in de-
tail. Studies on how to exploit subjective an-
notations might be interesting to explore, see
e.g. Reidsma and op den Akker (2008).
In the attempt to gauge the reliability of knowl-
edge, incorporating multiple levels of certainty
becomes necessary, as does indicators of in-
creased certainty. Given the similar rates of
agreement on hedging indicators and certainty
amplifiers (0.33 and 0.27 respectively; 0.30 and
0.35 for partial matches), the latter class seem
to be confirmed. It is an existing and impor-
tant phenomenon, although?like hedging indica-
tors?difficult to judge. Moreover, a differentia-
tion between types of statement is important due
to their?to different degrees?varying claims of
constituting knowledge. An automatic classifier
built on such an approach could be employed with
significant benefit to organizations actively man-
aging their collective knowledge. The advantage
of being aware of the reliability of knowledge are
conceivably manifold: it could, for instance, be
(1) provided as an attribute to end-users brows-
ing documents, (2) used as metadata by search
engines, (3) used in knowledge audits and knowl-
edge gap analyses, enabling organizations to learn
when knowledge in a particular area needs to be
consolidated. It is, of course, also applicable in a
more general information extraction sense: infor-
mation that is extracted from text needs to have a
certainty indicator attached to it.
A dimension other than certainty that has a clear
impact on knowledge is precision. It would be in-
teresting to evaluate the reliability of knowledge
based on a combination of certainty and precision.
The annotated World Bank corpus will be made
available for further research on the Web.
44
References
Mona T. Diab, Lori Levin, Teruko Mitamura, Owen
Rambow, Vinodkumar Prabhakaram, and Weiwei
Guo. 2009. Committed belief annotation and tag-
ging. In Proceedings of the Third Linguistic Annota-
tion Workshop, ACL-IJCNLP, pages 68?73, Suntec,
Singapore, August. ACL and AFNLP.
Ken Hyland. 1998. Hedging in Scientific Research Ar-
ticles. John Benjamins Publishing Company, Ams-
terdam/Philadelphia.
Halil Kilicoglu and Sabine Bergler. 2008. Recogniz-
ing speculative language in biomedical research ar-
ticles: a linguistically motivated perspective. BMC
Bioinformatics, 9.
George Lakoff. 1973. Hedges: A study in meaning
criteria and the logic of fuzzy concepts. Journal of
Philosophical Logic, 2:458?508.
Roser Morante and Walter Daelemans. 2009. Learning
the scope of hedge cues in biomedical texts. In Pro-
ceedings of the Workshop on BioNLP, pages 28?36,
Boulder, Colorado, June. Association for Computa-
tional Linguistics.
Philip V. Ogren. 2006. Knowtator: a prote?ge? plug-in
for annotated corpus construction. In Proceedings of
the 2006 Conference of the North American Chapter
of the Association for Computational Linguistics on
Human Language Technology, pages 273?275, Mor-
ristown, NJ, USA. Association for Computational
Linguistics.
Dennis Reidsma and Rieks op den Akker. 2008. Ex-
ploiting ?subjective? annotations. In HumanJudge
?08: Proceedings of the Workshop on Human Judge-
ments in Computational Linguistics, pages 8?16,
Morristown, NJ, USA. Association for Computa-
tional Linguistics.
Victoria L. Rubin, Elizabeth D. Liddy, and Noriko
Kando. 2006. Certainty identification in texts: Cat-
egorization model and manual tagging results. In
Computing Affect and Attitutde in Text: Theory and
Applications. Springer.
The World Bank Group. 2010. Documents
& Reports. http://go.worldbank.org/
3BU2Z3YZ40, Accessed May 13, 2010.
Veronika Vincze, Gyo?rgy Szaarvas, Richa?rd Farkas,
Gyo?rgy Mo?ra, and Ja?nos Csirik. 2008. The bio-
scope corpus: biomedical texts annotated for uncer-
tainty, negation and their scopes. BMC Bioinformat-
ics, 9.
Janyce Wiebe, Theresa Wilson, and Claire Cardie.
2005. Annotating expressions of opinions and emo-
tions in language. Language Resources and Evalu-
ation, 39:165?210.
J. W. Wilbur, A. Rzhetsky, and H. Shatkay. 2006. New
directions in biomedical text annotation: definitions,
guidelines and corpus construction. BMC Bioinfor-
matics, 7:356+, July.
45
Proceedings of the 2012 Workshop on Biomedical Natural Language Processing (BioNLP 2012), pages 56?64,
Montre?al, Canada, June 8, 2012. c?2012 Association for Computational Linguistics
Medical diagnosis lost in translation ? Analysis of uncertainty and negation
expressions in English and Swedish clinical texts
Danielle L Mowery
University of Pittsburgh
200 Meyran Ave
Pittsburgh, PA 15260
dlm31@pitt.edu
Sumithra Velupillai
Stockholm University
164 40 Kista
Stockholm, Sweden
sumithra@dsv.su.se
Wendy W Chapman
University of California San Diego
10100 Hopkins Dr
La Jolla, CA 92093
wwchapman@ucsd.edu
Abstract
In the English clinical and biomedical text do-
mains, negation and certainty usage are two
well-studied phenomena. However, few stud-
ies have made an in-depth characterization
of uncertainties expressed in a clinical set-
ting, and compared this between different an-
notation efforts. This preliminary, qualita-
tive study attempts to 1) create a clinical un-
certainty and negation taxonomy, 2) develop
a translation map to convert annotation la-
bels from an English schema into a Swedish
schema, and 3) characterize and compare two
data sets using this taxonomy. We define
a clinical uncertainty and negation taxonomy
and a translation map for converting annota-
tion labels between two schemas and report
observed similarities and differences between
the two data sets.
1 Introduction and Background
Medical natural language processing techniques are
potentially useful for extracting information con-
tained in clinical texts, such as emergency depart-
ment reports (Meystre et al, 2008). One impor-
tant aspect to take into account when developing ac-
curate information extraction tools is the ability to
distinguish negated, affirmed, and uncertain infor-
mation (Chu et al, 2006). Several research stud-
ies have targeted this problem and created anno-
tation schemas and manually annotated reference
standards for uncertainty and negation occurrence
in news documents (Saur?? and Pustejovsky (2009),
Wiebe et al (2001), Rubin et al (2006)), biomedical
research articles (Wilbur et al (2006), Vincze et al
(2008)), and clinical narratives (Uzuner et al (2011)
and Uzuner et al (2009)). There are encoding tools
developed for automatic identification of uncertainty
and negation in English, such as ConText (Harkema
et al, 2009), which relies on heuristics and keyword
lists, and MITRE?s CARAFE (Clark et al, 2011),
which combines heuristic and statistical techniques.
However, most relevant annotation schemas, ref-
erence standards, and encoding tools are built for
English documents. For smaller languages, such as
Swedish, resources are scarce.
We present a pilot, qualitative study to compare
two different annotation schemas and subsequent
annotated corpora for uncertainty modeling of dis-
order mentions, e.g., signs, symptoms, and diseases,
in clinical texts, for two different languages: English
and Swedish. We compare these annotation schemas
and their instantiation in the two languages in an at-
tempt to gain a deeper understanding of how uncer-
tainty and negation are expressed in different clini-
cal texts with an emphasis on creating a portable un-
certainty and negation application that generalizes
among clinical texts of different languages.
This pilot study is motivated for at least two
reasons. First, little attention has been given to
mapping, characterizing, or comparing annotation
schemas built for different languages or to character-
izing different types of uncertainty expressions and
the intention underlying those expressions. Such
knowledge is needed for building information ex-
traction tools that can accurately identify or track
differential diagnoses over time, particularly when
medical reasoning can be laden with uncertainty
about a disorder?s existence or change over time.
56
Second, building new resources for small lan-
guages is time consuming. Utilizing existing tools
and techniques already developed for one language,
such as English, could be an efficient way of devel-
oping new useful tools for other less exploited lan-
guages, such as Swedish.
Our overall goal is to move towards improving au-
tomatic information extraction from clinical texts by
leveraging language differences and similarities. In
order to address this issue, our aims in this study
are to 1) create a taxonomy for deepened charac-
terization of how uncertainty and negation is ex-
pressed in clinical texts, 2) compare two existing un-
certainty and negation annotation schemas from this
perspective, and 3) compare differences and similar-
ities in expressions of uncertainty and negation be-
tween two languages: English and Swedish.
2 Methods
In this pilot, qualitative comparison study, we used
grounded theory (Strauss and Corbin, 1990) to in-
ductively identify themes that characterize clini-
cal uncertainty and negation expressed in both En-
glish (University of Pittsburgh Medical Center) and
Swedish (Karolinska University Hospital) research
data sets derived from emergency department re-
ports.
2.1 Uncertainty/negation annotation schemas
Two independently developed annotation schemas
were used to annotate disorder mentions in the
clinical texts: a schema developed for English re-
ports (Mowery et al (2012)) and one for Swedish
(Velupillai et al (2011)). Each disorder mention
was pre-annotated and constituted the input to a sep-
arate set of annotators, who assigned values to a set
of attributes defined in the schema. For instance, in
the sentence ?Patient with possible pneumonia.?, an-
notators for the English data set assigned values to
four attributes for the instance of pneumonia:
? Existence(yes, no): whether the disorder was ever present
? AspectualPhase(initiation, continuation, culmination, un-
marked): the stage of the disorder in its progression
? Certainty(low, moderate, high, unmarked): amount of certainty
expressed about whether the disorder exists
? MentalState(yes, no): whether an outward thought or feeling
about the disorder?s existence is mentioned
In the Swedish schema, annotators assigned val-
ues to two attributes:
? Polarity(positive, negative): whether a disorder mention is in the
positive or negative polarity, i.e., affirmed (positive) or negated
(negative)
? Certainty(possibly, probably, certainly): gradation of certainty
for a disorder mention, to be assigned with a polarity value.
2.2 Data Sets
The English data set included 30 de-identified, full-
length emergency department reports annotated with
283 disorders related to influenza-like illnesses by
a board-certified infectious disease physician. Each
disorder was annotated with four attributes ? exis-
tence, aspectual phase, certainty and mental state ?
by two independent annotators (including DM) who
came to consensus after reviewing disagreements.
The Swedish data set included 1,297 assessment
sections from emergency department reports anno-
tated with approx. 2,000 disorders, automatically
marked from a manually created list of approx-
imately 300 unique disorders by two physicians.
The two physicians annotated each disorder mention
with attributes of polarity and certainty. A random
subset of approx. 200 annotated disorder mentions
from the data set were used for this qualitative study.
2.3 Study Process
In order to better understand how physicians de-
scribe uncertainty of the presence or absence of a
disorder, we evaluated the annotations from the two
data sets as follows: 1) created a clinical uncertainty
and negation taxonomy, 2) developed a translation
map for mapping attributes and values from the En-
glish schema into the Swedish schema, and 3) char-
acterized and compared both data sets and languages
using the taxonomy.
To create the uncertainty and negation taxonomy,
we conducted a literature review of recent annota-
tion schemas (e.g. Vincze et al (2008)), assignment
applications (e.g. Uzuner et al (2011), Harkema
et al (2009), Clark et al (2011), Chapman et al
(2011)), and observational studies (Lingard et al,
2003) about uncertainty or negation in the clinical
domain. From our review, we created a clinical tax-
onomy describing notable characteristics of uncer-
tainty and negation, which were added to and re-
fined using grounded theory, by inspecting the dis-
order annotations in our data sets and documenting
57
emerging themes consistent with issues found from
the literature review. For instance, one characteristic
of negation annotations found in the literature and in
our data sets is the existence of a lexical cue indicat-
ing that a disorder is negated, and the lexical cue can
occur before, within, or after the disorder mention.
The characteristics included in the taxonomy repre-
sent features describing the attributes of uncertainty
and negation in the data sets (see Section 3.1).
To develop the translation map between certainty
and negation values from each annotation schema,
authors DM and SV jointly reviewed each annotated
disorder mention from the English data set and as-
signed a Swedish polarity and certainty label, then
devised a map from the English schema into the
Swedish schema.
To characterize and compare manifestations of
uncertainty and negation using annotations from the
two data sets, DM and SV annotated each disorder
mention in both data sets with the features in the
clinical uncertainty and negation taxonomy. In the
English data set, each disorder was annotated by DM
and adjudicated by SV. In the Swedish data set, each
disorder was annotated by SV then translated into
English for adjudication by DM.
3 Results
3.1 Clinical Uncertainty and Negation
Taxonomy
We developed a clinical uncertainty and negation
taxonomy to characterize the linguistic manifesta-
tions of uncertainty and negation in clinical text
(Figure 1). We found three high-level features in
the literature and in our data sets: position of lexical
cue (i.e., position of the lexical expression indicat-
ing uncertainty or negation with respect to the dis-
order), opinion source (i.e. person believing there
is absence, presence, or uncertainty), and evidence
evaluation (i.e., reason for the uncertainty or nega-
tion belief).
Position of lexical cue demonstrated itself in the
data sets in three non-mutually exclusive ways:
? pre-disorder (lexical cue precedes the disorder) ?Patient denies
chest pain.?
? intra-disorder (lexical cue occurs within the name of the disor-
der) ?x-ray...possibly be indicative of pneumonia.?
? post-disorder (lexical cue occurs after the disorder)
?abdominal cramping..is unlikely.?
Opinion source exhibited the following values:
? dictating physician (dictating physician alone expressed pres-
ence, absence, or uncertainty regarding the disorder) ?I suspect
bacterial pneumonia.?
? dictating physician with consultation (dictating physician explic-
itly includes other clinical professional in statement) ?Discussing
with Dr. **NAME**, pneumonia can not be excluded.?
? other clinical care providers (other clinical team members ex-
plicitly stated as expressing presence, absence or uncertainty re-
garding the disorder) ?per patient?s primary doctor, pneumonia
is suspected.?
? patient (patient expressed presence, absence, or uncertainty re-
garding the disorder) ?Pt doesn?t think she has pneumonia.?
? unknown (ambiguous who is expressing presence, absence, or
uncertainty regarding the disorder) ?there was a short episode of
coughing.?
Evidence evaluation includes a modified subset
of values found in the model of uncertainty pro-
posed by Lingard et al (2003) to connote perceived
reasons for the provider uncertainty (and negation)
about the disorder mention as used in our data sets.
? limits of evidence (data limitations for hypothesis testing), one
diagnosis
? evidence contradicts (data contradicts expected hypothe-
sis), ?Blood test normal, but we still think Lyme disease.?
? evidence needed (evidence unavailable to test hypoth-
esis) ?Waiting for x-ray results to determine if it?s a
femur fracture.?
? evidence not convincing, but diagnosis asserted (data
doesn?t fully support proposed hypothesis), ?Slightly el-
evated levels of WBCs suggests infection.?
? limits of evidence, more than one diagnosis
? differential diagnoses enumerated (competing diagnoses
reasoned), ?bacterial infection vs. viral infection.?
? limits in source of evidence (untrusted evidence)
? non-clinical source (from non-provider source), ?Pt can?t
remember if she was diagnosed with COPD.?
? clinical source (from provider source), ?I do not agree
with Dr. X?s diagnosis of meningitis.?
? test source (from test e.g., poor quality), ?We cannot de-
termine from the x-ray if the mass is fluid or a tumor.?
? limitless possibilities (large number of likely diagnoses so diag-
nosis defaulted to most likely), ?This is probably an infection of
some sort.?
? other (no evidence limitation)
? asserting a diagnosis or disorder as affirmed (positive
case), ?Confirms nausea.?
? asserting a diagnosis or disorder as negated (negative
case), ?No vomiting.?
58
Figure 1: Uncertainty and negation taxonomy with features ? Position of lexical cue, Opinion source and Evidence evaluation ?
with corresponding values (nested lines and sub-lines).
3.2 Translation Map
In order to compare annotations between the data
sets, we developed a mapping procedure for convert-
ing the four annotated attribute values from the En-
glish schema into the two annotated attribute values
from the Swedish schema. This mapping procedure
uses two normalization steps, negation and certainty
(see Figure 2).
Using Figure 2, we explain the mapping proce-
dure to convert English annotations into Swedish
annotations. Our steps and rules are applied with
precedence, top down and left to right. For ?I have
no suspicion for bacterial infection for this patient?,
English annotations are Existence(no) AND Aspec-
tualPhase(null) AND Certainty(high) AND Men-
talState(yes), and Swedish annotations are Polar-
ity(negative) AND Certainty(probably). The map-
ping procedure applies two normalization steps,
negation and uncertainty, with the following rules.
The first step is negation normalization to convert
Existence and Aspectual Phase into Polarity anno-
tations. In this example, Existence(no) ? Polar-
ity(negative).
The second step is certainty normalization with
up to two sub steps. For Certainty mapping, in sum-
mary, map English NOT Certainty(unmarked) to
Swedish Certainty level, e.g., Certainty(high)
? Certainty(probably). For MentalState
mapping, if English Certainty(unmarked) AND
MentalState(yes), map to either Swedish Cer-
tainty(probably) OR Certainty(possibly) using
your best judgment; otherwise, map to Cer-
tainty(certainly). For our example sentence,
Certainty mapping was sufficient to map from the
English to the Swedish Certainty levels.
We found that these two schemas were mappable.
Despite the binary mapping splits from English Cer-
tainty(Moderate) ? Swedish Certainty(possibly)
OR Certainty(probably) and judgment calls neces-
sary for MentalState mapping, few annotations were
not easily mapped.
3.3 Characterization of English and Swedish
Data sets
In this study, we characterized our data sets accord-
ing to a clinical uncertainty and negation taxonomy
comprised of three concepts ? position of lexical
cue, opinion source, and evidence evaluation.
3.3.1 Position of lexical cue
In Table 1, we show examples of phrases from each
data set representing the Polarity and Certainty lev-
els in the taxonomy. In our data set, we did not
explicitly annotate markers for the highest certainty
levels in the positive polarity, such as ?definitely
has?. We did not encounter any of these cases in the
59
Figure 2: Map between values for attributes in Swedish and English schemas. Bolded rules indicate the rules used to assign values
to the example sentence (English sentence on top).
data set. We observed that most uncertainty expres-
sions precede a disorder mention. Few expressions
both precede and follow the disorder mention, or
within the disorder mention itself. We observed that
most expressions of uncertainty are conveyed using
positive polarity gradations such as ?probably? and
?possibly?, for example ?likely?, ?appears to have?,
?signs of?. Lexical cues of low levels of certainty in
the negative polarity were rare.
3.3.2 Opinion source
In Table 2, we report examples of the various in-
dividuals ? dictating physician, dictating physician
with consultation, other clinical care providers, pa-
tient, unknown ? that are the source of the belief
state for uncertainty about a disorder. We observed
explicit judgments or mental postulations e.g., ?I
judge? or implied speculations in which the physi-
cian was not the subject and passive expressions
were used e.g., ?patient appears to have?. In cases
of dictating physician with consultation, the physi-
cian speculated about the disorder using references
to other providers consulted to strengthen the as-
sessment e.g., ?Discussing with Dr...?. In cases of
other clinical care providers, there was no owner-
ship on the part of the dictating physician, but of
other members of the clinical care team e.g., ?Con-
sulting Attending (Infection) thinks...?. In cases for
patient, the patient is conveying statements of con-
fusion with respect to self-diagnosing e.g., ?Pat. re-
ports that she finds it difficult to discern...?. We ob-
served no expressions of uncertainty owned by the
patient in the English data set or by a relative in the
Swedish data set. In the unknown case, it is unclear
from the context of the report whether the specu-
lation is on the part of the physician to believe the
symptom reported or the relative unsure about re-
porting the symptoms e.g., ?there was apparently?.
3.3.3 Evidence evaluation
Below we list examples of the different rea-
sons for uncertainties that were identified. Not all
types were observed in both corpora (Not observed).
limits of evidence, one diagnosis
- evidence contradicts ? English: ?Likely upper GI bleed
with elevated bun, but normal h and h.?; Swedish: ?Kon-
sulterar infektionsjour som anser viros vara osannolikt
med tanke pa? normalt leverstatus. (Consulting Attend-
ing (infection) who thinks that virosis is improbable given
normal liver status.)?
- evidence needed ? English: ?chest x-ray was ordered
to rule out TB.?; Swedish: ?Diskuterar med RAH-jour;
vi bo?rjar utredning med CT-skalle med kontrast pa? mis-
stanke om metastaser och na?gon form av epileptiskt anfall
(Discussion with Attendant [CLINIC]; we start inves-
60
Table 1: Common lexical cues and their relative position to the disorder mention: Pre-disorder: uncertainty marker before disor-
der, Intra-disorder: uncertainty marker inside disorder, Post-disorder: uncertainty marker after disorder, }= schema compatibil-
ity/neutral case.
Table 2: Opinion source of uncertainty or negation types with English and Swedish examples.
tigation with CT-brain with contrast on suspicion for
metastasis and some form of epileptic seizure.)?
- evidence not convincing, but diagnosis asserted ? En-
glish: Not observed; Swedish: ?Fo?rmodligen en viros
eftersom man kan se en viss lymfocytopeni i diff (Proba-
bly a virosis since there is some lymphocyte in blood cell
count.)?
limits of evidence, more than one diagnosis
- differential diagnoses enumerated ? English: ?ques-
tionable right-sided increased density on the right side
of the chest x-ray that could possibly be indicative of
a pneumonia versus increased pulmonary vasculature?;
Swedish: ?Fo?refaller neurologiskt, blo?dning? Infarkt?
(Appears neurological, bleeding? Infarction?)?
limits in source of evidence
- non-clinical source ? English: ?I am not convinced that
he is perfectly clear on his situation..?; Swedish: ?Pat
uppger att hon har sva?rt att skilja pa? panika?ngest och an-
dra symtom. (Pat. reports that she finds it difficult to
discern panick disorder from other symptoms...)?
- clinical source ? English: ?there was no definite diagno-
sis and they thought it was a viral syndrome of unknown
type..?; Swedish: Not observed
- test source ? English: ?..confusion was possible related
a TIA without much facial droop appreciated on my
physical exam?; Swedish: ?Ter sig mest sannolikt som
reumatoid artrit ba?de klinisk och lab-ma?ssigt (Seems like
it most probably is rheumatoid arthritis both clinically
and lab-wise.)?
limitless possibilities ? English: ?I think this is probably a
viral problem.?; Swedish: ?Pat bedo?mes ha en fo?rkylning,
troligen virusinfektion. (Patient is evaluated as having a cold,
probably a virus infection.)?
other
61
- asserting dx or disorder as affirmed ? English: ?I sus-
pect that colon cancer is both the cause of the patient?s
bleeding..?; Swedish: Not observed
- asserting dx or disorder as negated ? English: ?...her
fever has abated.?; Swedish: Not observed
In many cases, the local context was sufficient for
understanding the evidential origins for uncertainty.
When a single disorder was mentioned, uncertainty
was due to data insufficient to make a definitive di-
agnosis because it contradicted a hypothesis, was
unavailable, or was not convincing. For instance,
data was to be ordered and the opportunity to inter-
pret it had not presented itself, such as ?..was or-
dered to rule out TB? or ?..start investigation with
CT-brain with contrast..?. In few cases, more than
one diagnosis was being enumerated due to a lim-
itation in the evidence or data gathered e.g., ?Ap-
pears neurological, bleeding? Infarction??. We ob-
served cases in which the source of the evidence pro-
duced uncertainty including both non-clinical and
clinical sources (care providers consulted and tests
produced). In cases of limitless possibilities, the
physician resorted to a common, default diagnosis
e.g., ?probably a virus infection?. Limitations of ev-
idence from a clinical source were not found in the
Swedish data set and few were found in the English
data set. We expect that more examples of this cat-
egory would be found in e.g. radiology reports in
which the quality of the image is a critical factor in
making an interpretation.
4 Discussion and Conclusion
From the resulting clinical taxonomy and charac-
terization, we observe some general differences and
similarities between the two data sets and languages.
The Swedish assessment entries are more verbose
compared to the English medical records in terms
of a more detailed account of the uncertainty and
what is being done by whom to derive a diagnosis
from a disorder mention. This might reflect cultural
differences in how documentation is both produced
and used. Differential diagnoses are often listed with
question marks (???) in the Swedish set, e.g., ?Dis-
order 1? Disorder 2? Disorder 3??, whereas in the
English data set enumerations are either listed or
competing, e.g., ?disorder 1 vs. disorder 2?. De-
spite these differences, there are many similarities
between the two data sets.
Mapping observations from the English schema
into the Swedish schema was not complicated
despite the difference in the modeled attributes.
In most cases, we determined that designating
attribute-value rules for negation and certainty nor-
malization steps was sufficient to accurately map ob-
servations between the language schemas without
changing an observation?s semantics. This finding
suggests that simple heuristics can be used to trans-
late annotations made from English trained tools
into the Swedish schema values.
The majority of the lexical markers are pre-
positioned in both languages, and the majority of
these markers are similar across the two languages,
e.g., ?likely?, ?possible?, ?suspicion for?. How-
ever, inflections and variants are more common in
Swedish, and the language allows for free word or-
der, this relation needs to be studied further. The
default case, i.e. affirmed, or certainly positive, was
rarely expressed through lexical markers.
When it comes to the opinion source of an un-
certainty or negation, we observed a pattern in the
use of passive voice, e.g. ?it was felt?, indicating
avoidance to commitment in a statement. Accurate
extraction of the opinion source of an expression
has important implications for a system that, for in-
stance, tracks the reasoning about a patient case over
time by source. This has been recognized and incor-
porated in other annotation efforts, for example for
news documents (Saur?? and Pustejovsky, 2009). In
the English data set, no cases of self-diagnosing are
found, i.e. a patient owning the expressed uncer-
tainty. In both data sets, an implicit dictating physi-
cian source is most common, i.e. there is no explicit
use of pronouns indicating the opinion holder. In
most cases it is clear that it is the writer?s (i.e. the
dictating physician?s) opinion that is expressed, but
in some cases, a larger context is needed for this
knowledge to be resolved.
Reviewing the evidential origins or reason for ex-
pressed uncertainty, for both the Swedish and En-
glish data sets, the category ?limits of evidence? is
most common. This reflects a clinical reality, where
many disorders require test results, radiology find-
ings and other similar procedures before ascertain-
ing a diagnosis. Although most cases of uncertainty
are manifested and strengthened through a lexical
62
marker, there are also instances where the uncer-
tainty is evident without such explicit markers, e.g.
the ordering of a test may in itself indicate uncer-
tainty.
4.1 Limitations
There are several limitations of this study. The
Swedish data set only contains parts of the medi-
cal record and the English data set is very small.
In the creation of the taxonomy and characteristics,
we have not focused on studying uncertainty lev-
els, i.e. distinctions between ?possibly? and ?prob-
ably?. The values of our taxonomy are preliminary
and may change as we develop the size of our data
set. Additionally, we only studied emergency de-
partment reports. We need to study other report
types to evaluate the generalizability of the taxon-
omy.
The two compared languages both origin from the
same language family (Germanic), which limits gen-
eralizability for other languages. Furthermore, the
definitions of disorders in the two sets differ to some
extent, i.e., English disorders are related to specific
influenza-like illnesses and Swedish to more general
disorders found in emergency departments.
4.2 Comparison to related work
Annotation schemas and reference standards for un-
certainty and negation have been created from dif-
ferent perspectives, for different levels and pur-
poses. The BioScope Corpus, for instance, contains
sentence-level uncertainty annotations with token-
level annotations for speculation and negation cues,
along with their linguistic scope (Vincze et al,
2008). In Wilbur et al (2006), five qualitative di-
mensions for characterizing biomedical articles are
defined, including levels of certainty. In the 2010
i2b2/VA Challenge on concepts, assertions and re-
lations in clinical texts, medical problem concepts
were annotated. The assertion task included six an-
notation classes (present, absent, possible, hypothet-
ical, conditional, not associated with the patient),
to be assigned to each medical problem concept
(Uzuner et al, 2011). Vincze et al (2011) present
a quantitative comparison of the intersection of two
English corpora annotated for negation and specula-
tion (BioScope and Genia Event) from two different
perspectives (linguistic and event-oriented).
We extend these schemas by characterizing the
underlying meaning and distinctions evident by the
linguistic expressions used to indicate uncertainty
and negation in the clinical domain and by exploring
the relationship between uncertainty and negation,
through an analysis and comparison of two differ-
ent annotation schemas. However, this study is not a
proposal for mapping to these schemas or others.
From an application perspective, uncertainty and
negation handling have been included in rule-based
systems such as NegEx and ConText, applied on dis-
order mentions. In Chapman et al (2011), a gener-
alized version of ConText is presented, with uncer-
tainty values (probably, definitely) linked to either a
positive or negative assertion, with an added indeter-
minate value. A previous study has shown promis-
ing results for adapting NegEx to Swedish (Skepp-
stedt, 2011), indicating that further extensions and
adaptations between the two languages for e.g. un-
certainty modeling should be viable. Machine-
learning based approaches outperform rule-based
for assertion classification according to results pre-
sented in Uzuner et al (2009). A machine-learning
approach was also used in the top performing sys-
tem in the 2010 i2b2/VA Challenge assertion task
(de Bruijn et al, 2011).
4.3 Implications and future work
With uncertainty lexicons for both Swedish and En-
glish, we hypothesize that we will be able to ex-
tend ConText to handle uncertainties in English as
well as in Swedish. This enables both improve-
ments over the existing system and the possibilities
of further comparing system performances between
languages. We will also experiment with machine-
learning approaches to detect and annotate uncer-
tainty and negation. We plan to extend both data
sets, the English data set using semi-automatically
translated disorders marked in the Swedish data set
to encode new disorder mentions, and the Swedish
data set by extracting the full medical records, thus
creating a larger set for comparison. We will extend
the taxonomy as needed e.g., syntactic and semantic
patterns, and investigate how to integrate the clini-
cal taxonomy to inform ConText by providing more
granular descriptions of the motivation behind the
uncertainty, thus bringing us closer to natural lan-
guage understanding.
63
Acknowledgments
For the English and Swedish data sets, we obtained
approval from the University of Pittsburgh IRB and
the Regional Ethical Review Board in Stockholm
(Etikpro?vningsna?mnden i Stockholm). The study is
part of the Interlock project, funded by the Stock-
holm University Academic Initiative and partially
funded by NLM Fellowship 5T15LM007059. Lex-
icons and probabilities will be made available and
updated on the iDASH NLP ecosystem under Re-
sources: http://idash.ucsd.edu/nlp/natural-language-
processing-nlp-ecosystem.
References
B. E. Chapman, S. Lee, H. Peter Kang, and W. W. Chap-
man. 2011. Document-level Classification of CT Pul-
monary Angiography Reports Based on an Extension
of the ConText Algorithm. Journal of Biomedical In-
formatics, 44:728?737.
D. Chu, J.N. Dowling, and WW Chapman. 2006. Eval-
uating the Effectiveness of Four Contextual Features
in Classifying Annotated Clinical Conditions in Emer-
gency Department Reports. In AMIA Annu Symp Proc,
pages 141?145.
C. Clark, J. Aberdeen, M. Coarr, D. Tresner-Kirsh,
B. Wellner, A. Yeh, and L. Hirschman. 2011. MITRE
system for Clinical Assertion Status Classification. J
Am Med Inform Assoc, 11(18):563?567.
B. de Bruijn, C. Cherry, S. Kiritchenko, J. Martin, and
X. Zhu. 2011. Machine-learned Solutions for Three
Stages of Clinical Information Extraction: The State of
the Art at i2b2 2010. Journal of the American Medical
Informatics Association, 18:557?562.
H. Harkema, J. N. Dowling, T. Thornblade, and W. W.
Chapman. 2009. ConText: An Algorithm for De-
termining Negation, Experiencer, and Temporal Status
from Clinical Reports. Journal of Biomedical Infor-
matics, 42:839?851.
L. Lingard, K. Garwood, C. F. Schryer, and M. M. Spaf-
ford. 2003. A Certain Art of Uncertainty: Case Pre-
sentation and the Development of Professional Iden-
tity. Social science medicine, 56(3):603?616.
S. M. Meystre, G. K. Savova, K. C. Kipper-Schuler, and
John E. Hurdle. 2008. Extracting Information from
Textual Documents in the Electronic Health Record: A
Review of Recent Research. IMIA Yearbook of Medi-
cal Informatics 2008. 47 Suppl 1:138-154.
D. Mowery, P. Jordan, J.M. Wiebe, H. Harkema, and
W.W. Chapman. 2012. Semantic Annotation of Clini-
cal Text: A Pilot Study. Unpublished.
V. L. Rubin, E. D. Liddy, and N. Kando. 2006. Cer-
tainty Identification in Texts: Categorization Model
and Manual Tagging Results. In Computing Affect and
Attitutde in Text: Theory and Applications. Springer.
R. Saur?? and J. Pustejovsky. 2009. FactBank: A Corpus
Annotated with Event Factuality. Language Resources
and Evaluation, 43(3):227?268?268, September.
M. Skeppstedt. 2011. Negation Detection in Swedish
Clinical Text: An Adaptation of NegEx to Swedish.
Journal of Biomedical Semantics, 2(Suppl. 3):S3.
A. L. Strauss and J. Corbin. 1990. Basics of Qual-
itative Research: Grounded Theory Procedures and
Techniques. Sage.
O?. Uzuner, X. Zhang, and T. Sibanda. 2009. Ma-
chine Learning and Rule-based Approaches to Asser-
tion Classification. Journal of the American Medical
Informatics Association, 16(1):109?115.
O?. Uzuner, B. R. South, S. Shen, and S. L. DuVall. 2011.
2010 i2b2/VA Challenge on Concepts, Assertions, and
Relations in Clinical Text. JAMIA, 18(5):552?556.
S. Velupillai, H. Dalianis, and M. Kvist. 2011. Factual-
ity Levels of Diagnoses in Swedish Clinical Text. In
A. Moen, S. K. Andersen, J. Aarts, and P. Hurlen, ed-
itors, Proc. XXIII International Conference of the Eu-
ropean Federation for Medical Informatics (User Cen-
tred Networked Health Care), pages 559 ? 563, Oslo,
August. IOS Press.
V. Vincze, G. Szarvas, R. Farkas, G. Mo?ra, and J. Csirik.
2008. The BioScope Corpus: Biomedical Texts An-
notated for Uncertainty, Negation and Their Scopes.
BMC Bioinformatics, 9(S-11).
V. Vincze, G. Szarvas, G. M?ora, T. Ohta, and R. Farkas.
2011. Linguistic Scope-based and Biological Event-
based Speculation and Negation Annotations in the
BioScope and Genia Event Corpora. Journal of
Biomedical Semantics, 2(Suppl. 5):S8.
J. Wiebe, R. Bruce, M. Bell, M. Martin, and T. Wilson.
2001. A Corpus Study of Evaluative and Specula-
tive Language. In Proceedings of the Second SIG-
dial Workshop on Discourse and Dialogue - Volume
16, SIGDIAL ?01, pages 1?10, Stroudsburg, PA, USA.
Association for Computational Linguistics.
J. W. Wilbur, A. Rzhetsky, and H. Shatkay. 2006.
New Directions in Biomedical Text Annotation: Def-
initions, Guidelines and Corpus Construction. BMC
Bioinformatics, 7:356+, July.
64
Proceedings of the 3rd Workshop on Predicting and Improving Text Readability for Target Reader Populations (PITR) @ EACL 2014, pages 74?83,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Improving Readability of Swedish Electronic Health Records
through Lexical Simplification: First Results
Gintar
?
e Grigonyt
?
e
a
, Maria Kvist
bc
, Sumithra Velupillai
b
, Mats Wir
?
en
a
a
Department of Linguistics, Stockholm University, Sweden
b
Department of Computer and Systems Sciences, Stockholm University, Sweden
c
Department of Learning, Informatics, Management and Ethics, Karolinska Institutet, Sweden
gintare@ling.su.se, maria.kvist@karolinska.se,
sumithra@dsv.su.se, mats.wiren@ling.su.se
Abstract
This paper describes part of an ongo-
ing effort to improve the readability of
Swedish electronic health records (EHRs).
An EHR contains systematic documenta-
tion of a single patient?s medical history
across time, entered by healthcare pro-
fessionals with the purpose of enabling
safe and informed care. Linguistically,
medical records exemplify a highly spe-
cialised domain, which can be superfi-
cially characterised as having telegraphic
sentences involving displaced or missing
words, abundant abbreviations, spelling
variations including misspellings, and ter-
minology. We report results on lexical
simplification of Swedish EHRs, by which
we mean detecting the unknown, out-of-
dictionary words and trying to resolve
them either as compounded known words,
abbreviations or misspellings.
1 Introduction
An electronic health record (EHR; Swedish: pa-
tientjournal) contains systematic documentation
of a single patient?s medical history across time,
entered by healthcare professionals with the pur-
pose of enabling safe and informed care. The
value of EHRs is further increased by the fact that
they provide a source of information for statis-
tics and research, and a documentation for the pa-
tient through the Swedish Patient Data Act. EHRs
collect information from a range of sources, such
as administration of drugs and therapies, test re-
sults, preoperative notes, operative notes, progress
notes, discharge notes, etc.
EHRs contain both structured parts (such as
details about the patient, lab results, diagnostic
codes, etc.) and unstructured parts (in the form of
free text). The free-text part of EHRs is referred
to as clinical text, as opposed to the kind of gen-
eral medical text found in medical journals, books
or web pages containing information about health
care. Clinical texts have many subdomains de-
pending on the medical speciality of the writer and
the intended reader. There are more formal kinds
of EHRs, such as discharge summaries and radiol-
ogy reports, directed to other physicians, and more
informal kinds such as daily notes, produced by
nurses and physicians (as memory notes for them-
selves or for the team). In spite of the Patient Data
Act, the patient is seldom seen as a receiver or
reader of the document.
Linguistically, health records exemplify a
highly specialised domain, which can be super-
ficially characterised as having telegraphic sen-
tences involving displaced or missing words,
abundant abbreviations, undisputed misspellings,
spelling variation which may or may not amount to
misspellings depending on the degree of prescrip-
tivism, and terminology. While this specialised
style has evolved as an efficient means of com-
munication between healthcare professionals, it
presents formidable challenges for laymen trying
to decode it.
In spite of this, there has been no previous work
on the problem of automatically improving the
readability of Swedish EHRs. As an initial at-
tempt in this direction, we provide an automatic
approach to the problem of lexical simplification,
by which we mean detecting the unknown, out of
dictionary words and trying to resolve them either
as compounds generated from known words, as
abbreviations or as misspellings. As an additional
result, we obtain a distribution of how prevalent
these problems are in the clinical domain.
2 Lexical challenges to readability of
EHRs
A major reason for the obstacles to readability of
EHRs for laymen stems from the fact that they
74
are written under time pressure by professionals,
for professionals (Kvist et al., 2011). This re-
sults in a telegraphic style, with omissions, ab-
breviations and misspellings, as reported for sev-
eral languages including Swedish, Finnish, En-
glish, French, Hungarian and German (Laippala
et al., 2009; Friedman et al., 2002; Hag`ege et
al., 2011; Surj?an and H?eja, 2003; Bretschneider et
al., 2013). The omitted words are often subjects,
verbs, prepositions and articles (Friedman et al.,
2002; Bretschneider et al., 2013).
Unsurprisingly, medical terminology abounds
in EHRs. What makes this problem an even
greater obstacle to readability is that many medical
terms (and their inflections) originate from Latin
or Greek. Different languages have adapted these
terms differently (Bretschneider et al., 2013). The
Swedish medical terminology went through a
change during the 1990s due to a swedification
of diagnostic expressions performed in the 1987
update of the Swedish version of ICD, the Inter-
national Classification of Diseases
1
. For this ver-
sion, the Swedish National Board of Health and
Welfare decided to partly change the terminology
of traditional Latin- and Greek-rooted words to a
spelling compatible to Swedish spelling rules, as
well as abandoning the original rules for inflec-
tion (Smedby, 1991). In this spelling reform, c
and ch pronounced as k was changed to k, ph was
changed to f, th to t, and oe was changed to e.
For example, the technical term for cholecsystitis
(inflammation of the gall bladder) is spelled kole-
cystit in contemporary Swedish, thus following the
convention of changing ch to k and removing the
Latin ending of -is. The results
2
of exact match-
ing to kolecystit (English: cholecystitis) and some
presumed spelling variants clearly demonstrate the
slow progress (Table 1).
As medical literature is predominantly written
in English nowadays, physicians increasingly get
exposed to the English spelling of Latin and Greek
words rather than the Swedish one. This has re-
sulted in a multitude of alternate spellings of sev-
eral medical terms. For example, tachycardia
(rapid heart) is correctly spelled takykardi, but is
1
http://www.who.int/classifications/
icd/en/
2
Based on a subset of the Stockholm Electronic Pa-
tient Record Corpus (Dalianis et al., 2012) of 100,000 daily
notes (DAY) written by physicians of varying disciplines (4
mill. tokens) and 435,000 radiology reports (X-RAY) writ-
ten by radiologists (20 mill. tokens). KORP: http://
spraakbanken.gu.se/korp/
Term KORP DAY X-RAY
kolecystit 51 48 84
colecystit 0 1 8
cholecystit 4 88 1613
Table 1: Alternate spellings of the Swedish
medical term kolecystit (eng. cholecystitis) in
the Swedish corpus collection Korp, daily notes
(DAY) and radiology reports (X-RAY), respec-
tively. Correct spelling in bold.
also frequently found as tachycardi, tachykardi,
and takycardi (Kvist et al., 2011). A similar
French study found this kind of spelling variation
to be abundant as well (Ruch et al., 2003).
EHRs also contain neologisms. These are often
verbs, typically describing events relating to the
patient in active form, such as ?the patient is in-
farcting? (Swedish: patienten infarcerar) instead
of the unintentional ?the patient is having a my-
ocardial infarction?. Similar phenomena are de-
scribed by Josefsson (1999).
Abbreviations and acronyms in EHRs can fol-
low standardised writing rules or be ad hoc (Liu
et al., 2001). They are often domain-specific
and may be found in medical dictionaries such
as MeSH
3
and Snomed CT
4
. For instance, 18 of
the 100 most common words in Swedish radiol-
ogy reports were abbreviations, and 10 of them
were domain-specific (Kvist and Velupillai, 2013).
Because many medical terms are multiword ex-
pressions that are repeated frequently in a pa-
tient?s EHR, the use of acronyms is very common.
Skeppstedt et al. (2012) showed that 14% of di-
agnostic expressions were abbreviated in Swedish
clinical text.
Abbreviations are often ambiguous. As an
example, 33% of the short abbreviations in the
UMLS terminology are ambiguous (Liu et al.,
2001). Pakhomov et al. (2005) found that the ab-
breviation RA had more than 20 expansions in the
UMLS terminology alone. Furthermore, a certain
word or expression can be shortened in several dif-
ferent ways. For instance, in a Swedish intensive
care unit, the drug Noradrenalin was creatively
written in 60 different ways by the nurses (Allvin
et al., 2011).
It should be noted that speech recognition, al-
though common in many hospitals around the
3
www.ncbi.nlm.nih.gov
4
http://www.ihtsdo.org/
75
world, has not been introduced in Sweden, and
many physicians and all nurses type the notes
themselves. This is one explanation to the vari-
ation with respect to abbreviations.
User studies have shown that the greatest bar-
riers for patients lie mainly in the frequent use
of abbreviations, jargon and technical terminol-
ogy (Pyper et al., 2004; Keselman et al., 2007;
Adnan et al., 2010). The most common com-
prehension errors made by laymen concern clini-
cal concepts, medical terminology and medication
names. Furthermore, there are great challenges for
higher-level processing like syntax and semantics
(Meystre et al., 2008; Wu et al., 2013). The re-
search presented in this paper focuses on lexical
simplification of clinical text.
3 Related research
We are aware of several efforts to construct au-
tomated text simplification tools for clinical text
in English (Kandula et al., 2010; Patrick et al.,
2010). For Swedish, there are few studies on med-
ical language from a readability perspective. Borin
et al. (2009) present a thorough investigation on
Swedish (and English) medical language, but EHR
texts are explicitly not included. This section sum-
marizes research on Swedish (clinical) text with
respect to lexical simplification by handling of ab-
breviations, terminology and spelling correction.
3.1 Abbreviation detection
Abbreviation identification in English biomedical
and clinical texts has been studied extensively (e.g.
Xu et al. (2007), Liu et al. (2001)). For detec-
tion of Swedish medical abbreviations, there are
fewer studies. Dann?ells (2006) reports detection
of acronyms in medical journal text with 98% re-
call and 94% precision by using part of speech
information and heuristic rules. Clinical Swedish
presents greater problems than medical texts, be-
cause of ad hoc abbreviations and noisier text. By
using lexicons and a few heuristic rules, Isenius et
al. (2012) report the best F-score of 79% for ab-
breviation detection in clinical Swedish.
3.2 Compound splitting
Good compound analysis is critical especially for
languages whose orthographies concatenate com-
pound components. Swedish is among those lan-
guages, in which every such concatenation thus
corresponds to a word. The most common ap-
proach to compound splitting is to base it on a lex-
icon providing restrictions on how different word
forms can be used for generating compounds. For
example, Sj?obergh and Kann (2006) used a lex-
icon derived from SAOL (the Swedish Academy
word list), and
?
Ostling and Wir?en (2013) used the
SALDO lexicon of Swedish morphology (Borin
and Forsberg, 2009). With this kind of approach,
compound splitting is usually very reliable for
genres like newspaper text, with typical accuracies
for Swedish around 97%, but performs poorer in
domain specific genres.
3.3 Terminology detection
The detection of English medical terminology is
a widely researched area. An example of term
detection in English clinical texts is Wang and
Patrick (2009) work based on rule-based and ma-
chine learning methods, reporting 84% precision.
For Swedish clinical text, Kokkinakis and
Thurin (2007) have employed domain terminol-
ogy matching and reached 98% precision and 87%
recall in detecting terms of disorders. Using sim-
ilar approaches, Skeppstedt et al. (2012), reached
75% precision and 55% recall in detecting terms
of disorders. With a machine learning based ap-
proach, improved results were obtained: 80%
precision, 82% recall (Skeppstedt et al., 2014).
Skeppstedt et al. (2012) have also demonstrated
the negative influence of abbreviations and mul-
tiword expressions in their findings.
3.4 Spelling correction
A system for general spelling correction of
Swedish is described by Kann et al. (1998), but
we are not aware of any previous work related to
spelling correction of Swedish clinical text. An
example of spelling correction of clinical text for
other languages is Tolentino et al. (2007), who use
several algorithms for word similarity detection,
including phonological homonym lookup and n-
grams for contextual disambiguation. They report
a precision of 64% on English medical texts. An-
other example is Patrick et al. (2010) and Patrick
and Nguyen (2011), who combine a mixture of
generation of spelling candidates based on ortho-
graphic and phonological edit distance, and a 2-
word window of contextual information for rank-
ing the spelling candidates resulting in an accuracy
of 84% on English patient records. Sikl?oski et al.
(2013) use a statistical machine translation model
76
Figure 1: Distribution of 100 PR dataset sentences by length (number of sentences on the y-axis and
number of tokens on the x-axis).
(with 3-grams) for spelling correction, achieving
88% accuracy on Hungarian medical texts.
4 Experimental data
This study uses clinical notes
5
from the Stockholm
Electronic Patient Record corpus containing more
than 600,000 patients of all ages from more than
500 health units during 2006?2013 (Dalianis et al.,
2012).
A randomly selected subset of 100 daily notes
from different EHRs written by physicians be-
tween 2009?2010 was used as a gold standard
dataset for evaluating abbreviation detection, com-
pound splitting and spelling corrections. This 100
daily notes dataset contains 433 sentences and
3,888 tokens, as determined by Stagger (
?
Ostling,
2013), a Swedish tokenizer and POS tagger. The
majority of sentences contain between 4?11 to-
kens (see Figure 1.)
The text snippet in Figure 2 provides an illus-
trative example of the characteristics of a health
record. What is immediately striking is the num-
ber of misspellings, abbreviations, compounds and
words of foreign origin. But also the syntax is
peculiar, alternating between telegraphic clauses
with implicit arguments, and long sentences with
complex embeddings.
5
Approved by the Regional Ethical Review Board in
Stockholm (Etikpr?ovningsn?amnden i Stockholm), permis-
sion number 2012/2028-31/5
5 Lexical normalization of EHRs
Normalization of lexis in clinical text relies heav-
ily on the lookup in available lexicons, corpora and
domain terminologies. Although these resources
usually cover the majority of words (i.e. tokens)
in texts, however due to the ever evolving lan-
guage and knowledge inside the domain, medi-
cal texts, when analysed with the NLP tools, also
contain unknown
6
words. These remaining words
that are not covered by any lexicon, or corpora re-
source, can be misspellings, abbreviations, com-
pounds (new word formations), words in foreign
languages (Latin, Greek, English), or new terms.
Our approach to dealing with unknown words
combines a rule-based abbreviation detection and
Swedish statistical language model-based com-
pound analysis and misspelling resolution.
The following sections describe three methods
that are applied in a pipeline manner. That is, first,
all known abbreviations are detected and marked;
second the unknown words are checked whether
they are compounds; finally, for the remaining un-
known words, context dependent word corrections
are made.
5.1 Detecting abbreviations
This section describes the heuristics and lexi-
con lookup-based abbreviation detection method.
The Swedish Clinical Abbreviation and Medi-
cal Terminology Matcher (SCATM) is based on
6
By unknown words we mean words that cannot be
looked up in available lexical resources or linguistically ana-
lyzed by POS tokenizer.
77
Figure 2: Characteristics of a health record: misspellings (underline), abbreviations (bold), compounds
(italic) and words of foreign origin (red).
SCAN (Isenius et al., 2012). The SCATM method
uses domain-adapted Stagger (
?
Ostling, 2013)
for the tokenization and POS-tagging of text.
The adapted version of Stagger handles clinical-
specific
7
abbreviations from three domains, i.e. ra-
diology, emergency, and dietology. SCATM also
uses several lexicons to determine whether a word
is a common word (in total 122,847 in the lexi-
con), an abbreviation (in total 7,455 in the lexi-
con), a medical term (in total 17,380 in the lexi-
con), or a name (both first and last names, in total
404,899 in the lexicon). All words that are at most
6 characters long, or contains the characters ?-?
and/or ?.? are checked against these lexicons in a
specific order in order to determine whether it is
an abbreviation or not.
The SCATM method uses various lexicons
8
of
Swedish medical terms, Swedish abbreviations,
7
Abbreviations that do not follow conventional orthogra-
phy styles, e.g. a typical abbreviation p.g.a. (en. due to) can
have the following variants p g a, pga, p. G. A., p. gr. a.
8
the sources of lexicons are: anatomin.se,
neuro.ki.se smittskyddsinstitutet.se,
medicinskordbok.se, runeberg.org, g3.
spraakdata.gu.se/saob, sv.wikipedia.org/
wiki/Lista_ver_frkortningar, karolinska.
se/Karolinska-Universitetslaboratoriet/
Sidor-om-PTA/Analysindex-alla-enheter/
Forkortningar/ and the list of Swedish names (Carlsson
and Dalianis, 2010).
Swedish words and Swedish names (first and last).
5.2 Compound splitting
For compound splitting, we use a collection of lex-
ical resources, the core of which is a full-form
dictionary produced by Nordisk spr?akteknologi
holding AS (NST), comprising 927,000 en-
tries
9
. In addition, various resources from the
medical domain have been mined for vocab-
ulary: Swedish SNOMED
10
terminology, the
L?akartidningen medical journal
11
corpus, and
Swedish Web health-care guides/manuals
12
.
A refinement of the basic lexicon-driven tech-
nique described in the related research section is
that our compound splitting makes use of contex-
tual disambiguation. As the example of hj?arteko
illustrates, this compound can be hypothetically
split into
13
:
hj?art+eko (en. cardiac+echo)
9
Available at: www.nb.no/Tilbud/Forske/
Spraakbanken/Tilgjengelege-ressursar/
Leksikalske-ressursar
10
www.socialstyrelsen.se/
nationellehalsa/nationelltfacksprak/
11
http://spraakbanken.gu.se/eng/
research/infrastructure/korp
12
www.1177.se and www.vardguiden.se
13
Korp (http://spraakbanken.gu.se/korp) is a collection of
Swedish corpora, comprising 1,784,019,272 tokens, as of
January 2014.
78
KORP freq.: 642 + 5,669
hj?arte+ko (en. beloved+cow)
KORP freq.: 8 + 8,597
For choosing the most likely composition in the
given context, we use the Stockholm Language
Model with Entropy (SLME) (
?
Ostling, 2012)
which is a simple n-gram language model.
The max probability defines the correct word
formation constituents:
hj?art+eko 2.3e-04
hj?arte+ko 5.1e-07
The SMLE is described in the following section.
5.3 Misspelling detection
The unknown words that are not abbreviations or
compounds can very likely be misspellings. Mis-
spellings can be a result of typing errors or the lack
of knowledge of the correct spelling.
Our approach to clinical Swedish misspellings
is based on the best practices of spell checkers
for Indo-European languages, namely the phonetic
similarity key method combined with a method
to measure proximity between the strings. In
our spelling correction method, the Edit distance
(Levenshtein, 1966) algorithm is used to measure
the proximity of orthographically possible can-
didates. The Soundex algorithm (Knuth, 1973)
shortlists the spelling candidates which are phono-
logically closest to the misspelled word. Further,
the spelling correction candidates are analyzed in
a context by using the SLME n-gram model.
The SLME employs the Google Web 1T 5-
gram, 10 European Languages, Version 1, dataset
for Swedish, which is the largest publically avail-
able Swedish data resource. The SLME is a sim-
ple n-gram language model, based on the Stupid
Backoff Model (Brants et al., 2007). The n-gram
language model calculates the probability of a
word in a given context:
P (w
L
1
) =
L
?
i=1
P (w
i
|w
i?1
1
) ?
L
?
i=1
?
P (w
i
|w
i?1
i?n+1
)
(1)
The maximum-likelihood probability estimates
for the n-grams are calculated by their relative fre-
quencies:
r(w
i
|w
i?1
i?n+1
) =
f(w
i
i?n+1
)
f(w
i?1
i?n+1
)
(2)
The smoothing is used when the complete n-
gram is not found. If r(w
i?1
i?n+1
) is not found,
then the model looks for r(w
i?1
i?n+2
) , r(w
i?1
i?n+3
),
and so on. The Stupid backoff (Brants et al.,
2007) smoothing method uses relative frequencies
instead of normalized probabilities and context-
dependent discounting. Equation (3) shows how
score S is calculated:
S(w
i
|w
i?1
i?k+1
) =
=
?
?
?
?
?
f(w
i
i?k+1
)
f(w
i?1
i?k+1
)
iff(w
i
i?k+1
)) > 0
?S(w
i
|w
i?1
i?k+2
) otherwise
(3)
The backoff parameter ? is set to 0.4, which was
heuristically determined by (Brants et al., 2007).
The recursion stops when the score for the last
context word is calculated. N is the size of the
corpus.
S(w
i
) =
f(w
i
)
N
(4)
The SLME n-gram model calculates the
probability of a word in a given context:
p(word|context). The following example
14
shows the case of spelling correction:
Original:
Vpl p?a onsdag. UK tortdag.
(en. Vpl on wednesday. UK thsday.)
torgdag (en. marketday): 4.2e-10
torsdag (en. Thursday): 1.1e-06
Corrected:
Vpl p?a onsdag. UK torsdag.
6 Experiments and results
Our approach to lexical normalization was
tested against a gold standard, namely, the 100
EHR daily notes dataset. The dataset was anno-
tated for abbreviations, compounds including ab-
breviations and misspellings by a physician.
We carried out the following experiments (see
Table 2):
1. SCATM to mark abbreviations and terms;
14
Vpl stands for V?ardplanering (en. planning for care), UK
stands for utskrivningsklar (en. ready for discharge).
79
Method Lexical normalization task Gold-
standard,
occurences
Precision, % Recall, %
SCATM 1 Abbreviation detection 550 91.1 81.0
SCATM 1a Abbreviations included in
compounds only
78 89.74 46.15
NoCM 1 Out-of-dictionary compound
splitting
97 83.5 -
NoCM 1a Out-of-dictionary com-
pounds which include
abbreviations
44 59.1 -
NoCM 2 Spelling correction 41 54.8 63.12
SCATM+NoCM Spelling correction 41 83.87 76.2
Table 2: Results of lexical normalization.
2. NoCM (lexical normalization of compounds
and misspellings as described in sections
5.2 and 5.3) to resolve compounds and mis-
spellings;
3. The combined experiment SCATM+NoCM
to resolve misspellings.
The last experimental setting was designed as a
solution to deal with compounds that include ab-
breviations. Marking abbreviations prior to the
spelling correction can help to reduce the number
of false positives.
The 433 sentences contained a total of 550 ab-
breviations (78 of these were constituents of com-
pound words), and 41 misspellings of which 13
were misspelled words containing abbreviations.
Due to the tokenization errors, a few sentence
boundaries were detected incorrectly, e.g. inter-
rupted dates and abbreviations. Because of this
some abbreviations were separated into different
sentences and thus added to false negatives and
false positives.
The first experiment (SCATM 1 and 1a) of de-
tecting abbreviations achieved both high precision
and recall. As a special case of demonstrating the
source of errors (see SCATM 1a) is the evaluation
of detecting abbreviations which are part of com-
pounds only. The low recall is due to the design of
the SCATM which does not handle words longer
than 6 characters, thus resulting in compounded
abbreviations like k?arlkir or ?overvak to go unde-
tected.
The evaluation of the second experiment
(NoCM 1, 1a and 2) showed that the majority
of out-of-dictionary compounds was resolved cor-
rectly (NoCM 1) and reached 83.5% precision.
Errors mainly occurred due to spelling candi-
date ranking, e.g. even+tull instead of eventuell
and compounds containing abbreviations and mis-
spelled words. As a special case of demonstrating
the source of errors of the latter (see NoCM 1a) is
the evaluation of those compounds
15
only which
contain abbreviations. The task of spelling correc-
tion (NoCM 2) performed poorly, reaching only
54.8% precision. This can be explained by failing
to resolve misspellings in compounds where ab-
breviations are compounded together with a mis-
spelled words, e.g. aciklocvirkonc (aciklovir kon-
centrate).
The third experiment (SCATM+NoCM) com-
bined abbreviation detection followed by the out-
of-dictionary word normalization (spelling cor-
rection and compound splitting). This setting
helped to resolve the earlier source of errors, i.e.
words that contain both misspelling(s) and abbre-
viation(s). The overall precision of spelling cor-
rection is 83.87%.
7 Conclusions
Our attempt to address the problem of lexical sim-
plification, and, in the long run, improve readabil-
ity of Swedish EHRs, by automatically detecting
and resolving out of dictionary words, achieves
91.1% (abbreviations), 83.5% (compound split-
ting) and 83.87% (spelling correction) precision,
respectively. These results are comparable to those
15
This number of compounds is derived from the number
of abbreviations included in compounds (from SCATM 1a)
by selecting only those out-of -dictionary words which do not
contain punctuation.
80
reported in similar studies on English and Hungar-
ian patient records (Patrick et al., 2010; Sikl?osi et
al., 2013).
Furthermore, the analysis of the gold standard
data revealed that around 14% of all words in
Swedish EHRs are abbreviations. More specifi-
cally, 2% of all the words are compounds includ-
ing abbreviations. In contrast, and somewhat un-
expectedly, only 1% are misspellings. This dis-
tribution result is an important finding for future
studies in lexical simplification and readability
studies of EHRs, as it might be useful for inform-
ing automatic processing approaches.
We draw two conclusions from this study. First,
to advance research into the field of readability
of EHRs, and thus to develop suitable readability
measures it is necessary to begin by taking these
findings into account and by relating abbrevia-
tions, spelling variation, misspellings, compounds
and terminology to reading comprehension.
Second, as a future guideline for the overall
pipeline for detecting and resolving unknown, out-
of-dictionary words, we suggest handling abbrevi-
ations in a first step, and then taking care of mis-
spellings and potential compounds. The most ur-
gent area for future improvement of the method is
to handle compound words containing both abbre-
viations and misspellings.
Acknowledgements
The authors wish to thank the anonymous review-
ers for valuable feedback. Maria Kvist and Sum-
ithra Velupillai were in part funded by the V?ardal
Foundation, Sumithra also by the Swedish Re-
search Council and the Swedish Fulbright com-
mission. We thank Robert
?
Ostling who pro-
vided the POS tagger and the Stockholm Lan-
guage Model with Entropy.
References
M. Adnan, J. Warren, and M. Orr. 2010. Assess-
ing text characteristics of electronic discharge sum-
maries and their implications for patient readabil-
ity. In Proceedings of the Fourth Australasian Work-
shop on Health Informatics and Knowledge Man-
agement - Volume 108, HIKM ?10, pages 77?84,
Darlinghurst, Australia, Australia. Australian Com-
puter Society, Inc.
H. Allvin, E. Carlsson, H. Dalianis, R. Danielsson-
Ojala, V. Daudaravicius, M. Hassel, D. Kokki-
nakis, H. Lundgren-Laine, G.H. Nilsson, ?. Nytr?,
S. Salanter?a, M. Skeppstedt, H. Suominen, and
S. Velupillai. 2011. Characteristics of Finnish and
Swedish intensive care nursing narratives: a com-
parative analysis to support the development of clin-
ical language technologies. Journal of Biomedical
Semantics, 2(Suppl 3):S1, doi:10.1186/2041-1480-
2-S3-S1, July.
L. Borin and M. Forsberg. 2009. All in the family: A
comparison of SALDO and WordNet. In Proceed-
ings of the Nodalida 2009 Workshop on WordNets
and other Lexical Semantic Resources, pages 7?12.
NEALT.
L. Borin, N. Grabar, M. Gronostaj, C. Hallett, D. Hard-
castle, D. Kokkinakis, S. Williams, and A. Willis.
2009. Semantic Mining Deliverable D27.2: Em-
powering the patient with language technology.
Technical report, Semantic Mining (NOE 507505).
T. Brants, A. C. Popat, P. Xu, F. J. Och, and J. Dean.
2007. Large language models in machine transla-
tion. In In Proceedings of the 2007 Joint Conference
EMNLP-CoNLL, pages 858?867.
C. Bretschneider, S. Zillner, and M. Hammon. 2013.
Identifying pathological findings in German radiol-
ogy reports using a syntacto-semantic parsing ap-
proach. In Proceedings of the 2013 Workshop on
Biomedical Natural Language Processing (BioNLP
2013). ACL.
E. Carlsson and H. Dalianis. 2010. Influence of Mod-
ule Order on Rule-Based De-identification of Per-
sonal Names in Electronic Patient Records Writ-
ten in Swedish. In Proceedings of the Seventh In-
ternational Conference on Language Resources and
Evaluation, LREC 2010, pages 3071?3075, Valletta,
Malta, May 19?21.
H. Dalianis, M. Hassel, A. Henriksson, and M. Skepp-
stedt. 2012. Stockholm EPR Corpus: A Clinical
Database Used to Improve Health Care. In Pierre
Nugues, editor, Proc. 4th SLTC, 2012, pages 17?18,
Lund, October 25-26.
D. Dann?ells. 2006. Automatic acronym recognition.
In Proceedings of the 11th conference on European
chapter of the Association for Computational Lin-
guistics (EACL).
C. Friedman, P. Kra, and A. Rzhetsky. 2002. Two
biomedical sublanguages: a description based on the
theories of Zellig Harris. Journal of Biomedical In-
formatics, 35(4):222?235.
C. Hag`ege, P. Marchal, Q. Gicquel, S. Darmoni,
S. Pereira, and M. Metzger. 2011. Linguistic
and temporal processing for discovering hospital ac-
quired infection from patient records. In Proceed-
ings of the ECAI 2010 Conference on Knowledge
Representation for Health-care, KR4HC?10, pages
70?84, Berlin, Heidelberg. Springer-Verlag.
N. Isenius, S. Velupillai, and M. Kvist. 2012. Initial
results in the development of scan: a swedish clini-
cal abbreviation normalizer. In Proceedings of the
81
CLEF 2012 Workshop on Cross-Language Evalu-
ation of Methods, Applications, and Resources for
eHealth Document Analysis - CLEFeHealth2012,
Rome, Italy, September. CLEF.
G. Josefsson. 1999. F?a feber eller tempa? N?agra
tankar om agentivitet i medicinskt fackspr?ak.
S. Kandula, D. Curtis, and Q. Zeng-Treitler. 2010. A
Semantic and Syntactic Text Simplification Tool for
Health Content. In Proc AMIA 2010, pages 366?
370.
V. Kann, R. Domeij, J. Hollman, and M. Tillenius.
1998. Implementation Aspects and Applications of
a Spelling Correction Algorithm. . Technical Report
TRITA-NA-9813, NADA, KTH.
A. Keselman, L. Slaughter, CA. Smith, H. Kim, G. Di-
vita, A. Browne, and et al. 2007. Towards
consumer-friendly PHRs: patients experience with
reviewing their health records. In AMIA Annu Symp
Proc 2007, pages 399?403.
D. E. Knuth, 1973. The Art of Computer Program-
ming: Volume 3, Sorting and Searching, pages 391?
392. Addison-Wesley.
D. Kokkinakis and A. Thurin. 2007. Identifica-
tion of Entity References in Hospital Discharge Let-
ters. In Proceedings of the 16th Nordic Conference
of Computational Linguistics (NODALIDA) 2007,
pages 329?332, Tartu, Estonia.
M. Kvist and S. Velupillai. 2013. Professional
Language in Swedish Radiology Reports ? Char-
acterization for Patient-Adapted Text Simplifica-
tion. In Proceedings of the Scandinavian Con-
ference on Health Informatics 2013, Copenhagen,
Denmark, August. Link?oping University Electronic
Press, Link?opings universitet.
M. Kvist, M. Skeppstedt, S. Velupillai, and H. Dalianis.
2011. Modeling human comprehension of swedish
medical records for intelligent access and summa-
rization systems, a physician?s perspective. In Proc.
9th Scandinavian Conference on Health Informat-
ics, SHI, Oslo, August.
V. Laippala, F. Ginter, S. Pyysalo, and T. Salakoski.
2009. Towards automated processing of clinical
Finnish: Sublanguage analysis and a rule-based
parser. Int journal of medical informatics, 78:e7?
e12.
VI Levenshtein. 1966. Binary Codes Capable of Cor-
recting Deletions, Insertions and Reversals. Soviet
Physics Doklady, 10:707?710.
H. Liu, Y. A. Lussier, and C. Friedman. 2001. Disam-
biguating Ambiguous Biomedical Terms in Biomed-
ical Narrative Text: An Unsupervised Method.
Journal of Biomedical Informatics, 34:249?261.
S. M. Meystre, G. K. Savova, K. C. Kipper-Schuler,
and John E. Hurdle. 2008. Extracting informa-
tion from textual documents in the electronic health
record: a review of recent research. IMIA Yearbook
of Medical Informatics 2008. 47 Suppl 1:138-154.
R.
?
Ostling and M. Wir?en, 2013. Compounding in
a Swedish Blog Corpus, pages 45?63. Stockholm
Studies in Modern Philology. New series 16. Stock-
holm university.
R.
?
Ostling. 2012.
http://www.ling.su.se/english/nlp/tools/slme/stockholm-
language-model-with-entropy-slme-1.101098 .
R.
?
Ostling. 2013. Stagger: an Open-Source Part of
Speech Tagger for Swedish. Northern European
Journal of Language Technology, 3:1?18.
S. Pakhomov, T. Pedersen, and C. G. Chute. 2005. Ab-
breviation and Acronym Disambiguation in Clinical
Discourse. In Proc AMIA 2005, pages 589?593.
J. Patrick and D. Nguyen. 2011. Automated Proof
Reading of Clinical Notes. In Helena Hong Gao
and Minghui Dong, editors, PACLIC, pages 303?
312. Digital Enhancement of Cognitive Develop-
ment, Waseda University.
J. Patrick, M. Sabbagh, S. Jain, and H. Zheng. 2010.
Spelling correction in Clinical Notes with Emphasis
on First Suggestion Accuracy. In 2nd Workshop on
Building and Evaluating Resources for Biomedical
Text Mining, pages 2?8.
C. Pyper, J. Amery, M. Watson, and C. Crook. 2004.
Patients experiences when accessing their on-line
electronic patient records in primary care. The
British Journal of General Practice, 54:38?43.
P. Ruch, R. Baud, and A. Geissb?uhler. 2003. Using
lexical disambiguation and named-entity recogni-
tion to improve spelling correction in the electronic
patient record. Artificial Intelligence in Medicine,
29(1-2):169?184.
B. Sikl?osi, A. Nov?ak, and G. Pr?osz?eky, 2013. Context-
Aware Correction of Spelling Errors in Hungar-
ian Medical Documents, pages 248?259. Number
Lecture Notes in Computer Science 7978. Springer
Berlin Heidelberg.
J. Sj?obergh and V. Kann. 2006. Vad kan statistik
avsl?oja om svenska sammans?attningar? Spr?ak och
stil, 1:199?214.
M. Skeppstedt, M. Kvist, and H Dalianis. 2012.
Rule-based Entity Recognition and Coverage of
SNOMED CT in Swedish Clinical Text. In Pro-
ceedings of the Eighth International Conference on
Language Resources and Evaluation, LREC 2012,
pages 1250?1257, Istanbul, Turkey, May 23?25.
M. Skeppstedt, M. Kvist, G. H. Nilsson, and H. Dalia-
nis. 2014. Automatic recognition of disorders,
findings, pharmaceuticals and body structures from
82
clinical text: An annotation and machine learn-
ing study. Journal of Biomedical Informatics,
http://dx.doi.org/10.1016/j.jbi.2014.01.012.
B. Smedby. 1991. Medicinens Spr?ak: spr?aket
i sjukdomsklassifikationen ? mer konsekvent
f?orsvenskning efterstr?avas [Language of Medicine:
the language of diagnose classification - more
consequent Swedification sought]. L?akartidningen,
pages 1519?1520.
G. Surj?an and G. H?eja. 2003. About the language of
Hungarian discharge reports. Stud Health Technol
Inform, 95:869?873.
H. D. Tolentino, M. D. Matters, W. Walop, B. Law,
W. Tong, F. Liu, P. A. Fontelo, K. Kohl, and D. C.
Payne. 2007. A UMLS-based spell checker for nat-
ural language processing in vaccine safety. BMC
Med. Inf. & Decision Making, 7.
Y. Wang and J. Patrick. 2009. Cascading classifiers for
named entity recognition in clinical notes. In Pro-
ceedings of the Workshop on Biomedical Informa-
tion Extraction, WBIE ?09, pages 42?49, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
D. T. Y. Wu, D. A. Hanauer, Q. Mei, P. M. Clark,
L. C. An, J. Lei, J. Proulx, Q. Zeng-Treitler, and
K. Zheng. 2013. Applying Multiple Methods to As-
sess the Readability of a Large Corpus of Medical
Documents. Stud Health Technol Inform, 192:647?
651.
H. Xu, P. D. Stetson, and C. Friedman. 2007. A Study
of Abbreviations in Clinical Notes. In Proc AMIA
2007, pages 821?825.
83
Proceedings of the 2014 Workshop on Biomedical Natural Language Processing (BioNLP 2014), pages 54?58,
Baltimore, Maryland USA, June 26-27 2014.
c?2014 Association for Computational Linguistics
Generating Patient Problem Lists from the ShARe Corpus using
SNOMED CT/SNOMED CT CORE Problem List
Danielle Mowery
Janyce Wiebe
University of Pittsburgh
Pittsburgh, PA
dlm31@pitt.edu
wiebe@cs.pitt.edu
Mindy Ross
University of California
San Diego
La Jolla, CA
mkross@ucsd.edu
Sumithra Velupillai
Stockholm University
Stockholm, SE
sumithra@dsv.su.se
Stephane Meystre
Wendy W Chapman
University of Utah
Salt Lake City, UT
stephane.meystre,
wendy.chapman@utah.edu
Abstract
An up-to-date problem list is useful for
assessing a patient?s current clinical sta-
tus. Natural language processing can help
maintain an accurate problem list. For in-
stance, a patient problem list from a clin-
ical document can be derived from indi-
vidual problem mentions within the clin-
ical document once these mentions are
mapped to a standard vocabulary. In
order to develop and evaluate accurate
document-level inference engines for this
task, a patient problem list could be gen-
erated using a standard vocabulary. Ad-
equate coverage by standard vocabularies
is important for supporting a clear rep-
resentation of the patient problem con-
cepts described in the texts and for interop-
erability between clinical systems within
and outside the care facilities. In this
pilot study, we report the reliability of
domain expert generation of a patient
problem list from a variety of clinical
texts and evaluate the coverage of anno-
tated patient problems against SNOMED
CT and SNOMED Clinical Observation
Recording and Encoding (CORE) Prob-
lem List. Across report types, we learned
that patient problems can be annotated
with agreement ranging from 77.1% to
89.6% F1-score and mapped to the CORE
with moderate coverage ranging from
45%-67% of patient problems.
1 Introduction
In the late 1960?s, Lawrence Weed published
about the importance of problem-oriented medi-
cal records and the utilization of a problem list
to facilitate care provider?s clinical reasoning by
reducing the cognitive burden of tracking cur-
rent, active problems from past, inactive problems
from the patient health record (Weed, 1970). Al-
though electronic health records (EHR) can help
achieve better documentation of problem-specific
information, in most cases, the problem list is
manually created and updated by care providers.
Thus, the problem list can be out-of-date con-
taining resolved problems or missing new prob-
lems. Providing care providers with problem list
update suggestions generated from clinical docu-
ments can improve the completeness and timeli-
ness of the problem list (Meystre and Haug, 2008).
In recent years, national incentive and standard
programs have endorsed the use of problem lists
in the EHR for tracking patient diagnoses over
time. For example, as part of the Electronic Health
Record Incentive Program, the Center for Medi-
care and Medicaid Services defined demonstra-
tion of Meaningful Use of adopted health infor-
mation technology in the Core Measure 3 objec-
tive as ?maintaining an up-to-date problem list of
current and active diagnoses in addition to histor-
ical diagnoses relevant to the patients care? (Cen-
ter for Medicare and Medicaid Services, 2013).
More recently, the Systematized Nomenclature of
Medicine Clinical Terms (SNOMED CT) has be-
come the standard vocabulary for representing and
documenting patient problems within the clinical
record. Since 2008, this list is iteratively refined
four times each year to produce a subset of gen-
eralizable clinical problems called the SNOMED
CT CORE Problem List. This CORE list repre-
sents the most frequent problem terms and con-
cepts across eight major healthcare institutions in
the United States and is designed to support in-
teroperability between regional healthcare institu-
tions (National Library of Medicine, 2009).
In practice, there are several methodologies ap-
plied to generate a patient problem list from clin-
ical text. Problem lists can be generated from
coded diagnoses such as the International Statis-
tical Classification of Disease (ICD-9 codes) or
54
concept labels such as Unified Medical Language
System concept unique identifiers (UMLS CUIs).
For example, Meystre and Haug (2005) defined 80
of the most frequent problem concepts from coded
diagnoses for cardiac patients. This list was gen-
erated by a physician and later validated by two
physicians independently. Coverage of coded pa-
tient problems were evaluated against the ICD-9-
CM vocabulary. Solti et al. (2008) extended the
work of Meystre and Haug (2005) by not limit-
ing the types of patient problems from any list
or vocabulary to generate the patient problem list.
They observed 154 unique problem concepts in
their reference standard. Although both studies
demonstrate valid methods for developing a pa-
tient problem list reference standard, neither study
leverages a standard vocabulary designed specifi-
cally for generating problem lists.
The goals of this study are 1) determine how
reliably two domain experts can generate a pa-
tient problem list leveraging SNOMED CT from
a variety of clinical texts and 2) assess the cover-
age of annotated patient problems from this corpus
against the CORE Problem List.
2 Methods
In this IRB-approved study, we obtained the
Shared Annotated Resource (ShARe) corpus
originally generated from the Beth Israel Dea-
coness Medical Center (Elhadad et al., un-
der review) and stored in the Multiparameter
Intelligent Monitoring in Intensive Care, ver-
sion 2.5 (MIMIC II) database (Saeed et al.,
2002). This corpus consists of discharge sum-
maries (DS), radiology (RAD), electrocardiogram
(ECG), and echocardiogram (ECHO) reports from
the Intensive Care Unit (ICU). The ShARe cor-
pus was selected because it 1) contains a variety of
clinical text sources, 2) links to additional patient
structured data that can be leveraged for further
system development and evaluation, and 3) has en-
coded individual problem mentions with semantic
annotations within each clinical document that can
be leveraged to develop and test document-level
inference engines. We elected to study ICU pa-
tients because they represent a sensitive cohort that
requires up-to-date summaries of their clinical sta-
tus for providing timely and effective care.
2.1 Annotation Study
For this annotation study, two annotators - a physi-
cian and nurse - were provided independent train-
ing to annotate clinically relevant problems e.g.,
signs, symptoms, diseases, and disorders, at the
document-level for 20 reports. The annotators
were given feedback based on errors over two it-
erations. For each patient problem in the remain-
ing set, the physician was instructed to review the
full text, span the a problem mention, and map the
problem to a CUI from SNOMED-CT using the
extensible Human Oracle Suite of Tools (eHOST)
annotation tool (South et al., 2012). If a CUI did
not exist in the vocabulary for the problem, the
physician was instructed to assign a ?CUI-less? la-
bel. Finally, the physician then assigned one of
five possible status labels - Active, Inactive, Re-
solved, Proposed, and Other - based on our pre-
vious study (Mowery et al., 2013) to the men-
tion representing its last status change at the con-
clusion of the care encounter. Patient problems
were not annotated as Negated since patient prob-
lem concepts are assumed absent at a document-
level (Meystre and Haug, 2005). If the patient
was healthy, the physician assigned ?Healthy - no
problems? to the text. To reduce the cognitive bur-
den of annotation and create a more robust refer-
ence standard, these annotations were then pro-
vided to a nurse for review. The nurse was in-
structed to add missing, modify existing, or delete
spurious patient problems based on the guidelines.
We assessed how reliably annotators agreed
with each other?s patient problem lists using inter-
annotator agreement (IAA) at the document-level.
We evaluated IAA in two ways: 1) by problem
CUI and 2) by problem CUI and status. Since
the number of problems not annotated (i.e., true
negatives (TN)) are very large, we calculated F1-
score as a surrogate for kappa (Hripcsak and Roth-
schild, 2005). F1-score is the harmonic mean of
recall and precision, calculated from true posi-
tive, false positive, and false negative annotations,
which were defined as follows:
true positive (TP) = the physician and nurse prob-
lem annotation was assigned the same CUI
(and status)
false positive (FP) = the physician problem anno-
tation (and status) did not exist among the
nurse problem annotations
55
false negative (FN) = the nurse problem anno-
tation (and status) did not exist among the
physician problem annotations
Recall =
TP
(TP + FN)
(1)
Precision =
TP
(TP + FP )
(2)
F1-score =
2
(Recall ? Precision)
(Recall + Precision)
(3)
We sampled 50% of the corpus and determined
the most common errors. These errors with
examples were programmatically adjudicated
with the following solutions:
Spurious problems: procedures
solution: exclude non-problems via guidelines
Problem specificity: CUI specificity differences
solution: select most general CUIs
Conflicting status: negated vs. resolved
solution: select second reviewer?s status
CUI/CUI-less: C0031039 vs. CUI-less
solution: select CUI since clinically useful
We split the dataset into about two-thirds train-
ing and one-third test for each report type. The re-
maining data analysis was performed on the train-
ing set.
2.2 Coverage Study
We characterized the composition of the reference
standard patient problem lists against two stan-
dard vocabularies SNOMED-CT and SNOMED-
CT CORE Problem List. We evaluated the cover-
age of patient problems against the SNOMED CT
CORE Problem List since the list was developed
to support encoding clinical observations such as
findings, diseases, and disorders for generating pa-
tient summaries like problem lists. We evaluated
the coverage of patient problems from the corpus
against the SNOMED-CT January 2012 Release
which leverages the UMLS version 2011AB. We
assessed recall (Eq 1), defining a TP as a patient
problem CUI occurring in the vocabulary and a
FN as a patient problem CUI not occurring in the
vocabulary.
3 Results
We report the results of our annotation study on
the full set and vocabulary coverage study on the
training set.
3.1 Annotation Study
The full dataset is comprised of 298 clinical doc-
uments - 136 (45.6%) DS, 54 (18.1%) ECHO,
54 (18.1%) RAD, and 54 (18.1%) ECG. Seventy-
four percent (221) of the corpus was annotated by
both annotators. Table 1 shows agreement overall
and by report, matching problem CUI and prob-
lem CUI with status. Inter-annotator agreement
for problem with status was slightly lower for all
report types with the largest agreement drop for
DS at 15% (11.6 points).
Report Type CUI CUI + Status
DS 77.1 65.5
ECHO 83.9 82.8
RAD 84.7 82.8
ECG 89.6 84.8
Table 1: Document-level IAA by report type for problem
(CUI) and problem with status (CUI + status)
We report the most common errors by frequency
in Table 2. By report type, the most common er-
rors for ECHO, RAD, and ECG were CUI/CUI-
less, and DS was Spurious Concepts.
Errors DS ECHO RAD ECG
SP 423 (42%) 26 (23%) 30 (35%) 8 (18%)
PS 139 (14%) 31 (27%) 8 (9%) 0 (0%)
CS 318 (32%) 9 (8%) 8 (9%) 14 (32%)
CC 110 (11%) 34 (30%) 37 (44%) 22 (50%)
Other 6 (>1%) 14 (13%) 2 (2%) 0 (0%)
Table 2: Error types by frequency - Spurious Problems (SP),
Problem Specificity (PS), Conflicting status (CS), CUI/CUI-
less (CC)
3.2 Coverage Study
In the training set, there were 203 clinical docu-
ments - 93 DS, 37 ECHO, 38 RAD, and 35 ECG.
The average number of problems were 22?10 DS,
10?4 ECHO, 6?2 RAD, and 4?1 ECG. There
are 5843 total current problems in SNOMED-CT
CORE Problem List. We observed a range of
unique SNOMED-CT problem concept frequen-
cies: 776 DS, 63 ECHO, 113 RAD, and 36 ECG
56
by report type. The prevalence of covered prob-
lem concepts by CORE is 461 (59%) DS, 36
(57%) ECHO, 71 (63%) RAD, and 16 (44%)
ECG. In Table 3, we report coverage of patient
problems for each vocabulary. No reports were
annotated as ?Healthy - no problems?. All reports
have SNOMED CT coverage of problem mentions
above 80%. After mapping problem mentions to
CORE, we observed coverage drops for all report
types, 24 to 36 points.
Report Patient Annotated with Mapped to
Type Problems SNOMED CT CORE
DS 2000 1813 (91%) 1335 (67%)
ECHO 349 300 (86%) 173 (50%)
RAD 190 156 (82%) 110 (58%)
ECG 95 77(81%) 43 (45%)
Table 3: Patient problem coverage by SNOMED-CT and
SNOMED-CT CORE
4 Discussion
In this feasibility study, we evaluated how reliably
two domain experts can generate a patient problem
list and assessed the coverage of annotated patient
problems against two standard clinical vocabular-
ies.
4.1 Annotation Study
Overall, we demonstrated that problems can be re-
liably annotated with moderate to high agreement
between domain experts (Table 1). For DS, agree-
ment scores were lowest and dropped most when
considering the problem status in the match crite-
ria. The most prevalent disagreement for DS was
Spurious problems (Table 2). Spurious problems
included additional events (e.g., C2939181: Mo-
tor vehicle accident), procedures (e.g., C0199470:
Mechanical ventilation), and modes of administra-
tion (e.g., C0041281: Tube feeding of patient) that
were outside our patient problem list inclusion cri-
teria. Some pertinent findings were also missed.
These findings are not surprising given on average
more problems occur in DS and the length of DS
documents are much longer than other document
types. Indeed, annotators are more likely to miss
a problem as the number of patient problems in-
crease.
Also, status differences can be attributed to mul-
tiple status change descriptions using expressions
of time e.g., ?cough improved then? and modal-
ity ?rule out pneumonia?, which are harder to
track and interpret over a longer document. The
most prevalent disagreements for all other doc-
ument types were CUI/CUI-less in which iden-
tifying a CUI representative of a clinical obser-
vation proved more difficult. An example of
Other disagreement was a sidedness mismatch
or redundant patient problem annotation. For
example, C0344911: Left ventricular dilatation
vs. C0344893: Right ventricular dilatation or
C0032285: Pneumonia was recorded twice.
4.2 Coverage Study
We observed that DS and RAD reports have higher
counts and coverage of unique patient problem
concepts. We suspect this might be because other
document types like ECG reports are more likely
to have laboratory observations, which may be
less prevalent findings in CORE. Across document
types, coverage of patient problems in the corpus
by SNOMED CT were high ranging from 81%
to 91% (Table 3). However, coverage of patient
problems by CORE dropped to moderate cover-
ages ranging from 45% to 67%. This suggests that
the CORE Problem List is more restrictive and
may not be as useful for capturing patient prob-
lems from these document types. A similar report
of moderate problem coverage with a more restric-
tive concept list was also reported by Meystre and
Haug (2005).
5 Limitations
Our study has limitations. We did not apply a tra-
ditional adjudication review between domain ex-
perts. In addition, we selected the ShARe corpus
from an ICU database in which vocabulary cover-
age of patient problems could be very different for
other domains and specialties.
6 Conclusion
Based on this feasibility study, we conclude that
we can generate a reliable patient problem list
reference standard for the ShARe corpus and
SNOMED CT provides better coverage of patient
problems than the CORE Problem List. In fu-
ture work, we plan to evaluate from each ShARe
report type, how well these patient problem lists
can be derived and visualized from the individ-
ual disease/disorder problem mentions leveraging
temporality and modality attributes using natu-
ral language processing and machine learning ap-
proaches.
57
Acknowledgments
This work was partially funded by NLM
(5T15LM007059 and 1R01LM010964), ShARe
(R01GM090187), Swedish Research Council
(350-2012-6658), and Swedish Fulbright Com-
mission.
References
Center for Medicare and Medicaid Services. 2013.
EHR Incentive Programs-Maintain Problem
List. http://www.cms.gov/Regulations-and-
Guidance/Legislation/EHRIncentivePrograms/
downloads/3 Maintain Problem ListEP.pdf.
Noemie Elhadad, Wendy Chapman, Tim OGorman,
Martha Palmer, and Guergana. Under Review
Savova. under review. The ShARe Schema for
the Syntactic and Semantic Annotation of Clinical
Texts.
George Hripcsak and Adam S. Rothschild. 2005.
Agreement, the F-measure, and Reliability in In-
formation Retrieval. J Am Med Inform Assoc,
12(3):296?298.
Stephane Meystre and Peter Haug. 2005. Automation
of a Problem List using Natural Language Process-
ing. BMC Medical Informatics and Decision Mak-
ing, 5(30).
Stephane M. Meystre and Peter J. Haug. 2008. Ran-
domized Controlled Trial of an Automated Problem
List with Improved Sensitivity. International Jour-
nal of Medical Informatics, 77:602?12.
Danielle L. Mowery, Pamela W. Jordan, Janyce M.
Wiebe, Henk Harkema, John Dowling, and
Wendy W. Chapman. 2013. Semantic Annotation
of Clinical Events for Generating a Problem List. In
AMIA Annu Symp Proc, pages 1032?1041.
National Library of Medicine. 2009. The
CORE Problem List Subset of SNOMED-
CT. Unified Medical Language System 2011.
http://www.nlm.nih.gov/research/umls/SNOMED-
CT/core subset.html.
Mohammed Saeed, C. Lieu, G. Raber, and Roger G.
Mark. 2002. MIMIC II: a massive temporal ICU
patient database to support research in intelligent pa-
tient monitoring. Comput Cardiol, 29.
Imre Solti, Barry Aaronson, Grant Fletcher, Magdolna
Solti, John H. Gennari, Melissa Cooper, and Thomas
Payne. 2008. Building an Automated Problem List
based on Natural Language Processing: Lessons
Learned in the Early Phase of Development. pages
687?691.
Brett R. South, Shuying Shen, Jianwei Leng, Tyler B.
Forbush, Scott L. DuVall, and Wendy W. Chapman.
2012. A prototype tool set to support machine-
assisted annotation. In Proceedings of the 2012
Workshop on Biomedical Natural Language Pro-
cessing, BioNLP ?12, pages 130?139. Association
for Computational Linguistics.
Lawrence Weed. 1970. Medical Records, Med-
ical Education and Patient Care: The Problem-
Oriented Record as a Basic Tool. Medical Pub-
lishers: Press of Case Western Reserve University,
Cleveland: Year Book.
58
Proceedings of the 2014 Workshop on Biomedical Natural Language Processing (BioNLP 2014), pages 88?92,
Baltimore, Maryland USA, June 26-27 2014.
c?2014 Association for Computational Linguistics
Temporal Expressions in Swedish Medical Text ? A Pilot Study
Sumithra Velupillai
Department of Computer and Systems Sciences
Stockholm University
Sweden
sumithra@dsv.su.se
Abstract
One of the most important features of
health care is to be able to follow a pa-
tient?s progress over time and identify
events in a temporal order. We describe
initial steps in creating resources for au-
tomatic temporal reasoning of Swedish
medical text. As a first step, we focus
on the identification of temporal expres-
sions by exploiting existing resources and
systems available for English. We adapt
the HeidelTime system and manually eval-
uate its performance on a small subset
of Swedish intensive care unit documents.
On this subset, the adapted version of Hei-
delTime achieves a precision of 92% and
a recall of 66%. We also extract the most
frequent temporal expressions from a sep-
arate, larger subset, and note that most ex-
pressions concern parts of days or specific
times. We intend to further develop re-
sources for temporal reasoning of Swedish
medical text by creating a gold standard
corpus also annotated with events and tem-
poral links, in addition to temporal expres-
sions and their normalised values.
1 Introduction
One of the most important features of health care
is to be able to follow patient progress over time
and identify clinically relevant events in a tempo-
ral order. In medical records, temporal informa-
tion is stored with explicit timestamps, but it is
also documented in free text in the clinical nar-
ratives. To meet our overall goal of building ac-
curate and useful information extraction systems
in the health care domain, our aim is to build re-
sources for temporal reasoning in Swedish clini-
cal text. For instance, in the example sentence
MR-unders?okningen av skallen ig?ar visade att
den v?a-sidiga f?or?andringen i thalamus minskat i
volym. (?The MRI-scan of the scull yesterday
showed that the left (abbreviated) side change in
thalamus has decreased in volume?), a temporal
reasoning system should extract the event (MRI-
scan of the scull) and the temporal expression
(yesterday), and be able to normalise the time ex-
pression to a specific date and classify the tempo-
ral relation.
In this pilot study we focus on the identifi-
cation of temporal expressions, utilising existing
resources and systems available for English. A
temporal expression is defined as any mention
of dates, times, durations, and frequencies, e.g.
?April 2nd?, ?10:50am?, ?five hours ago?, and
?every 2 hours?. When successfully identifying
such expressions, subsequent anchoring in time is
made possible.
Although English and Swedish are both Ger-
manic languages, there are some differences that
are important to take into account when adapt-
ing existing solutions developed for English to
Swedish, e.g. Swedish is more inflective and is
more compounding than English.
The purpose of this study is to initiate our work
on temporal reasoning for Swedish, and to evalu-
ate existing solutions adapted to Swedish. These
are our first steps towards the creation of a refer-
ence standard that can be used for evaluation of
future systems.
2 Background
Temporal reasoning has been the focus of several
international natural language processing (NLP)
challenges in the general domain such as ACE
1
,
TempEval-2 and 3 (Verhagen et al., 2010; Uz-
Zaman et al., 2013), and in the clinical domain
through the 2012 i2b2 challenge (Sun et al., 2013).
Most previous work has been performed on En-
1
http://www.itl.nist.gov/iad/mig/tests/ace/
88
glish documents, but the TempEval series have
also included other languages, e.g. Spanish. For
temporal modelling, the TimeML (Pustejovsky
et al., 2010) guidelines are widely used. The
TimeML standard denotes events (EVENT), tem-
poral expressions (TIMEX3) and temporal rela-
tions (TLINK).
For English, several systems have been devel-
oped for all or some of these subtasks, such as
the TARSQI Toolkit (Verhagen et al., 2005) and
SUTime (Chang and Manning, 2012). Both these
tools are rule-based, and rely on regular expres-
sions and gazetteers. The TARSQI Toolkit has
also been developed for the clinical domain: Med-
TTK (Reeves et al., 2013).
In other domains, and for other languages, Hei-
delTime (Str?otgen and Gertz, 2012) and TIMEN
(Llorens et al., 2012) are examples of other rule-
based systems. These are also developed to be
easily extendable to new domains and languages.
HeidelTime ranked first in the TempEval-3 chal-
lenge on TIMEX3:s, resulting in an F1 of 77.61
for the task of correctly identifying and normalis-
ing temporal expressions.
HeidelTime was also used in several participat-
ing systems in the i2b2 challenge (Lin et al., 2013;
Tang et al., 2013; Grouin et al., 2013) with suc-
cess. Top results for correctly identifying and nor-
malising temporal expressions in the clinical do-
main are around 66 F1 (Sun et al., 2013). The
system has also been adapted for French clinical
text (Hamon and Grabar, 2014).
3 Methods
The HeidelTime system was chosen for the ini-
tial development of a Swedish temporal expres-
sion identifier. Given that its architecture is de-
signed to be easily extendible for other languages
as well as domains, and after reviewing alternative
existing systems, we concluded that it was suitable
for this pilot study.
3.1 Data
We used medical records from an intensive care
unit (ICU) from the Stockholm EPR Corpus, a
clinical database from the Stockholm region in
Sweden
2
(Dalianis et al., 2012). Each medi-
cal record (document) contains all entries (notes)
2
Study approved by the Regional Ethical Review Board
in Stockholm (Etikpr?ovningsn?amnden i Stockholm), permis-
sion number 2012/834-31/5
about one patient a given day. The document con-
tains notes written by both physicians and nurses.
They also contain headings (e.g. Daganteckn-
ing (?Daily note?), Andning (?Breathing?)) and
timestamps for when a specific note/heading was
recorded in the medical record system. These are
excluded in this analysis.
Three subsets from this ICU dataset were used:
1) two randomly selected documents were used for
analysing and identifying domain specific time ex-
pressions and regular expressions to be added in
the adaptation of HeidelTime (development set),
2) a random sample of ten documents was used for
manual analysis and evaluation (test set), and 3) a
set of 100 documents was also extracted for the
purpose of empirically studying the types of tem-
poral expressions found in the data by the adapted
system (validation set).
3.2 Adaptation of HeidelTime and
Evaluation
The available resources (keywords and regular ex-
pression rules) in the HeidelTime system were ini-
tially translated automatically (Google translate
3
)
and manually corrected. Regular expressions were
modified to handle Swedish inflections and other
specific traits. An initial analysis on two separate,
randomly selected ICU notes (development set)
was performed, as a first step in adapting for both
the Swedish language and the clinical domain.
Results on the system performance were manu-
ally evaluated on the test set by one computational
linguistics researcher by analysing system outputs:
adding annotations when the system failed to iden-
tify a temporal expression, and correcting system
output errors. A contingency table was created
for calculating precision, recall and F1, the main
outcome measures. Moreover, the top most fre-
quent temporal expressions found by the system
on a separate set were extracted (validation set),
for illustration and analysis purposes.
4 Results
We report general statistics for the ICU corpus, re-
sults from the adaptation and evaluation of Hei-
delTime for Swedish (HTSwe) on the test set, and
the most frequent temporal expressions found by
HTSwe in a separate set of 100 ICU documents
(validation set).
3
http://translate.google.se
89
4.1 Data: ICU corpus
General statistics for the test set used in this study
is shown in Table 1. On average, each document
consists of 54.6 sentences, and each sentence con-
tains on average 8.7 tokens (including punctua-
tion). We observe that some sentences are very
short (min = 1), and there is great variability in
length, as can be seen through the standard devia-
tion.
# min - max avg ? std
Sentences 540 35 - 80 54.6?14.1
/document
Tokens 4749 1 - 52 8.7?5.7
/sentence
Table 1: General statistics for the test set (ten
ICU documents) used in this study. Minimum,
maximum, average and standard deviation for sen-
tences per document and tokens (including punc-
tuation) per sentence.
4.2 Adaptation and evaluation of
HeidelTime: HTSwe
The main modifications required in the adapta-
tion of HeidelTime to Swedish (HTSwe) involved
handling definite articles and plurals, e.g. adding
eftermiddag(en)?(ar)?(na)? (?afternoon?, ?the af-
ternoon?/?afternoons?/?the afternoons?). From
the analysis of the small development set, some
abbreviations were also added, e.g. em (?after-
noon?). Regular expressions for handling typical
ways dates are written in Swedish were added, e.g.
?020812? and ?31/12 -99? (day, month, year). In
order to avoid false positives, a rule for handling
measurements that could be interpreted as years
(e.g. 1900 ml) was also added (a negative rule).
Results from running HTSwe on the test set are
shown in Table 2. HTSwe correctly identified 105
temporal expressions, but missed 55 expressions
that should have been marked, and classified 9
expressions erroneously. In total, there are 160
TIMEX3s. Overall performance was 92% preci-
sion, 65% recall and F1 = 77%.
The main errors were due to faulty regular ex-
pressions for times, e.g. 13-tiden (?around 13 PM)
and missing keywords such as dygn (?day? - a
word to indicate a full day, i.e. 24 hours) and
lunchtid (?around lunch?). Some missing key-
words were specific for the clinical domain, e.g.
efternatten (?the after/late night?, typical for shift
indication). There were also some partial errors.
For instance, i dag (?today?) was only included
with the spelling idag in the system, thus generat-
ing a TIMEX3 output only for dag.
TIMEX3 Other
?
Annotator Annotator
TIMEX3 105 9 114
HTSwe
Other 55 4580 4635
HTSwe
?
160 4589 4749
Table 2: Contingency table, TIMEX3 annotations
by the annotator and the adapted HeidelTime sys-
tem for Swedish (HTSwe) on the test set. ?Other?
means all other tokens in the corpus. These results
yield a precision of 92%, a recall of 66%, and F1
= 77% for HTSwe.
On the validation set, 168 unique time expres-
sions were found by the system, and 1,178 in total.
The most frequent expressions all denote parts of
days, e.g. idag (?today?), nu (?now?), and natten
(?the night?), see Table 3. Specific times (mostly
specific hours) were also very common. Thus,
there were many translated expressions in the Hei-
delTime system that never occurred in the data.
TIMEX3 N %
idag (?today?) 164 14%
nu (?now?) 132 11%
natten (?the night?) 117 10%
morgonen (?the morning?) 96 8%
em (?afternoon?, abbreviated) 82 7%
kv?allen (?the evening?) 74 6%
ig?ar (?yesterday?) 49 4%
fm (?morning?, abbreviated) 34 3%
morgon (?morning?) 30 3%
natt (?night?) 26 2%
Total 1178 100%
Table 3: Most frequent (top ten, descending or-
der) TIMEX3s found by HTSwe on the validation
set (100 ICU documents). Total = all TIMEX3:s
found by HTSwe in the entire validation set. There
were 168 unique TIMEX3s in the validation set.
5 Discussion and Conclusion
We perform an initial study on automatic identifi-
cation of temporal expressions in Swedish clinical
90
text by translating and adapting the HeidelTime
system, and evaluating performance on Swedish
ICU records. Results show that precision is high
(92%), which is promising for our future develop-
ment of a temporal reasoning system for Swedish.
The main errors involve regular expressions for
time and some missing keywords; these expres-
sions will be added in our next iteration in this
work. Our results, F1 = 77%, are lower than state-
of-the-art systems for English clinical text, where
the top-performing system in the 2010 i2b2 Chal-
lenge achieved 90% F1 for TIMEX3 spans (Sun
et al., 2013). However, given the small size of this
study, results are encouraging, and we have cre-
ated a baseline system which can be used for fur-
ther improvements.
The adaptation and translation of HeidelTime
involved extending regular expressions and rules
to handle Swedish inflections and specific ways of
writing dates and times. Through a small, initial
analysis on a development set, some further ad-
ditions and modifications were made, which led
to the correct identification of common TIMEX3s
present in this type of document. A majority of
the expressions translated from the original system
was not found in the data. Hence, it is worthwhile
analysing a small subset to inform the adaptation
of HeidelTime.
The ICU notes are an interesting and suit-
able type of documentation for temporal reason-
ing studies, as they contain notes on the progress
of patients in constant care. However, from the re-
sults it is evident that the types of TIMEX3 expres-
sions are rather limited and mostly refer to parts
of days or specific times. Moreover, as recall was
lower (66%), there is clearly room for improve-
ment. We plan to extend our study to also include
other report types.
5.1 Limitations
There are several limitations in this study. The cor-
pus is very small, and evaluated only by one an-
notator, which limits the conclusions that can be
drawn from the analysis. For the creation of a ref-
erence standard, we plan to involve at least one
clinician, in order to get validation from a domain
expert, and to be able to calculate inter-annotator
agreement. The size of the corpus will also be in-
creased. We have not evaluated performance on
TIMEX3 normalisation, which, of course, is cru-
cial for an accurate temporal reasoning system.
For instance, we have not considered the category
Frequency, which is essential in the clinical do-
main to capture e.g. medication instructions and
dosages. Moreover, we have not annotated and
evaluated events. This is perhaps the most im-
portant part of a temporal reasoning system. We
plan to utilise existing named entity taggers de-
veloped in our group as a pre-annotation step in
the creation of our reference standard. The last
step involves annotating temporal links (TLINK)
between events and TIMEX3:s. We believe that
part-of-speech (PoS) and/or syntactic information
will be a very important component in an end-to-
end system for this task. We plan to tailor an exist-
ing Swedish PoS tagger, to better handle Swedish
clinical text.
5.2 Conclusion
Our main finding is that it is feasible to adapt Hei-
delTime to the Swedish clinical domain. More-
over, we have shown that the parts of days and
specific times are the most frequent temporal ex-
pressions in Swedish ICU documents.
This is the first step towards building resources
for temporal reasoning in Swedish. We believe
these results are useful for our continued endeav-
our in this area. Our next step is to add further
keywords and regular expressions to improve re-
call, and to evaluate TIMEX3 normalisation. Fol-
lowing that, we will annotate events and temporal
links.
To our knowledge, this is the first study on tem-
poral expression identification in Swedish clinical
text. All resulting gazetteers and guidelines in our
future work on temporal reasoning in Swedish will
be made publicly available.
Acknowledgments
The author wishes to thank the anonymous review-
ers for invaluable comments on this manuscript.
Thanks also to Danielle Mowery and Dr. Wendy
Chapman for all their support. This work was par-
tially funded by Swedish Research Council (350-
2012-6658) and Swedish Fulbright Commission.
References
Angel X. Chang and Christopher Manning. 2012.
SUTime: A library for recognizing and normaliz-
ing time expressions. In Nicoletta Calzolari (Con-
ference Chair), Khalid Choukri, Thierry Declerck,
91
Mehmet Uur Doan, Bente Maegaard, Joseph Mar-
iani, Jan Odijk, and Stelios Piperidis, editors, Pro-
ceedings of the Eight International Conference on
Language Resources and Evaluation (LREC?12), Is-
tanbul, Turkey, may. European Language Resources
Association (ELRA).
Hercules Dalianis, Martin Hassel, Aron Henriksson,
and Maria Skeppstedt. 2012. Stockholm EPR Cor-
pus: A Clinical Database Used to Improve Health
Care. In Pierre Nugues, editor, Proc. 4th SLTC,
2012, pages 17?18, Lund, October 25-26.
Cyril Grouin, Natalia Grabar, Thierry Hamon, Sophie
Rosset, Xavier Tannier, and Pierre Zweigenbaum.
2013. Eventual situations for timeline extraction
from clinical reports. JAMIA, 20:820?827.
Thierry Hamon and Natalia Grabar. 2014. Tuning Hei-
delTime for identifying time expressions in clinical
texts in English and French. In Proceedings of the
5th International Workshop on Health Text Mining
and Information Analysis (Louhi), pages 101?105,
Gothenburg, Sweden, April. Association for Com-
putational Linguistics.
Yu-Kai Lin, Hsinchun Chen, and Randall A. Brown.
2013. MedTime: A temporal information extraction
system for clinical narratives. Journal of Biomedical
Informatics, 46:20?28.
Hector Llorens, Leon Derczynski, Robert Gaizauskas,
and Estela Saquete. 2012. TIMEN: An Open Tem-
poral Expression Normalisation Resource. In Nico-
letta Calzolari (Conference Chair), Khalid Choukri,
Thierry Declerck, Mehmet Uur Doan, Bente Mae-
gaard, Joseph Mariani, Jan Odijk, and Stelios
Piperidis, editors, Proceedings of the Eight Interna-
tional Conference on Language Resources and Eval-
uation (LREC?12), Istanbul, Turkey, may. European
Language Resources Association (ELRA).
James Pustejovsky, Kiyong Lee, Harry Bunt, and Lau-
rent Romary. 2010. ISO-TimeML: An Interna-
tional Standard for Semantic Annotation. In Pro-
ceedings of the Seventh International Conference
on Language Resources and Evaluation (LREC?10),
Valletta, Malta, may. European Language Resources
Association (ELRA).
Ruth M. Reeves, Ferdo R. Ong, Michael E. Math-
eny, Joshua C. Denny, Dominik Aronsky, Glenn T.
Gobbel, Diane Montella, Theodore Speroff, and
Steven H. Brown. 2013. Detecting temporal expres-
sions in medical narratives. International Journal of
Medical Informatics, 82:118?127.
Jannik Str?otgen and Michael Gertz. 2012. Temporal
Tagging on Different Domains: Challenges, Strate-
gies, and Gold Standards. In Proceedings of the
Eigth International Conference on Language Re-
sources and Evaluation (LREC?12), pages 3746?
3753. ELRA.
Weiyi Sun, Anna Rumshisky, and
?
Ozlem Uzuner.
2013. Evaluating temporal relations in clinical text:
2012 i2b2 Challenge. JAMIA, 20(5):806?813.
Buzhou Tang, Yonghui Wu, Min Jiang, Yukun Chen,
Joshua C Denny, and Hua Xu. 2013. A hybrid sys-
tem for temporal information extraction from clini-
cal text. JAMIA, 20:828?835.
Naushad UzZaman, Hector Llorens, Leon Derczyn-
ski, James Allen, Marc Verhagen, and James Puste-
jovsky. 2013. SemEval-2013 Task 1: TempEval-3:
Evaluating Time Expressions, Events, and Temporal
Relations. In Second Joint Conference on Lexical
and Computational Semantics (*SEM), Volume 2:
Proceedings of the Seventh International Workshop
on Semantic Evaluation (SemEval 2013), pages 1?9,
Atlanta, Georgia, USA, June. Association for Com-
putational Linguistics.
Marc Verhagen, Inderjeet Mani, Roser Sauri, Robert
Knippen, Seok Bae Jang, Jessica Littman, Anna
Rumshisky, John Phillips, and James Pustejovsky.
2005. Automating Temporal Annotation with
TARSQI. In Proceedings of the ACL 2005 on
Interactive Poster and Demonstration Sessions,
ACLdemo ?05, pages 81?84, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Marc Verhagen, Roser Saur??, Tommaso Caselli, and
James Pustejovsky. 2010. SemEval-2010 Task 13:
TempEval-2. In Proceedings of the 5th Interna-
tional Workshop on Semantic Evaluation, SemEval
?10, pages 57?62, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
92
