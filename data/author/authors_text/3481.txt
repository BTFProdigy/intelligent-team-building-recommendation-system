Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 832?841,
Singapore, 6-7 August 2009.
c
?2009 ACL and AFNLP
Self-Training PCFG Grammars with Latent Annotations
Across Languages
Zhongqiang Huang
1
1
Laboratory for Computational Linguistics
and Information Processing
Institute for Advanced Computer Studies
University of Maryland, College Park
zqhuang@umiacs.umd.edu
Mary Harper
1,2
2
Human Language Technology
Center of Excellence
Johns Hopkins University
mharper@umiacs.umd.edu
Abstract
We investigate the effectiveness of self-
training PCFG grammars with latent anno-
tations (PCFG-LA) for parsing languages
with different amounts of labeled training
data. Compared to Charniak?s lexicalized
parser, the PCFG-LA parser was more ef-
fectively adapted to a language for which
parsing has been less well developed (i.e.,
Chinese) and benefited more from self-
training. We show for the first time that
self-training is able to significantly im-
prove the performance of the PCFG-LA
parser, a single generative parser, on both
small and large amounts of labeled train-
ing data. Our approach achieves state-
of-the-art parsing accuracies for a single
parser on both English (91.5%) and Chi-
nese (85.2%).
1 Introduction
There is an extensive research literature on build-
ing high quality parsers for English (Collins, 1999;
Charniak, 2000; Charniak and Johnson, 2005;
Petrov et al, 2006), however, models for parsing
other languages are less well developed. Take Chi-
nese for example; there have been several attempts
to develop accurate parsers for Chinese (Bikel and
Chiang, 2000; Levy and Manning, 2003; Petrov
and Klein, 2007), but the state-of-the-art perfor-
mance, around 83% F measure on Penn Chinese
Treebank (achieved by the Berkeley parser (Petrov
and Klein, 2007)) falls far short of performance
on English (?90-92%). As pointed out in (Levy
and Manning, 2003), there are many linguistic dif-
ferences between Chinese and English, as well as
structural differences between their corresponding
treebanks, and some of these make it a harder task
to parse Chinese. Additionally, the fact that the
available treebanked Chinese materials are more
limited than for English also increases the chal-
lenge of building high quality Chinese parsers.
Many of these differences would also tend to apply
to other less well investigated languages.
In this paper, we focus on English and Chinese
because the former is a language for which ex-
tensive parsing research has been conducted while
the latter is a language that has been less exten-
sively studied. We adapt and improve the Berke-
ley parser, which learns PCFG grammars with la-
tent annotations, and show through comparative
studies that this parser significantly outperforms
Charniak?s parser, which was initially developed
for English and subsequently ported to Chinese.
We focus on answering two questions: how well
does a parser perform across languages and how
much does it benefit from self-training?
The first question is of special interest when
choosing a parser that is designed for one language
and adapting it to another less studied language.
We improve the PCFG-LA parser by adding a
language-independent method for handling rare
words and adapt it to another language, Chinese,
by creating a method to better model Chinese un-
known words. Our results show that the PCFG-
LA parser performs significantly better than Char-
niak?s parser on Chinese, and is also somewhat
more accurate on English, although both parsers
have high accuracy.
The second question is important because la-
beled training data is often quite limited, espe-
cially for less well investigated languages, while
unlabeled data is ubiquitous. Early investigations
on self-training for parsing have had mixed re-
sults. Charniak (1997) reported no improvements
from self-training a PCFG parser on the standard
WSJ training set. Steedman et al (2003) re-
ported some degradation using a lexicalized tree
adjoining grammar parser and minor improve-
ment using Collins lexicalized PCFG parser; how-
ever, this gain was obtained only when the parser
832
was trained on a small labeled set. Reichart and
Rappoport (2007) obtained significant gains us-
ing Collins lexicalized parser with a different self-
training protocol, but again they only looked at
small labeled sets. McClosky et al (2006) effec-
tively utilized unlabeled data to improve parsing
accuracy on the standard WSJ training set, but they
used a two-stage parser comprised of Charniak?s
lexicalized probabilistic parser with n-best pars-
ing and a discriminative reranking parser (Char-
niak and Johnson, 2005), and thus it would be bet-
ter categorized as ?co-training? (McClosky et al,
2008). It is worth noting that their attempts at self-
training Charniak?s lexicalized parser directly re-
sulted in no improvement. There are other suc-
cessful semi-supervised training approaches for
dependency parsing, such as (Koo et al, 2008;
Wang et al, 2008), and it would be interesting
to investigate how they could be applied to con-
stituency parsing.
We show in this paper, for the first time, that
self-training is able to significantly improve the
performance of the PCFG-LA parser, a single gen-
erative parser, on both small and large amounts of
labeled training data, for both English and Chi-
nese. With self-training, a fraction of the WSJ
or CTB6 treebank training data is sufficient to
train a PCFG-LA parser that is able to achieve or
even beat the accuracies obtained using a single
parser trained on the entire treebank without self-
training. We conjecture based on our comparison
of the PCFG-LA parser to Charniak?s parser that
the addition of self-training data helps the former
parser learn more fine-grained latent annotations
without over-fitting.
The rest of this paper is organized as follows.
We describe the PCFG-LA parser and several en-
hancements in Section 2, and discuss self-training
in Section 3. We then outline the experimental
setup in Section 4, describe the results in Sec-
tion 5, and present a detailed analysis in Section 6.
The last section draws conclusions and describes
future work.
2 Parsing Model
The Berkeley parser (Petrov et al, 2006; Petrov
and Klein, 2007) is an efficient and effective parser
that introduces latent annotations (Matsuzaki et
al., 2005) to refine syntactic categories to learn
better PCFG grammars. In the example parse tree
in Figure 1(a), each syntactic category is split into
multiple latent subcategories, and accordingly the
original parse tree is decomposed into many parse
trees with latent annotations. Figure 1(b) depicts
one of such trees. The grammar and lexical rules
are split accordingly, e.g., NP?PRP is split into
different NP-i?PRP-j rules. The expansion prob-
abilities of these split rules are the parameters of a
PCFG-LA grammar.
S
She
PRP
NP VP
VBD
heard DT
NP
NN
the noise
.
.
.
NP?2
VBD?5PRP?3
She heard DT?2
the noise
NN?6
NP?6
.?1
S?1
VP?4
(a) (b)
Figure 1: (a) original treebank tree, (b) after latent
annotation.
The objective of training is to learn a grammar
with latent annotations that maximizes the like-
lihood of the training trees, i.e., the sum of the
likelihood of all parse trees with latent annota-
tions. Since the latent annotations are not avail-
able in the treebank, a variant of the EM algo-
rithm is utilized to learn the rule probabilities for
them. The Berkeley parser employs a hierarchi-
cal split-merge method that gradually increases the
number of latent annotations and adaptively allo-
cates them to different treebank categories to best
model the training data. In this paper, we call a
grammar trained after n split-merge steps an n-
th order grammar. The order of a grammar is a
step (not continuous) function of the number of la-
tent annotations because the split-merge algorithm
first splits each latent annotation into two and then
merges some of the splits back based on their abil-
ity to increase training likelihood.
For this paper, we implemented
1
our own ver-
sion of Berkeley parser. Updates include better
handling of rare words across languages, as well
as unknown Chinese words. The parser is able
to process difficult sentences robustly using adap-
tive beam expansion. The training algorithm was
updated to support a wide range of self-training
experiments (e.g., posterior-weighted unlabeled
data, introducing self-training in later iterations)
and to make use of multiple processors to paral-
lelize EM training. The parallelization is crucial
1
A major motivation for this implementation was to sup-
port some algorithms we are developing. Most of our en-
hancements will be merged with a future release of the Berke-
ley parser.
833
for training a model with large volumes of data in
a reasonable amount of time
2
.
We next describe the language-independent
method to handle rare words, which is impor-
tant for training better PCFG-LA grammars es-
pecially when the training data is limited in size,
and our unknown Chinese word handling method,
highlighting the importance of utilizing language-
specific features to enhance parsing performance.
As we will see later, both of these methods signif-
icantly improve parsing performance.
2.1 Rare Word Handling
Whereas rule expansions are frequently observed
in the treebank, word-tag co-occurrences are
sparser and more likely to suffer from over-fitting.
Although the lexicon smoothing method in the
Berkeley parser is able to make the word emis-
sion probabilities of different latent states of a
POS tag more alike, the EM training algorithm
still strongly discriminates among word identities.
Suppose word tag pairs ?w
1
, t? and ?w
2
, t? both
appear the same number of times in the training
data. In a PCFG grammar without latent annota-
tions, the probabilities of emitting these two words
given tag t would be the same, i.e., p(w
1
|t) =
p(w
2
|t). After introducing latent annotation x to
tag t, the emission probabilities of these two words
given a latent state t
x
may no longer be the same
because p(w
1
|t
x
) and p(w
2
|t
x
) are two indepen-
dent parameters that the EM algorithm optimizes
on. It is beneficial to learn subcategories of POS
tags to model different types of words, especially
for frequent words; however, it is not desirable to
strongly discriminate among rare words because it
could distract the model from learning about com-
mon phenomena.
To handle this problem, the probability of a la-
tent state t
x
generating a rare word w is forced
to be proportional to the emission probability of
word w given the surface tag t. This is achieved
by mapping all words with frequency less than
threshold
3
? to the unk symbol, and for each la-
tent state t
x
of a POS tag t, accumulating the word
tag statistics of these rare words to c
r
(t
x
, unk) =
?
w:c(w)<?
c(t
x
, w), and then redistributing them
among the rare words to estimate their emission
2
The parallel version is able to train our largest grammar
on a 8-core machine within a week, while the non-parallel
version is not able to finish even after 3 weeks.
3
The value of ? is tuned on the development set.
probabilities:
c(t
x
, w) = c
r
(t
x
, unk) ?
c(t, w)
c
r
(t, unk)
p(w|t
x
) = c(t
x
, w)/
?
w
c(t
x
, w)
2.2 Chinese Unknown Word Handling
The Berkeley parser utilizes statistics associated
with rare words (e.g., suffix, capitalization) to esti-
mate the emission probabilities of unknown words
at decoding time. This is adequate for for English,
however, only a limited number of classes of un-
known words, such as digits and dates, are handled
for Chinese. In this paper, we develop a character-
based unknown word model inspired by (Huang
et al, 2007) that reflects the fact that characters in
any position (prefix, infix, or suffix) can be predic-
tive of the part-of-speech (POS) type for Chinese
words. In our model, the word emission proba-
bility, p(w|t
x
), of an unknown word w given the
latent state t
x
of POS tag t is estimated by the ge-
ometric average of the emission probability of the
characters c
k
in the word:
P (w|t
x
) =
n
?
?
c
k
?w,P (c
k
|t)6=0
P (c
k
|t)
where n = |{c
k
? w|P (c
k
|t) 6= 0}|. Characters
not seen in the training data are ignored in the
computation of the geometric average. We back
off to use the rare word statistics regardless of
word identity when the above equation cannot be
used to compute the emission probability.
3 Parser Self-Training
Our hypothesis is that combining automatically la-
beled parses with treebank trees will help the EM
training of the PCFG-LA parser to make more in-
formed decisions about latent annotations and thus
generate more effective grammars. In this section,
we discuss how self-training is applied to train a
PCFG-LA parser.
There are several ways to automatically label
the data. A fairly standard method is to parse the
unlabeled sentences with a parser trained on la-
beled training data, and then combine the result-
ing parses with the treebank training data to re-
train the parser. This is the approach we chose
for self-training. An alternative approach is to run
EM directly on the labeled treebank trees and the
unlabeled sentences, without explicit parse trees
for the unlabeled sentences. However, because the
834
brackets would need to be determined for the un-
labeled sentences together with the latent annota-
tions, this would increase the running time from
linear in the number of expansion rules to cubic in
the length of the sentence.
Another important decision is how to weight
the gold standard and automatically labeled data
when training a new parser model. Errors in the
automatically labeled data could limit the accu-
racy of the self-trained model, especially when
there is a much greater quantity of automatically
labeled data than the gold standard training data.
To balance the gold standard and automatically
labeled data, one could duplicate the treebank
data to match the size of the automatically la-
beled data; however, the training of the PCFG-
LA parser would result in redundant applications
of EM computations over the same data, increas-
ing the cost of training. Instead we weight the
posterior probabilities computed for the gold and
automatically labeled data, so that they contribute
equally to the resulting grammar. Our preliminary
experiments show that balanced weighting is ef-
fective, especially for Chinese (about 0.4% abso-
lute improvement) where the automatic parse trees
have a relatively lower accuracy.
The training procedure of the PCFG-LA parser
gradually introduces more latent annotations dur-
ing each split-merge stage, and the self-labeled
data can be introduced at any of these stages. In-
troduction of the self-labeled data in later stages,
after some important annotations are learned from
the treebank, could result in more effective learn-
ing. We have found that a middle stage introduc-
tion (after 3 split-merge iterations) of the automat-
ically labeled data has an effect similar to balanc-
ing the weights of the gold and automatically la-
beled trees, possibly due to the fact that both meth-
ods place greater trust in the former than the latter.
In this study, we introduce the automatically la-
beled data at the outset and weight it equally with
the gold treebank training data in order to focus
our experiments to support a deeper analysis.
4 Experimental Setup
For the English experiments, sections from the
WSJ Penn Treebank are used as labeled training
data: section 2-19 for training, section 22 for de-
velopment, and section 23 as the test set. We also
used 210k
4
sentences of unlabeled news articles in
the BLLIP corpus for English self-training.
For the Chinese experiments, the Penn Chinese
Treebank 6.0 (CTB6) (Xue et al, 2005) is used
as labeled data. CTB6 includes both news articles
and transcripts of broadcast news. We partitioned
the news articles into train/development/test sets
following Huang et al (2007). The broadcast news
section is added to the training data because it
shares many of the characteristics of newswire text
(e.g., fully punctuated, contains nonverbal expres-
sions such as numbers and symbols). In addi-
tion, 210k sentences of unlabeled Chinese news
articles are used for self-training. Since the Chi-
nese parsers in our experiments require word-
segmented sentences as input, the unlabeled sen-
tences need to be word-segmented first. As shown
in (Harper and Huang, 2009), the accuracy of au-
tomatic word segmentation has a great impact on
Chinese parsing performance. We chose to use
the Stanford segmenter (Chang et al, 2008) in
our experiments because it is consistent with the
treebank segmentation and provides the best per-
formance among the segmenters that were tested.
To minimize the discrepancy between the self-
training data and the treebank data, we normalize
both CTB6 and the self-training data using UW
Decatur (Zhang and Kahn, 2008) text normaliza-
tion.
Table 1 summarizes the data set sizes used
in our experiments. We used slightly modi-
fied versions of the treebanks; empty nodes and
nonterminal-yield unary rules
5
, e.g., NP?VP, are
deleted using tsurgeon (Levy and Andrew, 2006).
Train Dev Test Unlabeled
English
39.8k 1.7k 2.4k 210k
(950.0k) (40.1k) (56.7k) (5,082.1k)
Chinese
24.4k 1.9k 2.0k 210k
(678.8k) (51.2k) (52.9k) (6,254.9k)
Table 1: The number of sentences (and words in
parentheses) in our experiments.
We trained parsers on 20%, 40%, 60%, 80%,
and 100% of the treebank training data to evaluate
4
This amount was constrained based on both CPU and
memory. We plan to investigate cloud computing to exploit
more unlabeled data.
5
As nonterminal-yield unary rules are less likely to be
posited by a statistical parser, it is common for parsers trained
on the standard Chinese treebank to have substantially higher
precision than recall. This gap between bracket recall and
precision is alleviated without loss of parse accuracy by delet-
ing the nonterminal-yield unary rules. This modification sim-
ilarly benefits both parsers we study here.
835
the effect of the amount of labeled training data on
parsing performance. We also compare how self-
training impacts the models trained with different
amounts of gold-standard training data. This al-
lows us to simulate scenarios where the language
has limited human-labeled resources.
We compare models trained only on the gold
labeled training data with those that utilize ad-
ditional unlabeled data. Self-training (PCFG-LA
or Charniak) proceeds in two steps. In the first
step, the parser is first trained on the allocated la-
beled training data (e.g., 40%) and is then used
to parse the unlabeled data. In the second step,
a new parser is trained on the weighted combina-
tion
6
of the allocated labeled training data and the
additional automatically labeled data. The devel-
opment set is used in each step to select the gram-
mar order with the best accuracy for the PCFG-LA
parser and to tune the smoothing parameters for
Charniak?s parser.
5 Results
In this section, we first present the effect of un-
known and rare word handling for the PCFG-LA
parser, and then compare and discuss the perfor-
mance of the PCFG-LA parser and Charniak?s
parser across languages with different amounts
of labeled training, either with or without self-
training.
5.1 Rare and Unknown Word Handling
Table 2 reports the effect of unknown and rare
word handing for the PCFG-LA parser trained on
100%
7
of the labeled training data. The rare word
handling improves the English parser by 0.68%
and the Chinese parser by 0.56% over the Berke-
ley parser. The Chinese unknown word handling
method alone improves the Chinese parser by
0.47%. The rare and unknown handling methods
together improve the Chinese parser by 0.92%. All
the improvements are statistically significant
8
.
We found that the rare word handling method
becomes more effective as the number of latent an-
notations increases, especially when there is not a
6
We balance the size of manually and automatically la-
beled data by posterior weighting for the PCFG-LA parsers
and by duplication for Charniak?s parser.
7
Greater improvements are obtained using smaller
amounts of labeled training data.
8
We use Bikel?s randomized parsing evaluation compara-
tor to determine the significance (p < 0.05) of difference
between two parsers? output.
English Chinese
PCFG-LA 89.95 83.23
+R 90.63 83.79
+U N/A 83.70
+R+U N/A 84.15
Table 2: Effects of rare word handling (+R) and
Chinese unknown handling (+U) on the test set.
sufficient amount of labeled training data. Shar-
ing statistics of the rare words during training re-
sults in more robust grammars with better pars-
ing performance. The unknown word handling
method also gives greater improvements on gram-
mars trained on smaller amounts of training data,
suggesting that it is quite helpful for modeling un-
seen words at decoding time. However, it tends to
be less effective when the number of latent anno-
tations increases, probably because the probability
estimation of unseen words based on surface tags
is less reliable for finer-gained latent annotations.
5.2 Labeled Data Only
When comparing the two parsers on both lan-
guages in Figure 2 with treebank training, it is
clear that they perform much better on English
than Chinese. While this is probably due in part
to the years of research on English, Chinese still
appears to be more challenging than English. The
comparison between the two parsing approaches
provides two interesting conclusions.
First, the PCFG-LA parser always performs sig-
nificantly better than Charniak?s parser on Chi-
nese, although both model English well. Ad-
mittedly Charniak?s parser has not been opti-
mized
9
on Chinese, but neither has the PCFG-
LA parser
10
. The lexicalized model in Charniak?s
parser was first optimized for English and required
sophisticated smoothing to deal with sparseness;
however, the lexicalized model developed for Chi-
nese works less well. In contrast, the PCFG-LA
parser learns the latent annotations from the data,
without any specification of what precisely should
be modeled and how it should be modeled. This
flexibility may help it better model new languages.
Second, while both parsers benefit from in-
creased amounts of gold standard training data,
the PCFG-LA parser gains more. The PCFG-LA
parser is initially poorer than Charniak?s parser
9
The Chinese port includes modification of the head table,
implementation of a Chinese punctuation model, etc.
10
The PCFG-LA parser without the unknown word han-
dling method still outperforms Charniak?s parser on Chinese.
836
 87
 88
 89
 90
 91
 92
 0.2  0.4  0.6  0.8  1
F s
co
re
Number of Labeled WSJ Training Trees
x 39,832
PCFG-LAPCFG-LA.ST CharniakCharniak.ST
(a) English
 76
 78
 80
 82
 84
 86
 0.2  0.4  0.6  0.8  1
F s
co
re
Number of Labeled CTB Training Trees
x 24,416
(b) Chinese
Figure 2: The performance of the PCFG-LA
parser and Charniak?s parser evaluated on the test
set, trained with different amounts of labeled train-
ing data, with and without self-training (ST).
when trained on 20% WSJ training data, proba-
bly because the training data is too small for it to
learn fine-grained annotations without over-fitting.
As more labeled training data becomes avail-
able, the performance of the PCFG-LA parser im-
proves quickly and finally outperforms Charniak?s
parser significantly. Moreover, performance of the
PCFG-LA parser continues to grow when more la-
beled training data is available, while the perfor-
mance of Charniak?s parser levels out at around
80% of the labeled data. The PCFG-LA parser im-
proves by 3.5% when moving from 20% to 100%
training data, compared to a 2.21% gain for Char-
niak?s parser. Similarly for Chinese, the PCFG-
LA parser also gains more (4.48% vs 3.63%).
5.3 Labeled + Self-Labeled
The PCFG-LA parser is also able to benefit more
from self-training than Charniak?s parser. On the
WSJ data set, Charniak?s parser benefits from self-
training initially when there is little labeled train-
ing data, but the improvement levels out quickly
as more labeled training trees become available.
In contrast, the PCFG-LA parser benefits consis-
tently from self-training
11
, even when using 100%
11
One may notice that the self-trained PCFG-LA parser
with 100% labeled WSJ data has a slightly lower test accu-
of the labeled training set. Similar trends are also
found for Chinese.
It should be noted that the PCFG-LA parser
trained on a fraction of the treebank training data
plus a large amount of self-labeled training data,
which comes with little or no cost, performs com-
parably or even better than grammars trained with
additional labeled training data. For example, the
self-trained PCFG-LA parser with 60% labeled
data is able to outperform the grammar trained
with 100% labeled training data alone for both En-
glish and Chinese. With self-training, even 40%
labeled WSJ training data is sufficient to train a
PCFG-LA parser that is comparable to the model
trained on the entire WSJ training data alone. This
is of significant importance, especially for lan-
guages with limited human-labeled resources.
One might conjecture that the PCFG-LA parser
benefits more from self-training than Charniak?s
parser because its self-labeled data has higher ac-
curacy. However, this is not true. As shown in Fig-
ure 2 (a), the PCFG-LA parser trained with 40%
of the WSJ training set alne has a much lower
performance (88.57% vs 89.96%) than Charniak?s
parser trained on the full WSJ training set. With
the same amount of self-training data (labeled by
each parser), the resulting PCFG-LA parser ob-
tains a much higher F score than the self-trained
Charniak?s parser (90.52% vs 90.18%). Similar
patterns can also be found for Chinese.
English Chinese
PCFG-LA 90.63 84.15
+ Self-training 91.46 85.18
Table 3: Final results on the test set.
Table 3 reports the final results on the test set
when trained on the entire WSJ or CTB6 training
set. For English, self-training contributes 0.83%
absolute improvement to the PCFG-LA parser,
which is comparable to the improvement obtained
from using semi-supervised training with the two-
stage parser in (McClosky et al, 2006). Note that
their improvement is achieved with the addition
of 2,000k unlabeled sentences using the combi-
nation of a generative parser and a discriminative
reranker, compared to using only 210k unlabeled
sentences with a single generative parser in our
approach. For Chinese, self-training results in a
racy than the self-trained PCFG-LA parser with 80% labeled
WSJ data. This is due to the variance in parser performance
when initialized with different seeds and the fact that the de-
velopment set is used to pick the best model for evaluation.
837
 87
 88
 89
 90
 91
 92
 93
 94
 95
 96
 97
 98
 0.2  0.4  0.6  0.8  1
F s
cor
e
Number of Labeled WSJ Training Trees
x 39,832
TestTest.ST TrainTrain.ST
 60
 65
 70
 75
 80
 85
 90
 95
 100
 0  1  2  3  4  5  6  7 103
104
105
106
107
F sc
ore
Num
ber 
of R
ules
 (log 
scale
)
Split-Merge Roundsfewer latent states more latent states
TestTest.ST TrainTrain.ST RulesRules.ST
 87
 88
 89
 90
 91
 92
 93
 94
 95
 96
 97
 0.2  0.4  0.6  0.8  1
F s
cor
e
Number of Labeled WSJ Training Trees
x 39,832
TestTest.ST TrainTrain.ST
5
6 6 6 6
6 7 7 7 7
(a) Charniak (b) PCFG-LA (20% WSJ) (c) PCFG-LA
Figure 3: (a) The training/test accuracy of Charniak?s parser trained on varying amounts of labeled
WSJ training data, with and without self-training (ST). (b) The training/test accuracy and the number
of nonzero rules of the PCFG-LA grammars trained on 20% of the labeled WSJ training data, w/ and
w/o ST. (c) The training/test accuracy of the PCFG-LA parser trained on varying amount of labeled WSJ
training data, w/ and w/o ST; the numbers along the training curves indicate the order of the grammars.
 76
 78
 80
 82
 84
 86
 88
 90
 92
 94
 0.2  0.4  0.6  0.8  1
F s
cor
e
Number of Labeled CTB Training Trees
x 24,416
TestTest.ST TrainTrain.ST
 55
 60
 65
 70
 75
 80
 85
 90
 95
 100
 0  1  2  3  4  5  6  7 103
104
105
106
107
F sc
ore
Non
zero
 Ru
les 
(log s
cale)
Split-Merge Roundsfewer latent states more latent states
TestTest.ST TrainTrain.ST RulesRules.ST
 78
 80
 82
 84
 86
 88
 90
 92
 94
 0.2  0.4  0.6  0.8  1
F s
cor
e
Number of Labeled CTB Training Trees
Train/Test Performance of the PCFG-LA Parser (CTB)
x 24,416
TestTest.ST TrainTrain.ST
4
5 5
6 6
6
6
6
7 7
(a) Charniak (b) PCFG-LA (20% CTB) (c) PCFG-LA
Figure 4: (a) The training/test accuracy of Charniak?s parser trained on varying amounts of labeled
CTB training data, with and without self-training (ST). (b) The training/test accuracy and the number
of nonzero rules of the PCFG-LA grammars trained on 20% of the labeled CTB training data, w/ and
w/o ST. (c) The training/test accuracy of the PCFG-LA parser trained on varying amount of labeled CTB
training data, w/ and w/o ST; the numbers along the training curves indicate the order of the grammars.
state-of-the-art parsing model with 85.18% accu-
racy (1.03% absolute improvement) on a represen-
tative test set. Both improvements are statistically
significant.
6 Analysis
In this section, we perform a series of analyses,
focusing on English (refer to Figure 3), to investi-
gate why the PCFG-LA parser benefits more from
additional data, most particularly automatically la-
beled data, when compared to Charniak?s parser.
Similar analyses have been done for Chinese with
similar results (refer to Figure 4).
Charniak?s parser is a lexicalized PCFG parser
that models lexicalized dependencies explicitly
observable in the training data and relies on
smoothing to avoid over-fitting. Although it is
able to benefit from more training data because of
broader lexicon and rule coverage and more robust
estimation of parameters, its ability to benefit from
the additional data is limited in the sense that it is
not able to generate additional predictive features
that are supported by this data. As shown in fig-
ure 3(a), the parsing accuracy of Charniak?s parser
on the test set improves as the amount of labeled
training data increases; however, the training accu-
racy
12
degrades as more data is added. Note that
the training accuracy
13
of Charniak?s parser also
12
The parser is tested on the treebank labeled set that the
parser is trained on.
13
The self-training data is combined with the labeled tree-
bank trees in a weighted manner; otherwise, the training ac-
curacy would be even lower.
838
decreases after the addition of self-training data.
This is expected for models like Charniak?s parser
with fixed model parameters; it is harder to model
more data with greater diversity. The addition of
self-labeled data helps on the test set initially but it
provides little gain when the labeled training data
becomes relatively large.
In contrast, the PCFG-LA grammar is able to
model the training data with different granulari-
ties. Fewer latent annotations are employed when
the training set is small. As the size of the train-
ing data increases, it is able to allocate more latent
annotations to better model the data. As shown in
Figure 3 (b), for a fixed amount (20%) of labeled
training data, the accuracy of the model on train-
ing data continues to improve as the number of la-
tent annotation increases. Although it is important
to limit the number of latent annotations to avoid
over-fitting, the ability to model training data ac-
curately given sufficient latent annotations is desir-
able when more training data is available. When
trained on the labeled data (20%) alone, the 5-th
order grammar achieves its optimal generalization
performance (based on the development set) and
begins to degrade afterwords. With the addition of
self-training data, the 5-th order grammar achieves
an even greater accuracy on the test set and its per-
formance continues to increase
14
when moving to
the 6-th or even 7-th order grammar.
Figure 3 (c) plots the training and test curves
of the English PCFG-LA parser with varying
amounts of labeled training data, with and with-
out self-training. This figure differs substantially
from Figure 3 (a). First, as mentioned earlier, the
PCFG-LA parser benefits much more from self-
training than Charniak?s parser with moderate to
large amounts of labeled training data. Second, in
contrast to Charniak?s parser for which training ac-
curacy degrades consistently as the amount of la-
beled training data increases, the training accuracy
of the PCFG-LA parser sometimes improves when
trained on more labeled training data (e.g., the best
model (at order 6) trained on 40%
15
labeled train-
14
Although the 20% self-trained grammar has a higher test
accuracy at the 7-th round than the 6-th round, the develop-
ment accuracy was better at the 6-th round, and thus we report
the test accuracy of the 6-th round grammar in Figure 3 (c).
15
For models trained with greater amounts of labeled train-
ing data, although their training accuracy becomes lower (due
to greater diversity) for the grammars (all at order 6) selected
by the development set, their 7-th order grammars (not re-
ported in the figure) actually have both higher training and
test accuracies than the 6-th order grammar trained on less
training data.
ing data alone has a higher training accuracy than
the best model (at order 5) trained on 20% labeled
training data). Third, the addition of self-labeled
data supports more accurate PCFG-LA grammars
with higher orders than those trained without self-
training, as evidenced by scores on both the train-
ing and test data. This suggests that the self-
trained grammars are able to utilize more latent
annotations to learn deeper dependencies.
 2
 4
 6
 8
 10
 12
 14
 16
 18
 20
 22
 0  2  4  6  8  10  12  14  16 0
 1.2
 2.4
 3.6
 4.8
 6
Re
lati
ve 
red
uct
ion
 of 
F e
rro
r (%
)
Nu
mb
er 
of b
rac
ket
s
Span Length
x 1e+4
+Labeled +Unlabled #Brackets
Figure 5: The relative reduction of bracketing er-
rors for different span lengths, evaluated on the
test set. The baseline model is the PCFG-LA
parser trained on 20% of the WSJ training data.
The +Unlabeled curve corresponds to the parser
trained with the additional automatically labeled
data and the +Labeled curve corresponds to the
parser trained with additional 20% labeled training
data. The counts of the brackets are computed on
the gold reference. Span length ?0? is designated
for the effect on preterminal POS tags to differ-
entiate it from the non-terminal brackets spanning
only one word.
Figure 5 compares the effect of additional tree-
bank labeled and automatically labeled data on the
relative reduction of bracketing errors for different
span lengths. It is clear from the figure that the im-
provement in parsing accuracy from self-training
is the result of better bracketing across all span
lengths
16
. However, even though the automati-
cally labeled training data provides more improve-
ment than the additional treebank labeled data in
terms of parsing accuracy, this data is less effective
at improving tagging accuracy than the additional
treebank labeled training data.
So, how could self-training improve rule esti-
mation when training the PCFG-LA parser with
more latent annotations? One possibility is that the
automatically labeled data smooths the parameter
16
There is a slight degradation in bracketing accuracy for
some spans longer than 16 words, but the effect is negligible
due to their low counts.
839
estimates in the EM algorithm, enabling effective
training of models with more parameters to learn
deeper dependencies. Let p(a ? b|e, t) be the
posterior probability of expanding subcategories a
to b given the event e, which is a rule expansion
on a treebank parse tree t. T
l
and T
u
are the sets
of gold and automatically labeled parse trees, re-
spectively. The update of the rule expansion prob-
ability p(a ? b) in self-training (with weighting
parameter ?) can be expressed as:
P
t?T
l
P
e?t
p(a? b|e, t) + ?
P
t?T
u
P
e?t
p(a? b|e, t)
P
b
(
P
t?T
l
P
e?t
p(a? b|e, t) + ?
P
t?T
u
P
e?t
p(a? b|e, t))
Since the unlabeled data is parsed by a lower
order grammar (with fewer latent annotations),
the expected counts from the automatically la-
beled data can be thought of as counts from a
lower-order grammar
17
that smooth the higher-
order (with more latent annotations) grammar.
We observe that many of the rule parameters of
the grammar trained on WSJ training data alone
have zero probabilities (rules with extremely low
probabilities are also filtered to zero), as was also
pointed out in (Petrov et al, 2006). On the one
hand, this is what we want because the grammar
should learn to avoid impossible rule expansions.
On the other hand, this might also be a sign of
over-fitting of the labeled training data. As shown
in Figure 3 (b), the grammar obtained with the ad-
dition of automatically labeled data contains many
more non-zero rules, and its performance contin-
ues to improve with more latent annotations. Sim-
ilar patterns also appear when using self-training
for other amounts of labeled training data. As is
partially reflected by the zero probability rules, the
addition of the automatically labeled data enables
the exploration of a broader parameter space with
less danger of over-fitting the data. Also note that
the benefit of the automatically labeled data is less
clear in the early training stages (i.e., when there
are fewer latent annotations), as can be seen in Fig-
ure 3 (b). This is probably because there is a small
number of free parameters and the treebank data is
sufficiently large for robust parameter estimation.
17
We also trained models using only the automatically la-
beled data without combining it with human-labeled training
data, but they were no more accurate than those trained on
the human-labeled training data alone without self-training.
7 Conclusion
In this paper, we showed that PCFG-LA parsers
can be more effectively applied to languages
where parsing is less well developed and that they
are able to benefit more from self-training than
lexicalized generative parsers. We show for the
first time that self-training is able to significantly
improve the performance of a PCFG-LA parser, a
single generative parser, on both small and large
amounts of labeled training data.
We conjecture based on our analysis that the
EM training algorithm is able to exploit the in-
formation available in both gold and automati-
cally labeled data with more complex grammars
while being less affected by over-fitting. Bet-
ter results would be expected by combining the
PCFG-LA parser with discriminative reranking
approaches (Charniak and Johnson, 2005; Huang,
2008) for self training. Self-training should also
benefit other discriminatively trained parsers with
latent annotations (Petrov and Klein, 2008), al-
though training would be much slower compared
to using generative models, as in our case.
In future work, we plan to scale up the training
process with more unlabeled training data (e.g.,
gigaword) and investigate automatic selection of
materials that are most suitable for self-training.
We also plan to investigate domain adaptation and
apply the model to other languages with modest
treebank resources. Finally, it is also important to
explore other ways to exploit the use of unlabeled
data.
Acknowledgments
This material is based upon work supported in
part by the Defense Advanced Research Projects
Agency (DARPA) under Contract No. HR0011-
06-C-0023 and NSF IIS-0703859. Any opinions,
findings and/or recommendations expressed in this
paper are those of the authors and do not necessar-
ily reflect the views of the funding agencies or the
institutions where the work was completed.
References
Daniel M. Bikel and David Chiang. 2000. Two sta-
tistical parsing models applied to the chinese tree-
bank. In Proceedings of the Second Chinese Lan-
guage Processing Workshop.
Pi-Chuan Chang, Michel Gally, and Christopher Man-
ning. 2008. Optimizing chinese word segmentation
840
for machine translation performance. In ACL 2008
Third Workshop on Statistical Machine Translation.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and maxent discriminative
reranking. In ACL.
Eugene Charniak. 1997. Statistical parsing with a
context-free grammar and word statistics. In ICAI.
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In ACL.
Michael John Collins. 1999. Head-driven statistical
models for natural language parsing. Ph.D. thesis,
University of Pennsylvania, Philadelphia, PA, USA.
Mary Harper and Zhongqiang Huang. 2009. Chinese
statistical parsing. To appear in The Gale Book.
Zhongqiang Huang, Mary Harper, and Wen Wang.
2007. Mandarin part-of-speech tagging and dis-
criminative reranking. In EMNLP.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In ACL.
Terry Koo, Xavier Carrera, and Michael Collins. 2008.
Simple semi-supervised dependency parsing. In
ACL.
Roger Levy and Galen Andrew. 2006. Tregex and tsur-
geon: Tools for querying and manipulating tree data
structures. In LREC.
Roger Levy and Christopher Manning. 2003. Is it
harder to parse chinese, or the chinese treebank. In
ACL.
Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii.
2005. Probabilistic CFG with latent annotations. In
ACL.
David McClosky, Eugene Charniak, and Mark John-
son. 2006. Effective self-training for parsing. In
HLT-NAACL.
David McClosky, Eugene Charniak, and Mark John-
son. 2008. When is self-training effective for pars-
ing? In COLING.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In HLT-NAACL.
Slav Petrov and Dan Klein. 2008. Sparse multi-scale
grammars for discriminative latent variable parsing.
In EMNLP.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In ACL.
Roi Reichart and Ari Rappoport. 2007. Self-training
for enhancement and domain adaptation of statistical
parsers trained on small datasets. In ACL.
Mark Steedman, Miles Osborne, Anoop Sarkar,
Stephen Clark, Rebecca Hwa, Julia Hockenmaier,
Paul Ruhlen, Steven Baker, and Jeremiah Crim.
2003. Bootstrapping statistical parsers from small
datasets. In EACL.
Qin Wang, Dale Schuurmans, and Dekang Lin. 2008.
Semi-supervised convex training for dependency
parsing. In ACL.
Nianwen Xue, Fei Xia, Fu-dong Chiou, and Marta
Palmer. 2005. The Penn Chinese Treebank: Phrase
structure annotation of a large corpus. Natural Lan-
guage Engineering.
Bin Zhang and Jeremy G. Kahn. 2008. Evaluation of
decatur text normalizer for language model training.
Technical report, University of Washington.
841
Proceedings of NAACL HLT 2009: Short Papers, pages 213?216,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Improving A Simple Bigram HMM Part-of-Speech Tagger by Latent
Annotation and Self-Training
Zhongqiang Huang?, Vladimir Eidelman?, Mary Harper??
?Laboratory for Computational Linguistics and Information Processing
Institute for Advanced Computer Studies
University of Maryland, College Park
?Human Language Technology Center of Excellence
Johns Hopkins University
{zqhuang,vlad,mharper}@umiacs.umd.edu
Abstract
In this paper, we describe and evaluate a bi-
gram part-of-speech (POS) tagger that uses
latent annotations and then investigate using
additional genre-matched unlabeled data for
self-training the tagger. The use of latent
annotations substantially improves the per-
formance of a baseline HMM bigram tag-
ger, outperforming a trigram HMM tagger
with sophisticated smoothing. The perfor-
mance of the latent tagger is further enhanced
by self-training with a large set of unlabeled
data, even in situations where standard bigram
or trigram taggers do not benefit from self-
training when trained on greater amounts of
labeled training data. Our best model obtains
a state-of-the-art Chinese tagging accuracy of
94.78% when evaluated on a representative
test set of the Penn Chinese Treebank 6.0.
1 Introduction
Part-of-speech (POS) tagging, the process of as-
signing every word in a sentence with a POS tag
(e.g., NN (normal noun) or JJ (adjective)), is pre-
requisite for many advanced natural language pro-
cessing tasks. Building upon the large body of re-
search to improve tagging performance for various
languages using various models (e.g., (Thede and
Harper, 1999; Brants, 2000; Tseng et al, 2005b;
Huang et al, 2007)) and the recent work on PCFG
grammars with latent annotations (Matsuzaki et al,
2005; Petrov et al, 2006), we will investigate the use
of fine-grained latent annotations for Chinese POS
tagging. While state-of-the-art tagging systems have
achieved accuracies above 97% in English, Chinese
POS tagging (Tseng et al, 2005b; Huang et al,
2007) has proven to be more challenging, and it is
the focus of this study.
The value of the latent variable approach for tag-
ging is that it can learn more fine grained tags to bet-
ter model the training data. Liang and Klein (2008)
analyzed the errors of unsupervised learning using
EM and found that both estimation and optimiza-
tion errors decrease as the amount of unlabeled data
increases. In our case, the learning of latent anno-
tations through EM may also benefit from a large
set of automatically labeled data to improve tagging
performance. Semi-supervised, self-labeled data has
been effectively used to train acoustic models for
speech recognition (Ma and Schwartz, 2008); how-
ever, early investigations of self-training on POS
tagging have mixed outcomes. Clark et al (2003)
reported positive results with little labeled training
data but negative results when the amount of labeled
training data increases. Wang et al (2007) reported
that self-training improves a trigram tagger?s accu-
racy, but this tagger was trained with only a small
amount of in-domain labeled data.
In this paper, we will investigate whether the
performance of a simple bigram HMM tagger can
be improved by introducing latent annotations and
whether self-training can further improve its perfor-
mance. To the best of our knowledge, this is the first
attempt to use latent annotations with self-training
to enhance the performance of a POS tagger.
2 Model
POS tagging using a hidden Markov model can be
considered as an instance of Bayesian inference,
213
wherein we observe a sequence of words and need
to assign them the most likely sequence of POS tags.
If ti1 denotes the tag sequence t1, ? ? ? , ti, and wi1denotes the word sequence w1, ? ? ? , wi, given the
first-order Markov assumption of a bigram tagger,
the best tag sequence ?(wn1 ) for sentence wn1 can be
computed efficiently as1:
?(wn1 ) = argmaxtn1 p(tn1 |wn1 )
? argmaxtn1
?
i
p(ti|ti?1)p(wi|ti)
with a set of transition parameters {p(b|a)}, for tran-
siting to tag b from tag a, and a set of emission
parameters {p(w|a)}, for generating word w from
tag a. A simple HMM tagger is trained by pulling
counts from labeled data and normalizing to get the
conditional probabilities.
It is well know that the independence assumption
of a bigram tagger is too strong in many cases. A
common practice for weakening the independence
assumption is to use a second-order Markov as-
sumption, i.e., a trigram tagger. This is similar to
explicitly annotating each POS tag with the preced-
ing tag. Rather than explicit annotation, we could
use latent annotations to split the POS tags, sim-
ilarly to the introduction of latent annotations to
PCFG grammars (Matsuzaki et al, 2005; Petrov
et al, 2006). For example, the NR tag may be
split into NR-1 and NR-2, and correspondingly the
POS tag sequence of ?Mr./NR Smith/NR saw/VV
Ms./NR Smith/NR? could be refined as: ?Mr./NR-2
Smith/NR-1 saw/VV-2 Ms./NR-2 Smith/NR-1?.
The objective of training a bigram tagger with la-
tent annotations is to find the transition and emission
probabilities associated with the latent tags such that
the likelihood of the training data is maximized. Un-
like training a standard bigram tagger where the POS
tags are observed, in the latent case, the latent tags
are not observable, and so a variant of EM algorithm
is used to estimate the parameters.
Given a sentence wn1 and its tag sequence tn1 , con-sider the i-th word wi and its latent tag ax ? a = ti
(which means ax is a latent tag of tag a, the i-th tag
in the sequence) and the (i + 1)-th word wi+1 and
its latent tag by ? b = ti+1, the forward, ?i+1(by) =
p(wi+11 , by), and backward, ?i(ax) = p(wni+1|ax),probabilities can be computed recursively:
?i+1(by) =
?
x
?i(ax)p(by|ax)p(wi+1|by)
1We assume that symbols exist implicitly for boundary con-
ditions.
?i(ax) =
?
y
p(by|ax)p(wi+1|by)?j+1(by)
In the E step, the posterior probabilities of co-
occurrence events can be computed as:
p(ax, by|w) ? ?i(ax)p(by|ax)?i+1(by)
p(ax, wi|w) ? ?i(ax)?i(ax)
In the M step, the above posterior probabilities are
used as weighted observations to update the transi-
tion and emission probabilities2:
p(by|ax) = c(ax, by)/
?
by
c(ax, by)
p(w|ax) = c(ax, w)/
?
w
c(ax, w)
A hierarchical split-and-merge method, similar
to (Petrov et al, 2006), is used to gradually increase
the number of latent annotations while allocating
them adaptively to places where they would pro-
duce the greatest increase in training likelihood (e.g.,
we observe heavy splitting in categories such as NN
(normal noun) and VV (verb), that cover a wide vari-
ety of words, but only minimal splitting in categories
like IJ (interjection) and ON (onomatopoeia)).
Whereas tag transition occurrences are frequent,
allowing extensive optimization using EM, word-tag
co-occurrences are sparser and more likely to suf-
fer from over-fitting. To handle this problem, we
map all words with frequency less than threshold3
? to symbol unk and for each latent tag accumu-
late the word tag statistics of these rare words to
cr(ax, unk) = ?w:c(w)<? c(ax, w). These statistics
are redistributed among the rare words (w : c(w) <
?) to compute their emission probabilities:
c(ax, w) = cr(ax, unk) ? c(a,w)/cr(a, unk)
p(w|ax) = c(ax, w)/
?
w
c(ax, w)
The impact of this rare word handling method will
be investigated in Section 3.
A character-based unknown word model, similar
to the one described in (Huang et al, 2007), is used
to handle unknown Chinese words during tagging.
A decoding method similar to the max-rule-product
method in (Petrov and Klein, 2007) is used to tag
sentences using our model.
3 Experiments
The Penn Chinese Treebank 6.0 (CTB6) (Xue et al,
2005) is used as the labeled data in our study. CTB6
2c(?) represents the count of the event.
3The value of ? is tuned on the development set.
214
contains news articles, which are used as the primary
source of labeled data in our experiments, as well as
broadcast news transcriptions. Since the news ar-
ticles were collected during different time periods
from different sources with a diversity of topics, in
order to obtain a representative split of train-test-
development sets, we divide them into blocks of 10
files in sorted order and for each block use the first
file for development, the second for test, and the re-
maining for training. The broadcast news data ex-
hibits many of the characteristics of newswire text
(it contains many nonverbal expressions, e.g., num-
bers and symbols, and is fully punctuated) and so is
also included in the training data set. We also uti-
lize a greater number of unlabeled sentences in the
self-training experiments. They are selected from
similar sources to the newswire articles, and are
normalized (Zhang and Kahn, 2008) and word seg-
mented (Tseng et al, 2005a). See Table 1 for a sum-
mary of the data used.
Train Dev Test Unlabeled
sentences 24,416 1904 1975 210,000
words 678,811 51,229 52,861 6,254,947
Table 1: The number of sentences and words in the data.
50 100 150 200 250 300 350 40091.5
92
92.5
93
93.5
94
94.5
Number of latent annotations
Tok
en 
acc
ura
cy (%
)
Bigram+LA:1
Bigram+LA:2
Trigram
Figure 1: The learning curves of the bigram tagger with
latent annotations on the development set.
Figure 1 plots the learning curves of two bigram
taggers with latent annotations (Bigram+LA:2 has
the special handling of rare words as described in
Section 2 while Bigram+LA:1 does not) and com-
pares its performance with a state-of-the-art trigram
HMM tagger (Huang et al, 2007) that uses trigram
transition and emission models together with bidi-
rectional decoding. Both bigram taggers initially
have much lower tagging accuracy than the trigram
tagger, due to its strong but invalid independence as-
sumption. As the number of latent annotations in-
creases, the bigram taggers are able to learn more
from the context based on the latent annotations,
and their performance improves significantly, out-
performing the trigram tagger. The performance
gap between the two bigram taggers suggests that
over-fitting occurs in the word emission model when
more latent annotations are available for optimiza-
tion; sharing the statistics among rare words alle-
viates some of the sparseness while supporting the
modeling of deeper dependencies among more fre-
quent events. In the later experiments, we use Bi-
gram+LA to denote the Bigram+LA:2 tagger.
Figure 2 compares the self-training capability of
three models (the bigram tagger w/ or w/o latent
annotations, and the aforementioned trigram tagger)
using different sizes of labeled training data and the
full set of unlabeled data. For each model, a tag-
ger is first trained on the allocated labeled training
data and is then used to tag the unlabeled data. A
new tagger is then trained on the combination4 of
the allocated labeled training data and the newly au-
tomatically labeled data.
0.1 0.2 0.4 0.6 0.8 189
90
91
92
93
94
95
Fraction of CTB6 training data
Tok
en 
acc
ura
cy (%
)
Bigram+LA+STBigram+LATrigram+STTrigramBigram+STBigram
Figure 2: The performance of three taggers evaluated on
the development set, before and after self-training with
different sizes of labeled training data.
There are two interesting observations that distin-
guish the bigram tagger with latent annotations from
the other two taggers. First, although all of the tag-
gers improve as more labeled training data is avail-
able, the performance gap between the bigram tag-
ger with latent annotations and the other two taggers
also increases. This is because more latent annota-
tions can be used to take advantage of the additional
training data to learn deeper dependencies.
Second, the bigram tagger with latent annotations
benefits much more from self-training, although it
4We always balance the size of manually and automatically
labeled data through duplication (for the trigram tagger) or pos-
terior weighting (for the bigram tagger w/ or w/o latent annota-
tions), as this provides superior performance.
215
already has the highest performance among the three
taggers before self-training. The bigram tagger
without latent annotations benefits little from self-
training. Except for a slight improvement when
there is a small amount of labeled training, self-
training slightly hurts tagging performance as the
amount of labeled data increases. The trigram tag-
ger benefits from self-training initially but eventu-
ally has a similar pattern to the bigram tagger when
trained on the full labeled set. The performance
of the latent bigram tagger improves consistently
with self-training. Although the gain decreases for
models trained on larger training sets, since stronger
models are harder to improve, self-training still con-
tributes significantly to model accuracy.
The final tagging performance on the test set is
reported in Table 2. All of the improvements are
statistically significant (p < 0.005).
Tagger Token Accuracy (%)
Bigram 92.25
Trigram 93.99
Bigram+LA 94.53
Bigram+LA+ST 94.78
Table 2: The performance of the taggers on the test set.
It is worth mentioning that we initially added la-
tent annotations to a trigram tagger, rather than a bi-
gram tagger, to build from a stronger starting point;
however, this did not work well. A trigram tagger re-
quires sophisticated smoothing to handle data spar-
sity, and introducing latent annotations exacerbates
the sparsity problem, especially for trigram word
emissions. The uniform extension of a bigram tag-
ger to a trigram tagger ignores whether the use of ad-
ditional context is helpful and supported by enough
data, nor is it able to use a longer context. In con-
trast, the bigram tagger with latent annotations is
able to learn different granularities for tags based on
the training data.
4 Conclusion
In this paper, we showed that the accuracy of a sim-
ple bigram HMM tagger can be substantially im-
proved by introducing latent annotations together
with proper handling of rare words. We also showed
that this tagger is able to benefit from self-training,
despite the fact that other models, such as bigram or
trigram HMM taggers, do not.
In the future work, we will investigate automatic
data selection methods to choose materials that are
most suitable for self-training and evaluate the effect
of the amount of automatically labeled data.
Acknowledgments
This work was supported by NSF IIS-0703859
and DARPA HR0011-06-C-0023 and HR0011-06-
2-001. Any opinions, findings and/or recommenda-
tions expressed in this paper are those of the authors
and do not necessarily reflect the views of the fund-
ing agencies.
References
T. Brants. 2000. TnT a statistical part-of-speech tagger.
In ANLP.
S. Clark, J. R. Curran, and M. Osborne. 2003. Bootstrap-
ping pos taggers using unlabelled data. In CoNLL.
Z. Huang, M. Harper, and W. Wang. 2007. Mandarin
part-of-speech tagging and discriminative reranking.
EMNLP.
P. Liang and D. Klein. 2008. Analyzing the errors of
unsupervised learning. In ACL.
J. Ma and R. Schwartz. 2008. Factors that affect unsu-
pervised training of acoustic models. In Interspeech.
T. Matsuzaki, Y. Miyao, and J. Tsujii. 2005. Probabilis-
tic CFG with latent annotations. In ACL. Association
for Computational Linguistics.
S. Petrov and D. Klein. 2007. Improved inference for
unlexicalized parsing. In HLT-NAACL.
S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006.
Learning accurate, compact, and interpretable tree an-
notation. In ACL.
S. M. Thede and M. P. Harper. 1999. A second-order
hidden markov model for part-of-speech tagging. In
ACL.
H. Tseng, P. Chang, G. Andrew, D. Jurafsky, and C. Man-
ning. 2005a. A conditional random field word seg-
menter. In SIGHAN Workshop on Chinese Language
Processing.
H. Tseng, D. Jurafsky, and C. Manning. 2005b. Morpho-
logical features help pos tagging of unknown words
across language varieties. In SIGHAN Workshop on
Chinese Language Processing.
W. Wang, Z. Huang, and M. Harper. 2007. Semi-
supervised learning for part-of-speech tagging of Man-
darin transcribed speech. In ICASSP.
N. Xue, F. Xia, F. Chiou, and M. Palmer. 2005. The
penn chinese treebank: Phrase structure annotation of
a large corpus. Natural Language Engineering.
B. Zhang and J. G. Kahn. 2008. Evaluation of decatur
text normalizer for language model training. Technical
report, University of Washington.
216
Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 1093?1102, Prague, June 2007. c?2007 Association for Computational Linguistics
Mandarin Part-of-Speech Tagging and Discriminative Reranking
Zhongqiang Huang1
1Purdue University
West Lafayette, IN 47907
zqhuang@purdue.edu
Mary P. Harper1,2
2University of Maryland
College Park, MD 20742
mharper@casl.umd.edu
Wen Wang
SRI International
Menlo Park, CA 94025
wwang@speech.sri.com
Abstract
We present in this paper methods to improve
HMM-based part-of-speech (POS) tagging
of Mandarin. We model the emission prob-
ability of an unknown word using all the
characters in the word, and enrich the stan-
dard left-to-right trigram estimation of word
emission probabilities with a right-to-left
prediction of the word by making use of the
current and next tags. In addition, we utilize
the RankBoost-based reranking algorithm
to rerank the N-best outputs of the HMM-
based tagger using various n-gram, mor-
phological, and dependency features. Two
methods are proposed to improve the gen-
eralization performance of the reranking al-
gorithm. Our reranking model achieves an
accuracy of 94.68% using n-gram and mor-
phological features on the Penn Chinese
Treebank 5.2, and is able to further improve
the accuracy to 95.11% with the addition of
dependency features.
1 Introduction
Part-of-speech (POS) tagging is potentially help-
ful for many advanced natural language processing
tasks, for example, named entity recognition, pars-
ing, and sentence boundary detection. Much re-
search has been done to improve tagging perfor-
mance for a variety of languages. The state-of-the-
art systems have achieved an accuracy of 97% for
English on the Wall Street Journal (WSJ) corpus
(which contains 4.5M words) using various mod-
els (Brants, 2000; Ratnaparkhi, 1996; Thede and
Harper, 1999). Lower accuracies have been reported
in the literature for Mandarin POS tagging (Tseng et
al., 2005; Xue et al, 2002). This is, in part, due to
the relatively small size and the different annotation
guidelines (e.g., granularity of the tag set) for the an-
notated corpus of Mandarin. Xue at el. (2002) and
Tseng at el. (2005) reported accuracies of 93% and
93.74% on CTB-I (Xue et al, 2002) (100K words)
and CTB 5.0 (500K words), respectively, each us-
ing a Maximum Entropy approach. The character-
istics of Mandarin make it harder to tag than En-
glish. Chinese words tend to have greater POS tag
ambiguity than English. Tseng at el. (2005) reported
that 29.9% of the words in CTB have more than one
POS assignment compared to 19.8% of the English
words in WSJ. Moreover, the morphological prop-
erties of Chinese words complicate the prediction of
POS type for unknown words.
These challenges for Mandarin POS tagging
suggest the need to develop more sophisticated
methods. In this paper, we investigate the use
of a discriminative reranking approach to in-
crease Mandarin tagging accuracy. Reranking ap-
proaches (Charniak and Johnson, 2005; Chen et al,
2002; Collins and Koo, 2005; Ji et al, 2006; Roark
et al, 2006) have been successfully applied to many
NLP applications, including parsing, named entity
recognition, sentence boundary detection, etc. To
the best of our knowledge, reranking approaches
have not been used for POS tagging, possibly due
to the already high levels of accuracy for English,
which leave little room for further improvement.
However, the relatively poorer performance of ex-
isting methods on Mandarin POS tagging makes
reranking a much more compelling technique to
evaluate. In this paper, we use reranking to improve
tagging performance of an HMM tagger adapted to
1093
Mandarin. Hidden Markov models are simple and
effective, but unlike discriminative models, such as
Maximum Entropy models (Ratnaparkhi, 1996) and
Conditional Random Fields (John Lafferty, 2001),
they have more difficulty utilizing a rich set of con-
ditionally dependent features. This limitation can be
overcome by utilizing reranking approaches, which
are able to make use of the features extracted from
the tagging hypotheses produced by the HMM tag-
ger. Reranking also has advantages over MaxEnt
and CRF models. It is able to use any features
extracted from entire labeled sentences, including
those that cannot be incorporated into MaxEnt and
CRF models due to inference difficulties. In addi-
tion, reranking methods are able to utilize the infor-
mation provided by N-best lists. Finally, the decod-
ing phase of reranking is much simpler.
The rest of the paper is organized as follows. We
describe the HMM tagger in Section 2. We discuss
the modifications to better handle unknown words in
Mandarin and to enrich the word emission probabil-
ities through the combination of bi-directional esti-
mations. In Section 3, we first describe the reranking
algorithm and then propose two methods to improve
its performance. We also describe the features that
will be used for Mandarin POS reranking in Sec-
tion 3. Experimental results are given in Section 4.
Conclusions and future work appear in Section 5.
2 The HMM Model
2.1 Porting English Tagger to Mandarin
The HMM tagger used in this work is a second-
order HMM tagger initially developed for English
by Thede and Harper (1999). This state-of-the-art
second-order HMM tagger uses trigram transition
probability estimations, P (ti|ti?2ti?1), and trigram
emission probability estimations, P (wi|ti?1ti). Let
ti1 denote the tag sequence t1, ? ? ? , ti, and w
i
1 denote
the word sequencew1, ? ? ? , wi. The tagging problem
can be formally defined as finding the best tag se-
quence ?(wN1 ) for the word sequence w
N
1 of length
N as follows1:
?(wN1 ) = arg max
tN1
P (tN1 |w
N
1 ) = arg max
tN1
P (tN1 w
N
1 )
P (wN1 )
= arg max
tN1
P (tN1 w
N
1 ) (1)
= arg max
tN1
?
i
P (ti|t
i?1
1 w
i?1
1 )P (wi|t
i
1w
i?1
1 )
1We assume that symbols exist implicitly for boundary con-
ditions.
? arg max
tN1
?
i
P (ti|ti?2ti?1)P (wi|ti?1ti) (2)
The best tag sequence ?(wN1 ) can be determined ef-
ficiently using the Viterbi algorithm.
For estimating emission probabilities of unknown
words (i.e., words that do not appear in the train-
ing data) in English (and similarly for other inflected
languages), a weighted sum of P (ski |ti?1ti) (with
k up to four) was used as an approximation, where
ski is the suffix of length k of word wi (e.g., s
1
i is
the last character of word wi). The suffix informa-
tion and three binary features (i.e., whether the word
is capitalized, whether the word is hyphenated, and
whether the word contains numbers) are combined
to estimate the emission probabilities of unknown
words.
The interpolation weights for smoothing tran-
sition, emission, and suffix probabilities were
estimated using the log-based Thede smoothing
method (Thede and Harper, 1999) as follows:
PThede(n-gram)
= ?(n-gram)PML(n-gram) +
(1? ?(n-gram))PThede((n-1)-gram)
where:
PML(n-gram) = the ML estimation
?(n-gram) = f(n-gram count)
f(x) =
loga(x+ 1) + b
loga(x+ 1) + (b+ 1)
While porting the HMM-based English POS tag-
ger to Mandarin is fairly straightforward for words
seen in the training data, some thought is required to
handle unknown words due to the morphology dif-
ferences between the two languages. First, in Man-
darin, there is no capitalization and no hyphenation.
Second, although Chinese has morphology, it is not
the same as in English; words tend to contain far
fewer characters than inflected words in English, so
word endings will tend to be short, say one or two
characters long. Hence, in our baseline model (de-
noted HMM baseline), we simply utilize word end-
ings of up to two characters in length along with a
binary feature of whether the word contains num-
bers or not. In the next two subsections, we describe
two ways in which we enhance this simple HMM
baseline model.
1094
2.2 Improving the Mandarin Unknown Word
Model
Chinese words are quite different from English
words, and the word formation process for Chinese
words can be quite complex (Packard, 2000). In-
deed, the last characters in a Chinese word are, in
some cases, most informative of the POS type, while
for others, it is the characters at the beginning. Fur-
thermore, it is not uncommon for a character in the
middle of a word to provide some evidence for the
POS type of the word. Hence, we chose to employ
a rather simple but effective method to estimate the
emission probability, P (wi|ti?1, ti), of an unknown
word, wi. We use the geometric average2 of the
emission probability of the characters in the word,
i.e., P (ck|ti?1, ti) with ck being the k-th character
in the word. Since some of the characters in wi may
not have appeared in any word tagged as ti in that
context in the training data, only characters that are
observed in this context are used in the computation
of the geometric average, as shown below:
P (wi|ti?1, ti) = n
? ?
ck?wi,P (ck|ti?1,ti)6=0
P (ck|ti?1, ti) (3)
where
n = |{ck ? wi|P (ck|ti?1, ti) 6= 0}|
2.3 Bi-directional Word Probability Estimation
In Equation 2, the word emission probability
P (wi|ti?1ti) is a left-to-right prediction that de-
pends on the current tag ti associated with wi, as
well as its previous tag ti?1. Although the interac-
tion between wi and the next tag ti+1 is captured to
some extent when ti+1 is generated by the model,
this implicit interaction may not be as effective as
adding the information more directly to the model.
Hence, we chose to apply the constraint explicitly in
our HMM framework by replacing P (wi|ti?1ti) in
Equation 2 with P ?(wi|ti?1ti)P 1??(wi|titi+1) for
both known and unknown words, with ?(wN1 ) deter-
mined by:
?(wN1 ) = arg max
tN1
?
i
(P (ti|ti?2ti?1)?
P?(wi|ti?1ti)P
1??(wi|titi+1)) (4)
2Based on preliminary testing, the geometric average pro-
vided greater tag accuracy than the arithmetic average.
This corresponds to a mixture model of two genera-
tion paths, one from the left and one from the right,
to approximate ?(wN1 ) in Equation 1 in a different
way.
?(wN1 ) = arg max
tN1
P (tN1 w
N
1 )
= arg max
tN1
P (tN1 )P (w
N
1 |t
N
1 )
P (tN1 ) ?
?
i
P (ti|ti?1ti?2)
P (wN1 |t
N
1 ) = P
?(wN1 |t
N
1 )P
1??(wN1 |t
N
1 )
?
?
i
P?(wi|ti?1ti)P
1??(wi|titi+1)
In this case, the decoding process involves the
computation of three local probabilities, i.e.,
P (ti|ti?2ti?1), P (wi|ti?1ti), and P (wi|titi+1).
By using a simple manipulation that shifts the
time index of P (wi|titi+1) in Equation 4 by two
time slices3 (i.e., by replacing P (wi|titi+1) with
P (wi?2|ti?2ti?1)), we are able to compute ?(wN1 )
in Equation 4 with the same asymptotic time com-
plexity of decoding as in Equation 2.
3 Discriminative Reranking
In this section, we describe our use of the
RankBoost-based (Freund and Schapire, 1997; Fre-
und et al, 1998) discriminative reranking approach
that was originally developed by Collins and Koo
(2005) for parsing. It provides an additional avenue
for improving tagging accuracy, and also allows us
to investigate the impact of various features on Man-
darin tagging performance. The reranking algorithm
takes as input a list of candidates produced by some
probabilistic model, in our case the HMM tagger,
and reranks these candidates based on a set of fea-
tures. We first introduce Collins? reranking algo-
rithm in Subsection 3.1, and then describe two mod-
ifications in Subsections 3.2 and 3.3 that were de-
signed to improve the generalization performance of
the reranking algorithm for our POS tagging task.
The reranking features that are used for POS tagging
are then described in Subsection 3.4.
3.1 Collins? Reranking Algorithm
For training the reranker for the POS tagging task,
there are n sentences {si : i = 1, ? ? ? , n} each with
ni candidates {xi,j : j = 1, ? ? ? , ni} along with
3Replacing P (wi|titi+1) with P (wi?1|ti?1ti) also gives
the same solution.
1095
the log-probability L(xi,j) produced by the HMM
tagger. Each tagging candidate xi,j in the training
data has a ?goodness? score Score(xi,j) that mea-
sures the similarity between the candidate and the
gold reference. For tagging, we use tag accuracy
as the similarity measure. Without loss of general-
ity, we assume that xi,1 has the highest score, i.e.,
Score(xi,1) ? Score(xi,j) for j = 2, ? ? ? , ni. To
summarize, the training data consists of a set of ex-
amples {xi,j : i = 1, ? ? ? , n; j = 1, ? ? ? , ni}, each
along with a ?goodness? score Score(xi,j) and a
log-probability L(xi,j).
A set of indicator functions {hk : k = 1, ? ? ? ,m}
are used to extract binary features {hk(xi,j) : k =
1, ? ? ? ,m} on each example xi,j . An example of an
indicator function for POS tagging is given below:
h2143(x) = 1 ifx contains n-gram ?go/VV to?
0 otherwise
Each indicator function hk is associated with a
weight parameter ?k which is real valued. In ad-
dition, a weight parameter ?0 is associated with
the log-probability L(xi,j). The ranking func-
tion of candidate xi,j is defined as ?0L(xi,j) +
m?
k=1
?khk(xi,j).
The objective of the training process is to set the
parameters ?? = {?0, ?1, ? ? ? , ?m} to minimize the
following loss function Loss(??) (which is an upper
bound on the training error):
Loss(??) =
?
i
ni?
j=2
Si,je
?Mi,j(??)
where Si,j is the weight function that gives the im-
portance of each example, and Mi,j(??) is the mar-
gin:
Si,j = Score(xi,1)? Score(xi,j)
Mi,j(??) = ?0(L(xi,1)? L(xi,j)) +
m?
k=1
?k(hk(xi,1)? hk(xi,j))
All of the ?i?s are initially set to zero. The value
of ?0 is determined first to minimize the loss func-
tion and is kept fixed afterwards. Then a greedy se-
quential 4 optimization method is used in each itera-
tion (i.e., a boosting round) to select the feature that
4Parallel optimization algorithms exist and have comparable
performance according to (Collins et al, 2002).
has the most impact on reducing the loss function
and then update its weight parameter accordingly.
For each k ? {1, ? ? ? ,m}, (hk(xi,1)? hk(xi,j)) can
only take one of the three values: +1, -1, or 0. Thus
the training examples can be divided into three sub-
sets with respect to k:
A+k = {(i, j) : (hk(xi,1)? hk(xi,j)) = +1}
A?k = {(i, j) : (hk(xi,1)? hk(xi,j)) = ?1}
A0k = {(i, j) : (hk(xi,1)? hk(xi,j)) = 0}
The new loss after adding the update parameter ?
to the parameter ?k is shown below:
Loss(??, k, ?) =
?
(i,j)?A+
k
Si,je
?Mi,j(??)?? +
?
(i,j)?A?
k
Si,je
?Mi,j(??)+? +
?
(i,j)?A0
k
Si,je
?Mi,j(??)
= e??W+k + e
?W?k +W
0
k
The best feature/update pair (k?, ??) that minimizes
Loss(??, k, ?) is determined using the following for-
mulas:
k? = arg max
k
?
?
?
?
?
W+k ?
?
W?k
?
?
?
? (5)
?? =
1
2
log
W+k?
W?k?
(6)
The update formula in Equation 6 is problematic
when either W+k? or W
?
k? is zero. W
+
k is zero if hk
never takes on a value 1 for any xi,1 with value 0 on
a corresponding xi,j for j = 2, ? ? ? , ni (and similarly
for W?k ). Collins introduced a smoothing parameter
 to address this problem, resulting in a slight modi-
fication to the update formula:
?? =
1
2
log
W+k? + Z
W?k? + Z
(7)
The value of  plays an important role in this for-
mula. If  is set too small, the smoothing factor Z
would not prevent setting ?? to a potentially overly
large absolute value, resulting in over-fitting. If  is
set too large, then the opposite condition of under-
training could result. The value of  is determined
based on a development set.
1096
3.2 Update Once
Collins? method allows multiple updates to the
weight of a feature based on Equations 5 and 7. We
found that for those features for which either W+k or
W?k equals zero, the update formula in Equation 7
can only increase their weight (in absolute value) in
one direction. Although these features are strong
and useful, setting their weights too large can be un-
desirable in that it limits the use of other features for
reducing the loss.
Based on this analysis, we have developed and
evaluated an update-once method, in which we use
the update formula in Equation 7 but limit weight
updates so that once a feature is selected on a cer-
tain iteration and its weight parameter is updated,
it cannot be updated again. Using this method, the
weights of the strong features are not allowed to pre-
vent additional features from being considered dur-
ing the training phase.
3.3 Regularized Reranking
Although the update-once method may attenuate
over-fitting to some extent, it also prevents adjust-
ing the value of any weight parameter that is initially
set too high or too low in an earlier boosting round.
In order to design a more sophisticated weight up-
date method that allows multiple updates in both di-
rections while penalizing overly large weights, we
have also investigated the addition of a regulariza-
tion term R(??), an exponential function of ??, to the
loss function:
RegLoss(??) =
?
i
ni?
j=2
Si,je
?Mi,j(??) +R(??)
R(??) =
m?
k=1
pk ? (e
??k + e?k ? 2)
where pk is the penalty weight of parameter ?k. The
reason that we chose this form of regularization is
that (e??k +e?k?2) is a symmetric, monotonically
decreasing function of |?k|, and more importantly it
provides a closed analytical expression of the weight
update formula similar to Equations 5 and 6. Hence,
the best feature/update pair for the regularized loss
function is defined as follows:
k? = arg max
k
?
?
?
?
?
W+k + pke
??k ?
?
W?k + pke
+?k
?
?
?
?
?? =
1
2
log
W+k? + pk?e
??k?
W?k? + pk?e
+?k?
There are many ways of choosing pk, the penalty
weight of ?k. In this paper, we use the values of
? ?(W+k +W
?
k ) at the beginning of the first iteration
(after ?0 is determined) for pk, where ? is a weight-
ing parameter to be tuned on the development set.
The regularized weight update formula has many ad-
vantages. It is always well defined no matter what
value W+k and W
?
k take, in contrast to Equation 6.
For all features, even in the case when either W+k or
W?k equals zero, the regularized update formula al-
lows weight updates in two directions. If the weight
is small, W+k and W
?
k have more impact on deter-
mining the weight update direction, however, when
the weight becomes large, the regularization factors
pke?? and pke+? favor reducing the weight.
3.4 Reranking Features
A reranking model has the flexibility of incorporat-
ing any type of feature extracted from N-best can-
didates. For the work presented in this paper, we
examine three types of features. For each window
of three word/tag pairs, we extract all the n-grams,
except those that are comprised of only one word/tag
pair, or only tags, or only words, or do not include
either the word or tag in the center word/tag pair.
These constitute the n-gram feature set.
In order to better handle unknown words, we also
extract the two most important types of morpho-
logical features5 that were utilized in (Tseng et al,
2005) for those words that appear no more than
seven times (following their convention) in the train-
ing set:
Affixation features: we use character n-gram pre-
fixes and suffixes for n up to 4. For example,
for word/tag pair D??/NN (Information-
Bag, i.e., folder), we add the following fea-
tures: (prefix1, D, NN), (prefix2, D?, NN),
(prefix3, D??, NN), (suffix1, ?, NN), (suf-
fix2,??, NN), (suffix3,D??, NN).
AffixPOS features6: we used the training set to
build a prefix/POS and suffix/POS dictionary
associating possible tags with each prefix and
5Tseng at el. also used other morphological features that
require additional resources to which we do not have access.
6AffixPOS features are somewhat different from the CTB-
Morph features used in (Tseng et al, 2005), where a mor-
pheme/POS dictionary with the possible tags for all morphemes
in the training set was used instead of two separate dictionaries
for prefix and suffix. AffixPOS features perform slightly better
in our task than the CTB-morph features.
1097
suffix in the training set. The AffixPOS fea-
tures indicate the set of tags a given affix could
have. For the same example D??/NN, D
occurred as prefix in both NN and VV words in
the training data. So we add the following fea-
tures based on the prefix D: (prefix, D, NN,
1, NN), (prefix, D, VV, 1, NN), and (prefix,
D, X, 0, NN) for every tag X not in {NN, VV},
where 1 and 0 are indicator values. Features are
extracted in the similar way for the suffix?.
The n-gram and morphological features are easy
to compute, however, they have difficulty in captur-
ing the long distance information related to syntac-
tic relationships that might help POS tagging ac-
curacy. In order to examine the effectiveness of
utilizing syntactic information in tagging, we have
also experimented with dependency features that are
extracted based on automatic parse trees. First a
bracketing parser (the Charniak parser (Charniak,
2000) in our case) is used to generate the parse
tree of a sentence, then the const2dep tool devel-
oped by Hwa was utilized to convert the bracket-
ing tree to a dependency tree based on the head
percolation table developed by the second author.
The dependency tree is comprised of a set of de-
pendency relations among word pairs. A depen-
dency relation is a triple ?word-a, word-b, relation?,
in which word-a is governed by word-b with gram-
matical relation denoted as relation. For example,
in the sentence ??(Tibet) ?N(economy) ?
?(construction) ??(achieves) >W(significant)
?(accomplishments)?, one example dependency
relation is ???, ?, mod?. Given these depen-
dency relations, we then extract dependency features
(in total 36 features for each relation) by examining
the POS tags of the words for each tagging candi-
date of a sentence. The relative positions of the word
pairs are also taken into account for some features.
For example, if?? and? in the above sentence
are tagged as VV and NN respectively in one can-
didate, then two example dependency features are
(dep-1, ??, VV, ?, NN, mod), (dep-14, ??,
VV, NN, right, mod), in which dep-1 and dep-14 are
feature types and right indicates that word-b (??)
is to the right of word-a (?).
4 Experiments
4.1 Data
The most recently released Penn Chinese Treebank
5.2 (denoted CTB, released by LDC) is used in our
experiments. It contains 500K words, 800K char-
acters, 18K sentences, and 900 data files, includ-
ing articles from the Xinhua news agency (China-
Mainland), Information Services Department of
HKSAR (Hongkong), and Sinorama magazine (Tai-
wan). Its format is similar to the English WSJ Penn
Treebank, and it was carefully annotated. There are
33 POS tags used, to which we add tags to discrim-
inate among punctuation types. The original POS
tag for punctuation was PU; we created new POS
tags for each distinct punctuation type (e.g., PU-?).
The CTB corpus was collected during different
time periods from different sources with a diversity
of articles. In order to obtain a representative split
of training, development, and test sets, we divide
the whole corpus into blocks of 10 files by sorted
order. For each block, the first file is used for de-
velopment, the second file is used for test, and the
remaining 8 files are used for training. Table 1 gives
the basic statistics on the data. The development
set is used to determine the parameter ? in Equa-
tion 4, the smoothing parameter  in Equation 7, the
weight parameter ? described in Section 3.3, and the
number of boosting rounds in the reranking model.
In order to train the reranking model, the method
in (Collins and Koo, 2005) is used to prepare the
N-best training examples. We divided the training
set into 20 chunks, with each chunk N-best tagged
by the HMM model trained on the combination of
the other 19 chunks. The development set is N-best
tagged by the HMM model trained on the training
set, and the test set is N-best tagged by the HMM
model trained on the combination of the training set
and the development set.
Train Dev Test
#Sentences 14925 1904 1975
#Words 404844 51243 52900
Table 1: The basic statistics on the data.
In the following subsections, we will first exam-
ine the HMM models alone to determine the best
HMM configuration to use to generate the N-best
candidates, and then evaluate the reranking mod-
els. Finally, we compare our performance with pre-
vious work. In this paper, we use the sign test
with p ? 0.01 to evaluate the statistical significance
of the difference between the performances of two
models.
1098
4.2 Results of the HMM taggers
The baseline HMM model ported directly from the
English tagger, as described in Subsection 2.1, has
an overall tag accuracy of 93.12% on the test set,
which is fairly low compared to the 97% accuracy
of many state-of-the-art taggers on WSJ for English.
By approximating the unknown word emission
probability using the characters in the word as in
Equation 3, the performance of the HMM tagger im-
proves significantly to 93.43%, suggesting that char-
acters in different positions of a Chinese word help
to disambiguate the word class of the entire word, in
contrast to English for which suffixes are most help-
ful.
Figure 1 depicts the impact of combining the left-
to-right and right-to-left word emission models us-
ing different weighting values (i.e., ?) on the devel-
opment set. Note that emission probabilities of un-
known words are estimated based on characters us-
ing the same ? for combination. When ? = 1.0, the
model uses only the standard left-to-right prediction
of words, while when ? = 0 it uses only the right-to-
left estimation. It is interesting to note that the right-
to-left estimation results in greater accuracy than the
left-to-right estimation. This might be because there
is stronger interaction between a word and its next
tag. Also as shown in Figure 1, the estimations in
the two directions are complementary to each other,
with ? = 0.5 performing best. The performance of
the HMM taggers on the test set is given in Table 2
for the best operating point, as well as the two other
extreme operating points to compare the left-to-right
and right-to-left constraints. Our best HMM tagger
further improves the tag accuracy significantly from
93.43% (? = 1.0) to 94.01% (? = 0.5).
Figure 1: The accuracy of the HMM tagger on the
development set with various ? values for combin-
ing the word emission probabilities.
Overall Known Unknown
HMM baseline 93.12% 94.65% 69.08%
HMM, ?=1.0 93.43% 94.71% 73.41%
HMM, ?=0.0 93.65% 94.88% 74.23%
HMM, ?=0.5 94.01% 95.21% 75.15%
Table 2: The performance of various HMM taggers
on the test set.
4.3 Results of the Reranking Models
The HMM tagger with the best accuracy (i.e., the
one with ? = 0.5 in Table 2) is used to generate
the N-Best tagging candidates, with a maximum of
100 candidates. As shown in Table 3, a maximum of
100-Best provides a reasonable margin for improve-
ment in the reranking task.
We first test the performance of the reranking
methods using only the n-gram feature set, which
contains around 18 million features. Later, we
will investigate the addition of morphological fea-
tures and dependency features. The smoothing
parameter  (for Collins? method and the update-
once method) and the weight parameter ? (for
the regularization method) both have great im-
pact on reranking performance. We trained vari-
ous reranking models with  values of 0.0001 ?
{1, 2.5, 5, 7.5, 10, 25, 50, 75, 100}, and ? values of
{0.1, 0.25, 0.5, 0.75, 1}. For all these parameter val-
ues, 600,000 rounds of iterations were executed on
the training set. The development set was used to
determine the early stopping point in training. If
not mentioned explicitly, all the results reported are
based on the best parameters tuned on the develop-
ment set.
1-Best 50-Best 100-Best
train 93.48% 96.96% 97.13%
dev 93.75% 97.68% 97.84%
test 93.19% 97.19% 97.35%
Table 3: The oracle tag accuracies of the 1-Best, 50-
Best, and 100-Best candidates in the training, devel-
opment, and test sets for the reranking experiments.
Note that the tagging candidates are prepared using
the method described in Subsection 4.1.
Table 4 reports the performance of the best HMM
tagger and the three reranking taggers on the test set.
All three reranking methods improve the HMM tag-
ger significantly. Also, the update-once and regu-
larization methods both outperform Collins? original
training method significantly.
1099
Overall Known Unknown
HMM, ?=0.5 94.01% 95.21% 75.15%
Collins 94.38% 95.56% 75.85%
Update-once 94.50% 95.67% 76.13%
Regularized 94.54% 95.70% 76.48%
Table 4: The performance on the test set of the
HMM tagger, and the reranking methods using the
n-gram features.
We observed that no matter which value the
smoothing parameter  takes, there are only about
10,000 non-zero features finally selected by Collins?
original method. In contrast, the two new methods
select substantially more features, as shown in Ta-
ble 5. As mentioned before, there are some strong
features that only appear in positive or negative sam-
ples, i.e., either W+k or W
?
k equals zero. Although
introducing the smoothing parameter  in Equation 7
prevents infinite weight values, the update to the
feature weights is no longer optimal (in terms of
minimizing the error function). Since the update
is not optimal, subsequent iterations may still fo-
cus on these features (and thus ignore other weaker
but informative features) and always increase their
weights in one direction, leading to biased training.
The update-once method at each iteration selects
a new feature that has the most impact in reduc-
ing the training loss function. It has the advantage
of preventing increasingly large weights from being
assigned to the strong features, enabling the update
of other features. The regularization method allows
multiple updates and also penalizes large weights.
Once a feature is selected and has its weight updated,
no matter how strong the feature is, the weight value
is optimal in terms of the current weights of other
features, so that the training algorithm would choose
another feature to update. A previously selected fea-
ture may be selected again if it becomes suboptimal
due to a change in the weights of other features.
#iterations #features percent
Collins 115400 10020 8.68%
Update-once 545100 545100 100%
Regularized 92500 70131 75.82%
Table 5: The number of iterations (for the best
performance), the number of selected features, and
the percentage of selected features, by Collins?
method, the update-once method, and the regular-
ization method on the development set.
Overall Known Unknown
HMM, ?=0.5 94.01% 95.21% 75.15%
Collins 94.44% 95.55% 77.05%
Update-once 94.68% 95.68% 78.91%
Regularized 94.64% 95.71% 77.84%
Table 6: The performance on the test set of the
HMM tagger and the reranking methods using n-
gram and morphological features.
We next add morphological features to the n-gram
features selected by the reranking methods7. As
can be seen by comparing Table 6 to Table 4, mor-
phological features improve the tagging accuracy of
unknown words. It should be noted that the im-
provement made by both update-one and regulariza-
tion methods is statistically significant over using n-
gram features alone; however, the improvement by
Collins? original method is not significant. This sug-
gests that the two new methods are able to utilize a
greater variety of features than the original method.
We trained several Charniak parsers using the
same method for the HMM taggers to generate auto-
matic parse trees for training, development, and test
data. The update-once method is used to evaluate
the effectiveness of dependency features for rerank-
ing, as shown in Table 7. The parser has an overall
tagging accuracy that is greater than that of the best
HMM tagger, but worse than that of the reranking
models using n-gram and morphological features. It
is interesting to note that reranking with the depen-
dency features alone improves the tagging accuracy
significantly, outperforming reranking models using
n-gram and morphological features. This suggests
that the long distance features based on the syntactic
structure of the sentence are very beneficial for POS
tagging of Mandarin. Moreover, n-gram and mor-
phological features are complementary to the depen-
dency features, with their combination performing
the best. The n-gram features improve the accuracy
on known words, while the morphological features
improve the accuracy on unknown words. The best
accuracy of 95.11% is an 18% relative reduction in
error compared to the best HMM tagger.
7Because the size of the combined feature set of all n-gram
features and morphological features is too large to be handled
by our server, we chose to add morphological features to the
n-gram features selected by the reranking methods, and then
retrain the reranking model.
1100
Overall Known Unknown
Parser 94.31% 95.57% 74.52%
dep 94.93% 96.01% 77.87%
dep+ngram 95.00% 96.11% 77.49%
dep+morph 94.98% 96.01% 78.79%
dep+ngram+morph 95.11% 96.12% 79.32%
Table 7: The tagging performance of the parser
and the update-once reranking models with depen-
dency features and their combination with n-gram
and morphological features.
4.4 Comparison to Previous Work
So how is our performance compared to previous
work? When working on the same training/test data
(CTB5.0 with the same pre-processing procedures)
as in (Tseng et al, 2005), our HMM model ob-
tained an accuracy of 93.72%, as compared to their
93.74% accuracy. Our reranking model8 using n-
gram and morphological features improves the ac-
curacy to 94.16%. Note that we did not use all the
morphological features as in (Tseng et al, 2005),
which would probably provide additional improve-
ment. The dependency features are expected to fur-
ther improve the performance, although they are not
included here in order to provide a relatively fair
comparison.
5 Conclusions and Future Work
We have shown that the characters in a word are
informative of the POS type of the entire word in
Mandarin, reflecting the fact that the individual Chi-
nese characters carry POS information to some de-
gree. The syntactic relationship among characters
may provide further information, which we leave
as future work. We have also shown that the ad-
ditional right-to-left estimation of word emission
probabilities is useful for HMM tagging of Man-
darin. This suggests that explicit modeling of bi-
directional interactions captures more sequential in-
formation. This could possibly help in other sequen-
tial modeling tasks.
We have also investigated using the reranking al-
gorithm in (Collins and Koo, 2005) for the Man-
darin POS tagging task, and found it quite effective
8Tseng at el.?s training/test split uses up the entire CTB cor-
pus, leaving no development data for tuning parameters. In
order to roughly measure reranking performance, we use the
update-once method to train the reranking model for 600,000
rounds with the other parameters tuned in Section 4. This sac-
rifices performance to some extent.
in improving tagging accuracy. The original algo-
rithm has a tendency to focus on a small subset of
strong features and ignore some of the other useful
features. We were able to improve the performance
of the reranking algorithm by utilizing two different
methods that make better use of more features. Both
are simple and yet effective. The effectiveness of de-
pendency features suggests that syntax-based long
distance features are important for improving part-
of-speech tagging performance in Mandarin. Al-
though parsing is computationally more demanding
than tagging, we hope to identify related features
that can be extracted more efficiently.
In future efforts, we plan to extract additional
reranking features utilizing more explicitly the char-
acteristics of Mandarin. We also plan to extend our
work to speech transcripts for Broadcast News and
Broadcast Conversation corpora, and explore semi-
supervised training methods for reranking.
Acknowledgments
This material is based upon work supported by
the Defense Advanced Research Projects Agency
(DARPA) under Contract No. HR0011-06-C-0023.
Any opinions, findings and conclusions or recom-
mendations expressed in this material are those of
the authors and do not necessarily reflect the views
of DARPA. We gratefully acknowledge the com-
ments from the anonymous reviewers.
References
Thorsten Brants. 2000. TnT a statistical part-of-speech
tagger. In ANLP, pages 224?231.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. In ACL.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of the first conference on North
American chapter of the Association for Computa-
tional Linguistics, pages 132?139, San Francisco, CA,
USA. Morgan Kaufmann Publishers Inc.
John Chen, Srinivas Bangalore, Michael Collins, and
Owen Rambow. 2002. Reranking an n-gram supertag-
ger. In the Sixth International Workshop on Tree Ad-
joining Grammars and Related Frameworks.
Michael Collins and Terry Koo. 2005. Discrimina-
tive reranking for natural language parsing. Compu-
tational Linguistics, 31(1):25?70.
1101
Michael Collins, Robert E. Schapire, and Yoram Singer.
2002. Logistic regression, adaboost and bregman dis-
tances. Machine Learning, 48(1):253?285.
Yoav Freund and Robert E. Schapire. 1997. A decision-
theoretic generalization of on-line learning and an ap-
plication to boosting. Journal of Computer and System
Sciences, 1(55):119?139.
Yoav Freund, Raj Iyer, Robert E. Schapire, and Yoram
Singer. 1998. An efficient boosting algorithm for
combining preferences. In the Fifteenth International
Conference on Machine Learning.
Heng Ji, Cynthia Rudin, and Ralph Grishman. 2006. Re-
ranking algorithms for name tagging. In HLT/NAACL
06 Workshop on Computationally Hard Problems and
Joint Inference in Speech and Language Processing.
Fernando Pereira John Lafferty, Andrew McCallum.
2001. Conditional random fields: Probabilistic models
for segmenting and labeling sequence data. In ICML.
Jerome Packard. 2000. The Morphology of Chinese.
Cambridge University Press.
Adwait Ratnaparkhi. 1996. A maximum entropy model
for part-of-speech tagging. In EMNLP.
Brian Roark, Yang Liu, Mary Harper, Robin Stewart,
Matthew Lease, Matthew Snover, Izhak Shafran, Bon-
nie Dorr, John Hale, Anna Krasnyanskaya, and Lisa
Yung. 2006. Reranking for sentence boundary detec-
tion in conversational speech. In ICASSP.
Scott M. Thede and Mary P. Harper. 1999. A second-
order hidden Markov model for part-of-speech tag-
ging. In ACL, pages 175?182.
Huihsin Tseng, Daniel Jurafsky, and Christopher Man-
ning. 2005. Morphological features help pos tagging
of unknown words across language varieties. In the
Fourth SIGHAN Workshop on Chinese Language Pro-
cessing.
Nianwen Xue, Fu dong Chiou, and Martha Palmer. 2002.
Building a large-scale annotated chinese corpus. In
COLING.
1102
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 12?22,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Self-training with Products of Latent Variable Grammars
Zhongqiang Huang?
?UMIACS
University of Maryland
College Park, MD
zqhuang@umd.edu
Mary Harper??
?HLT Center of Excellence
Johns Hopkins University
Baltimore, MD
mharper@umd.edu
Slav Petrov?
?Google Research
76 Ninth Avenue
New York, NY
slav@google.com
Abstract
We study self-training with products of latent
variable grammars in this paper. We show
that increasing the quality of the automatically
parsed data used for self-training gives higher
accuracy self-trained grammars. Our genera-
tive self-trained grammars reach F scores of
91.6 on the WSJ test set and surpass even
discriminative reranking systems without self-
training. Additionally, we show that multi-
ple self-trained grammars can be combined in
a product model to achieve even higher ac-
curacy. The product model is most effective
when the individual underlying grammars are
most diverse. Combining multiple grammars
that were self-trained on disjoint sets of un-
labeled data results in a final test accuracy of
92.5% on the WSJ test set and 89.6% on our
Broadcast News test set.
1 Introduction
The latent variable approach of Petrov et al (2006)
is capable of learning high accuracy context-free
grammars directly from a raw treebank. It starts
from a coarse treebank grammar (Charniak, 1997),
and uses latent variables to refine the context-free
assumptions encoded in the grammar. A hierarchi-
cal split-and-merge algorithm introduces grammar
complexity gradually, iteratively splitting (and po-
tentially merging back) each observed treebank cat-
egory into a number of increasingly refined latent
subcategories. The Expectation Maximization (EM)
algorithm is used to train the model, guaranteeing
that each EM iteration will increase the training like-
lihood. However, because the latent variable gram-
mars are not explicitly regularized, EM keeps fit-
ting the training data and eventually begins over-
fitting (Liang et al, 2007). Moreover, EM is a lo-
cal method, making no promises regarding the final
point of convergence when initialized from different
random seeds. Recently, Petrov (2010) showed that
substantial differences between the learned gram-
mars remain, even if the hierarchical splitting re-
duces the variance across independent runs of EM.
In order to counteract the overfitting behavior,
Petrov et al (2006) introduced a linear smoothing
procedure that allows training grammars for 6 split-
merge (SM) rounds without overfitting. The in-
creased expressiveness of the model, combined with
the more robust parameter estimates provided by the
smoothing, results in a nice increase in parsing ac-
curacy on a held-out set. However, as reported by
Petrov (2009) and Huang and Harper (2009), an ad-
ditional 7th SM round actually hurts performance.
Huang and Harper (2009) addressed the issue of
data sparsity and overfitting from a different angle.
They showed that self-training latent variable gram-
mars on their own output can mitigate data spar-
sity issues and improve parsing accuracy. Because
the capacity of the model can grow with the size
of the training data, latent variable grammars are
able to benefit from the additional training data, even
though it is not perfectly labeled. Consequently,
they also found that a 7th round of SM training was
beneficial in the presence of large amounts of train-
ing data. However, variation still remains in their
self-trained grammars and they had to use a held-out
set for model selection.
The observation of variation is not surprising;
EM?s tendency to get stuck in local maxima has been
studied extensively in the literature, resulting in vari-
ous proposals for model selection methods (e.g., see
12
Burnham and Anderson (2002)). What is perhaps
more surprising is that the different latent variable
grammars seem to capture complementary aspects
of the data. Petrov (2010) showed that a simple ran-
domization scheme produces widely varying gram-
mars. Quite serendipitously, these grammars can
be combined into an unweighted product model that
substantially outperforms the individual grammars.
In this paper, we combine the ideas of self-
training and product models and show that both
techniques provide complementary effects. We hy-
pothesize that the main factors contributing to the
final accuracy of the product model of self-trained
grammars are (i) the accuracy of the grammar used
to parse the unlabeled data for retraining (single
grammar versus product of grammars) and (ii) the
diversity of the grammars that are being combined
(self-trained grammars trained using the same auto-
matically labeled subset or different subsets). We
conduct a series of analyses to develop an under-
standing of these factors, and conclude that both di-
mensions are important for obtaining significant im-
provements over the standard product models.
2 Experimental Setup
2.1 Data
We conducted experiments in two genres: newswire
text and broadcast news transcripts. For the
newswire studies, we used the standard setup (sec-
tions 02-21 for training, 22 for development, and 23
for final test) of the WSJ Penn Treebank (Marcus et
al., 1999) for supervised training. The BLLIP cor-
pus (Charniak et al, 2000) was used as a source of
unlabeled data for self-training the WSJ grammars.
We ignored the parse trees contained in the BLLIP
corpus and retained only the sentences, which are
already segmented and tokenized for parsing (e.g.,
contractions are split into two tokens and punctua-
tion is separated from the words). We partitioned
the 1,769,055 BLLIP sentences into 10 equally sized
subsets1.
For broadcast news (BN), we utilized the Broad-
1We corrected some of the most egregious sentence segmen-
tation problems in this corpus, and so the number of sentences is
different than if one simply pulled the fringe of the trees. It was
not uncommon for a sentence split to occur on abbreviations,
such as Adm.
cast News treebank from Ontonotes (Weischedel et
al., 2008) together with the WSJ Penn Treebank for
supervised training, because their combination re-
sults in better parser models compared to using the
limited-sized BN corpus alone (86.7 F vs. 85.2 F).
The files in the Broadcast News treebank represent
news stories collected during different time periods
with a diversity of topics. In order to obtain a rep-
resentative split of train-test-development sets, we
divided them into blocks of 10 files sorted by alpha-
betical filename order. We used the first file in each
block for development, the second for test, and the
remaining files for training. This training set was
then combined with the entire WSJ treebank. We
also used 10 equally sized subsets from the Hub4
CSR 1996 utterances (Garofolo et al, 1996) for self-
training. The Hub 4 transcripts are markedly noisier
than the BLLIP corpus is, in part because it is harder
to sentence segment, but also because it was pro-
duced by human transcription of spoken language.
The treebanks were pre-processed differently for
the two genres. For newswire, we used a slightly
modified version of the WSJ treebank: empty
nodes and function labels were deleted and auxiliary
verbs were replaced with AUXB, AUXG, AUXZ,
AUXD, or AUXN to represent infinitive, progres-
sive, present, past, or past participle auxiliaries2.
The targeted use of the broadcast models is for pars-
ing broadcast news transcripts for language mod-
els in speech recognition systems. Therefore, in
addition to applying the transformations used for
newswire, we also replaced symbolic expressions
with verbal forms (e.g., $5 was replaced with five
dollars) and removed punctuation and case. The
Hub4 data was segmented into utterances, punctua-
tion was removed, words were down-cased, and con-
tractions were tokenized for parsing. Table 1 sum-
marizes the data set sizes used in our experiments,
together with average sentence length and standard
deviation.
2.2 Scoring
Parses from all models are compared with respective
gold standard parses using SParseval bracket scor-
ing (Roark et al, 2006). This scoring tool pro-
2Parsing accuracy is marginally affected. The average over
10 SM6 grammars with the transformation is 90.5 compared to
90.4 F without it, a 0.1% average improvement.
13
Genre Statistics Train Dev Test Unlabeled
Newswire
# sentences 39.8k 1.7k 2.4k 1,769.1k
# words 950.0k 40.1k 56.7k 43,057.0k
length Avg./Std. 28.9/11.2 25.1/11.8 25.1/12.0 24.3/10.9
Broadcast News
# sentences 59.0k 1.0k 1.1k 4,386.5k
# words 1,281.1k 17.1k 19.4k 77,687.9k
length Avg./Std. 17.3/11.3 17.4/11.3 17.7/11.4 17.7/12.8
Table 1: The number of words and sentences, together with average (Avg.) sentence length and its standard deviation
(Std.), for the data sets used in our experiments.
duces scores that are identical to those produced
by EVALB for WSJ. For Broadcast News, SParse-
val applies Charniak and Johnson?s (Charniak and
Johnson, 2001) scoring method for EDITED nodes3.
Using this method, BN scores were slightly (.05-.1)
lower than if EDITED constituents were treated like
any other, as in EVALB.
2.3 Latent Variable Grammars
We use the latent variable grammar (Matsuzaki et
al., 2005; Petrov et al, 2006) implementation of
Huang and Harper (2009) in this work. Latent vari-
able grammars augment the observed parse trees in
the treebank with a latent variable at each tree node.
This effectively splits each observed category into
a set of latent subcategories. An EM-algorithm is
used to fit the model by maximizing the joint like-
lihood of parse trees and sentences. To allocate the
grammar complexity only where needed, a simple
split-and-merge procedure is applied. In every split-
merge (SM) round, each latent variable is first split
in two and the model is re-estimated. A likelihood
criterion is used to merge back the least useful splits
(50% merge rate for these experiments). This itera-
tive refinement proceeds for 7 rounds, at which point
parsing performance on a held-out set levels off and
training becomes prohibitively slow.
Since EM is a local method, different initial-
izations will result in different grammars. In
fact, Petrov (2010) recently showed that this EM-
algorithm is very unstable and converges to widely
varying local maxima. These local maxima corre-
3Non-terminal subconstituents of EDITED nodes are re-
moved so that the terminal constituents become immediate chil-
dren of a single EDITED node, adjacent EDITED nodes are
merged, and they are ignored for span calculations of the other
constituents.
spond to different high quality latent variable gram-
mars that have captured different types of patterns in
the data. Because the individual models? mistakes
are independent to some extent, multiple grammars
can be effectively combined into an unweighted
product model of much higher accuracy. We build
upon this line of work and investigate methods to
exploit products of latent variable grammars in the
context of self-training.
3 Self-training Methodology
Different types of parser self-training have been pro-
posed in the literature over the years. All of them
involve parsing a set of unlabeled sentences with a
baseline parser and then estimating a new parser by
combining this automatically parsed data with the
original training data. McClosky et al (2006) pre-
sented a very effective method for self-training a
two-stage parsing system consisting of a first-stage
generative lexicalized parser and a second-stage dis-
criminative reranker. In their approach, a large
amount of unlabeled text is parsed by the two-stage
system and the parameters of the first-stage lexical-
ized parser are then re-estimated taking the counts
from the automatically parsed data into considera-
tion.
More recently Huang and Harper (2009) pre-
sented a self-training procedure based on an EM-
algorithm. They showed that the EM-algorithm that
is typically used to fit a latent variable grammar
(Matsuzaki et al, 2005; Petrov et al, 2006) to a tree-
bank can also be used for self-training on automati-
cally parsed sentences. In this paper, we investigate
self-training with products of latent variable gram-
mars. We consider three training scenarios:
ST-Reg Training Use the best single grammar to
14
Regular Best Average Product
SM6 90.8 90.5 92.0
SM7 90.4 90.1 92.2
Table 2: Performance of the regular grammars and their
products on the WSJ development set.
parse a single subset of the unlabeled data and
train 10 self-trained grammars using this single
set.
ST-Prod Training Use the product model to parse
a single subset of the unlabeled data and train
10 self-trained grammars using this single set.
ST-Prod-Mult Training Use the product model to
parse all 10 subsets of the unlabeled data and
train 10 self-trained grammars, each using a
different subset.
The resulting grammars can be either used individu-
ally or combined in a product model.
These three conditions provide different insights.
The first experiment allows us to investigate the
effectiveness of product models for standard self-
trained grammars. The second experiment enables
us to quantify how important the accuracy of the
baseline parser is for self-training. Finally, the third
experiment investigates a method for injecting some
additional diversity into the individual grammars to
determine whether a product model is most success-
ful when there is more variance among the individ-
ual models.
Our initial experiments and analysis will focus on
the development set of WSJ. We will then follow
up with an analysis of broadcast news (BN) to de-
termine whether the findings generalize to a second,
less structured type of data. It is important to con-
struct grammars capable of parsing this type of data
accurately and consistently in order to support struc-
tured language modeling (e.g., (Wang and Harper,
2002; Filimonov and Harper, 2009)).
4 Newswire Experiments
In this section, we compare single grammars and
their products that are trained in the standard way
with gold WSJ training data, as well as the three
self-training scenarios discussed in Section 3. We
ST-Reg Best Average Product
SM6 91.5 91.2 92.0
SM7 91.6 91.5 92.4
Table 3: Performance of the ST-Reg grammars and their
products on the WSJ development set.
report the F scores of both SM6 and SM7 grammars
on the development set in order to evaluate the ef-
fect of model complexity on the performance of the
self-trained and product models. Note that we use
6th round grammars to produce the automatic parse
trees for the self-training experiments. Parsing with
the product of the 7th round grammars is slow and
requires a large amount of memory (32GB). Since
we had limited access to such machines, it was in-
feasible for us to parse all of the unlabeled data with
the SM7 product grammars.
4.1 Regular Training
We begin by training ten latent variable models ini-
tialized with different random seeds using the gold
WSJ training set. Results are presented in Table 2.
The best F score attained by the individual SM6
grammars on the development set is 90.8, with an
average score of 90.5. The product of grammars
achieves a significantly improved accuracy at 92.04.
Notice that the individual SM7 grammars perform
worse on average (90.1 vs. 90.5) due to overfitting,
but their product achieves higher accuracy than the
product of the SM6 grammars (92.2 vs. 92.0). We
will further investigate the causes for this effect in
Section 5.
4.2 ST-Reg Training
Given the ten SM6 grammars from the previous sub-
section, we can investigate the three self-training
methods. In the first regime (ST-Reg), we use the
best single grammar (90.8 F) to parse a single subset
of the BLLIP data. We then train ten grammars from
different random seeds, using an equally weighted
combination of the WSJ training set with this sin-
gle set. These self-trained grammars are then com-
bined into a product model. As reported in Table 3,
4We use Dan Bikel?s randomized parsing evaluation com-
parator to determine the significance (p < 0.05) of the differ-
ence between two parsers? outputs.
15
ST-Prod Best Average Product
SM6 91.7 91.4 92.2
SM7 91.9 91.7 92.4
Table 4: Performance of the ST-Prod grammars and their
products on the WSJ development set.
thanks to the use of additional automatically labeled
training data, the individual SM6 ST-Reg grammars
perform significantly better than the individual SM6
grammars (91.2 vs. 90.5 on average), and the indi-
vidual SM7 ST-Reg grammars perform even better,
achieving a high F score of 91.5 on average.
The product of ST-Reg grammars achieves signif-
icantly better performance over the individual gram-
mars, however, the improvement is much smaller
than that obtained by the product of regular gram-
mars. In fact, the product of ST-Reg grammars per-
forms quite similarly to the product of regular gram-
mars despite the higher average accuracy of the in-
dividual grammars. This may be caused by the fact
that self-training on the same data tends to reduce
the variation among the self-trained grammars. We
will show in Section 5 that the diversity among the
individual grammars is as important as average ac-
curacy for the performance attained by the product
model.
4.3 ST-Prod Training
Since products of latent variable grammars perform
significantly better than individual latent variable
grammars, it is natural to try using the product
model for parsing the unlabeled data. To investi-
gate whether the higher accuracy of the automati-
cally labeled data translates into a higher accuracy
of the self-trained grammars, we used the product of
6th round grammars to parse the same subset of the
unlabeled data as in the previous experiment. We
then trained ten self-trained grammars, which we
call ST-Prod grammars. As can be seen in Table 4,
using the product of the regular grammars for label-
ing the self-training data results in improved individ-
ual ST-Prod grammars when compared with the ST-
Reg grammars, with 0.2 and 0.3 improvements for
the best SM6 and SM7 grammars, respectively. In-
terestingly, the best individual SM7 ST-Prod gram-
mar (91.9 F) performs comparably to the product of
ST-Prod-Mult Best Average Product
SM6 91.7 91.4 92.5
SM7 91.8 91.7 92.8
Table 5: Performance of the ST-Prod-Mult grammars and
their products on the WSJ development set.
the regular grammars (92.0 F) that was used to label
the BLLIP subset used for self-training. This is very
useful for practical reasons because a single gram-
mar is faster to parse with and requires less memory
than the product model.
The product of the SM6 ST-Prod grammars also
achieves a 0.2 higher F score compared to the prod-
uct of the SM6 ST-Reg grammars, but the product
of the SM7 ST-Prod grammars has the same perfor-
mance as the product of the SM7 ST-Reg grammars.
This could be due to the fact that the ST-Prod gram-
mars are no more diverse than the ST-Reg grammars,
as we will show in Section 5.
4.4 ST-Prod-Mult Training
When creating a product model of regular gram-
mars, Petrov (2010) used a different random seed for
each model and conjectured that the effectiveness of
the product grammars stems from the resulting di-
versity of the individual grammars. Two ways to
systematically introduce bias into individual mod-
els are to either modify the feature sets (Baldridge
and Osborne, 2008; Smith and Osborne, 2007) or
to change the training distributions of the individual
models (Breiman, 1996). Petrov (2010) attempted to
use the second method to train individual grammars
on either disjoint or overlapping subsets of the tree-
bank, but observed a performance drop in individ-
ual grammars resulting from training on less data,
as well as in the performance of the product model.
Rather than reducing the amount of gold training
data (or having treebank experts annotate more data
to support the diversity), we employ the self-training
paradigm to train models using a combination of the
same gold training data with different sets of the
self-labeled training data. This approach also allows
us to utilize a much larger amount of low-cost self-
labeled data than can be used to train one model by
partitioning the data into ten subsets and then train-
ing ten models with a different subset. Hence, in
16
-3
-2
-1
0
1
2
3
Total VP QP NP SBAR PP ADVP_PRT S WHNP ADJP
D
i
f
f
e
r
e
n
c
e
 
i
n
 
F
G0
G1
G2
G3
G4
G5
G6
G7
G8
G9
(a) Difference in F score between the product and the individual SM6 regular grammars.
-3
-2
-1
0
1
2
3
Total VP QP NP SBAR PP ADVP_PRT S WHNP ADJP
D
i
f
f
e
r
e
n
c
e
 
i
n
 
F
G0
G1
G2
G3
G4
G5
G6
G7
G8
G9
(b) Difference in F score between the product of SM6 regular grammars and the individual SM7 ST-Prod-Mult
grammars.
Figure 1: Difference in F scores between various individual grammars and representative product grammars.
the third self-training experiment, we use the prod-
uct of the regular grammars to parse all ten subsets
of the unlabeled data and train ten grammars, which
we call ST-Prod-Mult grammars, each using a dif-
ferent subset.
As shown in Table 5, the individual ST-Prod-Mult
grammars perform similarly to the individual ST-
Prod grammars. However, the product of the ST-
Prod-Mult grammars achieves significantly higher
accuracies than the product of the ST-Prod gram-
mars, with 0.3 and 0.4 improvements for SM6 and
SM7 grammars, respectively, suggesting that the use
of multiple self-training subsets plays an important
role in model combination.
5 Analysis
We conducted a series of analyses to develop an un-
derstanding of the factors affecting the effectiveness
of combining self-training with product models.
5.1 What Has Improved?
Figure 1(a) depicts the difference between the prod-
uct and the individual SM6 regular grammars on
overall F score, as well as individual constituent F
scores. As can be observed, there are significant
variations among the individual grammars, and the
product of the regular grammars improves almost all
categories, with a few exceptions (some individual
grammars do better on QP and WHNP constituents).
Figure 1(b) shows the difference between the
product of the SM6 regular grammars and the indi-
vidual SM7 ST-Prod-Mult grammars. Self-training
dramatically improves the quality of single gram-
mars. In most of the categories, some individ-
ual ST-Prod-Mult grammars perform comparably or
slightly better than the product of SM6 regular gram-
mars used to automatically label the unlabeled train-
ing set.
5.2 Overfitting vs. Smoothing
Figure 2(a) and 2(b) depict the learning curves of
the regular and the ST-Prod-Mult grammars. As
more latent variables are introduced through the iter-
ative SM training algorithm, the modeling capacity
of the grammars increases, leading to improved per-
formance. However, the performance of the regular
grammars drops after 6 SM rounds, as also previ-
ously observed in (Huang and Harper, 2009; Petrov,
2009), suggesting that the regular SM7 grammars
have overfit the relatively small-sized gold training
17
data. In contrast, the performance of the self-trained
grammars continues to improve in the 7th SM round.
Huang and Harper (2009) argued that the additional
self-labeled training data adds a smoothing effect to
the grammars, supporting an increase in model com-
plexity without overfitting.
Although the performance of the individual gram-
mars, both regular and self-trained, varies signif-
icantly and the product model consistently helps,
there is a non-negligible difference between the im-
provement achieved by the two product models over
their component grammars. The regular product
model improves upon its individual grammars more
than the ST-Prod-Mult product does in the later SM
rounds, as illustrated by the relative error reduction
curves in figures 2(a) and (b). In particular, the prod-
uct of the SM7 regular grammars gains a remarkable
2.1% absolute improvement over the average perfor-
mance of the individual regular SM7 grammars and
0.2% absolute over the product of the regular SM6
grammars, despite the fact that the individual regular
SM7 grammars perform worse than the SM6 gram-
mars. This suggests that the product model is able
to effectively exploit less smooth, overfit grammars.
We will examine this issue further in the next sub-
section.
5.3 Diversity
From the perspective of Products of Experts (Hin-
ton, 1999) or Logarithmic Opinion Pools (Smith et
al., 2005), each individual expert learns complemen-
tary aspects of the training data and the veto power
of product models enforces that the joint prediction
of their product has to be licensed by all individual
experts. One possible explanation of the observa-
tion in the previous subsection is that with the ad-
dition of more latent variables, the individual gram-
mars become more deeply specialized on certain as-
pects of the training data. This specialization leads
to greater diversity in their prediction preferences,
especially in the presence of a small training set.
On the other hand, the self-labeled training set size
is much larger, and so the specialization process is
therefore slowed down.
Petrov (2010) showed that the individually
learned grammars are indeed very diverse by look-
ing at the distribution of latent annotations across the
treebank categories, as well as the variation in over-
all and individual category F scores (see Figure 1).
However, these measures do not directly relate to the
diversity of the prediction preferences of the gram-
mars, as we observed similar patterns in the regular
and self-trained models.
Given a sentence s and a set of grammars G =
{G1, ? ? ? , Gn}, recall that the decoding algorithm of
the product model (Petrov, 2010) searches for the
best tree T such that the following objective function
is maximized:
?
r?T
?
G?G
log p(r|s,G)
where log p(r|s,G) is the log posterior probability
of rule r given sentence s and grammar G. The
power of the product model comes directly from the
diversity in log p(r|s,G) among individual gram-
mars. If there is little diversity, the individual
grammars would make similar predictions and there
would be little or no benefit from using a product
model. We use the average empirical variance of
the log posterior probabilities of the rules among the
learned grammars over a held-out set S as a proxy
of the diversity among the grammars:
?
s?S
?
G?G
?
r?R(G,s)
p(r|s,G)VAR(log(p(r|s,G)))
?
s?S
?
G?G
?
r?R(G,s)
p(r|s,G)
where R(G, s) represents the set of rules extracted
from the chart when parsing sentence s with gram-
mar G, and VAR(log(p(r|s,G))) is the variance of
log(p(r|s,G)) among all grammars G ? G.
Note that the average empirical variance is only
an approximation of the diversity among grammars.
In particular, this measure tends to be biased to pro-
duce larger numbers when the posterior probabili-
ties of rules tend to be small, because small differ-
ences in probability produce large changes in the log
scale. This happens for coarser grammars produced
in early SM stages when there is more uncertainty
about what rules to apply, with the rules remaining
in the parsing chart having low probabilities overall.
As shown in Figure 2(c), the average variances
all start at a high value and then drop, probably due
to the aforementioned bias. However, as the SM
iteration continues, the average variances increase
despite the bias. More interestingly, the variance
18
83
85
87
89
91
93
2 3 4 5 6 7
5%
9%
13%
17%
21%
25%
Regular Grammars
F
(a) SM Rounds
R
e
l
a
t
i
v
e
 
E
r
r
o
r
 
R
e
d
u
c
t
i
o
n
83
85
87
89
91
93
2 3 4 5 6 7
5%
9%
13%
17%
21%
25%
ST-Prod-Mult Grammars
F
(b) SM Rounds
R
e
l
a
t
i
v
e
 
E
r
r
o
r
 
R
e
d
u
c
t
i
o
n
Product Mean Error Reduction
0.1
0.2
0.3
0.4
0.5
2 3 4 5 6 7
Test
A
v
e
r
a
g
e
 
V
a
r
i
a
n
c
e
(c) SM Rounds
Regular
ST-Prod-Mult
ST-Prod
ST-Reg
Figure 2: Learning curves of the individual regular (a) and ST-Prod-Mult (b) grammars (average performance, with
minimum and maximum values indicated by bars) and their products before and after self-training on the WSJ de-
velopment set. The relative error reductions of the products are also reported. (c) The measured average empirical
variance among the grammars trained on WSJ.
among the regular grammars grows at a much faster
speed and is consistently greater when compared to
the self-trained grammars. This suggests that there
is more diversity among the regular grammars than
among the self-trained grammars, and explains the
greater improvement obtained by the regular product
model. It is also important to note that there is more
variance among the ST-Prod-Mult grammars, which
were trained on disjoint self-labeled training data,
and a greater improvement in their product model
relative to the ST-Reg and ST-Prod grammars, fur-
ther supporting the diversity hypothesis. Last but not
the least, the trend seems to indicate that the vari-
ance of the self-trained grammars would continue
increasing if EM training was extended by a few
more SM rounds, potentially resulting in even bet-
ter product models. It is currently impractical to test
this due to the dramatic increase in computational
requirements for an SM8 product model, and so we
leave it for future work.
5.4 Generalization to Broadcast News
We conducted the same set of experiments on the
broadcast news data set. While the development set
results in Table 6 show similar trends to the WSJ
results, the benefits from the combination of self-
training and product models appear even more pro-
nounced here. The best single ST-Prod-Mult gram-
mar (89.2 F) alone is able to outperform the product
of SM7 regular grammars (88.9 F), and their prod-
uct achieves another 0.7 absolute improvement, re-
sulting in a significantly better accuracy at 89.9 F.
Model Rounds Best Product
Regular
SM6 87.1 88.6
SM7 87.1 88.9
ST-Prod
SM6 88.5 89.0
SM7 89.0 89.6
ST-Prod-Mult
SM6 88.8 89.5
SM7 89.2 89.9
Table 6: F-score for various models on the BN develop-
ment set.
Figure 3 shows again that the benefits of self-
training and product models are complementary and
can be stacked. As can be observed, the self-
trained grammars have increasing F scores as the
split-merge rounds increase, while the regular gram-
mars have a slight decrease in F score after round 6.
In contrast to the newswire models, it appears that
the individual ST-Prod-Mult grammars trained on
broadcast news always perform comparably to the
product of the regular grammars at all SM rounds,
including the product of SM7 regular grammars.
This is noteworthy, given that the ST-Prod-Mult
grammars are trained on the output of the worse per-
forming product of the SM6 regular grammars. One
19
79
81
83
85
87
89
91
2 3 4 5 6 7
3%
6%
9%
12%
15%
Regular Gramamrs
F
(a) SM Rounds
R
e
l
a
t
i
v
e
 
E
r
r
o
r
 
R
e
d
u
c
t
i
o
n
79
81
83
85
87
89
91
2 3 4 5 6 7
3%
6%
9%
12%
15%
ST-Prod-Mult Gramamrs
F
(b) SM Rounds
R
e
l
a
t
i
v
e
 
E
r
r
o
r
 
R
e
d
u
c
t
i
o
n
Product Mean Error Reduction
0.1
0.2
0.3
0.4
0.5
2 3 4 5 6 7
Test
A
v
e
r
a
g
e
 
V
a
r
i
a
n
c
e
(c) SM Rounds
Regular
ST-Prod-Mult
ST-Prod
Figure 3: Learning curves of the individual regular (a) and ST-Prod-Mult (b) grammars (average performance, with
minimum and maximum values indicated by bars) and their products before and after self-training on the BN develop-
ment set. The relative error reductions of the products are also reported. (c) The measured average empirical variance
among the grammars trained on BN.
possible explanation is that we used more unlabeled
data for self-training the broadcast news grammars
than for the newswire grammars. The product of the
ST-Prod-Mult grammars provides further and signif-
icant improvement in F score.
6 Final Results
We evaluated the best single self-trained gram-
mar (SM7 ST-Prod), as well as the product of
the SM7 ST-Prod-Mult grammars on the WSJ test
set. Table 7 compares these two grammars to
a large body of related work grouped into sin-
gle parsers (SINGLE), discriminative reranking ap-
proaches (RE), self-training (SELF), and system
combinations (COMBO).
Our best single grammar achieves an accuracy
that is only slightly worse (91.6 vs. 91.8 in F score)
than the product model in Petrov (2010). This is
made possible by self-training on the output of a
high quality product model. The higher quality of
the automatically parsed data results in a 0.3 point
higher final F score (91.6 vs. 91.3) over the self-
training results in Huang and Harper (2009), which
used a single grammar for parsing the unlabeled
data. The product of the self-trained ST-Prod-Mult
grammars achieves significantly higher accuracies
with an F score of 92.5, a 0.7 improvement over the
product model in Petrov (2010).
8Our ST-Reg grammars are trained in the same way as in
Type Parser LP LR EX
S
IN
G
L
E Charniak (2000) 89.9 89.5 37.2
Petrov and Klein (2007) 90.2 90.1 36.7
Carreras et al (2008) 91.4 90.7 -
R
E Charniak and Johnson (2005) 91.8 91.2 44.8
Huang (2008) 92.2 91.2 43.5
S
E
L
F Huang and Harper (2009)8 91.6 91.1 40.4
McClosky et al (2006) 92.5 92.1 45.3
C
O
M
B
O Petrov (2010) 92.0 91.7 41.9
Sagae and Lavie (2006) 93.2 91.0 -
Fossum and Knight (2009) 93.2 91.7 -
Zhang et al (2009) 93.3 92.0 -
This Paper
Best Single 91.8 91.4 40.3
Best Product 92.7 92.2 43.1
Table 7: Final test set accuracies on WSJ.
Although our models are based on purely gen-
erative PCFG grammars, our best product model
performs competitively to the self-trained two-step
discriminative reranking parser of McClosky et al
(2006), which makes use of many non-local rerank-
ing features. Our parser also performs comparably
to other system combination approaches (Sagae and
Lavie, 2006; Fossum and Knight, 2009; Zhang et
al., 2009) with higher recall and lower precision,
Huang and Harper (2009) except that we keep all unary rules.
The reported numbers are from the best single ST-Reg grammar
in this work.
20
but again without using a discriminative reranking
step. We expect that replacing the first-step genera-
tive parsing model in McClosky et al (2006) with a
product of latent variable grammars would give even
higher parsing accuracies.
On the Broadcast News test set, our best perform-
ing single and product grammars (bolded in Table 6)
obtained F scores of 88.7 and 89.6, respectively.
While there is no prior work using our setup, we ex-
pect these numbers to set a high baseline.
7 Conclusions and Future Work
We evaluated methods for self-training high accu-
racy products of latent variable grammars with large
amounts of genre-matched data. We demonstrated
empirically on newswire and broadcast news genres
that very high accuracies can be achieved by training
grammars on disjoint sets of automatically labeled
data. Two primary factors appear to be determin-
ing the efficacy of our self-training approach. First,
the accuracy of the model used for parsing the unla-
beled data is important for the accuracy of the result-
ing single self-trained grammars. Second, the diver-
sity of the individual grammars controls the gains
that can be obtained by combining multiple gram-
mars into a product model. Our most accurate sin-
gle grammar achieves an F score of 91.6 on the WSJ
test set, rivaling discriminative reranking approaches
(Charniak and Johnson, 2005) and products of latent
variable grammars (Petrov, 2010), despite being a
single generative PCFG. Our most accurate product
model achieves an F score of 92.5 without the use of
discriminative reranking and comes close to the best
known numbers on this test set (Zhang et al, 2009).
In future work, we plan to investigate additional
methods for increasing the diversity of our self-
trained models. One possibility would be to utilize
more unlabeled data or to identify additional ways to
bias the models. It would also be interesting to deter-
mine whether further increasing the accuracy of the
model used for automatically labeling the unlabeled
data can enhance performance even more. A simple
but computationally expensive way to do this would
be to parse the data with an SM7 product model.
Finally, for this work, we always used products
of 10 grammars, but we sometimes observed that
subsets of these grammars produce even better re-
sults on the development set. Finding a way to se-
lect grammars from a grammar pool to achieve high
performance products is an interesting area of future
study.
8 Acknowledgments
This research was supported in part by NSF IIS-
0703859. Opinions, findings, and recommendations
expressed in this paper are those of the authors and
do not necessarily reflect the views of the funding
agency or the institutions where the work was com-
pleted.
References
Jason Baldridge and Miles Osborne. 2008. Active learn-
ing and logarithmic opinion pools for HPSG parse se-
lection. Natural Language Engineering.
Leo Breiman. 1996. Bagging predictors. Machine
Learning.
Kenneth P. Burnham and David R. Anderson. 2002.
Model Selection and Multimodel Inference: A Prac-
tical Information-Theoretic Approach. New York:
Springer-Verlag.
Xavier Carreras, Michael Collins, and Terry Koo. 2008.
Tag, dynamic programming, and the perceptron for ef-
ficient, feature-rich parsing. In CoNLL, pages 9?16.
Eugene Charniak and Mark Johnson. 2001. Edit detec-
tion and parsing for transcribed speech. In NAACL.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. In ACL.
Eugene Charniak, Don Blaheta, Niyu Ge, Keith Hall,
John Hale, and Mark Johnson, 2000. BLLIP 1987-89
WSJ Corpus Release 1. Linguistic Data Consortium,
Philadelphia.
Eugene Charniak. 1997. Statistical parsing with a
context-free grammar and word statistics. In ICAI.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In ACL.
Denis Filimonov and Mary Harper. 2009. A joint
language model with fine-grain syntactic tags. In
EMNLP, pages 1114?1123, Singapore, August.
Victoria Fossum and Kevin Knight. 2009. Combining
constituent parsers. In NAACL, pages 253?256.
John Garofolo, Jonathan Fiscus, William Fisher, and
David Pallett, 1996. CSR-IV HUB4. Linguistic Data
Consortium, Philadelphia.
Geoffrey E. Hinton. 1999. Products of experts. In
ICANN.
21
Zhongqiang Huang and Mary Harper. 2009. Self-
training PCFG grammars with latent annotations
across languages. In EMNLP.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In ACL.
Percy Liang, Slav Petrov, Michael I. Jordan, and Dan
Klein. 2007. The infinite PCFG using hierarchical
Dirichlet processes. In EMNLP.
Mitchell P. Marcus, Beatrice Santorini, Mary Ann
Marcinkiewicz, and Ann Taylor, 1999. Treebank-3.
Linguistic Data Consortium, Philadelphia.
Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii.
2005. Probabilistic CFG with latent annotations. In
ACL.
David McClosky, Eugene Charniak, and Mark Johnson.
2006. Effective self-training for parsing. In HLT-
NAACL.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In HLT-NAACL.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In ACL.
Slav Petrov. 2009. Coarse-to-Fine Natural Language
Processing. Ph.D. thesis, University of California at
Bekeley.
Slav Petrov. 2010. Products of random latent variable
grammars. In HLT-NAACL.
Brian Roark, Mary Harper, Yang Liu, Robin Stewart,
Matthew Lease, Matthew Snover, Izhak Shafran, Bon-
nie J. Dorr, John Hale, Anna Krasnyanskaya, and Lisa
Yung. 2006. SParseval: Evaluation metrics for pars-
ing speech. In LREC.
Kenji Sagae and Alon Lavie. 2006. Parser combination
by reparsing. In NAACL, pages 129?132.
Andrew Smith and Miles Osborne. 2007. Diversity
in logarithmic opinion pools. Lingvisticae Investiga-
tiones.
Andrew Smith, Trevor Cohn, and Miles Osborne. 2005.
Logarithmic opinion pools for conditional random
fields. In ACL.
Wen Wang and Mary P. Harper. 2002. The superarv lan-
guage model: Investigating the effectiveness of tightly
integrating multiple knowledge sources. In EMNLP,
pages 238?247, Philadelphia, July.
Ralph Weischedel, Sameer Pradhan, Lance Ramshaw,
Martha Palmer, Nianwen Xue, Mitchell Marcus, Ann
Taylor, Craig Greenberg, Eduard Hovy, Robert Belvin,
and Ann Houston, 2008. OntoNotes Release 2.0. Lin-
guistic Data Consortium, Philadelphia.
Hui Zhang, Min Zhang, Chew Lim Tan, and Haizhou
Li. 2009. K-best combination of syntactic parsers.
In EMNLP, pages 1552?1560.
22
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 138?147,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Soft Syntactic Constraints for Hierarchical Phrase-based Translation
Using Latent Syntactic Distributions
Zhongqiang Huang
Institute for Advanced Computer Studies
University of Maryland
College Park, MD 20742
zqhuang@umiacs.umd.edu
Martin C?mejrek and Bowen Zhou
IBM T. J. Watson Research Center
Yorktown Heights, NY 10598
{martin.cmejrek,zhou}@us.ibm.com
Abstract
In this paper, we present a novel approach
to enhance hierarchical phrase-based machine
translation systems with linguistically moti-
vated syntactic features. Rather than directly
using treebank categories as in previous stud-
ies, we learn a set of linguistically-guided la-
tent syntactic categories automatically from a
source-side parsed, word-aligned parallel cor-
pus, based on the hierarchical structure among
phrase pairs as well as the syntactic structure
of the source side. In our model, each X non-
terminal in a SCFG rule is decorated with a
real-valued feature vector computed based on
its distribution of latent syntactic categories.
These feature vectors are utilized at decod-
ing time to measure the similarity between the
syntactic analysis of the source side and the
syntax of the SCFG rules that are applied to
derive translations. Our approach maintains
the advantages of hierarchical phrase-based
translation systems while at the same time nat-
urally incorporates soft syntactic constraints.
1 Introduction
In recent years, syntax-based translation mod-
els (Chiang, 2007; Galley et al, 2004; Liu et
al., 2006) have shown promising progress in im-
proving translation quality, thanks to the incorpora-
tion of phrasal translation adopted from the widely
used phrase-based models (Och and Ney, 2004) to
handle local fluency and the engagement of syn-
chronous context-free grammars (SCFG) to handle
non-local phrase reordering. Approaches to syntax-
based translation models can be largely categorized
into two classes based on their dependency on anno-
tated corpus (Chiang, 2007). Linguistically syntax-
based models (e.g., (Yamada and Knight, 2001; Gal-
ley et al, 2004; Liu et al, 2006)) utilize structures
defined over linguistic theory and annotations (e.g.,
Penn Treebank) and guide the derivation of SCFG
rules with explicit parsing on at least one side of
the parallel corpus. Formally syntax-based mod-
els (e.g., (Wu, 1997; Chiang, 2007)) extract syn-
chronous grammars from parallel corpora based on
the hierarchical structure of natural language pairs
without any explicit linguistic knowledge or anno-
tations. In this work, we focus on the hierarchi-
cal phrase-based models of Chiang (2007), which
is formally syntax-based, and always refer the term
SCFG, from now on, to the grammars of this model
class.
On the one hand, hierarchical phrase-based mod-
els do not suffer from errors in syntactic constraints
that are unavoidable in linguistically syntax-based
models. Despite the complete lack of linguistic
guidance, the performance of hierarchical phrase-
based models is competitive when compared to lin-
guistically syntax-based models. As shown in (Mi
and Huang, 2008), hierarchical phrase-based models
significantly outperform tree-to-string models (Liu
et al, 2006; Huang et al, 2006), even when at-
tempts are made to alleviate parsing errors using
either forest-based decoding (Mi et al, 2008) or
forest-based rule extraction (Mi and Huang, 2008).
On the other hand, when properly used, syntac-
tic constraints can provide invaluable benefits to im-
prove translation quality. The tree-to-string mod-
els of Mi and Huang (2008) can actually signif-
138
icantly outperform hierarchical phrase-based mod-
els when using forest-based rule extraction together
with forest-based decoding. Chiang (2010) also ob-
tained significant improvement over his hierarchi-
cal baseline by using syntactic parse trees on both
source and target sides to induce fuzzy (not exact)
tree-to-tree rules and by also allowing syntactically
mismatched substitutions.
In this paper, we augment rules in hierarchical
phrase-based translation systems with novel syntac-
tic features. Unlike previous studies (e.g., (Zoll-
mann and Venugopal, 2006)) that directly use ex-
plicit treebank categories such as NP, NP/PP (NP
missing PP from the right) to annotate phrase pairs,
we induce a set of latent categories to capture the
syntactic dependencies inherent in the hierarchical
structure of phrase pairs, and derive a real-valued
feature vector for each X nonterminal of a SCFG
rule based on the distribution of the latent cate-
gories. Moreover, we convert the equality test of
two sequences of syntactic categories, which are ei-
ther identical or different, into the computation of
a similarity score between their corresponding fea-
ture vectors. In our model, two symbolically dif-
ferent sequences of syntactic categories could have
a high similarity score in the feature vector repre-
sentation if they are syntactically similar, and a low
score otherwise. In decoding, these feature vectors
are utilized to measure the similarity between the
syntactic analysis of the source side and the syntax
of the SCFG rules that are applied to derive trans-
lations. Our approach maintains the advantages of
hierarchical phrase-based translation systems while
at the same time naturally incorporates soft syntactic
constraints. To the best of our knowledge, this is the
first work that applies real-valued syntactic feature
vectors to machine translation.
The rest of the paper is organized as follows.
Section 2 briefly reviews hierarchical phrase-based
translation models. Section 3 presents an overview
of our approach, followed by Section 4 describing
the hierarchical structure of aligned phrase pairs and
Section 5 describing how to induce latent syntactic
categories. Experimental results are reported in Sec-
tion 6, followed by discussions in Section 7. Sec-
tion 8 concludes this paper.
2 Hierarchical Phrase-Based Translation
An SCFG is a synchronous rewriting system gener-
ating source and target side string pairs simultane-
ously based on a context-free grammar. Each syn-
chronous production (i.e., rule) rewrites a nonter-
minal into a pair of strings, ? and ?, where ? (or
?) contains terminal and nonterminal symbols from
the source (or target) language and there is a one-to-
one correspondence between the nonterminal sym-
bols on both sides. In particular, the hierarchical
model (Chiang, 2007) studied in this paper explores
hierarchical structures of natural language and uti-
lize only a unified nonterminal symbol X in the
grammar,
X ? ??, ?,??
where ? is the one-to-one correspondence between
X?s in ? and ?, and it can be indicated by un-
derscripted co-indexes. Two example English-to-
Chinese translation rules are represented as follows:
X ? ?give the pen to me,????? (1)
X ? ?giveX1 to me, X1??? (2)
The SCFG rules of hierarchical phrase-based
models are extracted automatically from corpora of
word-aligned parallel sentence pairs (Brown et al,
1993; Och and Ney, 2000). An aligned sentence pair
is a tuple (E,F,A), where E = e1 ? ? ? en can be in-
terpreted as an English sentence of length n, F =
f1 ? ? ? fm its translation of length m in a foreign lan-
guage, andA a set of links between words of the two
sentences. Figure 1 (a) shows an example of aligned
English-to-Chinese sentence pair. Widely adopted
in phrase-based models (Och and Ney, 2004), a pair
of consecutive sequences of words from E and F is
a phrase pair if all words are aligned only within the
sequences and not to any word outside. We call a se-
quence of words a phrase if it corresponds to either
side of a phrase pair, and a non-phrase otherwise.
Note that the boundary words of a phrase pair may
not be aligned to any other word. We call the phrase
pairs with all boundary words aligned tight phrase
pairs (Zhang et al, 2008). A tight phrase pair is the
minimal phrase pair among all that share the same
set of alignment links. Figure 1 (b) highlights the
tight phrase pairs in the example sentence pair.
139
65
4
3
2
1
1
2 3 4 5
(a) (b)
Figure 1: An example of word-aligned sentence pair (a)
with tight phrase pairs marked in a matrix representation
(b).
The extraction of SCFG rules proceeds as fol-
lows. In the first step, all phrase pairs below a max-
imum length are extracted as phrasal rules. In the
second step, abstract rules are extracted from tight
phrase pairs that contain other tight phrase pairs by
replacing the sub phrase pairs with co-indexed X-
nonterminals. Chiang (2007) also introduced several
requirements (e.g., there are at most two nontermi-
nals at the right hand side of a rule) to safeguard
the quality of the abstract rules as well as keeping
decoding efficient. In our example above, rule (2)
can be extracted from rule (1) with the following sub
phrase pair:
X ? ?the pen,???
The use of a unified X nonterminal makes hier-
archical phrase-based models flexible at capturing
non-local reordering of phrases. However, such flex-
ibility also comes at the cost that it is not able to
differentiate between different syntactic usages of
phrases. Suppose rule X ? ?I am readingX1, ? ? ? ?
is extracted from a phrase pair with I am reading a
book on the source side whereX1 is abstracted from
the noun phrase pair . If this rule is used to translate
I am reading the brochure of a book fair, it would
be better to apply it over the entire string than over
sub-strings such as I ... the brochure of. This is be-
cause the nonterminal X1 in the rule was abstracted
from a noun phrase on the source side of the training
data and would thus be better (more informative) to
be applied to phrases of the same type. Hierarchi-
cal phrase-based models are not able to distinguish
syntactic differences like this.
Zollmann and Venugopal (2006) attempted to ad-
dress this problem by annotating phrase pairs with
treebank categories based on automatic parse trees.
They introduced an extended set of categories (e.g.,
NP+V for she went and DT\NP for great wall, an
noun phrase with a missing determiner on the left)
to annotate phrase pairs that do not align with syn-
tactic constituents. Their hard syntactic constraint
requires that the nonterminals should match exactly
to rewrite with a rule, which could rule out poten-
tially correct derivations due to errors in the syn-
tactic parses as well as to data sparsity. For exam-
ple, NP cannot be instantiated with phrase pairs of
type DT+NN, in spite of their syntactic similarity.
Venugopal et al (2009) addressed this problem by
directly introducing soft syntactic preferences into
SCFG rules using preference grammars, but they
had to face the computational challenges of large
preference vectors. Chiang (2010) also avoided hard
constraints and took a soft alternative that directly
models the cost of mismatched rule substitutions.
This, however, would require a large number of pa-
rameters to be tuned on a generally small-sized held-
out set, and it could thus suffer from over-tuning.
3 Approach Overview
In this work, we take a different approach to intro-
duce linguistic syntax to hierarchical phrase-based
translation systems and impose soft syntactic con-
straints between derivation rules and the syntactic
parse of the sentence to be translated. For each
phrase pair extracted from a sentence pair of a
source-side parsed parallel corpus, we abstract its
syntax by the sequence of highest root categories,
which we call a tag sequence, that exactly1 domi-
nates the syntactic tree fragments of the source-side
phrase. Figure 3 (b) shows the source-side parse tree
of a sentence pair. The tag sequence for ?the pen?
is simply ?NP? because it is a noun phrase, while
phrase ?give the pen? is dominated by a verb fol-
lowed by a noun phrase, and thus its tag sequence is
?VBP NP?.
Let TS = {ts1, ? ? ? , tsm} be the set of all tag se-
quences extracted from a parallel corpus. The syntax
of each X nonterminal2 in a SCFG rule can be then
1In case of a non-tight phrase pair, we only abstract and
compare the syntax of the largest tight part.
2There are three X nonterminals (one on the left and two on
the right) for binary abstract rules, two for unary abstract rules,
and one for phrasal rules.
140
Tag Sequence Probability
NP 0.40
DT NN 0.35
DT NN NN 0.25
Table 1: The distribution of tag sequences forX1 inX ?
?I am reading X1, ? ? ? ?.
characterized by the distribution of tag sequences
~PX(TS) = (pX(ts1), ? ? ? , pX(tsm)), based on the
phrase pairs it is abstracted from. Table 1 shows
an example distribution of tag sequences for X1 in
X ? ?I am reading X1, ? ? ? ?.
Instead of directly using tag sequences, as we
discussed their disadvantages above, we represent
each of them by a real-valued feature vector. Sup-
pose we have a collection of n latent syntactic cate-
gories C = {c1, ? ? ? , cn}. For each tag sequence ts,
we compute its distribution of latent syntactic cate-
gories ~Pts(C) = (pts(c1), ? ? ? , pts(cn)). For exam-
ple, ~P?NP VP?(C) = {0.5, 0.2, 0.3} means that the la-
tent syntactic categories c1, c2, and c3 are distributed
as p(c1) = 0.5, p(c2) = 0.2, and p(c3) = 0.3 for tag
sequence ?NP VP?. We further convert the distribu-
tion to a normalized feature vector ~F (ts) to repre-
sent tag sequence ts:
~F (ts) = (f1(ts), ? ? ? , fn(ts))
=
(pts(c1), ? ? ? , pts(cn))
?(pts(c1), ? ? ? , pts(cn))?
The advantage of using real-valued feature vec-
tors is that the degree of similarity between two tag
sequences ts and ts? in the space of the latent syn-
tactic categories C can be simply computed as a dot-
product3 of their feature vectors:
~F (ts) ? ~F (ts?) =
?
1?i?n
fi(ts)fi(ts
?)
which computes a syntactic similarity score in the
range of 0 (totally syntactically different) to 1 (com-
pletely syntactically identical).
Similarly, we can represent the syntax of each X
nonterminal in a rule with a feature vector ~F (X),
computed as the sum of the feature vectors of tag
3Other measures such as KL-divergence in the probability
space are also feasible.
sequences weighted by the distribution of tag se-
quences of the nonterminal X:
~F (X) =
?
ts?TS
pX(ts)~F (ts)
Now we can impose soft syntactic constraints us-
ing these feature vectors when a SCFG rule is used
to translate a parsed source sentence. Given that aX
nonterminal in the rule is applied to a span with tag
sequence4 ts as determined by a syntactic parser, we
can compute the following syntax similarity feature:
SynSim(X, ts) = ? log(~F (ts) ? ~F (X))
Except that it is computed on the fly, this feature
can be used in the same way as the regular features
in hierarchical translation systems to determine the
best translation, and its feature weight can be tuned
in the same way together with the other features on
a held-out data set.
In our approach, the set of latent syntactic cate-
gories is automatically induced from a source-side
parsed, word-aligned parallel corpus based on the
hierarchical structure among phrase pairs along with
the syntactic parse of the source side. In what fol-
lows, we will explain the two critical aspects of
our approach, i.e., how to identify the hierarchi-
cal structures among all phrase pairs in a sentence
pair, and how to induce the latent syntactic cate-
gories from the hierarchy to syntactically explain the
phrase pairs.
4 Alignment-based Hierarchy
The aforementioned abstract rule extraction algo-
rithm of Chiang (2007) is based on the property that
a tight phrase pair can contain other tight phrase
pairs. Given two non-disjoint tight phrase pairs that
share at least one common alignment link, there are
only two relationships: either one completely in-
cludes another or they do not include one another
but have a non-empty overlap, which we call a non-
trivial overlap. In the second case, the intersection,
differences, and union of the two phrase pairs are
4A normalized uniform feature vector is used for tag se-
quences (of parsed test sentences) that are not seen on the train-
ing corpus.
141
Figure 2: A decomposition tree of tight phrase pairs with
all tight phrase pairs listed on the right. As highlighted,
the two non-maximal phrase pairs are generated by con-
secutive sibling nodes.
also tight phrase pairs (see Figure 1 (b) for exam-
ple), and the two phrase pairs, as well as their inter-
section and differences, are all sub phrase pairs of
their union.
Zhang et al (2008) exploited this property to con-
struct a hierarchical decomposition tree (Bui-Xuan
et al, 2005) of phrase pairs from a sentence pair to
extract all phrase pairs in linear time. In this pa-
per, we focus on learning the syntactic dependencies
along the hierarchy of phrase pairs. Our hierarchy
construction follows Heber and Stoye (2001).
Let P be the set of tight phrase pairs extracted
from a sentence pair. We call a sequentially-ordered
list5 L = (p1, ? ? ? , pk) of unique phrase pairs pi ? P
a chain if every two successive phrase pairs in L
have a non-trivial overlap. A chain is maximal if
it can not be extended to its left or right with other
phrase pairs. Note that any sub-sequence of phrase
pairs in a chain generates a tight phrase pair. In par-
ticular, chain L generates a tight phrase pair ?(L)
that corresponds exactly to the union of the align-
ment links in p ? L. We call the phrase pairs
generated by maximal chains maximal phrase pairs
and call the other phrase pairs non-maximal. Non-
maximal phrase pairs always overlap non-trivially
with some other phrase pairs while maximal phrase
pairs do not, and it can be shown that any non-
maximal phrase pair can be generated by a sequence
of maximal phrase pairs. Note that the largest tight
phrase pair that includes all alignment links in A is
also a maximal phrase pair.
5The phrase pairs can be sequentially ordered first by the
boundary positions of the source-side phrase and then by the
boundary positions of the target-side phrase.
give
the pen to me .
X B B B X X
X
X
X
PP
VBP
DT NN TO PRP .
NP
VP
S
give
the pen to me .
(a) (b)
X
X
B B B X X
X
X
X
X
X
X
B B B X X
X
X
X
X
VBP
X
X
B B B X X
X
X
X
X
DT
NN TO PRP
.
NP PP
CR
VP
S
I(!)
O(!)
X
X
B B B X X
X
X
X
X
VBP DT
NN TO PRP
.
S
CR
NP PP
CR
O(!)
I(!)
(c) (d)
Figure 3: (a) decomposition tree for the English side of
the example sentence pair with all phrases underlined, (b)
automatic parse tree of the English side, (c) two example
binarized decomposition trees with syntactic emissions
in depicted in (d), where the two dotted curves give an
example I(?) and O(?) that separate the forest into two
parts.
Lemma 1 Given two different maximal phrase
pairs p1 and p2, exactly one of the following alter-
natives is true: p1 and p2 are disjoint, p1 is a sub
phrase pair of p2, or p2 is a sub phrase pair of p1.
A direct outcome of Lemma 1 is that there is an
unique decomposition tree T = (N,E) covering all
of the tight phrase pairs of a sentence pair, where N
is the set of maximal phrase pairs and E is the set of
edges that connect between pairs of maximal phrase
pairs if one is a sub phrase pair of another. All of the
tight phrase pairs of a sentence pair can be extracted
directly from the nodes of the decomposition tree
(these phrase pairs are maximal), or generated by se-
quences of consecutive sibling nodes6 (these phrase
pairs are non-maximal). Figure 2 shows the decom-
position tree as well as all of the tight phrase pairs
that can be extracted from the example sentence pair
in Figure 1.
We focus on the source side of the decomposition
tree, and expand it to include all of the non-phrase
6Unaligned words may be added.
142
single words within the scope of the decomposition
tree as frontiers and attach each as a child of the low-
est node that contains the word. We then abstract the
trees nodes with two symbol, X for phrases, and B
for non-phrases, and call the result the decomposi-
tion tree of the source side phrases. Figure 3 (a) de-
picts such tree for the English side of our example
sentence pair. We further recursively binarize7 the
decomposition tree into a binarized decomposition
forest such that all phrases are directly represented
as nodes in the forest. Figure 3 (c) shows two of the
many binarized decomposition trees in the forest.
The binarized decomposition forest compactly
encodes the hierarchical structure among phrases
and non-phrases. However, the coarse abstraction
of phrases with X and non-phrases with B provides
little information on the constraints of the hierarchy.
In order to bring in syntactic constraints, we anno-
tate the nodes in the decomposition forest with syn-
tactic observations based on the automatic syntactic
parse tree of the source side. If a node aligns with
a constituent in the parse tree, we add the syntactic
category (e.g., NP) of the constituent as an emitted
observation of the node, otherwise, it crosses con-
stituent boundaries and we add a designated crossing
category CR as its observation. We call the resulting
forest a syntactic decomposition forest. Figure 3 (d)
shows two syntactic decomposition trees of the for-
est based on the parse tree in Figure 3 (b). We will
next describe how to learn finer-grained X and B
categories based on the hierarchical syntactic con-
straints.
5 Inducing Latent Syntactic Categories
If we designate a unique symbol S as the new root
of the syntactic decomposition forests introduced
in the previous section, it can be shown that these
forests can be generated by a probabilistic context-
free grammar G = (V,?, S,R, ?), where
? V = {S,X,B} is the set of nonterminals,
? ? is the set of terminals comprising treebank
categories plus the CR tag (the crossing cate-
gory),
7The intermediate binarization nodes are also labeled as ei-
ther X or B based on whether they exactly cover a phrase or
not.
? S ? V is the unique start symbol,
? R is the union of the set of production rules
each rewriting a nonterminal to a sequence of
nonterminals and the set of emission rules each
generating a terminal from a nonterminal,
? and ? assigns a probability score to each rule
r ? R.
Such a grammar can be derived from the set of
syntactic decomposition forests extracted from a
source-side parsed parallel corpus, with rule prob-
ability scores estimated as the relative frequencies
of the production and emission rules.
The X and B nonterminals in the grammar are
coarse representations of phrase and non-phrases
and do not carry any syntactic information at all.
In order to introduce syntax to these nonterminals,
we incrementally split8 them into a set of latent
categories {X1, ? ? ? , Xn} for X and another set
{B1, ? ? ? , Bn} for B, and then learn a set of rule
probabilities9 ? on the latent categories so that the
likelihood of the training forests are maximized. The
motivation is to let the latent categories learn differ-
ent preferences of (emitted) syntactic categories as
well as structural dependencies along the hierarchy
so that they can carry syntactic information. We call
them latent syntactic categories. The learned Xi?s
represent syntactically-induced finer-grained cate-
gories of phrases and are used as the set of latent
syntactic categories C described in Section 3. In re-
lated research, Matsuzaki et al (2005) and Petrov et
al. (2006) introduced latent variables to learn finer-
grained distinctions of treebank categories for pars-
ing, and Huang et al (2009) used a similar approach
to learn finer-grained part-of-speech tags for tag-
ging. Our method is in spirit similar to these ap-
proaches.
Optimization of grammar parameters to maximize
the likelihood of training forests can be achieved
8We incrementally split each nonterminal to 2, 4, 8, and fi-
nally 16 categories, with each splitting followed by several EM
iterations to tune model parameters. We consider 16 an appro-
priate number for latent categories, not too small to differentiate
between different syntactic usages and not too large for the extra
computational and storage costs.
9Each binary production rule is now associated with a 3-
dimensional matrix of probabilities, and each emission rule as-
sociated with a 1-dimensional array of probabilities.
143
by a variant of Expectation-Maximization (EM) al-
gorithm. Recall that our decomposition forests are
fully binarized (except the root). In the hypergraph
representation (Huang and Chiang, 2005), the hy-
peredges of our forests all have the same format10
?(V,W ), U?, meaning that node U expands to nodes
V and W with production rule U ? VW . Given
a forest F with root node R, we denote e(U) the
emitted syntactic category at node U and LR(U) (or
PL(W ), or PR(V ))11 the set of node pairs (V,W )
(or (U, V ), or (U,W )) such that ?(V,W ), U? is a hy-
peredge of the forest. Now consider node U , which
is either S, X , or B, in the forest. Let Ux be the
latent syntactic category12 of node U . We define
I(Ux) the part of the forest (includes e(U) but not
Ux) inside U , and O(Ux) the other part of the forest
(includes Ux but not e(U)) outside U , as illustrated
in Figure 3 (d). The inside-outside probabilities are
defined as:
PIN(Ux) = P (I(Ux)|Ux)
POUT(Ux) = P (O(Ux)|S)
which can be computed recursively as:
PIN(Ux) =
?
(V,W )?LR(U)
?
y,z
?(Ux ? e(U))
??(Ux ? VyWz)
?PIN(Vy)PIN(Wz)
POUT(Ux) =
?
(V,W )?PL(U)
?
y,z
?(Vy ? e(V ))
??(Vy ?WzUx)
?POUT(Vy)PIN(Wz)
+
?
(V,W )?PR(U)
?
y,z
?(Vy ? e(V ))
??(Vy ? UxWz)
?POUT(Vy)PIN(Wz)
In the E-step, the posterior probability of the oc-
currence of production rule13 Ux ? VyWz is com-
puted as:
P (Ux ? VyWz|F ) =
?(Ux ? e(U))
??(Ux ? VyWz)
?POUT(Ux)PIN(Vy)PIN(Ww)
PIN(R)
10The hyperedge corresponding to the root node has a differ-
ent format because it is unary, but it can be handled similarly.
When clear from context, we use the same variable to present
both a node and its label.
11LR stands for the left and right children, PL for the parent
and left children, and PR for the parent and right children.
12We never split the start symbol S, and denote S0 = S.
13The emission rules can be handled similarly.
In the M-step, the expected counts of rule Ux ?
VyWz for all latent categories Vy and Wz are accu-
mulated together and then normalized to obtain an
update of the probability estimation:
?(Ux ? VyWz) =
#(Ux ? VyWz)
?
(V ?,W ?)
?
y,z
#(Ux ? VyWz)
Recall that each node U labeled asX in a forest is
associated with a phrase whose syntax is abstracted
by a tag sequence. Once a grammar is learned, for
each such node with a corresponding tag sequence
ts in forest F , we compute the posterior probability
that the latent category of node U being Xi as:
P (Xi|ts) =
POUT(Ui)PIN(Ui)
PIN(R)
This contributes P (Xi|ts) evidence that tag se-
quence ts belongs to a Xi category. When all
of the evidences are computed and accumulated in
#(Xi, ts), they can then be normalized to obtain the
probability that the latent category of ts is Xi:
pts(Xi) =
#(Xi, ts)
?
i #(Xi, ts)
As described in Section 3, the distributions of latent
categories are used to compute the syntactic feature
vectors for the SCFG rules.
6 Experiments
We conduct experiments on two tasks, English-to-
German and English-to-Chinese, both aimed for
speech-to-speech translation. The training data for
the English-to-German task is a filtered subset of the
Europarl corpus (Koehn, 2005), containing ?300k
parallel bitext with ?4.5M tokens on each side. The
dev and test sets both contain 1k sentences with one
reference for each. The training data for the English-
to-Chinese task is collected from transcription and
human translation of conversations in travel domain.
It consists of ?500k parallel bitext with ?3M to-
kens14 on each side. Both dev and test sets contain
?1.3k sentences, each with two references. Both
14The Chinese sentences are automatically segmented into
words. However, BLEU scores are computed at character level
for tuning and evaluation.
144
corpora are also preprocessed with punctuation re-
moved and words down-cased to make them suitable
for speech translation.
The baseline system is our implementation of the
hierarchical phrase-based model of Chiang (2007),
and it includes basic features such as rule and
lexicalized rule translation probabilities, language
model scores, rule counts, etc. We use 4-gram lan-
guage models in both tasks, and conduct minimum-
error-rate training (Och, 2003) to optimize feature
weights on the dev set. Our baseline hierarchical
model has 8.3M and 9.7M rules for the English-to-
German and English-to-Chinese tasks, respectively.
The English side of the parallel data is
parsed by our implementation of the Berkeley
parser (Huang and Harper, 2009) trained on the
combination of Broadcast News treebank from
Ontonotes (Weischedel et al, 2008) and a speechi-
fied version of the WSJ treebank (Marcus et al,
1999) to achieve higher parsing accuracy (Huang et
al., 2010). Our approach introduces a new syntactic
feature and its feature weight is tuned in the same
way together with the features in the baseline model.
In this study, we induce 16 latent categories for both
X and B nonterminals.
Our approach identifies ?180k unique tag se-
quences for the English side of phrase pairs in both
tasks. As shown by the examples in Table 2, the syn-
tactic feature vector representation is able to identify
similar and dissimilar tag sequences. For instance,
it determines that the sequence of ?DT JJ NN? is
syntactically very similar to ?DT ADJP NN? while
very dissimilar to ?NN CD VP?. Notice that our la-
tent categories are learned automatically to maxi-
mize the likelihood of the training forests extracted
based on alignment and are not explicitly instructed
to discriminate between syntactically different tag
sequences. Our approach is not guaranteed to al-
ways assign similar feature vectors to syntactically
similar tag sequences. However, as the experimental
results show below, the latent categories are able to
capture some similarities among tag sequences that
are beneficial for translation.
Table 3 and 4 report the experimental results
on the English-to-German and English-to-Chinese
tasks, respectively. The addition of the syntax fea-
ture achieves a statistically significant improvement
(p ? 0.01) of 0.6 in BLEU on the test set of the
Baseline +Syntax ?
Dev 16.26 17.06 0.80
Test 16.41 17.01 0.60
Table 3: BLEU scores of the English-to-German task
(one reference).
Baseline +Syntax ?
Dev 46.47 47.39 0.92
Test 45.45 45.86 0.41
Table 4: BLEU scores of the English-to-Chinese task
(two references).
English-to-German task. This improvement is sub-
stantial given that only one reference is used for each
test sentence. On the English-to-Chinese task, the
syntax feature achieves a smaller improvement of
0.41 BLEU on the test set. One potential explanation
for the smaller improvement is that the sentences on
the English-to-Chinese task are much shorter, with
an average of only 6 words per sentence, compared
to 15 words in the English-to-German task. The
hypothesis space of translating a longer sentence is
much larger than that of a shorter sentence. There-
fore, there is more potential gain from using syn-
tax features to rule out unlikely derivations of longer
sentences, while phrasal rules might be adequate for
shorter sentences, leaving less room for syntax to
help as in the case of the English-to-Chinese task.
7 Discussions
The incorporation of the syntactic feature into the
hierarchical phrase-based translation system also
brings in additional memory load and computational
cost. In the worst case, our approach requires stor-
ing one feature vector for each tag sequence and one
feature vector for each nonterminal of a SCFG rule,
with the latter taking the majority of the extra mem-
ory storage. We observed that about 90% of the
X nonterminals in the rules only have one tag se-
quence, and thus the required memory space can be
significantly reduced by only storing a pointer to the
feature vector of the tag sequence for these nonter-
minals. Our approach also requires computing one
dot-product of two feature vectors for each nonter-
minal when a SCFG rule is applied to a source span.
145
Very similar Not so similar Very dissimilar
~F (ts) ? ~F (ts?) > 0.9 0.4 ? ~F (ts) ? ~F (ts?) ? 0.6 ~F (ts) ? ~F (ts?) < 0.1
DT JJ NN
DT NN DT JJ JJ NML NN PP NP NN
DT JJ JJ NN DT JJ CC INTJ VB NN CD VP
DT ADJP NN DT NN NN JJ RB NP IN CD
VP
VB VP PP JJ NN JJ NN TO VP
VB RB VB PP VB NN NN VB JJ WHNP DT NN
VB DT DT NN VB RB IN JJ IN INTJ NP
ADJP
JJ ADJP JJ JJ CC ADJP IN NP JJ
PDT JJ ADJP VB JJ JJ AUX RB ADJP
RB JJ ADVP WHNP JJ ADJP VP
Table 2: Examples of similar and dissimilar tag sequences.
This cost can be reduced, however, by caching the
dot-products of the tag sequences that are frequently
accessed.
There are other successful investigations to
impose soft syntactic constraints to hierarchical
phrase-based models by either introducing syntax-
based rule features such as the prior derivation
model of Zhou et al (2008) or by imposing con-
straints on translation spans at decoding time, e.g.,
(Marton and Resnik, 2008; Xiong et al, 2009;
Xiong et al, 2010). These approaches are all or-
thogonal to ours and it is expected that they can be
combined with our approach to achieve greater im-
provement.
This work is an initial effort to investigate latent
syntactic categories to enhance hierarchical phrase-
based translation models, and there are many direc-
tions to continue this line of research. First, while
the current approach imposes soft syntactic con-
straints between the parse structure of the source
sentence and the SCFG rules used to derive the
translation, the real-valued syntactic feature vectors
can also be used to impose soft constraints between
SCFG rules when rule rewrite occurs. In this case,
target side parse trees could also be used alone or to-
gether with the source side parse trees to induce the
latent syntactic categories. Second, instead of using
single parse trees during both training and decod-
ing, our approach is likely to benefit from exploring
parse forests as in (Mi and Huang, 2008). Third,
in addition to the treebank categories obtained by
syntactic parsing, lexical cues directly available in
sentence pairs could also to explored to guide the
learning of latent categories. Last but not the least,
it would be interesting to investigate discriminative
training approaches to learn latent categories that di-
rectly optimize on translation quality.
8 Conclusion
We have presented a novel approach to enhance
hierarchical phrase-based machine translation sys-
tems with real-valued linguistically motivated fea-
ture vectors. Our approach maintains the advan-
tages of hierarchical phrase-based translation sys-
tems while at the same time naturally incorpo-
rates soft syntactic constraints. Experimental results
showed that this approach improves the baseline hi-
erarchical phrase-based translation models on both
English-to-German and English-to-Chinese tasks.
We will continue this line of research and exploit
better ways to learn syntax and apply syntactic con-
straints to machine translation.
Acknowledgements
This work was done when the first author was visit-
ing IBM T. J. Watson Research Center as a research
intern. We would like to thank Mary Harper for
lots of insightful discussions and suggestions and the
anonymous reviewers for the helpful comments.
References
Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della
Pietra, and Robert L. Mercer. 1993. The mathemat-
146
ics of statistical machine translation: parameter esti-
mation. Computational Linguistics.
Binh Minh Bui-Xuan, Michel Habib, and Christophe
Paul. 2005. Revisiting T. Uno and M. Yagiura?s al-
gorithm. In ISAAC.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics.
David Chiang. 2010. Learning to translate with source
and target syntax. In ACL.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2004. What?s in a translation rule. In
HLT/NAACL.
Steffen Heber and Jens Stoye. 2001. Finding all common
intervals of k permutations. In CPM.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In International Workshop on Parsing Tech-
nology.
Zhongqiang Huang and Mary Harper. 2009. Self-
training PCFG grammars with latent annotations
across languages. In EMNLP.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
A syntax-directed translator with extended domain of
locality. In CHSLP.
Zhongqiang Huang, Vladimir Eidelman, and Mary
Harper. 2009. Improving a simple bigram hmm part-
of-speech tagger by latent annotation and self-training.
In NAACL.
Zhongqiang Huang, Mary Harper, and Slav Petrov. 2010.
Self-training with products of latent variable. In
EMNLP.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In MT Summit.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-
string alignment template for statistical machine trans-
lation. In ACL.
Mitchell P. Marcus, Beatrice Santorini, Mary Ann
Marcinkiewicz, and Ann Taylor, 1999. Treebank-3.
Linguistic Data Consortium, Philadelphia.
Yuval Marton and Philip Resnik. 2008. Soft syntactic
constraints for hierarchical phrased-based translation.
In ACL.
Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii.
2005. Probabilistic CFG with latent annotations. In
ACL.
Haitao Mi and Liang Huang. 2008. Forest-based transla-
tion rule extraction. In EMNLP.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In ACL.
Franz Josef Och and Hermann Ney. 2000. Improved
statistical alignment models. In ACL.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine transla-
tion. Computational Linguistics.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In ACL.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In ACL.
Ashish Venugopal, Andreas Zollmann, Noah A. Smith,
and Stephan Vogel. 2009. Preference grammars: soft-
ening syntactic constraints to improve statistical ma-
chine translation. In NAACL.
Ralph Weischedel, Sameer Pradhan, Lance Ramshaw,
Martha Palmer, Nianwen Xue, Mitchell Marcus, Ann
Taylor, Craig Greenberg, Eduard Hovy, Robert Belvin,
and Ann Houston, 2008. OntoNotes Release 2.0. Lin-
guistic Data Consortium, Philadelphia.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics.
Deyi Xiong, Min Zhang, Aiti Aw, and Haizhou Li. 2009.
A syntax-driven bracketing model for phrase-based
translation. In ACL-IJCNLP.
Deyi Xiong, Min Zhang, and Haizhou Li. 2010. Learn-
ing translation boundaries for phrase-based decoding.
In NAACL-HLT.
Kenji Yamada and Kevin Knight. 2001. A syntax-based
statistical translation model. In ACL.
Hao Zhang, Daniel Gildea, and David Chiang. 2008. Ex-
tracting synchronous grammar rules from word-level
alignments in linear time. In COLING.
Bowen Zhou, Bing Xiang, Xiaodan Zhu, and Yuqing
Gao. 2008. Prior derivation models for formally
syntax-based translation using linguistically syntactic
parsing and tree kernels. In SSST.
Andreas Zollmann and Ashish Venugopal. 2006. Syntax
augmented machine translation via chart parsing. In
StatMT.
147
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 821?831,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Lessons Learned in Part-of-Speech Tagging of Conversational Speech
Vladimir Eidelman?, Zhongqiang Huang?, and Mary Harper??
?Laboratory for Computational Linguistics and Information Processing
Institute for Advanced Computer Studies
University of Maryland, College Park, MD
?Human Language Technology Center of Excellence
Johns Hopkins University, Baltimore, MD
{vlad,zhuang,mharper}@umiacs.umd.edu
Abstract
This paper examines tagging models for spon-
taneous English speech transcripts. We ana-
lyze the performance of state-of-the-art tag-
ging models, either generative or discrimi-
native, left-to-right or bidirectional, with or
without latent annotations, together with the
use of ToBI break indexes and several meth-
ods for segmenting the speech transcripts (i.e.,
conversation side, speaker turn, or human-
annotated sentence). Based on these studies,
we observe that: (1) bidirectional models tend
to achieve better accuracy levels than left-to-
right models, (2) generative models seem to
perform somewhat better than discriminative
models on this task, and (3) prosody improves
tagging performance of models on conversa-
tion sides, but has much less impact on smaller
segments. We conclude that, although the use
of break indexes can indeed significantly im-
prove performance over baseline models with-
out them on conversation sides, tagging ac-
curacy improves more by using smaller seg-
ments, for which the impact of the break in-
dexes is marginal.
1 Introduction
Natural language processing technologies, such as
parsing and tagging, often require reconfiguration
when they are applied to challenging domains that
differ significantly from newswire, e.g., blogs, twit-
ter text (Foster, 2010), or speech. In contrast to
text, conversational speech represents a significant
challenge because the transcripts are not segmented
into sentences. Furthermore, the transcripts are of-
ten disfluent and lack punctuation and case informa-
tion. On the other hand, speech provides additional
information, beyond simply the sequence of words,
which could be exploited to more accurately assign
each word in the transcript a part-of-speech (POS)
tag. One potentially beneficial type of information
is prosody (Cutler et al, 1997).
Prosody provides cues for lexical disambigua-
tion, sentence segmentation and classification,
phrase structure and attachment, discourse struc-
ture, speaker affect, etc. Prosody has been found
to play an important role in speech synthesis sys-
tems (Batliner et al, 2001; Taylor and Black, 1998),
as well as in speech recognition (Gallwitz et al,
2002; Hasegawa-Johnson et al, 2005; Ostendorf et
al., 2003). Additionally, prosodic features such as
pause length, duration of words and phones, pitch
contours, energy contours, and their normalized val-
ues have been used for speech processing tasks like
sentence boundary detection (Liu et al, 2005).
Linguistic encoding schemes like ToBI (Silver-
man et al, 1992) have also been used for sentence
boundary detection (Roark et al, 2006; Harper et al,
2005), as well as for parsing (Dreyer and Shafran,
2007; Gregory et al, 2004; Kahn et al, 2005). In
the ToBI scheme, aspects of prosody such as tone,
prominence, and degree of juncture between words
are represented symbolically. For instance, Dreyer
and Shafran (2007) use three classes of automati-
cally detected ToBI break indexes, indicating major
intonational breaks with a 4, hesitation with a p, and
all other breaks with a 1.
Recently, Huang and Harper (2010) found that
they could effectively integrate prosodic informa-
821
tion in the form of this simplified three class ToBI
encoding when parsing spontaneous speech by us-
ing a prosodically enriched PCFG model with latent
annotations (PCFG-LA) (Matsuzaki et al, 2005;
Petrov and Klein, 2007) to rescore n-best parses
produced by a baseline PCFG-LA model without
prosodic enrichment. However, the prosodically en-
riched models by themselves did not perform sig-
nificantly better than the baseline PCFG-LA model
without enrichment, due to the negative effect that
misalignments between automatic prosodic breaks
and true phrase boundaries have on the model.
This paper investigates methods for using state-
of-the-art taggers on conversational speech tran-
scriptions and the effect that prosody has on tagging
accuracy. Improving POS tagging performance of
speech transcriptions has implications for improving
downstream applications that rely on accurate POS
tags, including sentence boundary detection (Liu
et al, 2005), automatic punctuation (Hillard et al,
2006), information extraction from speech, parsing,
and syntactic language modeling (Heeman, 1999;
Filimonov and Harper, 2009). While there have
been several attempts to integrate prosodic informa-
tion to improve parse accuracy of speech transcripts,
to the best of our knowledge there has been little
work on using this type of information for POS tag-
ging. Furthermore, most of the parsing work has
involved generative models and rescoring/reranking
of hypotheses from the generative models. In this
work, we will analyze several factors related to ef-
fective POS tagging of conversational speech:
? discriminative versus generative POS tagging
models (Section 2)
? prosodic features in the form of simplified ToBI
break indexes (Section 4)
? type of speech segmentation (Section 5)
2 Models
In order to fully evaluate the difficulties inherent in
tagging conversational speech, as well as the possi-
ble benefits of prosodic information, we conducted
experiments with six different POS tagging mod-
els. The models can be broadly separated into two
classes: generative and discriminative. As the first
of our generative models, we used a Hidden Markov
Model (HMM) trigram tagger (Thede and Harper,
1999), which serves to establish a baseline and to
gauge the difficulty of the task at hand. Our sec-
ond model, HMM-LA, was the latent variable bi-
gram HMM tagger of Huang et al (2009), which
achieved state-of-the-art tagging performance by in-
troducing latent tags to weaken the stringent Markov
independence assumptions that generally hinder tag-
ging performance in generative models.
For the third model, we implemented a bidirec-
tional variant of the HMM-LA (HMM-LA-Bidir)
that combines evidence from two HMM-LA tag-
gers, one trained left-to-right and the other right-to-
left. For decoding, we use a product model (Petrov,
2010). The intuition is that the context information
from the left and the right of the current position
is complementary for predicting the current tag and
thus, the combination should serve to improve per-
formance over the HMM-LA tagger.
Since prior work on parsing speech with prosody
has relied on generative models, it was necessary
to modify equations of the model in order to incor-
porate the prosodic information, and then perform
rescoring in order to achieve gains. However, it is
far simpler to directly integrate prosody as features
into the model by using a discriminative approach.
Hence, we also investigate several log-linear mod-
els, which allow us to easily include an arbitrary
number and varying kinds of possibly overlapping
and non-independent features.
First, we implemented a Conditional Random
Field (CRF) tagger, which is an attractive choice due
to its ability to learn the globally optimal labeling
for a sequence and proven excellent performance on
sequence labeling tasks (Lafferty et al, 2001). In
contrast to an HMM which optimizes the joint like-
lihood of the word sequence and tags, a CRF opti-
mizes the conditional likelihood, given by:
p?(t|w) =
exp
?
j ?jFj(t, w)
?
t exp
?
j ?jFj(t, w)
(1)
where the ??s are the parameters of the model to es-
timate and F indicates the feature functions used.
The denominator in (1) is Z?(x), the normalization
factor, with:
Fj(t, w) =
?
i
fj(t, w, i)
822
Class Model Name Latent Variable Bidirectional N-best-Extraction Markov Order
Generative
Trigram HMM
?
2nd
HMM-LA
? ?
1st
HMM-LA-Bidir
? ?
1st
Discriminative
Stanford Bidir
?
2nd
Stanford Left5 2nd
CRF 2nd
Table 1: Description of tagging models
The objective we need to maximize then becomes :
L =
?
n
?
?
?
j
?jFj(tn, wn)? logZ?(xn)
?
??
???2
2?2
where we use a spherical Gaussian prior to pre-
vent overfitting of the model (Chen and Rosen-
feld, 1999) and the wide-spread quasi-Newtonian
L-BFGS method to optimize the model parame-
ters (Liu and Nocedal, 1989). Decoding is per-
formed with the Viterbi algorithm.
We also evaluate state-of-the-art Maximum En-
tropy taggers: the Stanford Left5 tagger (Toutanova
and Manning, 2000) and the Stanford bidirectional
tagger (Toutanova et al, 2003), with the former us-
ing only left context and the latter bidirectional de-
pendencies.
Table 1 summarizes the major differences be-
tween the models along several dimensions: (1) gen-
erative versus discriminative, (2) directionality of
decoding, (3) the presence or absence of latent anno-
tations, (4) the availability of n-best extraction, and
(5) the model order.
In order to assess the quality of our models, we
evaluate them on the section 23 test set of the stan-
dard newswire WSJ tagging task after training all
models on sections 0-22. Results appear in Ta-
ble 2. Clearly, all the models have high accuracy
on newswire data, but the Stanford bidirectional tag-
ger significantly outperforms the other models with
the exception of the HMM-LA-Bidir model on this
task.1
1Statistically significant improvements are calculated using
the sign test (p < 0.05).
Model Accuracy
Trigram HMM 96.58
HMM-LA 97.05
HMM-LA-Bidir 97.16
Stanford Bidir 97.28
Stanford Left5 97.07
CRF 96.81
Table 2: Tagging accuracy on WSJ
3 Experimental Setup
In the rest of this paper, we evaluate the tag-
ging models described in Section 2 on conver-
sational speech. We chose to utilize the Penn
Switchboard (Godfrey et al, 1992) and Fisher tree-
banks (Harper et al, 2005; Bies et al, 2006) because
they provide gold standard tags for conversational
speech and we have access to corresponding auto-
matically generated ToBI break indexes provided by
(Dreyer and Shafran, 2007; Harper et al, 2005)2.
We utilized the Fisher dev1 and dev2 sets contain-
ing 16,519 sentences (112,717 words) as the primary
training data and the entire Penn Switchboard tree-
bank containing 110,504 sentences (837,863 words)
as an additional training source3. The treebanks
were preprocessed as follows: the tags of auxiliary
verbs were replaced with the AUX tag, empty nodes
2A small fraction of words in the Switchboard treebank do
not align with the break indexes because they were produced
based on a later refinement of the transcripts used to produce
the treebank. For these cases, we heuristically added break *1*
to words in the middle of a sentence and *4* to words that end
a sentence.
3Preliminary experiments evaluating the effect of training
data size on performance indicated using the additional Switch-
board data leads to more accurate models, and so we use the
combined training set.
823
and function tags were removed, words were down-
cased, punctuation was deleted, and the words and
their tags were extracted. Because the Fisher tree-
bank was developed using the lessons learned when
developing Switchboard, we chose to use its eval
portion for development (the first 1,020 tagged sen-
tences containing 7,184 words) and evaluation (the
remaining 3,917 sentences with 29,173 words).
We utilize the development set differently for the
generative and discriminative models. Since the EM
algorithm used for estimating the parameters in the
latent variable models introduces a lot of variabil-
ity, we train five models with a different seed and
then choose the best one based on dev set perfor-
mance. For the discriminative models, we tuned
their respective regularization parameters on the dev
set. All results reported in the rest of this paper are
on the test set.
4 Integration of Prosodic Information
In this work, we use three classes of automatically
generated ToBI break indexes to represent prosodic
information (Kahn et al, 2005; Dreyer and Shafran,
2007; Huang and Harper, 2010): 4, 1, and p.
Consider the following speech transcription exam-
ple, which is enriched with ToBI break indexes in
parentheses and tags: i(1)/PRP did(1)/VBD
n?t(1)/RB you(1)/PRP know(4)/VBP
i(1)/PRP did(1)/AUX n?t(1)/RB...
The speaker begins an utterance, and then restarts
the utterance. The automatically predicted break 4
associated with know in the utterance compellingly
indicates an intonational phrase boundary and could
provide useful information for tagging if we can
model it appropriately.
To integrate prosody into our generative models,
we utilize the method from (Dreyer and Shafran,
2007) to add prosodic breaks. As Figure 1 shows,
ToBI breaks provide a secondary sequence of ob-
servations that is parallel to the sequence of words
that comprise the sentence. Each break bi in the sec-
ondary sequence is generated by the same tag ti as
that which generates the corresponding wordwi, and
so it is conditionally independent of its correspond-
ing word given the tag:
P (w, b|t) = P (w|t)P (b|t)
PRP
i
1
VBD
did
1
RB
n?t
1
VBP
know
4
Figure 1: Parallel generation of words and breaks for the
HMM models
The HMM-LA taggers are then able to split tags to
capture implicit higher order interactions among the
sequence of tags, words, and breaks.
The discriminative models are able to utilize
prosodic features directly, enabling the use of con-
textual interactions with other features to further im-
prove tagging accuracy. Specifically, in addition to
the standard set of features used in the tagging lit-
erature, we use the feature templates presented in
Table 3, where each feature associates the break bi,
word wi, or some combination of the two with the
current tag ti4.
Break and/or word values Tag value
bi=B ti = T
bi=B & bi?1=C ti = T
wi=W & bi=B ti = T
wi+1=W & bi=B ti = T
wi+2=W & bi=B ti = T
wi?1=W & bi=B ti = T
wi?2=W & bi=B ti = T
wi=W & bi=B & bi?1=C ti = T
Table 3: Prosodic feature templates
5 Experiments
5.1 Conversation side segmentation
When working with raw speech transcripts, we ini-
tially have a long stream of unpunctuated words,
which is called a conversation side. As the average
length of conversation side segments in our data is
approximately 630 words, it poses quite a challeng-
ing tagging task. Thus, we hypothesize that it is on
these large segments that we should achieve the most
4We modified the Stanford taggers to handle these prosodic
features.
824
93
93.3
93.6
93.9
94.2
94.5
HMM-LA HMM-LA Bidir Stanford Bidir Stanford Left5 CRF
Baseline Prosody OracleBreak OracleBreak+Sent OracleSent OracleBreak-Sent Rescoring
Figure 2: Tagging accuracy on conversation sides
improvement from the addition of prosodic informa-
tion.
In fact, as the baseline results in Figure 2 show,
the accuracies achieved on this task are much lower
than those on the newswire task. The trigram HMM
tagger accuracy drops to 92.43%, while all the other
models fall to within the range of 93.3%-94.12%,
a significant departure from the 96-97.3% range on
newswire sentences. Note that the Stanford bidi-
rectional and HMM-LA tagger perform very simi-
larly, although the HMM-LA-Bidir tagger performs
significantly better than both. In contrast to the
newswire task on which the Stanford bidirectional
tagger performed the best, on this genre, it is slightly
worse than the HMM-LA tagger, albeit the differ-
ence is not statistically significant.
With the direct integration of prosody into the
generative models (see Figure 2), there is a slight but
statistically insignificant shift in performance. How-
ever, integrating prosody directly into the discrimi-
native models leads to significant improvements in
the CRF and Stanford Left5 taggers. The gain in
the Stanford bidirectional tagger is not statistically
significant, however, which suggests that the left-
to-right models benefit more from the addition of
prosody than bidirectional models.
5.2 Human-annotated sentences
Given the lack-luster performance of the tagging
models on conversation side segments, even with the
direct addition of prosody, we chose to determine the
performance levels that could be achieved on this
task using human-annotated sentences, which we
will refer to as sentence segmentation. Figure 3 re-
ports the baseline tagging accuracy on sentence seg-
ments, and we see significant improvements across
all models. The HMM Trigram tagger performance
increases to 93.00%, while the increase in accuracy
for the other models ranges from around 0.2-0.3%.
The HMM-LA taggers once again achieve the best
performance, with the Stanford bidirectional close
behind. Although the addition of prosody has very
little impact on either the generative or discrimina-
tive models when applied to sentences, the base-
line tagging models (i.e., not prosodically enriched)
significantly outperform all of the prosodically en-
riched models operating on conversation sides.
At this point, it would be apt to suggest us-
ing automatic sentence boundary detection to cre-
ate shorter segments. Table 4 presents the results
of using baseline models without prosodic enrich-
ment trained on the human-annotated sentences to
tag automatically segmented speech5. As can be
seen, the results are quite similar to the conversation
side segmentation performances, and thus signifi-
cantly lower than when tagging human-annotated
sentences. A caveat to consider here is that we break
the standard assumption that the training and test set
be drawn from the same distribution, since the train-
ing data is human-annotated and the test is automat-
ically segmented. However, it can be quite challeng-
ing to create a corpus to train on that represents the
biases of the systems that perform automatic sen-
tence segmentation. Instead, we will examine an-
5We used the Baseline Structural Metadata System de-
scribed in Harper et al (2005) to predict sentence boundaries.
825
93
93.3
93.6
93.9
94.2
94.5
HMM-LA HMM-LA Bidir Stanford Bidir Stanford Left5 CRF
Baseline Prosody OracleBreak Rescoring
Figure 3: Tagging accuracy on human-annotated segments
other segmentation method to shorten the segments
automatically, i.e., by training and testing on speaker
turns, which preserves the train-test match, in Sec-
tion 5.5.
Model Accuracy
HMM-LA 93.95
HMM-LA-Bidir 94.07
Stanford Bidir 93.77
Stanford Left5 93.35
CRF 93.29
Table 4: Baseline tagging accuracy on automatically de-
tected sentence boundaries
5.3 Oracle Break Insertion
As we believe one of the major roles that prosodic
cues serve for tagging conversation sides is as a
proxy for sentence boundaries, perhaps the efficacy
of the prosodic breaks can, at least partially, be at-
tributed to errors in the automatically induced break
indexes themselves, as they can misalign with syn-
tactic phrase boundaries, as discussed in Huang and
Harper (2010). This may degrade the performance
of our models more than the improvement achieved
from correctly placed breaks. Hence, we conduct
a series of experiments in which we systematically
eliminate noisy phrase and disfluency breaks and
show that under these improved conditions, prosodi-
cally enriched models can indeed be more effective.
To investigate to what extent noisy breaks are im-
peding the possible improvements from prosodically
enriched models, we replaced all 4 and p breaks in
the training and evaluation sets that did not align
to the correct phrase boundaries as indicated by the
treebank with break 1 for both the conversation sides
and human-annotated sentences. The results from
using Oracle Breaks on conversation sides can be
seen in Figure 2. All models except Stanford Left5
and HMM-LA-Bidir significantly improve in accu-
racy when trained and tested on the Oracle Break
modified data. On human-annotated sentences, Fig-
ure 3 shows improvements in accuracies across all
models, however, they are statistically insignificant.
To further analyze why prosodically enriched
models achieve more improvement on conversation
sides than on sentences, we conducted three more
Oracle experiments on conversation sides. For the
first, OracleBreak-Sent, we further modified the data
such that all breaks corresponding to a sentence
ending in the human-annotated segments were con-
verted to break 1, thus effectively only leaving in-
side sentence phrasal boundaries. This modification
results in a significant drop in performance, as can
be seen in Figure 2.
For the second, OracleSent, we converted all
the breaks corresponding to a sentence end in the
human-annotated segmentations to break 4, and all
the others to break 1, thus effectively only leaving
sentence boundary breaks. This performed largely
on par with OracleBreak, suggesting that the phrase-
aligned prosodic breaks seem to be a stand-in for
sentence boundaries.
Finally, in the last condition, OracleBreak+Sent,
we modified the OracleBreak data such that all
breaks corresponding to a sentence ending in the
human-annotated sentences were converted to break
826
93
93.3
93.6
93.9
94.2
94.5
HMM-LA HMM-LA Bidir Stanford Bidir Stanford Left5 CRF
Baseline Prosody Rescoring
Figure 4: Tagging accuracy on speaker turns
4 (essentially combining OracleBreak and Oracle-
Sent). As Figure 2 indicates, this modification re-
sults in the best tagging accuracies for all the mod-
els. All models were able to match or even improve
upon the baseline accuracies achieved on the human
segmented data. This suggests that when we have
breaks that align with phrasal and sentence bound-
aries, prosodically enriched models are highly effec-
tive.
5.4 N-best Rescoring
Based on the findings in the previous section and the
findings of (Huang and Harper, 2010), we next ap-
ply a rescoring strategy in which the search space
of the prosodically enriched generative models is re-
stricted to the n-best list generated from the base-
line model (without prosodic enrichment). In this
manner, the prosodically enriched model can avoid
poor tag sequences produced due to the misaligned
break indexes. As Figure 2 shows, using the base-
line conversation side model to produce an n-best
list for the prosodically enriched model to rescore
results in significant improvements in performance
for the HMM-LA model, similar to the parsing re-
sults of (Huang and Harper, 2010). The size of the
n-best list directly impacts performance, as reducing
to n = 1 is akin to tagging with the baseline model,
and increasing n ? ? amounts to tagging with the
prosodically enriched model. We experimented with
a number of different sizes for n and chose the best
one using the dev set. Figure 3 presents the results
for this method applied to human-annotated sen-
tences, where it produces only marginal improve-
ments6.
5.5 Speaker turn segmentation
The results presented thus far indicate that if we
have access to close to perfect break indexes, we
can use them effectively, but this is not likely to be
true in practice. We have also observed that tagging
accuracy on shorter conversation sides is greater
than longer conversation sides, suggesting that post-
processing the conversation sides to produce shorter
segments would be desirable.
We thus devised a scheme by which we could
automatically extract shorter speaker turn segments
from conversation sides. For this study, speaker
turns, which effectively indicate speaker alterna-
tions, were obtained by using the metadata in the
treebank to split the sentences into chunks based on
speaker change. Every time a speaker begins talk-
ing after the other speaker was talking, we start a
new segment for that speaker. In practice, this would
need to be done based on audio cues and automatic
transcriptions, so these results represent an upper
bound.
Figure 4 presents tagging results on speaker turn
segments. For most models, the difference in accu-
racy achieved on these segments and that of human-
annotated sentences is statistically insignificant. The
only exception is the Stanford bidirectional tagger,
6Rescoring using the CRF model was also performed, but
led to a performance degradation. We believe this is due to
the fact that the prosodically enriched CRF model was able to
directly use the break index information, and so restricting it to
the baseline CRF model search space limits the performance to
that of the baseline model.
827
0100
200
300
400
NNP RP AUX JJ PRP RB WDT VBP VBZ UH XX VB NN DT VBD IN
Number
 o
f Er
ro
rs
 Conv Baseline Conv Rescore Conv OracleBreak Sent Baseline
(a) Number of errors by part of speech category for the HMM-LA model with and without prosody
0
100
200
300
400
NNP RP AUX JJ PRP RB WDT VBP VBZ UH XX VB NN DT VBD IN
Number
 o
f Er
ro
rs
 Conv Baseline Conv Prosody Conv OracleBreak Sent Baseline
(b) Number of errors by part of speech category for the CRF model with and without prosody
Figure 5: Error reduction for prosodically enriched HMM-LA (a) and CRF (b) models
which performs worse on these slightly longer seg-
ments. With the addition of break indexes, we see
marginal changes in most of the models; only the
CRF tagger receives a significant boost. Thus, mod-
els achieve performance gains from tagging shorter
segments, but at the cost of limited usefulness of the
prosodic breaks. Overall, speaker turn segmenta-
tion is an attractive compromise between the original
conversation sides and human-annotated sentences.
6 Discussion
Across the different models, we have found that tag-
gers applied to shorter segments, either sentences or
speaker turns, do not tend to benefit significantly
from prosodic enrichment, in contrast to conversa-
tion sides. To analyze this further we broke down
the results by part of speech for the two models
for which break indexes improved performance the
most: the CRF and HMM-LA rescoring models,
which achieved an overall error reduction of 2.8%
and 2.1%, respectively. We present those categories
that obtained the greatest benefit from prosody in
Figure 5 (a) and (b). For both models, the UH cate-
gory had a dramatic improvement from the addition
of prosody, achieving up to a 10% reduction in error.
For the CRF model, other categories that saw im-
pressive error reductions were NN and VB, with
10% and 5%, respectively. Table 5 lists the prosodic
features that received the highest weight in the CRF
model. These are quite intuitive, as they seem to rep-
resent places where the prosody indicates sentence
or clausal boundaries. For the HMM-LA model,
the VB and DT tags had major reductions in error
of 13% and 10%, respectively. For almost all cat-
egories, the number of errors is reduced by the ad-
dition of breaks, and further reduced by using the
OracleBreak processing described above.
Weight Feature
2.2212 wi=um & bi=4 & t=UH
1.9464 wi=uh & bi=4 & t=UH
1.7965 wi=yes & bi=4 & t=UH
1.7751 wi=and & bi=4 & t=CC
1.7554 wi=so & bi=4 & t=RB
1.7373 wi=but & bi=4 & t=CC
Table 5: Top break 4 prosody features in CRF prosody
model
To determine more precisely the effect that the
segment size has on tagging accuracy, we extracted
the oracle tag sequences from the HMM-LA and
CRF baseline and prosodically enriched models
across conversation sides, sentences, and speaker
turn segments. As the plot in Figure 6 shows, as
we increase the n-best list size to 500, the ora-
cle accuracy of the models trained on sentences in-
828
92
94
96
98
100
2 5 10 20 50 100 200 500
Ac
cur
ac
y
 
N -Best size  
Sentences 
Speaker  tuns  
Conversation sides  
Figure 6: Oracle comparison: solid lines for sentences,
dashed lines for speaker turns, and dotted lines for con-
versation sides
creases rapidly to 99%; whereas, the oracle accu-
racy of models on conversation sides grow slowly
to between 94% and 95%. The speaker turn trained
models, however, behave closely to those using sen-
tences, climbing rapidly to accuracies of around
98%. This difference is directly attributable to the
length of the segments. As can be seen in Table 6,
the speaker turn segments are more comparable in
length to sentences.
Train Eval
Conv 627.87 ? 281.57 502.98 ? 151.22
Sent 7.52? 7.86 7.45 ? 8.29
Speaker 15.60? 29.66 15.27? 21.01
Table 6: Length statistics of different data segmentations
Next, we return to the large performance degrada-
tion when tagging speech rather than newswire text
to examine the major differences among the mod-
els. Using two of our best performing models, the
Stanford bidirectional and HMM-LA, in Figure 7
we present the categories for which performance
degradation was the greatest when comparing per-
formance of a tagger trained on WSJ to a tagger
trained on spoken sentences and conversation sides.
The performance decrease is quite similar across
both models, with the greatest degradation on the
NNP, RP, VBN, and RBS categories.
Unsurprisingly, both the discriminative and gen-
erative bidirectional models achieve the most im-
pressive results. However, the generative HMM-
LA and HMM-LA-Bidir models achieved the best
results across all three segmentations, and the best
overall result, of 94.35%, on prosodically enriched
sentence-segmented data. Since the Stanford bidi-
rectional model incorporates all of the features that
produced its state-of-the-art performance on WSJ,
we believe the fact that the HMM-LA outperforms
it, despite the discriminative model?s more expres-
sive feature set, is indicative of the HMM-LA?s abil-
ity to more effectively adapt to novel domains during
training. Another challenge for the discriminative
models is the need for regularization tuning, requir-
ing additional time and effort to train several mod-
els and select the most appropriate parameter each
time the domain changes. Whereas for the HMM-
LA models, although we also train several models,
they can be combined into a product model, such as
that described by Petrov (2010), in order to further
improve performance.
Since the prosodic breaks are noisier features than
the others incorporated in the discriminative models,
it may be useful to set their regularization param-
eter separately from the rest of the features, how-
ever, we have not explored this alternative. Our ex-
periments used human transcriptions of the conver-
sational speech; however, realistically our models
would be applied to speech recognition transcripts.
In such a case, word error will introduce noise in ad-
dition to the prosodic breaks. In future work, we will
evaluate the use of break indexes for tagging when
there is lexical error. We would also apply the n-
best rescoring method to exploit break indexes in the
HMM-LA bidirectional model, as this would likely
produce further improvements.
7 Conclusion
In this work, we have evaluated factors that are im-
portant for developing accurate tagging models for
speech. Given that prosodic breaks were effective
knowledge sources for parsing, an important goal
of this work was to evaluate their impact on vari-
ous tagging model configurations. Specifically, we
have examined the use of prosodic information for
tagging conversational speech with several different
discriminative and generative models across three
different speech transcript segmentations. Our find-
829
0%
10%
20%
30%
40%
50%
NNP VBN WP CD RP EX WRB WDT JJR POS JJS RBS
Err
or
 R
at
e
 
WSJ (Stanford-Bidir) WSJ (HMM-LA)
Sent (Stanford-Bidir) Sent (HMM-LA)
Conv (Stanford-Bidir) Conv (HMM-LA)
Figure 7: Comparison of error rates between the Standford Bidir and HMM-LA models trained on WSJ, sentences,
and conversation sides
ings suggest that generative models with latent an-
notations achieve the best performance in this chal-
lenging domain. In terms of transcript segmenta-
tion, if sentences are available, it is preferable to use
them. In the case that no such annotation is avail-
able, then using automatic sentence boundary detec-
tion does not serve as an appropriate replacement,
but if automatic speaker turn segments can be ob-
tained, then this is a good alternative, despite the fact
that prosodic enrichment is less effective.
Our investigation also shows that in the event that
conversation sides must be used, prosodic enrich-
ment of the discriminative and generative models
produces significant improvements in tagging accu-
racy (by direct integration of prosody features for
the former and by restricting the search space and
rescoring with the latter). For tagging, the most im-
portant role of the break indexes appears to be as a
stand in for sentence boundaries. The oracle break
experiments suggest that if the accuracy of the au-
tomatically induced break indexes can be improved,
then the prosodically enriched models will perform
as well, or even better, than their human-annotated
sentence counterparts.
8 Acknowledgments
This research was supported in part by NSF IIS-
0703859 and the GALE program of the Defense
Advanced Research Projects Agency, Contract No.
HR0011-06-2-001. Any opinions, findings, and rec-
ommendations expressed in this paper are those of
the authors and do not necessarily reflect the views
of the funding agency or the institutions where the
work was completed.
References
Anton Batliner, Bernd Mo?bius, Gregor Mo?hler, Antje
Schweitzer, and Elmar No?th. 2001. Prosodic models,
automatic speech understanding, and speech synthesis:
toward the common ground. In Eurospeech.
Ann Bies, Stephanie Strassel, Haejoong Lee, Kazuaki
Maeda, Seth Kulick, Yang Liu, Mary Harper, and
Matthew Lease. 2006. Linguistic resources for speech
parsing. In LREC.
Stanley F. Chen and Ronald Rosenfeld. 1999. A Gaus-
sian prior for smoothing maximum entropy models.
Technical report, Technical Report CMU-CS-99-108,
Carnegie Mellon University.
Anne Cutler, Delphine Dahan, and Wilma v an Donselaar.
1997. Prosody in comprehension of spoken language:
A literature review. Language and Speech.
Markus Dreyer and Izhak Shafran. 2007. Exploiting
prosody for PCFGs with latent annotations. In Inter-
speech.
Denis Filimonov and Mary Harper. 2009. A joint
language model with fine-grain syntactic tags. In
EMNLP.
Jennifer Foster. 2010. ?cba to check the spelling?: Inves-
tigating parser performance on discussion forum posts.
In NAACL-HLT.
Florian Gallwitz, Heinrich Niemann, Elmar No?th, and
Volker Warnke. 2002. Integrated recognition of words
and prosodic phrase boundaries. Speech Communica-
tion.
John J. Godfrey, Edward C. Holliman, and Jane Mc-
Daniel. 1992. SWITCHBOARD: Telephone speech
corpus for research and development. In ICASSP.
Michelle L. Gregory, Mark Johnson, and Eugene Char-
niak. 2004. Sentence-internal prosody does not help
parsing the way punctuation does. In NAACL.
Mary P. Harper, Bonnie J. Dorr, John Hale, Brian Roark,
Izhak Shafran, Matthew Lease, Yang Liu, Matthew
Snover, Lisa Yung, Anna Krasnyanskaya, and Robin
Stewart. 2005. 2005 Johns Hopkins Summer Work-
shop Final Report on Parsing and Spoken Structural
830
Event Detection. Technical report, Johns Hopkins
University.
Mark Hasegawa-Johnson, Ken Chen, Jennifer Cole,
Sarah Borys, Sung suk Kim, Aaron Cohen, Tong
Zhang, Jeung yoon Choi, Heejin Kim, Taejin Yoon,
and Ra Chavarria. 2005. Simultaneous recognition
of words and prosody in the boston university radio
speech corpus. speech communication. Speech Com-
munication.
Peter A. Heeman. 1999. POS tags and decision trees for
language modeling. In EMNLP.
Dustin Hillard, Zhongqiang Huang, Heng Ji, Ralph Gr-
ishman, Dilek Hakkani-Tur, Mary Harper, Mari Os-
tendorf, and Wen Wang. 2006. Impact of automatic
comma prediction on POS/name tagging of speech. In
ICASSP.
Zhongqiang Huang and Mary Harper. 2010. Appropri-
ately handled prosodic breaks help PCFG parsing. In
NAACL.
Zhongqiang Huang, Vladimir Eidelman, and Mary
Harper. 2009. Improving a simple bigram hmm part-
of-speech tagger by latent annotation and self-training.
In NAACL-HLT.
Jeremy G. Kahn, Matthew Lease, Eugene Charniak,
Mark Johnson, and Mari Ostendorf. 2005. Effective
use of prosody in parsing conversational speech. In
EMNLP-HLT.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic models
for segmenting and labeling sequence data. In ICML.
D. C. Liu and Jorge Nocedal. 1989. On the limited mem-
ory BFGS method for large scale optimization. Math-
ematical Programming.
Yang Liu, Andreas Stolcke, Elizabeth Shriberg, and Mary
Harper. 2005. Using conditional random fields for
sentence boundary detection in speech. In ACL.
Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii.
2005. Probabilistic CFG with latent annotations. In
ACL.
Mari Ostendorf, Izhak Shafran, and Rebecca Bates.
2003. Prosody models for conversational speech
recognition. In Plenary Meeting and Symposium on
Prosody and Speech Processing.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In HLT-NAACL.
Slav Petrov. 2010. Products of random latent variable
grammars. In HLT-NAACL.
Brian Roark, Yang Liu, Mary Harper, Robin Stewart,
Matthew Lease, Matthew Snover, Izhak Shafran, Bon-
nie Dorr, John Hale, Anna Krasnyanskaya, and Lisa
Yung. 2006. Reranking for sentence boundary detec-
tion in conversational speech. In ICASSP.
Kim Silverman, Mary Beckman, John Pitrelli, Mari Os-
tendorf, Colin Wightman, Patti Price, Janet Pierrehum-
bert, and Julia Hirshberg. 1992. ToBI: A standard for
labeling English prosody. In ICSLP.
Paul Taylor and Alan W. Black. 1998. Assigning
phrase breaks from part-of-speech sequences. Com-
puter Speech and Language.
Scott M. Thede and Mary P. Harper. 1999. A second-
order hidden markov model for part-of-speech tag-
ging. In ACL.
Kristina Toutanova and Christopher D. Manning. 2000.
Enriching the knowledge sources used in a maximum
entropy part-of-speech tagger. In EMNLP.
Kristina Toutanova, Dan Klein, Christopher D. Manning,
and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In NAACL.
831
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 556?566,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Factored Soft Source Syntactic Constraints for Hierarchical Machine
Translation
Zhongqiang Huang
Raytheon BBN Technologies
50 Moulton St
Cambridge, MA, USA
zhuang@bbn.com
Jacob Devlin
Raytheon BBN Technologies
50 Moulton St
Cambridge, MA, USA
jdevlin@bbn.com
Rabih Zbib
Raytheon BBN Technologies
50 Moulton St
Cambridge, MA, USA
rzbib@bbn.com
Abstract
This paper describes a factored approach to
incorporating soft source syntactic constraints
into a hierarchical phrase-based translation
system. In contrast to traditional approaches
that directly introduce syntactic constraints to
translation rules by explicitly decorating them
with syntactic annotations, which often ex-
acerbate the data sparsity problem and cause
other problems, our approach keeps transla-
tion rules intact and factorizes the use of syn-
tactic constraints through two separate mod-
els: 1) a syntax mismatch model that asso-
ciates each nonterminal of a translation rule
with a distribution of tags that is used to
measure the degree of syntactic compatibil-
ity of the translation rule on source spans; 2)
a syntax-based reordering model that predicts
whether a pair of sibling constituents in the
constituent parse tree of the source sentence
should be reordered or not when translated to
the target language. The features produced
by both models are used as soft constraints
to guide the translation process. Experiments
on Chinese-English translation show that the
proposed approach significantly improves a
strong string-to-dependency translation sys-
tem on multiple evaluation sets.
1 Introduction
Hierarchical phrase-based translation models (Chi-
ang, 2007) are widely used in machine translation
systems due to their ability to achieve local flu-
ency through phrasal translation and handle non-
local phrase reordering using synchronous context-
free grammars. A large number of previous works
have tried to introduce grammaticality to the trans-
lation process by incorporating syntactic constraints
into hierarchical translation models. Despite some
differences in the granularity of syntax units (e.g.,
tree fragments (Galley et al, 2004; Liu et al, 2006),
treebank tags (Shen et al, 2008; Chiang, 2010), and
extended tags (Zollmann and Venugopal, 2006)),
most previous work incorporates syntax into hier-
archical translation models by explicitly decorating
translation rules with syntactic annotations. These
approaches inevitably exacerbate the data sparsity
problem and cause other problems such as increased
grammar size, worsened derivational ambiguity, and
unavoidable parsing errors (Hanneman and Lavie,
2013).
In this paper, we propose a factored approach
that incorporates soft source syntactic constraints
into a hierarchical string-to-dependency translation
model (Shen et al, 2008). The general ideas are ap-
plicable to other hierarchical models as well. Instead
of enriching translation rules with explicit syntactic
annotations, we keep the original translation rules
intact, and factorize the use of source syntactic con-
straints through two separate models.
The first is a syntax mismatch model that intro-
duces source syntax into the nonterminals of transla-
tion rules, and measures the degree of syntactic com-
patibility between a translation rule and the source
spans it is applied to during decoding. When a hi-
erarchical translation rule is extracted from a par-
allel training sentence pair, we determine a tag for
each nonterminal based on the dependency parse of
the source sentence. Instead of fragmenting rule
statistics by directly labeling nonterminals with tags,
556
we keep the original string-to-dependency transla-
tion rules intact and associate each nonterminal with
a distribution of tags. That distribution is then used
to measure the syntactic compatibility between the
syntactic context from which the translation rule is
extracted and the syntactic analysis of a test sen-
tence.
The second is a syntax-based reordering model
that takes advantage of phrasal cohesion in transla-
tion (Fox, 2002). The reordering model takes a pair
of sibling constituents in the source parse tree as in-
put, and uses source syntactic clues to predict the
ordering distribution (straight vs. inverted) of their
translations on the target side. The resulting order-
ing distribution is used in the decoder at the word
pair level to guide the translation process. This sep-
arate reordering model allows us to utilize source
syntax to improve reordering in hierarchical trans-
lation models without having to explicitly annotate
translation rules with source syntax.
Our results show that both the syntax mismatch
model and the syntax-based reordering model are
able to achieve significant gains over a strong
Chinese-English MT baseline. The rest of the pa-
per is organized as follows. Section 2 discusses
related work in the literature. Section 3 provides
an overview of our baseline string-to-dependency
translation system. Section 4 describes the details
of the syntax mismatch and syntax-based reordering
models. Experimental results are presented in Sec-
tion 5. The last section concludes the paper.
2 Related Work
Attempts to use rich syntactic annotations do not
always result in improved performance when com-
pared to purely hierarchical models that do not
use linguistic guidance. For example, as shown
in (Mi and Huang, 2008), tree-to-string translation
models (Huang et al, 2006) only start to outper-
form purely hierarchical models when significant ef-
forts were made to alleviate parsing errors by using
forest-based approaches in both rule extraction and
decoding. Using only syntactic phrases is too re-
strictive in phrasal translation as many useful phrase
pairs are not syntactic constituents (Koehn et al,
2003). The syntax-augmented translation model
of Zollmann and Venugopal (2006) annotates non-
terminals in hierarchical rules with thousands of ex-
tended syntactic categories in order to capture the
syntactic variations of phrase pairs. This results
in exacerbated data sparsity problems, partially due
to the requirement of exact matches in nonterminal
substitutions between translation rules in the deriva-
tion. Several solutions were proposed. Shen et
al. (2009) and Chiang (2010) used soft match fea-
tures to explicitly model the substitution of nonter-
minals with different labels; Venugopal et al (2009)
used a preference grammar to soften the syntactic
constraints through the use of a preference distribu-
tion of syntactic categories; and recently Hanneman
and Lavie (2013) proposed a clustering approach
to reduce the number of syntactic categories. Our
proposed syntax mismatch model associates non-
terminals with a distribution of tags. It is simi-
lar to the preference grammar in (Venugopal et al,
2009); however, we use treebank tags and focus on
the syntactic compatibility between translation rules
and the source sentence. The work of Huang et al
(2010) is most similar to ours, with the main differ-
ence being that their syntactic categories are latent
and learned automatically in a data driven fashion
while we simply use treebank tags based on depen-
dency parsing. Marton and Resnik (2008) also ex-
ploited soft source syntax constraints without mod-
ifying translation rules. However, they focused on
the quality of translation spans based on the syn-
tactic analysis of the source sentence, while our
method explicitly models the syntactic compatibil-
ity between translation rules and source spans.
Most research on reordering in machine transla-
tion focuses on phrase-based translation models as
they are inherently weak at non-local reordering.
Previous efforts to improve reordering for phrase-
based systems can be largely classified into two cat-
egories. Approaches in the first category try to re-
order words in the source sentence in a preprocess-
ing step to reduce reordering in both word alignment
and MT decoding. The reordering decisions are ei-
ther made using manual or automatically learned
rules (Collins et al, 2005; Xia and McCord, 2004;
Xia and McCord, 2004; Genzel, 2010) based on the
syntactic analysis of the source sentence, or con-
structed through an optimization procedure that uses
feature-based reordering models trained on a word-
aligned parallel corpus (Tromble and Eisner, 2009;
557
Khapra et al, 2013). Approaches in the second cate-
gory try to explicitly model phrase reordering in the
translation process. These approaches range from
simple distance based distortion models (Koehn et
al., 2003) that globally penalizes reordering based
on the distorted distance, to lexicalized reordering
models (Koehn et al, 2005; Al-Onaizan and Pap-
ineni, 2006) that assign reordering preferences of
adjacent phrases for individual phrases, and to hi-
erarchical reordering models (Galley and Manning,
2008; Cherry, 2013) that handle reordering prefer-
ences beyond adjacent phrases. Although hierarchi-
cal translation models are capable of handling non-
local reordering, their accuracy is far from perfect.
Xu et al (2009) showed that the syntax-augmented
hierarchical model (Zollmann and Venugopal, 2006)
also benefits from reordering source words in a pre-
processing step. Explicitly adding syntax to trans-
lation rules helps with reordering in general, but it
introduces additional complexities, and is still lim-
ited by the context-free nature of hierarchical rules.
Our work exploits an alternative direction that uses
an external reordering model to improve word re-
ordering of hierarchical models. Gao et al (2011),
Xiong et al (2012), and Li et al (2013) also studied
external reordering models for hierarchical models.
However, they focused on specific word pairs such
as a word and its dependents or a predicate and its
arguments, while our proposed general framework
considers all word pairs in a sentence. Our syntax-
based reordering model exploits phrasal cohesion in
translation (Fox, 2002) by modeling the reordering
of sibling constituents in the source parse tree, which
is similar to the recent work of Yang et al (2012).
However, the latter focuses on finding the optimal
reordering of sibling constituents before MT decod-
ing, while our proposed model generates reordering
features that are used together with other MT fea-
tures to determine the optimal reordering during MT
decoding.
3 String-to-Dependency Translation
Our baseline translation system is based on a string-
to-dependency translation model similar to the im-
plementation in (Shen et al, 2008). It is an extension
of the hierarchical translation model of Chiang et al
(2006) that requires the target side of a phrase pair
to have a well-formed dependency structure, defined
as either of the two types:
? fixed structure: a single rooted dependency
sub-tree with each child being a complete con-
stituent. In this case, the phrase has a unique
head word inside the phrase, i.e., the root of
the dependency sub-tree. Each dependent of
the head word, together with all of its descen-
dants, is either completely inside the phrase or
completely outside the phrase. For example,
the phrase give him in Figure 1 (a) has a fixed
dependency structure with head word give.
? floating structure: a sequence of siblings with
each being a complete constituent. In this case,
the phrase is composed of a sequence of sibling
constituents whose common parent is outside
the phrase. For example, the phrase him that
brown coat in Figure 1 is a floating structure
whose common parent give is not in the phrase.
Requiring the target side to have a well-formed
dependency structure is less restrictive than requir-
ing it to be a syntactic constituent, allowing more
translation rules to be extracted. However, it still
results in fewer rules than pure hierarchical transla-
tion models and might hurt MT performance. The
well-formed dependency structure on the target side
makes it possible to introduce syntax features dur-
ing decoding. Shen et al (2008) obtained signif-
icant improvements from including a dependency
language model score in decoding, outweighing the
negative effect of the dependency constraint. Shen et
al. (2009) proposed an approach to label each non-
terminal, which can be either on the left-hand-side
(LHS) or the right-hand-side (RHS) of the rule, with
the head POS tag of the underlying target phrase if
it has a fixed dependency structure1, and measure
the mismatches between nonterminal labels when a
RHS nonterminal of a rule is substantiated with the
LHS nonterminal of another rule during decoding.
This also resulted in further improvements in MT
performance. Figure 1 (c) shows an example string-
to-dependency translation rule in our baseline sys-
tem.
1Nonterminals corresponding to floating structures keep
their default label ?X? as experiments show that it is not bene-
ficial to label them differently.
558
X :  give  X
2 
 X
1
 
X  :  X
1
    X
2
(b) pure hierarchical rule
VV  :  give  PRP
2  
NN
1
 
X  :  X
1
    X
2
(c) string-to-dependency rule
??  ??  ?  ??    ?
 give   him
   
that  brown  coat 
(a) word alignments
Figure 1: An example of extracting a string-to-
dependency translation rule from word alignments. The
nonterminals on the target side of the hierarchical rule
(b) all correspond to fixed dependency structures and so
they are labeled by the respective head tag in the string-
to-dependency rule (c).
4 Factored Syntactic Constraints
Although the string-to-dependency formulation
helps to improve the grammaticality of translations,
it lacks the ability to incorporate source syntax into
the translation process. We next describe a factored
approach to address this problem by utilizing source
syntax through two models: one that introduces syn-
tactic awareness to translation rules themselves, and
another that focuses on reordering based on the syn-
tactic analysis of the source.
4.1 Syntax Mismatch Model
A straightforward method to introduce awareness
of source syntax to translation rules is to apply
the same well-formed dependency constraint and
head POS annotation on the target side of string-
to-dependency translation rules to the source side.
However, as discussed earlier, this would signifi-
cantly reduce the number of rules that can be ex-
tracted, exacerbate data sparsity, and cause other
problems, especially given that the target side is al-
ready constrained by the dependency requirement.
A relaxed method is to bypass the dependency
constraint and only annotate source nonterminals
whose underlying phrase is a fixed dependency
structure with the head POS tag of the phrase. This
method would still extract all of the rules that can
be extracted from the baseline string-to-dependency
VV  :  give  PRP
2  
NN
1
 
X  :  X
1
    X
2
2
4
VV : 0.7
NN : 0.1
X : 0.2
3
5
2
4
NN : 0.8
VV : 0.1
X : 0.1
3
5
2
4
PN : 0.5
NN : 0.4
X : 0.1
3
5
VV
(a) nonterminal tag distributions
source:
gross:
NN
PN
span tag:
(b) source span tags

?
his
? ?
pen
?
me


give
dependency:
Figure 2: Example distribution of tags for nonterminals
on the source side (a) and example tags for source spans
(b)
translation model, but the extra annotation on non-
terminals can split a rule into multiple rules, with the
only difference being the nonterminal labels on the
source side. Unfortunately, our experiments have
shown that even this moderate annotation results
in significantly lower translation quality due to the
fragmentation of translation rules, and the increased
derivational ambiguity. We have also tried to include
some source tag mismatch features (with details de-
scribed later) to measure the syntactic compatibility
between the nonterminal labels of a translation rule
and the corresponding tags of source spans. This im-
proves translation accuracy, but not enough to com-
pensate for the performance drop caused by annotat-
ing source nonterminals.
Our proposed method introduces syntax to trans-
lation rules without sacrificing performance. Instead
of imposing dependency constraints or explicitly an-
notating source nonterminals, we keep the original
string-to-dependency translation rules intact and as-
sociate each nonterminal on the source side with a
distribution of tags. The tags are determined based
on the dependency structure of training samples. If
the underlying source phrase of a nonterminal is a
fixed dependency structure in a training sample, we
use the head POS tag of the phrase as the tag. Oth-
erwise, we use the default tag ?X? to denote float-
559
Feature Condition Value
f1 ts = X P(tr = X)
f2 ts = X P(tr ? X)
f3 ts ? X P(tr = X)
f4 ts ? X P(tr = ts)
f5 ts ? X P(tr ? X, tr ? ts)
Table 1: Source tag mismatch features. The default value
of each feature is zero if the source span tag ts does not
match the condition
ing structures and dependency structures that are not
well formed. As a result, we still extract the same
set of rules as in the baseline string-to-dependency
translation model, and also obtain a distribution of
tags for each nonterminal. Figure 2 (a) illustrates the
example tag distributions of a string-to-dependency
translation rule. The tag distributions provide an ap-
proximation of the source syntax of the training data
from which the translation rules are extracted. They
are used to measure the syntactic compatibility be-
tween a translation rule and the source spans it is
applied to. At decoding time, we parse the source
sentence and assign each span a tag in the same way
as it is done during rule extraction, as shown in the
example in Figure 2 (b). When a translation rule is
used to expand a derivation, for each nonterminal
(which can be on the LHS or RHS) on the source
side of the rule, five source tag mismatch features
are computed based on the distribution of tags P(tr)
on the rule nonterminal, and the tag ts on the cor-
responding source span. The features are defined in
Table 1. We use soft features instead of hard syn-
tactic constraints, and allow the tuning process to
choose the appropriate weight for each feature. As
shown in Section 5, these source syntax mismatch
features help to improve the baseline system.
4.2 Syntax-based Reordering Model
Most previous research on reordering models has fo-
cused on improving word reordering for statistical
phrase-based translation systems (e.g., (Collins et
al., 2005; Al-Onaizan and Papineni, 2006; Tromble
and Eisner, 2009)). There has been less work on im-
proving the reordering of hierarchical phrase-based
translation systems (see (Xu et al, 2009; Gao et al,
2011; Xiong et al, 2012) for a few exceptions), ex-
cept through explicit syntactic annotation of transla-
tion rules. It is generally assumed that hierarchical
models are inherently capable of handling both lo-
cal and non-local reorderings. However, many hier-
archical translation rules are noisy and have limited
context, and so may not be able to produce transla-
tions in the right order.
We propose a general framework that incorpo-
rates external reordering information into the decod-
ing process of hierarchical translation models. To
simplify the presentation, we make the assumption
that every source word translates to one or more
target words, and that the translations for a pair
of source words is either straight or inverted. We
discuss the general case later. Given a sentence
w1,?,wn, suppose we have a separate reordering
model that predicts Porder(oij), the probability distri-
bution of ordering oij ? {straight, inverted} between
the translations of any source word pair (wi,wj).
We can measure the goodness of a given hypothe-
sis h with respect to the ordering predicted by the
reordering model as the sum of log probabilities2
for ordering each pair of source words, as defined
in Equation 1:
forder(h) = ?
1?i<j?n
log Porder(oij = ohij) (1)
where ohij is the ordering between the translations of
source word pair (wi,wj) in hypothesis h. The re-
ordering score forder(h) can be computed efficiently
through recursion during hierarchical decoding as
follows:
? Base case: for phrasal (i.e. non-hierarchical)
rules, the ordering of translations for any word
pair covered by the source phrase can be deter-
mined based on the word alignment of the rule.
The value of the reordering score can be simply
computed according to Equation 1.
? Recursive case: when a hierarchical rule is used
to expand a partial derivation, two types of
word pairs are encountered: a) word pairs that
are covered exclusively by one of the nonter-
minals on the RHS of the rule, and b) other
2In practice, the log probability is thresholded to avoid neg-
ative infinity, which would otherwise result in a hard constraint.
560
(a)
VV  :  give  PRP
2  
NN
1
 
X  :  X
1
    X
2
source:
gross:
 
his
 
pen

me

give
(b)
word pair translation order
(?, ) inverted
(?, ?) inverted
(?, ) inverted
(?, ?) inverted
(?, ) inverted
(?, ?) inverted
(, ?) straight
(?, ?) previously considered
(?, ?) previously considered
(?, ?) previously considered
Figure 3: An example rule application (a) with the trans-
lation order of new source word pairs covered by the rule
shown in (b). The translation order of word pairs covered
by X1 is previously considered and is thus not shown.
word pairs. The reordering scores of the for-
mer would be already computed in previous
rule applications, and can simply be retrieved
from the partial derivation. Word pairs of the
latter case are new word pairs introduced by the
hierarchical rule, and their ordering can be de-
termined based on the alignment of the hierar-
chical rule. The value of the reordering score
of the new derivation is the sum of the reorder-
ing scores retrieved from the partial derivations
for the nonterminals and the reordering scores
of the new word pairs.
Figure 3 shows an example of determining the
ordering of translations when applying a string-to-
dependency rule. The alignment in the translation
rule is able to fully determine the translation order
for all new word pairs introduced by the rule. For
example, ??/pen? is covered by X1 in the rule and
the translation order for X1 and ??/give? is inverted
on the target side. Since ??/pen? is translated to-
gether with other words covered by X1 as a group,
we can determine that the translation order between
the source word pair ??/pen? and ??/give? is also
inverted on the target side. The words ??/his?,
???, ?? /pen? are all covered by the same nonter-
Reordering features
The syntactic production rule
The syntactic labels of the nodes in the context
The head POS tags of the nodes in the context
The dep. labels of the nodes in the context
The seq. of dep. labels connecting the two nodes
The length of the nodes in the context
Table 2: Features in the reordering model
minal X1 and thus their pairwise reordering scores
have already been considered in previous rule appli-
cations.
In practice, not all source words in a translation
rule are translated to a target word; sometimes there
is no clear ordering between the translations of two
source words. In such cases we use a binary discount
feature instead of the reordering feature.
This reordering framework relies on an external
model to provide the ordering probability distribu-
tion of source word pairs. In this paper, we inves-
tigate a simple maximum-entropy reordering model
based on the syntactic parse tree of the source sen-
tence. This allows us to take advantage of the source
syntax to improve reordering without using syntactic
annotations in translation rules. The syntax-based
reordering model attempts to predict the reordering
probability of a pair of sibling constituents in the
source parse tree, building on the fact that syntac-
tic phrases tend to move in a group during transla-
tion (Fox, 2002). The reordering model is trained on
a word-aligned corpus. For each pair of sibling con-
stituents in the source parse tree, we determine the
translation order on the target side based on word
alignments. If there is a clear ordering3, i.e., either
straight or inverted, on the target side, we include
the context of the constituent pair and its translation
order as a sample for training or evaluating the max-
imum entropy reordering model. Table 2 lists the
features of the reordering model.
The ordering distributions of source word pairs
are determined based on the ordering distributions
of sibling constituent pairs. For each pair of sib-
3If the translations overlap with other, the non-overlapping
parts are used to determine the translation order.
561
ling constituents4 in the parse tree of a source sen-
tence, we compute its distribution of translation or-
der using the reordering model. The distribution is
shared among all word pairs covered by the respec-
tive constituents, which guarantees that the order-
ing distribution of any source word pair is computed
exactly once. The ordering distributions of source
word pairs are then used through the general reorder-
ing framework in the decoder to guide the decoding
process.
5 Experiments
5.1 Experimental Setup
Our main experiments use the Chinese-English par-
allel training data and development sets released by
the LDC, and made available to the DARPA GALE
and BOLT programs. We train the translation model
on 100 million words of parallel data. We use a 8 bil-
lion words of English monolingual data to train two
language models: a trigram language model used in
chart decoding, and a 5-gram language model used
in n-best rescoring. The systems are tuned and eval-
uated on a mixture of newswire and web forum text
from the development sets available for the DARPA
GALE and BOLT programs, with up to 4 indepen-
dent references for each source sentence. We also
evaluate our final systems on both newswire and
web text from the NIST MT06 and MT08 evalua-
tions using an experimental setup compatible with
the NIST MT12 Chinese-English constrained track.
In this setup, the translation and language models
are trained on 35 million words of parallel data and
3.8 billion words of English monolingual data, re-
spectively. The systems are tuned on the MT02-
05 development sets. All systems are tuned and
evaluated on IBM BLEU (Papineni et al, 2002).
The baseline string-to-dependency translation sys-
tem uses more than 10 core features and a large num-
ber of sparse binary features similar to the method
described in (Chiang et al, 2009). It achieves trans-
lation accuracies comparable to the top ranked sys-
tems in the NIST MT12 evaluation.
4Note that the constituent pairs used to train the reordering
model are filtered to only contain these with clear ordering on
the target side, while no such pre-filtering is applied to con-
stituent pairs when applying the reordering model in translation.
We leave it to future work to address this mismatch problem.
GIZA++ (Och and Ney, 2003) is used for auto-
matic word alignment in all of the experiments. We
use Charniak?s parser (Charniak and Johnson, 2005)
on the English side to obtain string-to-dependency
translation rules, and use a latent variable PCFG
parser (Huang and Harper, 2009) to parse the source
side of the parallel training data as well as the
test sentences for extracting syntax mismatch and
reordering features. For both languages, depen-
dency structures are read off constituency trees us-
ing manual head word percolation rules. We use
a lexicon-based longest-match-first word segmenter
to tokenize source Chinese sentences. Since the
source tokenization used in our MT system is dif-
ferent from the treebank tokenization used to train
the Chinese parser, the source sentences are first to-
kenized using the treebank-trained Stanford Chinese
segmenter (Tseng et al, 2005), then parsed with
the Chinese parser, and finally projected to MT tok-
enization based on the character alignment between
the tokens. The syntax-based reordering model is
trained on a set of Chinese-English manual word
alignment corpora released by the LDC5.
5.2 Syntax Mismatch Model
We first conduct experiments on the GALE/BOLT
data sets to evaluate different strategies of incor-
porating source syntax into string-to-dependency
translation rules. As mentioned in Section 4.1, con-
straining the source side of translation rules to only
well-formed dependency structures is too restrictive
given that our baseline system already has depen-
dency constraint on the target side. We evaluate
the relaxed method that only annotates source non-
terminals with the head POS tag of the underlying
phrase if the phrase is a fixed dependency structure.
As shown in Table 3, nonterminal annotation results
in a big drop in performance, decreasing the BLEU
score of the baseline from 27.82 to 25.54. This sug-
gests that it is undesirable to further fragment the
translation rules. Introducing the syntax mismatch
features described in Section 4.1 helps to improve
5The alignment corpora are LDC2012E24, LDC2012E72,
LDC2012E95, and LDC2013E02. The reordering model can
also be trained on automatically aligned data; however, our ex-
periments show that using manual alignments results in a bet-
ter accuracy for the reordering model itself and more improve-
ments for the MT system.
562
BLEU
baseline 27.82
+ tag annotation only 25.54
+ tag annotation, mismatch feat. 25.90
+ tag distribution, mismatch feat. 28.23
Table 3: Effects of tag annotation, tag distribution, and
syntax mismatch features on MT performance on the
GALE/BOLT data set.
BLEU from 25.54 to 25.90. This improvement is
not large enough to compensate for the performance
drop caused by annotating the nonterminals.
Our proposed approach, on the other hand, does
not modify the translation rules in the baseline sys-
tem, but only associates each nonterminal with a dis-
tribution of tags. For that reason, it does not suffer
from the aforementioned problem. It achieves ex-
actly the same performance as the baseline system
if no source syntactic constraints are imposed dur-
ing decoding. When the source syntax mismatch
features are used, the proposed approach is able to
achieve a gain of 0.41 in BLEU over the baseline
system. Table 4 lists the learned weights of the syn-
tax mismatch features after MT tuning. The nega-
tive weights of f1 and f2 mean that the MT system
penalizes source spans that do not have a fixed de-
pendency structure, and it assigns a higher penalty
to rules whose nonterminals have a high probability
of being extracted from source phrases that do not
have a fixed dependency structure. When the source
span has a fixed dependency structure, the MT sys-
tem prefers translation rules that have a high proba-
bility of matching the tag on the source span (feature
f4) over the ones that do not match (features f3 and
f5). This result is consistent with our expectations
of the syntax mismatch features.
Feature Description Weight
f1 ts = X, tr = X ?1.543
f2 ts = X, tr ? X ?0.676
f3 ts ? X, tr = X 0.380
f4 ts ? X, tr ? X, tr = ts 1.677
f5 ts ? X, tr ? X, tr ? ts 0.232
Table 4: Learned syntax mismatch feature weight
5.3 Syntax-based Reordering Model
Before evaluating the syntax-based reordering
model, we would like to establish the upper bound
improvement that could be achieved using the gen-
eral reordering framework for hierarchical transla-
tion models. Towards that goal, we conduct an ora-
cle experiment on the GALE/BOLT development set
that uses the oracle translation order from the ref-
erence as the external reordering model. For each
source sentence in the development set, we pair it
with the first reference translation (out of up to 4 in-
dependent translations). We then add the sentence
pairs from the development set to the parallel train-
ing data and run GIZA++ to obtain word alignments.
We consider the GIZA++ word alignments for the
development set to be all correct, and use it to de-
termine the oracle order in the reference translation.
For the ordering distribution, we set the log proba-
bility of the reference translation order to 0 and the
reverse order to -1 to avoid negative infinity. As
shown in Table 5, the system tuned and evaluated
with the oracle reordering model significantly out-
performs the baseline by a large margin of 2.32 in
BLEU on the GALE/BOLT test set. This suggests
that there is room for potential improvement by us-
ing a fairly trained reordering model.
BLEU
baseline 27.82
+ oracle reorder 30.14
+ syntax reorder 28.40
Table 5: Effects of external reordering features on MT
performance on the GALE/BOLT test set.
We next evaluate the syntax-based reordering
model. We train the model on manually aligned
Chinese-English corpora. Since the tokenization
used in the manual alignment corpora is different
from the tokenization used in our MT system, the
manual alignment is projected to the MT tokeniza-
tion based on the character alignment between the
tokens. Some extraneously tagged alignment links
in the manual alignment corpora are not useful for
machine translation and are thus removed before
projecting the alignment. As described in Sec-
tion 4.2, the syntax-based reordering method mod-
563
els the translation order of sibling constituent pairs
in the source parse tree. As a result of strong phrasal
cohesion (Fox, 2002), we find that 94% of con-
stituent pairs have a clear ordering on the target
side. We only retain these constituent pairs for train-
ing and evaluating the reordering model. In order
to evaluate the accuracy of the maximum entropy
reordering model, we divide the manual alignment
corpora into 2/3 for training and 1/3 for evaluation.
A baseline that only chooses the majority order (i.e.
straight) has an accuracy of 69%, while the syntax-
based reordering model improves the accuracy to
79%.
The final reordering model used in MT is trained
on all of the samples extracted from the manual
alignment corpora. As shown in Table 5, the syntax-
based reordering feature improves the baseline by
0.58 in BLEU, which is a good improvement given
our strong baseline. Table 6 lists the number of
shifting errors in TER measurement (Snover et al,
2006) of various systems on the GALE/BOLT test
set. The syntax-based reordering model achieves a
6.1% reduction in the number of shifting errors in
the baseline system, and its combination with the
syntax mismatch model achieves an additional re-
duction of 0.6%. This suggests that the proposed
method helps to improve word reordering in transla-
tion.
Shifting errors
baseline 3205
+ syntax mismatch 3089
+ syntax reorder 3010
+ syntax mismatch and reorder 2990
Table 6: Number of shifting errors in TER measurement
of multiple systems on the GALE/BOLT test set
5.4 Final Results
Table 7 shows the final results on the GALE/BOLT
test set, as well as the NIST MT06 and MT08 test
sets. Both the syntax mismatch and the syntax-based
reordering features improve the baseline system, re-
sulting in moderate to significant gains in all of the
five test sets. The two features are complementary
to each other and their combination results in better
improvement in four out of the five test sets com-
pared to adding them separately. In three out of the
five test sets, the improvement from the combina-
tion of the two features is statistically significant at
the 95% confidence level over the baseline, with the
largest absolute improvement of 1.43 in BLEU ob-
tained on MT08 web.
6 Conclusion
In this paper, We have discussed problems resulting
from explicitly decorating translation rules with syn-
tactic annotations. We presented a factored approach
to incorporate soft source syntax mismatch and re-
ordering constraints to hierarchical machine transla-
tion, and showed how our models avoid the pitfalls
of the explicit decoration approach. Experiments on
Chinese-English translation show that the proposed
approach significantly improves a strong string-to-
dependency translation baseline on multiple evalu-
ation sets. There are many directions in which this
work can be continued. The syntax mismatch model
can be extended to dynamically adjust the transla-
tion distribution based on the syntactic compatibil-
ity between a translation rule and a source sentence.
It also might be beneficial to look beyond syntactic
constituent pairs when modeling reordering, given
that phrasal cohesion does not always hold in trans-
lation. The general framework that uses an external
reordering model in hierarchical models via features
can also be naturally extended to use multiple re-
ordering models.
Acknowledgments
This work was supported in part by DARPA/IPTO
Contract No. HR0011-12-C-0014 under the BOLT
Program. The views expressed are those of the au-
thors and do not reflect the official policy or position
of the Department of Defense or the U.S. Govern-
ment. The authors would like to thank the anony-
mous reviewers for their helpful comments.
References
Yaser Al-Onaizan and Kishore Papineni. 2006. Distor-
tion models for statistical machine translation. In Pro-
ceedings of the Annual Meeting of the Association for
Computational Linguistics.
564
MT06 news MT06 web MT08 news MT08 web GALE/BOLT
baseline 43.76 36.13 40.52 27.78 27.82
+ syntax mismatch 43.89 36.72 40.82+ 28.54- 28.23-
+ syntax reorder 44.01 36.40 41.23/ 28.95/ 28.40/
+ syntax mismatch and reorder 44.28+ 36.43+ 41.14- 29.21/ 28.62/
improvement over baseline +0.52 +0.30 +0.62 +1.43 +0.8
Table 7: Results on Chinese-English MT. The symbols +, -, and / indicate that the system is better than the baseline
at the 85%, 95%, and 99% confidence levels, respectively, as defined in (Koehn, 2004).
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. In Proceedings of the Annual Meeting of the As-
sociation for Computational Linguistics.
Colin Cherry. 2013. Improved reordering for phrase-
based translation using sparse features. In Proceed-
ings of the Conference of the North American Chapter
of the Association for Computational Linguistics.
David Chiang, Mona Diab, Nizar Habash, Owen Ram-
bow, and Safiullah Shareef. 2006. Parsing Arabic di-
alects. In Conference of the European Chapter of the
Association for Computational Linguistics.
David Chiang, Kevin Knight, and Wei Wang. 2009.
11,001 new features for statistical machine translation.
In Proceedings of the Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics.
David Chiang. 2010. Learning to translate with source
and target syntax. In Proceedings of the Annual Meet-
ing of the Association for Computational Linguistics.
Michael Collins, Philipp Koehn, and Ivona Kuc?erova?.
2005. Clause restructuring for statistical machine
translation. In Proceedings of the Annual Meeting of
the Association for Computational Linguistics.
Heidi Fox. 2002. Phrasal cohesion and statistical ma-
chine translation. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing.
Michel Galley and Christopher D. Manning. 2008. A
simple and effective hierarchical phrase reordering
model. In Proceedings of the Conference on Empir-
ical Methods in Natural Language Processing.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2004. What?s in a translation rule. In Pro-
ceedings of the Conference of the North American
Chapter of the Association for Computational Linguis-
tics on Human Language Technology.
Yang Gao, Philipp Koehn, and Alexandra Birch. 2011.
Soft dependency constraints for reordering in hierar-
chical phrase-based translation. In Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing.
Dmitriy Genzel. 2010. Automatically learning source-
side reordering rules for large scale machine transla-
tion. In Proceedings of the International Conference
on Computational Linguistics.
Greg Hanneman and Alon Lavie. 2013. Improving
syntax-augmented machine translation by coarsening
the label set. In Proceedings of the Conference of the
North American Chapter of the Association for Com-
putational Linguistics.
Zhongqiang Huang and Mary Harper. 2009. Self-
training PCFG grammars with latent annotations
across languages. In Proceedings of the Conference
on Empirical Methods in Natural Language Process-
ing.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
A syntax-directed translator with extended domain of
locality. In Proceedings of the Workshop on Computa-
tionally Hard Problems and Joint Inference in Speech
and Language Processing.
Zhongqiang Huang, Martin C?mejrek, and Bowen Zhou.
2010. Soft syntactic constraints for hierarchical
phrase-based translation using latent syntactic distri-
butions. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing.
Mitesh M. Khapra, Ananthakrishnan Ramanathan, and
Karthik Visweswariah. 2013. Improving reordering
performance using higher order and structural features.
In Proceedings of the Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of the Conference of the North American Chap-
ter of the Association for Computational Linguistics
on Human Language Technology.
Philipp Koehn, Amittai Axelrod, Alexandra Birch, Chris
Callison-Burch, Miles Osborne, and David Talbot.
2005. Edinburgh system description for the 2005 iwslt
565
speech translation evaluation. In International Work-
shop on Spoken Language Translation.
Philipp Koehn. 2004. Pharaoh: A bean search decoder
for phrase-based statistical machine translation mod-
els. In Proceedings of the Conference of Association
for Machine Translation in the Americas.
Junhui Li, Philip Resnik, and Hal Daume. 2013. Mod-
eling syntactic and semantic structures in hierarchi-
cal phrase-based translation. In Proceedings of the
Annual Meeting of the Association for Computational
Linguistics.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-
string alignment template for statistical machine trans-
lation. In Proceedings of the Annual Meeting of the
Association for Computational Linguistics.
Yuval Marton and Philip Resnik. 2008. Soft syntactic
constraints for hierarchical phrased-based translation.
In Proceedings of the Annual Meeting of the Associa-
tion for Computational Linguistics.
Haitao Mi and Liang Huang. 2008. Forest-based transla-
tion rule extraction. In Proceedings of the Conference
on Empirical Methods in Natural Language Process-
ing.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computional Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalua-
tion of machine translation. In Proceedings of the An-
nual Meeting on Association for Computational Lin-
guistics.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A
new string-to-dependency machine translation algo-
rithm with a target dependency language model. In
Proceedings of the Annual Meeting of the Association
for Computational Linguistics.
Libin Shen, Jinxi Xu, Bing Zhang, Spyros Matsoukas,
and Ralph Weischedel. 2009. Effective use of linguis-
tic and contextual information for statistical machine
translation. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of Association for Machine Translation
in the Americas.
Roy Tromble and Jason Eisner. 2009. Learning linear or-
dering problems for better translation. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing.
Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel
Jurafsky, and Christopher Manning. 2005. A condi-
tional random field word segmenter. In Proceedings
of the SIGHAN Workshop on Chinese Language Pro-
cessing.
Ashish Venugopal, Andreas Zollmann, Noah A. Smith,
and Stephan Vogel. 2009. Preference grammars: soft-
ening syntactic constraints to improve statistical ma-
chine translation. In Proceedings of the Conference
of the North American Chapter of the Association for
Computational Linguistics.
Fei Xia and Michael McCord. 2004. Improving a sta-
tistical mt system with automatically learned rewrite
patterns. In Proceedings of the International Confer-
ence on Computational Linguistics.
Deyi Xiong, Min Zhang, and Haizhou Li. 2012. Model-
ing the translation of predicate-argument structure for
smt. In Proceedings of the Annual Meeting of the As-
sociation for Computational Linguistics.
Peng Xu, Jaeho Kang, Michael Ringgaard, and Franz
Och. 2009. Using a dependency parser to improve smt
for subject-object-verb languages. In Proceeding of
the Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics.
Nan Yang, Mu Li, Dongdong Zhang, and Nenghai Yu.
2012. A ranking-based approach to word reordering
for statistical machine translation. In Proceedings of
the Annual Meeting of the Association for Computa-
tional Linguistics.
Andreas Zollmann and Ashish Venugopal. 2006. Syntax
augmented machine translation via chart parsing. In
Proceedings of the Workshop on Statistical Machine
Translation.
566
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 37?45,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Appropriately Handled Prosodic Breaks Help PCFG Parsing
Zhongqiang Huang1, Mary Harper1,2
1Laboratory for Computational Linguistics and Information Processing
Institute for Advanced Computer Studies
University of Maryland, College Park, MD USA
2Human Language Technology Center of Excellence
Johns Hopkins University, Baltimore, MD USA
{zqhuang,mharper}@umiacs.umd.edu
Abstract
This paper investigates using prosodic infor-
mation in the form of ToBI break indexes for
parsing spontaneous speech. We revisit two
previously studied approaches, one that hurt
parsing performance and one that achieved
minor improvements, and propose a new
method that aims to better integrate prosodic
breaks into parsing. Although these ap-
proaches can improve the performance of ba-
sic probabilistic context free grammar (PCFG)
parsers, they all fail to produce fine-grained
PCFG models with latent annotations (PCFG-
LA) (Matsuzaki et al, 2005; Petrov and Klein,
2007) that perform significantly better than the
baseline PCFG-LA model that does not use
break indexes, partially due to mis-alignments
between automatic prosodic breaks and true
phrase boundaries. We propose two alterna-
tive ways to restrict the search space of the
prosodically enriched parser models to the n-
best parses from the baseline PCFG-LA parser
to avoid egregious parses caused by incor-
rect breaks. Our experiments show that all
of the prosodically enriched parser models can
then achieve significant improvement over the
baseline PCFG-LA parser.
1 Introduction
Speech conveys more than a sequence of words to
a listener. An important additional type of informa-
tion that phoneticians investigate is called prosody,
which includes phenomena such as pauses, pitch,
energy, duration, grouping, and emphasis. For a
review of the role of prosody in processing spo-
ken language, see (Cutler et al, 1997). Prosody
can help with the disambiguation of lexical meaning
(via accents and tones) and sentence type (e.g., yes-
no question versus statement), provide discourse-
level information like focus, prominence, and dis-
course segment, and help a listener to discern a
speaker?s emotion or hesitancy, etc. Prosody often
draws a listener?s attention to important information
through contrastive pitch or duration patterns associ-
ated words or phrases. In addition, prosodic cues can
help one to segment speech into chunks that are hy-
pothesized to have a hierarchical structure, although
not necessarily identical to that of syntax. This sug-
gests that prosodic cues may help in the parsing of
speech inputs, the topic of this paper.
Prosodic information such as pause length, du-
ration of words and phones, pitch contours, en-
ergy contours, and their normalized values have
been used in speech processing tasks like sentence
boundary detection (Liu et al, 2005). In contrast,
other researchers use linguistic encoding schemes
like ToBI (Silverman et al, 1992), which encodes
tones, the degree of juncture between words, and
prominence symbolically. For example, a simplified
ToBI encoding scheme uses the symbol 4 for ma-
jor intonational breaks, p for hesitation, and 1 for all
other breaks (Dreyer and Shafran, 2007). In the lit-
erature, there have been several attempts to integrate
prosodic information to improve parse accuracy of
speech transcripts. These studies have used either
quantized acoustic measurements of prosody or au-
tomatically detected break indexes.
Gregory et al (2004) attempted to integrate quan-
tized prosodic features as additional tokens in the
same manner that punctuation marks are added
into text. Although punctuation marks can signif-
icantly improve parse accuracy of newswire text,
the quantized prosodic tokens were found harm-
37
ful to parse accuracy when inserted into human-
generated speech transcripts of the Switchboard cor-
pus. The authors hypothesized that the inserted
pseudo-punctuation break n-gram dependencies in
the parser model, leading to lower accuracies. How-
ever, another possible cause is that the prosody has
not been effectively utilized due to the fact that
it is overloaded; it not only provides information
about phrases, but also about the state of the speaker
and his/her sentence planning process. Hence, the
prosodic information may at times be more harmful
than helpful to parsing performance.
In a follow-on experiment, Kahn et al (2005), in-
stead of using raw quantized prosodic features, used
three classes of automatically detected ToBI break
indexes (1, 4, or p) and their posteriors. Rather than
directly incorporating the breaks into the parse trees,
they used the breaks to generate additional features
for re-ranking the n-best parse trees from a gener-
ative parsing model trained without prosody. They
were able to obtain a significant 0.6% improvement
on Switchboard over the generative parser, and a
more modest 0.1% to 0.2% improvement over the
reranking model that also utilizes syntactic features.
Dreyer and Shafran (2007) added prosodic breaks
into a generative parsing model with latent vari-
ables. They utilized three classes of ToBI break in-
dexes (1, 4, and p), automatically predicted by the
approach described in (Dreyer and Shafran, 2007;
Harper et al, 2005). Breaks were modeled as a se-
quence of observations parallel to the sentence and
each break was generated by the preterminal of the
preceding word, assuming that the observation of a
break, b, was conditionally independent of its pre-
ceding word, w, given preterminal X:
P (w, b|X) = P (w|X)P (b|X) (1)
Their approach has advantages over (Gregory et al,
2004) in that it does not break n-gram dependencies
in parse modeling. It also has disadvantages in that
the breaks are modeled by preterminals rather than
higher level nonterminals, and thus cannot directly
affect phrasing in a basic PCFG grammar. How-
ever, they addressed this independence drawback by
splitting each nonterminal into latent tags so that the
impact of prosodic breaks could be percolated into
the phrasing process through the interaction of la-
tent tags. They achieved a minor 0.2% improvement
over their baseline model without prosodic cues and
also found that prosodic breaks can be used to build
more compact grammars.
In this paper, we re-investigate the models of
(Gregory et al, 2004) and (Dreyer and Shafran,
2007), and propose a new way of modeling that
can potentially address the shortcomings of the two
previous approaches. We also attribute part of the
failure or ineffectiveness of the previously investi-
gated approaches to errors in the quantized prosodic
tokens or automatic break indexes, which are pre-
dicted based only on acoustic cues and could mis-
align with phrase boundaries. We illustrate that
these prosodically enriched models are in fact highly
effective if we systematically eliminate bad phrase
and hesitation breaks given their projection onto the
reference parse trees. Inspired by this, we pro-
pose two alternative rescoring methods to restrict
the search space of the prosodically enriched parser
models to the n-best parses from the baseline PCFG-
LA parser to avoid egregious parse trees. The effec-
tiveness of our rescoring method suggests that the
reranking approach of (Kahn et al, 2005) was suc-
cessful not only because of their prosodic feature de-
sign, but also because they restrict the search space
for reranking to n-best lists generated by a syntactic
model alone.
2 Experimental Setup
Due to our goal of investigating the effect of
prosodic information on the accuracy of state of the
art parsing of conversational speech, we utilize both
Penn Switchboard (Godfrey et al, 1992) and Fisher
treebanks (Harper et al, 2005; Bies et al, 2006), for
which we also had automatically generated break in-
dexes from (Dreyer and Shafran, 2007; Harper et al,
2005)1. The Fisher treebank is a higher quality pars-
ing resource than Switchboard due to its greater use
of audio and refined specifications for sentence seg-
mentation and disfluency markups, and so we utilize
its eval set for our parser evaluation; the first 1,020
trees (7,184 words) were used for development and
the remaining 3,917 trees (29,173 words) for eval-
uation. We utilized the Fisher dev1 and dev2 sets
containing 16,519 trees (112,717 words) as the main
training data source and used the Penn Switchboard
1A small fraction of words in the Switchboard treebank
could not be aligned with the break indexes that were produced
based on a later refinement of the transcription. We chose not
to alter the Switchboard treebank, so in cases of missing break
values, we heuristically added break *1* to words in the middle
of a sentence and *4* to words that end a sentence.
38
treebank containing 110,504 trees (837,863 words)
as an additional training source to evaluate the ef-
fect of training data size on parsing performance.
The treebank trees are normalized by downcasing
all terminal strings and deleting punctuation, empty
nodes, and nonterminal-yield unary rules that are not
related to edits.
We will compare2 three prosodically enriched
PCFG models described in the next section, with a
baseline PCFG parser. We will also utilize a state
of the art PCFG-LA parser (Petrov and Klein, 2007;
Huang and Harper, 2009) to examine the effect of
prosodic enrichment3. Unlike (Kahn et al, 2005),
we do not remove EDITED regions prior to parsing
because parsing of EDITED regions is likely to ben-
efit from prosodic information. Also, parses from all
models are compared with the gold standard parses
in the Fisher evaluation set using SParseval bracket
scoring (Harper et al, 2005; Roark et al, 2006)
without flattening the EDITED constituents.
3 Methods of Integrating Breaks
Rather than using quantized raw acoustic features as
in (Gregory et al, 2004), we use automatically gen-
erated ToBI break indexes as in (Dreyer and Shafran,
2007; Kahn et al, 2005) as the prosodic cues, and
investigate three alternative methods of modeling
prosodic breaks. Figure 1 shows parse trees for the
four models for processing the spontaneous speech
transcription she?s she would do, where the speaker
hesitated after saying she?s and then resumed with
another utterance she would do. Each word input
into the parser has an associated break index repre-
sented by the symbol 1, 4, or p enclosed in asterisks
indicating the break after the word. The automat-
ically detected break *4* after the contraction is a
strong indicator of an intonational phrase boundary
that might provide helpful information for parsing if
modeled appropriately. Figure 1 (a) shows the ref-
erence parse tree (thus the name REGULAR) where
the break indexes are not utilized.
The first method to incorporate break indexes,
BRKINSERT, shown in Figure 1 (b), treats the *p*
and *4* breaks as tokens, placing them under the
2We use Bikel?s randomized parsing evaluation comparator
to determine the significance (p < 0.005) of the difference be-
tween two parsers? outputs.
3Due to the randomness of parameter initialization in the
learning of PCFG-LA models with increasing numbers of latent
tags, we train each latent variable grammar with 10 different
seeds and report the average F score on the evaluation set.
would
MD
do
VB
VP
VP
she
PRP
NP
?s
VBZ
VP
she
PRP
NP
S
EDITED
S
*4* *4**1* *1* *1*
MD VB
VP
VP
PRP
NP
VBZ
VP
PRP
NP
S
EDITED
S
BREAK BREAK
would doshe?sshe *4* *4**1* *1* *1*
(a) REGULAR (b) BRKINSERT
would
MD
do
VB
VP
VP
she
PRP
NP
?s
VBZ
VP
she
PRP
NP
S
EDITED
S
*4* *4**1* *1* *1* would
MD
do
VB
VP
VP
she
PRP
NP
?s
VBZ
VP
she
PRP
NP
S
EDITED
S
*4* *4**1* *1* *1*
(c) BRKPOS (d) BRKPHRASE
Figure 1: Modeling Methods
highest nonterminal nodes so that the order of words
and breaks remain unchanged in the terminals. This
is similar to (Gregory et al, 2004), except that auto-
matically generated ToBI breaks are used rather than
quantized raw prosodic tokens.
The second method, BRKPOS, shown in Fig-
ure 1 (c), treats breaks as a sequence of observa-
tions parallel to the words in the sentence as in
(Dreyer and Shafran, 2007). The dotted edges in
Figure 1 (c) represent the relation between pretermi-
nals and prosodic breaks, and we call them prosodic
rewrites, with analogy to grammar rewrites and lex-
ical rewrites. The generation of words and prosodic
breaks is assumed to be conditionally independent
given the preterminal, as in Equation 1.
The third new method, BRKPHRASE, shown in
Figure 1 (d), also treats breaks as a sequence of ob-
servations parallel to the sentence; however, rather
than associating the prosodic breaks with the preter-
minals, each is generated by the highest nonterminal
(including preterminal) in the parse tree that covers
the preceding word as the right-most terminal. The
observation of break, b, is assumed to be condition-
ally independent of grammar or lexical rewrite, r,
given the nonterminal X:
P (r, b|X) = P (r|X)P (b|X) (2)
The relation is indicated by the dotted edges in Fig-
ure 1 (d), and it is also called a prosodic rewrite.
The potential advantage of BRKPHRASE is that it
does not break or fragment n-gram dependencies of
the grammar rewrites, as in the BRKINSERT method,
and it directly models the dependency between
breaks and phrases, which the BRKPOS method ex-
plicitly lacks.
4 Model Training
Since automatically generated prosodic breaks are
incorporated into the parse trees deterministi-
39
cally for all of the three enrichment methods
(BRKINSERT, BRKPOS, and BRKPHRASE), train-
ing a basic PCFG is straightforward; we simply pull
the counts of grammar rules, lexical rewrites, or
prosodic rewrites from the treebank and normalize
them to obtain their probabilities.
As is well known in the parsing community, the
basic PCFG does not provide state-of-the-art per-
formance due to its strong independence assump-
tions. We can relax these assumptions by explicitly
incorporating more information into the conditional
history, as in Charniak?s parser (Charniak, 2000);
however, this would require sophisticated engineer-
ing efforts to decide what to include in the history
and how to smooth probabilities appropriately due
to data sparsity. In this paper, we utilize PCFG-LA
models (Matsuzaki et al, 2005; Petrov and Klein,
2007) that split each nonterminal into a set of latent
tags and learn complex dependencies among the la-
tent tags automatically during training. The result-
ing model is still a PCFG, but it is probabilistically
context free on the latent tags, and the interaction
among the latent tags is able to implicitly capture
higher order dependencies among the original non-
terminals and observations. We follow the approach
in (Huang and Harper, 2009) to train the PCFG-LA
models.
5 Parsing
In a basic PCFG without latent variables, the goal
of maximum probability parsing is to find the most
likely parse tree given a sentence based on the gram-
mar. Suppose our grammar is binarized (so it con-
tains only unary and binary grammar rules). Given
an input sentence wn1 = w1, w2, ? ? ? , wn, the inside
probability, P (i, j, X), of the most likely sub-tree
that is rooted at nonterminal X and generates sub-
sequence wji can be computed recursively by:
P (i, j, X) = max(max
Y
P (i, j, Y )P (X ? Y ),
max
i<k<j,Y,Z
P (i, k, Y )P (k + 1, j, Z)P (X ? Y Z)) (3)
Backtracing the search process then returns the most
likely parse tree for the REGULAR grammar.
The same parsing algorithm can be directly ap-
plied to the BRKINSERT grammar given that the
break indexes are inserted appropriately into the in-
put sentence as additional tokens. Minor modifica-
tion is needed to extend the same parsing algorithm
to the BRKPOS grammar. The only difference is that
the inside probability of a preterminal is set accord-
ing to Equation 1. The rest of the algorithm proceeds
as in Equation 3.
However, parsing with the BRKPHRASE grammar
is more complicated because whether a nonterminal
generates a break or not is determined by whether
it is the highest nonterminal that covers the preced-
ing word as its right-most terminal. In this case,
the input observation also contains a sequence of
break indexes bn1 = b1, b2, ? ? ? , bn that is parallel
to the input sentence wn1 = w1, w2, ? ? ? , wn. Let
P (i, j, X, 0) be the probability of the most likely
sub-tree rooted at nonterminal X over span (i, j)
that generates word sequence wji , as well as break
index sequence bj?1i , excluding bj . According to
the independence assumption in Equation 2, with
the addition of prosodic edge X ? bj , the same
sub-tree also has the highest probability, denoted by
P (i, j, X, 1), of generating word sequence wji to-
gether with the break index sequence bji . Thus we
have:
P (i, j, X, 1) = P (i, j, X, 0)P (bj |X) (4)
The structural constraint that a break index is only
generated by the highest nonterminal that covers
the preceding word as the right-most terminal en-
ables a dynamic programming algorithm to compute
P (i, j, X, 0) and thus P (i, j, X, 1) efficiently. If the
sub-tree (without the prosodic edge that generates
bj) over span (i, j) is constructed from a unary rule
rewrite X ? Y , then the root nonterminal Y of
some best sub-tree over the same span (i, j) can not
generate break bj because it has a higher nontermi-
nal X that also covers word wj as its right-most ter-
minal. If the sub-tree is constructed from a binary
rule rewrite X ? Y Z, then the root nonterminal Y
of some best sub-tree over some span (i, k) will gen-
erate break bk because Y is the highest nonterminal
that covers word wk as the right-most terminal4. In
contrast, the root nonterminal Z of some best sub-
tree over some span (k+1, j) can not generate break
bj because Z has a higher nonterminal X that also
covers word wj as its right-most terminal. Hence,
4Use of left-branching is required for the BRKPHRASE
method to ensure that the prosodic breaks are associated with
the original nonterminals, not intermediate nonterminals in-
troduced by binarization. Binarization is needed for efficient
parametrization of PCFG-LA models and left- versus right-
branching binarization does not significantly affect model per-
formance; hence, we use left-branching for all models.
40
P (i, j, X, 1) and P (i, j, X, 0) can be computed re-
cursively by Equation 4 above and Equation 5 be-
low:
P (i, j, X, 0) = max(max
Y
P (i, j, Y, 0)P (X ? Y ),
max
i<k<j,Y,Z
P (i, k, Y, 1)P (k + 1, j, Z, 0)P (X ? Y Z)) (5)
Although dynamic programming algorithms exist
for maximum probability decoding of basic PCFGs
without latent annotations for all four methods, it is
an NP hard problem to find the most likely parse tree
using PCFG-LA models. Several alternative decod-
ing algorithms have been proposed in the literature
for parsing with latent variable grammars. We use
the best performing max-rule-product decoding al-
gorithm, which searches for the best parse tree that
maximizes the product of the posterior rule (either
grammar, lexical, or prosodic) probabilities, as de-
scribed in (Petrov and Klein, 2007) for our models
with latent annotations and extend the dynamic pars-
ing algorithm described in Equation 5 for the BRK-
PHRASE grammar with latent annotations.
6 Results on the Fisher Corpus
6.1 Prosodically Enriched Models
Table 1 reports the parsing accuracy of the four basic
PCFGs without latent annotations when trained on
the Fisher training data. All of the grammars have a
low F score of around 65% due to the overly strong
and incorrect independence assumptions. We ob-
serve that the BRKPHRASE grammar benefits most
from breaks, significantly improving the baseline
accuracy from 64.9% to 67.2%, followed by the
BRKINSERT grammar, which at 66.2% achieves a
smaller improvement. The BRKPOS grammar ben-
efits the least among the three because breaks are
attached to the preterminals and thus have less im-
pact on phrasing due to the independence assump-
tions in the basic PCFG. In contrast, both the BRK-
PHRASE and BRKINSERT methods directly model
the relationship between breaks and phrase bound-
aries through governing nonterminals; however, the
BRKPHRASE method does not directly change any
of the grammar rules in contrast to the BRKINSERT
method that more or less breaks n-gram dependen-
cies and fragments rule probabilities.
The bars labeled DIRECT in Figure 2 report the
parsing performance of the four PCFG-LA models
trained on Fisher. The introduction of latent anno-
tations significantly boosts parsing accuracies, pro-
viding relative improvements ranging from 16.8%
REGULAR BRKINSERT BRKPOS BRKPHRASE
64.9 66.2 65.2 67.2
Table 1: Fisher evaluation parsing results for the basic
PCFGs without latent annotations trained on the Fisher
training set.
up to 19.0% when trained on Fisher training data
due to the fact that the PCFG-LA models are able
to automatically learn more complex dependencies
not captured by basic PCFGs.
 82.5
 83.5
 84.5
 85.5
Regular BrkInsert BrkPos BrkPhrase
83.9
83.2
84.2 84.2
84.4
84.0
85.0
84.5
84.7
84.0
85.1
84.7 84.8
Direct
Oracle
OracleRescore
DirectRescore
Figure 2: Parsing results on the Fisher evaluation set
of the PCFG-LA models trained on the Fisher training
data. The DIRECT bars represent direct parsing results for
models trained and evaluated on the original data, ORA-
CLE bars for models trained and evaluated on the modi-
fied oracle data (see Subsection 6.2), and the ORACLE-
RESCORE and DIRECTRESCORE bars for results of the
two rescoring approaches (described in Subsection 6.3)
on the original evaluation data.
However, the prosodically enriched methods do
not significantly improve upon the REGULAR base-
line after the introduction of latent annotations. The
BRKPHRASE method only achieves a minor in-
significant 0.1% improvement over the REGULAR
baseline; whereas, the BRKINSERT method is a sig-
nificant 0.7% worse than the baseline. Similar re-
sults for BRKINSERT were reported in (Gregory et
al., 2004), where they attributed the degradation to
the fact that the insertion of the prosodic ?punctua-
tion? breaks the n-gram dependencies. Another pos-
sible cause is that the insertion of ?bad? breaks that
do not align with true phrase boundaries hurts per-
formance more than the benefits gained from ?good?
breaks due to the tightly integrated relationship be-
tween phrases and breaks. For the BRKPOS method,
the impact of break indexes is implicitly percolated
to the nonterminals through the interaction among
latent tags, as discussed in (Dreyer and Shafran,
2007), and its performance may thus be less affected
by the ?bad? breaks. With latent annotations (in con-
trast to the basic PCFG), the model is now signif-
icantly better than BRKINSERT and is on par with
BRKPHRASE.
41
6.2 Models with Oracle Breaks
In order to determine whether ?bad? breaks limit
the improvements in parsing performance from
prosodic enrichment, we conducted a simple ora-
cle experiment where all *p* and *4* breaks that
did not align with phrase boundaries in the tree-
bank were systematically converted to *1* breaks5.
When trained and evaluated on this modified ora-
cle data, all three prosodically enriched latent vari-
able models improve by about 1% and were then
able to achieve significant improvements over the
REGULAR PCFG-LA baseline, as shown by the bars
labeled ORACLE in Figure 2. It should be noted,
however, that the BRKINSERT method is much less
effective than the other two methods in the oracle
experiment, suggesting that broken n-gram depen-
dencies affect the model in addition to the erroneous
breaks.
6.3 N-Best Re-Scoring
As mentioned previously, prosody does not only
provide information about phrases, but also about
the state of the speaker and his/her sentence plan-
ning process. Given that our break detector uti-
lizes only acoustic knowledge to predict breaks, the
recognized *p* and *4* breaks may not correctly
reflect hesitations and phrase boundaries. Incor-
rectly recognized breaks could hurt parsing more
than the benefit brought from the correctly recog-
nized breaks, as demonstrated by superior perfor-
mance of the prosodically enhanced models in the
oracle experiment. We next describe two alternative
methods to make better use of automatic breaks.
In the first approach, which is called ORACLE-
RESCORE, we train the prosodically enhanced
grammars on cleaned-up break-annotated training
data, where misclassified *p* and *4* breaks are
converted to *1* breaks (as in the oracle experi-
ment). If these grammars were used to directly parse
the test sentences with automatically detected (un-
modified) breaks, the results would be quite poor
due to mismatch between the training and testing
conditions. However, we can automatically bias
against potentially misclassified *p* and *4* breaks
if we utilize information provided by n-best parses
from the baseline REGULAR PCFG-LA grammar.
5Other sources of errors include misclassification of *p*
breaks as *1* or *4* and misclassification of *4* breaks as *1*
or *p*. Although these errors are not repaired in the oracle ex-
periment, fixing them could potentially provide greater gains.
For each hypothesized parse tree in the n-best list,
the *p* and *4* breaks that do not align with the
phrase boundaries of the hypothesized parse tree are
converted to *1* breaks, and then a new score is
computed using the product of posterior rule proba-
bilities6, as in the max-rule-product criterion, for the
hypothesized parse tree using the grammars trained
on the cleaned-up training data. In this approach,
we convert the posterior probability, P (T |W, B),
of parse tree T given words W and breaks B
to P (B?|W, B)P (T |W, B?), where B? is the new
break sequence constrained by T , and simplify it to
P (T |W, B?), assuming that conversions to a new se-
quence of breaks as constrained by a hypothesized
parse tree are equally probable given the original se-
quence of breaks. We consider this to be a reason-
able assumption for a small n-best (n = 50) list with
reasonably good quality.
In the second approach, called DIRECTRESCORE,
we train the prosodically enhanced PCFG-LA mod-
els using unmodified, automatic breaks, and then
use them to rescore the n-best lists produced by
the REGULAR PCFG-LA model to avoid the poorer
parse trees caused by fully trusting automatic break
indexes. The size of the n-best list should not be too
small or too large, or the results would be like di-
rectly parsing with REGULAR when n = 1 or with
the prosodically enriched model when n ? ?.
The ORACLERESCORE and DIRECTRESCORE
bars in Figure 2 report the performance of the
prosodically enriched models with the correspond-
ing rescoring method. Both methods use the same
50-best lists produced by the baseline REGULAR
PCFG-LA model using the max-rule-product cri-
terion. Both rescoring methods produce signifi-
cant improvements in the performance of all three
prosodically enriched PCFG-LA models. The pre-
viously ineffective (0.7% worse than REGULAR)
BRKINSERT PCFG-LA model is now 0.3% and
0.5% better than the REGULAR baseline using
the ORACLERESCORE and DIRECTRESCORE ap-
proaches, respectively. The best performing BRK-
POS and BRKPHRASE rescoring models are 0.6-
0.9% better than the REGULAR baseline. It is in-
teresting to note that rescoring with models trained
on cleaned up prosodic breaks is somewhat poorer
6The product of posterior rule probabilities of a parse tree
is more suitable for rescoring than the joint probability of the
parse tree and the observables (words and breaks) because the
breaks are possibly different for different trees.
42
than models trained using all automatic breaks.
7 Models with Augmented Training Data
Figure 3 reports the evaluation results for mod-
els that are trained on the combination of Fisher
and Switchboard training data. With the additional
Switchboard training data, the nonterminals can be
split into more fine-grained latent tags, enabling the
learning of deeper dependencies without over-fitting
the limited sized Fisher training data. This improved
all models by at least 2.6% absolute. Note also that
the patterns observed for models trained using the
larger training set are quite similar to those from us-
ing the smaller training set in Figure 2. The prosod-
ically enriched models all benefit significantly from
the oracle breaks and from the rescoring methods.
The BRKPOS and BRKPHRASE methods, with the
additional training data, also achieve significant im-
provements over the REGULAR baseline without
rescoring.
 85
 86
 87
 88
Regular BrkInsert BrkPos BrkPhrase
86.5
86.3
87.4
86.8
87.1
86.8
87.7
87.2 87.3
86.8
87.5
87.2 87.3
Direct
Oracle
OracleRescore
DirectRescore
Figure 3: Parsing results on the Fisher evaluation set of
the PCFG-LA models trained on the Fisher+Switchboard
training data.
8 Error Analysis
In this section, we compare the errors of the
BRKPHRASE PCFG-LA model and the DIRECT-
RESCORE approach for that model to each other and
to the baseline PCFG-LA model without prosodic
breaks. All models are trained and tested on Fisher
as in Section 6. The results using other prosodically
enhanced PCFG-LA models and their rescoring al-
ternatives show similar patterns.
Figure 4 depicts the difference in F scores be-
tween BRKPHRASE and REGULAR and between
BRKPHRASE+DIRECTRESCORE and REGULAR on
a tree-by-tree basis in a 2D plot. Each quad-
rant also contains +/? signs roughly describing how
much BRKPHRASE+DIRECTRESCORE is better (+)
or worse (?) than BRKPHRASE and a pair of num-
bers (a, b), in which a represents the percentage of
sentences in that quadrant containing *p* or *4*
-20
-15
-10
-5
 0
 5
 10
 15
 20
-20 -15 -10 -5  0  5  10  15  20
F(
B
rk
P
h
ra
se
+
D
ir
ec
tR
es
co
re
)-
F(
R
eg
u
la
r)
F(BrkPhrase)-F(Regular)
-
(47.2%, 25.3%)
+++
(70.2%, 30.0%)
++
(48.2%, 27.6%)
---
(66.7%, 28.1%)
Figure 4: 2D plot of the difference in F scores be-
tween BRKPHRASE and REGULAR and between BRK-
PHRASE+DIRECTRESCORE and REGULAR, on a tree-
by-tree basis, where each dot represents a test sentence.
Each quadrant also contains +/? signs roughly describ-
ing how much BRKPHRASE+DIRECTRESCORE is better
(+) or worse (?) than BRKPHRASE and a pair of numbers
(a, b), in which a represents the percentage of sentences
in that quadrant containing *p* or *4* breaks that do not
align with true phrase boundaries, and b represents the
percentage of such *p* and *4* breaks among the total
number of *p* and *4* breaks in that quadrant.
breaks that do not align with true phrase bound-
aries, and b represents the percentage of such *p*
and *4* breaks among the total number of *p* and
*4* breaks in that quadrant.
Each dot in the top-right quadrant represents a
test sentence for which both BRKPHRASE and BRK-
PHRASE+DIRECTRESCORE produce better trees
than the baseline REGULAR PCFG-LA model. The
BRKPHRASE+DIRECTRESCORE approach is on av-
erage slightly worse than the BRKPHRASE method
(hence the single minus sign), although it also often
produces better parses than BRKPHRASE alone. In
contrast, the BRKPHRASE+DIRECTRESCORE ap-
proach on average makes many fewer errors than
BRKPHRASE (hence + +) as can be observed in the
bottom-left quadrant, where both approaches pro-
duce worse parse trees than the REGULAR base-
line. The most interesting quadrant is on the top-left
where the BRKPHRASE approach always produces
worse parses than the REGULAR baseline while the
BRKPHRASE+DIRECTRESCORE approach is able
to avoid these errors while producing better parses
than the baseline (hence + + +). Although the BRK-
PHRASE+DIRECTRESCORE approach can also pro-
duce worse parses than REGULAR, as in the bottom-
right quadrant (hence ? ? ?), altogether the quad-
rants suggest that, by restricting the search space
43
to the n-best lists produced by the baseline REG-
ULAR parser, the BRKPHRASE+DIRECTRESCORE
approach is able to avoid many bad parses trees
at the expense of somewhat poorer parses in cases
when BRKPHRASE is able to benefit from the full
search space.
The reader should note that the top-left quadrant
of Figure 4 has the highest percentage (70.2%) of
sentences with ?bad? *p* and *4* breaks and the
highest percentage (30.0%) of such ?bad? breaks
among all breaks. This evidence supports our argu-
ment that ?bad? breaks are harmful to parsing per-
formance and some parse errors caused by mislead-
ing breaks can be resolved by limiting the search
space of the prosodically enriched models to the
n-best lists produced by the baseline REGULAR
parser. However, the significant presence of ?bad?
breaks in the top-right quadrant also suggests that
the prosodically enriched models are able to pro-
duce better parses than the baseline despite the pres-
ence of ?bad? breaks, probably because the models
are trained on the mixture of both ?good? and ?bad?
breaks and are able to somehow learn to use ?good?
breaks while avoiding being misled by ?bad? breaks.
BRKPHRASE
REGULAR BRKPHRASE +DIRECTRESCORE
NP 90.4 90.4 90.9
VP 84.7 84.7 85.6
S 84.4 84.3 85.2
INTJ 93.0 93.4 93.4
PP 76.5 76.7 77.9
EDITED 60.4 62.2 63.3
SBAR 67.2 67.0 68.8
Table 2: F scores of the seven most frequent non-
terminals of the REGULAR, BRKPHRASE, and BRK-
PHRASE+DIRECTRESCORE models.
Table 2 reports the F scores of the seven most fre-
quent phrases for the REGULAR, BRKPHRASE, and
BRKPHRASE+DIRECTRESCORE methods trained
on Fisher. When comparing the BRKPHRASE
method to REGULAR, the break indexes help to im-
prove the score for edits most, followed by inter-
jections and prepositional phrases; however, they do
not improve the accuracy of any of the other phrases.
The BRKPHRASE+DIRECTRESCORE approach ob-
tains improvements on all of the major phrases.
Figure 5 (a) shows a reference parse tree of a
test sentence. The REGULAR approach correctly
parses the first half (omitted) of the sentence but
it fails to correctly interpret the second half (as
shown). The BRKPHRASE approach, in contrast,
is misguided by the incorrectly classified inter-
ruption point *p* after word ?has?, and so pro-
duces an incorrect parse early in the sentence. The
BRKPHRASE+DIRECTRESCORE approach is able
to provide the correct tree given the n-best list pro-
duced by the REGULAR approach, despite the break
index errors.
(a) Reference, BRKPHRASE+DIRECTRESCORE
(b) REGULAR (c) BRKPHRASE
Figure 5: Parses for like?1? has?p? anything?1? like?1?
affected?1? you?4? personally?4? or?1? anything?4?
9 Conclusions
We have investigated using prosodic information in
the form of automatically detected ToBI break in-
dexes for parsing spontaneous speech by compar-
ing three prosodic enrichment methods. Although
prosodic enrichment improves the basic PCFGs, that
performance gain disappears when latent variables
are used, partly due to the impact of misclassified
(?bad?) breaks that are assigned to words that do not
occur at phrase boundaries. However, we find that
by simply restricting the search space of the three
prosodically enriched latent variable parser models
to the n-best parses from the baseline PCFG-LA
parser, all of them attain significant improvements.
Our analysis more fully explains the positive results
achieved by (Kahn et al, 2005) from reranking with
prosodic features and suggests that the hypothesis
that inserted prosodic punctuation breaks n-gram de-
pendencies only partially explains the negative re-
sults of (Gregory et al, 2004). Our findings from
the oracle experiment suggest that integrating ToBI
classification with syntactic parsing should increase
the accuracy of both tasks.
Acknowledgments
We would like to thank Izhak Shafran for providing
break indexes for Fisher and Switchboard and for
44
comments on an earlier draft of this paper. This re-
search was supported in part by NSF IIS-0703859.
Opinions, findings, and recommendations expressed
in this paper are those of the authors and do not nec-
essarily reflect the views of the funding agency or
the institutions where the work was completed.
References
Ann Bies, Stephanie Strassel, Haejoong Lee, Kazuaki
Maeda, Seth Kulick, Yang Liu, Mary Harper, and
Matthew Lease. 2006. Linguistic resources for speech
parsing. In LREC.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In ACL.
Anne Cutler, Delphine Dahan, and Wilma v an Donselaar.
1997. Prosody in comprehension of spoken language:
A literature review. Language and Speech.
Markus Dreyer and Izhak Shafran. 2007. Exploiting
prosody for PCFGs with latent annotations. In Inter-
speech.
John J. Godfrey, Edward C. Holliman, and Jane Mc-
Daniel. 1992. SWITCHBOARD: Telephone speech
corpus for research and development. In ICASSP.
Michelle L. Gregory, Mark Johnson, and Eugene Char-
niak. 2004. Sentence-internal prosody does not help
parsing the way punctuation does. In NAACL.
Mary P. Harper, Bonnie J. Dorr, John Hale, Brian Roark,
Izhak Shafran, Matthew Lease, Yang Liu, Matthew
Snover, Lisa Yung, Anna Krasnyanskaya, and Robin
Stewart. 2005. 2005 Johns Hopkins Summer Work-
shop Final Report on Parsing and Spoken Structural
Event Detection. Technical report, Johns Hopkins
University.
Zhongqiang Huang and Mary Harper. 2009. Self-
training PCFG grammars with latent annotations
across languages. In EMNLP.
Jeremy G. Kahn, Matthew Lease, Eugene Charniak,
Mark Johnson, and Mari Ostendorf. 2005. Effective
use of prosody in parsing conversational speech. In
EMNLP-HLT.
Yang Liu, Andreas Stolcke, Elizabeth Shriberg, and Mary
Harper. 2005. Using conditional random fields for
sentence boundary detection in speech. In ACL.
Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii.
2005. Probabilistic CFG with latent annotations. In
ACL.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In HLT-NAACL.
Brian Roark, Mary Harper, Yang Liu, Robin Stewart,
Matthew Lease, Matthew Snover, Izhak Shafran, Bon-
nie J. Dorr, John Hale, Anna Krasnyanskaya, and Lisa
Yung. 2006. Sparseval: Evaluation metrics for pars-
ing speech. In LREC.
Kim Silverman, Mary Beckman, John Pitrelli, Mari Os-
tendorf, Colin Wightman, Patti Price, Janet Pierrehum-
bert, and Julia Hirshberg. 1992. ToBI: A standard for
labeling English prosody. In ICSLP.
45
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1370?1380,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Fast and Robust Neural Network Joint Models for Statistical Machine
Translation
Jacob Devlin, Rabih Zbib, Zhongqiang Huang,
Thomas Lamar, Richard Schwartz, and John Makhoul
Raytheon BBN Technologies, 10 Moulton St, Cambridge, MA 02138, USA
{jdevlin,rzbib,zhuang,tlamar,schwartz,makhoul}@bbn.com
Abstract
Recent work has shown success in us-
ing neural network language models
(NNLMs) as features in MT systems.
Here, we present a novel formulation for
a neural network joint model (NNJM),
which augments the NNLM with a source
context window. Our model is purely lexi-
calized and can be integrated into any MT
decoder. We also present several varia-
tions of the NNJM which provide signif-
icant additive improvements.
Although the model is quite simple, it
yields strong empirical results. On the
NIST OpenMT12 Arabic-English condi-
tion, the NNJM features produce a gain of
+3.0 BLEU on top of a powerful, feature-
rich baseline which already includes a
target-only NNLM. The NNJM features
also produce a gain of +6.3 BLEU on top
of a simpler baseline equivalent to Chi-
ang?s (2007) original Hiero implementa-
tion.
Additionally, we describe two novel tech-
niques for overcoming the historically
high cost of using NNLM-style models
in MT decoding. These techniques speed
up NNJM computation by a factor of
10,000x, making the model as fast as a
standard back-off LM.
This work was supported by DARPA/I2O Contract No.
HR0011-12-C-0014 under the BOLT program (Approved for
Public Release, Distribution Unlimited). The views, opin-
ions, and/or findings contained in this article are those of the
author and should not be interpreted as representing the of-
ficial views or policies, either expressed or implied, of the
Defense Advanced Research Projects Agency or the Depart-
ment of Defense.
1 Introduction
In recent years, neural network models have be-
come increasingly popular in NLP. Initially, these
models were primarily used to create n-gram neu-
ral network language models (NNLMs) for speech
recognition and machine translation (Bengio et al,
2003; Schwenk, 2010). They have since been ex-
tended to translation modeling, parsing, and many
other NLP tasks.
In this paper we use a basic neural network ar-
chitecture and a lexicalized probability model to
create a powerful MT decoding feature. Specifi-
cally, we introduce a novel formulation for a neu-
ral network joint model (NNJM), which augments
an n-gram target language model with an m-word
source window. Unlike previous approaches to
joint modeling (Le et al, 2012), our feature can be
easily integrated into any statistical machine trans-
lation (SMT) decoder, which leads to substantially
larger improvements than k-best rescoring only.
Additionally, we present several variations of this
model which provide significant additive BLEU
gains.
We also present a novel technique for training
the neural network to be self-normalized, which
avoids the costly step of posteriorizing over the
entire vocabulary in decoding. When used in con-
junction with a pre-computed hidden layer, these
techniques speed up NNJM computation by a fac-
tor of 10,000x, with only a small reduction on MT
accuracy.
Although our model is quite simple, we obtain
strong empirical results. We show primary results
on the NIST OpenMT12 Arabic-English condi-
tion. The NNJM features produce an improvement
of +3.0 BLEU on top of a baseline that is already
better than the 1st place MT12 result and includes
1370
a powerful NNLM. Additionally, on top of a sim-
pler decoder equivalent to Chiang?s (2007) origi-
nal Hiero implementation, our NNJM features are
able to produce an improvement of +6.3 BLEU ?
as much as all of the other features in our strong
baseline system combined.
We also show strong improvements on the
NIST OpenMT12 Chinese-English task, as well as
the DARPA BOLT (Broad Operational Language
Translation) Arabic-English and Chinese-English
conditions.
2 Neural Network Joint Model (NNJM)
Formally, our model approximates the probability
of target hypothesis T conditioned on source sen-
tence S. We follow the standard n-gram LM de-
composition of the target, where each target word
t
i
is conditioned on the previous n ? 1 target
words. To make this a joint model, we also condi-
tion on source context vector S
i
:
P (T |S) ? ?
|T |
i=1
P (t
i
|t
i?1
, ? ? ? , t
i?n+1
, S
i
)
Intuitively, we want to define S
i
as the window
that is most relevant to t
i
. To do this, we first say
that each target word t
i
is affiliated with exactly
one source word at index a
i
. S
i
is then them-word
source window centered at a
i
:
S
i
= s
a
i
?
m?1
2
, ? ? ? , s
a
i
, ? ? ? , s
a
i
+
m?1
2
This notion of affiliation is derived from the
word alignment, but unlike word alignment, each
target word must be affiliated with exactly one
non-NULL source word. The affiliation heuristic
is very simple:
(1) If t
i
aligns to exactly one source word, a
i
is
the index of the word it aligns to.
(2) If t
i
align to multiple source words, a
i
is the
index of the aligned word in the middle.
1
(3) If t
i
is unaligned, we inherit its affiliation
from the closest aligned word, with prefer-
ence given to the right.
2
An example of the NNJM context model for a
Chinese-English parallel sentence is given in Fig-
ure 1.
For all of our experiments we use n = 4 and
m = 11. It is clear that this model is effectively
an (n+m)-gram LM, and a 15-gram LM would be
1
We arbitrarily round down.
2
We have found that the affiliation heuristic is robust to
small differences, such as left vs. right preference.
far too sparse for standard probability models such
as Kneser-Ney back-off (Kneser and Ney, 1995)
or Maximum Entropy (Rosenfeld, 1996). Fortu-
nately, neural network language models are able
to elegantly scale up and take advantage of arbi-
trarily large context sizes.
2.1 Neural Network Architecture
Our neural network architecture is almost identi-
cal to the original feed-forward NNLM architec-
ture described in Bengio et al (2003).
The input vector is a 14-word context vector
(3 target words, 11 source words), where each
word is mapped to a 192-dimensional vector us-
ing a shared mapping layer. We use two 512-
dimensional hidden layers with tanh activation
functions. The output layer is a softmax over the
entire output vocabulary.
The input vocabulary contains 16,000 source
words and 16,000 target words, while the out-
put vocabulary contains 32,000 target words. The
vocabulary is selected by frequency-sorting the
words in the parallel training data. Out-of-
vocabulary words are mapped to their POS tag (or
OOV, if POS is not available), and in this case
P (POS
i
|t
i?1
, ? ? ? ) is used directly without fur-
ther normalization. Out-of-bounds words are rep-
resented with special tokens <src>, </src>,
<trg>, </trg>.
We chose these values for the hidden layer size,
vocabulary size, and source window size because
they seemed to work best on our data sets ? larger
sizes did not improve results, while smaller sizes
degraded results. Empirical comparisons are given
in Section 6.5.
2.2 Neural Network Training
The training procedure is identical to that of an
NNLM, except that the parallel corpus is used
instead of a monolingual corpus. Formally, we
seek to maximize the log-likelihood of the train-
ing data:
L =
?
i
log(P (x
i
))
where x
i
is the training sample, with one sample
for every target word in the parallel corpus.
Optimization is performed using standard back
propagation with stochastic gradient ascent (Le-
Cun et al, 1998). Weights are randomly initial-
ized in the range of [?0.05, 0.05]. We use an ini-
tial learning rate of 10
?3
and a minibatch size of
1371
Figure 1: Context vector for target word ?the?, using a 3-word target history and a 5-word source window
(i.e., n = 4 and m = 5). Here, ?the? inherits its affiliation from ?money? because this is the first aligned
word to its right. The number in each box denotes the index of the word in the context vector. This
indexing must be consistent across samples, but the absolute ordering does not affect results.
128.
3
At every epoch, which we define as 20,000
minibatches, the likelihood of a validation set is
computed. If this likelihood is worse than the pre-
vious epoch, the learning rate is multiplied by 0.5.
The training is run for 40 epochs. The training
data ranges from 10-30M words, depending on the
condition. We perform a basic weight update with
no L2 regularization or momentum. However, we
have found it beneficial to clip each weight update
to the range of [-0.1, 0.1], to prevent the training
from entering degenerate search spaces (Pascanu
et al, 2012).
Training is performed on a single Tesla K10
GPU, with each epoch (128*20k = 2.6M samples)
taking roughly 1100 seconds to run, resulting in
a total training time of ?12 hours. Decoding is
performed on a CPU.
2.3 Self-Normalized Neural Network
The computational cost of NNLMs is a significant
issue in decoding, and this cost is dominated by
the output softmax over the entire target vocabu-
lary. Even class-based approaches such as Le et
al. (2012) require a 2-20k shortlist vocabulary, and
are therefore still quite costly.
Here, our goal is to be able to use a fairly
large vocabulary without word classes, and to sim-
ply avoid computing the entire output layer at de-
code time.
4
To do this, we present the novel
technique of self-normalization, where the output
layer scores are close to being probabilities with-
out explicitly performing a softmax.
Formally, we define the standard softmax log
3
We do not divide the gradient by the minibatch size. For
those who do, this is equivalent to using an initial learning
rate of 10
?3
? 128 ? 10
?1
.
4
We are not concerned with speeding up training time, as
we already find GPU training time to be adequate.
likelihood as:
log(P (x)) = log
(
e
U
r
(x)
Z(x)
)
= U
r
(x)? log(Z(x))
Z(x) = ?
|V |
r
?
=1
e
U
r
?
(x)
where x is the sample, U is the raw output layer
scores, r is the output layer row corresponding to
the observed target word, and Z(x) is the softmax
normalizer.
If we could guarantee that log(Z(x)) were al-
ways equal to 0 (i.e., Z(x) = 1) then at decode
time we would only have to compute row r of the
output layer instead of the whole matrix. While
we cannot train a neural network with this guaran-
tee, we can explicitly encourage the log-softmax
normalizer to be as close to 0 as possible by aug-
menting our training objective function:
L =
?
i
[
log(P (x
i
))? ?(log(Z(x
i
))? 0)
2
]
=
?
i
[
log(P (x
i
))? ? log
2
(Z(x
i
))
]
In this case, the output layer bias weights are
initialized to log(1/|V |), so that the initial net-
work is self-normalized. At decode time, we sim-
ply use U
r
(x) as the feature score, rather than
log(P (x)). For our NNJM architecture, self-
normalization increases the lookup speed during
decoding by a factor of ?15x.
Table 1 shows the neural network training re-
sults with various values of the free parameter
?. In all subsequent MT experiments, we use
? = 10
?1
.
We should note that Vaswani et al (2013) im-
plements a method called Noise Contrastive Es-
timation (NCE) that is also used to train self-
normalized NNLMs. Although NCE results in
faster training time, it has the downside that there
1372
Arabic BOLT Val
? log(P (x)) | log(Z(x))|
0 ?1.82 5.02
10
?2
?1.81 1.35
10
?1
?1.83 0.68
1 ?1.91 0.28
Table 1: Comparison of neural network likelihood
for various ? values. log(P (x)) is the average
log-likelihood on a held-out set. | log(Z(x))| is
the mean error in log-likelihood when using U
r
(x)
directly instead of the true softmax probability
log(P (x)). Note that ? = 0 is equivalent to the
standard neural network objective function.
is no mechanism to control the degree of self-
normalization. By contrast, our ? parameter al-
lows us to carefully choose the optimal trade-off
between neural network accuracy and mean self-
normalization error. In future work, we will thor-
oughly compare self-normalization vs. NCE.
2.4 Pre-Computing the Hidden Layer
Although self-normalization significantly im-
proves the speed of NNJM lookups, the model
is still several orders of magnitude slower than a
back-off LM. Here, we present a ?trick? for pre-
computing the first hidden layer, which further in-
creases the speed of NNJM lookups by a factor of
1,000x.
Note that this technique only results in a signif-
icant speedup for self-normalized, feed-forward,
NNLM-style networks with one hidden layer. We
demonstrate in Section 6.6 that using one hidden
layer instead of two has minimal effect on BLEU.
For the neural network described in Section 2.1,
computing the first hidden layer requires mul-
tiplying a 2689-dimensional input vector
5
with
a 2689 ? 512 dimensional hidden layer matrix.
However, note that there are only 3 possible posi-
tions for each target word, and 11 for each source
word. Therefore, for every word in the vocabu-
lary, and for each position, we can pre-compute
the dot product between the word embedding and
the first hidden layer. These are computed offline
and stored in a lookup table, which is <500MB in
size.
Computing the first hidden layer now only re-
quires 15 scalar additions for each of the 512
hidden rows ? one for each word in the input
5
2689 = 14 words ? 192 dimensions + 1 bias
vector, plus the bias. This can be reduced to
just 5 scalar additions by pre-summing each 11-
word source window when starting a test sen-
tence. If our neural network has only one hid-
den layer and is self-normalized, the only remain-
ing computation is 512 calls to tanh() and a sin-
gle 513-dimensional dot product for the final out-
put score.
6
Thus, only ?3500 arithmetic opera-
tions are required per n-gram lookup, compared
to ?2.8M for self-normalized NNJM without pre-
computation, and ?35M for the standard NNJM.
7
Neural Network Speed
Condition lookups/sec sec/word
Standard 110 10.9
+ Self-Norm 1500 0.8
+ Pre-Computation 1,430,000 0.0008
Table 2: Speed of the neural network computa-
tion on a single CPU thread. ?lookups/sec? is the
number of unique n-gram probabilities that can be
computed per second. ?sec/word? is the amortized
cost of unique NNJM lookups in decoding, per
source word.
Table 2 shows the speed of self-normalization
and pre-computation for the NNJM. The decoding
cost is based on a measurement of ?1200 unique
NNJM lookups per source word for our Arabic-
English system.
8
By combining self-normalization and pre-
computation, we can achieve a speed of 1.4M
lookups/second, which is on par with fast back-
off LM implementations (Tanaka et al, 2013).
We demonstrate in Section 6.6 that using the self-
normalized/pre-computed NNJM results in only
a very small BLEU degradation compared to the
standard NNJM.
3 Decoding with the NNJM
Because our NNJM is fundamentally an n-gram
NNLM with additional source context, it can eas-
ily be integrated into any SMT decoder. In this
section, we describe the considerations that must
be taken when integrating the NNJM into a hierar-
chical decoder.
6
tanh() is implemented using a lookup table.
7
3500 ? 5? 512 + 2? 513; 2.8M ? 2? 2689? 512 +
2 ? 513; 35M ? 2 ? 2689 ? 512 + 2 ? 513 ? 32000. For
the sake of a fair comparison, these all use one hidden layer.
A second hidden layer adds 0.5M floating point operations.
8
This does not include the cost of duplicate lookups
within the same test sentence, which are cached.
1373
3.1 Hierarchical Parsing
When performing hierarchical decoding with an
n-gram LM, the leftmost and rightmost n ? 1
words from each constituent must be stored in the
state space. Here, we extend the state space to
also include the index of the affiliated source word
for these edge words. This does not noticeably in-
crease the search space. We also train a separate
lower-order n-gram model, which is necessary to
compute estimate scores during hierarchical de-
coding.
3.2 Affiliation Heuristic
For aligned target words, the normal affiliation
heuristic can be used, since the word alignment
is available within the rule. For unaligned words,
the normal heuristic can also be used, except when
the word is on the edge of a rule, because then the
target neighbor words are not necessarily known.
In this case, we infer the affiliation from the rule
structure. Specifically, if unaligned target word t
is on the right edge of an arc that covers source
span [s
i
, s
j
], we simply say that t is affiliated with
source word s
j
. If t is on the left edge of the arc,
we say it is affiliated with s
i
.
4 Model Variations
Recall that our NNJM feature can be described
with the following probability:
?
|T |
i=1
P (t
i
|t
i?1
, t
i?2
, ? ? ? , s
a
i
, s
a
i
?1
, s
a
i
+1
, ? ? ? )
This formulation lends itself to several natural
variations. In particular, we can reverse the trans-
lation direction of the languages, as well as the di-
rection of the language model.
We denote our original formulation as a source-
to-target, left-to-right model (S2T/L2R). We can
train three variations using target-to-source (T2S)
and right-to-left (R2L) models:
S2T/R2L
?
|T |
i=1
P (t
i
|t
i+1
, t
i+2
, ? ? ? , s
a
i
, s
a
i
?1
, s
a
i
+1
, ? ? ? )
T2S/L2R
?
|S|
i=1
P (s
i
|s
i?1
, s
i?2
, ? ? ? , t
a
?
i
, t
a
?
i
?1
, t
a
?
i
+1
, ? ? ? )
T2S/R2L
?
|S|
i=1
P (s
i
|s
i+1
, s
i+2
, ? ? ? , t
a
?
i
, t
a
?
i
?1
, t
a
?
i
+1
, ? ? ? )
where a
?
i
is the target-to-source affiliation, de-
fined analogously to a
i
.
The T2S variations cannot be used in decoding
due to the large target context required, and are
thus only used in k-best rescoring. The S2T/R2L
variant could be used in decoding, but we have not
found this beneficial, so we only use it in rescor-
ing.
4.1 Neural Network Lexical Translation
Model (NNLTM)
One issue with the S2T NNJM is that the prob-
ability is computed over every target word, so it
does not explicitly model NULL-aligned source
words. In order to assign a probability to every
source word during decoding, we also train a neu-
ral network lexical translation model (NNLMT).
Here, the input context is the 11-word source
window centered at s
i
, and the output is the tar-
get token t
s
i
which s
i
aligns to. The probabil-
ity is computed over every source word in the in-
put sentence. We treat NULL as a normal target
word, and if a source word aligns to multiple target
words, it is treated as a single concatenated token.
Formally, the probability model is:
?
|S|
i=1
P (t
s
i
|s
i
, s
i?1
, s
i+1
, ? ? ? )
This model is trained and evaluated like our
NNJM. It is easy and computationally inexpensive
to use this model in decoding, since only one neu-
ral network computation must be made for each
source word.
In rescoring, we also use a T2S NNLTM model
computed over every target word:
?
|T |
i=1
P (s
t
i
|t
i
, t
i?1
, t
i+1
, ? ? ? )
5 MT System
In this section, we describe the MT system used in
our experiments.
5.1 MT Decoder
We use a state-of-the-art string-to-dependency hi-
erarchical decoder (Shen et al, 2010). Our base-
line decoder contains a large and powerful set of
features, which include:
? Forward and backward rule probabilities
? 4-gram Kneser-Ney LM
? Dependency LM (Shen et al, 2010)
? Contextual lexical smoothing (Devlin, 2009)
? Length distribution (Shen et al, 2010)
? Trait features (Devlin and Matsoukas, 2012)
? Factored source syntax (Huang et al, 2013)
? 7 sparse feature types, totaling 50k features
(Chiang et al, 2009)
? LM adaptation (Snover et al, 2008)
1374
We also perform 1000-best rescoring with the
following features:
? 5-gram Kneser-Ney LM
? Recurrent neural network language model
(RNNLM) (Mikolov et al, 2010)
Although we consider the RNNLM to be part
of our baseline, we give it special treatment in the
results section because we would expect it to have
the highest overlap with our NNJM.
5.2 Training and Optimization
For Arabic word tokenization, we use the MADA-
ARZ tokenizer (Habash et al, 2013) for the BOLT
condition, and the Sakhr
9
tokenizer for the NIST
condition. For Chinese tokenization, we use a sim-
ple longest-match-first lexicon-based approach.
For word alignment, we align all of the train-
ing data with both GIZA++ (Och and Ney, 2003)
and NILE (Riesa et al, 2011), and concatenate the
corpora together for rule extraction.
For MT feature weight optimization, we use
iterative k-best optimization with an Expected-
BLEU objective function (Rosti et al, 2010).
6 Experimental Results
We present MT primary results on Arabic-English
and Chinese-English for the NIST OpenMT12 and
DARPA BOLT conditions. We also present a set
of auxiliary results in order to further analyze our
features.
6.1 NIST OpenMT12 Results
Our NIST system is fully compatible with the
OpenMT12 constrained track, which consists of
10M words of high-quality parallel training for
Arabic, and 25M words for Chinese.
10
The
Kneser-Ney LM is trained on 5B words of data
from English GigaWord. For test, we use
the ?Arabic-To-English Original Progress Test?
(1378 segments) and ?Chinese-to-English Orig-
inal Progress Test + OpenMT12 Current Test?
(2190 segments), which consists of a mix of
newswire and web data.
11
All test segments have
4 references. Our tuning set contains 5000 seg-
ments, and is a mix of the MT02-05 eval set as
well as held-out parallel training.
9
http://www.sakhr.com
10
We also make weak use of 30M-100M words of UN data
+ ISI comparable corpora, but this data provides almost no
benefit.
11
http://www.nist.gov/itl/iad/mig/openmt12results.cfm
NIST MT12 Test
Ar-En Ch-En
BLEU BLEU
OpenMT12 - 1st Place 49.5 32.6
OpenMT12 - 2nd Place 47.5 32.2
OpenMT12 - 3rd Place 47.4 30.8
? ? ? ? ? ? ? ? ?
OpenMT12 - 9th Place 44.0 27.0
OpenMT12 - 10th Place 41.2 25.7
Baseline (w/o RNNLM) 48.9 33.0
Baseline (w/ RNNLM) 49.8 33.4
+ S2T/L2R NNJM (Dec) 51.2 34.2
+ S2T NNLTM (Dec) 52.0 34.2
+ T2S NNLTM (Resc) 51.9 34.2
+ S2T/R2L NNJM (Resc) 52.2 34.3
+ T2S/L2R NNJM (Resc) 52.3 34.5
+ T2S/R2L NNJM (Resc) 52.8 34.7
?Simple Hier.? Baseline 43.4 30.1
+ S2T/L2R NNJM (Dec) 47.2 31.5
+ S2T NNLTM (Dec) 48.5 31.8
+ Other NNJMs (Resc) 49.7 32.2
Table 3: Primary results on Arabic-English and
Chinese-English NIST MT12 Test Set. The first
section corresponds to the top and bottom ranked
systems from the evaluation, and are taken from
the NIST website. The second section corresponds
to results on top of our strongest baseline. The
third section corresponds to results on top of a
simpler baseline. Within each section, each row
includes all of the features from previous rows.
BLEU scores are mixed-case.
Results are shown in the second section of Ta-
ble 3. On Arabic-English, the primary S2T/L2R
NNJM gains +1.4 BLEU on top of our baseline,
while the S2T NNLTM gains another +0.8, and
the directional variations gain +0.8 BLEU more.
This leads to a total improvement of +3.0 BLEU
from the NNJM and its variations. Considering
that our baseline is already +0.3 BLEU better than
the 1st place result of MT12 and contains a strong
RNNLM, we consider this to be quite an extraor-
dinary improvement.
12
For the Chinese-English condition, there is an
improvement of +0.8 BLEU from the primary
NNJM and +1.3 BLEU overall. Here, the base-
line system is already +0.8 BLEU better than the
12
Note that the official 1st place OpenMT12 result was our
own system, so we can assure that these comparisons are ac-
curate.
1375
best MT12 system. The smaller improvement on
Chinese-English compared to Arabic-English is
consistent with the behavior of our baseline fea-
tures, as we show in the next section.
6.2 ?Simple Hierarchical? NIST Results
The baseline used in the last section is a highly-
engineered research system, which uses a wide
array of features that were refined over a num-
ber of years, and some of which require linguis-
tic resources. Because of this, the baseline BLEU
scores are much higher than a typical MT system
? especially a real-time, production engine which
must support many language pairs.
Therefore, we also present results using a
simpler version of our decoder which emulates
Chiang?s original Hiero implementation (Chiang,
2007). Specifically, this means that we don?t
use dependency-based rule extraction, and our de-
coder only contains the following MT features: (1)
rule probabilities, (2) n-gram Kneser-Ney LM, (3)
lexical smoothing, (4) target word count, (5) con-
cat rule penalty.
Results are shown in the third section of Table 3.
The ?Simple Hierarchical? Arabic-English system
is -6.4 BLEU worse than our strong baseline, and
would have ranked 10th place out of 11 systems
in the evaluation. When the NNJM features are
added to this system, we see an improvement of
+6.3 BLEU, which would have ranked 1st place in
the evaluation.
Effectively, this means that for Arabic-English,
the NNJM features are equivalent to the combined
improvements from the string-to-dependency
model plus all of the features listed in Section 5.1.
For Chinese-English, the ?Simple Hierarchical?
system only degrades by -3.2 BLEU compared
to our strongest baseline, and the NNJM features
produce a gain of +2.1 BLEU on top of that.
6.3 BOLT Web Forum Results
DARPA BOLT is a major research project with the
goal of improving translation of informal, dialec-
tical Arabic and Chinese into English. The BOLT
domain presented here is ?web forum,? which was
crawled from various Chinese and Egyptian Inter-
net forums by LDC. The BOLT parallel training
consists of all of the high-quality NIST training,
plus an additional 3 million words of translated
forum data provided by LDC. The tuning and test
sets consist of roughly 5000 segments each, with
2 references for Arabic and 3 for Chinese.
Results are shown in Table 4. The baseline here
uses the same feature set as the strong NIST sys-
tem. On Arabic, the total gain is +2.6 BLEU,
while on Chinese, the gain is +1.3 BLEU.
BOLT Test
Ar-En Ch-En
BLEU BLEU
Baseline (w/o RNNLM) 40.2 30.6
Baseline (w/ RNNLM) 41.3 30.9
+ S2T/L2R NNJM (Dec) 42.9 31.9
+ S2T NNLTM (Dec) 43.2 31.9
+ Other NNJMs (Resc) 43.9 32.2
Table 4: Primary results on Arabic-English and
Chinese-English BOLT Web Forum. Each row
includes the aggregate features from all previous
rows.
6.4 Effect of k-best Rescoring Only
Table 5 shows performance when our S2T/L2R
NNJM is used only in 1000-best rescoring, com-
pared to decoding. The primary purpose of this is
as a comparison to Le et al (2012), whose model
can only be used in k-best rescoring.
BOLT Test
Ar-En
Without With
RNNLM RNNLM
BLEU BLEU
Baseline 40.2 41.3
S2T/L2R NNJM (Resc) 41.7 41.6
S2T/L2R NNJM (Dec) 42.8 42.9
Table 5: Comparison of our primary NNJM in de-
coding vs. 1000-best rescoring.
We can see that the rescoring-only NNJM per-
forms very well when used on top of a baseline
without an RNNLM (+1.5 BLEU), but the gain on
top of the RNNLM is very small (+0.3 BLEU).
The gain from the decoding NNJM is large in both
cases (+2.6 BLEU w/o RNNLM, +1.6 BLEU w/
RNNLM). This demonstrates that the full power of
the NNJM can only be harnessed when it is used
in decoding. It is also interesting to see that the
RNNLM is no longer beneficial when the NNJM
is used.
1376
6.5 Effect of Neural Network Configuration
Table 6 shows results using the S2T/L2R NNJM
with various configurations. We can see that re-
ducing the source window size, layer size, or vo-
cab size will all degrade results. Increasing the
sizes beyond the default NNJM has almost no ef-
fect (102%). Also note that the target-only NNLM
(i.e., Source Window=0) only obtains 33% of the
improvements of the NNJM.
BOLT Test
Ar-En
BLEU % Gain
?Simple Hier.? Baseline 33.8 -
S2T/L2R NNJM (Dec) 38.4 100%
Source Window=7 38.3 98%
Source Window=5 38.2 96%
Source Window=3 37.8 87%
Source Window=0 35.3 33%
Layers=384x768x768 38.5 102%
Layers=192x512 38.1 93%
Layers=128x128 37.1 72%
Vocab=64,000 38.5 102%
Vocab=16,000 38.1 93%
Vocab=8,000 37.3 83%
Activation=Rectified Lin. 38.5 102%
Activation=Linear 37.3 76%
Table 6: Results with different neural net-
work architectures. The ?default? NNJM in
the second row uses these parameters: SW=11,
L=192x512x512, V=32,000, A=tanh. All mod-
els use a 3-word target history (i.e., 4-gram LM).
?Layers? refers to the size of the word embedding
followed by the hidden layers. ?Vocab? refers to
the size of the input and output vocabularies. ?%
Gain? is the BLEU gain over the baseline relative
to the default NNJM.
6.6 Effect of Speedups
All previous results use a self-normalized neural
network with two hidden layers. In Table 7, we
compare this to using a standard network (with
two hidden layers), as well as a pre-computed neu-
ral network.
13
The ?Simple Hierarchical? base-
line is used here because it more closely approx-
imates a real-time MT engine. For the sake of
speed, these experiments only use the S2T/L2R
NNJM+S2T NNLTM.
13
The difference in score for self-normalized vs. pre-
computed is entirely due to two vs. one hidden layers.
Each result from Table 7 corresponds to a row
in Table 2 of Section 2.4. We can see that go-
ing from the standard model to the pre-computed
model only reduces the BLEU improvement from
+6.4 to +6.1, while increasing the NNJM lookup
speed by a factor of 10,000x.
BOLT Test
Ar-En
BLEU Gain
?Simple Hier.? Baseline 33.8 -
Standard NNJM 40.2 +6.4
Self-Norm NNJM 40.1 +6.3
Pre-Computed NNJM 39.9 +6.1
Table 7: Results for the standard NNs vs. self-
normalized NNs vs. pre-computed NNs.
In Table 2 we showed that the cost of unique
lookups for the pre-computed NNJM is only
?0.001 seconds per source word. This does not
include the cost of n-gram creation or cached
lookups, which amount to ?0.03 seconds per
source word in our current implementation.
14
However, the n-grams created for the NNJM can
be shared with the Kneser-Ney LM, which reduces
the cost of that feature. Thus, the total cost in-
crease of using the NNJM+NNLTM features in
decoding is only ?0.01 seconds per source word.
In future work we will provide more detailed
analysis regarding the usability of the NNJM in a
low-latency, high-throughput MT engine.
7 Related Work
Although there has been a substantial amount of
past work in lexicalized joint models (Marino et
al., 2006; Crego and Yvon, 2010), nearly all of
these papers have used older statistical techniques
such as Kneser-Ney or Maximum Entropy. How-
ever, not only are these techniques intractable to
train with high-order context vectors, they also
lack the neural network?s ability to semantically
generalize (Mikolov et al, 2013) and learn non-
linear relationships.
A number of recent papers have proposed meth-
ods for creating neural network translation/joint
models, but nearly all of these works have ob-
tained much smaller BLEU improvements than
ours. For each related paper, we will briefly con-
14
In our decoder, roughly 95% of NNJM n-gram lookups
within the same sentence are duplicates.
1377
trast their methodology with our own and summa-
rize their BLEU improvements using scores taken
directly from the cited paper.
Auli et al (2013) use a fixed continuous-space
source representation, obtained from LDA (Blei
et al, 2003) or a source-only NNLM. Also, their
model is recurrent, so it cannot be used in decod-
ing. They obtain +0.2 BLEU improvement on top
of a target-only NNLM (25.6 vs. 25.8).
Schwenk (2012) predicts an entire target phrase
at a time, rather than a word at a time. He obtains
+0.3 BLEU improvement (24.8 vs. 25.1).
Zou et al (2013) estimate context-free bilingual
lexical similarity scores, rather than using a large
context. They obtain an +0.5 BLEU improvement
on Chinese-English (30.0 vs. 30.5).
Kalchbrenner and Blunsom (2013) implement
a convolutional recurrent NNJM. They score a
1000-best list using only their model and are able
to achieve the same BLEU as using all 12 standard
MT features (21.8 vs 21.7). However, additive re-
sults are not presented.
The most similar work that we know of is Le et
al. (2012). Le?s basic procedure is to re-order the
source to match the linear order of the target, and
then segment the hypothesis into minimal bilin-
gual phrase pairs. Then, he predicts each target
word given the previous bilingual phrases. How-
ever, Le?s formulation could only be used in k-
best rescoring, since it requires long-distance re-
ordering and a large target context.
Le?s model does obtain an impressive +1.7
BLEU gain on top of a baseline without an NNLM
(25.8 vs. 27.5). However, when compared to
the strongest baseline which includes an NNLM,
Le?s best models (S2T + T2S) only obtain an +0.6
BLEU improvement (26.9 vs. 27.5). This is con-
sistent with our rescoring-only result, which indi-
cates that k-best rescoring is too shallow to take
advantage of the power of a joint model.
Le?s model also uses minimal phrases rather
than being purely lexicalized, which has two main
downsides: (a) a number of complex, hand-crafted
heuristics are required to define phrase boundaries,
which may not transfer well to new languages, (b)
the effective vocabulary size is much larger, which
substantially increases data sparsity issues.
We should note that our best results use six sep-
arate models, whereas all previous work only uses
one or two models. However, we have demon-
strated that we can obtain 50%-80% of the to-
tal improvement with only one model (S2T/L2R
NNJM), and 70%-90% with only two models
(S2T/L2R NNJM + S2T NNLTM). Thus, the one
and two-model conditions still significantly out-
perform any past work.
8 Discussion
We have described a novel formulation for a neural
network-based machine translation joint model,
along with several simple variations of this model.
When used as MT decoding features, these models
are able to produce a gain of +3.0 BLEU on top of
a very strong and feature-rich baseline, as well as
a +6.3 BLEU gain on top of a simpler system.
Our model is remarkably simple ? it requires no
linguistic resources, no feature engineering, and
only a handful of hyper-parameters. It also has no
reliance on potentially fragile outside algorithms,
such as unsupervised word clustering. We con-
sider the simplicity to be a major advantage. Not
only does this suggest that it will generalize well to
new language pairs and domains, but it also sug-
gests that it will be straightforward for others to
replicate these results.
Overall, we believe that the following factors set
us apart from past work and allowed us to obtain
such significant improvements:
1. The ability to use the NNJM in decoding
rather than rescoring.
2. The use of a large bilingual context vector,
which is provided to the neural network in
?raw? form, rather than as the output of some
other algorithm.
3. The fact that the model is purely lexicalized,
which avoids both data sparsity and imple-
mentation complexity.
4. The large size of the network architecture.
5. The directional variation models.
One of the biggest goals of this work is to quell
any remaining doubts about the utility of neural
networks in machine translation. We believe that
there are large areas of research yet to be explored.
For example, creating a new type of decoder cen-
tered around a purely lexicalized neural network
model. Our short term ideas include using more
interesting types of context in our input vector
(such as source syntax), or using the NNJM to
model syntactic/semantic structure of the target.
1378
References
Michael Auli, Michel Galley, Chris Quirk, and Geof-
frey Zweig. 2013. Joint language and translation
modeling with recurrent neural networks. In Pro-
ceedings of the 2013 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1044?
1054, Seattle, Washington, USA, October. Associa-
tion for Computational Linguistics.
Yoshua Bengio, R?ejean Ducharme, Pascal Vincent, and
Christian Jauvin. 2003. A neural probabilistic lan-
guage model. Journal of Machine Learning Re-
search, 3:1137?1155.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alocation. J. Mach. Learn.
Res., 3:993?1022, March.
David Chiang, Kevin Knight, and Wei Wang. 2009.
11,001 new features for statistical machine transla-
tion. In HLT-NAACL, pages 218?226.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228.
Josep Maria Crego and Franc?ois Yvon. 2010. Factored
bilingual n-gram language models for statistical ma-
chine translation. Machine Translation, 24(2):159?
175.
Jacob Devlin and Spyros Matsoukas. 2012. Trait-
based hypothesis selection for machine translation.
In Proceedings of the 2012 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
NAACL HLT ?12, pages 528?532, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Jacob Devlin. 2009. Lexical features for statistical
machine translation. Master?s thesis, University of
Maryland.
Nizar Habash, Ryan Roth, Owen Rambow, Ramy Es-
kander, and Nadi Tomeh. 2013. Morphological
analysis and disambiguation for dialectal arabic. In
HLT-NAACL, pages 426?432.
Zhongqiang Huang, Jacob Devlin, and Rabih Zbib.
2013. Factored soft source syntactic constraints for
hierarchical machine translation. In EMNLP, pages
556?566.
Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent
continuous translation models.
Reinhard Kneser and Hermann Ney. 1995. Im-
proved backing-off for m-gram language modeling.
In Acoustics, Speech, and Signal Processing, 1995.
ICASSP-95., 1995 International Conference on, vol-
ume 1, pages 181?184. IEEE.
Hai-Son Le, Alexandre Allauzen, and Franc?ois Yvon.
2012. Continuous space translation models with
neural networks. In Proceedings of the 2012 Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, NAACL HLT ?12, pages 39?
48, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Yann LeCun, L?eon Bottou, Genevieve B Orr, and
Klaus-Robert M?uller. 1998. Efficient backprop. In
Neural networks: Tricks of the trade, pages 9?50.
Springer.
Jos?e B Marino, Rafael E Banchs, Josep M Crego, Adri`a
De Gispert, Patrik Lambert, Jos?e AR Fonollosa, and
Marta R Costa-Juss`a. 2006. N-gram-based machine
translation. Computational Linguistics, 32(4):527?
549.
Tomas Mikolov, Martin Karafi?at, Lukas Burget, Jan
Cernock?y, and Sanjeev Khudanpur. 2010. Recur-
rent neural network based language model. In IN-
TERSPEECH, pages 1045?1048.
Tomas Mikolov, Wen tau Yih, and Geoffrey Zweig.
2013. Linguistic regularities in continuous space
word representations. In HLT-NAACL, pages 746?
751.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio.
2012. On the difficulty of training recurrent neural
networks. arXiv preprint arXiv:1211.5063.
Jason Riesa, Ann Irvine, and Daniel Marcu. 2011.
Feature-rich language-independent syntax-based
alignment for statistical machine translation. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP ?11,
pages 497?507, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Ronald Rosenfeld. 1996. A maximum entropy ap-
proach to adaptive statistical language modeling.
Computer, Speech and Language, 10:187?228.
Antti Rosti, Bing Zhang, Spyros Matsoukas, and
Rich Schwartz. 2010. BBN system descrip-
tion for WMT10 system combination task. In
WMT/MetricsMATR, pages 321?326.
Holger Schwenk. 2010. Continuous-space language
models for statistical machine translation. Prague
Bull. Math. Linguistics, 93:137?146.
Holger Schwenk. 2012. Continuous space translation
models for phrase-based statistical machine transla-
tion. In COLING (Posters), pages 1071?1080.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2010.
String-to-dependency statistical machine transla-
tion. Computational Linguistics, 36(4):649?671,
December.
1379
Matthew Snover, Bonnie Dorr, and Richard Schwartz.
2008. Language and translation model adaptation
using comparable corpora. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP ?08, pages 857?866,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Makoto Tanaka, Yasuhara Toru, Jun-ya Yamamoto, and
Mikio Norimatsu. 2013. An efficient language
model using double-array structures.
Ashish Vaswani, Yinggong Zhao, Victoria Fossum,
and David Chiang. 2013. Decoding with large-
scale neural language models improves translation.
In Proceedings of the 2013 Conference on Em-
pirical Methods in Natural Language Processing,
pages 1387?1392, Seattle, Washington, USA, Oc-
tober. Association for Computational Linguistics.
Will Y Zou, Richard Socher, Daniel Cer, and Christo-
pher D Manning. 2013. Bilingual word embeddings
for phrase-based machine translation. In Proceed-
ings of the 2013 Conference on Empirical Methods
in Natural Language Processing, pages 1393?1398.
1380
