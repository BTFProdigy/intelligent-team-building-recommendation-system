Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 367?375,
Beijing, August 2010
RTG based surface realisation for TAG
Claire Gardent
CNRS/LORIA
claire.gardent@loria.fr
Laura Perez-Beltrachini
Universite? Henri Poincare?/LORIA
laura.perez@loria.fr
Abstract
Surface realisation with grammars inte-
grating flat semantics is known to be NP
complete. In this paper, we present a new
algorithm for surface realisation based on
Feature Based Tree Adjoining Grammar
(FTAG) which draws on the observation
that an FTAG can be translated into a Reg-
ular Tree Grammar describing its deriva-
tion trees. We carry out an extensive test-
ing of several variants of this algorithm
using an automatically produced testsuite
and compare the results obtained with
those obtained using GenI, another FTAG
based surface realiser.
1 Introduction
As shown in (Brew, 1992; Koller and Striegnitz,
2002), Surface Realisation is NP-complete. Var-
ious optimisation techniques have therefore been
proposed to help improve practical runtimes. For
instance, (Kay, 1996) proposes to reduce the num-
ber of constituents built during realisation by only
considering for combination constituents with non
overlapping semantics and compatible indices.
(Kay, 1996; Carroll and Oepen, 2005; Gardent
and Kow, 2007) propose various techniques to re-
strict the combinatorics induced by intersective
modifiers all applying to the same structure. And
(Koller and Striegnitz, 2002; Gardent and Kow,
2007) describe two alternative techniques for re-
ducing the initial search space.
In this paper, we focus on the optimisation
mechanisms of two TAG based surface realisers
namely, GENI (Gardent and Kow, 2007) and the
algorithm we present in this paper namely, RT-
GEN (Perez-Beltrachini, 2009). GENI?s optimisa-
tion includes both a filtering process whose aim is
to reduce the initial search space and a two step,
?substitution before adjunction?, tree combination
phase whose effect is to delay modifier adjunc-
tion thereby reducing the number of intermediate
structures being built. In RTGEN on the other
hand, the initial FTAG is converted to a Regu-
lar Tree Grammar (RTG) describing its derivation
trees and an Earley algorithm, including sharing
and packing, is used to optimise tree combination.
We compare GENI with several variants of the
proposed RTGEN algorithm using an automati-
cally produced testsuite of 2 679 input formulae
and relate the RTGEN approach to existing work
on surface realisation optimisation.
The paper is structured as follows. We first
present the grammar used by both GENI and RT-
GEN, namely SEMXTAG (Section 2). We then de-
scribe the two surface realisation algorithms (Sec-
tion 3). In Section 4, we describe the empirical
evaluation carried out and present the results. Fi-
nally, Section 5 situates RTGEN with respect to
related work on surface realisation optimisation.
2 SemXTag
The grammar (SEMXTAG) used by GENI and
RTGEN is a Feature-Based Lexicalised Tree
Adjoining Grammar (FTAG) augmented with a
unification-based semantics as described in (Gar-
dent and Kallmeyer, 2003). We briefly introduce
each of these components and describe the gram-
mar coverage. We then show how this FTAG can
be converted to an RTG describing its derivation
trees.
367
2.1 FTAG.
A Feature-based TAG (Vijay-Shanker and Joshi,
1988) consists of a set of (auxiliary or initial) el-
ementary trees and of two tree-composition oper-
ations: substitution and adjunction. Initial trees
are trees whose leaves are labeled with substitu-
tion nodes (marked with a downarrow) or termi-
nal categories. Auxiliary trees are distinguished
by a foot node (marked with a star) whose cate-
gory must be the same as that of the root node.
Substitution inserts a tree onto a substitution node
of some other tree while adjunction inserts an aux-
iliary tree into a tree. In an FTAG, the tree nodes
are furthermore decorated with two feature struc-
tures (called top and bottom) which are unified
during derivation as follows. On substitution, the
top of the substitution node is unified with the top
of the root node of the tree being substituted in.
On adjunction, the top of the root of the auxiliary
tree is unified with the top of the node where ad-
junction takes place; and the bottom features of
the foot node are unified with the bottom features
of this node. At the end of a derivation, the top
and bottom of all nodes in the derived tree are
unified. Finally, each sentence derivation in an
FTAG is associated with both a derived tree rep-
resenting the phrase structure of the sentence and
a derivation tree recording how the correspond-
ing elementary trees were combined to form the
derived tree. Nodes in a derivation tree are la-
belled with the name of a TAG elementary tree.
Edges are labelled with a description of the opera-
tion used to combine the TAG trees whose names
label the edge vertices.
2.2 FTAG with semantics.
To associate semantic representations with natu-
ral language expressions, the FTAG is modified as
proposed in (Gardent and Kallmeyer, 2003).
NPj
John
name(j,john)
Sc
NP?s VPcb
Vba
runs
run(a,s)
VPx
often VP*
often(x)
? name(j,john), run(a,j), often(a)
Figure 1: Flat Semantics for ?John often runs?
Each elementary tree is associated with a flat
semantic representation. For instance, in Fig-
ure 1,1 the trees for John, runs and often are asso-
ciated with the semantics name(j,john), run(a,s)
and often(x) respectively. Importantly, the argu-
ments of a semantic functor are represented by
unification variables which occur both in the se-
mantic representation of this functor and on some
nodes of the associated syntactic tree. For in-
stance in Figure 1, the semantic index s occur-
ring in the semantic representation of runs also
occurs on the subject substitution node of the as-
sociated elementary tree. The value of semantic
arguments is determined by the unifications re-
sulting from adjunction and substitution. For in-
stance, the semantic index s in the tree for runs is
unified during substitution with the semantic in-
dex labelling the root node of the tree for John.
As a result, the semantics of John often runs is
{name(j,john),run(a,j),often(a)}.
2.3 SemXTAG.
SEMXTAG is an FTAG for English augmented
with a unification based compositional semantics
of the type described above. Its syntactic cover-
age approaches that of XTAG, the FTAG devel-
oped for English by the XTAG group (The XTAG
Research Group, 2001). Like this grammar, it
contains around 1300 elementary trees and cov-
ers auxiliaries, copula, raising and small clause
constructions, topicalization, relative clauses, in-
finitives, gerunds, passives, adjuncts, ditransitives
and datives, ergatives, it-clefts, wh-clefts, PRO
constructions, noun-noun modification, extraposi-
tion, sentential adjuncts, imperatives and resulta-
tives.
2.4 Converting SemXTAG to RTG
As shown in (Schmitz and Le Roux, 2008), an
FTAG can be converted to a Regular Tree Gram-
mar describing its derivation tree. In this section,
we briefly sketch this conversion process. For a
more precise description of this FTAG to RTG
conversion, the reader is referred to (Schmitz and
Le Roux, 2008).
1Cx/Cx abbreviate a node with category C and a
top/bottom feature structure including the feature-value pair
{ index : x}.
368
In the FTAG-to-RTG conversion, each SEMX-
TAG elementary tree is converted to a rule that
models its contribution to a TAG derivation tree.
A TAG derivation involves the selection of an ini-
tial tree, which has some nodes requiring substi-
tution and some permitting adjunction. Let us
think of the potential adjunction sites as requiring,
rather than permitting, adjunction, but such that
the requirement can be satisfied by ?null? adjunc-
tion. Inserting another tree into this initial tree sat-
isfies one of the substitution or adjunction require-
ments, but introduces some new requirements into
the resulting tree, in the form of its own substitu-
tion nodes and adjunction sites.
Thus, intuitively, the RTG representation of a
SEMXTAG elementary tree is a rule that rewrites
the satisfied requirement as a local tree whose root
is a unique identifier of the tree and whose leaves
are the introduced requirements. A requirement
of a substitution or adjunction of a tree of root
category X is written as XS or XA, respectively.
Here, for example, is the translation to RTG of the
FTAG tree (minus semantics) for run in Figure 1,
using the word anchoring the tree as its identifier
(the upperscripts abbreviates features structures:
b/t refers to the bottom/top feature structure and
the upper case letters to the semantic index value,
[idx : X] is abbreviated to X):
S[t:T ]S ? runs(S
[t:T,b:C]
A NP
[t:S]
S V P
[t:C,b:B]
A V
[t:B,b:A]
A )
The semantics of the SemXTAG tree are carried
over as-is to the corresponding RTG rule. Fur-
ther, the feature structures labelling the nodes of
the SemXTAG tree are converted into the RTG
rules so as to correctly interact with substitution
and adjunction (see (Schmitz and Le Roux, 2008)
for more details on this part of the conversion pro-
cess).
To account for the optionality of adjunction,
there are additional rules allowing any adjunction
requirement to be rewritten as the symbol ?, a ter-
minal symbol of the RTG.
The terminal symbols of the RTG are thus the
tree identifiers and the symbol ?, and its non-
terminals are XS and XA for each terminal or
non-terminal X of SemXTAG.
3 TAG-based surface realisation
We now present RTGEN and describe GENI, and
compare the optimisations they propose to deal
with the task complexity.
GENI and RTGEN are similar on several points.
They use the same grammar, namely SEMXTAG
(cf. Section 2). Further, they both pipeline three
main steps. First, lexical selection selects from
the grammar those elementary trees whose seman-
tics subsumes part of the input semantics. Second,
the tree combining phase systematically tries to
combine trees using substitution and adjunction.
Third, the retrieval phase extracts the yields of
the complete derived trees, thereby producing the
generated sentence(s).
GENI and RTGEN differ however with respect
to the trees they are working with (derived trees
in GENI vs derivation trees in RTGEN). They also
differ in how tree combination is handled. We now
describe these differences in more detail and ex-
plain how each approach address the complexity
issue.
3.1 GenI
The tree combining phase in GENI falls into two
main steps namely, filtering and tree combining.
Filtering. The so-called polarity filtering step
aims to reduce the initial search space. It elim-
inates from the initial search space all those sets
of TAG elementary trees which cover the input se-
mantics but cannot possibly lead to a valid derived
tree. In specific, this filtering removes all tree sets
covering the input semantics such that either the
category of a substitution node cannot be canceled
out by that of the root node of a different tree;
or a root node fails to have a matching substitu-
tion site. Importantly, this filtering relies solely
on categorial information ? feature information is
not used. Furthermore, auxiliary trees have no im-
pact on filtering since they provide and require the
same category thereby being ?polarity neutral el-
ements?.
Tree combining. The tree combining algorithm
used after filtering has taken place, is a bottom-up
tabular algorithm (Kay, 1996) optimised for TAGs.
This step, unlike the first, uses all the features
369
present in the grammar. To handle intersective
modifiers, the delayed modifiers insertion strategy
from (Carroll et al, 1999) is adapted to TAG as
follows. First, all possible derived trees are ob-
tained using only substitution. Next, adjunction
is applied. Although the number of intermediate
structures generated is still 2n for n modifiers, this
strategy has the effect of blocking these 2n struc-
tures from multiplying out with other structures in
the chart.
3.2 RTGen
RTGen synthesises different techniques that have
been observed in the past to improve surface re-
alisation runtimes. We first describe these tech-
niques i.e., the main features of RTGEN. We
then present three alternative ways of implement-
ing RTGEN which will be compared in the evalu-
ation.
3.2.1 RTGen?s main features
A main feature of RTGEN is that it focuses on
building derivation rather than derived trees. More
specifically, the first two steps of the surface real-
isation process (lexical selection, tree combining)
manipulate RTG rules describing the contribution
of the SEMXTAG elementary trees to the deriva-
tion tree rather than the elementary tree them-
selves. The derived trees needed to produce actual
sentences are only produced in the last phase i.e.,
the retrieval phase.
This strategy is inspired from a similar ap-
proach described in (Koller and Striegnitz, 2002)
which was shown to be competitive with state of
the art realisers on a small sample of example in-
put chosen for their inherent complexity. (Koller
and Striegnitz, 2002)?s approach combines trees
using a constraint based dependency parser rather
than an Earley algorithm so that it is difficult
to assess how much of the efficiency is due to
the parser and how much to the grammar con-
version. Intuitively however, the motivation un-
derlying the construction of a derivation rather
than a derived tree is that efficiency might be in-
creased because the context free derivation trees
(i) are simpler than the mildly context sensitive
trees generated by an FTAG and (ii) permit draw-
ing on efficient parsing and surface realisation al-
gorithms designed for such grammars.
Second, RTGEN makes use of the now standard
semantic criteria proposed in (Kay, 1996; Carroll
et al, 1999) to reduce the number of combinations
tried out by the realiser. On the one hand, two con-
stituents are combined by the algorithm?s infer-
ence rules only if they cover disjoint parts of the
input semantics. On the other hand, the seman-
tic indices present in both the input formula and
the lexically retrieved RTG trees are used to pre-
vent the generation of intermediate structures that
are not compatible with the input semantics. For
instance, given the input formula for ?John likes
Mary?, semantic indices will block the generation
of ?likes John? because this constituent requires
that the constituent for ?John? fills the patient slot
of ?likes? whereas the input semantics requires
that it fills the agent slot. In addition, chart items
in RTGEN are indexed by semantic indices to ef-
ficiently select chart items for combination.
Third, RTGEN implements a standard Earley
algorithm complete with sharing and packing.
Sharing allows for intermediate structures that are
common to several derivations to be represented
only once ? in addition to not being recomputed
each time. Packing means that partial derivation
trees with identical semantic coverage and similar
combinatorics (same number and type of substi-
tution and adjunction requirements) are grouped
together and that only one representative of such
groups is stored in the chart. In this way, interme-
diate structures covering the same set of intersec-
tive modifiers in a different order are only repre-
sented once and the negative impact of intersec-
tive modifiers is lessened (cf. (Brew, 1992)). . As
(Carroll and Oepen, 2005) have shown, packing
and sharing are important factors in improving ef-
ficiency. In particular, they show that an algorithm
with packing and sharing clearly outtperforms the
same algorithm without packing and sharing giv-
ing an up to 50 times speed-up for inputs with
large numbers of realizations.
3.2.2 Three ways to implement RTGen
Depending on how much linguistic information
(i.e. feature constraints from the feature struc-
tures) is preserved in the RTG rules, several RT-
GEN configurations can be tried out which each
370
reflect a different division of labour between con-
straint solving and structure building. To experi-
ment with these several configurations, we exploit
the fact that the FTAG-to-RTG conversion proce-
dure developed by Sylvain Schmitz permits spec-
ifying which features should be preserved by the
conversion.
RTGen-all. In this configuration, all the feature
structure information present in the SEMXTAG el-
ementary trees is carried over to the RTG rules.
As a result, tree combining and constraint solving
proceed simultaneously and the generated parse
forest contains the derivation trees of all the out-
put sentences.
RTGen-level0. In the RTGen-level0 configura-
tion, only the syntactic category and the seman-
tic features are preserved by the conversion. As
a result, the grammar information used by the
(derivation) tree building phase is comparable to
that used by GENI filtering step. In both cases,
the aim is to detect those sets of elementary trees
which cover the input semantics and such that all
syntactic requirements are satisfied while no syn-
tactic resource is left out. A further step is addi-
tionally needed to produce only those trees which
can be built from these tree sets when applying the
constraints imposed by other features. In GENI,
this additional step is carried out by the tree com-
bining phase, in RTGEN, it is realised by the ex-
traction phase i.e., the phase that constructs the
derived trees from the derivation trees produced
by the tree combining phase.
RTGen-selective. Contrary to parsing, surface
realisation only accesses the morphological lex-
icon last i.e., after sentence trees are built. Be-
cause throughout the tree combining phase, lem-
mas are handled rather than forms, much of the
morpho-syntactic feature information which is
necessary to block the construction of ill-formed
constituents is simply not available. It is therefore
meaningful to only include in the tree combining
phase those features whose value is available at
tree combining time. In a third experiment, we au-
tomatically identified those features from the ob-
served feature structure unification failures during
runs of the realisation algorithm. We then use only
these features (in combination with the semantic
features and with categorial information) during
tree combining.
4 Evaluation
To evaluate the impact of the different optimisa-
tion techniques discussed in the previous section,
we use two benchmarks generated automatically
from SEMXTAG (Gottesman, 2009).
The first benchmark (MODIFIERS) was de-
signed to test the realisers on cases involving in-
tersective modifiers. It includes 1 789 input for-
mulae with a varying number (from 0 to 4 modifi-
cations), type (N and VP modifications) and distri-
bution of intersective modifiers (n modifiers dis-
tributed differently over the predicate argument
structures). For instance, the formula in (1) in-
volves 2 N and 1 VP modification. Further,
it combines lexical ambiguity with modification
complexities, i.e. for the snore modifier the gram-
mar provides 10 trees.
(1) l1 : ?(x1, hr, hs), hr ? l2, hs ? l3, l2 :
man(x1), l2 : snoring(e1, x1), l2 : big(x1), l3 :
sleep(e2, x1), l4 : soundly(e2)
(A snoring big man sleeps soundly)
The second benchmark (COMPLEXITY) was
designed to test overall performance on cases of
differing complexity (input formulae of increas-
ing length, involving verbs with a various number
and types of arguments and with a varying num-
ber of and types of modifiers). It contains 890 dis-
tinct cases. A sample formula extracted from this
benchmark is shown in (2), which includes one
modification and to different verb types.
(2) h1 ? l4, l0 : want(e, h1), l1 : ?(x1, hr, hs), hr ?
l1, hs ? l0, l1 : man(x1), l1 : snoring(e1, x1), l3 :
?(x2, hp, hw , hu), hp ? l3, hw ? l4, hu ? l5, l3 :
monkey(x2), l4 : eat(e2, x2, e3), l5 : sleep(e3, x2)
(The snoring man wants the monkey to sleep)
To evaluate GENI and the various configurations
of RTGEN (RTGEN-all, RTGEN-level0, RTGEN-
selective), we ran the 4 algorithms in batch mode
on the two benchmarks and collected the follow-
ing data for each test case:
? Packed chart size : the number of chart items
built. This feature is only aplicable to RTGen
as GENI does not implement packing.
371
? Unpacked chart size : the number of interme-
diate and final structures available after un-
packing (or at the end of the tree combining
process in the case of GENI).
? Initial Search Space (ISS) : the number of all
possible combinations of elementary trees to
be explored given the result of lexical selec-
tion on the input semantics. That is, the prod-
uct of the number of FTAG elementary trees
selected by each literal in the input seman-
tics.
? Generation forest (GF) : the number of
derivation trees covering the input semantics.
The graph in Figure 2 shows the differences be-
tween the different strategies with respect to the
unpacked chart size metric.
A first observation is that RTGEN-all outper-
forms GENI in terms of intermediate structures
built . In other words, the Earley sharing and
packing strategy is more effective in reducing the
number of constituents built than the filtering and
substitution-before-adjunction optimisations used
by GENI. In fact, even when no feature informa-
tion is used at all (RTGEN-level0 plot), for more
complex test cases, packing and sharing is more
effective in reducing the chart size than filtering
and operation ordering.
Another interesting observation is that RTGEN-
all and RTGEN-selective have the same impact on
chart size (their plots coincide). This is unsurpris-
ing since the features used by RTGEN-selective
have been selected based on their ability to block
constituent combination. The features used in
RTGEN-selective mode are wh, xp, assign-comp,
mode, definite, inv, assign-case, rel-clause,
extracted and phon, in addition to the categorial
and semantic information. In other words, using
all 42 SEMXTAG grammar features has the same
impact on search space pruning as using only a
small subset of them. As explained in the previ-
ous section, this is probably due to the fact that
contrary to parsing, surface realisation only ac-
cesses the morphological lexicon after tree com-
bining takes place. Another possibility is that the
grammar is under constrained and that feature val-
ues are missing thereby inducing overgeneration.
Zooming in on cases involving three modifiers,
0 1 2 3 4 5 6 7
103
104
p
p
p
p
p
p
number of modifiers
u
n
pa
ck
ed
ch
ar
ts
iz
e
RTGEN-all
RTGEN-level0
p RTGEN-selective
GENI
Figure 2: Performance of realisation approaches
on the MODIFIERS benchmark, average unpacked
chart size as a function of the number of modifiers.
we show in Table 1 the average results for various
efficiency metrics 2. This provides a more detail
view of the performance of the differences among
the three RTGEN variants.
strategy GF chart unpacked-chart seconds
RTGen-all 15.05 918.31 2,538.98 0.99
RTGen-level0 1,118.06 2,018 6,898.28 1.41
RTGen-selective 27.08 910.34 2,531.23 0.44
Table 1: Average results on 610 test cases from
the MODIFIERS benchmark. Each test case has
3 modifications, distributed in various ways be-
tween adjectival and adverbial modifications. The
second column, Generation Forest (GF), is the
number of derivation trees present in the gener-
ated parse forest. The third and fourth columns
show the chart and unpacked chart sizes, respec-
tively. The last column shows the runtime in sec-
onds.
This data shows that running RTGEN with no
feature information leads not only to an increased
chart size but also to runtimes that are higher in
average than for full surface realisation i.e., reali-
sation using the full grammar complete with con-
2The two realisers being implemented in different
programming languages (RTGEN uses Prolog and GENI
Haskell), runtimes comparisons are not necessarily very
meaningful. Additionally, GENI does not provide time statis-
tics. After adding this functionality to GENI, we found that
overall GENI is faster on simple cases but slower on more
complex ones. We are currently working on optimising RT-
GEN prolog implementation before carrying out a full scale
runtime comparison.
372
0-
10
0
10
0-
10
00
10
00
-
50
00
50
00
-
10
00
0
10
00
0-
10
00
00
10
00
00
-
50
00
00
50
00
00
-
10
00
00
0
m
o
re
th
an
10
00
00
0
102
103
104
105
106
p
p p
p p
p p p
Initial Search Space (ISS) size
u
n
pa
ck
ed
ch
ar
ts
iz
e
RTGEN-all
RTGEN-level0
p RTGEN-selective
GENI
Figure 3: Performance of realisation approaches
on the COMPLEXITY benchmark, average un-
packed chart size as a function of the ISS com-
plexity.
straints.
Interestingly, it also shows that the selective
mode (RTGEN-selective) permits improving run-
times while achieving almost perfect disambigua-
tion in that the average number of derivation trees
(GF) produced is close to that produced when
using all features. The differences between the
two generation forests stems from packing. Using
only a subset of features favors packing, thereby
reducing the number of chart items built, but in-
troduces over- generation.
Graph 3 and Table 2 confirm the results ob-
tained using the MODIFIERS benchmark on a test-
set (COMPLEXITY) where input complexity varies
not only with respect to modification but also with
respect to the length of the input and to the de-
gree of lexical ambiguity. Typically, in a TAG, one
word or one semantic literal may be associated ei-
ther with one tree or with up to several hundred
trees (e.g., ditransitive verbs and verbs with sev-
eral subcategorisation types). By varying the type
and the number of verbs selected by the seman-
tic literals contained in the input semantics, the
COMPLEXITY benchmark provides a more exten-
sive way to test performance on cases of varying
complexity.
strategy GF chart unpacked-chart seconds
RTGen-all 14.77 693.39 2,427.82 0.81
RTGen-level0 162.02 2,114.16 6,954.84 1.09
RTGen-selective 15.31 692.9 2,427.2 0.36
Table 2: Average results on 335 cases with
10000 < ISS ? 100000, from the COMPLEXITY
benchmark. The columns show the same perfor-
mance metrics as in Table 1.
5 Related work
Much work has already been done on optimising
surface realisation. Because surface realisation
often draws on parsing techniques, work on pars-
ing optimisation is also relevant. In this section,
we briefly relate our proposal to another gram-
mar converting approach (Koller and Striegnitz,
2002); to another chart based approach (Carroll
and Oepen, 2005); and to approaches based on
statistical pruning (White, 2004; Bangalore and
Rambow, 2000).
5.1 Optimising surface realisation
Encoding into another grammatical formalism.
As already mentioned, the RTGEN approach is
closely related to the work of (Koller and Strieg-
nitz, 2002) where the XTAG grammar is con-
verted to a dependency grammar capturing its
derivation trees. This conversion enables the use
of a constraint based dependency parser, a parser
which was specifically developed for the efficient
parsing of free word order languages and is shown
to support an efficient handling of both lexical and
modifier attachment ambiguity.
Our proposal differs from this approach in three
main ways. First, contrary to XTAG, SEMX-
TAG integrates a full-fledged, unification based
compositional semantics thereby allowing for a
principled coupling between semantic represen-
tations and natural language expressions. Sec-
ond, the grammar conversion and the feature-
based RTGs used by RTGEN accurately trans-
lates the full range of unification mechanisms em-
ployed in FTAG wheras the conversion described
by (Koller and Striegnitz, 2002) does not take
into account feature structure information. Third,
the RTGEN approach was extensively tested on a
large benchmark using 3 different configurations
whilst (Koller and Striegnitz, 2002) results are re-
373
stricted to a few hand constructed example inputs.
Chart generation algorithm optimisations.
(Carroll and Oepen, 2005) provides an extensive
and detailed study of how various techniques used
to optimise parsing and surface realisation impact
the efficiency of a surface realiser based on a large
coverage Head-Driven Phrase Structure grammar.
Because they use different grammars, gram-
mar formalisms and different benchmarks, it is
difficult to compare the RTGEN and the HPSG
approach. However, one point is put forward
by (Carroll and Oepen, 2005) which it would
be interesting to integrate in RTGEN(Carroll and
Oepen, 2005) show that for packing to be effi-
cient, it is important that equivalence be checked
through subsumption, not through equality. RT-
GEN also implements a packing mechanism with
subsumption check, i.e. different ways of cov-
ering the same subset of the input semantics are
grouped together and represented in the chart by
the most general one. One difference however it
that RTGEN will pack analyses together as long
as the new ones are more specific cases. It will
not go backwards to recalculate the packing made
so far if a more general item is found (Stefan and
John, 2000). In this case the algorithm will pack
them under two different groups.
Statistical pruning. Various probabilistic tech-
niques have been proposed in surface realisation
to improve e.g., lexical selection, the handling of
intersective modifiers or ranking. For instance,
(Bangalore and Rambow, 2000) uses a tree model
to produce a single most probable lexical selec-
tion while in White?s system, the best paraphrase
is determined on the basis of n-gram scores. Fur-
ther, to address the fact that there are n! ways
to combine any n modifiers with a single con-
stituent, (White, 2004) proposes to use a language
model to prune the chart of identical edges rep-
resenting different modifier permutations, e.g., to
choose between fierce black cat and black fierce
cat. Similarly, (Bangalore and Rambow, 2000) as-
sumes a single derivation tree that encodes a word
lattice (a {fierce black, black fierce} cat), and uses
statistical knowledge to select the best linearisa-
tion. Our approach differs from these approaches
in that lexical selection is not filtered, intersective
modifiers are handled by the grammar (constraints
on the respective order of adjectives) and the chart
packing strategy (for optimisation), and ranking is
not performed. We are currently exploring the use
of Optimality Theory for ranking.
6 Conclusion
We presented RTGEN, a novel surface realiser for
FTAG grammars which builds on the observation
that an FTAG can be translated to a regular tree
grammar describing its derivation trees. Using
automatically constructed benchmarks, we com-
pared the performance of this realiser with that of
GENI, another state of the art realiser for FTAG.
We showed that RTGEN outperforms GENI in
terms of space i.e. that the Earley sharing and
packing strategy is more effective in reducing the
number of constituents built than the filtering and
substitution-before-adjunction optimisations used
by GENI. Moreover, we investigated three ways
of interleaving phrase structure and feature struc-
ture constraints and showed that, given a naive
constraint solving approach, the interleaving ap-
proach with selective features seems to provide
the best space/runtimes compromise.
Future work will concentrate on further investi-
gating the interplay in surface realisation between
phrase structure and feature structure constraints.
In particular, (Maxwell and Kaplan, 1994) shows
that a more sophisticated approach to constraint
solving and to its interaction with chart process-
ing renders the non interleaved approach more ef-
fective than the interleaved one. We plan to exam-
ine whether this observation applies to SEMXTAG
and RTGEN. Further, we intend to integrate Op-
timality Theory constraints in RTGEN so as sup-
port ranking of multiple outputs. Finally, we want
to further optimise RTGEN on intersective modi-
fiers using one the methods mentioned in Section
5.
References
Bangalore, S. and O. Rambow. 2000. Using TAGs, a
tree model and a language model for generation. In
Proceedings of TAG+5, Paris, France.
Brew, Chris. 1992. Letting the cat out of the bag:
generation for shake-and-bake mt. In Proceedings
374
of the 14th conference on Computational linguistics,
pages 610?616, Morristown, NJ, USA. Association
for Computational Linguistics.
Carroll, J. and S. Oepen. 2005. High efficiency re-
alization for a wide-coverage unification grammar.
2nd IJCNLP.
Carroll, J., A. Copestake, D. Flickinger, and
V. Paznan?ski. 1999. An efficient chart generator
for (semi-)lexicalist grammars. In Proceedings of
EWNLG ?99.
Gardent, C. and L. Kallmeyer. 2003. Semantic con-
struction in FTAG. In 10th EACL, Budapest, Hun-
gary.
Gardent, C. and E. Kow. 2007. Spotting overgenera-
tion suspect. In 11th European Workshop on Natu-
ral Language Generation (ENLG).
Gottesman, B. 2009. Generating examples. Mas-
ter?s thesis, Erasmus Mundus Master Language and
Communication Technology, Saarbrucken/Nancy.
Kay, Martin. 1996. Chart generation. In Proceedings
of the 34th annual meeting on Association for Com-
putational Linguistics, pages 200?204, Morristown,
NJ, USA. Association for Computational Linguis-
tics.
Koller, A. and K. Striegnitz. 2002. Generation as de-
pendency parsing. In Proceedings of the 40th ACL,
Philadelphia.
Maxwell, J. and R. Kaplan. 1994. The interface be-
tween phrasal and functional constraints. Computa-
tional Linguistics, 19(4).
Perez-Beltrachini, L. 2009. Using regular tree
grammars to reduce the search space in surface
realisation. Master?s thesis, Erasmus Mundus
Master Language and Communication Technology,
Nancy/Bolzano.
Schmitz, S. and J. Le Roux. 2008. Feature uni-
fication in TAG derivation trees. In Gardent, C.
and A. Sarkar, editors, Proceedings of the 9th In-
ternational Workshop on Tree Adjoining Grammars
and Related Formalisms (TAG+?08), pages 141?
148, Tu?bingen, Germany.
Stefan, Oepen and Carroll John. 2000. Parser engi-
neering and performance profiling. Journal of Nat-
ural Language Engineering, 6(1):81?98.
The XTAG Research Group. 2001. A lexicalised tree
adjoining grammar for english. Technical report,
Institute for Research in Cognitive Science, Univer-
sity of Pennsylvannia.
Vijay-Shanker, K. and AK Joshi. 1988. Feature Struc-
tures Based Tree Adjoining Grammars. Proceed-
ings of the 12th conference on Computational lin-
guistics, 55:v2.
White, M. 2004. Reining in CCG chart realization. In
INLG, pages 182?191.
375
Coling 2010: Poster Volume, pages 338?346,
Beijing, August 2010
Comparing the performance of two TAG-based surface realisers using
controlled grammar traversal
Claire Gardent
CNRS/LORIA
claire.gardent@loria.fr
Benjamin Gottesman
acrolinx GmbH
ben.gottesman@acrolinx.com
Laura Perez-Beltrachini
Universite? Henri Poincare?/LORIA
laura.perez@loria.fr
Abstract
We present GENSEM, a tool for generat-
ing input semantic representations for two
sentence generators based on the same re-
versible Tree Adjoining Grammar. We
then show how GENSEM can be used
to produced large and controlled bench-
marks and test the relative performance of
these generators.
1 Introduction
Although computational grammars are mostly
used for parsing, they can also be used to gener-
ate sentences. This has been done, for instance,
to detect overgeneration by the grammar (Gardent
and Kow, 2007). Sentences that are generated but
are ungrammatical indicate flaws in the grammar.
This has also been done to test a parser (Neder-
hof, 1996; Purdom, 1972). Using the sentences
generated from the grammar ensures that the sen-
tences given to the parser are in the language it de-
fines. Hence a parse failure necessarily indicates
a flaw in the parser?s design as opposed to a lack
of coverage by the grammar.
Here we investigate a third option, namely, the
focused benchmarking of sentence realisers based
on reversible grammars, i.e. on grammars that can
be used both to produce sentences from a semantic
representation and semantic representations from
a sentence.
More specifically, we present a linguistically-
controlled grammar traversal algorithm for Tree
Adjoining Grammar (TAG) which, when applied
to a reversible TAG, permits producing arbitrarily
many of the semantic representations associated
by this TAG with the sentences it generates. We
then show that the semantic representations thus
produced can be used to compare the relative per-
formance of two sentence generators based on this
grammar.
Although the present paper concentrates on
Tree Adjoining Grammar realisers, it is worth
pointing out that the semantic representations pro-
duced could potentially be used to evaluate any
surface realiser whose input is a flat semantic for-
mula.
Section 2 discusses related work and motivates
the approach. Section 3 presents GENSEM, the
DCG-based grammar traversal algorithm we de-
veloped. We show, in particular, that the use of
a DCG permits controlling grammar traversal in
such a way as to systematically generate sets of se-
mantic representations covering certain computa-
tionally or linguistically interesting cases. Finally,
Section 4 reports on the benchmarking of two sur-
face realisers with respect to a GENSEM-produced
benchmark.
2 Motivations
Previous work on benchmark construction for
testing the performance of surface realisers falls
into two camps depending on whether or not the
realiser uses a reversible grammar, that is, a gram-
mar that can be used for both parsing and genera-
tion.
To test a surface realiser based on a large
reversible Head-Driven Phrase Structure Gram-
mar (HPSG), Carroll et al (1999) use a small
test set of two hand-constructed and 40 parsing-
derived cases to test the impact of intersective
modifiers1 on generation performance. More re-
cently, Carroll and Oepen (2005) present a perfor-
1As first noted by Brew (1992) and Kay (1996), given a
set of n modifiers all modifying the same structure, all pos-
sible intermediate structures will be constructed, i.e., 2n+1.
338
mance evaluation which uses as a benchmark the
set of semantic representations produced by pars-
ing 130 sentences from the Penn Treebank and
manually selecting the correct semantic represen-
tations. Finally, White (2004) profiles a CCG2-
based sentence realiser using two domain-focused
reversible CCGs to produce two test suites of 549
and 276 ? semantic formula, target sentence ?
pairs, respectively.
For realisers that are not based on a reversible
grammar, there are approaches which derive large
sets of realiser input from the Penn Treebank
(PTB). For example, Langkilde-Geary (2002)
proposes to translate the PTB annotations into a
format accepted by her sentence generator Halo-
gen. The output of this generator can then be au-
tomatically compared with the PTB sentence from
which the corresponding input was derived. Simi-
larly, Callaway (2003) builds an evaluation bench-
mark by transforming PTB trees into a format
suitable for the KPML realiser he uses.
In all of the above cases, the data is derived
from real world sentences, thereby exemplifying
?real world complexity?. If the corpus is large
enough (as in the case of the PTB), the data can
furthermore be expected to cover a broad range of
syntactic phenomena. Moreover, the data, being
derived from real world sentences, is not biased
towards system-specific capabilities. Nonethe-
less, there are also limits to these approaches.
First, they fail to support graduated perfor-
mance testing on constructs such as intersective
modifiers or lexical ambiguity, which are known
to be problematic for surface realisation.
Second, the construction of the benchmark is in
both cases time consuming. In the reversible ap-
proach, for each input sentence, the correct inter-
pretation must be manually selected from among
the semantic formulae produced by the parser. As
a side effect, the constructed benchmarks remain
relatively small (825 in the case of White (2004);
130 in Carroll and Oepen (2005)). In the case
of a benchmark derived by transformation from
a syntactically annotated corpus, the implemen-
tation of the converter is both time-intensive and
corpus-bound. For instance, Callaway (2003) re-
2Combinatory Categorial Grammar
ports that the implementation of such a proces-
sor for the SURGE realiser was the most time-
consuming part of the evaluation with the result-
ing component containing 4000 lines of code and
900 rules.
As we shall show in the following sections,
the GENSEM approach to benchmark construction
aims to address both of these shortcomings. By
using a DCG to implement grammar traversal, it
permits both a full automation of the benchmark
creation and some control over the type and the
distribution of the benchmark items.
3 GenSem
As mentioned above, GENSEM is a grammar
traversal algorithm for TAG. We first present the
specific TAG used for traversal, namely SEMX-
TAG (Alahverdzhieva, 2008) (section 3.1). We
then show how to automatically derive a DCG
that describes the derivation trees of this gram-
mar (section 3.2). Finally, we show how this DCG
encoding permits generating formulae while en-
abling control over the set of semantic representa-
tions to be produced (section 3.3).
3.1 SemXTAG
The SEMXTAG grammar used by GENSEM and
by the two surface realisers is a Feature-Based
Lexicalised Tree Adjoining Grammar augmented
with a unification-based semantics as described by
Gardent and Kallmeyer (2003). We briefly intro-
duce each of these components and describe the
grammar coverage.
FTAG. A Feature-based TAG (Vijay-Shanker
and Joshi, 1988) consists of a set of (auxil-
iary or initial) elementary trees and of two tree-
composition operations: substitution and adjunc-
tion. Initial trees are trees whose leaves are la-
belled with substitution nodes (marked with a
downarrow) or terminal categories. Auxiliary
trees are distinguished by a foot node (marked
with a star) whose category must be the same as
that of the root node. Substitution inserts a tree
onto a substitution node of some other tree while
adjunction inserts an auxiliary tree into a tree. In
an FTAG, the tree nodes are furthermore deco-
rated with two feature structures (called top and
339
bottom) which are unified during derivation as
follows. On substitution, the top of the substitu-
tion node is unified with the top of the root node
of the tree being substituted in. On adjunction,
the top of the root of the auxiliary tree is unified
with the top of the node where adjunction takes
place; and the bottom features of the foot node are
unified with the bottom features of this node. At
the end of a derivation, the top and bottom of all
nodes in the derived tree are unified. Finally, each
sentence derivation in an FTAG is associated with
both a derived tree representing the phrase struc-
ture of the sentence and a derivation tree record-
ing how the corresponding elementary trees were
combined to form the derived tree.
FTAG with semantics. To associate seman-
tic representations with natural language expres-
sions, the FTAG is modified as proposed by Gar-
dent and Kallmeyer (2003).
NPj
John
name(j,john)
Sb
NP?c VPba
Va
runs
run(a,c)
VPx
often VP*x
often(x)
? name(j,john), run(a,j), often(a)
Figure 1: Flat semantics for ?John often runs?
Each elementary tree is associated with a flat
semantic representation. For instance, in Fig-
ure 1,3 the trees for John, runs, and often are asso-
ciated with the semantics name(j,john), run(a,c),
and often(x), respectively. Importantly, the argu-
ments of a semantic functor are represented by
unification variables which occur both in the se-
mantic representation of this functor and on some
nodes of the associated syntactic tree. For in-
stance in Figure 1, the semantic index c occur-
ring in the semantic representation of runs also
occurs on the subject substitution node of the as-
sociated elementary tree. The value of semantic
arguments is determined by the unifications re-
sulting from adjunction and substitution. For in-
stance, the semantic index c in the tree for runs is
3Cx/Cx abbreviate a node with category C and a
top/bottom feature structure including the feature-value pair
{ index : x}.
unified during substitution with the semantic in-
dex labelling the root node of the tree for John.
As a result, the semantics of John often runs is
{name(j,john),run(a,j),often(a)}.
SemXTAG. SEMXTAG is an FTAG for En-
glish augmented with a unification-based compo-
sitional semantics of the type described above.
Its syntactic coverage approaches that of XTAG,
the FTAG developed for English by the XTAG
group (The XTAG Research Group, 2001). Like
this grammar, it contains around 1300 elementary
trees and covers auxiliaries, copula, raising and
small clause constructions, topicalization, relative
clauses, infinitives, gerunds, passives, adjuncts,
ditransitives and datives, ergatives, it-clefts, wh-
clefts, PRO constructions, noun-noun modifica-
tion, extraposition, sentential adjuncts, impera-
tives and resultatives.
3.2 Converting SemXTAG to a DCG
We would like to be able to traverse SEMXTAG in
order to generate semantic representations that are
licensed by it. In the DCG formalism, a grammar
is represented as a set of Prolog definite clauses,
and Prolog?s query mechanism provides built-in
grammar traversal. We take advantage of this by
deriving a DCG from SEMXTAG and then using
Prolog queries to generate semantic representa-
tions that are associated with sentences in the lan-
guage described by it.
Another advantage of the DCG formalism is
that arbitrary Prolog goals can be inserted into a
rule, to constrain when the rule applies or to bind
variables occurring in it. We use this to ground
derivations with lexical items, which are repre-
sented using Prolog assertions. We also use it to
control Prolog?s grammar traversal in such a way
as to generate sets of semantic formulae covering
certain computationally interesting cases (see sec-
tion 3.3).
Our algorithm for converting SEMXTAG to a
DCG is inspired by Schmitz and Le Roux (2008),
who derive from an FTAG a feature-based reg-
ular tree grammar (RTG) whose language is the
derivation trees of the FTAG. Indeed, in our im-
plementation, we derive a DCG from such an
RTG, thereby taking advantage of a SEMXTAG-
340
to-RTG converter previously implemented by Syl-
vain Schmitz.
TAG to RTG. In the conversion to RTG4, each
elementary tree in SEMXTAG is converted to a
rule that models the contribution of the tree to
a TAG derivation. A TAG derivation involves
the selection of an initial tree, which has some
nodes requiring substitution and some permitting
adjunction. Let us think of the potential adjunc-
tion sites as requiring, rather than permitting, ad-
junction, but such that the requirement can be sat-
isfied by ?null? adjunction. Inserting another tree
into this initial tree satisfies one of the substitution
or adjunction requirements, but introduces some
new requirements into the resulting tree, in the
form of its own substitution nodes and adjunction
sites.
Thus, intuitively, the RTG representation of a
SEMXTAG elementary tree is a rule that rewrites
the satisfied requirement as a local tree whose root
is a unique identifier of the tree and whose leaves
are the introduced requirements. A requirement
of a substitution or adjunction of a tree of root
category X is written as XS or XA, respectively.
Here, for example, is the translation to RTG of the
TAG tree (minus semantics) for runs in Figure 1,
using the word anchoring the tree as its identifier
(the superscripts abbreviate feature structures: b/t
refers to the bottom/top feature structure and the
upper case letters to the semantic index value, so
[idx : X] is abbreviated to X):
S[t:T ]S ? runs(S
[t:T,b:B]
A NP
[t:C]
S V P
[t:B,b:A]
A V
[t:A]
A )
The semantics of the SEMXTAG tree are carried
over as-is to the corresponding RTG rule. Fur-
ther, the feature structures labelling the nodes of
SEMXTAG trees are carried over to the RTG rules
so as to correctly interact with substitution and
adjunction (see Schmitz and Le Roux (2008) for
more details on this part of the conversion pro-
cess).
To account for the optionality of adjunction,
there are additional rules allowing any adjunction
4For a more precise description of the FTAG to RTG con-
version see Schmitz and Le Roux (2008).
requirement to be rewritten as the symbol ?, a ter-
minal symbol of the RTG.
The terminal symbols of the RTG are thus the
tree identifiers and the symbol ?, and its non-
terminals are XS and XA for each terminal or
non-terminal X of SEMXTAG.
RTG to DCG. Since the right-hand side of each
RTG rule is a local tree ? that is, a tree of depth no
more than one ? we can flatten each of them into
a list consisting of the root node followed by the
leaves without losing any structural information.
This is the insight underlying the RTG-to-DCG
conversion step. Each RTG rule is converted to
a DCG rule that is essentially identical except for
this flattening of the right-hand side. Here is the
translation to DCG of the RTG rule above5:
rule(s,init,Top,Bot,Sem;S;N;VP;V)
--> [runs],
{lexicon(runs,n0V,[run])},
rule(s,aux,Top,[B],S),
rule(np,init,[C],_,N),
rule(vp,aux,[B],[A],VP),
rule(v,aux,[A],_,V),
{Sem =.. [run,A,C]}.
We represent non-terminals of the DCG us-
ing the rule predicate, whose five (non-hidden)6
arguments, in order, are the category, the sub-
script (init for subscript S, aux for subscript
A), the top and bottom feature values, and the se-
mantics. Feature structures are represented us-
ing Prolog lists with a fixed argument position
for each attribute in the grammar (in this ex-
ample, only the index attribute). The semantics
associated with the left-hand-side symbol (here,
Sem;S;N;VP;V, with the semicolon represent-
ing semantic conjunction) are composed of the se-
mantics associated with this rule and those associ-
ated with each of the right-hand-side symbols.
The language of the resulting DCG is neither
the language of the RTG nor the language of
SEMXTAG, and indeed the language of the DCG
does not interest us but rather its derivation trees.
5In practice, the lexicon is factored out, so there is no rule
specifically for runs, but one for intransitive verbs (n0V) in
general. Each rule hooks into the lexicon, so that a given
invocation of a rule is grounded by a particular lexical item.
6The ?? > notation is syntactic sugar for the usual Pro-
log : ? definite clause notation with two hidden arguments
on each predicate. The hidden arguments jointly represent
the list of terminals dominated by the symbol.
341
These are correlated one-to-one with the trees in
the language described by the RTG, i.e. with the
derivation trees of SEMXTAG, and the latter can
be trivially reconstructed from the DCG deriva-
tions. From a SEMXTAG derivation tree, one can
compose the semantic representation of the asso-
ciated sentence, and in fact this semantic compo-
sition occurs as a side effect of a Prolog query
against the DCG, allowing semantic representa-
tions licensed by SEMXTAG to be returned as
query results.
We define a Prolog predicate for querying
against the DCG, as follows. Its one input argu-
ment, Cat, is the label of the root node of the
derivation tree (typically s), and its one output ar-
gument, Sem, is the semantic representation asso-
ciated with that tree7.
genSem(Cat,Sem) :-
rule(Cat,init,_,_,Sem,_,[]).
3.3 Control parameters
In order to give the users some control over the
sorts of semantic representations that they get
back from a query against the DCG, we augment
the DCG in such a way as to allow control over
the TAG family8 of the root tree in the derivation
tree, over the number and type of adjunctions in
the derivation, and over the depth of substitutions.
To implement control over the family is quite sim-
ple: we need merely to index the DCG rules by
family and modify the GENSEM call accordingly.
For instance, the above DCG rule becomes :
rule(s,init,Top,Bot,n0V,Sem;S;NP;VP;V)
--> [runs],
{lexicon(runs,n0V,[run])},
...
We implement restrictions on adjunctions by
adding an additional argument to the grammar
symbols, namely a vector of non-negative inte-
gers representing the number of non-null adjunc-
tions of each type that are in the derivation sub-
tree dominated by the symbol. By ?type? of ad-
junction, we mean the category of the adjunc-
7The 6th and 7th arguments of the rule call are the hidden
arguments needed by the DCG.
8TAG families group together trees which belong to-
gether, in particular, the trees associated with various real-
isation of a specific subcategorisation type. Thus, here the
notion of TAG family is equivalent to that of subcategorisa-
tion type.
tion site. In DCG terms, a non-null adjunction
of a category X is represented as the expansion of
an x/aux symbol other than as ?. So, for ex-
ample, a DCG symbol associated with the vec-
tor [1,0,0,0,0], where the five dimensions of
the vector correspond to the n, np, v, vp, and s
categories, respectively, dominates a subtree con-
taining exactly one n/aux symbol expanded by
a non-epsilon rule, and no other aux symbol ex-
panded by a non-epsilon rule. We link the vector
associated with the root of the derivation to the
query predicate.
We define a special predicate to handle the
divvying up of a mother node?s vector among the
daughters, taking advantage of the fact that the
DCG formalism permits the insertion of arbitrary
Prolog goals into a rule.
Finally, we add an additional argument to the
DCG rule and to the GENSEM?s call to control
the traversal depth with respect to the number of
substitutions applied. The overall depth of each
derivation is therefore constrained both by the
user defined adjunctions and substitution depth
constraints.
Our query predicate now has four input argu-
ments and one output argument:
genSem(Cat,Fam,[N,NP,V,VP,S],Dth,Sem):-
rule(Cat,init,_,_,Fam,
[N,NP,V,VP,S],Dth,Sem,_,[]).
4 Using GENSEM for benchmarking
We now show how GENSEM can be put to work
for comparing two TAG-based surface realisers,
namely GENI (Gardent and Kow, 2007) and RT-
GEN (Perez-Beltrachini, 2009). These two realis-
ers follow globally similar algorithms but differ in
several respects. We show how GENSEM can be
used to produce benchmarks that are tailored to
test hypotheses about how these differences might
impact performance. We then use this GENSEM-
generated benchmark to compare the performance
of the two realisers.
4.1 GenI and RTGen
Both GENI and RTGEN use the SEMXTAG gram-
mar described in section 3.1. Moreover, both re-
alisers follow an algorithm pipelining three main
phases. First, lexical selection selects from the
342
grammar those elementary trees whose semantics
subsumes part of the input semantics. Second,
the tree combining phase systematically tries to
combine trees using substitution and adjunction.
Third, the retrieval phase extracts the yields of
the complete derived trees, thereby producing the
generated sentence(s).
There are also differences however. We now
spell these out and indicate how they might im-
pact the relative performance of the two surface
realisers.
Derived vs. derivation trees. While GENI con-
structs derived trees, RTGEN uses the RTG en-
coding of SEMXTAG sketched in the previous
section to construct derivation trees. These are
then unraveled into derived trees at the final re-
trieval stage. As noted by Koller and Striegnitz
(2002), these trees are simpler than TAG elemen-
tary trees, which can favourably impact perfor-
mance.
Interleaving of feature constraint solving and
syntactic analysis. GENI integrates in the tree
combining phase a filtering step in which the ini-
tial search space is pruned by eliminating from
it all combinations of TAG elementary trees that
cover the input semantics but cannot possibly lead
to a valid derived tree. This filtering eliminates
all combinations of trees such that either the cat-
egory of a substitution node cannot be cancelled
out by that of the root node of a different tree, or
a root node fails to have a matching substitution
site. Importantly, filtering ignores feature infor-
mation and tree combining takes place after filter-
ing. RTGEN, on the other hand, directly combines
derivation trees decorated with full feature struc-
ture information.
Handling of intersective modifiers. GENI and
RTGEN differ in their strategies for handling
modification.
Adapting Carroll and Oepen?s (2005) proposal
to TAG, GENI adopts a two-step tree-combining
process such that in the first step, only substitu-
tion applies, while in the second, only adjunc-
tion is used. Although the number of intermediate
structures generated is still 2n for n modifiers, this
strategy has the effect of blocking these 2n struc-
tures from multiplying out with other structures in
the chart.
RTGEN, on the other hand, uses a standard Ear-
ley algorithm that includes sharing and packing.
Sharing allows intermediate structures common
to several derivations to be represented once only
while packing groups together partial derivation
trees with identical semantic coverage and similar
combinatorics (same number and type of substitu-
tion and adjunction requirements), keeping only
one representative of such groups in the chart.
In this way, intermediate structures covering the
same set of intersective modifiers in a different
order are only represented once and the negative
impact of intersective modifiers is lessened.
4.2 Two GENSEM benchmarks
We use GENSEM to construct two benchmarks de-
signed to test the impact of the differences be-
tween the two realisers and, more specifically, to
compare the relative performance of both realisers
(i) on cases involving intersective modifiers and
(ii) on cases of varying overall complexity.
The MODIFIERS benchmark focuses on
intersective modifiers and contains semantic
formulae corresponding to sentences in-
volving an increasing number of modifiers.
Recall that GENSEM calls are of the form
gensem(Cat,Family,[N,NP,V,VP,S],Dth,Sem)
where N,NP,V,VP,S indicates the number of
required adjunctions in N, NP, V, VP and S,
respectively, while Family constrains the subcate-
gorisation type of the root tree in the derivations
produced by GENSEM. To produce formulae
involving the lexical selection of intersective
modifiers, we set the following constraints. Cat is
set to s and Family is set to either n0V (intransitive
verbs) or n0Vn1 (transitive verbs). Furthermore,
N and V P vary from 0 to 4 thereby requiring the
adjunction of 0 to 4 N and/or VP modifiers. All
other adjunction counters are set to null. To avoid
producing formulae with identical derivation trees
but distinct lemmas, we use a restricted lexicon
containing one lemma of each syntactic type,
e.g. one transitive verb, one intransitive verb, etc.
Given these settings, GENSEM produces 1 789
formulae whose adjunction requirements vary
from 1 to 6. For instance, the semantic formula
343
{sleep(b,c),man(c),a(c),blue(c),sleep(i,c),carefully(b)} (A
sleeping blue man sleeps carefully) extracted
from the MODIFIERS benchmark contains two
NP adjunctions and one VP adjunction.
The MODIFIERS benchmark is tailored to fo-
cus on cases involving a varying number of in-
tersective modifiers. To support a comparison of
the realisers on this dimension, it displays little or
no variation w.r.t. other dimensions, such as verb
type and non-modifying adjunctions.
To measure the performance of the two realisers
on cases of varying overall complexity, we con-
struct a second benchmark (COMPLEXITY) dis-
playing such variety. The GENSEM parameters
for the construction of this suite are the follow-
ing. The verb type (Family) is one of 28 possible
verb types9. The number and type of required ad-
junctions vary from 0 to 4 for N adjunctions, 0 to
1 for NP , 0 to 4 for V P and 0 to 1 for S. The re-
sulting benchmark contains 890 semantic formu-
lae covering an extensive set of verb types and of
adjunction requirements.
4.3 Results
Using the two GENSEM-generated benchmarks,
we now compare GENI and RTGEN. We plot the
average number of chart items against both the
number of intersective modifiers present in the in-
put (Figure 3) and the size of the Initial Search
Space (ISS), i.e., the number of combinations of
elementary TAG trees covering the input seman-
tics to be explored after the lexical selection step
(Figure 2). In our case, the ISS gives a more
meaningful idea about the complexity than con-
sidering only the number of input literals. In an
FTAG, the number of elementary trees selected
9The 28 verb types are
En1V,n0BEn1,n0lVN1Pn2,n0V,n0Va1,n0VAN1,n0VAN1Pn2,
n0VDAN1,n0VDAN1Pn2,n0VDN1,n0VDN1Pn2,n0Vn1,
n0VN1,n0Vn1Pn2,n0VN1Pn2,n0Vn2n1,n0Vpl,n0Vpln1,
n0Vpn1,n0VPn1,n0Vs1,REn1VA2,REn1VPn2,Rn0Vn1A2,
Rn0Vn1Pn2,s0V,s0Vn1,s0Vton1. The notational convention
for verb types is from XTAG and reads as follows. Sub-
scripts indicate the thematic role of the verb argument. n
indicates a nominal, Pn a PP and s a sentential argument. pl
is a verbal particle. Upper case letters describe the syntactic
functor type: V is a verb, E an ergative, R a resultative and
BE the copula. Sequences of upper case letters such as
VAN in n0VAN1 indicate a multiword functor with syntactic
categories V, A, and N. For instance, n0Vn1 indicates a verb
taking two nominal arguments (e.g., like) and n0VAN1 a
verb locution such as to cry bloody murder.
0-
10
0
10
0-
10
00
10
00
-
50
00
50
00
-
10
00
0
10
00
0-
10
00
00
10
00
00
-
50
00
00
50
00
00
-
10
00
00
0
m
o
re
th
an
10
00
00
0
102
103
104
105
106
p
p p
p p
p p p
Initial Search Space (ISS) size
u
n
pa
ck
ed
ch
ar
ts
iz
e
RTGEN-all
RTGEN-level0
p RTGEN-selective
GENI
Figure 2: Performance of realisation approaches
on the COMPLEXITY benchmark, average un-
packed chart size as a function of the ISS com-
plexity.
by a given literal may vary considerably depend-
ing on the number and the size of the tree families
selected by this literal. For instance, a literal se-
lecting the n0Vn2n1 class will select many more
trees than a literal selecting the n0V family be-
cause there are many more ways of realising the
three arguments of a ditransitive verb than the sin-
gle subject argument of an intransitive one. Chart
items include all elementary trees selected by the
lexical selection step as well as the intermediate
and final structures produced by the tree combin-
ing phase. In RTGEN, we distinguish between
the number of structures built before unpacking
(packed chart) and the number of structures ob-
tained after unpacking (unpacked chart).
Both realisers are implemetned in different pro-
gramming languages, GENI is implemented in
Haskell whereas RTGEN in Prolog. As for the
time results comparison, preliminary experiments
show that GENI is faster is simple input cases. On
the other hand, in the case of more complex cases,
the point of producing much less intermediate re-
sults pays off compared to the overhead of the
chart/agenda operations.
344
Overall efficiency. The plot in Figure 2 shows
the results obtained by running both realisers on
the COMPLEXITY benchmark. Recall (cf. sec-
tion 4.2) that the COMPLEXITY benchmark con-
tains input with varying verb arity and a varying
number of required adjunctions. Hence it provides
cases of increasing complexity in terms of ISS to
be explored. Furthermore, test cases in the bench-
mark trigger sentence realisation involving certain
TAG families, which have a certain number of
trees. Those trees within a family often have iden-
tical combinatorics but different features. Conse-
quently, the COMPLEXITY benchmark also pro-
vides an appropriate testbed for testing the im-
pact of feature structure information on the two
approaches to tree combination.
The graphs show that as complexity increases,
the performance delta between GENI and RT-
GEN increases. We conjecture that as complex-
ity grows, the filtering used by GENI does not
suffice to reduce the search space to a manage-
able size. Conversely, the overhead introduced by
RTGEN?s all-in-one, tree-combining Earley with
packing strategy seems compensated throughout
by the construction of a derivation rather than a
derived tree and pays off increasingly as complex-
ity increases.
Modifiers. Figure 3 plots the results obtained by
running the realisers on the MODIFIERS bench-
mark. Here again, RTGEN outperforms GENI and
the delta between the two realisers grows with the
number of intersective modifiers to be handled. A
closer look at the data shows that the global con-
straints set by GENSEM on the number of required
adjunctions covers an important range of varia-
tion in the data complexity. For instance, there
are cases where 4 modifiers modify the same NP
(or VP) and cases where the modifiers are dis-
tributed over two NPs. Similarly, literals intro-
duced into the formula by a GENSEM adjunction
requirement vary in terms of the number of auxil-
iary trees whose selection they trigger. The steep
curve in GENI?s plot suggests that although the
delayed adjunction mechanism helps in avoiding
the proliferation of intermediate incomplete mod-
ifiers? structures, the lexical ambiguity of modi-
fiers still poses a problem. In contrast, RTGEN?s
0 1 2 3 4 5 6 7
103
104
p
p
p
p
p
p
number of modifiers
u
n
pa
ck
ed
ch
ar
ts
iz
e
RTGEN-all
RTGEN-level0
p RTGEN-selective
GENI
Figure 3: Performance of realisation approaches
on the MODIFIERS benchmark, average unpacked
chart size as a function of the number of modifiers.
packing uniformly applies to word order varia-
tions and to the cases of lexical ambiguity raised
by intersective modifiers because the items have
the same combinatoric potential and the same se-
mantics.
5 Conclusion
Surface realisers are complex systems that need to
handle diverse input and require complex compu-
tation. Testing raises among other things the issue
of coverage ? how can the potential input space
be covered? ? and of test data creation ? should
this data be hand tailored, created randomly, or
derived from real world text?
In this paper, we presented an approach which
permits automating the creation of test input for
surface realisers whose input is a flat semantic for-
mula. The approach differs from other existing
evaluation schemes in two ways. First, it permits
producing arbitrarily many inputs. Second, it sup-
ports the construction of grammar-controlled, lin-
guistically focused benchmarks.
We are currently working on further extending
GENSEM with more powerful (recursive) control
restrictions on the grammar traversal; on com-
bining GENSEM with tools for detecting grammar
overgeneration; and on producing a benchmark
that could be made available to the community for
testing surface realisers whose input is either a de-
pendency tree or a flat semantic formula.
345
References
Alahverdzhieva, K. 2008. XTAG using XMG. Mas-
ter?s thesis, U. Nancy 2. Erasmus Mundus Master
?Language and Communication Technology?.
Brew, C. 1992. Letting the cat out of the bag: Gen-
eration for shake-and-bake MT. In Proceedings of
COLING ?92, Nantes, France.
Callaway, Charles B. 2003. Evaluating coverage for
large symbolic NLG grammars. In 18th IJCAI,
pages 811?817, Aug.
Carroll, John and Stephan Oepen. 2005. High effi-
ciency realization for a wide-coverage unification
grammar. 2nd IJCNLP.
Carroll, John, A. Copestake, D. Flickinger, and
V. Paznan?ski. 1999. An efficient chart generator
for (semi-)lexicalist grammars. In Proceedings of
EWNLG ?99.
Gardent, Claire and Laura Kallmeyer. 2003. Seman-
tic construction in FTAG. In 10th EACL, Budapest,
Hungary.
Gardent, Claire and Eric Kow. 2007. Spotting over-
generation suspects. In 11th European Workshop
on Natural Language Generation (ENLG).
Kay, Martin. 1996. Chart generation. In Proceedings
of the 34th annual meeting on Association for Com-
putational Linguistics, pages 200?204, Morristown,
NJ, USA. Association for Computational Linguis-
tics.
Koller, Alexander and Kristina Striegnitz. 2002. Gen-
eration as dependency parsing. In Proceedings of
the 40th ACL, Philadelphia.
Langkilde-Geary, Irene. 2002. An empirical verifi-
cation of coverage and correctness for a general-
purpose sentence generator. In Proceedings of the
INLG.
Nederhof, M.-J. 1996. Efficient generation of random
sentences. Natural Language Engineering, 2(1):1?
13.
Perez-Beltrachini, Laura. 2009. Using regular
tree grammars to reduce the search space in sur-
face realisation. Master?s thesis, Erasmus Mundus
Master Language and Communication Technology,
Nancy/Bolzano.
Purdom, Paul. 1972. A sentence generator for testing
parsers. BIT, 12(3):366?375.
Schmitz, S. and J. Le Roux. 2008. Feature uni-
fication in TAG derivation trees. In Gardent, C.
and A. Sarkar, editors, Proceedings of the 9th In-
ternational Workshop on Tree Adjoining Grammars
and Related Formalisms (TAG+?08), pages 141?
148, Tu?bingen, Germany.
The XTAG Research Group. 2001. A lexicalised tree
adjoining grammar for english. Technical report,
Institute for Research in Cognitive Science, Univer-
sity of Pennsylvannia.
Vijay-Shanker, K. and AK Joshi. 1988. Feature Struc-
tures Based Tree Adjoining Grammars. Proceed-
ings of the 12th conference on Computational lin-
guistics, 55:v2.
White, Michael. 2004. Reining in CCG chart realiza-
tion. In INLG, pages 182?191.
346
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 183?191,
Gothenburg, Sweden, April 26-30 2014. c?2014 Association for Computational Linguistics
Incremental Query Generation
Laura Perez-Beltrachini
Faculty of Computer Science
Free University of Bozen-Bolzano
Bozen-Bolzano, Italy
laura.perez@loria.fr
Claire Gardent
CNRS/LORIA
Nancy, France
claire.gardent@loria.fr
Enrico Franconi
Faculty of Computer Science
Free University of Bozen-Bolzano
Bozen-Bolzano, Italy
franconi@inf.unibz.it
Abstract
We present a natural language genera-
tion system which supports the incremen-
tal specification of ontology-based queries
in natural language. Our contribution is
two fold. First, we introduce a chart
based surface realisation algorithm which
supports the kind of incremental process-
ing required by ontology-based querying.
Crucially, this algorithm avoids confusing
the end user by preserving a consistent
ordering of the query elements through-
out the incremental query formulation pro-
cess. Second, we show that grammar
based surface realisation better supports
the generation of fluent, natural sounding
queries than previous template-based ap-
proaches.
1 Introduction
Previous research has shown that formal ontolo-
gies could be used as a means not only to provide
a uniform and flexible approach to integrating and
describing heterogeneous data sources, but also to
support the final user in querying them, thus im-
proving the usability of the integrated system. To
support the wide access to these data sources, it is
crucial to develop efficient and user-friendly ways
to query them (Wache et al., 2001).
In this paper, we present a Natural Language
(NL) interface of an ontology-based query tool,
called Quelo1, which allows the end user to for-
mulate a query without any knowledge either of
the formal languages used to specify ontologies, or
of the content of the ontology being used. Follow-
ing the conceptual authoring approach described
in (Tennant et al., 1983; Hallett et al., 2007), this
interface masks the composition of a formal query
1krdbapp.inf.unibz.it:8080/quelo
as the composition of an English text describ-
ing the equivalent information needs using natu-
ral language generation techniques. The natural
language generation system that we propose for
Quelo?s NL interface departs from similar work
(Hallett et al., 2007; Franconi et al., 2010a; Fran-
coni et al., 2011b; Franconi et al., 2010b; Franconi
et al., 2011a) in that it makes use of standard gram-
mar based surface realisation techniques. Our con-
tribution is two fold. First, we introduce a chart
based surface realisation algorithm which supports
the kind of incremental processing required by on-
tology driven query formulation. Crucially, this
algorithm avoids confusing the end user by pre-
serving a consistent ordering of the query ele-
ments throughout the incremental query formu-
lation process. Second, we show that grammar
based surface realisation better supports the gener-
ation of fluent, natural sounding queries than pre-
vious template-based approaches.
The paper is structured as follows. Section 2
discusses related work and situates our approach.
Section 3 describes the task being addressed
namely, ontology driven query formulation. It in-
troduces the input being handled, the constraints
under which generation operates and the opera-
tions the user may perform to build her query.
In Section 4, we present the generation algo-
rithm used to support the verbalisation of possi-
ble queries. Section 5 reports on an evaluation of
the system with respect to fluency, clarity, cover-
age and incrementality. Section 6 concludes with
pointers for further research.
2 Related Work
Our approach is related to two main strands of
work: incremental generation and conceptual au-
thoring.
Incremental Generation (Oh and Rudnicky,
2000) used an n-gram language model to stochas-
183
tically generate system turns. The language model
is trained on a dialog corpus manually annotated
with word and utterance classes. The generation
engine uses the appropriate language model for
the utterance class and generates word sequences
randomly according to the language model distri-
bution. The generated word sequences are then
ranked using a scoring mechanism and only the
best-scored utterance is kept. The system is incre-
mental is that each word class to be verbalised can
yield a new set of utterance candidates. However
it supports only addition not revisions. Moreover
it requires domain specific training data and man-
ual annotation while the approach we propose is
unsupervised and generic to any ontology.
(Dethlefs et al., 2013) use Conditional Random
Fields to find the best surface realisation from a
semantic tree. They show that the resulting sys-
tem is able to modify generation results on the fly
when new or updated input is provided by the dia-
log manager. While their approach is fast to ex-
ecute, it is limited to a restricted set of domain
specific attributes; requires a training corpus of
example sentences to define the space of possi-
ble surface realisations; and is based on a large
set (800 rules) of domain specific rules extracted
semi-automatically from the training corpus. In
contrast, we use a general, small size grammar
(around 50 rules) and a lexicon which is automat-
ically derived from the input ontologies. The re-
sulting system requires no training and thus can
be applied to any ontology with any given signa-
ture of concepts and relations. Another difference
between the two approaches concerns revisions:
while our approach supports revisions anywhere
in the input, the CRF approach proposed by (Deth-
lefs et al., 2013) only supports revisions occurring
at the end of the generated string.
There is also much work (Schlangen and
Skantze, 2009; Schlangen et al., 2009) in the do-
main of spoken dialog systems geared at mod-
elling the incremental nature of dialog and in par-
ticular, at developing dialog systems where pro-
cessing starts before the input is complete. In these
approaches, the focus is on developing efficient ar-
chitectures which support the timely interleaving
of parsing and generation. Instead, our aim is to
develop a principled approach to the incremental
generation of a user query which supports revision
and additions at arbitrary points of the query being
built; generates natural sounding text; and maxi-
mally preserves the linear order of the query.
Conceptual authoring Our proposal is closely
related to the conceptual authoring approach de-
scribed in (Hallett et al., 2007). In this approach,
a text generated from a knowledge base, describes
in natural language the knowledge encoded so far,
and the options for extending it. Starting with an
initial very general query (e.g., all things), the user
can formulate a query by choosing between these
options. Similarly, (Franconi et al., 2010a; Fran-
coni et al., 2011b; Franconi et al., 2010b; Fran-
coni et al., 2011a) describes a conceptual author-
ing approach to querying semantic data where in
addition , logical inference is used to semantically
constrain the possible completions/revisions dis-
played to the user.
Our approach departs from this work in that it
makes use of standard grammars and algorithms.
While previous work was based on procedures and
templates, we rely on a Feature-Based Tree Ad-
joining Grammar to capture the link between text
and semantics required by conceptual authoring;
and we adapt a chart based algorithm to support
the addition, the revision and the substitution of
input material. To avoid confusing the user, we
additionally introduce a scoring function which
helps preserve the linear order of the NL query.
The generation system we present is in fact inte-
grated in the Quelo interface developed by (Fran-
coni et al., 2011a) and compared with their previ-
ous template-based approach.
3 Incremental Generation of Candidate
Query Extensions
The generation task we address is the following.
Given a knowledge base K , some initial formal
query q and a focus point p in that query, the rea-
soning services supported by Quelo?s query logic
framework (see (Guagliardo, 2009)) will compute
a set of new queries rev(q) formed by adding,
deleting and revising the current query q at point
p. The task of the generator is then to produce
a natural language sentence for each new formal
query q? ? rev(q) which results from this revision
process. In other words, each time the user refines
a query q to produce a new query q?, the system
computes all revisions rev(q) of q? that are com-
patible with the underlying knowledge base using
a reasoner. Each of these possible revisions is then
input to the generator and the resulting revised NL
queries are displayed to the user. In what follows,
184
we assume that formal queries are represented us-
ing Description Logics (Baader, 2003).
The following examples show a possible se-
quence of NL queries, their corresponding DL rep-
resentation and the operations provided by Quelo
that can be performed on a query (bold face is used
to indicate the point in the query at which the next
revision takes place). For instance, the query in
(1c) results from adding the concept Y oung to the
query underlying (1b) at the point highlighted by
man.
(1) a. I am looking for something (initial query)
?
b. I am looking for a man (substitute con-
cept)
Man
c. I am looking for a young man (add com-
patible concept)
Man ? Y oung
d. I am looking for a young man who is
married to a person (add relation)
Man?Y oung??isMarried.(Person)
e. I am looking for a young married man
(substitute selection)
MarriedMan ? Y oung
f. I am looking for a married man (delete
concept)
MarriedMan
4 Generating Queries
Generation of KB queries differs from standard
natural language generation algorithms in two
main ways. First it should support the revi-
sions, deletions and additions required by incre-
mental processing. Second, to avoid confusing
the user, the revisions (modifications, extensions,
deletions) performed by the user should have a
minimal effect on the linear order of the NL query.
That is the generator is not free to produce any NL
variant verbalising the query but should produce
a verbalisation that is linearly as close as possi-
ble, modulo the revision applied by the user, to the
query before revisions. Thus for instance, given
the DL query (2) and assuming a linearisation of
that formula that matches the linear order it is pre-
sented in (see Section 4.2.1 below for a definition
of the linearisation of DL formulae), sentence (2b)
will be preferred over (2c).
(2) a. Car ? ?runOn.(Diesel) ?
?equippedWith.(AirCond)
b. A car which runs on Diesel and is
equipped with air conditioning
c. A car which is equipped with air condi-
tioning and runs on Diesel
In what follows, we describe the generation al-
gorithm used to verbalise possible extensions of
user queries as proposed by the Quelo tool. We
start by introducing and motivating the underlying
formal language supported by Quelo and the input
to the generator. We then describe the overall ar-
chitecture of our generator. Finally, we present the
incremental surface realisation algorithm support-
ing the verbalisation of the possible query exten-
sions.
4.1 The Input Language
Following (Franconi et al., 2010a; Franconi et al.,
2011b; Franconi et al., 2010b; Franconi et al.,
2011a) we assume a formal language for queries
that targets the querying of various knowledge and
data bases independent of their specification lan-
guage. To this end, it uses a minimal query lan-
guage L that is shared by most knowledge repre-
sentation languages and is supported by Descrip-
tion Logic (DL) reasoners namely, the language of
tree shaped conjunctive DL queries. Let R be a
set of relations and C be a set of concepts, then the
language of tree-shaped conjunctive DL queries is
defined as follows: S ::= C | ?R.(S) | S ? S
where R ? R, C ? C, ? denotes conjunction and
? is the existential quantifier.
A tree shaped conjunctive DL query can be rep-
resented as a tree where nodes are associated with
a set of concept names (node labels) and edges are
labelled with a relation name (edge labels). Figure
1 shows some example query trees.
4.2 NLG architecture
Our generator takes as input two L formula: the
formula representing the current query q and the
formula representing a possible revision r (addi-
tion/deletion/modification) of q. Given this in-
put, the system architecture follows a traditional
pipeline sequencing a document planner which (i)
linearises the input query and (ii) partition the in-
put into sentence size chunks; a surface realiser
mapping each sentence size L formula into a sen-
tence; and a referring expression generator verbal-
ising NPs.
4.2.1 Document Planning
The document planning module linearises the in-
put query and segments the resulting linearised
185
x{Man}
(a)
x
w
{Man}
{House}
livesIn
(b)
x
w
z
{Man}
{House}
livesIn
{RichPerson}
ownedBy
(c)
x
w
z
{Man}
{House,
Beautiful}
livesIn
{RichPerson}
ownedBy
(d)
?
?
?
?
x
y
w
z
{Man}
{Person}
{House,
Beautiful}
{RichPerson}
marriedTo livesIn
ownedBy
(e)
Figure 1: Example of query tree and incremental query construction.
query into sentence size chunks.
Query Linearisation Among the different
strategies investigated in (Dongilli, 2008) to
find a good order for the content contained in a
query tree the depth-first planning, i.e. depth-first
traversal of the query tree, was found to be the
most appropriate one. Partly because it is obtained
straightforward from the query tree but mostly
due to the fact that it minimizes the changes in the
text plan that are required by incremental query
modifications. Thus, (Franconi et al., 2010a)
defines a query linearisation as a strict total order2
on the query tree that satisfies the following
conditions:
? all labels associated with the edge?s leaving
node precede the edge label
? the edge label is followed by at least one label
associated with the edge?s arriving node
? between any two labels of a node there can
only be (distinct) labels of the same node
The specific linearisation adopted in Quelo is
defined by the depth-first traversal strategy of the
query tree and a total order on the children which
is based on the query operations. That is, the la-
bels of a node are ordered according to the se-
quence applications of the add compatible
concept operation. The children of a node are
inversely ordered according to the sequence of ap-
plications of the add relation operation.
According to this linearisation definition, for
the query tree (e) in Figure 1 the following linear
order is produced:
(3) a. Man marriedTo Person livesIn House
Beautiful ownedBy RichPeron
2A strict total order can be obtained by fixing an order in
the children nodes and traversing the tree according to some
tree traversal strategy.
Query Segmentation Given a linearised query
q, the document planner uses some heuristics
based on the number and the types of rela-
tions/concepts present in q to output a sequence
of sub-formulae each of which will be verbalised
as a sentence.
4.2.2 Incremental Surface Realisation and
Linearisation Constraints
We now describe the main module of the generator
namely the surface realiser which supports both
the incremental refinement of a query and a min-
imal modification of the linear order between in-
crements. This surface realiser is caracterised by
the following three main features.
Grammar-Based We use a symbolic, grammar-
based approach rather than a statistical one for two
reasons. First, there is no training corpus available
that would consist of knowledge base queries and
their increments. Second, the approach must be
portable and should apply to any knowledge base
independent of the domain it covers and indepen-
dent of the presence of a training corpus. By com-
bining a lexicon automatically extracted from the
ontology with a small grammar tailored to produce
natural sounding queries, we provide a generator
which can effectively apply to any ontology with-
out requiring the construction of a training corpus.
Chart-Based A chart-based architecture en-
hances efficiency by avoiding the recomputation
of intermediate structures while allowing for a
natural implementation of the revisions (addition,
deletion, substitution) operations required by the
incremental formulation of user queries. We show
how the chart can be used to implement these op-
erations.
Beam search. As already mentioned, for er-
gonomic reasons, the linear order of the gener-
ated NL query should be minimally disturbed dur-
ing query formulation. The generation system
186
should also be sufficiently fast to support a timely
Man/Machine interaction. We use beam search
and a customised scoring function both to preserve
linear order and to support efficiency.
We now introduce each of these components in
more details.
Feature-Based Tree Adjoining Grammar
A tree adjoining grammar (TAG) is a tuple
??, N, I,A, S? with ? a set of terminals, N a set
of non-terminals, I a finite set of initial trees, A a
finite set of auxiliary trees, and S a distinguished
non-terminal (S ? N ). Initial trees are trees
whose leaves are labeled with substitution nodes
(marked with a down-arrow) or with terminal
categories3 . Auxiliary trees are distinguished by
a foot node (marked with a star) whose category
must be the same as that of the root node.
Two tree-composition operations are used to
combine trees: substitution and adjunction. Sub-
stitution inserts a tree onto a substitution node of
some other tree while adjunction inserts an aux-
iliary tree into a tree. In a Feature-Based Lexi-
calised TAG (FB-LTAG), tree nodes are further-
more decorated with two feature structures which
are unified during derivation; and each tree is an-
chored with a lexical item. Figure 2 shows an ex-
ample toy FB-LTAG with unification semantics.
The dotted arrows indicate possible tree combina-
tions (substitution for John, adjunction for often).
As the trees are combined, the semantics is the
union of their semantics modulo unification. Thus
given the grammar and the derivation shown, the
semantics of John often runs is as shown namely,
named(j john), run(a,j), often(a).
NPj
John
l1:john(j)
S
b
NP?c VPb
a
Va
runs
lv:run(a,j)
VP
x
often VP*
x
lo:often(x)
l1:named(j john), lv:run(a,j), lv:often(a)
Figure 2: Derivation and Semantics for ?John often runs?
Chart-Based Surface Realisation Given an
FB-LTAG G of the type described above, sen-
tences can be generated from semantic formulae
by (i) selecting all trees in G whose semantics sub-
sumes part of the input formula and (ii) combining
3For a more detailed introduction to TAG and FB-LTAG,
see (Vijay-Shanker and Joshi, 1988).
these trees using the FB-LTAG combining opera-
tions namely substitution and adjunction. Thus for
instance, in Figure 2, given the semantics l1:named(j
john), lv:run(a,j), lv:often(a), the three trees shown are
selected. When combined they produce a com-
plete phrase structure tree whose yield (John runs
often) is the generated sentence.
Following (Gardent and Perez-Beltrachini,
2011), we implement an Earley style generation
algorithm for FB-LTAG which makes use of the
fact that the derivation trees of an FB-LTAG are
context free and that an FB-LTAG can be con-
verted to a a Feature-Based Regular Tree Gram-
mar (FB-RTG) describing the derivation trees of
this FB-LTAG4.
On the one hand, this Earley algorithm en-
hances efficiency in that (i) it avoids recomput-
ing intermediate structures by storing them and
(ii) it packs locally equivalent structures into a
single representative (the most general one). Lo-
cally equivalent structures are taken to be partial
derivation trees with identical semantic coverage
and similar combinatorics (same number and type
of substitution and adjunction requirements).
On the other hand, it naturally supports the
range of revisions required for the incremental for-
mulation of ontology-based queries. Let C be the
current chart i.e., the chart built when generating a
NL query from the formal query. Then additions,
revisions and deletion can be handled as follows.
? Add concept or property X: the grammar
units selected by X are added to the agenda5
and tried for combinations with the elements
of C .
? Substitute selection X with Y : all chart items
derived from a grammar unit selected by an
element of X are removed from the chart.
Conversely, all chart items derived from a
grammar unit selected by an element of Y are
added to the agenda. All items in the agenda
are then processed until generation halts.
? Delete selection X: all chart items derived
from a grammar unit selected by an element
of X are removed from the chart. Intermedi-
ate structures that had previously used X are
moved to the agenda and the agenda is pro-
cessed until generation halts.
4For more details on this algorithm, we refer the reader to
(Gardent and Perez-Beltrachini, 2010).
5The agenda is a book keeping device which stores all
items that needs to be processed i.e., which need to be tried
for combination with elements in the chart.
187
Beam Search To enhance efficiency and favor
those structures which best preserve the word or-
der while covering maximal input, we base our
beam search on a scoring function combining lin-
ear order and semantic coverage information. This
works as follows. First, we associate each literal
in the input query with its positional information
e.g.,
(4) a. man(x)[0] marriedTo(x y)[1]
person(y)[2] livesIn(x w)[3]
house(w)[4]
This positional information is copied over to
each FB-LTAG tree selected by a given literal and
is then used to compute a word order cost (C
wo
)
for each derived tree as follows:
C
wo
(t
i+j
) = C
wo
(t
i
) + C
wo
(t
j
) + C
wo
(t
i
+ t
j
)
That is the cost of a tree t
i+j
obtained by com-
bining t
i
and t
j
is the sum of the cost of each
of these trees plus the cost incurred by combin-
ing these two trees. We define this latter cost to
be proportional to the distance separating the ac-
tual position (ap
i
) of the tree (t
i
) being substi-
tuted/adjoined in from its required position (rp
i
).
If t
i
is substituted/adjoined at position n to the
right (left) of the anchor of a tree t
j
with posi-
tion p
j
, then the actual position of t
i
is pj + n
(pj ? n) and the cost of combining t
i
with t
j
is
| pj + n ? rp
i
| /? (| pj ? n ? rp
i
| /?) where
we empirically determined ? to be 1006.
Finally, the total score of a tree reflects the rela-
tion between the cost of the built tree, i.e. its word
order cost, and its semantic coverage, i.e. nb. of
literals from the input semantics:
S(t
i
) =
{
?(|literals| ? 1) C
wo
(t
i
) = 0
C
wo
(t
i
)/(|literals| ? 1) otherwise
The total score is defined by cases. Those trees
with C
wo
= 0 get a negative value according to
their input coverage (i.e. those that cover a larger
subset of the input semantics are favored as the
trees in the agenda are ordered by increasing total
score). Conversely, those trees with C
wo
> 0 get
a score that is the word order cost proportional to
the covered input.
In effect, this scoring mechanism favors trees
with low word order cost and large semantic cov-
erage. The beam search will select those trees with
lowest score.
6In the current implementation we assume that n = 1.
Furthermore, as t
i
might be a derived tree we also add to
C
wo
(t
i
+ t
j
) the cost computed on each tree t
k
used in the
derivation of t
i
with respect to t
j
.
4.2.3 Referring Expression Generation
The referring expression (RE) module takes as
input the sequence of phrase structure trees out-
put by the surface realiser and uses heuristics to
decide for each NP whether it should be ver-
balised as a pronoun, a definite or an indefinite
NP. These heuristics are based on the linear order
and morpho-syntactic information contained in the
phrase structure trees of the generated sentences.
5 Experiments and evaluation
We conducted evaluation experiments designed to
address the following questions:
? Does the scoring mechanism appropriately
capture the ordering constraints on the gen-
erated queries ? That is, does it ensure that
the generated queries respect the strict total
order of the query tree linearisation ?
? Does our grammar based approach produce
more fluent and less ambiguous NL query
than the initial template based approach cur-
rently used by Quelo ?
? Does the automatic extraction of lexicons
from ontology support generic coverage of
arbitrary ontologies ?
We start by describing the grammar used. We
then report on the results obtained for each of these
evaluation points.
5.1 Grammar and Lexicon
We specify an FB-LTAG with unification seman-
tics which covers a set of basic constructions used
to formulate queries namely, active and passive
transitive verbs, adjectives, prepositional phrases,
relative and elliptical clauses, gerund and partici-
ple modifiers. The resulting grammar consists of
53 FB-LTAG pairs of syntactic trees and semantic
schema.
To ensure the appropriate syntax/semantic in-
terface, we make explicit the arguments of a
relation using the variables associated with the
nodes of the query tree. Thus for instance,
given the rightmost query tree shown in Figure
1, the flat semantics input to surface realisation is
{Man(x), Person(y), House(w), Beautiful(w), RichPerson(z),
marriedTo(x,y), livesIn(x,w), ownedBy(w,z)}.
For each ontology, a lexicon mapping con-
cepts and relations to FB-LTAG trees is automat-
ically derived from the ontology using (Trevisan,
2010)?s approach. We specify for each experiment
below, the size of the extracted lexicon.
188
5.2 Linearisation
In this first experiment, we manually examined
whether the incremental algorithm we propose
supports the generation of NL queries whose word
order matches the linearisation of the input query
tree.
We created four series of queries such that each
serie is a sequence q
1
. . . q
n
where q
i+1
is an in-
crement of q
i
. That is, q
i+1
is derived from q
i
by adding, removing or substituting to q
i
a con-
cept or a relation. The series were devised so as to
encompass the whole range of possible operations
at different points of the preceding query (e.g., at
the last node/edge or on some node/edge occur-
ring further to the left of the previous query); and
include 14 revisions on 4 initial queries.
For all queries, the word order of the best NL
query produced by the generator was found to
match the linearisation of the DL query.
5.3 Fluency and Clarity
Following the so-called consensus model (Power
and Third, 2010), the current, template based ver-
sion of Quelo generates one clause per relation7.
Thus for instance, template-based Quelo will gen-
erate (5a) while our grammar based approach sup-
ports the generation of arguably more fluent sen-
tences such as (5b).
(5) a. I am looking for a car. Its make should
be a Land Rover. The body style of the
car should be an off-road car. The exterior
color of the car should be beige.
b. I am looking for car whose make is a Land
Rover, whose body style is an off-road car
and whose exterior color is beige.
We ran two experiments designed to assess how
fluency impacts users. The first experiment aims
to assess how Quelo template based queries are
perceived by the users in terms of clarity and flu-
ency, the second aims to compare these template
based queries with the queries produced by our
grammar-based approach.
Assessing Quelo template-based queries Us-
ing the Quelo interface, we generated a set of
41 queries chosen to capture different combina-
tions of concepts and relations. Eight persons
(four native speakers of English, four with C2
7This is modulo aggregation of relations. Thus two sub-
ject sharing relations may be realised in the same clause.
level of competence for foreign learners of En-
glish) were then asked to classify (a binary choice)
each query in terms of clarity and fluency. Fol-
lowing (Kow and Belz, 2012), we take Fluency
to be a single quality criterion intended to cap-
ture language quality as distinct from its meaning,
i.e. how well a piece of text reads. In contrast,
Clarity/ambiguity refers to ease of understanding
(Is the sentence easy to understand?). Taking the
average of the majority vote, we found that the
judges evaluated the queries as non fluent in 50%
of the cases and as unclear in 10% of the cases.
In other words, template based queries were found
to be disfluent about half of the time and unclear
to a lesser extent. The major observation made by
most of the participants was that the generated text
is too repetitive and lacks aggregation.
Figure 3: Online Evaluation.
Comparing template- and grammar-based
queries In this second experiment, we asked 10
persons (all proficient in the English language) to
compare pairs of NL queries where one query is
produced using templates and the other using our
grammar-based generation algorithm. The evalu-
ation was done online using the LG-Eval toolkit
(Kow and Belz, 2012) and geared to collect rel-
ative quality judgements using visual analogue
scales. After logging in, judges were given a de-
scription of the task. The sentence pairs were dis-
played as shown in Figure 3 with one sentence to
the left and the other to the right. The judges were
instructed to move the slider to the left to favor
the sentence shown on the left side of the screen;
and to the right to favor the sentence appearing to
the right. Not moving the slider means that both
sentences rank equally. To avoid creating a bias,
189
the sentences from both systems were equally dis-
tributed to both sides of the screen.
For this experiment, we used 14 queries built
from two ontologies, an ontology on cars and the
other on universities. The extracted lexicons for
each of these ontology contained 465 and 297 en-
tries respectively.
The results indicate that the queries generated
by the grammar based approach are perceived as
more fluent than those produced by the template
based approach (19.76 points in average for the
grammar based approach against 7.20 for the tem-
plate based approach). Furthermore, although the
template based queries are perceived as clearer
(8.57 for Quelo, 6.87 for our approach), the dif-
ference is not statistically significant (p < 0.5).
Overall thus, the grammar based approach appears
to produce verbalisations that are better accepted
by the users. Concerning clarity, we observed that
longer sentences let through by document plan-
ning were often deemed unclear. In future work,
we plan to improve clarity by better integrating
document planning and sentence realisation.
5.4 Coverage
One motivation for the symbolic based approach
was the lack of training corpus and the need for
portability: the query interface should be usable
independently of the underlying ontology and of
the existence of a training corpus. To support
coverage, we combined the grammar based ap-
proach with a lexicon which is automatically ex-
tracted from the ontology using the methodology
described in (Trevisan, 2010). When tested on
a corpus of 200 ontologies, this approach was
shown to be able to provide appropriate verbalisa-
tion templates for about 85% of the relation iden-
tifiers present in these ontologies. 12 000 relation
identifiers were extracted from the 200 ontologies
and 13 syntactic templates were found to be suf-
ficient to verbalise these relation identifiers (see
(Trevisan, 2010) for more details on this evalua-
tion).
That is, in general, the extracted lexicons permit
covering about 85% of the ontological data. In ad-
dition, we evaluated the coverage of our approach
by running the generator on 40 queries generated
from five distinct ontologies. The domains ob-
served are cinema, wines, human abilities, dis-
abilities, and assistive devices, e-commerce on the
Web, and a fishery database for observations about
an aquatic resource. The extracted lexicons con-
tained in average 453 lexical entries and the cov-
erage (proportion of DL queries for which the gen-
erator produced a NL query) was 87%.
Fuller coverage could be obtained by manually
adding lexical entries, or by developing new ways
of inducing lexical entries from ontologies (c.f.
e.g. (Walter et al., 2013)).
6 Conclusion
Conceptual authoring (CA) allows the user to
query a knowledge base without having any
knowledge either of the formal representation lan-
guage used to specify that knowledge base or of
the content of the knowledge base. Although this
approach builds on a tight integration between
syntax and semantics and requires an efficient pro-
cessing of revisions, existing CA tools predomi-
nantly make use of ad hoc generation algorithms
and restricted computational grammars (e.g., Def-
inite Clause Grammars or templates). In this pa-
per, we have shown that FB-LTAG and chart based
surface realisation provide a natural framework in
which to implement conceptual authoring. In par-
ticular, we show that the chart based approach nat-
urally supports the definition of an incremental al-
gorithm for query verbalisation; and that the added
fluency provided by the grammar based approach
potentially provides for query interfaces that are
better accepted by the human evaluators.
In the future, we would like to investigate the
interaction between context, document structuring
and surface realisation. In our experiments we
found out that this interaction strongly impacts flu-
ency whereby for instance, a complex sentence
might be perceived as more fluent than several
clauses but a too long sentence will be perceived
as difficult to read (non fluent). Using data that
can now be collected using our grammar based
approach to query verbalisation and generalising
over FB-LTAG tree names rather than lemmas or
POS tags, we plan to explore how e.g., Conditional
Random Fields can be used to model these inter-
actions.
Acknowledgments
We would like to thank Marco Trevisan, Paolo
Guagliardo and Alexandre Denis for facilitating
the access to the libraries they developed and to
Natalia Korchagina and the judges who partici-
pated in the evaluation experiments.
190
References
Franz Baader. 2003. The description logic handbook:
theory, implementation, and applications. Cam-
bridge university press.
Nina Dethlefs, Helen Hastie, Heriberto Cuaya?huitl, and
Oliver Lemon. 2013. Conditional Random Fields
for Responsive Surface Realisation using Global
Features. Proceedings of ACL, Sofia, Bulgaria.
Paolo Dongilli. 2008. Natural language rendering of a
conjunctive query. KRDB Research Centre Techni-
cal Report No. KRDB08-3). Bozen, IT: Free Univer-
sity of Bozen-Bolzano, 2:5.
E. Franconi, P. Guagliardo, and M. Trevisan. 2010a.
An intelligent query interface based on ontology
navigation. In Workshop on Visual Interfaces to the
Social and Semantic Web, VISSW, volume 10. Cite-
seer.
E. Franconi, P. Guagliardo, and M. Trevisan. 2010b.
Quelo: a NL-based intelligent query interface. In
Pre-Proceedings of the Second Workshop on Con-
trolled Natural Languages, volume 622.
E. Franconi, P. Guagliardo, S. Tessaris, and M. Tre-
visan. 2011a. A natural language ontology-driven
query interface. In 9th International Conference on
Terminology and Artificial Intelligence, page 43.
E. Franconi, P. Guagliardo, M. Trevisan, and S. Tes-
saris. 2011b. Quelo: an Ontology-Driven Query
Interface. In Description Logics.
C. Gardent and L. Perez-Beltrachini. 2010. RTG based
Surface Realisation for TAG. In COLING?10, Bei-
jing, China.
B. Gottesman Gardent, C. and L. Perez-Beltrachini.
2011. Using regular tree grammar to enhance sur-
face realisation. Natural Language Engineering,
17:185?201. Special Issue on Finite State Methods
and Models in Natural Language Processing.
Paolo Guagliardo. 2009. Theoretical foundations of
an ontology-based visual tool for query formulation
support. Technical report, KRDB Research Centre,
Free University of Bozen-Bolzano, October.
C. Hallett, D. Scott, and R. Power. 2007. Composing
questions through conceptual authoring. Computa-
tional Linguistics, 33(1):105?133.
Eric Kow and Anja Belz. 2012. LG-Eval: A Toolkit
for Creating Online Language Evaluation Experi-
ments. In LREC, pages 4033?4037.
Alice H Oh and Alexander I Rudnicky. 2000. Stochas-
tic language generation for spoken dialogue sys-
tems. In Proceedings of the 2000 ANLP/NAACL
Workshop on Conversational systems-Volume 3,
pages 27?32. Association for Computational Lin-
guistics.
R. Power and A. Third. 2010. Expressing owl ax-
ioms by english sentences: dubious in theory, fea-
sible in practice. In Proceedings of the 23rd Inter-
national Conference on Computational Linguistics:
Posters, pages 1006?1013. Association for Compu-
tational Linguistics.
David Schlangen and Gabriel Skantze. 2009. A gen-
eral, abstract model of incremental dialogue pro-
cessing. In Proceedings of the 12th Conference of
the European Chapter of the Association for Com-
putational Linguistics, pages 710?718. Association
for Computational Linguistics.
David Schlangen, Timo Baumann, and Michaela At-
terer. 2009. Incremental reference resolution: The
task, metrics for evaluation, and a bayesian filtering
model that is sensitive to disfluencies. In Proceed-
ings of the SIGDIAL 2009 Conference: The 10th An-
nual Meeting of the Special Interest Group on Dis-
course and Dialogue, pages 30?37. Association for
Computational Linguistics.
H. R Tennant, K. M Ross, R. M Saenz, C. W Thomp-
son, and J. R Miller. 1983. Menu-based natural lan-
guage understanding. In Proceedings of the 21st an-
nual meeting on Association for Computational Lin-
guistics, pages 151?158. Association for Computa-
tional Linguistics.
Marco Trevisan. 2010. A Portable Menuguided Nat-
ural Language Interface to Knowledge Bases for
Querytool. Ph.D. thesis, Masters thesis, Free Uni-
versity of Bozen-Bolzano (Italy) and University of
Groningen (Netherlands).
K. Vijay-Shanker and A. Joshi. 1988. Feature based
tags. In Proceedings of the 12th International Con-
ference of the Association for Computational Lin-
guistics, pages 573?577, Budapest.
Holger Wache, Thomas Voegele, Ubbo Visser, Heiner
Stuckenschmidt, Gerhard Schuster, Holger Neu-
mann, and Sebastian Hu?bner. 2001. Ontology-
based integration of information-a survey of existing
approaches. In IJCAI-01 workshop: ontologies and
information sharing, volume 2001, pages 108?117.
Citeseer.
Sebastian Walter, Christina Unger, and Philipp Cimi-
ano. 2013. A corpus-based approach for the induc-
tion of ontology lexica. In Natural Language Pro-
cessing and Information Systems, pages 102?113.
Springer.
191
The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 147?156,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Generating Grammar Exercises
Laura Perez-Beltrachini
Universite? de Lorraine
LORIA, UMR 7503
Vandoeuvre-le`s-Nancy
F-54500, France
laura.perez@loria.fr
Claire Gardent
CNRS, LORIA, UMR 7503
Vandoeuvre-le`s-Nancy
F-54500, France
claire.gardent@loria.fr
German Kruszewski
Inria, LORIA, UMR 7503
Villers-le`s-Nancy
F-54600, France
german.kruszewski@inria.fr
Abstract
Grammar exercises for language learning fall
into two distinct classes: those that are based
on ?real life sentences? extracted from exist-
ing documents or from the web; and those that
seek to facilitate language acquisition by pre-
senting the learner with exercises whose syn-
tax is as simple as possible and whose vo-
cabulary is restricted to that contained in the
textbook being used. In this paper, we in-
troduce a framework (called GramEx) which
permits generating the second type of gram-
mar exercises. Using generation techniques,
we show that a grammar can be used to
semi-automatically generate grammar exer-
cises which target a specific learning goal; are
made of short, simple sentences; and whose
vocabulary is restricted to that used in a given
textbook.
1 Introduction
Textbooks for language learning generally include
grammar exercises. Tex?s French Grammar 1 for in-
stance, includes at the end of each lecture, a set of
grammar exercises which target a specific pedagog-
ical goal such as learning the plural form of nouns
1Tex?s French Grammar http://www.laits.
utexas.edu/tex/ is an online pedagogical reference
grammar that combines explanations with surreal dialogues
and cartoon images. Tex?s French Grammar is arranged like
many other traditional reference grammars with the parts of
speech (nouns, verbs, etc.) used to categorize specific grammar
items (gender of nouns, irregular verbs). Individual grammar
items are carefully explained in English, then exemplified in a
dialogue, and finally tested in self-correcting, fill-in-the-blank
exercises.
or learning the placement of adjectives. Figure 1
shows the exercises provided by this book at the end
of the lecture on the plural formation of nouns. As
exemplified in this figure, these exercises markedly
differ from more advanced learning activities which
seek to familiarise the learner with ?real world sen-
tences?. To support in situ learning, this latter type
of activity presents the learner with sentences drawn
from the Web or from existing documents thereby
exposing her to a potentially complex syntax and to
a diverse vocabulary. In contrast, textbook grammar
exercises usually aim to facilitate the acquisition of
a specific grammar point by presenting the learner
with exercises made up of short sentences involving
a restricted vocabulary.
As shall be discussed in the next section, most ex-
isting work on the generation of grammar exercises
has concentrated on the automatic creation of the
first type of exercises i.e., exercises whose source
sentences are extracted from an existing corpus. In
this paper, we present a framework (called GramEx)
which addresses the generation of the second type of
grammar exercises used for language learning i.e.,
grammar exercises whose syntax and lexicon are
strongly controlled. Our approach uses generation
techniques to produce these exercises from an exist-
ing grammar describing both the syntax and the se-
mantics of natural language sentences. Given a ped-
agogical goal for which exercises must be produced,
the GramEx framework permits producing Fill in the
blank (FIB, the learner must fill a blank with an ap-
propriate form or phrase) and Shuffle (given a set of
lemmas or forms, the learner must use these to pro-
duce a phrase) exercises that target that specific goal.
147
Give the plural form of the noun indicated in parentheses.
Pay attention to both the article and the noun.
1. Bette aime _____ . (le bijoux)
2. Fiona aime ______ . (le cheval)
3. Joe-Bob aime ______ ame?ricaines. (la bie`re)
4. Tex n?aime pas ______ . (le choix)
5. Joe-Bob n?aime pas ______ difficiles. (le cours)
6. Tammy n?aime pas ______ . (l?ho?pital)
7. Eduard aime ______. (le tableau)
8. Bette aime ______ de Tex. (l?oeil)
9. Tex aime ______ franc?ais. (le poe`te)
10. Corey aime ______ fra??ches. (la boisson)
11. Tammy aime ______ ame?ricains. (le campus)
12. Corey n?aime pas ______ . (l?examen)
Figure 1: Grammar exercises from the Tex?s French Grammar textbook
The exercises thus generated use a simple syntax and
vocabulary similar to that used in the Tex?s French
Grammar textbook.
We evaluate the approach on several dimensions
using quantitative and qualitative metrics as well as a
small scale user-based evaluation. And we show that
the GramEx framework permits producing exercises
for a given pedagogical goal that are linguistically
and pedagogically varied.
The paper is structured as follows. We start by
discussing related work (Section 2). In Section 3,
we present the framework we developed to generate
grammar exercises. Section 4 describes the exper-
imental setup we used to generate exercise items.
Section 5 reports on an evaluation of the exercise
items produced and on the results obtained. Section
6 concludes.
2 Related Work
A prominent strand of research in Computer Aided
Language Learning (CALL) addresses the automa-
tion of exercise specifications relying on Natural
Language Processing (NLP) techniques (Mitkov et
al., 2006; Heilman and Eskenazi, 2007; Karama-
nis et al, 2006; Chao-Lin et al, 2005; Coniam,
1997; Sumita et al, 2005; Simon Smith, 2010; Lin
et al, 2007; Lee and Seneff, 2007). Mostly, this
work targets the automatic generation of so-called
objective test items i.e., test items such as multiple
choice questions, fill in the blank and cloze exercise
items, whose answer is strongly constrained and can
therefore be predicted and checked with high accu-
racy. These approaches use large corpora and ma-
chine learning techniques to automatically generate
the stems (exercise sentences), the keys (correct an-
swers) and the distractors (incorrect answers) that
are required by such test items.
Among these approaches, some proposals target
grammar exercises. Thus, (Chen et al, 2006) de-
scribes a system called FAST which supports the
semi-automatic generation of Multiple-Choice and
Error Detection exercises while (Aldabe et al, 2006)
presents the ArikiTurri automatic question genera-
tor for constructing Fill-in-the-Blank, Word Forma-
tion, Multiple Choice and Error Detection exercises.
These approaches are similar to the approach we
propose. First, a bank of sentences is built which are
automatically annotated with syntactic and morpho-
syntactic information. Second, sentences are re-
trieved from this bank based on their annotation and
on the linguistic phenomena the exercise is meant to
illustrate. Third, the exercise question is constructed
from the retrieved sentences. There are important
differences however.
First, in these approaches, the source sentences
used for building the test items are selected from
corpora. As a result, they can be very complex
and most of the generated test items are targeted
for intermediate or advanced learners. In addition,
some of the linguistic phenomena included in the
language schools curricula may be absent or insuf-
ficiently present in the source corpus (Aldabe et al,
2006). In contrast, our generation based approach
permits controlling both the syntax and the lexicon
of the generated exercices.
Second, while, in these approaches, the syntactic
and morpho-syntactic annotations associated with
the bank sentences are obtained using part-of-speech
tagging and chunking, in our approach, these are
obtained by a grammar-based generation process.
148
As we shall see below, the information thus asso-
ciated with sentences is richer than that obtained by
chunking. In particular, it contains detailed linguis-
tic information about the syntactic constructs (e.g.,
cleft subject) contained in the bank sentences. This
permits a larger coverage of the linguistic phenom-
ena that can be handled. For instance, we can re-
trieve sentences which contain a relativised cleft ob-
ject (e.g., This is the man whom Mary likes who
sleeps) by simply stipulating that the retrieved sen-
tences must be associated with the information Cleft
Object).
To sum up, our approach differs from most exist-
ing work in that it targets the production of syntac-
tically and lexically controlled grammar exercises
rather than producing grammar exercises based on
sentences extracted from an existing corpus.
3 Generating Exercises
Given a pedagogical goal (e.g., learning adjective
morphology), GramEx produces a set of exercise
items for practicing that goal. The item can be ei-
ther a FIB or a shuffle item; and GramEx produces
both the exercise question and the expected solution.
To generate exercise items, GramEx proceeds in
three main steps as follows. First, a generation
bank is constructed using surface realisation tech-
niques. This generation bank stores sentences that
have been generated together with the detailed lin-
guistic information associated by the generation al-
gorithm with each of these sentences. Next, sen-
tences that permit exercising the given pedagogical
goal are retrieved from the generation bank using a
constraint language that permits defining pedagog-
ical goals in terms of the linguistic properties as-
sociated by the generator with the generated sen-
tences. Finally, exercises are constructed from the
retrieved sentences using each retrieved sentence to
define FIB and Shuffle exercises; and the sentence
itself as the solution to the exercise.
We now discuss each of these steps in more detail.
3.1 Constructing a Generation bank
The generation bank is a database associating sen-
tences with a representation of their semantic con-
tent and a detailed description of their syntactic and
morphosyntactic properties. In other words, a gen-
Sentence realisation:
?Tammy a une voix douce?
Lemma-features pairs:
{?lemma?: ?Tammy?,
?lemma-features?: {anim:+,num:sg,det: +,wh:-,cat:n,
func:suj,xp: +, gen:f},
?trace?: {propername}},
{?lemma?: ?avoir?,
?lemma-features?: {aux-refl:-,inv:-,cat:v,pers:3,pron:-,
num:sg,mode:ind, aspect:indet,tense:pres,stemchange:-,
flexion:irreg},
?trace?: {CanonicalObject,CanonicalSubject,n0Vn1}},
{?lemma?: ?un?,
?lemma-features?: {wh:-,num:sg,mass:-,cat:d,
gen:f,def:+},
?trace?: {determiner}},
{?lemma?: ?voix?,
?lemma-features?: {bar:0,wh:-,cat:n,num:sg,
mass:-,gen:f,flexion:irreg,
?trace?: {noun}},
{?lemma?: ?doux?,
?lemma-features?: {num:sg,gen:f,flexion:irreg,cat:adj},
?trace?: {Epith,EpithPost}}
Figure 2: Morphosyntactic information associated by
GraDe with the sentence Tammy a un voix douce
eration bank is a set of (Si, Li, ?i) tuples where Si is
a sentence, Li is a set of linguistic properties true of
that sentence and ?i is its semantic representation.
To produce these tuples, we use the GraDe gram-
mar traversal algorithm described in (Gardent and
Kruszewski, 2012). Given a grammar and a set
of user-defined constraints, this algorithm gener-
ates sentences licensed by this grammar. The user-
defined constraints are either parameters designed to
constrain the search space and guarantee termina-
tion (e.g., upper-bound on the number and type of
recursive rules used or upper-bound on the depth of
the tree build by GraDe); or linguistic parameters
which permit constraining the output (e.g., by spec-
ifying a core semantics the output must verbalise or
by requiring the main verb to be of a certain type).
Here we use GraDe both to generate from manu-
ally specified semantic input; and from a grammar
(in this case an existing grammar is used and no
manual input need to be specified). As explained
in (Gardent and Kruszewski, 2012), when generat-
ing from a semantic representation, the output sen-
tences are constrained to verbalise that semantics but
the input semantics may be underspecified thereby
allowing for morpho-syntactic, syntactic and tem-
poral variants to be produced from a single se-
mantics. For instance, given the input semantics
149
L1:named(J bette n) A:le d(C RH SH) B:bijou n(C)
G:aimer v(E J C), GraDe will output among others
the following variants:
Bette aime le bijou (Bette likes the jewel),
Bette aime les bijoux (Bette likes the jewels),
C?est Bette qui aime le bijou (It is Bette who
likes the jewel), C?est Bette qui aime les bijoux
(It is Bette who likes the jewel), Bette aimait le
bijou (Bette liked the jewel), Bette aimait les
bijoux (Bette liked the jewels), ...
When generating from the grammar, the output
is even less constrained since all derivations com-
patible with the user-defined constraints will be pro-
duced irrespective of semantic content. For instance,
when setting GraDe with constraints restricting the
grammar traversal to only derive basic clauses con-
taining an intransitive verb, the output sentences in-
clude among others the following sentences:
Elle chante (She sings), La tatou chante-t?elle?
(Does the armadillo sing?), La tatou chante
(The armadillo sings), Chacun chante -t?il
(Does everyone sing? ), Chacun chante (Ev-
eryone sings), Quand chante la tatou? (When
does the armadillo sing?), ...
Figure 2 shows the linguistic properties associ-
ated with the sentence Tammy a une voix douce
(Tammy has a soft voice) by GraDe. To gener-
ate exercises, GramEx makes use of the morpho-
syntactic information associated with each lemma
i.e., the feature-value pairs occurring as values of the
lemma-features fields; and of their linguistic proper-
ties i.e., the items occurring as values of the trace
fields.
3.2 Retrieving Appropriate Sentences
To enable the retrieval of sentences that are appropri-
ate for a given pedagogical goal, we define a query
language on the linguistic properties assigned by
GraDe to sentences. We then express each peda-
gogical goal as a query in that language; and we use
these queries to retrieve from the generation bank
appropriate source sentences. For instance, to re-
trieve a sentence for building a FIB exercise where
the blank is a relative pronoun, we query the gen-
eration bank with the constraint RelativePronoun.
This will return all sentences in the generation bank
whose trace field contains the RelativePronoun
item i.e., all sentences containing a relative pronoun.
We then use this sentence to build both the exercise
question and its solution.
3.2.1 GramEx Query Language
We now define the query language used to retrieve
sentences that are appropriate to build an exercise
for a given pedagogical goal. Let B be a genera-
tion bank and let (Si, Li, ?i) be the tuples stored in
B. Then, a GramEx query q permits retrieving from
B the set of sentences Si ? (Si, Li, ?i) such that
Li satisfies q. In other words, GramEx queries per-
mit retrieving from the generation bank all sentences
whose linguistic properties satisfy those queries.
The syntax of the GramEx query language is as
follows:
BoolExpr ? BoolTerm
BoolTerm ? BoolFactor | BoolTerm ? BoolFactor
BoolFactor ? BoolUnary | BoolFactor ? BoolUnary
BoolUnary ? BoolPrimary | ? BoolPrimary
BoolPrimary ? PrimitiveCond | ( BoolExpr ) | [ BoolExpr ]
PrimitiveCond ? traceItem | feature = value
In words: the GramEx query language permits
defining queries that are arbitrary boolean con-
straints on the linguistic properties associated by
GraDe with each generated sentence. In addi-
tion, complex constraints can be named and reused
(macros); and expressions can be required to hold
on a single lexical item ([ BoolExpr] indicates that
BoolExpr should be satisfied by the linguistic prop-
erties of a single lexical item).
The signature of the language is the set of gram-
matical (traceItem) and morpho-syntactic proper-
ties (feature = value) associated by GraDe with
each generated sentence where traceItem is any
item occurring in the value of a trace field and
feature = value any feature/value pair occurring
in the value of a lemma-features field (cf. Fig-
ure 2). The Table below (Table 1) shows some of the
constraints that can be used to express pedagogical
goals in the GramEx query language.
3.2.2 Query Examples
The GramEx query language allows for very spe-
cific constraints to be expressed thereby providing
fine-grained control over the type of sentences and
therefore over the types of exercises that can be pro-
duced. The following example queries illustrate this.
150
Grammatical Properties (traceItem)
Argument Cleft, CleftSUbj, CleftOBJ, ...,
Realisation InvertedSubj
Questioned, QuSubj, ...
Relativised, RelSubj ...
Pronominalised, ProSubj, ...
Voice Active, Passive, Reflexive
Aux tse, modal, causal
Adjective Predicative,Pre/Post nominal
Adverb Sentential, Verbal
Morpho-Syntactic Properties (feature=value)
Tense present,future,past
Number mass, count, plural, singular
Inflexion reg,irreg
Table 1: Some grammatical and morpho-syntactic prop-
erties that can be used to specify pedagogical goals.
(1) a. EpithAnte
Tex pense que Tammy est une jolie tatou (Tex
thinks that Tammy is a pretty armadillo)
b. [Epith ? flexion: irreg]
Tex et Tammy ont une voix douce (Tex and
Tammy have a soft voice)
c. POBJinf ? CLAUSE
POBJinf ? (DE-OBJinf ? A-OBJinf)
CLAUSE ? Vfin??Mod ? ?CCoord? ?Sub
Tammy refuse de chanter (Tammy refuses to
sing)
Query (1a) shows a query for retrieving sentences
containing prenominal adjectives which uses the
grammatical (traceItem) property EpithAnte associ-
ated with preposed adjectives.
In contrast, Query (1b) uses both grammatical and
morpho-syntactic properties to retrieve sentences
containing a postnominal adjective with irregular in-
flexion. The square brackets in the query force the
conjunctive constraint to be satisfied by a single lex-
ical unit. That is, the query will be satisfied by sen-
tences containing a lexical item that is both a post-
nominal adjective and has irregular inflexion. This
excludes sentences including e.g., a postnominal ad-
jective and a verb with irregular inflexion.
Finally, Query (1c) shows a more complex case
where the pedagogical goal is defined in terms of
predefined macros themselves defined as GramEx
query expressions. The pedagogical goal is de-
fined as a query which retrieves basic clauses
(CLAUSE) containing a prepositional infinitival ob-
ject (POBJinf). A sentence containing a preposi-
tional infinitival object is in turn defined (second
line) as a prepositional object introduced either by
the de or the a` preposition. And a basic clause (3rd
line) is defined as a sentence containing a finite verb
and excluding modifiers, clausal or verb phrase co-
ordination (CCORD) and subordinated clauses2
3.3 Building Exercise Items
In the previous section, we saw the mechanism used
for selecting an appropriate sentence for a given
pedagogical goal. GramEx uses such selected sen-
tences as source or stem sentences to build exercise
items. The exercise question is automatically gen-
erated from the selected sentence based on its asso-
ciated linguistic properties. Currently, GramEx in-
cludes two main types of exercises namely, Fill in
the blank and Shuffle exercises.
FIB questions. FIB questions are built by remov-
ing a word from the target sentence and replacing it
with either: a blank (FIBBLNK), a lemma (FIBLEM)
or a set of features used to help the learner guess
the solution (FIBHINT). For instance, in an exercise
on pronouns, GramEx will use the gender, number
and person features associated with the pronoun by
the generation process and display them to specify
which pronominal form the learner is expected to
provide. The syntactic representation (cf. Figure 2)
associated by GraDe with the sentence is used to
search for the appropriate key word to be removed.
For instance, if the pedagogical goal is Learn Sub-
ject Pronouns and the sentence retrieved from the
generation bank is that given in (2a), GramEx will
produce the FIBHINT question in (2b) by search-
ing for a lemma with category cl (clitic) and feature
func=subj and using its gender value to provide the
learner with a hint constraining the set of possible
solutions.
(2) a. Elle adore les petits tatous
(She loves small armadillos)
b. ... adore les petits tatous (gender=fem)
Shuffle questions. Similarly to FIB questions,
shuffle exercise items are produced by inspecting
and using the target derivational information. More
specifically, lemmas are retrieved from the list of
2The expressions CCoord and Sub are themselves defined
rather than primitive expressions.
151
lemma-feature pairs. Function words are (option-
ally) deleted. And the remaining lemmas are ?shuf-
fled? (MSHUF). For instance, given the source sen-
tence (2a), the MSHUF question (2b) can be pro-
duced.
(3) a. Tammy adore la petite tatou
a. tatou / adorer / petit / Tammy
Note that in this case, there are several possible
solutions depending on which tense and number is
used by the learner. For such cases, we can either
use hints as shown above to reduce the set of pos-
sible solutions to one; or compare the learner?s an-
swer to the set of output produced by GraDe for the
semantics the sentence was produced from.
4 Experimental Setup
We carried out an experiment designed to assess the
exercises produced by GramEx. In what follows, we
describe the parameters of this experiment namely,
the grammar and lexicons used; the input and the
user-defined parameters constraining sentence gen-
eration; and the pedagogical goals being tested.
4.1 Grammar and Lexicon
The grammar used is a Feature-Based Lexicalised
Tree Adjoining Grammar for French augmented
with a unification-based compositional semantics.
This grammar contains around 1300 elementary
trees and covers auxiliaries, copula, raising and
small clause constructions, relative clauses, infini-
tives, gerunds, passives, adjuncts, wh-clefts, PRO
constructions, imperatives and 15 distinct subcate-
gorisation frames.
The syntactic and morpho-syntactic lexicons used
for generating were derived from various existing
lexicons, converted to fit the format expected by
GraDe and tailored to cover basic vocabulary as de-
fined by the lexicon used in Tex?s French Grammar.
The syntactic lexicon contains 690 lemmas and the
morphological lexicon 5294 forms.
4.2 Pedagogical Goals
We evaluate the approach on 16 pedagogical goals
taken from the Tex?s French Grammar book. For
each of these goals, we define the corresponding
linguistic characterization in the form of a GramEx
query. We then evaluate the exercises produced by
the system for each of these queries. The pedagog-
ical goals tested are the following (we indicate in
brackets the types of learning activity produced for
each teaching goal by the system):
? Adjectives: Adjective Order (MSHUF), Adjec-
tive Agreement (FIBLEM), Prenominal adjec-
tives (FIBLEM), Present and Past Participial
used as adjectives (FIBLEM), Regular and Ir-
regular Inflexion (FIBLEM), Predicative adjec-
tives (MSHUF)
? Prepositions: Prepositional Infinitival Object
(FIBBLNK), Modifier and Complement Prepo-
sitional Phrases (FIBBLNK)
? Noun: Gender (FIBLEM), Plural form (FI-
BLEM), Subject Pronoun (FIBHINT).
? Verbs: Pronominals (FIBLEM), -ir Verbs in
the present tense (FIBLEM), Simple past (FI-
BLEM), Simple future (FIBLEM), Subjunctive
Mode (FIBLEM).
4.3 GraDe?s Input and User-Defined
Parameters
GraDe?s configuration As mentioned in Sec-
tion 3, we run GraDe using two main configura-
tions. In the first configuration, GraDe search is con-
strained by an input core semantics which guides the
grammar traversal and forces the output sentence to
verbalise this core semantics. In this configuration,
GraDe will only produce the temporal variations
supported by the lexicon (the generated sentences
may be in any simple tense i.e., present, future,
simple past and imperfect) and the syntactic varia-
tions supported by the grammar for the same MRSs
(e.g., active/passive voice alternation and cleft argu-
ments).
Greater productivity (i.e., a larger output/input ra-
tio) can be achieved by providing GraDe with less
constrained input. Thus, in the second configura-
tion, we run GraDe not on core semantics but on the
full grammar. To constrain the search, we specify a
root constraint which requires that the main verb of
all output sentences is an intransitive verb. We also
set the constraints on recursive rules so as to exclude
the inclusion of modifiers. In sum, we ask GraDe to
produce all clauses (i) licensed by the grammar and
the lexicon; (ii) whose verb is intransitive; and (iii)
152
which do not include modifiers. Since the number
of sentences that can be produced under this con-
figuration is very large, we restrict the experiment
by using a lexicon containing a single intransitive
verb (chanter/To sing), a single common noun and a
single proper name. In this way, syntactically struc-
turally equivalent but lexically distinct variants are
excluded.
Input Semantics We use two different sets of in-
put semantics for the semantically guided configura-
tion: one designed to test the pedagogical coverage
of the system (Given a set of pedagogical goals, can
GramEx generate exercises that appropriately target
those goals?); and the other to illustrate linguistic
coverage (How much syntactic variety can the sys-
tem provide for a given pedagogical goal?).
The first set (D1) of semantic representations con-
tains 9 items representing the meaning of exam-
ple sentences taken from the Tex?s French Gram-
mar textbook. For instance, for the first item
in Figure 1, we use the semantic representation
L1:named(J bette n) A:le d(C RH SH) B:bijou n(C)
G:aimer v(E J C). With this first set of input seman-
tics, we test whether GramEx correctly produces the
exercises proposed in the Tex?s French Grammar
book. Each of the 9 input semantics corresponds to
a distinct pedagogical goal.
The second set (D2) of semantic representations
contains 22 semantics, each of them illustrating dis-
tinct syntactic configurations namely, intransitive,
transitive and ditransitive verbs; raising and control;
prepositional complements and modifiers; sentential
and prepositional subject and object complements;
pronominal verbs; predicative, attributive and par-
ticipial adjectives. With this set of semantics, we
introduce linguistically distinct material thereby in-
creasing the variability of the exercises i.e., making
it possible to have several distinct syntactic configu-
rations for the same pedagogical goal.
5 Evaluation, Results and Discussion
Using the experimental setup described in the previ-
ous section, we evaluate GramEx on the following
points:
? Correctness: Are the exercises produced by the
generator grammatical, meaningful and appro-
priate for the pedagogical goal they are associ-
ated with?
? Variability: Are the exercises produced linguis-
tically varied and extensive? That is, do the ex-
ercises for a given pedagogical goal instantiate
a large number of distinct syntactic patterns?
? Productivity: How much does GramEx support
the production, from a restricted number of se-
mantic input, of a large number of exercises?
Correctness To assess correctness, we randomly
selected 10 (pedagogical goal, exercise) pairs for
each pedagogical goal in Section 4.2 and asked two
evaluators to say for each pair whether the exer-
cise text and solutions were grammatical, meaning-
ful (i.e., semantically correct) and whether the ex-
ercise was adequate for the pedagogical goal. The
results are shown in Table 3 and show that the sys-
tem although not perfect is reliable. Most sources of
grammatical errors are cases where a missing word
in the lexicon fails to be inflected by the generator.
Cases where the exercise is not judged meaningful
are generally cases where a given syntactic construc-
tion seems odd for a given semantics content. For
instance, the sentence C?est Bette qui aime les bi-
joux (It is Bette who likes jewels) is fine but C?est
Bette qui aime des bijoux although not ungrammati-
cal sounds odd. Finally, cases judged inappropriate
are generally due to an incorrect feature being as-
signed to a lemma. For instance, avoir (To have) is
marked as an -ir verb in the lexicon which is incor-
rect.
Grammatical Meaningful Appropriate
91% 96% 92%
Table 3: Exercise Correctness tested on 10 randomly se-
lected (pedagogical goal, exercise pairs)
We also asked a language teacher to examine 70
exercises (randomly selected in equal number across
the different pedagogical goals) and give her judg-
ment on the following three questions:
? A. Do you think that the source sentence se-
lected for the exercise is appropriate to practice
the topic of the exercise? Score from 0 to 3 ac-
cording to the degree (0 inappropriate - 3 per-
fectly appropriate)
153
Nb. Ex. 1 2 4 5 6 12 17 18 20 21 23 26 31 37 138
Nb. Sem 1 4 6 1 4 3 1 1 1 1 1 1 1 1 1
Table 2: Exercise Productivity: Number of exercises produced per input semantics
? B. The grammar topic at hand together with
the complexity of the source sentence make
the item appropriate for which language level?
A1,A2,B1,B2,C13
? C. Utility of the exercise item: ambiguous (not
enough context information to solve it) / correct
For Question 1, the teacher graded 35 exercises as
3, 20 as 2 and 14 as 1 pointing to similar problems
as was independently noted by the annotators above.
For question B, she marked 29 exercises as A1/A2,
24 as A2, 14 as A2/B1 and 3 as A1 suggesting that
the exercises produced are non trivial. Finally, she
found that 5 out of the 70 exercises lacked context
and were ambiguously phrased.
Variability For any given pedagogical goal, there
usually are many syntactic patterns supporting learn-
ing. For instance, learning the gender of common
nouns can be practiced in almost any syntactic con-
figuration containing a common noun. We assess the
variability of the exercises produced for a given ped-
agogical goal by computing the number of distinct
morpho-syntactic configurations produced from a
given input semantics for a given pedagogical goal.
We count as distinct all exercise questions that are
derived from the same semantics but differ either
in syntax (e.g., passive/active distinction) or in mor-
phosyntax (determiner, number, etc.). Both types of
differences need to be learned and therefore produc-
ing exercises which, for a given pedagogical goal,
expose the learner to different syntactic and morpho-
syntactic patterns (all involving the construct to be
learned) is effective in supporting learning. How-
ever we did not take into account tense differences
as the impact of tense on the number of exercises
produced is shown by the experiment where we gen-
erate by traversing the grammar rather than from a
3A1, A2, B1, B2 and C1 are reference levels established
by the Common European Framework of Reference for
Languages: Learning, Teaching, Assessment (cf. http:
//en.wikipedia.org/wiki/Common_European_
Framework_of_Reference_for_Languages) for
grading an individual?s language proficiency.
semantics. Table 4 shows for each (input semantics,
teaching goal) pair the number of distinct patterns
observed. The number ranges from 1 to 21 distinct
patterns with very few pairs (3) producing a single
pattern, many (33) producing two patterns and a fair
number producing either 14 or 21 patterns.
Nb. PG 1 2 3 4 5 6
Nb. sent 213 25 8 14 10 6
Table 6: Pedagogical Productivity: Number of Teaching
Goals the source sentence produced from a given seman-
tics can be used for
Productivity When used to generate from seman-
tic representations (cf. Section 4.3), GramEx only
partially automates the production of grammar ex-
ercises. Semantic representations must be manually
input to the system for the exercises to be generated.
Therefore the issue arises of how much GramEx
helps automating exercise creation. Table 5 shows
the breakdown of the exercises produced per teach-
ing goal and activity type. In total, GramEx pro-
duced 429 exercises out of 28 core semantics yield-
ing an output/input ratio of 15 (429/28). Further, Ta-
ble 2 and 6 show the distribution of the ratio be-
tween (i) the number of exercises produced and the
number of input semantics and (ii) the number of
teaching goals the source sentences produced from
input semantics i can be used for. Table 6 (peda-
gogical productivity) shows that, in this first exper-
iment, a given input semantics can provide material
for exercises targeting up to 6 different pedagogi-
cal goals while Table 2 (exercise productivity) shows
that most of the input semantics produce between 2
and 12 exercises4.
When generating by grammar traversal, under the
constraints described in Section 4, from one input
4If the input semantics contains a noun predicate whose gen-
der is underspecified, the exercise productivity could be dou-
bled. This is the case for 4 of the input semantics in the dataset
D2; i.e. an input semantics containing the predicates tatou n(C)
petit a(C) will produce variations such as: la petite tatou (the
small armadillo (f)) and le petit tatou (the small armadillo (m)).
154
Nb. SP 1 2 3 4 5 6 7 8 9 10 14 21
(S,G) 3 33 16 7 2 4 6 1 4 1 2 6
Table 4: Variability: Distribution of the number of distinct sentential patterns that can be produced for a given peda-
gogical goal from a given input semantics
Pedagogical Goal FIBLEM FIBBLNK MSHUF FIBHINT
Preposition ? 28 ? ?
Prepositions with infinitives ? 8 ? ?
Subject pronouns?il ? ? ? 3
Noun number 11 ? ? ?
Noun gender ? 49 ? ?
Adjective order ? ? 30 ?
Adjective morphology 30 ? ? ?
Adjectives that precede the noun 24 ? ? ?
Attributive Adjectives ? ? 28 ?
Irregular adjectives 4 ? ? ?
Participles as adjectives 4 ? ? ?
Simple past 78 ? ? ?
Simple future 90 ? ? ?
-ir verbs in present 18 ? ? ?
Subjunctive mode 12 ? ? ?
Pronominal verbs 12 ? ? ?
Total 236 78 30 3
Table 5: Number and Types of Exercises Produced from the 28 input semantics
90 exercises are generated targeting 4 different ped-
agogical goals (i.e. 4 distinct linguistic phenomena).
6 Conclusion
We presented a framework (called GramEx) for gen-
erating grammar exercises which are similar to those
often used in textbooks for second language learn-
ing. These exercises target a specific learning goal;
and, they involve short sentences that make it eas-
ier for the learner to concentrate on the grammatical
point to be learned.
One distinguishing feature of the approach is the
rich linguistic information associated by the gen-
erator with the source sentences used to construct
grammar exercises. Although space restriction pre-
vented us from showing it here, this information
includes, in addition to the morphosyntactic infor-
mation and the grammatical properties illustrated in
Figure 2 and Table 1 respectively, a semantic rep-
resentation, a derivation tree showing how the parse
tree of each sentence was obtained and optionally,
an underspecified semantics capturing the core pred-
icate/argument and modifier/modifiee relationships
expressed by each sentence. We are currently ex-
ploring how this information could be used to ex-
tend the approach to transformation exercises (e.g.,
passive/active) where the relation between exercise
question and exercise solution is more complex than
in FIB exercises.
Another interesting question which needs further
investigation is how to deal with exercise items that
have multiple solutions such as example (3) above.
Here we plan to use the fact that underspecified se-
mantics in GraDe permits associating many variants
with a given semantics.
Acknowledgments
We would like to thank the language teacher, Tex?s
French Grammar developers, and the anonymous re-
viewers for their useful comments. The research
presented in this paper was partially supported
by the European Fund for Regional Development
within the framework of the INTERREG IV A Alle-
gro Project5.
5http://www.allegro-project.eu/ and http:
//talc.loria.fr/-ALLEGRO-Nancy-.html
155
References
Itziar Aldabe, Maddalen Lopez de Lacalle, Montse Mar-
itxalar, Edurne Martinez, and Larraitz Uria. 2006.
Arikiturri: an automatic question generator based on
corpora and nlp techniques. In Proceedings of the
8th international conference on Intelligent Tutoring
Systems, ITS?06, pages 584?594, Berlin, Heidelberg.
Springer-Verlag.
Liu Chao-Lin, Wang Chun-Hung, Gao Zhao-Ming, and
Huang Shang-Ming. 2005. Applications of lexical
information for algorithmically composing multiple-
choice cloze items. In Proceedings of the second
workshop on Building Educational Applications Us-
ing NLP, EdAppsNLP 05, pages 1?8, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Chia-Yin Chen, Hsien-Chin Liou, and Jason S. Chang.
2006. Fast: an automatic generation system for gram-
mar tests. In Proceedings of the COLING/ACL on
Interactive presentation sessions, COLING-ACL ?06,
pages 1?4, Stroudsburg, PA, USA. Association for
Computational Linguistics.
David Coniam. 1997. A preliminary inquiry into using
corpus word frequency data in the automatic genera-
tion of english language cloze tests. CALICO Journal,
14:15?33.
Claire Gardent and German Kruszewski. 2012. Gener-
ation for grammar engineering. In 11th International
Conference on Natural Language Generation (ENLG).
Michael Heilman and Maxine Eskenazi. 2007. Ap-
plication of automatic thesaurus extraction for com-
puter generation of vocabulary questions. In Proceed-
ings of Speech and Language Technology in Education
(SLaTE2007), pages 65?68.
Nikiforos Karamanis, Le An Ha, and Ruslan Mitkov.
2006. Generating multiple-choice test items from
medical text: A pilot study. In Proceedings of the
Fourth International Natural Language Generation
Conference, pages 111?113, Sydney, Australia.
John Lee and Stephanie Seneff. 2007. Automatic gener-
ation of cloze items for prepositions. Proceedings of
Interspeech, pages 2173?2176.
Yi-Chien Lin, Li-Chun Sung, and Meng Chang Chen.
2007. An Automatic Multiple-Choice Question Gen-
eration Scheme for English Adjective Understandings.
In Workshop on Modeling, Management and Gener-
ation of Problems/Questions in eLearning, the 15th
International Conference on Computers in Education
(ICCE 2007), pages pages 137?142.
Ruslan Mitkov, Le An Ha, and Nikiforos Karamanis.
2006. A computer-aided environment for generating
multiple-choice test items. Natural Language Engi-
neering, 12(2):177?194.
Adam Kilgarriff Simon Smith, P.V.S Avinesh. 2010.
Gap-fill Tests for Language Learners: Corpus-Driven
Item Generation. In Proceedings of ICON-2010: 8th
International Conference on Natural Language Pro-
cessing.
Eiichiro Sumita, Fumiaki Sugaya, and Seiichi Ya-
mamoto. 2005. Measuring non-native speakers? pro-
ficiency of english by using a test with automatically-
generated fill-in-the-blank questions. In Proceedings
of the second workshop on Building Educational Ap-
plications Using NLP, EdAppsNLP 05, pages 61?68,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
156
Proceedings of the SIGDIAL 2013 Conference, pages 357?359,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
Weakly and Strongly Constrained Dialogues for Language Learning
Claire Gardent
CNRS/LORIA, Nancy
claire.gardent@loria.fr
Alejandra Lorenzo
Universite? de Lorraine
LORIA, Nancy
alejandra.lorenzo@loria.fr
Laura Perez-Beltrachini
KRDB Research Centre
FUB, Bolzano
laura.perez@loria.fr
Lina Rojas-Barahona
Universite? de Lorraine
LORIA, Nancy
lina.rojas@loria.fr
Abstract
We present two dialogue systems for lan-
guage learning which both restrict the di-
alog to a specific domain thereby pro-
moting robustness and the learning of a
given vocabulary. The systems vary in how
much they constrain the learner?s answer :
one system places no other constrain on
the learner than that provided by the re-
stricted domain and the dialog context ; the
other provides the learner with an exercise
whose solution is the expected answer.
The first system uses supervised learning
for simulating a human tutor whilst the
second one uses natural language gener-
ation techniques to produce grammar ex-
ercises which guide the learner toward the
expected answer.
1 Introduction
Work on dialog based tutors for language learn-
ing includes both chatbot systems which maintain
a free flowing dialog with the learner (Shawar and
Atwell, 2007; Jia, 2004) and form-focused dia-
log systems which restrict the learner answer e.g.,
by providing her with an answer template to be
filled in for the dialog to continue (Wilske and
Wolska, 2011). While the former encourages lan-
guage practice with a virtual tutor and requires a
good knowledge of the language, the latter focuses
on linguistic forms and usually covers a more re-
stricted lexical field thereby being more amenable
to less advanced learners.
In these notes, we describe a dialog architecture
which (i) supports both free-flowing and form-
focused man/machine dialog ; and (ii) ensures that
in both cases, dialogs are restricted to a specific
lexical field. The free-flowing dialog system uses
supervised classification techniques to predict the
system dialog move based on the learner?s input
and does not explicitely constrain the learner?s an-
swer. In contrast, the dialog system for intermedi-
ate learners provides an exercise which the learner
must solve to construct her answer.
To restrict the dialog to a specific domain and
to improve system robustness, we make use of a
finite-state automaton (FSA) describing the range
of permissible interactions within a given domain.
This FSA serves to guide the collection of human-
human interactions necessary to train the classi-
fier ; to verify and if necessary to adjust the sys-
tem?s predicted answer ; and to support the gener-
ation of the exercise provided in the form-focused
dialog engine.
2 Finite State Automaton and Domain
Representation
To support focused dialog and promote robust-
ness, we make use of the FSA depicted in Fig-
ure 2. This FSA models a generic sequence of
(System Question, Learner Answer, System Re-
action) turns instantiating 4 main question/answer
patterns and two possible system reactions.
The system question systematically formulates
a question which explicitly lists a set of possible
answers.
The learner?s answer is then classified into four
main types. It selects one of the proposed answers ;
it chooses another alternative not mentioned in
the system?s question ; it indicates that the learner
does not know the correct answer or does not un-
derstand the question ; or it meets neither of these
categories.
Finally, for each answer, the system can con-
tinue the dialogue by making a ?confirmation?
which is used to reassure the learner in the free
dialogue system and to provide some feedback in
the guided dialogue system ; or just by ?inform-
ing? to the learner of the correct answer before
getting into the next question.
Given a specific domain, this generic FSA is
357
>S:Welcome to my house!, I?m the cat. Do you want
to play with me? || ask(ready)
>L:Ok. || answer(ready)
>S:Great! How are you doing? || ask(health)
>L: I?m doing very well, thanks!
|| answer(health, pref-option)
>S: Awesome! || confirm(health,pref-option)
>S: Come! I want to show you the kitchen!
|| inform(health)
>S:In the kitchen we prepare lunch. I prepare
soup very often. Do you prefer soup or salad?
|| ask(pref-meal)
>L:I prefer salad.
|| answer(pref-meal,pref-option)
>S:So do I! || confirm(pref-meal,pref-option)
>S:I also love cheese. || inform(pref-meal)
>S:Where do we usually keep the cheese?
|| ask(cheese where)
>L:In the fridge || answer(cheese where,pref-option)
>S:You are right || confirm(cheese where,pref-option)
>S:In the fridge || inform(cheese where)
...
>S:Thanks for this little chat! See you soon
|| quit
FIGURE 1 ? Example of a dialog (translated from
French to English)
instantiated differently for each question/answer
pair making up a dialog. In particular, in the cur-
rent demonstration system, it is instantiated to
model a dialog situated in the kitchen of a virtual
world. Figure 1 shows an example dialog.
3 Situated Dialogues for language
learning
Our dialog systems for language learning are in-
tegrated in a serious game called I-FLEG (Interac-
tive French Learning Game, (Amoia et al, 2012))
in which the learner can trigger grammar exercices
and interactive dialog sessions by clicking on the
objects present in the virtual world.
IFLEG integrates the two dialog systems for
language learning mentioned above namely, a
?free answer dialog system? where the learner an-
swer is guided only by the preceding dialog ex-
changes ; and a ?guided dialog system? which re-
stricts the set of permissible answers by providing
the learner with an exercise whose solution pro-
vides a possible answer given the current dialog
context.
3.1 Data collection
To provide the training data necessary to train
the free dialog system, we conducted a Wizard-
of-Oz experiment where language learners were
invited to engage in a conversation with the wiz-
ard, a French tutor. In these experiments, we fol-
lowed the methodology and used the tools for
data collection and annotation presented in (Rojas-
Barahona et al, 2012a). Given an FSA specifiying
a set of 5 questions the learner had to answer, the
wizard guided the learner through the dialog us-
ing this FSA. The resulting corpus consists of 52
dialogues and 1906 sentences.
3.2 Free answer Dialogue System
The free answer dialogue system simulates
the behavior of the wizard tutor by means of
a Logistic-Regression classifier, the FSA and
a generation-by-selection algorithm. The system
first uses the FSA to determine the next question
to be asked. Then for each question, the Logistic-
Regression classifier is used to map the learner an-
swer to a system dialog act. At this stage, the FSA
is used again, in two different ways. First, it is used
to ensure that the predicted system dialog act is
consistent with the states in the FSA. In case of a
mismatch, a valid dialog act is selected in the cur-
rent context. In particular, unpredicted ?preferred
options? and ?do not know? learner answers are
detected using keyword spotting methods. If the
classifier prediction conflicts with the prediction
made by key word spotting, it is ignored and the
FSA transition is prefereed.
Second, since the system has several consecu-
tive turns, and given that the classifier only pre-
dicts the next one, the FSA is used to determine
the following system dialog acts sequence. For
instance, if the predicted next system dialog act
was ?confirm?, according to the FSA the follow-
ing system dialog act is ?inform? and then eiher
the next question encoded in the FSA or ?quit?.
Training the simulator To train the classifier,
we labeled each learner sentence with the dialog
act caracterising the next system act. The features
used for trainig included context features (namely,
the four previous system dialogue acts) and the set
of content words present in the learner turns af-
ter filtering using tf*idf (Rojas Barahona et al,
2012b). Given the learner input and the current di-
alog context, the classifier predicts the next system
move.
Generation by Selection Given the system move
predicted by the dialog manager, the system turn
is produced by randomly selecting from the train-
ing corpus an utterance annotated with that dialog
move.
3.3 Guided dialogue system
Unlike the free answer dialogue, the guided di-
alogue strongly constrains the learner answer by
suggesting it in the form of a grammar exercise.
358
FIGURE 2 ? Finite-state automata that defines the different states in the dialog for each question Q X. S
defines the system, and P the learner.
In the guided dialogue system, the dialogue
paths contained in the training corpus are used to
decide on the next dialogue move. In a first step,
learner?s moves are labelled with the meaning rep-
resentation associated to them by the grammar un-
derlying the natural language generator used to
produce IFLEG grammar exercises. Given a se-
quence S/L contained in the training corpus with
S, a system turn and L the corresponding learner?s
turn, the system then constructs the exercise pro-
viding the learner?s answer using the methodology
described in (Perez-Beltrachini et al, 2012). First,
a sentence is generated from the meaning repre-
sentation of the learner answer. Next, the linguis-
tic information (syntactic tree, morpho-syntactic
information, lemmas) associated by the generator
with the generated sentence is used to build a shuf-
fle, a fill-in-the-blank or a transformation exercise.
Here is an example interaction produced by the
system :
S : Vous pre?fe?rez la soupe ou le fromage ? (Do you
prefer soup or salad ?)
Please answer using the following words : { je,
adorer, le, soupe }
This dialogue setting has several benefits. The
dialogue script provides a rich context for each
generated exercise item, learners are exposed to
example communicative interactions, and the sys-
tem can provide feedback by comparing the an-
swer entered by the learner against the expected
one.
4 Sample Dialogue
In this demo, the user will be able to interact
with both dialogue systems, situated in the kitchen
of a virtual world, and where the tutor prompts
the learner with questions about meals, drinks,
and various kitchen related activities such as floor
cleaning and food preferences.
References
M. Amoia, T. Bre?taudie`re, A. Denis, C. Gardent, and
L. Perez-Beltrachini. 2012. A Serious Game for Second
Language Acquisition in a Virtual Environment. Jour-
nal on Systemics, Cybernetics and Informatics (JSCI),
10(1) :24?34.
J. Jia. 2004. The study of the application of a web-based
chatbot system on the teaching of foreign languages. In
Society for Information Technology & Teacher Educa-
tion International Conference, volume 2004, pages 1201?
1207.
L. Perez-Beltrachini, C. Gardent, and G. Kruszewski. 2012.
Generating Grammar Exercises. In NAACL-HLT 7th
Workshop on Innovative Use of NLP for Building Educa-
tional Applications, Montreal, Canada, June.
L. M. Rojas-Barahona, A. Lorenzo, and C. Gardent. 2012a.
Building and exploiting a corpus of dialog interactions be-
tween french speaking virtual and human agents. In Pro-
ceedings of the 8th International Conference on Language
Resources and Evaluation.
L. M. Rojas Barahona, A. Lorenzo, and C. Gardent. 2012b.
An end-to-end evaluation of two situated dialog systems.
In Proceedings of the 13th Annual Meeting of the Special
Interest Group on Discourse and Dialogue, pages 10?19,
Seoul, South Korea, July. ACL.
B. Abu Shawar and E. Atwell. 2007. Chatbots : are they
really useful ? In LDV Forum, volume 22, pages 29?49.
S. Wilske and M. Wolska. 2011. Meaning versus form in
computer-assisted task-based language learning : A case
study on the german dative. JLCL, 26(1) :23?37.
359
