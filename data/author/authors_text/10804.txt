Concepts across categories
Hilke Reckman and Crit Cremers
Leiden University Centre for Linguistics (LUCL)
Leiden, Netherlands
{h. g. b. reckman,c. l. j. m. cremers }@let. leidenuniv. nl
Abstract
Verbs or adjectives and their nominalizations and certain adverb adjective pairs
can be argued to introduce the same concept. This can be shown through inference
patterns, which can be explained if we assume Davidsonian eventualities underlying
all predicates. We make a contribution to the underlying state discussion by inves-
tigating the advantages and disadvantages of Davidsonian versus Kimian states for
statives such as copular predicates. Findings are implemented in our parser Delilah.
1 Introduction
Several computational semantics systems have by now implemented a form of
event analysis for verbs [1,3]. There has been much debate on whether it is
desirable to assume underlying states, parallel to underlying events. Katz [9]
argues against an underlying state analysis, even for stative verbs, whereas
Parsons [12] is ready to accept an underlying state analysis, even for simple
nouns. It is clear that states are more problematic than events.
We discuss some cases where words of different categories can be argued
to introduce the same concept: verbs and their nominalizations and adjec-
tives and their corresponding abstract nouns. We show that underlying states
give us the same advantages as underlying events, with respect to recognizing
concepts across categories for the purpose of inference, as they reify the pred-
icates. We then discuss an alternative representation for copular expressions,
based on the conviction that the states in these expressions are ontologically
different from eventualities, and show that it has unfavorable consequences for
inference. We end with a short note on related adjective-adverb pairs.
The present research was carried out in the context of the Narrator project,
which aims at the development of a system for storage and retrieval of personal
illness relating narratives [13,14]. In this project we use and further develop
a semantic parser/generator for Dutch, Delilah [5,4]. Delilah is driven by a
Combinatory Categorial Grammar and has a semantic output in first order
logic with neo-Davidsonian event structures.
2 Verbs and their nominalizations
In this section we use nominalizations of verbs to illustrate our main consider-
ations. Sentence (1a) uses the noun operatie ?operation, surgery? and (1b) uses
the verb opereren ?operate?. The intuition is that (1a) and (1b) are equivalent.
They can be inferred from each other.
(1) a. Marie
Mary
onderging
underwent
een
an
operatie.
operation
?Mary went though/ had surgery.?
b. Marie
Mary
werd
was
geopereerd.
operated
?Mary was operated on.?
The same goes for (2a) and (2b), containing negation.
(2) a. Marie
Mary
onderging
underwent
geen
no
operatie.
operation
?Mary went though/ had surgery.?
b. Marie
Mary
werd
was
niet
not
geopereerd.
operated
?Mary was not operated on.?
Since the narratives in Narrator are about experiences of patients (in the
prototype being currently developed, on breast cancer), this kind of informa-
tion is rather relevant and should preferably not be missed or misinterpreted.
If one of the search criteria is, for example, that the narrative should tell about
a patient who had surgery, then each of these sentences above, if occurring in
a narrative, provides the relevant information to determine wether it meets
this search criterion or not. And of each pair, both variants provide the same
information.
Opereren en operatie introduce the same concept. Also the relation be-
tween opereren/operatie and Marie is the same in both (1a) and (1b). Ar-
guably it can also be inferred in both cases that there is yet someone else
involved who is not mentioned, a filler for the agent-slot of opereren/operatie.
A form of neo-Davidsonian event analysis can be used to give both sen-
tences the same semantic representation. The basic event representation for
both (1a) and (1b) is illustrated below. The representation is based on Parsons
[11]. (The ?concept of? relation is comparable to Jurafsky and Martin?s ?is-a?
[8].). The verb form is taken to name the concept. The verb can be consid-
ered as basic in a situation like this, because underived nouns do not usually
introduce events. As it does not lie within the scope of this paper to discuss
what is the best way to represent time/tense, we keep the representations very
simple in that respect.
(3) ?e.event(e) & concept of(e, operate) & agent of(e, x) & theme of(e,
Mary) & at-time(e, past)
For (1b) this kind of representation is quite standard, and event repre-
sentations for event-denoting nominalizations have also been suggested before
[11,7]. The verb ondergaan in (1a) plays a special role. It places the event in
time (makes it extensional) and it lets its subject be the theme of the surgery
event.
3 Adjectives and nouns
In the previous section we have looked at nominalizations of verbs, and seen
that event semantics helps us getting the right entailments. Now we will look
at adjectives and their nominalizations. The pair below is at least close to
equivalent. Who has an illness, is ill. Who is ill, has at least one illness.
(4) a. Marie
Mary
had
had
een
an
ziekte.
illness
?Mary had an illness.?
b. Marie
Mary
was
was
ziek.
ill
?Mary was ill.?
One could try to treat ?have an illness? as a kind of collocation and this
way have (4a) interpreted as ill(Mary). This, however leaves no space in the
representation for the determiner, which may vary in form and accordingly in
interpretation.
For the pair boos/boosheid, it is more difficult to come up with two equiva-
lent sentences, for lack of a suitable ?support verb?. Still we can observe that
(5a) entails (5b).
(5) a. Jan
Jan
probeerde
tried
zijn
his
boosheid
anger
te
to
verbergen.
hide
?Jan tried to hide his anger.?
b. Jan
Jan
was
was
boos.
angry
?Jan was angry?
For Katz, however, stative nominalizations denote either a fact or an ex-
tent/degree, but never a state. So (5a) could mean that Jan tried to hide (the
fact) that he was angry, or how angry he was, but not the state of his being
angry. At least the factive reading seems very intuitive here. It is not clear
whether there is also a stative reading. In some other contexts, though, a
factive reading is not possible. In (6a) boosheid is combined with a durational
predicate. (A fact does not have a duration; once a fact, always a fact.) An
extent or degree reading doesn?t seem to make a lot of sense either.
(6) a. Hun
their
boosheid
anger
duurt
lasts
nooit
never
lang.
long
?Their anger never lasts long.?
b. Ze
they
zijn
are
nooit
never
lang
long
boos.
angry
?They never are angry for a long time?
Besides, even if zijn boosheid in (5a) does only have a factive reading, how
should we represent the content of this fact in such a way that (5b) follows from
it and that we faithfully represent the quantifier? (His anger is deninite.) We
can?t choose a representation like angry(Jan), because of the quantifier. But
if we represent it as a noun (with a possessive kind of relation to Jan), while
still using a traditional representation for (5b), then we lose the entailment.
So even when embedded in a fact, reification of the predicate still yields better
representations.
These considerations lead us to the following type of representation for
sentences like (4b) and (5b).
(7) ?e.state(e) & concept of(e, ill/anger) & theme of(e, Marie/Jan) & at-
time(e, past)
Interestingly, for the adjective-noun pairs it is not always that clear and
systematic which is the basic form. For the verb - noun pairs above the verb
was always basic and the noun was its nominalization. There are also verbs
derived from nouns, but they follow a different pattern. Adjective - noun pairs
behave less systematically. In the pair verdrietig ?sad? - verdriet ?sadness?, the
adjective seems to be the derived form in Dutch, whereas in English the noun
has a nominalizing suffix. And for boos ?angry? - boosheid ?anger? it is the
other way around.
4 An alternative representation
We have seen that adjectives and their ?nominalizations? display the same
kind of inference patterns as verbs and their nominalizations, and that reifi-
cation of the predicate, through postulating an eventuality argument, makes
these patterns follow naturally. This reification seems to be the crucial point,
though. And since independent evidence for a Davidsonian analysis for sta-
tives is kind of shaky, we should investigate whether we really need the full
structure. Maienborn [10] proposes a representation for statives which does
involve reification of the predicate, but is different from the Davidsonian event
structure representation. In this section we discuss this alternative.
4.1 Kimian states
Maienborn argues for a distinction between Davidsonian states (D-states) and
Kimian states (K-states). Examples of verbs introducing D-states are stand,
sit and sleep. Examples of verbs introducing K-states are know, hate, resemble
and copular expressions. In the latter it is the copula that introduces the K-
state.
D-states introduce a normal Davidsonian argument, just like other eventu-
alities. For the K-states Maienborn shows that, like D-states, they are avail-
able to anaphoric reference and time modification, and therefore they need
a referential argument. This referential argument, she argues though, is of a
different ontological kind than Davidsonian eventuality arguments. It is of a
more abstract nature, similar to facts and propositions. The main argument
is their deviant combinatorial behavior. K-state verbs can not serve as the
infinitival complement of a verb of perception (see also examples (12b) and
(14a) later in this section), they cannot combine with most adverbials, such as
manner adverbs and instrumentals, and neither do they combine with locative
modifiers, all of this in contrast with D-states and other eventualities. This
brings her to the following (tentative) definition of K-states.
(8) Kimian states:
K-states are abstract objects for the exemplification of a property P
at a holder x at a time t.
Here are some of Maienborn?s (German) examples: (9a), with a D-state,
is represented as (9b), and (10a), with a K-state, is represented as (10b). The
representations are in a flat DRT notation.
(9) a. Carol
Carol
schla?ft.
sleeps
?Carol is sleeping.?
b. [se , v | sleep(s), theme(s, v), carol(v)]
(10) a. Carol
Carol
ist
is
mu?de.
tired
?Carol is tired.?
b. [sz , v | s ? [tired(v)], carol(v)]
The embedded box in (10b) contains the property that is the K-state, and
the discourse referent s reifies this property.
4.2 Some modifications
Engelberg [6] proposes a few modifications to this view on K-states. He argues
the K-state should not be introduced by the copula, but rather by the post-
copula predicate (e.g. an adjective), because attributively used adjectives also
show the relevant behavior, without being accompanied by a copula.
Also, he shows that it is problematic to put individuals introduced by an
NP under the copula in the box that is introduced by ??? and presents the
?content? of the state. Because in that case the state in (11a) (being related
to Opus) would be a different one then the state in (11b) (being related to
George). And while the states in (11b) and (11d) are the same, if Opus is
the tuba player of the Deathto?ngue, since the subject is in the outer box and
therefore extensionalized over, this is not the case for the states in (11a) and
(11c).
(11) a. George is related to Opus.
b. Opus is related to George.
c. George is related to the tuba player of the Deathto?ngue
d. The tuba player of the Deathto?ngue is related to George.
Identity relations between states get more coherent and intuitive if the
content of the box embedded under ??? is restricted to only the core predicate
(e.g. related(x, y)).
Now if Engelberg is right that K-states are not more fine grained than
events and D-states, and the content of the embedded K-state box is in all
cases only a core predicate, one can wonder what the advantage of the Kimian
style representation still is. For facts and propositions this kind of represen-
tation is useful, exactly because the content of a proposition is more than a
single predicate; it is a full-fledged proposition, and it makes sense to assign
a referential argument to the proposition as a whole. Individuals introduced
by NPs in embedded propositions are not extensionalized over. If George said
that he is related to Opus and if Opus is the tuba player of the Deathto?ngue,
it is not entailed that George said that he is related to the tuba player of the
Deathto?ngue. The main remaining difference between the D-state and K-state
representations seems to be that the K-state predicate directly predicates over
its argument(s), whereas in D-states this relation is mediated through theta
roles. It is not clear why this should be the case.
4.3 Entailments between K-state and D-state verbs
Representing K-states in a different format than D-states, also causes another
complication in the domain of inference. German liegen ?to lie? is a D-state
verb, hence the grammaticality of (12a). Sein ?to be? and also sich befinden
?to be located? are K-state verbs, as shown by the ungrammaticality of (12b).
(12) a. Ich
I
sah
saw
das
the
Buch
book
auf
on
dem
the
Tisch
table
liegen.
lie
?I saw the book lie on the table.?
b. *Ich
I
sah
saw
das
the
Buch
book
sich
refl
auf
on
dem
the
Tisch
table
befinden.
be-located
?I saw the book be located on the table?
But (13a) entails (13b). 1 (Not all German speakers seem to like the version
with the copula, but with befinden (13b) is certainly good.) If these two
predicates introduce two very different types of states that require different
styles of representation, this entailment is problematic.
(13) a. Das
the
Buch
book
liegt
lies
auf
on
dem
the
Tisch.
table
?The book is lying on the table.?
b. Das
the
Buch
book
befindet
located
sich/ist
refl/ is
auf
on
dem
the
Tisch.
table
?The book is (located) on the table?
It is of course conceivable that the verb liegen actually introduces two
substates, one of which is Kimian. Intuitively positional location verbs (with
their complements) such as liegen refer two different pieces of information.
One of these is the location of the subject (expressed by the complement)
and the other one is in what kind of position the subject is (upright or lying
flat...). The locational information will have to be the K-state that gets us
the entailment. That means that the positional information has to constitute
the D-state that saves the construction in (12a).
So far the problem seems fixable, be it at the cost of losing the clear-cut
distinction between D-state verbs and K-state verbs. (The positional location
verbs stand, sit and lie are actually quite a substantial group within the D-
state verb class). But it gets worse. The verb to sleep is a D-state verb and
to be asleep, being a copula construction, behaves like a K-state expression,
as is illustrated below.
(14) a. *Ik
I
zag
saw
Carol
Carol
diep
deep(ly)
in
in
slaap
sleep
zijn.
be
?I saw Carol be fast asleep.?
b. Ik
I
zag
saw
Carol
Carol
slapen.
sleep
?I saw Carol sleep.?
1 These examples can be reproduced in Dutch, but there the copula version of (13b) is
somewhat marginal.
But we can observe that (15a) entails (15b).
(15) a. Carol
Carol
was
was
diep
deep(ly)
in
in
slaap.
sleep
?Carol was fast asleep.?
b. Carol
Carol
sliep.
slept
?Carol was sleeping?
Here it is not plausible that (15a) contains a D-state as well as a K-state,
because the presence of this D-state should save (14a). 2
Although the distinction between two groups of statives with different
behavior is very convincing, we conclude that in a semantic representation for
inference purposes, it does not seem to be a good idea to treat to sleep and
to be asleep as fundamentally different kinds of entities. We therefore stick to
Davidsonian style representations for all states. The differences between the
two classes that Maienborn shows are of course real. But as they mainly seem
relevant for selectional restrictions, they can probably best be captured as
part of the feature structure of the predicates, in a computational system like
ours. In Delilah the decision of whether two constituents can combine to form
a new one depends on the unifiability of their graphs of features. Here one can
include a feature that says for example that a predicate is ?abstract?. Verbs of
perception, all kinds of adverbials and locative modifiers can then be specified
for combining only with concrete predicates. The semantic representation
then only needs to contain information that is relevant for inference.
5 Adjectives and adverbs
Adjectives and adverbs are closely related categories [2]. (The main group of
adverbs that also occur as adjectives are the manner adverbs.) If we assume
2 An anonymous reviewer proposed the representation (1a) for ?Carol was asleep?. Made
consistent with the view that a K-state is the exemplification of a property that would be
(1b). (Where the property is ?being the theme of a sleep event?)
(1) a. [s | s ? [s?, v | [sleep(s?), theme(s?, v), carol(v)]]
b. [sz , v | s ? [s?e | [sleep(s?), theme(s?, v)], carol(v)]
With a D-state embedded in a K-state, this looks like an interesting compromise. The main
problem with it, is that Maienborn introduces K-states next to D-states in order to derive
the different combinatory properties of K-states and D-states from their different ontological
status. Now if a K-states embeds a D-state, with the same ontological status as any other
D-state, one would expect the embedded D-state to also have the same properties as other
D-states, such as being able to have a location. This would make the positing of K-states
loose its main advantage.
underlying states for adjectives, we should do so for their adverbial counter-
parts as well. (This is one of the reasons Katz [9] does not want underlying
states for adjectives.) This is not necessarily problematic, because the Ger-
man dabei -construction which Maienborn uses as a diagnostic for whether a
predicate has a referential argument, also seems to work for adverbs. In (16)
the da in dabei refers to schnell. This means that schnell should introduce a
referential argument.
(16) Erstaunlich
amazing
ist,
is
wie
how
schnell
fast
und
and
dabei
thereat
zuverla?ssig
reliably
der
the
neue
new
Mozilla
Mozilla
Firebird
Firebird
Seiten
web sites
darstellt.
displays
?Amazing is, how quickly and reliably the new Mozilla Firebird dis-
plays web sites.?
This suggests that our representation for these kinds of adverbs can be similar
to the one that we have proposed for adjectives.
6 Conclusions and further research
We have shown that a nice side effect of (neo-)Davidsonian event representa-
tions, is that entailment relations between verbs and their nominalizations and
between adjectives and their corresponding nouns follow naturally, without
any extra machinery. We have defended the use of a Davidsonian represen-
tation for adjectives, by showing that assuming states of different ontological
sorts obscures certain inferential relations. Our point of view is that semantic
representations should only contain information that is needed for inference.
Information that is relevant for selectional restrictions should be accommo-
dated elsewhere, where it does not interfere with inference.
In our parser Delilah we have implemented event structures for verbs and
nominalizations of verbs. We will proceed with implementing the proposed
structures for adjectives along the same lines. We believe that in general
semantic parsers that aim at producing structures that support inference can
benefit from such an approach. Further research will have to show how much
we need to further refine our event structures, for example by systematically
including subevents.
Acknowledgements
This research was funded by Netherlands Organisation for Scientific Research
(NWO). Our participation in the workshop was funded by LUF (Leids Uni-
versiteits Fonds) and LUCL.
We also thank the reviewers for their comments.
References
[1] Bos, J., S. Clark, M. Steedman, J. R. Curran and J. Hockenmaier, Wide-
coverage semantic representations from a ccg parser, Proceedings of COLING-
04 (2004).
[2] Broekhuis, H., Adjectives and adjective phrases, Working Paper 2, University of
Tilburg (1999).
[3] Copestake, A., D. Flickinger, I. A. Sag and C. Pollard, Minimal recursion
semantics: An introduction (1999).
[4] Cremers, C., Formalizing the syntax (1999).
[5] Cremers, C., (?n) betekenis berekend, Nederlandse Taalkunde 7 (2002), pp. 375?
395.
[6] Engelberg, S., Kimian states and the grammar of predicative adjectives,
Theoretical Linguistics 31 (2005), pp. 331?347.
[7] Higginbotham, J., On events in linguistic semantics, in: J. Higginbotham,
F. Pianesi and A. Varzi, editors, Speaking of Events, Oxford University Press.,
Oxford, New York, 2000 pp. 49?79.
[8] Jurafsky, D. and J. H. Martin, ?Speech and Language Processing: An
Introduction to Natural Language Processing,? Computational Linguistics and
Speech Recognition, Prentice-Hall, Upper Saddle River, NJ, 2000.
[9] Katz, G., Anti neo-davidsonianism: Against a davidsonian semantics for state
sentences, in: C. Tenny and J. Pustejovsky, editors, Events as Grammatical
Objects, CSLI Publications, Stanford, CA, 2000 pp. 393?416.
[10] Maienborn, C., On the limits of the davidsonian approach: The case of copula
sentences, Theoretical Linguistics 31 (2005), pp. 275?316.
[11] Parsons, T., ?Events in the semantics of English: a study in subatomic
semantics,? MIT press, Massachusetts, 1990.
[12] Parsons, T., Underlying states and time travel., in: J. Higginbotham, F. Pianesi
and A. Varzi, editors, Speaking of Events, Oxford University Press, Oxford, New
York, 2000 pp. 81?93.
[13] Toussaint, P. and L. Wolf, Design of the narrator system: processing, storing
and retrieving medical narrative data, Proceedings of ISoLA-2004 (2004).
[14] Wolf, L., E. Hoenkamp, R. Overberg, H. Reckman and P. Toussaint, Design of
the narrator system: processing, storing and retrieving medical narrative data,
Society for Design and Process Science (Submitted).
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 513?519, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
teragram:
Rule-based detection of sentiment phrases using SAS Sentiment Analysis
Hilke Reckman, Cheyanne Baird, Jean Crawford, Richard Crowell,
Linnea Micciulla, Saratendu Sethi, and Fruzsina Veress
SAS Institute
10 Fawcett Street
Cambridge, MA 02138, USA
hilke.reckman@sas.com
Abstract
For SemEval-2013 Task 2, A and B (Sen-
timent Analysis in Twitter), we use a rule-
based pattern matching system that is based on
an existing ?Domain Independent? sentiment
taxonomy for English, essentially a highly
phrasal sentiment lexicon. We have made
some modifications to our set of rules, based
on what we found in the annotated training
data that was made available for the task. The
resulting system scores competitively, espe-
cially on task B.
1 Introduction
SAS taxonomies for sentiment analysis are primar-
ily topic-focused. They are designed to track sen-
timent around brands, entities, or other topics and
subtopics in a domain (Lange and Sethi, 2011;
Lakkaraju and Sethi, 2012; Albright and Lakkaraju,
2011). Domain-independent taxonomies have a
second function. In addition to performing topic-
focused tasks, they can be set up to perform senti-
ment analysis at the document level, classifying the
whole document as positive, negative, or neutral. In
this task all sentiment expressions are taken into ac-
count, rather than only those which are related to
the tracked topic. This second function is becom-
ing increasingly important. It allows for a broader
perspective that is complementary to topic-focused
opinion mining.
We participated in both subtask A and B of
SemEval-2013 Task 2: Sentiment Analysis in Twit-
ter (Wilson et al, 2013) with an adaptation of our
existing system. For task B, identifying the overall
sentiment of a tweet, our taxonomy mainly needed
some fine-tuning to specifically accommodate Twit-
ter data. (Normally tweets only make up a small
part of the data we work with.) We also made a
few adaptations to focus entirely on document level
sentiment, whereas originally the main focus of our
system was on tracking sentiment around products.
For task A, identifying the sentiment of ambiguous
phrases in a tweet, a few more modifications were
needed.
Our system is entirely rule-based, and the rules
are hand-written. In some cases, statistical text min-
ing approaches are used for the discovery of topics
and terms to facilitate rule writing. Our sentiment
analysis software does offer a statistical component,
but our experience is that purely rule-based models
work better for our typical sentiment analysis tasks.
Advantages of rules are that problems observed
in the output can be targeted directly, and the model
can become more and more refined over time. Also,
they allow for simple customization. In our brand-
centered work, we customize our taxonomies for
one or more brands that we want to track. When
we build a taxonomy for a new domain, we build
upon work we have done before in other domains.
The assignment of sentiment to certain phrases can
be sensitive to context where it needs to be. The
canceled task C, identifying sentiment related to a
topic, could have been approached successfully with
a rule-based approach, as our rules are specifically
designed to connect sentiment to targeted topics.
Section 2 describes the basic architecture of our
system, followed by a section on related work. Then
sections 4 and 5 describe the adaptations made for
513
each subtask and present the results. This is fol-
lowed by a more general discussion of our approach
in the light of these results in section 6, and the con-
clusion in section 7.
2 The base system
The datasets we normally use for the development
of our taxonomies include blogs, forums, news, and
Twitter. When developing a domain-specific taxon-
omy, we collect data for that particular domain, e.g.
Banking, Retail, Hospitality. We build the taxonomy
with the terms we encounter in those documents,
and test on a new set of documents. The Domain
Independent taxonomy started out as the common
base derived from several of these taxonomies, and
was then built out and tested using a wider range of
English-language documents. Since we used some
other tweets in the development of the original sys-
tem, our submission is considered unconstrained.
Our rules are patterns that match words or se-
quences of words, which makes our approach essen-
tially lexicon-based. Matching occurs left-to-right
and longer matches take precedence over shorter
ones. The top level rules in our sentiment taxonomy
are set up to recognize positive and negative word-
sequences. There is also a set of ?neutral? rules at
that level that block the assignment of positive or
negative sentiment in certain cases.
A positive or negative sequence can consist of a
single word from the positive or negative word-lists,
or a spelled out phrase from the positive or nega-
tive phrase-lists. Alternatively, it can be built up out
of multiple components, for example an emphatic
modifier and a sentiment term, or a negation and a
sentiment term. We call these sequences Positive
and Negative ?Contexts?, since they are contexts for
the topic-terms that we normally track.
Documents are preprocessed by an in-house POS-
tagger. Rules can require a word to have a particular
part of speech.
The words in the word-list, or in any of the other
rules, can be marked with an ?@?-sign to enable
morphological expansion, and in that case they will
match any of the forms in their paradigm. For ex-
ample ?love@? will match love, loves, loved, and
loving. This functionality is supported by a mor-
phological dictionary that links these forms to their
stem.
The rules are organized into lists that represent
useful concepts, which can be referred to in other
rules as a means of abstraction. For example the
rule:
def{Negation} def{PositiveAdjectives}
matches phrases that are composed of a negation (as
defined in the list named Negation) and a positive
adjective (as defined in the list named PositiveAd-
jectives). Negation includes rules like ?hasn?t been?,
?doesnt?[sic], ?not exactly the most?, etc., and Posi-
tiveAdjectives contains a rule that matches words in
PositiveWords if they are also tagged as adjectives.
For efficiency reasons the dependencies cannot be
circular, hence not allowing for recursion.
Distance rules can be used to capture a longer
span, matching a specified pattern at the beginning
and at the end, including arbitrary intervening words
up to a specified number. They can also be used to
make matching a term dependent on specified terms
in the context. For example,
(SENT, (DIST 4, ? a{ def{HigherIsBetter}}?,
? a{ def{Lowering}}?))
will capture phrases that say a company?s profit
(HigherIsBetter) went down (Lowering). The
SENT-operator prevents matching across sentence
boundaries.
(ORDDIST 7, ? def{PositiveContext}?,
? a{ def{PositiveAmbig}}?)
will capture ambiguous positive expressions when
they follow an unambiguously positive sequence
within a distance of 7 words.
This ensemble of lists and rules has grown rela-
tively organically, and is motivated by the data we
encounter. We introduce new distinctions when we
feel it will make a difference in terms of results,
or sometimes for ease of development and mainte-
nance.
Usually each sentiment expression has the same
weight, and one positive and one negative expres-
sion cancel each other out. However at the top level
we can introduce weights, and we have done so in
this model. We have created lists of weak positive
and negative expressions, and we gave those very
514
Positive:
? (ORDDIST 2, ? a{exceed@}?, ? a{expectation@}?)
? :Pro could not be happier
? blown away by
? def{Negation} want@ it to end
? above and beyond
? break@ down barriers
? can?t go wrong with
? dying to def{Consume}
? save@ me def{Money}
? (ALIGNED, ? c{treat@}?, ?:N?)
Negative:
? def{Negation} find def{NounPhrases}
def{PositivePhrases}
? (SENT, (ORDDIST 7, ? a{disappointed that}?,
? a{ def{PositivePhrases}}?))
? I would have loved
? def{Negation} accept@
? breach of def{PositiveWords}
? def{Money} magically disappears
? lack of training
? make@ no sense
? subject@ me to
? fun dealing with
Figure 1: Examples of rules for positive and negative
phrases and patterns.
low weights, so that they would only matter if there
were no regular-strength expressions present. We
limited some of those weak sentiment rules to sub-
task A only, but they clearly helped with recall there.
Negations in the default case turn positives into
negatives and negatives into neutrals. In addition to
negations we also have sentiment reversers, which
turn negatives into positives. Simple negations nor-
mally scope over a right-adjacent word or phrase, for
example a noun phrase or a verb. A special class of
clausal negations (I don?t think that) by approxima-
tion take scope over a clause.
This system contains roughly 2500 positive words
and 2000 positive phrases, and roughly 7500 neg-
ative words and 3000 negative phrases. Some ex-
amples are given in Figure 1. The neutral list also
contains about 2000 rules. Other helper lists such as
Negation, EmphaticModifiers, and Money typically
contain about a hundred rules each.
A system like this takes about six to eight weeks
to build for a new language. This requires a deve-
loper who is already familiar with the methodology,
and assumes existing support for the language, in-
cluding a morphological dictionary and a part-of-
speech tagger.
3 Related work
In tasks that are not topic-related, purely rule-based
models are rare, although the winning system of
SemEval-2010 Task 18 (Wu and Jin, 2010), some-
what similar to task A, was rule-based (Yang and
Liu, 2010). Liu (2010) suggests that more rule-
based work may be called for. However, there are
many other systems with a substantial rule-based
component (Nasukawa and Yi, 2003; Choi and
Cardie, 2008; Prabowo and Thelwall, 2009; Wilson
et al, 2005). Systems commonly have some rules
in place that account for the effect of negation (Wie-
gand et al, 2010) and modifiers. Sentiment lexicons
are widely used, but mainly contain single words
(Baccianella et al, 2010; Taboada et al, 2011). For
topic-related tasks, rule-based systems are a bit more
common (Ding et al, 2008).
4 Task A
Task A was to assign sentiment to a target in context.
The target in isolation would often be ambiguous. It
was a novel challenge to adapt our model for this
subtask.
Since we normally track sentiment around spe-
cific topics, we can usually afford to ignore highly
ambiguous phrases. Typical examples of this are
ambiguous emoticons and comments like no joke at
the end a sentence, or directly following it. When
these are used and could be disambiguated, usually
there is a less ambiguous term available that occurs
closer to the topic-term that we are interested in. (In
some cases we do use the topic as disambiguating
context.)
Also, we generally place slightly more empha-
sis on precision than on recall, assuming that with
enough data the important trends will emerge, even
if we ignore some of the unclear cases and outliers.
This makes the output cleaner and more pleasant to
515
work with for follow-up analysis.
4.1 Model adaptations and processing
We adapted our model to task A by introducing lists
of ambiguous positive and negative terms that were
then disambiguated in context, e.g. if there was an-
other sentiment term of a specified polarity nearby.
We also added some larger patterns that included an
ambiguous term, but as a whole had a much clearer
polarity. Below are some examples of rules for the
word like, which is highly ambiguous in English.
1. (ALIGNED, ? c{like@}?, ?:V?) (pos)
2. likes (pos)
3. I like (pos)
4. like magic (pos)
5. give it a ?like? (pos)
6. kinda like it (weakpos)
7. doesn?t seem like (hypothetical)
8. How can you like (neg)
9. don?t like (neg)
10. like to pretend (neg)
11. treated like a number (neg)
12. Is it like (neutral)
13. a bit like (neutral)
14. the likes of (neutral)
A seemingly obvious rule for like is (1), restrict-
ing it to usage as a verb. However, disambiguating
like is a difficult task for the tagger too, and the re-
sult is not always correct. Therefore this rule is a
fall-back case, when none of the longer rules apply.
Inflected forms such as (2) are pretty safe, with a
few exceptions, which can be caught by neutralizing
rules, such as (14). The hypothetical case, (7), is not
used in task A, but it is in task B.
A potential issue for our results on this task is that
our system only returns the longest match. So in a
sentence such as ?I didn?t like it?, if you ask people
to annotate like, they may say it is positive, whereas
the longer phrase didn?t like is negative. In the out-
put of our system, like will only be part of a negative
sequence. The information that it was originally rec-
ognized as a positive word cannot be retrieved at the
output level.
We found that the annotators for task A were in
general much more liberal in assigning sentiment
than we normally are. We made major gains by re-
moving some of our neutralizing rules, for example
those that neutralize sentiment in hypothetical con-
texts, and by classifying negations that were not part
of a larger recognized phrase as weak negatives.
The annotations in the development data were
sometimes confusing (see also section 6). We had
some difficulty in figuring out when certain terms
such as hope or miss you should be considered
positive and when negative. The verb apologize
turned out to be annotated sometimes positive and
sometimes negative in near identical tweets.
The test items were processed as follows:
1. run the sentiment model on the text (tweet/SMS)
2. identify the target phrase as a character span
3. collect detected sentiment that overlaps with the tar-
get phrase
(a) if there is no overlapping sentiment expres-
sion, the sentiment is neutral
(b) if there is exactly one overlapping sentiment
expression, that expression determines the
sentiment
(c) if there is more than one sentiment expression
that overlaps with the target, compute which
sentiment has more weight (and in case of a
draw, assign neutral)
4.2 Results
We get a higher precision for positive and negative
sentiment on task A than any of the other teams,
but we generally under-predict sentiment. Precision
on neutral sentiment is very low. Detecting neutral
phrases did not seem to be a very important goal in
the final version of this task, though. The results of
our predictions on the Twitter portion of the data are
shown in Figure 2.
These results are slightly different from what we
submitted, as we did not realize at the time of sub-
mission that the encoding of the text was different
in the test data than it had been in the previously re-
leased data. The submitted results are included in
the summarizing Table 1 at the end of the discussion
section.
Some targets are easily missed. We do not have
a good coverage of hashtags yet, for example. We
incorporate frequent misspellings that are common
in Twitter and SMS. However, we have no general
strategy in place to systematically recognize uncon-
ventionally spelled words (Eisenstein, 2013). For
516
gs \ pred positive negative neutral
positive 1821 77 888 2734
negative 47 1091 403 1541
neutral 11 6 143 160
1879 990 1382 4435
class precision recall f-score
positive 0.9691 0.6661 0.7895
negative 0.9293 0.7080 0.8037
neutral 0.1035 0.8938 0.1855
average(pos and neg) 0.7966
Figure 2: Confusion table and scores on task A, tweets
a project that processes Twitter data it would also
make sense to periodically scan for new hashtags
and add them to the rules if they carry sentiment.
However, a sentiment lexicon is never quite com-
plete.
Therefore we experimented with a guessing com-
ponent. If we do not detect any sentiment in the tar-
get sequence, we let our model make a guess, based
on the overall sentiment it assigns to the document,
assuming that an ambiguous target overall is more
likely to be positive in a positive context and neg-
ative in a negative context. (Note that this is differ-
ent from our disambiguation rules, which only apply
to explicitly listed items.) This gives us substantial
gains on this subtask (Figure 3). However, this may
not hold up in a similar task where there are more
neutral instances than there were here, as we see a
decrease in precision on positive and negative.
gs \ pred positive negative neutral
positive 2147 230 357 2734
negative 137 1249 155 1541
neutral 50 33 77 160
2334 1512 589 4435
class precision recall f-score
positive 0.9199 0.7853 0.8473
negative 0.8261 0.8105 0.8182
neutral 0.1307 0.4813 0.2056
average(pos and neg) 0.8327
Figure 3: Confusion table and scores on task A, tweets,
with guessing
5 Task B
Task B was to predict the overall sentiment of a
tweet. This was much closer to the task our tax-
onomy is designed for, and yet it turned out to be
different in subtle ways.
5.1 Model adaptations and processing
We quickly found that running the model as we had
adapted it for subtask A over-predicted sentiment
on subtask B. We therefore put most of our neu-
tralizing rules back in place for this subtask, and
restricted a subset of the weak sentiment terms to
subtask A only. We disabled the mechanism that
helped us catch ambiguous terms in subtask A (see
section 4.1).
For processing we used our standard method,
comparing the added weights of the positive and of
the negative sequences found. The highest score
wins. In case of a draw, the document is classified as
neutral. ?Unclassified? (no sentiment terms found)
also maps to neutral for this task. A confidence score
is computed, but not used here.
5.2 Results
Our system compares positively to those of the other
teams. Originally we were in 3rd place as a team
on the Twitter data. After correcting for the encod-
ing problem we rise to second (assuming the other
teams did not have the same problem). Among un-
constrained systems only, we are first on tweets and
second on SMS. The results, after the correction, are
shown in Figure 4. As for task A, the original results
are included in the final summarizing Table 1.
gs \ pred positive negative neutral
positive 1188 88 296 1572
negative 66 373 162 601
neutral 408 202 1030 1640
1662 663 1488 3813
class precision recall f-score
positive 0.7148 0.7557 0.7347
negative 0.5626 0.6206 0.5902
neutral 0.6922 0.6280 0.6586
average(pos and neg) 0.6624
Figure 4: Confusion table and scores on task B, tweets
517
6 Discussion
We modified an existing rule-based system for Sem-
Eval Task 2. While the development of this exist-
ing system was a considerable time investment, the
modifications for the two SemEval subtasks took
no more than about 2 person-weeks in total. The
models used in task A and B have a large common
base, and our rule-based approach measures up well
against other systems. This shows that if the work is
done once, it can be re-used, modified, and refined.
As mentioned in section 4.1, the annotations did
not always seem consistent. The guidelines did not
ask the annotators to keep in mind a particular task
or purpose for their annotations. However, the cor-
rect annotation of a tweet or fragment can vary de-
pending on the purpose of the annotation. Non-
arbitrary choices have to be made as to what counts
as sentiment: Do you try to identify cases of im-
plicit sentiment? Do you count cases of quoted or
reported ?3rd-party?-sentiment? . . . Ultimately it
depends on what you are interested in: Do you want
to: -track sentiment around certain topics? -know
how authors are feeling? -assess the general mood?
-track distressing versus optimistic messages in the
news? . . . While manual rule writing allows us to
choose a consistent strategy, it was not obvious what
the optimal strategy was in this SemEval task.
There were considerable differences in annotation
strategy between task A and task B, which shared the
same tweets. The threshold for detecting sentiment
appeared to be considerably lower in task A than in
task B. This suggests that different choices had been
made. These choices probably reflect how the anno-
tators perceived the tasks.
In our core business, we primarily track sentiment
around brands. One of the choices we made was
to also include good and bad news about the brand
(such as that the company?s stock went up or down)
where no explicit sentiment is expressed, because
the circulation of such messages reflects on the rep-
utation of the brand. (Liu (2010) points out that a
lot of sentiment is implicit.) In task B, we noticed
that ?newsy? tweets had a tendency to be annotated
as neutral. We did not have the time to thoroughly
adapt our model for that interpretation.
Both manually annotating training data for super-
vised machine learning and using training data for
manual rule writing require a lot of work. Both
can be crowd-sourced to a large extent if the pro-
cess is made simple enough, and the instructions
are clear enough. All methods that use lists of sen-
timent terms benefit from automatically extracting
such terms from a corpus (Qiu et al, 2009; Wiebe
and Riloff, 2005). As those methods become more
sophisticated, the work of rule writers becomes eas-
ier. Since the correct annotation depends on the task
at hand, and there are many different choices that
can be made, annotated data can be hard to reuse for
a slightly different task than the one for which it was
created. In rule-based models it is easier to leverage
earlier work and to slightly modify the model for a
new task. Both the rules and the model?s decision-
making process are human-interpretable.
Table 1 (next page) summarizes our results on the
various portions of the task, and under different con-
ditions. The results on SMS-data are consistently
lower than their counterparts on tweets, but they fol-
low the same pattern. We conclude that the model
generalizes to SMS, but not perfectly. This is not
surprising, since we have never looked at SMS-data
before, and the genre does appear to have some id-
iosyncrasies.
7 Conclusion
Our model is essentially a highly phrasal sentiment
lexicon. Ways of defining slightly more abstract pat-
terns keep the amount of work and the number of
rules manageable. The model is applied through pat-
tern matching on text, and returns a sentiment pre-
diction based on the number of positive and nega-
tive expressions found, based on the sum of their
weights. This is not mediated by any machine learn-
ing.
Slightly different versions of this system were em-
ployed in subtasks A and B. It turned out to be a
strong competitor in Task 2 of SemEval-2013, espe-
cially on subtask B, where it scored in the top three.
References
Russell Albright and Praveen Lakkaraju. 2011. Com-
bining knowledge and data mining to understand sen-
timent: A practical assessment of approaches. Techni-
cal report, SAS White Paper, January.
518
Task A Twitter Task A SMS Task B Twitter Task B SMS
F-score rank F-score rank F-score rank F-score rank
Submitted 0.7489 3of7 0.7283 4of7 0.6486 1of15 0.5910 2of15
13of23 11of19 3of34 5of29
After fixing encoding 0.7966 3of7 0.7454 3of7 0.6624 1of15 0.6014 1of15
11of23 8of19 2of34 4of29
With guessing 0.8327 (2of7) 0.7840 (2of7) NA NA
(8of23) (7of19)
Table 1: Summary of results. The first rank indication is relative to the other systems in the unconstrained category.
The second is relative to the total number of participating teams (by highest scoring system).
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2010. Sentiwordnet 3.0: An enhanced lexical
resource for sentiment analysis and opinion mining.
In Proceedings of the 7th conference on International
Language Resources and Evaluation (LREC10), Val-
letta, Malta, May.
Yejin Choi and Claire Cardie. 2008. Learning with com-
positional semantics as structural inference for subsen-
tential sentiment analysis. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, pages 793?801. Association for Compu-
tational Linguistics.
Xiaowen Ding, Bing Liu, and Philip S Yu. 2008. A
holistic lexicon-based approach to opinion mining. In
Proceedings of the international conference on Web
search and web data mining, pages 231?240. ACM.
Jacob Eisenstein. 2013. What to do about bad language
on the internet. In Proc. of NAACL.
Praveen Lakkaraju and Saratendu Sethi. 2012. Corre-
lating the analysis of opinionated texts using sas text
analytics with application of sabermetrics to cricket
statistics. In Proceedings of SAS Global Forum 2012,
number 136.
Kathy Lange and Saratendu Sethi. 2011. What are peo-
ple saying about your company, your products, or your
brand? In Proceedings of SAS Global Forum 2011,
number 158.
Bing Liu. 2010. Sentiment analysis and subjectivity.
Handbook of natural language processing, 2:568.
Tetsuya Nasukawa and Jeonghee Yi. 2003. Senti-
ment analysis: Capturing favorability using natural
language processing. In Proceedings of the 2nd in-
ternational conference on Knowledge capture, pages
70?77. ACM.
Rudy Prabowo and Mike Thelwall. 2009. Sentiment
analysis: A combined approach. Journal of Informet-
rics, 3(2):143?157.
Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen. 2009.
Expanding domain sentiment lexicon through double
propagation. In Proceedings of the 21st international
jont conference on Artifical intelligence, pages 1199?
1204.
Maite Taboada, Julian Brooke, Milan Tofiloski, Kimberly
Voll, and Manfred Stede. 2011. Lexicon-based meth-
ods for sentiment analysis. Computational linguistics,
37(2):267?307.
Janyce Wiebe and Ellen Riloff. 2005. Creating subjec-
tive and objective sentence classifiers from unanno-
tated texts. In Computational Linguistics and Intel-
ligent Text Processing, pages 486?497. Springer.
Michael Wiegand, Alexandra Balahur, Benjamin Roth,
Dietrich Klakow, and Andre?s Montoyo. 2010. A sur-
vey on the role of negation in sentiment analysis. In
Proceedings of the workshop on negation and specu-
lation in natural language processing, pages 60?68.
Association for Computational Linguistics.
Theresa Wilson, Paul Hoffmann, Swapna Somasun-
daran, Jason Kessler, Janyce Wiebe, Yejin Choi, Claire
Cardie, Ellen Riloff, and Siddharth Patwardhan. 2005.
Opinionfinder: A system for subjectivity analysis. In
Proceedings of HLT/EMNLP on Interactive Demon-
strations, pages 34?35. Association for Computational
Linguistics.
Theresa Wilson, Zornitsa Kozareva, Preslav Nakov, Alan
Ritter, Sara Rosenthal, and Veselin Stoyanov. 2013.
SemEval-2013 task 2: Sentiment analysis in twitter.
In Proceedings of the 7th International Workshop on
Semantic Evaluation. Association for Computational
Linguistics.
Yunfang Wu and Peng Jin. 2010. Semeval-2010 task
18: Disambiguating sentiment ambiguous adjectives.
In Proceedings of the 5th International Workshop on
Semantic Evaluation, pages 81?85. Association for
Computational Linguistics.
Shi-Cai Yang and Mei-Juan Liu. 2010. Ysc-dsaa: An
approach to disambiguate sentiment ambiguous adjec-
tives based on saaol. In Proceedings of the 5th In-
ternational Workshop on Semantic Evaluation, pages
440?443. Association for Computational Linguistics.
519
Extracting aspects of determiner meaning
from dialogue in a virtual world environment
Hilke Reckman, Jeff Orkin, and Deb Roy
MIT Media Lab
{reckman,jorkin,dkroy}@media.mit.edu
Abstract
We use data from a virtual world game for automated learning of words and grammatical con-
structions and their meanings. The language data are an integral part of the social interaction in the
game and consist of chat dialogue, which is only constrained by the cultural context, as set by the
nature of the provided virtual environment. Building on previous work, where we extracted a vocab-
ulary for concrete objects in the game by making use of the non-linguistic context, we now target
NP/DP grammar, in particular determiners. We assume that we have captured the meanings of a set
of determiners if we can predict which determiner will be used in a particular context. To this end we
train a classifier that predicts the choice of a determiner on the basis of features from the linguistic
and non-linguistic context.
1 Introduction
Determiners are among those words whose meanings are hardest to define in a dictionary. In NLP,
determiners are often considered ?stop words? that are not relevant for understanding the content of a
document and should be removed before any interesting processing is done. On the other hand, it has
been shown that children are sensitive to determiner choice already at a very early age, using these
function words in figuring out what content nouns are intended to refer to. Meanings of determiners have
been argued to include important pragmatic and discourse-related functions.
We have a corpus of dialogue that is grounded in a virtual environment. This means that in our data
there is a relation between what people are saying and what they are doing, providing cues as to what
they mean by the words and constructions they use. We have chosen to use a virtual world environment
to collect data in, rather than a real world environment, because relatively rich virtual worlds are by now
available that are able to provide an interesting level of grounding, whereas making sense of real world
scenes using computer vision is still very challenging. In addition, this choice allows us to conveniently
collect data online1.
Although there exists a rich body of computational linguistics research on learning from corpus data,
these corpora usually consist of text only. Only recently corpora that include non-linguistic context have
started to be collected and used for grounded learning of semantics (Chen et al, 2010; Frank et al,
2009; Fleischman and Roy, 2005; Gorniak and Roy, 2005). This kind of work offers new and insightful
perspectives on learning meanings of natural language words and constructions, based on the idea that
our own knowledge of natural language meanings is grounded in action and perception (Roy, 2005), and
that language is a complex adaptive system which evolves in a community through grounded interaction
(e.g. Steels, 2003). So far the language in virtually grounded datasets has often been restricted to either
descriptions or directives, so utterances can be paired fairly directly with the actions they describe. The
interaction in our data is much freer. That means that it is more representative for the data that human
learners get, and that our methods can be applied to a wider variety of data, possibly also to datasets
1Von Ahn and Dabbish (2004) were among the first to realize the potential of collecting human knowledge data online, in a
game setup, collecting a large image-labeling corpus.
245
that have not been collected specifically for this purpose. A related project is KomParse (Klu?wer et al,
2010). Piantadosi et al (2008) developed a Bayesian model that learns compositional semantic meanings
of different kinds of words, including quantifiers, but from completely artificial data.
Our research focuses on learning from data, rather than through interaction, though the latter may be
possible in a later stage of the project. An example of a virtual world project where language is learned
through interaction is ?Wubble World? (Hewlett et al, 2007). In the Give Challenge (Byron et al, 2009)
a virtual world setup is used to evaluate natural language generation systems.
In previous work we have extracted words and multi-word expressions that refer to a range of objects
that are prominent in our virtual environment (Reckman et al, 2010). Now we investigate if aspects
of determiner meaning can be learned from this dataset. The extracted knowledge of nouns makes
the learning of determiners possible. We study what factors contribute to the choice of the determiner
and how they relate to each other, by training a decision tree classifier using these factors as features.
The decision tree provides insight in which features are actually used, in which order, and to which
effect. The accuracy of the resulting classifier on a test set should give us an impression of how well
we understand the use of the different determiners. Although one may argue that this study is about use
rather than about meaning, we take it that meaning can only be learned through use, and it is meaning
that we are ultimately interested in. One of the overarching questions we are concerned with is what
knowledge about language and how it works is needed to extract knowledge about constructions and their
meanings from grounded data. Practically, a computational understanding of determiners will contribute
to determining the reference of referential expressions, particularly in situated dialogue, and to generating
felicitous referential expressions (cf. Belz et al, 2010).
We first introduce our dataset. Then we discuss the automated extraction of determiners. Subse-
quently, we motivate the features we use, present our classifier experiments, and discuss the results.
2 Data: The Restaurant Game
Orkin and Roy (2007) showed in The Restaurant Game project that current computer game technology
allows for simulating a restaurant at a high level-of-detail, and exploiting the game-play experiences
of thousands of players to capture a wider coverage of knowledge than what could be handcrafted by
a team of researchers. The restaurant theme was inspired by the idea of Schank and Abelson (1977),
who argued that the understanding of language requires the representation of common ground for ev-
eryday scenarios. The goal is automating characters with learned behavior and dialogue. The ongoing
Restaurant Game project has provided a rich dataset for linguistic and AI research. In an online two-
player game humans are anonymously paired to play the roles of customers and waitresses in a virtual
restaurant (http://theRestaurantGame.net). Players can chat with open-ended typed text, move around
the 3D environment, and manipulate 47 types of interactive objects through a point-and-click interface
(see figure 1). Every object provides the same interaction options: pick up, put down, give, inspect, sit
on, eat, and touch, but objects respond to these actions in different ways. The chef and bartender are
hard-coded to produce food items based on keywords in chat text. A game takes about 10-15 minutes to
play. Everything players say and do is logged in time-coded text files on our servers. Although player
interactions vary greatly, we have demonstrated that enough people do engage in common behavior that
it is possible for an automatic system to learn statistical models of typical behavior and language that
correlate highly with human judgment of typicality (Orkin and Roy, 2007).
Over 10.000 games have been collected. The dialogue is grounded in two (partially overlapping)
ways. Not only is there a simulated physical environment with objects that can be manipulated in various
ways, but also social patterns of recurring events provide an anchor for making sense of the dialogue.
Previous research results include a first implementation of a planner that drives AI characters playing the
game (Orkin and Roy, 2009).
The intuition is that a human student of English starting from scratch (but with some common sense
knowledge about restaurants), could learn quite a bit of English from studying the Restaurant Game
episodes; possibly enough to play the game. We try to computationally simulate such a learning process.
246
Figure 1: Screen-shots from The Restaurant Game, from left to right: third-person perspective, waitress?s
perspective with dialogue, menu for interacting with objects.
3 Extracting nouns
Previously, we extracted a vocabulary of referring expressions for a set of concrete objects, based on
which words and phrases have the highest relative frequency in the contexts in which the objects are
used (see figure 2). We extracted words and phrases that can refer to the food and drink items on the
restaurant?s menu, the menu, and the bill, and some other items. These expressions represent the core
nominal phrases in the game. We will use these expressions as a starting point to extract determiners
and nominal modifiers. We restrict ourselves to the ordered food and drink items, the menu and the bill,
expecting that these show a somewhat uniform and interesting behavior, as they are the objects that can
appear and disappear during the course of a game.
food type referring expressions
SOUP
?soup?
?vegetable soup?
?soup du jour?
?soup de jour?
SALAD ?salad?
?cobb salad?
SPAGHETTI ?spaghetti??spaghetti marinara?
FILET ?steak? ?filet??filet mignon?
SALMON ?salmon??grilled salmon?
LOBSTER ?lobster?
?lobster thermador?
CHEESECAKE
?cheesecake?
?cheese? ?cake?
?cherry cheesecake?
?cheese cake?
PIE ?pie??berry pie?
TART ?tart?
?nectarine tart?
drink type referring expressions
WATER ?water?
TEA ?tea?
COFFEE ?coffee?
BEER ?beer?
REDWINE ?red? ?wine?
?red wine?
WHITEWINE ?white?
?white wine?
item type referring expressions
MENU ?menu?
BILL ?bill?
?check?
Figure 2: Extracted referring expressions for relevant items.
The referring expressions for these object types have been extracted in an unsupervised manner
making use of the relative frequency of words and phrases in the context of the objects being used.
Words, bigrams and trigrams were validated against each other with the use of one threshold. For more
detail see (Reckman et al, 2010).
4 Extracting determiners
Extracting determiners totally unsupervised is a non-trivial task. Attempts to use the existing fully un-
supervised grammar induction algorithm ADIOS (Solan et al, 2005) did not give us the results we were
hoping for. Instead, we decided to make use of the knowledge of nouns that we already have and target
247
determiners directly, rather than having to induce a full grammar. In future work we will look into using
alternative grammar induction systems, for a wider range of learning tasks.
We first narrowed down our search space by collecting words that are positively associated with
the position directly to the left of the nominal expression above a high recall, low precision threshold
(phi=0.01)2. This should favor determiners and other nominal modifiers over, for example, verbs.
We expect determiners to appear with a wider range of different nouns than adjectival modifiers do.
Especially in this restricted domain, adjectives are more likely to be restricted to specific object types.
We consider pre-nominal terms that are general enough to appear with more than 5 different objects (out
of 17) to be determiner candidates. We also check that our candidates can be preceded by an utterance
boundary.
The word the is most strongly associated with the relevant position, combines with most different
nouns, and can occur as only element between a boundary and a noun. We therefore assume that at
least the is a determiner. We order the other candidates according to their similarity to the, measured
as the cosine distance in a vector-space, with their two words to the left and to the right as dimensions.
We accept words as determiners in order of similarity to the, starting with the most similar word, after
checking that they are in complementary distribution with all of the already accepted words, i.e. that the
word does not occur adjacent to any of those. This gives us the following determiners: the, my, your,
some, a, another, our, one, ur, two, 2.3
We can then identify adjectival modifiers by looking at what occurs between determiners and nouns.
By checking what else these modifiers can be preceded by (that is also in complementary distribution
with known determiners), we can do another round of determiner search, and that lets us add any to
our list. As nouns can also be immediately preceded by an utterance boundary, we establish that the
determiner position is not obligatorily filled.
Of course this is not a complete set of determiners, but they appear to be the most prominent ones in
the game. Real quantifiers are relatively rare and that is to be expected, given the setting. Perhaps more
surprisingly, this and that are not associated with the left-of-noun position. It turns out that they are not
used very frequently as determiners in the game, and much more as pronouns. In future work we will
extract pronouns, by looking for single words that have a distribution that is similar to the distribution of
full noun phrases with a determiner.
In the process of extracting determiners, we also extract adjectives and modifiers such as glass of.
With little extra effort we can build a vocabulary of these as well, including information as to which
nouns they are associated with. Their meanings, however, are in most cases not sufficiently grounded in
the game to be understood. We may in a more advanced stage of the project be able to figure out that the
adjective free makes the item less likely to appear on the bill, but the meaning of hot will always remain
unclear, as temperature is not modeled in the game. Finding words associated with the position to the left
of specific nouns can also help us further improve our vocabulary of referring expressions, for example
by identifying veg and veggie as alternatives for vegetable in vegetable soup4.
We took a shortcut by directly targeting the position left of the noun. This involves language-specific
knowledge about English. To make this method applicable to different languages and only use very
general knowledge at the start, we would first have to find out what the position of the determiner is.
This may be to the right of the noun or affixed to it. Not all languages have articles, but we can expect
determiners like my, your, another etc. to occur either adjacent to5, or morphologically expressed on the
noun6. In previous work we have shown how a construction for coordination can be extracted (Reckman
2The phi-score is a chi-square based association metric. Manning and Schu?tze (2000) argue that such metrics are suitable
to quantify collocational effects. We also used it in extracting the referring expressions.
3For the experiments we replace ur by your, and 2 by two. We assume this could in principle be done automatically, although
especially in the latter case this is not trivial.
4We do already have a list of spelling variants for all the terms, but veg and veggie were too different from the canonical
form to get through the edit-distance filter
5Obviously we do not catch floating quantifiers this way. We might catch their non-floating counterparts and then discover
that they occur in other positions as well.
6Several unsupervised morphological analyzers have been developed, which should in principle be run in an early stage of
learning. For English however, the only interesting morphology at play here is plural formation.
248
et al, 2010). Coordination, to our knowledge, occurs in all languages and this is probably a feature of
general human cognition, so it makes sense to assume it exists in a language and look for it in the data. It
can then be used as a probe on structure. Categories that are grammatically intimately connected to nouns
are more likely to be repeated in a coordination involving two nouns. If we look at our English data, for
example, we see that a lot more material tends to occur between and and the second noun-conjunct, than
between the first noun-conjunct and and, which suggests that things that are grammatically close to the
noun occur to the left of it.
5 Features
In this section we motivate the features we will use. To capture the full meaning of determiners, we
would probably have to model the mental states of the players. However, what we aim at here is a
preliminary understanding of determiners as a step towards the understanding of full sentences, and the
resolution of NP reference and co-reference, which would be prerequisites for any serious modeling of
mental states. So we are interested in what can be learned from directly observable features. The features
are theoretically motivated, and reflect the nature of the referent, whether the referent has been mentioned
before, whether the referent is present, and who the speaker and addressee are.
The first feature is object type. There are 17 different objects that we take into account: BEER,
BILL, CHEESECAKE, COFFEE, FILET, LOBSTER, MENU, PIE, REDWINE, SALAD, SALMON, SOUP,
SPAGHETTI, TART, TEA, WATER, and WHITEWINE. We expect this feature to matter, because in a
restaurant situation one usually orders ?the spaghetti?, but ?a beer?. This may be to some extent dependent
on what is on the menu, but not completely. Regardless of what is on the menu, ordering ?the Heineken?
seems to be more unusual than ordering ?the Merlot?. This may mean that our data is not entirely
representative of the general case, because of our restaurant setting. However, it cannot be excluded that
similar effects play a role in other settings, too. There is of course the effect of mass versus count nouns,
too, but this may be a bit masked, because of unit expressions like glass of. We chose to not include
these unit expressions as a feature, because the decision to use such modifiers can be considered part of
the decision on which determiner to use. So using the modifier as a feature, would be giving away part
of the solution to the determiner-choice problem.
The second feature captures the notion of discourse-old versus discourse-new. We distinguish be-
tween cases where an object of a particular type is mentioned for the first time, and where it has already
been mentioned before. In the latter case, we take it that the discourse referent has already been intro-
duced. The expected effect is that first mentions tend to be indefinite.7 This is only an approximation,
because sometimes a second object of the same type is introduced and we do not resolve the reference
of our instances.
The third and fourth features incorporate present versus future presence of the object, plus the posi-
tion of the utterance with respect to the central action involving the object. We keep track of the previous
and following action in which the object is involved. Actions of interest are restricted to the appearance
of the object and and its central action: ?eating? for food and drink items, ?looking at? for the menu, and
?paying? for the bill. Being involved in such an action also implies presence. Other intervening actions
are ignored. The features are ?preceding action? and ?following action?, and the values are ?appearance?,
?main action?, and ?none?. We expect indefinites before appearance, when the object is not yet present.
Note that these features rely entirely on non-linguistic context.
The fifth and sixth features identify speaker and addressee. The speaker can be the customer or the
waitress. For the addressee the relevant distinction is whether the staff (chef and bartender) are addressed
or not. We expect a tendency of the waitress using your when talking to the customer, and of the customer
using my more often. We expect more indefinites or absence of a determiner when the staff is spoken to.
These features are central to dialogue, and may reveal differences between the roles.
7This is a typical feature for languages that have articles, and may be expressed through other means in other languages.
249
6 Experiments
We use the decision tree classifier from the Natural Language ToolKit for Python (Loper and Bird, 2002)
and train and test it through 10-fold cross-validation on 74304 noun phrases from 5000 games, 23776 of
which actually have determiners. The noun phrases used all contain nouns that can refer to the selected
objects, though we cannot guarantee that they were intended to do so in all cases. In fact, we have seen
examples where this is clearly not the case, and for example filet, which normally refers to the FILET
object, is used in the context of salmon. This means that there is a level of noise in our data.
The instances where the determiner is absent are very dominant, and this part of the data is necessarily
noisy, because of rare determiners that we?ve missed8, and possibly rather heterogeneous, as there are
many reasons why people may choose to not type a determiner in chat. Therefore we focus on the
experiments where we have excluded these cases, as the results are more interesting. We will refer to
the data that excludes instances with no determiner as the restricted dataset. When instances with no
determiner are included, we will talk about the full dataset.
6.1 Baselines
In the experiments we compare the results of using the features to two different baselines. The simplest
baseline is to always choose the most frequent determiner. For the instances that have overt determiners,
the most frequent one is the. Always choosing the gives us a mean accuracy of 0.364. If we include
the instances with no overt determiners, that gives us a much higher baseline of 0.680, when the no
determiner option is always chosen. We call this the simple baseline.
The second baseline is the result of using only the object feature, and forms the basis of our experi-
ments. We call this the object-only baseline. On the restricted dataset the resulting classifier assigns the
determiner a to the objects BEER, COFFEE, PIE, REDWINE, SALAD, TEA, WATER, and WHITEWINE,
and the determiner the to BILL, CHEESECAKE, FILET, LOBSTER, MENU, SALMON, SOUP, SPAGHETTI,
and TART. This yields a mean accuracy of 0.520, which is a considerable improvement over the sim-
ple baseline that is relevant for this part of the data. If we look at the confusion matrix in figure 3 that
summarizes the results of all 10 object-only runs we see that the objects? preferences for definite versus
indefinite determiners are also visible in the way instances with determiners other than the and a are
misclassified. Instances with definite determiners are more often classified as the, and indefinites as a.
a another any my one our some the two your
a <4984> . . . . . . 2912 . .
another 608 <.> . . . . . 76 . .
any 56 . <.> . . . . 24 . .
my 238 . . <.> . . . 742 . .
one 354 . . . <.> . . 241 . .
our 28 . . . . <.> . 178 . .
some 1109 . . . . . <.> 438 . .
the 1270 . . . . . . <7383> . .
two 191 . . . . . . 58 <.> .
your 805 . . . . . . 2075 . <.>
Figure 3: Confusion matrix for the object-only baseline.
On the full dataset, the classifier assigns the to instances of BILL and MENU and no determiner
to everything else, reflecting the count/mass distinction, and resulting in a mean accuracy of 0.707.
This is also a statistically significant improvement over its baseline, but much less spectacular. The
definite/indefinite distinction that we saw with the restricted dataset, does not really emerge here.
8It is also hard to reliably recognize misspelled determiners as determiners tend to be very short words.
250
6.2 Adding the other features
In the core experiments of this paper we always use the object feature as a basis and measure the effect of
adding the other features, separately and in combination. All differences reported are significant, unless
stated otherwise. The table in figure 5 at the end of the section summarizes the results.
If we add the feature of whether the item has been mentioned before or not, we get more indefi-
nites, as was to be expected. On the restricted dataset, the MENU, PIE, and TART objects get a if not
mentioned previously, and the otherwise. The mean accuracy is 0.527, which is a statistically significant
improvement over the object-only baseline (the improvement is consistent over all 10 runs), but it seems
rather small, nevertheless. (Using the discourse feature without the object feature gives a score of 0.377.)
Adding information as to whether the customer has seen the menu does not make any difference. On the
full dataset the discourse feature matters only for MENU, which gets a if not previously mentioned. The
mean accuracy is 0.709.
If, instead, we add the action features we get a somewhat more substantial improvement for the
restricted dataset; a mean accuracy of 0.561. We also get a wider range of determiners: your tends to be
chosen after appearing and before eating, another after eating, and a between no action and appearing.
The order in which the following and preceding action features are applied by the classifier differs per
object. (The action features without the object feature give a mean accuracy score of 0.427.) For the full
dataset the mean accuracy is 0.714, again a consistent, but marginal improvement. However, a, the and
your are the only determiners used, in addition to the no determiner option.
Adding the speaker and addressee features to the object feature base gives the classifier a better grip
on your. More indefinites are used when the staff is addressed, your when the customer is spoken to.
However, my is still not picked up. The speaker and addressee features are used in both orders. The mean
accuracy is 0.540, which is better than with the discourse feature, but worse than with the action features.
(The speaker and addressee features without the object feature give a mean accuracy score of 0.424.) In
the case of the full dataset, the new features are barely used, and there is no consistent improvement over
the different runs. The mean accuracy is 0.711.
If we combine the action features and speaker/addressee features on top of the object feature basis,
we see a substantial improvement again for the restricted dataset. The mean accuracy is 0.592. Finally,
we get some cases of my being correctly classified, and also your is correctly classified significantly
more often than in the previous experiments. The object feature always comes first in the decision tree.
For the other features, all relative orders are attested. Adding the ?previously-mentioned? feature to
this combination (see also figure 4) improves this result a little bit more, to a mean accuracy of 0.594,
although we can expect the information contained in it to have a large overlap with the information in
other features, for example, items mentioned for the first time will typically not have appeared yet.
a another any my one our some the two your
a <5732> 163 1 11 20 . 70 1773 1 125
another 175 <350> . 2 . . 48 70 . 39
any 44 4 <.> . . . 2 29 . 1
my 154 19 . <9> . . 20 765 . 13
one 437 20 . 2 <16> . 4 70 . 46
our 29 1 . . . <.> 1 161 . 14
some 881 48 . 6 3 . <114> 421 . 74
the 1332 74 . 33 8 . 34 <6131> . 1040
two 191 10 . 2 . . 1 45 <.> .
your 218 88 . . . . 20 781 . <1773>
(row = reference; col = test)
Figure 4: Confusion matrix for the object, action, speaker/addressee and discourse features combined.
251
6.3 Linguistic context and dialogue acts
It will be part of future research to distinguish the different dialogue acts that the nominal phrases that
we studied can be part of. Identifying the ?task? that an expression is part of may have a similar effect.
Tasks of the type ?customer gets seated?, ?waitress serves food?, ?customer eats meal?, etc. are annotated
for supervised learning, and may consist of several actions and utterances (Orkin et al, 2010).
To give an indication that the dialogue act that an expression is part of may be informative as to the
correct choice of the determiner, we have done an extra experiment, where we have used the word before
and the word after the DP as features. This gives a tremendous amount of feature values, which are not
very insightful, due to the lack of generalization, and are a near guarantee for over-fitting. However,
it does yield an improvement over using the object-only baseline. Moreover, the preceding word and
following word features are now applied before the object feature. The mean accuracy in this experiment
was 0.562, which is comparable to the experiment with object and action features. At the same time we
get a wider range of determiners than we have had before, including some correctly classified instances
of our. On the full dataset we even get a higher accuracy score than in any of the other experiments:
0.769, also with a much wider range of determiners. We suspect that this local linguistic context gives
quite good cues as to whether the expression is part of a proper sentence or not, and that in the former
case an overt determiner is much more likely9. The results of all experiments are summarized in figure 5.
restricted full
simple baseline 0.364 0.680
object-only baseline 0.520 0.707
object + discourse 0.527 0.709
object + action 0.561 0.714
object + speaker 0.540 0.711
object + action + speaker 0.592 0.721
object + action + speaker + discourse 0.594 0.721
object + surrounding words 0.562 0.769
Figure 5: Summary of the testing results.
7 Discussion
Maybe the most surprising outcome is that the object type turns out to be the main factor in choosing
the determiner in this virtual restaurant setting. It would beinteresting to see this reproduced on the
data of two new games that are currently being developed, with novel scenarios, locations and objects.
At the same time, it is a strength of our approach, that we can simulate a specific setting and capture
its ideosyncrasies, learning domain-specific aspects of language, and hopefully eventually learn what
generalizes across different scenarios.
For the restricted dataset we see that, consistently, indefinites are mostly misclassified as a, and
definites mostly as the. If we evaluate only for definiteness, we get a mean accuracy of 0.800 for the case
with all features combined. We could distinguish these two classes of determiners on the basis of the
similarity of each determiner to the two dominant types. It is, however, the object feature that seems to
be mainly responsible for the gain in definiteness accuracy with respect to the simple baseline.
It is unsurprising that we haven?t learned much about one and two, except that they pattern with
indefinites, as we haven?t included features that have to do with the number of objects. There actually
are more numerals that appear in the game, but did not make it into our list of determiners, because
they did not occur with enough different objects. In the general case, we are doubtful that numerals are
sufficiently grounded in this game for their exact meanings to be learned. It may however be possible to
learn a one-two-many kind of distinction. This would also involve looking into plural morphology, and
remains for future research.
9We have observed that in several games people tend to just sum up food items, without embedding them in a sentence.
252
We also haven?t learned anything about our, except that it patterns with definites. It is not quite clear
what kind of features would be relevant to our in this setting.
For the possessive pronouns your and my we have learned that one tends to be linked to the waitress
as a speaker (and the customer as addressee) and the other to the customer. It will be challenging to reach
an understanding that goes deeper than this10. The range of interactions in the game may be too limited
to learn the meanings of all determiners in their full generality.
While we have treated a and another as different determiners, we have included cases of some more
under some. It may be worthwhile to include some more (and perhaps any more and one more as well) as
a separate determiner. However, our best classifier so far still cannot distinguish between a and another
very well.
The experiments with linguistic context suggest that dialogue act may make for an additional, pow-
erful, albeit indirect, feature. The fact that it helps to know when the main action involving the object
took place, rather than just its appearance, may also be taken to point in the same direction, as people
tend to say different kinds of things about an object before and after the main action.
Using a classifier seems to be a reasonable way of testing how well we understand determiners, as
long as our features provide insight. Although there is still a lot of room for improvement, there is likely
to be a ceiling effect at some point, because sometimes more than one option is felicitous. We also have
to keep in mind that chat is likely to be more variable than normal written or spoken language.
8 Conclusion
We have carried out an exploratory series of experiments, to see if meanings of determiners, a very
abstract linguistic category, could be learned from virtually grounded dialogue data. We have trained a
classifier on a set of theoretically motivated features, and used the testing phase to evaluate how well
these features predict the choice of the determiner.
Altogether, the results are encouraging. If we exclude instances with no determiner we reach an
accuracy of 0.594 over a baseline of 0.364. The features that identify the dialogue participants and
surrounding actions, including appearance, play an important role in this result, even though the object
type remains the main factor. A clear dichotomy between definite and indefinite determiners emerges.
The results for the complete dataset are a bit messier, and need more work.
In future work we will identify utterance types, or dialogue acts, that also rely on surrounding actions
and on the speaker and addressee. We will also look into resolving reference and co-reference.
Acknowledgments
This research was funded by a Rubicon grant from the Netherlands Organisation for Scientific Research
(NWO), project nr. 446-09-011.
References
Belz, A., E. Kow, J. Viethen, and A. Gatt (2010). Generating referring expressions in context: The
GREC task evaluation challenges. In Empirical Methods in Natural Language Generation, pp. 294?
327. Springer.
Byron, D., A. Koller, K. Striegnitz, J. Cassell, R. Dale, J. Moore, and J. Oberlander (2009). Report on
the First NLG Challenge on Generating Instructions in Virtual Environments (GIVE). In Proceedings
of the 12th European Workshop on Natural Language Generation, pp. 165?173. ACL.
Chen, D., J. Kim, and R. Mooney (2010). Training aMultilingual Sportscaster: Using Perceptual Context
to Learn Language. Journal of Artificial Intelligence Research 37, 397?435.
10For their personal pronoun counterparts you and I we might stand a better chance.
253
Fleischman, M. and D. Roy (2005). Why verbs are harder to learn than nouns: Initial insights from a
computational model of intention recognition in situated word learning. In 27th Annual Meeting of
the Cognitive Science Society, Stresa, Italy.
Frank, M., N. Goodman, and J. Tenenbaum (2009). Using speakers? referential intentions to model early
cross-situational word learning. Psychological Science 20(5), 578.
Gorniak, P. and D. Roy (2005). Probabilistic grounding of situated speech using plan recognition and
reference resolution. In Proceedings of the 7th international conference on Multimodal interfaces, pp.
143. ACM.
Hewlett, D., S. Hoversten, W. Kerr, P. Cohen, and Y. Chang (2007). Wubble world. In Proceedings of
the 3rd Conference on Artificial Intelligence and Interactive Entertainment.
Klu?wer, T., P. Adolphs, F. Xu, H. Uszkoreit, and X. Cheng (2010). Talking NPCs in a virtual game
world. In Proceedings of the ACL 2010 System Demonstrations, pp. 36?41. ACL.
Loper, E. and S. Bird (2002). NLTK: The natural language toolkit. In Proceedings of the ACL-02
Workshop on Effective tools and methodologies for teaching natural language processing and compu-
tational linguistics-Volume 1, pp. 70. ACL.
Manning, C. and H. Schu?tze (2000). Foundations of statistical natural language processing. MIT Press.
Orkin, J. and D. Roy (2007). The restaurant game: Learning social behavior and language from thousands
of players online. Journal of Game Development 3(1), 39?60.
Orkin, J. and D. Roy (2009). Automatic learning and generation of social behavior from collective
human gameplay. In Proceedings of The 8th International Conference on Autonomous Agents and
Multiagent Systems-Volume 1, pp. 385?392. International Foundation for Autonomous Agents and
Multiagent Systems.
Orkin, J., T. Smith, H. Reckman, and D. Roy (2010). Semi-Automatic Task Recognition for Interactive
Narratives with EAT & RUN. In Proceedings of the 3rd Intelligent Narrative Technologies Workshop
at the 5th International Conference on Foundations of Digital Games (FDG).
Piantadosi, S., N. Goodman, B. Ellis, and J. Tenenbaum (2008). A Bayesian model of the acquisition of
compositional semantics. In Proceedings of the Thirtieth Annual Conference of the Cognitive Science
Society. Citeseer.
Reckman, H., J. Orkin, and D. Roy (2010). Learning meanings of words and constructions, grounded in
a virtual game. In Proceedings of the 10th Conference on Natural Language Processing (KONVENS).
Roy, D. (2005). Semiotic schemas: A framework for grounding language in action and perception.
Artificial Intelligence 167(1-2), 170?205.
Schank, R. and R. Abelson (1977). Scripts, plans, goals and understanding: An inquiry into human
knowledge structures. Lawrence Erlbaum Associates Hillsdale, NJ.
Solan, Z., D. Horn, E. Ruppin, and S. Edelman (2005). Unsupervised learning of natural languages.
Proceedings of the National Academy of Sciences of the United States of America 102(33), 11629.
Steels, L. (2003). Evolving grounded communication for robots. Trends in cognitive sciences 7(7),
308?312.
Von Ahn, L. and L. Dabbish (2004). Labeling images with a computer game. In Proceedings of the
SIGCHI conference on Human factors in computing systems, pp. 319?326. ACM.
254
