Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 1?9,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Whitepaper of NEWS 2012 Shared Task on Machine Transliteration?
Min Zhang?, Haizhou Li?, Ming Liu?, A Kumaran?
?Institute for Infocomm Research, A*STAR, Singapore 138632
{mzhang,hli,mliu}@i2r.a-star.edu.sg
?Multilingual Systems Research, Microsoft Research India
A.Kumaran@microsoft.com
Abstract
Transliteration is defined as phonetic
translation of names across languages.
Transliteration of Named Entities (NEs)
is necessary in many applications, such
as machine translation, corpus alignment,
cross-language IR, information extraction
and automatic lexicon acquisition. All
such systems call for high-performance
transliteration, which is the focus of
shared task in the NEWS 2012 workshop.
The objective of the shared task is to pro-
mote machine transliteration research by
providing a common benchmarking plat-
form for the community to evaluate the
state-of-the-art technologies.
1 Task Description
The task is to develop machine transliteration sys-
tem in one or more of the specified language pairs
being considered for the task. Each language pair
consists of a source and a target language. The
training and development data sets released for
each language pair are to be used for developing
a transliteration system in whatever way that the
participants find appropriate. At the evaluation
time, a test set of source names only would be
released, on which the participants are expected
to produce a ranked list of transliteration candi-
dates in another language (i.e. n-best translitera-
tions), and this will be evaluated using common
metrics. For every language pair the participants
must submit at least one run that uses only the
data provided by the NEWS workshop organisers
in a given language pair (designated as ?standard?
run, primary submission). Users may submit more
?stanrard? runs. They may also submit several
?non-standard? runs for each language pair that
?http://translit.i2r.a-star.edu.sg/news2012/
use other data than those provided by the NEWS
2012 workshop; such runs would be evaluated and
reported separately.
2 Important Dates
Research paper submission deadline 25 March 2012
Shared task
Registration opens 18 Jan 2012
Registration closes 11 Mar 2012
Training/Development data release 20 Jan 2012
Test data release 12 Mar 2012
Results Submission Due 16 Mar 2012
Results Announcement 20 Mar 2012
Task (short) Papers Due 25 Mar 2012
For all submissions
Acceptance Notification 20 April 2012
Camera-Ready Copy Deadline 30 April 2012
Workshop Date 12/13/14 July 2012
3 Participation
1. Registration (18 Jan 2012)
(a) NEWS Shared Task opens for registra-
tion.
(b) Prospective participants are to register to
the NEWS Workshop homepage.
2. Training & Development Data (20 Jan 2012)
(a) Registered participants are to obtain
training and development data from the
Shared Task organiser and/or the desig-
nated copyright owners of databases.
(b) All registered participants are required
to participate in the evaluation of at least
one language pair, submit the results and
a short paper and attend the workshop at
ACL 2012.
3. Test data (12 March 2012)1
(a) The test data would be released on 12
March 2012, and the participants have a
maximum of 5 days to submit their re-
sults in the expected format.
(b) One ?standard? run must be submit-
ted from every group on a given lan-
guage pair. Additional ?standard? runs
may be submitted, up to 4 ?standard?
runs in total. However, the partici-
pants must indicate one of the submit-
ted ?standard? runs as the ?primary sub-
mission?. The primary submission will
be used for the performance summary.
In addition to the ?standard? runs, more
?non-standard? runs may be submitted.
In total, maximum 8 runs (up to 4 ?stan-
dard? runs plus up to 4 ?non-standard?
runs) can be submitted from each group
on a registered language pair. The defi-
nition of ?standard? and ?non-standard?
runs is in Section 5.
(c) Any runs that are ?non-standard? must
be tagged as such.
(d) The test set is a list of names in source
language only. Every group will pro-
duce and submit a ranked list of translit-
eration candidates in another language
for each given name in the test set.
Please note that this shared task is a
?transliteration generation? task, i.e.,
given a name in a source language one
is supposed to generate one or more
transliterations in a target language. It
is not the task of ?transliteration discov-
ery?, i.e., given a name in the source lan-
guage and a set of names in the target
language evaluate how to find the ap-
propriate names from the target set that
are transliterations of the given source
name.
4. Results (20 March 2012)
(a) On 20 March 2012, the evaluation re-
sults would be announced and will be
made available on the Workshop web-
site.
(b) Note that only the scores (in respective
metrics) of the participating systems on
each language pairs would be published,
and no explicit ranking of the participat-
ing systems would be published.
(c) Note that this is a shared evaluation task
and not a competition; the results are
meant to be used to evaluate systems on
common data set with common metrics,
and not to rank the participating sys-
tems. While the participants can cite the
performance of their systems (scores on
metrics) from the workshop report, they
should not use any ranking information
in their publications.
(d) Furthermore, all participants should
agree not to reveal identities of other
participants in any of their publications
unless you get permission from the other
respective participants. By default, all
participants remain anonymous in pub-
lished results, unless they indicate oth-
erwise at the time of uploading their re-
sults. Note that the results of all systems
will be published, but the identities of
those participants that choose not to dis-
close their identity to other participants
will be masked. As a result, in this case,
your organisation name will still appear
in the web site as one of participants, but
it will not be linked explicitly to your re-
sults.
5. Short Papers on Task (25 March 2012)
(a) Each submitting site is required to sub-
mit a 4-page system paper (short paper)
for its submissions, including their ap-
proach, data used and the results on ei-
ther test set or development set or by n-
fold cross validation on training set.
(b) The review of the system papers will be
done to improve paper quality and read-
ability and make sure the authors? ideas
and methods can be understood by the
workshop participants. We are aiming
at accepting all system papers, and se-
lected ones will be presented orally in
the NEWS 2012 workshop.
(c) All registered participants are required
to register and attend the workshop to
introduce your work.
(d) All paper submission and review will be
managed electronically through https://
www.softconf.com/acl2012/news2012/.2
4 Language Pairs
The tasks are to transliterate personal names or
place names from a source to a target language as
summarised in Table 1. NEWS 2012 Shared Task
offers 14 evaluation subtasks, among them ChEn
and ThEn are the back-transliteration of EnCh and
EnTh tasks respectively. NEWS 2012 releases
training, development and testing data for each of
the language pairs. NEWS 2012 continues all lan-
guage pairs that were evaluated in NEWS 2011. In
such cases, the training and development data in
the release of NEWS 2012 are the same as those
in NEWS 2011. However, the test data in NEWS
2012 are entirely new.
Please note that in order to have an accurate
study of the research progress of machine transla-
tion technology, different from previous practice,
the test/reference sets of NEWS 2011 are not re-
leased to the research community. Instead, we
use the test sets of NEWS 2011 as progress test
sets in NEWS 2012. NEWS 2012 participants are
requested to submit results on the NEWS 2012
progress test sets (i.e., NEWS 2011 test sets). By
doing so, we would like to do comparison studies
by comparing the NEWS 2012 and NEWS 2011
results on the progress test sets. We hope that we
can have some insightful research findings in the
progress studies.
The names given in the training sets for Chi-
nese, Japanese, Korean, Thai and Persian lan-
guages are Western names and their respective
transliterations; the Japanese Name (in English)
? Japanese Kanji data set consists only of native
Japanese names; the Arabic data set consists only
of native Arabic names. The Indic data set (Hindi,
Tamil, Kannada, Bangla) consists of a mix of In-
dian and Western names.
Examples of transliteration:
English ? Chinese
Timothy ???
English ? Japanese Katakana
Harrington ??????
English ? Korean Hangul
Bennett ? ??
Japanese name in English ? Japanese Kanji
Akihiro ???
English ? Hindi
San Francisco ? ????????????????
English ? Tamil
London ? ??????
English ? Kannada
Tokyo ? ??????
Arabic ? Arabic name in English
? 
Khalid
????
5 Standard Databases
Training Data (Parallel)
Paired names between source and target lan-
guages; size 7K ? 37K.
Training Data is used for training a basic
transliteration system.
Development Data (Parallel)
Paired names between source and target lan-
guages; size 1K ? 2.8K.
Development Data is in addition to the Train-
ing data, which is used for system fine-tuning
of parameters in case of need. Participants
are allowed to use it as part of training data.
Testing Data
Source names only; size 1K ? 2K.
This is a held-out set, which would be used
for evaluating the quality of the translitera-
tions.
Progress Testing Data
Source names only; size 0.6K ? 2.6K.
This is the NEWS 2011 test set, it is held-out
for progress study.
1. Participants will need to obtain licenses from
the respective copyright owners and/or agree
to the terms and conditions of use that are
given on the downloading website (Li et al,
2004; MSRI, 2010; CJKI, 2010). NEWS
2011 will provide the contact details of each
individual database. The data would be pro-
vided in Unicode UTF-8 encoding, in XML
format; the results are expected to be sub-
mitted in UTF-8 encoding in XML format.
The XML formats details are available in Ap-
pendix A.
2. The data are provided in 3 sets as described
above.
3. Name pairs are distributed as-is, as provided
by the respective creators.3
Name origin Source script Target script Data Owner Data Size Task IDTrain Dev Progress Test 2012 Test
Western English Chinese Institute for Infocomm Research 37K 2.8K 2K 1K EnCh
Western Chinese English Institute for Infocomm Research 28K 2.7K 2.2K 1K ChEn
Western English Korean Hangul CJK Institute 7K 1K 609 1K EnKo
Western English Japanese Katakana CJK Institute 26K 2K 1.8K 1K EnJa
Japanese English Japanese Kanji CJK Institute 10K 2K 571 1K JnJk
Arabic Arabic English CJK Institute 27K 2.5K 2.6K 1K ArEn
Mixed English Hindi Microsoft Research India 12K 1K 1K 1K EnHi
Mixed English Tamil Microsoft Research India 10K 1K 1K 1K EnTa
Mixed English Kannada Microsoft Research India 10K 1K 1K 1K EnKa
Mixed English Bangla Microsoft Research India 13K 1K 1K 1K EnBa
Western English Thai NECTEC 27K 2K 2K 1K EnTh
Western Thai English NECTEC 25K 2K 1.9K 1K ThEn
Western English Persian Sarvnaz Karimi / RMIT 10K 2K 2K 1K EnPe
Western English Hebrew Microsoft Research India 9.5K 1K 1K 1K EnHe
Table 1: Source and target languages for the shared task on transliteration.
(a) While the databases are mostly man-
ually checked, there may be still in-
consistency (that is, non-standard usage,
region-specific usage, errors, etc.) or in-
completeness (that is, not all right varia-
tions may be covered).
(b) The participants may use any method to
further clean up the data provided.
i. If they are cleaned up manually, we
appeal that such data be provided
back to the organisers for redistri-
bution to all the participating groups
in that language pair; such sharing
benefits all participants, and further
ensures that the evaluation provides
normalisation with respect to data
quality.
ii. If automatic cleanup were used,
such cleanup would be considered a
part of the system fielded, and hence
not required to be shared with all
participants.
4. Standard Runs We expect that the partici-
pants to use only the data (parallel names)
provided by the Shared Task for translitera-
tion task for a ?standard? run to ensure a fair
evaluation. One such run (using only the data
provided by the shared task) is mandatory for
all participants for a given language pair that
they participate in.
5. Non-standard Runs If more data (either par-
allel names data or monolingual data) were
used, then all such runs using extra data must
be marked as ?non-standard?. For such ?non-
standard? runs, it is required to disclose the
size and characteristics of the data used in the
system paper.
6. A participant may submit a maximum of 8
runs for a given language pair (including the
mandatory 1 ?standard? run marked as ?pri-
mary submission?).
6 Paper Format
Paper submissions to NEWS 2012 should follow
the ACL 2012 paper submission policy, includ-
ing paper format, blind review policy and title and
author format convention. Full papers (research
paper) are in two-column format without exceed-
ing eight (8) pages of content plus two (2) extra
page for references and short papers (task paper)
are also in two-column format without exceeding
four (4) pages content plus two (2) extra page for
references. Submission must conform to the offi-
cial ACL 2012 style guidelines. For details, please
refer to the ACL 2012 website2.
7 Evaluation Metrics
We plan to measure the quality of the translitera-
tion task using the following 4 metrics. We accept
up to 10 output candidates in a ranked list for each
input entry.
Since a given source name may have multiple
correct target transliterations, all these alternatives
are treated equally in the evaluation. That is, any
of these alternatives are considered as a correct
transliteration, and the first correct transliteration
in the ranked list is accepted as a correct hit.
2http://www.ACL2012.org/4
The following notation is further assumed:
N : Total number of names (source
words) in the test set
ni : Number of reference transliterations
for i-th name in the test set (ni ? 1)
ri,j : j-th reference transliteration for i-th
name in the test set
ci,k : k-th candidate transliteration (system
output) for i-th name in the test set
(1 ? k ? 10)
Ki : Number of candidate transliterations
produced by a transliteration system
1. Word Accuracy in Top-1 (ACC) Also
known as Word Error Rate, it measures correct-
ness of the first transliteration candidate in the can-
didate list produced by a transliteration system.
ACC = 1 means that all top candidates are cor-
rect transliterations i.e. they match one of the ref-
erences, and ACC = 0 means that none of the top
candidates are correct.
ACC =
1
N
N?
i=1
{
1 if ? ri,j : ri,j = ci,1;
0 otherwise
}
(1)
2. Fuzziness in Top-1 (Mean F-score) The
mean F-score measures how different, on average,
the top transliteration candidate is from its closest
reference. F-score for each source word is a func-
tion of Precision and Recall and equals 1 when the
top candidate matches one of the references, and
0 when there are no common characters between
the candidate and any of the references.
Precision and Recall are calculated based on the
length of the Longest Common Subsequence be-
tween a candidate and a reference:
LCS(c, r) =
1
2
(|c|+ |r| ? ED(c, r)) (2)
where ED is the edit distance and |x| is the length
of x. For example, the longest common subse-
quence between ?abcd? and ?afcde? is ?acd? and
its length is 3. The best matching reference, that
is, the reference for which the edit distance has
the minimum, is taken for calculation. If the best
matching reference is given by
ri,m = argmin
j
(ED(ci,1, ri,j)) (3)
then Recall, Precision and F-score for i-th word
are calculated as
Ri =
LCS(ci,1, ri,m)
|ri,m|
(4)
Pi =
LCS(ci,1, ri,m)
|ci,1|
(5)
Fi = 2
Ri ? Pi
Ri + Pi
(6)
? The length is computed in distinct Unicode
characters.
? No distinction is made on different character
types of a language (e.g., vowel vs. conso-
nants vs. combining diereses etc.)
3. Mean Reciprocal Rank (MRR) Measures
traditional MRR for any right answer produced by
the system, from among the candidates. 1/MRR
tells approximately the average rank of the correct
transliteration. MRR closer to 1 implies that the
correct answer is mostly produced close to the top
of the n-best lists.
RRi =
{
minj 1j if ?ri,j , ci,k : ri,j = ci,k;
0 otherwise
}
(7)
MRR =
1
N
N?
i=1
RRi (8)
4. MAPref Measures tightly the precision in the
n-best candidates for i-th source name, for which
reference transliterations are available. If all of
the references are produced, then the MAP is 1.
Let?s denote the number of correct candidates for
the i-th source word in k-best list as num(i, k).
MAPref is then given by
MAPref =
1
N
N?
i
1
ni
(
ni?
k=1
num(i, k)
)
(9)
8 Contact Us
If you have any questions about this share task and
the database, please email to
Mr. Ming Liu
Institute for Infocomm Research (I2R),
A*STAR
1 Fusionopolis Way
#08-05 South Tower, Connexis
Singapore 138632
mliu@i2r.a-star.edu.sg5
Dr. Min Zhang
Institute for Infocomm Research (I2R),
A*STAR
1 Fusionopolis Way
#08-05 South Tower, Connexis
Singapore 138632
mzhang@i2r.a-star.edu.sg
References
[CJKI2010] CJKI. 2010. CJK Institute.
http://www.cjk.org/.
[Li et al2004] Haizhou Li, Min Zhang, and Jian Su.
2004. A joint source-channel model for machine
transliteration. In Proc. 42nd ACL Annual Meeting,
pages 159?166, Barcelona, Spain.
[MSRI2010] MSRI. 2010. Microsoft Research India.
http://research.microsoft.com/india.
6
A Training/Development Data
? File Naming Conventions:
NEWS12 train XXYY nnnn.xml
NEWS12 dev XXYY nnnn.xml
NEWS12 test XXYY nnnn.xml
NEWS11 test XXYY nnnn.xml
(progress test sets)
? XX: Source Language
? YY: Target Language
? nnnn: size of parallel/monolingual
names (?25K?, ?10000?, etc)
? File formats:
All data will be made available in XML for-
mats (Figure 1).
? Data Encoding Formats:
The data will be in Unicode UTF-8 encod-
ing files without byte-order mark, and in the
XML format specified.
B Submission of Results
? File Naming Conventions:
You can give your files any name you like.
During submission online you will need to
indicate whether this submission belongs to
a ?standard? or ?non-standard? run, and if it
is a ?standard? run, whether it is the primary
submission.
? File formats:
All data will be made available in XML for-
mats (Figure 2).
? Data Encoding Formats:
The results are expected to be submitted in
UTF-8 encoded files without byte-order mark
only, and in the XML format specified.
7
<?xml version="1.0" encoding="UTF-8"?>
<TransliterationCorpus
CorpusID = "NEWS2012-Train-EnHi-25K"
SourceLang = "English"
TargetLang = "Hindi"
CorpusType = "Train|Dev"
CorpusSize = "25000"
CorpusFormat = "UTF8">
<Name ID=fl1fl>
<SourceName>eeeeee1</SourceName>
<TargetName ID="1">hhhhhh1_1</TargetName>
<TargetName ID="2">hhhhhh1_2</TargetName>
...
<TargetName ID="n">hhhhhh1_n</TargetName>
</Name>
<Name ID=fl2fl>
<SourceName>eeeeee2</SourceName>
<TargetName ID="1">hhhhhh2_1</TargetName>
<TargetName ID="2">hhhhhh2_2</TargetName>
...
<TargetName ID="m">hhhhhh2_m</TargetName>
</Name>
...
<!-- rest of the names to follow -->
...
</TransliterationCorpus>
Figure 1: File: NEWS2012 Train EnHi 25K.xml
8
<?xml version="1.0" encoding="UTF-8"?>
<TransliterationTaskResults
SourceLang = "English"
TargetLang = "Hindi"
GroupID = "Trans University"
RunID = "1"
RunType = "Standard"
Comments = "HMM Run with params: alpha=0.8 beta=1.25">
<Name ID="1">
<SourceName>eeeeee1</SourceName>
<TargetName ID="1">hhhhhh11</TargetName>
<TargetName ID="2">hhhhhh12</TargetName>
<TargetName ID="3">hhhhhh13</TargetName>
...
<TargetName ID="10">hhhhhh110</TargetName>
<!-- Participants to provide their
top 10 candidate transliterations -->
</Name>
<Name ID="2">
<SourceName>eeeeee2</SourceName>
<TargetName ID="1">hhhhhh21</TargetName>
<TargetName ID="2">hhhhhh22</TargetName>
<TargetName ID="3">hhhhhh23</TargetName>
...
<TargetName ID="10">hhhhhh110</TargetName>
<!-- Participants to provide their
top 10 candidate transliterations -->
</Name>
...
<!-- All names in test corpus to follow -->
...
</TransliterationTaskResults>
Figure 2: Example file: NEWS2012 EnHi TUniv 01 StdRunHMMBased.xml
9
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 10?20,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Report of NEWS 2012 Machine Transliteration Shared Task
Min Zhang?, Haizhou Li?, A Kumaran? and Ming Liu ?
?Institute for Infocomm Research, A*STAR, Singapore 138632
{mzhang,hli,mliu}@i2r.a-star.edu.sg
?Multilingual Systems Research, Microsoft Research India
A.Kumaran@microsoft.com
Abstract
This report documents the Machine
Transliteration Shared Task conducted as
a part of the Named Entities Workshop
(NEWS 2012), an ACL 2012 workshop.
The shared task features machine translit-
eration of proper names from English to
11 languages and from 3 languages to
English. In total, 14 tasks are provided.
7 teams participated in the evaluations.
Finally, 57 standard and 1 non-standard
runs are submitted, where diverse translit-
eration methodologies are explored and
reported on the evaluation data. We report
the results with 4 performance metrics.
We believe that the shared task has
successfully achieved its objective by pro-
viding a common benchmarking platform
for the research community to evaluate the
state-of-the-art technologies that benefit
the future research and development.
1 Introduction
Names play a significant role in many Natural
Language Processing (NLP) and Information Re-
trieval (IR) systems. They are important in Cross
Lingual Information Retrieval (CLIR) and Ma-
chine Translation (MT) as the system performance
has been shown to positively correlate with the
correct conversion of names between the lan-
guages in several studies (Demner-Fushman and
Oard, 2002; Mandl and Womser-Hacker, 2005;
Hermjakob et al, 2008; Udupa et al, 2009). The
traditional source for name equivalence, the bilin-
gual dictionaries ? whether handcrafted or sta-
tistical ? offer only limited support because new
names always emerge.
All of the above point to the critical need for ro-
bust Machine Transliteration technology and sys-
tems. Much research effort has been made to ad-
dress the transliteration issue in the research com-
munity (Knight and Graehl, 1998; Meng et al,
2001; Li et al, 2004; Zelenko and Aone, 2006;
Sproat et al, 2006; Sherif and Kondrak, 2007;
Hermjakob et al, 2008; Al-Onaizan and Knight,
2002; Goldwasser and Roth, 2008; Goldberg and
Elhadad, 2008; Klementiev and Roth, 2006; Oh
and Choi, 2002; Virga and Khudanpur, 2003; Wan
and Verspoor, 1998; Kang and Choi, 2000; Gao
et al, 2004; Zelenko and Aone, 2006; Li et al,
2009b; Li et al, 2009a). These previous work
fall into three categories, i.e., grapheme-based,
phoneme-based and hybrid methods. Grapheme-
based method (Li et al, 2004) treats translitera-
tion as a direct orthographic mapping and only
uses orthography-related features while phoneme-
based method (Knight and Graehl, 1998) makes
use of phonetic correspondence to generate the
transliteration. Hybrid method refers to the com-
bination of several different models or knowledge
sources to support the transliteration generation.
The first machine transliteration shared task (Li
et al, 2009b; Li et al, 2009a) was held in NEWS
2009 at ACL-IJCNLP 2009. It was the first time
to provide common benchmarking data in diverse
language pairs for evaluation of state-of-the-art
techniques. While the focus of the 2009 shared
task was on establishing the quality metrics and
on baselining the transliteration quality based on
those metrics, the 2010 shared task (Li et al,
2010a; Li et al, 2010b) expanded the scope of
the transliteration generation task to about a dozen
languages, and explored the quality depending on
the direction of transliteration, between the lan-
guages. In NEWS 2011 (Zhang et al, 2011a;
Zhang et al, 2011b), we significantly increased
the hand-crafted parallel named entities corpora to
include 14 different language pairs from 11 lan-
guage families, and made them available as the
common dataset for the shared task. NEWS 2012
was a continued effort of NEWS 2011, NEWS10
2010 and NEWS 2009.
The rest of the report is organised as follows.
Section 2 outlines the machine transliteration task
and the corpora used and Section 3 discusses the
metrics chosen for evaluation, along with the ratio-
nale for choosing them. Sections 4 and 5 present
the participation in the shared task and the results
with their analysis, respectively. Section 6 con-
cludes the report.
2 Transliteration Shared Task
In this section, we outline the definition and the
description of the shared task.
2.1 ?Transliteration?: A definition
There exists several terms that are used inter-
changeably in the contemporary research litera-
ture for the conversion of names between two
languages, such as, transliteration, transcription,
and sometimes Romanisation, especially if Latin
scripts are used for target strings (Halpern, 2007).
Our aim is not only at capturing the name con-
version process from a source to a target lan-
guage, but also at its practical utility for down-
stream applications, such as CLIR and MT. There-
fore, we adopted the same definition of translit-
eration as during the NEWS 2009 workshop (Li
et al, 2009a) to narrow down ?transliteration? to
three specific requirements for the task, as fol-
lows:?Transliteration is the conversion of a given
name in the source language (a text string in the
source writing system or orthography) to a name
in the target language (another text string in the
target writing system or orthography), such that
the target language name is: (i) phonemically
equivalent to the source name (ii) conforms to the
phonology of the target language and (iii) matches
the user intuition of the equivalent of the source
language name in the target language, consider-
ing the culture and orthographic character usage
in the target language.?
Following NEWS 2011, in NEWS 2012, we
still keep the three back-transliteration tasks. We
define back-transliteration as a process of restor-
ing transliterated words to their original lan-
guages. For example, NEWS 2012 offers the tasks
to convert western names written in Chinese and
Thai into their original English spellings, and ro-
manized Japanese names into their original Kanji
writings.
2.2 Shared Task Description
Following the tradition of NEWS workshop se-
ries, the shared task at NEWS 2012 is specified
as development of machine transliteration systems
in one or more of the specified language pairs.
Each language pair of the shared task consists of a
source and a target language, implicitly specifying
the transliteration direction. Training and develop-
ment data in each of the language pairs have been
made available to all registered participants for de-
veloping a transliteration system for that specific
language pair using any approach that they find
appropriate.
At the evaluation time, a standard hand-crafted
test set consisting of between 500 and 3,000
source names (approximately 5-10% of the train-
ing data size) have been released, on which the
participants are required to produce a ranked list
of transliteration candidates in the target language
for each source name. The system output is
tested against a reference set (which may include
multiple correct transliterations for some source
names), and the performance of a system is cap-
tured in multiple metrics (defined in Section 3),
each designed to capture a specific performance
dimension.
For every language pair each participant is re-
quired to submit at least one run (designated as a
?standard? run) that uses only the data provided by
the NEWS workshop organisers in that language
pair, and no other data or linguistic resources. This
standard run ensures parity between systems and
enables meaningful comparison of performance
of various algorithmic approaches in a given lan-
guage pair. Participants are allowed to submit
more ?standard? runs, up to 4 in total. If more than
one ?standard? runs is submitted, it is required to
name one of them as a ?primary? run, which is
used to compare results across different systems.
In addition, up to 4 ?non-standard? runs could be
submitted for every language pair using either data
beyond that provided by the shared task organisers
or linguistic resources in a specific language, or
both. This essentially may enable any participant
to demonstrate the limits of performance of their
system in a given language pair.
The shared task timelines provide adequate time
for development, testing (more than 1 month after
the release of the training data) and the final re-
sult submission (4 days after the release of the test
data).11
2.3 Shared Task Corpora
We considered two specific constraints in select-
ing languages for the shared task: language diver-
sity and data availability. To make the shared task
interesting and to attract wider participation, it is
important to ensure a reasonable variety among
the languages in terms of linguistic diversity, or-
thography and geography. Clearly, the ability of
procuring and distributing a reasonably large (ap-
proximately 10K paired names for training and
testing together) hand-crafted corpora consisting
primarily of paired names is critical for this pro-
cess. At the end of the planning stage and after
discussion with the data providers, we have cho-
sen the set of 14 tasks shown in Table 1 (Li et al,
2004; Kumaran and Kellner, 2007; MSRI, 2009;
CJKI, 2010).
NEWS 2012 leverages on the success of NEWS
2011 by utilizing the training set of NEWS 2011 as
the training data of NEWS 2012 and the dev data
of NEWS 2011 as the dev data of NEWS 2012.
NEWS 2012 provides entirely new test data across
all 14 tasks for evaluation.
The names given in the training sets for Chi-
nese, Japanese, Korean, Thai, Persian and Hebrew
languages are Western names and their respective
transliterations; the Japanese Name (in English)
? Japanese Kanji data set consists only of native
Japanese names; the Arabic data set consists only
of native Arabic names. The Indic data set (Hindi,
Tamil, Kannada, Bangla) consists of a mix of In-
dian and Western names.
For all of the tasks chosen, we have been
able to procure paired names data between the
source and the target scripts and were able to
make them available to the participants. For
some language pairs, such as English-Chinese and
English-Thai, there are both transliteration and
back-transliteration tasks. Most of the task are just
one-way transliteration, although Indian data sets
contained mixture of names of both Indian and
Western origins. The language of origin of the
names for each task is indicated in the first column
of Table 1.
Finally, it should be noted here that the corpora
procured and released for NEWS 2012 represent
perhaps the most diverse and largest corpora to be
used for any common transliteration tasks today.
3 Evaluation Metrics and Rationale
The participants have been asked to submit results
of up to four standard and four non-standard runs.
One standard run must be named as the primary
submission and is used for the performance sum-
mary. Each run contains a ranked list of up to
10 candidate transliterations for each source name.
The submitted results are compared to the ground
truth (reference transliterations) using 4 evalua-
tion metrics capturing different aspects of translit-
eration performance. The same as the NEWS
2011, we have dropped two MAP metrics used
in NEWS 2009 because they don?t offer additional
information to MAPref . Since a name may have
multiple correct transliterations, all these alterna-
tives are treated equally in the evaluation, that is,
any of these alternatives is considered as a correct
transliteration, and all candidates matching any of
the reference transliterations are accepted as cor-
rect ones.
The following notation is further assumed:
N : Total number of names (source
words) in the test set
ni : Number of reference transliterations
for i-th name in the test set (ni ? 1)
ri,j : j-th reference transliteration for i-th
name in the test set
ci,k : k-th candidate transliteration (system
output) for i-th name in the test set
(1 ? k ? 10)
Ki : Number of candidate transliterations
produced by a transliteration system
3.1 Word Accuracy in Top-1 (ACC)
Also known as Word Error Rate, it measures cor-
rectness of the first transliteration candidate in the
candidate list produced by a transliteration system.
ACC = 1 means that all top candidates are cor-
rect transliterations i.e. they match one of the ref-
erences, and ACC = 0 means that none of the top
candidates are correct.
ACC =
1
N
N?
i=1
{
1 if ?ri,j : ri,j = ci,1;
0 otherwise
}
(1)
3.2 Fuzziness in Top-1 (Mean F-score)
The mean F-score measures how different, on av-
erage, the top transliteration candidate is from its
closest reference. F-score for each source word12
Name origin Source script Target script Data Owner Data Size Task IDTrain Dev Test
Western English Chinese Institute for Infocomm Research 37K 2.8K 2K 1K EnCh
Western Chinese English Institute for Infocomm Research 28K 2.7K 2.2K 1K ChEn
Western English Korean Hangul CJK Institute 7K 1K 609 1K EnKo
Western English Japanese Katakana CJK Institute 26K 2K 1.8K 1K EnJa
Japanese English Japanese Kanji CJK Institute 10K 2K 571 1K JnJk
Arabic Arabic English CJK Institute 27K 2.5K 2.6K 1K ArEn
Mixed English Hindi Microsoft Research India 12K 1K 1K 1K EnHi
Mixed English Tamil Microsoft Research India 10K 1K 1K 1K EnTa
Mixed English Kannada Microsoft Research India 10K 1K 1K 1K EnKa
Mixed English Bangla Microsoft Research India 13K 1K 1K 1K EnBa
Western English Thai NECTEC 27K 2K 2K 1K EnTh
Western Thai English NECTEC 25K 2K 1.9K 1K ThEn
Western English Persian Sarvnaz Karimi / RMIT 10K 2K 2K 1K EnPe
Western English Hebrew Microsoft Research India 9.5K 1K 1K 1K EnHe
Table 1: Source and target languages for the shared task on transliteration.
is a function of Precision and Recall and equals 1
when the top candidate matches one of the refer-
ences, and 0 when there are no common characters
between the candidate and any of the references.
Precision and Recall are calculated based on
the length of the Longest Common Subsequence
(LCS) between a candidate and a reference:
LCS(c, r) =
1
2
(|c|+ |r| ? ED(c, r)) (2)
where ED is the edit distance and |x| is the length
of x. For example, the longest common subse-
quence between ?abcd? and ?afcde? is ?acd? and
its length is 3. The best matching reference, that
is, the reference for which the edit distance has
the minimum, is taken for calculation. If the best
matching reference is given by
ri,m = argmin
j
(ED(ci,1, ri,j)) (3)
then Recall, Precision and F-score for i-th word
are calculated as
Ri =
LCS(ci,1, ri,m)
|ri,m|
(4)
Pi =
LCS(ci,1, ri,m)
|ci,1|
(5)
Fi = 2
Ri ? Pi
Ri + Pi
(6)
? The length is computed in distinct Unicode
characters.
? No distinction is made on different character
types of a language (e.g., vowel vs. conso-
nants vs. combining diereses etc.)
3.3 Mean Reciprocal Rank (MRR)
Measures traditional MRR for any right answer
produced by the system, from among the candi-
dates. 1/MRR tells approximately the average
rank of the correct transliteration. MRR closer to 1
implies that the correct answer is mostly produced
close to the top of the n-best lists.
RRi =
{
minj 1j if ?ri,j , ci,k : ri,j = ci,k;
0 otherwise
}
(7)
MRR =
1
N
N?
i=1
RRi (8)
3.4 MAPref
Measures tightly the precision in the n-best can-
didates for i-th source name, for which reference
transliterations are available. If all of the refer-
ences are produced, then the MAP is 1. Let?s de-
note the number of correct candidates for the i-th
source word in k-best list as num(i, k). MAPref
is then given by
MAPref =
1
N
N?
i
1
ni
(
ni?
k=1
num(i, k)
)
(9)
4 Participation in Shared Task
7 teams submitted their transliteration results. Ta-
ble 3 shows the details of registration tasks. Teams
are required to submit at least one standard run for
every task they participated in. In total, we re-
ceive 57 standard and 1 non-standard runs. Table 2
shows the number of standard and non-standard
runs submitted for each task. It is clear that the
most ?popular? task is the transliteration from En-
glish to Chinese being attempted by 7 participants.13
English to
Chinese
Chinese to
English
English to
Thai
Thai to En-
glish
English to
Hindi
English to
Tamil
English to
Kannada
Language pair code EnCh ChEn EnTh ThEn EnHi EnTa EnKa
Standard runs 14 5 2 2 2 2 2
Non-standard runs 0 0 0 0 0 0 0
English to
Japanese
Katakana
English
to Korean
Hangul
English to
Japanese
Kanji
Arabic to
English
English to
Bengali
(Bangla)
English to
Persian
English to
Hebrew
Language pair code EnJa EnKo JnJk ArEn EnBa EnPe EnHe
Standard runs 3 4 4 5 4 4 4
Non-standard runs 0 1 0 0 0 0 0
Table 2: Number of runs submitted for each task. Number of participants coincides with the number of
standard runs submitted.
Team
ID
Organisation EnCh ChEn EnTh ThEn EnHi EnTa EnKa EnJa EnKo JnJk ArEn EnBa EnPe EnHe
1 University of Alberta x
2 NICT x x x x x x x x x x x x x x
3 MIT@Lab of HIT x
4 IASL, Academia
Sinica
x
5 Yahoo Japan Corpora-
tion
x x x x x x x x
6 Yuan Ze University x
7 CMU x x x x x x x x x x x x x x
Table 3: Participation of teams in different tasks.
5 Task Results and Analysis
5.1 Standard runs
All the results are presented numerically in Ta-
bles 4?17, for all evaluation metrics. These are the
official evaluation results published for this edition
of the transliteration shared task.
The methodologies used in the ten submitted
system papers are summarized as follows. Similar
to their NEWS 2011 system, Finch et al (2012)
employ non-Parametric Bayesian method to co-
segment bilingual named entities for model train-
ing and report very good performance. This sys-
tem is based on phrase-based statistical machine
transliteration (SMT) (Finch and Sumita, 2008),
an approach initially developed for machine trans-
lation (Koehn et al, 2003), where the SMT sys-
tem?s log-linear model is augmented with a set of
features specifically suited to the task of translit-
eration. In particular, the model utilizes a fea-
ture based on a joint source-channel model, and
a feature based on a maximum entropy model that
predicts target grapheme sequences using the local
context of graphemes and grapheme sequences in
both source and target languages. Different from
their NEWS 2011 system, in order to solve the
data sparseness issue, they use two RNN-based
LM to project the grapheme set onto a smaller hid-
den representation: one for the target grapheme se-
quence and the other for the sequence of grapheme
sequence pair used to generate the target.
Zhang et al (2012) also use the statistical
phrase-based SMT framework. They propose the
fine-grained English segmentation algorithm and
other new features and achieve very good perfor-
mance. Wu et al (2012) uses m2m-aligner and
DirecTL-p decoder and two re-ranking methods:
co-occurrence at web corpus and JLIS-Reranking
method based on the features from alignment re-
sults. They report very good performance at
English-Korean tasks. Okuno (2012) studies the
mpaligner (an improvement of m2m-aligner) and14
shows that mpaligner is more effective than m2m-
aligner. They also find that de-romanization is cru-
cial to JnJk task and mora is the best alignment
unit for EnJa task. Ammar et al (2012) use CRF
as the basic model but with two innovations: a
training objective that optimizes toward any of a
set of possible correct labels (i.e., multiple refer-
ences) and a k-best reranking with non-local fea-
tures. Their results on ArEn show that the two
features are very effective in accuracy improve-
ment. Kondrak et al (2012) study the language-
specific adaptations in the context of two language
pairs: English to Chinese (Pinyin representation)
and Arabic to English (letter mapping). They con-
clude that Pinyin representation is useful while let-
ter mapping is less effective. Kuo et al (2012) ex-
plore two-stage CRF for Enligsh-to-Chinese task
and show that the two-stage CRF outperform tra-
ditional one-stage CRF.
5.2 Non-standard runs
For the non-standard runs, we pose no restrictions
on the use of data or other linguistic resources.
The purpose of non-standard runs is to see how
best personal name transliteration can be, for a
given language pair. In NEWS 2012, only one
non-standard run (Wu et al, 2012) was submitted.
Their reported web-based re-validation method is
very effective.
6 Conclusions and Future Plans
The Machine Transliteration Shared Task in
NEWS 2012 shows that the community has a con-
tinued interest in this area. This report summa-
rizes the results of the shared task. Again, we
are pleased to report a comprehensive calibra-
tion and baselining of machine transliteration ap-
proaches as most state-of-the-art machine translit-
eration techniques are represented in the shared
task.
In addition to the most popular techniques such
as Phrase-Based Machine Transliteration (Koehn
et al, 2003), CRF, re-ranking, DirecTL-p de-
coder, Non-Parametric Bayesian Co-segmentation
(Finch et al, 2011), and Multi-to-Multi Joint
Source Channel Model (Chen et al, 2011) in the
NEWS 2011, we are delighted to see that sev-
eral new techniques have been proposed and ex-
plored with promising results reported, including
RNN-based LM (Finch et al, 2012), English Seg-
mentation algorithm (Zhang et al, 2012), JLIS-
reranking method (Wu et al, 2012), improved
m2m-aligner (Okuno, 2012), multiple reference-
optimized CRF (Ammar et al, 2012), language
dependent adaptation (Kondrak et al, 2012) and
two-stage CRF (Kuo et al, 2012). As the stan-
dard runs are limited by the use of corpus, most of
the systems are implemented under the direct or-
thographic mapping (DOM) framework (Li et al,
2004). While the standard runs allow us to con-
duct meaningful comparison across different al-
gorithms, we recognise that the non-standard runs
open up more opportunities for exploiting a vari-
ety of additional linguistic corpora.
Encouraged by the success of the NEWS work-
shop series, we would like to continue this event
in the future conference to promote the machine
transliteration research and development.
Acknowledgements
The organisers of the NEWS 2012 Shared Task
would like to thank the Institute for Infocomm
Research (Singapore), Microsoft Research In-
dia, CJK Institute (Japan), National Electronics
and Computer Technology Center (Thailand) and
Sarvnaz Karim / RMIT for providing the corpora
and technical support. Without those, the Shared
Task would not be possible. We thank those par-
ticipants who identified errors in the data and sent
us the errata. We also want to thank the members
of programme committee for their invaluable com-
ments that improve the quality of the shared task
papers. Finally, we wish to thank all the partici-
pants for their active participation that have made
this first machine transliteration shared task a com-
prehensive one.
15
References
Yaser Al-Onaizan and Kevin Knight. 2002. Machine
transliteration of names in arabic text. In Proc.
ACL-2002Workshop: Computational Apporaches to
Semitic Languages, Philadelphia, PA, USA.
Waleed Ammar, Chris Dyer, and Noah Smith. 2012.
Transliteration by sequence labeling with lattice en-
codings and reranking. In Proc. Named Entities
Workshop at ACL 2012.
Yu Chen, Rui Wang, and Yi Zhang. 2011. Statisti-
cal machine transliteration with multi-to-multi joint
source channel model. In Proc. Named Entities
Workshop at IJCNLP 2011.
CJKI. 2010. CJK Institute. http://www.cjk.org/.
D. Demner-Fushman and D. W. Oard. 2002. The ef-
fect of bilingual term list size on dictionary-based
cross-language information retrieval. In Proc. 36-th
Hawaii Int?l. Conf. System Sciences, volume 4, page
108.2.
Andrew Finch and Eiichiro Sumita. 2008. Phrase-
based machine transliteration. In Proc. 3rd Int?l.
Joint Conf NLP, volume 1, Hyderabad, India, Jan-
uary.
Andrew Finch, Paul Dixon, and Eiichiro Sumita. 2011.
Integrating models derived from non-parametric
bayesian co-segmentation into a statistical machine
transliteration system. In Proc. Named Entities
Workshop at IJCNLP 2011.
Andrew Finch, Paul Dixon, and Eiichiro Sumita. 2012.
Rescoring a phrase-based machine transliteration
systemwith recurrent neural network language mod-
els. In Proc. Named Entities Workshop at ACL 2012.
Wei Gao, Kam-Fai Wong, and Wai Lam. 2004.
Phoneme-based transliteration of foreign names for
OOV problem. In Proc. IJCNLP, pages 374?381,
Sanya, Hainan, China.
Yoav Goldberg andMichael Elhadad. 2008. Identifica-
tion of transliterated foreign words in Hebrew script.
In Proc. CICLing, volume LNCS 4919, pages 466?
477.
Dan Goldwasser and Dan Roth. 2008. Translitera-
tion as constrained optimization. In Proc. EMNLP,
pages 353?362.
Jack Halpern. 2007. The challenges and pitfalls
of Arabic romanization and arabization. In Proc.
Workshop on Comp. Approaches to Arabic Script-
based Lang.
Ulf Hermjakob, Kevin Knight, and Hal Daume?. 2008.
Name translation in statistical machine translation:
Learning when to transliterate. In Proc. ACL,
Columbus, OH, USA, June.
Byung-Ju Kang and Key-Sun Choi. 2000.
English-Korean automatic transliteration/back-
transliteration system and character alignment. In
Proc. ACL, pages 17?18, Hong Kong.
Alexandre Klementiev and Dan Roth. 2006. Weakly
supervised named entity transliteration and discov-
ery from multilingual comparable corpora. In Proc.
21st Int?l Conf Computational Linguistics and 44th
Annual Meeting of ACL, pages 817?824, Sydney,
Australia, July.
Kevin Knight and Jonathan Graehl. 1998. Machine
transliteration. Computational Linguistics, 24(4).
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In Proc. HLT-NAACL.
Grzegorz Kondrak, Xingkai Li, and Mohammad
Salameh. 2012. Transliteration experiments on chi-
nese and arabic. In Proc. Named Entities Workshop
at ACL 2012.
A Kumaran and T. Kellner. 2007. A generic frame-
work for machine transliteration. In Proc. SIGIR,
pages 721?722.
Chan-Hung Kuo, Shih-Hung Liu, Mike Tian-Jian
Jiang, Cheng-Wei Lee, and Wen-Lian Hsu. 2012.
Cost-benefit analysis of two-stage conditional
random fields based english-to-chinese machine
transliteration. In Proc. Named Entities Workshop
at ACL 2012.
Haizhou Li, Min Zhang, and Jian Su. 2004. A joint
source-channel model for machine transliteration.
In Proc. 42nd ACL Annual Meeting, pages 159?166,
Barcelona, Spain.
Haizhou Li, A Kumaran, Vladimir Pervouchine, and
Min Zhang. 2009a. Report of NEWS 2009 machine
transliteration shared task. In Proc. Named Entities
Workshop at ACL 2009.
Haizhou Li, A Kumaran, Min Zhang, and Vladimir
Pervouchine. 2009b. ACL-IJCNLP 2009 Named
Entities Workshop ? Shared Task on Translitera-
tion. In Proc. Named Entities Workshop at ACL
2009.
Haizhou Li, A Kumaran, Min Zhang, and Vladimir
Pervouchine. 2010a. Report of news 2010 translit-
eration generation shared task. In Proc. Named En-
tities Workshop at ACL 2010.
Haizhou Li, A Kumaran, Min Zhang, and Vladimir
Pervouchine. 2010b. Whitepaper of news 2010
shared task on transliteration generation. In Proc.
Named Entities Workshop at ACL 2010.
T. Mandl and C. Womser-Hacker. 2005. The effect of
named entities on effectiveness in cross-language in-
formation retrieval evaluation. In Proc. ACM Symp.
Applied Comp., pages 1059?1064.16
Helen M. Meng, Wai-Kit Lo, Berlin Chen, and Karen
Tang. 2001. Generate phonetic cognates to han-
dle name entities in English-Chinese cross-language
spoken document retrieval. In Proc. ASRU.
MSRI. 2009. Microsoft Research India.
http://research.microsoft.com/india.
Jong-Hoon Oh and Key-Sun Choi. 2002. An English-
Korean transliteration model using pronunciation
and contextual rules. In Proc. COLING 2002,
Taipei, Taiwan.
Yoh Okuno. 2012. Applying mpaligner to machine
transliteration with japanese-specific heuristics. In
Proc. Named Entities Workshop at ACL 2012.
Tarek Sherif and Grzegorz Kondrak. 2007. Substring-
based transliteration. In Proc. 45th Annual Meeting
of the ACL, pages 944?951, Prague, Czech Repub-
lic, June.
Richard Sproat, Tao Tao, and ChengXiang Zhai. 2006.
Named entity transliteration with comparable cor-
pora. In Proc. 21st Int?l Conf Computational Lin-
guistics and 44th Annual Meeting of ACL, pages 73?
80, Sydney, Australia.
Raghavendra Udupa, K. Saravanan, Anton Bakalov,
and Abhijit Bhole. 2009. ?They are out there, if
you know where to look?: Mining transliterations
of OOV query terms for cross-language informa-
tion retrieval. In LNCS: Advances in Information
Retrieval, volume 5478, pages 437?448. Springer
Berlin / Heidelberg.
Paola Virga and Sanjeev Khudanpur. 2003. Translit-
eration of proper names in cross-lingual information
retrieval. In Proc. ACL MLNER, Sapporo, Japan.
Stephen Wan and Cornelia Maria Verspoor. 1998. Au-
tomatic English-Chinese name transliteration for de-
velopment of multilingual resources. In Proc. COL-
ING, pages 1352?1356.
Chun-Kai Wu, Yu-Chun Wang, and Richard Tzong-
Han Tsai. 2012. English-korean named entity
transliteration using substring alignment and re-
ranking methods. In Proc. Named Entities Work-
shop at ACL 2012.
Dmitry Zelenko and Chinatsu Aone. 2006. Discrimi-
native methods for transliteration. In Proc. EMNLP,
pages 612?617, Sydney, Australia, July.
Min Zhang, A Kumaran, and Haizhou Li. 2011a.
Whitepaper of news 2011 shared task on machine
transliteration. In Proc. Named Entities Workshop
at IJCNLP 2011.
Min Zhang, Haizhou Li, A Kumaran, and Ming Liu.
2011b. Report of news 2011 machine transliteration
shared task. In Proc. Named Entities Workshop at
IJCNLP 2011.
Chunyue Zhang, Tingting Li, and Tiejun Zhao. 2012.
Syllable-based machine transliteration with extra
phrase features. In Proc. Named Entities Workshop
at ACL 2012.
17
Team ID ACC F -score MRR MAPref Organisation
Primary runs
3 0.330357 0.66898 0.413062 0.320285 MIT@Lab of HIT
1 0.325397 0.67228 0.418079 0.316296 University of Alberta
2 0.310516 0.66585 0.44664 0.307788 NICT
4 0.310516 0.662467 0.37696 0.299266 IASL, Academia Sinica
5 0.300595 0.655091 0.376025 0.292252 Yahoo Japan Corporation
7 0.031746 0.430698 0.055574 0.030265 CMU
Non-primary standard runs
3 0.330357 0.676232 0.407755 0.3191 MIT@Lab of HIT
1 0.325397 0.673053 0.409452 0.316055 University of Alberta
1 0.324405 0.668165 0.424517 0.316248 University of Alberta
3 0.31746 0.666551 0.399476 0.308187 MIT@Lab of HIT
4 0.298611 0.658836 0.362263 0.288725 IASL, Academia Sinica
5 0.298611 0.656974 0.357481 0.289373 Yahoo Japan Corporation
4 0.294643 0.651988 0.357495 0.284274 IASL, Academia Sinica
4 0.290675 0.653565 0.370733 0.282545 IASL, Academia Sinica
Table 4: Runs submitted for English to Chinese task.
Team ID ACC F -score MRR MAPref Organisation
Primary runs
2 0.20314 0.736058 0.308801 0.199569 NICT
3 0.176644 0.701791 0.257324 0.172991 MIT@Lab of HIT
7 0.030422 0.489705 0.048211 0.03004 CMU
5 0.012758 0.258962 0.017354 0.012758 Yahoo Japan Corporation
Non-primary standard runs
5 0.007851 0.258013 0.012163 0.007851 Yahoo Japan Corporation
Table 5: Runs submitted for Chinese to English back-transliteration task.
Team ID ACC F -score MRR MAPref Organisation
Primary runs
2 0.122168 0.746824 0.183318 0.122168 NICT
7 0.000809 0.288585 0.001883 0.000809 CMU
Table 6: Runs submitted for English to Thai task.
Team ID ACC F -score MRR MAPref Organisation
Primary runs
2 0.139968 0.765534 0.21551 0.139968 NICT
7 0 0.417451 0.000566 0 CMU
Table 7: Runs submitted for Thai to English back-transliteration task.
18
Team ID ACC F -score MRR MAPref Organisation
Primary runs
2 0.668 0.923347 0.73795 0.661278 NICT
7 0.048 0.645666 0.087842 0.048528 CMU
Table 8: Runs submitted for English to Hindi task.
Team ID ACC F -score MRR MAPref Organisation
Primary runs
2 0.592 0.908444 0.67881 0.5915 NICT
7 0.052 0.638029 0.083728 0.052 CMU
Table 9: Runs submitted for English to Tamil task.
Team ID ACC F -score MRR MAPref Organisation
Primary runs
2 0.546 0.900557 0.640534 0.545361 NICT
7 0.116 0.737857 0.180234 0.11625 CMU
Table 10: Runs submitted for English to Kannada task.
Team ID ACC F -score MRR MAPref Organisation
Primary runs
2 0.400774 0.810109 0.522758 0.397386 NICT
5 0.362052 0.802701 0.468973 0.35939 Yahoo Japan Corporation
7 0 0.147441 0.00038 0 CMU
Table 11: Runs submitted for English to Japanese Katakana task.
Team ID ACC F -score MRR MAPref Organisation
Primary runs
6 0.398095 0.731212 0.398095 0.396905 Yuan Ze University
2 0.38381 0.721247 0.464553 0.383095 NICT
5 0.334286 0.687794 0.411264 0.334048 Yahoo Japan Corporation
7 0 0 0.00019 0 CMU
Non-standard runs
6 0.458095 0.756755 0.484048 0.458095 Yuan Ze University
Table 12: Runs submitted for English to Korean task.
Team ID ACC F -score MRR MAPref Organisation
Primary runs
2 0.513242 0.693184 0.598304 0.418708 NICT
5 0.512329 0.693029 0.581803 0.400505 Yahoo Japan Corporation
7 0 0 0 0 CMU
Non-primary standard runs
5 0.511416 0.691131 0.580485 0.402127 Yahoo Japan Corporation
Table 13: Runs submitted for English to Japanese Kanji back-transliteration task.19
Team ID ACC F -score MRR MAPref Organisation
Primary runs
2 0.588235 0.929787 0.709003 0.506991 NICT
7 0.58391 0.925292 0.694338 0.367162 CMU
1 0.583045 0.932959 0.670457 0.42041 University of Alberta
Non-primary standard runs
7 0.57699 0.93025 0.678898 0.330353 CMU
7 0.573529 0.925306 0.675125 0.328782 CMU
Table 14: Runs submitted for Arabic to English task.
Team ID ACC F -score MRR MAPref Organisation
Primary runs
2 0.46 0.891476 0.582944 0.458417 NICT
5 0.404 0.882395 0.514541 0.402917 Yahoo Japan Corporation
7 0.178 0.783893 0.248674 0.177139 CMU
Non-primary standard runs
5 0.398 0.880286 0.510148 0.396528 Yahoo Japan Corporation
Table 15: Runs submitted for English to Bengali (Bangla) task.
Team ID ACC F -score MRR MAPref Organisation
Primary runs
5 0.658349 0.940642 0.761223 0.639873 Yahoo Japan Corporation
2 0.65547 0.941044 0.773843 0.642663 NICT
7 0.18618 0.803002 0.311881 0.184961 CMU
Non-primary standard runs
5 0.054702 0.627335 0.082754 0.054367 Yahoo Japan Corporation
Table 16: Runs submitted for English to Persian task.
Team ID ACC F -score MRR MAPref Organisation
Primary runs
5 0.190909 0.808491 0.253575 0.19 Yahoo Japan Corporation
2 0.153636 0.787254 0.228649 0.152727 NICT
7 0.097273 0.759444 0.130955 0.096818 CMU
Non-primary standard runs
5 0.165455 0.803019 0.241948 0.164545 Yahoo Japan Corporation
Table 17: Runs submitted for English to Hebrew task.
20
