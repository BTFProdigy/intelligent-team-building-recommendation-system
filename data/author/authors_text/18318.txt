Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 588?599, Dublin, Ireland, August 23-29 2014.
Enforcing Topic Diversity in a Document Recommender for Conversations
Maryam Habibi
Idiap Research Institute and EPFL
Rue Marconi 19, CP 592
1920 Martigny, Switzerland
maryam.habibi@idiap.ch
Andrei Popescu-Belis
Idiap Research Institute
Rue Marconi 19, CP 592
1920 Martigny, Switzerland
andrei.popescu-belis@idiap.ch
Abstract
This paper addresses the problem of building concise, diverse and relevant lists of documents,
which can be recommended to the participants of a conversation to fulfill their information needs
without distracting them. These lists are retrieved periodically by submitting multiple implicit
queries derived from the pronounced words. Each query is related to one of the topics identified
in the conversation fragment preceding the recommendation, and is submitted to a search engine
over the English Wikipedia. We propose in this paper an algorithm for diverse merging of these
lists, using a submodular reward function that rewards the topical similarity of documents to
the conversation words as well as their diversity. We evaluate the proposed method through
crowdsourcing. The results show the superiority of the diverse merging technique over several
others which not enforce the diversity of topics.
1 Introduction
We present a diverse retrieval technique for ranking documents that are spontaneously retrieved and
recommended to people during a conversation. These documents represent potentially useful information
for the conversation participants. The information needs of the participants are represented by implicit
queries which are built in the background based on their current speech, specifically from keywords
obtained from the conversation transcripts. Since people usually mention several topics even during a
short conversation span, such keyword sets are made of content words related to different topics. When
juxtaposed in an implicit query, these topics may have noisy effects on the retrieval results (Bhogal et al.,
2007; Carpineto and Romano, 2012).
The purpose of this paper is to present a method for merging lists of documents retrieved through
multiple implicit queries prepared for short conversations spans. Several topically-separated queries are
constructed from keywords, and generate several lists of documents. The goal of the method proposed
here is to generate a unique and concise list of documents that can be recommended in real time to the
conversation participants. The list should cover the maximum number of implicit queries and therefore
topics. To merge the lists of documents according to these criteria, we use inspiration from extractive
text summarization (Lin and Bilmes, 2011; Li et al., 2012) and from our own previous work on diverse
keyword extraction (Habibi and Popescu-Belis, 2013). The method proposed here rewards at the same
time topic similarity ? to select the most relevant documents to the conversation fragment ? and topic
diversity ? to cover the maximum number of implicit queries and therefore topics in a concise and
relevant list of recommendations, if more than one topic is discussed in the conversation fragment.
Several studies have been previously carried out on merging lists of results in information retrieval.
Despite the superficial similarity, the problem here is in fact different from distributed information re-
trieval, where several lists of results from different search engines for the same query must be merged.
Moreover, many studies addressed the topic diversification approach for re-ranking the retrieved results
of a single query. However, these approaches are not directly applicable to multiple queries.
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
588
The paper is organized as follows. In Section 2 we review existing techniques for merging and re-
ranking lists of search results which are applicable here. We then explain the general framework of our
document recommender system in Section 3. In Section 4 we describe the proposed algorithm for diverse
merging of lists of recommendations. Section 5 presents the data, the parameters setting, and evaluation
tasks for comparing document lists. In Section 6 we first demonstrate empirically the benefits, for just-in-
time document recommendation, of separating users? information needs into multiple topically-separated
queries rather than using a unique query. Then, we compare the proposed diverse merging technique with
several alternative ones, showing that it outperforms them according to human judgments of relevance,
and also exemplify the results on one conversation fragment given in the Appendix A.
2 Related Work
Just-in-time document retrieval systems have been designed to recommend to their users documents
which are potentially relevant to their activities, e.g. individual users authoring documents or browsing
various repositories, or small groups holding business or private meetings (Hart and Graham, 1997;
Rhodes and Maes, 2000; Popescu-Belis et al., 2008). When using a document recommender system,
people are generally unwilling to examine a large number of recommended documents, mainly because
this would distract them from their main activity. Several solutions to this problem have been proposed.
For instance, the Watson document recommender system (Budzik and Hammond, 2000), designed
for reading or writing activities, clustered the document results and selected from each cluster the best
representative to generate a list of recommendations. Clustering results is not suitable for our application
where the mixture of topics in a single query will degrade the document results aimed to be clustered
(Bhogal et al., 2007; Carpineto and Romano, 2012), and consequently may have a damaging effect on
the clusters? representatives. The second part of the method, which selected the best representative of
the clusters in the final document list can be helpful; however, its effectiveness relies on having clusters
with the same level of importance (Wu and McClean, 2007).
Many studies in information retrieval addressed the problem of diverse ranking, which can be stated as
a tradeoff between finding relevant versus diverse information (Robertson, 1997). The existing diverse
ranking proposals differ in their diversifying policies and definitions, which can be categorized into im-
plicit methods (Carbonell and Goldstein, 1998; Zhai et al., 2003; Radlinski and Dumais, 2006; Wang
and Zhu, 2009) or explicit ones (Agrawal et al., 2009; Carterette and Chandar, 2009; Santos et al., 2010;
Vargas et al., 2012). The implicit approaches assume that similar documents will cover similar aspects
of a query, and have to be demoted in the ranking to promote relative novelty and reduce overall redun-
dancy. In one of the earliest approaches, Carbonell and Goldstein (1998) introduced Maximal Marginal
Relevance (MMR) to re-rank documents based on a tradeoff between the relevance of document results
and relative novelty as a measure of diversity. MMR was also used by Radlinski and Dumais (2006) to
re-rank results from a query set which is generated for a user query and represents a variety of potential
user intents.
Instead of implicitly accounting for the aspects covered by each document, another option is to ex-
plicitly model these aspects within the diversification approach. Agrawal et al. (2009) introduced a
submodular objective function to minimize the probability of average user dissatisfaction by assuming
a taxonomy of information and modeling user query aspects at the topical level of this taxonomy. Al-
ternatively, Santos et al. (2010) proposed another submodular objective function to maximize coverage
and minimize redundancy with respect to query aspects modeled in a keyword-based representation form
instead of a predefined taxonomy.
In our case, the recommender system for conversational environments requires diversity in the results
of multiple topically-separated queries, rather than of a single ambiguous query. Therefore, a new ap-
proach will be proposed, and will be compared in particular to a version of the explicit diversification
approach (Santos et al., 2010) adapted to our problem.
3 Framework of our Document Recommender System
We have designed the Automatic Content Linking Device (ACLD), a speech-based just-in-time document
recommender system for business meetings (Popescu-Belis et al., 2008; Popescu-Belis et al., 2011).
589
Transcript of conversation fragment
Extract the best k keywords that cover all the 
main topics with high probability
Topical clustering of keywords to prepare M
multiple topic-aware queries 
},...,{
1 kccC 
Retrieval system
},...,{},,...,{
11 MM wwWqqQ 
Retrieval system
},...,{
1 MllL  l
Diverse ranking 
(DivS)
Similarity merging 
(SimM)
Round-robin
merging
Diverse merging 
(DivM)
},...,{
1 NddS },...,{ 1 NddS },...,{ 1 NddS },...,{ 1 NddS 
list of relevant documents,
(1)
(2)
(3)
(4)
Figure 1: The four stages of our document recommendation approach (shown vertically: 1?4) and the
four options considered in this paper (bottom line: SimM, Round-robin, DivM, and DivS).
The ACLD monitors the ongoing conversation, and formulates queries based on the words detected by
a real-time automatic speech recognition (ASR) system (Garner et al., 2009). The queries are fired
periodically to retrieve documents which are then recommended to users by displaying their titles along
with relevant excerpts. As these queries are built and triggered in the background, they are referred to as
?implicit queries?, as opposed to ?explicit? ones that could be formulated by users. Just-in-time document
recommendation in the ACLD system proceeds according to the steps shown in Figure 1, which displays
at step 4 the various options for merging lists of results that are the focus of this paper.
Prior to the first processing step outlined in Figure 1, the ACLD must decide when to make a recom-
mendation, and what portion of the conversation prior to that moment should be used. This question is
beyond the scope of this paper, and remains to be fully investigated, using verbal and non-verbal criteria.
Here, for the reasons explained in Section 5.2, the ACLD recommends documents every two minutes,
segmenting the conversation at the end of the nearest utterance and using the entire conversation frag-
ment since the previous recommendation. Although in practice the results of the current recommendation
process are merged with the previous ones (using a weighted mechanism that embodies the idea of ?per-
sistence? of documents over time), in this paper we will consider the recommendation for each fragment
independently of the previous one.
The recommendation process represented in Figure 1 starts by extracting a set of keywords, C, from
the words recognized by the ASR system from the users? conversations. The keywords are extracted
using the diverse keyword extraction technique that we proposed (Habibi and Popescu-Belis, 2013),
which maximizes the coverage of the topics of a text by the extracted keyword set, as we also target in
this paper. Then, implicit queries which express the users? information needs are formulated using the
keyword set, following two alternative approaches depicted in step 2 of Figure 1. In a baseline model
(right side of the figure), a single query is built for the conversation fragment using the entire keyword
list as an implicit query. In the approach we are advocating, multiple topically-separated queries are
produced for the conversation fragment (step 2, left side of the figure). This is described in a separate
document (Habibi and Popescu-Belis, submitted), but can be outlined as follows. The implicit queries are
obtained by clustering the above-mentioned keyword set into several topically-separated subsets, each
one corresponding to an abstract topic obtained using topic modeling techniques (similarly to the model
590
presented in Subsection 4.1). Each subset is an implicit query, and is weighted based on the importance
of the topic to which it is associated.
In step 3, we separately submit each implicit query to the Apache Lucene search engine over the
English Wikipedia and obtain several lists of relevant articles. Finally, we merge and re-rank these
lists before recommendation (step 4). One baseline alternative is the explicit diverse ranking technique
proposed by Santos et al. (2010) for diversifying the primary search results retrieved for a single query,
shown on the right side of the figure. To compare the methods, we adapted this latter method to make
it applicable to our system when a single implicit query is built for a conversation fragment, by defining
query aspects using the abstract topics employed for query and document representation. The method is
noted DivS as it diversifies documents from a single list.
Our proposal lies at step 4. As represented on the left side of Figure 1, in our system, we merge the lists
of documents retrieved for multiple implicit queries. We thus propose a new method noted DivM and we
compare it with two other merging techniques. The first one, noted SimM, ignores the diversity of topics
in the list of results and ranks documents only by considering their topic similarity to the conversation
fragment. The second one is the merging technique used by the above-mentioned Watson system (Budzik
and Hammond, 2000), which uses Round robin merging, hence it is noted Round-robin. In contrast, our
proposed method, DivM, is a diverse merging technique which we now proceed to define formally.
4 Diverse Merging of the Results of Multiple Queries
The diverse merging of retrieved document lists is the process of creating a short, diverse and relevant list
of recommended documents which covers the maximum number of topics of each conversation fragment.
The merging algorithm rewards diversity by decreasing the gain of selecting documents from a list as
the number of its previously selected documents increases. The method proceeds in two steps. First,
we represent queries and the corresponding list of candidate documents from the Apache Lucene search
engine using topic modeling techniques, and then we rank documents by using topical similarity and
rewarding the coverage of different lists.
4.1 Document and Query Representation
A topic model represents the abstract topics which occur in a collection of documents ? here, preferably,
a collection that is representative of the domain of the conversations. Once trained, topic models such
as Probabilistic Latent Semantic Analysis (PLSA) or Latent Dirichlet Allocation (LDA) can be used
to determine the distribution of abstract topics in each set of words composing either a conversation
fragment, or a query, or a document. LDA implemented in the Mallet toolkit (McCallum, 2002) is used
here to train topic models because it does not suffer from the over-fitting of PLSA (Blei et al., 2003).
We first learn a probability model for observing a word v in a document d through the set of abstract
topics T = {t
1
, ..., t
z
, ..., t
Z
}, where Z is the number of topics, using the Mallet toolkit:
p(v|d) =
Z
?
z=1
p(v|t
z
) ? p(t
z
|d) (1)
The topic-word distribution p(v|t
z
) and the document-topic distribution p(t
z
|d), which are obtained
using topic modeling, respectively show the contribution of the word v in the construction of the topic
t
z
, and the distribution of topic t
z
in the document d with respect to the other topics.
We represent each new text or fragment A (e.g. from a conversation or document) by a set of proba-
bility distributions over all abstract topics T noted as P (A) = {p(t
1
|A), ..., p(t
z
|A), ..., p(t
Z
|A)} where
p(t
z
|A) is inferred using the Gibbs sampling implemented by the Mallet toolkit given the topic models
previously learned. We associate to each new document d
i
and query q
j
a set of topic probabilities ac-
cording to the above definition noted respectively as P (d
i
) = {p(t
1
|d
i
), ..., p(t
z
|d
i
), ..., p(t
Z
|d
i
)} and
P (q
j
) = {p(t
1
|q
j
), ..., p(t
z
|q
j
), ..., p(t
Z
|q
j
)}.
4.2 Diverse Merging Problem
As stated above, our goal is to recommend a short ranked list of documents answering the users? informa-
tion needs hypothesized in a conversation fragment, which are modeled by multiple topic-aware implicit
591
queries as described in Section 3. We build the final list of recommended documents by merging the
document lists, one from each implicit query, with the objective of the maximum coverage of the topics
of the conversation fragment. Since each document list contains documents found by a search engine
given an implicit query, which was prepared for one of the main topics of the conversation fragment,
we merge the lists by selecting documents from the maximum number of lists in addition to maximizing
their topical similarity to the conversation fragment.
The problem of diverse merging of lists thus amounts to finding a ranked subset of documents S ?
?
M
i=1
l
i
, which are the most representative of all the result lists l
i
, and potentially the most informative with
respect to the conversation fragment and the information needs that are implicitly stated. This problem
is an instance of the maximum coverage problem, which is known to be NP-hard. Our formulation and
solution proceed as follows.
Let us consider a set of implicit queries Q = {q
1
, ..., q
M
}, and the corresponding set of document
lists L = {l
1
, ..., l
M
} resulting from each query. M is the number of implicit queries of the fragment,
and each l
i
is a list of documents {d
1
, ..., d
N
i
} which are retrieved for query q
i
. We define the weight
w
i
of each query q
i
as the importance within the conversation fragment of the topics represented in the
query q
i
, and compute it as the topical similarity of q
i
to the fragment, as shown in Equation 2. In this
equation, q is the query made from the whole keyword set, which we call a collective query, and includes
keywords for all the main topics of the conversation fragment in one query. In turn, we associate to q a
set of probabilities over abstract topics, P (q) = {p(t
1
|q), ..., p(t
Z
|q)}, similar to the representation of
implicit queries explained in Subsection 4.1.
w
i
=
Z
?
z=1
p(t
z
|q
i
) ? p(t
z
|q) (2)
4.3 Defining a Diverse Reward Function
Although the maximum coverage problem is NP-hard, it has been shown that a greedy algorithm can
find an approximate solution guaranteed to be within a factor of (1 ? 1/e) ' 0.63 of the optimal one if
the coverage function is submodular and monotone non-decreasing
1
(Nemhauser et al., 1978). Several
monotone submodular functions have been proposed in various domains for a similar underlying prob-
lem, such as explicit diverse re-ranking of retrieval results (Agrawal et al., 2009; Santos et al., 2010;
Vargas et al., 2012), extractive summarization of a text (Lin and Bilmes, 2011; Li et al., 2012), or our
own model of diverse keyword extraction from a text (Habibi and Popescu-Belis, 2013).
We define a monotone submodular function for diverse merging of document lists inspired by the latter
two applications, who proposed a power function with a scaling exponent between 0 and 1 for diverse
selection of sentences (or keywords) covering the maximum number of topics of a given document with a
fixed number of items. To adapt these techniques to the problem of diverse merging, from the perspective
of capturing users? information needs in the set of recommended documents, we define here a reward
function enforcing the diverse merging of the lists of document results.
We first estimate the topical similarity of the document subset S
i
= S ? l
i
to the collective query q
(see Subsection 4.2) as r
S
i
:
r
S
i
=
?
d?S
i
Z
?
z=1
p(t
z
|d) ? p(t
z
|q) (3)
We then propose the following reward function f for each S
i
containing relevant documents selected
from l
i
(results of implicit query q
i
), where w
i
is the topical similarity of q
i
to the conversation fragment
(see Equation 2), and ? is an exponent parameter between 0 and 1. This reward function is submodular
because it has the diminishing returns property when r
S
i
increases.
f : r
S
i
? w
i
? r
?
S
i
(4)
1
A function F is submodular if ?A ? B ? T \ t, F (A + t) ? F (A) ? F (B + t) ? F (B) (diminishing returns) and is
monotone non-decreasing if ?A ? B, F (A) ? F (B).
592
The set S is ultimately ranked by maximizing the cumulative reward function R(S) over all the lists,
written as follows:
R(S) =
M
?
i=1
w
i
? r
?
S
i
(5)
The probability of selecting documents from the list of results for q
i
thus depends on w
i
, the topical
similarity of the query to the conversation fragment. This is in contrast to choosing the best representative
document from the list of documents relevant to each query, like in the Watson system, which does not
select more documents for queries with higher weight before considering lower weight ones. Our model
rewards diversity to increase the chance of choosing documents from all the lists of results retrieved for
implicit queries.
4.4 Finding the Optimal Document List
Since R(S) is a monotone submodular function, we propose a greedy algorithm (Alg. 1) to maximize
R(S). If ? = 1, the reward function ignores the diversity constraint, because it does not penalize multiple
selections from the same list l
i
and ranks documents only depending on their similarity to the collective
query and on the weights of implicit queries. However, when 0<?<1, as soon as a document is selected
from the list of results of an implicit query, other documents from the same list start having diminishing
returns as competitors for selection. Decreasing the value of ? increases the impact of the diversity
constraint on ranking documents, which augments the chance of recommending documents from other
document lists.
Input : query set Q of size M with probabilities, set of weights W , set of lists of document results
L with probabilities, number of recommended documents k
Output: set of recommended documents S
S ? ?;
for i = 1 toM step 1 do
S
i
? ?;
end
while |S| ? k do
S ? S ? argmax
d?((?
M
i=1
l
i
)\S)
(g(d)) where g(d) =
?
M
i=1
w
i
? [r
{d}?l
i
+ r
S
i
]
?
;
for i = 1 toM step 1 do
S
i
= l
i
? S;
end
end
return S;
Algorithm 1: Diverse merging of document results for recommendation.
5 Data, Settings and Evaluation Method
The experiments were performed on conversational data from the ELEA Corpus (Emergent LEader Anal-
ysis, Sanchez-Cortes et al. (2012)). Implicit queries were formulated as presented above in Figure 1 using
keywords extracted from each conversation fragment, defined as below (Subsection 5.1). Each subset
of keywords obtained by topical clustering of the keyword set resulted in an implicit query. The lists of
document results for each implicit query were obtained by submitting the query to the Apache Lucene
search engine
2
over the English Wikipedia
3
. These initial lists of results were ultimately merged into
final recommendation lists of documents using the four alternative methods from Figure 1, including the
one we proposed. This section presents the data, system parameters, and evaluation methods used in our
experiments.
2
Available from http://lucene.apache.org.
3
A local copy was downloaded from http://dumps.wikimedia.org.
593
5.1 Conversational Corpus
The ELEA Corpus comprises nearly ten hours of recorded meetings in English and French. Each meeting
consists in a role play game in which participants play survivors of an airplane crash in a mountainous
region. They must rank a list of 12 items with respect to their utility for surviving until they are rescued.
We used from the ELEA corpus four English conversations of around fifteen minutes each, which have
been manually transcribed and segmented at the speaker turn level.
One of the most important issues for a just-in-time document recommender system is to determine
the appropriate timing of the recommendations, and the size of the context to use for computing them.
Here, awaiting future investigations
4
, we decided to make recommendations approximately every two
minutes, at the end of an ongoing speaker turn, and consider as input the words uttered since the previous
recommendation. A segment size of two minutes enables us to collect an appropriate number of words
(neither too small nor too large) in order to extract keywords, model the topics, and formulate implicit
queries. Based on our experience with the ACLD, it also corresponds to an acceptable frequency for
receiving suggestions.
Therefore, our test data comprises 26 two-minute segments, each of them ending at a speaker change.
On average, segments contain 278 words (including stop words). Once topic modeling is applied, the
average number of topics per fragment is 5, with an observed minimum of 3 and a maximum of 9.
5.2 Parameter Settings for Experimentation
As document search is performed over the English Wikipedia, we trained our topic models on this corpus
as well. We used only a subset of it for tractability reasons, i.e. about 125,000 articles as in other studies
(Hoffman et al., 2010). The subset is randomly selected from the entire English Wikipedia. As in
previous studies, we fixed the number of topics at 100 (Boyd-Graber et al., 2009; Hoffman et al., 2010).
The exponent of the submodular function was set to ? = 0.75, as in our diverse keyword extraction
study (Habibi and Popescu-Belis, 2013). This was found to be the best value for diverse merging of lists
of results, as it leads to a reasonable balance between relevance and diversity in the aggregated list of
documents. Of course, if sufficient training data were available, this could be used to optimize ?.
The number of recommended documents was fixed at five in our experiments. This value was selected
again based on user preferences observed with the ACLD. Moreover, this is also the value of the average
number of topics in a conversation fragment, which allows the system to cover on average one result per
topic. Experiments with other values were not carried out due to the cost of evaluation.
5.3 Evaluation Protocol and Metrics
We designed a task that measures the relevance of recommended document lists for each of the test
conversation fragment. Based on validation experiments in our previous work (Habibi and Popescu-
Belis, 2012), the task requires subjects to compare two lists obtained by two different methods. Using a
web browser, the subjects had to read the conversation transcript, answer several control questions about
its content, and then decide which of the two lists provides more relevant documents, with the following
options: the first list is better than the second one; the second is better than the first; both are equally
relevant; or both are equally irrelevant. The position of each system (first or second) was randomized
across the tasks.
The 26 comparison tasks (one for each ELEA fragment) were crowdsourced via Amazon?s Mechanical
Turk as ?human intelligence tasks? (HITs). For each HIT we recruited ten workers, only accepting
those with greater than 95% approval rate and more than 1000 previously approved HITs (qualification
control). We only kept answers from the workers who answered correctly our control questions about
each HIT. Each worker could answer the entire set of 26 HITs, or part of it. We observed that the average
time spent per HIT was around 90 seconds.
4
For instance, they could combine an analysis of non-verbal information to detect ?interruptibility? and of verbal information
to detect topic changes and perform online segmentation (Mohri et al., 2010). Topic changes, however, are not appropriate
moments to make recommendations because it would be useless to recommend documents about a topic that the users no
longer discuss (Jones and Brown, 2004).
594
To consolidate the comparative judgments over a large number of subjects and conversation fragments,
and compute an aggregated score, we applied a qualification control factor to the human judgments (to
reduce the effect of judgments which disagree with the majority vote) and another one to the HITs (to
reduce the impact of undecided HITs on the global scores). This was done by using the PCC-H metric,
defined and validated in our previous work (Habibi and Popescu-Belis, 2012), which provides two scores,
one for each document list, summing up to 100%; a higher value indicates a better list. In addition to
PCC-H, we also provide below (Table 1) the raw preference scores for each comparison, i.e. the number
of times a system was preferred over another one, although PCC-H was shown to be a more reliable
indicator of quality.
6 Experimental Results
We merged and re-ranked the document lists intended to be recommended during a conversation by the
four methods presented above in Section 3 and Figure 1. Three methods merge lists of results from
topically-separated queries: SimM only considers their similarity with the fragment; Round-robin picks
the best document in each list; and our proposal, DivM, considers the diversity and importance of topics.
A fourth method, DivS, uses one query made of all keywords extracted from the conversation fragment,
and ranks the documents using the diverse re-ranking technique proposed by Santos et al. (2010).
Binary comparisons were performed between pairs of techniques, using crowdsourcing over 26 con-
versation fragments of the ELEA Corpus, and aiming to minimize the number of binary comparisons
while still ordering completely the methods according to their perceived quality.
6.1 Diverse Re-ranking vs. Similarity Merging
We first performed a comparison between the top five documents generated by two recommendation
strategies, DivS and SimM, over 26 conversation fragments of the ELEA Corpus. The consolidated rele-
vance score (PCC-H) is 75% for SimM vs. 25% for DivS, as shown in Table 1. These scores indicate the
superiority of SimM over DivS. In other words, separating the mixture of topics of a fragment into mul-
tiple topically-separated queries mitigates the negative effect of the mixture of topics on the suggestions.
6.2 Comparison across Merging Techniques
Binary comparisons were then performed between pairs of merging techniques (SimM, Round-robin,
and DivM), using the same experimental settings. The PCC-H scores are 62% for DivM vs. 38% for
Round-robin, 59% for DivM vs. 41% for SimM, and 56% for Round-robin vs. 44% for SimM, as shown
in Table 1. The scores show that the diverse merging of lists of documents improves recommendations,
and indicate the following high to low ranking: DivM > Round-robin > SimM.
SimM ranks lowest in this ordering, likely because of the ignorance of diversity in the list of results.
Round-robin is second, likely because it disregards the major differences of importance among implicit
queries in a conversation fragment. The results of the comparisons confirm that the DivM technique,
which merges lists of documents by considering the diversity of topics in the list of recommendations,
in proportion to their importance in the conversation, is the most satisfying to the majority of human
subjects.
6.3 Impact of the Topical Diversity of Fragments
To further examine the benefits of our method, we studied its sensitivity to the number of topics in the
conversation fragments. For this purpose, we divided the set of test fragments into two subsets. The first
one (noted ?A? in Table 1) gathers the fragments for which fewer than or exactly five main topics (and
therefore implicit queries) have been computed. The other fragments, with more than five main topics,
form the second subset (noted ?B?). The value of five corresponds to the average number of main topics
per fragment as well as to the number of recommended documents in our experiments.
As shown in Table 1, although there is an improvement in the comparison scores of DivS over SimM
when the number of conveyed topics in the fragments is higher than the number of recommended doc-
uments (subset B), the comparison scores indicate the superiority of SimM over DivS in both cases, and
595
PCC-H relevance score (%) Raw preferences (%)
Compared methods A B A ? B A ? B
(m
1
vs.m
2
) m
1
m
2
m
1
m
2
m
1
m
2
m
1
m
2
SimM vs. DivS 80 20 70 30 75 25 70 30
Round-robin vs. SimM 33 67 68 32 56 44 52 48
DivM vs. Round-robin 64 36 60 40 62 38 58 42
DivM vs. SimM 54 46 60 40 59 41 58 42
Table 1: Comparative scores of the recommended document lists from four methods: DivS, SimM,
Round-robin, and DivM, evaluated by human judges over the ELEA Corpus. Subset A gathers frag-
ments with fewer than or exactly five topics, while subset B gathers all the other fragments. The results
imply the following ranking: DivM > Round-robin > SimM > DivS.
confirm the benefit of the diverse merging techniques. When comparing Round-robin versus SimM, the
scores show the superiority of the former method when the number of conveyed topics in fragments is
higher than the number of recommended documents, because it provides a diverse lists of documents
in which documents relevant to less important topics are not displayed. However, when the number of
topics is smaller than the number of recommendations, SimM provides better results. The reason of the
decrease in the scores of Round-robin is likely the ignorance of the actual importance of the main topics
when ranking documents. Overall, as shown in Table 1, regardless of the number of topics conveyed in
the fragments, DivM always outperforms Round-robin and SimM.
6.4 Example of Document Results
To illustrate how DivM surpasses the other techniques, we consider an example from one of the conver-
sation fragments of the ELEA Corpus. The manual transcript of this conversation fragment is given in
the Appendix A. As described in Section 5, the conversation participants had to select a list of 12 items
vital to survive in winter while waiting to be rescued. The keywords extracted from the manual transcript
of this fragment by our method (Habibi and Popescu-Belis, 2013) are: fire, lighter, cloth, shoe, cold, die,
igloo, walking. As our keyword extraction method was shown to be robust to ASR noise, we only use
here the reference transcripts (Habibi and Popescu-Belis, submitted).
We display the topically-aware implicit queries prepared by our method from this keyword list along
with their weights in Table 2. Then, in Table 3 we show the retrieval results (five highest-ranked
Wikipedia pages) obtained by the four methods using the reference transcript of this fragment.
As shown in Table 2, each implicit query corresponds to one of the main topics of the fragment with
a specific weight. In this example, the main topics spoken in the fragment are about making an igloo,
lightening a fire, having warm clothes, and suitable shoes for walking.
As shown in Table 3, DivS provides two irrelevant documents likely because the single (collective)
query does not separate the mixture of topics in the conversation fragment, and leads to some poor results
(Wikipedia pages) such as ?Cold Fire (Koontz novel)?. SimM slightly improves the results by separating
the discussed topics of the conversation fragment into multiple queries. However, it does not cover all the
Implicit queries Weights
q
1
= {fire, cold, igloo, lighter} w
1
= 0.110
q
2
= {shoe, lighter, walking} w
2
= 0.097
q
3
= {cloth} w
3
= 0.058
q
4
= {die} w
4
= 0.040
q
5
= {igloo} w
5
= 0.026
Table 2: Example of implicit queries built from the keyword list extracted from a sample fragment of the
ELEA Corpus. Each query covers one of the main topics of the fragment and has a different weight.
596
DivS SimM Round-robin DivM
Flint spark lighter Igloo Igloo Igloo
Extended Cold Flint spark lighter Shoe Shoe
Weather Clothing System
Cold Fire (Koontz novel) Lighter Jersey (clothing) Flint spark lighter
Igloo Lighter (barge) Die Hard Jersey (clothing)
Walking Worcester Cold Flint spark lighter Lighter
Storage Warehouse fire
Table 3: Example of retrieved Wikipedia pages from the four different methods tested in this paper.
Results of diverse merging (DivM) appear to cover more topics relevant to the conversation fragment
than other methods. The average ranking (DivM > Round-robin > SimM > DivS) is also observed in
this example.
topics mentioned in the fragment due to mostly focusing on the single topic represented by q
1
. Round-
robin further enhances the results by adding diversity, but as it gives the same level of importance to all
topics, it provides a poor result like ?Die Hard? from a topic of the conversation fragment with a small
weight. The results of DivM appear to be the most useful ones, as they include other articles relevant
to q
1
, q
2
, and q
3
before showing results relevant to the low weight queries q
4
and q
5
. Therefore, in this
example, DivM provides better ranking of documents by covering the largest number of main topics
mentioned in the fragment.
7 Conclusion
We proposed a diverse merging technique for combining lists of documents from multiple topically-
separated implicit queries, prepared using keyword lists obtained from the transcripts of conversation
fragments. Our diverse merging method DivM provides a short, diverse, and relevant list of recommen-
dations, which avoids distracting participants that would consider it during the conversation. We also
compared DivM to existing merging techniques, in terms of comprehensiveness and relevance of the
final recommended list of documents to the conversation fragment. The human judgments collected via
Amazon Mechanical Turk showed that DivM outperforms all other methods.
Moreover, these results emphasized the benefit of splitting the keyword set into multiple topically-
separated queries: the suggested lists of documents from DivS (which accounts for the diversity of results
by re-ranking the documents of a single list) were indeed found less relevant than those from SimM and
the other two methods, which merged results from multiple queries.
In the future, the diverse merging method DivM will be integrated in the ACLD just-in-time retrieval
system for conversational environments, with implicit queries that are prepared from the ASR transcript
of users? conversation. User-oriented evaluation experiments will be conducted. We will also enable the
system to answer explicit queries asked by users, considering contextual factors to improve the relevance
of the answers, which will complement the recommendation functionality based on implicit queries.
Acknowledgments
The authors are grateful to the Swiss National Science Foundation for its support through the IM2 NCCR
on Interactive Multimodal Information Management (2002-2013, see http://www.im2.ch), and to
the Hasler Foundation for its support through the REMUS project (Re-ranking Multiple Search Results
for Just-in-Time Document Recommendation, 2014). The authors also thank the anonymous reviewers
for their helpful suggestions.
References
Rakesh Agrawal, Sreenivas Gollapudi, Alan Halverson, and Samuel Ieong. 2009. Diversifying search results. In
Proceedings of the Second ACM International Conference on Web Search and Data Mining, pages 5?14. ACM.
597
Jagdev Bhogal, Andy Macfarlane, and Peter Smith. 2007. A review of ontology based query expansion. Informa-
tion and Processing Management, 43:866?886.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent Dirichlet Allocation. Journal of Machine
Learning Research, 3:993?1022.
Jonathan Boyd-Graber, Jordan Chang, Sean Gerrish, Chong Wang, and David Blei. 2009. Reading tea leaves:
How humans interpret topic models. In Proceedings of the 23rd Annual Conference on Neural Information
Processing Systems (NIPS), pages 1?9.
Jay Budzik and Kristian J. Hammond. 2000. User interactions with everyday applications as context for just-
in-time information access. In Proceedings of the 5th International Conference on Intelligent User Interfaces
(IUI), pages 44?51. ACM.
Jaime Carbonell and Jade Goldstein. 1998. The use of MMR, diversity-based reranking for reordering documents
and producing summaries. In Proceedings of the 21st annual international ACM SIGIR conference on Research
and development in information retrieval, pages 335?336. ACM.
Claudio Carpineto and Giovanni Romano. 2012. A survey of automatic query expansion in information retrieval.
ACM Computing Surveys (CSUR), 44(1):1?56.
Ben Carterette and Praveen Chandar. 2009. Probabilistic models of ranking novel documents for faceted topic
retrieval. In Proceedings of the 18th ACM conference on Information and knowledge management, pages 1287?
1296.
Philip N. Garner, John Dines, Thomas Hain, Asmaa El Hannani, Martin Karafiat, Danil Korchagin, Mike Lincoln,
Vincent Wan, and Le Zhang. 2009. Real-time ASR from meetings. In Proceedings of Interspeech 2009 (10th
Annual Conference of the International Speech Communication Association), pages 2119?2122.
Maryam Habibi and Andrei Popescu-Belis. 2012. Using crowdsourcing to compare document recommendation
strategies for conversations. In Workshop on Recommendation Utility Evaluation: Beyond RMSE (RUE 2011),
pages 15?20.
Maryam Habibi and Andrei Popescu-Belis. 2013. Diverse keyword extraction from conversations. In Proceedings
of the ACL 2013 (51th Annual Meeting of the Association for Computational Linguistics), pages 651?657.
Maryam Habibi and Andrei Popescu-Belis. submitted. Keyword extraction and clustering for document recom-
mendation in conversations. Manuscript submitted for publication.
Peter E. Hart and Jamey Graham. 1997. Query-free information retrieval. International Journal of Intelligent
Systems Technologies and Applications, 12(5):32?37.
Matthew D. Hoffman, David M. Blei, and Francis Bach. 2010. Online learning for Latent Dirichlet Allocation.
Proceedings of 24th Annual Conference on Neural Information Processing Systems, 23:856?864.
Gareth J.F. Jones and Peter J. Brown. 2004. Context-aware retrieval for ubiquitous computing environments. In
Mobile and ubiquitous information access, pages 227?243. Springer.
Jingxuan Li, Lei Li, and Tao Li. 2012. Multi-document summarization via submodularity. Applied Intelligence,
37(3):420?430.
Hui Lin and Jeff Bilmes. 2011. A class of submodular functions for document summarization. In Proceedings of
the ACL 2011 (49th Annual Meeting of the Association for Computational Linguistics), pages 510?520.
Andrew K. McCallum. 2002. MALLET: A machine learning for language toolkit. http://mallet.cs.umass.edu.
Mehryar Mohri, Pedro Moreno, and Eugene Weinstein. 2010. Discriminative topic segmentation of text and
speech. In International Conference on Artificial Intelligence and Statistics, pages 533?540.
George L. Nemhauser, Laurence A. Wolsey, and Marshall L. Fisher. 1978. An analysis of approximations for
maximizing submodular set functions. Mathematical Programming Journal, 14(1):265?294.
Andrei Popescu-Belis, Erik Boertjes, Jonathan Kilgour, Peter Poller, Sandro Castronovo, Theresa Wilson, Alejan-
dro Jaimes, and Jean Carletta. 2008. The AMIDA Automatic Content Linking Device: Just-in-time document
retrieval in meetings. In Proceedings of MLMI 2008 (Machine Learning for Multimodal Interaction), LNCS
5237, pages 272?283.
598
Andrei Popescu-Belis, Majid Yazdani, Alexandre Nanchen, and Philip N. Garner. 2011. A speech-based just-in-
time retrieval system using semantic search. In Proceedings of 49th Annual Meeting of the ACL, pages 80?85.
Filip Radlinski and Susan Dumais. 2006. Improving personalized web search using result diversification. In Pro-
ceedings of the 29th annual international ACM SIGIR conference on Research and development in information
retrieval, pages 691?692. ACM.
Bradley J. Rhodes and Pattie Maes. 2000. Just-in-time information retrieval agents. IBM Systems Journal,
39(3.4):685?704.
Stephen E. Robertson. 1997. The probability ranking principle in IR. In Karen Sparck Jones and Peter Willett,
editors, Readings in information retrieval, pages 281?286. Morgan Kaufmann Publishers Inc.
Dairazalia Sanchez-Cortes, Oya Aran, Marianne Schmid Mast, and Daniel Gatica-Perez. 2012. A nonverbal
behavior approach to identify emergent leaders in small groups. IEEE Trans. on Multimedia, 14(3):816?832.
Rodrygo L.T. Santos, Craig Macdonald, and Iadh Ounis. 2010. Exploiting query reformulations for web search
result diversification. In Proceedings of the 19th Int. Conf. on the World Wide Web, pages 881?890. ACM.
Sa?ul Vargas, Pablo Castells, and David Vallet. 2012. Explicit relevance models in intent-oriented information
retrieval diversification. In Proceedings of the 35th international ACM SIGIR conference on Research and
development in information retrieval, pages 75?84. ACM.
Jun Wang and Jianhan Zhu. 2009. Portfolio theory of information retrieval. In Proceedings of the 32nd interna-
tional ACM SIGIR conference on Research and development in information retrieval, pages 115?122. ACM.
Shengli Wu and Sally McClean. 2007. Result merging methods in distributed information retrieval with overlap-
ping databases. Information Retrieval, 10(3):297?319.
Cheng Xiang Zhai, William W. Cohen, and John Lafferty. 2003. Beyond independent relevance: methods and
evaluation metrics for subtopic retrieval. In Proceedings of the 26th annual international ACM SIGIR confer-
ence on Research and development in information retrieval, pages 10?17. ACM.
Appendix A. Transcript of a Conversation Fragment from the ELEA Corpus
The following transcript of a conversation fragment (speakers noted A through C) was submitted to the
document recommender system and is exemplified in Section 6.4. The corresponding implicit queries
and recommendations are respectively shown in Tables 2 and 3.
A: okay I start.
B: how how do you want to proceed?
A: I guess -
C: yes what is the most important?
A: I guess fire light.
B: fire lighter?
A: fire, yes. I would say if we had something we can fire with -- I guess that
the lighter is useful in getting some sparks.
B: hopefully.
A: so we can use either newspaper or -- something like that.
C: but again - first it is more important to have enough err clothes.
A: and for me, more important to know where to go. I would say that the compass.
C: I mean -- if you don?t have enough clothes so -- at one point you can --
B: you can die.
C: yes you can -- you will die. so first issue, try to keep yourself alive and
then you can --
A: but -- but you already have some --
B: basics. you everything. you have enormous which is and so is no shoes here.
C: okay that we have shoes so -- okay.
B: because seventy kilometers will take you how many days? err in the snow --
what do you think?
A: two or three.
B: it can be two or three days?
C: yes, but okay you cannot always have fire with you -- but you need always
have clothes with you. I mean it is the only thing that protects you when you are
walking.
B: oh yes. and erm you can make an igloo during the evening. not that cold.
only about five degrees. so lighting a fire is not so important.
C: I guess fire is an extra. I mean it is important but err for me first it is
important that when you keep walking you should be protected.
599
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 651?657,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Diverse Keyword Extraction from Conversations
Maryam Habibi
Idiap Research Institute and EPFL
Rue Marconi 19, CP 592
1920 Martigny, Switzerland
maryam.habibi@idiap.ch
Andrei Popescu-Belis
Idiap Research Institute
Rue Marconi 19, CP 592
1920 Martigny, Switzerland
andrei.popescu-belis@idiap.ch
Abstract
A new method for keyword extraction
from conversations is introduced, which
preserves the diversity of topics that are
mentioned. Inspired from summarization,
the method maximizes the coverage of
topics that are recognized automatically
in transcripts of conversation fragments.
The method is evaluated on excerpts of the
Fisher and AMI corpora, using a crowd-
sourcing platform to elicit comparative
relevance judgments. The results demon-
strate that the method outperforms two
competitive baselines.
1 Introduction
The goal of keyword extraction from texts is to
provide a set of words that are representative of
the semantic content of the texts. In the applica-
tion intended here, keywords are automatically ex-
tracted from transcripts of conversation fragments,
and are used to formulate queries to a just-in-time
document recommender system. It is thus impor-
tant that the keyword set preserves the diversity of
topics from the conversation. While the first key-
word extraction methods ignored topicality as they
were based on word frequencies, more recent me-
thods have considered topic modeling factors for
keyword extraction, but without specifically set-
ting a topic diversity constraint, which is impor-
tant for naturally-occurring conversations.
In this paper, we propose a new method for key-
word extraction that rewards both word similarity,
to extract the most representative words, and word
diversity, to cover several topics if necessary. The
paper is organized as follows. In Section 2 we re-
view existing methods for keyword extraction. In
Section 3 we describe our proposal, which relies
on topic modeling and a novel topic-aware diverse
keyword extraction algorithm. Section 4 presents
the data and tasks for comparing sets of keywords.
In Section 5 we show that our method outperforms
two existing ones.
2 State of the Art in Keyword Extraction
Numerous studies have been conducted to auto-
matically extract keywords from a text or a tran-
scribed conversation. The earliest techniques have
used word frequencies (Luhn, 1957), TFIDF val-
ues (Salton et al, 1975; Salton and Buckley,
1988), and pairwise word co-occurrence frequen-
cies (Matsuo and Ishizuka, 2004) to rank words
for extraction. These approaches do not con-
sider word meaning, so they may ignore low-
frequency words which together indicate a highly-
salient topic (Nenkova and McKeown, 2012).
To improve over frequency-based methods, se-
veral ways to use lexical semantic information
have been proposed. Semantic relations be-
tween words can be obtained from a manually-
constructed thesaurus such as WordNet, or from
Wikipedia, or from an automatically-built the-
saurus using latent topic modeling techniques.
Ye et al (2007) used the frequency of all words
belonging to the same WordNet concept set, while
the Wikifier system (Csomai and Mihalcea, 2007)
relied on Wikipedia links to compute a substitute
to word frequency. Harwath and Hazen (2012)
used topic modeling with PLSA to build a the-
saurus, which they used to rank words based on
topical similarity to the topics of a transcribed con-
versation. To consider dependencies among se-
lected words, word co-occurrence has been com-
bined with PageRank by Mihalcea and Tarau
(2004), and additionally with WordNet by Wang
et al (2007), or with topical information by Z. Liu
et al (2010). However, as shown empirically by
Mihalcea and Tarau (2004) and by Z. Liu et al
(2010) with various co-occurrence windows, such
approaches have difficulties modeling long-range
dependencies between words related to the same
651
topic. Z. Liu et al (2009b) used part-of-speech in-
formation and word clustering techniques, while
F. Liu et al (2009a) added this information to
the TFIDF method so as to consider both word
dependency and semantic information. However,
although they considered topical similarity, the
above methods did not explicitly reward diversity
and might miss secondary topics.
Supervised methods have been used to learn a
model for extracting keywords with various learn-
ing algorithms (Turney, 1999; Frank et al, 1999;
Hulth, 2003). These approaches, however, rely on
the availability of in-domain training data, and the
objective functions they use for learning do not
consider yet the diversity of keywords.
3 Diverse Keyword Extraction
We propose to build a topical representation of
a conversation fragment, and then to select key-
words using topical similarity while also reward-
ing the diversity of topic coverage, inspired by
recent summarization methods (Lin and Bilmes,
2011; Li et al, 2012).
3.1 Representing Topic Information
Topic models such as Probabilistic Latent Seman-
tic Analysis (PLSA) or Latent Dirichlet Allocation
(LDA) can be used to determine the distribution
over the topic z of a word w, noted p(z|w), from a
large amount of training documents. LDA imple-
mented in the Mallet toolkit (McCallum, 2002) is
used in this paper because it does not suffer from
the overfitting issue of PLSA (Blei et al, 2003).
The distribution of each topic z in a given con-
versation fragment t, noted p(z|t), can be com-
puted by summing over all probabilities p(z|w) of
the N words w spoken in the fragment:
p(z|t) = 1N
?
w?t
p(z|w).
3.2 Selecting Keywords
The problem of keyword extraction with maximal
topic coverage is formulated as follows. If a con-
versation fragment t mentions a set of topics Z,
and each word w from the fragment t can evoke a
subset of the topics in Z, then the goal is to find
a subset of unique words S ? t, with |S| ? k,
which maximzes the number of covered topics for
each number of keywords k.
This problem is an instance of the maximum
coverage problem, which isNP -hard. Nemhauser
et al (1978) showed that a greedy algorithm can
find an approximate solution guaranteed to be
within (1 ? 1e ) ' 0.63 of the optimal solutionif the coverage function is submodular and mono-
tone nondecreasing1.
To find a monotone submodular function for
keyword extraction, we used inspiration from re-
cent work on extractive summarization methods
(Lin and Bilmes, 2011; Li et al, 2012), which pro-
posed a square root function for diverse selection
of sentences to cover the maximum number of key
concepts of a given document. The function re-
wards diversity by increasing the gain of selecting
a sentence including a concept that was not yet
covered by a previously selected sentence. This
must be adapted for keyword extraction by defin-
ing an appropriate reward function.
We first introduce rS,z , the topical similarity
with respect to topic z of the keyword set S se-
lected from the fragment t, defined as follows:
rS,z =
?
w?S
p(z|w) ? p(z|t).
We then propose the following reward function
for each topic, where p(z|t) is the importance of
the topic and ? is a parameter between 0 and 1:
f : rS,z ? p(z|t) ? r?S,z .
This is clearly a submodular function with di-
minishing returns as rS,z increases.
Finally, the keywords S ? t, with |S| ? k,
are chosen by maximizing the cumulative reward
function over all the topics, formulated as follows:
R(S) =
?
z?Z
p(z|t) ? r?S,z .
Since R(S) is submodular, the greedy algo-
rithm for maximizing R(S) is shown as Algo-
rithm 1 on the next page, with r{w},z being similar
to rS,z with S = {w}. If ? = 1, the reward func-
tion is linear and only measures the topical simila-
rity of words with the main topics of t. However,
when 0 < ? < 1, as soon as a word is selected
from a topic, other words from the same topic start
having diminishing gains.
4 Data and Evaluation Method
The proposed keyword extraction method was
tested on two conversational corpora, the Fisher
1A function F is submodular if ?A ? B ? T \ t, F (A+
t) ? F (A) ? F (B + t) ? F (B) (diminishing returns) and
is monotone nondecreasing if ?A ? B, F (A) ? F (B).
652
(a) (b)
Please select one of the following options:
1. Image (a) represents the conversation fragment better than (b).
2. Image (b) represents the conversation fragment better than (a).
3. Both (a) and (b) offer a good representation of the conversation.
4. None of (a) and (b) offer a good representation of the conversation.
Figure 1: Example of a HIT based on an AMI discussion about the impact on sales of some features of
remote controls (the conversation transcript is given in the Appendix). The word cloud was generated
using WordleTM from the list produced by the diverse keyword extraction method with ? = 0.75 (noted
D(.75)) for image (a) and by a topic similarity method (TS) for image (b). TS over-represents the topic
?color? by selecting three words related to it, but misses other topics such as ?remote control?, ?losing a
device? and ?buying a device? which are also representative of the fragment.
Input : a given text t, a set of topics Z, the
number of keywords k
Output: a set of keywords S
S ? ?;
while |S| ? k do
S ? S ? {argmaxw?t\S(h(w))where
h(w) =
?
z?Z p(z|t)[r{w},z + rS,z]?};
end
return S;
Algorithm 1: Diverse keyword extraction.
Corpus (Cieri et al, 2004), and the AMI Meeting
Corpus (Carletta, 2007). The former corpus con-
tains about 11,000 topic-labeled telephone conver-
sations, on 40 pre-selected topics (one per con-
versation). We created a topic model using Mal-
let over two thirds of the Fisher Corpus, given its
large number of single-topic documents, with 40
topics. The remaining data is used to build 11
artificial ?conversations? (1-2 minutes long) for
testing, by concatenating 11 times three fragments
about three different topics.
The AMI Corpus contains 171 half-hour meet-
ings about remote control design, which include
several topics each ? so they cannot be directly
used for learning topic models. While selecting
for testing 8 conversation fragments of 2-3 min-
utes each, we trained topic models on a subset of
the English Wikipedia (10% or 124,684 articles).
Following several previous studies, the number of
topics was set to 100 (Boyd-Graber et al, 2009;
Hoffman et al, 2010).
To evaluate the relevance (or representative-
ness) of extracted keywords with respect to a
conversation fragment, we designed comparison
tasks. In each task, a fragment is shown, followed
by three control questions about its content, and
then by two lists of nine keywords each, from two
different extraction methods. To improve readabil-
ity, the keyword lists are presented to the judges
using a word cloud representation generated by
WordleTM (http://www.wordle.net), in which the
words ranked higher are emphasized in the word
cloud (see example in Figure 1). The judges had
to read the conversation transcript, answer the con-
trol questions, and then decide which word cloud
better represents the content of the conversation.
The tasks were crowdsourced via Amazon?s
Mechanical Turk (AMT) as ?human intelligence
tasks? (HITs). One of them is exemplified in Fig-
ure 1, without the control questions, and the re-
spective conversation transcript is given in the Ap-
pendix. Ten workers were recruited for each cor-
pus. An example of judgment counts for each of
the 8 AMI HITs comparing two methods is shown
in Table 1. After collecting judgments, the com-
parative relevance values were computed by first
applying a qualification control factor to the hu-
man judgments, and then averaging results over
all judgments (Habibi and Popescu-Belis, 2012).
Moreover, to verify the diversity of the key-
653
0.65
0.7
0.75
0.8
0.85
0.9
0.95
1
1.05
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15
D(0.50)
D(0.75)
TS
WF
Ranking 
??NDCG val
ue
s 
Figure 2: Average ?-NDCG over the 11 conversations from the Fisher Corpus, for 1 to 15 extracted
keywords.
word set, we use the ?-NDCG measure (Clarke
et al, 2008) proposed for information retrieval,
which rewards a mixture of relevance and diver-
sity ? with equal weights when ? = .5 as set here.
We only apply ?-NDCG to the three-topic con-
versation fragments from the Fisher Corpus, rel-
evance of a keyword being set to 1 when it be-
longs to the fragment corresponding to the topic.
A higher value indicates that keywords are more
uniformly distributed across the three topics.
5 Experimental Results
We have compared several versions of the diverse
keyword extraction method, noted D(?), for ? ?
{.5, .75, 1}, with two other methods. The first
one uses only word frequency (not including stop-
words) and is noted WF. We did not use TFIDF
because it sets low weights on keywords that are
repeated in many fragments but which are never-
theless important to extract. The second method is
based on topical similarity (noted TS) but does not
specifically enforce diversity (Harwath and Hazen,
2012). In fact TS coincides with D(1), so it is
noted TS. As the relevance of keywords for D(.5)
was already quite low, we did not test lower values
of ?. Similarly, we did not test additional values
of ? above .5 because the resulting word lists were
very similar to tested values.
First of all, we compared the four methods with
respect to the diversity constraint over the con-
HIT A B C D E F G H
TS more relevant 4 1 1 1 2 2 1 1
D(.75) more rel. 4 1 8 9 6 6 6 8
Both relevant 2 5 1 0 2 2 3 1
Both irrelevant 0 3 0 0 0 0 0 0
Table 1: Number of answers for each of the four
options of the comparative evaluation task, from
ten human judges. The 8 HITs compare the D(.75)
and TS methods on 8 AMI HITs.
Corpus Compared methods Relevance (%)
(m1 vs. m2) m1 m2
Fisher D(.75) vs. TS 68 32
TS vs. WF 82 18
WF vs. D(.5) 95 5
AMI D(.75) vs. TS 78 22
TS vs. WF 60 40
WF vs. D(.5) 78 22
Table 2: Comparative relevance scores of keyword
extraction methods based on human judgments.
catenated fragments of the Fisher Corpus, by us-
ing ?-NDCG to measure how evenly the extracted
keywords were distributed across the three topics.
Figure 2 shows results averaged over 11 conversa-
tions for various sizes of the keyword set (1?15).
The average ?-NDCG values for D(.75) and D(.5)
are similar, and clearly higher than WF and TS
for all ranks (except, of course, for a single key-
word). The values for TS are quite low, and only
increase for a large number of keywords, demon-
strating that TS does not cope well with topic di-
versity, but on the contrary first selects keywords
from the dominant topic. The values for WF are
more uniform as it does not consider topics at all.
To measure the overall representativeness of
keywords, we performed binary comparisons be-
tween the outputs of each method, using crowd-
sourcing, over 11 fragments from the Fisher Cor-
pus and 8 fragments from AMI. The goal is to
rank the methods, so we only report here on
the comparisons required for complete ordering.
AMT workers compared two lists of nine key-
words each, with four options: X more represen-
tative or relevant than Y , or vice-versa, or both
relevant, or both irrelevant. Table 1 shows the
judgments collected when comparing the output of
D(.75) with TS on the AMI Corpus. Workers dis-
agreed for the first two HITs, but then found that
the keywords extracted by D(.75) were more rep-
resentative compared to TS. The consolidated rel-
654
evance (Habibi and Popescu-Belis, 2012) is 78%
for D(.75) vs. 22% for TS.
The averaged relevance values for all compar-
isons needed to rank the four methods are shown
in Table 2 separately for the Fisher and AMI Cor-
pora. Although the exact differences vary, the hu-
man judgments over the two corpora both indi-
cate the following ranking: D(.75) > TS > WF >
D(.5). The optimal value of ? is thus around .75,
and with this value, our diversity-aware method
extracts more representative keyword sets than TS
and WF. The differences between methods are
larger for the Fisher Corpus, due to the artificial
fragments that concatenate three topics, but they
are still visible on the natural fragments of the
AMI Corpus. The low scores of D(.5) are found
to be due, upon inspection, to the low relevance
of keywords. In particular, the comparative rele-
vance of D(.75) vs. D(.5) on the Fisher Corpus is
very large (96% vs. 4%).
6 Conclusion
The diverse keyword extraction method with ? =
.75 provides the keyword sets that are judged most
representative of the conversation fragments (two
conversational datasets) by a large number of hu-
man judges recruited via AMT, and has the high-
est ?-NDCG value. Therefore, enforcing both rel-
evance and diversity brings an effective improve-
ment to keyword extraction.
Setting ? for a new dataset remains an issue,
and requires a small development data set. How-
ever, preliminary experiments with a third dataset
showed that ? = .75 remains a good value.
In the future, we will use keywords to re-
trieve documents from a repository and recom-
mend them to conversation participants by formu-
lating topically-separate queries.
Appendix: Conversation transcript of
AMI ES2005a meeting (00:00:5-00:01:52)
The following transcript of a four-party conversa-
tions (speakers noted A through D) was submitted
to our keyword extraction method and a baseline
one, generating respectively the two word clouds
shown in Figure 1.
A: The only the only remote controls
I?ve used usually come with the
television, and they?re fairly basic.
So uh
D: Yeah. Yeah.
C: Mm-hmm.
D: Yeah, I was thinking that as well,
I think the the only ones that I?ve seen
that you buy are the sort of one for
all type things where they?re, yeah. So
presumably that might be an idea to
C: Yeah the universal ones. Yeah.
A: Mm. But but to sell it for twenty
five you need a lot of neat features.
For sure.
D: put into.
C: Yeah.
D: Yeah, yeah. Uh ?cause I mean, what
uh twenty five Euros, that?s about I
dunno, fifteen Pounds or so?
C: Mm-hmm, it?s about that.
D: And that?s quite a lot for a remote
control.
A: Yeah, yeah.
C: Mm. Um well my first thoughts
would be most remote controls are grey
or black. As you said they come with
the TV so it?s normally just your basic
grey black remote control functions, so
maybe we could think about colour? Make
that might make it a bit different from
the rest at least. Um, and as you say,
we need to have some kind of gimmick, so
um I thought maybe something like if you
lose it and you can whistle, you know
those things?
D: Uh-huh. Mm-hmm. Okay. The the
keyrings, yeah yeah. Okay, that?s cool.
C: Because we always lose our remote
control.
B: Uh yeah uh, being as a Marketing
Expert I will like to say like before
deciding the cost of this remote control
or any other things we must see the
market potential for this product like
what is the competition in the market?
What are the available prices of the
other remote controls in the prices?
What speciality other remote controls
are having and how complicated it is to
use these remote controls as compared to
other remote controls available in the
market.
D: Okay.
B: So before deciding or before
finalising this project, we must discuss
all these things, like and apart from
this, it should be having a good look
also, because people really uh like
to play with it when they are watching
movies or playing with or playing with
their CD player, MP three player like
any electronic devices. They really
want to have something good, having a
good design in their hands, so, yes, all
this.
Acknowledgments
The authors are grateful to the Swiss National Sci-
ence Foundation for its financial support through
the IM2 NCCR on Interactive Multimodal Infor-
mation Management (see www.im2.ch).
655
References
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet Allocation. Journal of Ma-
chine Learning Research, 3:993?1022.
Jonathan Boyd-Graber, Jordan Chang, Sean Gerrish,
Chong Wang, and David Blei. 2009. Reading tea
leaves: How humans interpret topic models. In Pro-
ceedings of the 23rd Annual Conference on Neural
Information Processing Systems (NIPS).
Jean Carletta. 2007. Unleashing the killer corpus:
Experiences in creating the multi-everything AMI
Meeting Corpus. Language Resources and Evalu-
ation Journal, 41(2):181?190.
Christopher Cieri, David Miller, and Kevin Walker.
2004. The Fisher Corpus: a resource for the next
generations of speech-to-text. In Proceedings of 4th
International Conference on Language Resources
and Evaluation (LREC), pages 69?71.
Charles L. A. Clarke, Maheedhar Kolla, Gordon V.
Cormack, Olga Vechtomova, Azin Ashkan, Stefan
Bu?ttcher, and Ian MacKinnon. 2008. Novelty and
diversity in information retrieval evaluation. In Pro-
ceedings of the 31st annual international ACM SI-
GIR conference on Research and development in in-
formation retrieval, pages 659?666.
Andras Csomai and Rada Mihalcea. 2007. Linking
educational materials to encyclopedic knowledge.
Frontiers in Artificial Intelligence and Applications,
158:557.
Eibe Frank, Gordon W. Paynter, Ian H. Witten, Carl
Gutwin, and Craig G. Nevill-Manning. 1999.
Domain-specific keyphrase extraction. In Proceed-
ings of the 16th International Joint Conference on
Artificial Intelligence (IJCAI 1999), pages 668?673,
Stockholm, Sweden.
Maryam Habibi and Andrei Popescu-Belis. 2012. Us-
ing crowdsourcing to compare document recom-
mendation strategies for conversations. In Work-
shop on Recommendation Utility Evaluation: Be-
yond RMSE (RUE 2011), page 15.
David Harwath and Timothy J. Hazen. 2012. Topic
identification based extrinsic evaluation of summa-
rization techniques applied to conversational speech.
In Proceedings of International Conference on
Acoustics, Speech and Signal Processing (ICASSP),
pages 5073?5076. IEEE.
Matthew D. Hoffman, David M. Blei, and Francis
Bach. 2010. Online learning for Latent Dirichlet
Allocation. Proceedings of 24th Annual Conference
on Neural Information Processing Systems, 23:856?
864.
Anette Hulth. 2003. Improved automatic keyword ex-
traction given more linguistic knowledge. In Pro-
ceedings of the Conference on Empirical Methods
in Natural Language Processing (EMNLP 2003),
pages 216?223, Sapporo, Japan.
Jingxuan Li, Lei Li, and Tao Li. 2012. Multi-
document summarization via submodularity. Ap-
plied Intelligence, 37(3):420?430.
Hui Lin and Jeff Bilmes. 2011. A class of submodular
functions for document summarization. In Proceed-
ings of the 49th Annual Meeting of the ACL.
Feifan Liu, Deana Pennell, Fei Liu, and Yang Liu.
2009a. Unsupervised approaches for automatic key-
word extraction using meeting transcripts. In Pro-
ceedings of the 2009 Annual Conference of the
North American Chapter of the ACL (HLT-NAACL),
pages 620?628.
Zhiyuan Liu, Wenyi Huang, Yabin Zheng, and
Maosong Sun. 2009b. Clustering to find exemplar
terms for keyphrase extraction. In Proceedings of
the 2009 Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP 2009), pages
257?266.
Zhiyuan Liu, Wenyi Huang, Yabin Zheng, and
Maosong Sun. 2010. Automatic keyphrase extrac-
tion via topic decomposition. In Proceedings of the
2010 Conference on Empirical Methods in Natural
Language Processing (EMNLP 2010), pages 366?
376.
Hans Peter Luhn. 1957. A statistical approach to
mechanized encoding and searching of literary in-
formation. IBM Journal of Research and Develop-
ment, 1(4):309?317.
Yutaka Matsuo and Mitsuru Ishizuka. 2004. Key-
word extraction from a single document using word
co-occurrence statistical information. International
Journal on Artificial Intelligence Tools, 13(1):157?
169.
Andrew K. McCallum. 2002. MALLET:
A machine learning for language toolkit.
http://mallet.cs.umass.edu.
Rada Mihalcea and Paul Tarau. 2004. TextRank:
Bringing order into texts. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP 2004), pages 404?411,
Barcelona.
George L. Nemhauser, Laurence A. Wolsey, and Mar-
shall L. Fisher. 1978. An analysis of approxi-
mations for maximizing submodular set functions.
Mathematical Programming Journal, 14(1):265?
294.
Ani Nenkova and Kathleen McKeown, 2012. A Survey
of Text Summarization Techniques, chapter 3, pages
43?76. Springer.
Gerard Salton and Christopher Buckley. 1988. Term-
weighting approaches in automatic text retrieval.
Information Processing and Management Journal,
24(5):513?523.
656
Gerard Salton, Chung-Shu Yang, and Clement T. Yu.
1975. A theory of term importance in automatic text
analysis. Journal of the American Society for Infor-
mation Science, 26(1):33?44.
Peter Turney. 1999. Learning to extract keyphrases
from text. Technical Report ERB-1057, National
Research Council Canada (NRC).
Jinghua Wang, Jianyi Liu, and Cong Wang. 2007.
Keyword extraction based on PageRank. In Ad-
vances in Knowledge Discovery and Data Mining
(Proceedings of PAKDD 2007), LNAI 4426, pages
857?864. Springer-Verlag, Berlin.
Shiren Ye, Tat-Seng Chua, Min-Yen Kan, and Long
Qiu. 2007. Document concept lattice for text un-
derstanding and summarization. Information Pro-
cessing and Management, 43(6):1643?1662.
657
