Proceedings of the NAACL HLT Student Research Workshop and Doctoral Consortium, pages 7?12,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Solving the ?Who?s Mark Johnson? Puzzle:
Information Extraction Based Cross Document Coreference
Jian Huang?? Sarah M. Taylor? Jonathan L. Smith? Konstantinos A. Fotiadis? C. Lee Giles?
?Information Sciences and Technology
Pennsylvania State University, University Park, PA 16802, USA
??Advanced Technology Office, Lockheed Martin IS&GS
?4350 N. Fairfax Drive, Suite 470, Arlington, VA 22203, USA
?230 Mall Blvd, King of Prussia, PA 19406, USA
Abstract
Cross Document Coreference (CDC) is the
problem of resolving the underlying identity
of entities across multiple documents and is a
major step for document understanding.
We develop a framework to efficiently
determine the identity of a person based on
extracted information, which includes unary
properties such as gender and title, as well as
binary relationships with other named entities
such as co-occurrence and geo-locations.
At the heart of our approach is a suite of
similarity functions (specialists) for matching
relationships and a relational density-based
clustering algorithm that delineates name
clusters based on pairwise similarity. We
demonstrate the effectiveness of our methods
on the WePS benchmark datasets and point
out future research directions.
1 Introduction
The explosive growth of web data offers users both
the opportunity and the challenge to discover and
integrate information from disparate sources. As
alluded to in the title, a search query of the common
name ?Mark Johnson? refers to as many as 70
namesakes in the top 100 search results from the
Yahoo! search engine, only one of whom is the
Brown University professor and co-author of an
ACL 2006 paper (see experiments). Cross document
coreference (CDC) (Bagga and Baldwin, 1998) is a
distinct technology that consolidates named entities
across documents according to their real referents.
Despite the variety of styles and content in different
text, CDC can break the boundaries of documents
and cluster those mentions referring to the same
?Contact author: jhuang@ist.psu.edu
Mark Johnson. As unambiguous person references
are key to many tasks, e.g. social network analysis,
this work focuses on person named entities. The
method can be later extended to organizations.
We highlight the key differences between our
proposed CDC system with past person name
search systems. First, we seek to transcend the
simple bag of words approaches in earlier CDC
work by leveraging state-of-the-art information
extraction (IE) tools for disambiguation. The
main advantage is that our IE based approach has
access to accurate information such as a person?s
work titles, geo-locations, relationships and other
attributes. Traditional IR approaches, on the other
hand, may naively use the terms in a document
which can significantly hamper accuracy. For
instance, an article about Hillary Clinton may
contain references to journalists, politicians who
make comments about her. Even with careful word
selection, such textual features may still confuse the
disambiguation system about the true identity of the
person. The information extraction process in our
work can thus be regarded as an intelligent feature
selection step for disambiguation. Second, after
coreferencing, our system not only yields clusters
of documents, but also structured information
which is highly useful for automated document
understanding and data mining.
We review related work on CDC next and
describe our approach in Section 3. The methods
are evaluated on benchmark datasets in Section 4.
We discuss directions for future improvement in
Section 5 and conclude in Section 6.
2 Related Work
There is a long tradition of work on the within
document coreference (WDC) problem in NLP,
7
which links named entities with the same referent
within a document into a WDC chain. State-of-
the-art WDC systems, e.g. (Ng and Cardie, 2001),
leverage rich lexical features and use supervised
and unsupervised machine learning methods.
Research on cross document coreference began
more recently. (Bagga and Baldwin, 1998) proposed
a CDC system to merge the WDC chains using the
Vector Space Model on the summary sentences.
(Gooi and Allan, 2004) simplified this approach by
eliminating the WDC module without significant
deterioration in performance. Clustering approaches
(e.g. hierarchical agglomerative clustering (Mann
and Yarowsky, 2003)) have been commonly used
for CDC due to the variety of data distributions
of different names. Our work goes beyond the
simple co-occurrence features (Bagga and Baldwin,
1998) and the limited extracted information (e.g.
biographical information in (Mann and Yarowsky,
2003) that is relatively scarce in web data) using
the broad range of relational information with the
support of information extraction tools. There
are also other related research problems. (Li et
al., 2004) solved the robust reading problem by
adopting a probabilistic view on how documents are
generated and how names are sprinkled into them.
Our previous work (Huang et al, 2006) resolved
the author name ambiguity problem based on the
metadata records extracted from academic papers.
3 Methods
The overall framework of our CDC system works
as follows. Given a document, the information
extraction tool first extracts named entities and
constructs WDC chains. It also creates linkages
(relationships) between entities. The similarity
between a pair of relationships in WDC chains
is measured by an awakened similarity specialist
and the similarity between two WDC chains is
determined by the mixture of awakened specialists?
predictions. Finally, a density-based clustering
method generates clusters corresponding to real
world entities. We describe these steps in detail.
3.1 Entity and Relationship Extraction
Given a document, an information extraction
tool is first used to extract named entities and
perform within document coreference. Hence,
named entities in each document are divided into
a set of WDC chains, each chain corresponding
to one real world entity. In addition, state-of-
the-art IE tools are capable of creating relational
information between named entities. We use an
IE tool AeroText1 (Taylor, 2004) for this purpose.
Besides the attribute information about the person
named entity (first/middle/last names, gender,
mention, etc), AeroText also extracts relationship
information between named entities, such as
Family, List, Employment, Ownership, Citizen-
Resident-Religion-Ethnicity, etc, as specified in the
Automatic Content Extraction (ACE) evaluation.
The input to the CDC system is a set of WDC chains
(with relationship information stored in them) and
the CDC task is to merge these WDC chains2.
3.2 Similarity Features
We design a suite of similarity functions to
determine whether the relationships in a pair of
WDC chains match, divided into three groups:
Text similarity. To decide whether two names
in the co-occurrence or family relationship match,
we use SoftTFIDF (Cohen et al, 2003), which has
shown best performance among various similarity
schemes tested for name matching. SoftTFIDF is
a hybrid matching scheme that combines the token-
based TFIDF with the Jaro-Winkler string distance
metric. This permits inexact matching of named
entities due to name variations, spelling errors, etc.
Semantic similarity. Text or syntactic similarity is
not always sufficient for matching relationships. For
instance, although the mentions ?U.S. President?
and ?Commander-in-chief? have no textual overlap,
they are semantically highly related as they can be
synonyms. We use WordNet and the information
theory based JC semantic distance (Jiang and
Conrath, 1997) to measure the semantic similarity
between concepts in relationships such as mention,
employment, ownership and so on.
1AeroText is a text mining application for content
analysis, with main focus on information extraction
including entity extraction and intrasource link analysis
(see http://en.wikipedia.org/wiki/AeroText).
2We make no distinctions whether WDC chains are
extracted from the same document. Indeed, the CDC system
can correct the WDC errors due to lack of information for
merging named entities within a document.
8
Other rule-based similarity. Several other
cases require special treatment. For example, the
employment relationships of Senator and D-N.Y.
should match based on domain knowledge. Also,
we design rule-based similarity functions to handle
nicknames (Bill and William), acronyms (COLING
for International Conference on Computational
Linguistics), and geographical locations3.
3.3 Learning a Similarity Matrix
After the similarity features between a pair of
WDC chains are computed, we need to compute
the pairwise distance metric for clustering. (Cohen
et al, 2003) trained a binary SVM model and
interpreted its confidence in predicting the negative
class as the distance metric. In our case of using
information extraction results for disambiguation,
however, only some of the similarity features are
present based on the availability of relationships
in two WDC chains. Therefore, we treat each
similarity function as a subordinate predicting
algorithm (called specialist) and utilize the
specialist learning framework (Freund et al, 1997)
to combine the predictions. Here, a specialist is
awake only when the same relationships are present
in two WDC chains. Also, a specialist can refrain
from making a prediction for an instance if it is
not confident enough. In addition to the similarity
scores, specialists have different weights, e.g. a
match in a family relationship is considered more
important than in a co-occurrence relationship.
The Specialist Exponentiated Gradient (SEG)
(Freund et al, 1997) algorithm is adopted to learn
to mix the specialists? prediction. Given a set
of T training instances {xt} (xt,i denotes the
i-th specialist?s prediction), the SEG algorithm
minimizes the square loss of the outcome y? in an
online manner (Algorithm 1). In each learning
iteration, SEG first predict y?t using the set of awake
experts Et with respect to instance xt. The true
outcome yt (1 for coreference and 0 otherwise) is
then revealed and square loss L is incurred. SEG
then updates the weight distribution p accordingly.
To sum up, the similarity between a pair of
3Though a rich set of similarity features has been built for
matching the relationships, they may not encompass all possible
cases in real world documents. The goal of this work, however,
is to focus on the algorithms instead of knowledge engineering.
Algorithm 1 SEG (Freund et al, 1997)
Input: Initial weight distribution p1;
learning rate ? > 0; training set {xt}
1: for t=1 to T do
2: Predict using:
y?t =
?
i?Et ptixt,i?
i?Et pti
(1)
3: Observe true label yt and incur square loss
L(y?t, yt) = (y?t ? yt)2
4: Update weight distribution: for i ? Et
pt+1i = ptie?2?xt,i(y?t?yt)
?
j?Et ptj?
j?Et ptje?2?xt,i(y?t?yt)
pt+1i = pti, otherwise
5: end for
Output: Model p
WDC chains wi and wj can be represented in a
similarity matrix R, with ri,j computed by the SEG
prediction step using the learned weight distribution
p (Equation 1). A relational clustering algorithm
then clusters entities using R, as we introduce next.
3.4 Relational Clustering
The set of WDC chains to be clustered are
represented by a relational similarity matrix. Most
of the work in clustering, however, is only capable
of clustering numerical object data (e.g. K-means).
Relational clustering algorithms, on the other hand,
cluster objects based on the less direct measurement
of similarity between object pairs. We choose to
use a density based clustering algorithm DBSCAN
(Ester et al, 1996) mainly for two reasons.
First, most clustering algorithm require the
number of clusters K as an input parameter. The
optimal K can apparently vary greatly for names
with different frequency and thus is a sensitive
parameter. Even if a cluster validity index is used
to determine K, it usually requires running the
underlying clustering algorithm multiple times
and hence is inefficient for large scale data.
DBSCAN, as a density based clustering method,
only requires density parameters such as the
radius of the neighborhood ? that are universal for
different datasets. As we show in the experiment,
9
density parameters are relatively insensitive for
disambiguation performance.
Second, the distance metric in relational space
may be non-Euclidean, rendering many clustering
algorithms ineffective (e.g. single linkage clustering
algorithm is known to generate chain-shaped
clusters). Density-based clustering, on the other
hand, can generate clusters of arbitrary shapes since
only objects in dense areas are placed in a cluster.
DBSCAN induces a density-based cluster by
the core objects, i.e. objects having more than
a specified number of other data objects in their
neighborhood of size ?. In each clustering step, a
seed object is checked to determine whether it?s a
core object and if so, it induces other points of the
same cluster using breadth first search (otherwise
it?s considered as a noise point). In interest of
space, we refer readers to (Ester et al, 1996) for
algorithmic details of DBSCAN and now turn
our attention to evaluating the disambiguation
performance of our methods.
4 Experiments
We first formally define the evaluation metrics,
followed by the introduction to the benchmark test
sets and the system?s performance.
4.1 Evaluation Measures
We evaluate the performance of our method using
the standard purity and inverse purity clustering
metrics. Let a set of clusters C = {C1, ..., Cs}
denote the system?s output and a set of categories
D = {D1, ..., Dt} be the gold standard. Both C and
D are partitions of the WDC chains {w1, ..., wn}
(n = ?i |Ci| = ?j |Dj |). First, the precision of
a cluster Ci w.r.t. a category Dj is defined as,
Precision(Ci, Dj) = |Ci ?Dj ||Ci|
Purity is defined as the weighted average of the
maximum precision achieved by the clusters on one
of the categories,
Purity(C,D) =
s?
i=1
|Ci|
n maxj Precision(Ci, Dj)
Hence purity penalizes putting noise WDC chains in
a cluster. Trivially, the maximum purity (i.e. 1) can
be achieved by making one cluster per WDC chain
(referred to as the one-in-one baseline).
Reversing the role of clusters and categories,
Inverse purity(C,D) def= Purity(D, C). Inverse
Purity penalizes splitting WDC chains belonging
to the same category into different clusters. The
maximum inverse purity can be achieved by putting
all chain in one cluster (all-in-one baseline).
Purity and inverse purity are similar to the
precision and recall measures commonly used
in information retrieval. There is a tradeoff
relationship between the two and their harmonic
mean F0.5 is used for performance evaluation.
4.2 Datasets
We evaluate our methods using the benchmark
test collection from the ACL SemEval-2007 web
person search task (WePS hereafter) (Artiles et al,
2007). The test collection consists of three sets of
documents for 10 different names, sampled from
the English Wikipedia (famous people), participants
of the ACL 2006 conference (computer scientists)
and common names from the US Census data,
respectively. For each ambiguous name, the top 100
documents retrieved from the Yahoo! Search API
were annotated by human annotators according to
the actual entity of the name. This yields on average
45 different real world entities per set and about 3k
documents in total.
We note that the annotation in WePS makes the
simplifying assumption that each document refers to
only one real world person among the namesakes
in question. The CDC task in the perspective of
this paper, however, is to merge the WDC chains
rather than documents. Hence in our evaluation,
we adopt the document label to annotate the WDC
chain from the document that corresponds to the
person name search query. Despite the difference,
the results of the one-in-one and all-in-one baselines
are almost identical to those reported in the WePS
evaluation (F0.5 = 0.61, 0.40 respectively). Hence
the performance reported here is comparable to the
official evaluation results (Artiles et al, 2007).
4.3 Experiment Results
We computed the similarity features from the WDC
chains extracted from the WePS training data and
subsampled the non-coreferent pairs to generate a
10
Table 1: Cross document coreference performance
(macro-averaged scores, I-Pur denotes inverse purity).
Test set Method Purity I-Pur F0.5
Wikipedia AT-CDC 0.684 0.725 0.687
ACL-06 AT-CDC 0.792 0.657 0.712
US Census AT-CDC 0.772 0.700 0.722
Global
Average
AT-CDC 0.749 0.695 0.708
One-in-one 1.000 0.482 0.618
All-in-one 0.279 1.000 0.389
training set of around 32k pairwise instances. We
then used the SEG algorithm to learn the weight
distribution model. The macro-averaged cross
document coreference results on the WePS test
sets are reported in Table 1. The F0.5 score of our
CDC system (AT-CDC) is 0.708, comparable to the
test results of the first tier systems in the official
evaluation. The two baselines are also included.
Because the test set is very ambiguous (on average
only two documents per real world entity), the
one-in-one baseline has relatively high F0.5 score.
The Wikipedia, ACL06 and US Census sets
have on average 56, 31 and 50 entities per name
respectively. We notice that as the data set becomes
more ambiguous, purity decreases implying
it?s harder for the system to discard irrelevant
documents from a cluster. The other case is true
for inverse purity. In particular, we are interested in
how the coreference performance changes with the
number of entities per name (which can be viewed
as the ambiguity level of a data set). This is shown
in Figure 1. We observe that in general the harmonic
mean of the purity is fairly stable across different
number of entities per dataset (generally within
the band between 0.6 and 0.8). This is important
because the system?s performance does not vary
greatly with the underlying data characteristics.
There is a particular name (with only one underlying
referent) that appears to be an outlier in performance
in Figure 1. After examining the extraction results,
we notice that the extracted relationships refer to
the same person?s employment, coauthors and geo-
locations. The generated CDC clusters correctly
reflect the different aspects of the person but the
system is unable to link them together due to the
lack of information for merging. This motivates us
to further improve performance in future work.
Figure 2 shows how the coreference performance
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  10  20  30  40  50  60  70  80  90  100
F(0
.5)
Number of entities per name 
Figure 1: Coreference performance for names with
different number of real world entities.
changes with different density parameter ?. We
observe that as we increase the size of the ?
neighborhood, inverse purity increases indicating
that more correct coreference decisions are made.
On the other hand, purity decreases as more noise
WDC chains appear in clusters. Due to this tradeoff
relationship, the F score is fairly stable with a wide
range of ? values and hence the density parameter is
rather insensitive (compared to, say, the number of
clusters K).
5 Future Work
We see several opportunities to improve the
coreference performance of the proposed methods.
First, though the system?s performance compares
favorably with the WePS submissions, we observe
that purity is higher than inverse purity, indicating
that the system finds it more difficult to link
coreferent documents than to discard noise from
0.3 0.35 0.4 0.45 0.5 0.550
0.2
0.4
0.6
0.8
1
?
DBSCAN with different parameter settings
Purity
Inverse Purity
F(0.5)
Figure 2: Coreference performance with different ?.
11
clusters. Thus coreferencing based solely on the
information generated by an information extraction
tool may not always be sufficient. For one, it
remains a huge challenge to develop a general
purpose information extraction tool capable of
applying to web documents with widely different
formats, styles, content, etc. Also, even if the
extraction results are perfect, relationships extracted
from different documents may be of different
types (family memberships vs. geo-locations) and
cannot be directly matched against one another. We
are exploring several methods to complement the
extracted relationships using other information:
? Context-aided CDC. The context where an named
entity is extracted can be leveraged for coreference.
The bag of words in the context tend to be less noisy
than that from the entire document. Moreover, we
can use noun phrase chunkers to extract base noun
phrases from the context. These word or phrase level
features can serve as a safenet when the IE tool fails.
? Topic-based CDC. Similar to (Li et al, 2004),
document topics can be used to ameliorate the
sparsity problem. For example, the topics Sport
and Education are important cues for differentiating
mentions of ?Michael Jordan?, which may refer to a
basketball player, a computer science professor, etc.
Second, as noted in the top WePS run (Chen and
Martin, 2007), feature development is important in
achieving good coreference performance. We aim
to improve the set of similarity specialists in our
system by leveraging large knowledge bases.
Moreover, although the CDC system is developed
in the web person search context, the methods are
also applicable to other scenarios. For instance,
there is tremendous interest in building structured
databases from unstructured text such as enterprise
documents and news articles for data mining, where
CDC is a key step for ?understanding? documents
from disparate sources. We plan to continue our
investigations along these lines.
6 Conclusions
We have presented and implemented an information
extraction based Cross Document Coreference
(CDC) system that employs supervised and
unsupervised learning methods. We evaluated
the proposed methods with experiments on a
large benchmark disambiguation collection, which
demonstrate that the proposed methods compare
favorably with the top runs in the SemEval
evaluation. We believe that by incorporating
information such as context and topic, besides the
extracted relationships, the performance of the CDC
can be further improved. We have outlined research
plans to address this and several other issues.
References
Javier Artiles, Julio Gonzalo, and Satoshi Sekine. 2007.
The SemEval-2007 WePS evaluation: Establishing a
benchmark for the web people search task. In Proc 4th
Int?l Workshop on Semantic Evaluations (SemEval).
Amit Bagga and Breck Baldwin. 1998. Entity-based
cross-document coreferencing using the vector space
model. In Proc. of 36th ACL and 17th COLING.
Ying Chen and James Martin. 2007. Towards robust
unsupervised personal name disambiguation. In
Proceedings of EMNLP and CoNLL, pages 190?198.
William W. Cohen, Pradeep Ravikumar, and Stephen E.
Fienberg. 2003. A comparison of string distance
metrics for name-matching tasks. In Proc. of IJCAI
Workshop on Information Integration on the Web.
Martin Ester, Hans-Peter Kriegel, Jorg Sander, and
Xiaowei Xu. 1996. A density-based algorithm for
discovering clusters in large spatial databases with
noise. In Proceedings of 2nd KDD, pages 226 ? 231.
Yoav Freund, Robert E. Schapire, Yoram Singer, and
Manfred K. Warmuth. 1997. Using and combining
predictors that specialize. In Proceedings of 29th ACM
symposium on Theory of computing (STOC).
Chung H. Gooi and James Allan. 2004. Cross-document
coreference on a large scale corpus. In HLT-NAACL.
Jian Huang, Seyda Ertekin, and C. Lee Giles.
2006. Efficient name disambiguation for large scale
databases. In Proc. of 10th PKDD, pages 536 ? 544.
Jay J. Jiang and David W. Conrath. 1997. Semantic
similarity based on corpus statistics and lexical
taxonomy. In Proceedings of ROCLING X.
Xin Li, Paul Morie, and Dan Roth. 2004. Robust
reading: Identification and tracing of ambiguous
names. In Proceedings of HLT-NAACL, pages 17?24.
Gideon S. Mann and David Yarowsky. 2003.
Unsupervised personal name disambiguation. In
Proceedings of HLT-NAACL, pages 33?40.
Vincent Ng and Claire Cardie. 2001. Improving machine
learning approaches to coreference resolution. In
Proceedings of the 40th ACL, pages 104?111.
Sarah M. Taylor. 2004. Information extraction
tools: Deciphering human language. IT Professional,
6(6):28 ? 34.
12
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 414?422,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Profile Based Cross-Document Coreference
Using Kernelized Fuzzy Relational Clustering
Jian Huang? Sarah M. Taylor? Jonathan L. Smith? Konstantinos A. Fotiadis? C. Lee Giles?
?College of Information Sciences and Technology
Pennsylvania State University, University Park, PA 16802, USA
{jhuang, giles}@ist.psu.edu
?Advanced Technology Office, Lockheed Martin IS&GS, Arlington, VA 22203, USA
{sarah.m.taylor, jonathan.l.smith, konstantinos.a.fotiadis}@lmco.com
Abstract
Coreferencing entities across documents
in a large corpus enables advanced
document understanding tasks such as
question answering. This paper presents
a novel cross document coreference
approach that leverages the profiles
of entities which are constructed by
using information extraction tools and
reconciled by using a within-document
coreference module. We propose to
match the profiles by using a learned
ensemble distance function comprised
of a suite of similarity specialists. We
develop a kernelized soft relational
clustering algorithm that makes use of
the learned distance function to partition
the entities into fuzzy sets of identities.
We compare the kernelized clustering
method with a popular fuzzy relation
clustering algorithm (FRC) and show 5%
improvement in coreference performance.
Evaluation of our proposed methods
on a large benchmark disambiguation
collection shows that they compare
favorably with the top runs in the
SemEval evaluation.
1 Introduction
A named entity that represents a person, an or-
ganization or a geo-location may appear within
and across documents in different forms. Cross
document coreference (CDC) is the task of con-
solidating named entities that appear in multiple
documents according to their real referents. CDC
is a stepping stone for achieving intelligent in-
formation access to vast and heterogeneous text
corpora, which includes advanced NLP techniques
such as document summarization and question an-
swering. A related and well studied task is within
document coreference (WDC), which limits the
scope of disambiguation to within the boundary of
a document. When namesakes appear in an article,
the author can explicitly help to disambiguate, us-
ing titles and suffixes (as in the example, ?George
Bush Sr. ... the younger Bush?) besides other
means. Cross document coreference, on the other
hand, is a more challenging task because these
linguistics cues and sentence structures no longer
apply, given the wide variety of context and styles
in different documents.
Cross document coreference research has re-
cently become more popular due to the increasing
interests in the web person search task (Artiles
et al, 2007). Here, a search query for a person
name is entered into a search engine and the
desired outputs are documents clustered according
to the identities of the entities in question. In
our work, we propose to drill down to the sub-
document mention level and construct an entity
profile with the support of information extraction
tools and reconciled with WDC methods. Hence
our IE based approach has access to accurate
information such as a person?s mentions and geo-
locations for disambiguation. Simple IR based
CDC approaches (e.g. (Gooi and Allan, 2004)), on
the other hand, may simply use all the terms and
this can be detrimental to accuracy. For example, a
biography of John F. Kennedy is likely to mention
members of his family with related positions,
besides references to other political figures. Even
with careful word selection, these textual features
can still confuse the disambiguation system about
the true identity of the person.
We propose to handle the CDC task using a
novel kernelized fuzzy relational clustering algo-
rithm, which allows probabilistic cluster mem-
bership assignment. This not only addresses the
intrinsic uncertainty nature of the CDC problem,
but also yields additional performance improve-
ment. We propose to use a specialist ensemble
414
learning approach to aggregate the diverse set of
similarities in comparing attributes and relation-
ships in entity profiles. Our approach is first fully
described in Section 2. The effectiveness of the
proposed method is demonstrated using real world
benchmark test sets in Section 3. We review
related work in cross document coreference and
conclude in Section 5.
2 Methods
2.1 Document Level and Profile Based CDC
We make distinctions between document level and
profile based cross document coreference. Docu-
ment level CDC makes a simplifying assumption
that a named entity (and its variants) in a document
has one underlying real identity. The assump-
tion is generally acceptable but may be violated
when a document refers to namesakes at the same
time (e.g. George W. Bush and George H. W.
Bush referred to as George or President Bush).
Furthermore, the context surrounding the person
NE President Clinton can be counterproductive
for disambiguating the NE Senator Clinton, with
both entities likely to appear in a document at the
same time. The simplified document level CDC
has nevertheless been used in the WePS evaluation
(Artiles et al, 2007), called the web people task.
In this work, we advocate profile based disam-
biguation that aims to leverage the advances in
NLP techniques. Rather than treating a document
as simply a bag of words, an information extrac-
tion tool first extracts NE?s and their relationships.
For the NE?s of interest (i.e. persons in this work),
a within-document coreference (WDC) module
then links the entities deemed as referring to
the same underlying identity into a WDC chain.
This process includes both anaphora resolution
(resolving ?He? and its antecedent ?President Clin-
ton?) and entity tracking (resolving ?Bill? and
?President Clinton?). Let E = {e1, ..., eN} denote
the set of N chained entities (each corresponding
to a WDC chain), provided as input to the CDC
system. We intentionally do not distinguish which
document each ej belongs to, as profile based
CDC can potentially rectify WDC errors by lever-
aging information across document boundaries.
Each ei is represented as a profile which contains
the NE, its attributes and associated relationships,
i.e. ej =< ej,1, ..., ej,L > (ej,l can be a textual
attribute or a pointer to another entity). The profile
based CDC method generates a partition of E ,
represented by a partition matrix U (where uij
denotes the membership of an entity ej to the i-
th identity cluster). Therefore, the chained entities
placed in a name cluster are deemed as coreferent.
Profile based CDC addresses a finer grained
coreference problem in the mention level, enabled
by the recent advances in IE and WDC techniques.
In addition, profile based CDC facilitates user
information consumption with structured informa-
tion and short summary passages. Next, we focus
on the relational clustering algorithm that lies at
the core of the profile based CDC system. We then
turn our attention to the specialist learning algo-
rithm for the distance function used in clustering,
capable of leveraging the available training data.
2.2 CDC Using Fuzzy Relational Clustering
2.2.1 Preliminaries
Traditionally, hard clustering algorithms (where
uij ? {0, 1}) such as complete linkage hierarchi-
cal agglomerative clustering (Mann and Yarowsky,
2003) have been applied to the disambiguation
problem. In this work, we propose to use fuzzy
clustering methods (relaxing the membership con-
dition to uij ? [0, 1]) as a better way of handling
uncertainty in cross document coreference. First,
consider the following motivating example,
Example. The named entity President Bush is
extracted from the sentence ?President Bush ad-
dressed the nation from the Oval Office Monday.?
? Without additional cues, a hard clustering
algorithm has to arbitrarily assign the
mention ?President Bush? to either the NE
?George W. Bush? or ?George H. W. Bush?.
? A soft clustering algorithm, on the other
hand, can assign equal probability to the two
identities, indicating low entropy or high
uncertainty in the solution. Additionally, the
soft clustering algorithm can assign lower
probability to the identity ?Governor Jeb
Bush?, reflecting a less likely (though not
impossible) coreference decision.
We first formalize the cross document corefer-
ence problem as a soft clustering problem, which
minimizes the following objective function:
JC(E) =
C?
i=1
N?
j=1
umijd2(ej ,vi) (1)
s.t.
C?
i=1
uij = 1 and
N?
j=1
uij > 0, uij ? [0, 1]
415
where vi is a virtual (implicit) prototype of the i-th
cluster (ej ,vi ? D) and m controls the fuzziness
of the solution (m > 1; the solution approaches
hard clustering as m approaches 1). We will
further explain the generic distance function d :
D ? D ? R in the next subsection. The goal
of the optimization is to minimize the sum of
deviations of patterns to the cluster prototypes.
The clustering solution is a fuzzy partition P? =
{Ci}, where ej ? Ci if and only if uij > ?.
We note from the outset that the optimization
functional has the same form as the classical
Fuzzy C-Means (FCM) algorithm (Bezdek, 1981),
but major differences exist. FCM, as most ob-
ject clustering algorithms, deals with object data
represented in a vectorial form. In our case, the
data is purely relational and only the mutual rela-
tionships between entities can be determined. To
be exact, we can define the similarity/dissimilarity
between a pair of attributes or relationships of
the same type l between entities ej and ek as
s(l)(ej , ek). For instance, the similarity between
the occupations ?President? and ?Commander in
Chief? can be computed using the JC semantic
distance (Jiang and Conrath, 1997) with WordNet;
the similarity of co-occurrence with other people
can be measured by the Jaccard coefficient. In the
next section, we propose to compute the relation
strength r(?, ?) from the component similarities
using aggregation weights learned from training
data. Hence the N chained entities to be clustered
can be represented as relational data using an n?n
matrix R, where rj,k = r(ej , ek). The Any Rela-
tion Clustering Algorithm (ARCA) (Corsini et al,
2005; Cimino et al, 2006) represents relational
data as object data using their mutual relation
strength and uses FCM for clustering. We adopt
this approach to transform (objectify) a relational
pattern ej into an N dimensional vector rj (i.e.
the j-th row in the matrix R) using a mapping
? : D ? RN . In other words, each chained entity
is represented as a vector of its relation strengths
with all the entities. Fuzzy clusters can then
be obtained by grouping closely related patterns
using object clustering algorithm.
Furthermore, it is well known that FCM
is a spherical clustering algorithm and thus
is not generally applicable to relational data
which may yield relational clusters of arbitrary
and complicated shapes. Also, the distance in
the transformed space may be non-Euclidean,
rendering many clustering algorithms ineffective
(many FCM extensions theoretically require
the underlying distance to satisfy certain metric
properties). In this work, we propose kernelized
ARCA (called KARC) which uses a kernel-
induced metric to handle the objectified relational
data, as we introduce next.
2.2.2 Kernelized Fuzzy Clustering
Kernelization (Scho?lkopf and Smola, 2002) is a
machine learning technique to transform patterns
in the data space to a high-dimensional feature
space so that the structure of the data can be more
easily and adequately discovered. Specifically, a
nonlinear transformation ? maps data in RN to
H of possibly infinite dimensions (Hilbert space).
The key idea is the kernel trick ? without explicitly
specifying ? and H, the inner product in H can
be computed by evaluating a kernel function K in
the data space, i.e. < ?(ri),?(rj) >= K(ri, rj)
(one of the most frequently used kernel func-
tions is the Gaussian RBF kernel: K(rj , rk) =
exp(???rj ? rk?2)). This technique has been
successfully applied to SVMs to classify non-
linearly separable data (Vapnik, 1995). Kerneliza-
tion preserves the simplicity in the formalism of
the underlying clustering algorithm, meanwhile it
yields highly nonlinear boundaries so that spheri-
cal clustering algorithms can apply (e.g. (Zhang
and Chen, 2003) developed a kernelized object
clustering algorithm based on FCM).
Let wi denote the objectified virtual cluster vi,
i.e. wi = ?(vi). Using the kernel trick, the
squared distance between ?(rj) and ?(wi) in the
feature space H can be computed as:
??(rj)? ?(wi)?2H (2)
= < ?(rj)? ?(wi),?(rj)? ?(wi) >
= < ?(rj),?(rj) > ?2 < ?(rj),?(wi) >
+ < ?(wi),?(wi) >
= 2? 2K(rj ,wi) (3)
assuming K(r, r) = 1. The KARC algorithm
defines the generic distance d as d2(ej ,vi) def=
??(rj)??(wi)?2H = ??(?(ej))??(?(vi))?2H
(we also use d2ji as a notational shorthand).
Using Lagrange Multiplier as in FCM, the opti-
mal solution for Equation (1) is:
uij =
?
??
??
[
C?
h=1
(
d2ji
d2jh
)1/(m?1)]?1
, (d2ji 6= 0)
1 , (d2ji = 0)
(4)
416
?(wi) =
N?
k=1
umik?(rk)
N?
k=1
umik
(5)
Since ? is an implicit mapping, Eq. (5) can
not be explicitly evaluated. On the other hand,
plugging Eq. (5) into Eq. (3), d2ji can be explicitly
represented by using the kernel matrix,
d2ji = 2? 2 ?
N?
k=1
umikK(rj , rk)
N?
k=1
umik
(6)
With the derivation, the kernelized fuzzy clus-
tering algorithm KARC works as follows. The
chained entities E are first objectified into the
relation strength matrix R using SEG, the details
of which are described in the following section.
The Gram matrix K is then computed based on
the relation strength vectors using the kernel func-
tion. For a given number of clusters C, the
initialization step is done by randomly picking C
patterns as cluster centers, equivalently, C indices
{n1, .., nC} are randomly picked from {1, .., N}.
D0 is initialized by setting d2ji = 2? 2K(rj , rni).
KARC alternately updates the membership matrix
U and the kernel distance matrix D until conver-
gence or running more than maxIter iterations
(Algorithm 1). Finally, the soft partition is gen-
erated based on the membership matrix U , which
is the desired cross document coreference result.
Algorithm 1 KARC Alternating Optimization
Input: Gram matrix K; #Clusters C; threshold ?
initialize D0
t ? 0
repeat
t ? t+ 1
// 1? Update membership matrix U t:
uij = (d
2
ji)
? 1m?1
?C
h=1 (d2jh)
? 1m?1
// 2? Update kernel distance matrix Dt:
d2ji = 2? 2 ?
N?
k=1
umikKjk
N?
k=1
umik
until (t > maxIter) or
(t > 1 and |U t ? U t?1| < ?)
P? ? Generate soft partition(U t, ?)
Output: Fuzzy partition P?
2.2.3 Cluster Validation
In the CDC setting, the number of true underlying
identities may vary depending on the entities? level
of ambiguity (e.g. name frequency). Selecting the
optimal number of clusters is in general a hard
research question in clustering1. We adopt the
Xie-Beni Index (XBI) (Xie and Beni, 1991) as in
ARCA, which is one of the most popular cluster
validities for fuzzy clustering algorithms. Xie-
Beni Index (XBI) measures the goodness of clus-
tering using the ratio of the intra-cluster variation
and the inter-cluster separation. We measure the
kernelized XBI (KXBI) in the feature space as,
KXBI =
C?
i=1
N?
j=1
umij ??(rj)? ?(wi)?2H
N ? min
1?i<j?C
??(wi)? ?(wj)?2H
where the nominator is readily computed using D
and the inter-cluster separation in the denominator
can be evaluated using the similar kernel trick
above (details omitted). Note that KXBI is only
defined for C > 1. Thus we pick the C that
corresponds to the first minimum of KXBI, and
then compare its objective function value JC with
the cluster variance (J1 for C = 1). The optimal
C is chosen from the minimum of the two2.
2.3 Specialist Ensemble Learning of Relation
Strengths between Entities
One remaining element in the overall CDC ap-
proach is how the relation strength rj,k between
two entities is computed. In (Cohen et al, 2003),
a binary SVM model is trained and its confidence
in predicting the non-coreferent class is used as
the distance metric. In our case of using in-
formation extraction results for disambiguation,
however, only some of the similarity features are
present based on the available relationships in two
profiles. In this work, we propose to treat each
similarity function as a specialist that specializes
in computing the similarity of a particular type
of relationship. Indeed, the similarity function
between a pair of attributes or relationships may in
itself be a sophisticated component algorithm. We
utilize the specialist ensemble learning framework
(Freund et al, 1997) to combine these component
1In particular, clustering algorithms that regularize the
optimization with cluster size are not applicable in our case.
2In practice, the entities to be disambiguated tend to be
dominated by several major identities. Hence performance
generally does not vary much in the range of large C values.
417
similarities into the relation strength for clustering.
Here, a specialist is awakened for prediction only
when the same type of relationships are present in
both chained entities. A specialist can choose not
to make a prediction if it is not confident enough
for an instance. These aspects contrast with the
traditional insomniac ensemble learning methods,
where each component learner is always available
for prediction (Freund et al, 1997). Also, spe-
cialists have different weights (in addition to their
prediction) on the final relation strength, e.g. a
match in a family relationship is considered more
important than in a co-occurrence relationship.
Algorithm 2 SEG (Freund et al, 1997)
Input: Initial weight distribution p1;
learning rate ? > 0; training set {< st, yt >}
1: for t=1 to T do
2: Predict using:
y?t =
?
i?Et ptisti?
i?Et pti
(7)
3: Observe the true label yt and incur square
loss L(y?t, yt) = (y?t ? yt)2
4: Update weight distribution: for i ? Et
pt+1i =
ptie?2?x
t
i(y?t?yt)
?
j?Et
ptje?2?x
t
i(y?t?yt)
?
?
j?Et
ptj (8)
Otherwise: pt+1i = pti
5: end for
Output: Model p
The ensemble relation strength model is learned
as follows. Given training data, the set of chained
entities Etrain is extracted as described earlier. For
a pair of entities ej and ek, a similarity vector
s is computed using the component similarity
functions for the respective attributes and rela-
tionships, and the true label is defined as y =
I{ej and ek are coreferent}. The instances are
subsampled to yield a balanced pairwise train-
ing set {< st, yt >}. We adopt the Special-
ist Exponentiated Gradient (SEG) (Freund et al,
1997) algorithm to learn the mixing weights of the
specialists? prediction (Algorithm 2) in an online
manner. In each training iteration, an instance
< st, yt > is presented to the learner (with Et
denoting the set of indices of awake specialists in
st). The SEG algorithm first predicts the value y?t
based on the awake specialists? decisions. The true
value yt is then revealed and the learner incurs a
square loss between the predicted and the true val-
ues. The current weight distribution p is updated
to minimize square loss: awake specialists are
promoted or demoted in their weights according to
the difference between the predicted and the true
value. The learning iterations can run a few passes
till convergence, and the model is learned in linear
time with respect to T and is thus very efficient. In
prediction time, let E(jk) denote the set of active
specialists for the pair of entities ej and ek, and
s(jk) denote the computed similarity vector. The
predicted relation strength rj,k is,
rj,k =
?
i?E(jk) pis(jk)i?
i?E(jk) pi
(9)
2.4 Remarks
Before we conclude this section, we make several
comments on using fuzzy clustering for cross
document coreference. First, instead of conduct-
ing CDC for all entities concurrently (which can
be computationally intensive with a large cor-
pus), chained entities are first distributed into non-
overlapping blocks. Clustering is performed for
each block which is a drastically smaller problem
space, while entities from different blocks are
unlikely to be coreferent. Our CDC system uses
phonetic blocking on the full name, so that name
variations arising from translation, transliteration
and abbreviation can be accommodated. Ad-
ditional link constraints checking is also imple-
mented to improve scalability though these are not
the main focus of the paper.
There are several additional benefits in using
a fuzzy clustering method besides the capabil-
ity of probabilistic membership assignments in
the CDC solution. In the clustered web search
context, splitting a true identity into two clusters
is perceived as a more severe error than putting
irrelevant records in a cluster, as it is more difficult
for the user to collect records in different clusters
(to reconstruct the real underlying identity) than
to prune away noisy records. While there is no
universal way to handle this with hard clustering,
soft clustering algorithms can more easily avoid
the false negatives by allowing records to prob-
abilistically appear in different clusters (subject
to the sum of 1) using a more lenient threshold.
Also, while there is no real prototypical elements
in relational clustering, soft relational clustering
418
methods can naturally rank the profiles within
a cluster according to their membership levels,
which is an additional advantage for enhancing
user consumption of the disambiguation results.
3 Experiments
In this section, we first formally define the evalu-
ation metrics, followed by the introduction to the
benchmark test sets and the system?s performance.
3.1 Evaluation Metrics
We benchmarked our method using the standard
purity and inverse purity clustering metrics as in
the WePS evaluation. Let a set of clusters P =
{Ci} denote the system?s partition as aforemen-
tioned and a set of categories Q = {Dj} be the
gold standard. The precision of a cluster Ci with
respect to a category Dj is defined as,
Precision(Ci,Dj) = |Ci ? Dj ||Ci|
Purity is in turn defined as the weighted average
of the maximum precision achieved by the clusters
on one of the categories,
Purity(P,Q) =
C?
i=1
|Ci|
n maxj Precision(Ci,Dj)
where n = ? |Ci|. Hence purity penalizes putting
noise chained entities in a cluster. Trivially, the
maximum purity (i.e. 1) can be achieved by
making one cluster per chained entity (referred to
as the one-in-one baseline). Reversing the role of
clusters and categories, Inverse purity(P,Q) def=
Purity(Q,P). Inverse Purity penalizes splitting
chained entities belonging to the same category
into different clusters. The maximum inverse
purity can be similarly achieved by putting all
entities into one cluster (all-in-one baseline).
Purity and inverse purity are similar to the
precision and recall measures commonly used in
IR. The F score, F = 1/(? 1Purity + (1 ?
?) 1InversePurity ), is used in performance evalua-
tion. ? = 0.2 is used to give more weight to
inverse purity, with the justification for the web
person search mentioned earlier.
3.2 Dataset
We evaluate our methods using the benchmark
test collection from the ACL SemEval-2007 web
person search task (WePS) (Artiles et al, 2007).
The test collection consists of three sets of 10
different names, sampled from ambiguous names
from English Wikipedia (famous people), partici-
pants of the ACL 2006 conference (computer sci-
entists) and common names from the US Census
data, respectively. For each name, the top 100
documents retrieved from the Yahoo! Search API
were annotated, yielding on average 45 real world
identities per set and about 3k documents in total.
As we note in the beginning of Section 2, the
human markup for the entities corresponding to
the search queries is on the document level. The
profile-based CDC approach, however, is to merge
the mention-level entities. In our evaluation, we
adopt the document label (and the person search
query) to annotate the entity profiles that corre-
sponds to the person name search query. Despite
the difference, the results of the one-in-one and
all-in-one baselines are almost identical to those
reported in the WePS evaluation (F = 0.52, 0.58
respectively). Hence the performance reported
here is comparable to the official evaluation results
(Artiles et al, 2007).
3.3 Information Extraction and Similarities
We use an information extraction tool AeroText
(Taylor, 2004) to construct the entity profiles.
AeroText extracts two types of information for
an entity. First, the attribute information about
the person named entity includes first/middle/last
names, gender, mention, etc. In addition,
AeroText extracts relationship information
between named entities, such as Family, List,
Employment, Ownership, Citizen-Resident-
Religion-Ethnicity and so on, as specified in the
ACE evaluation. AeroText resolves the references
of entities within a document and produces the
entity profiles, used as input to the CDC system.
Note that alternative IE or WDC tools, as well
as additional attributes or relationships, can be
readily used in the CDC methods we proposed.
A suite of similarity functions is designed to
determine if the attributes relationships in a pair
of entity profiles match or not:
Text similarity. To decide whether two names
in the co-occurrence or family relationship match,
we use the SoftTFIDF measure (Cohen et al,
2003), which is a hybrid matching scheme that
combines the token-based TFIDF with the Jaro-
Winkler string distance metric. This permits in-
exact matching of named entities due to name
419
variations, typos, etc.
Semantic similarity. Text or syntactic similarity
is not always sufficient for matching relationships.
WordNet and the information theoretic semantic
distance (Jiang and Conrath, 1997) are used to
measure the semantic similarity between concepts
in relationships such as mention, employment,
ownership, etc.
Other rule-based similarity. Several other
cases require special treatment. For example,
the employment relationships of Senator and
D-N.Y. should match based on domain knowledge.
Also, we design dictionary-based similarity
functions to handle nicknames (Bill and William),
acronyms (COLING for International Conference
on Computational Linguistics), and geo-locations.
3.4 Evaluation Results
From the WePS training data, we generated a
training set of around 32k pairwise instances as
previously stated in Section 2.3. We then used
the SEG algorithm to learn the weight distribution
model. We tuned the parameters in the KARC
algorithm using the training set with discrete grid
search and chose m = 1.6 and ? = 0.3. The RBF
kernel (Gaussian) is used with ? = 0.015.
Table 1: Cross document coreference performance
(I. Purity denotes inverse purity).
Method Purity I. Purity F
KARC-S 0.657 0.795 0.740
KARC-H 0.662 0.762 0.710
FRC 0.484 0.840 0.697
One-in-one 1.000 0.482 0.524
All-in-one 0.279 1.000 0.571
The macro-averaged cross document corefer-
ence on the WePS test sets are reported in Table
1. The F score of our CDC system (KARC-
S) is 0.740, comparable to the test results of the
first tier systems in the official evaluation. The
two baselines are also included. Since different
feature sets, NLP tools, etc are used in different
benchmarked systems, we are also interested in
comparing the proposed algorithm with differ-
ent soft relational clustering variants. First, we
?harden? the fuzzy partition produced by KARC
by allowing an entity to appear in the cluster
with highest membership value (KARC-H). Purity
improves because of the removal of noise entities,
though at the sacrifice of inverse purity and the
Table 2: Cross document coreference performance
on subsets (I. Purity denotes inverse purity).
Test set Identity Purity I. Purity F
Wikipedia 56.5 0.666 0.752 0.717
ACL-06 31.0 0.783 0.771 0.773
US Census 50.3 0.554 0.889 0.754
F score deteriorates. We also implement a pop-
ular fuzzy relational clustering algorithm called
FRC (Dave and Sen, 2002), whose optimization
functional directly minimizes with respect to the
relation matrix. With the same feature sets and
distance function, KARC-S outperforms FRC in F
score by about 5%. Because the test set is very am-
biguous (on average only two documents per real
world entity), the baselines have relatively high F
score as observed in the WePS evaluation (Artiles
et al, 2007). Table 2 further analyzes KARC-
S?s result on the three subsets Wikipedia, ACL06
and US Census. The F score is higher in the
less ambiguous (the average number of identities)
dataset and lower in the more ambiguous one, with
a spread of 6%.
We study how the cross document coreference
performance changes as we vary the fuzziness in
the solution (controlled by m). In Figure 1, as
m increases from 1.4 to 1.9, purity improves by
10% to 0.67, which indicates that more correct
coreference decisions (true positives) can be made
in a softer configuration. The complimentary is
true for inverse purity, though to a lesser extent.
In this case, more false negatives, corresponding
to the entities of different coreferents incorrectly
 0.5
 0.55
 0.6
 0.65
 0.7
 0.75
 0.8
 0.85
 1.4  1.5  1.6  1.7  1.8  1.9
m
KARC performance with different m
purityinverse purityF
Figure 1: Purity, inverse purity and F score with
different fuzzifiers m.
420
 0.6
 0.65
 0.7
 0.75
 0.8
 0.85
 0.1  0.2  0.3  0.4  0.5  0.6
?
KARC performance with different ?
purityinverse purityF
Figure 2: CDC performance with different ?.
linked, are made in a softer partition. The F
score peaks at 0.74 (m = 1.6) and then slightly
decreases, as the gain in purity is outweighed by
the loss in inverse purity.
Figure 2 evaluates the impact of the different
settings of ? (the threshold of including a chained
entity in the fuzzy cluster) on the coreference
performance. We observe that as we increase
?, purity improves indicating less ?noise? entities
are included in the solution. On the other hand,
inverse purity decreases meaning more coreferent
entities are not linked due to the stricter threshold.
Overall, the changes in the two metrics offset each
other and the F score is relatively stable across a
broad range of ? settings.
4 Related Work
The original work in (Bagga and Baldwin, 1998)
proposed a CDC system by first performing WDC
and then disambiguating based on the summary
sentences of the chains. This is similar to ours in
that mentions rather than documents are clustered,
leveraging the advances in state-of-the-art WDC
methods developed in NLP, e.g. (Ng and Cardie,
2001; Yang et al, 2008). On the other hand, our
work goes beyond the simple bag-of-word features
and vector space model in (Bagga and Baldwin,
1998; Gooi and Allan, 2004) with IE results. (Wan
et al, 2005) describes a person resolution system
WebHawk that clusters web pages using some
extracted personal information including person
name, title, organization, email and phone number,
besides lexical features. (Mann and Yarowsky,
2003) extracts biographical information, which is
relatively scarce in web data, for disambiguation.
With the support of state-of-the-art information
extraction tools, the profiles of entities in this work
covers a broader range of relational information.
(Niu et al, 2004) also leveraged IE support, but
their approach was evaluated on a small artificial
corpus. Also, the pairwise distance model is
insomniac (i.e. all similarity specialists are awake
for prediction) and our work extends this with a
specialist learning framework.
Prior work has largely relied on using hier-
archical clustering methods for CDC, with the
threshold for stopping the merging set using the
training data, e.g. (Mann and Yarowsky, 2003;
Chen and Martin, 2007; Baron and Freedman,
2008). The fuzzy relational clustering method
proposed in this paper we believe better addresses
the uncertainty aspect of the CDC problem.
There are also orthogonal research directions
for the CDC problem. (Li et al, 2004) solved the
CDC problem by adopting a probabilistic view on
how documents are generated and how names are
sprinkled into them. (Bunescu and Pasca, 2006)
showed that external information from Wikipedia
can improve the disambiguation performance.
5 Conclusions
We have presented a profile-based Cross Docu-
ment Coreference (CDC) approach based on a
novel fuzzy relational clustering algorithm KARC.
In contrast to traditional hard clustering methods,
KARC produces fuzzy sets of identities which
better reflect the intrinsic uncertainty of the CDC
problem. Kernelization, as used in KARC, enables
the optimization of clustering that is spherical
in nature to apply to relational data that tend to
have complicated shapes. KARC partitions named
entities based on their profiles constructed by an
information extraction tool. To match the pro-
files, a specialist ensemble algorithm predicts the
pairwise distance by aggregating the similarities of
the attributes and relationships in the profiles. We
evaluated the proposed methods with experiments
on a large benchmark collection and demonstrate
that the proposed methods compare favorably with
the top runs in the SemEval evaluation.
The focus of this work is on the novel learning
and clustering methods for coreference. Future
research directions include developing rich feature
sets and using corpus level or external informa-
tion. We believe that such efforts can further im-
prove cross document coreference performance.
421
References
Javier Artiles, Julio Gonzalo, and Satoshi Sekine.
2007. The SemEval-2007 WePS evaluation:
Establishing a benchmark for the web people search
task. In Proceedings of the 4th International
Workshop on Semantic Evaluations (SemEval-
2007), pages 64?69.
Amit Bagga and Breck Baldwin. 1998. Entity-based
cross-document coreferencing using the vector
space model. In Proceedings of 36th International
Conference On Computational Linguistics (ACL)
and 17th international conference on Computational
linguistics (COLING), pages 79?85.
Alex Baron and Marjorie Freedman. 2008. Who
is who and what is what: Experiments in cross-
document co-reference. In Proceedings of the
2008 Conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 274?283.
J. C. Bezdek. 1981. Pattern Recognition with Fuzzy
Objective Function Algoritms. Plenum Press, NY.
Razvan Bunescu and Marius Pasca. 2006. Using
encyclopedic knowledge for named entity disam-
biguation. In Proceedings of the 11th Conference
of the European Chapter of the Association for
Computational Linguistics (EACL), pages 9?16.
Ying Chen and James Martin. 2007. Towards
robust unsupervised personal name disambiguation.
In Proc. of 2007 Joint Conference on Empirical
Methods in Natural Language Processing and
Computational Natural Language Learning.
Mario G. C. A. Cimino, Beatrice Lazzerini, and
Francesco Marcelloni. 2006. A novel approach
to fuzzy clustering based on a dissimilarity relation
extracted from data using a TS system. Pattern
Recognition, 39(11):2077?2091.
William W. Cohen, Pradeep Ravikumar, and
Stephen E. Fienberg. 2003. A comparison of
string distance metrics for name-matching tasks.
In Proceedings of IJCAI Workshop on Information
Integration on the Web.
Paolo Corsini, Beatrice Lazzerini, and Francesco
Marcelloni. 2005. A new fuzzy relational clustering
algorithm based on the fuzzy c-means algorithm.
Soft Computing, 9(6):439 ? 447.
Rajesh N. Dave and Sumit Sen. 2002. Robust fuzzy
clustering of relational data. IEEE Transactions on
Fuzzy Systems, 10(6):713?727.
Yoav Freund, Robert E. Schapire, Yoram Singer, and
Manfred K. Warmuth. 1997. Using and combining
predictors that specialize. In Proceedings of the
twenty-ninth annual ACM symposium on Theory of
computing (STOC), pages 334?343.
Chung H. Gooi and James Allan. 2004. Cross-
document coreference on a large scale corpus. In
Proceedings of the Human Language Technology
Conference of the North American Chapter of
the Association for Computational Linguistics
(NAACL), pages 9?16.
Jay J. Jiang and David W. Conrath. 1997.
Semantic similarity based on corpus statistics and
lexical taxonomy. In Proceedings of International
Conference Research on Computational Linguistics.
Xin Li, Paul Morie, and Dan Roth. 2004. Robust
reading: Identification and tracing of ambiguous
names. In Proceedings of the Human Language
Technology Conference and the North American
Chapter of the Association for Computational
Linguistics (HLT-NAACL), pages 17?24.
Gideon S. Mann and David Yarowsky. 2003.
Unsupervised personal name disambiguation. In
Conference on Computational Natural Language
Learning (CoNLL), pages 33?40.
Vincent Ng and Claire Cardie. 2001. Improving ma-
chine learning approaches to coreference resolution.
In Proceedings of the 40th Annual Meeting of the
Association for Computational Linguistics (ACL),
pages 104?111.
Cheng Niu, Wei Li, and Rohini K. Srihari. 2004.
Weakly supervised learning for cross-document
person name disambiguation supported by infor-
mation extraction. In Proceedings of the 42nd
Annual Meeting on Association for Computational
Linguistics (ACL), pages 597?604.
Bernhard Scho?lkopf and Alex Smola. 2002. Learning
with Kernels. MIT Press, Cambridge, MA.
Sarah M. Taylor. 2004. Information extraction tools:
Deciphering human language. IT Professional,
6(6):28 ? 34.
Vladimir Vapnik. 1995. The Nature of Statistical
Learning Theory. Springer-Verlag New York.
Xiaojun Wan, Jianfeng Gao, Mu Li, and Binggong
Ding. 2005. Person resolution in person search
results: WebHawk. In Proceedings of the 14th
ACM international conference on Information and
knowledge management (CIKM), pages 163?170.
Xuanli Lisa Xie and Gerardo Beni. 1991. A validity
measure for fuzzy clustering. IEEE Transactions
on Pattern Analysis and Machine Intelligence,
13(8):841 ? 847.
Xiaofeng Yang, Jian Su, Jun Lang, Chew L. Tan,
Ting Liu, and Sheng Li. 2008. An entity-
mention model for coreference resolution with
inductive logic programming. In Proceedings of
the 46th Annual Meeting of the Association for
Computational Linguistics (ACL), pages 843?851.
Dao-Qiang Zhang and Song-Can Chen. 2003.
Clustering incomplete data using kernel-based fuzzy
c-means algorithm. Neural Processing Letters,
18(3):155 ? 162.
422
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 483?491,
Beijing, August 2010
Enhancing Cross Document Coreference of Web Documents
with Context Similarity and Very Large Scale Text Categorization
Jian Huang
Information Sciences and Technology
Pennsylvania State University
jhuang@ist.psu.edu
Pucktada Treeratpituk
Information Sciences and Technology
Pennsylvania State University
pxt162@ist.psu.edu
Sarah M. Taylor
Lockheed Martin IS&GS
sarah.m.taylor@lmco.com
C. Lee Giles
Information Sciences and Technology
Pennsylvania State University
giles@ist.psu.edu
Abstract
Cross Document Coreference (CDC) is
the task of constructing the coreference
chain for mentions of a person across a set
of documents. This work offers a holistic
view of using document-level categories,
sub-document level context and extracted
entities and relations for the CDC task.
We train a categorization component with
an efficient flat algorithm using thousands
of ODP categories and over a million web
documents. We propose to use ranked cat-
egories as coreference information, partic-
ularly suitable for web documents that are
widely different in style and content. An
ensemble composite coreference function,
amenable to inactive features, combines
these three levels of evidence for disam-
biguation.
A thorough feature importance study is
conducted to analyze how these three
components contribute to the coreference
results. The overall solution is evaluated
using the WePS benchmark data and
demonstrate superior performance.
1 Introduction
Cross Document Coreference (CDC) is the task
to determine whether Named Entities (NE) from
different documents refer to the same underlying
identity. CDC enables a range of advanced NLP
applications such as automated text summariza-
tion and question answering (e.g. list-type ques-
tions). CDC has mainly been developed from two
perspectives.
First, in the Message Understanding Confer-
ence (MUC-6), CDC was viewed as an advanced
task performed based on a set of Information
Extraction (IE) artifacts. IE has been one of the
central topics in NLP since the 1970s and gained
much success in transforming natural language
text to structured text. IE on the Web, however,
is inherently very challenging. For one, the Web
is comprised of such heterogenous content that
IE systems, many of which are developed on
tidy and domain-specific corpora, may achieve
relatively limited coverage. Also, the content of
web documents may not even be in the natural
language form. Hence, though IE based features
are quite precise, it is rather difficult to achieve
good coverage that?s necessary to disambiguate
person entities on the Web.
Recently, there is significant research interest in
a related task called Web Person Search (WePS)
(Artiles et al, 2007), which seeks to determine
whether two documents refer to the same person
given a person name search query. Many systems
employed the simple vector space model and word
co-occurrence features for this task. Though more
robust with better coverage, these methods are
more susceptible to irrelevant words with regard
to the entity of interest.
Rather than relying solely on IE based or word
co-occurrence features, this work adopts a holistic
view of the different types of features useful for
cross document coreference. Specifically, the
main features of our proposed CDC approach are:
483
? The proposed approach covers the entire
spectrum of document level, sub-document
context level and entity/relation level
disambiguation evidence. In particular,
we propose to use document categories
as robust document level evidence. This
comprehensive design naturally combines
state-of-the-art categorization, information
extraction and IE-driven IR methods and
compensates the limitation of each of them.
? The features used in this work are domain in-
dependent and thus are particularly suitable
for coreferencing web documents.
? The composite pairwise coreference function
in this work can readily incorporate a set
of heterogenous features that are not always
active or are in different ranges, making
it easily extensible to additional features.
Moreover, we thoroughly study the contri-
bution of each component and its features
to gain insight on improving cross document
coreference performance.
In this work, three components specialize in
generating the aforementioned three levels of fea-
tures as coreference decisions. Thus we refer to
them as experts. After reviewing prior work on
CDC, we describe the methods of each of these
components in detail and present empirical results
where appropriate. We then show how these
components (and its features) are aggregated to
predict pairwise coreference using an ensemble
method. We evaluate the contribution of each
component and the overall CDC results on a
benchmark dataset. Finally, we conclude and
discuss future work.
2 Related Work
Compared to the traditional (within-document)
coreference resolution problem, cross document
coreference is a much harder problem due to the
divergence of contents and the lack of consistent
discourse information across documents.
(Bagga and Baldwin, 1998b) presented one of
the first CDC systems, which relied solely on the
contextual words of the named entities. (Gooi
and Allan, 2004) used a 55-word window as
the context without significant accuracy penalty.
As these approaches only considered word co-
occurrence, they were more susceptible to genre
differences. Recent CDC work has sought Infor-
mation Extraction (IE) support. Extracted NEs
and relationships were considered in (Niu et al,
2004) for improved CDC performance.
Many of these earlier CDC methods were
evaluated on small and tidy news articles. CDC
for Web documents is even more challenging.
(Wan et al, 2005) proposed a web person
resolution system called WebHawk, which
extracted several attributes such as title,
organization, email and phone number using
patterns. These features however only covered
small amount of disambiguation evidence and
certain types of web pages (such as personal
home pages). The more recent Web Person
Search (WePS) task (Artiles et al, 2007) has
created a benchmark dataset which is also used
in this work. Different from CDC which aims to
resolve mention level NEs, WePS distinguishes
documents retrieved by a name search query
according to the underlying identity. The top-
performing system (Chen and Martin, 2007)
in this task extracted phrasal contextual and
document-level entities as rich features for
coreference. Similar IR features are also used by
other WePS systems as they are more robust to
the variety of web pages (Artiles et al, 2007).
Instead of focusing on local information, (Li
et al, 2004) proposed a generative model of
entity co-occurrence to capture global document
level information. However, inference in gen-
erative models is expensive for large scale web
data. Our work instead considers document cat-
egories/topics that can be efficiently predicted
and easily interpretable by users. Hand-tuned
weights were used in (Baron and Freedman, 2008)
and a linear classifier was used in (Li et al,
2004) to combine the extracted features. Our
composite pairwise coreference function is based
on an ensemble classifier and is more robust and
capable of handling inactive features.
3 Text Categorization Aided CDC
Consider the following scenario for motivation.
When a user searches for ?Michael Jordan?,
the official web page of the basketball player
484
?Michael Jordan?1 contains mostly his career
statistics, whereas the homepage of ?Michael
I. Jordan? the professor2 contains his titles,
contact information and advising students.
Neither of these pages contain complete natural
language sentences that most IE and NLP tools
are designed to process. We propose to use
document categories (trained from a very large
scale and general purpose taxonomy, Open
Directory Project (ODP)) as document level
features for CDC. In this example, one can easily
differentiate these namesakes by categorizing the
former as ?Top/Sports/Basketball/Professional?
and the latter as ?Top/Computer/Artificial
Intelligence/Machine Learning?. We first
introduce the method to categorize Web
documents; then we show how to combine
these categories for coreferencing.
3.1 Very Large Scale Text Categorization
To handle the web CDC problem, the catago-
rization component needs to be able to catego-
rize documents of widely different topics. The
Open Directory Project (ODP), the largest and
most comprehensive human edited directory of
the Web3, contains hundreds of thousands of
categories labeled for 2 million Web pages. Lever-
aging this vast amount of web data and the large
Web taxonomy has called for the development of
very efficient text categorization methods. There
is significant research interest in scaling up to
categorize millions of pages to thousands of cat-
egories and beyond, called the many class classi-
fication setting (Madani and Huang, 2008). Flat
classification methods (e.g. (Crammer et al,
2006; Madani and Huang, 2008)), which treat
hierarchical categories as flat classes, have been
very successful due to their superior scalability
and simplicity compared to classical hierarchical
one-against-rest categorization. Flat methods also
achieve high accuracy that is on par with, or better
than, the traditional counterparts.
We adopt a flat multiclass online classification
algorithm Passive Aggressive (PA) (Crammer et
al., 2006) to predict ranked categories for web
1See www.nba.com/playerfile/michael jordan/index.html
2See www.eecs.berkeley.edu/?jordan/
3See http://www.dmoz.org/about.html for details.
documents. For a categorization problem with C
categories, PA associates each category k with a
weight vector wk, called its prototype. The degree
of confidence for predicting category k with re-
spect to an instance x4 (both in online training and
testing) is determined by the similarity between
the instance and the prototype ? the inner product
wk ? x. PA predicts a ranked list of categories
according to this confidence.
PA is a family of online and large-margin based
classifiers. Given an instance (xt, yt) during
online learning, the multiclass margin marg in
PA5 is the difference between the score of the true
category yt and that of the highest ranked false
positive category s, i.e.
marg = wyt ? xt ?ws ? xt (1)
where s = argmaxs 6=yt ws ? xt.A positive margin value indicates that the algo-
rithm makes a correct prediction. One is however
not only satisfied with a positive margin value, but
also seeks to achieve a margin value of at least
1. When this is not satisfied, the online algorithm
suffers a multiclass hinge loss:
Lmc(w; (xt, yt)) =
{
0 marg ? 1
1?marg otherwise
where w = (w1, ..,wC) denotes the concatena-
tion of the C prototypes (into a vector).
In an online learning step, the PA-II variant
updates the category prototype with the solution
of this constrained optimization problem,
wt+1 = argmin
w
1
2 ?w ?wt?
2 +A?2 (2)
s.t. Lmc(w; (xt, yt)) ? ?. (3)
Essentially, if the margin is met (also imply-
ing no misclassification), PA passively accepts
the current solution. Otherwise, PA aggressively
learns the new prototype which satisfies the loss
constraint and stays as close to the one previously
learned as possible. To cope with label noise, PA-
II introduces a slack variable ? in the optimization
4x is the vector representation of word frequencies of the
corresponding document, L2 normalized.
5For brevity of presentation, we consider the single label
multiclass categorization setting.
485
for a gentler update, a technique previously em-
ployed to derive soft-margin classifiers (Vapnik,
1998). A is a parameter that controls the aggres-
siveness of the update.
The solution to the above optimization problem
amounts to only changing the two prototypes
violating the margin in the update step:
wytt+1 = w
yt
t + ?xt wst+1 = wst ? ?xt
where ? = Lmc?xt?2+ 12A .To conclude, PA treats the hierarchy as flat cat-
egories for multiclass classification. It is similar
to Multiclass Perceptron (Crammer and Singer,
2003) but only updates two vectors per iteration
and thus is more efficient.
3.2 Categories as Coreference Evidence
Conceptually, the text categorization component
can be viewed as a function that maps a document
d to a ranked list of top K categories along with
their respective confidence scores, i.e.
?(d) = {< c1, s1 >, .., < cK , sK >}
We leverage these document categories to mea-
sure the pairwise similarity of any two docu-
ments, sim(?(du), ?(dv)), for entity disambigua-
tion. Given a taxonomy T , we first formally
define the affinity between a category c and one
of its ancestor category c? in T as:
affinity(c; c?) = 1? len(c, c
?)
depth(T )
where len is the length of the shortest path be-
tween the two categories and depth(T) denotes the
depth of the taxonomy. In other words, affinity is
the complementary of the normalized path length
between c and its ancestor c?.
Using graph theory terminology, LCA(c1, c2)
denote the lowest common ancestor of two cate-
gories c1 and c2 in T . Given two category lists,
?(du) = {< cu1 , su1 >, .., < cuK , suK >} and
?(dv) = {< cv1, sv1 >, .., < cvK , svK >}, we use
the LCA(cui , cvj ) of each category pair cui and cvj
as the basis to measure similarity. Formally, we
transform ?(du) to a K ?K dimensional vector:
~v(du) = [affinity(cui ;LCA(cui , cvj )) ? sui ]T (4)
where i, j = 1..K. In other words, we project
?(du) into a vector in the space spanned by the
LCAs of category pairs. Using the same bases,
we can derive ~v(dv) analogically.
With this transformation, ?(du) and ?(dv)
are expressed in the common bases, i.e. their
LCAs. Therefore, the similarity between the top
K categories of two documents can be measured
by the inner product of these two vectors:
sim(?(du), ?(dv)) = ~v(du) ? ~v(dv) (5)
3.3 Empirical Studies
To handle the diverse topics of Web documents,
we leverage the ODP data to train the many class
categorization algorithm. The public ODP data
contains 361,621 categories and links to over 2
million pages. We crawled the original web pages
from these links, which yielded 1.9 million pages
(50GB in size). The taxonomy was condensed to
depth three6 and then very rare categories (having
less than 5 instances) were discarded. The data
set is created with these categories and the vector
representation of the term weights of the extracted
raw text. This dataset has 1,889,683 instances and
4,891 categories in total. Finally, stratified 80-
20 split was performed on this dataset, i.e. 1.5M
pages for training and 377K pages for testing.
Figure 1: Categorization performance at different
positions in the ODP test set.
As we view the taxonomy as a set of flat
categories and we are interested in the top K
categories, we use the recall at K metric for eval-
uation. Recall at K is defined as the percentage
of instances having their true category ranked
6The original taxonomy has average depth 7, which is
too deep for the coreference purpose in this work and many
categories have too few instances for training.
486
among the top K slots in the category list. For
a single label dataset (most ODP pages have one
category) and K = 1, this is the accuracy metric
in multiclass classification. Note that in the many
class setting, recall at 1 is a very strict metric
as no credit is given for predicting the parent,
children or sibling categories; also, documents
may have valid secondary topics not labeled by
humans. Figure 1 shows recall at K in the test
set. We observe that the algorithm is able to
predict the category for 58.7% of the instances
in the first rank and more than 77% in top three.
There is only diminishing gains when we consider
the categories further down the list. Hence we
choose to use the similarity of the top 1 and top
3 categories (named TC1 and TC3, respectively)
and study their contributions for the CDC task.
3.4 Remarks
In this section, the entire document in the rep-
resentation of its categories is used as a unit
of analysis for CDC. Categorization based CDC
works best with namesakes appearing in docu-
ments of relatively heterogenous topics, which
is usually the case for web documents. Indeed,
experienced web searchers would add terms such
as ?baseball player? to the name search queries for
more relevant results; Wikipedia also (manually)
disambiguates namesakes by their professions.
Categorization can also be adopted as a robust
faceted search system for handling name search
queries: users select the interested category/facet
to efficiently disambiguate and filter out irrelevant
results. The majority of web persons can be
readily distinguished by the different underlying
categories of the documents where they appear.
For more homogeneous corpora or less benevolent
cases, the next sections introduce two comple-
mentary CDC strategies.
4 Information Extraction for CDC
Consider the following two snippets retrieved
with regard to the query ?George Bush?:
[Snippet 1]: ?George W. Bush and Bill Clinton
are trying to get Congress to allow Haiti to triple
the number of exports ...?
[Snippet 2]: ?George H. W. Bush succeeded
Reagan as the 41st U.S. President.?
Using categories alone in this case is insuffi-
cient as both will be assigned similar categories
such as ?Politics? or ?History/U.S.?. Also, it?s not
uncommon for these entities to co-occur in the
same document and thus making them even more
confounding. Properly disambiguating these two
mentions requires the usage of local informa-
tion: for instance, the extraction of full names,
the detection of co-occurring NEs and contextual
information. We introduce an IE system that
extracts precise disambiguation evidence in this
section and describe using the extraction context
as additional information in the next section.
Our CDC system leverages a state-of-the-art
commercial IE system AeroText (Taylor, 2004).
The IE system employs manually created knowl-
edge bases with statistically trained models to
extract named entities, detect, classify and link
relations between NEs. A summary of the most
important IE-based features that we use are listed
in Table 1. Based on the extracted attributes and
relations, we further define their pairwise simi-
larity used as coreference features. This ranges
from simple compatibility checking for ?gender?,
textual soft matching for ?names?, to sophisticated
semantic matching for ?mentions? and ?locations?
using WordNet. (Huang et al, 2009) provides
more detailed discussions on the development of
these IE based coreference features.
We note that several existing state-of-the-art
IE systems are also capable of extracting these
features. In particular, Named Entity Recognition
(NER) which focuses on a small set of predefined
categories of named entities (e.g. persons, orga-
nization, location) as well as the detection and
tracking of preselected relations have achieved
venerable empirical success in practice7. Also,
within document coreference is a mature and
well-studied technology in NLP (e.g. (Ng and
Cardie, 2002)). Therefore, our CDC system can
readily adopt alternative IE toolkits.
5 Context Matching
As mentioned earlier, achieving high extraction
accuracy and coverage for diverse web documents
7The Automatic Content Extraction (ACE) evaluation
and the Text Analysis Conference (TAC) also have IE-based
entity tracking tasks that are relevant to this component.
487
is still a challenging and open research problem
even for the state-of-the-art IE systems. We note
that one of the natural outcomes from extraction is
the context of the NE of interest, which covers the
NE with its surrounding text. For a specific NE,
our CDC system uses the context built from the
sentences which form the NE?s within document
coreference chain. The context is then represented
as a term vector whose terms are weighted by the
TF-IDF weighing scheme. For a pair of NEs, the
context matching component measures the cosine
similarity of their context term vectors.
Essentially, this component alone is similar to
the method presented in the seminal CDC work
in (Bagga and Baldwin, 1998b). We however note
that simply applying a predetermined threshold on
the context similarity for CDC as in this earlier
work is not sufficient. First, this method narrowly
focuses on the local word occurrence and may
miss the big picture, i.e. the correlation that exists
in the global scope of a document. Also, mere
word occurrence is incapable of accounting for the
variation of word choices or placing special em-
phases on evidence such as co-occurring named
entities, relations, etc. The categorization and IE
components presented earlier in this work over-
come these two pitfalls of the simple IR-based
approach. We will further showcase the advantage
of our comprehensive approach in section 7.2.
6 Composite Pairwise Coreference
In the previous sections, we describe the com-
ponents to obtain document, sub-document and
entity level disambiguation evidence in detail. In
this section, we propose to use Random Forest
(RF) to combine the experts components into one
single composite pairwise similarity score. RF is
an ensemble classifier, composed of a collection
of randomized decision trees (Breiman, 2001).
Each randomized tree is built on a different boot-
strap sample of the training data. Randomness is
also introduced into the tree construction process:
the variable selection for each split is conducted
not on the entire feature set, but from a small
random subset of features. Gini index is used as
the criteria in selecting the best split. Additionally,
each tree is unpruned, to keep the prediction
bias low. By aggregating many trees that are
lowly-correlated (through bootstrap sampling and
random variable selection), RF also reduces the
prediction variance.
An ensemble method such as Random Forests
is very suitable for the CDC task. First, the col-
lection of randomized decision trees is analogous
to a panel of different experts, where each makes
its decision using different criteria and different
features. Previously, RF has been used to aggre-
gate various features in the author disambiguation
task (Treeratpituk and Giles, 2009). One of the
significant challenges in combining these different
features in our CDC setting is that not all of them
are always active. For instance, the IE tool may
extract an employment relation for one entity and
a list relation for another. Also, when the IE
tool cannot infer the gender information or when
the categorization component does not confidently
predict the top K categories (e.g. all with low
scores), it?s desirable to not supply those features
for coreferencing. The traditional technique to
impute the missing values, e.g. by replacing them
with the mean value, is not suitable in this case.
In our work, we specify a special level ?NA? in
the decision tree base learner. In our development
set, this treatment improves pairwise coreference
accuracy by more than 6%.
Figure 2 shows the convergence plot of the
composite pairwise coreference function based on
Random Forest8. We observe that the Out-Of-Bag
8The R random forest (Liaw and Wiener, 2002) was used.
Figure 2: Convergence of OOB errors of the
composite pairwise coreference function using the
training portion of the WePS dataset.
488
(OOB) errors 9 drastically decrease with the first
50 trees and then level off (without signs of over-
fitting). Thus we choose to use the model built
with the first 100 trees for prediction. Overall, our
model can achieve more than 85% accuracy for
pairwise coreference prediction.
7 Experiments
We evaluate our CDC approach with the bench-
mark dataset from the ACL-2007 SemEval Web
Person Search (WePS) evaluation campaign (Ar-
tiles et al, 2007). The WePS task is: given a name
search query, cluster the search result documents
according to the underlying referents. Compared
to the CDC task which clusters mention level
entities, a simplifying assumption is made in this
task that each document refers to only one identity
with respect to the query. The WePS dataset
contains the training and test set. The training
set contains the top 100 web search results of
49 names from the Web03 corpus (Mann and
Yarowsky, 2003), Wikipedia and European Con-
ference on Digital Library (ECDL) participants;
the test data are comprised of the top 100 docu-
ments of 30 names from Wikipedia, US Census
and ACL participants.
Table 1: Expert component and their feature sets.
Feature Component Description
TC1 Categorization Sim. of the top 1 categoriesTC3 Sim. of the top 3 categories
CNTX Context Sim. of context
NAME
IE (attribute)
Sim. of full/first/last names
MENT Sim. of mentions
GEND Sim. of genders
EMP
IE (relation)
Sim. of full/first/last names
LIST Sim. of co-occurring persons
LOC Sim. of locations
FAM Sim. of family members
7.1 Evaluation of Pairwise Coreference
We conduct a thorough study of the importance
of the individual expert components and their
features with the WePS training set. Table 1 shows
the three components of the systems, their main
features and descriptions.
The importance of these expert components and
their features are illustrated in Figure 3. One of
9OOB error is an unbiased estimate of test error in RF
(Breiman, 2001), computed as the average misclassification
rates of each tree with samples not used for its construction.
Figure 3: Importance of the expert components
and their features found by Random Forest (note
the small spread in MeanDecreaseAccuracy).
the most important features is CNTX, this confirms
that the prior work on CDC (e.g. (Bagga and
Baldwin, 1998b)) can achieve good results with
the IE-driven context similarity feature (or its vari-
ation). The text categorization component also
contributes very important features. In particular,
TC3 is more significant than TC1 for reducing
the Gini index because it recalls more correct
categories. On the other hand, TC1 is slightly
more important than TC3 for its contribution to
accuracy, indicating TC1 is more precise (with
less noise categories). For the IE component,
attribute features NAME and MENT are the most
useful. As aforementioned, the IE component
may not always extract the relation features such
as EMP, LIST, LOC and FAM, and hence they
seemingly have limited effect on model learning
(with relatively low reduction in Gini index).
These relation features are however very accu-
rate when extracted and are present for predic-
tion. Therefore, they are strong disambiguation
evidence and their removal would significantly
hamper performance.
7.2 Evaluation for Web Person Search
Using the confidence of the pairwise corefer-
ence prediction as a distance metric, we adopt a
density-based clustering method DBSCAN (Ester
et al, 1996) as in (Huang et al, 2006)10 to induce
the person clusters. The final set of evaluation is
based on these person clusters generated for the
WePS test set.
Two sets of metrics are used to evaluate the
overall system. First, we use the B-CUBED
10DBSCAN is a robust and scalable algorithm suitable
for clustering relational data. In interest of space, we refer
readers to (Ester et al, 1996) for the original algorithm.
489
Table 2: Cross document coreference perfor-
mance (I. Pur. denotes inverse purity).
Method Purity I. Pur. F B-CUBED
CDC 0.812 0.796 0.793 0.775
CNTX 0.863 0.601 0.678 0.675
TC1+3 0.620 0.776 0.660 0.634
OIO 1.000 0.482 0.618 0.618
AIO 0.279 1.000 0.389 0.238
scores designed in (Bagga and Baldwin, 1998a)
for evaluating cross document coreference perfor-
mance. Second, we use the purity, inverse purity
and their F score as in WePS (Artiles et al, 2007).
Purity penalizes placing noise entities in a cluster,
while inverse purity penalizes splitting coreferent
entities into separate clusters.
Table 2 shows the performance of the
macro-averaged cross document coreference
performance on the WePS test sets. Note that
though our evaluation is based on the mention
level entities, the baselines One-In-One (OIO,
placing each entity in a separate cluster) and All-
In-One (AIO, putting all entities in one cluster)
have almost identical results as those in the
evaluation11. OIO can yield good performance,
indicating that the names in test data are highly
ambiguous. As alluded to in the title, context and
categories both are very useful disambiguation
features. CNTX is essentially very similar to the
system presented in (Bagga and Baldwin, 1998b)
and is a strong baseline12 (outperforming 3/4
of the systems in WePS). Note that CNTX has
high purity but inferior inverse purity, indicating
that using the context extracted by the IE system
alone is unable to link many coreferent entities.
Interestingly, we observe that using only the
top-K categories (TC1+3) can also achieve
competitive F score, though in a very different
manner. TC1+3 recalls much more coreferent
entities (significantly improving inverse purity),
but at the same time also introduces noise.
Finally, adding document categories and using
IE results (i.e. using all features in Table 1),
our CDC system achieves 22% and 18% relative
11Most person names in this set have only one underlying
identity per document; thus the results are comparable
despite the simplifying assumption of the WePS evaluation.
12We use context similarity 0.2 as the clustering threshold
(which has the best performance in training data).
improvement compared to CNTX in F (purity)
and B-CUBED scores, respectively. In particular,
inverse purity improves by 46% relatively, imply-
ing that the additional evidence significantly im-
proves the recall of coreferent entities (when there
is a lack of context similarity in the traditional
method). Overall, the comprehensive approach
in this work outperforms the top-tiered systems in
the WePS evaluation.
8 Conclusion and Future Work
This work proposes a synergy of three levels of
analysis for the web cross document coreference
task. On the document level, we use text cate-
gories, trained from thousands of ODP categories
and over a million pages, as a concise representa-
tion of the documents. Categorization is a robust
strategy for coreferencing web documents with
diverse topics, formats and when there is a lack of
extraction coverage or word matching. Two types
of sub-document level evidence are also used in
our approach. First, we apply an information ex-
traction system to extract attributes and relations
of named entities from the documents and per-
form within document coreference. Second, we
use the context of the entities, a natural outcome
of the IE system as a focused description of the
named entity that may miss the extraction process.
A CDC system has been implemented based on
the IE and the text categorization components
to provide a comprehensive solution to the web
CDC task. We demonstrate the importance of
each component in our system and benchmark
our system with the WePS dataset which shows
superior CDC performance.
There are a number of interesting directions for
future research. Recently, Open IE was proposed
in (Etzioni et al, 2008) for Web information
extraction. This can be a more powerful alter-
native to traditional IE toolkits for Web CDC,
though measuring the semantic similarity for a
vast variety of relations can be another research
issue. Employing external background knowledge
such as Wikipedia (Han and Zhao, 2009) while
maintaining scalability can also be an orthogonal
direction for further improvement.
490
References
Artiles, Javier, Julio Gonzalo, and Satoshi Sekine.
2007. The SemEval-2007 WePS evaluation:
Establishing a benchmark for the web people search
task. In Proceedings of the 4th International
Workshop on Semantic Evaluations (SemEval),
pages 64?69.
Bagga, Amit and Breck Baldwin. 1998a. Algorithms
for scoring coreference chains. In First Inter-
national Conference on Language Resources and
Evaluation Workshop on Linguistics Coreference.
Bagga, Amit and Breck Baldwin. 1998b. Entity-based
cross-document coreferencing using the vector
space model. In Proceedings of the 36th ACL and
17th COLING, pages 79?85.
Baron, Alex and Marjorie Freedman. 2008. Who
is who and what is what: experiments in cross-
document co-reference. In Proceedings of the
Conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 274?283.
Breiman, Leo. 2001. Random forests. Machine
Learning, 45(1):5?32.
Chen, Ying and James Martin. 2007. Towards robust
unsupervised personal name disambiguation. In
Proc. of EMNLP and CoNLL, pages 190?198.
Crammer, Koby and Yoram Singer. 2003. A family of
additive online algorithms for category ranking. J.
Machine Learning Research, 3:1025?1058.
Crammer, Koby, Ofer Dekel, Joseph Keshet, Shai
Shalev-Shwartz, and Yoram Singer. 2006. Online
passive-aggressive algorithms. Journal of Machine
Learning Research (JMLR), 7:551?585.
Ester, M., H. Kriegel, J. Sander, and X. Xu. 1996. A
density-based algorithm for discovering clusters in
large spatial databases with noise. In Proceedings
of the 2nd KDD Conference, pages 226 ? 231.
Etzioni, Oren, Michele Banko, Stephen Soderland,
and Daniel S. Weld. 2008. Open information
extraction from the web. Communications of ACM,
51(12):68?74.
Gooi, Chung H. and James Allan. 2004. Cross-
document coreference on a large scale corpus. In
Proceedings of HLT-NAACL 2004, pages 9?16.
Han, Xianpei and Jun Zhao. 2009. Named entity
disambiguation by leveraging Wikipedia semantic
knowledge. In Proceedings of the 18th Conf. on
Information and knowledge management (CIKM),
pages 215?224.
Huang, Jian, Seyda Ertekin, and C. Lee Giles.
2006. Efficient name disambiguation for large scale
databases. In Proc. of 10th European Conference
on Principles and Practice of Knowledge Discovery
in Databases (PKDD), pages 536 ? 544.
Huang, Jian, Sarah M. Taylor, Jonathan L. Smith,
Konstantinos A. Fotiadis, and C. Lee Giles. 2009.
Profile based cross-document coreference using
kernelized soft relational clustering. In Proceedings
of the 47th Annual Meeting of the Association for
Computational Linguistics (ACL), pages 414?422.
Li, Xin, Paul Morie, and Dan Roth. 2004. Robust
reading: Identification and tracing of ambiguous
names. In Proceedings of the Human Language
Technology Conference and the North American
Chapter of the Association for Computational
Linguistics (HLT-NAACL), pages 17?24.
Liaw, Andy and Matthew Wiener. 2002. Classification
and regression by randomforest. R News, 2(3).
Madani, Omid and Jian Huang. 2008. On updates
that constrain the features? connections during
learning. In Proceedings of the 14th ACM SIGKDD
International Conference on Knowledge Discovery
& Data Mining (KDD), pages 515?523.
Mann, Gideon S. and David Yarowsky. 2003.
Unsupervised personal name disambiguation. In
Proceedings of the seventh conference on Natural
language learning (CoNLL), pages 33?40.
Ng, Vincent and Claire Cardie. 2002. Identifying
anaphoric and non-anaphoric noun phrases to im-
prove coreference resolution. In Proceedings of the
19th International Conference on Computational
Linguistics (COLING), pages 1?7.
Niu, Cheng, Wei Li, and Rohini K. Srihari.
2004. Weakly supervised learning for cross-
document person name disambiguation supported
by information extraction. In Proceedings of
the 42nd Annual Meeting on Association for
Computational Linguistics (ACL), pages 598?605.
Taylor, Sarah M. 2004. Information extraction tools:
Deciphering human language. IT Professional,
6(6):28 ? 34.
Treeratpituk, Pucktada and C. Lee Giles. 2009.
Disambiguating authors in academic publications
using random forests. In Proceedings of the
ACM/IEEE Joint Conference on Digital libraries
(JCDL), pages 39?48.
Vapnik, V. 1998. Statistical Learning Theory. John
Wiley and Sons, Inc., New York.
Wan, Xiaojun, Jianfeng Gao, Mu Li, and Binggong
Ding. 2005. Person resolution in person search
results: WebHawk. In Proceedings of the 14th
ACM International Conference on Information and
Knowledge management (CIKM), pages 163?170.
491
Proceedings of NAACL-HLT 2013, pages 259?269,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Measuring Term Informativeness in Context
Zhaohui Wu
Computer Science and Engineering
The Pennsylvania State University
University Park, PA 16802, USA
zzw109@psu.edu
C. Lee Giles
Information Sciences and Technology
Computer Science and Engineering
The Pennsylvania State University
University Park, PA 16802, USA
giles@ist.psu.edu
Abstract
Measuring term informativeness is a funda-
mental NLP task. Existing methods, mostly
based on statistical information in corpora, do
not actually measure informativeness of a term
with regard to its semantic context. This pa-
per proposes a new lightweight feature-free
approach to encode term informativeness in
context by leveraging web knowledge. Given
a term and its context, we model context-
aware term informativeness based on semantic
similarity between the context and the term?s
most featured context in a knowledge base,
Wikipedia. We apply our method to three ap-
plications: core term extraction from snippets
(text segment), scientific keywords extraction
(paper), and back-of-the-book index genera-
tion (book). The performance is state-of-the-
art or close to it for each application, demon-
strating its effectiveness and generality.
1 Introduction
Computationally measuring importance of a word
in text, or ?term informativeness? (Kireyev, 2009;
Rennie and Jaakkola, 2005), is fundamental to many
NLP tasks such as keyword extraction, text catego-
rization, clustering, and summarization, etc. Various
features derived from statistical and linguistic infor-
mation can be helpful in encoding term informative-
ness, whereas practical feature definition and selec-
tion are usually ad hoc, data-driven and application
dependent. Statistical information based on term
frequency (TF) and document frequency (DF) tend
to be more effective in finding keywords in large
corpora, but can have issues with small amounts of
text or small corpora. Linguistic information such
as POS tag patterns often require manual selection
based on prior applications. We contend that few
methods actually measure the informativeness of a
term to the discourse unit it contains. For example,
given a context such as ?A graph comprises nodes
(also called vertices) connected by links (also known
as edges or arcs)?, it is difficult to measure the
term informativeness of ?graph?, ?nodes?, or ?links?
based on any statistical or linguistic information.
This raises many issues. Is there a fundamental
and less ad hoc way to measure the term informa-
tiveness of a word within a discourse unit? Can we
actually find a general approach based on compre-
hensive and high-level ?knowledge? and not have
to nitpick over features? Can this new metric be
effectively applied to real world applications? To
answer these questions, we develop a new term in-
formativeness metric, motivated by query-document
relevance in information retrieval. The higher the
relevance score a query-document pair is, the more
informative the query is to the document. If a sim-
ilar principle also exists between word and con-
text and there is an effective search engine return-
ing ranked contexts for a given word, then we con-
tend that word is more informative in the higher rank
contexts. To see the term informativeness of three
words ?graph?, ?nodes? and ?links? in context, we
manually check the search results from Wikipedia,
Google, and Bing. We found that very similar con-
texts are among the top 5 ranked results of ?graph?
while no such contexts appear in that of the other
two words. Thus, we define a context-aware term
informativeness based on the semantic relatedness
259
between the context and the term?s featured contexts
(or the top important contexts that cover most of a
term?s semantics).
We apply the context-aware term informativeness
(CTI) to three typical NLP applications: core term
extraction in snippets, keyword extraction and back-
of-the-book index generation. Experiments show
that the method is effective and efficient. Moreover,
the metric can be easily combined with other meth-
ods, or as a feature for learning algorithms.
The remainder of this paper is organized as fol-
lows. Section 2 reviews the literature of term infor-
mativeness measurements. Section 3 proposes the
formal definition of the context-aware term informa-
tiveness as well as its practical implementation using
Web knowledge. Section 4 studies the three appli-
cations. Finally, we conclude with discussion and
future work.
2 Related Work
Most known approaches to measure term informa-
tiveness fall into basically two categories: statistics-
based and semantic-based.
Statistics-based methods, such as TFIDF (Salton
and Buckley, 1988), ResidualIDF(RIDF), Variance,
Burstiness and Gain, are based on derivations from
term frequency (TF) and document frequency (DF).
Sprck Jones defines IDF or inverse document fre-
quency as:
IDF (w) = ?log2(dfw/D) (1)
where D is the size of the corpus (Jones, 1972;
Jones, 1973). Based on a finding that informative
words tend to have large deviation between IDF
and collection frequency fw(the total number of oc-
currence of a word), many other informativeness
scores have been proposed. Bookstein and Swan-
son (Bookstein and Swanson, 1974) introduced the
xI as:
XI = fw ? dfw
Church and Gale (1995) introduced
variance(w) = 1D ? 1
D
?
d=1
(tdw ? t?w) (2)
where tdw denotesw?s TF in d and t?w = fw/D indi-
cates its mean expected word rate. Another measure
suggested by them is
burstiness(w) = fwdfw
(3)
which tends to compare collection frequency and
document frequency directly. Informative words
were found to have IDF scores that are larger than
what would be expected according to the Poisson
model; residual IDF (RIDF) was introduced to mea-
sure this deviation
RIDF (w) = IDF (w)? ?IDF (w) (4)
where ?IDF (w) = ?log2(1 ? e?t?w). In addition,
Papineni (2001) introduced the notion of gain as
gain(w) = dfwD
(dfw
D ? 1? log(
dfw
D )
)
(5)
More recently, Rennie and Jaakkola (2005) intro-
duced an informativeness score based on the fit of
a word?s frequency to a mixture of 2 Unigram dis-
tribution and applied it to named entity detection. It
is worth noting that term necessity, which measures
the probability that a term occurs in documents rel-
evant to a given query, has been well studied in In-
formation Retrieval community (Zhao and Callan,
2010; Yang and Callan, 2010). Though our CIT is
not designed for probabilistic retrieval models, we
may apply it to measure the term necessity in a query
by considering it as a context.
Despite extensive research on semantic analysis
and understanding of word and text (Deerwester et
al., 1990; Budanitsky and Hirst, 2006; Cilibrasi and
Vitanyi, 2007; Gabrilovich and Markovitch, 2007;
Agirre et al, 2009; Yazdani and Popescu-Belis,
2012), little work studied the measurement of the
semantics of term informativeness. An exception
is the LSAspec from Kireyev (2009), based on la-
tent semantic analysis (Deerwester et al, 1990),
which is defined as the ratio of a term?s LSA vec-
tor length to its document frequency and thus can
be interpreted as the rate of vector length growth.
However, latent semantic models such as LSA are
notoriously hard to interpret since the ?latent con-
cepts? cannot be readily mapped to human knowl-
edge (Gabrilovich and Markovitch, 2007). Our ap-
proach explicitly leverages the semantics of word
and text using existing knowledge bases.
260
Previous methods, all corpus-based, might be ef-
fective in identifying informative words at the doc-
ument or corpus level, but do not the ability to cap-
ture term informativeness in a particular context due
to their absence of semantics and obliviousness of
context. Our method measures the term informative-
ness within a context in a semantic-based approach,
regardless of the absence of statistical information.
3 Context-aware Term Informativeness
3.1 Context
A context of a word or phrase may refer to a few
words nearby (He et al, 2010), a sentence or para-
graph (Soricut and Marcu, 2003), or even a set
of documents containing it (Cilibrasi and Vitanyi,
2007). Here we define context as a syntactic unit
of discourse such as a sentence or paragraph, for ex-
ample, ?PL/SQL is one of three key programming
languages embedded in the Oracle Database?, or
?There are two types of functions in PL/SQL?. The
universal context set U(t) of a word t is defined as
all the contexts containing it in the web. Different
contexts vary in their authority just like web pages
vary. For the two examples, we could argue that
the first context is much more ?authoritative? than
the second. This can be verified by their popular-
ity on Google; (all results from actual search en-
gines were at the time of this publication) the first
retrieves approximately 302,000 exact matching re-
sults while the second retrieves only one. We con-
sider this as the number of citations of a context,
which, to some extent, indicates its ?authority?. We
define the source of a context as the set of all docu-
ments citing it. Here ?citing? instead of ?containing?
is used because some documents may not literally
contain an exact copy of the context.
Given a term t, define its universal context set
U(t) = {ci} and the source of ci is S(ci) = {dij}.
Ideally, the authority of a context will be contributed
by every document citing it. Therefore, we define
the authority score of a context as
CA(ci) =
?
j
DA(dij) (6)
where DA(dij) denotes the authority contributed by
dij . It is very difficult to acquire the universal con-
text set of a term. Considering that usually we only
care about the top few results of a query returned by
search engines and ignore a large faction of less im-
portant ones, it is reasonable to assume that a term?s
semantics will be well covered by a few important
contexts. We therefore define the featured context
set of term t, or Uf (t), as the top k contexts with
the highest authority scores, where k is an applica-
tion dependent parameter. In our experiments, the
default k for the Wikipedia based implementation is
20.
3.2 Term Informativeness
We now consider how to measure the term informa-
tiveness in context. Using the context ?PL/SQL is
one of three key programming languages embedded
in the Oracle Database? (denoted by Cp) as an ex-
ample, for its term ?PL/SQL?, the top three contexts
returned by Google are
1. PL/SQL (Procedural Language/Structured Query Language) is
Oracle Corporation?s procedural extension language for SQL and
the Oracle relational database.
2. PL/SQL is Oracle?s procedural extension to industry-standard
SQL. PL/SQL naturally, efficiently, and safely extends SQL.
3. This Oracle PL SQL tutorial teaches you the basics of program-
ming in PL/SQL like cursors, stored procedures, PlSQL func-
tions.
Those contexts, though being diverse in actual
meaning, all have semantic relatedness to Cp. Even
someone who does not completely understand them
can gain some meaning by observing common
words such as ?Oracle?, ?database? and ?program-
ming?. However, checking the Google results for
?Oracle Database? or ?programming languages?, we
will find little relatedness between them and Cp.
This suggests that if term ta in context c is more in-
formative than tb, then most likely the contexts from
ta?s featured context set will be more related to c
than will tb. Thus, given a term t and its featured
context set Uf (t) = {c1, ..., ck}, we define the term
informativeness of t in context ci as
I(t, ci) =
?
cj?Uf (t)
?(ci, cj) ? CA(cj) (7)
where ?(ci, cj) is the semantic relatedness of
ci and cj , which can be computed by various
semantic relatedness metrics such as Wikipedia
based (Gabrilovich and Markovitch, 2007; Yazdani
261
and Popescu-Belis, 2012), Wordnet based (Agirre et
al., 2009; Budanitsky and Hirst, 2006), or simple co-
sine similarity and Jaccard similarity based (Zobel
and Moffat, 1998).
The context-aware term informativeness (CTI) in-
troduced above is a formal and general definition.
As such the definition in Equation (7) includes sev-
eral features such as context authority score, fea-
tured context set, semantic relatedness, and knowl-
edge base, any or all of which could be flexible for
different applications.
3.3 Implementation
Here, we present a simple practical implementation
using Wikipedia as the knowledge base and the con-
text authority estimated by the discounted rank of
the Wikipedia document. Note that the problem is
how to compute CA(cj) for each context in Uf . We
rewrite Equation (6) as
CA(ci) = DA(di0) +
?
j ?=0
DA(dij) (8)
where di0 is the original document of ci and all the
others are further derivatives of ?citing? ci. For ex-
ample, theWikipedia page of ?PL/SQL? will be con-
sidered as the original document of Cp while all
other documents citing Cp are its derivatives. Intu-
itively, the authority of a context will mainly rely on
the authority of its original document. Here, we sim-
ply assume that the context authority depends only
on its original document, or
CA(ci) ? DA(di0) (9)
We then take the top ranked document returned by
the web knowledge base as the original document.
We present a practical implementation of CTI in
Algorithm 1. The discounted rank is used to rep-
resent the relative context authority score of each
context in Uf . We use Wikipedia as our knowl-
edge base to implement the metric since it is cur-
rently one of the largest and most readily available
knowledge repositories and, more importantly, pro-
vides free, unlimited and fast query APIs1. Given
any keyword, the Wikipedia query API will return
the ranked Wikipedia entries along with the contexts
containing the keyword. We set the default value 20
1http://www.mediawiki.org/wiki/API:Query
for k, or len(Uf ). Note that there could be other
variations of this implementation. For example, we
could rule out duplicate or very similar results in the
Uf . Search engines such as Google and Bing are
also potential sources since they return high qual-
ity web pages along with the contexts containing the
query keyword.
In terms of scalability, the proposed method is
inherently parallelizable, not only at the document
level, but also a the context level, since computing
CTI does not depend on any other context in the doc-
ument. In addition, we do not need to issue the same
query more than once. Our strategy is to locally
cache the returned results of every seen query. For a
new term seen in a previous query, we can directly
access the local cached file. If we have built a large
local pool, the queries will rarely go to a search en-
gine or other source. Given a corpus size N (words
in total), the number of actual issued queries will
be at most the number of unique terms, which is far
less than O(N). Of course, new terms never seen will
have to be processed, but there will be fewer of these
over time.
Algorithm 1: Wikipedia-based I(t, ci)
1 Input: t, ci
2 Output: I(t, ci)
3 begin
4 I ?? 0;
5 Uf ?? queryWikipedia(t);
6 for j ? range(len(Uf )) do
7 s?? ?(ci, Uf [j]);
8 if j > 0 then
9 I ?? I + s/ log(j + 1);
10 else I ?? I + s
11 return I;
4 Applications
4.1 Core Terms Extraction from Snippets
We first investigate CTI in a well defined setting.
That is, if we have a collection of terms such that
its most important context is a ?definition,? e.g.
?database? and ?A database is a structured collec-
tion of data, which are typically organized to model
relevant aspects of reality, in a way that supports
262
Exemplary snippets of computer science terms Top 5 terms ranked by CTI
Acrobat, a document exchange software from Adobe Systems, provides a platform-independent means of
creating, viewing, and printing documents. Acrobat can convert a DOS, Windows, UNIX or Macintosh
documents into a Portable Document Format (PDF) which can be displayed on any computer with an
Acrobat reader. The Acrobat reader can be downloaded free from the Adobe website.
Acrobat:3.19
Acrobat reader:2.94
Portable Document Format:2.08
Adobe website:2.03
Adobe Systems:1.82
Data mining (DM), also known as Knowledge-Discovery in Databases (KDD) or Knowledge-Discovery
and Data Mining (KDD), is the process of automatically searching large volumes of data for patterns. Data
mining uses automated data analysis techniques to uncover previously undetected relationships among
data items. Data mining often involves the analysis of data stored in a data warehouse. Three of the major
data mining techniques are regression, classification and clustering.
data mining:3.77
data mining techniques:3.64
KDD:1.79
Knowledge-Discovery:1.66
data analysis techniques:1.20
Firefox, also known as Mozilla Firefox, is a free, open source, cross-platform, graphical web browser
developed by the Mozilla Corporation and hundreds of volunteers. Firefox includes an integrated pop-up
blocker, tabbed browsing, live bookmarks, support for open standards, and an extension mechanism for
adding functionality. Although other browsers have some of these features, Firefox became the first such
browser to include them all and achieve wide adoption.
Mozilla Firefox:3.89
firefox:3.13
web browser:2.44
browser:2.39
graphical web browser:2.35
Table 1: Term ranked by CTI from exemplary snippets
processes requiring this information?, can CTI iden-
tify ?database? as the most informative term in this
context? To construct the term-context pairs, we
could use the Wikipedia title and the top ranked
context returned by searching the title using the
Wikipedia API. Then we could test our metric based
on other search engines such as Google or Bing.
Testing manually, we found the results compare well
to the search engine results, since both Google and
Bing give top ranks to Wikipedia pages if the query
keyword is a Wikipedia title. For further analy-
sis, we need a collection of term-context pairs from
other sources different from Wikipedia. Fortunately,
we found a list of 1255 computer science terms
with its definition snippets manually created by Web
users 2. The snippets are literally different from
those contexts in Wikipedia and some of the terms
are even not Wikipedia titles, e.g. bBlog, BetBug,
etc. These can be part of an ?initial? evaluation. The
core term extraction algorithm works in the follow-
ing steps for each term-context pair:
1. Extract all n-grams (1 ? n ? 4) in the context
as candidates
2. For each candidate, calculate its CTI using
Wikipedia based implementation
3. Return the top K highest CTI as core terms
We used the top 20 returned Wikipedia contexts
as a featured context set Uf and apply the cosine
similarity for ?. We show some exemplary snippets
2http://www.labautopedia.org/mw/index.php/List of
programming and computer science terms
K Precision (%) Recall (%) F1(%)
1 37.5 37.5 37.5
2 35.1 55.2 42.9
3 32.3 64.7 43.1
4 31.3 72.2 43.7
5 27.6 76.3 40.5
10 20.0 88.1 32.6
Table 2: Results on computer science term extrac-
tion from descriptive snippets
with its top 5 core terms and their CTI scores in Ta-
ble 1. The overall performance is shown in Table 2,
in terms of precision, recall and F1 scores based on
the only one titled term of each snippet as the ground
truth. CTI can correctly find the core term for 37.5%
snippets. If we take the top 5 results, then the recall
increase to 76.3%.
Though the algorithm can be easily parallelized,
sequentially runtime on all snippets took only
slightly more than a minute on a 2.35GHz Intel(R)
Xeon(R) 4 processors, 23GB of RAM, and Red Hat
Enterprise Linux Server(5.7) machine. However, the
time could vary due to network conditions.
Though these results look promising, but it could
be due to the high lexical similarity between this
dataset and Wikipedia content. To test on a more
general corpora, we explore more real world tasks.
4.2 Keyword Extraction
There is a rich literature on keyword extraction prob-
lem (Frank et al, 1999; Witten et al, 1999; Turney,
2000; Hulth et al, 2003; Tomokiyo and Hurst, 2003;
263
Wiki20 citeulike180
Method P R F P R F
TFIDF 13.7 17.8 15.5 14.4 16.0 15.2
KEA 18.4 21.5 19.8 20.4 22.3 21.3
CTI 19.6 22.7 21.0 18.5 21.4 19.8
Table 3: Results on Wiki20 and citeulike180
Mihalcea and Tarau, 2004; Medelyan and Witten,
2008; Liu, 2010), most of which is treated as a clas-
sification or ranking problem with corresponding
machine learning algorithms that use statistical and
linguistic features in a corpus. Here, we consider
the task as finding the most informative keywords in
a document. Given a document d = {ci}, our key-
word extraction algorithm based on CTI works as
follows.
1. For each context ci in a document, compute the
semantic relatedness s(ci, d) between ci and d
2. For each n-gram (1 ? n ? 4) t in ci, calculate
I(t, ci) using Wikipedia based implementation
3. Select the top keywords with the highest
?
i I(t, ci) ? s(ci, d)
Note that for the last step keywords are selected
based on a summarized weighted informativeness
score over a document. Obviously, the pure co-
sine or Jaccard similarity is not a good choice
to measure semantic relatedness between two text
segments of very low lexical similarity. We thus
use the Wikipedia based ESA (Gabrilovich and
Markovitch, 2007) to compute the semantic relat-
edness s(ci, d) and ?(ci, cj). To make the cal-
culation more efficient, only the Wikipedia pages
whose title is contained in the dataset are used to
build the concept space. We ran the algorithm on
several datasets including Wiki20 (Medelyan et al,
2008), citeulike180 (Medelyan et al, 2009) and Se-
mEval2010 (Kim et al, 2010) 3.
Though keyword extraction as a research topic
has a rich literature, to the best of our knowledge
there is no large scale datasets publicly available.
The Wiki20 dataset contains 20 computer science
articles each with around 5 terms labeled by 15
different teams. Every term is a Wikipedia title.
3http://code.google.com/p/maui-indexer/downloads/list
Method Precision (%) Recall (%) F1(%)
TFIDF 14.9 15.3 15.1
HUMB 27.2 27.8 27.5
CTI 19.3 20.1 19.7
CTI+ 25.3 26.2 25.7
Table 4: Results on SemEval2010
The citeulike180 contains a set of 180 papers each
tagged with around three tags by 332 users. For each
dataset, the collection of all labeled keywords by dif-
ferent taggers are considered as the gold standard
for a document. We use the set of all keywords for
evaluation; otherwise a more complicated evaluation
metrics for each dataset will be needed. It would
be better to investigate other weighting schemes.
However, the datasets here are relatively small and
the number of tags on which at least two annota-
tors agreed is significantly small; weighting the key-
words might not make too much difference. KEA 4
builds a Naive Bayes model using features TFIDF,
first occurrence, length of a phrase, and node de-
gree (number of candidates that are semantically re-
lated to this phrase) (Witten et al, 1999). First oc-
currence is computed as the percentage of the doc-
ument preceding the first occurrence of the term in
the document. We compute the node degree as the
textrank (Mihalcea and Tarau, 2004) degree in a doc-
ument by simply relating two candidate terms with
each other if they are in the same context. KEA
uses 5 fold cross validation. All precision P, re-
call R and F1 F results are over the top 10 candi-
date keywords and the micro-averaged results of the
first two datasets are shown in Table 3. The CTI-
based algorithm works better than KEA on Wiki20
but slightly worse on citeulike180. We argue that
the reason might be two-fold. First, CTI does not
use any inter-document or corpus information while
KEA learns from the corpus. As such, CTI might not
perform as well as supervised learning methods for a
domain dependent large corpus. Second, the labeled
keywords in Wiki20 are all Wikipedia titles while
those in citeulike are general tags labeled by volun-
tary web users. CTI would give more preference to
Wikipedia titles since their featured context set re-
turned from Wikipedia is more semantically repre-
sentative than other non-Wikipedia title words.
4http://www.nzdl.org/Kea/
264
Dataset #Books #Words #Contexts Main domains
Gutenberg 55 7,164,463 301,581 History, Art, Psychology, Philosophy, Literature, Zoology
Open Book 213 22,279,530 1,135,919 Computer Science, Engineering, Information Science
Table 5: Datasets for book index generation evaluation
The SemEval2010 dataset contains a set of 284
scientific papers with 15 keyphrases assigned by
readers and authors. 144 of them are selected as
training set while the other 100 are for testing. A
comparison of CTI to the results from TFIDF and
the best reported results HUMB (Lopez and Romary,
2010) is shown in Table 4. It achieves 19.8% by
micro-averaged F1 score, ranking 11th out of the 19
systems submitted to the competition (Kim et al,
2010). However, by adding the structural features
used by HUMB into CTI, we can improve the per-
formance by around 6%, making our results close
to that of HUMB. The structural information is en-
coded as weights for context that is located in ti-
tle, abstract, section titles and general content. Each
weight can be regarded as the prior probability that a
keyword will appear in the corresponding location,
whose value can be set according to the fraction of
the number of keyword occurrences of this type of
location with respect to the number of all keyword
occurrences in the entire training set. Here they are
set to be 0.3, 0.4, 0.25, and 0.05.
4.3 Back-of-the-book Index Generation
A back-of-the-book index (or book index) is a col-
lection of words or phrases, often alphabetically ar-
ranged as an index, created to give readers impor-
tant location of important information in a given
book. Usually indexing is done by freelancers hired
by authors or publishers, namely professional in-
dexers 5. Csomai and Mihalcea first evaluated the
performance of different informativeness measure-
ments for selecting book index terms (2007) and
then investigated automatic book index generation
in a supervised learning framework (2008) using
syntactic features, linguistical features, encyclope-
dic features, etc., as a keyword extraction problem
rather than building a actual book index.
A set of keywords is not a back-of-the-book in-
dex. What really matters for such an index is that
5http://www.asindexing.org/
an index term or phrase points to its proper loca-
tion in the text. For example, in ?pattern recognition
and machine learning? by Bishop, ?hidden Markov
model? appears in more than 20 pages while the
actual index entry has only 2 pages as its locators.
Thus the actual problem is to identify a index term
with its context. As such, learning a robust and ef-
ficient model for real book indexes is challenging.
First, books from different domains vary in vocabu-
lary composition and structure style, requiring vari-
ous indexing specialties. There are different index-
ing guides for medicine (Wyman, 1999), psychol-
ogy (Hornyak, 2002), and law (Kendrick and Zafran,
2001). Second, book indexing is a highly subjec-
tive work and indexes of different books are always
created by different professional indexers who have
their own preferences and background (Diodato and
Gandt, 1991; Diodato, 1994). Third, the training
set is extremely unbalanced. As we found in our
dataset, the index size is only 0.42% of the length of
book on average. All these motivate us to explore the
automatic creation of index terms that are aware of
the context at the term?s locations (locators). To do
so we propose the following efficient training-free
and domain independent approach:
1. For each context ci in a book, compute its
weight wi based on structural features
2. For each candidate term t in ci, calculate
I(t, ci) using Wikipedia based implementation
3. Select term-context pairs with the highest wi ?
I(t, ci) as index entries
The weight in step 1 represents the relative im-
portance of a context in a book. w(c) = 1 ?
cid(c)?cid(titlec)
Ntitlec
measures the weight based on the
normalized distance from the context to its direct
chapter or sub-chapter title, where cid(c) denotes the
id of context c, titlec the title of context c andNtitlec
the number of contexts under titlec. To select candi-
date terms, we first filter the improbable index terms
265
based on POS patterns using the Standard POS Tag-
ger (Toutanova et al, 2003). We then select multi-
word keyphrases based on Pointwise Mutual Infor-
mation (PMI) (Church and Hanks, 1990), which was
shown to be the best metric to measure word associ-
ations (Terra and Clarke, 2003).
To evaluate our back-of-the-book index gener-
ation method, we conduct extensive experiments
on books in various domains, from the Gutenberg
dataset and the open book dataset described in Ta-
ble 5. The first one was created by (Csomai and
Mihalcea, 2006), containing 55 free books collected
from Gutenburg6. Since the dataset does not pro-
vide the locators of index terms, we can only serve
the evaluation as a keyword extraction task. The sec-
ond dataset was collected from CiteSeer repository,
most of which are in computer science and engineer-
ing. We extracted the paged body text and the back
index using Pdfbox7. Having each index term asso-
ciated with its locators (page numbers), we can per-
form an evaluation for different methods, not based
solely on keyword extraction.
We first compare CTI with other metrics on both
datasets for keywords extraction since all other met-
rics are context-oblivious. CTI selects index terms
based on the sum of a term?s CTI scores over all its
contexts, the same as the algorithm used in Section
4.2. The results are shown in Table 6, where the in-
dex size = n indicates the number of output terms is
n times of the true book index size for each book.
The scores are the average recall over a dataset.
The CTI outperforms all other 7 metrics in the two
datasets as the output index size increases. More-
over, results show that TF and TFIDF are better than
RIDF in identifying book index terms, which seems
contradictory to previous findings (Church and Gale,
1995). A possible reason is that a book is much
longer than a regular document thus enhancing TF
as a better indicator of keywords but weakening the
role of IDF . We believe this is why Variance, Gain,
and Burstiness, which relies on DF , are less effec-
tive here. Wikipedia keyphraseness (Csomai andMi-
halcea, 2008) can only find a small fraction of index
terms because it emphasizes Wikipedia titles that
have high in-degree in hyper-link network formed
6www.gutenberg.org/
7pdfbox.apache.org/
1 2 3 4 5
5
10
15
20
25
30
35
40
A
v
e
r
a
g
e
 
R
e
c
a
l
l
 
(
%
)
Index size
 TFIDF+FO
 TFIDF+CW
 CI-Indexer
(a) TFIDF
1 2 3 4 5
10
15
20
25
30
35
40
A
v
e
r
a
g
e
 
R
e
c
a
l
l
 
(
%
)
Index size
 KEA+FO
 KEA+CW
 CI-Indexer
(b) KEA
1 2 3 4 5
10
15
20
25
30
35
40
A
v
e
r
a
g
e
 
R
e
c
a
l
l
 
(
%
)
Index size
 SLD+FO
 SLD+CW
 CI-Indexer
(c) SLD
5 10 15 20
10
20
30
40
50
A
v
e
r
a
g
e
 
R
e
c
a
l
l
(
%
)
k
Index size: 1 
Index size: 2
Index size: 3
Index size: 4
Index size: 5
(d) size of featured context set
Figure 1: Results for book index generation
by Wikipedia terms. However, a book index covers
much broader terms not titled in Wikipedia.
We then compare with three baselines TFIDF,
KEA, and SLD (supervised learning using decision
tree in Csomai?s (2008)) on the second dataset. For
SLD, we use all the features except the discourse
comprehension based ones which were too com-
plicate to implement. We choose a decision tree
because its training is much faster than the other
two models while its performance is quite close to
the best. We follow Csomai?s setting to choose
90%(192) books for training and the other 10%(21)
for test. We set two strategies to make the baselines
context-aware. First, we select the page of a term?s
first occurrence as its locator, denoted by ?+FO? in
Figure 1. Second, we apply the context weight to
them, denoted by ?+CW?. ?CI-Indexer? denotes our
method. The results are shown in Figure 1a, 1b and
1c respectively. For all the three baselines, adding
context weight gives better performance than us-
ing the simple first occurrence guess, especially for
TFIDF. KEA benefits least from the context weights,
suggesting its first occurrence and node degree fea-
tures play a similar role as the context weight fea-
tures. SLD outperforms TFIDF and KEA under
both strategies probably because of the new fea-
tures of POS pattern and Wikipedia keyphraseness.
?SLD+CW? is the closest to ours. Finally, we show
in Figure 1d that increasing the size of featured con-
text set for CTI from 5 to 20 can slightly improve
266
Dataset Open book dataset Gutenberg dataset
Index size 1 2 3 4 5 1 2 3 4 5
Variance 2.4 4.8 7.5 10.4 13.4 1.1 2.9 5.3 8.0 11.0
Gain 2.9 6.4 10.2 14.3 18.2 4.9 9.0 14 18.6 23.0
Wikipedia keyphraseness 5.3 9.5 13.5 16.4 20.5 9.2 14.1 18.5 21.4 24.3
Burstiness 6.0 11.4 16.6 21.4 25.8 10.0 15.8 20.2 23.1 26.2
RIDF 8.6 14.5 19.5 23.9 28.0 10.4 15.9 20.1 23.2 26.3
TF 9.8 16.9 23.3 29.0 31.7 10.4 17.6 23.5 28.1 30.7
TFIDF 10.3 17.3 23.8 29.3 33.6 11.8 19.9 24.7 28.9 32.9
CTI 12.4 19.2 25.1 31.5 35.5 14.9 22.3 26.9 29.3 34.5
Table 6: Average recall(%) comparisons as the output index size increases
performance in different index size settings.
4.4 Discussion
The three applications are (incrementally) designed
for different goals. The first is a toy applica-
tion to show the potential capability of this ap-
proach, regardless of syntactic or statistical informa-
tion. Clearly, there are simple heuristics that can
work very well for this task, e.g. the first term
of the context. TF or TFIDF also performs quite
well. We can rewrite each context (by reordering the
terms, changing sentence structures, or substituting
the core terms with pronouns) to make them inef-
fective. However, this will not effect our method,
because what it essentially measures is a term?s in-
formativeness among a list of terms appearing in the
same context. However, for keyword extraction, a
topic with a rich literature, to the best of our knowl-
edge, has no publicly available large scale datasets,
which makes SemEval2010 the best available. We
believe our application on back-of-the-book index
generation showed how CTI can scale real world
large corpora and will scale to millions of books
since each book can be processed separately.
Based on the applications we explored, we can
see that the practical utility of CTI used alone could
be limited, especially for context-oblivious tasks.
It seems reasonable that this method does not out-
perform supervised learning methods designed for
keyword extraction. However, our method shows
what simple but elegant methods can achieve with-
out the overhead of machine learning, especially for
context-aware scenarios such as finding book index
terms.
5 Conclusion and Future Work
We developed a new web knowledge based method
for encoding informativeness of terms within a unit
of discourse. It is totally feature-free, corpus-free,
easy to implement, and inherently parallelizable.
Three typical applications on text snippets, scien-
tific papers and non-fiction books show its effec-
tiveness. The segmentation of context, the size of
featured context set, the semantic relatedness met-
ric ?, and the knowledge base might more or less
affect the final performance of CTI in terms of ac-
curacy or efficiency. For all applications, we treat a
paragraph as an individual context, which is not nec-
essary a complete discourse unit. However, it may
not be fair to set the same number for all context
terms. In addition, selection of semantic relatedness
and knowledge bases need further investigation. The
Wikipedia-based implementation might be a good
choice for the definitional snippets, scientific arti-
cles and text books since they are all ?educational?
resources sharing a similar concept space. However,
it is an open question as whether it works for corpora
such as tweets, online reviews, and forum posts.
Based on the proposed methods and encouraging
results, it would be interesting to build an online in-
dexing tool which automatically finds informative
terms in generic text and generates a back-of-the-
book index for a sets of papers, books, theses and
other collections.
6 Acknowledgments
We gratefully acknowledge partial support from the
National Science Foundation and useful comments
from referees.
267
References
E. Agirre, E. Alfonseca, K. Hall, J. Kravalova, M. Pasca,
and A. Soroa. 2009. A study on similarity and
relatedness using distributional and WordNet-based
approaches. In Proceedings of NAACL-HLT 2009,
pp. 19?27.
A. Bookstein and Don R. Swanson. 1974. Probabilistic
models for automatic indexing. Journal of the Ameri-
can Society for Information Science, 25(5):312?316.
A. Budanitsky and G. Hirst. 2012. Evaluating WordNet-
based measures of semantic distance. Computational
Linguistics 32:13?47.
K. W. Church and W. A. Gale. 1995. Inverse document
frequency(IDF): A measure of deviation from poisson.
In Proceedings of the Third Workshop on Very Large
Corpora 1995, pp. 121?130.
K. W. Church and P. Hanks. 1990. Word association
norms, mutual information and lexicography. Compu-
tational Linguistics, 16:22?29.
Rudi L. Cilibrasi and Paul M.B. Vitanyi. 2007. The
Google similarity distance. IEEE Transaction on
Knowledge and Data Engineering, 19(3):370?383.
A. Csomai and R. Mihalcea. 2006. Creating a testbed for
the evaluation of automatically generated back-of-the-
book indexes. In Proceedings of the 7th international
conference on Computational Linguistics and Intelli-
gent Text Processing 2006, pp. 429?440.
A. Csomai and R. Mihalcea. 2007. Investigations in
unsupervised back-of-the-book indexing. In Proceed-
ings of the Florida Artificial Intelligence Research So-
ciety 2007, pp. 211?216.
A. Csomai and R. Mihalcea. 2008. Linguistically Moti-
vated Features for Enhanced Back-of-the-Book Index-
ing. In Proceedings of ACL 2008, pp. 932?940.
S. Deerwester, S. T. Dumais, G. W. Furnas, T. K. Lan-
dauer, and R. Harshman. 1990. Indexing by Latent
Semantic Analysis. Journal of the American Society
for Information Science 41:391-407.
V. Diodato and G. Gandt. 1991. Back of book indexes
and the characteristics of author and nonauthor index-
ing: Report of an exploratory study. Journal of the
American Society for Information Science, 42:341?
350.
V. Diodato. 1994. User preferences for features in back
of book indexes. Journal of the American Society for
Information Science, 45:529?536.
E. Frank, G. W. Paynter, I. H. Witten, and C. Gutwin.
1999 Domainspecic keyphrase extraction. In Pro-
ceedings IJCAI 1999, pp. 668-673.
E. Gabrilovich and S. Markovitch. 2007. Computing se-
mantic relatedness using Wikipedia-based explicit se-
mantic analysis. In Proceedings of IJCAI 2007, pp. 6?
12.
Q. He, J. Pei, D. Kifer, P. Mitra, and C. L. Giles. 2010.
Context-aware citation recommendation. In Proceed-
ings of WWW 2010, pp. 421?430.
B. Hornyak. 2002. Indexing Specialties: Psychology.
Medford, NJ : Information Today, Inc.
A. Hulth. 2003. Improved automatic keyword extraction
given more linguistic knowledge. In Proceedings of
EMNLP 2003, pp. 216-223.
Karen Sprck Jones. 1972. A statistical interpretation of
term specificity and its application in retrieval. Jour-
nal of Documentation, 28(1):11?21.
Karen Sprck Jones. 1973. Index term weighting. Infor-
mation Storage and Retrieval, 9(11):619?633.
P. Kendrick and E. L. Zafran. 2001. Indexing Specialties:
Law. Medford, NJ : Information Today, Inc.
Su Nam Kim, Olena Medelyan, Min-Yen Kan, and Tim-
othy Baldwin. 2010. Semeval-2010 task 5 : Auto-
matic keyphrase extraction from scientic articles. In
Proceedings of the 5th SIGLEX Workshop on Seman-
tic Evaluation. 2010, pp. 21?26.
K. Kireyev. 2009. Semantic-based Estimation of
Term Informativeness. In Proceedings of NAACL-HLT
2009, pp. 530?538.
Z. Liu, W. Huang, Y. Zheng, and M. Sun. 2010. Au-
tomatic keyphrase extraction via topic decomposition.
In Proceedings of EMNLP 2010, pp. 366?376.
P. Lopez and L. Romary. 2010. HUMB: Automatic Key
Term Extraction from Scientic Articles in GROBID.
In Proceedings of the 5th SIGLEX Workshop on Se-
mantic Evaluation. 2010, pp. 248-251.
O. Medelyan and I. H. Witten. 2008. Domain-
independent automatic keyphrase indexing with small
training sets. J. Am. Soc. Inf. Sci. Technol. 59:1026-
1040.
O. Medelyan, I. H. Witten, and D. Milne. 2008. Topic
indexing with Wikipedia. In Proceedings of AAAI08
Workshop on Wikipedia and Artificial Intelligence: an
Evolving Synergy 2008, pp. 19?24.
O. Medelyan, E. Frank, and I. H. Witten. 2009. Human-
competitive tagging using automatic keyphrase extrac-
tion. In Proceedings of EMNLP 2009, pp. 1318?1327.
R. Mihalcea and P. Tarau. 2004. TextRank: Bringing
Order into Texts. In Proceedings of EMNLP 2004,
pp. 404-411.
L. Page, S. Brin, R. Motwani, and T. Winograd. 1998.
The PageRank Citation Ranking: Bringing Order to
the Web. Technical Report. Stanford InfoLab.
K. Papineni. 2001. Why inverse document frequency?
In Proceedings of NAACL-HLT 2001, pp. 1?8.
J. D. M. Rennie, and T. Jaakkola. 2005. Using Term
Informativeness for Named Entity Detection. In Pro-
ceedings of SIGIR 2005, pp. 353?360.
268
G. Salton, and C. Buckley. 1988. Term-weighting ap-
proaches in automatic text retrieval. Information Pro-
cessing & Management 24(5):513?523.
R. Soricut and D. Marcu. 2003. Sentence level dis-
course parsing using syntactic and lexical information.
In Proceedings of NAACL-HLT 2003, pp. 149?156.
E. Terra and C. L. Clarke. 2003. Frequency estimates for
statistical word similarity measures. In Proceedings of
NAACL-HLT 2003, pp. 165?172.
T. Tomokiyo and M. Hurst. 2003. A language model ap-
proach to keyphrase extraction. In Proceedings of the
ACL 2003 workshop on Multiword expressions: anal-
ysis, acquisition and treatment, 2003, pp. 3340.
K. Toutanova, D. Klein, C. Manning, and Y. Singer.
2003. Feature-Rich Part-of-Speech Tagging with a
Cyclic Dependency Network. In Proceedings of
NAACL-HLT 2003, pp. 252-259.
P. D. Turney. 2000. Learning Algorithms for Keyphrae
Extraction. Information Retrieval 2:303-336.
I. H. Witten, G. W. Paynter, E. Frank, C. Gutwin, and
C. G. Nevill-Manning. 1999. Kea: practical automatic
keyphrase extraction. In Proceedings of the fourth
ACM conference on Digital libraries, 1999, pp. 254?
255.
L. P. Wyman. 1999. Indexing Specialities: Medicine.
Medford, NJ : Information Today, Inc.
M. Yazdani and A. Popescu-Belis. 2012. Computing
text semantic relatedness using the contents and links
of a hypertext encyclopedia. Artificial Intelligence
194:176?202.
J. Zobel and A. Moffat. 1998. Exploring the similarity
space. ACM SIGIR Forum 32(1):18?34.
Le Zhao and Jamie Callan. 2010. Term necessity predic-
tion. In Proceedings of the 19th ACM international
conference on Information and knowledge manage-
ment, 2010, pp. 259?268.
Hui Yang and Jamie Callan. 2009. A metric-based
framework for automatic taxonomy induction. In Pro-
ceedings of the ACL, 2009, pp. 271?279 .
269
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 182?185,
Uppsala, Sweden, 15-16 July 2010.
c?2010 Association for Computational Linguistics
SEERLAB: A System for Extracting Keyphrases from Scholarly
Documents
Pucktada Treeratpituk
1
Pradeep Teregowda
2
Jian Huang
1
C. Lee Giles
1,2
1
Information Sciences and Technology
2
Computer Science and Engineering
Pennsylvania State University, University Park, PA, USA
Abstract
We describe the SEERLAB system
that participated in the SemEval 2010?s
Keyphrase Extraction Task. SEERLAB
utilizes the DBLP corpus for generating
a set of candidate keyphrases from a
document. Random Forest, a supervised
ensemble classifier, is then used to select
the top keyphrases from the candidate set.
SEERLAB achieved a 0.24 F-score in
generating the top 15 keyphrases, which
places it sixth among 19 participating sys-
tems. Additionally, SEERLAB performed
particularly well in generating the top 5
keyphrases with an F-score that ranked
third.
1 Introduction
Keyphrases are phrases that represent the impor-
tant topics of a document. There are two types of
keyphrases associated with scholarly publications:
author-assigned ones and reader-assigned ones. In
the Keyphrase Extraction Task (Kim et al, 2010),
each system receives two set of scientific papers
from the ACM digital library; a training set and
a testing set. The author-assigned keyphrases and
reader-assigned keyphrases are given for each pa-
per in the training set. The objective is to produce
the keyphrases for each article in the testing set.
This paper is organized as followed. First, We
describe our keyphrase extraction system, SEER-
LAB. We then discuss its performance in SemEval
2010. Lastly, we analyze the effectiveness of each
feature used by SEERLAB, and provide a sum-
mary of our findings.
2 System Description
SEERLAB consists of three main components: a
section parser, a candidate keyphrase extractor,
and a keyphrase ranker. To generate keyphrases
for a paper, the section parser first segments the
document into pre-defined generic section types.
Secondly, the candidate keyphrase extractor gen-
erates a list of candidate phrases based on the doc-
ument content. Then, the keyphrase ranker ranks
each candidate according to the likelihood that it
is a keyphrase. The top candidates are selected as
keyphrases of the paper.
2.1 Section Parser
The goal of the section parser is to parse each doc-
ument into the same set of pre-defined sections.
However, segmenting a scientific article into pre-
defined section types is not trivial. While schol-
arly publications generally contains similar sec-
tions (such as Abstract and Conclusion), a sec-
tion?s exact header description and the order in
which it appears can vary from document to docu-
ment. For example, the ?Related Work? section is
sometimes referred to as ?Previous Research? or
?Previous Work.? Also, while the ?Related Work?
section often appears right after the introduction,
it could also appear near the end of a paper.
(Nguyen and Kan, 2007) had success in us-
ing a maximum entropy (ME) classifier to clas-
sify sections into 14 generic section types includ-
ing those such as Motivation, Acknowledgement,
References. However, their approach requires an-
notated training data, which is not always avail-
able. Instead, SEERLAB uses regular expres-
sions to parse each document into 6 generic sec-
tion types: Title, Abstract, Introduction, Related
Work, Methodology + Experiments, and Conclu-
sion + Future Work. We decided to go with the
smaller number of section types (only 6), unlike
previous work in (Nguyen and Kan, 2007), be-
cause we believed that many sections, such as Ac-
knowledgement, are irrelevant to the task.
182
2.2 Extracting Candidate Keyphrases
In this section, we describe how SEERLAB de-
rives a set of candidate keyphrases for a given doc-
ument. The goal of the candidate extractor is to in-
clude as many actual keyphrases in the candidate
set as possible, while keeping the number of can-
didates small. The performance of the candidate
extractor determines the maximum achievable Re-
call of the whole system. The more correct can-
didates extracted at this step, the higher the possi-
ble Recall. But a bigger candidate set potentially
could lower Precision. In our implementation, we
decided to ignore the Methodology + Experiments
sections to limit the size of candidate sets.
First, SEERLAB extracts a list of bigrams, tri-
grams and quadgrams that appear at least 3 times
in titles of papers in DBLP
1
, ignoring those that
contain stopwords. Prepositions such as ?of?,
?for?, ?to? are allowed to be present in the ngrams.
From 2,144,390 titles in DBLP, there are 376,577
of such ngrams. It then constructs a trie (a prefix-
tree) of all ngrams so that it can later perform the
longest-prefix matching lookup efficiently.
To generate candidates from a body of text, we
start the cursor at the beginning of the text. The
DBLP trie is then used to find the longest-prefix
match. If no match is found, the cursor is moved
to the next word in the text. If a match is found,
the matched phrase is extracted and added to the
candidate set, while the cursor is moved to the end
of the matched phrase. The process is repeated
until the cursor reaches the end of the text.
However, the trie constructed as described
above can only produce non-unigram candidates
that appear in the DBLP corpus. For example,
it is incapable of generating candidates such as
?preference elicitation problem,? which does not
appear in DBLP, and ?bet,? which is an unigram.
To remedy such limitations, for each document we
also include its top 30 most frequent unigrams,
its top 30 non-unigram ngrams and the acronyms
found in the document as candidates.
Our method of extracting candidate keyphrases
differs from most previous work. Previous work
(Kim and Kan, 2009; Nguyen and Kan, 2007) uses
hand-crafted regular expressions for candidate ex-
tractions. Many of these rules also require POS
(part of speech) inputs. In contrast, our method
is corpus-driven and requires no additional input
from the POS tagger. Additionally, our approach
1
http://www.informatik.uni-trier.de/ ley/db/index.html
allows us to effectively include phrases that appear
only once in the document as candidates, as long
as they appear more than twice in the DBLP data.
2.3 Ranking Keyphrases
We train a supervised Random Forest (RF) clas-
sifier to identify keyphrases from a candidate set.
A Random Forest is a collection of decision trees,
where its prediction is simply the aggregated votes
of each tree. Thus, for each candidate phrase, the
number of votes that it receives is used as its fit-
ness score. Candidates with the top fitness scores
are then chosen as keyphrases. The detail of the
Random Forest algorithm and the features used in
the model are given below.
2.3.1 Features
We represent each candidate as a vector of fea-
tures. There are the total of 11 features.
N: The length of the keyphrase.
ACRO: A binary feature indicating whether the
keyphrase appears as an acronym in the document.
TF
doc
: The number of times that the keyphrase
appears in the document.
DF: The document frequency. This is com-
puted based on the DBLP data. For document-
specific candidates (unigrams and those not found
in DBLP), their DFs are set to 1.
TFIDF: The TFIDF weight of the keyphrase,
computed using TF
doc
and DF.
TF
headers
: The number of occurrences that the
keyphrase appears in any section headers and sub-
section headers.
TF
section
i
: The number of occurrences that
the keyphrase appears in the section
i
, where
section
i
? {Title, Abstract, Introduction, Related
Work, Conclusion}. These accounted for the total
of 5 features.
2.3.2 Random Forest
Since a random forest (RF) is an ensemble clas-
sifier combining multiple decision trees (Breiman,
2001), it makes predictions by aggregating votes
of each of the trees. To built a random forest, mul-
tiple bootstrap samples are drawn from the origi-
nal training data, and an unpruned decision tree is
built from each bootstrap sample. At each node
in a tree, when selecting a feature to split, the se-
lection is done not on the full feature set but on a
randomly selected subset of features instead. The
183
Gini index
2
, which measures the class dispersion
within a node, is used to determine the best splits.
RFs have been successfully applied to various
classification problems with comparable results
to other state-of-the-art classifiers such as SVM
(Breiman, 2001; Treeratpituk and Giles, 2009). It
achieves high accuracy by keeping a low bias of
decision trees while reducing the variance through
the introduction of randomness.
One concern in training Random Forests for
identifying keyphrases is the data imbalanced
problem. On average, 130 candidates are extracted
per document but only 8 out of 130 are correct
keyphrases (positive examples). Since the training
data is highly imbalanced, the resulting RF classi-
fier tends to be biased towards the negative class
examples. There are two methods for dealing with
imbalanced data in Random Forests (Chen et al,
2004). The first approach is to incorporate class
weights into the algorithm, giving higher weights
to the minority classes, so that misclassifying a
minority class is penalized more. The other ap-
proach is to adjust the sampling strategy by down-
sampling the majority class so that each tree is
grown on a more balanced data. In SEERLAB,
we employ the down-sampling strategy to correct
the imbalanced data problem (See Section 3).
3 Results
In this section, we discuss the performance and
the implementation detail of our system in the
Keyphrase Extraction Task. Each model in the ex-
periment is trained on the training data, containing
144 documents, and is evaluated on a separate data
set of 100 documents. The performance of each
model is measured using Precision (P), Recall (R)
and F-measure (F) for the top 5, 10 and 15 can-
didates. A keyphrase is considered correct if and
only if it exactly matches one of the answer keys.
No partial credit is given.
Three baseline systems were provided by the or-
ganizer: TF.IDF,NB andME. All baselines use the
simple unigrams, bigrams and trigrams as candi-
dates and TFIDF as features. TF.IDF is an unsu-
pervised method that ranks each candidate based
on TFIDF scores. NB and ME are supervised
Naive Bayes and Maximum Entropy respectively.
We use the randomForest package in R for our
2
For a set S of data with K classes, its Gini index is defined
as: Gini(S) =
P
K
j=1
p
2
j
, where p
i
denotes the probability
of observing class i in S.
tf.headers
tf.conclusion
tf.related_work
tf.abs
tf.intro
tf.doc
acro
tf.title
tfidf
df
n
0.02 0.06
MeanDecreaseAccuracy
acro
tf.title
tf.related_work
tf.headers
tf.abs
tf.conclusion
n
tf.intro
tf.doc
df
tfidf
0 10 25
MeanDecreaseGini
Figure 1: Variable importance for each feature
keyphrase ranker (Liaw and Wiener, 2002). All
RF models are built with the following parame-
ters: the number of trees = 200 and the number of
features considered at each split = 3. The average
training and testing time are around 15s and 5s.
Table 1. compares the performance of three
different SEERLAB models against the baselines.
RF
0
is the basic model, where the training data
is imbalanced. For RF
1:1
, the negative examples
are down-sampled to make the data balanced. For
RF
1:7
, the negative examples are down-sampled
to where its ratio with the positive examples is 7
to 1. All three models significantly outperform
the baselines. The RF
1:7
model has the high-
est performance, while the RF
1:1
model performs
slightly worse than the basic model RF
0
. This
shows that while the sampling strategy helps, over-
doing it can hurt the performance. The optimal
sampling ratio (RF
1:7
) is chosen according to a
10-fold cross-validation on the training data. For
the top 15 candidates, RF
1:7
?s F-score (C) ranks
sixth among the 19 participants with a 24.34% F-
score approximately 1% lower than the third place
team. We also observed that SEERLAB performs
quite well for the top 5 candidates with 39% Preci-
sion (C). Its F-scores at the top 5, 19.84% (C) and
18.19% (R), place SEERLAB third and second re-
spectively among other participants.
Figure 1. shows two variable importance in-
dicators for each feature: mean decrease accu-
racy (MDA) and mean decrease Gini (MDG).
Both indicators measure each feature?s contribu-
tion in identifying whether a candidate phrase is
a keyphrase. The MDA of a feature is computed
by randomly permuting the value of that feature in
the training data and then measuring the decrease
in prediction accuracy. If the permuted feature is
184
System by top 5 candidates top 10 candidates top 15 candidates
P R F P R F P R F
TF.IDF R 17.80 7.39 10.44 13.90 11.54 12.61 11.60 14.45 12.87
C 22.00 7.50 11.19 17.70 12.07 14.35 14.93 15.28 15.10
NB R 16.80 6.98 9.86 13.30 11.05 12.07 11.40 14.20 12.65
C 21.40 7.30 10.89 17.30 11.80 14.03 14.53 14.87 14.70
ME R 16.80 6.98 9.86 13.30 11.05 12.07 11.40 14.20 12.65
C 21.40 7.30 10.89 17.30 11.80 14.03 14.53 14.87 14.70
SEERLAB (RF
0
) R 29.00 12.04 17.02 22.50 18.69 20.42 18.20 22.67 20.19
C 36.00 12.28 18.31 28.20 19.24 22.87 22.53 23.06 22.79
SEERLAB (RF
1:1
) R 26.00 10.80 15.26 20.80 17.28 18.88 17.40 21.68 19.31
C 32.00 10.91 16.27 26.00 17.74 21.09 21.93 22.44 22.18
SEERLAB (RF
1:7
) R 31.00 12.87 18.19 24.10 20.02 21.87 19.33 24.09 21.45
C 39.00 13.30 19.84 29.70 20.26 24.09 24.07 24.62 24.34
Table 1: Performance (%) comparison for the Keyphrase Extraction Task. R (Reader) indicates that the
reader-assigned keyword is used as the gold-standard and C (Combined) means that both author-assigned
and reader-assigned keyword sets are used.
a very good predictor, then the prediction accu-
racy should decrease substantially from the orig-
inal model. The MDG of a feature implies that
average Gini decreases for the nodes in the forest
that use that feature as the splitting criteria.
TFIDF and DF are good indicators of perfor-
mance according to both MDA and MDG. Both
are very effective when used as splitting criteria,
and the prediction accuracy is very sensitive to
them. Surprisingly, the length of the phrase (N)
also has high importance. Also, TF
title
and ACRO
have high MDA but low MDG. They have high
MDA because if a candidate phrase is an acronym
or appears in the title, it is highly likely that it
is a keyphrase. However, most keyphrases are
not acronyms and do not appear in titles. Thus,
on average as splitting criteria, they do not de-
crease Gini index by much, resulting in a low
MDG. Also, TF
related work
and TF
headers
have
lower MDA and MDG than TF of other sections
(TF
intro
, TF
abs
, and TF
conclusion
). This might
suggest that the occurrences in the ?Related Work?
section or section headers are not strong indica-
tors of being a keyphrase as the occurrences in the
sections ?Introduction,? ?Abstract? and ?Conclu-
sion.?
4 Conclusion
We have described our SEERLAB system that
participated in the Keyphrase Extraction Task.
SEERLAB combines unsupervised corpus-based
approach with Random Forests to identify
keyphrases. The experimental results show that
our system performs well in the Keyphrase Ex-
traction Task, especially on the top 5 key phrase
candidates. We also show that the down-sampling
strategy can be used to enhance our performance.
References
Leo Breiman. 2001. Random forests. Machine Learn-
ing, Jan.
Chao Chen, Andy Liaw, and Leo Breiman. 2004. Us-
ing random forest to learn imbalanced data. Techni-
cal Report, University of California, Berkeley.
Su Nam Kim and Min-Yen Kan. 2009. Re-examining
automatic keyphrase extraction approaches in scien-
tific articles. Proceedings of the Workshop on Mul-
tiword Expressions, ACL-IJCNLP, Jan.
Su Nam Kim, Olena Medelyan, Min-Yen Kan, and
Timothy Baldwin. 2010. Semeval-2010 task 5: Au-
tomatic keyphrase extraction from scienctific article.
ACL workshop on Semantic Evaluations (SemEval
2010).
Andy Liaw and Matthew Wiener. 2002. Classification
and regression by randomforest. R News.
Thuy Dung Nguyen and Min-Yen Kan. 2007.
Keyphrase extraction in scientific publications. Pro-
ceedings of International Conference on Asian Dig-
ital Libraries (ICADL?07), Jan.
Pucktada Treeratpituk and C Lee Giles. 2009. Dis-
ambiguating authors in academic publications using
random forests. In Proceedings of the Joint Confer-
ence on Digital Libraries (JCDL?09), Jan.
185
