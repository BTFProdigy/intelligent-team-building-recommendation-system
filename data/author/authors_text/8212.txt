Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 560?567,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Loss Minimization in Parse Reranking
Ivan Titov
Department of Computer Science
University of Geneva
24, rue Ge?ne?ral Dufour
CH-1211 Gene`ve 4, Switzerland
ivan.titov@cui.unige.ch
James Henderson
School of Informatics
University of Edinburgh
2 Buccleuch Place
Edinburgh EH8 9LW, United Kingdom
james.henderson@ed.ac.uk
Abstract
We propose a general method for reranker
construction which targets choosing the
candidate with the least expected loss,
rather than the most probable candidate.
Different approaches to expected loss ap-
proximation are considered, including es-
timating from the probabilistic model used
to generate the candidates, estimating
from a discriminative model trained to
rerank the candidates, and learning to ap-
proximate the expected loss. The pro-
posed methods are applied to the parse
reranking task, with various baseline mod-
els, achieving significant improvement
both over the probabilistic models and the
discriminative rerankers. When a neural
network parser is used as the probabilistic
model and the Voted Perceptron algorithm
with data-defined kernels as the learning
algorithm, the loss minimization model
achieves 90.0% labeled constituents F1
score on the standard WSJ parsing task.
1 Introduction
The reranking approach is widely used in pars-
ing (Collins and Koo, 2005; Koo and Collins,
2005; Henderson and Titov, 2005; Shen and Joshi,
2003) as well as in other structured classifica-
tion problems. For structured classification tasks,
where labels are complex and have an internal
structure of interdependency, the 0-1 loss consid-
ered in classical formulation of classification al-
gorithms is not a natural choice and different loss
functions are normally employed. To tackle this
problem, several approaches have been proposed
to accommodate loss functions in learning algo-
rithms (Tsochantaridis et al, 2004; Taskar et al,
2004; Henderson and Titov, 2005). A very differ-
ent use of loss functions was considered in the ar-
eas of signal processing and machine translation,
where direct minimization of expected loss (Min-
imum Bayes Risk decoding) on word sequences
was considered (Kumar and Byrne, 2004; Stol-
cke et al, 1997). The only attempt to use Mini-
mum Bayes Risk (MBR) decoding in parsing was
made in (Goodman, 1996), where a parsing al-
gorithm for constituent recall minimization was
constructed. However, their approach is limited
to binarized PCFG models and, consequently, is
not applicable to state-of-the-art parsing meth-
ods (Charniak and Johnson, 2005; Henderson,
2004; Collins, 2000). In this paper we consider
several approaches to loss approximation on the
basis of a candidate list provided by a baseline
probabilistic model.
The intuitive motivation for expected loss mini-
mization can be seen from the following example.
Consider the situation where there are a group of
several very similar candidates and one very dif-
ferent candidate whose probability is just slightly
larger than the probability of any individual candi-
date in the group, but much smaller than their total
probability. A method which chooses the maxi-
mum probability candidate will choose this outlier
candidate, which is correct if you are only inter-
ested in getting the label exactly correct (i.e. 0-1
loss), and you think the estimates are accurate. But
if you are interested in a loss function where the
loss is small when you choose a candidate which
is similar to the correct candidate, then it is better
to choose one of the candidates in the group. With
this choice the loss will only be large if the outlier
turns out to be correct, while if the outlier is cho-
sen then the loss will be large if any of the group
are correct. In other words, the expected loss of
560
choosing a member of the group will be smaller
than that for the outlier.
More formally, the Bayes risk of a model y =
h(x) is defined as
R(h) = Ex,y?(y, h(x)), (1)
where the expectation is taken over all the possi-
ble inputs x and labels y and ?(y, y?) denotes a
loss incurred by assigning x to y? when the correct
label is y. We assume that the loss function pos-
sesses values within the range from 0 to 1, which
is equivalent to the requirement that the loss func-
tion is bounded in (Tsochantaridis et al, 2004). It
follows that an optimal reranker h? is one which
chooses the label y that minimizes the expected
loss:
h?(x) = arg min
y??G(x)
?
y
P (y|x)?(y, y?), (2)
where G(x) denotes a candidate list provided by
a baseline probabilistic model for the input x.
In this paper we propose different approaches to
loss approximation. We apply them to the parse
reranking problem where the baseline probabilis-
tic model is a neural network parser (Henderson,
2003), and to parse reranking of candidates pro-
vided by the (Collins, 1999) model. The result-
ing reranking method achieves very significant im-
provement in the considered loss function and im-
provement in most other standard measures of ac-
curacy.
In the following three sections we will discuss
three approaches to learning such a classifier. The
first two derive a classification criteria for use with
a predefined probability model (the first genera-
tive, the second discriminative). The third de-
fines a kernel for use with a classification method
for minimizing loss. All use previously proposed
learning algorithms and optimization criteria.
2 Loss Approximation with a
Probabilistic Model
In this section we discuss approximating the ex-
pected loss using probability estimates given by
a baseline probabilistic model. Use of probabil-
ity estimates is not a serious limitation of this
approach because in practice candidates are nor-
mally provided by some probabilistic model and
its probability estimates are used as additional fea-
tures in the reranker (Collins and Koo, 2005; Shen
and Joshi, 2003; Henderson and Titov, 2005).
In order to estimate the expected loss on the ba-
sis of a candidate list, we make the assumption that
the total probability of the labels not in the can-
didate list is sufficiently small that the difference
?(x, y?) of expected loss between the labels in the
candidate list and the labels not in the candidate
list does not have an impact on the loss defined
in (1):
?(x, y?) =
?
y/?G(x) P (y|x)?(y, y?)
?
y/?G(x) P (y|x)
? (3)
?
y?G(x) P (y|x)?(y, y?)
?
y?G(x) P (y|x)
This gives us the following approximation to the
expected loss for the label:
l(x, y?) =
?
y?G(x) P (y|x)?(y, y?)
?
y?G(x) P (y|x)
. (4)
For the reranking case, often the probabilistic
model only estimates the joint probability P (x, y).
However, neither this difference nor the denomi-
nator in (4) affects the classification. Thus, replac-
ing the true probabilities with their estimates, we
can define the classifier
h?(x) = arg min
y??G(x)
?
y?G(x)
P (x, y|??)?(y, y?), (5)
where ?? denotes the parameters of the probabilis-
tic model learned from the training data. This ap-
proach for expected loss approximation was con-
sidered in the context of word error rate minimiza-
tion in speech recognition, see for example (Stol-
cke et al, 1997).
3 Estimating Expected Loss with
Discriminative Classifiers
In this section we propose a method to improve on
the loss approximation used in (5) by constructing
the probability estimates using a trained discrimi-
native classifier. Special emphasis is placed on lin-
ear classifiers with data-defined kernels for rerank-
ing (Henderson and Titov, 2005), because they do
not require any additional domain knowledge not
already encoded in the probabilistic model, and
they have demonstrated significant improvement
over the baseline probabilistic model for the parse
reranking task. This kernel construction can be
motivated by the existence of a function which
maps a linear function in the feature space of the
kernel to probability estimates which are superior
to the estimates of the original probabilistic model.
561
3.1 Estimation with Fisher Kernels
The Fisher kernel for structured classification
is a trivial generalization of one of the best
known data-defined kernels for binary classifica-
tion (Jaakkola and Haussler, 1998). The Fisher
score of an example input-label pair (x, y) is a
vector of partial derivatives of the log-likelihood
of the example with respect to the model parame-
ters1:
?FK?? (x, y) = (6)
(logP (x, y|??), ?logP (x,y|??)??1
,..., ?logP (x,y|??)??l
).
This kernel defines a feature space which is appro-
priate for estimating the discriminative probability
in the candidate list in the form of a normalized
exponential
P (x, y)
?
y??G(x) P (x, y?)
? (7)
exp(w?T ?FK?? (x, y))
?
y??G(x) exp(w?T ?FK?? (x, y
?))
for some choice of the decision vector w = w?
with the first component equal to one.
It follows that it is natural to use an estimator
of the discriminative probability P (y|x) in expo-
nential form and, therefore, the appropriate form
of the loss minimizing classifier is the following:
h?FK(x) = (8)
arg min
y??G(x)
?
y?G(x)
exp(Aw?T ?FK?? (x, y
?))?(y, y?),
where w? is learned during classifier training and
the scalar parameter A can be tuned on the devel-
opment set. From the construction of the Fisher
kernel, it follows that the optimal value A is ex-
pected to be close to inverse of the first component
of w?, 1/w?1.
If an SVM is used to learn the classifier, then
the form (7) is the same as that proposed by (Platt,
1999), where it is proposed to use the logistic sig-
moid of the SVM output as the probability estima-
tor for binary classification problems.
1The first component logP (x, y|??) is not in the strict
sense part of the Fisher score, but usually added to kernel
features in practice (Henderson and Titov, 2005).
3.2 Estimation with TOP Kernels for
Reranking
The TOP Reranking kernel was defined in (Hen-
derson and Titov, 2005), as a generalization of the
TOP kernel (Tsuda et al, 2002) proposed for bi-
nary classification tasks. The feature extractor for
the TOP reranking kernel is given by:
?TK?? (x, y) = (9)
(v(x, y, ??), ?v(x, y, ??)??1
,..., ?v(x, y, ??)??l
),
where
v(x, y, ??) = log P (x, y|??)? log
?
y??G(x)?{y}
P (x, y?|??).
The TOP reranking kernel has been demon-
strated to perform better than the Fisher kernel
for the parse reranking task (Henderson and Titov,
2005). The construction of this kernel is moti-
vated by the minimization of the classification er-
ror of a linear classifier wT ???(x, y). This linear
classifier has been shown to converge, assuming
estimation of the discriminative probability in the
candidate list can be in the form of the logistic sig-
moid (Titov and Henderson, 2005):
P (x, y)
?
y??G(x) P (x, y?)
? (10)
1
1 + exp(?w?T ?TK?? (x, y))
for some choice of the decision vector w = w?
with the first component equal to one. From this
fact, the form of the loss minimizing classifier fol-
lows:
h?TK(x) = (11)
arg min
y??G(x)
?
y?G(x)
g(Aw?T ?TK?? (x, y
?))?(y, y?),
where g is the logistic sigmoid and the scalar pa-
rameter A should be selected on the development
set. As for the Fisher kernel, the optimal value of
A should be close to 1/w?1.
3.3 Estimates from Arbitrary Classifiers
Although in this paper we focus on approaches
which do not require additional domain knowl-
edge, the output of most classifiers can be used
to estimate the discriminative probability in equa-
tion (7). As mentioned above, the form of (7)
562
is appropriate for the SVM learning task with
arbitrary kernels, as follows from (Platt, 1999).
Also, for models which combine classifiers using
votes (e.g. the Voted Perceptron), the number of
votes cast for each candidate can be used to de-
fine this discriminative probability. The discrim-
inative probability of a candidate is simply the
number of votes cast for that candidate normalized
across candidates. Intuitively, we can think of this
method as treating the votes as a sample from the
discriminative distribution.
4 Expected Loss Learning
In this section, another approach to loss approx-
imation is proposed. We consider learning a lin-
ear classifier to choose the least loss candidate,
and propose two constructions of data-defined loss
kernels which define different feature spaces for
the classification. In addition to the kernel, this
approach differs from the previous one in that the
classifier is assumed to be linear, rather than the
nonlinear functions in equations (8) and (11).
4.1 Loss Kernel
The Loss Kernel feature extractor is composed of
the logarithm of the loss estimated by the proba-
bilistic model and its first derivatives with respect
to each model parameter:
?LK?? (x, y) = (12)
(v(x, y, ??), ?v(x, y, ??)??1
,..., ?v(x, y, ??)??l
),
where
v(x, y, ??) = log(
?
y??G(x)
P (y?, x|??)?(y?, y)).
The motivation for this kernel is very similar to
that for the Fisher kernel for structured classifica-
tion. The feature space of the kernel guarantees
convergence of an estimator for the expected loss
if the estimator is in normalized exponential form.
The standard Fisher kernel for structured classifi-
cation is a special case of this Loss Kernel when
?(y, y?) is 0-1 loss.
4.2 Loss Logit Kernel
As the Loss kernel was a generalization of the
Fisher kernel to arbitrary loss function, so the Loss
Logit Kernel is a generalization of the TOP kernel
for reranking. The construction of the Loss Logit
Kernel, like the TOP kernel for reranking, can be
motivated by the minimization of the classification
error of a linear classifier wT ?LLK?? (x, y), where
?LLK?? (x, y) is the feature extractor of the kernel
given by:
?LLK?? (x, y) = (13)
(v(x, y, ??), ?v(x, y, ??)??1
,..., ?v(x, y, ??)??l
),
where
v(x, y, ??) = log(
?
y??G(x)
P (y?|x, ??)(1??(y?, y)))?
log(
?
y??G(x)
P (y?|x, ??)?(y?, y)).
5 Experimental Evaluation
To perform empirical evaluations of the proposed
methods, we considered the task of parsing the
Penn Treebank Wall Street Journal corpus (Mar-
cus et al, 1993). First, we perform experiments
with SVM Struct (Tsochantaridis et al, 2004) as
the learner. Since SVM Struct already uses the
loss function during training to rescale the margin
or slack variables, this learner allows us to test the
hypothesis that loss functions are useful in pars-
ing not only to define the optimization criteria but
also to define the classifier and to define the feature
space. However, SVM Struct training for large
scale parsing experiments is computationally ex-
pensive2, so here we use only a small portion of
the available training data to perform evaluations
of the different approaches. In the other two sets
of experiments, described below, we test our best
model on the standard Wall Street Journal parsing
benchmark (Collins, 1999) with the Voted Percep-
tron algorithm as the learner.
5.1 The Probabilistic Models of Parsing
To perform the experiments with data-defined ker-
nels, we need to select a probabilistic model of
parsing. Data-defined kernels can be applied to
any kind of parameterized probabilistic model.
For our first set of experiments, we choose
to use a publicly available neural network based
probabilistic model of parsing (Henderson, 2003).
2In (Shen and Joshi, 2003) it was proposed to use an
ensemble of SVMs trained the Wall Street Journal corpus,
but the generalization performance of the resulting classifier
might be compromised in this approach.
563
This parsing model is a good candidate for our ex-
periments because it achieves state-of-the-art re-
sults on the standard Wall Street Journal (WSJ)
parsing problem (Henderson, 2003), and data-
defined kernels derived from this parsing model
have recently been used with the Voted Percep-
tron algorithm on the WSJ parsing task, achiev-
ing a significant improvement in accuracy over the
neural network parser alone (Henderson and Titov,
2005). This gives us a baseline which is hard to
beat, and allows us to compare results of our new
approaches with the results of the original data-
defined kernels for reranking.
The probabilistic model of parsing in (Hender-
son, 2003) has two levels of parameterization. The
first level of parameterization is in terms of a
history-based generative probability model. These
parameters are estimated using a neural network,
the weights of which form the second level of pa-
rameterization. This approach allows the prob-
ability model to have an infinite number of pa-
rameters; the neural network only estimates the
bounded number of parameters which are relevant
to a given partial parse. We define data-defined
kernels in terms of the second level of parameteri-
zation (the network weights).
For the last set of experiments, we used the
probabilistic model described in (Collins, 1999)
(model 2), and the Tree Kernel (Collins and Duffy,
2002). However, in these experiments we only
used the estimates from the discriminative classi-
fier, so the details of the probabilistic model are
not relevant.
5.2 Experiments with SVM Struct
Both the neural network probabilistic model and
the kernel based classifiers were trained on sec-
tion 0 (1,921 sentences, 40,930 words). Section 24
(1,346 sentences, 29,125 words) was used as the
validation set during the neural network learning
and for choosing parameters of the models. Sec-
tion 23 (2,416 sentences, 54,268 words) was used
for the final testing of the models.
We used a publicly available tagger (Ratna-
parkhi, 1996) to provide the part-of-speech tags
for each word in the sentence. For each tag, there
is an unknown-word vocabulary item which is
used for all those words which are not sufficiently
frequent with that tag to be included individually
in the vocabulary. For these experiments, we only
included a specific tag-word pair in the vocabu-
R P F1 CM
SSN 80.9 81.7 81.3 18.3
TRK 81.1 82.4 81.7 18.2
SSN-Estim 81.4 82.3 81.8 18.3
LLK-Learn 81.2 82.4 81.8 17.6
LK-Learn 81.5 82.2 81.8 17.8
FK-Estim 81.4 82.6 82.0 18.3
TRK-Estim 81.5 82.8 82.1 18.6
Table 1: Percentage labeled constituent recall (R),
precision (P), combination of both (F1) and per-
centage complete match (CM) on the testing set.
lary if it occurred at least 20 time in the training
set, which (with tag-unknown-word pairs) led to
the very small vocabulary of 271 tag-word pairs.
The same model was used both for choosing the
list of candidate parses and for the probabilistic
model used for loss estimation and kernel feature
extraction. For training and testing of the kernel
models, we provided a candidate list consisting of
the top 20 parses found by the probabilistic model.
For the testing set, selecting the candidate with an
oracle results in an F1 score of 89.1%.
We used the SVM Struct software pack-
age (Tsochantaridis et al, 2004) to train the SVM
for all the approaches based on discriminative
classifier learning, with slack rescaling and lin-
ear slack penalty. The loss function is defined as
?(y, y?) = 1 ? F1(y, y?), where F1 denotes F1
measure on bracketed constituents. This loss was
used both for rescaling the slacks in the SVM and
for defining our classification models and kernels.
We performed initial testing of the models on
the validation set and preselected the best model
for each of the approaches before testing it on
the final testing set. Standard measures of pars-
ing accuracy, plus complete match accuracy, are
shown in table 1.3 As the baselines, the table in-
cludes the results of the standard TOP reranking
kernel (TRK) (Henderson and Titov, 2005) and
the baseline probabilistic model (SSN) (Hender-
son, 2003). SSN-Estim is the model using loss
estimation on the basic probabilistic model, as ex-
plained in section 2. LLK-Learn and LK-Learn are
the models which define the kernel based on loss,
using the Loss Logit Kernel (equation (13)) and
the Loss Kernel (equation (12)), respectively. FK-
Estim and TRK-Estim are the models which esti-
3All our results are computed with the evalb pro-
gram (Collins, 1999).
564
mate the loss with data-defined kernels, using the
Fisher Kernel (equation (8)) and the TOP Rerank-
ing kernel (equation (11)), respectively.
All our proposed models show better F1 accu-
racy than the baseline probabilistic model SSN,
and all these differences are statistically signifi-
cant.4 The difference in F1 between TRK-Estim
and FK-Estim is not statistically significant, but
otherwise TRK-Estim demonstrates a statistically
significant improvement over all other models. It
should also be noted that exact match measures for
TRK-Estim and SSN-Estim are not negatively af-
fected, even though the F1 loss function was opti-
mized. It is important to point out that SSN-Estim,
which improves significantly over SSN, does not
require the learning of a discriminative classifier,
and differs from the SSN only by use of the dif-
ferent classification model (equation (5)), which
means that it is extremely easy to apply in prac-
tice.
One surprising aspect of these results is the fail-
ure of LLK-Learn and LK-Learn to achieve im-
provement over SSN-Estim. This might be ex-
plained by the difficulty of learning a linear ap-
proximation to (4). Under this explanation, the
performance of LLK-Learn and LK-Learn could
be explained by the fact that the first component of
their kernels is a monotonic function of the SSN-
Estim estimation. To test this hypothesis, we did
an additional experiment where we removed the
first component of Loss Logit Kernel (13) from
the feature vector and performed learning. Sur-
prisingly, the model achieved virtually the same
results, rather than the predicted worse perfor-
mance. This result might indicate that the LLK-
Learn model still can be useful for different prob-
lems where discriminative learning gives more ad-
vantage over generative approaches.
These experimental results demonstrate that
the loss approximation reranking approaches pro-
posed in this paper demonstrate significant im-
provement over the baseline models, achieving
about the same relative error reduction as previ-
ously achieved with data-defined kernels (Hender-
son and Titov, 2005). This improvement is despite
the fact that the loss function is already used in the
definition of the training criteria for all the mod-
els except SSN. It is also interesting to note that
the best result on the validation set for estimation
4We measured significance of all the experiments in this
paper with the randomized significance test (Yeh, 2000).
of the loss with data-defined kernels (12) and (13)
was achieved when the parameter A is close to the
inverse of the first component of the learned de-
cision vector, which confirms the motivation for
these kernels.
5.3 Experiments with Voted Perceptron and
Data-Defined Kernels
The above experiments with the SVM Struct
demonstrate empirically the viability of our ap-
proaches. The aim of experiments on the entire
WSJ is to test whether our approaches still achieve
significant improvement when more accurate gen-
erative models are used, and also to show that
they generalize well to learning methods different
from SVMs. We perform experiments on the stan-
dard WSJ parsing data using the standard split into
training, validation and testing sets. We replicate
completely the setup of experiments in (Hender-
son and Titov, 2005). For a detailed description of
the experiment setup, we refer the reader to (Hen-
derson and Titov, 2005). We only note here that
the candidate list has 20 candidates, and, for the
testing set, selecting the candidate with an oracle
results in an F1 score of 95.4%.
We selected the TRK-Estim approach for these
experiments because it demonstrated the best re-
sults in the previous set of experiments (5.2). We
trained the Voted Perceptron (VP) modification
described in (Henderson and Titov, 2005) with the
TOP Reranking kernel. VP is not a linear classi-
fier, so we were not able to use a classifier in the
form (11). Instead the normalized counts of votes
given to the candidate parses were used as proba-
bility estimates, as discussed in section 3.3.
The resulting accuracies of this model are pre-
sented in table 2, together with results of the
TOP Reranking kernel VP (Henderson and Titov,
2005) and the SSN probabilistic model (Hender-
son, 2003). Model TRK-Estim achieves signifi-
cantly better results than the previously proposed
models, which were evaluated in the same exper-
imental setup. Again, the relative error reduction
is about the same as that of TRK. The resulting
system, consisting of the generative model and
the reranker, achieves results at the state-of-the-art
level. We believe that this method can be applied
to most parsing models to achieve a significant im-
provement.
565
R P F1
Henderson, 2003 88.8 89.5 89.1
Henderson&Titov, 2005 89.1 90.1 89.6
TRK-Estim 89.5 90.5 90.0
Table 2: Percentage labeled constituent recall (R),
precision (P), combination of both (F1) on the test-
ing set.
5.4 Experiments with Voted Perceptron and
Tree Kernel
In this series of experiments we validate the state-
ment in section 3.3, where we suggested that loss
approximation from a discriminative classifier is
not limited only to models with data-defined ker-
nels. We apply the same method as used in
the TRK-Estim model above to the Tree Ker-
nel (Collins and Duffy, 2002), which we call the
TK-Estim model.
We replicated the parse reranking experimen-
tal setup used for the evaluation of the Tree Ker-
nel in (Collins and Duffy, 2002), where the can-
didate list was provided by the generative proba-
bilistic model (Collins, 1999) (model 2). A list of
on average 29 candidates was used, with an oracle
F1 score on the testing set of 95.0%. We trained
VP using the same parameters for the Tree Ker-
nel and probability feature weighting as described
in (Collins and Duffy, 2002). A publicly avail-
able efficient implementation of the Tree Kernel
was utilized to speed up computations (Moschitti,
2004). As in the previous section, votes of the per-
ceptron were used to define the probability esti-
mate used in the classifier.
The results for the MBR decoding method (TK-
Estim), defined in section 3.3, along with the stan-
dard Tree Kernel VP results (Collins and Duffy,
2002) (TK) and the probabilistic baseline (Collins,
1999) (CO99) are presented in table 3. The pro-
posed model improves in F1 score over the stan-
dard VP results. Differences between all the mod-
els are statistically significant. The error reduction
of TK-Estim is again about the same as the error
reduction of TK. This improvement is achieved
without adding any additional linguistic features.
It is important to note that the model improves
in other accuracy measures as well. We would
expect even better results with MBR-decoding if
larger n-best lists are used. The n-best parsing al-
gorithm (Huang and Chiang, 2005) can be used to
efficiently produce candidate lists as large as 106
R P F1? CB 0C 2C
CO99 88.1 88.3 88.2 1.06 64.0 85.1
TK 88.6 88.9 88.7 0.99 66.5 86.3
TK-Estim 89.0 89.5 89.2 0.91 66.6 87.4
* F1 for previous models may have rounding errors.
Table 3: Result on the testing set. Percentage la-
beled constituent recall (R), precision (P), combi-
nation of both (F1), an average number of cross-
ing brackets per sentence (CB), percentage of sen-
tences with 0 and ? 2 crossing brackets (0C and
2C, respectively).
parse trees with the model of (Collins, 1999).
6 Conclusions
This paper considers methods for the estimation of
expected loss for parse reranking tasks. The pro-
posed methods include estimation of the loss from
a probabilistic model, estimation from a discrim-
inative classifier, and learning of the loss using a
specialized kernel. An empirical comparison of
these approaches on parse reranking tasks is pre-
sented. Special emphasis is given to data-defined
kernels for reranking, as they do not require the
introduction of any additional domain knowledge
not already encoded in the probabilistic model.
The best approach, estimation of the loss on the
basis of a discriminative classifier, achieves very
significant improvements over the baseline gener-
ative probabilistic models and the discriminative
classifier itself. Though the largest improvement is
demonstrated in the measure which corresponds to
the considered loss functional, other measures of
accuracy are also improved. The proposed method
achieves 90.0% F1 score on the standard Wall
Street Journal parsing task when the SSN neural
network is used as the probabilistic model and VP
with a TOP Reranking kernel as the discriminative
classifier.
Acknowledgments
We would like to thank Michael Collins and
Terry Koo for providing us their data and use-
ful comments on experimental setup, and Alessan-
dro Moschitti for providing us the source code for
his Tree Kernel implementation. We also thank
anonymous reviewers for their constructive com-
ments.
566
References
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and MaxEnt discriminative
reranking. In Proc. 43rd Meeting of Association for
Computational Linguistics, pages 173?180, Ann Ar-
bor, MI.
Michael Collins and Nigel Duffy. 2002. New rank-
ing algorithms for parsing and tagging: Kernels
over discrete structures and the voted perceptron.
In Proc. 40th Meeting of Association for Computa-
tional Linguistics, pages 263?270, Philadelphia, PA.
Michael Collins and Terry Koo. 2005. Discriminative
reranking for natural language parsing. Computa-
tional Linguistics, 31(1):25?69.
Michael Collins. 1999. Head-Driven Statistical Mod-
els for Natural Language Parsing. Ph.D. thesis,
University of Pennsylvania, Philadelphia, PA.
Michael Collins. 2000. Discriminative reranking for
natural language parsing. In Proc. 17th Int. Conf. on
Machine Learning, pages 175?182, Stanford, CA.
Joshua Goodman. 1996. Parsing algorithms and meth-
ods. In Proc. 34th Meeting of the Association for
Computational Linguistics, pages 177?183, Santa
Cruz, CA.
James Henderson and Ivan Titov. 2005. Data-defined
kernels for parse reranking derived from probabilis-
tic models. In Proc. 43rd Meeting of Association for
Computational Linguistics, Ann Arbor, MI.
James Henderson. 2003. Inducing history representa-
tions for broad coverage statistical parsing. In Proc.
joint meeting of North American Chapter of the As-
sociation for Computational Linguistics and the Hu-
man Language Technology Conf., pages 103?110,
Edmonton, Canada.
James Henderson. 2004. Discriminative training of
a neural network statistical parser. In Proc. 42nd
Meeting of Association for Computational Linguis-
tics, Barcelona, Spain.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In Proc. 9th Int. Workshop on Parsing Tech-
nologies, Vancouver, Canada.
Tommi S. Jaakkola and David Haussler. 1998. Ex-
ploiting generative models in discriminative classi-
fiers. Advances in Neural Information Processes
Systems 11.
Terry Koo and Michael Collins. 2005. Hidden-
variable models for discriminative reranking. In
Proc. Conf. on Empirical Methods in Natural Lan-
guage Processing, Vancouver, B.C., Canada.
Shankar Kumar and William Byrne. 2004. Minimum
bayes-risk decoding for statistical machine transla-
tion. In Proceedings of the Human Language Tech-
nology Conference and Meeting of the North Amer-
ican Chapter of the Association for Computational
Linguistics, Boston, MA.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: The Penn Treebank. Computa-
tional Linguistics, 19(2):313?330.
Alessandro Moschitti. 2004. A study on convolutional
kernels for shallow semantic parsing. In Proc. 42nd
Meeting of the Association for Computational Lin-
guistics, Barcelona, Spain.
John C. Platt. 1999. Probabilistic outputs for sup-
port vector machines and comparision to regular-
ized likelihood methods. In A. Smola, P. Bartlett,
B. Scholkopf, and D. Schuurmans, editors, Ad-
vances in Large Margin Classifiers, pages 61?74.
MIT Press.
Adwait Ratnaparkhi. 1996. A maximum entropy
model for part-of-speech tagging. In Proc. Conf. on
Empirical Methods in Natural Language Process-
ing, pages 133?142, Univ. of Pennsylvania, PA.
Libin Shen and Aravind K. Joshi. 2003. An SVM
based voting algorithm with application to parse
reranking. In Proc. of the 7th Conf. on Computa-
tional Natural Language Learning, pages 9?16, Ed-
monton, Canada.
Andreas Stolcke, Yochai Konig, and Mitchel Wein-
traub. 1997. Explicit word error minimization in
n-best list rescoring. In Proc. of 5th European Con-
ference on Speech Communication and Technology,
pages 163?165, Rhodes, Greece.
Ben Taskar, Dan Klein, Michael Collins, Daphne
Koller, and Christopher Manning. 2004. Max-
margin parsing. In Proc. Conf. on Empirical Meth-
ods in Natural Language Processing, Barcelona,
Spain.
Ivan Titov and James Henderson. 2005. Deriving ker-
nels from MLP probability estimators for large cate-
gorization problems. In International Joint Confer-
ence on Neural Networks, Montreal, Canada.
Ioannis Tsochantaridis, Thomas Hofmann, Thorsten
Joachims, and Yasemin Altun. 2004. Support vector
machine learning for interdependent and structured
output spaces. In Proc. 21st Int. Conf. on Machine
Learning, pages 823?830, Banff, Alberta, Canada.
K. Tsuda, M. Kawanabe, G. Ratsch, S. Sonnenburg,
and K. Muller. 2002. A new discriminative ker-
nel from probabilistic models. Neural Computation,
14(10):2397?2414.
Alexander Yeh. 2000. More accurate tests for the
statistical significance of the result differences. In
Proc. 17th International Conf. on Computational
Linguistics, pages 947?953, Saarbruken, Germany.
567
Proceedings of ACL-08: HLT, pages 308?316,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
A Joint Model of Text and Aspect Ratings for Sentiment Summarization
Ivan Titov
Department of Computer Science
University of Illinois at Urbana-Champaign
Urbana, IL 61801
titov@uiuc.edu
Ryan McDonald
Google Inc.
76 Ninth Avenue
New York, NY 10011
ryanmcd@google.com
Abstract
Online reviews are often accompanied with
numerical ratings provided by users for a set
of service or product aspects. We propose
a statistical model which is able to discover
corresponding topics in text and extract tex-
tual evidence from reviews supporting each of
these aspect ratings ? a fundamental problem
in aspect-based sentiment summarization (Hu
and Liu, 2004a). Our model achieves high ac-
curacy, without any explicitly labeled data ex-
cept the user provided opinion ratings. The
proposed approach is general and can be used
for segmentation in other applications where
sequential data is accompanied with corre-
lated signals.
1 Introduction
User generated content represents a unique source of
information in which user interface tools have facil-
itated the creation of an abundance of labeled con-
tent, e.g., topics in blogs, numerical product and ser-
vice ratings in user reviews, and helpfulness rank-
ings in online discussion forums. Many previous
studies on user generated content have attempted to
predict these labels automatically from the associ-
ated text. However, these labels are often present
in the data already, which opens another interesting
line of research: designing models leveraging these
labelings to improve a wide variety of applications.
In this study, we look at the problem of aspect-
based sentiment summarization (Hu and Liu, 2004a;
Popescu and Etzioni, 2005; Gamon et al, 2005;
Nikos? Fine Dining
Food 4/5 ?Best fish in the city?, ?Excellent appetizers?
Decor 3/5 ?Cozy with an old world feel?, ?Too dark?
Service 1/5 ?Our waitress was rude?, ?Awful service?
Value 5/5 ?Good Greek food for the $?, ?Great price!?
Figure 1: An example aspect-based summary.
Carenini et al, 2006; Zhuang et al, 2006).1 An
aspect-based summarization system takes as input
a set of user reviews for a specific product or ser-
vice and produces a set of relevant aspects, the ag-
gregated sentiment for each aspect, and supporting
textual evidence. For example, figure 1 summarizes
a restaurant using aspects food, decor, service, and
value plus a numeric rating out of 5.
Standard aspect-based summarization consists of
two problems. The first is aspect identification and
mention extraction. Here the goal is to find the set
of relevant aspects for a rated entity and extract all
textual mentions that are associated with each. As-
pects can be fine-grained, e.g., fish, lamb, calamari,
or coarse-grained, e.g., food, decor, service. Sim-
ilarly, extracted text can range from a single word
to phrases and sentences. The second problem is
sentiment classification. Once all the relevant as-
pects and associated pieces of texts are extracted,
the system should aggregate sentiment over each as-
pect to provide the user with an average numeric or
symbolic rating. Sentiment classification is a well
studied problem (Wiebe, 2000; Pang et al, 2002;
Turney, 2002) and in many domains users explicitly
1We use the term aspect to denote properties of an object
that can be rated by a user as in Snyder and Barzilay (2007).
Other studies use the term feature (Hu and Liu, 2004b).
308
Food: 5; Decor: 5; Service: 5; Value: 5
The chicken was great. On top of that our service was
excellent and the price was right. Can?t wait to go back!
Food: 2; Decor: 1; Service: 3; Value: 2
We went there for our anniversary. My soup was cold and
expensive plus it felt like they hadn?t painted since 1980.
Food: 3; Decor: 5; Service: 4; Value: 5
The food is only mediocre, but well worth the cost.
Wait staff was friendly. Lot?s of fun decorations.
?
Food ?The chicken was great?, ?My soup wascold?, ?The food is only mediocre?
Decor ?it felt like they hadn?t painted since1980?, ?Lots of fun decorations?
Service ?service was excellent?,?Wait staff was friendly?
Value ?the price was right?, ?My soup was coldand expensive?, ?well worth the cost?
Figure 2: Extraction problem: Produce aspect mentions from a corpus of aspect rated reviews.
provide ratings for each aspect making automated
means unnecessary.2 Aspect identification has also
been thoroughly studied (Hu and Liu, 2004b; Ga-
mon et al, 2005; Titov and McDonald, 2008), but
again, ontologies and users often provide this infor-
mation negating the need for automation.
Though it may be reasonable to expect a user to
provide a rating for each aspect, it is unlikely that
a user will annotate every sentence and phrase in a
review as being relevant to some aspect. Thus, it
can be argued that the most pressing challenge in
an aspect-based summarization system is to extract
all relevant mentions for each aspect, as illustrated
in figure 2. When labeled data exists, this prob-
lem can be solved effectively using a wide variety
of methods available for text classification and in-
formation extraction (Manning and Schutze, 1999).
However, labeled data is often hard to come by, es-
pecially when one considers all possible domains of
products and services. Instead, we propose an un-
supervised model that leverages aspect ratings that
frequently accompany an online review.
In order to construct such model, we make two
assumptions. First, ratable aspects normally repre-
sent coherent topics which can be potentially dis-
covered from co-occurrence information in the text.
Second, we hypothesize that the most predictive fea-
tures of an aspect rating are features derived from
the text segments discussing the corresponding as-
pect. Motivated by these observations, we construct
a joint statistical model of text and sentiment ratings.
The model is at heart a topic model in that it as-
signs words to a set of induced topics, each of which
may represent one particular aspect. The model is
extended through a set of maximum entropy classi-
fiers, one per each rated aspect, that are used to pre-
2E.g., http://zagat.com and http://tripadvisor.com.
dict the sentiment rating towards each of the aspects.
However, only the words assigned to an aspects cor-
responding topic are used in predicting the rating
for that aspect. As a result, the model enforces that
words assigned to an aspects? topic are predictive of
the associated rating. Our approach is more general
than the particular statistical model we consider in
this paper. For example, other topic models can be
used as a part of our model and the proposed class of
models can be employed in other tasks beyond senti-
ment summarization, e.g., segmentation of blogs on
the basis of topic labels provided by users, or topic
discovery on the basis of tags given by users on so-
cial bookmarking sites.3
The rest of the paper is structured as follows. Sec-
tion 2 begins with a discussion of the joint text-
sentiment model approach. In Section 3 we provide
both a qualitative and quantitative evaluation of the
proposed method. We conclude in Section 4 with an
examination of related work.
2 The Model
In this section we describe a new statistical model
called the Multi-Aspect Sentiment model (MAS),
which consists of two parts. The first part is based on
Multi-Grain Latent Dirichlet Allocation (Titov and
McDonald, 2008), which has been previously shown
to build topics that are representative of ratable as-
pects. The second part is a set of sentiment pre-
dictors per aspect that are designed to force specific
topics in the model to be directly correlated with a
particular aspect.
2.1 Multi-Grain LDA
The Multi-Grain Latent Dirichlet Allocation model
(MG-LDA) is an extension of Latent Dirichlet Allo-
cation (LDA) (Blei et al, 2003). As was demon-
3See e.g. del.ico.us (http://del.ico.us).
309
strated in Titov and McDonald (2008), the topics
produced by LDA do not correspond to ratable as-
pects of entities. In particular, these models tend to
build topics that globally classify terms into product
instances (e.g., Creative Labs Mp3 players versus
iPods, or New York versus Paris Hotels). To com-
bat this, MG-LDA models two distinct types of top-
ics: global topics and local topics. As in LDA, the
distribution of global topics is fixed for a document
(a user review). However, the distribution of local
topics is allowed to vary across the document.
A word in the document is sampled either from
the mixture of global topics or from the mixture of
local topics specific to the local context of the word.
It was demonstrated in Titov and McDonald (2008)
that ratable aspects will be captured by local topics
and global topics will capture properties of reviewed
items. For example, consider an extract from a re-
view of a London hotel: ?. . . public transport in Lon-
don is straightforward, the tube station is about an 8
minute walk . . . or you can get a bus for ?1.50?. It
can be viewed as a mixture of topic London shared
by the entire review (words: ?London?, ?tube?, ???),
and the ratable aspect location, specific for the local
context of the sentence (words: ?transport?, ?walk?,
?bus?). Local topics are reused between very differ-
ent types of items, whereas global topics correspond
only to particular types of items.
In MG-LDA a document is represented as a set
of sliding windows, each covering T adjacent sen-
tences within a document.4 Each window v in docu-
ment d has an associated distribution over local top-
ics ?locd,v and a distribution defining preference for lo-
cal topics versus global topics pid,v. A word can be
sampled using any window covering its sentence s,
where the window is chosen according to a categor-
ical distribution ?d,s. Importantly, the fact that win-
dows overlap permits the model to exploit a larger
co-occurrence domain. These simple techniques are
capable of modeling local topics without more ex-
pensive modeling of topic transitions used in (Grif-
fiths et al, 2004; Wang and McCallum, 2005; Wal-
lach, 2006; Gruber et al, 2007). Introduction of a
symmetrical Dirichlet prior Dir(?) for the distribu-
tion ?d,s can control the smoothness of transitions.
4Our particular implementation is over sentences, but sliding
windows in theory can be over any sized fragment of text.
(a) (b)
Figure 3: (a) MG-LDA model. (b) An extension of MG-
LDA to obtain MAS.
The formal definition of the model with Kgl
global and K loc local topics is as follows: First,
draw Kgl word distributions for global topics ?glz
from a Dirichlet prior Dir(?gl) and K loc word dis-
tributions for local topics ?locz? - from Dir(?loc).
Then, for each document d:
? Choose a distribution of global topics ?gld ? Dir(?gl).
? For each sentence s choose a distribution over sliding
windows ?d,s(v) ? Dir(?).
? For each sliding window v
? choose ?locd,v ? Dir(?loc),
? choose pid,v ? Beta(?mix).
? For each word i in sentence s of document d
? choose window vd,i ? ?d,s,
? choose rd,i ? pid,vd,i ,
? if rd,i = gl choose global topic zd,i ? ?gld ,
? if rd,i= loc choose local topic zd,i??locd,vd,i ,
? choose word wd,i from the word distribution ?
rd,i
zd,i .
Beta(?mix) is a prior Beta distribution for choos-
ing between local and global topics. In Figure 3a the
corresponding graphical model is presented.
2.2 Multi-Aspect Sentiment Model
MG-LDA constructs a set of topics that ideally cor-
respond to ratable aspects of an entity (often in a
many-to-one relationship of topics to aspects). A
major shortcoming of this model ? and all other un-
supervised models ? is that this correspondence is
not explicit, i.e., how does one say that topic X is re-
ally about aspect Y? However, we can observe that
numeric aspect ratings are often included in our data
by users who left the reviews. We then make the
assumption that the text of the review discussing an
aspect is predictive of its rating. Thus, if we model
the prediction of aspect ratings jointly with the con-
struction of explicitly associated topics, then such a
310
model should benefit from both higher quality topics
and a direct assignment from topics to aspects. This
is the basic idea behind the Multi-Aspect Sentiment
model (MAS).
In its simplest form, MAS introduces a classifier
for each aspect, which is used to predict its rating.
Each classifier is explicitly associated to a single
topic in the model and only words assigned to that
topic can participate in the prediction of the senti-
ment rating for the aspect. However, it has been ob-
served that ratings for different aspects can be cor-
related (Snyder and Barzilay, 2007), e.g., very neg-
ative opinion about room cleanliness is likely to re-
sult not only in a low rating for the aspect rooms,
but also is very predictive of low ratings for the as-
pects service and dining. This complicates discovery
of the corresponding topics, as in many reviews the
most predictive features for an aspect rating might
correspond to another aspect. Another problem with
this overly simplistic model is the presence of opin-
ions about an item in general without referring to
any particular aspect. For example, ?this product is
the worst I have ever purchased? is a good predic-
tor of low ratings for every aspect. In such cases,
non-aspect ?background? words will appear to be the
most predictive. Therefore, the use of the aspect sen-
timent classifiers based only on the words assigned
to the corresponding topics is problematic. Such a
model will not be able to discover coherent topics
associated with each aspect, because in many cases
the most predictive fragments for each aspect rating
will not be the ones where this aspect is discussed.
Our proposal is to estimate the distribution of pos-
sible values of an aspect rating on the basis of the
overall sentiment rating and to use the words as-
signed to the corresponding topic to compute cor-
rections for this aspect. An aspect rating is typically
correlated to the overall sentiment rating5 and the
fragments discussing this particular aspect will help
to correct the overall sentiment in the appropriate di-
rection. For example, if a review of a hotel is gen-
erally positive, but it includes a sentence ?the neigh-
borhood is somewhat seedy? then this sentence is
predictive of rating for an aspect location being be-
low other ratings. This rectifies the aforementioned
5In the dataset used in our experiments all three aspect rat-
ings are equivalent for 5,250 reviews out of 10,000.
problems. First, aspect sentiment ratings can often
be regarded as conditionally independent given the
overall rating, therefore the model will not be forced
to include in an aspect topic any words from other
aspect topics. Secondly, the fragments discussing
overall opinion will influence the aspect rating only
through the overall sentiment rating. The overall
sentiment is almost always present in the real data
along with the aspect ratings, but it can be coarsely
discretized and we preferred to use a latent overall
sentiment.
The MAS model is presented in Figure 3b. Note
that for simplicity we decided to omit in the figure
the components of the MG-LDA model other than
variables r, z and w, though they are present in the
statistical model. MAS also allows for extra unasso-
ciated local topics in order to capture aspects not ex-
plicitly rated by the user. As in MG-LDA, MAS has
global topics which are expected to capture topics
corresponding to particular types of items, such Lon-
don hotels or seaside resorts for the hotel domain. In
figure 3b we shaded the aspect ratings ya, assuming
that every aspect rating is present in the data (though
in practice they might be available only for some re-
views). In this model the distribution of the overall
sentiment rating yov is based on all the n-gram fea-
tures of a review text. Then the distribution of ya, for
every rated aspect a, can be computed from the dis-
tribution of yov and from any n-gram feature where
at least one word in the n-gram is assigned to the
associated aspect topic (r = loc, z = a).
Instead of having a latent variable yov,6 we use a
similar model which does not have an explicit no-
tion of yov. The distribution of a sentiment rating ya
for each rated aspect a is computed from two scores.
The first score is computed on the basis of all the n-
grams, but using a common set of weights indepen-
dent of the aspect a. Another score is computed only
using n-grams associated with the related topic, but
an aspect-specific set of weights is used in this com-
putation. More formally, we consider the log-linear
distribution:
P (ya = y|w, r, z)?exp(bay+
?
f?w
Jf,y+paf,r,zJaf,y), (1)
where w, r, z are vectors of all the words in a docu-
6Preliminary experiments suggested that this is also a feasi-
ble approach, but somewhat more computationally expensive.
311
ment, assignments of context (global or local) and
topics for all the words in the document, respec-
tively. bay is the bias term which regulates the prior
distribution P (ya = y), f iterates through all the
n-grams, Jy,f and Jay,f are common weights and
aspect-specific weights for n-gram feature f . paf,r,z
is equal to a fraction of words in n-gram feature f
assigned to the aspect topic (r = loc, z = a).
2.3 Inference in MAS
Exact inference in the MAS model is intractable.
Following Titov and McDonald (2008) we use a col-
lapsed Gibbs sampling algorithm that was derived
for the MG-LDA model based on the Gibbs sam-
pling method proposed for LDA in (Griffiths and
Steyvers, 2004). Gibbs sampling is an example of a
Markov Chain Monte Carlo algorithm (Geman and
Geman, 1984). It is used to produce a sample from
a joint distribution when only conditional distribu-
tions of each variable can be efficiently computed.
In Gibbs sampling, variables are sequentially sam-
pled from their distributions conditioned on all other
variables in the model. Such a chain of model states
converges to a sample from the joint distribution. A
naive application of this technique to LDA would
imply that both assignments of topics to words z
and distributions ? and ? should be sampled. How-
ever, (Griffiths and Steyvers, 2004) demonstrated
that an efficient collapsed Gibbs sampler can be con-
structed, where only assignments z need to be sam-
pled, whereas the dependency on distributions ? and
? can be integrated out analytically.
In the case of MAS we also use maximum a-
posteriori estimates of the sentiment predictor pa-
rameters bay, Jy,f and Jay,f . The MAP estimates for
parameters bay , Jy,f and Jay,f are obtained by us-
ing stochastic gradient ascent. The direction of the
gradient is computed simultaneously with running a
chain by generating several assignments at each step
and averaging over the corresponding gradient esti-
mates. For details on computing gradients for log-
linear graphical models with Gibbs sampling we re-
fer the reader to (Neal, 1992).
Space constraints do not allow us to present either
the derivation or a detailed description of the sam-
pling algorithm. However, note that the conditional
distribution used in sampling decomposes into two
parts:
P (vd,i = v, rd,i = r, zd,i = z|v?, r?, z?,w, y) ?
?d,iv,r,z ? ?d,ir,z, (2)
where v?, r? and z? are vectors of assignments of
sliding windows, context (global or local) and top-
ics for all the words in the collection except for the
considered word at position i in document d; y is the
vector of sentiment ratings. The first factor ?d,iv,r,z is
responsible for modeling co-occurrences on the win-
dow and document level and coherence of the topics.
This factor is proportional to the conditional distri-
bution used in the Gibbs sampler of the MG-LDA
model (Titov and McDonald, 2008). The last fac-
tor quantifies the influence of the assignment of the
word (d, i) on the probability of the sentiment rat-
ings. It appears only if ratings are known (observ-
able) and equals:
?d,ir,z =
?
a
P (yda|w, r?, rd,i = r, z?, zd,i = z)
P (yda|w, r?, z?, rd,i = gl)
,
where the probability distribution is computed as de-
fined in expression (1), yda is the rating for the ath
aspect of review d.
3 Experiments
In this section we present qualitative and quantita-
tive experiments. For the qualitative analysis we
show that topics inferred by the MAS model cor-
respond directly to the associated aspects. For the
quantitative analysis we show that the MAS model
induces a distribution over the rated aspects which
can be used to accurately predict whether a text frag-
ment is relevant to an aspect or not.
3.1 Qualitative Evaluation
To perform qualitative experiments we used a set
of reviews of hotels taken from TripAdvisor.com7
that contained 10,000 reviews (109,024 sentences,
2,145,313 words in total). Every review was
rated with at least three aspects: service, location
and rooms. Each rating is an integer from 1 to 5.
The dataset was tokenized and sentence split auto-
matically.
7(c) 2005-06, TripAdvisor, LLC All rights reserved
312
rated aspect top words
service staff friendly helpful service desk concierge excellent extremely hotel great reception english pleasant help
location hotel walk location station metro walking away right minutes close bus city located just easy restaurants
local rooms room bathroom shower bed tv small water clean comfortable towels bath nice large pillows space beds tub
topics - breakfast free coffee internet morning access buffet day wine nice lobby complimentary included good fruit
- $ night parking rate price paid day euros got cost pay hotel worth euro expensive car extra deal booked
- room noise night street air did door floor rooms open noisy window windows hear outside problem quiet sleep
global - moscow st russian petersburg nevsky russia palace hermitage kremlin prospect river prospekt kempinski
topics - paris tower french eiffel dame notre rue st louvre rer champs opera elysee george parisian du pantheon cafes
Table 1: Top words from MAS for hotel reviews.
Krooms top words
2 rooms clean hotel room small nice comfortable modern good quite large lobby old decor spacious decorated bathroom size
room noise night street did air rooms door open noisy window floor hear windows problem outside quiet sleep bit light
3 room clean bed comfortable rooms bathroom small beds nice large size tv spacious good double big space huge king
room floor view rooms suite got views given quiet building small balcony upgraded nice high booked asked overlooking
room bathroom shower air water did like hot small towels door old window toilet conditioning open bath dirty wall tub
4 room clean rooms comfortable bed small beds nice bathroom size large modern spacious good double big quiet decorated
check arrived time day airport early room luggage took late morning got long flight ready minutes did taxi bags went
room noise night street did air rooms noisy open door hear windows window outside quiet sleep problem floor conditioning
bathroom room shower tv bed small water towels bath tub large nice toilet clean space toiletries flat wall sink screen
Table 2: Top words for aspect rooms with different number of topicsKrooms.
We ran the sampling chain for 700 iterations to
produce a sample. Distributions of words in each
topic were estimated as the proportion of words as-
signed to each topic, taking into account topic model
priors ?gl and ?loc. The sliding windows were cho-
sen to cover 3 sentences for all the experiments. All
the priors were chosen to be equal to 0.1. We used
15 local topics and 30 global topics. In the model,
the first three local topics were associated to the
rating classifiers for each aspects. As a result, we
would expect these topics to correspond to the ser-
vice, location, and rooms aspects respectively. Un-
igram and bigram features were used in the senti-
ment predictors in the MAS model. Before apply-
ing the topic models we removed punctuation and
also removed stop words using the standard list of
stop words,8 however, all the words and punctuation
were used in the sentiment predictors.
It does not take many chain iterations to discover
initial topics. This happens considerably faster than
the appropriate weights of the sentiment predictor
being learned. This poses a problem, because, in the
beginning, the sentiment predictors are not accurate
enough to force the model to discover appropriate
topics associated with each of the rated aspects. And
as soon as topic are formed, aspect sentiment predic-
tors cannot affect them anymore because they do not
8http://www.dcs.gla.ac.uk/idom/ir resources/linguistic utils/
stop words
have access to the true words associated with their
aspects. To combat this problem we first train the
sentiment classifiers by assuming that paf,r,z is equal
for all the local topics, which effectively ignores the
topic model. Then we use the estimated parame-
ters within the topic model.9 Secondly, we mod-
ify the sampling algorithm. The conditional prob-
ability used in sampling, expression (2), is propor-
tional to the product of two factors. The first factor,
?d,iv,r,z , expresses a preference for topics likely from
the co-occurrence information, whereas the second
one, ?d,ir,z , favors the choice of topics which are pre-
dictive of the observable sentiment ratings. We used
(?d,ir,z)1+0.95
tq in the sampling distribution instead of
?d,ir,z , where t is the iteration number. q was chosen
to be 4, though the quality of the topics seemed to
be indistinguishable with any q between 3 and 10.
This can be thought of as having 1 + 0.95tq ratings
instead of a single vector assigned to each review,
i.e., focusing the model on prediction of the ratings
rather than finding the topic labels which are good at
explaining co-occurrences of words. These heuris-
tics influence sampling only during the first itera-
tions of the chain.
Top words for some of discovered local topics, in-
9Initial experiments suggested that instead of doing this
?pre-training? we could start with very large priors ?loc and
?mix, and then reduce them through the course of training.
However, this is significantly more computationally expensive.
313
 0
 10
 20
 30
 40
 50
 60
 70
 80
 90
 100
 0  10  20  30  40  50  60  70  80  90  100
Recall
Pr
ec
isi
on
topic model
max?ent classifier
topic model
max?ent classifier
 0
 10
 20
 30
 40
 50
 60
 70
 80
 90
 100
 0  10  20  30  40  50  60  70  80  90  100
Recall
Pr
ec
isi
on
max?ent classifier
1 topic
2 topics
3 topics
4 topics
 0
 10
 20
 30
 40
 50
 60
 70
 80
 90
 100
 0  10  20  30  40  50  60  70  80  90  100
Recall
Pr
ec
isi
on
(a) (b) (c)
Figure 4: (a) Aspect service. (b) Aspect location. (c) Aspect rooms.
cluding the first 3 topics associated with the rated as-
pects, and also top words for some of global topics
are presented in Table 1. We can see that the model
discovered as its first three topics the correct associ-
ated aspects: service, location, and rooms. Other lo-
cal topics, as for the MG-LDA model, correspond to
other aspects discussed in reviews (breakfast, prices,
noise), and as it was previously shown in Titov and
McDonald (2008), aspects for global topics corre-
spond to the types of reviewed items (hotels in Rus-
sia, Paris hotels) or background words.
Notice though, that the 3rd local topic induced for
the rating rooms is slightly narrow. This can be ex-
plained by the fact that the aspect rooms is a central
aspect of hotel reviews. A very significant fraction
of text in every review can be thought of as a part of
the aspect rooms. These portions of reviews discuss
different coherent sub-aspects related to the aspect
rooms, e.g., the previously discovered topic noise.
Therefore, it is natural to associate several topics to
such central aspects. To test this we varied the num-
ber of topics associated with the sentiment predictor
for the aspect rooms. Top words for resulting top-
ics are presented in Table 2. It can be observed that
the topic model discovered appropriate topics while
the number of topics was below 4. With 4 topics
a semantically unrelated topic (check-in/arrival) is
induced. Manual selection of the number of topics
is undesirable, but this problem can be potentially
tackled with Dirichlet Process priors or a topic split
criterion based on the accuracy of the sentiment pre-
dictor in the MAS model. We found that both ser-
vice and location did not benefit by the assignment
of additional topics to their sentiment rating models.
The experimental results suggest that the MAS
model is reliable in the discovery of topics corre-
sponding to the rated aspects. In the next section
we will show that the induced topics can be used to
accurately extract fragments for each aspect.
3.2 Sentence Labeling
A primary advantage of MAS over unsupervised
models, such as MG-LDA or clustering, is that top-
ics are linked to a rated aspect, i.e., we know ex-
actly which topics model which aspects. As a re-
sult, these topics can be directly used to extract tex-
tual mentions that are relevant for an aspect. To test
this, we hand labeled 779 random sentences from
the dataset considered in the previous set of experi-
ments. The sentences were labeled with one or more
aspects. Among them, 164, 176 and 263 sentences
were labeled as related to aspects service, location
and rooms, respectively. The remaining sentences
were not relevant to any of the rated aspects.
We compared two models. The first model uses
the first three topics of MAS to extract relevant men-
tions based on the probability of that topic/aspect be-
ing present in the sentence. To obtain these probabil-
ities we used estimators based on the proportion of
words in the sentence assigned to an aspects? topic
and normalized within local topics. To improve the
reliability of the estimator we produced 100 sam-
ples for each document while keeping assignments
of the topics to all other words in the collection fixed.
The probability estimates were then obtained by av-
eraging over these samples. We did not perform
any model selection on the basis of the hand-labeled
data, and tested only a single model of each type.
314
For the second model we trained a maximum en-
tropy classifier, one per each aspect, using 10-fold
cross validation and unigram/bigram features. Note
that this is a supervised system and as such repre-
sents an upper-bound in performance one might ex-
pect when comparing an unsupervised model such
as MAS. We chose this comparison to demonstrate
that our model can find relevant text mentions with
high accuracy relative to a supervised model. It is
difficult to compare our model to other unsupervised
systems such as MG-LDA or LDA. Again, this is
because those systems have no mechanism for di-
rectly correlating topics or clusters to corresponding
aspects, highlighting the benefit of MAS.
The resulting precision-recall curves for the as-
pects service, location and rooms are presented
in Figure 4. In Figure 4c, we varied the number
of topics associated with the aspect rooms.10 The
average precision we obtained (the standard mea-
sure proportional to the area under the curve) is
75.8%, 85.5% for aspects service and location, re-
spectively. For the aspect rooms these scores are
equal to 75.0%, 74.5%, 87.6%, 79.8% with 1?4 top-
ics per aspect, respectively. The logistic regression
models achieve 80.8%, 94.0% and 88.3% for the as-
pects service, location and rooms. We can observe
that the topic model, which does not use any explic-
itly aspect-labeled text, achieves accuracies lower
than, but comparable to a supervised model.
4 Related Work
There is a growing body of work on summariz-
ing sentiment by extracting and aggregating senti-
ment over ratable aspects and providing correspond-
ing textual evidence. Text excerpts are usually ex-
tracted through string matching (Hu and Liu, 2004a;
Popescu and Etzioni, 2005), sentence clustering
(Gamon et al, 2005), or through topic models (Mei
et al, 2007; Titov and McDonald, 2008). String ex-
traction methods are limited to fine-grained aspects
whereas clustering and topic model approaches must
resort to ad-hoc means of labeling clusters or topics.
However, this is the first work we are aware of that
uses a pre-defined set of aspects plus an associated
signal to learn a mapping from text to an aspect for
10To improve readability we smoothed the curve for the as-
pect rooms.
the purpose of extraction.
A closely related model to ours is that of Mei et
al. (2007) which performs joint topic and sentiment
modeling of collections. Our model differs from
theirs in many respects: Mei et al only model senti-
ment predictions for the entire document and not on
the aspect level; They treat sentiment predictions as
unobserved variables, whereas we treat them as ob-
served signals that help to guide the creation of top-
ics; They model co-occurrences solely on the docu-
ment level, whereas our model is based onMG-LDA
and models both local and global contexts.
Recently, Blei and McAuliffe (2008) proposed an
approach for joint sentiment and topic modeling that
can be viewed as a supervised LDA (sLDA) model
that tries to infer topics appropriate for use in a
given classification or regression problem. MAS and
sLDA are similar in that both use sentiment predic-
tions as an observed signal that is predicted by the
model. However, Blei et al do not consider multi-
aspect ranking or look at co-occurrences beyond the
document level, both of which are central to our
model. Parallel to this study Branavan et al (2008)
also showed that joint models of text and user anno-
tations benefit extractive summarization. In partic-
ular, they used signals from pros-cons lists whereas
our models use aspect rating signals.
5 Conclusions
In this paper we presented a joint model of text and
aspect ratings for extracting text to be displayed in
sentiment summaries. The model uses aspect ratings
to discover the corresponding topics and can thus ex-
tract fragments of text discussing these aspects with-
out the need of annotated data. We demonstrated
that the model indeed discovers corresponding co-
herent topics and achieves accuracy in sentence la-
beling comparable to a standard supervised model.
The primary area of future work is to incorporate the
model into an end-to-end sentiment summarization
system in order to evaluate it at that level.
Acknowledgments
This work benefited from discussions with Sasha
Blair-Goldensohn and Fernando Pereira.
315
References
David M. Blei and Jon D. McAuliffe. 2008. Supervised
topic models. In Advances in Neural Information Pro-
cessing Systems (NIPS).
D.M. Blei, A.Y. Ng, and M.I. Jordan. 2003. Latent
Dirichlet alocation. Journal of Machine Learning Re-
search, 3(5):993?1022.
S.R.K. Branavan, H. Chen, J. Eisenstein, and R. Barzi-
lay. 2008. Learning document-level semantic proper-
ties from free-text annotations. In Proceedings of the
Annual Conference of the Association for Computa-
tional Linguistics.
G. Carenini, R. Ng, and A. Pauls. 2006. Multi-Document
Summarization of Evaluative Text. In Proceedings of
the Conference of the European Chapter of the Asso-
ciation for Computational Linguistics.
M. Gamon, A. Aue, S. Corston-Oliver, and E. Ringger.
2005. Pulse: Mining customer opinions from free text.
In Proc. of the 6th International Symposium on Intelli-
gent Data Analysis, pages 121?132.
S. Geman and D. Geman. 1984. Stochastic relaxation,
Gibbs distributions, and the Bayesian restoration of
images. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 6:721?741.
T. L. Griffiths and M. Steyvers. 2004. Finding scien-
tific topics. Proceedings of the Natural Academy of
Sciences, 101 Suppl 1:5228?5235.
T. L. Griffiths, M. Steyvers, D. M. Blei, and J. B. Tenen-
baum. 2004. Integrating topics and syntax. In Ad-
vances in Neural Information Processing Systems.
A. Gruber, Y. Weiss, and M. Rosen-Zvi. 2007. Hidden
Topic Markov Models. In Proceedings of the Confer-
ence on Artificial Intelligence and Statistics.
M. Hu and B. Liu. 2004a. Mining and summarizing
customer reviews. In Proceedings of the 2004 ACM
SIGKDD international conference on Knowledge dis-
covery and data mining, pages 168?177. ACM Press
New York, NY, USA.
M. Hu and B. Liu. 2004b. Mining Opinion Features
in Customer Reviews. In Proceedings of Nineteenth
National Conference on Artificial Intellgience.
C. Manning and M. Schutze. 1999. Foundations of Sta-
tistical Natural Language Processing. MIT Press.
Q. Mei, X. Ling, M.Wondra, H. Su, and C.X. Zhai. 2007.
Topic sentiment mixture: modeling facets and opin-
ions in weblogs. In Proceedings of the 16th Interna-
tional Conference on World Wide Web, pages 171?180.
Radford Neal. 1992. Connectionist learning of belief
networks. Artificial Intelligence, 56:71?113.
B. Pang, L. Lee, and S. Vaithyanathan. 2002. Thumbs
up? Sentiment classification using machine learning
techniques. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing.
A.M. Popescu and O. Etzioni. 2005. Extracting product
features and opinions from reviews. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing (EMNLP).
B. Snyder and R. Barzilay. 2007. Multiple Aspect Rank-
ing using the Good Grief Algorithm. In Proceedings
of the Joint Conference of the North American Chapter
of the Association for Computational Linguistics and
Human Language Technologies, pages 300?307.
I. Titov and R. McDonald. 2008. Modeling online re-
views with multi-grain topic models. In Proceedings
of the 17h International Conference on World Wide
Web.
P. Turney. 2002. Thumbs up or thumbs down? Senti-
ment orientation applied to unsupervised classification
of reviews. In Proceedings of the Annual Conference
of the Association for Computational Linguistics.
Hanna M. Wallach. 2006. Topic modeling; beyond bag
of words. In International Conference on Machine
Learning.
Xuerui Wang and Andrew McCallum. 2005. A note on
topical n-grams. Technical Report UM-CS-2005-071,
University of Massachusetts.
J. Wiebe. 2000. Learning subjective adjectives from cor-
pora. In Proceedings of the National Conference on
Artificial Intelligence.
L. Zhuang, F. Jing, and X.Y. Zhu. 2006. Movie re-
view mining and summarization. In Proceedings of
the 15th ACM international conference on Information
and knowledge management (CIKM), pages 43?50.
316
Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL): Shared Task, pages 37?42,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
A Latent Variable Model of Synchronous Syntactic-Semantic Parsing for
Multiple Languages
Andrea Gesmundo
Univ Geneva
Dept Computer Sci
Andrea.Gesmundo@
unige.ch
James Henderson
Univ Geneva
Dept Computer Sci
James.Henderson@
unige.ch
Paola Merlo
Univ Geneva
Dept Linguistics
Paola.Merlo@
unige.ch
Ivan Titov?
Univ Illinois at U-C
Dept Computer Sci
titov@uiuc.edu
Abstract
Motivated by the large number of languages
(seven) and the short development time (two
months) of the 2009 CoNLL shared task, we
exploited latent variables to avoid the costly
process of hand-crafted feature engineering,
allowing the latent variables to induce features
from the data. We took a pre-existing gener-
ative latent variable model of joint syntactic-
semantic dependency parsing, developed for
English, and applied it to six new languages
with minimal adjustments. The parser?s ro-
bustness across languages indicates that this
parser has a very general feature set. The
parser?s high performance indicates that its la-
tent variables succeeded in inducing effective
features. This system was ranked third overall
with a macro averaged F1 score of 82.14%,
only 0.5% worse than the best system.
1 Introduction
Recent research in syntax-based statistical machine
translation and the recent availability of syntac-
tically annotated corpora for multiple languages
(Nivre et al, 2007) has provided a new opportunity
for evaluating the cross-linguistic validity of statis-
tical models of syntactic structure. This opportu-
nity has been significantly expanded with the 2009
CoNLL shared task on syntactic and semantic pars-
ing of seven languages (Hajic? et al, 2009) belonging
to several different language families.
We participate in this task with a generative,
history-based model proposed in the CoNLL 2008
0Authors in alphabetical order.
shared task for English (Henderson et al, 2008) and
further improved to tackle non-planar dependencies
(Titov et al, 2009). This model maximises the joint
probability of the syntactic and semantic dependen-
cies and thereby enforces that the output structure be
globally coherent, but the use of synchronous pars-
ing allows it to maintain separate structures for the
syntax and semantics. The probabilistic model is
based on Incremental Sigmoid Belief Networks (IS-
BNs), a recently proposed latent variable model for
syntactic structure prediction, which has shown very
good performance for both constituency (Titov and
Henderson, 2007a) and dependency parsing (Titov
and Henderson, 2007b). The use of latent variables
enables this architecture to be extended to learning
a synchronous parse of syntax and semantics with-
out overly restrictive assumptions about the linking
between syntactic and semantic structures.
In this work, we evaluate the ability of this
method to generalise across several languages. We
take the model as it was developed for English, and
apply it directly to all seven languages. The only
fine-tuning was to evaluate whether to include one
feature type which we had previously found did not
help for English, but helped overall. No other fea-
ture engineering was done. The use of latent vari-
ables to induce features automatically from the data
gives our method the adaptability necessary to per-
form well across all seven languages, and demon-
strates the lack of language specificity in the models
of Henderson et al (2008) and Titov et al (2009).
The main properties of this model, that differen-
tiate it from other approaches, is the use of syn-
chronous syntactic and semantic derivations and the
37
use of online planarisation of crossing semantic de-
pendencies. This system was ranked third overall
with a macro averaged F1 score of 82.14%, only
0.5% worse than the best system.
2 The Synchronous Model
The use of synchronous parsing allows separate
structures for syntax and semantics, while still mod-
eling their joint probability. We use the approach
to synchronous parsing proposed in Henderson et al
(2008), where we start with two separate derivations
specifying each of the two structures, then synchro-
nise these derivations at each word. The individual
derivations are based on Nivre?s shift-reduce-style
parsing algorithm (Nivre et al, 2006), as discussed
further below. First we illustrate the high-level struc-
ture of the model, discussed in more detail in Hen-
derson et al (2008).
Let Td be a syntactic dependency tree with
derivation D1d, ..., Dmdd , and Ts be a semantic
dependency graph with derivation D1s , ..., Dmss .
To define derivations for the joint structure
Td, Ts, we divide the two derivations into the
chunks between shifting each word onto the
stack, ctd = Db
t
d
d , ..., D
etd
d and cts = Db
t
ss , ..., De
t
ss ,
where Dbtd?1d = Db
t
s?1s = Shiftt?1 and
De
t
d+1
d = De
t
s+1s = Shiftt. Then the actions of
the synchronous derivations consist of quadruples
Ct = (ctd, Switch, cts, Shiftt), where Switch means
switching from syntactic to semantic mode. This
gives us the following joint probability model,
where n is the number of words in the input.
P (Td, Ts) = ?nt=1 P (Ct|C1, . . . , Ct?1) (1)
These synchronous derivations C1, . . . , Cn only re-
quire a single input queue, since the Shift actions are
synchronised, but they require two separate stacks,
one for the syntactic derivation and one for the se-
mantic derivation.
The probability of each synchronous derivation
chunk Ct is the product of four factors, related to
the syntactic level, the semantic level and the two
synchronising steps. The probability of ctd is de-
composed into one probability for each derivation
action Di, conditioned on its history using the chain
rule, and likewise for cts. These probabilities are es-
timated using the method described in section 3.
Syn cross Sem cross Sem tree No parse
Cat 0% 0% 61.4% 0%
Chi 0% 28.0% 28.6% 9.5%
Cze 22.4% 16.3% 6.1% 1.8%
Eng 7.6% 43.9% 21.4% 3.9%
Ger 28.1% 1.3% 97.4% 0.0%
Jap 0.9% 38.3% 11.2% 14.4%
Spa 0% 0% 57.1% 0%
Table 1: For each language, percentage of training sen-
tences with crossing arcs in syntax and semantics, with
semantic arcs forming a tree, and which were not parsable
using the Swap action.
One of the main characteristics of our syn-
chronous representation, unlike other synchronous
representations of syntax and semantics (Nesson et
al., 2008), is that the synchronisation is done on
words, rather than on structural components. We
take advantage of this freedom and adopt different
methods for handling crossing arcs for syntax and
for semantics.
While both syntax and semantics are represented
as dependency graphs, these graphs differ substan-
tially in their properties. Some statistics which in-
dicate these differences are shown in table 1. For
example, English syntactic dependencies form trees,
while semantic dependency structures are only trees
21.4% of the time, since in general each struc-
ture does not form a connected graph and some
nodes may have more than one parent. The syn-
tactic dependency structures for only 7.6% of En-
glish sentences contain crossing arcs, while 43.9%
of the semantic dependency structures contain cross-
ing arcs. Due to variations both in language char-
acteristics and annotation decisions across corpora,
these differences between syntax and semantics vary
across the seven languages, but they are consis-
tent enough to motivate the development of new
techniques specifically for handling semantic depen-
dency structures. In particular, we use a different
method for parsing crossing arcs.
For parsing crossing semantic arcs (i.e. non-
planar graphs), we use the approach proposed in
Titov et al (2009), which introduces an action Swap
that swaps the top two elements on the parser?s
stack. The Swap action allows the parser to reorder
words online during the parse. This allows words
to be processed in different orders during different
38
portions of the parse, so some arcs can be specified
using one ordering, then other arcs can be specified
using another ordering. Titov et al (2009) found that
only using the Swap action as a last resort is the best
strategy for English (compared to using it preemp-
tively to address future crossing arcs) and we use
the same strategy here for all languages.
Syntactic graphs do not use a Swap action.
We adopt the HEAD method of Nivre and Nils-
son (2005) to de-projectivise syntactic dependencies
outside of parsing.1
3 Features and New Developments
The synchronous derivations described above are
modelled with a type of Bayesian Network called an
Incremental Sigmoid Belief Network (ISBN) (Titov
and Henderson, 2007a). As in Henderson et al
(2008), the ISBN model distinguishes two types of
latent states: syntactic states, when syntactic deci-
sions are considered, and semantic states, when se-
mantic decision are considered. Latent states are
vectors of binary latent variables, which are condi-
tioned on variables from previous states via a pattern
of connecting edges determined by the previous de-
cisions. These latent-to-latent connections are used
to engineer soft biases which reflect the relevant do-
mains of locality in the structure being built. For
these we used the set of connections proposed in
Titov et al (2009), which includes latent-to-latent
connections both from syntax states to semantics
states and vice versa. The latent variable vectors are
also conditioned on a set of observable features of
the derivation history. For these features, we start
with the feature set from Titov et al (2009), which
extends the semantic features proposed in Hender-
son et al (2008) to allow better handling of the non-
planar structures in semantics. Most importantly, all
the features previously included for the top of the
stack were also included for the word just under the
top of the stack. To this set we added one more type
of feature, discussed below.
We made some modifications to reflect differ-
ences in the task definition between the 2008 and
2009 shared tasks, and experimented with one
type of features which had been previously imple-
1The statistics in Table 1 suggest that, for some languages,
swapping might be beneficial for syntax as well.
mented. For the former modifications, the system
was adapted to allow the use of the PFEAT and
FILLPRED fields in the data, which both resulted
in improved accuracy for all the languages. The
PFEAT data field (automatically predicted morpho-
logical features) was introduced in the system in
two ways, as an atomic feature bundle that is pre-
dicted when predicting the word, and split into its
elementary components when conditioning on a pre-
vious word, as was done in Titov and Henderson
(2007b). Because the testing data included a spec-
ification of which words were annotated as predi-
cates (the FILLPRED data field), we constrained the
parser?s output so as to be consistent with this speci-
fication. For rare predicates, if the predicate was not
in the parser?s lexicon (extracted from the training
set), then a sense was taken from the list of senses
reported in the Lexicon and Frame Set resources
available for the closed challenge. If this informa-
tion was not available, then a default sense was con-
structed based on the automatically predicted lemma
(PLEMMA) of the predicate.
We also made use of a previously implemented
type of feature that allows the prediction of a seman-
tic link between two words to be conditioned on the
syntactic dependency already predicted between the
same two words. While this feature had previously
not helped for English, it did result in an overall im-
provement across the languages.
Also, in comparison with previous experiments,
the search beam used in the decoding phase was in-
creased from 50 to 80, producing a small improve-
ment in the overall development score.
All development effort took about two person-
months, mostly by someone who had no previous
experience with the system. Most of this time was
spent on the above differences in the task definition
between the 2008 and 2009 shared tasks.
4 Results and Discussion
We participated in the joint task of the closed chal-
lenge, as described in Hajic? et al (2009). The
datasets used in this challenge are described in Taule?
et al (2008) (Catalan and Spanish), Palmer and Xue
(2009) (Chinese), Hajic? et al (2006) (Czech), Sur-
deanu et al (2008) (English), Burchardt et al (2006)
(German), and Kawahara et al (2002) (Japanese).
39
Rank Average Catalan Chinese Czech English German Japanese Spanish
macro F1 3 82.14 82.66 76.15 83.21 86.03 79.59 84.91 82.43
syntactic acc 1 @85.77 @87.86 76.11 @80.38 88.79 87.29 92.34 @87.64
semantic F1 3 78.42 77.44 76.05 86.02 83.24 71.78 77.23 77.19
Table 2: The three main scores for our system. Rank is within task.
Rank Ave Cze-ood Eng-ood Ger-ood
macro F1 3 75.93 @80.70 75.76 71.32
syn Acc 2 78.01 @76.41 80.84 76.77
sem F1 3 73.63 84.99 70.65 65.25
Table 3: Results on out-of-domain for our system. Rank
is within task.
The official results on the testing set are shown in
tables 2, 3, and 4. The symbol ?@? indicates the
best result across systems. In table 5, we show our
rankings across the different datasets, amongst sys-
tems submitted for the same task.
The overall score used to rank systems is the un-
weighted average of the syntactic labeled accuracy
and the semantic labeled F1 measure, across all lan-
guages (?macro F1? in table 2). We were ranked
third, out of 14 systems. There was only a 0.5% dif-
ference between our score and that of the best sys-
tem, while there was a 1.29% difference between our
score and the fourth ranked system. Only consid-
ering syntactic accuracy, we had the highest aver-
age score of all systems, with the highest individual
score for Catalan, Czech, and Spanish. Only con-
sidering semantic F1, we were again ranked third.
Our results for out-of-domain data (table 3) achieved
a similar level of success, although here we were
ranked second for average syntactic accuracy. Our
precision on semantic arcs was generally much bet-
ter than our recall (shown in table 4). However,
other systems had a similar imbalance, resulting in
no change in our third place ranking for semantic
precision and for semantic recall. Only when the se-
mantic precision is averaged with syntactic accuracy
do we squeeze into second place (?macro Prec?).
To get a more detailed picture of the strengths
and weaknesses of our system, we computed its rank
within each dataset, shown in table 5. Overall, our
system is robust across languages, with little fluc-
tuation in ranking for the overall score, including
for out-of-domain data. The one noticeable excep-
tion to this consistency is the syntactic score for En-
data time (min) macro F1
Czech 25% 5007 73.84
50% 3699 77.57
75% 4201 79.10
100% 6870 80.55
English 25% 1300 79.02
50% 1899 81.61
75% 3196 82.41
100% 3191 83.27
Table 6: Training times and development set accuracies
using different percentages of the training data, for Czech
and English.
glish out-of-domain data. The other ranks for En-
glish out-of-domain and English in-domain scores
are also on the poor side. These results support our
claim that our parser has not undergone much hand-
tuning, since it was originally developed for English.
It is not currently clear whether this relative differ-
ence reflects a English-specific weakness in our sys-
tem, or that many of the other systems have been
fine-tuned for English.
On the higher end of our dataset rankings, we
do relatively well on Catalan, Czech, and Span-
ish. Catalan and Spanish are unique amongst these
datasets in that they have no crossing arcs in their
semantic structure. Czech seems to have semantic
structures which are relatively well handled by our
derivations with Swap. As indicated above in ta-
ble 1, only 2% of sentences are unparsable, despite
16% requiring the Swap action. However, this argu-
ment does not explain why our parser did relatively
poorly on German semantic dependencies. Regard-
less, these observations would suggest that our sys-
tem is still having trouble with crossing dependen-
cies, despite the introduction of the Swap operation,
and that our learning method could achieve better
performance with an improved treatment of cross-
ing semantic dependencies.
Table 6 shows how accuracies and training times
vary with the size of the training dataset, for Czech
and English. Training times vary in part because
40
Rank Ave Cat Chi Cze Eng Ger Jap Spa Cze-ood Eng-ood Ger-ood
semantic Prec 3 81.60 79.08 80.93 87.45 84.92 75.60 83.75 79.44 85.90 72.89 75.19
semantic Rec 3 75.56 75.87 71.73 @84.64 81.63 68.33 71.65 75.05 @84.09 68.55 57.63
macro Prec 2 83.68 83.47 78.52 83.91 86.86 81.44 88.05 83.54 81.16 76.86 @75.98
macro Rec 3 80.66 @81.86 73.92 @82.51 85.21 77.81 81.99 81.35 @80.25 74.70 67.20
Table 4: Semantic precision and recall and macro precision and recall for our system. Rank is within task.
Rank by Ave Cat Chi Cze Eng Ger Jap Spa Ave-ood Cze-ood Eng-ood Ger-ood
macro F1 3 2 3 2 4 4 3 2 3 1 4 3
syntactic Acc 1 1 4 1 3 2 2 1 2 1 7 2
semantic F1 3 2 4 2 4 5 4 2 3 2 4 3
Table 5: Our system?s rank within task according to the three main measures, for each dataset.
?1.4
?1.2
?1
?0.8
?0.6
?0.4
?0.2
 0
 0  10  20  30  40  50
M
ac
ro
 F
1 
D
iff
er
en
ce
Words per Second
Jap
Spa
Cat
Ger
Eng
Cze
Chi
Figure 1: Difference in development set macro F1 as the
search beam is decreased from the submitted beam (80)
to 40, 20, 10, and 5, plotted against parser speed.
random variations can result in different numbers of
training cycles before convergence. Accuracies ap-
pear to be roughly log-linear with data size.
Figure 1 shows how the accuracy of the parser de-
grades as we speed it up by decreasing the search
beam used in decoding, for each language. For some
languages, a slightly smaller search beam is actually
more accurate,2 but for smaller beams the trade-off
of accuracy versus words-per-second is roughly lin-
ear. Parsing time per word is also linear in beam
width, with a zero intercept.
5 Conclusion
In the joint task of the closed challenge of the
CoNLL 2009 shared task (Hajic? et al, 2009), we in-
vestigated how well a model of syntactic-semantic
dependency parsing developed for English would
2This fact suggests that we could have gotten improved re-
sults by tailoring the search beam to individual languages.
generalise to the other six languages. This model
provides a single generative probability of the joint
syntactic and semantic dependency structures, but
allows separate representations for these two struc-
tures by parsing the two structures synchronously.
Finding the statistical correlations both between and
within these structures is facilitated through the use
of latent variables, which induce features automat-
ically from the data, thereby greatly reducing the
need for hand-coded feature engineering.
This latent variable model proved very robust
across languages, achieving a ranking of between
second and fourth on each language, including for
out-of-domain data. The extent to which the parser
does not rely on hand-crafting is underlined by the
fact that its worst ranking is for English, the lan-
guage for which it was developed (particularly for
out-of-domain data). The parser was ranked third
overall out of 14 systems, with a macro averaged F1
score of 82.14%, only 0.5% worse than the best sys-
tem.
Both joint learning and conditioning decisions
about semantic dependencies on latent representa-
tions of syntactic parsing states were crucial to the
success of our model, as was previously demon-
strated in Henderson et al (2008). There, remov-
ing this conditioning led to a 3.5% drop in the SRL
score. This result seems to contradict the gen-
eral trend in the CoNLL-2008 shared task, where
joint learning had only limited success. The lat-
ter fact may be explained by recent theoretical re-
sults demonstrating that pipelines can be preferable
to joint learning (Roth et al, 2009) when no shared
hidden representation is learnt. Our system (Hender-
son et al, 2008) was the only one which attempted to
41
learn a common hidden representation for this mul-
titask learning problem and also was the only one
which achieved significant gain from joint parameter
estimation. We believe that learning shared hidden
representations for related NLP problems is a very
promising direction for further research.
Acknowledgements
We thank Gabriele Musillo and Dan Roth for help and
advice. This work was partly funded by Swiss NSF
grants 100015-122643 and PBGE22-119276, European
Community FP7 grant 216594 (CLASSiC, www.classic-
project.org), US NSF grant SoD-HCER-0613885 and
DARPA (Bootstrap Learning Program).
References
Aljoscha Burchardt, Katrin Erk, Anette Frank, Andrea
Kowalski, Sebastian Pado?, and Manfred Pinkal. 2006.
The SALSA corpus: a German corpus resource for
lexical semantics. In Proceedings of the 5th Interna-
tional Conference on Language Resources and Evalu-
ation (LREC-2006), Genoa, Italy.
Jan Hajic?, Jarmila Panevova?, Eva Hajic?ova?, Petr
Sgall, Petr Pajas, Jan S?te?pa?nek, Jir??? Havelka, Marie
Mikulova?, and Zdene?k Z?abokrtsky?. 2006. Prague De-
pendency Treebank 2.0.
Jan Hajic?, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Anto`nia Mart??, Llu??s
Ma`rquez, Adam Meyers, Joakim Nivre, Sebastian
Pado?, Jan S?te?pa?nek, Pavel Stran?a?k, Mihai Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The CoNLL-
2009 shared task: Syntactic and semantic depen-
dencies in multiple languages. In Proceedings of
the 13th Conference on Computational Natural Lan-
guage Learning (CoNLL-2009), June 4-5, Boulder,
Colorado, USA.
James Henderson, Paola Merlo, Gabriele Musillo, and
Ivan Titov. 2008. A latent variable model of syn-
chronous parsing for syntactic and semantic dependen-
cies. In Proceedings of CONLL 2008, pages 178?182,
Manchester, UK.
Daisuke Kawahara, Sadao Kurohashi, and Ko?iti Hasida.
2002. Construction of a Japanese relevance-tagged
corpus. In Proceedings of the 3rd International
Conference on Language Resources and Evaluation
(LREC-2002), pages 2008?2013, Las Palmas, Canary
Islands.
Rebecca Nesson, Giorgio Satta, and Stuart M. Shieber.
2008. Optimal k-arization of synchronous tree-
adjoining grammar. In Proceedings of ACL-08: HLT,
pages 604?612, Columbus, Ohio, June.
Joakim Nivre and Jens Nilsson. 2005. Pseudo-projective
dependency parsing. In Proc. 43rd Meeting of Asso-
ciation for Computational Linguistics, pages 99?106,
Ann Arbor, MI.
Joakim Nivre, Johan Hall, Jens Nilsson, Gulsen Eryigit,
and Svetoslav Marinov. 2006. Pseudo-projective de-
pendency parsing with support vector machines. In
Proc. of the Tenth Conference on Computational Nat-
ural Language Learning, pages 221?225, New York,
USA.
Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan Mc-
Donald, Jens Nilsson, Sebastian Riedel, and Deniz
Yuret. 2007. The CoNLL 2007 shared task on depen-
dency parsing. In Proceedings of the CoNLL Shared
Task Session of EMNLP-CoNLL 2007, pages 915?932,
Prague, Czech Republic, June.
Martha Palmer and Nianwen Xue. 2009. Adding seman-
tic roles to the Chinese Treebank. Natural Language
Engineering, 15(1):143?172.
Dan Roth, Kevin Small, and Ivan Titov. 2009. Sequential
learning of classifiers for structured prediction prob-
lems. In AISTATS, Clearwater, Florida, USA.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu??s Ma`rquez, and Joakim Nivre. 2008. The CoNLL-
2008 shared task on joint parsing of syntactic and se-
mantic dependencies. In Proceedings of the 12th Con-
ference on Computational Natural Language Learning
(CoNLL-2008).
Mariona Taule?, Maria Anto`nia Mart??, and Marta Re-
casens. 2008. AnCora: Multilevel Annotated Corpora
for Catalan and Spanish. In Proceedings of the 6th
International Conference on Language Resources and
Evaluation (LREC-2008), Marrakesh, Morroco.
Ivan Titov and James Henderson. 2007a. Constituent
parsing with Incremental Sigmoid Belief Networks. In
Proceedings of the 45th Annual Meeting of the Asso-
ciation of Computational Linguistics, pages 632?639,
Prague, Czech Republic.
Ivan Titov and James Henderson. 2007b. Fast and ro-
bust multilingual dependency parsing with a genera-
tive latent variable model. In Proc. Joint Conf. on Em-
pirical Methods in Natural Language Processing and
Computational Natural Language Learning (EMNLP-
CoNLL 2007), Prague, Czech Republic. (CoNLL
Shared Task).
Ivan Titov, James Henderson, Paola Merlo, and Gabriele
Musillo. 2009. Online graph planarisation for syn-
chronous parsing of semantic and syntactic dependen-
cies. In Proc. Twenty-First International Joint Confer-
ence on Artificial Intelligence (IJCAI-09), Pasadena,
California.
42
Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007, pp. 947?951,
Prague, June 2007. c?2007 Association for Computational Linguistics
Fast and Robust Multilingual Dependency Parsing
with a Generative Latent Variable Model
Ivan Titov
University of Geneva
24, rue Ge?ne?ral Dufour
CH-1211 Gene`ve 4, Switzerland
ivan.titov@cui.unige.ch
James Henderson
University of Edinburgh
2 Buccleuch Place
Edinburgh EH8 9LW, United Kingdom
james.henderson@ed.ac.uk
Abstract
We use a generative history-based model to
predict the most likely derivation of a de-
pendency parse. Our probabilistic model is
based on Incremental Sigmoid Belief Net-
works, a recently proposed class of la-
tent variable models for structure predic-
tion. Their ability to automatically in-
duce features results in multilingual pars-
ing which is robust enough to achieve accu-
racy well above the average for each indi-
vidual language in the multilingual track of
the CoNLL-2007 shared task. This robust-
ness led to the third best overall average la-
beled attachment score in the task, despite
using no discriminative methods. We also
demonstrate that the parser is quite fast, and
can provide even faster parsing times with-
out much loss of accuracy.
1 Introduction
The multilingual track of the CoNLL-2007 shared
task (Nivre et al, 2007) considers dependency pars-
ing of texts written in different languages. It re-
quires use of a single dependency parsing model
for the entire set of languages; model parameters
are estimated individually for each language on the
basis of provided training sets. We use a recently
proposed dependency parser (Titov and Hender-
son, 2007b)1 which has demonstrated state-of-the-
art performance on a selection of languages from the
1The ISBN parser will be soon made downloadable from the
authors? web-page.
CoNLL-X shared task (Buchholz and Marsi, 2006).
This parser employs a latent variable model, Incre-
mental Sigmoid Belief Networks (ISBNs), to de-
fine a generative history-based model of projective
parsing. We used the pseudo-projective transforma-
tion introduced in (Nivre and Nilsson, 2005) to cast
non-projective parsing tasks as projective. Follow-
ing (Nivre et al, 2006), the encoding scheme called
HEAD in (Nivre and Nilsson, 2005) was used to en-
code the original non-projective dependencies in the
labels of the projectivized dependency tree. In the
following sections we will briefly discuss our modi-
fications to the ISBN parser, experimental setup, and
achieved results.
2 The Probability Model
Our probability model uses the parsing order pro-
posed in (Nivre et al, 2004), but instead of perform-
ing deterministic parsing as in (Nivre et al, 2004),
this ordering is used to define a generative history-
based model, by adding word prediction to the Shift
parser action. We also decomposed some parser ac-
tions into sub-sequences of decisions. We split arc
prediction decisions (Left-Arcr and Right-Arcr) each
into two elementary decisions: first the parser cre-
ates the corresponding arc, then it assigns a relation
r to the arc. Similarly, we decompose the decision
to shift a word into a decision to shift and a pre-
diction of the word. We used part-of-speech tags
and fine-grain word features, which are given in the
data, to further decompose word predictions. First
we predict the fine-grain part-of-speech tag for the
word, then the set of word features (treating each
set as an atomic value), and only then the particu-
947
lar word form. This approach allows us to both de-
crease the effect of sparsity and to avoid normaliza-
tion across all the words in the vocabulary, signifi-
cantly reducing the computational expense of word
prediction. When conditioning on words, we treated
each word feature individually, as this proved to be
useful in (Titov and Henderson, 2007b).
The probability of each parser decision, condi-
tioned on the complete parse history, is modeled
using a form a graphical model called Incremental
Sigmoid Belief Networks. ISBNs, originally pro-
posed for constituent parsing in (Titov and Hender-
son, 2007a), use vectors of binary latent variables to
encode information about the parse history. These
history variables are similar to the hidden state of
a Hidden Markov Model. But unlike the graphi-
cal model for an HMM, which would specify con-
ditional dependency edges only between adjacent
states in the parse history, the ISBN graphical model
can specify conditional dependency edges between
latent variables which are arbitrarily far apart in the
parse history. The source state of such an edge is
determined by the partial parse structure built at the
time of the destination state, thereby allowing the
conditional dependency edges to be appropriate for
the structural nature of the parsing problem. In par-
ticular, they allow conditional dependencies to be
local in the parse structure, not just local in the his-
tory sequence. In this they are similar to the class
of neural networks proposed in (Henderson, 2003)
for constituent parsing. In fact, in (Titov and Hen-
derson, 2007a) it was shown that this neural network
can be viewed as a coarse approximation to the cor-
responding ISBN model.
Traditional statistical parsing models also condi-
tion on features which are local in the parse struc-
ture, but these features need to be explicitly defined
before learning, and require careful feature selec-
tion. This is especially difficult for languages un-
known to the parser developer, since the number of
possible features grows exponentially with the struc-
tural distance considered.
The ISBN model uses an alternative approach,
where latent variables are used to induce features
during learning. The most important problem in de-
signing an ISBN is to define an appropriate struc-
tural locality for each parser decision. This is done
by choosing a fixed set of relationships between
parser states, where the information which is needed
to make the decision at the earlier state is also use-
ful in making the decision at the later state. The la-
tent variables for these related states are then con-
nected with conditional dependency edges in the
ISBN graphical model. Longer conditional depen-
dencies are then possible through chains of these im-
mediate conditional dependencies, but there is an in-
ductive bias toward shorter chains. This bias makes
it important that the set of chosen relationships de-
fines an appropriate notion of locality. However,
as long as there exists some chain of relationships
between any two states, then any statistical depen-
dency which is clearly manifested in the data can be
learned, even if it was not foreseen by the designer.
This provides a potentially powerful form of feature
induction, which is nonetheless biased toward a no-
tion of locality appropriate for the nature of the prob-
lem.
In our experiments we use the same definition of
structural locality as was proposed for the ISBN de-
pendency parser in (Titov and Henderson, 2007b).
The current state is connected to previous states us-
ing a set of 7 distinct relationships defined in terms
of each state?s parser configuration, which includes
of a stack and a queue. Specifically, the current state
is related to the last previous state whose parser con-
figuration has: the same queue, the same stack, a
stack top which is the rightmost right child of the
current stack top, a stack top which is the leftmost
left child of the current stack top, a front of the queue
which is the leftmost child of the front of the cur-
rent queue, a stack top which is the head word of
the current stack top, a front of the queue which is
the current stack top. Different model parameters
are trained for each of these 7 types of relationship,
but the same parameters are used everywhere in the
graphical model where the relationship holds.
Each latent variable in the ISBN parser is also
conditionally dependent on a set of explicit features
of the parsing history. As long as these explicit fea-
tures include all the new information from the last
parser decision, the performance of the model is not
very sensitive to this design choice. We used the
base feature model defined in (Nivre et al, 2006)
for all the languages but Arabic, Chinese, Czech,
and Turkish. For Arabic, Chinese, and Czech, we
used the same feature models used in the CoNLL-X
948
shared task by (Nivre et al, 2006), and for Turkish
we used again the base feature model but extended
it with a single feature: the part-of-speech tag of the
token preceding the current top of the stack.
3 Parsing
Exact inference in ISBN models is not tractable, but
effective approximations were proposed in (Titov
and Henderson, 2007a). Unlike (Titov and Hender-
son, 2007b), in the shared task we used only the
simplest feed-forward approximation, which repli-
cates the computation of a neural network of the type
proposed in (Henderson, 2003). We would expect
better performance with the more accurate approxi-
mation based on variational inference proposed and
evaluated in (Titov and Henderson, 2007a). We did
not try this because, on larger treebanks it would
have taken too long to tune the model with this bet-
ter approximation, and using different approxima-
tion methods for different languages would not be
compatible with the shared task rules.
To search for the most probable parse, we use the
heuristic search algorithm described in (Titov and
Henderson, 2007b), which is a form of beam search.
In section 4 we show that this search leads to quite
efficient parsing.
To overcome a minor shortcoming of the pars-
ing algorithm of (Nivre et al, 2004) we introduce a
simple language independent post-processing step.
Nivre?s parsing algorithm allows unattached nodes
to stay on the stack at the end of parsing, which is
reasonable for treebanks with unlabeled attachment
to root. However, this sometimes happens with lan-
guages where only labeled attachment to root is al-
lowed. In these cases (only 35 tokens in Greek, 17
in Czech, 1 in Arabic, on the final testing set) we
attached them using a simple rule: if there are no
tokens in the sentence attached to root, then the con-
sidered token is attached to root with the most fre-
quent root-attachment relation used for its part-of-
speech tag. If there are other root-attached tokens in
the sentence, it is attached to the next root-attached
token with the most frequent relation. Preference is
given to the most frequent attachment direction for
its part-of-speech tag. This rule guarantees that no
loops are introduced by the post-processing.
4 Experiments
We evaluated the ISBN parser on all the languages
considered in the shared task (Hajic? et al, 2004;
Aduriz et al, 2003; Mart?? et al, 2007; Chen et
al., 2003; Bo?hmova? et al, 2003; Marcus et al,
1993; Johansson and Nugues, 2007; Prokopidis et
al., 2005; Csendes et al, 2005; Montemagni et al,
2003; Oflazer et al, 2003). ISBN models were
trained using a small development set taken out from
the training set, which was used for tuning learn-
ing and decoding parameters, for early stopping and
very coarse feature engineering.2 The sizes of the
development sets were different: starting from less
than 2,000 tokens for smaller treebanks to 5,000 to-
kens for the largest one. The relatively small sizes
of the development sets limited our ability to per-
form careful feature selection, but this should not
have significantly affected the model performance,
as discussed in section 2.3 We used frequency cut-
offs: we ignored any property (word form, lemma,
feature) which occurs in the training set less than
a given threshold. We used a threshold of 20 for
Greek and Chinese and a threshold of 5 for the rest.
Because cardinalities of each of these sets (sets of
word forms, lemmas and features) effect the model
efficiency, we selected the larger threshold when val-
idation results with the smaller threshold were com-
parable. For the ISBN latent variables, we used vec-
tors of length 80, based on our previous experience.
Results on the final testing set are presented in ta-
ble 1. The model achieves relatively high scores on
each individual language, significantly better than
each average result in the shared task. This leads
to the third best overall average results in the shared
task, both in average labeled attachment score and
in average unlabeled attachment score. The absolute
error increase in labeled attachment score over the
best system is only 0.4%. We attribute ISBN?s suc-
cess mainly to its ability to automatically induce fea-
tures, as this significantly reduces the risk of omit-
ting any important highly predictive features. This
makes an ISBN parser a particularly good baseline
when considering a new treebank or language, be-
2We plan to make all the learning and decoding parameters
available on our web-page.
3Use of cross-validation with our model is relatively time-
consuming and, thus, not quite feasible for the shared task.
949
Ara Bas Cat Chi Cze Eng Gre Hun Ita Tur Ave
LAS 74.1 75.5 87.4 82.1 77.9 88.4 73.5 77.9 82.3 79.8 79.90
UAS 83.2 81.9 93.4 87.9 84.2 89.7 81.2 82.2 86.3 86.2 85.62
Table 1: Labeled attachment score (LAS) and unlabeled attachment score (UAS) on the final testing sets
 78.5
 79
 79.5
 80
 80.5
 81
 0  20  40  60  80  100  120  140
Av
er
ag
e 
LA
S
Parsing Time per Token, ms
Figure 1: Average labeled attachment score on
Basque, Chinese, English, and Turkish development
sets as a function of parsing time per token
cause it does not require much effort in feature en-
gineering. As was demonstrated in (Titov and Hen-
derson, 2007b), even a minimal set of local explicit
features achieves results which are non-significantly
different from a carefully chosen set of explicit fea-
tures, given the language independent definition of
locality described in section 2.
It is also important to note that the model is
quite efficient. Figure 1 shows the tradeoff be-
tween accuracy and parsing time as the width of the
search beam is varied, on the development set. This
curve plots the average labeled attachment score
over Basque, Chinese, English, and Turkish as a
function of parsing time per token.4 Accuracy of
only 1% below the maximum can be achieved with
average processing time of 17 ms per token, or 60
tokens per second.5
We also refer the reader to (Titov and Henderson,
2007b) for more detailed analysis of the ISBN de-
pendency parser results, where, among other things,
it was shown that the ISBN model is especially ac-
curate at modeling long dependencies.
4A piecewise-linear approximation for each individual lan-
guage was used to compute the average. Experiments were run
on a standard 2.4 GHz desktop PC.
5For Basque, Chinese, and Turkish this time is below 7 ms,
but for English it is 38 ms. English, along with Catalan, required
the largest beam across all 10 languages. Note that accuracy in
the lowest part of the curve can probably be improved by vary-
ing latent vector size and frequency cut-offs. Also, efficiency
was not the main goal during the implementation of the parser,
and it is likely that a much faster implementation is possible.
5 Conclusion
We evaluated the ISBN dependency parser in the
multilingual shared task setup and achieved com-
petitive accuracy on every language, and the third
best average score overall. The proposed model re-
quires minimal design effort because it relies mostly
on automatic feature induction, which is highly de-
sirable when using new treebanks or languages. The
parsing time needed to achieve high accuracy is also
quite small, making this model a good candidate for
use in practical applications.
The fact that our model defines a probability
model over parse trees, unlike the previous state-
of-the-art methods (Nivre et al, 2006; McDonald et
al., 2006), makes it easier to use this model in ap-
plications which require probability estimates, such
as in language processing pipelines or for language
modeling. Also, as with any generative model,
it should be easy to improve the parser?s accu-
racy with discriminative reranking, such as discrim-
inative retraining techniques (Henderson, 2004) or
data-defined kernels (Henderson and Titov, 2005),
with or even without the introduction of any addi-
tional linguistic features.
Acknowledgments
This work was funded by Swiss NSF grant 200020-
109685, UK EPSRC grant EP/E019501/1, and EU
FP6 grant 507802 for project TALK.
References
A. Abeille?, editor. 2003. Treebanks: Building and Using
Parsed Corpora. Kluwer.
I. Aduriz, M. J. Aranzabe, J. M. Arriola, A. Atutxa,
A. Diaz de Ilarraza, A. Garmendia, and M. Oronoz.
2003. Construction of a Basque dependency treebank.
In Proc. of the 2nd Workshop on Treebanks and Lin-
guistic Theories (TLT), pages 201?204.
A. Bo?hmova?, J. Hajic?, E. Hajic?ova?, and B. Hladka?. 2003.
The PDT: a 3-level annotation scenario. In Abeille?
(Abeille?, 2003), chapter 7, pages 103?127.
950
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proc. of the Tenth Conference on Computational Nat-
ural Language Learning, New York, USA.
K. Chen, C. Luo, M. Chang, F. Chen, C. Chen, C. Huang,
and Z. Gao. 2003. Sinica treebank: Design criteria,
representational issues and implementation. In Abeille?
(Abeille?, 2003), chapter 13, pages 231?248.
D. Csendes, J. Csirik, T. Gyimo?thy, and A. Kocsor. 2005.
The Szeged Treebank. Springer.
J. Hajic?, O. Smrz?, P. Zema?nek, J. ?Snaidauf, and E. Bes?ka.
2004. Prague Arabic dependency treebank: Develop-
ment in data and tools. In Proc. of the NEMLAR In-
tern. Conf. on Arabic Language Resources and Tools,
pages 110?117.
James Henderson and Ivan Titov. 2005. Data-defined
kernels for parse reranking derived from probabilis-
tic models. In Proc. 43rd Meeting of Association for
Computational Linguistics, Ann Arbor, MI.
James Henderson. 2003. Inducing history representa-
tions for broad coverage statistical parsing. In Proc.
joint meeting of North American Chapter of the Asso-
ciation for Computational Linguistics and the Human
Language Technology Conf., pages 103?110, Edmon-
ton, Canada.
James Henderson. 2004. Discriminative training of
a neural network statistical parser. In Proc. 42nd
Meeting of Association for Computational Linguistics,
Barcelona, Spain.
R. Johansson and P. Nugues. 2007. Extended
constituent-to-dependency conversion for English. In
Proc. of the 16th Nordic Conference on Computational
Linguistics (NODALIDA).
M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993.
Building a large annotated corpus of English: the Penn
Treebank. Computational Linguistics, 19(2):313?330.
M. A. Mart??, M. Taule?, L. Ma`rquez, and M. Bertran.
2007. CESS-ECE: A multilingual and multilevel
annotated corpus. Available for download from:
http://www.lsi.upc.edu/?mbertran/cess-ece/.
Ryan McDonald, Kevin Lerman, and Fernando Pereira.
2006. Multilingual dependency analysis with a two-
stage discriminative parser. In Proc. of the Tenth Con-
ference on Computational Natural Language Learn-
ing, New York, USA.
S. Montemagni, F. Barsotti, M. Battista, N. Calzolari,
O. Corazzari, A. Lenci, A. Zampolli, F. Fanciulli,
M. Massetani, R. Raffaelli, R. Basili, M. T. Pazienza,
D. Saracino, F. Zanzotto, N. Nana, F. Pianesi, and
R. Delmonte. 2003. Building the Italian Syntactic-
Semantic Treebank. In Abeille? (Abeille?, 2003), chap-
ter 11, pages 189?210.
Joakim Nivre and Jens Nilsson. 2005. Pseudo-projective
dependency parsing. In Proc. 43rd Meeting of Asso-
ciation for Computational Linguistics, pages 99?106,
Ann Arbor, MI.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2004.
Memory-based dependency parsing. In Proc. of the
Eighth Conference on Computational Natural Lan-
guage Learning, pages 49?56, Boston, USA.
Joakim Nivre, Johan Hall, Jens Nilsson, Gulsen Eryigit,
and Svetoslav Marinov. 2006. Pseudo-projective de-
pendency parsing with support vector machines. In
Proc. of the Tenth Conference on Computational Nat-
ural Language Learning, pages 221?225, New York,
USA.
J. Nivre, J. Hall, S. Ku?bler, R. McDonald, J. Nils-
son, S. Riedel, and D. Yuret. 2007. The CoNLL
2007 shared task on dependency parsing. In Proc.
of the CoNLL 2007 Shared Task. Joint Conf. on Em-
pirical Methods in Natural Language Processing and
Computational Natural Language Learning (EMNLP-
CoNLL).
K. Oflazer, B. Say, D. Zeynep Hakkani-T u?r, and G. Tu?r.
2003. Building a Turkish treebank. In Abeille?
(Abeille?, 2003), chapter 15, pages 261?277.
P. Prokopidis, E. Desypri, M. Koutsombogera, H. Papa-
georgiou, and S. Piperidis. 2005. Theoretical and
practical issues in the construction of a Greek depen-
dency treebank. In Proc. of the 4th Workshop on Tree-
banks and Linguistic Theories (TLT), pages 149?160.
Ivan Titov and James Henderson. 2007a. Constituent
parsing with incremental sigmoid belief networks. In
Proc. 45th Meeting of Association for Computational
Linguistics (ACL), Prague, Czech Republic.
Ivan Titov and James Henderson. 2007b. A latent vari-
able model for generative dependency parsing. In
Proc. 10th Int. Conference on Parsing Technologies
(IWPT), Prague, Czech Republic.
951
Proceedings of the 43rd Annual Meeting of the ACL, pages 181?188,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Data-Defined Kernels for Parse Reranking
Derived from Probabilistic Models
James Henderson
School of Informatics
University of Edinburgh
2 Buccleuch Place
Edinburgh EH8 9LW, United Kingdom
james.henderson@ed.ac.uk
Ivan Titov
Department of Computer Science
University of Geneva
24, rue Ge?ne?ral Dufour
CH-1211 Gene`ve 4, Switzerland
ivan.titov@cui.unige.ch
Abstract
Previous research applying kernel meth-
ods to natural language parsing have fo-
cussed on proposing kernels over parse
trees, which are hand-crafted based on do-
main knowledge and computational con-
siderations. In this paper we propose a
method for defining kernels in terms of
a probabilistic model of parsing. This
model is then trained, so that the param-
eters of the probabilistic model reflect the
generalizations in the training data. The
method we propose then uses these trained
parameters to define a kernel for rerank-
ing parse trees. In experiments, we use
a neural network based statistical parser
as the probabilistic model, and use the
resulting kernel with the Voted Percep-
tron algorithm to rerank the top 20 parses
from the probabilistic model. This method
achieves a significant improvement over
the accuracy of the probabilistic model.
1 Introduction
Kernel methods have been shown to be very ef-
fective in many machine learning problems. They
have the advantage that learning can try to optimize
measures related directly to expected testing perfor-
mance (i.e. ?large margin? methods), rather than
the probabilistic measures used in statistical models,
which are only indirectly related to expected test-
ing performance. Work on kernel methods in natural
language has focussed on the definition of appropri-
ate kernels for natural language tasks. In particu-
lar, most of the work on parsing with kernel meth-
ods has focussed on kernels over parse trees (Collins
and Duffy, 2002; Shen and Joshi, 2003; Shen et
al., 2003; Collins and Roark, 2004). These kernels
have all been hand-crafted to try reflect properties
of parse trees which are relevant to discriminating
correct parse trees from incorrect ones, while at the
same time maintaining the tractability of learning.
Some work in machine learning has taken an al-
ternative approach to defining kernels, where the
kernel is derived from a probabilistic model of the
task (Jaakkola and Haussler, 1998; Tsuda et al,
2002). This way of defining kernels has two ad-
vantages. First, linguistic knowledge about parsing
is reflected in the design of the probabilistic model,
not directly in the kernel. Designing probabilistic
models to reflect linguistic knowledge is a process
which is currently well understood, both in terms of
reflecting generalizations and controlling computa-
tional cost. Because many NLP problems are un-
bounded in size and complexity, it is hard to specify
all possible relevant kernel features without having
so many features that the computations become in-
tractable and/or the data becomes too sparse.1 Sec-
ond, the kernel is defined using the trained param-
eters of the probabilistic model. Thus the kernel is
in part determined by the training data, and is auto-
matically tailored to reflect properties of parse trees
which are relevant to parsing.
1For example, see (Henderson, 2004) for a discussion of
why generative models are better than models parameterized to
estimate the a posteriori probability directly.
181
In this paper, we propose a new method for de-
riving a kernel from a probabilistic model which is
specifically tailored to reranking tasks, and we ap-
ply this method to natural language parsing. For the
probabilistic model, we use a state-of-the-art neural
network based statistical parser (Henderson, 2003).
The resulting kernel is then used with the Voted Per-
ceptron algorithm (Freund and Schapire, 1998) to
reranking the top 20 parses from the probabilistic
model. This method achieves a significant improve-
ment over the accuracy of the probabilistic model
alone.
2 Kernels Derived from Probabilistic
Models
In recent years, several methods have been proposed
for constructing kernels from trained probabilistic
models. As usual, these kernels are then used with
linear classifiers to learn the desired task. As well as
some empirical successes, these methods are moti-
vated by theoretical results which suggest we should
expect some improvement with these classifiers over
the classifier which chooses the most probable an-
swer according to the probabilistic model (i.e. the
maximum a posteriori (MAP) classifier). There is
guaranteed to be a linear classifier for the derived
kernel which performs at least as well as the MAP
classifier for the probabilistic model. So, assuming
a large-margin classifier can optimize a more ap-
propriate criteria than the posterior probability, we
should expect the derived kernel?s classifier to per-
form better than the probabilistic model?s classifier,
although empirical results on a given task are never
guaranteed.
In this section, we first present two previous ker-
nels and then propose a new kernel specifically for
reranking tasks. In each of these discussions we
need to characterize the parsing problem as a classi-
fication task. Parsing can be regarded as a mapping
from an input space of sentences x?X to a struc-
tured output space of parse trees y?Y . On the basis
of training sentences, we learn a discriminant func-
tion F : X ? Y ? R. The parse tree y with the
largest value for this discriminant function F (x, y)
is the output parse tree for the sentence x. We focus
on the linear discriminant functions:
Fw(x, y) = <w,?(x, y)>,
where ?(x, y) is a feature vector for the sentence-
tree pair, w is a parameter vector for the discrim-
inant function, and <a, b> is the inner product of
vectors a and b. In the remainder of this section, we
will characterize the kernel methods we consider in
terms of the feature extractor ?(x, y).
2.1 Fisher Kernels
The Fisher kernel (Jaakkola and Haussler, 1998) is
one of the best known kernels belonging to the class
of probability model based kernels. Given a genera-
tive model of P (z|??) with smooth parameterization,
the Fisher score of an example z is a vector of partial
derivatives of the log-likelihood of the example with
respect to the model parameters:
???(z) = (
?logP (z|??)
??1
, . . . , ?logP (z|??)??l ).
This score can be regarded as specifying how the
model should be changed in order to maximize the
likelihood of the example z. Then we can define the
similarity between data points as the inner product
of the corresponding Fisher scores. This kernel is
often referred to as the practical Fisher kernel. The
theoretical Fisher kernel depends on the Fisher in-
formation matrix, which is not feasible to compute
for most practical tasks and is usually omitted.
The Fisher kernel is only directly applicable to
binary classification tasks. We can apply it to our
task by considering an example z to be a sentence-
tree pair (x, y), and classifying the pairs into cor-
rect parses versus incorrect parses. When we use the
Fisher score ???(x, y) in the discriminant function F ,
we can interpret the value as the confidence that the
tree y is correct, and choose the y in which we are
the most confident.
2.2 TOP Kernels
Tsuda (2002) proposed another kernel constructed
from a probabilistic model, called the Tangent vec-
tors Of Posterior log-odds (TOP) kernel. Their TOP
kernel is also only for binary classification tasks, so,
as above, we treat the input z as a sentence-tree pair
and the output category c ? {?1,+1} as incor-
rect/correct. It is assumed that the true probability
distribution is included in the class of probabilis-
tic models and that the true parameter vector ?? is
unique. The feature extractor of the TOP kernel for
182
the input z is defined by:
???(z) = (v(z, ??),
?v(z,??)
??1
, . . . , ?v(z,??)??l ),
where v(z, ??) = logP (c=+1|z, ??) ?
logP (c=?1|z, ??).
In addition to being at least as good as the
MAP classifier, the choice of the TOP kernel fea-
ture extractor is motivated by the minimization of
the binary classification error of a linear classifier
<w,???(z)> + b. Tsuda (2002) demonstrates that
this error is closely related to the estimation error of
the posterior probability P (c=+1|z, ??) by the esti-
mator g(<w,???(z)> + b), where g is the sigmoid
function g(t) = 1/(1 + exp (?t)).
The TOP kernel isn?t quite appropriate for struc-
tured classification tasks because ???(z) is motivated
by binary classificaton error minimization. In the
next subsection, we will adapt it to structured classi-
fication.
2.3 A TOP Kernel for Reranking
We define the reranking task as selecting a parse tree
from the list of candidate trees suggested by a proba-
bilistic model. Furthermore, we only consider learn-
ing to rerank the output of a particular probabilistic
model, without requiring the classifier to have good
performance when applied to a candidate list pro-
vided by a different model. In this case, it is natural
to model the probability that a parse tree is the best
candidate given the list of candidate trees:
P (yk|x, y1, . . . , ys) =
P (x,yk)?
t
P (x,yt)
,
where y1, . . . , ys is the list of candidate parse trees.
To construct a new TOP kernel for reranking, we
apply an approach similar to that used for the TOP
kernel (Tsuda et al, 2002), but we consider the prob-
ability P (yk|x, y1, . . . , ys, ??) instead of the proba-
bility P (c=+1|z, ??) considered by Tsuda. The re-
sulting feature extractor is given by:
???(x, yk) = (v(x, yk, ??),
?v(x,yk,??)
??1
, . . . , ?v(x,yk,??)??l ),
where v(x, yk, ??) = logP (yk|y1, . . . , ys, ??) ?
log
?
t6=k P (yt|y1, . . . , ys, ??). We will call this ker-
nel the TOP reranking kernel.
3 The Probabilistic Model
To complete the definition of the kernel, we need
to choose a probabilistic model of parsing. For
this we use a statistical parser which has previously
been shown to achieve state-of-the-art performance,
namely that proposed in (Henderson, 2003). This
parser has two levels of parameterization. The first
level of parameterization is in terms of a history-
based generative probability model, but this level is
not appropriate for our purposes because it defines
an infinite number of parameters (one for every pos-
sible partial parse history). When parsing a given
sentence, the bounded set of parameters which are
relevant to a given parse are estimated using a neural
network. The weights of this neural network form
the second level of parameterization. There is a fi-
nite number of these parameters. Neural network
training is applied to determine the values of these
parameters, which in turn determine the values of
the probability model?s parameters, which in turn
determine the probabilistic model of parse trees.
We do not use the complete set of neural network
weights to define our kernels, but instead we define a
third level of parameterization which only includes
the network?s output layer weights. These weights
define a normalized exponential model, with the net-
work?s hidden layer as the input features. When we
tried using the complete set of weights in some small
scale experiments, training the classifier was more
computationally expensive, and actually performed
slightly worse than just using the output weights.
Using just the output weights also allows us to make
some approximations in the TOP reranking kernel
which makes the classifier learning algorithm more
efficient.
3.1 A History-Based Probability Model
As with many other statistical parsers (Ratnaparkhi,
1999; Collins, 1999; Charniak, 2000), Henderson
(2003) uses a history-based model of parsing. He
defines the mapping from phrase structure trees to
parse sequences using a form of left-corner parsing
strategy (see (Henderson, 2003) for more details).
The parser actions include: introducing a new con-
stituent with a specified label, attaching one con-
stituent to another, and predicting the next word of
the sentence. A complete parse consists of a se-
quence of these actions, d1,..., dm, such that per-
forming d1,..., dm results in a complete phrase struc-
ture tree.
Because this mapping to parse sequences is
183
one-to-one, and the word prediction actions in
a complete parse d1,..., dm specify the sentence,
P (d1,..., dm) is equivalent to the joint probability of
the output phrase structure tree and the input sen-
tence. This probability can be then be decomposed
into the multiplication of the probabilities of each
action decision di conditioned on that decision?s
prior parse history d1,..., di?1.
P (d1,..., dm) = ?iP (di|d1,..., di?1)
3.2 Estimating Decision Probabilities with a
Neural Network
The parameters of the above probability model are
the P (di|d1,..., di?1). There are an infinite num-
ber of these parameters, since the parse history
d1,..., di?1 grows with the length of the sentence. In
other work on history-based parsing, independence
assumptions are applied so that only a finite amount
of information from the parse history can be treated
as relevant to each parameter, thereby reducing the
number of parameters to a finite set which can be
estimated directly. Instead, Henderson (2003) uses
a neural network to induce a finite representation
of this unbounded history, which we will denote
h(d1,..., di?1). Neural network training tries to find
such a history representation which preserves all the
information about the history which is relevant to es-
timating the desired probability.
P (di|d1,..., di?1) ? P (di|h(d1,..., di?1))
Using a neural network architecture called Simple
Synchrony Networks (SSNs), the history representa-
tion h(d1,..., di?1) is incrementally computed from
features of the previous decision di?1 plus a finite
set of previous history representations h(d1,..., dj),
j < i ? 1. Each history representation is a finite
vector of real numbers, called the network?s hidden
layer. As long as the history representation for po-
sition i ? 1 is always included in the inputs to the
history representation for position i, any information
about the entire sequence could be passed from his-
tory representation to history representation and be
used to estimate the desired probability. However,
learning is biased towards paying more attention to
information which passes through fewer history rep-
resentations.
To exploit this learning bias, structural locality is
used to determine which history representations are
input to which others. First, each history representa-
tion is assigned to the constituent which is on the top
of the parser?s stack when it is computed. Then ear-
lier history representations whose constituents are
structurally local to the current representation?s con-
stituent are input to the computation of the correct
representation. In this way, the number of represen-
tations which information needs to pass through in
order to flow from history representation i to his-
tory representation j is determined by the structural
distance between i?s constituent and j?s constituent,
and not just the distance between i and j in the
parse sequence. This provides the neural network
with a linguistically appropriate inductive bias when
it learns the history representations, as explained in
more detail in (Henderson, 2003).
Once it has computed h(d1,..., di?1), the SSN
uses a normalized exponential to estimate a proba-
bility distribution over the set of possible next deci-
sions di given the history:
P (di|d1,..., di?1, ?) ?
exp(<?di ,h(d1,...,di?1)>)?
t?N(di?1)
exp(<?t,h(d1,...,di?1)>)
,
where by ?t we denote the set of output layer
weights, corresponding to the parser action t,
N(di?1) defines a set of possible next parser actions
after the step di?1 and ? denotes the full set of model
parameters.
We trained SSN parsing models, using the on-line
version of Backpropagation to perform the gradient
descent with a maximum likelihood objective func-
tion. This learning simultaneously tries to optimize
the parameters of the output computation and the pa-
rameters of the mappings h(d1,..., di?1). With multi-
layered networks such as SSNs, this training is not
guaranteed to converge to a global optimum, but in
practice a network whose criteria value is close to
the optimum can be found.
4 Large-Margin Optimization
Once we have defined a kernel over parse trees, gen-
eral techniques for linear classifier optimization can
be used to learn the given task. The most sophis-
ticated of these techniques (such as Support Vec-
tor Machines) are unfortunately too computationally
expensive to be used on large datasets like the Penn
Treebank (Marcus et al, 1993). Instead we use a
184
method which has often been shown to be virtu-
ally as good, the Voted Perceptron (VP) (Freund and
Schapire, 1998) algorithm. The VP algorithm was
originally applied to parse reranking in (Collins and
Duffy, 2002) with the Tree kernel. We modify the
perceptron training algorithm to make it more suit-
able for parsing, where zero-one classification loss
is not the evaluation measure usually employed. We
also develop a variant of the kernel defined in sec-
tion 2.3, which is more efficient when used with the
VP algorithm.
Given a list of candidate trees, we train the clas-
sifier to select the tree with largest constituent F1
score. The F1 score is a measure of the similarity
between the tree in question and the gold standard
parse, and is the standard way to evaluate the accu-
racy of a parser. We denote the k?th candidate tree
for the j?th sentence xj by yjk. Without loss of gener-
ality, let us assume that yj1 is the candidate tree with
the largest F1 score.
The Voted Perceptron algorithm is an ensem-
ble method for combining the various intermediate
models which are produced during training a per-
ceptron. It demonstrates more stable generalization
performance than the normal perceptron algorithm
when the problem is not linearly separable (Freund
and Schapire, 1998), as is usually the case.
We modify the perceptron algorithm by introduc-
ing a new classification loss function. This modifi-
cation enables us to treat differently the cases where
the perceptron predicts a tree with an F1 score much
smaller than that of the top candidate and the cases
where the predicted and the top candidates have sim-
ilar score values. The natural choice for the loss
function would be ?(yjk, y
j
1) = F1(y
j
1) ? F1(y
j
k),
where F1(yjk) denotes the F1 score value for the
parse tree yjk. This approach is very similar to slack
variable rescaling for Support Vector Machines pro-
posed in (Tsochantaridis et al, 2004). The learning
algorithm we employed is presented in figure 1.
When applying kernels with a large training cor-
pus, we face efficiency issues because of the large
number of the neural network weights. Even though
we use only the output layer weights, this vector
grows with the size of the vocabulary, and thus can
be large. The kernels presented in section 2 all lead
to feature vectors without many zero values. This
w = 0
for j = 1 .. n
for k = 2 .. s
if <w,?(xj , yjk)> > <w, ?(xj , y
j
1)>
w = w + ?(yjk, y
j
1)(?(x
j , yj1)? ?(x
j , yjk))
Figure 1: The modified perceptron algorithm
happens because we compute the derivative of the
normalization factor used in the network?s estima-
tion of P (di|d1,..., di?1). This normalization factor
depends on the output layer weights corresponding
to all the possible next decisions (see section 3.2).
This makes an application of the VP algorithm in-
feasible in the case of a large vocabulary.
We can address this problem by freezing the
normalization factor when computing the feature
vector. Note that we can rewrite the model log-
probability of the tree as:
logP (y|?) =
?
i log (
exp(<?di ,h(d1,...,di?1)>)?
t?N(di?1)
exp(<?t,h(d1,...,di?1)>)
) =
?
i(<?di , h(d1,..., di?1)>)??
i log
?
t?N(di?1) exp(<?t, h(d1,..., di?1)>).
We treat the parameters used to compute the first
term as different from the parameters used to com-
pute the second term, and we define our kernel only
using the parameters in the first term. This means
that the second term does not effect the derivatives
in the formula for the feature vector ?(x, y). Thus
the feature vector for the kernel will contain non-
zero entries only in the components corresponding
to the parser actions which are present in the candi-
date derivation for the sentence, and thus in the first
vector component. We have applied this technique
to the TOP reranking kernel, the result of which we
will call the efficient TOP reranking kernel.
5 The Experimental Results
We used the Penn Treebank WSJ corpus (Marcus et
al., 1993) to perform empirical experiments on the
proposed parsing models. In each case the input to
the network is a sequence of tag-word pairs.2 We re-
port results for two different vocabulary sizes, vary-
ing in the frequency with which tag-word pairs must
2We used a publicly available tagger (Ratnaparkhi, 1996) to
provide the tags.
185
occur in the training set in order to be included ex-
plicitly in the vocabulary. A frequency threshold of
200 resulted in a vocabulary of 508 tag-word pairs
(including tag-unknown word pairs) and a threshold
of 20 resulted in 4215 tag-word pairs. We denote
the probabilistic model trained with the vocabulary
of 508 by the SSN-Freq?200, the model trained with
the vocabulary of 4215 by the SSN-Freq?20.
Testing the probabilistic parser requires using a
beam search through the space of possible parses.
We used a form of beam search which prunes the
search after the prediction of each word. We set the
width of this post-word beam to 40 for both testing
of the probabilistic model and generating the candi-
date list for reranking. For training and testing of
the kernel models, we provided a candidate list con-
sisting of the top 20 parses found by the generative
probabilistic model. When using the Fisher kernel,
we added the log-probability of the tree given by the
probabilistic model as the feature. This was not nec-
essary for the TOP kernels because they already con-
tain a feature corresponding to the probability esti-
mated by the probabilistic model (see section 2.3).
We trained the VP model with all three kernels
using the 508 word vocabulary (Fisher-Freq?200,
TOP-Freq?200, TOP-Eff-Freq?200) but only the ef-
ficient TOP reranking kernel model was trained with
the vocabulary of 4215 words (TOP-Eff-Freq?20).
The non-sparsity of the feature vectors for other ker-
nels led to the excessive memory requirements and
larger testing time. In each case, the VP model was
run for only one epoch. We would expect some im-
provement if running it for more epochs, as has been
empirically demonstrated in other domains (Freund
and Schapire, 1998).
To avoid repeated testing on the standard testing
set, we first compare the different models with their
performance on the validation set. Note that the val-
idation set wasn?t used during learning of the kernel
models or for adjustment of any parameters.
Standard measures of accuracy are shown in ta-
ble 1.3 Both the Fisher kernel and the TOP kernels
show better accuracy than the baseline probabilistic
3All our results are computed with the evalb program fol-
lowing the standard criteria in (Collins, 1999), and using the
standard training (sections 2?22, 39,832 sentences, 910,196
words), validation (section 24, 1346 sentence, 31507 words),
and testing (section 23, 2416 sentences, 54268 words) sets
(Collins, 1999).
LR LP F?=1
SSN-Freq?200 87.2 88.5 87.8
Fisher-Freq?200 87.2 88.8 87.9
TOP-Freq?200 87.3 88.9 88.1
TOP-Eff-Freq?200 87.3 88.9 88.1
SSN-Freq?20 88.1 89.2 88.6
TOP-Eff-Freq?20 88.2 89.7 88.9
Table 1: Percentage labeled constituent recall (LR),
precision (LP), and a combination of both (F?=1) on
validation set sentences of length at most 100.
model, but only the improvement of the TOP kernels
is statistically significant.4 For the TOP kernel, the
improvement over baseline is about the same with
both vocabulary sizes. Also note that the perfor-
mance of the efficient TOP reranking kernel is the
same as that of the original TOP reranking kernel,
for the smaller vocabulary.
For comparison to previous results, table 2 lists
the results on the testing set for our best model
(TOP-Efficient-Freq?20) and several other statisti-
cal parsers (Collins, 1999; Collins and Duffy, 2002;
Collins and Roark, 2004; Henderson, 2003; Char-
niak, 2000; Collins, 2000; Shen and Joshi, 2004;
Shen et al, 2003; Henderson, 2004; Bod, 2003).
First note that the parser based on the TOP efficient
kernel has better accuracy than (Henderson, 2003),
which used the same parsing method as our base-
line model, although the trained network parameters
were not the same. When compared to other kernel
methods, our approach performs better than those
based on the Tree kernel (Collins and Duffy, 2002;
Collins and Roark, 2004), and is only 0.2% worse
than the best results achieved by a kernel method for
parsing (Shen et al, 2003; Shen and Joshi, 2004).
6 Related Work
The first application of kernel methods to parsing
was proposed by Collins and Duffy (2002). They
used the Tree kernel, where the features of a tree are
all its connected tree fragments. The VP algorithm
was applied to rerank the output of a probabilistic
model and demonstrated an improvement over the
baseline.
4We measured significance with the randomized signifi-
cance test of (Yeh, 2000).
186
LR LP F?=1?
Collins99 88.1 88.3 88.2
Collins&Duffy02 88.6 88.9 88.7
Collins&Roark04 88.4 89.1 88.8
Henderson03 88.8 89.5 89.1
Charniak00 89.6 89.5 89.5
TOP-Eff-Freq?20 89.1 90.1 89.6
Collins00 89.6 89.9 89.7
Shen&Joshi04 89.5 90.0 89.8
Shen et al03 89.7 90.0 89.8
Henderson04 89.8 90.4 90.1
Bod03 90.7 90.8 90.7
* F?=1 for previous models may have rounding errors.
Table 2: Percentage labeled constituent recall (LR),
precision (LP), and a combination of both (F?=1) on
the entire testing set.
Shen and Joshi (2003) applied an SVM based
voting algorithm with the Preference kernel defined
over pairs for reranking. To define the Preference
kernel they used the Tree kernel and the Linear ker-
nel as its underlying kernels and achieved state-of-
the-art results with the Linear kernel.
In (Shen et al, 2003) it was pointed out that
most of the arbitrary tree fragments allowed by the
Tree kernel are linguistically meaningless. The au-
thors suggested the use of Lexical Tree Adjoining
Grammar (LTAG) based features as a more linguis-
tically appropriate set of features. They empiri-
cally demonstrated that incorporation of these fea-
tures helps to improve reranking performance.
Shen and Joshi (2004) proposed to improve mar-
gin based methods for reranking by defining the
margin not only between the top tree and all the
other trees in the candidate list but between all the
pairs of parses in the ordered candidate list for the
given sentence. They achieved the best results when
training with an uneven margin scaled by the heuris-
tic function of the candidates positions in the list.
One potential drawback of this method is that it
doesn?t take into account the actual F1 score of the
candidate and considers only the position in the list
ordered by the F1 score. We expect that an im-
provement could be achieved by combining our ap-
proach of scaling updates by the F1 loss with the
all pairs approach of (Shen and Joshi, 2004). Use
of the F1 loss function during training demonstrated
better performance comparing to the 0-1 loss func-
tion when applied to a structured classification task
(Tsochantaridis et al, 2004).
All the described kernel methods are limited to
the reranking of candidates from an existing parser
due to the complexity of finding the best parse given
a kernel (i.e. the decoding problem). (Taskar et
al., 2004) suggested a method for maximal mar-
gin parsing which employs the dynamic program-
ming approach to decoding and parameter estima-
tion problems. The efficiency of dynamic program-
ming means that the entire space of parses can be
considered, not just a candidate list. However, not
all kernels are suitable for this method. The dy-
namic programming approach requires the feature
vector of a tree to be decomposable into a sum over
parts of the tree. In particular, this is impossible with
the TOP and Fisher kernels derived from the SSN
model. Also, it isn?t clear whether the algorithm
remains tractable for a large training set with long
sentences, since the authors only present results for
sentences of length less than or equal to 15.
7 Conclusions
This paper proposes a method for deriving a ker-
nel for reranking from a probabilistic model, and
demonstrates state-of-the-art accuracy when this
method is applied to parse reranking. Contrary to
most of the previous research on kernel methods in
parsing, linguistic knowledge does not have to be ex-
pressed through a list of features, but instead can be
expressed through the design of a probability model.
The parameters of this probability model are then
trained, so that they reflect what features of trees are
relevant to parsing. The kernel is then derived from
this trained model in such a way as to maximize its
usefulness for reranking.
We performed experiments on parse reranking us-
ing a neural network based statistical parser as both
the probabilistic model and the source of the list
of candidate parses. We used a modification of
the Voted Perceptron algorithm to perform reranking
with the kernel. The results were amongst the best
current statistical parsers, and only 0.2% worse than
the best current parsing methods which use kernels.
We would expect further improvement if we used
different models to derive the kernel and to gener-
187
ate the candidates, thereby exploiting the advantages
of combining multiple models, as do the better per-
forming methods using kernels.
In recent years, probabilistic models have become
commonplace in natural language processing. We
believe that this approach to defining kernels would
simplify the problem of defining kernels for these
tasks, and could be very useful for many of them.
In particular, maximum entropy models also use a
normalized exponential function to estimate proba-
bilities, so all the methods discussed in this paper
would be applicable to maximum entropy models.
This approach would be particularly useful for tasks
where there is less data available than in parsing, for
which large-margin methods work particularly well.
References
Rens Bod. 2003. An efficient implementation of a new
DOP model. In Proc. 10th Conf. of European Chap-
ter of the Association for Computational Linguistics,
Budapest, Hungary.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proc. 1st Meeting of North American
Chapter of Association for Computational Linguistics,
pages 132?139, Seattle, Washington.
Michael Collins and Nigel Duffy. 2002. New ranking
algorithms for parsing and tagging: Kernels over dis-
crete structures and the voted perceptron. In Proc.
40th Meeting of Association for Computational Lin-
guistics, pages 263?270.
Michael Collins and Brian Roark. 2004. Incremental
parsing with the perceptron algorithm. In Proc. 42th
Meeting of Association for Computational Linguistics,
Barcelona, Spain.
Michael Collins. 1999. Head-Driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, Univer-
sity of Pennsylvania, Philadelphia, PA.
Michael Collins. 2000. Discriminative reranking for nat-
ural language parsing. In Proc. 17th Int. Conf. on Ma-
chine Learning, pages 175?182, Stanford, CA.
Yoav Freund and Robert E. Schapire. 1998. Large
margin classification using the perceptron algorithm.
In Proc. of the 11th Annual Conf. on Computational
Learning Theory, pages 209?217, Madisson WI.
James Henderson. 2003. Inducing history representa-
tions for broad coverage statistical parsing. In Proc.
joint meeting of North American Chapter of the Asso-
ciation for Computational Linguistics and the Human
Language Technology Conf., pages 103?110, Edmon-
ton, Canada.
James Henderson. 2004. Discriminative training of
a neural network statistical parser. In Proc. 42nd
Meeting of Association for Computational Linguistics,
Barcelona, Spain.
Tommi S. Jaakkola and David Haussler. 1998. Ex-
ploiting generative models in discriminative classi-
fiers. Advances in Neural Information Processes Sys-
tems 11.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
Linguistics, 19(2):313?330.
Adwait Ratnaparkhi. 1996. A maximum entropy model
for part-of-speech tagging. In Proc. Conf. on Empir-
ical Methods in Natural Language Processing, pages
133?142, Univ. of Pennsylvania, PA.
Adwait Ratnaparkhi. 1999. Learning to parse natural
language with maximum entropy models. Machine
Learning, 34:151?175.
Libin Shen and Aravind K. Joshi. 2003. An SVM based
voting algorithm with application to parse reranking.
In Proc. of the 7th Conf. on Computational Natural
Language Learning, pages 9?16, Edmonton, Canada.
Libin Shen and Aravind K. Joshi. 2004. Flexible margin
selection for reranking with full pairwise samples. In
Proc. of the 1st Int. Joint Conf. on Natural Language
Processing, Hainan Island, China.
Libin Shen, Anoop Sarkar, and Aravind K. Joshi. 2003.
Using LTAG based features in parse reranking. In
Proc. of Conf. on Empirical Methods in Natural Lan-
guage Processing, Sapporo, Japan.
Ben Taskar, Dan Klein, Michael Collins, Daphne Koller,
and Christopher Manning. 2004. Max-margin pars-
ing. In Proc. Conf. on Empirical Methods in Natural
Language Processing, Barcelona, Spain.
Ioannis Tsochantaridis, Thomas Hofmann, Thorsten
Joachims, and Yasemin Altun. 2004. Support vec-
tor machine learning for interdependent and structured
output spaces. In Proc. 21st Int. Conf. on Machine
Learning, pages 823?830, Banff, Alberta, Canada.
K. Tsuda, M. Kawanabe, G. Ratsch, S. Sonnenburg,
and K. Muller. 2002. A new discriminative ker-
nel from probabilistic models. Neural Computation,
14(10):2397?2414.
Alexander Yeh. 2000. More accurate tests for the sta-
tistical significance of the result differences. In Proc.
17th International Conf. on Computational Linguis-
tics, pages 947?953, Saarbruken, Germany.
188
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 632?639,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Constituent Parsing with Incremental Sigmoid Belief Networks
Ivan Titov
Department of Computer Science
University of Geneva
24, rue Ge?ne?ral Dufour
CH-1211 Gene`ve 4, Switzerland
ivan.titov@cui.unige.ch
James Henderson
School of Informatics
University of Edinburgh
2 Buccleuch Place
Edinburgh EH8 9LW, United Kingdom
james.henderson@ed.ac.uk
Abstract
We introduce a framework for syntactic
parsing with latent variables based on a form
of dynamic Sigmoid Belief Networks called
Incremental Sigmoid Belief Networks. We
demonstrate that a previous feed-forward
neural network parsing model can be viewed
as a coarse approximation to inference with
this class of graphical model. By construct-
ing a more accurate but still tractable ap-
proximation, we significantly improve pars-
ing accuracy, suggesting that ISBNs provide
a good idealization for parsing. This gener-
ative model of parsing achieves state-of-the-
art results on WSJ text and 8% error reduc-
tion over the baseline neural network parser.
1 Introduction
Latent variable models have recently been of in-
creasing interest in Natural Language Processing,
and in parsing in particular (e.g. (Koo and Collins,
2005; Matsuzaki et al, 2005; Riezler et al, 2002)).
Latent variables provide a principled way to in-
clude features in a probability model without need-
ing to have data labeled with those features in ad-
vance. Instead, a labeling with these features can
be induced as part of the training process. The
difficulty with latent variable models is that even
small numbers of latent variables can lead to com-
putationally intractable inference (a.k.a. decoding,
parsing). In this paper we propose a solution to
this problem based on dynamic Sigmoid Belief Net-
works (SBNs) (Neal, 1992). The dynamic SBNs
which we peopose, called Incremental Sigmoid Be-
lief Networks (ISBNs) have large numbers of latent
variables, which makes exact inference intractable.
However, they can be approximated sufficiently well
to build fast and accurate statistical parsers which in-
duce features during training.
We use SBNs in a generative history-based model
of constituent structure parsing. The probability of
an unbounded structure is decomposed into a se-
quence of probabilities for individual derivation de-
cisions, each decision conditioned on the unbounded
history of previous decisions. The most common ap-
proach to handling the unbounded nature of the his-
tories is to choose a pre-defined set of features which
can be unambiguously derived from the history (e.g.
(Charniak, 2000; Collins, 1999)). Decision prob-
abilities are then assumed to be independent of all
information not represented by this finite set of fea-
tures. Another previous approach is to use neural
networks to compute a compressed representation of
the history and condition decisions on this represen-
tation (Henderson, 2003; Henderson, 2004). It is
possible that an unbounded amount of information
is encoded in the compressed representation via its
continuous values, but it is not clear whether this is
actually happening due to the lack of any principled
interpretation for these continuous values.
Like the former approach, we assume that there
are a finite set of features which encode the relevant
information about the parse history. But unlike that
approach, we allow feature values to be ambiguous,
and represent each feature as a distribution over (bi-
nary) values. In other words, these history features
are treated as latent variables. Unfortunately, inter-
632
preting the history representations as distributions
over discrete values of latent variables makes the ex-
act computation of decision probabilities intractable.
Exact computation requires marginalizing out the la-
tent variables, which involves summing over all pos-
sible vectors of discrete values, which is exponential
in the length of the vector.
We propose two forms of approximation for dy-
namic SBNs, a neural network approximation and
a form of mean field approximation (Saul and Jor-
dan, 1999). We first show that the previous neural
network model of (Henderson, 2003) can be viewed
as a coarse approximation to inference with ISBNs.
We then propose an incremental mean field method,
which results in an improved approximation over
the neural network but remains tractable. The re-
sulting parser achieves significantly higher accuracy
than the neural network parser (90.0% F-measure vs
89.1%). We argue that this correlation between bet-
ter approximation and better accuracy suggests that
dynamic SBNs are a good abstract model for natural
language parsing.
2 Sigmoid Belief Networks
A belief network, or a Bayesian network, is a di-
rected acyclic graph which encodes statistical de-
pendencies between variables. Each variable Si in
the graph has an associated conditional probability
distributions P (Si|Par(Si)) over its values given
the values of its parents Par(Si) in the graph. A
Sigmoid Belief Network (Neal, 1992) is a particu-
lar type of belief networks with binary variables and
conditional probability distributions in the form of
the logistic sigmoid function:
P (Si =1|Par(Si)) =
1
1+exp(??Sj?Par(Si) JijSj)
,
where Jij is the weight for the edge from variable
Sj to variable Si. In this paper we consider a gen-
eralized version of SBNs where we allow variables
with any range of discrete values. We thus general-
ize the logistic sigmoid function to the normalized
exponential (a.k.a. softmax) function to define the
conditional probabilities for non-binary variables.
Exact inference with all but very small SBNs
is not tractable. Initially sampling methods were
used (Neal, 1992), but this is also not feasible for
large networks, especially for the dynamic models
of the type described in section 2.2. Variational
methods have also been proposed for approximat-
ing SBNs (Saul and Jordan, 1999). The main idea of
variational methods (Jordan et al, 1999) is, roughly,
to construct a tractable approximate model with a
number of free parameters. The free parameters are
set so that the resulting approximate model is as
close as possible to the original graphical model for
a given inference problem.
2.1 Mean Field Approximation Methods
The simplest example of a variation method is the
mean field method, originally introduced in statis-
tical mechanics and later applied to unsupervised
neural networks in (Hinton et al, 1995). Let us de-
note the set of visible variables in the model (i.e. the
inputs and outputs) by V and hidden variables by
H = h1, . . . , hl. The mean field method uses a fully
factorized distribution Q as the approximate model:
Q(H|V ) =
?
i
Qi(hi|V ).
where each Qi is the distribution of an individual
latent variable. The independence between the vari-
ables hi in this approximate distribution Q does not
imply independence of the free parameters which
define the Qi. These parameters are set to min-
imize the Kullback-Leibler divergence (Cover and
Thomas, 1991) between the approximate distribu-
tion Q(H|V ) and the true distribution P (H|V ):
KL(Q?P ) =
?
H
Q(H|V ) ln Q(H|V )P (H|V ) , (1)
or, equivalently, to maximize the expression:
LV =
?
H
Q(H|V ) ln P (H, V )Q(H|V ) . (2)
The expression LV is a lower bound on the log-
likelihood ln P (V ). It is used in the mean field
theory (Saul and Jordan, 1999) to approximate the
likelihood. However, in our case of dynamic graph-
ical models, we have to use a different approach
which allows us to construct an incremental parsing
method without needing to introduce the additional
parameters proposed in (Saul and Jordan, 1999).
We will describe our modification of the mean field
method in section 3.3.
633
2.2 Dynamics
Dynamic Bayesian networks are Bayesian networks
applied to arbitrarily long sequences. A new set of
variables is instantiated for each position in the se-
quence, but the edges and weights for these variables
are the same as in other positions. The edges which
connect variables instantiated for different positions
must be directed forward in the sequence, thereby
allowing a temporal interpretation of the sequence.
Typically a dynamic Bayesian Network will only in-
volve edges between adjacent positions in the se-
quence (i.e. they are Markovian), but in our parsing
models the pattern of interconnection is determined
by structural locality, rather than sequence locality,
as in the neural networks of (Henderson, 2003).
Using structural locality to define the graph in a
dynamic SBN means that the subgraph of edges with
destinations at a given position cannot be determined
until all the parser decisions for previous positions
have been chosen. We therefore call these models
Incremental SBNs, because, at any given position
in the parse, we only know the graph of edges for
that position and previous positions in the parse. For
example in figure 1, discussed below, it would not
be possible to draw the portion of the graph after t,
because we do not yet know the decision dtk.
The incremental specification of model structure
means that we cannot use an undirected graphical
model, such as Conditional Random Fields. With
a directed dynamic model, all edges connecting the
known portion of the graph to the unknown portion
of the graph are directed toward the unknown por-
tion. Also there are no variables in the unknown
portion of the graph whose values are known (i.e. no
visible variables), because at each step in a history-
based model the decision probability is conditioned
only on the parsing history. Only visible variables
can result in information being reflected backward
through a directed edge, so it is impossible for any-
thing in the unknown portion of the graph to affect
the probabilities in the known portion of the graph.
Therefore inference can be performed by simply ig-
noring the unknown portion of the graph, and there
is no need to sum over all possible structures for the
unknown portion of the graph, as would be neces-
sary for an undirected graphical model.
Figure 1: Illustration of an ISBN.
3 The Probabilistic Model of Parsing
In this section we present our framework for syn-
tactic parsing with dynamic Sigmoid Belief Net-
works. We first specify the form of SBN we propose,
namely ISBNs, and then two methods for approx-
imating the inference problems required for pars-
ing. We only consider generative models of pars-
ing, since generative probability models are simpler
and we are focused on probability estimation, not
decision making. Although the most accurate pars-
ing models (Charniak and Johnson, 2005; Hender-
son, 2004; Collins, 2000) are discriminative, all the
most accurate discriminative models make use of a
generative model. More accurate generative models
should make the discriminative models which use
them more accurate as well. Also, there are some
applications, such as language modeling, which re-
quire generative models.
3.1 The Graphical Model
In ISBNs, we use a history-based model, which de-
composes the probability of the parse as:
P (T ) = P (D1, ..., Dm) =
?
t
P (Dt|D1, . . . , Dt?1),
where T is the parse tree and D1, . . . , Dm is its
equivalent sequence of parser decisions. Instead of
treating each Dt as atomic decisions, it is convenient
to further split them into a sequence of elementary
decisions Dt = dt1, . . . , dtn:
P (Dt|D1, . . . , Dt?1) =
?
k
P (dtk|h(t, k)),
where h(t, k) denotes the parsing history
D1, . . . , Dt?1, dt1, . . . , dtk?1. For example, a
634
decision to create a new constituent can be divided
in two elementary decisions: deciding to create a
constituent and deciding which label to assign to it.
We use a graphical model to define our proposed
class of probability models. An example graphical
model for the computation of P (dtk|h(t, k)) is
illustrated in figure 1.
The graphical model is organized into vectors
of variables: latent state variable vectors St? =
st?1 , . . . , st
?
n , representing an intermediate state of the
parser at derivation step t?, and decision variable
vectors Dt? = dt?1 , . . . , dt
?
l , representing a parser de-
cision at derivation step t?, where t? ? t. Variables
whose value are given at the current decision (t, k)
are shaded in figure 1, latent and output variables are
left unshaded.
As illustrated by the arrows in figure 1, the prob-
ability of each state variable st?i depends on all the
variables in a finite set of relevant previous state and
decision vectors, but there are no direct dependen-
cies between the different variables in a single state
vector. Which previous state and decision vectors
are connected to the current state vector is deter-
mined by a set of structural relations specified by
the parser designer. For example, we could select
the most recent state where the same constituent was
on the top of the stack, and a decision variable rep-
resenting the constituent?s label. Each such selected
relation has its own distinct weight matrix for the
resulting edges in the graph, but the same weight
matrix is used at each derivation position where the
relation is relevant.
As indicated in figure 1, the probability of each
elementary decision dt?k depends both on the current
state vector St? and on the previously chosen ele-
mentary action dt?k?1 from Dt
?
. This probability dis-
tribution has the form of a normalized exponential:
P (dt?k = d|St
?, dt?k?1)=
?h(t?,k)(d) e
?
j Wdjs
t?
j
?
d??h(t?,k)(d?) e
?
jWd?js
t?
j
, (3)
where ?h(t?,k) is the indicator function of a set of
elementary decisions that may possibly follow the
parsing history h(t?, k), and the Wdj are the weights.
For our experiments, we replicated the same pat-
tern of interconnection between state variables as
described in (Henderson, 2003).1 We also used the
1In the neural network of (Henderson, 2003), our variables
same left-corner parsing strategy, and the same set of
decisions, features, and states. We refer the reader to
(Henderson, 2003) for details.
Exact computation with this model is not
tractable. Sampling of parse trees from the model
is not feasible, because a generative model defines a
joint model of both a sentence and a tree, thereby re-
quiring sampling over the space of sentences. Gibbs
sampling (Geman and Geman, 1984) is also impos-
sible, because of the huge space of variables and
need to resample after making each new decision in
the sequence. Thus, we know of no reasonable alter-
natives to the use of variational methods.
3.2 A Feed-Forward Approximation
The first model we consider is a strictly incremental
computation of a variational approximation, which
we will call the feed-forward approximation. It can
be viewed as the simplest form of mean field approx-
imation. As in any mean field approximation, each
of the latent variables is independently distributed.
But unlike the general case of mean field approxi-
mation, in the feed-forward approximation we only
allow the parameters of the distributions Qi to de-
pend on the distributions of their parents. This addi-
tional constraint increases the potential for a large
Kullback-Leibler divergence with the true model,
defined in expression (1), but it significantly simpli-
fies the computations.
The set of hidden variables H in our graphical
model consists of all the state vectors St? , t? ? t,
and the last decision dtk. All the previously observed
decisions h(t, k) comprise the set of visible vari-
ables V . The approximate fully factorisable distri-
bution Q(H|V ) can be written as:
Q(H|V ) = qtk(dtk)
?
t?,i
(
?t?i
)st?i (1 ? ?t?i
)1?st?i .
where ?t?i is the free parameter which determines the
distribution of state variable i at position t?, namely
its mean, and qtk(dtk) is the free parameter which de-
termines the distribution over decisions dtk.
Because we are only allowed to use information
about the distributions of the parent variables to
map to their ?units?, and our dependencies/edges map to their
?links?.
635
compute the free parameters ?t?i , the optimal assign-
ment of values to the ?t?i is:
?t?i = ?
(
?t?i
)
,
where ? denotes the logistic sigmoid function and
?t?i is a weighted sum of the parent variables? means:
?t?i =
?
t???RS(t?)
?
j
J?(t
?,t??)
ij ?t
??
j +
?
t???RD(t?)
?
k
B?(t
?,t??)
idt??k
, (4)
where RS(t?) is the set of previous positions with
edges from their state vectors to the state vector at t?,
RD(t?) is the set of previous positions with edges
from their decision vectors to the state vector at t?,
?(t?, t??) is the relevant relation between the position
t?? and the position t?, and J?ij and B?id are weight
matrices.
In order to maximize (2), the approximate distri-
bution of the next decisions qtk(d) should be set to
qtk(d) =
?h(t,k) (d) e
?
j Wdj?
t
j
?
d? ?h(t,k) (d?) e
?
j Wd?j?
t
j
, (5)
as follows from expression (3). The resulting esti-
mate of the tree probability is given by:
P (T ) ?
?
t,k
qtk(dtk).
This approximation method replicates exactly the
computation of the feed-forward neural network
in (Henderson, 2003), where the above means ?t?i
are equivalent to the neural network hidden unit acti-
vations. Thus, that neural network probability model
can be regarded as a simple approximation to the
graphical model introduced in section 3.1.
In addition to the drawbacks shared by any mean
field approximation method, this feed-forward ap-
proximation cannot capture backward reasoning.
By backward (a.k.a. top-down) reasoning we mean
the need to update the state vector means ?t?i after
observing a decision dtk, for t? ? t. The next section
discusses how backward reasoning can be incorpo-
rated in the approximate model.
3.3 A Mean Field Approximation
This section proposes a more accurate way to ap-
proximate ISBNs with mean field methods, which
we will call the mean field approximation. Again,
we are interested in finding the distribution Q which
maximizes the quantity LV in expression (2). The
decision distribution qtk(dtk) maximizes LV when it
has the same dependence on the state vector means
?tk as in the feed-forward approximation, namely ex-
pression (5). However, as we mentioned above, the
feed-forward computation does not allow us to com-
pute the optimal values of state means ?t?i .
Optimally, after each new decision dtk, we should
recompute all the means ?t?i for all the state vec-
tors St? , t? ? t. However, this would make the
method intractable, due to the length of derivations
in constituent parsing and the interdependence be-
tween these means. Instead, after making each deci-
sion dtk and adding it to the set of visible variables V ,
we recompute only means of the current state vector
St.
The denominator of the normalized exponential
function in (3) does not allow us to compute LV ex-
actly. Instead, we use a simple first order approxi-
mation:
EQ[ln
?
d
?h(t,k) (d) exp(
?
j
Wdjstj)]
? ln
?
d
?h(t,k)(d) exp(
?
j
Wdj?tj), (6)
where the expectation EQ[. . .] is taken over the state
vector St distributed according to the approximate
distribution Q.
Unfortunately, even with this assumption there is
no analytic way to maximize LV with respect to the
means ?tk, so we need to use numerical methods.
Assuming (6), we can rewrite the expression (2) as
follows, substituting the true P (H, V ) defined by
the graphical model and the approximate distribu-
tion Q(H|V ), omitting parts independent of ?tk:
Lt,kV =
?
i
??ti ln ?ti ? (1 ? ?ti) ln
(
1 ? ?ti
)
+?ti?ti +
?
k?<k
?h(t,k?)(dtk?)
?
j
Wdtk?j?
t
j
?
?
k?<k
ln
?
?
?
d
?h(t,k?)(d) exp(
?
j
Wdj?tj)
?
?, (7)
here, ?ti is computed from the previous relevant state
means and decisions as in (4). This expression is
636
concave with respect to the parameters ?ti, so the
global maximum can be found. We use coordinate-
wise ascent, where each ?ti is selected by an efficient
line search (Press et al, 1996), while keeping other
?ti? fixed.
3.4 Parameter Estimation
We train these models to maximize the fit of the
approximate model to the data. We use gradient
descent and a maximum likelihood objective func-
tion. This requires computation of the gradient of
the approximate log-likelihood with respect to the
model parameters. In order to compute these deriva-
tives, the error should be propagated all the way
back through the structure of the graphical model.
For the feed-forward approximation, computation of
the derivatives is straightforward, as in neural net-
works. But for the mean field approximation, it re-
quires computation of the derivatives of the means
?ti with respect to the other parameters in expres-
sion (7). The use of a numerical search in the mean
field approximation makes the analytical computa-
tion of these derivatives impossible, so a different
method needs to be used to compute their values. If
maximization of Lt,kV is done until convergence, then
the derivatives of Lt,kV with respect to ?ti are close to
zero:
F t,ki =
?Lt,kV
??ti
? 0 for all i.
This system of equations allows us to use implicit
differentiation to compute the needed derivatives.
4 Experimental Evaluation
In this section we evaluate the two approximations
to dynamic SBNs discussed in the previous section,
the feed-forward method equivalent to the neural
network of (Henderson, 2003) (NN method) and the
mean field method (MF method). The hypothesis
we wish to test is that the more accurate approxima-
tion of dynamic SBNs will result in a more accurate
model of constituent structure parsing. If this is true,
then it suggests that dynamic SBNs of the form pro-
posed here are a good abstract model of the nature
of natural language parsing.
We used the Penn Treebank WSJ corpus (Marcus
et al, 1993) to perform the empirical evaluation of
the considered approaches. It is expensive to train
R P F1
Bikel, 2004 87.9 88.8 88.3
Taskar et al, 2004 89.1 89.1 89.1
NN method 89.1 89.2 89.1
Turian and Melamed, 2006 89.3 89.6 89.4
MF method 89.3 90.7 90.0
Charniak, 2000 90.0 90.2 90.1
Table 1: Percentage labeled constituent recall (R),
precision (P), combination of both (F1) on the test-
ing set.
the MF approximation on the whole WSJ corpus, so
instead we use only sentences of length at most 15,
as in (Taskar et al, 2004) and (Turian and Melamed,
2006). The standard split of the corpus into training
(sections 2?22, 9,753 sentences), validation (section
24, 321 sentences), and testing (section 23, 603 sen-
tences) was performed.2
As in (Henderson, 2003; Turian and Melamed,
2006) we used a publicly available tagger (Ratna-
parkhi, 1996) to provide the part-of-speech tag for
each word in the sentence. For each tag, there is an
unknown-word vocabulary item which is used for all
those words which are not sufficiently frequent with
that tag to be included individually in the vocabu-
lary. We only included a specific tag-word pair in the
vocabulary if it occurred at least 20 time in the train-
ing set, which (with tag-unknown-word pairs) led to
the very small vocabulary of 567 tag-word pairs.
During parsing with both the NN method and the
MF method, we used beam search with a post-word
beam of 10. Increasing the beam size beyond this
value did not significantly effect parsing accuracy.
For both of the models, the state vector size of 40
was used. All the parameters for both the NN and
MF models were tuned on the validation set. A sin-
gle best model of each type was then applied to the
final testing set.
Table 1 lists the results of the NN approximation
and the MF approximation, along with results of dif-
2Training of our MF method on this subset of WSJ took less
than 6 days on a standard desktop PC. We would expect that
a model for the entire WSJ corpus can be trained in about 3
months time. The training time is about linear with the num-
ber of words, but a larger state vector is needed to accommo-
date all the information. The long training times on the entire
WSJ would not allow us to tune the model parameters properly,
which would have increased the randomness of the empirical
comparison, although it would be feasible for building a sys-
tem.
637
ferent generative and discriminative parsing meth-
ods (Bikel, 2004; Taskar et al, 2004; Turian and
Melamed, 2006; Charniak, 2000) evaluated in the
same experimental setup. The MF model improves
over the baseline NN approximation, with an error
reduction in F-measure exceeding 8%. This im-
provement is statically significant.3 The MF model
achieves results which do not appear to be signifi-
cantly different from the results of the best model
in the list (Charniak, 2000). It should also be noted
that the model (Charniak, 2000) is the most accu-
rate generative model on the standard WSJ parsing
benchmark, which confirms the viability of our gen-
erative model.
These experimental results suggest that Incre-
mental Sigmoid Belief Networks are an appropriate
model for natural language parsing. Even approxi-
mations such as those tested here, with a very strong
factorisability assumption, allow us to build quite
accurate parsing models. The main drawback of our
proposed mean field approach is the relative compu-
tational complexity of the numerical procedure used
to maximize Lt,kV . But this approximation has suc-
ceeded in showing that a more accurate approxima-
tion of ISBNs results in a more accurate parser. We
believe this provides strong justification for more ac-
curate approximations of ISBNs for parsing.
5 Related Work
There has not been much previous work on graph-
ical models for full parsing, although recently sev-
eral latent variable models for parsing have been
proposed (Koo and Collins, 2005; Matsuzaki et al,
2005; Riezler et al, 2002). In (Koo and Collins,
2005), an undirected graphical model is used for
parse reranking. Dependency parsing with dynamic
Bayesian networks was considered in (Peshkin and
Savova, 2005), with limited success. Their model
is very different from ours. Roughly, it considered
the whole sentence at a time, with the graphical
model being used to decide which words correspond
to leaves of the tree. The chosen words are then
removed from the sentence and the model is recur-
sively applied to the reduced sentence.
Undirected graphical models, in particular Condi-
3We measured significance of all the experiments in this pa-
per with the randomized significance test (Yeh, 2000).
tional Random Fields, are the standard tools for shal-
low parsing (Sha and Pereira, 2003). However, shal-
low parsing is effectively a sequence labeling prob-
lem and therefore differs significantly from full pars-
ing. As discussed in section 2.2, undirected graph-
ical models do not seem to be suitable for history-
based full parsing models.
Sigmoid Belief Networks were used originally
for character recognition tasks, but later a dynamic
modification of this model was applied to the rein-
forcement learning task (Sallans, 2002). However,
their graphical model, approximation method, and
learning method differ significantly from those of
this paper.
6 Conclusions
This paper proposes a new generative framework
for constituent parsing based on dynamic Sigmoid
Belief Networks with vectors of latent variables.
Exact inference with the proposed graphical model
(called Incremental Sigmoid Belief Networks) is
not tractable, but two approximations are consid-
ered. First, it is shown that the neural network
parser of (Henderson, 2003) can be considered as a
simple feed-forward approximation to the graphical
model. Second, a more accurate but still tractable
approximation based on mean field theory is pro-
posed. Both methods are empirically compared, and
the mean field approach achieves significantly better
results, which are non-significantly different from
the results of the most accurate generative parsing
model (Charniak, 2000) on our testing set. The fact
that a more accurate approximation leads to a more
accurate parser suggests that ISBNs are a good ab-
stract model for constituent structure parsing. This
empirical result motivates research into more accu-
rate approximations of dynamic SBNs.
We focused in this paper on generative models
of parsing. The results of such a generative model
can be easily improved by a discriminative rerank-
ing model, even without any additional feature en-
gineering. For example, the discriminative train-
ing techniques successfully applied in (Henderson,
2004) to the feed-forward neural network model can
be directly applied to the mean field model pro-
posed in this paper. The same is true for rerank-
ing with data-defined kernels, with which we would
638
expect similar improvements as were achieved with
the neural network parser (Henderson and Titov,
2005). Such improvements should situate the result-
ing model among the best current parsing models.
References
Dan M. Bikel. 2004. Intricacies of Collins? parsing
model. Computational Linguistics, 30(4).
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and MaxEnt discriminative rerank-
ing. In Proc. ACL, pages 173?180, Ann Arbor, MI.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proc. ACL, pages 132?139, Seattle, Wash-
ington.
Michael Collins. 1999. Head-Driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, Univer-
sity of Pennsylvania, Philadelphia, PA.
Michael Collins. 2000. Discriminative reranking for nat-
ural language parsing. In Proc. ICML, pages 175?182,
Stanford, CA.
Thomas M. Cover and Joy A. Thomas. 1991. Elements
of Information Theory. John Wiley, New York, NY.
S. Geman and D. Geman. 1984. Stochastic relaxation,
Gibbs distributions, and the Bayesian restoration of
images. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 6:721?741.
James Henderson and Ivan Titov. 2005. Data-defined
kernels for parse reranking derived from probabilistic
models. In Proc. ACL, Ann Arbor, MI.
James Henderson. 2003. Inducing history representa-
tions for broad coverage statistical parsing. In Proc.
HLT-NAACL, pages 103?110, Edmonton, Canada.
James Henderson. 2004. Discriminative training of
a neural network statistical parser. In Proc. ACL,
Barcelona, Spain.
G. Hinton, P. Dayan, B. Frey, and R. Neal. 1995.
The wake-sleep algorithm for unsupervised neural net-
works. Science, 268:1158?1161.
M. I. Jordan, Z.Ghahramani, T. S. Jaakkola, and L. K.
Saul. 1999. An introduction to variational methods for
graphical models. In Michael I. Jordan, editor, Learn-
ing in Graphical Models. MIT Press, Cambridge, MA.
Terry Koo and Michael Collins. 2005. Hidden-variable
models for discriminative reranking. In Proc. EMNLP,
Vancouver, B.C., Canada.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
Linguistics, 19(2):313?330.
Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii.
2005. Probabilistic CFG with latent annotations. In
Proc. ACL, Ann Arbor, MI.
Radford Neal. 1992. Connectionist learning of belief
networks. Artificial Intelligence, 56:71?113.
Leon Peshkin and Virginia Savova. 2005. Dependency
parsing with dynamic bayesian network. In AAAI,
20th National Conference on Artificial Intelligence,
Pittsburgh, Pennsylvania.
W. Press, B. Flannery, S. Teukolsky, and W. Vetterling.
1996. Numerical Recipes. Cambridge University
Press, Cambridge, UK.
Adwait Ratnaparkhi. 1996. A maximum entropy model
for part-of-speech tagging. In Proc. EMNLP, pages
133?142, Univ. of Pennsylvania, PA.
Stefan Riezler, Tracy H. King, Ronald M. Kaplan,
Richard Crouch, John T. Maxwell, and Mark John-
son. 2002. Parsing the Wall Street Journal using a
Lexical-Functional Grammar and discriminative esti-
mation techniques. In Proc. ACL, Philadelphia, PA.
Brian Sallans. 2002. Reinforcement Learning for Fac-
tored Markov Decision Processes. Ph.D. thesis, Uni-
versity of Toronto, Toronto, Canada.
Lawrence K. Saul and Michael I. Jordan. 1999. A
mean field learning algorithm for unsupervised neu-
ral networks. In Michael I. Jordan, editor, Learning in
Graphical Models, pages 541?554. MIT Press, Cam-
bridge, MA.
Fei Sha and Fernando Pereira. 2003. Shallow parsing
with conditional random fields. In Proc. HLT-NAACL,
Edmonton, Canada.
Ben Taskar, Dan Klein, Michael Collins, Daphne Koller,
and Christopher Manning. 2004. Max-margin pars-
ing. In Proc. EMNLP, Barcelona, Spain.
Joseph Turian and Dan Melamed. 2006. Advances in
discriminative parsing. In Proc. COLING-ACL, Syd-
ney, Australia.
Alexander Yeh. 2000. More accurate tests for the sta-
tistical significance of the result differences. In Proc.
COLING, pages 947?953, Saarbruken, Germany.
639
Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X),
pages 6?13, New York City, June 2006. c?2006 Association for Computational Linguistics
Porting Statistical Parsers with Data-Defined Kernels
Ivan Titov
University of Geneva
24, rue Ge?ne?ral Dufour
CH-1211 Gene`ve 4, Switzerland
ivan.titov@cui.unige.ch
James Henderson
University of Edinburgh
2 Buccleuch Place
Edinburgh EH8 9LW, United Kingdom
james.henderson@ed.ac.uk
Abstract
Previous results have shown disappointing
performance when porting a parser trained
on one domain to another domain where
only a small amount of data is available.
We propose the use of data-defined ker-
nels as a way to exploit statistics from a
source domain while still specializing a
parser to a target domain. A probabilistic
model trained on the source domain (and
possibly also the target domain) is used to
define a kernel, which is then used in a
large margin classifier trained only on the
target domain. With a SVM classifier and
a neural network probabilistic model, this
method achieves improved performance
over the probabilistic model alone.
1 Introduction
In recent years, significant progress has been made
in the area of natural language parsing. This re-
search has focused mostly on the development of
statistical parsers trained on large annotated corpora,
in particular the Penn Treebank WSJ corpus (Marcus
et al, 1993). The best statistical parsers have shown
good results on this benchmark, but these statistical
parsers demonstrate far worse results when they are
applied to data from a different domain (Roark and
Bacchiani, 2003; Gildea, 2001; Ratnaparkhi, 1999).
This is an important problem because we cannot ex-
pect to have large annotated corpora available for
most domains. While identifying this problem, pre-
vious work has not proposed parsing methods which
are specifically designed for porting parsers. Instead
they propose methods for training a standard parser
with a large amount of out-of-domain data and a
small amount of in-domain data.
In this paper, we propose using data-defined ker-
nels and large margin methods to specifically ad-
dress porting a parser to a new domain. Data-defined
kernels are used to construct a new parser which ex-
ploits information from a parser trained on a large
out-of-domain corpus. Large margin methods are
used to train this parser to optimize performance on
a small in-domain corpus.
Large margin methods have demonstrated sub-
stantial success in applications to many machine
learning problems, because they optimize a mea-
sure which is directly related to the expected test-
ing performance. They achieve especially good per-
formance compared to other classifiers when only
a small amount of training data is available. Most
of the large margin methods need the definition of a
kernel. Work on kernels for natural language parsing
has been mostly focused on the definition of kernels
over parse trees (e.g. (Collins and Duffy, 2002)),
which are chosen on the basis of domain knowledge.
In (Henderson and Titov, 2005) it was proposed to
apply a class of kernels derived from probabilistic
models to the natural language parsing problem.
In (Henderson and Titov, 2005), the kernel is con-
structed using the parameters of a trained proba-
bilistic model. This type of kernel is called a data-
defined kernel, because the kernel incorporates in-
formation from the data used to train the probabilis-
tic model. We propose to exploit this property to
transfer information from a large corpus to a statis-
6
tical parser for a different domain. Specifically, we
propose to train a statistical parser on data including
the large corpus, and to derive the kernel from this
trained model. Then this derived kernel is used in a
large margin classifier trained on the small amount
of training data available for the target domain.
In our experiments, we consider two different
scenarios for porting parsers. The first scenario is
the pure porting case, which we call ?transferring?.
Here we only require a probabilistic model trained
on the large corpus. This model is then reparameter-
ized so as to extend the vocabulary to better suit the
target domain. The kernel is derived from this repa-
rameterized model. The second scenario is a mixture
of parser training and porting, which we call ?focus-
ing?. Here we train a probabilistic model on both
the large corpus and the target corpus. The kernel
is derived from this trained model. In both scenar-
ios, the kernel is used in a SVM classifier (Tsochan-
taridis et al, 2004) trained on a small amount of data
from the target domain. This classifier is trained to
rerank the candidate parses selected by the associ-
ated probabilistic model. We use the Penn Treebank
Wall Street Journal corpus as the large corpus and
individual sections of the Brown corpus as the tar-
get corpora (Marcus et al, 1993). The probabilis-
tic model is a neural network statistical parser (Hen-
derson, 2003), and the data-defined kernel is a TOP
reranking kernel (Henderson and Titov, 2005).
With both scenarios, the resulting parser demon-
strates improved accuracy on the target domain over
the probabilistic model alone. In additional experi-
ments, we evaluate the hypothesis that the primary
issue for porting parsers between domains is differ-
ences in the distributions of words in structures, and
not in the distributions of the structures themselves.
We partition the parameters of the probability model
into those which define the distributions of words
and those that only involve structural decisions, and
derive separate kernels for these two subsets of pa-
rameters. The former model achieves virtually iden-
tical accuracy to the full model, but the later model
does worse, confirming the hypothesis.
2 Data-Defined Kernels for Parsing
Previous work has shown how data-defined kernels
can be applied to the parsing task (Henderson and
Titov, 2005). Given the trained parameters of a prob-
abilistic model of parsing, the method defines a ker-
nel over sentence-tree pairs, which is then used to
rerank a list of candidate parses.
In this paper, we focus on the TOP reranking ker-
nel defined in (Henderson and Titov, 2005), which
are closely related to Fisher kernels. The rerank-
ing task is defined as selecting a parse tree from the
list of candidate trees (y1, . . . , ys) suggested by a
probabilistic model P (x, y|??), where ?? is a vector of
model parameters learned during training the prob-
abilistic model. The motivation for the TOP rerank-
ing kernel is given in (Henderson and Titov, 2005),
but for completeness we note that the its feature ex-
tractor is given by:
???(x, yk) =
(v(x, yk, ??), ?v(x,yk,??)??1 , . . . ,
?v(x,yk,??)
??l ),
(1)
where v(x, yk, ??) = log P (x, yk|??) ?
log ?t6=k P (x, yt|??). The first feature reflects
the score given to (x, yk) by the probabilistic
model (relative to the other candidates for x), and
the remaining features reflect how changing the
parameters of the probabilistic model would change
this score for (x, yk).
The parameters ?? used in this feature extractor do
not have to be exactly the same as the parameters
trained in the probabilistic model. In general, we
can first reparameterize the probabilistic model, pro-
ducing a new model which defines exactly the same
probability distribution as the old model, but with a
different set of adjustable parameters. For example,
we may want to freeze the values of some parame-
ters (thereby removing them from ??), or split some
parameters into multiple cases (thereby duplicating
their values in ??). This flexibility allows the features
used in the kernel method to be different from those
used in training the probabilistic model. This can be
useful for computational reasons, or when the kernel
method is not solving exactly the same problem as
the probabilistic model was trained for.
3 Porting with Data-Defined Kernels
In this paper, we consider porting a parser trained on
a large amount of annotated data to a different do-
main where only a small amount of annotated data
is available. We validate our method in two different
7
scenarios, transferring and focusing. Also we verify
the hypothesis that addressing differences between
the vocabularies of domains is more important than
addressing differences between their syntactic struc-
tures.
3.1 Transferring to a Different Domain
In the transferring scenario, we are given just a prob-
abilistic model which has been trained on a large
corpus from a source domain. The large corpus is
not available during porting, and the small corpus
for the target domain is not available during training
of the probabilistic model. This is the case of pure
parser porting, because it only requires the source
domain parser, not the source domain corpus. Be-
sides this theoretical significance, this scenario has
the advantage that we only need to train a single
probabilistic parser, thereby saving on training time
and removing the need for access to the large cor-
pus once this training is done. Then any number of
parsers for new domains can be trained, using only
the small amount of annotated data available for the
new domain.
Our proposed porting method first constructs a
data-defined kernel using the parameters of the
trained probabilistic model. A large margin clas-
sifier with this kernel is then trained to rerank the
top candidate parses produced by the probabilistic
model. Only the small target corpus is used during
training of this classifier. The resulting parser con-
sists of the original parser plus a very computation-
ally cheap procedure to rerank its best parses.
Whereas training of standard large margin meth-
ods, like SVMs, isn?t feasible on a large corpus, it
is quite tractable to train them on a small target cor-
pus.1 Also, the choice of the large margin classifier
is motivated by their good generalization properties
on small datasets, on which accurate probabilistic
models are usually difficult to learn.
We hypothesize that differences in vocabulary
across domains is one of the main difficulties with
parser portability. To address this problem, we pro-
pose constructing the kernel from a probabilistic
model which has been reparameterized to better suit
1In (Shen and Joshi, 2003) it was proposed to use an en-
semble of SVMs trained the Wall Street Journal corpus, but we
believe that the generalization performance of the resulting clas-
sifier is compromised in this approach.
the target domain vocabulary. As in other lexicalized
statistical parsers, the probabilistic model we use
treats words which are not frequent enough in the
training set as ?unknown? words (Henderson, 2003).
Thus there are no parameters in this model which
are specifically for these words. When we consider
a different target domain, a substantial proportion
of the words in the target domain are treated as un-
known words, which makes the parser only weakly
lexicalized for this domain.
To address this problem, we reparameterize the
probability model so as to add specific parameters
for the words which have high enough frequency
in the target domain training set but are treated as
unknown words by the original probabilistic model.
These new parameters all have the same values as
their associated unknown words, so the probability
distribution specified by the model does not change.
However, when a kernel is defined with this repa-
rameterized model, the kernel?s feature extractor in-
cludes features specific to these words, so the train-
ing of a large margin classifier can exploit differ-
ences between these words in the target domain. Ex-
panding the vocabulary in this way is also justified
for computational reasons; the speed of the proba-
bilistic model we use is greatly effected by vocabu-
lary size, but the large-margin method is not.
3.2 Focusing on a Subdomain
In the focusing scenario, we are given the large cor-
pus from the source domain. We may also be given
a parsing model, but as with other approaches to this
problem we simply throw this parsing model away
and train a new one on the combination of the source
and target domain data. Previous work (Roark and
Bacchiani, 2003) has shown that better accuracy can
be achieved by finding the optimal re-weighting be-
tween these two datasets, but this issue is orthogonal
to our method, so we only consider equal weighting.
After this training phase, we still want to optimize
the parser for only the target domain.
Once we have the trained parsing model, our pro-
posed porting method proceeds the same way in this
scenario as in transferring. However, because the
original training set aleady includes the vocabulary
from the target domain, the reparameterization ap-
proach defined in the preceding section is not nec-
essary so we do not perform it. This reparameter-
8
ization could be applied here, thereby allowing us
to use a statistical parser with a smaller vocabulary,
which can be more computationally efficient both
during training and testing. However, we would ex-
pect better accuracy of the combined system if the
same large vocabulary is used both by the proba-
bilistic parser and the kernel method.
3.3 Vocabulary versus Structure
It is commonly believed that differences in vo-
cabulary distributions between domains effects the
ported parser performance more significantly than
the differences in syntactic structure distributions.
We would like to test this hypothesis in our frame-
work. The probabilistic model (Henderson, 2003)
allows us to distinguish between those parameters
responsible for the distributions of individual vocab-
ulary items, and those parameters responsible for the
distributions of structural decisions, as described in
more details in section 4.2. We train two additional
models, one which uses a kernel defined in terms of
only vocabulary parameters, and one which uses a
kernel defined in terms of only structure parameters.
By comparing the performance of these models and
the model with the combined kernel, we can draw
conclusion on the relative importance of vocabulary
and syntactic structures for parser portability.
4 An Application to a Neural Network
Statistical Parser
Data-defined kernels can be applied to any kind
of parameterized probabilistic model, but they are
particularly interesting for latent variable models.
Without latent variables (e.g. for PCFG models), the
features of the data-defined kernel (except for the
first feature) are a function of the counts used to esti-
mate the model. For a PCFG, each such feature is a
function of one rule?s counts, where the counts from
different candidates are weighted using the probabil-
ity estimates from the model. With latent variables,
the meaning of the variable (not just its value) is
learned from the data, and the associated features of
the data-defined kernel capture this induced mean-
ing. There has been much recent work on latent
variable models (e.g. (Matsuzaki et al, 2005; Koo
and Collins, 2005)). We choose to use an earlier
neural network based probabilistic model of pars-
ing (Henderson, 2003), whose hidden units can be
viewed as approximations to latent variables. This
parsing model is also a good candidate for our exper-
iments because it achieves state-of-the-art results on
the standard Wall Street Journal (WSJ) parsing prob-
lem (Henderson, 2003), and data-defined kernels de-
rived from this parsing model have recently been
used with the Voted Perceptron algorithm on the
WSJ parsing task, achieving a significant improve-
ment in accuracy over the neural network parser
alone (Henderson and Titov, 2005).
4.1 The Probabilistic Model of Parsing
The probabilistic model of parsing in (Henderson,
2003) has two levels of parameterization. The first
level of parameterization is in terms of a history-
based generative probability model. These param-
eters are estimated using a neural network, the
weights of which form the second level of param-
eterization. This approach allows the probability
model to have an infinite number of parameters; the
neural network only estimates the bounded number
of parameters which are relevant to a given partial
parse. We define our kernels in terms of the second
level of parameterization (the network weights).
A history-based model of parsing first defines a
one-to-one mapping from parse trees to sequences
of parser decisions, d1,..., dm (i.e. derivations). Hen-
derson (2003) uses a form of left-corner parsing
strategy, and the decisions include generating the
words of the sentence (i.e. it is generative). The
probability of a sequence P (d1,..., dm) is then de-
composed into the multiplication of the probabilities
of each parser decision conditioned on its history of
previous decisions ?iP (di|d1,..., di?1).
4.2 Deriving the Kernel
The complete set of neural network weights isn?t
used to define the kernel, but instead reparameteriza-
tion is applied to define a third level of parameteriza-
tion which only includes the network?s output layer
weights. As suggested in (Henderson and Titov,
2005) use of the complete set of weights doesn?t
lead to any improvement of the resulting reranker
and makes the reranker training more computation-
ally expensive.
Furthermore, to assess the contribution of vocab-
ulary and syntactic structure differences (see sec-
9
tion 3.3), we divide the set of the parameters into vo-
cabulary parameters and structural parameters. We
consider the parameters used in the estimation of the
probability of the next word given the history repre-
sentation as vocabulary parameters, and the param-
eters used in the estimation of structural decision
probabilities as structural parameters. We define the
kernel with structural features as using only struc-
tural parameters, and the kernel with vocabulary fea-
tures as using only vocabulary parameters.
5 Experimental Results
We used the Penn Treebank WSJ corpus and the
Brown corpus to evaluate our approach. We used
the standard division of the WSJ corpus into train-
ing, validation, and testing sets. In the Brown corpus
we ran separate experiments for sections F (informa-
tive prose: popular lore), K (imaginative prose: gen-
eral fiction), N (imaginative prose: adventure and
western fiction), and P (imaginative prose: romance
and love story). These sections were selected be-
cause they are sufficiently large, and because they
appeared to be maximally different from each other
and from WSJ text. In each Brown corpus section,
we selected every third sentence for testing. From
the remaining sentences, we used 1 sentence out of
20 for the validation set, and the remainder for train-
ing. The resulting datasets sizes are presented in ta-
ble 1.
For the large margin classifier, we used the SVM-
Struct (Tsochantaridis et al, 2004) implementation
of SVM, which rescales the margin with F1 mea-
sure of bracketed constituents (see (Tsochantaridis
et al, 2004) for details). Linear slack penalty was
employed.2
5.1 Experiments on Transferring across
Domains
To evaluate the pure porting scenario (transferring),
described in section 3.1, we trained the SSN pars-
ing model on the WSJ corpus. For each tag, there is
an unknown-word vocabulary item which is used for
all those words not sufficiently frequent with that tag
to be included individually in the vocabulary. In the
2Training of the SVM takes about 3 hours on a standard
desktop PC. Running the SVM is very fast, once the probabilis-
tic model has finished computing the probabilities needed to
select the candidate parses.
testing training validation
WSJ 2,416 39,832 1,346
(54,268) (910,196) (31,507)
Brown F 1,054 2,005 105
(23,722) (44,928) (2,300)
Brown K 1,293 2,459 129
(21,215) (39,823) (1,971)
Brown N 1,471 2,797 137
(22,142) (42,071) (2,025)
Brown P 1,314 2,503 125
(21,763) (41,112) (1,943)
Table 1: Number of sentences (words) for each
dataset.
vocabulary of the parser, we included the unknown-
word items and the words which occurred in the
training set at least 20 times. This led to the vo-
cabulary of 4,215 tag-word pairs.
We derived the kernel from the trained model for
each target section (F, K, N, P) using reparameteriza-
tion discussed in section 3.1: we included in the vo-
cabulary all the words which occurred at least twice
in the training set of the corresponding section. This
approach led to a smaller vocabulary than that of the
initial parser but specifically tied to the target do-
main (3,613, 2,789, 2,820 and 2,553 tag-word pairs
for sections F, K, N and P respectively). There is no
sense in including the words from the WSJ which do
not appear in the Brown section training set because
the classifier won?t be able to learn the correspond-
ing components of its decision vector. The results
for the original probabilistic model (SSN-WSJ) and
for the kernel method (TOP-Transfer) on the testing
set of each section are presented in table 2.3
To evaluate the relative contribution of our porting
technique versus the use of the TOP kernel alone,
we also used this TOP kernel to train an SVM on the
WSJ corpus. We trained the SVM on data from the
development set and section 0, so that the size of this
dataset (3,267 sentences) was about the same as for
each Brown section.4 This gave us a ?TOP-WSJ?
3All our results are computed with the evalb program fol-
lowing the standard criteria in (Collins, 1999).
4We think that using an equivalently sized dataset provides
a fair test of the contribution of the TOP kernel alone. It would
also not be computationally tractable to train an SVM on the full
WSJ dataset without using different training techniques, which
would then compromise the comparison.
10
model, which we tested on each of the four Brown
sections. In each case, the TOP-WSJ model did
worse than the original SSN-WSJ model, as shown
in table 2. This makes it clear that we are getting no
improvement from simply using a TOP kernel alone
or simply using more data, and all our improvement
is from the proposed porting method.
5.2 Experiments on Focusing on a Subdomain
To perform the experiments on the approach sug-
gested in section 3.2 (focusing), we trained the SSN
parser on the WSJ training set joined with the train-
ing set of the corresponding section. We included
in the vocabulary only words which appeared in the
joint training set at least 20 times. Resulting vocab-
ularies comprised 4,386, 4,365, 4,367 and 4,348 for
sections F, K, N and P, respectively.5 Experiments
were done in the same way as for the parser transfer-
ring approach, but reparameterization was not per-
formed. Standard measures of accuracy for the orig-
inal probabilistic model (SSN-WSJ+Br) and the ker-
nel method (TOP-Focus) are also shown in table 2.
For the sake of comparison, we also trained the
SSN parser on only training data from one of the
Brown corpus sections (section P), producing a
?SSN-Brown? model. This model achieved an F1
measure of only 81.0% for the P section testing
set, which is worse than all the other models and
is 3% lower than our best results on this testing set
(TOP-Focus). This result underlines the need to port
parsers from domains in which there are large anno-
tated datasets.
5.3 Experiments Comparing Vocabulary to
Structure
We conducted the same set of experiments with the
kernel with vocabulary features (TOP-Voc-Transfer
and TOP-Voc-Focus) and with the kernel with the
structural features (TOP-Str-Transfer and TOP-Str-
Focus). Average results for classifiers with these
kernels, as well as for the original kernel and the
baseline, are presented in table 3.
5We would expect some improvement if we used a smaller
threshold on the target domain, but preliminary results suggest
that this improvement would be small.
section LR LP F?=1
TOP-WSJ F 83.9 84.9 84.4
SSN-WSJ F 84.4 85.2 84.8
TOP-Transfer F 84.5 85.6 85.0
SSN-WSJ+Br F 84.2 85.2 84.7
TOP-Focus F 84.6 86.0 85.3
TOP-WSJ K 81.8 82.3 82.1
SSN-WSJ K 82.2 82.6 82.4
TOP-Transfer K 82.4 83.5 83.0
SSN-WSJ+Br K 83.1 84.2 83.6
TOP-Focus K 83.6 85.0 84.3
TOP-WSJ N 83.3 84.5 83.9
SSN-WSJ N 83.5 84.6 84.1
TOP-Transfer N 84.3 85.7 85.0
SSN-WSJ+Br N 85.0 86.5 85.7
TOP-Focus N 85.0 86.7 85.8
TOP-WSJ P 81.3 82.1 81.7
SSN-WSJ P 82.3 83.0 82.6
TOP-Transfer P 82.7 83.8 83.2
SSN-WSJ+Br P 83.1 84.3 83.7
TOP-Focus P 83.3 84.8 84.0
Table 2: Percentage labeled constituent recall (LR),
precision (LP), and a combination of both (F?=1) on
the individual test sets.
5.4 Discussion of Results
For the experiments which directly test the useful-
ness of our proposed porting technique (SSN-WSJ
versus TOP-Transfer), our technique demonstrated
improvement for each of the Brown sections (ta-
ble 2), and this improvement was significant for
three out of four of the sections (K, N, and P).6 This
demonstrates that data-defined kernels are an effec-
tive way to port parsers to a new domain.
For the experiments which combine training a
new probability model with our porting technique
(SSN-WSJ+Br versus TOP-Focus), our technique
still demonstrated improvement over training alone.
There was improvement for each of the Brown sec-
tions, and this improvement was significant for two
6We measured significance in F1 measure at the 5% level
with the randomized significance test of (Yeh, 2000). We think
that the reason the improvement on section F was only signif-
icant at the 10% level was that the baseline model (SSN-WSJ)
was particularly lucky, as indicated by the fact that it did even
better than the model trained on the combination of datasets
(SSN-WSJ+Br).
11
LR LP F?=1
SSN-WSJ 83.1 83.8 83.5
TOP-Transfer 83.5 84.7 84.1
TOP-Voc-Transfer 83.5 84.7 84.1
TOP-Str-Transfer 83.1 84.3 83.7
SSN-WSJ+Br 83.8 85.0 84.4
TOP-Focus 84.1 85.6 84.9
TOP-Voc-Focus 84.1 85.6 84.8
TOP-Str-Focus 83.9 85.4 84.7
Table 3: Average accuracy of the models on chapters
F, K, N and P of the Brown corpus.
out of four of the sections (F and K). This demon-
strates that, even when the probability model is well
suited to the target domain, there is still room for
improvement from using data-defined kernels to op-
timize the parser specifically to the target domain
without losing information about the source domain.
One potential criticism of these conclusions is that
the improvement could be the result of reranking
with the TOP kernel, and have nothing to do with
porting. The lack of an improvement in the TOP-
WSJ results discussed in section 5.1 clearly shows
that this cannot be the explanation. The opposite
criticism is that the improvement could be the result
of optimizing to the target domain alone. The poor
performance of the SSN-Brown model discussed in
section 5.2 makes it clear that this also cannot be
the explanation. Therefore reranking with data de-
fined kernels must be both effective at preserving
information about the source domain and effective
at specializing to the target domain.
The experiments which test the hypothesis that
differences in vocabulary distributions are more im-
portant than difference in syntactic structure distri-
butions confirm this belief. Results for the classi-
fier which uses the kernel with only vocabulary fea-
tures are better than those for structural features in
each of the four sections with both the Transfer and
Focus scenarios. In addition, comparing the results
of TOP-Transfer with TOP-Voc-Transfer and TOP-
Focus with TOP-Voc-Focus, we can see that adding
structural features in TOP-Focus and TOP-Transfer
leads to virtually no improvement. This suggest that
differences in vocabulary distributions are the only
issue we need to address, although this result could
possibly also be an indication that our method did
not sufficiently exploit structural differences.
In this paper we concentrate on the situation
where a parser is needed for a restricted target do-
main, for which only a small amount of data is avail-
able. We believe that this is the task which is of
greatest practical interest. For this reason we do not
run experiments on the task considered in (Gildea,
2001) and (Roark and Bacchiani, 2003), where they
are porting from the restricted domain of the WSJ
corpus to the more varied domain of the Brown cor-
pus as a whole. However, to help emphasize the
success of our proposed porting method, it is rele-
vant to show that even our baseline models are per-
forming better than this previous work on parser
portability. We trained and tested the SSN parser in
their ?de-focusing? scenario using the same datasets
as (Roark and Bacchiani, 2003). When trained
only on the WSJ data (analogously to the SSN-
WSJ baseline for TOP-Transfer) it achieves results
of 82.9%/83.4% LR/LP and 83.2% F1, and when
trained on data from both domains (analogously
to the SSN-WSJ+Br baselines for TOP-Focus) it
achieves results of 86.3%/87.6% LR/LP and 87.0%
F1. These results represent a 2.2% and 1.3% in-
crease in F1 over the best previous results, respec-
tively (see the discussion of (Roark and Bacchiani,
2003) below).
6 Related Work
Most research in the field of parsing has focused on
the Wall Street Journal corpus. Several researchers
have addressed the portability of these WSJ parsers
to other domains, but mostly without addressing the
issue of how a parser can be designed specifically
for porting to another domain. Unfortunately, no di-
rect empirical comparison is possible between our
results and results with other parsers, because there
is no standard portability benchmark to date where a
small amount of data from a target domain is used.
(Ratnaparkhi, 1999) performed portability exper-
iments with a Maximum Entropy parser and demon-
strated that the parser trained on WSJ achieves far
worse results on the Brown corpus sections. Adding
a small amount of data from the target domain im-
proves the results, but accuracy is still much lower
than the results on the WSJ. They reported results
when their parser was trained on the WSJ training
12
set plus a portion of 2,000 sentences from a Brown
corpus section. They achieved 80.9%/80.3% re-
call/precision for section K, and 80.6%/81.3% for
section N.7 Our analogous method (TOP-Focus)
achieved much better accuracy (3.7% and 4.9% bet-
ter F1, respectively).
In addition to portability experiments with the
parsing model of (Collins, 1997), (Gildea, 2001)
provided a comprehensive analysis of parser porta-
bility. On the basis of this analysis, a tech-
nique for parameter pruning was proposed leading
to a significant reduction in the model size with-
out a large decrease of accuracy. Gildea (2001)
only reports results on sentences of 40 or less
words on all the Brown corpus sections combined,
for which he reports 80.3%/81.0% recall/precision
when training only on data from the WSJ corpus,
and 83.9%/84.8% when training on data from the
WSJ corpus and all sections of the Brown corpus.
(Roark and Bacchiani, 2003) performed experi-
ments on supervised and unsupervised PCFG adap-
tation to the target domain. They propose to use
the statistics from a source domain to define pri-
ors over weights. However, in their experiments
they used only trivial sub-cases of this approach,
namely, count merging and model interpolation.
They achieved very good improvement over their
baseline and over (Gildea, 2001), but the absolute
accuracies were still relatively low (as discussed
above). They report results with combined Brown
data (on sentences of 100 words or less), achieving
81.3%/80.9% when training only on the WSJ cor-
pus and 85.4%/85.9% with their best method using
the data from both domains.
7 Conclusions
This paper proposes a novel technique for improv-
ing parser portability, applying parse reranking with
data-defined kernels. First a probabilistic model of
parsing is trained on all the available data, including
a large set of data from the source domain. This
model is used to define a kernel over parse trees.
Then this kernel is used in a large margin classifier
7The sizes of Brown sections reported in (Ratnaparkhi,
1999) do not match the sizes of sections distributed in the Penn
Treebank 3.0 package, so we couldn?t replicate their split. We
suspect that a preliminary version of the corpus was used for
their experiments.
trained on a small set of data only from the target do-
main. This classifier is used to rerank the top parses
produced by the probabilistic model on the target do-
main. Experiments with a neural network statistical
parser demonstrate that this approach leads to im-
proved parser accuracy on the target domain, with-
out any significant increase in computational cost.
References
Michael Collins and Nigel Duffy. 2002. New ranking algo-
rithms for parsing and tagging: Kernels over discrete struc-
tures and the voted perceptron. In Proc. ACL 2002 , pages
263?270, Philadelphia, PA.
Michael Collins. 1997. Three generative, lexicalized models
for statistical parsing. In Proc. ACL/EACL 1997 , pages 16?
23, Somerset, New Jersey.
Michael Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, University of
Pennsylvania, Philadelphia, PA.
Daniel Gildea. 2001. Corpus variation and parser performance.
In Proc. EMNLP 2001 , Pittsburgh, PA.
James Henderson and Ivan Titov. 2005. Data-defined kernels
for parse reranking derived from probabilistic models. In
Proc. ACL 2005 , Ann Arbor, MI.
James Henderson. 2003. Inducing history representations for
broad coverage statistical parsing. In Proc. NAACL/HLT
2003 , pages 103?110, Edmonton, Canada.
Terry Koo and Michael Collins. 2005. Hidden-variable models
for discriminative reranking. In Proc. EMNLP 2005 , Van-
couver, B.C., Canada.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated corpus
of English: The Penn Treebank. Computational Linguistics,
19(2):313?330.
Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii. 2005.
Probabilistic CFG with latent annotations. In Proc. ACL
2005 , Ann Arbor, MI.
Adwait Ratnaparkhi. 1999. Learning to parse natural language
with maximum entropy models. Machine Learning, 34:151?
175.
Brian Roark and Michiel Bacchiani. 2003. Supervised and
unsuperised PCFG adaptation to novel domains. In Proc.
HLT/ACL 2003 , Edmionton, Canada.
Libin Shen and Aravind K. Joshi. 2003. An SVM based voting
algorithm with application to parse reranking. In Proc. 7th
Conf. on Computational Natural Language Learning, pages
9?16, Edmonton, Canada.
Ioannis Tsochantaridis, Thomas Hofmann, Thorsten Joachims,
and Yasemin Altun. 2004. Support vector machine learning
for interdependent and structured output spaces. In Proc.
21st Int. Conf. on Machine Learning, pages 823?830, Banff,
Alberta, Canada.
Alexander Yeh. 2000. More accurate tests for the statistical
significance of the result differences. In Proc. 17th Int. Conf.
on Computational Linguistics, pages 947?953, Saarbruken,
Germany.
13
Proceedings of the 10th Conference on Parsing Technologies, pages 144?155,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
A Latent Variable Model for Generative Dependency Parsing
Ivan Titov
University of Geneva
24, rue Ge?ne?ral Dufour
CH-1211 Gene`ve 4, Switzerland
ivan.titov@cui.unige.ch
James Henderson
University of Edinburgh
2 Buccleuch Place
Edinburgh EH8 9LW, United Kingdom
james.henderson@ed.ac.uk
Abstract
We propose a generative dependency pars-
ing model which uses binary latent variables
to induce conditioning features. To define
this model we use a recently proposed class
of Bayesian Networks for structured predic-
tion, Incremental Sigmoid Belief Networks.
We demonstrate that the proposed model
achieves state-of-the-art results on three dif-
ferent languages. We also demonstrate that
the features induced by the ISBN?s latent
variables are crucial to this success, and
show that the proposed model is particularly
good on long dependencies.
1 Introduction
Dependency parsing has been a topic of active re-
search in natural language processing during the last
several years. The CoNLL-X shared task (Buch-
holz and Marsi, 2006) made a wide selection of
standardized treebanks for different languages avail-
able for the research community and allowed for
easy comparison between various statistical meth-
ods on a standardized benchmark. One of the sur-
prising things discovered by this evaluation is that
the best results are achieved by methods which
are quite different from state-of-the-art models for
constituent parsing, e.g. the deterministic parsing
method of (Nivre et al, 2006) and the minimum
spanning tree parser of (McDonald et al, 2006).
All the most accurate dependency parsing models
are fully discriminative, unlike constituent parsing
where all the state of the art methods have a genera-
tive component (Charniak and Johnson, 2005; Hen-
derson, 2004; Collins, 2000). Another surprising
thing is the lack of latent variable models among
the methods used in the shared task. Latent vari-
able models would allow complex features to be in-
duced automatically, which would be highly desir-
able in multilingual parsing, where manual feature
selection might be very difficult and time consum-
ing, especially for languages unknown to the parser
developer.
In this paper we propose a generative latent vari-
able model for dependency parsing. It is based on
Incremental Sigmoid Belief Networks (ISBNs), a
class of directed graphical model for structure pre-
diction problems recently proposed in (Titov and
Henderson, 2007), where they were demonstrated
to achieve competitive results on the constituent
parsing task. As discussed in (Titov and Hender-
son, 2007), computing the conditional probabili-
ties which we need for parsing is in general in-
tractable with ISBNs, but they can be approximated
efficiently in several ways. In particular, the neu-
ral network constituent parsers in (Henderson, 2003)
and (Henderson, 2004) can be regarded as coarse ap-
proximations to their corresponding ISBN model.
ISBNs use history-based probability models. The
most common approach to handling the unbounded
nature of the parse histories in these models is to
choose a pre-defined set of features which can be
unambiguously derived from the history (e.g. (Char-
niak, 2000; Collins, 1999; Nivre et al, 2004)). De-
cision probabilities are then assumed to be indepen-
dent of all information not represented by this finite
set of features. ISBNs instead use a vector of binary
144
latent variables to encode the information about the
parser history. This history vector is similar to the
hidden state of a Hidden Markov Model. But un-
like the graphical model for an HMM, which speci-
fies conditional dependency edges only between ad-
jacent states in the sequence, the ISBN graphical
model can specify conditional dependency edges be-
tween states which are arbitrarily far apart in the
parse history. The source state of such an edge is de-
termined by the partial output structure built at the
time of the destination state, thereby allowing the
conditional dependency edges to be appropriate for
the structural nature of the problem being modeled.
This structure sensitivity is possible because ISBNs
are a constrained form of switching model (Mur-
phy, 2002), where each output decision switches the
model structure used for the remaining decisions.
We build an ISBN model of dependency parsing
using the parsing order proposed in (Nivre et al,
2004). However, instead of performing determin-
istic parsing as in (Nivre et al, 2004), we use this
ordering to define a generative history-based model,
by integrating word prediction operations into the
set of parser actions. Then we propose a simple, lan-
guage independent set of relations which determine
how latent variable vectors are interconnected by
conditional dependency edges in the ISBN model.
ISBNs also condition the latent variable vectors on a
set of explicit features, which we vary in the experi-
ments.
In experiments we evaluate both the performance
of the ISBN dependency parser compared to previ-
ous work, and the ability of the ISBN model to in-
duce complex history features. Our model achieves
state-of-the-art performance on the languages we
test, significantly outperforming the model of (Nivre
et al, 2006) on two languages out of three and
demonstrating about the same results on the third.
In order to test the model?s feature induction abili-
ties, we train models with two different sets of ex-
plicit conditioning features: the feature set individu-
ally tuned by (Nivre et al, 2006) for each considered
language, and a minimal set of local features. These
models achieve comparable accuracy, unlike with
the discriminative SVM-based approach of (Nivre et
al., 2006), where careful feature selection appears to
be crucial. We also conduct a controlled experiment
where we used the tuned features of (Nivre et al,
2006) but disable the feature induction abilities of
our model by elimination of the edges connecting
latent state vectors. This restricted model achieves
far worse results, showing that it is exactly the ca-
pacity of ISBNs to induce history features which is
the key to its success. It also motivates further re-
search into how feature induction techniques can be
exploited in discriminative parsing methods.
We analyze how the relation accuracy changes
with the length of the head-dependent relation,
demonstrating that our model very significantly out-
performs the state-of-the-art baseline of (Nivre et
al., 2006) on long dependencies. Additional exper-
iments suggest that both feature induction abilities
and use of the beam search contribute to this im-
provement.
The fact that our model defines a probability
model over parse trees, unlike the previous state-of-
the-art methods (Nivre et al, 2006; McDonald et al,
2006), makes it easier to use this model in appli-
cations which require probability estimates, e.g. in
language processing pipelines. Also, as with any
generative model, it may be easy to improve the
parser?s accuracy by using discriminative retraining
techniques (Henderson, 2004) or data-defined ker-
nels (Henderson and Titov, 2005), with or even with-
out introduction of any additional linguistic features.
In addition, there are some applications, such as lan-
guage modeling, which require generative models.
Another advantage of generative models is that they
do not suffer from the label bias problems (Bot-
tou, 1991), which is a potential problem for con-
ditional or deterministic history-based models, such
as (Nivre et al, 2004).
In the remainder of this paper, we will first review
general ISBNs and how they can be approximated.
Then we will define the generative parsing model,
based on the algorithm of (Nivre et al, 2004), and
propose an ISBN for this model. The empirical part
of the paper then evaluates both the overall accuracy
of this method and the importance of the model?s
capacity to induce features. Additional related work
will be discussed in the last section before conclud-
ing.
145
2 The Latent Variable Architecture
In this section we will begin by briefly introduc-
ing the class of graphical models we will be us-
ing, Incremental Sigmoid Belief Networks (Titov
and Henderson, 2007). ISBNs are designed specif-
ically for modeling structured data. They are latent
variable models which are not tractable to compute
exactly, but two approximations exist which have
been shown to be effective for constituent parsing
(Titov and Henderson, 2007). Finally, we present
how these approximations can be trained.
2.1 Incremental Sigmoid Belief Networks
An ISBN is a form of Sigmoid Belief Network
(SBN) (Neal, 1992). SBNs are Bayesian Networks
with binary variables and conditional probability
distributions in the form:
P (Si = 1|Par(Si)) = ?(
?
Sj?Par(Si)
JijSj),
where Si are the variables, Par(Si) are the variables
which Si depends on (its parents), ? denotes the lo-
gistic sigmoid function, and Jij is the weight for the
edge from variable Sj to variable Si in the graphi-
cal model. SBNs are similar to feed-forward neural
networks, but unlike neural networks, SBNs have a
precise probabilistic semantics for their hidden vari-
ables. ISBNs are based on a generalized version of
SBNs where variables with any range of discrete val-
ues are allowed. The normalized exponential func-
tion (?soft-max?) is used to define the conditional
probability distributions at these nodes.
To extend SBNs for processing arbitrarily long se-
quences, such as a parser?s sequence of decisions
D1, ..., Dm, SBNs are extended to a form of Dy-
namic Bayesian Network (DBN). In DBNs, a new
set of variables is instantiated for each position in
the sequence, but the edges and weights are the same
for each position in the sequence. The edges which
connect variables instantiated for different positions
must be directed forward in the sequence, thereby
allowing a temporal interpretation of the sequence.
Incremental Sigmoid Belief Networks (Titov and
Henderson, 2007) differ from simple dynamic SBNs
in that they allow the model structure to depend on
the output variable values. Specifically, a decision is
allowed to effect the placement of any edge whose
destination is after the decision. This results in a
form of switching model (Murphy, 2002), where
each decision switches the model structure used for
the remaining decisions. The incoming edges for
a given position are a discrete function of the se-
quence of decisions which precede that position.
This makes the ISBN an ?incremental? model, not
just a dynamic model. The structure of the model is
determined incrementally as the decision sequence
proceeds.
ISBNs are designed to allow the model structure
to depend on the output values without overly com-
plicating the inference of the desired conditional
probabilities P (Dt|D1, . . . , Dt?1), the probability
of the next decision given the history of previous de-
cisions. In particular, it is never necessary to sum
over all possible model structures, which in general
would make inference intractable.
2.2 Modeling Structures with ISBNs
ISBNs are designed for modeling structured data
where the output structure is not given as part of
the input. In dependency parsing, this means they
can model the probability of an output dependency
structure when the input only specifies the sequence
of words (i.e. parsing). The difficulty with such
problems is that the statistical dependencies in the
dependency structure are local in the structure, and
not necessarily local in the word sequence. ISBNs
allow us to capture these statistical dependencies in
the model structure by having model edges depend
on the output variables which specify the depen-
dency structure. For example, if an output specifies
that there is a dependency arc from word wi to word
wj , then any future decision involving wj can di-
rectly depend on its head wi. This allows the head
wi to be treated as local to the dependent wj even if
they are far apart in the sentence.
This structurally-defined notion of locality is par-
ticularly important for the model?s latent variables.
When the structurally-defined model edges connect
latent variables, information can be propagated be-
tween latent variables, thereby providing an even
larger structural domain of locality than that pro-
vided by single edges. This provides a poten-
tially powerful form of feature induction, which is
nonetheless biased toward a notion of locality which
is appropriate for the structure of the problem.
146
2.3 Approximating ISBNs
(Titov and Henderson, 2007) proposes two approxi-
mations for inference in ISBNs, both based on vari-
ational methods. The main idea of variational meth-
ods (Jordan et al, 1999) is, roughly, to construct a
tractable approximate model with a number of free
parameters. The values of the free parameters are set
so that the resulting approximate model is as close as
possible to the original graphical model for a given
inference problem.
The simplest example of a variation method is the
mean field method, which uses a fully factorized dis-
tribution Q(H|V ) = ?i Qi(hi|V ) as the approxi-
mate model, where V are the visible (i.e. known)
variables, H = h1, . . . , hl are the hidden (i.e. la-
tent) variables, and each Qi is the distribution of an
individual latent variable hi. The free parameters of
this approximate model are the means ?i of the dis-
tributions Qi.
(Titov and Henderson, 2007) proposes two ap-
proximate models based on the variational approach.
First, they show that the neural network of (Hen-
derson, 2003) can be viewed as a coarse mean field
approximation of ISBNs, which they call the feed-
forward approximation. This approximation im-
poses the constraint that the free parameters ?i of
the approximate model are only allowed to depend
on the distributions of their parent variables. This
constraint increases the potential for a large approx-
imation error, but it significantly simplifies the com-
putations by allowing all the free parameters to be
set in a single pass over the model.
The second approximation proposed in (Titov and
Henderson, 2007) takes into consideration the fact
that, after each decision is made, all the preceding
latent variables should have their means ?i updated.
This approximation extends the feed-forward ap-
proximation to account for the most important com-
ponents of this update. They call this approxima-
tion the mean field approximation, because a mean
field approximation is applied to handle the statisti-
cal dependencies introduced by the new decisions.
This approximation was shown to be a more accu-
rate approximation of ISBNs than the feed-forward
approximation, but remain tractable. It was also
shown to achieve significantly better accuracy on
constituent parsing.
2.4 Learning
Training these approximations of ISBNs is done to
maximize the fit of the approximate models to the
data. We use gradient descent, and a regularized
maximum likelihood objective function. Gaussian
regularization is applied, which is equivalent to the
weight decay standardly used in neural networks.
Regularization was reduced through the course of
learning.
Gradient descent requires computing the deriva-
tives of the objective function with respect to the
model parameters. In the feed-forward approxima-
tion, this can be done with the standard Backpropa-
gation learning used with neural networks. For the
mean field approximation, propagating the error all
the way back through the structure of the graphical
model requires a more complicated calculation, but
it can still be done efficiently (see (Titov and Hen-
derson, 2007) for details).
3 The Dependency Parsing Algorithm
The sequences of decisions D1, ..., Dm which we
will be modeling with ISBNs are the sequences of
decisions made by a dependency parser. For this we
use the parsing strategy for projective dependency
parsing introduced in (Nivre et al, 2004), which
is similar to a standard shift-reduce algorithm for
context-free grammars (Aho et al, 1986). It can
be viewed as a mixture of bottom-up and top-down
parsing strategies, where left dependencies are con-
structed in a bottom-up fashion and right dependen-
cies are constructed top-down. For details we refer
the reader to (Nivre et al, 2004). In this section we
briefly describe the algorithm and explain how we
use it to define our history-based probability model.
In this paper, as in the CoNLL-X shared task,
we consider labeled dependency parsing. The state
of the parser is defined by the current stack S, the
queue I of remaining input words and the partial la-
beled dependency structure constructed by previous
parser decisions. The parser starts with an empty
stack S and terminates when it reaches a configura-
tion with an empty queue I . The algorithm uses 4
types of decisions:
1. The decision Left-Arcr adds a dependency arc
from the next input word wj to the word wi on
top of the stack and selects the label r for the
147
relation between wi and wj . Word wi is then
popped from the stack.
2. The decision Right-Arcr adds an arc from the
word wi on top of the stack to the next input
word wj and selects the label r for the relation
between wi and wj .
3. The decision Reduce pops the word wi from
the stack.
4. The decision Shiftwj shifts the word wj from
the queue to the stack.
Unlike the original definition in (Nivre et al, 2004)
the Right-Arcr decision does not shift wj to the
stack. However, the only thing the parser can do
after a Right-Arcr decision is to choose the Shiftwj
decision. This subtle modification does not change
the actual parsing order, but it does simplify the def-
inition of our graphical model, as explained in sec-
tion 4.
We use a history-based probability model, which
decomposes the probability of the parse according
to the parser decisions:
P (T ) = P (D1, ..., Dm) =
?
t
P (Dt|D1, . . . , Dt?1),
where T is the parse tree and D1, . . . , Dm is its
equivalent sequence of parser decisions. Since we
need a generative model, the action Shiftwj also pre-
dicts the next word in the queue I , wj+1, thus the
P (Shiftwi |D1, . . . , Dt?1) is a probability both of
the shift operation and the word wj+1 conditioned
on current parsing history.1
Instead of treating each Dt as an atomic decision,
it is convenient to split it into a sequence of elemen-
tary decisions Dt = dt1, . . . , dtn:
P (Dt|D1, . . . , Dt?1) =
?
k
P (dtk|h(t, k)),
1In preliminary experiments, we also considered look-
ahead, where the word is predicted earlier than it appears at the
head of the queue I , and ?anti-look-ahead?, where the word is
predicted only when it is shifted to the stack S. Early predic-
tion allows conditioning decision probabilities on the words in
the look-ahead and, thus, speeds up the search for an optimal
decision sequence. However, the loss of accuracy with look-
ahead was quite significant. The described method, where a
new word is predicted when it appears at the head of the queue,
led to the most accurate model and quite efficient search. The
anti-look-ahead model was both less accurate and slower.
Figure 1: An ISBN for estimating P (dtk|h(t, k)).
where h(t, k) denotes the parsing history
D1, . . . , Dt?1, dt1, . . . , dtk?1. We split Left-Arcr
and Right-Arcr each into two elementary decisions:
first, the parser decides to create the corresponding
arc, then, it decides to assign a relation r to the
arc. Similarly, we decompose the decision Shiftwj
into an elementary decision to shift a word and a
prediction of the word wj+1. In our experiments we
use datasets from the CoNLL-X shared task, which
provide additional properties for each word token,
such as its part-of-speech tag and some fine-grain
features. This information implicitly induces word
clustering, which we use in our model: first we
predict a part-of-speech tag for the word, then a set
of word features, treating feature combination as an
atomic value, and only then a particular word form.
This approach allows us to both decrease the effect
of sparsity and to avoid normalization across all the
words in the vocabulary, significantly reducing the
computational expense of word prediction.
4 An ISBN for Dependency Parsing
In this section we define the ISBN model we use for
dependency parsing. An example of this ISBN for
estimating P (dtk|h(t, k)) is illustrated in figure 1. It
is organized into vectors of variables: latent state
variable vectors St? = st?1 , . . . , st
?
n , representing an
intermediate state at position t?, and decision vari-
able vectors Dt? , representing a decision at position
t?, where t? ? t. Variables whose value are given at
the current decision (t, k) are shaded in figure 1, la-
tent and current decision variables are left unshaded.
As illustrated by the edges in figure 1, the prob-
ability of each state variable st?i (the individual cir-
cles in St?) depends on all the variables in a finite
set of relevant previous state and decision vectors,
148
but there are no direct dependencies between the dif-
ferent variables in a single state vector. For each
relevant decision vector, the precise set of decision
variables which are connected in this way can be
adapted to a particular language. As long as these
connected decisions include all the new information
about the parse, the performance of the model is not
very sensitive to this choice. This is because ISBNs
have the ability to induce their own complex features
of the parse history, as demonstrated in the experi-
ments in section 6.
The most important design decision in building
an ISBN model is choosing the finite set of relevant
previous state vectors for the current decision. By
connecting to a previous state, we place that state in
the local context of the current decision. This speci-
fication of the domain of locality determines the in-
ductive bias of learning with ISBNs. When deciding
what information to store in its latent variables, an
ISBN is more likely to choose information which
is immediately local to the current decision. This
stored information then becomes local to any fol-
lowing connected decision, where it again has some
chance of being chosen as relevant to that decision.
In this way, the information available to a given deci-
sion can come from arbitrarily far away in the chain
of interconnected states, but it is much more likely
to come from a state which is relatively local. Thus,
we need to choose the set of local (i.e. connected)
states in accordance with our prior knowledge about
which previous decisions are likely to be particularly
relevant to the current decision.
To choose which previous decisions are particu-
larly relevant to the current decision, we make use
of the partial dependency structure which has been
decided so far in the parse. Specifically, the current
latent state vector is connected to a set of 7 previous
latent state vectors (if they exist) according to the
following relationships:
1. Input Context: the last previous state with the
same queue I .
2. Stack Context: the last previous state with the
same stack S.
3. Right Child of Top of S: the last previous state
where the rightmost right child of the current
stack top was on top of the stack.
4. Left Child of Top of S: the last previous state
where the leftmost left child of the current stack
top was on top of the stack.
5. Left Child of Front of I2 : the last previous
state where the leftmost child of the front ele-
ment of I was on top of the stack.
6. Head of Top: the last previous state where the
head word of the current stack top was on top
of the stack.
7. Top of S at Front of I: the last previous state
where the current stack top was at the front of
the queue.
Each of these 7 relations has its own distinct weight
matrix for the resulting edges in the ISBN, but the
same weight matrix is used at each position where
the relation is relevant.
All these relations but the last one are motivated
by linguistic considerations. The current decision is
primarily about what to do with the current word on
the top of the stack and the current word on the front
of the queue. The Input Context and Stack Context
relationships connect to the most recent states used
for making decisions about each of these words. The
Right Child of Top of S relationship connects to a
state used for making decisions about the most re-
cently attached dependent of the stack top. Simi-
larly, the Left Child of Front of I relationship con-
nects to a state for the most recently attached depen-
dent of the queue front. The Left Child of Top of S
is the first dependent of the stack top, which is a par-
ticularly informative dependent for many languages.
Likewise, the Head of Top can tell us a lot about the
stack top, if it has been chosen already.
A second motivation for including a state in the
local context of a decision is that it might contain in-
formation which has no other route for reaching the
current decision. In particular, it is generally a good
idea to ensure that the immediately preceding state is
always included somewhere in the set of connected
states. This requirement ensures that information, at
least theoretically, can pass between any two states
in the decision sequence, thereby avoiding any hard
2We refer to the head of the queue as the front, to avoid
unnecessary ambiguity of the word head in the context of de-
pendency parsing.
149
independence assumptions. The last relation, Top of
S at Front of I , is included mainly to fulfill this re-
quirement. Otherwise, after a Shiftwj operation, the
preceding state would not be selected by any of the
relationships.
As indicated in figure 1, the probability of each
elementary decision dt?k depends both on the current
state vector St? and on the previously chosen ele-
mentary action dt?k?1 from Dt
?
. This probability dis-
tribution has the form of a normalized exponential:
P (dt?k = d|St
? , dt?k?1)=
?h(t?,k) (d) e
?
j Wdjs
t?
j
?
d??h(t?,k) (d?) e
?
j Wd?js
t?
j
,
where ?h(t?,k) is the indicator function of the set of
elementary decisions that may possibly follow the
last decision in the history h(t?, k), and the Wdj are
the weights. Now it is easy to see why the origi-
nal decision Right-Arcr (Nivre et al, 2004) had to
be decomposed into two distinct decisions: the de-
cision to construct a labeled arc and the decision to
shift the word. Use of this composite Right-Arcr
would have required the introduction of individual
parameters for each pair (w, r), where w is an arbi-
trary word in the lexicon and r - an arbitrary depen-
dency relation.
5 Searching for the Best Tree
ISBNs define a probability model which does not
make any a-priori assumptions of independence be-
tween any decision variables. As we discussed in
section 4 use of relations based on partial output
structure makes it possible to take into account sta-
tistical interdependencies between decisions closely
related in the output structure, but separated by mul-
tiple decisions in the input structure. This property
leads to exponential complexity of complete search.
However, the success of the deterministic parsing
strategy which uses the same parsing order (Nivre et
al., 2006), suggests that it should be relatively easy
to find an accurate approximation to the best parse
with heuristic search methods. Unlike (Nivre et al,
2006), we can not use a lookahead in our generative
model, as was discussed in section 3, so a greedy
method is unlikely to lead to a good approximation.
Instead we use a pruning strategy similar to that de-
scribed in (Henderson, 2003), where it was applied
to a considerably harder search problem: constituent
parsing with a left-corner parsing order.
We apply fixed beam pruning after each deci-
sion Shiftwj , because knowledge of the next word
in the queue I helps distinguish unlikely decision
sequences. We could have used best-first search be-
tween Shiftwj operations, but this still leads to rela-
tively expensive computations, especially when the
set of dependency relations is large. However, most
of the word pairs can possibly participate only in a
very limited number of distinct relations. Thus, we
pursue only a fixed number of relations r after each
Left-Arcr and Right-Arcr operation.
Experiments with a variety of post-shift beam
widths confirmed that very small validation perfor-
mance gains are achieved with widths larger than 30,
and sometimes even a beam of 5 was sufficient. We
found also that allowing 5 different relations after
each dependency prediction operation was enough
that it had virtually no effect on the validation accu-
racy.
6 Empirical Evaluation
In this section we evaluate the ISBN model for
dependency parsing on three treebanks from the
CoNLL-X shared task. We compare our genera-
tive models with the best parsers from the CoNLL-
X task, including the SVM-based parser of (Nivre et
al., 2006) (the MALT parser), which uses the same
parsing algorithm. To test the feature induction abil-
ities of our model we compare results with two fea-
ture sets, the feature set tuned individually for each
language by (Nivre et al, 2006), and another fea-
ture set which includes only obvious local features.
This simple feature set comprises only features of
the word on top of the stack S and the front word
of the queue I . We compare the gain from using
tuned features with the similar gain obtained by the
MALT parser. To obtain these results we train the
MALT parser with the same two feature sets.3
In order to distinguish the contribution of ISBN?s
feature induction abilities from the contribution of
3The tuned feature sets were obtained from
http://w3.msi.vxu.se/?nivre/research/MaltParser.html. We
removed lookahead features for ISBN experiments but
preserved them for experiments with the MALT parser. Anal-
ogously, we extended simple features with 3 words lookahead
for the MALT parser experiments.
150
our estimation method and search, we perform an-
other experiment. We use the tuned feature set and
disable the feature induction abilities of the model
by removing all the edges between latent variables
vectors. Comparison of this restricted model with
the full ISBN model shows how important the fea-
ture induction is. Also, comparison of this restricted
model with the MALT parser, which uses the same
set of features, indicates whether our generative esti-
mation method and use of beam search is beneficial.
6.1 Experimental Setup
We used the CoNLL-X distributions of Danish
DDT treebank (Kromann, 2003), Dutch Alpino tree-
bank (van der Beek et al, 2002) and Slovene SDT
treebank (Dzeroski et al, 2006). The choice of these
treebanks was motivated by the fact that they all
are freely distributed and have very different sizes
of their training sets: 195,069 tokens for Dutch,
94,386 tokens for Danish and only 28,750 tokens for
Slovene. As it is generally believed that discrimina-
tive models win over generative models with a large
amount of training data, so we expected to see simi-
lar trend in our results. Test sets are about equal and
contain about 5,000 scoring tokens.
We followed the experimental setup of the shared
task and used all the information provided for the
languages: gold standard part-of-speech tags and
coarse part-of-speech tags, word form, word lemma
(lemma information was not available for Danish)
and a set of fine-grain word features. As we ex-
plained in section 3, we treated these sets of fine-
grain features as an atomic value when predicting
a word. However, when conditioning on words, we
treated each component of this composite feature in-
dividually, as it proved to be useful on the develop-
ment set. We used frequency cutoffs: we ignored
any property (e.g., word form, feature or even part-
of-speech tag4) which occurs in the training set less
than 5 times. Following (Nivre et al, 2006), we used
pseudo-projective transformation they proposed to
cast non-projective parsing tasks as projective.
ISBN models were trained using a small devel-
opment set taken out from the training set, which
was used for tuning learning parameters and for
4Part-of-speech tags for multi-word units in the Danish tree-
bank were formed as concatenation of tags of the words, which
led to quite sparse set of part-of-speech tags.
early stopping. The sizes of the development sets
were: 4,988 tokens for larger Dutch corpus, 2,504
tokens for Danish and 2,033 tokens for Slovene.
The MALT parser was trained always using the en-
tire training set. We expect that the mean field ap-
proximation should demonstrate better results than
feed-forward approximation on this task as it is the-
oretically expected and confirmed on the constituent
parsing task (Titov and Henderson, 2007). How-
ever, the sizes of testing sets would not allow us
to perform any conclusive analysis, so we decided
not to perform these comparisons here. Instead we
used the mean field approximation for the smaller
two corpora and used the feed-forward approxima-
tion for the larger one. Training the mean field ap-
proximations on the larger Dutch treebank is feasi-
ble, but would significantly reduce the possibilities
for tuning the learning parameters on the develop-
ment set and, thus, would increase the randomness
of model comparisons.
All model selection was performed on the devel-
opment set and a single model of each type was
applied to the testing set. We used a state vari-
able vector consisting of 80 binary variables, as it
proved sufficient on the preliminary experiments.
For the MALT parser we replicated the parameters
from (Nivre et al, 2006) as described in detail on
their web site.
The labeled attachment scores for the ISBN with
tuned features (TF) and local features (LF) and
ISBN with tuned features and no edges connect-
ing latent variable vectors (TF-NA) are presented
in table 1, along with results for the MALT parser
both with tuned and local feature, the MST parser
(McDonald et al, 2006), and the average score
(Aver) across all systems in the CoNLL-X shared
task. The MST parser is included because it demon-
strated the best overall result in the task, non signif-
icantly outperforming the MALT parser, which, in
turn, achieved the second best overall result. The la-
beled attachment score is computed using the same
method as in the CoNLL-X shared task, i.e. ignor-
ing punctuation. Note, that though we tried to com-
pletely replicate training of the MALT parser with
the tuned features, we obtained slightly different re-
sults. The original published results for the MALT
parser with tuned features were 84.8% for Danish,
78.6% for Dutch and 70.3% for Slovene. The im-
151
Danish Dutch Slovene
ISBN TF 85.0 79.6 72.9
LF 84.5 79.5 72.4
TF-NA 83.5 76.4 71.7
MALT TF 85.1 78.2 70.5
LF 79.8 74.5 66.8
MST 84.8 79.2 73.4
Aver 78.3 70.7 65.2
Table 1: Labeled attachment score on the testing sets
of Danish, Dutch and Slovene treebanks.
provement of the ISBN models (TF and LF) over
the MALT parser is statistically significant for Dutch
and Slovene. Differences between their results on
Danish are not statistically significant.
6.2 Discussion of Results
The ISBN with tuned features (TF) achieved signif-
icantly better accuracy than the MALT parser on 2
languages (Dutch and Slovene), and demonstrated
essentially the same accuracy on Danish. The results
of the ISBN are among the two top published results
on all three languages, including the best published
results on Dutch. All three models, MST, MALT and
ISBN, demonstrate much better results than the av-
erage result in the CoNLL-X shared task. These re-
sults suggest that our generative model is quite com-
petitive with respect to the best models, which are
both discriminative.5 We would expect further im-
provement of ISBN results if we applied discrimina-
tive retraining (Henderson, 2004) or reranking with
data-defined kernels (Henderson and Titov, 2005),
even without introduction of any additional features.
We can see that the ISBN parser achieves about
the same results with local features (LF). Local fea-
tures by themselves are definitely not sufficient for
the construction of accurate models, as seen from
the results of the MALT parser with local features
(and look-ahead). This result demonstrates that IS-
BNs are a powerful model for feature induction.
The results of the ISBN without edges connecting
latent state vectors is slightly surprising and suggest
that without feature induction the ISBN is signifi-
cantly worse than the best models. This shows that
5Note that the development set accuracy predicted correctly
the testing set ranking of ISBN TF, LF and TF-NA models on
each of the datasets, so it is fair to compare the best ISBN result
among the three with other parsers.
to root 1 2 3 - 6 > 6
Da ISBN 95.1 95.7 90.1 84.1 74.7
MALT 95.4 96.0 90.8 84.0 71.6
Du ISBN 79.8 92.4 86.2 81.4 71.1
MALT 73.1 91.9 85.0 76.2 64.3
Sl ISBN 76.1 92.5 85.6 79.6 54.3
MALT 59.9 92.1 85.0 78.4 47.1
Av ISBN 83.6 93.5 87.3 81.7 66.7
MALT 76.2 93.3 87.0 79.5 61.0
Improv 7.5 0.2 0.4 2.2 5.7
Table 2: F1 score of labeled attachment as a function
of dependency length on the testing sets of Danish,
Dutch and Slovene.
the improvement is coming mostly from the abil-
ity of the ISBN to induce complex features and not
from either using beam search or from the estima-
tion procedure. It might also suggest that genera-
tive models are probably worse for the dependency
parsing task than discriminative approaches (at least
for larger datasets). This motivates further research
into methods which combine powerful feature in-
duction properties with the advantage of discrimina-
tive training. Although discriminative reranking of
the generative model is likely to help, the derivation
of fully discriminative feature induction methods is
certainly more challenging.
In order to better understand differences in per-
formance between ISBN and MALT, we analyzed
how relation accuracy changes with the length of
the head-dependent relation. The harmonic mean
between precision and recall of labeled attachment,
F1 measure, for the ISBN and MALT parsers with
tuned features is presented in table 2. F1 score is
computed for four different ranges of lengths and
for attachments directly to root. Along with the re-
sults for each of the languages, the table includes
their mean (Av) and the absolute improvement of
the ISBN model over MALT (Improv). It is easy
to see that accuracy of both models is generally sim-
ilar for small distances (1 and 2), but as the distance
grows the ISBN parser starts to significantly outper-
form MALT, achieving 5.7% average improvement
on dependencies longer than 6 word tokens. When
the MALT parser does not manage to recover a long
dependency, the highest scoring action it can choose
is to reduce the dependent from the stack without
specifying its head, thereby attaching the dependent
152
to the root by default. This explains the relatively
low F1 scores for attachments to root (evident for
Dutch and Slovene): though recall of attachment to
root is comparable to that of the ISBN parser (82.4%
for MALT against 84.2% for ISBN, on average over
3 languages), precision for the MALT parser is much
worse (71.5% for MALT against 83.1% for ISBN,
on average).
The considerably worse accuracy of the MALT
parser on longer dependencies might be explained
both by use of a non-greedy search method in the
ISBN and the ability of ISBNs to induce history fea-
tures. To capture a long dependency, the MALT
parser should keep a word on the stack during a
long sequence of decision. If at any point during
the intermediate steps this choice seems not to be
locally optimal, then the MALT parser will choose
the alternative and lose the possibility of the long
dependency.6 By using a beam search, the ISBN
parser can maintain the possibility of the long de-
pendency in its beam even when other alternatives
seem locally preferable. Also, long dependences are
often more difficult, and may be systematically dif-
ferent from local dependencies. The designer of a
MALT parser needs to discover predictive features
for long dependencies by hand, whereas the ISBN
model can automatically discover them. Thus we
expect that the feature induction abilities of ISBNs
have a strong effect on the accuracy of long depen-
dences. This prediction is confirmed by the differ-
ences between the results of the normal ISBN (TF)
and the restricted ISBN (TF-NA) model. The TF-
NA model, like the MALT parser, is biased toward
attachment to root; it attaches to root 12.0% more
words on average than the normal ISBN, without
any improvement of recall and with a great loss of
precision. The F1 score on long dependences for the
TF-NA model is also negatively effected in the same
way as for the MALT parser. This confirms that the
ability of the ISBN model to induce features is a ma-
jor factor in improving accuracy of long dependen-
cies.
6The MALT parser is trained to keep the word as long as
possible: if both Shift and Reduce decisions are possible during
training, it always prefers to shift. Though this strategy should
generally reduce the described problem, it is evident from the
low precision score for attachment to root, that it can not com-
pletely eliminate it.
7 Related Work
There has not been much previous work on latent
variable models for dependency parsing. Depen-
dency parsing with Dynamic Bayesian Networks
was considered in (Peshkin and Savova, 2005), with
limited success. Roughly, the model considered
the whole sentence at a time, with the DBN being
used to decide which words correspond to leaves
of the tree. The chosen words are then removed
from the sentence and the model is recursively ap-
plied to the reduced sentence. Recently several la-
tent variable models for constituent parsing have
been proposed (Koo and Collins, 2005; Matsuzaki
et al, 2005; Prescher, 2005; Riezler et al, 2002).
In (Matsuzaki et al, 2005) non-terminals in a stan-
dard PCFG model are augmented with latent vari-
ables. A similar model of (Prescher, 2005) uses a
head-driven PCFG with latent heads, thus restrict-
ing the flexibility of the latent-variable model by us-
ing explicit linguistic constraints. While the model
of (Matsuzaki et al, 2005) significantly outperforms
the constrained model of (Prescher, 2005), they both
are well below the state-of-the-art in constituent
parsing. In (Koo and Collins, 2005), an undirected
graphical model for constituent parse reranking uses
dependency relations to define the edges. Thus, it
should be easy to apply a similar method to rerank-
ing dependency trees.
Undirected graphical models, in particular Condi-
tional Random Fields, are the standard tools for shal-
low parsing (Sha and Pereira, 2003). However, shal-
low parsing is effectively a sequence labeling prob-
lem and therefore differs significantly from full pars-
ing. As discussed in (Titov and Henderson, 2007),
undirected graphical models do not seem to be suit-
able for history-based parsing models.
Sigmoid Belief Networks (SBNs) were used orig-
inally for character recognition tasks, but later a dy-
namic modification of this model was applied to the
reinforcement learning task (Sallans, 2002). How-
ever, their graphical model, approximation method,
and learning method differ significantly from those
of this paper. The extension of dynamic SBNs with
incrementally specified model structure (i.e. Incre-
mental Sigmoid Belief Networks, used in this pa-
per) was proposed and applied to constituent parsing
in (Titov and Henderson, 2007).
153
8 Conclusions
We proposed a latent variable dependency parsing
model based on Incremental Sigmoid Belief Net-
works. Unlike state-of-the-art dependency parsers,
it uses a generative history-based model. We demon-
strated that it achieves state-of-the-art results on a
selection of languages from the CoNLL-X shared
task. The parser uses a vector of latent variables
to represent an intermediate state and uses rela-
tions defined on the output structure to construct the
edges between latent state vectors. These proper-
ties make it a powerful feature induction method
for dependency parsing, and it achieves competi-
tive results even with very simple explicit features.
The ISBN model is especially accurate at modeling
long dependences, achieving average improvement
of 5.7% over the state-of-the-art baseline on depen-
dences longer than 6 words. Empirical evaluation
demonstrates that competitive results are achieved
mostly because of the ability of the model to in-
duce complex features and not because of the use of
a generative probability model or a specific search
method. As with other generative models, it can be
further improved by the application of discrimina-
tive reranking techniques. Discriminative methods
are likely to allow it to significantly improve over
the current state-of-the-art in dependency parsing.7
Acknowledgments
This work was funded by Swiss NSF grant 200020-
109685, UK EPSRC grant EP/E019501/1, and EU
FP6 grant 507802 for project TALK. We thank
Joakim Nivre and Sandra Ku?bler for an excellent
tutorial on dependency parsing given at COLING-
ACL 2006.
References
Alfred V. Aho, Ravi Sethi, and Jeffrey D. Ullman. 1986.
Compilers: Principles, Techniques and Tools. Addi-
son Wesley.
Leon Bottou. 1991. Une approche the?oretique de
l?apprentissage connexionniste: Applications a` la re-
connaissance de la parole. Ph.D. thesis, Universite? de
Paris XI, Paris, France.
7The ISBN dependency parser will be soon made download-
able from the authors? web-page.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proc. of the Tenth Conference on Computational Nat-
ural Language Learning, New York, USA.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and MaxEnt discriminative rerank-
ing. In Proc. 43rd Meeting of Association for Compu-
tational Linguistics, pages 173?180, Ann Arbor, MI.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proc. 1st Meeting of North American
Chapter of Association for Computational Linguistics,
pages 132?139, Seattle, Washington.
Michael Collins. 1999. Head-Driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, Univer-
sity of Pennsylvania, Philadelphia, PA.
Michael Collins. 2000. Discriminative reranking for nat-
ural language parsing. In Proc. 17th Int. Conf. on Ma-
chine Learning, pages 175?182, Stanford, CA.
S. Dzeroski, T. Erjavec, N. Ledinek, P. Pajas, Z. Zabokrt-
sky, and A. Zele. 2006. Towards a Slovene depen-
dency treebank. In Proc. Int. Conf. on Language Re-
sources and Evaluation (LREC), Genoa, Italy.
James Henderson and Ivan Titov. 2005. Data-defined
kernels for parse reranking derived from probabilis-
tic models. In Proc. 43rd Meeting of Association for
Computational Linguistics, Ann Arbor, MI.
James Henderson. 2003. Inducing history representa-
tions for broad coverage statistical parsing. In Proc.
joint meeting of North American Chapter of the Asso-
ciation for Computational Linguistics and the Human
Language Technology Conf., pages 103?110, Edmon-
ton, Canada.
James Henderson. 2004. Discriminative training of
a neural network statistical parser. In Proc. 42nd
Meeting of Association for Computational Linguistics,
Barcelona, Spain.
M. I. Jordan, Z.Ghahramani, T. S. Jaakkola, and L. K.
Saul. 1999. An introduction to variational methods for
graphical models. In Michael I. Jordan, editor, Learn-
ing in Graphical Models. MIT Press, Cambridge, MA.
Terry Koo and Michael Collins. 2005. Hidden-variable
models for discriminative reranking. In Proc. Conf. on
Empirical Methods in Natural Language Processing,
Vancouver, B.C., Canada.
Matthias T. Kromann. 2003. The Danish dependency
treebank and the underlying linguistic theory. In Pro-
ceedings of the 2nd Workshop on Treebanks and Lin-
guistic Theories (TLT), Vaxjo, Sweden.
154
Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii.
2005. Probabilistic CFG with latent annotations. In
Proceedings of the 43rd Annual Meeting of the ACL,
Ann Arbor, MI.
Ryan McDonald, Kevin Lerman, and Fernando Pereira.
2006. Multilingual dependency analysis with a two-
stage discriminative parser. In Proc. of the Tenth Con-
ference on Computational Natural Language Learn-
ing, New York, USA.
Kevin P. Murphy. 2002. Dynamic Belief Networks:
Representation, Inference and Learning. Ph.D. thesis,
University of California, Berkeley, CA.
Radford Neal. 1992. Connectionist learning of belief
networks. Artificial Intelligence, 56:71?113.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2004.
Memory-based dependency parsing. In Proc. of the
Eighth Conference on Computational Natural Lan-
guage Learning, pages 49?56, Boston, USA.
Joakim Nivre, Johan Hall, Jens Nilsson, Gulsen Eryigit,
and Svetoslav Marinov. 2006. Pseudo-projective de-
pendency parsing with support vector machines. In
Proc. of the Tenth Conference on Computational Nat-
ural Language Learning, pages 221?225, New York,
USA.
Leon Peshkin and Virginia Savova. 2005. Dependency
parsing with dynamic Bayesian network. In AAAI,
20th National Conference on Artificial Intelligence,
Pittsburgh, Pennsylvania.
Detlef Prescher. 2005. Head-driven PCFGs with latent-
head statistics. In Proc. 9th Int. Workshop on Parsing
Technologies, Vancouver, Canada.
Stefan Riezler, Tracy H. King, Ronald M. Kaplan,
Richard Crouch, John T. Maxwell, and Mark John-
son. 2002. Parsing the Wall Street Journal using a
Lexical-Functional Grammar and discriminative esti-
mation techniques. In Proc. 40th Meeting of Associa-
tion for Computational Linguistics, Philadelphia, PA.
Brian Sallans. 2002. Reinforcement Learning for Fac-
tored Markov Decision Processes. Ph.D. thesis, Uni-
versity of Toronto, Toronto, Canada.
Fei Sha and Fernando Pereira. 2003. Shallow parsing
with conditional random fields. In Proc. joint meet-
ing of North American Chapter of the Association for
Computational Linguistics and the Human Language
Technology Conf., Edmonton, Canada.
Ivan Titov and James Henderson. 2007. Constituent
parsing with incremental sigmoid belief networks. In
Proc. 45th Meeting of Association for Computational
Linguistics, Prague, Czech Republic.
L. van der Beek, G. Bouma, J. Daciuk, T. Gaustad,
R. Malouf, G van Noord, R. Prins, and B. Villada.
2002. The Alpino dependency treebank. Computa-
tional Linguistic in the Netherlands (CLIN).
155
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1354?1359,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Predicting the resolution of referring expressions from user behavior
Nikos Engonopoulos1 Mart??n Villalba1 Ivan Titov2 Alexander Koller1
1University of Potsdam, Germany 2University of Amsterdam, Netherlands
{nikolaos.engonopoulos, martin.villalba}@uni-potsdam.de
titov@uva.nl, koller@ling.uni-potsdam.de
Abstract
We present a statistical model for predicting
how the user of an interactive, situated NLP
system resolved a referring expression. The
model makes an initial prediction based on the
meaning of the utterance, and revises it con-
tinuously based on the user?s behavior. The
combined model outperforms its components
in predicting reference resolution and when to
give feedback.
1 Introduction
Speakers and listeners in natural communication are
engaged in a highly interactive process. In order to
achieve some communicative goal, the speaker will
perform an utterance which they believe has a high
chance of achieving that goal. They will then moni-
tor the listener?s behavior to see whether this goal is
actually being achieved. This process is a core part
of what is commonly called grounding in the dia-
logue literature (see e.g. (Clark, 1996; Traum, 1994;
Paek and Horvitz, 1999; Hirst et al, 1994)). Inter-
active computer systems that are to carry out an ef-
fective and efficient conversation with a user must
model this grounding process, and should ideally re-
spond to the user?s observed behavior in real time.
For instance, if the user of a pedestrian navigation
system takes a wrong turn, the system should inter-
pret this as evidence of misunderstanding and bring
the user back on track.
We focus here on the problem of predicting how
the user has resolved a referring expression (RE) that
was generated by the system, i.e. a noun phrase that
is intended to identify some object uniquely to the
listener. A number of authors have recently offered
statistical models for parts of this problem. Golland
et al (2010) and Garoufi and Koller (2011) have
presented log-linear models for predicting how the
listener will resolve a given RE in a given scene;
however, these models do not update the probabil-
ity model based on observing the user?s reactions.
Nakano et al (2007), Buschmeier and Kopp (2012),
and Koller et al (2012) all predict what the listener
understood based on their behavior, but do not con-
sider the RE itself in the model. The models of
Frank and Goodman (2012) and Vogel et al (2013)
aim at explaining the effect of implicatures on the
listener?s RE resolution process in terms of hypothe-
sized interactions, but do not actually support a real-
time interaction between a system and a user.
In this paper, we show how to predict how the
listener has resolved an RE by combining a statis-
tical model of RE resolution based on the RE itself
with a statistical model of RE resolution based on
the listener?s behavior. To our knowledge, this is
the first approach to combine two such models ex-
plicitly. We consider the RE grounding problem in
the context of interactive, situated natural language
generation (NLG) for the GIVE Challenge (Koller et
al., 2010a), where NLG systems must generate real-
time instructions in virtual 3D environments. Our
evaluation is based on interaction corpora from the
GIVE-2 and GIVE-2.5 Challenges, which contain
the systems? utterances along with the behavior of
human hearers in response to these utterances. We
find that the combined model predicts RE resolu-
tion more accurately than each of the two compo-
nent models alone. We see this as a first step towards
implementing an actual interactive system that per-
forms human-like grounding based on our RE reso-
lution model.
1354
Figure 1: An example scene in the GIVE environment.
2 Problem definition
In the GIVE Challenge, an interactive NLG system
faces the task of guiding a human instruction fol-
lower (IF) through a treasure-hunt game in a vir-
tual 3D environment (see Fig. 1). To complete the
task, the IF must press a number of buttons in the
correct order; these buttons are the colored boxes in
Fig. 1, and are scattered all over the virtual environ-
ment. The IF can move around freely in the virtual
environment, but has no prior knowledge about the
world. The NLG system?s task is to guide the IF to-
wards the successful completion of the treasure-hunt
task. To this end, it is continuously being informed
about the IF?s movements and visual field, and can
generate written utterances at any time. As a com-
parative evaluation effort, the GIVE Challenges con-
nected NLG systems to thousands of users over the
Internet (see e.g. Koller et al (2010a) for details).
Many system utterances are manipulation instruc-
tions, such as ?press the blue button?, containing an
RE in the form of a definite NP. We call a given part
of an interaction between the system and the IF an
episode of that interaction if it starts with a manip-
ulation instruction, ends with the IF performing an
action (i.e., pressing a button), and contains only
IF movements and no further utterances in between.
Not all manipulation instructions initiate an episode,
because the system may decide to perform further
utterances (not containing REs) before the IF per-
forms their action. An NLG system will choose the
RE for an instruction at runtime out of potentially
many semantically valid alternatives (?the blue but-
ton?, ?the button next to the chair?, ?the button to
the right of the red button?, etc.). Ideally, it will pre-
dict which of these REs has the highest chance to be
understood by the IF, given the current scene, and
utter an instruction that uses this RE.
After uttering the manipulation instruction, the
system needs to ascertain whether the IF understood
the RE correctly, i.e. it must engage in grounding.
A naive grounding mechanism might wait until the
IF actually presses a button and check whether it was
the right one. This is what many NLG systems in the
GIVE Challenges actually did. However, this can
make the communication ineffective (IF performs
many useless actions) and risky (IF may press the
wrong button and lose). Thus, it is important that the
system updates its prediction of how the IF resolved
the RE continuously by observing the IF?s behavior,
before the actual button press. For instance, if the
IF walks towards the target, this might reinforce the
system?s belief in a correct understanding; turning
away or exiting the room could be strong evidence
of the opposite. The system can then exploit the up-
dated prediction to give the IF feedback (?no, the
blue button?) to prevent costly mistakes.
We address these challenges by estimating the
probability distribution over the possible objects to
which the IF may resolve the RE. We then update
this distribution in real time by observing the IF?s
movements. More specifically, assume that a sys-
tem tries to refer to some object a? among some set
A of available objects. Given an RE r generated for
a? at time t0, the state of the world s at t0, and the
observed behavior ?(t) of the user at t ? t0, we
estimate the probability p(a|r, s, ?(t)) that the user
resolved r to an object a ? A. When generating the
instruction, an optimal NLG system will use the RE
r that maximizes p(a?|r, s, ?(t0)). It can then track
p(a|r, s, ?(t)) for time points t > t0 throughout the
episode, and generate feedback when p(a?|r, s, ?(t))
exceeds p(a?|r, s, ?(t)) for some a? 6= a?; that is,
when the updated probability distribution predicts
that the IF resolved r to an incorrect button.
3 A model of RE resolution
In order to model the distribution over possible ob-
jects, we assume the following generative story:
when receiving an instruction containing an RE r at
a given world state s, the IF resolves it to an object
a; depending on the object a, the IF then moves to-
wards it, exhibiting behavior ?. These assumptions
correspond to the following factorization:
p(a, ?|r, s) = p(?|a)p(a|r, s)
1355
The posterior probability distribution over objects a
can be obtained by applying the Bayes rule and us-
ing the above assumptions:
p(a|r, s, ?) ? p(a|r, s)p(a|?)/p(a)
For simplicity, we assume a uniform p(a) over all
objects in a world. We can thus represent p(a|r, s, ?)
as the normalized product of a semantic model
psem(a|r, s) and an observational model pobs(a|?).
We use log-linear models for both, and train them
separately. The feature functions we use only con-
sider general properties of objects (such as color and
distance), and not the identity of the objects them-
selves. This means that we can train a model on one
virtual environment (containing a certain set of ob-
jects), and then apply the model to another virtual
environment, containing a different set of objects.
Semantic model The semantic model estimates
for each object a in the environment the initial prob-
ability psem(a|r, s) that the IF will understand a
given RE r uttered in a scene s as referring to a. It
represents the meaning of r, contextualized to s, and
is only ever evaluated at the time t0 of the utterance.
The features used by this model are:
? Semantic features aim to encode whether r
is a good description of a. IsColorModifying
evaluates to 1 if a?s color appears as an adjec-
tive modifying the head noun of r, e.g. ?the
blue button?. IsRelPosModifying evaluates to
1 if a?s relative position to the IF is mentioned
as an adjective in r, e.g. ?the left button?.
? Confusion features capture the hypothesis that
the IF may be confused by the description of a
landmark when resolving the RE; e.g. an RE
like ?the button next to the red button? might
confuse the IF into pressing a red button, rather
than the one meant by the system. These are
the same features as in the Semantic case, but
looking for modifier keywords in the entire RE,
including the head.
? Salience features account for the fact that an
IF is more likely to resolve r to a if a was visu-
ally salient in s. IsVisible evaluates to 1 if a is
visible to the IF in s. IsInRoom evaluates to 1
if the IF and a are in the same room. IsTarget-
InFront evaluates to 1 if the angular distance
towards a, i.e. the absolute angle between the
camera direction and the straight line from the
IF to a, is less than pi4 . VisualSalience approx-
imates the visual salience of Kelleher and van
Genabith (2004), a weighted count of the num-
ber of pixels on which a is rendered (pixels near
the center of the screen have higher weights).
Observational model The observational model
estimates for each object a the probability pobs(a|?)
that the IF will interact with a, given the IF?s recent
behavior ?(t) = (?1, . . . , ?n), where ?i is the state
of the world at time t? (i? 1) ? 500ms, and n ? 1
is the length of the observed behavior. pobs is con-
stantly re-evaluated for times t > t0 as the IF moves
around. pobs uses the following features:
? Linear distance features assume that the clos-
est button is also the one the IF understood. In-
Room returns the number of frames ?i in ? in
which the IF and a are in the same room. But-
tonDistance returns the distance between the IF
and a at ?1 divided by a constant such that the
result never exceeds 1. If a is neither in the
same room nor visible, the feature returns 1.
? Angular distance features analyze the direc-
tion in which the IF looks. TargetInFront re-
turns the angular distance towards a at ?1. An-
gleToTarget returns TargetInFront divided by
pi, or 1 if a is neither in the same room nor
visible. LinearRegAngleTo applies linear re-
gression to a list of observed angular distances
towards a over all frames ?i, and returns the
slope of the regression as a measure of varia-
tion. Negative values indicate that the IF turned
towards a, while positive values mean the op-
posite. If a is neither visible nor in the same
room as the IF at ?i, the angle is set to pi.
? Combined distance feature: a weighted sum
of linear and angular distance towards a, called
overall distance in Koller et al (2012).
? Salience features capture visual salience and
its change over time. Defining VSi as the result
of applying the psem feature VisualSalience to
?i and a, LastVisualSalience returns VSn. Lin-
earRegVisualSalience applies linear regression
to all values VSi and returns the slope as a mea-
sure of change in salience. VisualSalienceSum
returns (?ni=1VSi) ?VS1. This emphasizes the
contribution of VS1, which we assume is the
1356
most reliable predictor of the IF?s intentions.
? Binary features aim to detect concrete behav-
ior patterns: LastIsVisible applies the psem fea-
ture IsVisible to ?1, and IsClose evaluates to 1
if the IF is close enough and correctly oriented
to manipulate a in the GIVE environment at ?1.
4 Evaluation
Data We evaluated our model using data from the
GIVE-2 (Koller et al, 2010b) and the GIVE-2.5
Challenges (Striegnitz et al, 2011), obtained from
GIVE Organizers (2012). These datasets constitute
interaction corpora, in which the IF?s activities in
the virtual environment were recorded along with
the utterances automatically generated by the par-
ticipating NLG systems. The data consists of 1833
games for GIVE-2 and 687 games for GIVE-2.5.
To extract training data for our model from the
GIVE-2.5 data, we first identified moments in the
recorded data where the IF pressed a button. From
these, we discarded all instances from the tutorial
phase of the GIVE game and those that happened
within 200 ms after the previous utterance, as these
clearly didn?t happen in response to it. This yielded
6478 training instances for pobs, each consisting of
? at 1 second before the action, and the button a
which the IF pressed. We chose n = 4 for rep-
resenting ?, except to ensure that the features only
considered IF behavior that happened in response to
an utterance. We achieved this by reducing n for the
first few frames after each utterance, such that the
time of ?n was always after the time of the utter-
ance. Finally, we selected those instances which are
episodes in the sense of Section 2, i.e. those in which
the last utterance before the action contained an RE
r. This gave us 3414 training instances for psem,
each consisting of a, r, the time t0 of the utterance,
and the world state s at time t0.
We obtained test instances from the GIVE-2 data
in the same way. This yielded 5028 instances, each
representing an episode. We chose GIVE-2 for test-
ing because the mean episode length is higher (3.3s,
vs. 2.0s in GIVE-2.5), thus making the evaluation
more challenging. Feature selection was done using
the training data and a similar dataset from Koller et
al. (2012). Note that the test data and training data
are based on distinct sets of three virtual environ-
l
l l
l
0.0
0.2
0.4
0.6
Ac
cur
ac
y
l combined
semantic
observational
KGSC
random visible
(a)
l l
l l
?3 ?2 ?1 0
0.0
0.2
0.4
0.6
Time before action (sec)
Ac
cur
ac
y
l combined
semantic
observational
KGSC
random visible
(b)
Figure 2: Prediction accuracy for (a) all episodes, (b) un-
successful episodes as a function of time.
ments each, and were obtained with different NLG
systems and users. This demonstrates the ability of
our model to generalize to unseen environments.
An example video showing our models? predic-
tions on some training episodes can be found at
http://tinyurl.com/re-demo-v.
Prediction accuracy We first evaluated the abil-
ity of our model to predict the button to which
the IF resolved each RE. For each test instance
?r, s, ?, a?, we compare the object returned by
arg maxa p(a|r, s, ?(t)) to the one manipulated by
the IF. We call the proportion of correctly classified
instances the prediction accuracy.
Fig. 2a compares our model?s prediction accuracy
to that of several baselines. We plot prediction ac-
curacy as a function of the time at which the model
is queried for a prediction, by evaluating at 3s, 2s,
1s, and 0s before the button press. The graph is
based on the 2094 test instances with an episode
length of at least three seconds, to ensure that re-
sults for different prediction times are comparable.
As expected, prediction accuracy increases as we ap-
proach the time of the action. Furthermore, the com-
bined model outperforms both psem and pobs reli-
ably. This indicates that the component models pro-
1357
vide complementary useful information. Our model
also outperforms two more baselines: KGSC pre-
dicts that the IF will press the button with the min-
imal overall distance, which is the distance metric
used by the ?movement-based system? of Koller et
al. (2012); random visible selects a random button
from the ones that are currently visible to the IF.
The fact that this last baseline does not approach 1
at action time suggests that multiple buttons tend to
be visible when the IF presses one, confirming that
the prediction task is not trivial.
Correctly predicting the button that the IF will
press is especially useful, and challenging, in those
cases where the IF pressed a different button than
the one the NLG system intended. Fig. 2b shows
a closer look at the 125 unsuccessful episodes of at
least three seconds in the test data. These tend to
be hard instances, and thus as expected, prediction
accuracy drops for all systems. However, by inte-
grating semantic and observational information, the
combined model compensates better for this than all
other systems, with an accuracy of 37.6% against
31.2% for each individual component.
Feedback appropriateness Second, we evaluated
the ability of our model to predict whether the user
misunderstood the RE and requires feedback. For all
the above models, we assumed a simple feedback
mechanism which predicts that the user misunder-
stood the RE if p(a?) ? p(a?) > ? for some object
a? 6= a?, where ? is a confidence threshold; we used
? = 0.1 here. We can thus test on recorded data in
which no actual feedback can be given anymore.
We evaluated the models on the 848 test episodes
of at least 3s in which the NLG systems logged the
button they tried to refer to. The results are shown
in Fig. 3 in terms of F1 measure. Here precision is
the proportion of instances in which the IF pressed
the wrong button (i.e., where feedback should have
been given) among the instances where the model
actually suggested feedback. Recall is the propor-
tion of instances in which the model suggested feed-
back among the instances where the IF pressed the
wrong button. Again, the combined model outper-
forms its components and the baselines, primarily
due to increased recall. The difference is particu-
larly pronounced early on, which would be useful in
giving timely feedback in an actual real-time system.
l l
l l
?3 ?2 ?1 0
0.0
0.2
0.4
0.6
Time before action (s)
Fee
dba
ck 
F1
l combined
semantic
observational
KGSC
random visible
Figure 3: Feedback F1-measure as a function of time.
5 Conclusion and future work
We presented a statistical model for predicting how
a user will resolve the REs generated by an interac-
tive, situated NLG system. The model continuously
updates an initial estimate based on the meaning of
the RE with a model of the user?s behavior. It out-
performs its components and two baselines on pre-
diction and feedback accuracy.
Our model captures a real-time grounding process
on the part of the interactive system. We thus believe
that it provides a solid foundation for detecting mis-
understandings and generating suitable feedback in
an end-to-end dialogue system. We have presented
our model in terms of a situated dialogue setting,
where clues about what the hearer understood can be
observed directly. However, we believe that the fun-
damental mechanism should apply to other domains
as well. This would amount to finding observable
linguistic and non-linguistic clues of hearer under-
standing that can be used as features of pobs.
The immediate next step for future research is
to extend our model to an implemented end-to-end
situated NLG system for the GIVE Challenge, and
evaluate whether this actually improves task perfor-
mance. This requires, in particular, to compute the
RE that is optimal with respect to psem. We will fur-
thermore improve pobs by switching to a more tem-
porally dynamic probability model.
Acknowledgments. We thank Konstantina
Garoufi and the anonymous reviewers for their
insightful comments and suggestions. The first two
authors were supported by the SFB 632 ?Informa-
tion Structure?; Titov?s work was supported by the
Cluster of Excellence at Saarland University.
1358
References
Hendrik Buschmeier and Stefan Kopp. 2012. Adapting
language production to listener feedback behaviour.
In Proceedings of the Interdisciplinary Workshop on
Feedback Behaviors in Dialog.
Herbert C. Clark. 1996. Using Language. Cambridge
University Press.
Michael C. Frank and Noah D. Goodman. 2012. Predict-
ing pragmatic reasoning in language games. Science,
336(6084):998.
Konstantina Garoufi and Alexander Koller. 2011. Com-
bining symbolic and corpus-based approaches for the
generation of successful referring expressions. In Pro-
ceedings of the 13th European Workshop on Natural
Language Generation (ENLG).
GIVE Organizers. 2012. Give challenge web-
site: Corpora. http://give-challenge.org/
research/page.php?id=corpora.
Dave Golland, Percy Liang, and Dan Klein. 2010. A
game-theoretic approach to generating spatial descrip-
tions. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Processing
(EMNLP).
Graeme Hirst, Susan McRoy, Peter Heeman, Philip Ed-
monds, and Diane Horton. 1994. Repairing conver-
sational misunderstandings and non-understandings.
Speech Communications, 15:213?229.
J. D. Kelleher and J. van Genabith. 2004. Visual salience
and reference resolution in simulated 3-D environ-
ments. Artificial Intelligence Review, 21(3).
Alexander Koller, Kristina Striegnitz, Donna Byron, Jus-
tine Cassell, Robert Dale, Johanna Moore, and Jon
Oberlander. 2010a. The First Challenge on Generat-
ing Instructions in Virtual Environments. In E. Krah-
mer and M. Theune, editors, Empirical Methods in
Natural Language Generation, number 5790 in LNCS,
pages 337?361. Springer.
Alexander Koller, Kristina Striegnitz, Andrew Gargett,
Donna Byron, Justine Cassell, Robert Dale, Johanna
Moore, and Jon Oberlander. 2010b. Report on the
Second NLG Challenge on Generating Instructions in
Virtual Environments (GIVE-2). In Proceedings of the
6th International Natural Language Generation Con-
ference (INLG).
Alexander Koller, Konstantina Garoufi, Maria Staudte,
and Matthew Crocker. 2012. Enhancing referential
success by tracking hearer gaze. In Proceedings of the
13th Annual SIGdial Meeting on Discourse and Dia-
logue (SIGDIAL), Seoul.
Yukiko Nakano, Kazuyoshi Murata, Mika Enomoto,
Yoshiko Arimoto, Yasuhiro Asa, and Hirohiko
Sagawa. 2007. Predicting evidence of understanding
by monitoring user?s task manipulation in multimodal
conversations. In Proceedings of the ACL 2007 Demo
and Poster Sessions.
Tim Paek and Eric Horvitz. 1999. Uncertainty, utility,
and misunderstanding: A decision-theoretic perspec-
tive on grounding in conversational systems. In AAAI
Fall Symposium on Psychological Models of Commu-
nication in Collaborative Systems.
Kristina Striegnitz, Alexandre Denis, Andrew Gargett,
Konstantina Garoufi, Alexander Koller, and Mariet
Theune. 2011. Report on the Second Second Chal-
lenge on Generating Instructions in Virtual Environ-
ments (GIVE-2.5). In Proceedings of the 13th Eu-
ropean Workshop on Natural Language Generation
(ENLG).
David Traum. 1994. A computational theory of ground-
ing in natural language conversation. Ph.D. thesis,
University of Rochester.
Adam Vogel, Christopher Potts, and Dan Jurafsky.
2013. Implicatures and nested beliefs in approximate
Decentralized-POMDPs. In Proceedings of ACL.
1359
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 12?22,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
A Bayesian Approach to Unsupervised Semantic Role Induction
Ivan Titov Alexandre Klementiev
Saarland University
Saarbru?cken, Germany
{titov|aklement}@mmci.uni-saarland.de
Abstract
We introduce two Bayesian models for un-
supervised semantic role labeling (SRL)
task. The models treat SRL as clustering
of syntactic signatures of arguments with
clusters corresponding to semantic roles.
The first model induces these clusterings
independently for each predicate, exploit-
ing the Chinese Restaurant Process (CRP)
as a prior. In a more refined hierarchical
model, we inject the intuition that the clus-
terings are similar across different predi-
cates, even though they are not necessar-
ily identical. This intuition is encoded as
a distance-dependent CRP with a distance
between two syntactic signatures indicating
how likely they are to correspond to a single
semantic role. These distances are automat-
ically induced within the model and shared
across predicates. Both models achieve
state-of-the-art results when evaluated on
PropBank, with the coupled model consis-
tently outperforming the factored counter-
part in all experimental set-ups.
1 Introduction
Semantic role labeling (SRL) (Gildea and Juraf-
sky, 2002), a shallow semantic parsing task, has
recently attracted a lot of attention in the com-
putational linguistic community (Carreras and
Ma`rquez, 2005; Surdeanu et al 2008; Hajic? et
al., 2009). The task involves prediction of predi-
cate argument structure, i.e. both identification of
arguments as well as assignment of labels accord-
ing to their underlying semantic role. For exam-
ple, in the following sentences:
(a) [A0 Mary] opened [A1 the door].
(b) [A0 Mary] is expected to open [A1 the door].
(c) [A1 The door] opened.
(d) [A1 The door] was opened [A0 by Mary].
Mary always takes an agent role (A0) for the pred-
icate open, and door is always a patient (A1).
SRL representations have many potential appli-
cations in natural language processing and have
recently been shown to be beneficial in question
answering (Shen and Lapata, 2007; Kaisser and
Webber, 2007), textual entailment (Sammons et
al., 2009), machine translation (Wu and Fung,
2009; Liu and Gildea, 2010; Wu et al 2011; Gao
and Vogel, 2011), and dialogue systems (Basili et
al., 2009; van der Plas et al 2011), among others.
Though syntactic representations are often predic-
tive of semantic roles (Levin, 1993), the interface
between syntactic and semantic representations is
far from trivial. The lack of simple determinis-
tic rules for mapping syntax to shallow semantics
motivates the use of statistical methods.
Although current statistical approaches have
been successful in predicting shallow seman-
tic representations, they typically require large
amounts of annotated data to estimate model pa-
rameters. These resources are scarce and ex-
pensive to create, and even the largest of them
have low coverage (Palmer and Sporleder, 2010).
Moreover, these models are domain-specific, and
their performance drops substantially when they
are used in a new domain (Pradhan et al 2008).
Such domain specificity is arguably unavoidable
for a semantic analyzer, as even the definitions
of semantic roles are typically predicate specific,
and different domains can have radically different
distributions of predicates (and their senses). The
necessity for a large amounts of human-annotated
data for every language and domain is one of the
major obstacles to the wide-spread adoption of se-
mantic role representations.
These challenges motivate the need for unsu-
pervised methods which, instead of relying on la-
beled data, can exploit large amounts of unlabeled
texts. In this paper, we propose simple and effi-
12
cient hierarchical Bayesian models for this task.
It is natural to split the SRL task into two
stages: the identification of arguments (the iden-
tification stage) and the assignment of semantic
roles (the labeling stage). In this and in much
of the previous work on unsupervised techniques,
the focus is on the labeling stage. Identification,
though an important problem, can be tackled with
heuristics (Lang and Lapata, 2011a; Grenager and
Manning, 2006) or, potentially, by using a super-
vised classifier trained on a small amount of data.
We follow (Lang and Lapata, 2011a), and regard
the labeling stage as clustering of syntactic sig-
natures of argument realizations for every predi-
cate. In our first model, as in most of the previous
work on unsupervised SRL, we define an indepen-
dent model for each predicate. We use the Chi-
nese Restaurant Process (CRP) (Ferguson, 1973)
as a prior for the clustering of syntactic signatures.
The resulting model achieves state-of-the-art re-
sults, substantially outperforming previous meth-
ods evaluated in the same setting.
In the first model, for each predicate we inde-
pendently induce a linking between syntax and se-
mantics, encoded as a clustering of syntactic sig-
natures. The clustering implicitly defines the set
of permissible alternations, or changes in the syn-
tactic realization of the argument structure of the
verb. Though different verbs admit different alter-
nations, some alternations are shared across mul-
tiple verbs and are very frequent (e.g., passiviza-
tion, example sentences (a) vs. (d), or dativiza-
tion: John gave a book to Mary vs. John gave
Mary a book) (Levin, 1993). Therefore, it is nat-
ural to assume that the clusterings should be sim-
ilar, though not identical, across verbs.
Our second model encodes this intuition by re-
placing the CRP prior for each predicate with
a distance-dependent CRP (dd-CRP) prior (Blei
and Frazier, 2011) shared across predicates. The
distance between two syntactic signatures en-
codes how likely they are to correspond to a sin-
gle semantic role. Unlike most of the previous
work exploiting distance-dependent CRPs (Blei
and Frazier, 2011; Socher et al 2011; Duan et al
2007), we do not encode prior or external knowl-
edge in the distance function but rather induce it
automatically within our Bayesian model. The
coupled dd-CRP model consistently outperforms
the factored CRP counterpart across all the experi-
mental settings (with gold and predicted syntactic
parses, and with gold and automatically identified
arguments).
Both models admit efficient inference: the es-
timation time on the Penn Treebank WSJ corpus
does not exceed 30 minutes on a single proces-
sor and the inference algorithm is highly paral-
lelizable, reducing inference time down to sev-
eral minutes on multiple processors. This sug-
gests that the models scale to much larger corpora,
which is an important property for a successful
unsupervised learning method, as unlabeled data
is abundant.
The rest of the paper is structured as follows.
Section 2 begins with a definition of the seman-
tic role labeling task and discuss some specifics
of the unsupervised setting. In Section 3, we de-
scribe CRPs and dd-CRPs, the key components
of our models. In Sections 4 ? 6, we describe
our factored and coupled models and the infer-
ence method. Section 7 provides both evaluation
and analysis. Finally, additional related work is
presented in Section 8.
2 Task Definition
In this work, instead of assuming the availabil-
ity of role annotated data, we rely only on auto-
matically generated syntactic dependency graphs.
While we cannot expect that syntactic structure
can trivially map to a semantic representation
(Palmer et al 2005)1, we can use syntactic cues
to help us in both stages of unsupervised SRL.
Before defining our task, let us consider the two
stages separately.
In the argument identification stage, we imple-
ment a heuristic proposed in (Lang and Lapata,
2011a) comprised of a list of 8 rules, which use
nonlexicalized properties of syntactic paths be-
tween a predicate and a candidate argument to it-
eratively discard non-arguments from the list of
all words in a sentence. Note that inducing these
rules for a new language would require some lin-
guistic expertise. One alternative may be to an-
notate a small number of arguments and train a
classifier with nonlexicalized features instead.
In the argument labeling stage, semantic roles
are represented by clusters of arguments, and la-
beling a particular argument corresponds to decid-
ing on its role cluster. However, instead of deal-
1Although it provides a strong baseline which is diffi-
cult to beat (Grenager and Manning, 2006; Lang and Lapata,
2010; Lang and Lapata, 2011a).
13
ing with argument occurrences directly, we rep-
resent them as predicate specific syntactic signa-
tures, and refer to them as argument keys. This
representation aids our models in inducing high
purity clusters (of argument keys) while reducing
their granularity. We follow (Lang and Lapata,
2011a) and use the following syntactic features to
form the argument key representation:
? Active or passive verb voice (ACT/PASS).
? Argument position relative to predicate
(LEFT/RIGHT).
? Syntactic relation to its governor.
? Preposition used for argument realization.
In the example sentences in Section 1, the argu-
ment keys for candidate arguments Mary for sen-
tences (a) and (d) would be ACT:LEFT:SBJ and
PASS:RIGHT:LGS->by,2 respectively. While
aiming to increase the purity of argument key
clusters, this particular representation will not al-
ways produce a good match: e.g. the door in
sentence (c) will have the same key as Mary in
sentence (a). Increasing the expressiveness of the
argument key representation by flagging intransi-
tive constructions would distinguish that pair of
arguments. However, we keep this particular rep-
resentation, in part to compare with the previous
work.
In this work, we treat the unsupervised seman-
tic role labeling task as clustering of argument
keys. Thus, argument occurrences in the corpus
whose keys are clustered together are assigned the
same semantic role. Note that some adjunct-like
modifier arguments are already explicitly repre-
sented in syntax and thus do not need to be clus-
tered (modifiers AM-TMP, AM-MNR, AM-LOC, and
AM-DIR are encoded as ?syntactic? relations TMP,
MNR, LOC, and DIR, respectively (Surdeanu et al
2008)); instead we directly use the syntactic labels
as semantic roles.
3 Traditional and Distance-dependent
CRPs
The central components of our non-parametric
Bayesian models are the Chinese Restaurant Pro-
cesses (CRPs) and the closely related Dirichlet
Processes (DPs) (Ferguson, 1973).
CRPs define probability distributions over par-
titions of a set of objects. An intuitive metaphor
2LGS denotes a logical subject in a passive construction
(Surdeanu et al 2008).
for describing CRPs is assignment of tables to
restaurant customers. Assume a restaurant with a
sequence of tables, and customers who walk into
the restaurant one at a time and choose a table to
join. The first customer to enter is assigned the
first table. Suppose that when a client number i
enters the restaurant, i ? 1 customers are sitting
at each of the k ? (1, . . . ,K) tables occupied so
far. The new customer is then either seated at one
of theK tables with probability Nki?1+? , whereNk
is the number customers already sitting at table
k, or assigned to a new table with the probability
?
i?1+? . The concentration parameter ? encodes
the granularity of the drawn partitions: the larger
?, the larger the expected number of occupied ta-
bles. Though it is convenient to describe CRP in a
sequential manner, the probability of a seating ar-
rangement is invariant of the order of customers?
arrival, i.e. the process is exchangeable. In our
factored model, we use CRPs as a prior for clus-
tering argument keys, as we explain in Section 4.
Often CRP is used as a part of the Dirich-
let Process mixture model where each subset in
the partition (each table) selects a parameter (a
meal) from some base distribution over parame-
ters. This parameter is then used to generate all
data points corresponding to customers assigned
to the table. The Dirichlet processes (DP) are
closely connected to CRPs: instead of choosing
meals for customers through the described gener-
ative story, one can equivalently draw a distribu-
tion G over meals from DP and then draw a meal
for every customer from G. We refer the reader
to Teh (2010) for details on CRPs and DPs. In
our method, we use DPs to model distributions of
arguments for every role.
In order to clarify how similarities between
customers can be integrated in the generative pro-
cess, we start by reformulating the traditional
CRP in an equivalent form so that distance-
dependent CRP (dd-CRP) can be seen as its gen-
eralization. Instead of selecting a table for each
customer as described above, one can equiva-
lently assume that a customer i chooses one of
the previous customers ci as a partner with prob-
ability 1i?1+? and sits at the same table, or occu-
pies a new table with the probability ?i?1+? . The
transitive closure of this seating-with relation de-
termines the partition.
A generalization of this view leads to the defini-
tion of the distance-dependent CRP. In dd-CRPs,
14
a customer i chooses a partner ci = j with
the probability proportional to some non-negative
score di,j (di,j = dj,i) which encodes a similarity
between the two customers.3 More formally,
p(ci = j|D,?) ?
{
di,j , i 6= j
?, i = j
(1)
where D is the entire similarity graph. This pro-
cess lacks the exchangeability property of the tra-
ditional CRP but efficient approximate inference
with dd-CRP is possible with Gibbs sampling.
For more details on inference with dd-CRPs, we
refer the reader to Blei and Frazier (2011).
Though in previous work dd-CRP was used ei-
ther to encode prior knowledge (Blei and Fra-
zier, 2011) or other external information (Socher
et al 2011), we treat D as a latent variable
drawn from some prior distribution over weighted
graphs. This view provides a powerful approach
for coupling a family of distinct but similar clus-
terings: the family of clusterings can be drawn by
first choosing a similarity graph D for the entire
family and then re-usingD to generate each of the
clusterings independently of each other as defined
by equation (1). In Section 5, we explain how we
use this formalism to encode relatedness between
argument key clusterings for different predicates.
4 Factored Model
In this section we describe the factored method
which models each predicate independently. In
Section 2 we defined our task as clustering of ar-
gument keys, where each cluster corresponds to a
semantic role. If an argument key k is assigned
to a role r (k ? r), all of its occurrences are la-
beled r.
Our Bayesian model encodes two common as-
sumptions about semantic roles. First, we enforce
the selectional restriction assumption: we assume
that the distribution over potential argument fillers
is sparse for every role, implying that ?peaky? dis-
tributions of arguments for each role r are pre-
ferred to flat distributions. Second, each role nor-
mally appears at most once per predicate occur-
rence. Our inference will search for a clustering
which meets the above requirements to the maxi-
mal extent.
3It may be more standard to use a decay function f :
R ? R and choose a partner with the probability propor-
tional to f(?di,j). However, the two forms are equivalent
and using scores di,j directly is more convenient for our in-
duction purposes.
Our model associates two distributions with
each predicate: one governs the selection of argu-
ment fillers for each semantic role, and the other
models (and penalizes) duplicate occurrence of
roles. Each predicate occurrence is generated in-
dependently given these distributions. Let us de-
scribe the model by first defining how the set of
model parameters and an argument key clustering
are drawn, and then explaining the generation of
individual predicate and argument instances. The
generative story is formally presented in Figure 1.
We start by generating a partition of argument
keys Bp with each subset r ? Bp representing
a single semantic role. The partitions are drawn
from CRP(?) (see the Factored model section of
Figure 1) independently for each predicate. The
crucial part of the model is the set of selectional
preference parameters ?p,r, the distributions of ar-
guments x for each role r of predicate p. We
represent arguments by their syntactic heads,4 or
more specifically, by either their lemmas or word
clusters assigned to the head by an external clus-
tering algorithm, as we will discuss in more detail
in Section 7.5 For the agent role A0 of the pred-
icate open, for example, this distribution would
assign most of the probability mass to arguments
denoting sentient beings, whereas the distribution
for the patient role A1 would concentrate on ar-
guments representing ?openable? things (doors,
boxes, books, etc).
In order to encode the assumption about sparse-
ness of the distributions ?p,r, we draw them from
the DP prior DP (?,H(A)) with a small concen-
tration parameter ?, the base probability distribu-
tionH(A) is just the normalized frequencies of ar-
guments in the corpus. The geometric distribution
?p,r is used to model the number of times a role
r appears with a given predicate occurrence. The
decision whether to generate at least one role r is
drawn from the uniform Bernoulli distribution. If
0 is drawn then the semantic role is not realized
for the given occurrence, otherwise the number
of additional roles r is drawn from the geometric
distribution Geom(?p,r). The Beta priors over ?
4For prepositional phrases, we take as head the head noun
of the object noun phrase as it encodes crucial lexical infor-
mation. However, the preposition is not ignored but rather
encoded in the corresponding argument key, as explained
in Section 2.
5Alternatively, the clustering of arguments could be in-
duced within the model, as done in (Titov and Klementiev,
2011).
15
Clustering of argument keys:
Factored model:
for each predicate p = 1, 2, . . . :
Bp ? CRP (?) [partition of arg keys]
Coupled model:
D ? NonInform [similarity graph]
for each predicate p = 1, 2, . . . :
Bp ? dd-CRP (?,D) [partition of arg keys]
Parameters:
for each predicate p = 1, 2, . . . :
for each role r ? Bp:
?p,r ? DP (?,H(A)) [distrib of arg fillers]
?p,r ? Beta(?0, ?1) [geom distr for dup roles]
Data Generation:
for each predicate p = 1, 2, . . . :
for each occurrence l of p:
for every role r ? Bp:
if [n ? Unif(0, 1)] = 1: [role appears at least once]
GenArgument(p, r) [draw one arg]
while [n ? ?p,r] = 1: [continue generation]
GenArgument(p, r) [draw more args]
GenArgument(p, r):
kp,r ? Unif(1, . . . , |r|) [draw arg key]
xp,r ? ?p,r [draw arg filler]
Figure 1: Generative stories for the factored and cou-
pled models.
can indicate the preference towards generating at
most one argument for each role. For example,
it would express the preference that a predicate
open typically appears with a single agent and a
single patient arguments.
Now, when parameters and argument key clus-
terings are chosen, we can summarize the re-
mainder of the generative story as follows. We
begin by independently drawing occurrences for
each predicate. For each predicate role we in-
dependently decide on the number of role occur-
rences. Then we generate each of the arguments
(see GenArgument) by generating an argument
key kp,r uniformly from the set of argument keys
assigned to the cluster r, and finally choosing its
filler xp,r, where the filler is either a lemma or a
word cluster corresponding to the syntactic head
of the argument.
5 Coupled Model
As we argued in Section 1, clusterings of argu-
ment keys implicitly encode the pattern of alter-
nations for a predicate. E.g., passivization can be
roughly represented with the clustering of the key
ACT:LEFT:SBJ with PASS:RIGHT:LGS->by
and ACT:RIGHT:OBJ with PASS:LEFT:SBJ.
The set of permissible alternations is predicate-
specific,6 but nevertheless they arguably repre-
sent a small subset of all clusterings of argu-
ment keys. Also, some alternations are more
likely to be applicable to a verb than others: for
example, passivization and dativization alterna-
tions are both fairly frequent, whereas, locative-
preposition-drop alternation (Mary climbed up the
mountain vs. Mary climbed the mountain) is less
common and applicable only to several classes
of predicates representing motion (Levin, 1993).
We represent this observation by quantifying how
likely a pair of keys is to be clustered. These
scores (di,j for every pair of argument keys i and
j) are induced automatically within the model,
and treated as latent variables shared across pred-
icates. Intuitively, if data for several predicates
strongly suggests that two argument keys should
be clustered (e.g., there is a large overlap be-
tween argument fillers for the two keys) then the
posterior will indicate that di,j is expected to be
greater for the pair {i, j} than for some other pair
{i?, j?} for which the evidence is less clear. Con-
sequently, argument keys i and j will be clustered
even for predicates without strong evidence for
such a clustering, whereas i? and j? will not.
One argument against coupling predicates may
stem from the fact that we are using unlabeled
data and may be able to obtain sufficient amount
of learning material even for less frequent pred-
icates. This may be a valid observation, but an-
other rationale for sharing this similarity structure
is the hypothesis that alternations may be easier
to detect for some predicates than for others. For
example, argument key clustering of predicates
with very restrictive selectional restrictions on ar-
gument fillers is presumably easier than clustering
for predicates with less restrictive and overlap-
ping selectional restriction, as compactness of se-
lectional preferences is a central assumption driv-
ing unsupervised learning of semantic roles. E.g.,
predicates change and defrost belong to the same
Levin class (change-of-state verbs) and therefore
admit similar alternations. However, the set of po-
tential patients of defrost is sufficiently restricted,
6Or, at least specific to a class of predicates (Levin,
1993).
16
whereas the selectional restrictions for the patient
of change are far less specific and they overlap
with selectional restrictions for the agent role, fur-
ther complicating the clustering induction task.
This observation suggests that sharing clustering
preferences across verbs is likely to help even if
the unlabeled data is plentiful for every predicate.
More formally, we generate scores di,j , or
equivalently, the full labeled graph D with ver-
tices corresponding to argument keys and edges
weighted with the similarity scores, from a prior.
In our experiments we use a non-informative prior
which factorizes over pairs (i.e. edges of the
graph D), though more powerful alternatives can
be considered. Then we use it, in a dd-CRP(?,
D), to generate clusterings of argument keys for
every predicate. The rest of the generative story is
the same as for the factored model. The part rele-
vant to this model is shown in the Coupled model
section of Figure 1.
Note that this approach does not assume that
the frequencies of syntactic patterns correspond-
ing to alternations are similar, and a large value
for di,j does not necessarily mean that the corre-
sponding syntactic frames i and j are very fre-
quent in a corpus. What it indicates is that a large
number of different predicates undergo the corre-
sponding alternation; the frequency of the alterna-
tion is a different matter. We believe that this is an
important point, as we do not make a restricting
assumption that an alternation has the same dis-
tributional properties for all verbs which undergo
this alternation.
6 Inference
An inference algorithm for an unsupervised
model should be efficient enough to handle vast
amounts of unlabeled data, as it can easily be ob-
tained and is likely to improve results. We use
a simple approximate inference algorithm based
on greedy MAP search. We start by discussing
MAP search for argument key clustering with the
factored model and then discuss its extension ap-
plicable to the coupled model.
6.1 Role Induction
For the factored model, semantic roles for every
predicate are induced independently. Neverthe-
less, search for a MAP clustering can be expen-
sive, as even a move involving a single argument
key implies some computations for all its occur-
rences in the corpus. Instead of more complex
MAP search algorithms (see, e.g., (Daume III,
2007)), we use a greedy procedure where we start
with each argument key assigned to an individual
cluster, and then iteratively try to merge clusters.
Each move involves (1) choosing an argument key
and (2) deciding on a cluster to reassign it to. This
is done by considering all clusters (including cre-
ating a new one) and choosing the most probable
one.
Instead of choosing argument keys randomly at
the first stage, we order them by corpus frequency.
This ordering is beneficial as getting clustering
right for frequent argument keys is more impor-
tant and the corresponding decisions should be
made earlier.7 We used a single iteration in our
experiments, as we have not noticed any benefit
from using multiple iterations.
6.2 Similarity Graph Induction
In the coupled model, clusterings for different
predicates are statistically dependent, as the simi-
larity structureD is latent and shared across pred-
icates. Consequently, a more complex inference
procedure is needed. For simplicity here and in
our experiments, we use the non-informative prior
distribution over D which assigns the same prior
probability to every possible weight di,j for every
pair {i, j}.
Recall that the dd-CRP prior is defined in terms
of customers choosing other customers to sit with.
For the moment, let us assume that this relation
among argument keys is known, that is, every ar-
gument key k for predicate p has chosen an argu-
ment key cp,k to ?sit? with. We can compute the
MAP estimate for all di,j by maximizing the ob-
jective:
argmax
di,j , i 6=j
?
p
?
k?Kp
log
dk,cp,k
?
k??Kp dk,k?
,
where Kp is the set of all argument keys for the
predicate p. We slightly abuse the notation by us-
ing di,i to denote the concentration parameter ?
in the previous expression. Note that we also as-
sume that similarities are symmetric, di,j = dj,i.
If the set of argument keysKp would be the same
for every predicate, then the optimal di,j would
7This idea has been explored before for shallow semantic
representations (Lang and Lapata, 2011a; Titov and Klemen-
tiev, 2011).
17
be proportional to the number of times either i se-
lects j as a partner, or j chooses i as a partner.8
This no longer holds if the sets are different, but
the solution can be found efficiently using a nu-
meric optimization strategy; we use the gradient
descent algorithm.
We do not learn the concentration parameter
?, as it is used in our model to indicate the de-
sired granularity of semantic roles, but instead
only learn di,j (i 6= j). However, just learning
the concentration parameter would not be suffi-
cient as the effective concentration can be reduced
or increased arbitrarily by scaling all the similar-
ities di,j (i 6= j) at once, as follows from expres-
sion (1). Instead, we enforce the normalization
constraint on the similarities di,j . We ensure that
the prior probability of choosing itself as a part-
ner, averaged over predicates, is the same as it
would be with uniform di,j (di,j = 1 for every
key pair {i, j}, i 6= j). This roughly says that
we want to preserve the same granularity of clus-
tering as it was with the uniform similarities. We
accomplish this normalization in a post-hoc fash-
ion by dividing the weights after optimization by
?
p
?
k,k??Kp, k? 6=k dk,k?/
?
p |Kp|(|Kp| ? 1).
If D is fixed, partners for every predicate p and
every k can be found using virtually the same al-
gorithm as in Section 6.1: the only difference is
that, instead of a cluster, each argument key itera-
tively chooses a partner.
Though, in practice, both the choice of partners
and the similarity graphs are latent, we can use an
iterative approach to obtain a joint MAP estimate
of ck (for every k) and the similarity graph D by
alternating the two steps.9
Notice that the resulting algorithm is again
highly parallelizable: the graph induction stage
is fast, and induction of the seat-with relation
(i.e. clustering argument keys) is factorizable over
predicates.
One shortcoming of this approach is typical
for generative models with multiple ?features?:
when such a model predicts a latent variable, it
tends to ignore the prior class distribution and re-
lies solely on features. This behavior is due to
the over-simplifying independence assumptions.
It is well known, for instance, that the poste-
8Note that weights di,j are invariant under rescaling
when the rescaling is also applied to the concentration pa-
rameter ?.
9In practice, two iterations were sufficient.
rior with Naive Bayes tends to be overconfident
due to violated conditional independence assump-
tions (Rennie, 2001). The same behavior is ob-
served here: the shared prior does not have suf-
ficient effect on frequent predicates.10 Though
different techniques have been developed to dis-
count the over-confidence (Kolcz and Chowdhury,
2005), we use the most basic one: we raise the
likelihood term in power 1T , where the parameter
T is chosen empirically.
7 Empirical Evaluation
7.1 Data and Evaluation
We keep the general setup of (Lang and Lapata,
2011a), to evaluate our models and compare them
to the current state of the art. We run all of our
experiments on the standard CoNLL 2008 shared
task (Surdeanu et al 2008) version of Penn Tree-
bank WSJ and PropBank. In addition to gold
dependency analyses and gold PropBank annota-
tions, it has dependency structures generated au-
tomatically by the MaltParser (Nivre et al 2007).
We vary our experimental setup as follows:
? We evaluate our models on gold and auto-
matically generated parses, and use either
gold PropBank annotations or the heuristic
from Section 2 to identify arguments, result-
ing in four experimental regimes.
? In order to reduce the sparsity of predicate
argument fillers we consider replacing lem-
mas of their syntactic heads with word clus-
ters induced by a clustering algorithm as a
preprocessing step. In particular, we use
Brown (Br) clustering (Brown et al 1992)
induced over RCV1 corpus (Turian et al
2010). Although the clustering is hierarchi-
cal, we only use a cluster at the lowest level
of the hierarchy for each word.
We use the purity (PU) and collocation (CO) met-
rics as well as their harmonic mean (F1) to mea-
sure the quality of the resulting clusters. Purity
measures the degree to which each cluster con-
tains arguments sharing the same gold role:
PU =
1
N
?
i
max
j
|Gj ? Ci|
where if Ci is the set of arguments in the i-th in-
duced cluster,Gj is the set of arguments in the jth
10The coupled model without discounting still outper-
forms the factored counterpart in our experiments.
18
gold cluster, and N is the total number of argu-
ments. Collocation evaluates the degree to which
arguments with the same gold roles are assigned
to a single cluster. It is computed as follows:
CO =
1
N
?
j
max
i
|Gj ? Ci|
We compute the aggregate PU, CO, and F1
scores over all predicates in the same way as
(Lang and Lapata, 2011a) by weighting the scores
of each predicate by the number of its argument
occurrences. Note that since our goal is to evalu-
ate the clustering algorithms, we do not include
incorrectly identified arguments (i.e. mistakes
made by the heuristic defined in Section 2) when
computing these metrics.
We evaluate both factored and coupled models
proposed in this work with and without Brown
word clustering of argument fillers (Factored,
Coupled, Factored+Br, Coupled+Br). Our mod-
els are robust to parameter settings, they were
tuned (to an order of magnitude) on the develop-
ment set and were the same for all model variants:
? = 1.e-3, ? = 1.e-3, ?0 = 1.e-3, ?1 = 1.e-10,
T = 5. Although they can be induced within the
model, we set them by hand to indicate granular-
ity preferences. We compare our results with the
following alternative approaches. The syntactic
function baseline (SyntF) simply clusters predi-
cate arguments according to the dependency re-
lation to their head. Following (Lang and Lapata,
2010), we allocate a cluster for each of 20 most
frequent relations in the CoNLL dataset and one
cluster for all other relations. We also compare
our performance with the Latent Logistic classifi-
cation (Lang and Lapata, 2010), Split-Merge clus-
tering (Lang and Lapata, 2011a), and Graph Parti-
tioning (Lang and Lapata, 2011b) approaches (la-
beled LLogistic, SplitMerge, and GraphPart, re-
spectively) which achieve the current best unsu-
pervised SRL results in this setting.
7.2 Results
7.2.1 Gold Arguments
Experimental results are summarized in Ta-
ble 1. We begin by comparing our models to the
three existing clustering approaches on gold syn-
tactic parses, and using gold PropBank annota-
tions to identify predicate arguments. In this set of
experiments we measure the relative performance
of argument clustering, removing the identifica-
gold parses auto parses
PU CO F1 PU CO F1
LLogistic 79.5 76.5 78.0 77.9 74.4 76.2
SplitMerge 88.7 73.0 80.1 86.5 69.8 77.3
GraphPart 88.6 70.7 78.6 87.4 65.9 75.2
Factored 88.1 77.1 82.2 85.1 71.8 77.9
Coupled 89.3 76.6 82.5 86.7 71.2 78.2
Factored+Br 86.8 78.8 82.6 83.8 74.1 78.6
Coupled+Br 88.7 78.1 83.0 86.2 72.7 78.8
SyntF 81.6 77.5 79.5 77.1 70.9 73.9
Table 1: Argument clustering performance with gold
argument identification. Bold-face is used to highlight
the best F1 scores.
tion stage, and minimize the noise due to auto-
matic syntactic annotations. All four variants of
the models we propose substantially outperform
other models: the coupled model with Brown
clustering of argument fillers (Coupled+Br) beats
the previous best model SplitMerge by 2.9% F1
score. As mentioned in Section 2, our approach
specifically does not cluster some of the modifier
arguments. In order to verify that this and argu-
ment filler clustering were not the only aspects
of our approach contributing to performance im-
provements, we also evaluated our coupled model
without Brown clustering and treating modifiers
as regular arguments. The model achieves 89.2%
purity, 74.0% collocation, and 80.9% F1 scores,
still substantially outperforming all of the alter-
native approaches. Replacing gold parses with
MaltParser analyses we see a similar trend, where
Coupled+Br outperforms the best alternative ap-
proach SplitMerge by 1.5%.
7.2.2 Automatic Arguments
Results are summarized in Table 2.11 The
precision and recall of our re-implementation of
the argument identification heuristic described in
Section 2 on gold parses were 87.7% and 88.0%,
respectively, and do not quite match 88.1% and
87.9% reported in (Lang and Lapata, 2011a).
Since we could not reproduce their argument
identification stage exactly, we are omitting their
results for the two regimes, instead including the
results for our two best models Factored+Br and
Coupled+Br. We see a similar trend, where the
coupled system consistently outperforms its fac-
tored counterpart, achieving 85.8% and 83.9% F1
11Note, that the scores are computed on correctly iden-
tified arguments only, and tend to be higher in these ex-
periments probably because the complex arguments get dis-
carded by the heuristic.
19
gold parses auto parses
PU CO F1 PU CO F1
Factored+Br 87.8 82.9 85.3 85.8 81.1 83.4
Coupled+Br 89.2 82.6 85.8 87.4 80.7 83.9
SyntF 83.5 81.4 82.4 81.4 79.1 80.2
Table 2: Argument clustering performance with auto-
matic argument identification.
for gold and MaltParser analyses, respectively.
We observe that consistently through the four
regimes, sharing of alternations between predi-
cates captured by the coupled model outperforms
the factored version, and that reducing the argu-
ment filler sparsity with clustering also has a sub-
stantial positive effect. Due to the space con-
straints we are not able to present detailed anal-
ysis of the induced similarity graph D, however,
argument-key pairs with the highest induced sim-
ilarity encode, among other things, passivization,
benefactive alternations, near-interchangeability
of some subordinating conjunctions and preposi-
tions (e.g., if and whether), as well as, restoring
some of the unnecessary splits introduced by the
argument key definition (e.g., semantic roles for
adverbials do not normally depend on whether the
construction is passive or active).
8 Related Work
Most of SRL research has focused on the super-
vised setting (Carreras and Ma`rquez, 2005; Sur-
deanu et al 2008), however, lack of annotated re-
sources for most languages and insufficient cover-
age provided by the existing resources motivates
the need for using unlabeled data or other forms
of weak supervision. This work includes methods
based on graph alignment between labeled and
unlabeled data (Fu?rstenau and Lapata, 2009), us-
ing unlabeled data to improve lexical generaliza-
tion (Deschacht and Moens, 2009), and projection
of annotation across languages (Pado and Lapata,
2009; van der Plas et al 2011). Semi-supervised
and weakly-supervised techniques have also been
explored for other types of semantic representa-
tions but these studies have mostly focused on re-
stricted domains (Kate and Mooney, 2007; Liang
et al 2009; Titov and Kozhevnikov, 2010; Gold-
wasser et al 2011; Liang et al 2011).
Unsupervised learning has been one of the cen-
tral paradigms for the closely-related area of re-
lation extraction, where several techniques have
been proposed to cluster semantically similar ver-
balizations of relations (Lin and Pantel, 2001;
Banko et al 2007). Early unsupervised ap-
proaches to the SRL problem include the work
by Swier and Stevenson (2004), where the Verb-
Net verb lexicon was used to guide unsupervised
learning, and a generative model of Grenager and
Manning (2006) which exploits linguistic priors
on syntactic-semantic interface.
More recently, the role induction problem has
been studied in Lang and Lapata (2010) where
it has been reformulated as a problem of detect-
ing alterations and mapping non-standard link-
ings to the canonical ones. Later, Lang and La-
pata (2011a) proposed an algorithmic approach
to clustering argument signatures which achieves
higher accuracy and outperforms the syntactic
baseline. In Lang and Lapata (2011b), the role
induction problem is formulated as a graph parti-
tioning problem: each vertex in the graph corre-
sponds to a predicate occurrence and edges repre-
sent lexical and syntactic similarities between the
occurrences. Unsupervised induction of seman-
tics has also been studied in Poon and Domin-
gos (2009) and Titov and Klementiev (2010) but
the induced representations are not entirely com-
patible with the PropBank-style annotations and
they have been evaluated only on a question an-
swering task for the biomedical domain. Also, the
related task of unsupervised argument identifica-
tion was considered in Abend et al(2009).
9 Conclusions
In this work we introduced two Bayesian models
for unsupervised role induction. They treat the
task as a family of related clustering problems,
one for each predicate. The first factored model
induces each clustering independently, whereas
the second model couples them by exploiting a
novel technique for sharing clustering preferences
across a family of clusterings. Both methods
achieve state-of-the-art results with the coupled
model outperforming the factored counterpart in
all regimes.
Acknowledgements
The authors acknowledge the support of the MMCI
Cluster of Excellence, and thank Hagen Fu?rstenau,
Mikhail Kozhevnikov, Alexis Palmer, Manfred Pinkal,
Caroline Sporleder and the anonymous reviewers for
their suggestions, and Joel Lang for answering ques-
tions about their methods and data.
20
References
Omri Abend, Roi Reichart, and Ari Rappoport. 2009.
Unsupervised argument identification for semantic
role labeling. In ACL-IJCNLP.
Michele Banko, Michael J Cafarella, Stephen Soder-
land, Matt Broadhead, and Oren Etzioni. 2007.
Open information extraction from the web. In IJ-
CAI.
Roberto Basili, Diego De Cao, Danilo Croce,
Bonaventura Coppola, and Alessandro Moschitti.
2009. Cross-language frame semantics transfer in
bilingual corpora. In CICLING.
David M. Blei and Peter Frazier. 2011. Distance de-
pendent chinese restaurant processes. Journal of
Machine Learning Research, 12:2461?2488.
Peter F. Brown, Vincent Della Pietra, Peter V. deSouza,
Jenifer C. Lai, and Robert L. Mercer. 1992. Class-
based n-gram models for natural language. Compu-
tational Linguistics, 18(4):467?479.
Xavier Carreras and Llu??s Ma`rquez. 2005. Intro-
duction to the CoNLL-2005 Shared Task: Semantic
Role Labeling. In CoNLL.
Hal Daume III. 2007. Fast search for dirichlet process
mixture models. In AISTATS.
Koen Deschacht and Marie-Francine Moens. 2009.
Semi-supervised semantic role labeling using the
Latent Words Language Model. In EMNLP.
Jason Duan, Michele Guindani, and Alan Gelfand.
2007. Generalized spatial dirichlet process models.
Biometrika, 94:809?825.
Thomas S. Ferguson. 1973. A Bayesian analysis
of some nonparametric problems. The Annals of
Statistics, 1(2):209?230.
Hagen Fu?rstenau and Mirella Lapata. 2009. Graph
alignment for semi-supervised semantic role label-
ing. In EMNLP.
Qin Gao and Stephan Vogel. 2011. Corpus expansion
for statistical machine translation with semantic role
label substitution rules. In ACL:HLT.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic
labelling of semantic roles. Computational Linguis-
tics, 28(3):245?288.
Dan Goldwasser, Roi Reichart, James Clarke, and Dan
Roth. 2011. Confidence driven unsupervised se-
mantic parsing. In ACL.
Trond Grenager and Christoph Manning. 2006. Unsu-
pervised discovery of a statistical verb lexicon. In
EMNLP.
Jan Hajic?, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Anto`nia Mart??, Llu??s
Ma`rquez, Adam Meyers, Joakim Nivre, Sebastian
Pado?, Jan S?te?pa?nek, Pavel Stran?a?k, Mihai Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The CoNLL-
2009 shared task: Syntactic and semantic depen-
dencies in multiple languages. In Proceedings
of the 13th Conference on Computational Natural
Language Learning (CoNLL-2009), June 4-5.
Michael Kaisser and Bonnie Webber. 2007. Question
answering based on semantic roles. In ACL Work-
shop on Deep Linguistic Processing.
Rohit J. Kate and Raymond J. Mooney. 2007. Learn-
ing language semantics from ambigous supervision.
In AAAI.
Aleksander Kolcz and Abdur Chowdhury. 2005. Dis-
counting over-confidence of naive bayes in high-
recall text classification. In ECML.
Joel Lang and Mirella Lapata. 2010. Unsupervised
induction of semantic roles. In ACL.
Joel Lang and Mirella Lapata. 2011a. Unsupervised
semantic role induction via split-merge clustering.
In ACL.
Joel Lang and Mirella Lapata. 2011b. Unsupervised
semantic role induction with graph partitioning. In
EMNLP.
Beth Levin. 1993. English Verb Classes and Alter-
nations: A Preliminary Investigation. University of
Chicago Press.
Percy Liang, Michael I. Jordan, and Dan Klein. 2009.
Learning semantic correspondences with less super-
vision. In ACL-IJCNLP.
Percy Liang, Michael Jordan, and Dan Klein. 2011.
Learning dependency-based compositional seman-
tics. In ACL: HLT.
Dekang Lin and Patrick Pantel. 2001. DIRT ? discov-
ery of inference rules from text. In KDD.
Ding Liu and Daniel Gildea. 2010. Semantic role fea-
tures for machine translation. In Coling.
J. Nivre, J. Hall, S. Ku?bler, R. McDonald, J. Nilsson,
S. Riedel, and D. Yuret. 2007. The CoNLL 2007
shared task on dependency parsing. In EMNLP-
CoNLL.
Sebastian Pado and Mirella Lapata. 2009. Cross-
lingual annotation projection for semantic roles.
Journal of Artificial Intelligence Research, 36:307?
340.
Alexis Palmer and Caroline Sporleder. 2010. Evalu-
ating FrameNet-style semantic parsing: the role of
coverage gaps in FrameNet. In COLING.
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The
proposition bank: An annotated corpus of semantic
roles. Computational Linguistics, 31(1):71?106.
Hoifung Poon and Pedro Domingos. 2009. Unsuper-
vised semantic parsing. In EMNLP.
Sameer Pradhan, Wayne Ward, and James H. Martin.
2008. Towards robust semantic role labeling. Com-
putational Linguistics, 34:289?310.
Jason Rennie. 2001. Improving multi-class text
classification with Naive bayes. Technical Report
AITR-2001-004, MIT.
M. Sammons, V. Vydiswaran, T. Vieira, N. Johri,
M. Chang, D. Goldwasser, V. Srikumar, G. Kundu,
Y. Tu, K. Small, J. Rule, Q. Do, and D. Roth. 2009.
Relation alignment for textual entailment recogni-
tion. In Text Analysis Conference (TAC).
21
Dan Shen and Mirella Lapata. 2007. Using semantic
roles to improve question answering. In EMNLP.
Richard Socher, Andrew Maas, and Christopher Man-
ning. 2011. Spectral chinese restaurant processes:
Nonparametric clustering based on similarities. In
AISTATS.
Mihai Surdeanu, Adam Meyers Richard Johansson,
Llu??s Ma`rquez, and Joakim Nivre. 2008. The
CoNLL-2008 shared task on joint parsing of syn-
tactic and semantic dependencies. In CoNLL 2008:
Shared Task.
Richard Swier and Suzanne Stevenson. 2004. Unsu-
pervised semantic role labelling. In EMNLP.
Yee Whye Teh. 2010. Dirichlet processes. In Ency-
clopedia of Machine Learning. Springer.
Ivan Titov and Alexandre Klementiev. 2011. A
Bayesian model for unsupervised semantic parsing.
In ACL.
Ivan Titov and Mikhail Kozhevnikov. 2010.
Bootstrapping semantic analyzers from non-
contradictory texts. In ACL.
Joseph Turian, Lev Ratinov, and Yoshua Bengio.
2010. Word representations: A simple and general
method for semi-supervised learning. In ACL.
Lonneke van der Plas, Paola Merlo, and James Hen-
derson. 2011. Scaling up automatic cross-lingual
semantic role annotation. In ACL.
Dekai Wu and Pascale Fung. 2009. Semantic roles for
SMT: A hybrid two-pass model. In NAACL.
Dekai Wu, Marianna Apidianaki, Marine Carpuat, and
Lucia Specia, editors. 2011. Proc. of Fifth Work-
shop on Syntax, Semantics and Structure in Statisti-
cal Translation. ACL.
22
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 49?57,
Gothenburg, Sweden, April 26-30 2014.
c
?2014 Association for Computational Linguistics
A Hierarchical Bayesian Model for
Unsupervised Induction of Script Knowledge
Lea Frermann
1
l.frermann@ed.ac.uk
Ivan Titov
2
titov@uva.nl
1
ILCC, School of Informatics, University of Edinburgh, United Kingdom
2
ILLC, University of Amsterdam, Netherlands
3
Department of Computational Linguistics, Saarland University, Germany
Manfred Pinkal
3
pinkal@coli.uni-sb.de
Abstract
Scripts representing common sense
knowledge about stereotyped sequences
of events have been shown to be a valu-
able resource for NLP applications. We
present a hierarchical Bayesian model for
unsupervised learning of script knowledge
from crowdsourced descriptions of human
activities. Events and constraints on event
ordering are induced jointly in one unified
framework. We use a statistical model
over permutations which captures event
ordering constraints in a more flexible
way than previous approaches. In order
to alleviate the sparsity problem caused
by using relatively small datasets, we
incorporate in our hierarchical model an
informed prior on word distributions. The
resulting model substantially outperforms
a state-of-the-art method on the event
ordering task.
1 Introduction
A script is a ?predetermined, stereotyped se-
quence of actions that define a well-known sit-
uation? (Schank and Abelson, 1975). While
humans acquire such common-sense knowledge
over their lifetime, it constitutes a bottleneck for
many NLP systems. Effective question answer-
ing and summarization are impossible without a
form of story understanding, which in turn has
been shown to benefit from access to databases of
script knowledge (Mueller, 2004; Miikkulainen,
1995). Knowledge about the typical ordering of
events can further help assessing document co-
herence and generating coherent text. Here, we
present a general method for acquiring data bases
of script knowledge.
Our work may be regarded as complementary to
existing work on learning script knowledge from
natural text (cf. (Chambers and Jurafsky, 2008)),
as not all types of scripts are elaborated in natural
text ? being left implicit because of assumed read-
ers? world knowledge. Our model, operating on
data obtained in a cheap way by crowdsourcing,
is applicable to any kind of script and can fill this
gap. We follow work in inducing script knowl-
edge from explicit instantiations of scripts, so-
called event sequence descriptions (ESDs) (Reg-
neri et al., 2010). Our data consists of sets of
ESDs, each set describing a well-known situation
we will call scenario (e.g., ?washing laundry?).
An ESD consists of a sequence of events, each
describing an action defining part of the scenario
(e.g., ?place the laundry in the washing machine?).
We refer to descriptions of the same event across
ESDs as event types. We refer to entities involved
in a scenario as participants (e.g., a ?washing ma-
chine? or a ?detergent?), and to sets of participant
descriptions describing the same entity as partici-
pant types.
For each type of scenario, our model clusters
descriptions which refer to the same type of event,
and infers constraints on the temporal order in
which the events types occur in a particular sce-
nario. Common characteristics of ESDs such as
event optionality and varying degrees of temporal
flexibility of event types make this task nontrivial.
We propose a model which, in contrast to previ-
ous approaches, explicitly targets these character-
istics. We develop a Bayesian formulation of the
script learning problem, and present a generative
model for joint learning of event types and order-
ing constraints, arguing that the temporal position
of an event in an ESD provides a strong cue for its
type, and vice versa. Our model is unsupervised
in that no event- or participant labels are required
for training.
We model constraints on the order of event
types using a statistical model over permutations,
the Generalized Mallows Model (GMM; Fligner
49
and Verducci (1986)). With the GMM we can flex-
ibly model apparent characteristics of scripts, such
as event type-specific temporal flexibility. Assum-
ing that types of participants provide a strong cue
for the type of event they are observed in, we use
participant types as a latent variable in our model.
Finally, by modeling event type occurrence using
Binomial distributions, we can model event op-
tionality, a characteristic of scripts that previous
approaches did not capture.
We evaluate our model on a data set of ESDs
collected via web experiments from non-expert
annotators by Regneri et al. (2010) and compare
our model against their approach. Our model
achieves an absolute average improvement of 7%
over the model of Regneri et al. on the task of
event ordering.
For our unsupervised Bayesian model the lim-
ited size of this training set constitutes an ad-
ditional challenge. In order to alleviate this
problem, we use an informed prior on the word
distributions. Instead of using Dirichlet priors
which do not encode a-priori correlations between
words, we incorporate a logistic normal distri-
bution with the covariance matrix derived from
WordNet. While we will show that prior knowl-
edge as defined above enables the application of
our model to small data sets, we emphasize that
the model is generally widely applicable for two
reasons. First, the data, collected using crowd-
sourcing, is comparatively easy and cheap to ex-
tend. Secondly, our model is domain independent
and can be applied to scenario descriptions from
any domain without any modification. Note that
parameters were tuned on held-out scenarios, and
no scenario-specific tuning was performed.
2 Related Work
In the 1970s, scripts were introduced as a way to
equip AI systems with world knowledge (Schank
and Abelson, 1975; Barr and Feigenbaum, 1986).
Task-specific script databases were developed
manually. FrameNet (Baker et al., 1998) follows a
similar idea, in defining verb frames together with
argument types that can fill the verbs? argument
slots. Frames can then be combined into ?scenario
frames?. Manual composition of such databases,
is arguably expensive and does not scale well.
This paper follows a series of more recent work
which aims to infer script knowledge automati-
cally from data. Chambers and Jurafsky (2008)
present a system which learns narrative chains
from newswire texts. Relevant phrases are iden-
tified based on shared protagonists. The phrases
are clustered into equivalence classes and tempo-
rally ordered using a pipeline of methods. We
work with explicit event sequence descriptions of
a specific scenario, arguing that large-scale com-
mon sense knowledge is hard to acquire from nat-
ural text, since it is often left implicit. Regneri
et al. (2010) induce script knowledge from ex-
plicit ESDs using a graph-based method. Event
types and ordering constraints are induced by
aligning descriptions of equivalent events using
WordNet-based semantic similarity. On this basis
an abstract graph-representation (Temporal Script
Graph; TSG) of the scenario is computed, us-
ing Multiple Sequence Alignment (MSA). Our
work follows the work of Regneri et al. (2010),
in that we use the same data and aim to focus on
the same task. However, the two approaches de-
scribed above employ a pipeline architecture and
treat event learning and learning ordering con-
straints as separate problems. In contrast, we pro-
pose to learn both tasks jointly. We incorporate
both tasks in a hierarchical Bayesian model, thus
using one unified framework.
A related task, unsupervised frame induction,
has also been considered in the past (Titov and
Klementiev, 2011; Modi et al., 2012; O?Connor,
2012); the frame representations encode events
and participants but ignore the temporal aspect of
script knowledge.
We model temporal constraints on event type
orderings with the Generalized Mallows Model
(GMM; Mallows (1957); Fligner and Verducci
(1986); Klementiev et al. (2008)), a statistical
model over permutations. The GMM is a flexi-
ble model which can specify item-specific sensi-
tivity to perturbation from the item?s position in
the canonical permutation. With the GMM we are
thus able to model event type-specific temporal
flexibility ? a feature of scripts that MSA cannot
capture.
The GMM has been successfully applied to
modeling ordering constraints in NLP tasks. Chen
et al. (2009) augment classical topic models with
a GMM, under the assumption that topics in struc-
tured domains (e.g., biographies in Wikipedia)
tend to follow an underlying canonical ordering,
an assumption which matches well our data (the
annotators were asked to follow the temporal or-
50
der of events in their descriptions (Regneri et al.,
2010)). Chen et al. show that for these domains
their approach significantly outperforms Marko-
vian modeling of topics. This is expected as
Markov models (MMs) are not very appropriate
for representing linear structure with potentially
missing topics (e.g., they cannot encode that ev-
ery topic is assigned to at most one continuous
fragment of text). Also GMMs are preferable for
smaller collections such as ours, as the parameter
number is linear in the number of topics (i.e., for
us, event types) rather than quadratic as in Markov
models. We are not aware of previous work on
modeling events with GMMs. Conversely, MMs
were considered in the very recent work of Che-
ung et al. (2013) in the context of script induction
from news corpora where the Markovian assump-
tion is much more natural.
There exists a body of work for learning par-
ticipant types involved in scripts. Regneri et al.
(2011) extend their work by inducing participant
types on the basis of the TSG, using structural in-
formation about participant mentions in the TSG
as well as WordNet similarity, which they then
combine into an Integer Linear Program. Simi-
larly, Chambers and Jurafsky (2009) extend their
work on narrative chains, presenting a system with
which they jointly learn event types and semantic
roles of the participants involved, but do not con-
sider event orderings. We include participant types
as a latent feature in our model, assuming that par-
ticipant mentions in an event description are a pre-
dictive feature for the corresponding event type.
One way of alleviating the problem of small
data sets is incorporating informed prior knowl-
edge. Raina et al. (2006) encode word correlations
in a variance-covariance matrix of a multivariate
normal distribution (MVN), and sample prior pa-
rameter vectors from it, thus introducing depen-
dencies among the parameters. They induce the
covariances from supervised learning tasks in the
transfer learning set-up. We use the same idea, but
obtain word covariances from WordNet relations.
In a slightly different setting, covariance matrices
of MVNs have been used in topic models to induce
correlation between topics in documents (Blei and
Lafferty, 2006).
3 Problem Formulation
Our input consists of a corpus of scenario-specific
ESDs, and our goal is to label each event descrip-
tion in an ESD with one event type e. We specify
the number of possible event types E a priori as a
number exceeding the number of event types in all
the scripts considered. The model will select an
effective subset of those types.
Assume a scenario-specific corpus c, consist-
ing of D ESDs, c = {d
1
, ..., d
D
}. Each
ESD d
i
consists of N
d
event descriptions d
i
=
{d
i,1
, ..., d
i,N
i
}. Boundaries between descriptions
of single events are marked in the data. For each
event description d
i,n
a bag of participant descrip-
tions is extracted. Each participant description
corresponds to one noun phrase as identified au-
tomatically by a dependency parser (cf. Regneri
et al. (2011)). We also associate participant types
with participant descriptions, these types are latent
and induced at the inference stage.
Given such a corpus of ESDs, our model assigns
each event description d
i,n
in an ESD d
i
one event
type z
d
i,n
= e, where e ? {1, ..., E}. Assuming
that all ESDs are generated from the same under-
lying set of event types, our objective is to assign
the same event type to equivalent event descrip-
tions across all ESDs in the corpus.
We furthermore assume that there exists a
canonical temporal ordering of event types for
each scenario type, and that events in observed
scenarios tend to follow this ordering, but allowing
for some flexibility. The event labeling sequence
z
d
i
of an entire ESD should reflect this canonical
ordering. This allows us to use global structural
patterns of ESDs in the event type assignments,
and thus introducing dependence between event
types through their position in the sequence.
4 The Model
Before we describe our model, we briefly explain
the Generalized Mallows Model (GMM) which
we use to encode a preference for linear ordering
of events in a script.
4.1 The (Generalized) Mallows Model
The Mallows Model (MM) is a statistical model
over orderings (Mallows, 1957). It takes two pa-
rameters ?, the canonical ordering, and ? > 0,
a dispersion parameter. The dispersion parame-
ter is a penalty for the divergence d(pi,?) of an
observed ordering pi from the canonical ordering
?. The divergence can be any distance metric but
Kendall?s tau distance (?bubble-sort? distance), a
number of swaps needed to bring pi in the order ?,
51
is arguably the most common choice. The proba-
bility of an observed ordering pi is defined as
P (pi|?,?) =
e
?? d(pi,?)
?(?)
,
where ?(?) is a normalization factor. The distri-
bution is centered around the canonical ordering
(as d(?,?) = 0), and the probability decreases
exponentially with an increasing distance. For our
purposes, without loss of generality, we can as-
sume that ? is the identity permutation, that is
? = [1, . . . , n], where n is the number of items.
The Mallows model has been generalized to
take as a parameter a vector of item-specific
dispersion parameters ? (Fligner and Verducci,
1986). In order to introduce this extension, we
first need to reformulate Kendall?s tau in a way
that captures item-specific distance. An ordering
pi of n items can be equivalently represented by
a vector of inversion counts v of length n ? 1,
where each component v
i
equals the number of
items j > i that occur before item i in pi. For
example, for an observed ordering pi = [2,1,0] the
inversion vector v = (2, 1).
1
Then the generalized
Mallows model (GMM) is defined as
GMM(pi|?) ?
?
i
e
??
i
v
i
.
The GMM can be factorized into item-specific
components, which allows for efficient inference:
GMM
i
(v
i
|?
i
) ? e
??
i
v
i
. (1)
Intuitively, we will be able to induce event type-
specific penalty parameters, and will thus be able
to model individual degrees of temporal flexibility
among the event types.
Since the GMM is member of the exponential
family, a conjugate prior can be defined, which
allows for efficient learning of the parameters ?
(Fligner and Verducci, 1990). Like the GMM, its
prior distribution GMM
0
can be factorized into
independent components for each item i:
GMM
0
(?
i
|v
i,0
, ?
0
) ? e
??
i
v
i,0
?log(?
i
(?
i
))?
0
. (2)
The parameters v
i,0
and ?
0
represent our prior
beliefs about flexibility for each item i, and the
strength of these beliefs, respectively.
1
Trivially, the inversion count for the last element in the
canonical ordering is always 0.
4.2 The Generative Story
Our model encodes two fundamental assumptions,
based on characteristics observed in the data: (1)
We assume that each event type can occur at most
once per ESD; (2) Each participant type is as-
sumed to occur at most once per event type.
The formalized generative story is given in Fig-
ure 1. For each document (ESD) d, we decide in-
dependently for each event type e whether to re-
alize it or not by drawing from Binomial(?
e
).
2
We obtain a binary event vector t where t
e
= 1 if
event type e is realized and t
e
= 0 otherwise. We
draw an event ordering pi from GMM(?), repre-
sented as a vector of inversion counts.
Now, we pass event types in the order defined
by pi. For each realized event type i (i.e., i :
t
i
= 1), we first generate a word (normally a
predicate) from the corresponding language model
Mult(?
i
). Then we independently decide for each
participant type p whether to realize it or not with
the probability Binomial(?
i
p
). If realized, the
participant word (its syntactic head) is generated
from the participant language model Mult($
p
).
Note that though the distribution controlling
frequency of participant generation (?
i
j
) is event
type-specific, the language model associated with
the participant (Mult($
j
)) is shared across
events, thus, ensuring that participant types are de-
fined across events.
The learnt binary realization parameters ? and
?
e
should ensure that an appropriate number of
events and participants is generated (e.g. the real-
ization probability for obligatory events, observed
in almost every ESD for a particular scenario,
should be close to 1).
Priors We draw the parameters for the binomial
distributions from the Beta distribution, which al-
lows us to model a global preference for using
only few event types and only few participant
types for each event type. We draw the parame-
ters of the multinomials from the Dirichlet distri-
bution, and can thus model a preference towards
sparsity. The GMM parameter vector ? is drawn
from GMM
0
(c.f. Equation (2)).
4.3 Adding Prior Knowledge
Since we are faced with a limited amount of train-
ing data, we augment the model described above
2
We slightly abuse the notation by dropping the super-
script d for ESD-specific variables.
52
Generation of parameters
for event type e = 1, . . . , E do
?
e
? Beta(?
+
, ?
?
) [ freq of event ]
?
e
? Dirichlet(?) [event lang mod]
for participant type p = 1, . . . , P do
?
e
p
? Beta(?
+
, ?
?
) [ freq of ptcpt ]
for participant type p = 1, . . . , P do
$
p
? Dirichlet(?) [ ptcpt lang mod ]
for event type e = 1, . . . , E ? 1 do
?
e
? GMM
0
(?
0
,?
0
) [ ordering params]
Generation of ESD d
for event type e = 1, . . . , E do
t
e
? Binomial(?
e
) [ realized events ]
pi ? GMM(?,?) [ event ordering ]
for event i from pi s.th. t
i
=1 do
w
i
?Mult(?
i
) [ event lexical unit ]
for participant type p = 1, . . . , P do
u
p
? Binomial(?
e
p
) [ realized ptcpts ]
if u
p
= 1 then
w
p
?Mult($
p
) [ ptcpt lexical unit]
Figure 1: The generative story of the basic model.
to encode correlations between semantically simi-
lar words in the priors for language models. We
describe our approach by first introducing the
model extension allowing for injecting prior cor-
relations between words, and then explaining how
the word correlations are derived from WordNet
(Fellbaum, 1998). Since the event vocabulary
and the participant vocabulary are separate in our
model, the following procedure is carried out sep-
arately, but equivalently, for the two vocabularies.
4.3.1 Modeling Word Correlation
Dirichlet distributions do not provide a way to en-
code correlations between words. To tackle this
problem we add another level in the model hier-
archy: instead of specifying priors Dirichlet(?)
and Dirichlet(?) directly, we generate them for
each event type e and participant type p using mul-
tivariate normal distributions.
The modification for the generative story is
shown in Figure 2. In this extension, each event
type e and participant type p has a different associ-
ated (nonsymmetric) Dirichlet prior, ?
e
and ?
p
, re-
spectively. The generative story for choosing ?
e
is
the following: A vector ?
e
is drawn from the zero-
mean normal distribution N(?
?
,0), where ?
?
is
Generation of parameters ?
e
and $
p
for event type e = 1, . . . , E do
?
e
? N(?
?
, 0)
for all words w do
?
e
w
=exp(?
e
w
)/
?
w
?
exp(?
e
w
?
) [ Dir prior]
?
e
? Dirichlet(?
e
) [event lang mod]
for participant type p = 1, . . . , P do
?
p
? N(?
?
, 0)
for all words w do
?
p
w
=exp(?
p
w
)/
?
w
?
exp(?
p
w
?
) [ Dir prior]
$
p
? Dirichlet(?
p
) [ ptcpt lang mod ]
Figure 2: The modified parameter generation pro-
cedure for ?
e
and $
p
to encode word correlations.
the covariance matrix encoding the semantic relat-
edness of words (see Section 4.3.2). The vector?s
dimensionality corresponds to size of the vocab-
ulary of event words. Then, the vector is expo-
nentiated and normalized to yield ?
e
.
3
The same
procedure is used to choose ?
p
as shown in Figure
2.
4.3.2 Defining Semantic Similarity
We use WordNet to obtain semantic similarity
scores for each pair of words in our vocabulary.
Since we work on limited domains, we define a
subset of WordNet as all synsets that any word in
our vocabulary is a member of, plus the hypernym
sets of all these synsets. We then create a feature
vector for each word f(w
i
) as follows:
f(w
i
)
n
=
{
1 any sense of w
i
? synset n
0 otherwise
The similarity of two words w
i
and w
j
is de-
fined as the dot product f(w
i
) ?f(w
j
). We use this
similarity to define the covariance matrices ?
?
and
?
?
. Each component (i, j) stores the similarity
between words w
i
and w
j
as defined above. Note
that the matrices are guaranteed to be valid covari-
ance matrices, as they are positive semidefinite by
construction.
5 Inference
Our goal is to infer the set of labelings z of our
corpus of ESDs. A labeling z consists of event
3
In fact, Dirichlet concentration parameters do not need
to sum to one. We experimented with normalizing them to
yield a different constant, thus regulating the influence of the
prior, but have not observed much of improvement from this
extension.
53
types t, participant types u and event ordering pi.
Additionally, we induce parameters of our model:
ordering dispersion parameters (?) and the lan-
guage model parameters ? and ?. We induce these
variables conditioned on all the observable words
in the data setw. Since direct joint sampling from
the posterior distributions is intractable, we use
Gibbs sampling for approximate inference. Since
we chose conjugate prior distributions over the pa-
rameter distributions, we can ?collapse? the Gibbs
sampler by integrating out all parameters (Grif-
fiths and Steyvers, 2004), except for the ones listed
above. The unnormalized posterior can be written
as the following product of terms:
P (z,?,?, ?|w) ?
?
e
DCM
e
?
p
DCM
p
?
e
BBM
e
?
p
BBM
ep
?
e
GMM
e
MN
e
?
p
MN
p
.
The terms DCM
e
and DCM
p
are Dirichlet com-
pound multinomials associated with event-specific
and participant-specific language models:
DCM
e
=
?(
?
v
?
e
v
)
?(
?
v
N
e
v
+ ?
e
v
)
?
v
?(N
e
v
+ ?
e
v
)
?(?
e
v
)
DCM
p
=
?(
?
v
?
p
v
)
?(
?
v
N
p
v
+ ?
p
v
)
?
v
?(N
p
v
+ ?
p
v
)
?(?
p
v
)
,
where N
e
v
and N
p
v
is the number of times word
type v is assigned to event e and participant p,
respectively. The terms BBM
e
and BBM
ep
are
the Beta-Binomial distributions associated with
generating event types and generating participant
types for each event type (i.e. encoding optionality
of events and participants):
BBM
e
?
?(N
+
e
+ ?
+
)?(N
?
e
+ ?
?
)
?(N
+
e
+N
?
e
+ ?
+
+ ?
?
)
BBM
ep
?
?
e
?
p
?(N
+
ep
+ ?
+
)?(N
?
ep
+ ?
?
)
?(N
+
ep
+N
?
ep
+ ?
+
+ ?
?
)
,
where N
+
e
and N
?
e
is the number of ESDs where
event type is generated and the number of ESD
where it is not generated, respectively. N
+
ep
and
N
?
ep
are analogously defined for participant types
(for each event type e). The term GMM
e
is as-
sociated with the inversion count distribution for
event type e and has the form
GMM
e
? GMM
0
(?
e
;
?
d
v
d
e
+ v
e,0
?
0
N + ?
0
, N + ?
0
),
where GMM
0
is defined in expression (2) and v
d
e
is the inversion count for event e in ESD d. N is
the cumulative number of event occurrences in the
data set.
Finally, MN
e
and MN
p
correspond to the
probability of drawing ?
e
and ?
p
from the cor-
responding normal distributions, as discussed in
Section 4.3.1.
Though, at each step of Gibbs sampling, com-
ponents of z could potentially be sampled by
considering the full unnormalized posterior, this
clearly can be made much more efficient by ob-
serving that only a fraction of terms affect the cor-
responding conditional probability. For example,
when sampling an event type for a given event
in a ESD d, only the terms DCM
e
, BBM
ep
and
BBM
e
for all e and p are affected. For DCMs it
can be simplified further as only a few word types
are affected. Due to space constraints, we cannot
describe the entire sampling algorithms but it natu-
rally follows from the above equations and is sim-
ilar to the one described in Chen et al. (2009).
For sampling the other parameters of our model,
ranking dispersion parameters ? and the language
model parameters ? and ?, we use slice sampling
(MacKay, 2002). For each event type e we draw
its dispersion parameter ?
e
independently from the
slice sampler.
After every n
th
iteration we resample ? and
? for all language models to capture the corre-
lations. However, to improve mixing time, we
also resample components ?
k
i
and ?
l
i
when word
i has changed event membership from type k to
type l. In addition we define classes of closely
related words (heuristically based on the covari-
ance matrix) by classifying words as related when
their similarity exceeds an empirically determined
threshold. We also resample all components ?
k
j
and ?
l
j
for each word j that related to word i. We
re-normalize ?
m
and ?
n
after resampling to up-
date the Dirichlet concentration parameters. The
same procedure is used for participant language
models (parameters ?).
6 Evaluation
In our evaluation, we evaluate the quality of the
event clusters induced by the model and the ex-
tent to which the clusters capture the global event
ordering underlying the script, as well as the bene-
fit of the GMM and the informed prior knowledge.
We start by describing data and evaluation metrics.
54
Scenario Name ]ESDs Avg len
OMICS corpus
Cook in microwave 59 5.03
Answer the telephone 55 4.47
Buy from vending machine 32 4.53
Make coffee 38 5.00
R10 corpus
Iron clothes 19 8.79
Make scrambled eggs 20 10.3
Eat in fast food restaurant 15 8.93
Return food (in a restaurant) 15 5.93
Take a shower 21 11.29
Take the bus 19 8.53
Table 1: Test scenarios used in experiments (left),
the size of the corresponding corpus (middle), and
the average length of an ESD in events (right).
6.1 Data
We use the data sets presented in Regneri et al.
(2010) (henceforth R10) for development and test-
ing. The data is comprised of ESDs from two cor-
pora. R10 collected a corpus, consisting of sets of
ESDs for a variety of scenarios, via a web exper-
iment from non-expert annotators. In addition we
use ESDs from the OMICS corpus
4
(Kochender-
fer and Gupta, 2003), which consists of instantia-
tions of descriptions of several ?stories?, but is re-
stricted to indoor activities. The details of our data
are displayed in Table 1. For each event descrip-
tion we extract all noun phrases, as automatically
identified by Regneri et al. (2011), separating par-
ticipant descriptions from action descriptions. We
remove articles and pronouns, and reduce NPs to
their head words.
6.2 Gold Standard and Evaluation Metrics
We follow R10 in evaluating induced event types
and orderings in a binary classification setting.
R10 collected a gold standard by classifying pairs
of event descriptions w.r.t. whether or not they are
paraphrases. Our model classifies two event de-
scriptions as equivalent whenever z
e
1
= z
e
2
.
Equivalently, R10 classify ordered pairs of
event descriptions as to whether they are presented
in their natural order. Assuming the identity order-
ing as canonical ordering in the Generalized Mal-
lows Model, event types tending to occur earlier
in the script should be assigned lower cluster IDs
than event types occurring later. Thus, whenever
z
e
1
< z
e
2
, our the model predicts that two event
descriptions occur in their natural order.
4
http://csc.media.mit.edu/
Event Paraphrase Evt. Ordering
P R F P R F
Ret. Food 0.92 0.52 0.67 0.87 0.72 0.79
-GMM 0.70 0.30 0.42 0.46 0.44 0.45
-COVAR 0.92 0.52 0.67 0.77 0.67 0.71
Vending 0.76 0.78 0.77 0.90 0.74 0.81
-GMM 0.74 0.39 0.51 0.64 0.47 0.54
-COVAR 0.74 0.87 0.80 0.85 0.73 0.78
Shower 0.68 0.67 0.67 0.85 0.84 0.85
-GMM 0.36 0.17 0.23 0.42 0.38 0.40
-COVAR 0.64 0.44 0.52 0.77 0.73 0.75
Microwave 0.85 0.80 0.82 0.91 0.74 0.82
-GMM 0.88 0.30 0.45 0.67 0.62 0.64
-COVAR 0.89 0.81 0.85 0.92 0.82 0.87
Table 2: Comparison of model variants: For each
scenario: The full model (top), a version without
the GMM (-GMM), and a version with a uniform
Dirichlet prior over language models (-COVAR).
We evaluate the output of our model against the
described gold standard, using Precision, Recall
and F1 as evaluation metrics, so that our results are
directly comparable to R10. We tune our parame-
ters on a development set of 5 scenarios which are
not used in testing.
6.3 Results
Table 3 presents the results of our two evaluation
tasks. While on the event paraphrase task the R10
system performs slightly better, our model out-
performs the R10 system on the event ordering
task by a substantial margin of 7 points average
F-score. While both systems perform similarly on
the task of event type induction, we induce a joint
model for both objectives. The results show that,
despite the limited amount of data, and the more
complex learning objective, our model succeeds in
inducing event types and ordering constraints.
In order to demonstrate the benefit of the GMM,
we compare the performance of our model to a
variant which excludes this component (-GMM),
cf. Table 2. The results confirm our expectation
that biasing the model towards encouraging a lin-
ear ordering on the event types provides a strong
cue for event cluster inference.
As an example of a clustering learnt by our
model, consider the following event chain:
{get} ? {open,take} ? {put,place} ?
{close} ? {set,select,enter,turn} ? {start}
? {wait} ? {remove,take,open} ?
{push,press,turn}
We display the most frequent words in the clusters
55
Scenario Event Paraphrase Task Event Ordering Task
Precision Recall F1 Precision Recall F1
R10 BS R10 BS R10 BS R10 BS R10 BS R10 BS
Coffee 0.50 0.47 0.94 0.58 0.65 0.52 0.70 0.68 0.78 0.57 0.74 0.62
Telephone 0.93 0.92 0.85 0.72 0.89 0.81 0.83 0.92 0.86 0.87 0.84 0.89
Bus 0.65 0.52 0.87 0.43 0.74 0.47 0.80 0.76 0.80 0.76 0.80 0.76
Iron 0.52 0.65 0.94 0.56 0.67 0.60 0.78 0.87 0.72 0.69 0.75 0.77
Scr. Eggs 0.58 0.92 0.86 0.65 0.69 0.76 0.67 0.77 0.64 0.59 0.66 0.67
Vending 0.59 0.76 0.83 0.78 0.69 0.77 0.84 0.90 0.85 0.74 0.84 0.81
Microwave? 0.75 0.85 0.75 0.80 0.75 0.82 0.47 0.91 0.83 0.74 0.60 0.82
Shower? 0.70 0.68 0.88 0.67 0.78 0.67 0.48 0.85 0.82 0.84 0.61 0.85
Fastfood? 0.50 0.74 0.73 0.87 0.59 0.80 0.53 0.97 0.81 0.65 0.64 0.78
Ret. Food? 0.73 0.92 0.68 0.52 0.71 0.67 0.48 0.87 0.75 0.72 0.58 0.79
Average 0.645 0.743 0.833 0.658 0.716 0.689 0.658 0.850 0.786 0.717 0.706 0.776
Table 3: Results of our model for the event paraphrase task (left) and event type ordering task (right).
Our system (BS) is compared to the system in Regneri et al. (2010) (R10). We were able to obtain the
R10 system from the authors and evaluate on additional scenarios for which no results are reported in
the paper. These additional scenarios are marked with a dot (?).
inferred for the ?Microwave? scenario. Clusters
are sorted by event type ID. Note that the word
?open? is assigned to two event types in the se-
quence, which is intuitively reasonable. This illus-
trates why assuming a deterministic mapping from
predicates to events (as in Chambers and Jurafsky
(2008)) is limiting for our dataset.
We finally examined the influence of the in-
formed prior component, comparing to a model
variant which uses uniform Dirichlet parameters
(-COVAR; see Table 2). As expected, using an in-
formed prior component leads to improved perfor-
mance on scenario types with fewer training ESDs
available (?Take a shower? and ?Return food?; cf.
Table 1). For scenarios with a larger set of training
documents no reliable benefit from the informed
prior is observable. We did not optimize this com-
ponent, e.g. by testing more sophisticated meth-
ods for construction of the covariance matrix, but
expect to be able to improve its reliability.
7 Discussion
The evaluation shows that our model is able to
create meaningful event type clusters, which re-
semble the underlying event ordering imposed by
the scenario. We achieve an absolute average im-
provement of 7% over a state-of-the-art model. In
contrast to previous approaches to script induc-
tion, our model does not include specifically cus-
tomized components, and is thus flexibly applica-
ble without additional engineering effort.
Our model provides a clean, statistical formula-
tion of the problem of jointly inducing event types
and their ordering. Using a Bayesian model al-
lows for flexible enhancement of the model. One
straightforward next step would be to explore the
influence of participants, and try to jointly infer
them with our current set of latent variables.
Statistical models highly rely on a sufficient
amount of training data in order to be able to
induce latent structures. The limited amount of
training data in our case is a bottleneck for the per-
formance. The model performs best on the two
scenarios with the most training data (?Telephone?
and ?Microwave?), which supports this assump-
tion. We showed, however, that our model can be
applied to small data sets through incorporation of
informed prior knowledge without supervision.
8 Conclusion
We presented a hierarchical Bayesian model for
joint induction of event clusters and constraints on
their orderings from sets of ESDs. We incorporate
the Generalized Mallows Model over orderings.
The evaluation shows that our model successfully
induces event clusters and ordering constraints.
We compare our joint, statistical model to a
pipeline based model using MSA for event clus-
tering. Our system outperforms the system on the
task of event ordering induction by a substantial
margin, while achieving comparable results in the
event induction task. We could further explicitly
show the benefit of modeling global ESD struc-
ture, using the GMM.
In future work we plan to apply our model to
larger data sets, and to examine the role of par-
ticipants in our model, exploring the potential of
inferring them jointly with our current objectives.
56
Acknowledgments
We thank Michaela Regneri for substantial support
with the script data, and Mirella Lapata for helpful
comments.
References
Collin F. Baker, Charles J. Fillmore, and John B.
Lowe. 1998. The berkeley framenet project.
In Proceedings of the 36th Annual Meeting of
the Association for Computational Linguistics
and 17th International Conference on Compu-
tational Linguistics, pages 86?90.
A. Barr and E.A. Feigenbaum. 1986. The hand-
book of artificial intelligence. 1 (1981). The
Handbook of Artificial Intelligence. Addison-
Wesley.
David Blei and John Lafferty. 2006. Correlated
topic models. In Advances in Neural Informa-
tion Processing Systems 18, pages 147?154.
Nathanael Chambers and Dan Jurafsky. 2008. Un-
supervised learning of narrative event chains. In
Proceedings of ACL-08: HLT, pages 789?797.
Nathanael Chambers and Dan Jurafsky. 2009. Un-
supervised learning of narrative schemas and
their participants. In Proceedings of the 47th
Annual Meeting of the Association for Compu-
tational Linguistics, pages 602?610.
H. Chen, S. R. K. Branavan, R. Barzilay, and D. R.
Karger. 2009. Content modeling using latent
permutations. Journal of Artificial Intelligence
Research, 36(1):129?163.
Christiane Fellbaum, editor. 1998. WordNet: an
electronic lexical database. MIT Press.
M. Fligner and J. Verducci. 1986. Distance based
ranking models. Journal of the Royal Statistical
Society, Series B, 48:359?369.
M. Fligner and J. Verducci. 1990. Posterior prob-
abilities for a consensus ordering. Psychome-
trika, 55:53?63.
T. L. Griffiths and M. Steyvers. 2004. Finding
scientific topics. Proceedings of the National
Academy of Sciences, 101(Suppl. 1):5228?
5235.
Alexandre Klementiev, Dan Roth, and Kevin
Small. 2008. Unsupervised rank aggregation
with distance-based models. In Proceedings of
the 25th International Conference on Machine
Learning, pages 472?479.
Mykel J. Kochenderfer and Rakesh Gupta. 2003.
Common sense data acquisition for indoor mo-
bile robots. In Proceedings of the Nineteenth
National Conference on Artificial Intelligence
(AAAI-04), pages 605?610.
D. J. C. MacKay. 2002. Information Theory, Infer-
ence & Learning Algorithms. Cambridge Uni-
versity Press, New York, NY, USA.
C. L. Mallows. 1957. Non-null ranking models.
Biometrika, 44:114?130.
Risto Miikkulainen. 1995. Script-based inference
and memory retrieval in subsymbolic story pro-
cessing. Applied Intelligence, pages 137?163.
Ashutosh Modi, Ivan Titov, and Alexandre Kle-
mentiev. 2012. Unsupervised induction of
frame-semantic representations. In Proceedings
of the NAACL-HLT Workshop on the Induction
of Linguistic Structure, pages 1?7.
Erik T. Mueller. 2004. Understanding script-based
stories using commonsense reasoning. Cogni-
tive Systems Research, 5(4):307?340.
Brendan O?Connor. 2012. Bayesian unsupervised
frame learning from text. Technical report,
Carnegie Mellon University.
Rajat Raina, Andrew Y. Ng, and Daphne Koller.
2006. Constructing informative priors using
transfer learning. In Proceedings of the 23rd In-
ternational Conference on Machine Learning,
pages 713?720.
Michaela Regneri, Alexander Koller, and Manfred
Pinkal. 2010. Learning script knowledge with
web experiments. In Proceedings of the 48th
Annual Meeting of the Association for Compu-
tational Linguistics, pages 979?988.
Michaela Regneri, Alexander Koller, Josef Rup-
penhofer, and Manfred Pinkal. 2011. Learning
script participants from unlabeled data. In Pro-
ceedings of RANLP 2011, pages 463?470.
Roger C. Schank and Robert P. Abelson. 1975.
Scripts, plans, and knowledge. In Proceedings
of the 4th International Joint Conference on Ar-
tificial Intelligence, IJCAI?75, pages 151?157.
Ivan Titov and Alexandre Klementiev. 2011. A
bayesian model for unsupervised semantic pars-
ing. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Lin-
guistics: Human Language Technologies, pages
1445?1455.
57
Improved Estimation of Entropy for
Evaluation of Word Sense Induction
Linlin Li?
Microsoft Development Center Norway
Ivan Titov??
University of Amsterdam
Caroline Sporleder?
Trier University
Information-theoretic measures are among the most standard techniques for evaluation of
clustering methods including word sense induction (WSI) systems. Such measures rely on
sample-based estimates of the entropy. However, the standard maximum likelihood estimates
of the entropy are heavily biased with the bias dependent on, among other things, the number of
clusters and the sample size. This makes the measures unreliable and unfair when the number
of clusters produced by different systems vary and the sample size is not exceedingly large. This
corresponds exactly to the setting of WSI evaluation where a ground-truth cluster sense number
arguably does not exist and the standard evaluation scenarios use a small number of instances of
each word to compute the score. We describe more accurate entropy estimators and analyze their
performance both in simulations and on evaluation of WSI systems.
1. Introduction
The task of word sense induction (WSI) has grown in popularity recently. WSI has the
advantage of not assuming a predefined inventory of senses. Rather, senses are induced
in an unsupervised fashion on the basis of corpus evidence (Schu?tze 1998; Purandare
and Pedersen 2004). WSI systems can therefore better adapt to different target domains
that may require sense inventories of different granularities. However, the fact that WSI
systems do not rely on fixed inventories also makes it notoriously difficult to evaluate
and compare their performance. WSI evaluation is a type of cluster evaluation problem.
Although cluster evaluation has received much attention (see, e.g., Dom 2001; Strehl
and Gosh 2002; Meila 2007), it is still not a solved problem. Finding a good way to
score partially incorrect clusters is particularly difficult. Several solutions have been
? Microsoft Development Center Norway. E-mail: linlin@coli.uni-saarland.de.
?? Institute for Logic, Language and Computation. E-mail: titov@uva.nl.
? Computational Linguistics and Digital Humanities, Trier University, 54286 Trier, Germany.
E-mail: sporledc@uni-trier.de.
Submission received: 14 March 2013; revised version received: 13 September 2013; accepted for publication:
20 November 2013
doi:10.1162/COLI a 00196
? 2014 Association for Computational Linguistics
Computational Linguistics Volume 40, Number 3
proposed but information theoretic measures have been among the most successful
and widely used techniques. One example is the normalized mutual information, also
known as V-measure (Strehl and Gosh 2002; Rosenberg and Hirschberg 2007), which
has, for example, been adopted in the SemEval 2010 WSI task (Manandhar et al. 2010).
All information theoretic measures of cluster quality essentially rely on sample-
based estimates of entropy. For instance, the mutual information I(c, k) between a gold
standard class c and an output cluster k can be written H(c) + H(k) ? H(k, c), where
H(c) and H(k) are the marginal entropies of c and k, respectively, and H(k, c) is their joint
entropy. The most standard estimator is the maximum-likelihood (ML) estimator, which
substitutes the probability of each event (cluster, classes, or cluster-class pair occurrence)
with its normalized empirical frequency.
Entropy estimators, even though consistent, are biased. This means that the expected
estimate of the entropy on a finite sample set is different from the true value. It is
also different from an expected estimate on a larger test set generated from the same
distribution, as the bias depends on the size of the sample. This discrepancy negatively
affects entropy-based evaluation measures, such as the V-measure. This is different from
supervised classification evaluation, where the classification accuracy on a finite test set
is expected to be equal to the error rate (for the independent and identically distributed,
i.i.d.) case, though it can be different due to variance (due to choice of the test set). As
long as the number of samples is large with respect to the number of classes and clusters,
the estimate is sufficiently close to the true entropy. Otherwise, the quality of entropy
estimators matters and the bias of the estimator can be large. This problem is especially
prominent for the ML estimator (Miller 1955).
In WSI, we are faced with exactly those conditions that negatively affect the entropy
estimators. In a typical setting, the number of examples per word is small?for example,
less than 100 on average for the SemEval 2010 WSI task. The number of clusters, on
the other hand, can be fairly high, with some systems outputting more than 10 sense
clusters per word on average. Because the bias of an entropy estimator is dependent
on, among other things, the number of clusters, the ranking of different WSI systems is
partly affected by the number of clusters they produce. Even worse, the ranking is also
affected by the size of the test set. The problem is exacerbated when computing the joint
entropy between clusters and classes, H(k, c), because this requires estimating the joint
probability of cluster-class pairs for which the statistics are even more sparse.
The bias problem of entropy estimators has long been known in the information
theory community and many studies have addressed this issue (e.g., Miller 1955; Batu
et al. 2002; Grasberger and Schu?rmann 1996). In this article, we compare different
estimators and their influence on the computed evaluation scores. We run simulations
using a Zipfian distribution where we know the true entropy. We also compare different
estimators against the SemEval 2010 WSI benchmark. Our results strongly suggest that
there are estimators, namely, the best-upper-bound (BUB) estimator (Paninski 2003)
and jackknifed (Tukey 1958; Quenouille 1956) estimators, which are clearly preferable
to the commonly used ML estimators.
2. Clustering Evaluation
2.1 Information-Theoretic Measures
The main challenge in evaluating clustering methods is that successful measures should
be able to compare solutions found at different levels of granularity. In other words,
one cannot assume that there exists one-to-one mapping between the predicted clusters
672
Li, Titov, and Sporleder Improved Estimation of Entropy for Word Sense Induction
and the gold-standard classes. A natural approach would be to consider arbitrary
statistical dependencies between the cluster assignment k and the class assignment c.
The standard measure of statistical dependence of two random variables is the Shannon
mutual information I(k, c) (MI). MI is 0 if two variables are independent, and it is equal
to the entropy of a variable if another variable is deterministically dependent on it.
Clearly, such measure would favor clusterings with higher entropy, and, consequently,
normalized versions of MI are normally used to evaluate clusterings. One instance of
normalized MI actively used in the context of WSI evaluation is the V-measure, or
symmetric uncertainty (Witte and Frank 2005; Rosenberg and Hirschberg 2007):
V(k, c) =
2I(k, c)
H(k) + H(c)
=
2(H(k) + H(c) ? H(k, c))
H(k) + H(c)
though other forms of MI normalization have also been explored (Strehl and Gosh 2002).
Because the true marginal and the joint entropies are not known, the standard
maximum likelihood estimators (also called plug-in estimators of entropy) are normally
used instead. The ML estimates H? have the analytical form of an entropy with the
normalized empirical frequency substituted instead of the unknown true membership
probabilities, for example:
H?(c) =
m
?
i=1
?
ni
N log
ni
N (1)
where ni is the number of times cluster i appears in the set, m is the number of clusters,
and N is the size of the set (i.e., the sample size).
The ML estimators of entropy are consistent but heavily negatively biased (see
Section 3 for details). In other words, the expectation of H? is lower than the true entropy,
and this discrepancy increases with the number of clusters m and decreases with the
sample size N. When m is comparable to N, the ML estimator is known to be very
inaccurate (Paninski 2004).
Note that for V-measure estimation the main source of the estimation error is the
joint entropy H(k, c),1 as the number of possible pairs (c, k) for most systems would be
large whereas the total number of occurrences will remain the same as for the estimation
of H(c) and H(k). Therefore, the absolute value of the bias for H?(c, k) will exceed the
aggregate bias of the estimators of marginal entropy, H?(c) and H?(k). As a result, the
V-measure will be positively biased, and this bias would be especially high for systems
predicting a large number of clusters.
This phenomenon has been previously noticed (Manandhar et al. 2010) but no satis-
factory explanation has been given. The shortcomings of the ML estimator are especially
easy to see on the example of a baseline system that assigns every instance in the testing
set to an individual cluster. This baseline, when averaged over the 100 target words, out-
performs all the participants? systems of the SemEval-2010 task on the standard testing
set (Manandhar and Klapaftis 2009). Though we cannot compute the true bias for any
real system, the computation is trivial for this baseline. The true V-measure is equal to 0,
1 V-measure can be expressed via entropies in a number of different ways, although, for ML estimation
they are all equivalent. For some more complex estimators, including some of the ones considered here,
the resulting estimates will be somewhat different depending on the decomposition. We will focus on the
symmetric form presented here.
673
Computational Linguistics Volume 40, Number 3
as the baseline can be regarded as a limiting case of a stochastic system that picks up one
of the m clusters under the uniform distribution with m ? ?; the mutual information
between any class labels and clustering produced by such model equals 0 for every m.
However, the ML estimate for the V-measure is V?(k, c) = 2H?(c)/(log N + H?(c)). For the
testing set of SemEval 2010, this estimate, averaged over all the words, yields 31.7%,
which by far exceeds the best result of any system (16.2%). On an infinite (or sufficiently
large) set, however, its performance would change to the worst. This is a problem not
only for the baseline but for any system which outputs a large number of classes: The
error measures computed on the small test set are far from their expectations on the
new data. We will see in our quantitative analyses (Section 5) that using more accurate
estimators will have the most significant effect on both the V-measure and on the ranks
of systems which output richer clustering, agreeing with this argument.
Though in this analysis we focused on the V-measure, other information theoretic
measures have also been proposed. Examples of such measures include the variation
of information measure (Meila 2007) VI(c, k) = H(c|k) + H(k|c) and Q0 measure (Dom
2001) H(c|k). This argument applies to these evaluation measures as well, and they can
all be potentially improved by using more accurate estimators.
2.2 Alternative Measures
Not only information-theoretic measures have been proposed for clustering evaluation.
An alternative evaluation strategy is to attempt to find the best possible mapping
between the predicted clusters and the gold-standard classes and then apply standard
measures like precision, recall, and F-score. However, if the best mapping is selected on
the test set the result can be overoptimistic, especially for rich clusterings. Consequently,
such methods constrain the set of permissible mappings to a restricted family. For exam-
ple, for the F-score, one considers only mappings from each class to a single predicted
cluster (Zhao and Karypis 2005; Agirre and Soroa 2007). This restriction is generally too
strong for many clustering problems (Meila 2007; Rosenberg and Hirschberg 2007), and
especially inappropriate for the WSI evaluation setting, as it penalizes sense induction
systems that induce more fine-grained senses than the ones present in the gold-standard
sense set.
The Paired F-score (Manandhar et al. 2010) is somewhat less restrictive than the
F-score measures in that it defines precision and recall in terms of pairs of instances (i.e.,
effectively evaluating systems based on the proportion of correct links). However, the
Paired F-score has the undesirable property that it ranks those systems highest which
put all instances in one cluster, thereby obtaining perfect recall.
As an alternative, the supervised evaluation measure has been proposed (Agirre
et al. 2006). This approach in addition to the testing set uses an auxiliary mapping set.
First the mapping is induced on the mapping set, then the quality of the mapping is
evaluated on the testing set. One problem with this evaluation scenario is that the size
of the mapping set has an effect on the results and there is no obvious criterion for
selecting the right size of the mapping set. For the WSI task, the importance of the set
size was empirically confirmed when the evaluation set was split in proportions 80:20
(80% for the mapping sets, and 20% for testing) instead of the original 60:40 split: The
scores of all top 10 systems improved and the ranking changed as well (Manandhar
et al. 2010) (see also Table 1 later in this article).
Further cluster evaluation measures have been proposed for other language pro-
cessing tasks, such as B3 (Bagga and Baldwin 1998) or CEAF (Luo 2005) for coreference
resolution evaluation. In this article, we are concerned with entropy-based measures.
674
Li, Titov, and Sporleder Improved Estimation of Entropy for Word Sense Induction
For a more general assessment of measures for clustering evaluation see Amigo et al.
(2009) and Klapaftis and Manandhar (2013).
3. Entropy Estimation
Given the influence that information theory has had on many fields, including signal
processing, neurophysiology, and psychology, to name a few, it is not surprising that the
topic of entropy estimation has received considerable attention over the last 50 years.2
However, much of the work has focused on settings where the number of classes is
significantly lower than the size of the sample. More recently a set-up where the sample
size N is comparable to the number of classes m has started to receive attention (Paninski
2003, 2004).
In this section, we start by discussing the intuition for why the ML estimator is
heavily biased in this setting. Though unbiased estimators of entropy do not exist,3
various techniques have been proposed to reduce the bias while controlling the vari-
ance (Grasberger and Schu?rmann 1996; Batu et al. 2002). We will discuss widely used
bias-corrected estimators, the ML estimator with Miller-Madow bias correction (Miller
1955) and the jackknifed estimator (Strong et al. 1998). Then we turn to the more
recent technique proposed specifically for the N ? m setting, the best-upper-bound
(BUB) estimator (Paninski 2003). We will conclude this section by explaining how these
estimators can be computed using stochastic (weighted) output of WSI systems.
3.1 Standard Estimators of Entropy
As we discussed earlier, the ML estimator (1) is negatively biased. For a fixed distri-
bution p, a little algebra can be used to show that the bias of the maximum likelihood
estimator can be written as
H ? Ep(H?) = Ep(D(p? ? p))
where Ep denotes an expectation under p, D is the Kullback-Leibler (KL) divergence,
and p? is the empirical distribution in the sample of N elements drawn i.i.d. from p.
Because the KL-divergence is always non-negative, it follows that the bias is always
non-positive. It also follows that the expected divergence is larger if the size of the
sample is small. In fact, this expression can be used to obtain the asymptotic bias rate
(N ? ?) (Miller 1955). The bias rate derived in this way would suggest a form of
correction to the ML estimator, called Miller-Madow bias correction H?MM = H? +
m??1
N ,
where m? is an estimate of m, as the true size of support m may not be known. In our
experiments, we use a basic estimator m? which is just the number of different clusters
(classes or cluster-class pairs depending on the considered entropy) appearing in the
sample. We will call the estimator H?MM the Miller-Madow (MM) estimator. As the MM
estimator is motivated by the asymptotic behavior of the bias, it is not very appropriate
for N ? m.
2 For a relatively recent overview of progress in entropy estimation research see, for example, the
proceedings of the NIPS 2003 workshop on entropy estimation.
3 The expectation of any estimate from i.i.d. samples is a polynomial function of class probabilities.
The entropy is non-polynomial and therefore unbiased estimators do not exist.
675
Computational Linguistics Volume 40, Number 3
The bias of the ML estimator decreases with the size of the sample. Intuitively, an
estimate of the discrepancy in estimates produced from samples of different sizes can
be used to correct the ML estimator: If an estimate based on N ? 1 samples significantly
exceeds the estimate from N samples, then the bias of the estimator is still large.
Roughly, this intuition is encoded in the jackknifed (JK) estimator (Strong et al. 1998):
H?JK = NH? ?
N ? 1
N
N
?
j=1
H?
?j
where H?
?j is the ML estimator based on the original sample excluding the example j.
3.2 BUB Estimator
We can observe that all the previous estimators can be expressed in the form of a linear
function of the ordered histogram statistics
H?(a) =
N
?
j=0
aj,Nhj (2)
where hj is the number of classes which appear j times in the sample:
hj =
m
?
i=1
[[ni = j]] (3)
where [[ ]] denotes the indicator function. The coefficients aj,N for the ML, MM, and JK
estimators are equal to:
aML,j,N = ?
j
N log
j
N
aMM,j,N = ?
j
N log
j
N +
1 ? jN
N
aJK,j,N = NaML,j,N ?
N ? 1
N ((N ? j)aML,j,N?1 + jaML,j?1,N?1)
This observation suggests that it makes sense to study an estimator of the general
form H?(a) as defined in Equation (2). Upper bounds on the bias and variance of such
estimators4 have been stated in Paninski (2003). These bounds imply an upper bound
on the standard measure of estimator performance, mean squared error (MSE, the
sum of the variance and squared bias). The worst-case estimator is then obtained
by selecting a to minimize the upper bound on MSE, and, therefore, it is called the
best-upper-bound estimator. This optimization problem5 corresponds to a regularized
4 We argued that variance is not particularly important for the ML estimator with N ? m. However,
for an arbitrary estimator of the form of Equation (2) this may not be true, as the coefficients aj,N
may be oscillating, resulting in an estimator with a large variance (Antos and Kontoyiannis 2001).
5 More formally, its modification where the L2 norm is optimized instead of the original L?
optimization set-up.
676
Li, Titov, and Sporleder Improved Estimation of Entropy for Word Sense Induction
least-squares problem and can be solved analytically (see Appendix A and Paninski
[2003] for technical details).
This technique is fairly general, and can potentially be used to minimize the bound
for a particular type of distribution. This direction can be promising, as the types of
distributions observed in WSI are normally fairly skewed (arguably Zipfian) and tighter
bounds on MSE may be possible. In this work, we use the universal worst-case bounds
advocated in Paninski (2003).
3.3 Estimation with Stochastic Predictions
As many WSI systems maintain a distribution over predicted clusters, in SemEval 2010
the participants were encouraged to provide a weighted prediction (i.e., a distribution
over potential clusters for each example) instead of predicting just a single most-likely
cluster.
We interpret the weighted output of a system on an example l as a categorical
trial with the probabilities of outcomes p?(l)i provided by the model where i is an index
of the cluster. Therefore a stochastic output of a system on the test set represents
a distribution over samples generated from these trials; the estimator can be com-
puted as an expectation under this distribution. For estimators of the form of Equa-
tion (2), we can exploit the linearity of expectations and write the expected value of the
estimator as
Ep?
[
H?(a)
]
=
N
?
j=0
aj,NEp?
[
hj
]
where Ep?
[
hj
]
is the expected number of classes with j counts. We can rewrite it using the
linearity property again, this time for expression (3):
Ep?
[
H?(a)
]
=
N
?
j=0
aj,N
m
?
i=1
P?(ni = j, N) (4)
where P?i(ni = j, N) is the distribution over the number of counts for non-identical
Bernoulli trials p?(l)i and 1 ? p?
(l)
i (l = 1, . . . , N), known as the Poisson binomial distribu-
tion, a generalization of the standard binomial distribution. The probabilities can be
efficiently computed using one of alternative recursive formulas (Wang 1993). One
of the simplest schemes, with good numerical stability properties, is the recursive
computation:
P?i(ni = j, t) = P?i(ni = j ? 1, t ? 1)p?
(j)
i + P?i(ni = j, t ? 1)(1 ? p?
(j)
i )
where j = 1, . . . , N.
4. Simulations
Because the true entropy (and V-measure) is not known on the WSI task, we start with
simulations where we generated samples from a known distribution and can compare
677
Computational Linguistics Volume 40, Number 3
0 5 10 15 20 25 30 35 40 45 50
0
0.5
1
1.5
2
2.5
Sample Size (N)
En
tro
py
 
 
H
BUB
JK
MM
ML
Figure 1
The estimated and true entropy for uniform distribution.
the estimates (and their biases) with the true entropy. In all our experiments, we set the
number of clusters m to 10 and varied the sample size N (Figure 1). Each point on the
graph is the result of averaging over 1,000 sampling experiments.6
The distribution of senses for a given word is normally skewed: For most words
the vast majority of occurrences correspond to one or two most common senses even
though the total number of senses can be quite large (Kilgarriff 2004). This type of long-
tail distribution can be modeled with Zipf?s law. Consequently, most of our experiments
consider Zipfian distributions. For Zipf?s law, the probability of choosing an element
with rank k is proportional to 1ks , where s is a shape parameter. Small values of the
parameter s correspond to flatter distributions; the distributions with a larger s are
increasingly more skewed. The estimators? prediction for Zipfian distributions with
a different s are shown in Figure 2. For s = 4, over 90% of the probability mass is
concentrated on a single class. For every distribution we plot the true entropy (H)
and the estimated values; compare results with a uniform distribution as seen in
Figure 1.
In all figures, we observe that over the entire range of sample sizes, the bias
for the bias-corrected estimates is indeed reduced substantially with respect to that
of the ML estimator. This difference is particularly large for smaller N?the realistic
setting for the computation of H(c, k) for the WSI task. For the uniform distribution
and flatter Zipf distributions (s = 1 and 2), the JK estimator seems preferable for all
but the smallest sample sizes (N > 3). The BUB estimator outperforms the JK estimator
with very skewed distributions (s = 3 and s = 4) and in most cases provides the least
biased estimates with very small N. However, these results with very small sample sizes
(N ? 2) may not have much practical relevance as any estimator is highly inaccurate in
this mode. The MM bias correction, as expected, is not sufficient for small N. Although
it outperforms the ML estimates, its error is consistently larger than those of other bias-
correction strategies.
Overall, the simulations suggest that the ML estimators are not very appropriate for
entropy estimation with the types of distributions which are likely to be observed in the
6 In this way we study only the bias of estimators.
678
Li, Titov, and Sporleder Improved Estimation of Entropy for Word Sense Induction
0 5 10 15 20 25 30 35 40 45 50
0
0.5
1
1.5
2
2.5
Sample Size (N)
En
tro
py
 
 
H
BUB
JK
MM
ML
0 5 10 15 20 25 30 35 40 45 50
0
0.2
0.4
0.6
0.8
1
1.2
1.4
Sample Size (N)
En
tro
py
 
 
H
BUB
JK
MM
ML
(a) s = 1 (b) s = 2
0 5 10 15 20 25 30 35 40 45 50
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
Sample Size (N)
En
tro
py
 
 
H
BUB
JK
MM
ML
0 5 10 15 20 25 30 35 40 45 50
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
Sample Size (N)
En
tro
py
 
 
H
BUB
JK
MM
ML
(c) s = 3 (d) s = 4
Figure 2
The estimated and true entropy of Zipf?s law.
WSI tasks. Both the JK and BUB estimators are considerably less biased alternatives to
the ML estimations.
5. Effects on WSI Evaluation
To gauge the effect of the bias problem on WSI evaluation, we computed how the
ranking of the SemEval 2010 systems (Manandhar et al. 2010) were affected by different
estimators. The SemEval 2010 organizers supplied a test set containing 8,915 manually
annotated examples covering 100 polysemous lemmas.
The average number of gold standard senses per lemma was 3.79. Overall,
27 systems participated and were ranked according to their performance on the test
set, applying the V-measure evaluation as well as paired F-score and a supervised
evaluation scheme. The systems were also compared against three baselines. For the
Most Frequent Sense (MFS) baseline all test instances of a given target lemma are
grouped into one cluster, that is, there is exactly one cluster per lemma. The second
baseline, Random, assigns each instance randomly to one of four clusters. The last
baseline, proposed in Manandhar and Klapaftis (2009), 1-cluster-per-instance (1ClI),
produces as many clusters as there are instances in the test set.
Table 1 gives an overview of the different systems and the three baselines (shown in
italics). The systems are presented in the order in which they were given in the official
679
Computational Linguistics Volume 40, Number 3
Table 1
V-measure computed with different estimators. Supervised recall is shown for comparison
(80:20 and 60:40 splits for mapping/evaluation, numbers as provided by Manandhar et al. 2010).
The corresponding ranks are shown in parentheses.
System C# ML MM JK BUB Supervised Recall
80:20 60:40
1ClI 89.1 31.6 (1) 29.5 (1) 27.4 (1) ?3.6 (29) ? ?
Hermit 10.8 16.2 (2) 13.1 (4) 10.7 (4) 11.0 (2) 58.3 (17) 57.3 (18)
UoY 11.5 15.7 (4) 14.3 (2) 13.1 (2) 11.4 (1) 62.4 (1) 62.0 (1)
KSU KDD 17.5 15.7 (3) 13.2 (3) 11.0 (3) 7.6 (3) 52.2 (24) 50.4 (25)
Duluth-WSI 4.1 9.0 (5) 6.9 (5) 5.7 (5) 5.6 (5) 60.5 (2) 59.5 (5)
Duluth-WSI-SVD 4.1 9.0 (6) 6.9 (6) 5.7 (6) 5.6 (6) 60.5 (3) 59.5 (4)
Duluth-R-110 9.7 8.6 (7) 4.7 (16) 1.9 (20) 3 (17) 54.8 (23) 53.6 (23)
Duluth-WSI-Co 2.5 7.9 (8) 6.4 (7) 5.7 (7) 5.7 (4) 60.8 (4) 60.1 (2)
KCDC-PCGD 2.9 7.8 (9) 6.3 (8) 5.5 (8) 5.2 (7) 59.5 (9) 59.1 (7)
KCDC-PC 2.9 7.5 (10) 6.2 (9) 5.4 (9) 5.0 (8) 59.7 (8) 58.9 (9)
KCDC-PC-2 2.9 7.1 (11) 5.7 (12) 4.9 (12) 4.5 (13) 59.8 (7) 58.9 (8)
Duluth-Mix-Narrow-Gap 2.4 6.9 (15) 5.5 (14) 4.8 (14) 4.8 (9) 56.6 (21) 56.2 (21)
KCDC-GD-2 2.8 6.9 (14) 5.7 (11) 4.9 (11) 4.6 (12) 58.7 (13) 57.9 (15)
KCDC-GD 2.8 6.9 (12) 5.8 (10) 5.0 (10) 4.6 (11) 59.0 (11) 58.3 (11)
Duluth-Mix-Narrow-PK2 2.7 6.8 (16) 5.4 (15) 4.6 (15) 4.6 (10) 56.1 (22) 55.7 (22)
Duluth-MIX-PK2 2.7 5.6 (17) 4.3 (17) 3.5 (17) 3.5 (16) 51.6 (25) 50.5 (24)
Duluth-R-15 5.0 5.3 (18) 2.4 (20) 0.7 (24) 1.3 (22) 56.8 (20) 56.5 (19)
Duluth-WSI-Co-Gap 1.6 4.8 (19) 4.1 (18) 3.8 (16) 4.1 (15) 60.3 (5) 59.5 (3)
Random 4.0 4.4 (20) 1.9 (22) 0.5 (25) 0.8 (24) 57.3 (19) 56.5 (20)
Duluth-R-13 3.0 3.6 (21) 1.5 (25) 0.5 (26) 0.7 (25) 58.0 (18) 57.6 (17)
Duluth-WSI-Gap 1.4 3.1 (22) 2.6 (19) 2.5 (18) 2.7 (18) 59.8 (6) 59.3 (6)
Duluth-Mix-Gap 1.6 3.0 (23) 2.3 (21) 1.9 (19) 2.0 (19) 50.6 (26) 49.8 (26)
Duluth-Mix-Uni-PK2 2.0 2.4 (24) 1.8 (23) 1.5 (21) 1.4 (21) 19.3 (27) 19.1 (27)
Duluth-R-12 2.0 2.3 (25) 0.8 (27) 0.2 (27) 0.3 (28) 58.5 (16) 57.7 (16)
KCDC-PT 1.5 1.9 (26) 1.6 (24) 1.4 (22) 1.5 (20) 58.9 (12) 58.3 (13)
Duluth-Mix-Uni-Gap 1.4 1.4 (27) 1.0 (26) 0.8 (23) 1.0 (23) 18.7 (28) 18.9 (28)
KCDC-GDC 2.8 6.9 (13) 5.7 (13) 4.8 (13) 4.5 (14) 59.1 (10) 58.3 (10)
MFS 1.0 0 (29) 0.0 (29) 0.0 (28) 0.5 (27) 58.7 (15) 58.3 (12)
Duluth-WSI-SVD-Gap 1.0 0.0 (28) 0.0 (28) 0.0 (29) 0.5 (26) 58.7 (14) 58.2 (14)
KCDC-PC-2* 2.9 5.7 7.2 2.3 2.2 ? ?
UoY* 11.5 25.1 22.8 17.8 5.0 ? ?
SemEval 2010 results table (Table 4 in Manandhar et al. (2010), p. 66). Table 1 shows
the average number of clusters per word (C#), the V-measure computed with different
estimators (ML, MM, JK, and BUB), and the rankings it produces (in brackets).7 For
comparison, the results of a supervised evaluation are also shown. The bottom two rows
(KCDC-PC-2? and UoY?) show the scores computed from the stochastic (weighted)
output (Section 3.3) for systems KCDC-PC-2 and UoY, respectively. Other systems did
not produce weighted output.
7 The ranking produced by the ML estimator should mirror that of the official results. In some cases it
does not?for example, system UoY was placed before KSU in the official results, whereas the ML
estimator would predict the reverse order. As the difference in V-measure is small, we attribute this
discrepancy to rounding errors. The system KCDC-GDC seems to be misplaced in the official results
list; according to V-measure it should be ranked higher. Our ranking was computed before rounding,
and there were no ties.
680
Li, Titov, and Sporleder Improved Estimation of Entropy for Word Sense Induction
0 2 4 6 8 10 12 14 16 18
0
0.01
0.02
0.03
0.04
0.05
0.06
0.07
Average Cluster Number
M
L?
JK
0 2 4 6 8 10 12 14 16 18
?0.01
0
0.01
0.02
0.03
0.04
0.05
0.06
0.07
0.08
0.09
Average Cluster Number
M
L?
BU
B
(a) H? - H?JK (b) H? - H?BUB
Figure 3
Discrepancy in estimates as a function of the predicted number of classes.
The 27 systems vary widely in the number of average clusters they output
per lemma, ranging from 1.02 (Duluth-WSI-SVD) to 17.5 (KSU-KDD). To assess the
influence of the cluster granularity on the entropy estimates, we compared the estimates
given by the ML estimator against those given by JK and BUB for different numbers of
clusters. Figure 3 plots the cluster numbers output by the systems against the estimate
difference for ML vs. JK (Figure 3a) and ML vs. BUB (Figure 3b). If two estimators
agree perfectly (i.e., produce the same estimate), their difference should always be
zero, independent of the number of clusters. As can be seen, this is not the case. As
expected, the difference is larger for systems with larger numbers of clusters, such as
KSU-KDD. This trend will result in unfair preference towards systems producing richer
clusterings.
Figure 4 shows the effect that the discrepancy in estimates has on the rankings
produced by using either of the three estimators. Figure 4a plots the ranking of the
ML estimator against JK, and Figure 4b plots the ranking of ML against BUB. Dots that
lie on the diagonal line indicate systems whose rank has not changed. It can be seen that
this only applies to a minority of the systems. In general, there are significant differences
between the rankings produced by ML and those by JK or BUB. We have seen that the
ML estimator can lead to counterintuitive and undesirable results, such as ranking the
0 5 10 15 20 25 30
0
5
10
15
20
25
30
ML Ranking (SemEval10)
JK
 R
an
kin
g
0 5 10 15 20 25 30
0
5
10
15
20
25
30
ML Ranking (SemEval10)
BU
B 
Ra
nk
in
g
(a) H? vs. H?JK (b) H? vs. H?BUB
Figure 4
Discrepancy in rankings as a function of the predicted number of classes.
681
Computational Linguistics Volume 40, Number 3
1-cluster-per-instance baseline highest. The BUB estimator corrects this and assigns the
last rank to this baseline.8
The estimate for the V-measure is based on the estimates of the marginal and joint
entropies. To confirm our intuition that joint entropies are more significantly corrected,
we looked into the differences between estimates of each entropy for five systems with
the largest number of clusters (excluding the 1C1I baseline). The average differences
in estimation of H(k) and H(k, c) between JK and ML estimators are 0.08 and 0.16,
respectively, confirming our hypothesis. Analogous discrepancies for the pair BUB vs.
ML are 0.15 and 0.06, respectively. The differences in the entropy of the gold standard
clustering, H(c), is less significant (< 0.02 for both methods) as the gold standard is less
fine-grained than the clusters proposed by these five systems.
For the stochastic version evaluation, we can observe that the score for the KCDC-
PC-2 system is mostly decreased with respect to the ?deterministic? evaluation (except
for the MM estimator). Conversely, the score of UoY is mostly improved except to
the prediction of the BUB estimator. These differences are somewhat surprising: The
stochastic version resulted in significantly larger disagreement between the estimators
than the deterministic version. We do not yet have a satisfactory explanation for this
phenomenon.
It is important to notice that for the vast majority of the systems there is agreement
between the scores of the JK and BUB estimator, wheres the ML estimator significantly
overestimates the V-measure for most of the systems. This observation, coupled with
the observed behavior of the JK and BUB estimators in the simulations, suggest that
their predictions are considerably more reliable than predictions of the plug-in ML
estimator.
Comparing the V-measure (BUB) rankings to those obtained by supervised evalu-
ation (last two columns in Table 1) shows noticeable differences. Several systems that
rank highly according to the V-measure occupy the lower end of the scale when evalu-
ated according to supervised recall (Hermit, KSU KDD, Duluth-Mix-Narrow-Gap).
6. Conclusions
In this work, we analyzed the shortcomings of information-theoretic measures in the
context of WSI evaluation and argued that main drawbacks of these approaches, such
as the preference for the systems predicting richer clusterings or assigning the top score
to the 1-cluster-per-instance baseline, are caused by the bias of the underlying sample-
based estimates of entropy. We studied alternative estimators, including one specifically
designed to deal with cases where the number of examples is comparable with the num-
ber of clusters. Two of the considered estimators, the jackknifed estimator and the best-
upper-bound estimator, achieve consistently and significantly less biased results than
the standard ML estimator when evaluated in simulations with Zipfian distributions.
The corresponding estimates in the WSI evaluation context can result in significant
changes in scores and relative rankings, with systems producing richer clusterings more
severely affected. We believe that these results strongly suggest that more accurate
estimates of entropy should be used in future evaluations of sense induction systems.
Other unsupervised tasks in natural language processing, such as word clustering or
8 Note that the V-measure is actually negative here. Though this is not possible for the true V-measure,
the estimated V-measure expresses a difference between the estimated joint entropy and the marginal
entropies and can be negative.
682
Li, Titov, and Sporleder Improved Estimation of Entropy for Word Sense Induction
named entity disambiguation, may also benefit from using information-theoretic scores
based on more accurate estimators.
Appendix A: Derivation for the BUB Estimator
We provide a brief derivation for the BUB estimator and refer the reader to Paninski
(2003) for details and discussion. The BUB estimator is obtained by minimizing an upper
bound on MSE for estimators H(a) (see Equation (2)). First, MSE is bounded from above
by maximizing the variance and the bias independently:
max
p
Bp(H?(a))
2
+ Vp(H?(a)) ? max
p
Bp(H?(a))
2
+ max
p
Vp(H?(a)) (A.1)
where p = (p1, . . . , pm) is an underlying discrete measure; Bp and Vp are the bias and the
variance of the estimator given p. Then individual bounds both for the squared bias and
the variance can be constructed.
We start by deriving a bound for the bias. Using linearity of expectation, the expec-
tation of H?(a) can be written as
Ep(H?(a)) =
N
?
j=0
aj,NEp(hj) =
N
?
j=0
aj,N
m
?
i=1
Bj,N(pi)
where Bj,N(x) is the binomial polynomial
(
N
j
)
xj(1 ? x)N?j. Then, with simple algebra,
we have
Bp(H?(a)) =
m
?
i=1
?
?
N
?
j=0
aj,NBj,N(pi) ? H(pi)
?
?
where H(x) = ?x log x, the entropy function. A uniform upper bound can be obtained
by bounding each term in the sum:
|Bp(H?(a))| ? m sup
x
|
N
?
j=0
aj,NBj,N(x) ? H(x)|
However, this bound is not too tight as it would overemphasize importance of the
approximation quality for components i with pi close to 1. Intuitively, the behavior near
0 is more important, as there can be more components pi close to 0. Paninski (2003)
generalizes this bound by considering a weighted version
|Bp(H?(a))| ? 2 sup
x
f (x)|
N
?
j=0
aj,NBj,N(x) ? H(x)| (A.2)
with the function f chosen to emphasize smaller components
f (x) =
{
m, x < 1m
1/x, x ? 1m
683
Computational Linguistics Volume 40, Number 3
As shown in Antos and Kontoyiannis (2001) and in Paninski (2003), bounds on
the variance of the estimator H?(a) can be derived using either McDiarmid or Steele
bounds (Steele 1986). For the Steele bound, it has the form
Vp(H?(a)) < N max
0?j<N
(aj+1 ? aj)
2 (A.3)
Finally, MSE can be bounded by substituting Equations (A.2) and (A.3) into the
inequality (A.1). For computational reasons, instead of choosing a to minimize the
bound, the L2 relaxation of the L? loss is used, resulting in a regularized least-squares
problem.
Acknowledgments
The research was carried out when the
authors were at Saarland University. It was
funded by the German Research Foundation
DFG (Cluster of Excellence on Multimodal
Computing and Interaction, Saarland
University, Germany). We would like to
thank the anonymous reviewers for their
valuable feedback.
References
Agirre, Eneko, David Mart??nez, Oier Lo?pez
de Lacalle, and Aitor Soroa. 2006. Evaluating
and optimizing the parameters of an
unsupervised graph-based WSD algorithm.
In Workshop on TextGraphs, at HLT-NAACL
2006, pages 89?96, New York, NY.
Agirre, Eneko and Aitor Soroa. 2007.
Semeval-2007 task 02: Evaluating word
sense induction and discrimination
systems. In Proceedings of the 4th
International Workshop on Semantic
Evaluation, pages 7?12, Prague.
Amigo, Enrique, Julio Gonzalo, Javier
Artiles, and Felisa Verdejo. 2009. A
comparison of extrinsic clustering
evaluation metrics based on formal
constraints. Information Retrieval,
12(4):461?486.
Antos, A. and I. Kontoyiannis. 2001.
Convergence properties of functional
estimates of discrete distributions. Random
Structures and Algorithms, 19:163?193.
Bagga, Amit and Breck Baldwin. 1998.
Algorithms for scoring coreference chains.
In The First International Conference on
Language Resources and Evaluation Workshop
on Linguistics Coreference, pages 563?566,
Granada.
Batu, Tugkan, Sanjoy Dasgupta, Ravi Kumar,
and Ronitt Rubinfeld. 2002. The complexity
of approximating entropy. In Symposium
on the Theory of Computing (STOC),
pages 678?687, Montreal.
Dom, Byron E. 2001. An information-
theoretic external cluster-validity measure.
Technical Report No. RJ10219, IBM.
Grasberger, P. and T. Schu?rmann. 1996.
Entropy estimation of symbol sequences.
CHAOS, 6(3):414?427.
Kilgarriff, Adam. 2004. How dominant
is the commonest sense of a word? In
Sojka, Kopecek, and Pala, editors, Text,
Speech, Dialogue, volume 3206 of Lecture
Notes in Artificial Intelligence. Springer,
pages 103?112.
Klapaftis, Ioannis P. and Suresh Manandhar.
2013. Evaluating word sense induction
and disambiguation methods. Language
Resources and Evaluation, 47(3):1?27.
Luo, Xiaoqiang. 2005. On coreference
resolution performance metrics. In
Proceedings of the Conference on Human
Language Technology and Empirical Methods
in Natural Language Processing (HLT-05),
pages 25?32, Vancouver.
Manandhar, Suresh and Ioannis Klapaftis.
2009. Semeval-2010 task 14: Evaluation
setting for word sense induction &
disambiguation systems. In Proceedings of
the Workshop on Semantic Evaluations:
Recent Achievements and Future Directions
(SEW-2009), pages 117?122, Boulder, CO.
Manandhar, Suresh, Ioannis Klapaftis,
Dmitriy Dligach, and Sameer Pradhan.
2010. Semeval-2010 task 14: Word sense
induction & disambiguation. In Proceedings
of the 5th International Workshop on Semantic
Evaluation, pages 63?68, Uppsala.
Meila, Marina. 2007. Comparing
clusterings?an information based
distance. Journal of Multivariate Analysis,
98:873?895.
Miller, G. 1955. Note on the bias of
information estimates. Information Theory
in Psychology II-B, pages 95?100.
Paninski, Liam. 2003. Estimation of entropy
and mutual information. Neural
Computation, 15:1,191?1,253.
684
Li, Titov, and Sporleder Improved Estimation of Entropy for Word Sense Induction
Paninski, Liam. 2004. Estimating entropy
of m bins given fewer than m samples.
IEEE Transactions on Information Theory,
50(9):2,200?2,203.
Purandare, Amruta and Ted Pedersen. 2004.
Word sense discrimination by clustering
contexts in vector and similarity spaces.
In Proceedings of the CoNLL, pages 41?48,
Boston, MA.
Quenouille, M. 1956. Notes on bias and
estimation. Biometrika, 43:353?360.
Rosenberg, Andrew and Julia Hirschberg.
2007. V-measure: A conditional entropy-
based external cluster evaluation
measure. In Proceedings of the 2007
EMNLP-CoNll Joint Conference,
pages 410?420, Prague.
Schu?tze, Hinrich. 1998. Automatic word
sense discrimination. Computational
Linguistics, 24(1):97?123.
Steele, J. Michael. 1986. An Efron-Stein
inequality for nonsymmetric statistics.
Annals of Statistics, 14:753?758.
Strehl, Alexander and Joydeep Gosh. 2002.
Cluster ensembles: A knowledge reuse
framework for combining multiple
partitions. Journal of Machine Learning
Research, 3:583?617.
Strong, S., R. Koberle, S. R. van de Ruyter,
and W. Bialek. 1998. Entropy and
information in neural spike trains.
Physical Review Letters, 80:197?202.
Tukey, J. 1958. Bias and confidence in not
quite large samples. Annals of Mathematical
Statistics, 29:614.
Wang, Y. H. 1993. On the number of
successes in independent trials. Statistica
Sinica 3, 2:295?312.
Witte, Ian and Eibe Frank. 2005. Data
Mining: Practical Machine Learning Tools
and Techniques. Morgan Kaufmann,
Amsterdam.
Zhao, Y. and G. Karypis. 2005. Hierarchical
clustering algorithms for document
datasets. Data Mining and Knowledge
Discovery, 10(2):141?168.
685

Tutorials, NAACL-HLT 2013, pages 10?12,
Atlanta, Georgia, June 9 2013. c?2013 Association for Computational Linguistics
Semantic Role Labeling
Martha Palmer?, Ivan Titov?, Shumin Wu?
?University of Colorado
?Saarland University
Martha.Palmer@colorado.edu
titovian,wushumin@gmail.com
1 Overview
This tutorial will describe semantic role labeling, the assignment of semantic roles
to eventuality participants in an attempt to approximate a semantic representation
of an utterance. The linguistic background and motivation for the definition of
semantic roles will be presented, as well as the basic approach to semantic role
annotation of large amounts of corpora. Recent extensions to this approach that
encompass light verb constructions and predicative adjectives will be included,
with reference to their impact on English, Arabic, Hindi and Chinese. Current
proposed extensions such as Abstract Meaning Representations and richer event
representations will also be touched on.
Details of machine learning approaches will be provided, beginning with fully
supervised approaches that use the annotated corpora as training material. The
importance of syntactic parse information and the contributions of different feature
choices, including tree kernels, will be discussed, as well as the advantages and
disadvantages of particular machine learning algorithms and approaches such as
joint inference. Appropriate considerations for evaluation will be presented as well
as successful uses of semantic role labeling in NLP applications.
We will also cover techniques for exploiting unlabeled corpora and transfer-
ring models across languages. These include methods, which project annotations
across languages using parallel data, induce representations solely from unlabeled
corpora (unsupervised methods) or exploit a combination of a small amount of hu-
man annotation and a large unlabeled corpus (semi-supervised techniques). We
will discuss methods based on different machine learning paradigms, including
generative Bayesian models, graph-based algorithms and bootstrapping style tech-
niques.
10
2 Outline
I. Introduction, background and annotation
? Motivation ? who did what to whom
? Linguistic Background
? Basic Annotation approach
? Recent extensions
? Language Specific issues with English, Arabic, Hindi and Chinese
? Semlink ? Mapping between PropBank, VerbNet and FrameNet.
? The next step ? Events and Abstract Meaning Representations
II. Supervised Machine Learning for SRL
? Identification and Classification
? Features (tree kernel, English vs. Chinese)
? Choice of ML method and feature combinations (kernel vs feature space)
? Joint Inference
? Impact of Parsing
? Evaluation
? Applications (including multi-lingual)
III. Semi-supervised and Unsupervised Approaches
? Cross-lingual annotation projection methods and direct transfer of SRL mod-
els across languages
? Semi-supervised learning methods
? Unsupervised induction
? Adding supervision and linguistic priors to unsupervised methods
11
3 Speaker Bios
Martha Palmer1 is a Professor of Linguistics and Computer Science, and a Fel-
low of the Institute of Cognitive Science at the University of Colorado. Her current
research is aimed at building domain-independent and language independent tech-
niques for semantic interpretation based on linguistically annotated data, such as
Proposition Banks. She has been the PI on NSF, NIH and DARPA projects for
linguistic annotation (syntax, semantics and pragmatics) of English, Chinese, Ko-
rean, Arabic and Hindi. She has been a member of the Advisory Committee for
the DARPA TIDES program, Chair of SIGLEX, Chair of SIGHAN, a past Presi-
dent of the Association for Computational Linguistics, and is a Co-Editor of JNLE
and of LiLT and is on the CL Editorial Board. She received her Ph.D. in Artificial
Intelligence from the University of Edinburgh in 1985.
Ivan Titov2 joined the Saarland University as a junior faculty and head of a
research group in November 2009, following a postdoc at the University of Illi-
nois at Urbana-Champaign. He received his Ph.D. in Computer Science from the
University of Geneva in 2008 and his master?s degree in Applied Mathematics
and Informatics from the St. Petersburg State Polytechnic University (Russia) in
2003. His research interests are in statistical natural language processing (models
of syntax, semantics and sentiment) and machine learning (structured prediction
methods, latent variable models, Bayesian methods).
Shumin Wu is a Computer Science PhD student (advised by Dr. Martha
Palmer) at the University of Colorado. His current research is aimed at developing
and applying semantic mapping (aligning and jointly inferring predicate-argument
structures between languages) to Chinese dropped-pronoun recovery/alignment,
automatic verb class induction, and other applications relevant to machine transla-
tion.
1http://verbs.colorado.edu/?mpalmer/
2http://people.mmci.uni-saarland.de/?titov/
12
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 958?967,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Bootstrapping Semantic Analyzers from Non-Contradictory Texts
Ivan Titov Mikhail Kozhevnikov
Saarland University
Saarbru?cken, Germany
{titov|m.kozhevnikov}@mmci.uni-saarland.de
Abstract
We argue that groups of unannotated texts
with overlapping and non-contradictory
semantics represent a valuable source of
information for learning semantic repre-
sentations. A simple and efficient infer-
ence method recursively induces joint se-
mantic representations for each group and
discovers correspondence between lexical
entries and latent semantic concepts. We
consider the generative semantics-text cor-
respondence model (Liang et al, 2009)
and demonstrate that exploiting the non-
contradiction relation between texts leads
to substantial improvements over natu-
ral baselines on a problem of analyzing
human-written weather forecasts.
1 Introduction
In recent years, there has been increasing inter-
est in statistical approaches to semantic parsing.
However, most of this research has focused on su-
pervised methods requiring large amounts of la-
beled data. The supervision was either given in
the form of meaning representations aligned with
sentences (Zettlemoyer and Collins, 2005; Ge and
Mooney, 2005; Mooney, 2007) or in a some-
what more relaxed form, such as lists of candidate
meanings for each sentence (Kate and Mooney,
2007; Chen and Mooney, 2008) or formal repre-
sentations of the described world state for each
text (Liang et al, 2009). Such annotated resources
are scarce and expensive to create, motivating the
need for unsupervised or semi-supervised tech-
niques (Poon and Domingos, 2009). However,
unsupervised methods have their own challenges:
they are not always able to discover semantic
equivalences of lexical entries or logical forms or,
on the contrary, cluster semantically different or
even opposite expressions (Poon and Domingos,
2009). Unsupervised approaches can only rely on
distributional similarity of contexts (Harris, 1968)
to decide on semantic relatedness of terms, but this
information may be sparse and not reliable (Weeds
and Weir, 2005). For example, when analyzing
weather forecasts it is very hard to discover in an
unsupervised way which of the expressions among
?south wind?, ?wind from west? and ?southerly?
denote the same wind direction and which are not,
as they all have a very similar distribution of their
contexts. The same challenges affect the problem
of identification of argument roles and predicates.
In this paper, we show that groups of unanno-
tated texts with overlapping and non-contradictory
semantics provide a valuable source of informa-
tion. This form of weak supervision helps to
discover implicit clustering of lexical entries and
predicates, which presents a challenge for purely
unsupervised techniques. We assume that each
text in a group is independently generated from
a full latent semantic state corresponding to the
group. Importantly, the texts in each group do
not have to be paraphrases of each other, as they
can verbalize only specific parts (aspects) of the
full semantic state, yet statements about the same
aspects must not contradict each other. Simulta-
neous inference of the semantic state for the non-
contradictory and semantically overlapping docu-
ments would restrict the space of compatible hy-
potheses, and, intuitively, ?easier? texts in a group
will help to analyze the ?harder? ones.1
As an illustration of why this weak supervi-
sion may be valuable, consider a group of two
non-contradictory texts, where one text mentions
?2.2 bn GBP decrease in profit?, whereas another
one includes a passage ?profit fell by 2.2 billion
pounds?. Even if the model has not observed
1This view on this form of supervision is evocative of co-
training (Blum and Mitchell, 1998) which, roughly, exploits
the fact that the same example can be ?easy? for one model
but ?hard? for another one.
958
Current?temperature?is?about?70F,
Cith?high?of?around?75F?amd?low
of?around?64u
Overcast,
Rain?is?quite?possible?tonight,
as?t-storms?arru
South?wind?of?around?19?mph.
Cu
u r euA?slight?chance?of?showers?
Mostly?cloudy,??
with?a?high?near?75.
South?wind?between?15?and?20?mph,
Chance?of?precipitation?is?30%.
with?gusts?as?high?as?30?mph.
and?thunderstorms?after?noon.
Thunderstorms?and?pouring?are?possiblr
throughout?the?day,
with?precipitation?chance?of?about?25%.
possibly?growing?up?to?75?F?during?the?day,
as?south?wind?blows?at?about?20?mphu
The?sky?is?heavyu
It?is?70?F?now,
erntr?mep?raiesnrabao70F,anshabaogfanmdaba5lfanrmhaba5w6
Csh4Os?iesnrbo70Ffnv4r bc6
Rpqeiesnrbo70Ffanshbwfanmdb0-fanrmhb0l6
t?rSst1verhesm9iesnrbo70Ffnshb0wfnmdb.0fnrmhb 0o6
eAph4r?MAmhSriesnrbo70Ffnv4rbSAmhSr6
y?rr2shR3mshMAmhSriesnrbF57.wfnv4rb776
q9rreMAmhSriesnrb%o70F%fnv4rb776
qTkSvIr?iesnrbo70FfpST reb5l7Fww6
Csh4ctrr4iesnrbo70F,an shbFgfnmdb00fnrmhbF-fapSTrebFw70w6
?mshMAmhSriesnrbo70Ffnv4rbSAmhSr6
Csh4MAs99iesnrbo70Ffnshbwfnmdbwfnrmhbw6
aaaaaaaaaaaaaaaauuuuuu
Figure 1: An example of three non-contradictory weather forecasts and their alignment to the semantic
representation. Note that the semantic representation (the block in the middle) is not observable in
training.
the word ?fell? before, it is likely to align these
phrases to the same semantic form because of sim-
ilarity of their arguments. And this alignment
would suggest that ?fell? and ?decrease? refer to
the same process, and should be clustered together.
This would not happen for the pair ?fell? and ?in-
crease? as similarity of their arguments would nor-
mally entail contradiction. Similarly, in the exam-
ple mentioned earlier, when describing a forecast
for a day with expected south winds, texts in the
group can use either ?south wind? or ?southerly?
to indicate this fact but no texts would verbalize
it as ?wind from west?, and therefore these ex-
pressions will be assigned to different semantic
clusters. However, it is important to note that the
phrase ?wind from west? may still appear in the
texts, but in reference to other time periods, un-
derlying the need for modeling alignment between
grouped texts and their latent meaning representa-
tion.
As much of the human knowledge is re-
described multiple times, we believe that non-
contradictory and semantically overlapping texts
are often easy to obtain. For example, consider
semantic analysis of news articles or biographies.
In both cases we can find groups of documents re-
ferring to the same events or persons, and though
they will probably focus on different aspects and
have different subjective passages, they are likely
to agree on the core information (Shinyama and
Sekine, 2003). Alternatively, if such groupings are
not available, it may still be easier to give each se-
mantic representation (or a state) to multiple an-
notators and ask each of them to provide a tex-
tual description, instead of annotating texts with
semantic expressions. The state can be communi-
cated to them in a visual or audio form (e.g., as
a picture or a short video clip) ensuring that their
interpretations are consistent.
Unsupervised learning with shared latent se-
mantic representations presents its own chal-
lenges, as exact inference requires marginalization
over possible assignments of the latent semantic
state, consequently, introducing non-local statisti-
cal dependencies between the decisions about the
semantic structure of each text. We propose a sim-
ple and fairly general approximate inference algo-
rithm for probabilistic models of semantics which
is efficient for the considered model, and achieves
favorable results in our experiments.
In this paper, we do not consider models
which aim to produce complete formal meaning
of text (Zettlemoyer and Collins, 2005; Mooney,
2007; Poon and Domingos, 2009), instead focus-
ing on a simpler problem studied in (Liang et al,
2009). They investigate grounded language ac-
quisition set-up and assume that semantics (world
state) can be represented as a set of records each
consisting of a set of fields. Their model seg-
ments text into utterances and identifies records,
fields and field values discussed in each utter-
ance. Therefore, one can think of this problem as
an extension of the semantic role labeling prob-
lem (Carreras and Marquez, 2005), where predi-
cates (i.e. records in our notation) and their ar-
guments should be identified in text, but here ar-
guments are not only assigned to a specific role
(field) but also mapped to an underlying equiv-
alence class (field value). For example, in the
weather forecast domain field sky cover should get
the same value given expressions ?overcast? and
?very cloudy? but a different one if the expres-
959
sions are ?clear? or ?sunny?. This model is hard
to evaluate directly as text does not provide in-
formation about all the fields and does not neces-
sarily provide it at the sufficient granularity level.
Therefore, it is natural to evaluate their model
on the database-text alignment problem (Snyder
and Barzilay, 2007), i.e. measuring how well the
model predicts the alignment between the text and
the observable records describing the entire world
state. We follow their set-up, but assume that in-
stead of having access to the full semantic state
for every training example, we have a very small
amount of data annotated with semantic states and
a larger number of unannotated texts with non-
contradictory semantics.
We study our set-up on the weather forecast
data (Liang et al, 2009) where the original textual
weather forecasts were complemented by addi-
tional forecasts describing the same weather states
(see figure 1 for an example). The average overlap
between the verbalized fields in each group of non-
contradictory forecasts was below 35%, and more
than 60% of fields are mentioned only in a single
forecast from a group. Our model, learned from
100 labeled forecasts and 259 groups of unanno-
tated non-contradictory forecasts (750 texts in to-
tal), achieved 73.9% F1. This compares favorably
with 69.1% shown by a semi-supervised learning
approach, though, as expected, does not reach the
score of the model which, in training, observed se-
mantics states for all the 750 documents (77.7%
F1).
The rest of the paper is structured as follows.
In section 2 we describe our inference algorithm
for groups of non-contradictory documents. Sec-
tion 3 redescribes the semantics-text correspon-
dence model (Liang et al, 2009) in the context of
our learning scenario. In section 4 we provide an
empirical evaluation of the proposed method. We
conclude in section 5 with an examination of ad-
ditional related work.
2 Inference with Non-Contradictory
Documents
In this section we will describe our inference
method on a higher conceptual level, not speci-
fying the underlying meaning representation and
the probabilistic model. An instantiation of the
algorithm for the semantics-text correspondence
model is given in section 3.2.
Statistical models of parsing can often be re-
garded as defining the probability distribution of
meaning m and its alignment a with the given
text w, P (m,a,w) = P (a,w|m)P (m). The
semantics m can be represented either as a logical
formula (see, e.g., (Poon and Domingos, 2009)) or
as a set of field values if database records are used
as a meaning representation (Liang et al, 2009).
The alignment a defines how semantics is verbal-
ized in the text w, and it can be represented by
a meaning derivation tree in case of full semantic
parsing (Poon and Domingos, 2009) or, e.g., by
a hierarchical segmentation into utterances along
with an utterance-field alignment in a more shal-
low variation of the problem. In semantic parsing,
we aim to find the most likely underlying seman-
tics and alignment given the text:
(m?, a?) = argmax
m,a
P (a,w|m)P (m). (1)
In the supervised case, where a and m are observ-
able, estimation of the generative model parame-
ters is generally straightforward. However, in a
semi-supervised or unsupervised case variational
techniques, such as the EM algorithm (Demp-
ster et al, 1977), are often used to estimate the
model. As common for complex generative mod-
els, the most challenging part is the computation
of the posterior distributions P (a,m|w) on the
E-step which, depending on the underlying model
P (m,a,w), may require approximate inference.
As discussed in the introduction, our goal is to
integrate groups of non-contradictory documents
into the learning procedure. Let us denote by
w1,...,wK a group of non-contradictory docu-
ments. As before, the estimation of the poste-
rior probabilities P (mi,ai|w1 . . .wK) presents
the main challenge. Note that the decision about
mi is now conditioned on all the texts wj rather
than only on wi. This conditioning is exactly what
drives learning, as the information about likely se-
mantics mj of text j affects the decision about
choice of mi:
P (mi|w1,...,wK) ?
?
ai
P (ai,wi|mi)?
?
?
m?i,a?i
P (mi|m?i)P (m?i,a?i,w?i), (2)
where x?i denotes {xj : j 6= i}. P (mi|m?i)
is the probability of the semantics mi given all
the meanings m?i. This probability assigns zero
weight to inconsistent meanings, i.e. such mean-
960
ings (m1,...,mK) that ?Ki=1mi is not satisfiable,
2
and models dependencies between components in
the composite meaning representation (e.g., argu-
ments values of predicates). As an illustration, in
the forecast domain it may express that clouds, and
not sunshine, are likely when it is raining. Note,
that this probability is different from the probabil-
ity that mi is actually verbalized in the text.
Unfortunately, these dependencies between mi
and wj are non-local. Even though the dependen-
cies are only conveyed via {mj : j 6= i} the space
of possible meaningsm is very large even for rela-
tively simple semantic representations, and, there-
fore, we need to resort to efficient approximations.
One natural approach would be to use a form
of belief propagation (Pearl, 1982; Murphy et al,
1999), where messages pass information about
likely semantics between the texts. However, this
approach is still expensive even for simple mod-
els, both because of the need to represent distribu-
tions over m and also because of the large number
of iterations of message exchange needed to reach
convergence (if it converges).
An even simpler technique would be to parse
texts in a random order conditioning each mean-
ing m?k for k ? {1,...,K} on all the previous se-
mantics m?<k = m
?
1,...,m
?
k?1:
m?k = argmax
mk
P (wk|mk)P (mk|m
?
<k).
Here, and in further discussion, we assume that
the above search problem can be efficiently solved,
exactly or approximately. However, a major weak-
ness of this algorithm is that decisions about com-
ponents of the composite semantic representation
(e.g., argument values) are made only on the ba-
sis of a single text, which first mentions the cor-
responding aspects, without consulting any future
texts k? > k, and these decisions cannot be revised
later.
We propose a simple algorithm which aims to
find an appropriate order of the greedy inference
by estimating how well each candidate semantics
m?k would explain other texts and at each step se-
lecting k (and m?k) which explains them best.
The algorithm, presented in figure 23, con-
structs an ordering of texts n = (n1,..., nK)
2Note that checking for satisfiability may be expensive or
intractable depending on the formalism.
3We slightly abuse notation by using set operations with
the lists n and m? as arguments. Also, for all the document
indices j we use j /? S to denote j ? {1,...,K}\S.
1: n := (), m? := ()
2: for i := 1 : K ? 1 do
3: for j /? n do
4: m?j := argmaxmj P (mj ,wj |m
?)
5: end for
6: ni := argmaxj /?n P (m?j ,wj |m
?)?
?
?
k/?n?{j}maxmk P (mk,wk|m
?, m?j)
7: m?i := m?ni
8: end for
9: nK := {1,...,K}\n
10: m?K := argmaxmnK P (mnK ,wnK |m
?)
Figure 2: The approximate inference algorithm.
and corresponding meaning representations m? =
(m?1,...,m
?
K), where m
?
k is the predicted mean-
ing representation of text wnk . It starts with an
empty ordering n = () and an empty list of mean-
ings m? = () (line 1). Then it iteratively pre-
dicts meaning representations m?j conditioned on
the list of semantics m? = (m?1,...,m
?
i?1) fixed
on the previous stages and does it for all the re-
maining texts wj (lines 3-5). The algorithm se-
lects a single meaning m?j which maximizes the
probability of all the remaining texts and excludes
the text j from future consideration (lines 6-7).
Though the semantics mk (k /? n?{j}) used in
the estimates (line 6) can be inconsistent with each
other, the final list of meanings m? is guaranteed
to be consistent. It holds because on each iteration
we add a single meaning m?ni to m
? (line 7), and
m?ni is guaranteed to be consistent with m
?, as the
semantics m?ni was conditioned on the meaning
m? during inference (line 4).
An important aspect of this algorithm is that un-
like usual greedy inference, the remaining (?fu-
ture?) texts do affect the choice of meaning rep-
resentations made on the earlier stages. As soon
as semantics m?k are inferred for every k, we find
ourselves in the set-up of learning with unaligned
semantic states considered in (Liang et al, 2009).
The induced alignments a1,...,aK of semantics
m? to texts w1,...,wK at the same time induce
alignments between the texts. The problem of pro-
ducing multiple sequence alignment, especially in
the context of sentence alignments, has been ex-
tensively studied in NLP (Barzilay and Lee, 2003).
In this paper, we use semantic structures as a pivot
for finding the best alignment in the hope that pres-
ence of meaningful text alignments will improve
the quality of the resulting semantic structures by
enforcing a form of agreement between them.
961
3 A Model of Semantics
In this section we redescribe the semantics-text
correspondence model (Liang et al, 2009) with an
extension needed to model examples with latent
states, and also explain how the inference algo-
rithm defined in section 2 can be applied to this
model.
3.1 Model definition
Liang et al (2009) considered a scenario where
each text was annotated with a world state, even
though alignment between the text and the state
was not observable. This is a weaker form of
supervision than the one traditionally considered
in supervised semantic parsing, where the align-
ment is also usually provided in training (Chen and
Mooney, 2008; Zettlemoyer and Collins, 2005).
Nevertheless, both in training and testing the
world state is observable, and the alignment and
the text are conditioned on the state during infer-
ence. Consequently, there was no need to model
the distribution of the world state. This is differ-
ent for us, and we augment the generative story by
adding a simplistic world state generation step.
As explained in the introduction, the world
states s are represented by sets of records (see the
block in the middle of figure 1 for an example of
a world state). Each record is characterized by a
record type t ? {1,..., T}, which defines the set of
fields F (t). There are n(t) records of type t and
this number may change from document to docu-
ment. For example, there may be more than a sin-
gle record of type wind speed, as they may refer
to different time periods but all these records have
the same set of fields, such as minimal, maximal
and average wind speeds. Each field has an asso-
ciated type: in our experiments we consider only
categorical and integer fields. We write s(t)n,f = v
to denote that n-th record of type t has field f set
to value v.
Each document k verbalizes a subset of the en-
tire world state, and therefore semantics mk of
the document is an assignment to |mk| verbalized
fields: ?|mk|q=1 (s
(tq)
nq ,fq
= vq), where tq, nq, fq are
the verbalized record types, records and fields, re-
spectively, and vq is the assigned field value. The
probability of meaning mk then equals the prob-
ability of this assignment with other state vari-
ables left non-observable (and therefore marginal-
ized out). In this formalism checking for con-
tradiction is trivial: two meaning representations
Figure 3: The semantics-text correspondence
model with K documents sharing the same latent
semantic state.
contradict each other if they assign different val-
ues to the same field of the same record.
The semantics-text correspondence model de-
fines a hierarchical segmentation of text: first, it
segments the text into fragments discussing differ-
ent records, then the utterances corresponding to
each record are further segmented into fragments
verbalizing specific fields of that record. An exam-
ple of a segmented fragment is presented in fig-
ure 4. The model has a designated null-record
which is aligned to words not assigned to any
record. Additionally there is a null-field in each
record to handle words not specific to any field.
In figure 3 the corresponding graphical model is
presented. The formal definition of the model for
documents w1,...,wK sharing a semantic state is
as follows:
? Generation of world state s:
? For each type ? ? {1,..., T} choose a number of
records of that type n(?) ? Unif(1,..., nmax).
? For each record s(?)n , n ? {1, .., n(?)} choose
field values s(?)nf for all fields f ? F
(?) from the
type-specific distribution.
? Generation of the verbalizations, for each document
wk, k ? {1,...,K}:4
? Record Types: Choose a sequence of verbalized
record types t = (t1,..., t|t|) from the first-order
Markov chain.
? Records: For each type ti choose a verbalized
record ri from all the records of that type: l ?
Unif(1,..., n(?)), ri := s
(ti)
l .
? Fields: For each record ri choose a sequence of
verbalized fields f i = (fi1,..., fi|fi|) from the
first-order Markov chain (fij ? F (ti)).
? Length: For each field fij , choose length cij ?
Unif(1,..., cmax).
? Words: Independently generate cij words from
the field-specific distribution P (w|fij , rifij ).
4We omit index k in the generative story and figure 3 to
simplify the notation.
962
Figure 4: A segmentation of a text fragment into records and fields.
Note that, when generating fields, the Markov
chain is defined over fields and the transition pa-
rameters are independent of the field values rifij .
On the contrary, when drawing a word, the distri-
bution of words is conditioned on the value of the
corresponding field.
The form of word generation distributions
P (w|fij , rifij ) depends on the type of the field
fi,j . For categorical fields, the distribution of
words is modeled as a distinct multinomial for
each field value. Verbalizations of numerical fields
are generated via a perturbation on the field value
rifij : the value rifij can be perturbed by either
rounding it (up or down) or distorting (up or down,
modeled by a geometric distribution). The param-
eters corresponding to each form of generation are
estimated during learning. For details on these
emission models, as well as for details on model-
ing record and field transitions, we refer the reader
to the original publication (Liang et al, 2009).
In our experiments, when choosing a world
state s, we generate the field values independently.
This is clearly a suboptimal regime as often there
are very strong dependencies between field val-
ues: e.g., in the weather domain many record
types contain groups of related fields defining min-
imal, maximal and average values of some param-
eter. Extending the method to model, e.g., pair-
wise dependencies between field values is rela-
tively straightforward.
As explained above, semantics of a textm is de-
fined by the assignment of state variables s. Anal-
ogously, an alignment a between semantics m
and a text w is represented by all the remaining
latent variables: by the sequence of record types
t = (t1,..., t|t|), choice of records ri for each ti,
the field sequence f i and the segment length cij
for every field fij .
3.2 Learning and inference
We select the model parameters ? by maximiz-
ing the marginal likelihood of the data, where
the data D is given in the form of groups w =
{w1,...,wK} sharing the same latent state:5
max
?
?
w?D
?
s
P (s)
?
k
?
r,f ,c
P (r,f , c,wk|s, ?).
To estimate the parameters, we use the
Expectation-Maximization algorithm (Dempster
et al, 1977). When the world state is observ-
able, learning does not require any approxima-
tions, as dynamic programming (a form of the
forward-backward algorithm) can be used to in-
fer the posterior distribution on the E-step (Liang
et al, 2009). However, when the state is latent,
dependencies are not local anymore, and approxi-
mate inference is required.
We use the algorithm described in section 2 (fig-
ure 2) to infer the state. In the context of the
semantics-text correspondence model, as we dis-
cussed above, semantics m defines the subset of
admissible world states. In order to use the algo-
rithm, we need to understand how the conditional
probabilities of the form P (m?|m) are computed,
as they play the key role in the inference proce-
dure (see equation (2)). If there is a contradiction
(m??m) then P (m?|m) = 0, conversely, if m?
is subsumed by m (m ? m?) then this proba-
bility is 1. Otherwise, P (m?|m) equals the prob-
ability of new assignments ?|m
?\m|
q=1 (s
(t?q)
n?q ,f ?q
= v?q)
(defined by m?\m) conditioned on the previously
fixed values of s (given by m). Summarizing,
when predicting the most likely semantics m?j
(line 4), for each span the decoder weighs alter-
natives of either (1) aligning this span to the pre-
viously induced meaning m?, or (2) aligning it to
a new field and paying the cost of generation of its
value.
The exact computation of the most probable se-
mantics (line 4 of the algorithm) is intractable, and
we have to resort to an approximation. Instead
of predicting the most probable semantics m?j we
search for the most probable pair (a?j , m?j), thus
assuming that the probability mass is mostly con-
centrated on a single alignment. The alignment aj
5For simplicity, we assume here that all the examples are
unlabeled.
963
is then discarded and not used in any other compu-
tations. Though the most likely alignment a?j for
a fixed semantic representation m?j can be found
efficiently using a Viterbi algorithm, computing
the most probable pair (a?j , m?j) is still intractable.
We use a modification of the beam search algo-
rithm, where we keep a set of candidate meanings
(partial semantic representations) and compute an
alignment for each of them using a form of the
Viterbi algorithm.
As soon as the meaning representations m? are
inferred, we find ourselves in the set-up studied
in (Liang et al, 2009): the state s is no longer
latent and we can run efficient inference on the
E-step. Though some fields of the state s may
still not be specified by m?, we prohibit utterances
from aligning to these non-specified fields.
On the M-step of EM the parameters are es-
timated as proportional to the expected marginal
counts computed on the E-step. We smooth the
distributions of values for numerical fields with
convolution smoothing equivalent to the assump-
tion that the fields are affected by distortion in the
form of a two-sided geometric distribution with
the success rate parameter equal to 0.67. We use
add-0.1 smoothing for all the remaining multino-
mial distributions.
4 Empirical Evaluation
In this section, we consider the semi-supervised
set-up, and present evaluation of our approach on
on the problem of aligning weather forecast re-
ports to the formal representation of weather.
4.1 Experiments
To perform the experiments we used a subset
of the weather dataset introduced in (Liang et
al., 2009). The original dataset contains 22,146
texts of 28.7 words on average, there are 12
types of records (predicates) and 36.0 records per
forecast on average. We randomly chose 100
texts along with their world states to be used as
the labeled data.6 To produce groups of non-
contradictory texts we have randomly selected a
subset of weather states, represented them in a vi-
sual form (icons accompanied by numerical and
6In order to distinguish from completely unlabeled exam-
ples, we refer to examples labeled with world states as la-
beled examples. Note though that the alignments are not ob-
servable even for these labeled examples. Similarly, we call
the models trained from this data supervised though full su-
pervision was not available.
symbolic parameters) and then manually anno-
tated these illustrations. These newly-produced
forecasts, when combined with the original texts,
resulted in 259 groups of non-contradictory texts
(650 texts, 2.5 texts per group). An example of
such a group is given in figure 1.
The dataset is relatively noisy: there are incon-
sistencies due to annotation mistakes (e.g., number
distortions), or due to different perception of the
weather by the annotators (e.g., expressions such
as ?warm? or ?cold? are subjective). The overlap
between the verbalized fields in each group was
estimated to be below 35%. Around 60% of fields
are mentioned only in a single forecast from a
group, consequently, the texts cannot be regarded
as paraphrases of each other.
The test set consists of 150 texts, each corre-
sponding to a different weather state. Note that
during testing we no longer assume that docu-
ments share the state, we treat each document in
isolation. We aimed to preserve approximately the
same proportion of new and original examples as
we had in the training set, therefore, we combined
50 texts originally present in the weather dataset
with additional 100 newly-produced texts. We an-
notated these 100 texts by aligning each line to one
or more records,7 whereas for the original texts the
alignments were already present. Following Liang
et al (2009) we evaluate the models on how well
they predict these alignments.
When estimating the model parameters, we fol-
lowed the training regime prescribed in (Liang et
al., 2009). Namely, 5 iterations of EM with a basic
model (with no segmentation or coherence mod-
eling), followed by 5 iterations of EM with the
model which generates fields independently and,
at last, 5 iterations with the full model. Only
then, in the semi-supervised learning scenarios,
we added unlabeled data and ran 5 additional it-
erations of EM.
Instead of prohibiting records from crossing
punctuation, as suggested by Liang et al (2009),
in our implementation we disregard the words not
attached to specific fields (attached to the null-
field, see section 3.1) when computing spans of
records. To speed-up training, only a single record
of each type is allowed to be generated when run-
ning inference for unlabeled examples on the E-
7The text was automatically tokenized and segmented into
lines, with line breaks at punctuation characters. Information
about the line breaks is not used during learning and infer-
ence.
964
P R F1
Supervised BL 63.3 52.9 57.6
Semi-superv BL 68.8 69.4 69.1
Semi-superv, non-contr 78.8 69.5 73.9
Supervised UB 69.4 88.6 77.9
Table 1: Results (precision, recall and F1) on the
weather forecast dataset.
step of the EM algorithm, as it significantly re-
duces the search space. Similarly, though we pre-
served all records which refer to the first time pe-
riod, for other time periods we removed all the
records which declare that the corresponding event
(e.g., rain or snowfall) is not expected to happen.
This preprocessing results in the oracle recall of
93%.
We compare our approach (Semi-superv, non-
contr) with two baselines: the basic supervised
training on 100 labeled forecasts (Supervised BL)
and with the semi-supervised training which disre-
gards the non-contradiction relations (Semi-superv
BL). The learning regime, the inference proce-
dure and the texts for the semi-supervised baseline
were identical to the ones used for our approach,
the only difference is that all the documents were
modeled as independent. Additionally, we report
the results of the model trained with all the 750
texts labeled (Supervised UB), its scores can be
regarded as an upper bound on the results of the
semi-supervised models. The results are reported
in table 1.
4.2 Discussion
Our training strategy results in a substantially
more accurate model, outperforming both the su-
pervised and semi-supervised baselines. Surpris-
ingly, its precision is higher than that of the model
trained on 750 labeled examples, though admit-
tedly it is achieved at a very different recall level.
The estimation of the model with our approach
takes around one hour on a standard desktop PC,
which is comparable to 40 minutes required to
train the semi-supervised baseline.
In these experiments, we consider the problem
of predicting alignment between text and the cor-
responding observable world state. The direct
evaluation of the meaning recognition (i.e. se-
mantic parsing) accuracy is not possible on this
dataset, as the data does not contain information
which fields are discussed. Even if it would pro-
value top words
0-25 clear, small, cloudy, gaps, sun
25-50 clouds, increasing, heavy, produce, could
50-75 cloudy, mostly, high, cloudiness, breezy
75-100 amounts, rainfall, inch, new, possibly
Table 2: Top 5 words in the word distribution for
field mode of record sky cover, function words and
punctuation are omitted.
vide this information, the documents do not ver-
balize the state at the necessary granularity level
to predict the field values. For example, it is not
possible to decide to which bucket of the field sky
cover the expression ?cloudy? refers to, as it has a
relatively uniform distribution across 3 (out of 4)
buckets. The problem of predicting text-meaning
alignments is interesting in itself, as the extracted
alignments can be used in training of a statisti-
cal generation system or information extractors,
but we also believe that evaluation on this prob-
lem is an appropriate test for the relative compar-
ison of the semantic analyzers? performance. Ad-
ditionally, note that the success of our weakly-
supervised scenario indirectly suggests that the
model is sufficiently accurate in predicting seman-
tics of an unlabeled text, as otherwise there would
be no useful information passed in between se-
mantically overlapping documents during learning
and, consequently, no improvement from sharing
the state.8
To confirm that the model trained by our ap-
proach indeed assigns new words to correct fields
and records, we visualize top words for the field
characterizing sky cover (table 2). Note that the
words ?sun?, ?cloudiness? or ?gaps? were not ap-
pearing in the labeled part of the data, but seem to
be assigned to correct categories. However, cor-
relation between rain and overcast, as also noted
in (Liang et al, 2009), results in the wrong assign-
ment of the rain-related words to the field value
corresponding to very cloudy weather.
5 Related Work
Probably the most relevant prior work is an ap-
proach to bootstrapping lexical choice of a gen-
eration system using a corpus of alternative pas-
8We conducted preliminary experiments on synthetic data
generated from a random semantic-correspondence model.
Our approach outperformed the baselines both in predicting
?text?-state correspondence and in the F1 score on the pre-
dicted set of field assignments (?text meanings?).
965
sages (Barzilay and Lee, 2002), however, in their
work all the passages were annotated with un-
aligned semantic expressions. Also, they as-
sumed that the passages are paraphrases of each
other, which is stronger than our non-contradiction
assumption. Sentence and text alignment has
also been considered in the related context of
paraphrase extraction (see, e.g., (Dolan et al,
2004; Barzilay and Lee, 2003)) but this prior
work did not focus on inducing or learning se-
mantic representations. Similarly, in information
extraction, there have been approaches for pat-
tern discovery using comparable monolingual cor-
pora (Shinyama and Sekine, 2003) but they gener-
ally focused only on discovery of a single pattern
from a pair of sentences or texts.
Radev (2000) considered types of potential rela-
tions between documents, including contradiction,
and studied how this information can be exploited
in NLP. However, this work considered primarily
multi-document summarization and question an-
swering problems.
Another related line of research in machine
learning is clustering or classification with con-
straints (Basu et al, 2004), where supervision is
given in the form of constraints. Constraints de-
clare which pairs of instances are required to be
assigned to the same class (or required to be as-
signed to different classes). However, we are not
aware of any previous work that generalized these
methods to structured prediction problems, as triv-
ial equality/inequality constraints are probably too
restrictive, and a notion of consistency is required
instead.
6 Summary and Future Work
In this work we studied the use of weak supervi-
sion in the form of non-contradictory relations be-
tween documents in learning semantic represen-
tations. We argued that this type of supervision
encodes information which is hard to discover in
an unsupervised way. However, exact inference
for groups of documents with overlapping seman-
tic representation is generally prohibitively expen-
sive, as the shared latent semantics introduces non-
local dependences between semantic representa-
tions of individual documents. To combat it, we
proposed a simple iterative inference algorithm.
We showed how it can be instantiated for the
semantics-text correspondence model (Liang et
al., 2009) and evaluated it on a dataset of weather
forecasts. Our approach resulted in an improve-
ment over the scores of both the supervised base-
line and of the traditional semi-supervised learn-
ing.
There are many directions we plan on inves-
tigating in the future for the problem of learn-
ing semantics with non-contradictory relations. A
promising and challenging possibility is to con-
sider models which induce full semantic represen-
tations of meaning. Another direction would be
to investigate purely unsupervised set-up, though
it would make evaluation of the resulting method
much more complex. One potential alternative
would be to replace the initial supervision with a
set of posterior constraints (Graca et al, 2008) or
generalized expectation criteria (McCallum et al,
2007).
Acknowledgements
The authors acknowledge the support of the Excel-
lence Cluster on Multimodal Computing and Inter-
action (MMCI). Thanks to Alexandre Klementiev,
Alexander Koller, Manfred Pinkal, Dan Roth, Car-
oline Sporleder and the anonymous reviewers for
their suggestions, and to Percy Liang for answer-
ing questions about his model.
References
Regina Barzilay and Lillian Lee. 2002. Bootstrap-
ping lexical choice via multiple-sequence align-
ment. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP), pages 164?171.
Regina Barzilay and Lillian Lee. 2003. Learning
to paraphrase: An unsupervised approach using
multiple-sequence alignment. In Proceedings of the
Conference on Human Language Technology and
North American chapter of the Association for Com-
putational Linguistics (HLT-NAACL).
Sugatu Basu, Arindam Banjeree, and Raymond
Mooney. 2004. Active semi-supervision for pair-
wise constrained clustering. In Proc. of the SIAM
International Conference on Data Mining (SDM),
pages 333?344.
A. Blum and T. Mitchell. 1998. Combining labeled
and unlabeled data with co-training. In COLT: Pro-
ceedings of the Workshop on Computational Learn-
ing Theory, Morgan Kaufmann Publishers, pages
209?214.
Xavier Carreras and Lluis Marquez. 2005. Introduc-
tion to the conll-2005 shared task: Semantic role la-
beling. In Proceedings of CoNLL-2005, Ann Arbor,
MI USA.
966
David L. Chen and Raymond L. Mooney. 2008. Learn-
ing to sportcast: A test of grounded language acqui-
sition. In Proc. of International Conference on Ma-
chine Learning, pages 128?135.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the
EM algorithms. Journal of the Royal Statistical So-
ciety. Series B (Methodological), 39(1):1?38.
P. Diaconis and B. Efron. 1983. Computer-intensive
methods in statistics. Scientific American, pages
116?130.
Bill Dolan, Chris Quirk, and Chris Brockett. 2004.
Unsupervised construction of large paraphrase cor-
pora: Exploiting massively parallel news sources.
In Proceedings of the Conference on Computational
Linguistics (COLING), pages 350?356.
Ruifang Ge and Raymond J. Mooney. 2005. A sta-
tistical semantic parser that integrates syntax and
semantics. In Proceedings of the Ninth Confer-
ence on Computational Natural Language Learning
(CONLL-05), Ann Arbor, Michigan.
Joao Graca, Kuzman Ganchev, and Ben Taskar. 2008.
Expectation maximization and posterior constraints.
Advances in Neural Information Processing Systems
20 (NIPS).
Zellig Harris. 1968. Mathematical structures of lan-
guage. Wiley.
Rohit J. Kate and Raymond J. Mooney. 2007. Learn-
ing language semantics from ambigous supervision.
In Association for the Advancement of Artificial In-
telligence (AAAI), pages 895?900.
Percy Liang, Michael I. Jordan, and Dan Klein. 2009.
Learning semantic correspondences with less super-
vision. In Proc. of the Annual Meeting of the Asso-
ciation for Computational Linguistics and Interna-
tional Joint Conference on Natural Language Pro-
cessing (ACL-IJCNLP).
Andrew McCallum, Gideon Mann, and Gregory
Druck. 2007. Generalized expectation criteria.
Technical Report TR 2007-60, University of Mas-
sachusetts, Amherst, MA.
Raymond J. Mooney. 2007. Learning for semantic
parsing. In Proceedings of the 8th International
Conference on Computational Linguistics and Intel-
ligent Text Processing, pages 982?991.
Kevin P. Murphy, Yair Weiss, and Michael I. Jordan.
1999. Loopy belief propagation for approximate in-
ference: An empirical study. In Proc. of Uncertainty
in Artificial Intelligence (UAI), pages 467?475.
Judea Pearl. 1982. Reverend bayes on inference en-
gines: A distributed hierarchical approach. In Proc.
of the National Conference on Artificial Intelligence
(AAAI), pages 133?136.
Hoifung Poon and Pedro Domingos. 2009. Unsuper-
vised semantic parsing. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing, (EMNLP-09).
Dragomir Radev. 2000. A common theory of infor-
mation fusion from multiple text sources step one:
Cross-document structure. In 1st SIGdial Workshop
on Discourse and Dialogue, pages 74?83.
Yusuke Shinyama and Satoshi Sekine. 2003. Para-
phrase acquisition for information extraction. In
Proceedings of Second International Workshop on
Paraphrasing (IWP2003), pages 65?71.
Benjamin Snyder and Regina Barzilay. 2007.
Database-text alignment via structured multilabel
classification. In Proceedings of International Joint
Conference on Artificial Intelligence (IJCAI-05),
pages 1713?1718.
J. Weeds and W. Weir. 2005. Co-occurrence retrieval:
A flexible framework for lexical distributional simi-
larity. Computational Linguistics, 31(4):439?475.
Luke Zettlemoyer and Michael Collins. 2005. Learn-
ing to map sentences to logical form: Structured
classification with probabilistic categorial grammar.
In Proceedings of the Twenty-first Conference on
Uncertainty in Artificial Intelligence, Edinburgh,
UK, August.
967
Proceedings of the ACL 2010 Conference Short Papers, pages 151?155,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Unsupervised Discourse Segmentation
of Documents with Inherently Parallel Structure
Minwoo Jeong and Ivan Titov
Saarland University
Saarbru?cken, Germany
{m.jeong|titov}@mmci.uni-saarland.de
Abstract
Documents often have inherently parallel
structure: they may consist of a text and
commentaries, or an abstract and a body,
or parts presenting alternative views on
the same problem. Revealing relations be-
tween the parts by jointly segmenting and
predicting links between the segments,
would help to visualize such documents
and construct friendlier user interfaces. To
address this problem, we propose an un-
supervised Bayesian model for joint dis-
course segmentation and alignment. We
apply our method to the ?English as a sec-
ond language? podcast dataset where each
episode is composed of two parallel parts:
a story and an explanatory lecture. The
predicted topical links uncover hidden re-
lations between the stories and the lec-
tures. In this domain, our method achieves
competitive results, rivaling those of a pre-
viously proposed supervised technique.
1 Introduction
Many documents consist of parts exhibiting a high
degree of parallelism: e.g., abstract and body of
academic publications, summaries and detailed
news stories, etc. This is especially common with
the emergence of the Web 2.0 technologies: many
texts on the web are now accompanied with com-
ments and discussions. Segmentation of these par-
allel parts into coherent fragments and discovery
of hidden relations between them would facilitate
the development of better user interfaces and im-
prove the performance of summarization and in-
formation retrieval systems.
Discourse segmentation of the documents com-
posed of parallel parts is a novel and challeng-
ing problem, as previous research has mostly fo-
cused on the linear segmentation of isolated texts
(e.g., (Hearst, 1994)). The most straightforward
approach would be to use a pipeline strategy,
where an existing segmentation algorithm finds
discourse boundaries of each part independently,
and then the segments are aligned. Or, conversely,
a sentence-alignment stage can be followed by a
segmentation stage. However, as we will see in our
experiments, these strategies may result in poor
segmentation and alignment quality.
To address this problem, we construct a non-
parametric Bayesian model for joint segmenta-
tion and alignment of parallel parts. In com-
parison with the discussed pipeline approaches,
our method has two important advantages: (1) it
leverages the lexical cohesion phenomenon (Hal-
liday and Hasan, 1976) in modeling the paral-
lel parts of documents, and (2) ensures that the
effective number of segments can grow adap-
tively. Lexical cohesion is an idea that topically-
coherent segments display compact lexical distri-
butions (Hearst, 1994; Utiyama and Isahara, 2001;
Eisenstein and Barzilay, 2008). We hypothesize
that not only isolated fragments but also each
group of linked fragments displays a compact and
consistent lexical distribution, and our generative
model leverages this inter-part cohesion assump-
tion.
In this paper, we consider the dataset of ?En-
glish as a second language? (ESL) podcast1, where
each episode consists of two parallel parts: a story
(an example monologue or dialogue) and an ex-
planatory lecture discussing the meaning and us-
age of English expressions appearing in the story.
Fig. 1 presents an example episode, consisting of
two parallel parts, and their hidden topical rela-
tions.2 From the figure we may conclude that there
is a tendency of word repetition between each pair
of aligned segments, illustrating our hypothesis of
compactness of their joint distribution. Our goal is
1http://www.eslpod.com/
2Episode no. 232 post on Jan. 08, 2007.
151
I have a day job, but I recently started a small business on the side.
I didn't know anything about accounting and my friend, Roland, said that he would give me some advice.
Roland: So, the reason that you need to do your bookkeeping is so you can manage your cash flow.
This podcast is all about business vocabulary related to accounting. The title of the podcast is Business Bookkeeping. ... The story begins by Magdalena saying that she has a day job. A day job is your regular job that you work at from nine in the morning 'til five in the afternoon, for        example. She also has a small business on the side. ... Magdalena continues by saying that she didn't know anything about accounting and her friend,      Roland, said he would give her some advice. Accounting is the job of keeping correct records of the money you spend; it's very similar to      bookkeeping. ... Roland begins by saying that the reason that you need to do your bookkeeping is so you can       manage your cash flow. Cash flow, flow, means having enough money to run your business - to pay your bills. ... ...
Story Lecture transcript
...
Figure 1: An example episode of ESL podcast. Co-occurred words are represented in italic and underline.
to divide the lecture transcript into discourse units
and to align each unit to the related segment of the
story. Predicting these structures for the ESL pod-
cast could be the first step in development of an
e-learning system and a podcast search engine for
ESL learners.
2 Related Work
Discourse segmentation has been an active area
of research (Hearst, 1994; Utiyama and Isahara,
2001; Galley et al, 2003; Malioutov and Barzilay,
2006). Our work extends the Bayesian segmenta-
tion model (Eisenstein and Barzilay, 2008) for iso-
lated texts, to the problem of segmenting parallel
parts of documents.
The task of aligning each sentence of an abstract
to one or more sentences of the body has been
studied in the context of summarization (Marcu,
1999; Jing, 2002; Daume? and Marcu, 2004). Our
work is different in that we do not try to extract
the most relevant sentence but rather aim to find
coherent fragments with maximally overlapping
lexical distributions. Similarly, the query-focused
summarization (e.g., (Daume? and Marcu, 2006))
is also related but it focuses on sentence extraction
rather than on joint segmentation.
We are aware of only one previous work on joint
segmentation and alignment of multiple texts (Sun
et al, 2007) but their approach is based on similar-
ity functions rather than on modeling lexical cohe-
sion in the generative framework. Our application,
the analysis of the ESL podcast, was previously
studied in (Noh et al, 2010). They proposed a su-
pervised method which is driven by pairwise clas-
sification decisions. The main drawback of their
approach is that it neglects the discourse structure
and the lexical cohesion phenomenon.
3 Model
In this section we describe our model for discourse
segmentation of documents with inherently paral-
lel structure. We start by clarifying our assump-
tions about their structure.
We assume that a document x consists of K
parallel parts, that is, x = {x(k)}k=1:K , and
each part of the document consists of segments,
x(k) = {s(k)i }i=1:I . Note that the effective num-
ber of fragments I is unknown. Each segment can
either be specific to this part (drawn from a part-
specific language model ?(k)i ) or correspond to
the entire document (drawn from a document-level
language model ?(doc)i ). For example, the first
and the second sentences of the lecture transcript
in Fig. 1 are part-specific, whereas other linked
sentences belong to the document-level segments.
The document-level language models define top-
ical links between segments in different parts of
the document, whereas the part-specific language
models define the linear segmentation of the re-
maining unaligned text.
Each document-level language model corre-
sponds to the set of aligned segments, at most one
segment per part. Similarly, each part-specific lan-
guage model corresponds to a single segment of
the single corresponding part. Note that all the
documents are modeled independently, as we aim
not to discover collection-level topics (as e.g. in
(Blei et al, 2003)), but to perform joint discourse
segmentation and alignment.
Unlike (Eisenstein and Barzilay, 2008), we can-
not make an assumption that the number of seg-
ments is known a-priori, as the effective number of
part-specific segments can vary significantly from
document to document, depending on their size
and structure. To tackle this problem, we use
Dirichlet processes (DP) (Ferguson, 1973) to de-
152
fine priors on the number of segments. We incor-
porate them in our model in a similar way as it
is done for the Latent Dirichlet Allocation (LDA)
by Yu et al (2005). Unlike the standard LDA, the
topic proportions are chosen not from a Dirichlet
prior but from the marginal distribution GEM(?)
defined by the stick breaking construction (Sethu-
raman, 1994), where ? is the concentration param-
eter of the underlying DP distribution. GEM(?)
defines a distribution of partitions of the unit inter-
val into a countable number of parts.
The formal definition of our model is as follows:
? Draw the document-level topic proportions ?(doc) ?
GEM(?(doc)).
? Choose the document-level language model ?(doc)i ?
Dir(?(doc)) for i ? {1, 2, . . .}.
? Draw the part-specific topic proportions ?(k) ?
GEM(?(k)) for k ? {1, . . . ,K}.
? Choose the part-specific language models ?(k)i ?
Dir(?(k)) for k ? {1, . . . ,K} and i ? {1, 2, . . .}.
? For each part k and each sentence n:
? Draw type t(k)n ? Unif(Doc, Part).
? If (t(k)n = Doc); draw topic z
(k)
n ? ?(doc); gen-
erate words x(k)n ?Mult(?
(doc)
z(k)n
)
? Otherwise; draw topic z(k)n ? ?(k); generate
words x(k)n ?Mult(?
(k)
z(k)n
).
The priors ?(doc), ?(k), ?(doc) and ?(k) can be
estimated at learning time using non-informative
hyperpriors (as we do in our experiments), or set
manually to indicate preferences of segmentation
granularity.
At inference time, we enforce each latent topic
z(k)n to be assigned to a contiguous span of text,
assuming that coherent topics are not recurring
across the document (Halliday and Hasan, 1976).
It also reduces the search space and, consequently,
speeds up our sampling-based inference by reduc-
ing the time needed for Monte Carlo chains to
mix. In fact, this constraint can be integrated in the
model definition but it would significantly compli-
cate the model description.
4 Inference
As exact inference is intractable, we follow Eisen-
stein and Barzilay (2008) and instead use a
Metropolis-Hastings (MH) algorithm. At each
iteration of the MH algorithm, a new potential
alignment-segmentation pair (z?, t?) is drawn from
a proposal distribution Q(z?, t?|z, t), where (z, t)
(a) (b) (c)
Figure 2: Three types of moves: (a) shift, (b) split
and (c) merge.
is the current segmentation and its type. The new
pair (z?, t?) is accepted with the probability
min
(
1,
P (z?, t?,x)Q(z?, t?|z, t)
P (z, t,x)Q(z, t|z?, t?)
)
.
In order to implement the MH algorithm for our
model, we need to define the set of potential moves
(i.e. admissible changes from (z, t) to (z?, t?)),
and the proposal distribution Q over these moves.
If the actual number of segments is known and
only a linear discourse structure is acceptable, then
a single move, shift of the segment border (Fig.
2(a)), is sufficient (Eisenstein and Barzilay, 2008).
In our case, however, a more complex set of moves
is required.
We make two assumptions which are moti-
vated by the problem considered in Section 5:
we assume that (1) we are given the number of
document-level segments and also that (2) the
aligned segments appear in the same order in each
part of the document. With these assumptions in
mind, we introduce two additional moves (Fig.
2(b) and (c)):
? Split move: select a segment, and split it at
one of the spanned sentences; if the segment
was a document-level segment then one of
the fragments becomes the same document-
level segment.
? Merge move: select a pair of adjacent seg-
ments where at least one of the segments is
part-specific, and merge them; if one of them
was a document-level segment then the new
segment has the same document-level topic.
All the moves are selected with the uniform prob-
ability, and the distance c for the shift move is
drawn from the proposal distribution proportional
to c?1/cmax . The moves are selected indepen-
dently for each part.
Although the above two assumptions are not
crucial as a simple modification to the set of moves
would support both introduction and deletion of
document-level fragments, this modification was
not necessary for our experiments.
153
5 Experiment
5.1 Dataset and setup
Dataset We apply our model to the ESL podcast
dataset (Noh et al, 2010) of 200 episodes, with
an average of 17 sentences per story and 80 sen-
tences per lecture transcript. The gold standard
alignments assign each fragment of the story to a
segment of the lecture transcript. We can induce
segmentations at different levels of granularity on
both the story and the lecture side. However, given
that the segmentation of the story was obtained by
an automatic sentence splitter, there is no reason
to attempt to reproduce this segmentation. There-
fore, for quantitative evaluation purposes we fol-
low Noh et al (2010) and restrict our model to
alignment structures which agree with the given
segmentation of the story. For all evaluations, we
apply standard stemming algorithm and remove
common stop words.
Evaluationmetrics To measure the quality of seg-
mentation of the lecture transcript, we use two
standard metrics, Pk (Beeferman et al, 1999) and
WindowDiff (WD) (Pevzner and Hearst, 2002),
but both metrics disregard the alignment links (i.e.
the topic labels). Consequently, we also use the
macro-averaged F1 score on pairs of aligned span,
which measures both the segmentation and align-
ment quality.
Baseline Since there has been little previous re-
search on this problem, we compare our results
against two straightforward unsupervised base-
lines. For the first baseline, we consider the
pairwise sentence alignment (SentAlign) based
on the unigram and bigram overlap. The sec-
ond baseline is a pipeline approach (Pipeline),
where we first segment the lecture transcript with
BayesSeg (Eisenstein and Barzilay, 2008) and
then use the pairwise alignment to find their best
alignment to the segments of the story.
Our model We evaluate our joint model of seg-
mentation and alignment both with and without
the split/merge moves. For the model without
these moves, we set the desired number of seg-
ments in the lecture to be equal to the actual num-
ber of segments in the story I . In this setting,
the moves can only adjust positions of the seg-
ment borders. For the model with the split/merge
moves, we start with the same number of segments
I but it can be increased or decreased during in-
ference. For evaluation of our model, we run our
inference algorithm from five random states, and
Method Pk WD 1? F1
Uniform 0.453 0.458 0.682
SentAlign 0.446 0.547 0.313
Pipeline (I) 0.250 0.249 0.443
Pipeline (2I+1) 0.268 0.289 0.318
Our model (I) 0.193 0.204 0.254
+split/merge 0.181 0.193 0.239
Table 1: Results on the ESL podcast dataset. For
all metrics, lower values are better.
take the 100,000th iteration of each chain as a sam-
ple. Results are the average over these five runs.
Also we perform L-BFGS optimization to auto-
matically adjust the non-informative hyperpriors
after each 1,000 iterations of sampling.
5.2 Result
Table 1 summarizes the obtained results. ?Uni-
form? denotes the minimal baseline which uni-
formly draws a random set of I spans for each lec-
ture, and then aligns them to the segments of the
story preserving the linear order. Also, we con-
sider two variants of the pipeline approach: seg-
menting the lecture on I and 2I + 1 segments, re-
spectively.3 Our joint model substantially outper-
forms the baselines. The difference is statistically
significant with the level p < .01 measured with
the paired t-test. The significant improvement over
the pipeline results demonstrates benefits of joint
modeling for the considered problem. Moreover,
additional benefits are obtained by using the DP
priors and the split/merge moves (the last line in
Table 1). Finally, our model significantly outper-
forms the previously proposed supervised model
(Noh et al, 2010): they report micro-averaged F1
score 0.698 while our best model achieves 0.778
with the same metric. This observation confirms
that lexical cohesion modeling is crucial for suc-
cessful discourse analysis.
6 Conclusions
We studied the problem of joint discourse segmen-
tation and alignment of documents with inherently
parallel structure and achieved favorable results on
the ESL podcast dataset outperforming the cas-
caded baselines. Accurate prediction of these hid-
den relations would open interesting possibilities
3The use of the DP priors and the split/merge moves on
the first stage of the pipeline did not result in any improve-
ment in accuracy.
154
for construction of friendlier user interfaces. One
example being an application which, given a user-
selected fragment of the abstract, produces a sum-
mary from the aligned segment of the document
body.
Acknowledgment
The authors acknowledge the support of the
Excellence Cluster on Multimodal Computing
and Interaction (MMCI), and also thank Mikhail
Kozhevnikov and the anonymous reviewers for
their valuable comments, and Hyungjong Noh for
providing their data.
References
Doug Beeferman, Adam Berger, and John Lafferty.
1999. Statistical models for text segmentation.
Computational Linguistics, 34(1?3):177?210.
David M. Blei, Andrew Ng, and Michael I. Jordan.
2003. Latent dirichlet alocation. JMLR, 3:993?
1022.
Hal Daume? and Daniel Marcu. 2004. A phrase-based
hmm approach to document/abstract alignment. In
Proceedings of EMNLP, pages 137?144.
Hal Daume? and Daniel Marcu. 2006. Bayesian query-
focused summarization. In Proceedings of ACL,
pages 305?312.
Jacob Eisenstein and Regina Barzilay. 2008. Bayesian
unsupervised topic segmentation. In Proceedings of
EMNLP, pages 334?343.
Thomas S. Ferguson. 1973. A Bayesian analysis of
some non-parametric problems. Annals of Statistics,
1:209?230.
Michel Galley, Kathleen R. McKeown, Eric Fosler-
Lussier, and Hongyan Jing. 2003. Discourse seg-
mentation of multi-party conversation. In Proceed-
ings of ACL, pages 562?569.
M. A. K. Halliday and Ruqaiya Hasan. 1976. Cohe-
sion in English. Longman.
Marti Hearst. 1994. Multi-paragraph segmentation of
expository text. In Proceedings of ACL, pages 9?16.
Hongyan Jing. 2002. Using hidden Markov modeling
to decompose human-written summaries. Computa-
tional Linguistics, 28(4):527?543.
Igor Malioutov and Regina Barzilay. 2006. Minimum
cut model for spoken lecture segmentation. In Pro-
ceedings of ACL, pages 25?32.
Daniel Marcu. 1999. The automatic construction of
large-scale corpora for summarization research. In
Proceedings of ACM SIGIR, pages 137?144.
Hyungjong Noh, Minwoo Jeong, Sungjin Lee,
Jonghoon Lee, and Gary Geunbae Lee. 2010.
Script-description pair extraction from text docu-
ments of English as second language podcast. In
Proceedings of the 2nd International Conference on
Computer Supported Education.
Lev Pevzner and Marti Hearst. 2002. A critique and
improvement of an evaluation metric for text seg-
mentation. Computational Linguistics, 28(1):19?
36.
Jayaram Sethuraman. 1994. A constructive definition
of Dirichlet priors. Statistica Sinica, 4:639?650.
Bingjun Sun, Prasenjit Mitra, C. Lee Giles, John Yen,
and Hongyuan Zha. 2007. Topic segmentation
with shared topic detection and alignment of mul-
tiple documents. In Proceedings of ACM SIGIR,
pages 199?206.
Masao Utiyama and Hitoshi Isahara. 2001. A statis-
tical model for domain-independent text segmenta-
tion. In Proceedings of ACL, pages 491?498.
Kai Yu, Shipeng Yu, and Vokler Tresp. 2005. Dirichlet
enhanced latent semantic analysis. In Proceedings
of AISTATS.
155
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 62?71,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Domain Adaptation by Constraining Inter-Domain Variability
of Latent Feature Representation
Ivan Titov
Saarland University
Saarbruecken, Germany
titov@mmci.uni-saarland.de
Abstract
We consider a semi-supervised setting for do-
main adaptation where only unlabeled data is
available for the target domain. One way to
tackle this problem is to train a generative
model with latent variables on the mixture of
data from the source and target domains. Such
a model would cluster features in both do-
mains and ensure that at least some of the la-
tent variables are predictive of the label on the
source domain. The danger is that these pre-
dictive clusters will consist of features specific
to the source domain only and, consequently,
a classifier relying on such clusters would per-
form badly on the target domain. We in-
troduce a constraint enforcing that marginal
distributions of each cluster (i.e., each latent
variable) do not vary significantly across do-
mains. We show that this constraint is effec-
tive on the sentiment classification task (Pang
et al, 2002), resulting in scores similar to
the ones obtained by the structural correspon-
dence methods (Blitzer et al, 2007) without
the need to engineer auxiliary tasks.
1 Introduction
Supervised learning methods have become a stan-
dard tool in natural language processing, and large
training sets have been annotated for a wide vari-
ety of tasks. However, most learning algorithms op-
erate under assumption that the learning data orig-
inates from the same distribution as the test data,
though in practice this assumption is often violated.
This difference in the data distributions normally re-
sults in a significant drop in accuracy. To address
this problem a number of domain-adaptation meth-
ods has recently been proposed (see e.g., (Daume?
and Marcu, 2006; Blitzer et al, 2006; Bickel et al,
2007)). In addition to the labeled data from the
source domain, they also exploit small amounts of
labeled data and/or unlabeled data from the target
domain to estimate a more predictive model for the
target domain.
In this paper we focus on a more challenging
and arguably more realistic version of the domain-
adaptation problem where only unlabeled data is
available for the target domain. One of the most
promising research directions on domain adaptation
for this setting is based on the idea of inducing a
shared feature representation (Blitzer et al, 2006),
that is mapping from the initial feature representa-
tion to a new representation such that (1) examples
from both domains ?look similar? and (2) an accu-
rate classifier can be trained in this new representa-
tion. Blitzer et al (2006) use auxiliary tasks based
on unlabeled data for both domains (called pivot fea-
tures) and a dimensionality reduction technique to
induce such shared representation. The success of
their domain-adaptation method (Structural Corre-
spondence Learning, SCL) crucially depends on the
choice of the auxiliary tasks, and defining them can
be a non-trivial engineering problem for many NLP
tasks (Plank, 2009). In this paper, we investigate
methods which do not use auxiliary tasks to induce
a shared feature representation.
We use generative latent variable models (LVMs)
learned on all the available data: unlabeled data for
both domains and on the labeled data for the source
domain. Our LVMs use vectors of latent features
62
to represent examples. The latent variables encode
regularities observed on unlabeled data from both
domains, and they are learned to be predictive of
the labels on the source domain. Such LVMs can
be regarded as composed of two parts: a mapping
from initial (normally, word-based) representation
to a new shared distributed representation, and also
a classifier in this representation. The danger of this
semi-supervised approach in the domain-adaptation
setting is that some of the latent variables will cor-
respond to clusters of features specific only to the
source domain, and consequently, the classifier re-
lying on this latent variable will be badly affected
when tested on the target domain. Intuitively, one
would want the model to induce only those features
which generalize between domains. We encode this
intuition by introducing a term in the learning ob-
jective which regularizes inter-domain difference in
marginal distributions of each latent variable.
Another, though conceptually similar, argument
for our method is coming from theoretical re-
sults which postulate that the drop in accuracy of
an adapted classifier is dependent on the discrep-
ancy distance between the source and target do-
mains (Blitzer et al, 2008; Mansour et al, 2009;
Ben-David et al, 2010). Roughly, the discrepancy
distance is small when linear classifiers cannot dis-
tinguish between examples from different domains.
A necessary condition for this is that the feature ex-
pectations do not vary significantly across domains.
Therefore, our approach can be regarded as mini-
mizing a coarse approximation of the discrepancy
distance.
The introduced term regularizes model expecta-
tions and it can be viewed as a form of a general-
ized expectation (GE) criterion (Mann and McCal-
lum, 2010). Unlike the standard GE criterion, where
a model designer defines the prior for a model ex-
pectation, our criterion postulates that the model ex-
pectations should be similar across domains.
In our experiments, we use a form of Harmonium
Model (Smolensky, 1986) with a single layer of bi-
nary latent variables. Though exact inference with
this class of models is infeasible we use an effi-
cient approximation (Bengio and Delalleau, 2007),
which can be regarded either as a mean-field approx-
imation to the reconstruction error or a determinis-
tic version of the Contrastive Divergence sampling
method (Hinton, 2002). Though such an estimator
is biased, in practice, it yields accurate models. We
explain how the introduced regularizer can be inte-
grated into the stochastic gradient descent learning
algorithm for our model.
We evaluate our approach on adapting sentiment
classifiers on 4 domains: books, DVDs, electronics
and kitchen appliances (Blitzer et al, 2007). The
loss due to transfer to a new domain is very sig-
nificant for this task: in our experiments it was
approaching 9%, in average, for the non-adapted
model. Our regularized model achieves 35% aver-
age relative error reduction with respect to the non-
adapted classifier, whereas the non-regularized ver-
sion demonstrates a considerably smaller reduction
of 26%. Both the achieved error reduction and the
absolute score match the results reported in (Blitzer
et al, 2007) for the best version1 of the SCL method
(SCL-MI, 36%), suggesting that our approach is a
viable alternative to SCL.
The rest of the paper is structured as follows. In
Section 2 we introduce a model which uses vec-
tors of latent variables to model statistical dependen-
cies between the elementary features. In Section 3
we discuss its applicability in the domain-adaptation
setting, and introduce constraints on inter-domain
variability as a way to address the discovered lim-
itations. Section 4 describes approximate learning
and inference algorithms used in our experiments.
In Section 5 we provide an empirical evaluation of
the proposed method. We conclude in Section 6 with
further examination of the related work.
2 The Latent Variable Model
The adaptation method advocated in this paper is ap-
plicable to any joint probabilistic model which uses
distributed representations, i.e. vectors of latent
variables, to abstract away from hand-crafted fea-
tures. These models, for example, include Restricted
Boltzmann Machines (Smolensky, 1986; Hinton,
2002) and Sigmoid Belief Networks (SBNs) (Saul
et al, 1996) for classification and regression tasks,
Factorial HMMs (Ghahramani and Jordan, 1997)
for sequence labeling problems, Incremental SBNs
for parsing problems (Titov and Henderson, 2007a),
1Among the versions which do not exploit labeled data from
the target domain.
63
as well as different types of Deep Belief Net-
works (Hinton and Salakhutdinov, 2006). The
power of these methods is in their ability to automat-
ically construct new features from elementary ones
provided by the model designer. This feature induc-
tion capability is especially desirable for problems
where engineering features is a labor-intensive pro-
cess (e.g., multilingual syntactic parsing (Titov and
Henderson, 2007b)), or for multitask learning prob-
lems where the nature of interactions between the
tasks is not fully understood (Collobert and Weston,
2008; Gesmundo et al, 2009).
In this paper we consider classification tasks,
namely prediction of sentiment polarity of a user re-
view (Pang et al, 2002), and model the joint distri-
bution of the binary sentiment label y ? {0, 1} and
the multiset of text features x, xi ? X . The hidden
variable vector z (zi ? {0, 1}, i = 1, . . . ,m) en-
codes statistical dependencies between components
of x and also dependencies between the label y and
the featuresx. Intuitively, the model can be regarded
as a logistic regression classifier with latent features.
The model assumes that the features and the latent
variable vector are generated jointly from a globally-
normalized model and then the label y is gener-
ated from a conditional distribution dependent on
z. Both of these distributions, P (x, z) and P (y|z),
are parameterized as log-linear models and, conse-
quently, our model can be seen as a combination of
an undirected Harmonium model (Smolensky, 1986)
and a directed SBN model (Saul et al, 1996). The
formal definition is as follows:
(1) Draw (x, z) ? P (x, z|v),
(2) Draw label y ? ?(w0 +
?m
i=1 wizi),
where v and w are parameters, ? is the logistic sig-
moid function, ?(t) = 1/(1 + e?t), and the joint
distribution of (x, z) is given by the Gibbs distribu-
tion:
P (x, z|v) ? exp(
|x|?
j=1
vxj0+
n?
i=1
v0izi+
|x|,n?
j,i=1
vxjizi).
Figure 1 presents the corresponding graphical
model. Note that the arcs between x and z are undi-
rected, whereas arcs between y and z are directed.
The parameters of this model ? = (v,w) can be
estimated by maximizing joint likelihood L(?) of
labeled data for the source domain {x(l), y(l)}l?SL
...
...x
z
y
v
w
Figure 1: The latent variable model: x, z, y are random
variables, dependencies between x and z are parameter-
ized by matrix v, and dependencies between z and y - by
vector w.
and unlabeled data for the source and target domain
{x(l)}l?SU?TU , where SU and TU stand for the un-
labeled datasets for the source and target domains,
respectively. However, given that, first, amount of
unlabeled data |SU ? TU | normally vastly exceeds
the amount of labeled data |SL| and, second, the
number of features for each example |x(l)| is usually
large, the label y will have only a minor effect on
the mapping from the initial features x to the latent
representation z (i.e. on the parameters v). Conse-
quently, the latent representation induced in this way
is likely to be inappropriate for the classification task
in question. Therefore, we follow (McCallum et al,
2006) and use a multi-conditional objective, a spe-
cific form of hybrid learning, to emphasize the im-
portance of labels y:
L(?, ?)=?
?
l?SL
logP (y(l)|x(l), ?)+
?
l?SU?TU?SL
logP (x(l)|?),
where ? is a weight, ? > 1.
Direct maximization of the objective is prob-
lematic, as it would require summation over all
the 2m latent vectors z. Instead we use a mean-
field approximation. Similarly, an efficient ap-
proximate inference algorithm is used to compute
argmaxy P (y|x, ?) at testing time. The approxima-
tions are described in Section 4.
3 Constraints on Inter-Domain Variability
As we discussed in the introduction, our goal is
to provide a method for domain adaptation based
on semi-supervised learning of models with dis-
tributed representations. In this section, we first dis-
cuss the shortcomings of domain adaptation with
the above-described semi-supervised approach and
motivate constraints on inter-domain variability of
64
the induced shared representation. Then we pro-
pose a specific form of this constraint based on the
Kullback-Leibler (KL) divergence.
3.1 Motivation for the Constraints
Each latent variable zi encodes a cluster or a com-
bination of elementary features xj . At least some
of these clusters, when induced by maximizing the
likelihood L(?, ?) with sufficiently large ?, will be
useful for the classification task on the source do-
main. However, when the domains are substan-
tially different, these predictive clusters are likely
to be specific only to the source domain. For ex-
ample, consider moving from reviews of electronics
to book reviews: the cluster of features related to
equipment reliability and warranty service will not
generalize to books. The corresponding latent vari-
able will always be inactive on the books domain
(or always active, if negative correlation is induced
during learning). Equivalently, the marginal distri-
bution of this variable will be very different for both
domains. Note that the classifier, defined by the vec-
torw, is only trained on the labeled source examples
{x(l), y(l)}l?SL and therefore it will rely on such la-
tent variables, even though they do not generalize
to the target domain. Clearly, the accuracy of such
classifier will drop when it is applied to target do-
main examples. To tackle this issue, we introduce a
regularizing term which penalizes differences in the
marginal distributions between the domains.
In fact, we do not need to consider the behavior
of the classifier to understand the rationale behind
the introduction of the regularizer. Intuitively, when
adapting between domains, we are interested in rep-
resentations z which explain domain-independent
regularities rather than in modeling inter-domain
differences. The regularizer favors models which fo-
cus on the former type of phenomena rather than the
latter.
Another motivation for the form of regularization
we propose originates from theoretical analysis of
the domain adaptation problems (Ben-David et al,
2010; Mansour et al, 2009; Blitzer et al, 2007).
Under the assumption that there exists a domain-
independent scoring function, these analyses show
that the drop in accuracy is upper-bounded by the
quantity called discrepancy distance. The discrep-
ancy distance is dependent on the feature represen-
tation z, and the input distributions for both domains
PS(z) and PT (z), and is defined as
dz(S,T )=max
f,f ?
|EPS [f(z)6=f
?(z)]?EPT [f(z)6=f
?(z)]|,
where f and f ? are arbitrary linear classifiers
in the feature representation z. The quantity
EP [f(z)6=f ?(z)] measures the probability mass as-
signed to examples where f and f ? disagree. Then
the discrepancy distance is the maximal change in
the size of this disagreement set due to transfer be-
tween the domains. For a more restricted class of
classifiers which rely only on any single feature2
zi, the distance is equal to the maximum over the
change in the distributions P (zi). Consequently, for
arbitrary linear classifiers we have:
dz(S,T ) ? max
i=1,...,m
|EPS [zi = 1]? EPT [zi = 1]|.
It follows that low inter-domain variability of the
marginal distributions of latent variables is a neces-
sary condition for low discrepancy distance. Min-
imizing the difference in the marginal distributions
can be regarded as a coarse approximation to the
minimization of the distance. However, we have
to concede that the above argument is fairly infor-
mal, as the generalization bounds do not directly
apply to our case: (1) our feature representation
is learned from the same data as the classifier, (2)
we cannot guarantee that the existence of a domain-
independent scoring function is preserved under the
learned transformation x?z and (3) in our setting
we have access not only to samples from P (z|x, ?)
but also to the distribution itself.
3.2 The Expectation Criterion
Though the above argument suggests a specific form
of the regularizing term, we believe that the penal-
izer should not be very sensitive to small differ-
ences in the marginal distributions, as useful vari-
ables (clusters) are likely to have somewhat differ-
ent marginal distributions in different domains, but
it should severely penalize extreme differences.
To achieve this goal we instead propose to use the
symmetrized Kullback-Leibler (KL) divergence be-
tween the marginal distributions as the penalty. The
2We consider only binary features here.
65
derivative of the symmetrized KL divergence is large
when one of the marginal distributions is concen-
trated at 0 or 1 with another distribution still having
high entropy, and therefore such configurations are
severely penalized.3 Formally, the regularizer G(?)
is defined as
G(?) =
m?
i=1
D(PS(zi|?)||PT (zi|?))
+D(PT (zi|?)||PS(zi|?)), (1)
where PS(zi) and PT (zi) stand for the training sam-
ple estimates of the marginal distributions of latent
features, for instance:
PT (zi = 1|?) =
1
|TU |
?
l?TU
P (zi = 1|x(l), ?).
We augment the multi-conditional log-likelihood
L(?, ?) with the weighted regularization term G(?)
to get the composite objective function:
LR(?, ?, ?) = L(?, ?)? ?G(?), ? > 0.
Note that this regularization term can be regarded
as a form of the generalized expectation (GE) crite-
ria (Mann and McCallum, 2010), where GE criteria
are normally defined as KL divergences between a
prior expectation of some feature and the expecta-
tion of this feature given by the model, where the
prior expectation is provided by the model designer
as a form of weak supervision. In our case, both ex-
pectations are provided by the model but on different
domains.
Note that the proposed regularizer can be trivially
extended to support the multi-domain case (Mansour
et al, 2008) by considering symmetrized KL diver-
gences for every pair of domains or regularizing the
distributions for every domain towards their average.
More powerful regularization terms can also be
motivated by minimization of the discrepancy dis-
tance but their optimization is likely to be expensive,
whereas LR(?, ?, ?) can be optimized efficiently.
3An alternative is to use the Jensen-Shannon (JS) diver-
gence, however, our preliminary experiments seem to suggest
that the symmetrized KL divergence is preferable. Though the
two divergences are virtually equivalent when the distributions
are very similar (their ratio tends to a constant as the distribu-
tions go closer), the symmetrized KL divergence stronger penal-
izes extreme differences and this is important for our purposes.
4 Learning and Inference
In this section we describe an approximate learning
algorithm based on the mean-field approximation.
Though we believe that our approach is independent
of the specific learning algorithm, we provide the de-
scription for completeness. We also describe a sim-
ple approximate algorithm for computing P (y|x, ?)
at test time.
The stochastic gradient descent algorithm iter-
ates over examples and updates the weight vector
based on the contribution of every considered exam-
ple to the objective function LR(?, ?, ?). To com-
pute these updates we need to approximate gradients
of ?? logP (y(l)|x(l), ?) (l ? SL), ?? logP (x(l)|?)
(l ? SL ? SU ? TU ) as well as to estimate the con-
tribution of a given example to the gradient of the
regularizer??G(?). In the next sections we will de-
scribe how each of these terms can be estimated.
4.1 Conditional Likelihood Term
We start by explaining the mean-field approximation
of logP (y|x, ?). First, we compute the means ? =
(?1, . . . , ?m):
?i = P (zi = 1|x,v) = ?(v0i +
?|x|
j=1 vxji).
Now we can substitute them instead of z to approx-
imate the conditional probability of the label:
P (y = 1|x, ?) =
?
z P (y|z,w)P (z|x,v)
? ?(w0 +
?m
i=1wi?i).
We use this estimate both at testing time and also
to compute gradients ?? logP (y(l)|x(l), ?) during
learning. The gradients can be computed efficiently
using a form of back-propagation. Note that with
this approximation, we do not need to normalize
over the feature space, which makes the model very
efficient at classification time.
This approximation is equivalent to the computa-
tion of the two-layer perceptron with the soft-max
activation function (Bishop, 1995). However, the
above derivation provides a probabilistic interpreta-
tion of the hidden layer.
4.2 Unlabeled Likelihood Term
In this section, we describe how the unlabeled like-
lihood term is optimized in our stochastic learning
66
algorithm. First, we note that, given the directed
nature of the arcs between z and y, the weights
w do not affect the probability of input x, that is
P (x|?) = P (x|v).
Instead of directly approximating the gradient
?v logP (x(l)|v), we use a deterministic version of
the Contrastive Divergence (CD) algorithm, equiv-
alent to the mean-field approximation of the recon-
struction error used in training autoassociaters (Ben-
gio and Delalleau, 2007). The CD-based estimators
are biased estimators but are guaranteed to converge.
Intuitively, maximizing the likelihood of unlabeled
data is closely related to minimizing the reconstruc-
tion error, that is training a model to discover such
mapping parameters u that z encodes all the neces-
sary information to accurately reproduce x(l) from z
for every training example x(l). Formally, the mean-
field approximation to the negated reconstruction er-
ror is defined as
L?(x(l),v) = logP (x(l)|?,v),
where the means, ?i = P (zi = 1|x(l),v), are com-
puted as in the preceding section. Note that when
computing the gradient of?vL?, we need to take into
account both the forward and backward mappings:
the computation of the means ? from x(l) and the
computation of the log-probability of x(l) given the
means ?:
dL?
dvki
=
?L?
?vki
+
?L?
??i
d?i
dvki
.
4.3 Regularization Term
The criterion G(?) is also independent of the classi-
fier parametersw, i.e. G(?) = G(v), and our goal is
to compute the contribution of a considered example
l to the gradient?vG(v).
The regularizer G(v) is defined as in equation (1)
and it is a function of the sample-based domain-
specific marginal distributions of latent variables PS
and PT :
PT (zi = 1|?) =
1
|TU |
?
l?TU
?(l)i ,
where the means ?(l)i = P (zi = 1|x
(l),v); PS can
be re-written analogously. G(v) is dependent on the
parameters v only via the mean activations of the
latent variables ?(l), and contribution of each exam-
ple l can be computed by straightforward differenti-
ation:
dG(l)(v)
dvki
=(log
p
p?
?log
1? p
1? p?
?
p?
p
+
1? p?
1? p
)
d?(l)i
dvki
,
where p = PS(zi = 1|?) and p? = PT (zi = 1|?)
if l is from the source domain, and, inversely, p =
PT (zi = 1|?) and p? = PS(zi = 1|?), otherwise.
One problem with the above expression is that
the exact computation of PS and PT requires re-
computation of the means ?(l) for all the exam-
ples after each update of the parameters, resulting
in O(|SL ? SU ? TU |2) complexity of each iteration
of stochastic gradient descent. Instead, we shuffle
examples and use amortization; we approximate PS
at update t by:
P? (t)S (zi = 1)=
{
(1??)P? (t?1)S (zi=1)+??
(l)
i , l?SL?SU
P? (t?1)S (zi = 1), otherwise,
where l is an example considered at update t. The
approximation P?T is computed analogously.
5 Empirical Evaluation
In this section we empirically evaluate our approach
on the sentiment classification task. We start with
the description of the experimental set-up and the
baselines, then we present the results and discuss the
utility of the constraint on inter-domain variability.
5.1 Experimental setting
To evaluate our approach, we consider the same
dataset as the one used to evaluate the SCL
method (Blitzer et al, 2007). The dataset is com-
posed of labeled and unlabeled reviews of four dif-
ferent product types: books, DVDs, electronics and
kitchen appliances. For each domain, the dataset
contains 1,000 labeled positive reviews and 1,000 la-
beled negative reviews, as well as several thousands
of unlabeled examples (4,919 reviews per domain in
average: ranging from 3,685 for DVDs to 5,945 for
kitchen appliances). As in Blitzer et al (2007), we
randomly split each labelled portion into 1,600 ex-
amples for training and 400 examples for testing.
67
70
75
80
85
Books
70.8
72.7
74.7
76.5
75.6
83.3
DVD Electronics Kitchen Average
Base
NoReg
Reg
Reg+
In-domain
73.3
74.6
74.8
76.2
75.4
82.8
77.6
75.6
73.9
76.6
77.9
78.8
84.6
NoReg+
74.6
76.0
78.9
80.2
85.8
79.0
77.7
83.2
82.1
80.0
86.5
Figure 2: Averages accuracies when transferring to books, DVD, electronics and kitchen appliances domains, and
average accuracy over all 12 domain pairs.
We evaluate the performance of our domain-
adaptation approach on every ordered pair of do-
mains. For every pair, the semi-supervised meth-
ods use labeled data from the source domain and
unlabeled data from both domains. We compare
them with two supervised methods: a supervised
model (Base) which is trained on the source do-
main data only, and another supervised model (In-
domain) which is learned on the labeled data from
the target domain. The Base model can be regarded
as a natural baseline model, whereas the In-domain
model is essentially an upper-bound for any domain-
adaptation method. All the methods, supervised and
semi-supervised, are based on the model described
in Section 2.
Instead of using the full set of bigram and unigram
counts as features (Blitzer et al, 2007), we use a fre-
quency cut-off of 30 to remove infrequent ngrams.
This does not seem to have an adverse effect on the
accuracy but makes learning very efficient: the av-
erage training time for the semi-supervised methods
was about 20 minutes on a standard PC.
We coarsely tuned the parameters of the learning
methods using a form of cross-validation. Both the
parameter of the multi-conditional objective ? (see
Section 2) and the weighting for the constraint ? (see
Section 3.2) were set to 5. We used 25 iterations of
stochastic gradient descent. The initial learning rate
and the weight decay (the inverse squared variance
of the Gaussian prior) were set to 0.01, and both pa-
rameters were reduced by the factor of 2 every it-
eration the objective function estimate went down.
The size of the latent representation was equal to 10.
The stochastic weight updates were amortized with
the momentum (?) of 0.99.
We trained the model both without regularization
of the domain variability (NoReg, ? = 0), and with
the regularizing term (Reg). For the SCL method
to produce an accurate classifier for the target do-
main it is necessary to train a classifier using both the
induced shared representation and the initial non-
transformed representation. In our case, due to joint
learning and non-convexity of the learning problem,
this approach would be problematic.4 Instead, we
combine predictions of the semi-supervised mod-
els Reg and NoReg with the baseline out-of-domain
model (Base) using the product-of-experts combina-
tion (Hinton, 2002), the corresponding methods are
called Reg+ and NoReg+, respectively.
In all our models, we augmented the vector z with
an additional component set to 0 for examples in the
source domain and to 1 for the target domain exam-
ples. In this way, we essentially subtracted a un-
igram domain-specific model from our latent vari-
able model in the hope that this will further reduce
the domain dependence of the rest of the model pa-
rameters. In preliminary experiments, this modifica-
tion was beneficial for all the models including the
non-constrained one (NoReg).
5.2 Results and Discussion
The results of all the methods are presented in Fig-
ure 2. The 4 leftmost groups of results correspond
to a single target domain, and therefore each of
4The latent variables are not likely to learn any useful map-
ping in the presence of observable features. Special training
regimes may be used to attempt to circumvent this problem.
68
them is an average over experiments on 3 domain-
pairs, for instance, the group Books represents an
average over adaptation experiments DVDs?books,
electronics?books, kitchen?books. The rightmost
group of the results corresponds to the average over
all 12 experiments. First, observe that the total drop
in the accuracy when moving to the target domain is
8.9%: from 84.6% demonstrated by the In-domain
classifier to 75.6% shown by the non-adapted Base
classifier. For convenience, we also present the er-
rors due to transfer in a separate Table 1: our best
method (Reg+) achieves 35% relative reduction of
this loss, decreasing the gap to 5.7%.
Now, let us turn to the question of the utility of the
constraints. First, observe that the non-regularized
version of the model (NoReg) often fails to outper-
form the baseline and achieves the scores consider-
ably worse than the results of the regularized ver-
sion (2.6% absolute difference). We believe that
this happens because the clusters induced when opti-
mizing the non-regularized learning objective are of-
ten domain-specific. The regularized model demon-
strates substantially better results slightly beating
the baseline in most cases. Still, to achieve a
larger decrease of the domain-adaptation error, it
was necessary to use the combined models, Reg+
and NoReg+. Here, again, the regularized model
substantially outperforms the non-regularized one
(35% against 26% relative error reduction for Reg+
and NoReg+, respectively).
In Table 1, we also compare the results of
our method with the results of the best ver-
sion of the SCL method (SCL-MI) reported
in Blitzer et al (2007). The average error reduc-
tions for our method Reg+ and for the SCL method
are virtually equal. However, formally, these two
numbers are not directly comparable. First, the ran-
dom splits are different, though this is unlikely to
result in any significant difference, as the split pro-
portions are the same and the test sets are suffi-
ciently large. Second, the absolute scores achieved
in Blitzer et al (2007) are slightly worse than those
demonstrated in our experiments both for supervised
and semi-supervised methods. In absolute terms,
our Reg+ method outperforms the SCL method by
more than 1%: 75.6% against 74.5%, in average.
This is probably due to the difference in the used
learning methods: optimization of the Huber loss vs.
D Base NoReg Reg NoReg+ Reg+ SCL-MI
B 10.6 12.4 7.7 8.6 6.7 5.8
D 9.5 8.2 8.0 6.6 7.3 6.1
E 8.2 13.0 9.7 6.8 5.5 5.5
K 7.5 8.8 6.5 4.4 3.3 5.6
Av 8.9 10.6 8.0 6.6 5.7 5.8
Table 1: Drop in the accuracy score due to the transfer
for the 4 domains: (B)ooks, (D)VD, (E)electronics and
(K)itchen appliances, and in average over the domains.
our latent variable model.5 This comparison sug-
gests that our domain-adaptation method is a viable
alternative to SCL.
Also, it is important to point out that the SCL
method uses auxiliary tasks to induce the shared
feature representation, these tasks are constructed
on the basis of unlabeled data. The auxiliary tasks
and the original problem should be closely related,
namely they should have the same (or similar) set
of predictive features. Defining such tasks can be
a challenging engineering problem. On the senti-
ment classification task in order to construct them
two steps need to be performed: (1) a set of words
correlated with the sentiment label is selected, and,
then (2) prediction of each such word is regarded a
distinct auxiliary problem. For many other domains
(e.g., parsing (Plank, 2009)) the construction of an
effective set of auxiliary tasks is still an open prob-
lem.
6 Related Work
There is a growing body of work on domain adapta-
tion. In this paper, we focus on the class of meth-
ods which induce a shared feature representation.
Another popular class of domain-adaptation tech-
niques assume that the input distributions P (x) for
the source and the target domain share support, that
is every example x which has a non-zero probabil-
ity on the target domain must have also a non-zero
probability on the source domain, and vice-versa.
Such methods tackle domain adaptation by instance
re-weighting (Bickel et al, 2007; Jiang and Zhai,
2007), or, similarly, by feature re-weighting (Sat-
pal and Sarawagi, 2007). In NLP, most features
5The drop in accuracy for the SCL method in Table 1 is is
computed with respect to the less accurate supervised in-domain
classifier considered in Blitzer et al (2007), otherwise, the com-
puted drop would be larger.
69
are word-based and lexicons are very different for
different domains, therefore such assumptions are
likely to be overly restrictive.
Various semi-supervised techniques for domain-
adaptation have also been considered, one example
being self-training (McClosky et al, 2006). How-
ever, their behavior in the domain-adaptation set-
ting is not well-understood. Semi-supervised learn-
ing with distributed representations and its applica-
tion to domain adaptation has previously been con-
sidered in (Huang and Yates, 2009), but no attempt
has been made to address problems specific to the
domain-adaptation setting. Similar approaches has
also been considered in the context of topic mod-
els (Xue et al, 2008), however the preference to-
wards induction of domain-independent topics was
not explicitly encoded in the learning objective or
model priors.
A closely related method to ours is that
of (Druck and McCallum, 2010) which performs
semi-supervised learning with posterior regulariza-
tion (Ganchev et al, 2010). Our approach differs
from theirs in many respects. First, they do not fo-
cus on the domain-adaptation setting and do not at-
tempt to define constraints to prevent the model from
learning domain-specific information. Second, their
expectation constraints are estimated from labeled
data, whereas we are trying to match expectations
computed on unlabeled data for two domains.
This approach bears some similarity to the adap-
tation methods standard for the setting where la-
belled data is available for both domains (Chelba
and Acero, 2004; Daume? and Marcu, 2006). How-
ever, instead of ensuring that the classifier param-
eters are similar across domains, we favor models
resulting in similar marginal distributions of latent
variables.
7 Discussion and Conclusions
In this paper we presented a domain-adaptation
method based on semi-supervised learning with dis-
tributed representations coupled with constraints fa-
voring domain-independence of modeled phenom-
ena. Our approach results in competitive domain-
adaptation performance on the sentiment classifica-
tion task, rivalling that of the state-of-the-art SCL
method (Blitzer et al, 2007). Both of these meth-
ods induce a shared feature representation but un-
like SCL our method does not require construction
of any auxiliary tasks in order to induce this repre-
sentation. The primary area of the future work is to
apply our method to structured prediction problems
in NLP, such as syntactic parsing or semantic role la-
beling, where construction of auxiliary tasks proved
problematic. Another direction is to favor domain-
invariability not only of the expectations of individ-
ual variables but rather those of constraint functions
involving latent variables, features and labels.
Acknowledgements
The author acknowledges the support of the Cluster
of Excellence on Multimodal Computing and Inter-
action at Saarland University and thanks the anony-
mous reviewers for their helpful comments and sug-
gestions.
References
Shai Ben-David, John Blitzer, Koby Crammer, Alex
Kulesza, Fernando Pereira, and Jennifer Wortman
Vaughan. 2010. A theory of learning from different
domains. Machine Learning, 79:151?175.
Yoshua Bengio and Olivier Delalleau. 2007. Justify-
ing and generalizing contrastive divergence. Techni-
cal Report TR 1311, Department IRO, University of
Montreal, November.
S. Bickel, M. Bru?eckner, and T. Scheffer. 2007. Dis-
criminative learning for differing training and test dis-
tributions. In Proc. of the International Conference on
Machine Learning (ICML), pages 81?88.
Christopher M. Bishop. 1995. Neural Networks for Pat-
tern Recognition. Oxford University Press, Oxford,
UK.
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In Proc. of EMNLP.
John Blitzer, Mark Dredze, and Fernando Pereira. 2007.
Biographies, bollywood, boom-boxes and blenders:
Domain adaptation for sentiment classification. In
Proc. 45th Meeting of Association for Computational
Linguistics (ACL), Prague, Czech Republic.
John Blitzer, Koby Crammer, Alex Kulesza, Fernando
Pereira, and Jennifer Wortman. 2008. Learning
bounds for domain adaptation. In Proc. Advances In
Neural Information Processing Systems (NIPS ?07).
Ciprian Chelba and Alex Acero. 2004. Adaptation of
maximum entropy capitalizer: Little data can help a
lot. In Proc. of the Conference on Empirical Meth-
ods for Natural Language Processing (EMNLP), pages
285?292.
70
R. Collobert and J. Weston. 2008. A unified architecture
for natural language processing: Deep neural networks
with multitask learning. In International Conference
on Machine Learning, ICML.
Hal Daume? and Daniel Marcu. 2006. Domain adaptation
for statistical classifiers. Journal of Artificial Intelli-
gence, 26:101?126.
Gregory Druck and Andrew McCallum. 2010. High-
performance semi-supervised learning using discrim-
inatively constrained generative models. In Proc. of
the International Conference on Machine Learning
(ICML), Haifa, Israel.
Kuzman Ganchev, Joao Graca, Jennifer Gillenwater, and
Ben Taskar. 2010. Posterior regularization for struc-
tured latent variable models. Journal of Machine
Learning Research (JMLR), pages 2001?2049.
Andrea Gesmundo, James Henderson, Paola Merlo, and
Ivan Titov. 2009. Latent variable model of syn-
chronous syntactic-semantic parsing for multiple lan-
guages. In CoNLL 2009 Shared Task.
Zoubin Ghahramani and Michael I. Jordan. 1997. Fac-
torial hidden Markov models. Machine Learning,
29:245?273.
G. E. Hinton and R. R. Salakhutdinov. 2006. Reducing
the dimensionality of data with neural networks. Sci-
ence, 313:504?507.
Geoffrey E. Hinton. 2002. Training Products of Experts
by Minimizing Contrastive Divergence. Neural Com-
putation, 14:1771?1800.
Fei Huang and Alexander Yates. 2009. Distributional
representations for handling sparsity in supervised se-
quence labeling. In Proceedings of the Annual Meet-
ing of the Association for Computational Linguistics
(ACL).
Jing Jiang and ChengXiang Zhai. 2007. Instance weight-
ing for domain adaptation in nlp. In Proc. of the
Annual Meeting of the ACL, pages 264?271, Prague,
Czech Republic, June. Association for Computational
Linguistics.
Gideon S. Mann and Andrew McCallum. 2010. General-
ized expectation criteria for semi-supervised learning
with weakly labeled data. Journal of Machine Learn-
ing Research, 11:955?984.
Yishay Mansour, Mehryar Mohri, and Afshin Ros-
tamizadeh. 2008. Domain adaptation with multiple
sources. In Advances in Neural Information Process-
ing Systems.
Yishay Mansour, Mehryar Mohri, and Afshin Ros-
tamizadeh. 2009. Domain adaptation: Learning
bounds and algorithms. In Proceedings of The 22nd
Annual Conference on Learning Theory (COLT 2009),
Montreal, Canada.
Andrew McCallum, Chris Pal, Greg Druck, and Xuerui
Wang. 2006. Multi-conditional learning: Genera-
tive/discriminative training for clustering and classifi-
cation. In AAAI.
David McClosky, Eugene Charniak, and Mark Johnson.
2006. Reranking and self-training for parser adapta-
tion. In Proc. of the Annual Meeting of the ACL and
the International Conference on Computational Lin-
guistics, Sydney, Australia.
B. Pang, L. Lee, and S. Vaithyanathan. 2002. Thumbs
up? Sentiment classification using machine learning
techniques. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing.
Barbara Plank. 2009. Structural correspondence learning
for parse disambiguation. In Proceedings of the Stu-
dent Research Workshop at EACL 2009, pages 37?45,
Athens, Greece, April. Association for Computational
Linguistics.
Sandeepkumar Satpal and Sunita Sarawagi. 2007. Do-
main adaptation of conditional probability models via
feature subsetting. In Proceedings of 11th European
Conference on Principles and Practice of Knowledge
Discovery in Databases (PKDD), Warzaw, Poland.
Lawrence K. Saul, Tommi Jaakkola, and Michael I. Jor-
dan. 1996. Mean field theory for sigmoid belief
networks. Journal of Artificial Intelligence Research,
4:61?76.
Paul Smolensky. 1986. Information processing in dy-
namical systems: foundations of harmony theory. In
D. Rumehart and J McCelland, editors, Parallel dis-
tributed processing: explorations in the microstruc-
tures of cognition, volume 1 : Foundations, pages 194?
281. MIT Press.
Ivan Titov and James Henderson. 2007a. Constituent
parsing with Incremental Sigmoid Belief Networks. In
Proc. 45th Meeting of Association for Computational
Linguistics (ACL), pages 632?639, Prague, Czech Re-
public.
Ivan Titov and James Henderson. 2007b. Fast and robust
multilingual dependency parsing with a generative la-
tent variable model. In Proc. of the CoNLL shared
task, Prague, Czech Republic.
G.-R. Xue, W. Dai, Q. Yang, and Y. Yu. 2008. Topic-
bridged PLSA for cross-domain text classification. In
Proceedings of the SIGIR Conference.
71
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1445?1455,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
A Bayesian Model for Unsupervised Semantic Parsing
Ivan Titov
Saarland University
Saarbruecken, Germany
titov@mmci.uni-saarland.de
Alexandre Klementiev
Johns Hopkins University
Baltimore, MD, USA
aklement@jhu.edu
Abstract
We propose a non-parametric Bayesian model
for unsupervised semantic parsing. Follow-
ing Poon and Domingos (2009), we consider
a semantic parsing setting where the goal is to
(1) decompose the syntactic dependency tree
of a sentence into fragments, (2) assign each
of these fragments to a cluster of semanti-
cally equivalent syntactic structures, and (3)
predict predicate-argument relations between
the fragments. We use hierarchical Pitman-
Yor processes to model statistical dependen-
cies between meaning representations of pred-
icates and those of their arguments, as well
as the clusters of their syntactic realizations.
We develop a modification of the Metropolis-
Hastings split-merge sampler, resulting in an
efficient inference algorithm for the model.
The method is experimentally evaluated by us-
ing the induced semantic representation for
the question answering task in the biomedical
domain.
1 Introduction
Statistical approaches to semantic parsing have re-
cently received considerable attention. While some
methods focus on predicting a complete formal rep-
resentation of meaning (Zettlemoyer and Collins,
2005; Ge and Mooney, 2005; Mooney, 2007), others
consider more shallow forms of representation (Car-
reras and Ma`rquez, 2005; Liang et al, 2009). How-
ever, most of this research has concentrated on su-
pervised methods requiring large amounts of labeled
data. Such annotated resources are scarce, expensive
to create and even the largest of them tend to have
low coverage (Palmer and Sporleder, 2010), moti-
vating the need for unsupervised or semi-supervised
techniques.
Conversely, research in the closely related task
of relation extraction has focused on unsupervised
or minimally supervised methods (see, for example,
(Lin and Pantel, 2001; Yates and Etzioni, 2009)).
These approaches cluster semantically equivalent
verbalizations of relations, often relying on syn-
tactic fragments as features for relation extraction
and clustering (Lin and Pantel, 2001; Banko et al,
2007). The success of these methods suggests that
semantic parsing can also be tackled as clustering
of syntactic realizations of predicate-argument rela-
tions. While a similar direction has been previously
explored in (Swier and Stevenson, 2004; Abend et
al., 2009; Lang and Lapata, 2010), the recent work
of (Poon and Domingos, 2009) takes it one step
further by not only predicting predicate-argument
structure of a sentence but also assigning sentence
fragments to clusters of semantically similar expres-
sions. For example, for a pair of sentences on Fig-
ure 1, in addition to inducing predicate-argument
structure, they aim to assign expressions ?Steelers?
and ?the Pittsburgh team? to the same semantic
class Steelers, and group expressions ?defeated?
and ?secured the victory over?. Such semantic rep-
resentation can be useful for entailment or question
answering tasks, as an entailment model can ab-
stract away from specifics of syntactic and lexical
realization relying instead on the induced semantic
representation. For example, the two sentences in
Figure 1 have identical semantic representation, and
therefore can be hypothesized to be equivalent.
1445
Ravens??defeated??Steelers
Ravenas?
dftSlrtS
cuh?vl io??y?nl
Ravv?n Pbbfv?vo
Ravens?secured?the?victory?over?the?Pittsburgh?team
io??y?nl
Ravenas?
lrtS dftS
bbgfh?n
cuh?vl
Ravv?n Pbbfv?vo
vmfd
Figure 1: An example of two different syntactic trees with a common semantic representation WinPrize(Ravens,
Steelers).
From the statistical modeling point of view, joint
learning of predicate-argument structure and dis-
covery of semantic clusters of expressions can also
be beneficial, because it results in a more compact
model of selectional preference, less prone to the
data-sparsity problem (Zapirain et al, 2010). In this
respect our model is similar to recent LDA-based
models of selectional preference (Ritter et al, 2010;
Se?aghdha, 2010), and can even be regarded as their
recursive and non-parametric extension.
In this paper, we adopt the above definition of un-
supervised semantic parsing and propose a Bayesian
non-parametric approach which uses hierarchical
Pitman-Yor (PY) processes (Pitman, 2002) to model
statistical dependencies between predicate and ar-
gument clusters, as well as distributions over syn-
tactic and lexical realizations of each cluster. Our
non-parametric model automatically discovers gran-
ularity of clustering appropriate for the dataset, un-
like the parametric method of (Poon and Domingos,
2009) which have to perform model selection and
use heuristics to penalize more complex models of
semantics. Additional benefits generally expected
from Bayesian modeling include the ability to en-
code prior linguistic knowledge in the form of hy-
perpriors and the potential for more reliable model-
ing of smaller datasets. More detailed discussion of
relation between the Markov Logic Network (MLN)
approach of (Poon and Domingos, 2009) and our
non-parametric method is presented in Section 3.
Hierarchical Pitman-Yor processes (or their spe-
cial case, hierarchical Dirichlet processes) have pre-
viously been used in NLP, for example, in the con-
text of syntactic parsing (Liang et al, 2007; John-
son et al, 2007). However, in all these cases the
effective size of the state space (i.e., the number
of sub-symbols in the infinite PCFG (Liang et al,
2007), or the number of adapted productions in the
adaptor grammar (Johnson et al, 2007)) was not
very large. In our case, the state space size equals
the total number of distinct semantic clusters, and,
thus, is expected to be exceedingly large even for
moderate datasets: for example, the MLN model in-
duces 18,543 distinct clusters from 18,471 sentences
of the GENIA corpus (Poon and Domingos, 2009).
This suggests that standard inference methods for hi-
erarchical PY processes, such as Gibbs sampling,
Metropolis-Hastings (MH) sampling with uniform
proposals, or the structured mean-field algorithm,
are unlikely to result in efficient inference: for ex-
ample in standard Gibbs sampling all thousands of
alternatives should be considered at each sampling
move. Instead, we use a split-merge MH sampling
algorithm, which is a standard and efficient infer-
ence tool for non-hierarchical PY processes (Jain
and Neal, 2000; Dahl, 2003) but has not previously
been used in hierarchical setting. We extend the
sampler to include composition-decomposition of
syntactic fragments in order to cluster fragments of
variables size, as in the example Figure 1, and also
include the argument role-syntax alignment move
which attempts to improve mapping between seman-
tic roles and syntactic paths for some fixed predicate.
Evaluating unsupervised models is a challenging
task. We evaluate our model both qualitatively, ex-
amining the revealed clustering of syntactic struc-
tures, and quantitatively, on a question answering
task. In both cases, we follow (Poon and Domingos,
2009) in using the corpus of biomedical abstracts.
Our model achieves favorable results significantly
outperforming the baselines, including state-of-the-
art methods for relation extraction, and achieves
scores comparable to those of the MLN model.
The rest of the paper is structured as follows. Sec-
tion 2 begins with a definition of the semantic pars-
ing task. Sections 3 and 4 give background on the
MLN model and the Pitman-Yor processes, respec-
tively. In Sections 5 and 6, we describe our model
and the inference method. Section 7 provides both
qualitative and quantitative evaluation. Finally, ad-
1446
ditional related work is presented in Section 8.
2 Semantic Parsing
In this section, we briefly define the unsupervised
semantic parsing task and underlying aspects and as-
sumptions relevant to our model.
Unlike (Poon and Domingos, 2009), we do not
use the lambda calculus formalism to define our task
but rather treat it as an instance of frame-semantic
parsing, or a specific type of semantic role label-
ing (Gildea and Jurafsky, 2002). The reason for this
is two-fold: first, the frame semantics view is more
standard in computational linguistics, sufficient to
describe induced semantic representation and conve-
nient to relate our method to the previous work. Sec-
ond, lambda calculus is a considerably more power-
ful formalism than the predicate-argument structure
used in frame semantics, normally supporting quan-
tification and logical connectors (for example, nega-
tion and disjunction), neither of which is modeled
by our model or in (Poon and Domingos, 2009).
In frame semantics, the meaning of a predicate
is conveyed by a frame, a structure of related con-
cepts that describes a situation, its participants and
properties (Fillmore et al, 2003). Each frame is
characterized by a set of semantic roles (frame el-
ements) corresponding to the arguments of the pred-
icate. It is evoked by a frame evoking element (a
predicate). The same frame can be evoked by differ-
ent but semantically similar predicates: for exam-
ple, both verbs ?buy? and ?purchase? evoke frame
Commerce buy in FrameNet (Fillmore et al, 2003).
The aim of the semantic role labeling task is to
identify all of the frames evoked in a sentence and
label their semantic role fillers. We extend this task
and treat semantic parsing as recursive prediction of
predicate-argument structure and clustering of argu-
ment fillers. Thus, parsing a sentence into this rep-
resentation involves (1) decomposing the sentence
into lexical items (one or more words), (2) assigning
a cluster label (a semantic frame or a cluster of ar-
gument fillers) to every lexical item, and (3) predict-
ing argument-predicate relations between the lexical
items. This process is illustrated in Figure 1. For
the leftmost example, the sentence is decomposed
into three lexical items: ?Ravens?, ?defeated?
and ?Steelers?, and they are assigned to clusters
Ravens, WinPrize and Steelers, respectively.
Then Ravens and Steelers are selected as a
Winner and an Opponent in the WinPrize frame.
In this work, we define a joint model for the label-
ing and argument identification stages. Similarly to
core semantic roles in FrameNet, semantic roles are
treated as frame-specific in our model, as our model
does not try to discover any correspondences be-
tween roles in different frames.
As you can see from the above description, frames
(which groups predicates with similar meaning such
as the WinPrize frame in our example) and clus-
ters of argument fillers (Ravens and Steelers) are
treated in our definition in a similar way. For con-
venience, we will refer to both types of clusters as
semantic classes.1
This definition of semantic parsing is closely re-
lated to a realistic relation extraction setting, as both
clustering of syntactic forms of relations (or extrac-
tion patterns) and clustering of argument fillers for
these relations is crucial for automatic construction
of knowledge bases (Yates and Etzioni, 2009).
In this paper, we make three assumptions. First,
we assume that each lexical item corresponds to a
subtree of the syntactic dependency graph of the
sentence. This assumption is similar to the ad-
jacency assumption in (Zettlemoyer and Collins,
2005), though ours may be more appropriate for lan-
guages with free or semi-free word order, where syn-
tactic structures are inherently non-projective. Sec-
ond, we assume that the semantic arguments are lo-
cal in the dependency tree; that is, one lexical item
can be a semantic argument of another one only if
they are connected by an arc in the dependency tree.
This is a slight simplification of the semantic role
labeling problem but one often made. Thus, the ar-
gument identification and labeling stages consist of
labeling each syntactic arc with a semantic role la-
bel. In comparison, the MLN model does not explic-
itly assume contiguity of lexical items and does not
make this directionality assumption but their clus-
tering algorithm uses initialization and clusterization
moves such that the resulting model also obeys both
of these constraints. Third, as in (Poon and Domin-
gos, 2009), we do not model polysemy as we assume
1Semantic classes correspond to lambda-form clusters in
(Poon and Domingos, 2009) terminology.
1447
that each syntactic fragment corresponds to a single
semantic class. This is not a model assumption and
is only used at inference as it reduces mixing time of
the Markov chain. It is not likely to be restrictive for
the biomedical domain studied in our experiments.
As in some of the recent work on learning se-
mantic representations (Eisenstein et al, 2009; Poon
and Domingos, 2009), we assume that dependency
structures are provided for every sentence. This as-
sumption allows us to construct models of seman-
tics not Markovian within a sequence of words (see
for an example a model described in (Liang et al,
2009)), but rather Markovian within a dependency
tree. Though we include generation of the syntac-
tic structure in our model, we would not expect that
this syntactic component would result in an accurate
syntactic model, even if trained in a supervised way,
as the chosen independence assumptions are over-
simplistic. In this way, we can use a simple gener-
ative story and build on top of the recent success in
syntactic parsing.
3 Relation to the MLN Approach
The work of (Poon and Domingos, 2009) models
joint probability of the dependency tree and its latent
semantic representation using Markov Logic Net-
works (MLNs) (Richardson and Domingos, 2006),
selecting parameters (weights of first-order clauses)
to maximize the probability of the observed depen-
dency structures. For each sentence, the MLN in-
duces a Markov network, an undirected graphical
model with nodes corresponding to ground atoms
and cliques corresponding to ground clauses.
The MLN is a powerful formalism and allows for
modeling complex interaction between features of
the input (syntactic trees) and latent output (seman-
tic representation), however, unsupervised learn-
ing of semantics with general MLNs can be pro-
hibitively expensive. The reason for this is that
MLNs are undirected models and when learned to
maximize likelihood of syntactically annotated sen-
tences, they would require marginalization over se-
mantic representation but also over the entire space
of syntactic structures and lexical units. Given the
complexity of the semantic parsing task and the need
to tackle large datasets, even approximate methods
are likely to be infeasible. In order to overcome
this problem, (Poon and Domingos, 2009) group pa-
rameters and impose local normalization constraints
within each group. Given these normalization con-
straints, and additional structural constraints satis-
fied by the model, namely that the clauses should
be engineered in such a way that they induce tree-
structured graphs for every sentence, the parameters
can be estimated by a variant of the EM algorithm.
The class of such restricted MLNs is equivalent
to the class of directed graphical models over the
same set of random variables corresponding to frag-
ments of syntactic and semantic structure. Given
that the above constraints do not directly fit into the
MLN methodology, we believe that it is more nat-
ural to regard their model as a directed model with
an underlying generative story specifying how the
semantic structure is generated and how the syntac-
tic parse is drawn for this semantic structure. This
view would facilitate understanding what kind of
features can easily be integrated into the model, sim-
plify application of non-parametric Bayesian tech-
niques and expedite the use of inference techniques
designed specifically for directed models. Our ap-
proach makes one step in this direction by proposing
a non-parametric version of such generative model.
4 Hierarchical Pitman-Yor Processes
The central component of our non-parametric
Bayesian model are Pitman-Yor (PY) processes,
which are a generalization of the Dirichlet processes
(DPs) (Ferguson, 1973). We use PY processes to
model distributions of semantic classes appearing as
an argument of other semantic classes. We also use
them to model distributions of syntactic realizations
for each semantic class and distributions of syntactic
dependency arcs for argument types. In this section
we present relevant background on PY processes.
For a more detailed consideration we refer the reader
to (Teh et al, 2006).
The Pitman-Yor process over a set S, denoted
PY (?, ?,H), is a stochastic process whose samples
G0 constitute probability measures on partitions of
S. In practice, we do not need to draw measures,
as they can be analytically marginalized out. The
conditional distribution of xj+1 given the previous
j draws, with G0 marginalized out, follows (Black-
1448
well and MacQueen, 1973)
xj+1|x1, . . . xj ?
K?
k=1
jk ? ?
j+?
??k +
K? + ?
j+?
H. (1)
where ?1, . . . , ?K are K values assigned to
x1, x2, . . . , xj . The number of times ?k was as-
signed is denoted jk, so that j =
?K
k=1 jk. The
parameter ? < 1 controls how heavy the tail of the
distribution is: when it approaches 1, a new value is
assigned to every draw, when ? = 0 the PY process
reduces to DP. The expected value of K scales as
O(?n?) with the number of draws n, while it scales
only logarithmically for DP processes. PY processes
are expected to be more appropriate for many NLP
problems, as they model power-law type distribu-
tions common for natural language (Teh, 2006).
Hierarchical Dirichlet Processes (HDP) or hierar-
chical PY processes are used if the goal is to draw
several related probability measures for the same
set S. For example, they can be used to generate
transition distributions of a Markov model, HDP-
HMM (Teh et al, 2006; Beal et al, 2002). For
such a HMM, the top-level state proportions are
drawn from the top-level stick breaking construction
? ? GEM(?, ?), and then the individual transi-
tion distributions for every state z = 1, 2, . . . ?z are
drawn from PY (?, ??, ??). The parameters ?? and
?? control how similar the individual transition dis-
tributions ?z are to the top-level state proportions ?,
or, equivalently, how similar the transition distribu-
tions are to each other.
5 A Model for Semantic Parsing
Our model of semantics associates with each seman-
tic class a set of distributions which govern the gen-
eration of corresponding syntactic realizations2 and
the selection of semantic classes for its arguments.
Each sentence is generated starting from the root of
its dependency tree, recursively drawing a seman-
tic class, its syntactic realization, arguments and se-
mantic classes for the arguments. Below we de-
scribe the model by first defining the set of the model
parameters and then explaining the generation of in-
2Syntactic realizations are syntactic tree fragments, and
therefore they correspond both to syntactic and lexical varia-
tions.
dividual sentences. The generative story is formally
presented in Figure 2.
We associate with each semantic class c, c =
1, 2, . . . , a distribution of its syntactic realizations
?c. For example, for the frame WinPrize illus-
trated in Figure 1 this distribution would concen-
trate at syntactic fragments corresponding to lexical
items ?defeated?, ?secured the victory? and ?won?.
The distribution is drawn from DP (w(C), H(C)),
where H(C) is a base measure over syntactic sub-
trees. We use a simple generative process to define
the probability of a subtree, the underlying model is
similar to the base measures used in the Bayesian
tree-substitution grammars (Cohn et al, 2009). We
start by generating a word w uniformly from the
treebank distribution, then we decide on the num-
ber of dependents of w using the geometric distribu-
tion Geom(q(C)). For every dependent we generate
a dependency relation r and a lexical form w? from
P (r|w)P (w?|r), where probabilities P are based on
add-0.1 smoothed treebank counts. The process is
continued recursively. The smaller the parameter
q(C), the lower is the probability assigned to larger
sub-trees.
Parameters ?c,t and ?
+
c,t, t = 1, . . . , T , de-
fine a distribution over vectors (m1,m2, . . . ,mT )
where mt is the number of times an argument of
type t appears for a given semantic frame occur-
rence3. For the frame WinPrize these parameters
would enforce that there exists exactly one Winner
and exactly one Opponent for each occurrence of
WinPrize. The parameter ?c,t defines the probabil-
ity of having at least one argument of type t. If 0 is
drawn from ?c,t then mt = 0, otherwise the number
of additional arguments of type t (mt ? 1) is drawn
from the geometric distribution Geom(?+c,t). This
generative story is flexible enough to accommodate
both argument types which appear at most once per
semantic class occurrence (e.g., agents), and argu-
ment types which frequently appear multiple times
per semantic class occurrence (e.g., arguments cor-
responding to descriptors).
Parameters ?c,t, t = 1, . . . , T , define the dis-
3For simplicity, we assume that each semantic class has T
associated argument types, note that this is not a restrictive as-
sumption as some of the argument types can remain unused,
and T can be selected to be sufficiently large to accommodate
all important arguments.
1449
Parameters:
? ? GEM(?0, ?0) [top-level proportions of classes]
?root ? PY (?root, ?root, ?) [distrib of sem classes at root]
for each sem class c = 1, 2, . . . :
?c ? DP (w(C), H(C)) [distribs of synt realizations]
for each arg type t = 1, 2, . . . T :
?c,t ? Beta(?0, ?1) [first argument generation]
?+c,t ? Beta(?
+
0 , ?
+
1 ) [geom distr for more args]
?c,t ? DP (w(A), H(A)) [distribs of synt paths]
?c,t ? PY (?, ?, ?) [distrib of arg fillers]
Data Generation:
for each sentence:
croot ? ?root [choose sem class for root]
GenSemClass(croot)
GenSemClass(c):
s ? ?c [draw synt realization]
for each arg type t = 1, . . . , T :
if [n ? ?c,t] = 1: [at least one arg appears]
GenArgument(c, t) [draw one arg]
while [n ? ?+c,t] = 1: [continue generation]
GenArgument(c, t) [draw more args]
GenArgument(c, t):
ac,t ? ?c,t [draw synt relation]
c?c,t ? ?c,t [draw sem class for arg]
GenSemClass(c?c,t) [recurse]
Figure 2: The generative story for the Bayesian model for
unsupervised semantic parsing.
tributions over syntactic paths for the argument
type t. In our example, for argument type
Opponent, this distribution would associate most
of the probability mass with relations pp over, dobj
and pp against. These distributions are drawn from
DP (w(A), H(A)). In this paper we only consider
paths consisting of a single relation, therefore the
base probability distributionH(A) is just normalized
frequencies of dependency relations in the treebank.
The crucial part of the model are the selection-
preference parameters ?c,t, the distributions of se-
mantic classes c? for each argument type t of class
c. For arguments Winner and Opponent of the
frame WinPrize these distributions would assign
most of the probability mass to semantic classes de-
noting teams or players. Distributions ?c,t are drawn
from a hierarchical PY process: first, top-level pro-
portions of classes ? are drawn fromGEM(?0, ?0),
and then the individual distributions ?c,t over c? are
chosen from PY (?, ?, ?).
For each sentence, we first generate a class corre-
sponding to the root of the dependency tree from the
root-specific distribution of semantic classes ?root.
Then we recursively generate classes for the entire
sentence. For a class c, we generate the syntactic
realization s and for each of the T types, decide
how many arguments of that type to generate (see
GenSemClass in Figure 2). Then we generate each
of the arguments (see GenArgument) by first gen-
erating a syntactic arc ac,t, choosing a class as its
filler c?c,t and, finally, recursing.
6 Inference
In our model, latent states, modeled with hierarchi-
cal PY processes, correspond to distinct semantic
classes and, therefore, their number is expected to
be very large for any reasonable model of semantics.
As a result, many standard inference techniques,
such as Gibbs sampling, or the structured mean-field
method are unlikely to result in tractable inference.
One of the standard and most efficient samplers for
non-hierarchical PY processes are split-merge MH
samplers (Jain and Neal, 2000; Dahl, 2003). In this
section we explain how split-merge samplers can be
applied to our model.
6.1 Split and Merge Moves
On each move, split-merge samplers decide either
to merge two states into one (in our case, merge two
semantic classes), or split one state into two. These
moves can be computed efficiently for our model of
semantics. Note that for any reasonable model of
semantics only a small subset of the entire set of se-
mantic classes can be used as an argument for some
fixed semantic class due to selectional preferences
exhibited by predicates. For instance, only teams or
players can fill arguments of the frame WinPrize
in our running example. As a result, only a small
number of terms in the joint distribution has to be
evaluated on every move we may consider.
When estimating the model, we start with assign-
ing each distinct word (or, more precisely, a tuple
of a word?s stem and its part-of-speech tag) to an
individual semantic class. Then, we would iterate
by selecting a random pair of class occurrences, and
decide, at random, whether we attempt to perform a
split-merge move or a compose-decompose move.
1450
6.2 Compose and Decompose Moves
The compose-decompose operations modify syntac-
tic fragments assigned to semantic classes, com-
posing two neighboring dependency sub-trees or
decomposing a dependency sub-tree. If the two
randomly-selected syntactic fragments s and s? cor-
respond to different classes, c and c?, we attempt
to compose them into s? and create a new semantic
class c?. All occurrences of s? are assigned to this new
class c?. For example, if two randomly-selected oc-
currences have syntactic realizations ?secure? and
?victory? they can be composed to obtain the syn-
tactic fragment ?secure
dobj
??? victory?. This frag-
ment will be assigned to a new semantic class which
can later be merged with other classes, such as the
ones containing syntactic realizations ?defeat? or
?win?.
Conversely, if both randomly-selected syntactic
fragments are already composed in the correspond-
ing class, we attempt to split them.
6.3 Role-Syntax Alignment Move
Merge, compose and decompose moves require re-
computation of mapping between argument types
(semantic roles) and syntactic fragments. Comput-
ing the best statistical mapping is infeasible and
proposing a random mapping will result in many
attempted moves being rejected. Instead we use
a greedy randomized search method called Gibbs
scan (Dahl, 2003). Though it is a part of the above 3
moves, this alignment move is also used on its own
to induce semantic arguments for classes (frames)
with a single syntactic realization.
The Gibbs scan procedure is also used during the
split move to select one of the newly introduced
classes for each considered syntactic fragment.
6.4 Informed Proposals
Since the number of classes is very large, selecting
examples at random would result in a relatively low
proportion of moves getting accepted, and, conse-
quently, in a slow-mixing Markov chain. Instead of
selecting both class occurrences uniformly, we se-
lect the first occurrence from a uniform distribution
and then use a simple but effective proposal distri-
bution for selecting the second class occurrence.
Let us denote the class corresponding to the first
occurrence as c1 and its syntactic realization as s1
with a head word w1. We begin by selecting uni-
formly randomly whether to attempt a compose-
decompose or a split-merge move.
If we chose a compose-decompose move, we look
for words (children) which can be attached below
the syntactic fragment s1. We use the normalized
counts of these words conditioned on the parent s1 to
select the second word w2. We then select a random
occurrence of w2; if it is a part of syntactic realiza-
tion of c1 then a decompose move is attempted. Oth-
erwise, we try to compose the corresponding clus-
ters together.
If we selected a split-merge move, we use a dis-
tribution based on the cosine similarity of lexical
contexts of the words. The context is represented
as a vector of counts of all pairs of the form (head
word, dependency type) and (dependent, depen-
dency type). So, instead of selecting a word occur-
rence uniformly, each occurrence of every word w2
is weighted by its similarity to w1, where the simi-
larity is based on the cosine distance.
As the moves are dependent only on syntactic rep-
resentations, all the proposal distributions can be
computed once at the initialization stage.4
7 Empirical Evaluation
We induced a semantic representation over a collec-
tion of texts and evaluated it by answering questions
about the knowledge contained in the corpus. We
used the GENIA corpus (Kim et al, 2003), a dataset
of 1999 biomedical abstracts, and a set of questions
produced by (Poon and Domingos, 2009). A exam-
ple question is shown in Figure 3.
All model hyperpriors were set to maximize the
posterior, except for w(A) and w(C), which were set
to 1.e?10 and 1.e?35, respectively. Inference was
run for around 300,000 sampling iterations until the
percentage of accepted split-merge moves became
lower than 0.05%.
Let us examine some of the induced semantic
classes (Table 1) before turning to the question an-
swering task. Almost all of the clustered syntactic
4In order to minimize memory usage, we used frequency
cut-off of 10. For split-merge moves, we select words based
on the cosine distance if the distance is below 0.95 and sample
the remaining words uniformly. This also reduces the required
memory usage.
1451
Class Variations
1 motif, sequence, regulatory element, response ele-
ment, element, dna sequence
2 donor, individual, subject
3 important, essential, critical
4 dose, concentration
5 activation, transcriptional activation, transactiva-
tion
6 b cell, t lymphocyte, thymocyte, b lymphocyte, t
cell, t-cell line, human lymphocyte, t-lymphocyte
7 indicate, reveal, document, suggest, demonstrate
8 augment, abolish, inhibit, convert, cause, abrogate,
modulate, block, decrease, reduce, diminish, sup-
press, up-regulate, impair, reverse, enhance
9 confirm, assess, examine, study, evaluate, test, re-
solve, determine, investigate
10 nf-kappab, nf-kappa b, nfkappab, nf-kb
11 antiserum, antibody, monoclonal antibody, ab, an-
tisera, mab
12 tnfalpha, tnf-alpha, il-6, tnf
Table 1: Examples of the induced semantic classes.
realizations have a clear semantic connection. Clus-
ter 6, for example, clusters lymphocytes with the ex-
ception of thymocyte, a type of cell which gener-
ates T cells. Cluster 8 contains verbs roughly corre-
sponding to Cause change of position on a
scale frame in FrameNet. Verbs in class 9 are used
in the context of providing support for a finding or
an action, and many of them are listed as evoking
elements for the Evidence frame in FrameNet.
Argument types of the induced classes also show
a tendency to correspond to semantic roles. For ex-
ample, an argument type of class 2 is modeled as
a distribution over two argument parts, prep of and
prep from. The corresponding arguments define the
origin of the cells (transgenic mouse, smoker, volun-
teer, donor, . . . ).
We now turn to the QA task and compare our
model (USP-BAYES) with the results of baselines
considered in (Poon and Domingos, 2009). The first
set of baselines looks for answers by attempting to
match a verb and its argument in the question with
the input text. The first version (KW) simply re-
turns the rest of the sentence on the other side of the
verb, while the second (KW-SYN) uses syntactic in-
formation to extract the subject or the object of the
verb.
Other baselines are based on state-of-the-art re-
lation extraction systems. When the extracted rela-
tion and one of the arguments match those in a given
Total Correct Accuracy
KW 150 67 45%
KW-SYN 87 67 77%
TR-EXACT 29 23 79%
TR-SUB 152 81 53%
RS-EXACT 53 24 45%
RS-SUB 196 81 41%
DIRT 159 94 59%
USP-MLN 334 295 88%
USP-BAYES 325 259 80%
Table 2: Performance on the QA task.
question, the second argument is returned as an an-
swer. The systems include TextRunner (TR) (Banko
et al, 2007), RESOLVER (RS) (Yates and Etzioni,
2009) and DIRT (Lin and Pantel, 2001). The EX-
ACT versions of the methods return answers when
they match the question argument exactly, and the
SUB versions produce answers containing the ques-
tion argument as a substring.
Similarly to the MLN system (USP-MLN), we
generate answers as follows. We use our trained
model to parse a question, i.e. recursively decom-
pose it into lexical items and assign them to seman-
tic classes induced at training. Using this semantic
representation, we look for the type of an argument
missing in the question, which, if found, is reported
as an answer. It is clear that overly coarse clusters
of argument fillers or clustering of semantically re-
lated but not equivalent relations can hurt precision
for this evaluation method.
Each system is evaluated by counting the answers
it generates, and computing the accuracy of those
answers.5 Table 2 summarizes the results. First,
both USP models significantly outperform all other
baselines: even though the accuracy of KW-SYN
and TR-EXACT are comparable with our accuracy,
the number of correct answers returned by USP-
Bayes is 4 and 11 times smaller than those of KW-
SYN and TR-EXACT, respectively. While we are
not beating the MLN baseline, the difference is not
significant. The effective number of questions is rel-
atively small (less than 80 different questions are an-
swered by any of the models). More than 50% of
USP-BAYES mistakes were due to wrong interpre-
tation of only 5 different questions. From another
point of view, most of the mistakes are explained
5The true recall is not known, as computing it would require
exhaustive annotation of the entire corpus.
1452
Question: What does cyclosporin A suppress?
Answer: expression of EGR-2
Sentence: As with EGR-3 , expression of EGR-2 was blocked
by cyclosporin A .
Question: What inhibits tnf-alpha?
Answer: IL -10
Sentence: Our previous studies in human monocytes have
demonstrated that interleukin ( IL ) -10 inhibits lipopolysac-
charide ( LPS ) -stimulated production of inflammatory cy-
tokines , IL-1 beta , IL-6 , IL-8 , and tumor necrosis factor (
TNF ) -alpha by blocking gene transcription .
Figure 3: An example of questions, answers by our model
and the corresponding sentences from the dataset.
by overly coarse clustering corresponding to just 3
classes, namely, 30%, 25% and 20% of errors are
due to the clusters 6, 8 and 12 (Figure 1), respec-
tively. Though all these clusters have clear semantic
interpretation (white blood cells, predicates corre-
sponding to changes and cykotines associated with
cancer progression, respectively), they appear to be
too coarse for the QA method we use in our exper-
iments. Though it is likely that tuning and differ-
ent heuristics may result in better scores, we chose
not to perform excessive tuning, as the evaluation
dataset is fairly small.
8 Related Work
There is a growing body of work on statistical learn-
ing for different versions of the semantic parsing
problem (e.g., (Gildea and Jurafsky, 2002; Zettle-
moyer and Collins, 2005; Ge and Mooney, 2005;
Mooney, 2007)), however, most of these methods
rely on human annotation, or some weaker forms of
supervision (Kate and Mooney, 2007; Liang et al,
2009; Titov and Kozhevnikov, 2010; Clarke et al,
2010) and very little research has considered the un-
supervised setting.
In addition to the MLN model (Poon and Domin-
gos, 2009), another unsupervised method has been
proposed in (Goldwasser et al, 2011). In that work,
the task is to predict a logical formula, and the only
supervision used is a lexicon providing a small num-
ber of examples for every logical symbol. A form of
self-training is then used to bootstrap the model.
Unsupervised semantic role labeling with a gen-
erative model has also been considered (Grenager
and Manning, 2006), however, they do not attempt
to discover frames and deal only with isolated pred-
icates. Another generative model for SRL has been
proposed in (Thompson et al, 2003), but the param-
eters were estimated from fully annotated data.
The unsupervised setting has also been consid-
ering for the related problem of learning narrative
schemas (Chambers and Jurafsky, 2009). However,
their approach is quite different from our Bayesian
model as it relies on similarity functions.
Though in this work we focus solely on the un-
supervised setting, there has been some success-
ful work on semi-supervised semantic-role label-
ing, including the Framenet version of the prob-
lem (Fu?rstenau and Lapata, 2009). Their method
exploits graph alignments between labeled and un-
labeled examples, and, therefore, crucially relies on
the availability of labeled examples.
9 Conclusions and Future Work
In this work, we introduced a non-parametric
Bayesian model for the semantic parsing problem
based on the hierarchical Pitman-Yor process. The
model defines a generative story for recursive gener-
ation of lexical items, syntactic and semantic struc-
tures. We extend the split-merge MH sampling algo-
rithm to include composition-decomposition moves,
and exploit the properties of our task to make it effi-
cient in the hierarchical setting we consider.
We plan to explore at least two directions in our
future work. First, we would like to relax some of
unrealistic assumptions made in our model: for ex-
ample, proper modeling of alterations requires joint
generation of syntactic realizations for predicate-
argument relations (Grenager and Manning, 2006;
Lang and Lapata, 2010), similarly, proper model-
ing of nominalization implies support of arguments
not immediately local in the syntactic structure. The
second general direction is the use of the unsuper-
vised methods we propose to expand the coverage of
existing semantic resources, which typically require
substantial human effort to produce.
Acknowledgements
The authors acknowledge the support of the MMCI Clus-
ter of Excellence, and thank Chris Callison-Burch, Alexis
Palmer, Caroline Sporleder, Ben Van Durme and the
anonymous reviewers for their helpful comments and
suggestions.
1453
References
O. Abend, R. Reichart, and A. Rappoport. 2009. Unsu-
pervised argument identification for semantic role la-
beling. In Proceedings of ACL-IJCNLP, pages 28?36,
Singapore.
Michele Banko, Michael J Cafarella, Stephen Soderland,
Matt Broadhead, and Oren Etzioni. 2007. Open in-
formation extraction from the web. In Proc. of the In-
ternational Joint Conference on Artificial Intelligence
(IJCAI), pages 2670?2676.
Matthew J. Beal, Zoubin Ghahramani, and Carl E. Ras-
mussen. 2002. The infinite hidden markov model. In
Machine Learning, pages 29?245. MIT Press.
David Blackwell and James B. MacQueen. 1973. Fergu-
son distributions via polya urn schemes. The Annals
of Statistics, 1(2):353?355.
Xavier Carreras and Llu??s Ma`rquez. 2005. Introduction
to the CoNLL-2005 Shared Task: Semantic Role La-
beling. In Proceedings of the 9th Conference on Natu-
ral Language Learning, CoNLL-2005, Ann Arbor, MI
USA.
Nathanael Chambers and Dan Jurafsky. 2009. Unsu-
pervised learning of narrative schemas and their par-
ticipants. In Proc. of the Annual Meeting of the As-
sociation for Computational Linguistics and Interna-
tional Joint Conference on Natural Language Process-
ing (ACL-IJCNLP).
James Clarke, Dan Goldwasser, Ming-Wei Chang, and
Dan Roth. 2010. Driving semantic parsing from the
world?s response. In Proc. of the Conference on Com-
putational Natural Language Learning (CoNLL).
Trevor Cohn, Sharon Goldwater, and Phil Blunsom.
2009. Inducing compact but accurate tree-substitution
grammars. In HLT-NAACL, pages 548?556.
David B. Dahl. 2003. An improved merge-split sampler
for conjugate dirichlet process mixture models. Tech-
nical Report 1086, Department of Statistics, Univer-
sity of Wiscosin - Madison, November.
Jacob Eisenstein, James Clarke, Dan Goldwasser, and
Dan Roth. 2009. Reading to learn: Constructing
features from semantic abstracts. In Proceedings of
EMNLP.
Thomas S. Ferguson. 1973. A bayesian analysis of
some nonparametric problems. The Annals of Statis-
tics, 1(2):209?230.
C. J. Fillmore, C. R. Johnson, and M. R. L. Petruck.
2003. Background to framenet. International Journal
of Lexicography, 16:235?250.
Hagen Fu?rstenau and Mirella Lapata. 2009. Graph align-
ment for semi-supervised semantic role labeling. In
Proceedings of Empirical Methods in Natural Lan-
guage Processing (EMNLP).
Ruifang Ge and Raymond J. Mooney. 2005. A statistical
semantic parser that integrates syntax and semantics.
In Proceedings of the Ninth Conference on Computa-
tional Natural Language Learning (CONLL-05), Ann
Arbor, Michigan.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic la-
belling of semantic roles. Computational Linguistics,
28(3):245?288.
Dan Goldwasser, Roi Reichart, James Clarke, and Dan
Roth. 2011. Confidence driven unsupervised semantic
parsing. In Proc. of the Meeting of Association for
Computational Linguistics (ACL), Portland, OR, USA.
Trond Grenager and Christoph Manning. 2006. Unsu-
pervised discovery of a statistical verb lexicon. In Pro-
ceedings of Empirical Methods in Natural Language
Processing (EMNLP).
Sonia Jain and Radford Neal. 2000. A split-merge
markov chain monte carlo procedure for the dirichlet
process mixture model. Journal of Computational and
Graphical Statistics, 13:158?182.
Mark Johnson, Thomas L. Griffiths, and Sharon Goldwa-
ter. 2007. Bayesian inference for PCFGs via Markov
chain Monte Carlo. In Human Language Technologies
2007: The Conference of the North American Chap-
ter of the Association for Computational Linguistics,
Rochester, USA.
Rohit J. Kate and Raymond J. Mooney. 2007. Learning
language semantics from ambigous supervision. In
Association for the Advancement of Artificial Intelli-
gence (AAAI), pages 895?900.
Jin-Dong Kim, Tomoko Ohta, Yuka Tateisi, and Jun?ichi
Tsujii. 2003. Genia corpus?a semantically annotated
corpus for bio-textmining. Bioinformatics, 19:i180?
i182.
Joel Lang and Mirella Lapata. 2010. Unsupervised in-
duction of semantic roles. In Proceedings of the 48rd
Annual Meeting of the Association for Computational
Linguistics (ACL), Uppsala, Sweden.
Percy Liang, Slav Petrov, Michael Jordan, and Dan Klein.
2007. The infinite PCFG using hierarchical dirich-
let processes. In Joint Conf. on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning (EMNLP-CoNLL), pages
688?697, Prague, Czech Republic.
Percy Liang, Michael I. Jordan, and Dan Klein. 2009.
Learning semantic correspondences with less supervi-
sion. In Proc. of the Annual Meeting of the Association
for Computational Linguistics and International Joint
Conference on Natural Language Processing (ACL-
IJCNLP).
Dekang Lin and Patrick Pantel. 2001. Dirt ? discovery
of inference rules from text. In Proc. of International
Conference on Knowledge Discovery and Data Min-
ing, pages 323?328.
1454
Raymond J. Mooney. 2007. Learning for semantic pars-
ing. In Proceedings of the 8th International Confer-
ence on Computational Linguistics and Intelligent Text
Processing, pages 982?991.
Alexis Palmer and Caroline Sporleder. 2010. Evaluating
framenet-style semantic parsing: the role of coverage
gaps in framenet. In Proceedings of the Conference on
Computational Linguistics (COLING-2000), Beijing.
Jim Pitman. 2002. Poisson-dirichlet and gem invari-
ant distributions for split-and-merge transformations
of an interval partition. Combinatorics, Probability
and Computing, 11:501?514.
Hoifung Poon and Pedro Domingos. 2009. Unsuper-
vised semantic parsing. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing, (EMNLP-09).
Matt Richardson and Pedro Domingos. 2006. Markov
logic networks. Machine Learning, 62:107?136.
Alan Ritter, Mausam, and Oren Etzioni. 2010. A latent
dirichlet alocation method for selectional preferences.
In Proceedings of the 48rd Annual Meeting of the As-
sociation for Computational Linguistics (ACL), Upp-
sala, Sweden.
Diarmuid O? Se?aghdha. 2010. Latent variable models
of selectional preference. In Proceedings of the 48rd
Annual Meeting of the Association for Computational
Linguistics (ACL), Uppsala, Sweden.
R. Swier and S. Stevenson. 2004. Unsupervised seman-
tic role labelling. In Proceedings of EMNLP, pages
95?102, Barcelona, Spain.
Y. W. Teh, M. I. Jordan, M. J. Beal, and D. M. Blei. 2006.
Hierarchical Dirichlet processes. Journal of the Amer-
ican Statistical Association, 101(476):1566?1581.
Y. W. Teh. 2006. A hierarchical Bayesian language
model based on Pitman-Yor processes. In Proceed-
ings of the 21st International Conference on Computa-
tional Linguistics and 44th Annual Meeting of the As-
sociation for Computational Linguistics, pages 985?
992.
Cynthia A. Thompson, Roger Levy, and Christopher D.
Manning. 2003. A generative model for semantic role
labeling. In In Senseval-3, pages 397?408.
Ivan Titov and Mikhail Kozhevnikov. 2010. Bootstrap-
ping semantic analyzers from non-contradictory texts.
In Proceedings of the 48rd Annual Meeting of the As-
sociation for Computational Linguistics (ACL), Upp-
sala, Sweden.
Alexander Yates and Oren Etzioni. 2009. Unsupervised
methods for determining object and relation synonyms
on the web. Journal of Artificial Intelligence Research,
34:255?296.
B. Zapirain, E. Agirre, L. L. Ma`rquez, and M. Surdeanu.
2010. Improving semantic role classification with se-
lectional prefrences. In Proceedings of the Meeting
of the North American chapter of the Association for
Computational Linguistics (NAACL 2010), Los Ange-
les.
Luke Zettlemoyer and Michael Collins. 2005. Learn-
ing to map sentences to logical form: Structured clas-
sification with probabilistic categorial grammar. In
Proceedings of the Twenty-first Conference on Uncer-
tainty in Artificial Intelligence, Edinburgh, UK, Au-
gust.
1455
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 647?656,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Crosslingual Induction of Semantic Roles
Ivan Titov Alexandre Klementiev
Saarland University
Saarbru?cken, Germany
{titov|aklement}@mmci.uni-saarland.de
Abstract
We argue that multilingual parallel data pro-
vides a valuable source of indirect supervision
for induction of shallow semantic representa-
tions. Specifically, we consider unsupervised
induction of semantic roles from sentences an-
notated with automatically-predicted syntactic
dependency representations and use a state-
of-the-art generative Bayesian non-parametric
model. At inference time, instead of only
seeking the model which explains the mono-
lingual data available for each language, we
regularize the objective by introducing a soft
constraint penalizing for disagreement in ar-
gument labeling on aligned sentences. We
propose a simple approximate learning algo-
rithm for our set-up which results in efficient
inference. When applied to German-English
parallel data, our method obtains a substantial
improvement over a model trained without us-
ing the agreement signal, when both are tested
on non-parallel sentences.
1 Introduction
Learning in the context of multiple languages simul-
taneously has been shown to be beneficial to a num-
ber of NLP tasks from morphological analysis to
syntactic parsing (Kuhn, 2004; Snyder and Barzilay,
2010; McDonald et al, 2011). The goal of this work
is to show that parallel data is useful in unsupervised
induction of shallow semantic representations.
Semantic role labeling (SRL) (Gildea and Juraf-
sky, 2002) involves predicting predicate argument
structure, i.e. both the identification of arguments
and their assignment to underlying semantic roles.
For example, in the following sentences:
(a) [A0Peter] blamed [A1Mary] [A2for planning a theft].
(b) [A0Peter] blamed [A2planning a theft] [A1on Mary].
(c) [A1Mary] was blamed [A2for planning a theft] [A0by
Peter]
the arguments ?Peter?, ?Mary?, and ?planning a theft?
of the predicate ?blame? take the agent (A0), patient
(A1) and reason (A2) roles, respectively. In this
work, we focus on predicting argument roles.
SRL representations have many potential appli-
cations in NLP and have recently been shown
to benefit question answering (Shen and Lapata,
2007; Kaisser and Webber, 2007), textual entailment
(Sammons et al, 2009), machine translation (Wu
and Fung, 2009; Liu and Gildea, 2010; Wu et al,
2011; Gao and Vogel, 2011), and dialogue systems
(Basili et al, 2009; van der Plas et al, 2011), among
others. Though syntactic representations are often
predictive of semantic roles (Levin, 1993), the inter-
face between syntactic and semantic representations
is far from trivial. Lack of simple deterministic rules
for mapping syntax to shallow semantics motivates
the use of statistical methods.
Most of the current statistical approaches to SRL
are supervised, requiring large quantities of human
annotated data to estimate model parameters. How-
ever, such resources are expensive to create and only
available for a small number of languages and do-
mains. Moreover, when moved to a new domain,
performance of these models tends to degrade sub-
stantially (Pradhan et al, 2008). Sparsity of anno-
tated data motivates the need to look to alternative
647
resources. In this work, we make use of unsuper-
vised data along with parallel texts and learn to in-
duce semantic structures in two languages simulta-
neously. As does most of the recent work on unsu-
pervised SRL, we assume that our data is annotated
with automatically-predicted syntactic dependency
parses and aim to induce a model of linking between
syntax and semantics in an unsupervised way.
We expect that both linguistic relatedness and
variability can serve to improve semantic parses in
individual languages: while the former can pro-
vide additional evidence, the latter can serve to re-
duce uncertainty in ambiguous cases. For example,
in our sentences (a) and (b) representing so-called
blame alternation (Levin, 1993), the same informa-
tion is conveyed in two different ways and a success-
ful model of semantic role labeling needs to learn
the corresponding linkings from the data. Induc-
ing them solely based on monolingual data, though
possible, may be tricky as selectional preferences
of the roles are not particularly restrictive; similar
restrictions for patient and agent roles may further
complicate the process. However, both sentences
(a) and (b) are likely to be translated in German
as ?[A0Peter] beschuldigte [A1Mary] [A2einen Dieb-
stahl zu planen]?. Maximizing agreement between
the roles predicted for both languages would pro-
vide a strong signal for inducing the proper linkings
in our examples.
In this work, we begin with a state-of-the-art
monolingual unsupervised Bayesian model (Titov
and Klementiev, 2012) and focus on improving its
performance in the crosslingual setting. It induces
a linking between syntax and semantics, encoded as
a clustering of syntactic signatures of predicate ar-
guments. The clustering implicitly defines the set of
permissible alternations. For predicates present in
both sides of a bitext, we guide models in both lan-
guages to prefer clusterings which maximize agree-
ment between predicate argument structures pre-
dicted for each aligned predicate pair. We experi-
mentally show the effectiveness of the crosslingual
learning on the English-German language pair.
Our model admits efficient inference: the estima-
tion time on CoNLL 2009 data (Hajic? et al, 2009)
and Europarl v.6 bitext (Koehn, 2005) does not ex-
ceed 5 hours on a single processor and the infer-
ence algorithm is highly parallelizable, reducing in-
ference time down to less than half an hour on mul-
tiple processors. This suggests that the models scale
to much larger corpora, which is an important prop-
erty for a successful unsupervised learning method,
as unlabeled data is abundant.
In summary, our contributions are as follows.
? This work is the first to consider the crosslin-
gual setting for unsupervised SRL.
? We propose a form of agreement penalty and
show its efficacy on English-German language
pair when used in conjunction with a state-of-
the-art non-parametric Bayesian model.
? We demonstrate that efficient approximate in-
ference is feasible in the multilingual setting.
The rest of the paper is organized as follows. Sec-
tion 2 begins with a definition of the crosslingual
semantic role induction task we address in this pa-
per. In Section 3, we describe the base monolingual
model, and in Section 4 we propose an extension for
the crosslingual setting. In Section 5, we describe
our inference procedure. Section 6 provides both
evaluation and analysis. Finally, additional related
work is presented in Section 7.
2 Problem Definition
As we mentioned in the introduction, in this work
we focus on the labeling stage of semantic role la-
beling. Identification, though an important prob-
lem, can be tackled with heuristics (Lang and Lap-
ata, 2011a; Grenager and Manning, 2006; de Marn-
effe et al, 2006) or potentially by using a supervised
classifier trained on a small amount of data.
Instead of assuming the availability of role an-
notated data, we rely only on automatically gener-
ated syntactic dependency graphs in both languages.
While we cannot expect that syntactic structure can
trivially map to a semantic representation1, we can
make use of syntactic cues. In the labeling stage,
semantic roles are represented by clusters of ar-
guments, and labeling a particular argument corre-
sponds to deciding on its role cluster. However, in-
stead of dealing with argument occurrences directly,
1Although it provides a strong baseline which is difficult to
beat (Grenager and Manning, 2006; Lang and Lapata, 2010;
Lang and Lapata, 2011a).
648
we represent them as predicate-specific syntactic
signatures, and refer to them as argument keys. This
representation aids our models in inducing high pu-
rity clusters (of argument keys) while reducing their
granularity. We follow (Lang and Lapata, 2011a)
and use the following syntactic features for English
to form the argument key representation:
? Active or passive verb voice (ACT/PASS).
? Arg. position relative to predicate (LEFT/RIGHT).
? Syntactic relation to its governor.
? Preposition used for argument realization.
In the example sentences in Section 1, the argu-
ment keys for candidate arguments Peter for sen-
tences (a) and (c) would be ACT:LEFT:SBJ and
PASS:RIGHT:LGS->by,2 respectively. While aim-
ing to increase the purity of argument key clusters,
this particular representation will not always pro-
duce a good match: e.g. planning a theft in sen-
tence (b) will have the same key as Mary in sen-
tence (a). Increasing the expressiveness of the ar-
gument key representation by using features of the
syntactic frame would enable us to distinguish that
pair of arguments. However, we keep this particular
representation, in part to compare with the previous
work. In German, we do not include the relative po-
sition features, because they are not very informative
due to variability in word order.
In sum, we treat the unsupervised semantic role
labeling task as clustering of argument keys. Thus,
argument occurrences in the corpus whose keys are
clustered together are assigned the same semantic
role. The objective of this work is to improve ar-
gument key clusterings by inducing them simulta-
neously in two languages.
3 Monolingual Model
In this section we describe one of the Bayesian mod-
els for semantic role induction proposed in (Titov
and Klementiev, 2012). Before describing our
method, we briefly introduce the central compo-
nents of the model: the Chinese Restaurant Pro-
cesses (CRPs) and Dirichlet Processes (DPs) (Fer-
guson, 1973; Pitman, 2002). For more details we
refer the reader to (Teh, 2007).
2LGS denotes a logical subject in a passive construction
(Surdeanu et al, 2008).
3.1 Chinese Restaurant Processes
CRPs define probability distributions over partitions
of a set of objects. An intuitive metaphor for de-
scribing CRPs is assignment of tables to restaurant
customers. Assume a restaurant with a sequence of
tables, and customers who walk into the restaurant
one at a time and choose a table to join. The first
customer to enter is assigned the first table. Sup-
pose that when a client number i enters the restau-
rant, i ? 1 customers are sitting at each of the k ?
(1, . . . ,K) tables occupied so far. The new cus-
tomer is then either seated at one of the K tables
with probability Nki?1+? , where Nk is the number of
customers already sitting at table k, or assigned to a
new table with the probability ?i?1+? , ? > 0.
If we continue and assume that for each table ev-
ery customer at a table orders the same meal, with
the meal for the table chosen from an arbitrary base
distributionH , then all ordered meals will constitute
a sample from the Dirichlet Process DP (?,H).
An important property of the non-parametric pro-
cesses is that a model designer does not need to spec-
ify the number of tables (i.e. clusters) a-priori as it
is induced automatically on the basis of the data and
also depending on the choice of the concentration
parameter ?. This property is crucial for our task,
as the intended number of roles cannot possibly be
specified for every predicate.
3.2 The Generative Story
In Section 2 we defined our task as clustering of ar-
gument keys, where each cluster corresponds to a
semantic role. If an argument key k is assigned to a
role r (k ? r), all of its occurrences are labeled r.
The Bayesian model encodes two common as-
sumptions about semantic roles. First, it enforces the
selectional restriction assumption: namely it stip-
ulates that the distribution over potential argument
fillers is sparse for every role, implying that ?peaky?
distributions of arguments for each role r are pre-
ferred to flat distributions. Second, each role nor-
mally appears at most once per predicate occur-
rence. The inference algorithm will search for a
clustering which meets the above requirements to
the maximal extent.
The model associates two distributions with each
predicate: one governs the selection of argument
649
fillers for each semantic role, and the other mod-
els (and penalizes) duplicate occurrence of roles.
Each predicate occurrence is generated indepen-
dently given these distributions. Let us describe the
model by first defining how the set of model param-
eters and an argument key clustering are drawn, and
then explaining the generation of individual predi-
cate and argument instances. The generative story is
formally presented in Figure 1.
For each predicate p, we start by generating a par-
tition of argument keys Bp with each subset r ?
Bp representing a single semantic role. The parti-
tions are drawn from CRP(?) independently for each
predicate. The crucial part of the model is the set of
selectional preference parameters ?p,r, the distribu-
tions of arguments x for each role r of predicate p.
We represent arguments by lemmas of their syntac-
tic heads.3
The preference for sparseness of the distributions
?p,r is encoded by drawing them from the DP prior
DP (?,H(A)) with a small concentration parameter
?, the base probability distribution H(A) is just the
normalized frequencies of arguments in the corpus.
The geometric distribution ?p,r is used to model the
number of times a role r appears with a given predi-
cate occurrence. The decision whether to generate at
least one role r is drawn from the uniform Bernoulli
distribution. If 0 is drawn then the semantic role is
not realized for the given occurrence, otherwise the
number of additional roles r is drawn from the ge-
ometric distribution Geom(?p,r). The Beta priors
over ? can indicate the preference towards generat-
ing at most one argument for each role.
Now, when parameters and argument key clus-
terings are chosen, we can summarize the remain-
der of the generative story as follows. We begin by
independently drawing occurrences for each predi-
cate. For each predicate role we independently de-
cide on the number of role occurrences. Then each
of the arguments is generated (see GenArgument)
by choosing an argument key kp,r uniformly from
the set of argument keys assigned to the cluster r,
and finally choosing its filler xp,r, where the filler is
the lemma of the syntactic head of the argument.
3For prepositional phrases, the head noun of the object noun
phrase is taken as it encodes crucial lexical information. How-
ever, the preposition is not ignored but rather encoded in the
corresponding argument key, as explained in Section 2.
Clustering of argument keys:
for each predicate p = 1, 2, . . . :
Bp ? CRP (?) [partition of arg keys]
Parameters:
for each predicate p = 1, 2, . . . :
for each role r ? Bp:
?p,r ? DP (?,H(A)) [distrib of arg fillers]
?p,r ? Beta(?0, ?1) [geom distr for dup roles]
Data generation:
for each predicate p = 1, 2, . . . :
for each occurrence s of p:
for every role r ? Bp:
if [n ? Unif(0, 1)] = 1: [role appears at least once]
GenArgument(p, r) [draw one arg]
while [n ? ?p,r] = 1: [continue generation]
GenArgument(p, r) [draw more args]
GenArgument(p, r):
kp,r ? Unif(1, . . . , |r|) [draw arg key]
xp,r ? ?p,r [draw arg filler]
Figure 1: The generative story for predicate-argument
structure.
4 Multilingual Extension
As we argued in Section 1, our goal is to penalize
for disagreement in semantic structures predicted for
each language on parallel data. In doing so, as in
much of previous work on unsupervised induction of
linguistic structures, we rely on automatically pro-
duced word alignments. In Section 6, we describe
how we use word alignment to decide if two argu-
ments are aligned; for now, we assume that (noisy)
argument alignments are given.
Intuitively, when two arguments are aligned in
parallel data, we expect them to be labeled with the
same semantic role in both languages. This corre-
spondence is simpler than the one expected in mul-
tilingual induction of syntax and morphology where
systematic but unknown relation between structures
in two language is normally assumed (e.g., (Snyder
et al, 2008)). A straightforward implementation of
this idea would require us to maintain one-to-one
mapping between semantic roles across languages.
Instead of assuming this correspondence, we penal-
ize for the lack of isomorphism between the sets of
roles in aligned predicates with the penalty depen-
dent on the degree of violation. This softer approach
650
is more appropriate in our setting, as individual ar-
gument keys do not always deterministically map to
gold standard roles4 and strict penalization would
result in the propagation of the corresponding over-
coarse clusters to the other language. Empirically,
we observed this phenomenon on the held-out set
with the increase of the penalty weight.
Encoding preference for the isomorphism directly
in the generative story is problematic: sparse Dirich-
let priors can be used in a fairly trivial way to encode
sparsity of the mapping in one direction or another
but not in both. Instead, we formalize this preference
with a penalty term similar to the expectation criteria
in KL-divergence form introduced in McCallum et
al. (2007). Specifically, we augment the joint proba-
bility with a penalty term computed on parallel data:
?
p(1), p(2)
(
? ?(1)
?
r(1)?B
p(1)
fr(1) argmax
r(2)?B
p(2)
log P? (r(2)|r(1))
??(2)
?
r(2)?B
p(2)
fr(2) argmax
r(1)?B
p(1)
log P? (r(1)|r(2))
)
,
where P? (r(l)|r(l
?)) is the proportion of times the role
r(l
?) of predicate p(l
?) in language l? is aligned to the
role r(l) of predicate p(l) in language l, and fr(l) is
the total number of times the role is aligned, ?(l) is a
non-negative constant. The rationale for introducing
the individual weighting fr(l) is two-fold. First, the
proportions P? (r(l)|r(l
?)) are more ?reliable? when
computed from larger counts. Second, more fre-
quent roles should have higher penalty as they com-
pete with the joint probability term, the likelihood
part of which scales linearly with role counts.
Space restrictions prevent us from discussing the
close relation between this penalty formulation and
the existing work on injecting prior and side infor-
mation in learning objectives in the form of con-
straints (McCallum et al, 2007; Ganchev et al,
2010; Chang et al, 2007).
In order to support efficient and parallelizable in-
ference, we simplify the above penalty by consider-
ing only disjoint pairs of predicates, instead of sum-
ming over all pairs p(1) and p(2). When choosing
4The average purity for argument keys with automatic argu-
ment identification and using predicted syntactic trees, before
any clustering, is approximately 90.2% on English and 87.8%
on German.
the pairs, we aim to cover the maximal number of
alignment counts so as to preserve as much informa-
tion from parallel corpora as possible. This objective
corresponds to the classic maximum weighted bipar-
tite matching problem with the weight for each edge
p(1) and p(2) equal to the number of times the two
predicates were aligned in parallel data. We use the
standard polynomial algorithm (the Hungarian algo-
rithm, (Kuhn, 1955)) to find an optimal solution.
5 Inference
An inference algorithm for an unsupervised model
should be efficient enough to handle vast amounts
of unlabeled data, as it can easily be obtained and is
likely to improve results. We use a simple approx-
imate inference algorithm based on greedy search.
We start by discussing search for the maximum a-
posteriori clustering of argument keys in the mono-
lingual set-up and then discuss how it can be ex-
tended to accommodate the role alignment penalty.
5.1 Monolingual Setting
In the model, a linking between syntax and seman-
tics is induced independently for each predicate.
Nevertheless, searching for a MAP clustering can
be expensive: even a move involving a single ar-
gument key implies some computations for all its
occurrences in the corpus. Instead of more com-
plex MAP search algorithms (see, e.g., (Daume III,
2007)), we use a greedy procedure where we start
with each argument key assigned to an individual
cluster, and then iteratively try to merge clusters.
Each move involves (1) choosing an argument key
and (2) deciding on a cluster to reassign it to. This is
done by considering all clusters (including creating
a new one) and choosing the most probable one.
Instead of choosing argument keys randomly at
the first stage, we order them by corpus frequency.
This ordering is beneficial as getting clustering right
for frequent argument keys is more important and
the corresponding decisions should be made earlier.5
We used a single iteration in our experiments, as we
have not noticed any benefit from using multiple it-
erations.
5This has been explored before for shallow semantic rep-
resentations (Lang and Lapata, 2011a; Titov and Klementiev,
2011).
651
5.2 Incorporating the Alignment Penalty
Inference in the monolingual setting is done inde-
pendently for each predicate, as the model factor-
izes over the predicates. The role alignment penalty
introduces interdependencies between the objectives
for each bilingual predicate pair chosen by the as-
signment algorithm as discussed in Section 4. For
each pair of predicates, we search for clusterings
to maximize the sum of the log-probability and the
negated penalty term.
At first glance it may seem that the alignment
penalty can be easily integrated into the greedy MAP
search algorithm: instead of considering individual
argument keys, one could use pairs of argument keys
and decide on their assignment to clusters jointly.
However, given that there is no isomorphic mapping
between argument keys across languages, this solu-
tion is unlikely to be satisfactory.6 Instead, we use
an approximate inference procedure similar in spirit
to annotation projection techniques.
For each predicate, we first induce semantic roles
independently for the first language, as described
in Section 5.1, and then use the same algorithm for
the second language but take the penalty term into
account. Then we repeat the process in the reverse
direction. Among these two solutions, we choose
the one which yields the higher objective value. In
this way, we begin with producing a clustering for
the side which is easier to cluster and provides more
clues for the other side.7
6 Empirical Evaluation
We begin by describing the data and evaluation met-
rics we use before discussing results.
6.1 Data
We run our main experiments on the English-
German section of Europarl v6 parallel corpus
6We also considered a variation of this idea where a pair of
argument keys is chosen randomly proportional to their align-
ment frequency and multiple iterations are repeated. Despite
being significantly slower than our method, it did not provide
any improvement in accuracy.
7In preliminary experiments, we studied an even simpler in-
ference method where the projection direction was fixed for all
predicates. Though this approach did outperform the monolin-
gual model, the results were substantially worse than achieved
with our method.
(Koehn, 2005) and the CoNLL 2009 distributions
of the Penn Treebank WSJ corpus (Marcus et al,
1993) for English and the SALSA corpus (Burchardt
et al, 2006) for German. As standard for unsuper-
vised SRL, we use the entire CoNLL training sets
for evaluation, and use held-out sets for model se-
lection and parameter tuning.
Syntactic annotation. Although the CoNLL 2009
dataset aleady has predicted dependency structures,
we could not reproduce them so that we could use
the same parser to annotate Europarl. We chose to
reannotate it, since using different parsing models
for both datasets would be undesirable. We used
MaltParser (Nivre et al, 2007) for English and the
syntactic component of the LTH system (Johansson
and Nugues, 2008) for German.
Predicate and argument identification. We select all
non-auxiliary verbs as predicates. For English, we
identify their arguments using a heuristic proposed
in (Lang and Lapata, 2011a). It is comprised of a
list of 8 rules, which use nonlexicalized properties
of syntactic paths between a predicate and a candi-
date argument to iteratively discard non-arguments
from the list of all words in a sentence. For Ger-
man, we use the LTH argument identification classi-
fier. Accuracy of argument identification on CoNLL
2009 using predicted syntactic analyses was 80.7%
and 86.5% for English and German, respectively.
Argument alignment. We use GIZA++ (Och and
Ney, 2003) to produce word alignments in Europarl:
we ran it in both directions and kept the intersec-
tion of the induced word alignments. For every ar-
gument identified in the previous stage, we chose a
set of words consisting of the argument?s syntactic
head and, for prepositional phrases, the head noun
of the object noun phrase. We mark arguments in
two languages as aligned if there is any word align-
ment between the corresponding sets and if they are
arguments of aligned predicates.
6.2 Evaluation Metrics
We use the standard purity (PU) and collocation
(CO) metrics as well as their harmonic mean (F1) to
measure the quality of the resulting clusters. Purity
measures the degree to which each cluster contains
arguments sharing the same gold role:
652
PU =
1
N
?
i
max
j
|Gj ? Ci|
where Ci is the set of arguments in the i-th induced
cluster, Gj is the set of arguments in the jth gold
cluster, and N is the total number of arguments.
Collocation evaluates the degree to which arguments
with the same gold roles are assigned to a single
cluster:
CO =
1
N
?
j
max
i
|Gj ? Ci|
We compute the aggregate PU, CO, and F1 scores
over all predicates in the same way as (Lang and La-
pata, 2011a) by weighting the scores of each pred-
icate by the number of its argument occurrences.
Since our goal is to evaluate the clustering algo-
rithms, we do not include incorrectly identified ar-
guments when computing these metrics.
6.3 Parameters and Set-up
Our models are robust to parameter settings; the pa-
rameters were tuned (to an order of magnitude) to
optimize the F1 score on the held-out development
set and were as follows. Parameters governing du-
plicate role generation, ?(?)0 and ?
(?)
1 , and penalty
weights ?(?) were set to be the same for both lan-
guages, and are 100, 1.e-3 and 10, respectively. The
concentration parameters were set as follows: for
English, they were set to ?(1) = 1.e-3, ?(1) = 1.e-3,
and, for German, they were ?(2) = 0.1, ?(2) = 1.
Domains of Europarl (parliamentary proceedings)
and German/English CoNLL data (newswire) are
substantially different. Since the influence of do-
main shift is not the focus of work, we try to min-
imize its effect by computing the likelihood part of
the objective on CoNLL data alone. This also makes
our setting more comparable to prior work.8
6.4 Results
Base monolingual model. We begin by evaluat-
ing our base monolingual model MonoBayes alone
against the current best approaches to unsupervised
semantic role induction. Since we do not have ac-
cess to the systems, we compare on the marginally
different English CoNLL 2008 (Surdeanu et al,
8Preliminary experiments on the entire dataset show a slight
degradation in performance.
PU CO F1
LLogistic 79.5 76.5 78.0
GraphPart 88.6 70.7 78.6
SplitMerge 88.7 73.0 80.1
MonoBayes 88.1 77.1 82.2
SyntF 81.6 77.5 79.5
Table 1: Argument clustering performance with gold
argument identification and gold syntactic parses on
CoNLL 2008 shared-task dataset. Bold-face is used to
highlight the best F1 scores.
2008) shared task dataset used in their experiments.
We report the results using gold argument identifi-
cation and gold syntactic parses in order to focus
the evaluation on the argument labeling stage and to
minimize the noise due to automatic syntactic anno-
tations. The methods are Latent Logistic classifica-
tion (Lang and Lapata, 2010), Split-Merge cluster-
ing (Lang and Lapata, 2011a), and Graph Partition-
ing (Lang and Lapata, 2011b) (labeled LLogistic,
SplitMerge, and GraphPart, respectively) achieving
the current best unsupervised SRL results in this set-
ting. Additionally, we compute the syntactic func-
tion baseline (SyntF), which simply clusters predi-
cate arguments according to the dependency relation
to their head. Following (Lang and Lapata, 2010),
we allocate a cluster for each of 20 most frequent
relations in the CoNLL dataset and one cluster for
all other relations. Our model substantially outper-
forms other models (see Table 1).
Multilingual extensions. Next, we improve our
model performance using agreement as an addi-
tional supervision signal during training (see Sec-
tion 4). We compare the performance of indi-
vidual English and German models induced sepa-
rately (MonoBayes) with the jointly induced mod-
els (MultiBayes) as well as the syntactic baseline,
see Table 2.9 While we see little improvement
in F1 for English, the German system improves
by 1.8%. For German, the crosslingual learning
also results in 1.5% improvement over the syntac-
tic baseline, which is considered difficult to outper-
form (Grenager and Manning, 2006; Lang and Lap-
ata, 2010). Note that recent unsupervised SRL meth-
9Note that the scores are computed on correctly identified ar-
guments only, and tend to be higher in these experiments prob-
ably because the complex arguments get discarded by the argu-
ment identifier.
653
English German
PU CO F1 PU CO F1
MonoBayes 87.5 80.1 83.6 86.8 75.7 80.9
MultiBayes 86.8 80.7 83.7 85.0 80.6 82.7
SyntF 81.5 79.4 80.4 83.1 79.3 81.2
Table 2: Results on CoNLL 2009 with automatic argu-
ment identification and automatic syntactic parses.
ods do not always improve on it, see Table 1.
The relatively low expressivity and limited purity
of our argument keys (see discussion in Section 4)
are likely to limit potential improvements when us-
ing them in crosslingual learning. The natural next
step would be to consider crosslingual learning with
a more expressive model of the syntactic frame and
syntax-semantics linking.
7 Related Work
Unsupervised learning in crosslingual setting has
been an active area of research in recent years. How-
ever, most of this research has focused on induc-
tion of syntactic structures (Kuhn, 2004; Snyder
et al, 2009) or morphologic analysis (Snyder and
Barzilay, 2008) and we are not aware of any pre-
vious work on induction of semantic representa-
tions in the crosslingual setting. Learning of se-
mantic representations in the context of monolin-
gual weakly-parallel data was studied in Titov and
Kozhevnikov (2010) but their setting was semi-
supervised and they experimented only on a re-
stricted domain.
Most of the SRL research has focused on the
supervised setting, however, lack of annotated re-
sources for most languages and insufficient cover-
age provided by the existing resources motivates
the need for using unlabeled data or other forms
of weak supervision. This includes methods based
on graph alignment between labeled and unlabeled
data (Fu?rstenau and Lapata, 2009), using unlabeled
data to improve lexical generalization (Deschacht
and Moens, 2009), and projection of annotation
across languages (Pado and Lapata, 2009; van der
Plas et al, 2011). Semi-supervised and weakly-
supervised techniques have also been explored for
other types of semantic representations but these
studies again have mostly focused on restricted do-
mains (Kate and Mooney, 2007; Liang et al, 2009;
Goldwasser et al, 2011; Liang et al, 2011).
Early unsupervised approaches to the SRL task
include (Swier and Stevenson, 2004), where the
VerbNet verb lexicon was used to guide unsuper-
vised learning, and a generative model of Grenager
and Manning (2006) which exploits linguistic priors
on syntactic-semantic interface.
More recently, the role induction problem has
been studied in Lang and Lapata (2010) where it
has been reformulated as a problem of detecting al-
ternations and mapping non-standard linkings to the
canonical ones. Later, Lang and Lapata (2011a) pro-
posed an algorithmic approach to clustering argu-
ment signatures which achieves higher accuracy and
outperforms the syntactic baseline. In Lang and La-
pata (2011b), the role induction problem is formu-
lated as a graph partitioning problem: each vertex in
the graph corresponds to a predicate occurrence and
edges represent lexical and syntactic similarities be-
tween the occurrences. Unsupervised induction of
semantics has also been studied in Poon and Domin-
gos (2009) and Titov and Klementiev (2011) but the
induced representations are not entirely compatible
with the PropBank-style annotations and they have
been evaluated only on a question answering task
for the biomedical domain. Also, a related task of
unsupervised argument identification has been con-
sidered in Abend et al (2009).
8 Conclusions
This work adds unsupervised semantic role labeling
to the list of NLP tasks benefiting from the crosslin-
gual induction setting. We show that an agreement
signal extracted from parallel data provides indi-
rect supervision capable of substantially improving
a state-of-the-art model for semantic role induction.
Although in this work we focused primarily on
improving performance for each individual lan-
guage, cross-lingual semantic representation could
be extracted by a simple post-processing step. In
future work, we would like to model cross-lingual
semantics explicitly.
Acknowledgements
The work was supported by the MMCI Cluster of Excel-
lence and a Google research award. The authors thank
Mikhail Kozhevnikov, Alexis Palmer, Manfred Pinkal,
Caroline Sporleder and the anonymous reviewers for their
suggestions.
654
References
Omri Abend, Roi Reichart, and Ari Rappoport. 2009.
Unsupervised argument identification for semantic
role labeling. In ACL-IJCNLP.
Roberto Basili, Diego De Cao, Danilo Croce, Bonaven-
tura Coppola, and Alessandro Moschitti. 2009. Cross-
language frame semantics transfer in bilingual cor-
pora. In CICLING.
A. Burchardt, K. Erk, A. Frank, A. Kowalski, S. Pado,
and M. Pinkal. 2006. The SALSA corpus: a german
corpus resource for lexical semantics. In LREC.
Ming-Wei Chang, Lev Ratinov, and Dan Roth.
2007. Guiding semi-supervision with constraint-
driven learning. In ACL.
Hal Daume III. 2007. Fast search for dirichlet process
mixture models. In AISTATS.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
LREC 2006.
Koen Deschacht and Marie-Francine Moens. 2009.
Semi-supervised semantic role labeling using the La-
tent Words Language Model. In EMNLP.
Thomas S. Ferguson. 1973. A Bayesian analysis of
some nonparametric problems. The Annals of Statis-
tics, 1(2):209?230.
Hagen Fu?rstenau and Mirella Lapata. 2009. Graph align-
ment for semi-supervised semantic role labeling. In
EMNLP.
Kuzman Ganchev, Joao Graca, Jennifer Gillenwater, and
Ben Taskar. 2010. Posterior regularization for struc-
tured latent variable models. Journal of Machine
Learning Research (JMLR), 11:2001?2049.
Qin Gao and Stephan Vogel. 2011. Corpus expansion for
statistical machine translation with semantic role label
substitution rules. In ACL:HLT.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic la-
belling of semantic roles. Computational Linguistics,
28(3):245?288.
Dan Goldwasser, Roi Reichart, James Clarke, and Dan
Roth. 2011. Confidence driven unsupervised semantic
parsing. In ACL.
Trond Grenager and Christoph Manning. 2006. Un-
supervised discovery of a statistical verb lexicon. In
EMNLP.
Jan Hajic?, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Anto`nia Mart??, Llu??s
Ma`rquez, Adam Meyers, Joakim Nivre, Sebastian
Pado?, Jan S?te?pa?nek, Pavel Stran?a?k, Mihai Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The conll-2009
shared task: Syntactic and semantic dependencies in
multiple languages. In CoNLL 2009: Shared Task.
Richard Johansson and Pierre Nugues. 2008.
Dependency-based semantic role labeling of Prop-
Bank. In EMNLP.
Michael Kaisser and Bonnie Webber. 2007. Question
answering based on semantic roles. In ACL Workshop
on Deep Linguistic Processing.
Rohit J. Kate and Raymond J. Mooney. 2007. Learning
language semantics from ambigous supervision. In
AAAI.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In Proceedings of the
MT Summit.
Harold W. Kuhn. 1955. The hungarian method for the
assignment problem. Naval Research Logistics Quar-
terly, 2:83?97.
Jonas Kuhn. 2004. Experiments in parallel-text based
grammar induction. In ACL.
Joel Lang and Mirella Lapata. 2010. Unsupervised in-
duction of semantic roles. In ACL.
Joel Lang and Mirella Lapata. 2011a. Unsupervised se-
mantic role induction via split-merge clustering. In
ACL.
Joel Lang and Mirella Lapata. 2011b. Unsupervised
semantic role induction with graph partitioning. In
EMNLP.
Beth Levin. 1993. English Verb Classes and Alter-
nations: A Preliminary Investigation. University of
Chicago Press.
Percy Liang, Michael I. Jordan, and Dan Klein. 2009.
Learning semantic correspondences with less supervi-
sion. In ACL-IJCNLP.
Percy Liang, Michael Jordan, and Dan Klein. 2011.
Learning dependency-based compositional semantics.
In ACL: HLT.
Ding Liu and Daniel Gildea. 2010. Semantic role fea-
tures for machine translation. In Coling.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
Linguistics, 19(2):313?330.
Andrew McCallum, Gideon Mann, and Gregory Druck.
2007. Generalized expectation criteria. Techni-
cal Report TR 2007-60, University of Massachusetts,
Amherst, MA.
Ryan McDonald, Slav Petrov, and Keith Hall. 2011.
Multi-source transfer of delexicalized dependency
parsers. In EMNLP.
J. Nivre, J. Hall, S. Ku?bler, R. McDonald, J. Nils-
son, S. Riedel, and D. Yuret. 2007. The CoNLL
2007 shared task on dependency parsing. In EMNLP-
CoNLL.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29:19?51.
655
Sebastian Pado and Mirella Lapata. 2009. Cross-lingual
annotation projection for semantic roles. Journal of
Artificial Intelligence Research, 36:307?340.
Jim Pitman. 2002. Poisson-Dirichlet and GEM invari-
ant distributions for split-and-merge transformations
of an interval partition. Combinatorics, Probability
and Computing, 11:501?514.
Hoifung Poon and Pedro Domingos. 2009. Unsuper-
vised semantic parsing. In EMNLP.
Sameer Pradhan, Wayne Ward, and James H. Martin.
2008. Towards robust semantic role labeling. Com-
putational Linguistics, 34:289?310.
M. Sammons, V. Vydiswaran, T. Vieira, N. Johri,
M. Chang, D. Goldwasser, V. Srikumar, G. Kundu,
Y. Tu, K. Small, J. Rule, Q. Do, and D. Roth. 2009.
Relation alignment for textual entailment recognition.
In Text Analysis Conference (TAC).
Dan Shen and Mirella Lapata. 2007. Using semantic
roles to improve question answering. In EMNLP.
Benjamin Snyder and Regina Barzilay. 2008. Unsuper-
vised multilingual learning for morphological segmen-
tation. In ACL.
Benjamin Snyder and Regina Barzilay. 2010. Climbing
the tower of Babel: Unsupervised multilingual learn-
ing. In ICML.
Benjamin Snyder, Tahira Naseem, Jacob Eisenstein, and
Regina Barzilay. 2008. Unsupervised multilingual
learning for POS tagging. In EMNLP.
Benjamin Snyder, Tahira Naseem, and Regina Barzilay.
2009. Unsupervised multilingual grammar induction.
In ACL.
Mihai Surdeanu, Adam Meyers Richard Johansson, Llu??s
Ma`rquez, and Joakim Nivre. 2008. The CoNLL-2008
shared task on joint parsing of syntactic and semantic
dependencies. In CoNLL 2008: Shared Task.
Richard Swier and Suzanne Stevenson. 2004. Unsuper-
vised semantic role labelling. In EMNLP.
Yee Whye Teh. 2007. Dirichlet process. Encyclopedia
of Machine Learning.
Ivan Titov and Alexandre Klementiev. 2011. A Bayesian
model for unsupervised semantic parsing. In ACL.
Ivan Titov and Alexandre Klementiev. 2012. A Bayesian
approach to unsupervised semantic role induction. In
EACL.
Ivan Titov and Mikhail Kozhevnikov. 2010. Bootstrap-
ping semantic analyzers from non-contradictory texts.
In ACL.
Lonneke van der Plas, Paola Merlo, and James Hender-
son. 2011. Scaling up automatic cross-lingual seman-
tic role annotation. In ACL.
Dekai Wu and Pascale Fung. 2009. Semantic roles for
SMT: A hybrid two-pass model. In NAACL.
Dekai Wu, Marianna Apidianaki, Marine Carpuat, and
Lucia Specia, editors. 2011. Proc. of Fifth Work-
shop on Syntax, Semantics and Structure in Statistical
Translation. ACL.
656
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1190?1200,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Cross-lingual Transfer of Semantic Role Labeling Models
Mikhail Kozhevnikov and Ivan Titov
Saarland University, Postfach 15 11 50
66041 Saarbru?cken, Germany
{mkozhevn|titov}@mmci.uni-saarland.de
Abstract
Semantic Role Labeling (SRL) has be-
come one of the standard tasks of natural
language processing and proven useful as
a source of information for a number of
other applications. We address the prob-
lem of transferring an SRL model from
one language to another using a shared
feature representation. This approach is
then evaluated on three language pairs,
demonstrating competitive performance as
compared to a state-of-the-art unsuper-
vised SRL system and a cross-lingual an-
notation projection baseline. We also con-
sider the contribution of different aspects
of the feature representation to the perfor-
mance of the model and discuss practical
applicability of this method.
1 Background and Motivation
Semantic role labeling has proven useful in many
natural language processing tasks, such as ques-
tion answering (Shen and Lapata, 2007; Kaisser
and Webber, 2007), textual entailment (Sammons
et al, 2009), machine translation (Wu and Fung,
2009; Liu and Gildea, 2010; Gao and Vogel, 2011)
and dialogue systems (Basili et al, 2009; van der
Plas et al, 2009).
Multiple models have been designed to auto-
matically predict semantic roles, and a consider-
able amount of data has been annotated to train
these models, if only for a few more popular lan-
guages. As the annotation is costly, one would like
to leverage existing resources to minimize the hu-
man effort required to construct a model for a new
language.
A number of approaches to the construction of
semantic role labeling models for new languages
have been proposed. On one end of the scale is
unsupervised SRL, such as Grenager and Manning
(2006), which requires some expert knowledge,
but no labeled data. It clusters together arguments
that should bear the same semantic role, but does
not assign a particular role to each cluster. On the
other end is annotating a new dataset from scratch.
There are also intermediate options, which often
make use of similarities between languages. This
way, if an accurate model exists for one language,
it should help simplify the construction of a model
for another, related language.
The approaches in this third group often use par-
allel data to bridge the gap between languages.
Cross-lingual annotation projection systems (Pado?
and Lapata, 2009), for example, propagate infor-
mation directly via word alignment links. How-
ever, they are very sensitive to the quality of par-
allel data, as well as the accuracy of a source-
language model on it.
An alternative approach, known as cross-lingual
model transfer, or cross-lingual model adaptation,
consists of modifying a source-language model to
make it directly applicable to a new language. This
usually involves constructing a shared feature rep-
resentation across the two languages. McDon-
ald et al (2011) successfully apply this idea to
the transfer of dependency parsers, using part-of-
speech tags as the shared representation of words.
A later extension of Ta?ckstro?m et al (2012) en-
riches this representation with cross-lingual word
clusters, considerably improving the performance.
In the case of SRL, a shared representation that
is purely syntactic is likely to be insufficient, since
structures with different semantics may be realized
by the same syntactic construct, for example ?in
August? vs ?in Britain?. However with the help of
recently introduced cross-lingual word represen-
1190
tations, such as the cross-lingual clustering men-
tioned above or cross-lingual distributed word rep-
resentations of Klementiev et al (2012), we may
be able to transfer models of shallow semantics in
a similar fashion.
In this work we construct a shared feature repre-
sentation for a pair of languages, employing cross-
lingual representations of syntactic and lexical in-
formation, train a semantic role labeling model on
one language and apply it to the other one. This
approach yields an SRL model for a new language
at a very low cost, effectively requiring only a
source language model and parallel data.
We evaluate on five (directed) language pairs ?
EN-ZH, ZH-EN, EN-CZ, CZ-EN and EN-FR, where
EN, FR, CZ and ZH denote English, French, Czech
and Chinese, respectively. The transferred model
is compared against two baselines: an unsuper-
vised SRL system and a model trained on the out-
put of a cross-lingual annotation projection sys-
tem.
In the next section we will describe our setup,
then in section 3 present the shared feature repre-
sentation we use, discuss the evaluation data and
other technical aspects in section 4, present the
results and conclude with an overview of related
work.
2 Setup
The purpose of the study is not to develop a yet
another semantic role labeling system ? any exist-
ing SRL system can (after some modification) be
used in this setup ? but to assess the practical ap-
plicability of cross-lingual model transfer to this
problem, compare it against the alternatives and
identify its strong/weak points depending on a par-
ticular setup.
2.1 Semantic Role Labeling Model
We consider the dependency-based version of se-
mantic role labeling as described in Hajic? et al
(2009) and transfer an SRL model from one lan-
guage to another. We only consider verbal pred-
icates and ignore the predicate disambiguation
stage. We also assume that the predicate identifi-
cation information is available ? in most languages
it can be obtained using a relatively simple heuris-
tic based on part-of-speech tags.
The model performs argument identification
and classification (Johansson and Nugues, 2008)
separately in a pipeline ? first each candidate is
classified as being or not being a head of an argu-
ment phrase with respect to the predicate in ques-
tion and then each of the arguments is assigned a
role from a given inventory. The model is factor-
ized over arguments ? the decisions regarding the
classification of different arguments are made in-
dependently of each other.
With respect to the use of syntactic annotation
we consider two options: using an existing depen-
dency parser for the target language and obtaining
one by means of cross-lingual transfer (see sec-
tion 4.2).
Following McDonald et al (2011), we assume
that a part-of-speech tagger is available for the tar-
get language.
2.2 SRL in the Low-resource Setting
Several approaches have been proposed to obtain
an SRL model for a new language with little or
no manual annotation. Unsupervised SRL mod-
els (Lang and Lapata, 2010) cluster the arguments
of predicates in a given corpus according to their
semantic roles. The performance of such models
can be impressive, especially for those languages
where semantic roles correlate strongly with syn-
tactic relation of the argument to its predicate.
However, assigning meaningful role labels to the
resulting clusters requires additional effort and the
model?s parameters generally need some adjust-
ment for every language.
If the necessary resources are already available
for a closely related language, they can be uti-
lized to facilitate the construction of a model for
the target language. This can be achieved ei-
ther by means of cross-lingual annotation projec-
tion (Yarowsky et al, 2001) or by cross-lingual
model transfer (Zeman and Resnik, 2008).
This last approach is the one we are considering
in this work, and the other two options are treated
as baselines. The unsupervised model will be fur-
ther referred to as UNSUP and the projection base-
line as PROJ.
2.3 Evaluation Measures
We use the F1 measure as a metric for the argu-
ment identification stage and accuracy as an ag-
gregate measure of argument classification perfor-
mance. When comparing to the unsupervised SRL
system the clustering evaluation measures are used
instead. These are purity and collocation
1191
PU = 1N
?
i
max
j
|Gj ? Ci|
CO = 1N
?
j
max
i
|Gj ? Ci|,
where Ci is the set of arguments in the i-th induced
cluster, Gj is the set of arguments in the jth gold
cluster and N is the total number of arguments.
We report the harmonic mean of the two (Lang and
Lapata, 2011) and denote it F c1 to avoid confusing
it with the supervised metric.
3 Model Transfer
The idea of this work is to abstract the model away
from the particular source language and apply it
to a new one. This setup requires that we use the
same feature representation for both languages, for
example part-of-speech tags and dependency rela-
tion labels should be from the same inventory.
Some features are not applicable to certain lan-
guages because the corresponding phenomena are
absent in them. For example, consider a strongly
inflected language and an analytic one. While the
latter can usually convey the information encoded
in the word form in the former one (number, gen-
der, etc.), finding a shared feature representation
for such information is non-trivial. In this study
we will confine ourselves to those features that are
applicable to all languages in question, namely:
part-of-speech tags, syntactic dependency struc-
tures and representations of the word?s identity.
3.1 Lexical Information
We train a model on one language and apply it to a
different one. In order for this to work, the words
of the two languages have to be mapped into a
common feature space. It is also desirable that
closely related words from both languages have
similar representations in this space.
Word mapping. The first option is simply to use
the source language words as the shared represen-
tation. Here every source language word would
have itself as its representation and every target
word would map into a source word that corre-
sponds to it. In other words, we supply the model
with a gloss of the target sentence.
The mapping (bilingual dictionary) we use is
derived from a word-aligned parallel corpus, by
identifying, for each word in the target language,
the word in the source language it is most often
aligned to.
Cross-lingual clusters. There is no guarantee
that each of the words in the evaluation data is
present in our dictionary, nor that the correspond-
ing source-language word is present in the training
data, so the model would benefit from the ability
to generalize over closely related words. This can,
for example, be achieved by using cross-lingual
word clusters induced in Ta?ckstro?m et al (2012).
We incorporate these clusters as features into our
model.
3.2 Syntactic Information
Part-of-speech Tags. We map part-of-speech tags
into the universal tagset following Petrov et al
(2012). This may have a negative effect on the
performance of a monolingual model, since most
part-of-speech tagsets are more fine-grained than
the universal POS tags considered here. For exam-
ple Penn Treebank inventory contains 36 tags and
the universal POS tagset ? only 12. Since the finer-
grained POS tags often reflect more language-
specific phenomena, however, they would only be
useful for very closely related languages in the
cross-lingual setting.
The universal part-of-speech tags used in eval-
uation are derived from gold-standard annotation
for all languages except French, where predicted
ones had to be used instead.
Dependency Structure. Another important aspect
of syntactic information is the dependency struc-
ture. Most dependency relation inventories are
language-specific, and finding a shared representa-
tion for them is a challenging problem. One could
map dependency relations into a simplified form
that would be shared between languages, as it is
done for part-of-speech tags in Petrov et al (2012).
The extent to which this would be useful, however,
depends on the similarity of syntactic-semantic in-
terfaces of the languages in question.
In this work we discard the dependency rela-
tion labels where the inventories do not match and
only consider the unlabeled syntactic dependency
graph. Some discrepancies, such as variations in
attachment order, may be present even there, but
this does not appear to be the case with the datasets
we use for evaluation. If a target language is poor
in resources, one can obtain a dependency parser
for the target language by means of cross-lingual
model transfer (Zeman and Resnik, 2008). We
1192
take this into account and evaluate both using the
original dependency structures and the ones ob-
tained by means of cross-lingual model transfer.
3.3 The Model
The model we use is based on that of Bjo?rkelund
et al (2009). It is comprised of a set of linear clas-
sifiers trained using Liblinear (Fan et al, 2008).
The feature model was modified to accommodate
the cross-lingual cluster features and the reranker
component was not used.
We do not model the interaction between differ-
ent argument roles in the same predicate. While
this has been found useful, in the cross-lingual
setup one has to be careful with the assumptions
made. For example, modeling the sequence of
roles using a Markov chain (Thompson et al,
2003) may not work well in the present setting,
especially between distant languages, as the order
or arguments is not necessarily preserved. Most
constraints that prove useful for SRL (Chang et
al., 2007) also require customization when applied
to a new language, and some rely on language-
specific resources, such as a valency lexicon. Tak-
ing into account the interaction between different
arguments of a predicate is likely to improve the
performance of the transferred model, but this is
outside the scope of this work.
3.4 Feature Selection
Compatibility of feature representations is neces-
sary but not sufficient for successful model trans-
fer. We have to make sure that the features we use
are predictive of similar outcomes in the two lan-
guages as well.
Depending on the pair of languages in ques-
tion, different aspects of the feature representation
will retain or lose their predictive power. We can
be reasonably certain that the identity of an ar-
gument word is predictive of its semantic role in
any language, but it might or might not be true
of, for example, the word directly preceding the
argument word. It is therefore important to pre-
POS part-of-speech tags
Synt unlabeled dependency graph
Cls cross-lingual word clusters
Gloss glossed word forms
Deprel dependency relations
Table 1: Feature groups.
vent the model from capturing overly specific as-
pects of the source language, which we do by con-
fining the model to first-order features. We also
avoid feature selection, which, performed on the
source language, is unlikely to help the model to
better generalize to the target one. The experi-
ments confirm that feature selection and the use
of second-order features degrade the performance
of the transferred model.
3.5 Feature Groups
For each word, we use its part-of-speech tag,
cross-lingual cluster id, word identity (glossed,
when evaluating on the target language) and its
dependency relation to its parent. Features associ-
ated with an argument word include the attributes
of the predicate word, the argument word, its par-
ent, siblings and children, and the words directly
preceding and following it. Also included are the
sequences of part-of-speech tags and dependency
relations on the path between the predicate and the
argument.
Since we are also interested in the impact of dif-
ferent aspects of the feature representation, we di-
vide the features into groups as summarized in ta-
ble 1 and evaluate their respective contributions to
the performance of the model. If a feature group
is enabled ? the model has access to the corre-
sponding source of information. For example, if
only POS group is enabled, the model relies on
the part-of-speech tags of the argument, the pred-
icate and the words to the right and left of the ar-
gument word. If Synt is enabled too, it also uses
the POS tags of the argument?s parent, children
and siblings.
Word order information constitutes an implicit
group that is always available. It includes the
Position feature, which indicates whether the
argument is located to the left or to the right of
the predicate, and allows the model to look up the
attributes of the words directly preceding and fol-
lowing the argument word. The model we com-
pare against the baselines uses all applicable fea-
ture groups (Deprel is only used in EN-CZ and
CZ-EN experiments with original syntax).
4 Evaluation
4.1 Datasets and Preprocessing
Evaluation of the cross-lingual model transfer re-
quires a rather specific kind of dataset. Namely,
the data in both languages has to be annotated
1193
with the same set of semantic roles following the
same (or compatible) guidelines, which is seldom
the case. We have identified three language pairs
for which such resources are available: English-
Chinese, English-Czech and English-French.
The evaluation datasets for English and Chi-
nese are those from the CoNLL Shared Task
2009 (Hajic? et al, 2009) (henceforth CoNLL-ST).
Their annotation in the CoNLL-ST is not identi-
cal, but the guidelines for ?core? semantic roles
are similar (Kingsbury et al, 2004), so we eval-
uate only on core roles here. The data for the
second language pair is drawn from the Prague
Czech-English Dependency Treebank 2.0 (Hajic?
et al, 2012), which we converted to a format simi-
lar to that of CoNLL-ST1. The original annotation
uses the tectogrammatical representation (Hajic?,
2002) and an inventory of semantic roles (or func-
tors), most of which are interpretable across vari-
ous predicates. Also note that the syntactic anno-
tation of English and Czech in PCEDT 2.0 is quite
similar (to the extent permitted by the difference
in the structure of the two languages) and we can
use the dependency relations in our experiments.
For English-French, the English CoNLL-ST
dataset was used as a source and the model was
evaluated on the manually annotated dataset from
van der Plas et al (2011). The latter contains one
thousand sentences from the French part of the Eu-
roparl (Koehn, 2005) corpus, annotated with se-
mantic roles following an adapted version of Prop-
Bank (Palmer et al, 2005) guidelines. The au-
thors perform annotation projection from English
to French, using a joint model of syntax and se-
mantics and employing heuristics for filtering. We
use a model trained on the output of this projec-
tion system as one of the baselines. The evalua-
tion dataset is relatively small in this case, so we
perform the transfer only one-way, from English
to French.
The part-of-speech tags in all datasets were re-
placed with the universal POS tags of Petrov et al
(2012). For Czech, we have augmented the map-
pings to account for the tags that were not present
in the datasets from which the original mappings
were derived. Namely, tag ?t? is mapped to
?VERB? and ?Y? ? to ?PRON?.
We use parallel data to construct a bilingual
dictionary used in word mapping, as well as
in the projection baseline. For English-Czech
1see http://www.ml4nlp.de/code-and-data/treex2conll
and English-French, the data is drawn from Eu-
roparl (Koehn, 2005), for English-Chinese ? from
MultiUN (Eisele and Chen, 2010). The word
alignments were obtained using GIZA++ (Och
and Ney, 2003) and the intersection heuristic.
4.2 Syntactic Transfer
In the low-resource setting, we cannot always
rely on the availability of an accurate dependency
parser for the target language. If one is not avail-
able, the natural solution would be to use cross-
lingual model transfer to obtain it.
Unfortunately, the models presented in the pre-
vious work, such as Zeman and Resnik (2008),
McDonald et al (2011) and Ta?ckstro?m et al
(2012), were not made available, so we repro-
duced the direct transfer algorithm of McDonald
et al (2011), using Malt parser (Nivre, 2008) and
the same set of features. We did not reimple-
ment the projected transfer algorithm, however,
and used the default training procedure instead of
perceptron-based learning. The dependency struc-
ture thus obtained is, of course, only a rough ap-
proximation ? even a much more sophisticated al-
gorithm may not perform well when transferring
syntax between such languages as Czech and En-
glish, given the inherent difference in their struc-
ture. The scores are shown in table 2.
We will henceforth refer to the syntactic annota-
tions that were provided with the datasets as orig-
inal, as opposed to the annotations obtained by
means of syntactic transfer.
4.3 Baselines
Unsupervised Baseline: We are using a version
of the unsupervised semantic role induction sys-
tem of Titov and Klementiev (2012a) adapted to
Setup UAS, %
EN-ZH 35
ZH-EN 42
EN-CZ 36
CZ-EN 39
EN-FR 67
Table 2: Syntactic transfer accuracy, unlabeled at-
tachment score (percent). Note that in case of
French we evaluate against the output of a super-
vised system, since manual annotation is not avail-
able for this dataset. This score does not reflect the
true performance of syntactic transfer.
1194
the shared feature representation considered in or-
der to make the scores comparable with those
of the transfer model and, more importantly, to
enable evaluation on transferred syntax. Note
that the original system, tailored to a more ex-
pressive language-specific syntactic representa-
tion and equipped with heuristics to identify ac-
tive/passive voice and other phenomena, achieves
higher scores than those we report here.
Projection Baseline: The projection baseline we
use for English-Czech and English-Chinese is a
straightforward one: we label the source side of a
parallel corpus using the source-language model,
then identify those verbs on the target side that are
aligned to a predicate, mark them as predicates and
propagate the argument roles in the same fashion.
A model is then trained on the resulting training
data and applied to the test set.
For English-French we instead use the output of
a fully featured projection model of van der Plas et
al. (2011), published in the CLASSiC project.
5 Results
In order to ensure that the results are consistent,
the test sets, except for the French one, were par-
titioned into five equal parts (of 5 to 10 thousand
sentences each, depending on the dataset) and the
evaluation performed separately on each one. All
evaluation figures for English, Czech or Chinese
below are the average values over the five sub-
sets. In case of French, the evaluation dataset is
too small to split it further, so instead we ran the
evaluation five times on a randomly selected 80%
sample of the evaluation data and averaged over
those. In both cases the results are consistent over
the subsets, the standard deviation does not exceed
0.5% for the transfer system and projection base-
line and 1% for the unsupervised system.
5.1 Argument Identification
We summarize the results in table 3. Argument
identification is known to rely heavily on syntac-
tic information, so it is unsurprising that it proves
inaccurate when transferred syntax is used. Our
simple projection baseline suffers from the same
problem. Even with original syntactic information
available, the performance of argument identifica-
tion is moderate. Note that the model of (van der
Plas et al, 2011), though relying on more expres-
sive syntax, only outperforms the transferred sys-
tem by 3% (F1) on this task.
Setup Syntax TRANS PROJ
EN-ZH trans 34.5 13.9
ZH-EN trans 32.6 15.6
EN-CZ trans 46.3 12.4
CZ-EN trans 42.3 22.2
EN-FR trans 61.6 43.5
EN-ZH orig 51.7 19.6
ZH-EN orig 53.2 29.7
EN-CZ orig 63.9 59.3
CZ-EN orig 67.3 60.9
EN-FR orig 71.0 51.3
Table 3: Argument identification, transferred
model vs. projection baseline, F1.
Most unsupervised SRL approaches assume
that the argument identification is performed
by some external means, for example heuristi-
cally (Lang and Lapata, 2011). Such heuristics
or unsupervised approaches to argument identifi-
cation (Abend et al, 2009) can also be used in the
present setup.
5.2 Argument Classification
In the following tables, TRANS column contains
the results for the transferred system, UNSUP ?
for the unsupervised baseline and PROJ ? for pro-
jection baseline. We highlight in bold the higher
score where the difference exceeds twice the max-
imum of the standard deviation estimates of the
two results.
Table 4 presents the unsupervised evaluation re-
sults. Note that the unsupervised model performs
as well as the transferred one or better where the
Setup Syntax TRANS UNSUP
EN-ZH trans 83.3 73.9
ZH-EN trans 79.2 67.6
EN-CZ trans 66.4 66.1
CZ-EN trans 68.2 68.7
EN-FR trans 74.6 65.1
EN-ZH orig 84.5 89.7
ZH-EN orig 79.2 83.0
EN-CZ orig 74.1 74.0
CZ-EN orig 74.6 76.7
EN-FR orig 73.3 72.3
Table 4: Argument classification, transferred
model vs. unsupervised baseline in terms of the
clustering metric F c1 (see section 2.3).
1195
Setup Syntax TRANS PROJ
EN-ZH trans 70.1 69.2
ZH-EN trans 65.6 61.3
EN-CZ trans 50.1 46.3
CZ-EN trans 53.3 54.7
EN-FR trans 65.1 66.1
EN-ZH orig 71.7 69.7
ZH-EN orig 66.1 64.4
EN-CZ orig 59.0 53.2
CZ-EN orig 61.0 60.8
EN-FR orig 63.0 68.0
Table 5: Argument classification, transferred
model vs. projection baseline, accuracy.
original syntactic dependencies are available. In
the more realistic scenario with transferred syn-
tax, however, the transferred model proves more
accurate.
In table 5 we compare the transferred system
with the projection baseline. It is easy to see
that the scores vary strongly depending on the lan-
guage pair, due to both the difference in the anno-
tation scheme used and the degree of relatedness
between the languages. The drop in performance
when transferring the model to another language
is large in every case, though, see table 6.
Setup Target Source
EN-ZH 71.7 87.1
ZH-EN 66.1 86.2
EN-CZ 59.0 80.1
CZ-EN 61.0 75.4
EN-FR 63.0 82.5
Table 6: Model accuracy on the source and target
language using original syntax. The source lan-
guage scores for English vary between language
pairs because of the difference in syntactic anno-
tation and role subset used.
We also include the individual F1 scores for
the top-10 most frequent labels for EN-CZ trans-
fer with original syntax in table 7. The model
provides meaningful predictions here, despite low
overall accuracy.
Most of the labels2 are self-explanatory: Pa-
tient (PAT), Actor (ACT), Time (TWHEN), Effect
(EFF), Location (LOC), Manner (MANN), Ad-
dressee (ADDR), Extent (EXT). CPHR marks the
2http://ufal.mff.cuni.cz/?toman/pcedt/en/functors.html
Label Freq. F1 Re. Pr.
PAT 14707 69.4 70.0 68.7
ACT 14303 81.1 81.7 80.4
TWHEN 3631 70.6 65.1 77.0
EFF 2601 45.4 67.2 34.3
LOC 1990 41.8 35.3 51.3
MANN 1208 54.0 63.8 46.9
ADDR 1045 30.2 34.4 26.8
CPHR 791 20.4 13.1 45.0
EXT 708 42.2 40.5 44.1
DIR3 695 20.1 17.3 23.9
Table 7: EN-CZ transfer (with original syntax), F1,
recall and precision for the top-10 most frequent
roles.
nominal part of a complex predicate, as in ?to have
[a plan]CPHR?, and DIR3 indicates destination.
5.3 Additional Experiments
We now evaluate the contribution of different as-
pects of the feature representation to the perfor-
mance of the model. Table 8 contains the results
for English-French.
Features Orig Trans
POS 47.5 47.5
POS, Synt 53.0 53.1
POS, Cls 53.7 53.7
POS, Gloss 63.7 63.7
POS, Synt, Cls 55.9 56.4
POS, Synt, Gloss 65.2 66.3
POS, Cls, Gloss 61.5 61.5
POS, Synt, Cls, Gloss 63.0 65.1
Table 8: EN-FR model transfer accuracy with dif-
ferent feature subsets, using original and trans-
ferred syntactic information.
The fact that the model performs slightly bet-
ter with transferred syntax may be explained by
two factors. Firstly, as we already mentioned, the
original syntactic annotation is also produced au-
tomatically. Secondly, in the model transfer setup
it is more important how closely the syntactic-
semantic interface on the target side resembles that
on the source side than how well it matches the
?true? structure of the target language, and in this
respect a transferred dependency parser may have
an advantage over one trained on target-language
data.
The high impact of the Gloss features here
1196
may be partly attributed to the fact that the map-
ping is derived from the same corpus as the eval-
uation data ? Europarl (Koehn, 2005) ? and partly
by the similarity between English and French in
terms of word order, usage of articles and prepo-
sitions. The moderate contribution of the cross-
lingual cluster features are likely due to the insuf-
ficient granularity of the clustering for this task.
For more distant language pairs, the contribu-
tions of individual feature groups are less inter-
pretable, so we only highlight a few observations.
First of all, both EN-CZ and CZ-EN benefit notice-
ably from the use of the original syntactic annota-
tion, including dependency relations, but not from
the transferred syntax, most likely due to the low
syntactic transfer performance. Both perform bet-
ter when lexical information is available, although
the improvement is not as significant as in the case
of French ? only up to 5%.
The situation with Chinese is somewhat compli-
cated in that adding lexical information here fails
to yield an improvement in terms of the metric
considered. This is likely due to the fact that we
consider only the core roles, which can usually be
predicted with high accuracy based on syntactic
information alone.
6 Related Work
Development of robust statistical models for core
NLP tasks is a challenging problem, and adapta-
tion of existing models to new languages presents
a viable alternative to exhaustive annotation for
each language. Although the models thus obtained
are generally imperfect, they can be further refined
for a particular language and domain using tech-
niques such as active learning (Settles, 2010; Chen
et al, 2011).
Cross-lingual annotation projection (Yarowsky
et al, 2001) approaches have been applied ex-
tensively to a variety of tasks, including POS
tagging (Xi and Hwa, 2005; Das and Petrov,
2011), morphology segmentation (Snyder and
Barzilay, 2008), verb classification (Merlo et al,
2002), mention detection (Zitouni and Florian,
2008), LFG parsing (Wro?blewska and Frank,
2009), information extraction (Kim et al, 2010),
SRL (Pado? and Lapata, 2009; van der Plas et al,
2011; Annesi and Basili, 2010; Tonelli and Pi-
anta, 2008), dependency parsing (Naseem et al,
2012; Ganchev et al, 2009; Smith and Eisner,
2009; Hwa et al, 2005) or temporal relation pre-
diction (Spreyer and Frank, 2008). Interestingly,
it has also been used to propagate morphosyntac-
tic information between old and modern versions
of the same language (Meyer, 2011).
Cross-lingual model transfer methods (McDon-
ald et al, 2011; Zeman and Resnik, 2008; Durrett
et al, 2012; S?gaard, 2011; Lopez et al, 2008)
have also been receiving much attention recently.
The basic idea behind model transfer is similar to
that of cross-lingual annotation projection, as we
can see from the way parallel data is used in, for
example, McDonald et al (2011).
A crucial component of direct transfer ap-
proaches is the unified feature representation.
There are at least two such representations of
lexical information (Klementiev et al, 2012;
Ta?ckstro?m et al, 2012), but both work on word
level. This makes it hard to account for phenom-
ena that are expressed differently in the languages
considered, for example the syntactic function of
a certain word may be indicated by a preposi-
tion, inflection or word order, depending on the
language. Accurate representation of such infor-
mation would require an extra level of abstrac-
tion (Hajic?, 2002).
A side-effect of using adaptation methods is that
we are forced to use the same annotation scheme
for the task in question (SRL, in our case), which
in turn simplifies the development of cross-lingual
tools for downstream tasks. Such representations
are also likely to be useful in machine translation.
Unsupervised semantic role labeling meth-
ods (Lang and Lapata, 2010; Lang and Lapata,
2011; Titov and Klementiev, 2012a; Lorenzo and
Cerisara, 2012) also constitute an alternative to
cross-lingual model transfer.
For an overview of of semi-supervised ap-
proaches we refer the reader to Titov and Klemen-
tiev (2012b).
7 Conclusion
We have considered the cross-lingual model trans-
fer approach as applied to the task of semantic role
labeling and observed that for closely related lan-
guages it performs comparably to annotation pro-
jection approaches. It allows one to quickly con-
struct an SRL model for a new language without
manual annotation or language-specific heuristics,
provided an accurate model is available for one of
the related languages along with a certain amount
of parallel data for the two languages. While an-
1197
notation projection approaches require sentence-
and word-aligned parallel data and crucially de-
pend on the accuracy of the syntactic parsing and
SRL on the source side of the parallel corpus,
cross-lingual model transfer can be performed us-
ing only a bilingual dictionary.
Unsupervised SRL approaches have their ad-
vantages, in particular when no annotated data is
available for any of the related languages and there
is a syntactic parser available for the target one,
but the annotation they produce is not always suf-
ficient. In applications such as Information Re-
trieval it is preferable to have precise labels, rather
than just clusters of arguments, for example.
Also note that when applying cross-lingual
model transfer in practice, one can improve upon
the performance of the simplistic model we use
for evaluation, for example by picking the features
manually, taking into account the properties of the
target language. Domain adaptation techniques
can also be employed to adjust the model to the
target language.
Acknowledgments
The authors would like to thank Alexandre Kle-
mentiev and Ryan McDonald for useful sugges-
tions and Ta?ckstro?m et al (2012) for sharing the
cross-lingual word representations. This research
is supported by the MMCI Cluster of Excellence.
References
Omri Abend, Roi Reichart, and Ari Rappoport. 2009.
Unsupervised argument identification for semantic
role labeling. In Proceedings of the Joint Con-
ference of the 47th Annual Meeting of the ACL
and the 4th International Joint Conference on Nat-
ural Language Processing of the AFNLP, ACL ?09,
pages 28?36, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Paolo Annesi and Roberto Basili. 2010. Cross-lingual
alignment of FrameNet annotations through hidden
Markov models. In Proceedings of the 11th interna-
tional conference on Computational Linguistics and
Intelligent Text Processing, CICLing?10, pages 12?
25, Berlin, Heidelberg. Springer-Verlag.
Roberto Basili, Diego De Cao, Danilo Croce, Bonaven-
tura Coppola, and Alessandro Moschitti. 2009.
Cross-language frame semantics transfer in bilin-
gual corpora. In Alexander F. Gelbukh, editor, Pro-
ceedings of the 10th International Conference on
Computational Linguistics and Intelligent Text Pro-
cessing, pages 332?345.
Anders Bjo?rkelund, Love Hafdell, and Pierre Nugues.
2009. Multilingual semantic role labeling. In Pro-
ceedings of the Thirteenth Conference on Computa-
tional Natural Language Learning (CoNLL 2009):
Shared Task, pages 43?48, Boulder, Colorado, June.
Association for Computational Linguistics.
Ming-Wei Chang, Lev Ratinov, and Dan Roth. 2007.
Guiding semi-supervision with constraint-driven
learning. In ACL.
Chenhua Chen, Alexis Palmer, and Caroline Sporleder.
2011. Enhancing active learning for semantic role
labeling via compressed dependency trees. In Pro-
ceedings of 5th International Joint Conference on
Natural Language Processing, pages 183?191, Chi-
ang Mai, Thailand, November. Asian Federation of
Natural Language Processing.
Dipanjan Das and Slav Petrov. 2011. Unsupervised
part-of-speech tagging with bilingual graph-based
projections. Proceedings of the Association for
Computational Linguistics.
Greg Durrett, Adam Pauls, and Dan Klein. 2012. Syn-
tactic transfer using a bilingual lexicon. In Pro-
ceedings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning, pages 1?11,
Jeju Island, Korea, July. Association for Computa-
tional Linguistics.
Andreas Eisele and Yu Chen. 2010. MultiUN:
A multilingual corpus from United Nation docu-
ments. In Proceedings of the Seventh International
Conference on Language Resources and Evaluation
(LREC?10). European Language Resources Associ-
ation (ELRA).
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR:
A library for large linear classification. Journal of
Machine Learning Research, 9:1871?1874.
Kuzman Ganchev, Jennifer Gillenwater, and Ben
Taskar. 2009. Dependency grammar induction via
bitext projection constraints. In Proceedings of the
47th Annual Meeting of the ACL, pages 369?377,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Qin Gao and Stephan Vogel. 2011. Corpus expan-
sion for statistical machine translation with seman-
tic role label substitution rules. In Proceedings of
the 49th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies, pages 294?298, Portland, Oregon, USA.
Trond Grenager and Christopher D. Manning. 2006.
Unsupervised discovery of a statistical verb lexicon.
In Proceedings of EMNLP.
Jan Hajic?. 2002. Tectogrammatical representation:
Towards a minimal transfer in machine translation.
In Robert Frank, editor, Proceedings of the 6th In-
ternational Workshop on Tree Adjoining Grammars
1198
and Related Frameworks (TAG+6), pages 216?
226, Venezia. Universita di Venezia.
Jan Hajic?, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Anto`nia Mart??, Llu??s
Ma`rquez, Adam Meyers, Joakim Nivre, Sebastian
Pado?, Jan S?te?pa?nek, Pavel Stran?a?k, Mihai Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The CoNLL-
2009 shared task: Syntactic and semantic dependen-
cies in multiple languages. In Proceedings of the
Thirteenth Conference on Computational Natural
Language Learning (CoNLL 2009): Shared Task,
pages 1?18, Boulder, Colorado.
Jan Hajic?, Eva Hajic?ova?, Jarmila Panevova?, Petr
Sgall, Ondr?ej Bojar, Silvie Cinkova?, Eva Fuc???kova?,
Marie Mikulova?, Petr Pajas, Jan Popelka, Jir???
Semecky?, Jana S?indlerova?, Jan S?te?pa?nek, Josef
Toman, Zden?ka Ures?ova?, and Zdene?k Z?abokrtsky?.
2012. Announcing Prague Czech-English depen-
dency treebank 2.0. In Nicoletta Calzolari (Con-
ference Chair), Khalid Choukri, Thierry Declerck,
Mehmet Ug?ur Dog?an, Bente Maegaard, Joseph Mar-
iani, Jan Odijk, and Stelios Piperidis, editors, Pro-
ceedings of the Eight International Conference on
Language Resources and Evaluation (LREC?12), Is-
tanbul, Turkey, May. European Language Resources
Association (ELRA).
Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara
Cabezas, and Okan Kolak. 2005. Bootstrapping
parsers via syntactic projection across parallel text.
Natural Language Engineering, 11(3):311?325.
Richard Johansson and Pierre Nugues. 2008.
Dependency-based semantic role labeling of Prop-
Bank. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Process-
ing, pages 69?78, Honolulu, Hawaii.
Michael Kaisser and Bonnie Webber. 2007. Question
answering based on semantic roles. In ACL Work-
shop on Deep Linguistic Processing.
Seokhwan Kim, Minwoo Jeong, Jonghoon Lee, and
Gary Geunbae Lee. 2010. A cross-lingual an-
notation projection approach for relation detection.
In Proceedings of the 23rd International Conference
on Computational Linguistics, COLING ?10, pages
564?571, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Paul Kingsbury, Nianwen Xue, and Martha Palmer.
2004. Propbanking in parallel. In In Proceedings
of the Workshop on the Amazing Utility of Paral-
lel and Comparable Corpora, in conjunction with
LREC?04.
Alexandre Klementiev, Ivan Titov, and Binod Bhat-
tarai. 2012. Inducing crosslingual distributed rep-
resentations of words. In Proceedings of the Inter-
national Conference on Computational Linguistics
(COLING), Bombay, India.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In Conference Pro-
ceedings: the tenth Machine Translation Summit,
pages 79?86, Phuket, Thailand. AAMT.
Joel Lang and Mirella Lapata. 2010. Unsuper-
vised induction of semantic roles. In Human Lan-
guage Technologies: The 2010 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics, pages 939?947, Los
Angeles, California, June. Association for Compu-
tational Linguistics.
Joel Lang and Mirella Lapata. 2011. Unsupervised
semantic role induction via split-merge clustering.
In Proc. of Annual Meeting of the Association for
Computational Linguistics (ACL).
Ding Liu and Daniel Gildea. 2010. Semantic role
features for machine translation. In Proceedings of
the 23rd International Conference on Computational
Linguistics (Coling 2010), Beijing, China.
Adam Lopez, Daniel Zeman, Michael Nossal, Philip
Resnik, and Rebecca Hwa. 2008. Cross-language
parser adaptation between related languages. In
IJCNLP-08 Workshop on NLP for Less Privileged
Languages, pages 35?42, Hyderabad, India, Jan-
uary.
Alejandra Lorenzo and Christophe Cerisara. 2012.
Unsupervised frame based semantic role induction:
application to French and English. In Proceedings
of the ACL 2012 Joint Workshop on Statistical Pars-
ing and Semantic Processing of Morphologically
Rich Languages, pages 30?35, Jeju, Republic of Ko-
rea, July. Association for Computational Linguistics.
Ryan McDonald, Slav Petrov, and Keith Hall. 2011.
Multi-source transfer of delexicalized dependency
parsers. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
EMNLP ?11, pages 62?72, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Paola Merlo, Suzanne Stevenson, Vivian Tsang, and
Gianluca Allaria. 2002. A multi-lingual paradigm
for automatic verb classification. In Proceedings
of the 40th Annual Meeting of the Association for
Computational Linguistics (ACL?02), pages 207?
214, Philadelphia, PA.
Roland Meyer. 2011. New wine in old wineskins??
Tagging old Russian via annotation projection
from modern translations. Russian Linguistics,
35(2):267(15).
Tahira Naseem, Regina Barzilay, and Amir Globerson.
2012. Selective sharing for multilingual dependency
parsing. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 629?637, Jeju Island, Korea, July. Asso-
ciation for Computational Linguistics.
Joakim Nivre. 2008. Algorithms for deterministic in-
cremental dependency parsing. Comput. Linguist.,
34(4):513?553, December.
1199
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1).
Sebastian Pado? and Mirella Lapata. 2009. Cross-
lingual annotation projection for semantic roles.
Journal of Artificial Intelligence Research, 36:307?
340.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The Proposition Bank: An annotated cor-
pus of semantic roles. Computational Linguistics,
31:71?105.
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012.
A universal part-of-speech tagset. In Proceedings of
LREC, May.
Mark Sammons, Vinod Vydiswaran, Tim Vieira,
Nikhil Johri, Ming wei Chang, Dan Goldwasser,
Vivek Srikumar, Gourab Kundu, Yuancheng Tu,
Kevin Small, Joshua Rule, Quang Do, and Dan
Roth. 2009. Relation alignment for textual en-
tailment recognition. In Text Analysis Conference
(TAC).
Burr Settles. 2010. Active learning literature survey.
Computer Sciences Technical Report, 1648.
Dan Shen and Mirella Lapata. 2007. Using semantic
roles to improve question answering. In EMNLP.
David A Smith and Jason Eisner. 2009. Parser adap-
tation and projection with quasi-synchronous gram-
mar features. In Proceedings of the 2009 Confer-
ence on Empirical Methods in Natural Language
Processing, pages 822?831. Association for Com-
putational Linguistics.
Benjamin Snyder and Regina Barzilay. 2008. Cross-
lingual propagation for morphological analysis. In
Proceedings of the 23rd national conference on Ar-
tificial intelligence.
Anders S?gaard. 2011. Data point selection for cross-
language adaptation of dependency parsers. In Pro-
ceedings of the 49th Annual Meeting of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies, volume 2 of HLT ?11, pages
682?686, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Kathrin Spreyer and Anette Frank. 2008. Projection-
based acquisition of a temporal labeller. Proceed-
ings of IJCNLP 2008.
Oscar Ta?ckstro?m, Ryan McDonald, and Jakob Uszko-
reit. 2012. Cross-lingual word clusters for direct
transfer of linguistic structure. In Proc. of the An-
nual Meeting of the North American Association
of Computational Linguistics (NAACL), pages 477?
487, Montre?al, Canada.
Cynthia A. Thompson, Roger Levy, and Christopher D.
Manning. 2003. A generative model for seman-
tic role labeling. In Proceedings of the 14th Eu-
ropean Conference on Machine Learning, ECML
2003, pages 397?408, Dubrovnik, Croatia.
Ivan Titov and Alexandre Klementiev. 2012a. A
Bayesian approach to unsupervised semantic role in-
duction. In Proc. of European Chapter of the Asso-
ciation for Computational Linguistics (EACL).
Ivan Titov and Alexandre Klementiev. 2012b. Semi-
supervised semantic role labeling: Approaching
from an unsupervised perspective. In Proceedings
of the International Conference on Computational
Linguistics (COLING), Bombay, India, December.
Sara Tonelli and Emanuele Pianta. 2008. Frame infor-
mation transfer from English to Italian. In Proceed-
ings of LREC 2008.
Lonneke van der Plas, James Henderson, and Paola
Merlo. 2009. Domain adaptation with artificial
data for semantic parsing of speech. In Proc. 2009
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 125?128, Boulder, Colorado.
Lonneke van der Plas, Paola Merlo, and James Hen-
derson. 2011. Scaling up automatic cross-lingual
semantic role annotation. In Proceedings of the 49th
Annual Meeting of the Association for Computa-
tional Linguistics: Human Language Technologies,
HLT ?11, pages 299?304, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Alina Wro?blewska and Anette Frank. 2009. Cross-
lingual projection of LFG F-structures: Building
an F-structure bank for Polish. In Eighth Interna-
tional Workshop on Treebanks and Linguistic Theo-
ries, page 209.
Dekai Wu and Pascale Fung. 2009. Can semantic
role labeling improve SMT? In Proceedings of 13th
Annual Conference of the European Association for
Machine Translation (EAMT 2009), Barcelona.
Chenhai Xi and Rebecca Hwa. 2005. A backoff
model for bootstrapping resources for non-English
languages. In Proceedings of the conference on Hu-
man Language Technology and Empirical Methods
in Natural Language Processing, pages 851?858,
Stroudsburg, PA, USA.
David Yarowsky, Grace Ngai, and Ricahrd Wicen-
towski. 2001. Inducing multilingual text analysis
tools via robust projection across aligned corpora. In
Proceedings of Human Language Technology Con-
ference.
Daniel Zeman and Philip Resnik. 2008. Cross-
language parser adaptation between related lan-
guages. In Proceedings of the IJCNLP-08 Workshop
on NLP for Less Privileged Languages, pages 35?
42, Hyderabad, India, January. Asian Federation of
Natural Language Processing.
Imed Zitouni and Radu Florian. 2008. Mention detec-
tion crossing the language barrier. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing.
1200
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1630?1639,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
A Bayesian Model for Joint Unsupervised Induction
of Sentiment, Aspect and Discourse Representations
Angeliki Lazaridou
University of Trento
angeliki.lazaridou@unitn.it
Ivan Titov
Saarland University
titov@mmci.uni-saarland.de
Caroline Sporleder
Trier University
csporled@coli.uni-sb.de
Abstract
We propose a joint model for unsuper-
vised induction of sentiment, aspect and
discourse information and show that by in-
corporating a notion of latent discourse re-
lations in the model, we improve the pre-
diction accuracy for aspect and sentiment
polarity on the sub-sentential level. We
deviate from the traditional view of dis-
course, as we induce types of discourse re-
lations and associated discourse cues rel-
evant to the considered opinion analysis
task; consequently, the induced discourse
relations play the role of opinion and as-
pect shifters. The quantitative analysis that
we conducted indicated that the integra-
tion of a discourse model increased the
prediction accuracy results with respect to
the discourse-agnostic approach and the
qualitative analysis suggests that the in-
duced representations encode a meaning-
ful discourse structure.
1 Introduction
With the rapid growth of the Web, it is becoming
increasingly difficult to discern useful from irrel-
evant information, particularly in user-generated
content, such as product reviews. To make it easier
for the reader to separate the wheat from the chaff,
it is necessary to structure the available informa-
tion. In the review domain, this is done in aspect-
based sentiment analysis which aims at identify-
ing text fragments in which opinions are expressed
about ratable aspects of products, such as ?room
quality? or ?service quality?. Such fine-grained
analysis can serve as the first step in aspect-based
sentiment summarization (Hu and Liu, 2004), a
task with many practical applications.
Aspect-based summarization is an active re-
search area for which various techniques have
been developed, both statistical (Mei et al, 2007;
Titov and McDonald, 2008b) and not (Hu and Liu,
2004), and relying on different types of supervi-
sion sources, such as sentiment-annotated texts or
polarity lexica (Turney and Littman, 2002). Most
methods rely on local information (bag-of-words,
short ngrams or elementary syntactic fragments)
and do not attempt to account for more complex
interactions. However, these local lexical repre-
sentations by themselves are often not sufficient to
infer a sentiment or aspect for a fragment of text.
For instance, in the following example taken from
a TripAdvisor1 review:
Example 1. The room was nice but let?s not talk
about the view.
it is difficult to deduce on the basis of local lexical
features alone that the opinion about the view is
negative. The clause let?s not talk about the view
could by itself be neutral or even positive given the
right context (e.g., I?ve never seen such a fancy ho-
tel room, my living room doesn?t look that cool...
and let?s not talk about the view). However, the
contrast relation signaled by the connective but
makes it clear that the second clause has a nega-
tive polarity. The same observations can be made
about transitions between aspects: changes in as-
pect are often clearly marked by discourse connec-
tives. Importantly, some of these cues are not dis-
course connectives in the strict linguistic sense and
are specific to the review domain (e.g., the phrase
I would also in a review indicates that the topic
is likely to be changed). In order to accurately
predict sentiment and topic,2 a model needs to ac-
1http://www.tripadvisor.com/
2In what follows, we use the terms aspect and topic, inter-1630
count for these discourse phenomena and cannot
rely solely on local lexical information.
These issues have not gone unnoticed to the re-
search community. Consequently, there has re-
cently been an increased interest in models that
leverage content and discourse structure in senti-
ment analysis tasks. However, discourse-level in-
formation is typically incorporated in a pipeline
architecture, either in the form of sentiment po-
larity shifters (Polanyi and Zaenen, 2006; Naka-
gawa et al, 2010) that operate on the lexical level
or by using discourse relations (Taboada et al,
2008; Zhou et al, 2011) that comply with dis-
course theories like Rhetorical Structure Theory
(RST) (Mann and Thompson, 1988). Such ap-
proaches have a number of disadvantages. First,
they require additional resources, such as lists of
polarity shifters or discourse connectives which
signal specific relations. These resources are avail-
able only for a handful of languages. Second, re-
lying on a generic discourse analysis step that is
carried out before sentiment analysis may intro-
duce additional noise and lead to error propaga-
tion. Furthermore, these techniques will not nec-
essarily be able to induce discourse relations in-
formative for the sentiment analysis domain (Voll
and Taboada, 2007).
An alternative approach is to define a task-
specific scheme of discourse relations (Somasun-
daran et al, 2009). This previous work showed
that task-specific discourse relations are helpful in
predicting sentiment, however, in doing so they re-
lied on gold-standard discourse annotation at test
time rather than predicting it automatically or in-
ducing it jointly with sentiment polarity.
We take a different approach and induce dis-
course and sentiment information jointly in an un-
supervised (or weakly supervised) manner. This
has the advantage of not having to pre-specify a
mapping from discourse cues to discourse rela-
tions; our model induces this automatically, which
makes it portable to new domains and languages.
Joint induction of discourse and sentiment struc-
ture also has the added benefit that the model is
able to learn exactly those aspects of discourse
structure that are relevant for sentiment analysis.
We start with a relatively standard joint model
of sentiment and topic, which can be regarded as a
cross-breed between the JST model (Lin and He,
2009) and the ASUM model (Jo and Oh, 2011),
changeably as well as sentiment levels and opinion polarity.
both state-of-the-art techniques. This model is
weakly supervised, as it relies solely on document-
level (i.e. not aspect-specific) opinion polarity la-
bels to induce topics and sentiment on the sub-
sentential level. In order to test our hypothesis
that discourse information is beneficial, we add
a discourse modeling component. Note that in
modeling discourse we do not exploit any kind
of supervision. We demonstrate that the resulting
model outperforms the baseline on a product re-
view dataset (see Section 5).
To the best of our knowledge, unsupervised
joint induction of discourse structure, sentiment
and topic information has not been considered
before, particularly not in the context of the
aspect-based sentiment analysis task. Importantly,
our method for discourse modeling is a general
method which can be integrated in virtually any
LDA-style model of aspect and sentiment.
2 Modeling Discourse Structure
Discourse cues typically do not directly indicate
sentiment polarity (or aspect). However, they can
indicate how polarity (or aspect) changes as the
text unfolds. As we have seen in the examples
above, changes in polarity can happen on a sub-
sentential level, i.e., between adjacent clauses or,
from a discourse-theoretic point of view, between
adjacent elementary discourse units (EDUs). To
model these changes we need a strong linguistic
signal, for example, in the form of discourse con-
nectives or other discourse cues. We hypothesize
that these are more likely to occur at the beginning
of an EDU than in the middle or at the end. This is
certainly true for most of the traditional discourse
relation cues (particularly connectives).
Changes in polarity or aspect are often cor-
related with specific discourse relations, such as
?contrast?. However, not all relations are rele-
vant and there is no one-to-one correspondence
between relations and sentiment changes.3 Fur-
thermore, if a discourse relation signals a change,
it is typically ambiguous whether this change oc-
curs with the polarity (example 1) or the aspect
(the room was nice but the breakfast was even bet-
ter) or both (the room was nice but the breakfast
was awful). Therefore, we do not explicitly model
3The ?explanation? relation, for example, can occur with
a polarity change (We were upgraded to a really nice room
because the hotel made a terrible blunder with our booking)
but does not have to (The room was really nice because the
hotel was newly renovated).1631
Name Description
AltSame different polarity, same aspect
SameAlt same polarity, different aspect
AltAlt different polarity and aspect
Table 1: Discourse relations
generic discourse relations; instead, inspired by
the work of Somasundaran et al (2008), we define
three very general relations which encode how po-
larity and aspect change (Table 1). Note that we
do not have a discourse relation SameSame since
we do not expect to have strong linguistic evidence
which states that an EDU contains the same senti-
ment information as the previous one.4 However,
we assume that the sentiment and topic flow is
fairly smooth in general. In other words, for two
adjacent EDUs not connected by any of the above
three relations, the prior probability of staying at
the same topic and sentiment level is higher than
picking a new topic and sentiment level (i.e. we
use ?sticky states? (Fox et al, 2008)).
3 Model
In this section we describe our Bayesian model,
first the discourse-agnostic model and then an ex-
tension needed to encode discourse information.
The formal generative story is presented in Fig-
ure 1: the red fragments correspond to the dis-
course modeling component. In order to obtain the
generative story for the discourse-agnostic model,
they simply need to be ignored.
3.1 Discourse-agnostic model
In our approach we make an assumption that all
the words in an EDU correspond to the same topic
and sentiment level. We also assume that an over-
all sentiment of the document is defined, this is
the only supervision we use in inducing the model.
Unlike some of the previous work (e.g., (Titov and
McDonald, 2008a)), we do not constrain aspect-
specific sentiment to be the same across the docu-
ment. We describe our discourse-agnostic model
by first describing the set of corpus-level and
document-level parameters, and then explain how
the content of each document is generated.
Drawing model parameters On the corpus
level, for every topic z ? {1, . . . ,K} and ev-
ery sentiment polarity level y ? {?1, 0,+1},
we start by drawing a unigram language model
4The typical connective in this situation would be and
which is highly ambiguous and can signal several traditional
discourse relations.
from a Dirichlet prior. For example, the language
model of the aspect service may indicate that the
word friendly is used to express a positive opinion,
whereas the word rude expresses a negative one.
Similarly, for every topic z and every over-
all sentiment polarity y?, we draw a distribution
?y?,z over opinion polarity in this topic z. Intu-
itively, one would expect the sentiment of an as-
pect to more often agree with the overall sentiment
y? than not. This intuition is encoded in an asym-
metric Dirichlet prior Dir(? y?) for ?y?,z : ? y? =
(?y?,1, . . . , ?y?,M ), ?y?,y = ? + ??y,y?, where ?y,y? is
the Kronecker symbol, ? and ? are nonnegative
scalar parameters. Using these ?heavy-diagonal?
priors is crucial, as this is the way to ensure that
the overall sentiment level is tied to the aspect-
specific sentiment level. Otherwise, sentiment lev-
els will be specific to individual aspects (e.g., the
?+1? sentiment for one topic may correspond to
a ?-1? sentiment for another one). Without this
property we would not be able to encode soft con-
straints imposed by the discourse relations.
Drawing documents On the document level, as
in the standard LDA model, we choose the distri-
bution over topics for the document from a sym-
metric Dirichlet prior parametrized by ?, which is
used to control sparsity of topic assignments. Fur-
thermore, we draw the global sentiment y?d from a
uniform distribution.
The generation of a document is done on the
EDU-by-EDU basis. In this work, we assume
that EDU segmentation is provided by the prepro-
cessing step. First, we generate the aspect zd,s
for EDU s according to the distribution of top-
ics ?d. Then, we choose a sentiment level yd,s
for the considered EDU from the categorical dis-
tribution ?y?d,zd,s , conditioned on the aspect zd,s,
as well as on the global sentiment of the document
y?d. Finally, we generate the bag of words for the
EDU by drawing the words from the aspect- and
sentiment-specific language model.
This model can be seen as a variant of a state-of-
the-art model for jointly inducing sentiment and
aspect at the sentence level (Jo and Oh, 2011), or,
more precisely, as its combination with the JST
model (Lin and He, 2009), adapted to the specifics
of our setting. Both these models have been shown
to perform well on sentiment and topic prediction
tasks, outperforming earlier models, such as the
TSM model (Mei et al, 2007). Consequently, it
can be considered as a strong baseline.1632
3.2 Discourse-informed model
In order to integrate discourse information into the
discourse-agnostic model, we need to define a set
of extra parameters and random variables.
Drawing model parameters First, at the corpus
level, we draw a distribution ?? over four discourse
relations: three relations as defined in Table 1 and
an additional dummy relation 4 to indicate that
there is no relation between two adjacent EDUs
(NoRelation). This distribution is drawn from an
asymmetric Dirichlet prior parametrized by a vec-
tor of hyperparameters ?. These parameters en-
code the intuition that most pairs of EDUs do not
exhibit a discourse relation relevant for the task
(i.e. favor NoRelation), that is ?4 has a distinct
and larger value than other parameters ?4?.
Every discourse relation c (including
NoRelation which is treated here as Same-
Same) is associated with two groups of transition
distributions, one governing transitions of sen-
timent (??c) and another one controlling topic
transitions (??c). The parameter ??c,ys , defines a
distribution over sentiment polarity for the EDU
s+ 1 given the sentiment for the sth EDU ys and
the discourse relation c. This distribution encodes
our beliefs about sentiment transitions between
EDUs s and s+ 1 related through c. For example,
the distribution ??SameAlt,+1 would assign higher
probability mass to the positive sentiment polarity
(+1) than to the other 2 sentiment levels (0,
-1). Similarly, the parameter ??c,zs , defines a
distribution over K aspects.
These two families of transition distributions
are each defined in the following way. For the dis-
tribution ??, for relations that favor changing the
aspect (SameAlt and AltAlt), the probability of the
preferred (K-1) transitions is proportional to ??
and for the remaining transitions it is proportional
to 1. On the other hand, for the relations that fa-
vor keeping the same aspect (NoRelation and Alt-
Same), the probability of the preferred transition is
proportional to ???, whereas the probability of the
(K-1) remaining transitions is again proportional
to 1. For the sentiment transitions, the distribution
??c,ys is defined in the analogous way (but depends
on ?? and ???). These scalars are hand-coded and
define soft constraints that discourse relations im-
pose on the local flow of sentiment and aspects.
The parameter ??c is a language model over dis-
course cues w?, which are not restricted to uni-
grams but can generate phrases of arbitrary (and
variable) size. For this reason, we draw them
from a Dirichlet process (DP) (i.e. one for each
discourse relation, except for NoRelation). The
base measure G0 provides the probability of an n-
word sequence calculated with the bigram prob-
ability model estimated from the corpus.5 This
model component bears strong similarities to the
Bayesian model of word segmentation (Goldwa-
ter et al, 2009), though we use the DP process
to generate only the prefix of the EDU, whereas
the rest of the EDU is generated from the bag-of-
words model.
Drawing documents As pointed out above, the
content generation is broken into two steps, where
first we draw the discourse cue w?d,s from ??c and
then we generate the remaining words.
The second difference at the data generation
step (Figure 1) is in the way the aspect and sen-
timent labels are drawn. As the discourse rela-
tion between the EDUs has already been chosen,
we have some expectations about the values of the
sentiment and aspect of the following EDU, which
are encoded by the distributions ?? and ??. These
are only soft constraints that have to be taken into
consideration along with the information provided
by the aspect-sentiment model. This coupling of
information naturally translates into the product-
of-experts (PoE) (Hinton, 1999) approach, where
two sources of information jointly contribute to
the final result. The PoE model seems to be more
appropriate here than a mixture model, as we do
not want the discourse transition to overpower the
sentiment-topic model. In the PoE model, in or-
der for an outcome to be chosen, it needs to have
a non-negligible probability under both models.
4 Inference
Since exact inference of our model is intractable,
we use collapsed Gibbs sampling. The variables
that need to be inferred are the topic assignments
z, the sentiment assignments y, the discourse re-
lations c and the discourse cue w? (or, more pre-
cisely, its length) and are all sampled jointly (for
each EDU) since we expect them to be highly de-
pendent. All other variables (i.e. unknown dis-
tributions) could be marginalized out to obtain a
collapsed Gibbs sampler (Griffiths and Steyvers,
2004).
5This measure is improper but it serves the purpose of
favoring long cues, the behavior arguably desirable for our
application.1633
Global parameters:
?? ? Dir(?) [distrib of disc rel]
for each discourse relation c = 1, .., 4:
??c ? DP(?,Go) [distrib of disc rel specific disc cues]
??c,k - fixed [distrib of rel specific aspect transitions]
??c,y - fixed [distrib of rel specific sent transitions]
for each aspect k = 1, 2...K:
for each sentiment y = ?1, 0,+1:
?k,y ? Dir(?k) [unigram language models]
for each global sentiment y? = ?1, 0,+1:
?y?,k ? Dir(?) [sent distrib given overall sentiment]
Data Generation:
for each document d:
y?d ? Unif(?1, 0,+1) [global sentiment]
?d ? Dir(?) [distr over aspects]
for every EDU s:
cd,s ? ?? [draw disc relation]
if cd,s 6= NoRelation
w?d,s ? ??cd,s [draw disc cue]
zd,s ? ?d ? ??cd,s, zd,s?1 [draw aspect]
yd,s ? ?y?d,zd,s? ??cd,s,yd,s?1 [draw sentiment level]for each word after disc cue:
wd,s ? ?zd,s,yd,s [draw words]
Figure 1: The generative story for the joint model.
The components responsible for modeling dis-
course information are emphasized in red: when
dropped, one is left with the discourse-agnostic
model.
Unfortunately, the use of the PoE model pre-
vents us from marginalizing the parameters ex-
actly. Instead, as in Naseem et al (2009), we re-
sort to an approximation. We assume that zd,s and
yd,s are drawn twice; once from the document spe-
cific distribution and once from the discourse tran-
sition distributions. Under this simplification, we
can easily derive the conditional probabilities for
the collapsed Gibbs sampling.
5 Experiments
To the best of our knowledge, this is the first work
that aims at evaluating directly the joint informa-
tion of the sentiment and aspect assignment at the
sub-sentential level of full reviews; most existing
studies either focus on indirect evaluation of the
produced models (e.g., classifying the overall sen-
timent of sentences (Titov and McDonald, 2008a;
Brody and Elhadad, 2010) or even reviews (Naka-
gawa et al, 2010; Jo and Oh, 2011)) or evaluated
solely at the sentential or even document level.
Consequently, in order to evaluate our methods,
we created a new dataset which will be publicly
released.
Aspects Frequency
service 246
value 55
location 121
rooms 316
sleep quality 56
cleanliness 59
amenities 180
food 81
recommendation 121
rest 306
Total 1541
Table 2: Distribution of aspects in the data.
Dataset and Annotation The dataset we created
consists of 13559 hotel reviews from TripAdvi-
sor.com.6 Since our modeling is performed on the
EDU level, all sentences where segmented using
the SLSEG software package.7 As a result, our
dataset consists of 322,935 EDUs.
For creating the gold standard, 9 annotators an-
notated a random subset of our dataset (65 re-
views, 1541 EDUs). The annotators were pre-
sented with the whole review partitioned in EDUs
and were asked to annotate every EDU with the
aspect and sentiment (i.e. +1, 0 or ?1) it ex-
presses. Table 2 presents the distribution of as-
pects in the dataset. The distribution of the sen-
timents is uniform. The label rest captures cases
where EDUs do not refer to any aspect or to a very
rare aspect. The inter-annotator agreement (IAA),
as measured in terms of Cohen?s kappa score, was
66% for the aspect labeling, 70% for the sentiment
annotation and 61% for the joint task of sentiment
and aspect annotation. Though these scores may
not seem very high, they are similar to the ones re-
ported in related sentiment annotation efforts (see
e.g., Ganu et al (2009)).
Experimental setup In order to quantitatively
evaluate the model predictions, we run two sets of
experiments. In the first, we treat the task as an un-
supervised classification problem and evaluate the
output of the models directly against the gold stan-
dard annotation. This is a very challenging set-up,
as the model has no prior information about the
aspects defined (Table 2). In the second set of
experiments, we show that aspects and sentiments
induced by our model can be used to construct in-
formative features for supervised classification. In
6Downloadable from http://clic.cimec.
unitn.it/?angeliki.lazaridou/datasets/
ACL2013Sentiment.tar.gz
7www.sfu.ca/?mtaboada/research/SLSeg.
html1634
Model Precision Recall F1
Random 3.9 3.8 3.8
SentAsp 15.0 10.2 9.2
Discourse 16.5 13.8 10.8
Table 3: Results in terms of macro-averaged pre-
cision, recall and F1.
Model Unmarked Marked
SentAsp 9.2 5.4
Discourse 9.3 11.5
Table 4: Separate evaluation (F1) of the ?marked?
and the ?unmarked? EDUs.
all the cases, we compare the discourse-agnostic
and the discourse-informed models.
In order to induce the model, we let the sampler
run for 2000 iterations. We use the last sample to
define the labeling. The number of topics K was
set to 10 in order to match the number of aspects
defined in our annotation scheme (see Table 2).
The hyperpriors were chosen in a qualitative ex-
periment over a subset of our dataset by manually
inspecting the produced languages models. The
resulting values are: ? = 10?3, ? = 5 ? 10?4,
? = 5 ? 10?4, ? = 10?3, ?4 = 103, ?4? = 10?4,
?? = 85 and ??? = ?? = ??? = 5.
5.1 Direct clustering evaluation
Our labels encoding aspect and sentiment level can
be regarded as clusters. Consequently we can ap-
ply techniques developed in the context of cluster-
ing evaluation. We use a version of the standard
metrics considered for the word sense induction
task (Agirre and Soroa, 2007) where a clustering
is converted to a classification problem. This is
achieved by splitting the gold standard into two
subsets; the training portion is used to choose one-
to-one correspondence from the gold classes to the
induced clusters and then the chosen mapping is
applied to the testing portion. We perform 10-fold
cross validation and report precision, recall and F1
score. Our dataset is very skewed and the majority
class (rest) is arguably the least important, so we
use macro-averaging over labels and then average
those across folds to arrive to the reported num-
bers. We compare the discourse-informed model
(Discourse) against two baselines; the discourse-
agnostic SentAsp model and Random which as-
signs a random label to an EDU while respecting
the distribution of labels in the training set.
Table 3 presents the first analysis conducted on
the full set of EDUs. We observe that by incor-
porating latent discourse relation we improve per-
Content Aspect Polarity
1 but certainly off its greatness value neg
2 and while small they are nice rooms pos
3 but it is not free for all guests amenities neg
4 and the water was brown clean neg
5 and no tea making facilities rooms neg
6 when i checked out service pos
7 and if you do not service neg
8 when we got home clean neu
Table 5: Examples of EDUs where local informa-
tion is not sufficiently informative.
formance over the discourse-agnostic model Sen-
tAsp (statistically significant according to paired t-
test with p < 0.01). Note that fairly low scores in
this evaluation setting are expected for any unsu-
pervised model of sentiment and topics, as models
are unsupervised both in the aspect-specific senti-
ment and in topic labels and the total number of
labels is 28 (all aspects can be associated with the
3 sentiment levels except for rest which can only
be used with neutral (0) sentiment). Consequently,
induced topics, though informative (as we confirm
in Section 5.3), may not correspond to the topics
defined in the gold standard. For example, one
well-known property of LDA-style topic models
is their tendency to induce topics which account
for similar fraction of words in the dataset (Jagar-
lamudi et al, 2012), thus, over-splitting ?heavy?
topics (e.g. rooms in our case). The same, though
to lesser degree, is true for sentiment levels where
the border between neutral and positive (or nega-
tive) is also vaguely defined.
To gain insight into our model, we conducted
an experiment similar to the one presented in So-
masundaran et al (2009). We divide the dataset in
two subsets; one containing all EDUs starting with
a discourse cue (?marked?) and one containing the
remaining EDUs (?unmarked?). We hypothesize
that the effect of the discourse-aware model should
be stronger on the first subset, since the presence
of the connective indicates the possibility of a dis-
course relation with the previous EDU. The set of
discourse connectives is taken from the Penn Dis-
course Treebank (Prasad et al, 2008), thus creat-
ing a list of 240 potential connectives.
Table 5 presents a subset of ?marked? EDUs for
which trying to assign the sentiment and aspect
out of context (i.e. without the previous EDU) is
a difficult task. In examples 1-3 there is no ex-
plicit mention of the aspect. However, there is
an anaphoric expression (marked in bold) which1635
refers to a mention of the aspect in some previous
EDU. On the other hand, in examples 4 and 5 there
is an ambiguity in the choice of aspect; in example
5, tea making facilities can refer to a breakfast at
the hotel (label food) or to facilities in the room
(label rooms). Finally, examples 6-8 are too short
and not informative at all which indicates that the
segmentation tool does not always predict a de-
sirable segmentation. Consequently, automatic in-
duction of segmentation may be a better option.
Table 4 presents quantitative results of this anal-
ysis. Although the performance over the ?un-
marked? example is the same for the two mod-
els, this is not the case for the ?marked? instances
where the discourse-informed model leverages the
discourse signal and achieves better performance.
This behavior agrees with our initial hypothesis,
and suggests that our discourse representation,
though application-specific, relies in part on the
information encoded in linguistically-defined dis-
course cues. We will confirm this intuition in the
qualitative evaluation section. The increase for the
?marked? EDUs does not translate into greater dif-
ferences for the overall scores (Table 3) as marked
relations are considerably less frequent than un-
marked ones in our gold standard (i.e. 35% of the
EDUs are ?marked?). Nevertheless, this clearly
suggests that the discourse-informed model is in
fact capable of exploiting discourse signal.
5.2 Qualitative analysis
To investigate the quality of the induced discourse
structure, we present the most frequent discourse
cues extracted for every discourse relation. Ta-
ble 6 presents a selection of cues that best explain
the discourse relation they have been associated
with. A general observation is that among the cues
there are not only ?traditional? discourse connec-
tives like even though, although, and, but also cues
that are discriminative for the specific application.
In relation SameAlt we can mostly observe
phrases that tend to introduce a new aspect, since
an explicit mention of it is provided (e.g the loca-
tion is, the room was) and more specific phrases
like in addition are used to introduce a new aspect
with the same sentiment. However, these cues re-
veal important information about the aspect of the
EDU, and since they are associated with the lan-
guage model ??, they are not visible anymore to
the language model of aspects ?.
Cues for the relation AltSame also include
Discourse Discourse Cues
relation
SameAlt the location is , the room was, the hotel
has, and the room, and the bed, breakfast
was, the staff were, in addition, good luck
AltSame but, and, it was, and it was, and they, al-
though, and it, but it, but it was, however,
which was, this is, this was, they were,
the only thing, even though, unfortunately,
needless to say, fortunately
AltAlt the room was, the staff were, the only, the
hotel is, but the, however, also, or, overall
i, unfortunately, we will definitely, on the
plus, the only downside , even though, and
even though, i would definately
Table 6: Induced cues from the discourse relations
phrases that contain some anaphoric expressions,
which might refer to previous mentions of an as-
pect in the discourse (i.e. previous EDU). We ex-
pect that since there is an anaphoric expression,
explicit lexical features for the aspect will be miss-
ing, making thus the decision concerning aspect
assignment ambiguous for any discourse-agnostic
model. Interestingly, we found the expressions un-
fortunately, fortunately, the only thing in the same
relation, since all indicate a change in sentiment.
Finally, AltAlt can be viewed as a mixture of the
other two relations. Furthermore, for this relation
we can find expressions that tend to be used at the
end of a review, since at this point we normally
change the aspect and often even sentiment. Some
examples of these cases are overall, we will defi-
nitely and even the misspelled version of the latter
i would definately.
5.3 Features in supervised learning
As an additional experiment to demonstrate infor-
mative of the output of the two models, we de-
sign a supervised learning task of predicting sen-
timent and topic of EDUs. In this setting, the
feature vector of every EDU consists of its bag-
of-word-representation to which we add two extra
features; the models? predictions of topic and sen-
timent. We train a support vector machine with a
polynomial kernel using the default parameters of
Weka8 and perform 10-fold cross-validation.
Table 7 presents results of this analysis in terms
of accuracy for four classification tasks, i.e. pre-
dicting both sentiment and topic, only sentiment
and only topic for all EDUs, as well as predict-
ing sentiment and topic for the ?marked? dataset.
First, we observe that incorporation of the topic-
8http://www.cs.waikato.ac.nz/ml/weka/1636
Features aspect+sentiment aspect sentiment Marked only
(28 classes) (10 classes) (3 classes) sentiment+aspect (28 classes)
only unigrams 36.3 49.8 57.1 26.2
unigrams + SentAsp 38.0 50.4 59.3 27.8
unigrams + Discourse 39.1 52.4 59.4 29.1
Table 7: Supervised learning at the EDU level (accuracy)
model features on a unigram-only model results
in an improvement in classification performance
across all tasks (predicting sentiment, predicting
aspects, or both); as a matter of fact, our accu-
racy results for predicting sentiment are compa-
rable to the sentence-level results presented by
Ta?ckstro?m and McDonald (2011). We have to
stress that accuracies for the joint task (i.e. pre-
dicting both sentiment and topic) are expected to
be lower since it can also be seen as the product
of the two other tasks (i.e. predicting only senti-
ment and only topic). We also observe that the fea-
tures induced from the Discourse model result in
higher accuracy than the ones from the discourse-
agnostic model SentAsp both in the complete set
of EDUs and the ?marked? subset, results that are
in line with the ones presented in Table 4. Fi-
nally, the fact that the results for the complete set
of EDUs are higher than the ones for the ?marked?
dataset clearly suggests that the latter constitute a
hard case for sentiment analysis, in which exploit-
ing discourse signal proves to be beneficial.
6 Related Work
Recently, there has been significant interest in
leveraging content structure for a number of NLP
tasks (Webber et al, 2011). Sentiment analysis
has not been an exception to this and discourse has
been used in order to enforce constraints on the
assignment of polarity labels at several granular-
ity levels, ranging from the lexical level (Polanyi
and Zaenen, 2006) to the review level (Taboada
et al, 2011). One way to deal with this prob-
lem is to model the interactions by using a pre-
compiled set of polarity shifters (Nakagawa et al,
2010; Polanyi and Zaenen, 2006; Sadamitsu et al,
2008). Socher et al (2011) defined a recurrent
neural network model, which, in essence, learns
those polarity shifters relying on sentence-level
sentiment labels. Though successful, this model is
unlikely to capture intra-sentence non-local phe-
nomena such as effect of discourse connectives,
unless it is provided with syntactic information
as an input. This may be problematic for the
noisy sentiment-analysis domain and especially
for poor-resource languages. Similar to our work,
others have focused on modeling interactions be-
tween phrases and sentences. However, this has
been achieved by either using a subset of relations
that can be found in discourse theories (Zhou et
al., 2011; Asher et al, 2008; Snyder and Barzi-
lay, 2007) or by using directly (Taboada et al,
2008) the output of discourse parsers (Soricut and
Marcu, 2003). Discourse cues as predictive fea-
tures of topic boundaries have also been consid-
ered in Eisenstein and Barzilay (2008). This work
was extended by Trivedi and Eisenstein (2013),
where discourse connectors are used as features
for modeling subjectivity transitions.
Another related line of research was presented
in Somasundaran et al (2009) where a domain-
specific discourse scheme is considered. Simi-
larly to our set-up, discourse relations enforce con-
straints on sentiment polarity of associated sen-
timent expressions. Somasundaran et al (2009)
show that gold-standard discourse information en-
coded in this way provides a useful signal for pre-
diction of sentiment, but they leave automatic dis-
course relation prediction for future work. They
use an integer linear programming framework to
enforce agreement between classifiers and soft
constraints provided by discourse annotations.
This contrasts with our work; we do not rely on
expert discourse annotation, but rather induce both
discourse relations and cues jointly with aspect
and sentiment.
7 Conclusions and Future Work
In this work, we showed that by jointly induc-
ing discourse information in the form of discourse
cues, we can achieve better predictions for aspect-
specific sentiment polarity. Our contribution con-
sists in proposing a general way of how discourse
information can be integrated in any LDA-style
discourse-agnostic model of aspect and sentiment.
In the future, we aim at modeling more flexible
sets of discourse relations and automatically in-
ducing discourse segmentation relevant to the task.
1637
References
Eneko Agirre and Aitor Soroa. 2007. Semeval-2007
task 02: Evaluating word sense induction and dis-
crimination systems. In Proceedings of the Se-
mEval, pages 7?12.
Nicholas Asher, Farah Benamara, and Yvette Yannick
Mathieu. 2008. Distilling opinion in discourse: A
preliminary study. Proceedings of Coling, pages 5?
8.
Samuel Brody and Noemie Elhadad. 2010. An unsu-
pervised aspect-sentiment model for online reviews.
In Proceedings of NAACL, pages 804?812.
Jacob Eisenstein and Regina Barzilay. 2008. Bayesian
unsupervised topic segmentation. In Proceedings of
EMNLP, pages 334?343.
Emily B Fox, Erik B Sudderth, Michael I Jordan, and
Alan S Willsky. 2008. An HDP-HMM for systems
with state persistence. In Proceedings of ICML.
Gayatree Ganu, Noemie Elhadad, and Amelie Marian.
2009. Beyond the stars: Improving rating predic-
tions using review text content. In Proceedings of
WebDB.
Sharon Goldwater, Thomas L Griffiths, and Mark John-
son. 2009. A bayesian framework for word segmen-
tation: Exploring the effects of context. Cognition,
112(1):21?54.
Thomas L Griffiths and Mark Steyvers. 2004. Find-
ing scientific topics. Proceedings of the National
Academy of Sciences of the United States of Amer-
ica, 101(Suppl 1):5228?5235.
Geoffrey E Hinton. 1999. Products of experts. In Pro-
ceedings of ICANN, volume 1, pages 1?6.
Minqing Hu and Bing Liu. 2004. Mining and sum-
marizing customer reviews. In Proceedings of ACM
SIGKDD, pages 168?177.
Jagadeesh Jagarlamudi, Hal Daume? III, and Raghaven-
dra Udupa. 2012. Incorporating lexical priors into
topic models. Proceedings of EACL, pages 204?
213.
Yohan Jo and Alice H Oh. 2011. Aspect and senti-
ment unification model for online review analysis.
In Proceedings of WSDM, pages 815?824.
Chenghua Lin and Yulan He. 2009. Joint senti-
ment/topic model for sentiment analysis. In Pro-
ceeding of CIKM, pages 375?384.
William C Mann and Sandra A Thompson. 1988.
Rhetorical structure theory: Toward a functional the-
ory of text organization. Text, 8(3):243?281.
Qiaozhu Mei, Xu Ling, Matthew Wondra, Hang Su,
and ChengXiang Zhai. 2007. Topic sentiment mix-
ture: modeling facets and opinions in weblogs. In
Proceedings of WWW, pages 171?180.
Tetsuji Nakagawa, Kentaro Inui, and Sadao Kurohashi.
2010. Dependency tree-based sentiment classifica-
tion using crfs with hidden variables. In Proceed-
ings of NAACL, pages 786?794.
Tahira Naseem, Benjamin Snyder, Jacob Eisen-
stein, and Regina Barzilay. 2009. Multilin-
gual part-of-speech tagging: Two unsupervised ap-
proaches. Journal of Artificial Intelligence Re-
search, 36(1):341?385.
Livia Polanyi and Annie Zaenen. 2006. Contextual
valence shifters. Computing attitude and affect in
text: Theory and applications, pages 1?10.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, Livio Robaldo, Aravind Joshi, and Bonnie
Webber. 2008. The penn discourse treebank 2.0. In
Proceedings of LREC.
Kugatsu Sadamitsu, Satoshi Sekine, and Mikio Ya-
mamoto. 2008. Sentiment analysis based on proba-
bilistic models using inter-sentence information. In
Proceedings of ACL, pages 2892?2896.
Benjamin Snyder and Regina Barzilay. 2007. Multiple
aspect ranking using the good grief algorithm. In
Proceedings of HLT-NAACL, pages 300?307.
Richard Socher, Jeffrey Pennington, Eric H Huang,
Andrew Y Ng, and Christopher D Manning. 2011.
Semi-supervised recursive autoencoders for predict-
ing sentiment distributions. In Proceedings of
EMNLP, pages 151?161.
Swapna Somasundaran, Janyce Wiebe, and Josef Rup-
penhofer. 2008. Discourse level opinion interpreta-
tion. In Proceedings of Coling, pages 801?808.
Swapna Somasundaran, Galileo Namata, Janyce
Wiebe, and Lise Getoor. 2009. Supervised and
unsupervised methods in employing discourse rela-
tions for improving opinion polarity classification.
In Proceedings of EMNLP, pages 170?179.
Radu Soricut and Daniel Marcu. 2003. Sentence level
discourse parsing using syntactic and lexical infor-
mation. In Proceedings of NAACL, pages 149?156.
Maite Taboada, Kimberly Voll, and Julian Brooke.
2008. Extracting sentiment as a function of dis-
course structure and topicality. Simon Fraser Uni-
versity, Tech. Rep, 20.
Maite Taboada, Julian Brooke, Milan Tofiloski, Kim-
berly Voll, and Manfred Stede. 2011. Lexicon-
based methods for sentiment analysis. Computa-
tional Linguistics, 37(2):267?307.
Oscar Ta?ckstro?m and Ryan McDonald. 2011. Semi-
supervised latent variable models for sentence-level
sentiment analysis. In Proceedings of ACL, pages
569?574.
Ivan Titov and Ryan McDonald. 2008a. A joint model
of text and aspect ratings for sentiment summariza-
tion. In Proceedings of ACL, pages 308?316.1638
Ivan Titov and Ryan McDonald. 2008b. Modeling on-
line reviews with multi-grain topic models. In Pro-
ceedings of WWW, pages 112?120.
Rakshit Trivedi and Jacob Eisenstein. 2013. Discourse
connectors for latent subjectivity in sentiment anal-
ysis. In In Proceedings of NAACL.
Peter D Turney and Michael L Littman. 2002. Un-
supervised learning of semantic orientation from a
hundred-billion-word corpus.
Kimberly Voll and Maite Taboada. 2007. Not all
words are created equal: Extracting semantic orien-
tation as a function of adjective relevance. In Pro-
ceedings of Australian Conf. on AI.
Bonnie Webber, Markus Egg, and Valia Kordoni.
2011. Discourse structure and language technology.
Natural Language Engineering, 1(1):1?54.
Lanjun Zhou, Binyang Li, Wei Gao, Zhongyu Wei,
and Kam-Fai Wong. 2011. Unsupervised discovery
of discourse relations for eliminating intra-sentence
polarity ambiguities. In Proceedings EMNLP, pages
162?171.
1639
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 579?585,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Cross-lingual Model Transfer Using Feature Representation Projection
Mikhail Kozhevnikov
MMCI, University of Saarland
Saarbr?ucken, Germany
mkozhevn@mmci.uni-saarland.de
Ivan Titov
ILLC, University of Amsterdam
Amsterdam, Netherlands
titov@uva.nl
Abstract
We propose a novel approach to cross-
lingual model transfer based on feature
representation projection. First, a com-
pact feature representation relevant for the
task in question is constructed for either
language independently and then the map-
ping between the two representations is
determined using parallel data. The tar-
get instance can then be mapped into
the source-side feature representation us-
ing the derived mapping and handled di-
rectly by the source-side model. This ap-
proach displays competitive performance
on model transfer for semantic role label-
ing when compared to direct model trans-
fer and annotation projection and suggests
interesting directions for further research.
1 Introduction
Cross-lingual model transfer approaches are con-
cerned with creating statistical models for var-
ious tasks for languages poor in annotated re-
sources, utilising resources or models available
for these tasks in other languages. That includes
approaches such as direct model transfer (Ze-
man and Resnik, 2008) and annotation projec-
tion (Yarowsky et al, 2001). Such methods have
been successfully applied to a variety of tasks,
including POS tagging (Xi and Hwa, 2005; Das
and Petrov, 2011; T?ackstr?om et al, 2013), syntac-
tic parsing (Ganchev et al, 2009; Smith and Eis-
ner, 2009; Hwa et al, 2005; Durrett et al, 2012;
S?gaard, 2011), semantic role labeling (Pad?o and
Lapata, 2009; Annesi and Basili, 2010; Tonelli
and Pianta, 2008; Kozhevnikov and Titov, 2013)
and others.
Direct model transfer attempts to find a shared
feature representation for samples from the two
languages, usually generalizing and abstract-
ing away from language-specific representations.
Once this is achieved, instances from both lan-
guages can be mapped into this space and a model
trained on the source-language data directly ap-
plied to the target language. If parallel data is
available, it can be further used to enforce model
agreement on this data to adjust for discrepancies
between the two languages, for example by means
of projected transfer (McDonald et al, 2011).
The shared feature representation depends on
the task in question, but most often each aspect
of the original feature representation is handled
separately. Word types, for example, may be re-
placed by cross-lingual word clusters (T?ackstr?om
et al, 2012) or cross-lingual distributed word rep-
resentations (Klementiev et al, 2012). Part-of-
speech tags, which are often language-specific,
can be converted into universal part-of-speech
tags (Petrov et al, 2012) and morpho-syntactic
information can also be represented in a unified
way (Zeman et al, 2012; McDonald et al, 2013;
Tsarfaty, 2013). Unfortunately, the design of such
representations and corresponding conversion pro-
cedures is by no means trivial.
Annotation projection, on the other hand, does
not require any changes to the feature represen-
tation. Instead, it operates on translation pairs,
usually on sentence level, applying the available
source-side model to the source sentence and
transferring the resulting annotations through the
word alignment links to the target one. The quality
of predictions on source sentences depends heav-
ily on the quality of parallel data and the domain
it belongs to (or, rather, the similarity between this
domain and that of the corpus the source-language
model was trained on). The transfer itself also
introduces errors due to translation shifts (Cyrus,
2006) and word alignment errors, which may lead
to inaccurate predictions. These issues are gen-
erally handled using heuristics (Pad?o and Lapata,
2006) and filtering, for example based on align-
ment coverage (van der Plas et al, 2011).
579
Figure 1: Dependency-based semantic role labeling example. The top arcs depict dependency relations,
the bottom ones ? semantic role structure. Rendered with https://code.google.com/p/whatswrong/.
1.1 Motivation
The approach proposed here, which we will refer
to as feature representation projection (FRP), con-
stitutes an alternative to direct model transfer and
annotation projection and can be seen as a com-
promise between the two.
It is similar to direct transfer in that we also
use a shared feature representation. Instead of
designing this representation manually, however,
we create compact monolingual feature represen-
tations for source and target languages separately
and automatically estimate the mapping between
the two from parallel data. This allows us to make
use of language-specific annotations and account
for the interplay between different types of infor-
mation. For example, a certain preposition at-
tached to a token in the source language might
map into a morphological tag in the target lan-
guage, which would be hard to handle for tradi-
tional direct model transfer other than using some
kind of refinement procedure involving parallel
data. Note also that any such refinement procedure
applicable to direct transfer would likely work for
FRP as well.
Compared to annotation projection, our ap-
proach may be expected to be less sensitive to par-
allel data quality, since we do not have to com-
mit to a particular prediction on a given instance
from parallel data. We also believe that FRP
may profit from using other sources of informa-
tion about the correspondence between source and
target feature representations, such as dictionary
entries, and thus have an edge over annotation pro-
jection in those cases where the amount of parallel
data available is limited.
2 Evaluation
We evaluate feature representation projection on
the task of dependency-based semantic role label-
ing (SRL) (Haji?c et al, 2009).
This task consists in identifying predicates and
their arguments in sentences and assigning each
argument a semantic role with respect to its pred-
icate (see figure 1). Note that only a single word
? the syntactic head of the argument phrase ? is
marked as an argument in this case, as opposed
to constituent- or span-based SRL (Carreras and
M`arquez, 2005). We focus on the assignment of
semantic roles to identified arguments.
For the sake of simplicity we cast it as a multi-
class classification problem, ignoring the interac-
tion between different arguments in a predicate. It
is well known that such interaction plays an impor-
tant part in SRL (Punyakanok et al, 2008), but it
is not well understood which kinds of interactions
are preserved across languages and which are not.
Also, should one like to apply constraints on the
set of semantic roles in a given predicate, or, for
example, use a reranker (Bj?orkelund et al, 2009),
this can be done using a factorized model obtained
by cross-lingual transfer.
In our setting, each instance includes the word
type and part-of-speech and morphological tags (if
any) of argument token, its parent and correspond-
ing predicate token, as well as their dependency
relations to their respective parents. This repre-
sentation is further denoted ?
0
.
2.1 Approach
We consider a pair of languages (L
s
, L
t
) and
assume that an annotated training set D
s
T
=
{(x
s
, y
s
)} is available in the source language as
well as a parallel corpus of instance pairs D
st
=
{(
x
s
, x
t
)}
and a target dataset D
t
E
=
{
x
t
}
that
needs to be labeled.
We design a pair of intermediate compact
monolingual feature representations ?
s
1
and ?
t
1
and models M
s
and M
t
to map source and target
samples x
s
and x
t
from their original representa-
tions, ?
s
0
and ?
t
0
, to the new ones. We use the par-
580
allel instances in the new feature representation
?
D
st
=
{(
x
s
1
, x
t
1
)}
=
{(
M
s
(x
s
),M
t
(x
t
)
)}
to determine the mapping M
ts
(usually, linear) be-
tween the two spaces:
M
ts
= argmax
M
?
(x
s
1
,x
t
1
?
?
D
st
)
?
?
?
x
s
1
?M(x
t
1
)
?
?
?
2
Then a classification model M
y
is trained on the
source training data
?
D
s
T
= {(x
s
1
, y
s
)} = {(M
s
(x
s
), y
s
)}
and the labels are assigned to the target samples
x
t
? D
t
E
using a composition of the models:
y
t
= M
y
(M
ts
(M
t
(x
t
)))
2.2 Feature Representation
Our objective is to make the feature represen-
tation sufficiently compact that the mapping be-
tween source and target feature spaces could be
reliably estimated from a limited amount of paral-
lel data, while preserving, insofar as possible, the
information relevant for classification.
Estimating the mapping directly from raw cat-
egorical features (?
0
) is both computationally ex-
pensive and likely inaccurate ? using one-hot en-
coding the feature vectors in our experiments
would have tens of thousands of components.
There is a number of ways to make this repre-
sentation more compact. To start with, we re-
place word types with corresponding neural lan-
guage model representations estimated using the
skip-gram model (Mikolov et al, 2013a). This
corresponds to M
s
and M
t
above and reduces the
dimension of the feature space, making direct es-
timation of the mapping practical. We will refer to
this representation as ?
1
.
To go further, one can, for example, apply
dimensionality reduction techniques to obtain a
more compact representation of ?
1
by eliminating
redundancy or define auxiliary tasks and produce
a vector representation useful for those tasks. In
source language, one can even directly tune an in-
termediate representation for the target problem.
2.3 Baselines
As mentioned above we compare the performance
of this approach to that of direct transfer and an-
notation projection. Both baselines are using the
same set of features as the proposed model, as de-
scribed earlier.
The shared feature representation for di-
rect transfer is derived from ?
0
by replacing
language-specific part-of-speech tags with univer-
sal ones (Petrov et al, 2012) and adding cross-
lingual word clusters (T?ackstr?om et al, 2012) to
word types. The word types themselves are left as
they are in the source language and replaced with
their gloss translations in the target one (Zeman
and Resnik, 2008). In English-Czech and Czech-
English we also use the dependency relation infor-
mation, since the annotations are partly compati-
ble.
The annotation projection baseline implementa-
tion is straightforward. The source-side instances
from a parallel corpus are labeled using a classi-
fier trained on source-language training data and
transferred to the target side. The resulting anno-
tations are then used to train a target-side classifier
for evaluation. Note that predicate and argument
identification in both languages is performed us-
ing monolingual classifiers and only aligned pairs
are used in projection. A more common approach
would be to project the whole structure from the
source language, but in our case this may give
unfair advantage to feature representation projec-
tion, which relies on target-side argument identifi-
cation.
2.4 Tools
We use the same type of log-linear classifiers
in the model itself and the two baselines to
avoid any discrepancy due to learning proce-
dure. These classifiers are implemented using
PYLEARN2 (Goodfellow et al, 2013), based on
THEANO (Bergstra et al, 2010). We also use this
framework to estimate the linear mapping M
ts
be-
tween source and target feature spaces in FRP.
The 250-dimensional word representations for
?
1
are obtained using WORD2VEC tool. Both
monolingual data and that from the parallel cor-
pus are included in the training. In Mikolov et al
(2013b) the authors consider embeddings of up to
800 dimensions, but we would not expect to bene-
fit as much from larger vectors since we are using
a much smaller corpus to train them. We did not
tune the size of the word representation to our task,
as this would not be appropriate in a cross-lingual
transfer setup, but we observe that the classifier
is relatively robust to their dimension when evalu-
581
ated on source language ? in our experiments the
performance of the monolingual classifier does not
improve significantly if the dimension is increased
past 300 and decreases only by a small margin
(less than one absolute point) if it is reduced to
100. It should be noted, however, that the dimen-
sion that is optimal in this sense is not necessarily
the best choice for FRP, especially if the amount
of available parallel data is limited.
2.5 Data
We use two language pairs for evaluation:
English-Czech and English-French. In the first
case, the data is converted from Prague Czech-
English Dependency Treebank 2.0 (Haji?c et al,
2012) using the script from Kozhevnikov and
Titov (2013). In the second, we use CoNLL 2009
shared task (Haji?c et al, 2009) corpus for English
and the manually corrected dataset from van der
Plas et al (2011) for French. Since the size of
the latter dataset is relatively small ? one thou-
sand sentences ? we reserve the whole dataset for
testing and only evaluate transfer from English to
French, but not the other way around. Datasets for
other languages are sufficiently large, so we take
30 thousand samples for testing and use the rest
as training data. The validation set in each exper-
iment is withheld from the corresponding training
corpus and contains 10 thousand samples.
Parallel data for both language pairs is de-
rived from Europarl (Koehn, 2005), which we pre-
process using MATE-TOOLS (Bj?orkelund et al,
2009; Bohnet, 2010).
3 Results
The classification error of FRP and the baselines
given varying amount of parallel data is reported
in figures 2, 3 and 4. The training set for each
language is fixed. We denote the two baselines AP
(annotation projection) and DT (direct transfer).
The number of parallel instances in these exper-
iments is shown on a logarithmic scale, the values
considered are 2, 5, 10, 20 and 50 thousand pairs.
Please note that we report only a single value
for direct transfer, since this approach does not ex-
plicitly rely on parallel data. Although some of
the features ? namely, gloss translations and cross-
lingual clusters ? used in direct transfer are, in fact,
derived from parallel data, we consider the effect
of this on the performance of direct transfer to be
indirect and outside the scope of this work.
2 5 10 20 50
0.34
0.36
0.38
0.40
0.42
Number of parallel instances, ?10
3
Error
FRP
AP
DT
Figure 2: English-Czech transfer results
2 5 10 20 50
0.32
0.34
0.36
0.38
0.40
Number of parallel instances, ?10
3
Error
FRP
AP
DT
Figure 3: Czech-English transfer results
The rather inferior performance of direct trans-
fer baseline on English-French may be partially
attributed to the fact that it cannot rely on depen-
dency relation features, as the corpora we consider
make use of different dependency relation inven-
tories. Replacing language-specific dependency
annotations with the universal ones (McDonald
et al, 2013) may help somewhat, but we would
still expect the methods directly relying on paral-
lel data to achieve better results given a sufficiently
large parallel corpus.
Overall, we observe that the proposed method
with ?
1
representation demonstrates performance
competitive to direct transfer and annotation pro-
jection baselines.
582
2 5 10 20 50
0.34
0.36
0.38
0.40
Number of parallel instances, ?10
3
Error
FRP
AP
DT
Figure 4: English-French transfer results
4 Additional Related Work
Apart from the work on direct/projected transfer
and annotation projection mentioned above, the
proposed method can be seen as a more explicit
kind of domain adaptation, similar to Titov (2011)
or Blitzer et al (2006).
It is also somewhat similar in spirit to Mikolov
et al (2013b), where a small number of word
translation pairs are used to estimate a mapping
between distributed representations of words in
two different languages and build a word transla-
tion model.
5 Conclusions
In this paper we propose a new method of cross-
lingual model transfer, report initial evaluation re-
sults and highlight directions for its further devel-
opment.
We observe that the performance of this method
is competitive with that of established cross-
lingual transfer approaches and its application re-
quires very little manual adjustment ? no heuris-
tics or filtering and no explicit shared feature rep-
resentation design. It also retains compatibility
with any refinement procedures similar to pro-
jected transfer (McDonald et al, 2011) that may
have been designed to work in conjunction with
direct model transfer.
6 Future Work
This paper reports work in progress and there is
a number of directions we would like to pursue
further.
Better Monolingual Representations The rep-
resentation we used in the initial evaluation does
not discriminate between aspects that are relevant
for the assignment of semantic roles and those that
are not. Since we are using a relatively small set of
features to start with, this does not present much of
a problem. In general, however, retaining only rel-
evant aspects of intermediate monolingual repre-
sentations would simplify the estimation of map-
ping between them and make FRP more robust.
For source language, this is relatively straight-
forward, as the intermediate representation can be
directly tuned for the problem in question using
labeled training data. For target language, how-
ever, we assume that no labeled data is available
and auxiliary tasks have to be used to achieve this.
Alternative Sources of Information The
amount of parallel data available for many
language pairs is growing steadily. However,
cross-lingual transfer methods are often applied
in cases where parallel resources are scarce or of
poor quality and must be used with care. In such
situations an ability to use alternative sources of
information may be crucial. Potential sources
of such information include dictionary entries or
information about the mapping between certain
elements of syntactic structure, for example a
known part-of-speech tag mapping.
The available parallel data itself may also be
used more comprehensively ? aligned arguments
of aligned predicates, for example, constitute only
a small part of it, while the mapping of vector rep-
resentations of individual tokens is likely to be the
same for all aligned words.
Multi-source Transfer One of the strong points
of direct model transfer is that it naturally fits the
multi-source transfer setting. There are several
possible ways of adapting FRP to such a setting.
It remains to be seen which one would produce
the best results and how multi-source feature rep-
resentation projection would compare to, for ex-
ample, multi-source projected transfer (McDonald
et al, 2011).
Acknowledgements
The authors would like to acknowledge the
support of MMCI Cluster of Excellence and
Saarbr?ucken Graduate School of Computer Sci-
ence and thank the anonymous reviewers for their
suggestions.
583
References
Paolo Annesi and Roberto Basili. 2010. Cross-lingual
alignment of FrameNet annotations through hidden
Markov models. In Proceedings of the 11
th
interna-
tional conference on Computational Linguistics and
Intelligent Text Processing, CICLing?10, pages 12?
25. Springer-Verlag.
James Bergstra, Olivier Breuleux, Fr?ed?eric Bastien,
Pascal Lamblin, Razvan Pascanu, Guillaume Des-
jardins, Joseph Turian, David Warde-Farley, and
Yoshua Bengio. 2010. Theano: a CPU and
GPU math expression compiler. In Proceedings
of the Python for Scientific Computing Conference
(SciPy), Austin, TX.
Anders Bj?orkelund, Love Hafdell, and Pierre Nugues.
2009. Multilingual semantic role labeling. In Pro-
ceedings of the Thirteenth Conference on Computa-
tional Natural Language Learning (CoNLL 2009):
Shared Task, pages 43?48, Boulder, Colorado, June.
Association for Computational Linguistics.
John Blitzer, Ryan McDonal, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In Proc. Conference on Empirical
Methods in Natural Language Processing, Sydney,
Australia.
Bernd Bohnet. 2010. Top accuracy and fast depen-
dency parsing is not a contradiction. In Proceedings
of the 23
rd
International Conference on Computa-
tional Linguistics (Coling 2010), pages 89?97, Bei-
jing, China, August.
Xavier Carreras and Llu??s M`arquez. 2005. Introduc-
tion to the CoNLL-2005 shared task: Semantic role
labeling. In Proceedings of CoNLL-2005, Ann Ar-
bor, MI USA.
Lea Cyrus. 2006. Building a resource for studying
translation shifts. CoRR, abs/cs/0606096.
Dipanjan Das and Slav Petrov. 2011. Unsuper-
vised part-of-speech tagging with bilingual graph-
based projections. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies, pages
600?609, Portland, Oregon, USA, June. Association
for Computational Linguistics.
Greg Durrett, Adam Pauls, and Dan Klein. 2012. Syn-
tactic transfer using a bilingual lexicon. In Pro-
ceedings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning, pages 1?11,
Jeju Island, Korea, July. Association for Computa-
tional Linguistics.
Kuzman Ganchev, Jennifer Gillenwater, and Ben
Taskar. 2009. Dependency grammar induction via
bitext projection constraints. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP, pages
369?377, Suntec, Singapore, August. Association
for Computational Linguistics.
Ian J. Goodfellow, David Warde-Farley, Pascal Lam-
blin, Vincent Dumoulin, Mehdi Mirza, Razvan Pas-
canu, James Bergstra, Fr?ed?eric Bastien, and Yoshua
Bengio. 2013. Pylearn2: a machine learning re-
search library. CoRR, abs/1308.4214.
Jan Haji?c, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Ant`onia Mart??, Llu??s
M`arquez, Adam Meyers, Joakim Nivre, Sebastian
Pad?o, Jan
?
St?ep?anek, Pavel Stra?n?ak, Mihai Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The CoNLL-
2009 shared task: Syntactic and semantic dependen-
cies in multiple languages. In Proceedings of the
Thirteenth Conference on Computational Natural
Language Learning (CoNLL 2009): Shared Task,
pages 1?18, Boulder, Colorado.
Jan Haji?c, Eva Haji?cov?a, Jarmila Panevov?a, Petr
Sgall, Ond?rej Bojar, Silvie Cinkov?a, Eva Fu?c??kov?a,
Marie Mikulov?a, Petr Pajas, Jan Popelka, Ji?r??
Semeck?y, Jana
?
Sindlerov?a, Jan
?
St?ep?anek, Josef
Toman, Zde?nka Ure?sov?a, and Zden?ek
?
Zabokrtsk?y.
2012. Announcing Prague Czech-English depen-
dency treebank 2.0. In Nicoletta Calzolari (Con-
ference Chair), Khalid Choukri, Thierry Declerck,
Mehmet U?gur Do?gan, Bente Maegaard, Joseph Mar-
iani, Jan Odijk, and Stelios Piperidis, editors, Pro-
ceedings of the Eight International Conference on
Language Resources and Evaluation (LREC?12), Is-
tanbul, Turkey, May. European Language Resources
Association (ELRA).
Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara
Cabezas, and Okan Kolak. 2005. Bootstrapping
parsers via syntactic projection across parallel text.
Natural Language Engineering, 11(3):311?325.
Alexandre Klementiev, Ivan Titov, and Binod Bhat-
tarai. 2012. Inducing crosslingual distributed rep-
resentations of words. In Proceedings of the Inter-
national Conference on Computational Linguistics
(COLING), Bombay, India.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In Conference Pro-
ceedings: the tenth Machine Translation Summit,
pages 79?86, Phuket, Thailand. AAMT.
Mikhail Kozhevnikov and Ivan Titov. 2013. Cross-
lingual transfer of semantic role labeling models.
In Proceedings of the 51st Annual Meeting of the
Association for Computational Linguistics (Volume
1: Long Papers), pages 1190?1200, Sofia, Bulgaria,
August. Association for Computational Linguistics.
Ryan McDonald, Slav Petrov, and Keith Hall. 2011.
Multi-source transfer of delexicalized dependency
parsers. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
EMNLP ?11, pages 62?72, Edinburgh, United King-
dom. Association for Computational Linguistics.
584
Ryan McDonald, Joakim Nivre, Yvonne Quirmbach-
Brundage, Yoav Goldberg, Dipanjan Das, Kuz-
man Ganchev, Keith Hall, Slav Petrov, Hao
Zhang, Oscar T?ackstr?om, Claudia Bedini, N?uria
Bertomeu Castell?o, and Jungmee Lee. 2013. Uni-
versal dependency annotation for multilingual pars-
ing. In Proceedings of the 51st Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 2: Short Papers), pages 92?97, Sofia, Bulgaria,
August. Association for Computational Linguistics.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient estimation of word represen-
tations in vector space. CoRR, abs/1301.3781.
Tomas Mikolov, Quoc V. Le, and Ilya Sutskever.
2013b. Exploiting similarities among languages for
machine translation. CoRR, abs/1309.4168.
Sebastian Pad?o and Mirella Lapata. 2006. Optimal
constituent alignment with edge covers for semantic
projection. In Proc. 44
th
Annual Meeting of Associ-
ation for Computational Linguistics and 21
st
Inter-
national Conf. on Computational Linguistics, ACL-
COLING 2006, pages 1161?1168, Sydney, Aus-
tralia.
Sebastian Pad?o and Mirella Lapata. 2009. Cross-
lingual annotation projection for semantic roles.
Journal of Artificial Intelligence Research, 36:307?
340.
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012.
A universal part-of-speech tagset. In Proceedings of
LREC, May.
Vasin Punyakanok, Dan Roth, and Wen-tau Yih. 2008.
The importance of syntactic parsing and inference in
semantic role labeling. Computational Linguistics,
34(2):257?287.
David A Smith and Jason Eisner. 2009. Parser adap-
tation and projection with quasi-synchronous gram-
mar features. In Proceedings of the 2009 Confer-
ence on Empirical Methods in Natural Language
Processing, pages 822?831. Association for Com-
putational Linguistics.
Anders S?gaard. 2011. Data point selection for cross-
language adaptation of dependency parsers. In Pro-
ceedings of the 49
th
Annual Meeting of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies, volume 2 of HLT ?11, pages
682?686, Portland, Oregon. Association for Com-
putational Linguistics.
Oscar T?ackstr?om, Ryan McDonald, and Jakob Uszko-
reit. 2012. Cross-lingual word clusters for direct
transfer of linguistic structure. In Proc. of the An-
nual Meeting of the North American Association
of Computational Linguistics (NAACL), pages 477?
487, Montr?eal, Canada.
Oscar T?ackstr?om, Dipanjan Das, Slav Petrov, Ryan
McDonald, and Joakim Nivre. 2013. Token and
type constraints for cross-lingual part-of-speech tag-
ging. Transactions of the Association for Computa-
tional Linguistics, 1:1?12.
Ivan Titov. 2011. Domain adaptation by constraining
inter-domain variability of latent feature representa-
tion. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 62?71, Portland,
Oregon, USA, June. Association for Computational
Linguistics.
Sara Tonelli and Emanuele Pianta. 2008. Frame infor-
mation transfer from English to Italian. In Proceed-
ings of LREC 2008.
Reut Tsarfaty. 2013. A unified morpho-syntactic
scheme of stanford dependencies. In Proceedings of
the 51st Annual Meeting of the Association for Com-
putational Linguistics (Volume 2: Short Papers),
pages 578?584, Sofia, Bulgaria, August. Associa-
tion for Computational Linguistics.
Lonneke van der Plas, Paola Merlo, and James Hen-
derson. 2011. Scaling up automatic cross-lingual
semantic role annotation. In Proceedings of the 49
th
Annual Meeting of the Association for Computa-
tional Linguistics: Human Language Technologies,
HLT ?11, pages 299?304, Portland, Oregon, USA.
Association for Computational Linguistics.
Chenhai Xi and Rebecca Hwa. 2005. A backoff
model for bootstrapping resources for non-english
languages. In Proceedings of Human Language
Technology Conference and Conference on Empiri-
cal Methods in Natural Language Processing, pages
851?858, Vancouver, British Columbia, Canada,
October. Association for Computational Linguistics.
David Yarowsky, Grace Ngai, and Richard Wicen-
towski. 2001. Inducing multilingual text analysis
tools via robust projection across aligned corpora.
In Proceedings of the first international conference
on Human language technology research, pages 1?
8. Association for Computational Linguistics.
Daniel Zeman and Philip Resnik. 2008. Cross-
language parser adaptation between related lan-
guages. In Proceedings of the IJCNLP-08 Workshop
on NLP for Less Privileged Languages, pages 35?
42, Hyderabad, India, January. Asian Federation of
Natural Language Processing.
Daniel Zeman, David Mare?cek, Martin Popel,
Loganathan Ramasamy, Jan
?
St?ep?anek, Zden?ek
?
Zabokrtsk?y, and Jan Haji?c. 2012. Hamledt: To
parse or not to parse? In Nicoletta Calzolari (Con-
ference Chair), Khalid Choukri, Thierry Declerck,
Mehmet U?gur Do?gan, Bente Maegaard, Joseph Mar-
iani, Jan Odijk, and Stelios Piperidis, editors, Pro-
ceedings of the Eight International Conference on
Language Resources and Evaluation (LREC?12), Is-
tanbul, Turkey, may. European Language Resources
Association (ELRA).
585
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
and the Shared Task, pages 317?327, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational Linguistics
Bootstrapping Semantic Role Labelers from Parallel Data
Mikhail Kozhevnikov Ivan Titov
Saarland University, Postfach 15 11 50
66041 Saarbru?cken, Germany
{mkozhevn|titov}@mmci.uni-saarland.de
Abstract
We present an approach which uses the sim-
ilarity in semantic structure of bilingual par-
allel sentences to bootstrap a pair of seman-
tic role labeling (SRL) models. The setting
is similar to co-training, except for the inter-
mediate model required to convert the SRL
structure between the two annotation schemes
used for different languages. Our approach
can facilitate the construction of SRL models
for resource-poor languages, while preserving
the annotation schemes designed for the tar-
get language and making use of the limited re-
sources available for it. We evaluate the model
on four language pairs, English vs German,
Spanish, Czech and Chinese. Consistent im-
provements are observed over the self-training
baseline.
1 Introduction
The success of statistical modeling methods in a va-
riety of natural language processing (NLP) tasks in
the last decade depended crucially on the availability
of annotated resources for their training. And while
sizable resources for most standard tasks are only
available for a few languages, the human effort re-
quired to achieve reasonable performance on such
tasks for other languages may be significantly re-
duced by leveraging existing resources and the sim-
ilarities between languages.
This idea has lead to the development of cross-
lingual annotation projection approaches, which
make use of parallel corpora (Pado? and Lapata,
2009), as well as attempts to adapt models directly
to other languages (McDonald et al, 2011). In this
paper we consider correspondences between SRL
structures in translated sentences from a different
perspective. Most cross-lingual annotation projec-
tion approaches transfer the source language anno-
tation scheme to the target language without modifi-
cation, which makes it hard to combine their output
with existing target language resources, as annota-
tion schemes may vary significantly. We instead ad-
dress the problem of information transfer between
two existing annotation schemes (figure 1) for a pair
of languages using an intermediate model of role
correspondence (RCM). An RCM models the prob-
ability of a pair of corresponding arguments being
assigned a certain pair of roles. We then use it to
guide a pair of monolingual models toward compat-
ible predictions on parallel data in order to extend
the coverage and/or accuracy of one or both models.
Romanian is not taught in their schools .
Ve ?kol?ch se neu?? rumunsky .
A1
PAT
AM-LOC
LOC
AM-NEG
Figure 1: Role correspondence in parallel sentences, an
example.
The notion of compatibility here is highly non-
trivial, even for sentences translated as close to the
original as possible. Zhuang and Zong (2010), for
example, observe that in the English-Chinese paral-
lel PropBank (Palmer et al, 2005b) corresponding
arguments often bear different labels, even though
the same inventory of semantic roles is used for both
317
languages and the annotation guidelines are similar.
When different annotation schemes are considered,
the problem is further complicated by the difference
in the granularity of semantic roles used and varying
notions of what is an argument and what is not.
Manually annotated training data for such a model
is hard to come by. Instead, we propose an itera-
tive procedure similar to bootstrapping, where the
parameters of the RCM are initially estimated from
a parallel corpus automatically annotated with se-
mantic roles using the monolingual models indepen-
dently, and then the RCM is used to refine these an-
notations via a joint inference procedure, serving to
enforce consistency on the predictions of monolin-
gual models on parallel sentences. The obtained an-
notations on the parallel corpus are expected to be
of higher quality than the independent predictions of
the models, so they can be used to improve the SRL
models? performance and/or coverage. We evalu-
ate this approach by augmenting the original train-
ing data with the annotations obtained on parallel
data and observing the change in the model?s perfor-
mance. This is especially useful if one of the lan-
guages is relatively poor in resources, in which case
the proposed procedure will help propagate infor-
mation from the stronger model to the weaker one.
Even if the two models are comparable in their pre-
dictive power, we may be able to benefit from the
fact that certain semantic roles are realized less am-
biguously in one language than in another. We will
henceforth refer to these two alternatives as the pro-
jection and symmetric setups.
The paper is structured as follows. In the next sec-
tion we present our approach and discuss the issues
of role correspondence modeling, then describe the
implementation and datasets used in evaluation in
section 3, present the evaluation and results in sec-
tion 4 and conclude with the discussion of related
work in section 5.
2 Approach
We consider bootstrapping a pair of SRL models on
a parallel corpus, using the correspondence between
their predictions on parallel sentences to guide the
learning. The models are forced toward compatible
predictions, where the notion of compatibility is de-
fined by a (statistical) role correspondence model.
Let us consider a pair of languages, ? and ?,
and their corresponding datasets T 0? and T
0
? , anno-
tated with semantic roles (the upper indices here de-
note the iteration number). We will refer to these
as the initial training sets. We also assume that a
word-aligned parallel corpus is available for the pair
of languages, which we denote P , with the pred-
icates and their respective arguments identified on
both sides.
The procedure is then as follows: we train mono-
lingual models M0? and M
0
? on T
0
? and T
0
? , respec-
tively, apply them to the two sides of the parallel
corpus, resulting in a labeling P 0. We collect the se-
mantic role co-occurrence information and train the
role correspondence model C0 on it, then proceed to
the joint inference step involving M0?, M
0
? and C
0,
resulting in a refined labeling P 1 of the parallel cor-
pus. The two sides of the P 1 are then used to aug-
ment the initial training sets, yielding T 1? and T
1
? ,
and new models M1? and M
1
? are trained on these.
The process can then be repeated using M1? and M
1
?
instead of the initial models.
We report the model?s performance on a held-out
test set, drawn from the same corpus as the corre-
sponding initial training set.
The procedure can be seen as a form of co-
training (Blum and Mitchell, 1998) of a pair of
monolingual SRL models. In our case, however, the
question of the models? agreement is not as trivial as
in most applications of co-training, requiring a sta-
tistical model of its own (Ci).
In the low-resource (projection) setup our ap-
proach is also similar to self-training with weak su-
pervision coming from the stronger model.
Note that although the approach is iterative, we
have observed no significant improvements from re-
peating the procedure, possibly owing to the noise
introduced by the errors in preprocessing. In the
evaluation we run only one iteration. In the notation
introduced above, the self-training baseline model
(SELF) is trained on P 0? , the joint model (JOINT) ?
on P 1? and the combined model (COMB) ? on T
1
? .
2.1 Modeling Role Correspondence
It is necessary to distinguish between semantic
roles and their interpretation in a particular con-
text. The former can be defined in a variety of
318
ways, depending on the formalism used. In case of
FrameNet (Baker et al, 1998), for example, the in-
terpretation of a semantic role (frame element) is ex-
plicitly provided for each separate frame, so a frame
and a frame element label together describe the se-
mantics of an argument. PropBank (Palmer et al,
2005a) follows a mixed strategy ? the labels for a
relatively small set of core roles are numbered and
their interpretations are provided separately for each
predicate (although those of the first two roles, A0
and A1, consistently denote what is known as Proto-
Agent and Proto-Patient), while modifiers (Merlo
and Leybold, 2001) bear labels that are interpreted
consistently across all predicates. Other resources,
such as Prague Dependency Treebank (Hajic? et al,
2006), use a single set of semantic roles (functors),
which are interpretable across different predicates.
From the standpoint of defining the semantic sim-
ilarity of parallel sentences, the important implica-
tion is that we cannot assume that the corresponding
arguments should bear the same label, even if the
annotation schemes used are compatible (Zhuang
and Zong, 2010). Nor can we write down a single
mapping between the roles that will be valid across
different predicates (figure 2), which motivates the
need for a statistical model of semantic role corre-
spondence.
I do not have these concerns
Yo no tengo tales preocupaciones
A0
arg1-tem
A1
arg2-atr
Parliament adopted the resolution
El Parlamento aprueba la resoluci?n
A0
arg0-agt
A1
arg1-pat
We would like to know their names
Nos gustar?a conocer sus nombres
A0
arg2-ben
A1
arg1-tem
Figure 2: Predicate-specific role mapping. Note that A0
corresponds to art0-agt, art1-tem or art2-ben,
depending on the predicate.
We assume the existence of a one-to-one map-
ping between semantic roles for a given predicate
pair. As the mappings are not completely indepen-
dent ? at least some roles have the same interpre-
tation across different predicate pairs, ? we choose
to build a single model, which relies on features de-
rived from the pair of predicates in question, rather
than create a separate model for each predicate pair.
The model can then make decisions specific to par-
ticular predicates or predicate pairs, where sufficient
data has been observed or back off to a generic map-
ping where there is not enough data.
For the purpose of this study, we choose to sep-
arately model the probability of a target role, given
the source one and the necessary contextual infor-
mation and vice versa. These two components are
referred to as projection models and realized as a
pair of linear classifiers.
Training such a model in a conventional fash-
ion would require a rather specific kind of dataset,
namely a parallel corpus annotated with semantic
roles, and assuming the availability of such data
would severely limit the applicability of the ap-
proach proposed, as, to our knowledge, it is cur-
rently only available for two language pairs, namely
English-Chinese (Palmer et al, 2005b) and English-
Czech (Hajic? et al, 2012). We instead use the auto-
matically produced annotations on a parallel corpus,
effectively enforcing consistency on the role corre-
spondence in the monolingual models? predictions.
2.2 Joint Inference
The joint inference would have been simplest if the
arguments were classified independently. This as-
sumption is too restrictive, though, since the inter-
dependencies between the arguments can be used
to improve the accuracy of semantic role label-
ing (Roth and Yih, 2005).
2.2.1 Projection Setup
In the projection setup we assume that the model
for one of the languages, which we will henceforth
refer to as source, is much better informed than
the one for the other language, referred to as tar-
get, so we only have to propagate the information
one way. The scoring functions of these two mod-
els will be denoted fs and ft, respectively, and that
of the projection model from source to target ? fst.
Source and target sentences are denoted Ss and St,
319
and aligned predicates in these sentences ? ps and
pt. The task is then to identify the target language
role assignment rt that would maximize the objec-
tive L(rt) = ?tft(rt, St, pt) + ?stfst(rt, rs, ps, pt),
where rs = argmaxrfs(rs, Ss, ps) is the role as-
signment of the source-side arguments as predicted
by the monolingual model and ? are the weights as-
sociated with the models.
The exact maximization of this objective is com-
putationally expensive, so we resort to an approx-
imation. We chose to use the dual decomposition
method primarily because it fits the structure of the
objective particularly well (in that it is a sum of the
objectives of two independent models) and since it
allows a wide range of monolingual models to be
used in this setup. The only requirement here is that
the monolingual model must be able to incorporate
a bias toward or away from a certain prediction.
To apply this approximation, we decouple the
rt variables into rt and rst and get L1(rt, rst) =
?tft(rt, St, pt) + ?stfst(rst, rs, ps, pt) under the
condition that rt = rst. Applying the Lagrangian
relaxation, we replace the hard equality constraint
on rt and rst with a soft one, using slack variables ?,
which results in the following objective:
min?maxrt,rstL
?
1(rt, rst, ?) =
?tft(rt, St, pt) + ?stfst(rst, rs, ps, pt)+ (1)
?
i
?
r?Rt
?i,r
(
I(rit = r)? I(r
i
st = r)
)
,
where i indexes aligned argument pairs and I is an
indicator function. This is equivalent to
min?maxrt,rstL
?
1(rt, rst, ?) =
min?
(
maxrtgt(rt, St, pt, ?)+ (2)
maxrstgst(rst, rs, ps, pt, ?)
)
,
where
gt(rt, St, pt, ?) =
?tft(rt, St, pt) +
?
i
?
r?Rt
?i,rI(rit = r)
gstp(rst, rs, ps, pt, ?) = (3)
?stfst(rst, rs, ps, pt)?
?
i
?
r?Rt
?i,rI(rist = r)
are the augmented objectives of the two component
models, incorporating bias factors on various possi-
ble predictions.
The minimization with respect to ? is per-
formed using a subgradient descent algorithm fol-
lowing Sontag et al (2011). Whenever the method
converges, it converges to the global maximum of
the sum of the objectives. We found that in our case
it reaches a solution within the first 1000 iterations
over 99% of the time.
2.2.2 Symmetric Setup
If the models have comparable accuracy, the
above inference procedure can be extended to per-
form projection both ways. Formulating this as a
dual decomposition problem would require using
three separate components, two for the monolingual
models and one for the RCM, which would have to
make its own predictions for the semantic roles on
both sides without conditioning on the predictions
of the monolingual models. This calls for a different
kind of model than the one we use ? a model that
will rely on a (possibly simplified) feature represen-
tation of the source and target arguments to jointly
predict their labels. Instead, we perform the pro-
jection setup inference procedure in both directions
simultaneously, interleaving gradient descent steps
and allowing the projection models to access the up-
dated predictions of the monolingual models. This
results in a block gradient descent algorithm with the
following updates:
rn+1t = argmaxrtgt(rt, St, pt, ?
n
t )
rn+1s = argmaxrtgs(rs, Ss, ps, ?
n
s )
rn+1st = argmaxrstgst(rst, r
n
s , ps, pt, ?
n
t )
rn+1ts = argmaxrtsgts(rts, r
n
t , pt, ps, ?
n
s ) (4)
?i?r?Rs?
n+1,i,r
s = ?
n,i,r
s +
?s(n)(I(r
n,i
ts = r)? I(r
n,i
s = r))
?i?r?Rt?
n+1,i,r
t = ?
n,i,r
t +
?t(n)(I(r
n,i
st = r)? I(r
n,i
t = r)),
where ?s(n) = ?t(n) =
?0
n+1 is the update rate func-
tion for step n, and gs and gts are defined as in (3),
but with the direction reversed.
This procedure allows us to use the same RCM
implementation as in the projection setup. More-
over, the inference procedure for projection setup is
320
a special case of this one with ?s(n) set to 0. The
algorithm also demonstrates convergence similar to
that of the projection version, although it lacks the
optimality guarantees.
3 Experimental Setup
We evaluate our approach on four language pairs,
namely English vs German, Spanish, Czech and
Chinese, which we will denote en-de, en-es,
en-cz and en-zh respectively.
3.1 Parallel Data
The parallel data for the first three language pairs
is drawn from Europarl v6 (Koehn, 2005) and
from MultiUN (Eisele and Chen, 2010) for English-
Chinese. We applied Stanford Tokenizer for En-
glish, tokenizer scripts (Koehn, 2005) provided
with the Europarl corpus to German, Spanish and
Czech, and Stanford Chinese Segmenter (Chang et
al., 2008) to Chinese, then performed POS-tagging,
morphology tagging (where applicable) and depen-
dency parsing using MATE-tools (Bohnet, 2010).
Word alignments were acquired using
GIZA++ (Och and Ney, 2003) with its stan-
dard settings. Predicate identification on the parallel
data was done using the supervised classifiers of
the monolingual SRL systems, except for German,
where a simple heuristic had to be used instead,
as only some of the predicates are marked in
the training data, which makes it hard to train a
supervised classifier. Following van der Plas et al
(2011), we then retain only those sentences where
all identified predicates were aligned.
In the experiments we used 50 thousand predicate
pairs in each case, as increasing the amount further
did not yield noticeable benefits, while increasing
the running time.
3.2 Annotated Data
The CoNLL?09 (Hajic? et al, 2009) datasets were
used as a source of annotated data for all languages.
Only verbal predicates were considered and pre-
dicted syntax was used in evaluation.
We consider subsets of the training data in order
to emulate the scenario with a resource-poor lan-
guage. Due to the different sources the datasets
were derived from, sentences contain different pro-
portions of annotated predicates depending on the
language. The German corpus, for example, con-
tains about 6 times fewer argument labels per sen-
tence than the English one. We will therefore in-
dicate the sizes of the datasets used in the number
of argument labels they contain, referred to as in-
stances, rather than the number of predicates or sen-
tences. The corpus for English, for example, con-
tains 6.2 such instances per sentence on average.
We use the 20 thousand instances of the available
data as the training corpus for each language and
split the rest equally between the development and
the test set. The secondary (?out-of-domain?) test
sets are preserved as they are.
In dependency-based SRL, only heads of syntac-
tic constituents are marked with semantic roles. The
heads of corresponding arguments may or may not
align, however, even if the arguments are lexically
very similar, because their syntactic structure may
differ. In general, one would have to identify the
whole phrase for each argument and take into ac-
count the links between constituents, rather than sin-
gle words (Pado? and Lapata, 2005). As reconstruct-
ing the constituents from the dependency tree is non-
trivial (Hwang et al, 2010), we are using a heuristic
to address the most common version of this problem,
i.e. a preposition or an auxiliary verb being an argu-
ment head. In such a case we also take into account
any alignment links involving the head?s immediate
descendants.
3.3 Implementation
Our system is based on that of Bjo?rkelund et al
(2009). It is a pipeline system comprised of a set of
binary or multiclass linear classifiers. Both here and
in the projection model, the classifiers are trained
using Liblinear (Fan et al, 2008).
We employed a uniqueness constraint on role la-
bels (Chang et al, 2007), preventing some of them
from being assigned to more than one argument in
the same predicate, which appears to be more reli-
able in a low-resource setting we consider than the
reranker the original system employed. The con-
straint is enforced in the monolingual model infer-
ence using a beam-search approximation with the
beam size of 10. The label uniqueness information
was derived from the training sets.
321
3.4 The Projection Model
Each projection model is realized by a single lin-
ear classifier applied to each argument pair indepen-
dently. It relies on features derived from the source
semantic role and source and target predicates, and
predicts the semantic role for the argument in the
target sentence.
The features include the source semantic role and
its conjunctions with (lowercased) forms and lem-
mata of the source and target predicates. For ex-
ample, assuming the source semantic role is A3 and
the source and target predicates are went and ging
(past tense of ?gehen?, German), the features would
be as shown in figure 3.
FORMPAIR=A3-went-ging
LEMMAPAIR=A3-go-gehen
FORMSRC=A3-went
FORMTGT=A3-ging
LEMMASRC=A3-go
LEMMATGT=A3-gehen
LABEL=A3
Figure 3: Projection model features example.
3.5 Parameters
In case of projection there are two parameters, ?st
and ?t, ? the weights of the component models in the
objective. Only their relative values matter (except
in the choice of ?0), so we set ?t to 1 and only tune
the weight of the role correspondence model.
In the symmetric setup, the objective takes
the form L(rt, rs) = ?tft(rt, St, pt) +
?stfst(rt, rs, ps, pt) + ?sfs(rs, Ss, ps) +
?tsfts(rs, rt, pt, ps). Since we assume that the
two monolingual models here have comparable
performance, we do not tune their relative weights,
setting both ?s and ?t to 1.
We also use the same weight for both projection
models, ?st = ?ts, and this value plays an important
role ? it basically indicates how strongly we insist
on the role correspondence models? correctness. If
this weight is set to 0, the RCM will accept the ini-
tial predictions the monolingual models make, and if
it is set to a sufficiently large value, the predictions
of the monolingual models will be biased until they
match the mapping suggested by the RCM. The op-
timal weight will therefore depend on the language
pair, the sizes of the initial training sets and the RCM
used. We use the value of 0.7 in all projection ex-
periments and 0.5 in the symmetric setup, however,
as excessive tuning may be undesirable in the low-
resource setting.
3.6 Domains
One important factor in the understanding of the
evaluation figures presented is the fact that sources
of annotated and parallel data belong to different do-
mains. The former usually contains some sort of
newswire text ? Wall Street Journal in case of En-
glish, Xinhua newswire, Hong Kong news and Sino-
rama news magazine for Chinese, etc. Parallel data,
on the other hand, comes from the proceedings of
European Parliament and United Nations, which are
quite different. For example, the sentences in the
latter domain often start with someone being ad-
dressed, either by name or by title, which can hardly
be expected to occur as often in a newspaper or a
magazine article.
As is well-known, the performance of many sta-
tistical tools drops significantly outside the domain
they were trained on (Pradhan et al, 2008), and the
preprocessing and SRL models used here are no ex-
ception, which results in relatively low quality of
the initial predictions on the parallel text. The low
argument identification performance, in particular,
is presumably due to inaccurate dependency parses,
on which it heavily relies. Several approached have
been proposed to improve the accuracy of depen-
dency parsers and other tools on out-of-domain data,
but this is beyond the scope of this paper. In some
cases (though seldom), sources of parallel data be-
longing to the same domain as the annotated training
data can be obtained.
Another concern is that the performance of a
model trained on automatically labeled parallel data
as measured on a test set we use may not reflect the
quality of these annotations. To assess the resulting
model?s coverage, it would be interesting to evaluate
it on data outside the original domain, so we con-
sider the out-of-domain (OOD) test sets as provided
for the CoNLL Shared Task 2009 where available.
Perhaps the most interesting one of these is the
German OOD test set, which is drawn from Europarl
(as is the parallel data we use). It was originally
annotated with syntactic dependency trees and se-
322
mantic structure in the SALSA format (Burchardt
et al, 2006) for Pado? and Lapata (2005), and then
converted into a PropBank-like form for the CoNLL
Shared Task 2009 (Hajic? et al, 2009). The OOD
test set for English is drawn from the Brown cor-
pus (Francis and Kucera, 1967) and the one for
Czech ? from a Czech translation of Wall Street
Journal articles (Hajic? et al, 2012).
4 Evaluation
The first question we are interested in is how the
joint inference affects the quality of the automati-
cally obtained annotations on the parallel data. To
answer this, we will run the monolingual models in-
dependently and jointly, then train models on the
output of these two procedures and compare their
performance on a test set. Note that we do not add
the initial training data at this point, so the initial
model scores are provided for reference, rather than
as a baseline.
4.1 Projection Setup
A small initial training set of 600 instances was used
here for the target language here and the full training
set (20000 instances) for the source one. ?st was set
to 0.7 in all experiments in this section.
INIT SELF JOINT ?SELF
en-cz* 61.11 60.68 63.01 2.33
en-cz 62.45 62.15 63.11 0.96
en-de* 66.81 63.96 67.64 3.69
en-de 70.40 68.34 70.13 1.79
en-es 64.20 64.51 66.01 1.50
en-zh 75.80 73.52 74.87 1.35
cz-en* 66.82 63.95 64.97 1.02
cz-en 74.92 71.60 71.90 0.29
de-en* 66.82 63.58 63.21 -0.37
de-en 74.93 71.31 70.72 -0.59
es-en* 66.82 63.95 64.18 0.23
es-en 74.93 71.47 72.09 0.62
zh-en* 66.82 64.51 63.67 -0.83
zh-en 74.93 72.26 71.24 -1.01
Table 1: Projection setup results: self-training baseline,
refined model and the difference in their performance.
Asterisk indicates out-of-domain test set, statistically sig-
nificant improvements are highlighted in bold.
In table 1, we present the accuracy of the model
trained on the output of the joint inference (JOINT)
against that of the self-training baseline (SELF). The
?SELF column contains the difference between the
two. Note that the SELF model is trained on the
parallel data automatically annotated using mono-
lingual SRL models (not mixed with the initial train-
ing set), since we are interested in the effect of joint
inference on the quality of the annotation obtained.
Where the improvement is positive and statistically
significant with p < 0.005 according to the permuta-
tion test (Good, 2000), they are highlighted in bold.
We can see that the refined model (JOINT) outper-
forms the self-training baseline in most cases by a
moderate, but statistically significant margin, which
indicates that the joint inference does improve the
quality of annotations on the parallel corpus.
The slightly higher improvement on the German
OOD test set supports our hypothesis that the proce-
dure enhances the performance of the model on par-
allel data, as the data for this test set is also drawn
from the Europarl corpus. The improvement over
the initial model (?INIT) in this case is statistically
significant with p < 0.05. Higher p-value may be
attributed to the smaller test set size.
Figure 4 shows how the performance of the JOINT
model changes with the size of the initial training
set. The improvements are smaller for en-cz, en-
de and en-zh, but they are also statistically signifi-
cant for initial training sets of up to 2000 instances.
Projection to English from other languages performs
worse.
Figure 4: Projection setup, English-Spanish, model per-
formance as a function of the size of the initial training
set.
323
4.2 Combining
In practice, automatically obtained annotations are
usually combined with the existing labeled data. For
this purpose, the initial training set is replicated so
as to constitute 0.3 (an empirically chosen value that
appears to work well in most experiments) of the
size of the automatically labeled dataset. We com-
pare the performance of the model trained on the re-
sulting dataset (COMB) with that of the JOINT model
and the initial models. The results are presented in
table 2. We omit projection from other languages to
English, since the JOINT model there fails to outper-
form the initial model and we do not expect to ben-
efit from adding the automatically annotated data to
the initial training set in this case.
INIT JOINT COMB ?JOINT ?INIT
en-cz* 61.11 63.01 62.98 -0.03 1.87
en-cz 62.45 63.11 63.30 0.19 0.85
en-de* 66.81 67.64 67.64 0.00 0.84
en-de 70.39 70.19 70.53 0.34 0.15
en-es 64.20 66.01 66.01 0.00 1.81
en-zh 75.80 74.87 75.03 0.16 -0.77
Table 2: The effect of adding automatically obtained an-
notation to the initial training set. Asterisk indicates out-
of-domain test set, statistically significant improvements
are highlighted in bold.
4.3 Symmetric Setup
In the symmetric setup evaluation, we use a slightly
larger initial training set of 1400 instances for both
source and target language. The projection model
weight is set to 0.5. Table 3 shows the accuracy of
the JOINT model and the SELF baseline.
Note that here, unlike section 4.1, the joint in-
ference is run once and then a model is trained for
each language and evaluated on the corresponding
test set(s).
The results support our intuition that joint infer-
ence helps improve the quality of the resulting an-
notations, at least in some cases.
4.4 Oracle RCM
It would be useful to know to what extent the per-
formance of the role correspondence model affects
the quality of the output (and thus the performance
of the resulting model). The RCM we use is rather
INIT SELF JOINT ?SELF
en-cz* 67.07 66.15 68.18 2.02
en-cz 67.56 66.42 66.72 0.30
en-de* 67.64 66.72 68.57 1.84
en-de 75.13 71.97 73.57 1.60
en-es 68.14 67.80 69.04 1.24
en-zh 76.28 72.96 75.22 2.26
cz-en* 69.37 66.45 66.22 -0.23
cz-en 77.32 74.72 75.02 0.31
de-en* 69.37 66.45 66.68 0.23
de-en 77.32 73.56 73.72 0.17
es-en* 69.37 66.64 66.40 -0.23
es-en 77.32 74.05 74.89 0.84
zh-en* 69.37 66.08 65.53 -0.56
zh-en 77.32 74.48 74.25 -0.24
Table 3: Comparing JOINT model against the self-
training baseline in symmetric setup. Asterisk indicates
out-of-domain test set, statistically significant improve-
ments are highlighted in bold.
simplistic, and we believe it can be substantially im-
proved for any given language pair by incorporat-
ing prior knowledge and/or using external sources
of information. In order to estimate the potential
impact of such improvements, we simulate a better
informed projection model, giving it access to the
predictions of more accurate monolingual models on
the parallel data ? those trained on the full training
set, rather than the initial training set used in this par-
ticular experiment. We refer to the resulting RCM as
oracle and assess the difference it makes, compared
to a regular one (table 4).
5 Related Work
There is a number of approaches to semi-supervised
semantic role labeling, and most suggest that some
external supervision is required for such approaches
to work (He and Gildea, 2006), such as measures of
syntactic and semantic similarity (Fu?rstenau and La-
pata, 2009) or external confidence measures (Gold-
wasser et al, 2011). The alternative we propose is
primarily motivated by the research on annotation
projection (Pado? and Lapata, 2009; van der Plas
et al, 2011; Annesi and Basili, 2010; Naseem et
al., 2012) and direct transfer (Durrett et al, 2012;
S?gaard, 2011; Lopez et al, 2008; McDonald et al,
2011). The key difference of the present approach
compared to annotation projection is that we assume
324
INIT SELF JOINT ?SELF ?INIT
en-cz* 61.11 60.68 72.49 11.81 11.38
en-cz 62.45 62.15 70.19 8.04 7.74
en-de* 66.81 63.96 76.78 12.82 9.97
en-de 70.39 68.34 79.22 10.88 8.84
en-es 64.20 64.51 75.43 10.92 11.23
en-zh 75.80 73.52 76.75 3.22 0.94
cz-en* 66.82 63.95 70.75 6.80 3.93
cz-en 74.93 71.60 79.70 8.10 4.76
de-en* 66.82 63.58 69.46 5.88 2.64
de-en 74.93 71.31 77.34 6.03 2.41
es-en* 66.82 63.95 69.92 5.97 3.10
es-en 74.93 71.47 79.55 8.08 4.62
zh-en* 66.82 64.51 67.19 2.68 0.37
zh-en 74.93 72.26 76.51 4.26 1.58
Table 4: Oracle RCM performance, projection setup: ini-
tial model, self-training baseline, refined model and its
improvement over the other two. Asterisk indicates out-
of-domain test set, statistically significant improvements
are highlighted in bold.
the availability of some amount of training data for
the target language, possibly using a different inven-
tory of semantic roles.
As mentioned previously, from the training point
of view this approach can be seen as similar to co-
training (Blum and Mitchell, 1998), other applica-
tions of which to NLP are too numerous to list here.
Most closely related is the joint inference in
Zhuang and Zong (2010), the main difference being
that it relies on a manually annotated parallel corpus,
aligned on the argument level, and evaluates only the
inference procedure and only on in-domain data.
Other related approaches include Kim et al
(2010), where a cross-lingual transfer of relations
is performed (which basically represent parts of
the predicate-argument structure considered by SRL
methods), and Frermann and Bond (2012), where
semantic structure matching is used to rank HPSG
parses for parallel sentences.
Unsupervised semantic role labeling meth-
ods (Lang and Lapata, 2010; Lang and Lapata,
2011; Titov and Klementiev, 2012a; Lorenzo and
Cerisara, 2012) present an alternative to the cross-
lingual information propagation approaches such as
ours, and at least one the methods in this area also
makes use of parallel data (Titov and Klementiev,
2012b).
Conclusions
We have presented an approach to information trans-
fer between SRL systems for different language
pairs using parallel data. The task proves challeng-
ing due to non-trivial mapping between the role la-
bels used in different SRL annotation schemes and
the nature of parallel data ? the difference in do-
mains and the limited accuracy of the preprocess-
ing tools. We observe consistent improvements over
self-training baseline from using joint inference and
the experiments suggest that improving the role cor-
respondence model, for example using language-
specific prior knowledge or external data sources,
may dramatically increase the performance of the re-
sulting system.
Acknowledgments
The authors acknowledge the support of the MMCI
Cluster of Excellence and thank Alexandre Klemen-
tiev and Manfred Pinkal for valuable suggestions.
References
Paolo Annesi and Roberto Basili. 2010. Cross-lingual
alignment of framenet annotations through hidden
markov models. In Proceedings of the 11th interna-
tional conference on Computational Linguistics and
Intelligent Text Processing, CICLing?10, pages 12?25,
Berlin, Heidelberg. Springer-Verlag.
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley FrameNet project. In Pro-
ceedings of the Thirty-Sixth Annual Meeting of the
Association for Computational Linguistics and Sev-
enteenth International Conference on Computational
Linguistics (ACL-COLING?98), pages 86?90, Mon-
treal, Canada.
Anders Bjo?rkelund, Love Hafdell, and Pierre Nugues.
2009. Multilingual semantic role labeling. In Pro-
ceedings of the Thirteenth Conference on Computa-
tional Natural Language Learning (CoNLL 2009):
Shared Task, pages 43?48, Boulder, Colorado, June.
Association for Computational Linguistics.
Avrim Blum and Tom Mitchell. 1998. Combining la-
beled and unlabeled data with co-training. In Proceed-
ings of the Workshop on Computational Learning The-
ory (COLT 98).
Bernd Bohnet. 2010. Top accuracy and fast dependency
parsing is not a contradiction. In Proceedings of the
23rd International Conference on Computational Lin-
325
guistics (Coling 2010), pages 89?97, Beijing, China,
August. Coling 2010 Organizing Committee.
Aljoscha Burchardt, Katrin Erk, Anette Frank, Andrea
Kowalski, Sebastian Pado, and Manfred Pinkal. 2006.
The SALSA corpus: a German corpus resource for
lexical semantics. In Proceedings of LREC 2006,
Genoa, Italy.
M.W. Chang, L. Ratinov, and D. Roth. 2007. Guiding
semi-supervision with constraint-driven learning. Ur-
bana, 51:61801.
Pi-Chuan Chang, Michel Galley, and Christopher D.
Manning. 2008. Optimizing chinese word segmen-
tation for machine translation performance. In Pro-
ceedings of the Third Workshop on Statistical Machine
Translation, StatMT ?08, pages 224?232, Stroudsburg,
PA, USA. Association for Computational Linguistics.
Greg Durrett, Adam Pauls, and Dan Klein. 2012. Syntac-
tic transfer using a bilingual lexicon. In Proceedings
of the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, pages 1?11, Jeju Island,
Korea, July. Association for Computational Linguis-
tics.
Andreas Eisele and Yu Chen. 2010. MultiUN: A multi-
lingual corpus from united nation documents. In Nico-
letta Calzolari (Conference Chair), Khalid Choukri,
Bente Maegaard, Joseph Mariani, Jan Odijk, Stelios
Piperidis, Mike Rosner, and Daniel Tapias, editors,
Proceedings of the Seventh International Conference
on Language Resources and Evaluation (LREC?10),
Valletta, Malta, May. European Language Resources
Association (ELRA).
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui
Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A li-
brary for large linear classification. Journal of Ma-
chine Learning Research, 9:1871?1874.
S. Francis and H. Kucera. 1967. Computing Analysis
of Present-day American English. Brown University
Press, Providence, RI.
Lea Frermann and Francis Bond. 2012. Cross-lingual
parse disambiguation based on semantic correspon-
dence. In Proceedings of the 50th Annual Meeting
of the Association for Computational Linguistics (Vol-
ume 2: Short Papers), pages 125?129, Jeju Island, Ko-
rea, July. Association for Computational Linguistics.
Hagen Fu?rstenau and Mirella Lapata. 2009. Graph
alignment for semi-supervised semantic role labeling.
In Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing, pages 11?
20, Singapore.
D. Goldwasser, R. Reichart, J. Clarke, and D. Roth.
2011. Confidence driven unsupervised semantic pars-
ing. In ACL.
P. Good. 2000. Permutation Tests: A Practical
Guide to Resampling Methods for Testing Hypotheses.
Springer.
J. Hajic?, J. Panevova?, E. Hajic?ova?, P. Sgall, P. Pajas,
J. S?te?pa?nek, J. Havelka, M. Mikulova?, Z. Z?abokrtsky`,
and M. S?evc???kova?-Raz??mova?. 2006. Prague depen-
dency treebank 2.0. LDC.
Jan Hajic?, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Anto`nia Mart??, Llu??s
Ma`rquez, Adam Meyers, Joakim Nivre, Sebastian
Pado?, Jan S?te?pa?nek, Pavel Stran?a?k, Mihai Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The CoNLL-
2009 shared task: Syntactic and semantic dependen-
cies in multiple languages. In Proceedings of the Thir-
teenth Conference on Computational Natural Lan-
guage Learning (CoNLL 2009): Shared Task, pages
1?18, Boulder, Colorado.
Jan Hajic?, Eva Hajic?ova?, Jarmila Panevova?, Petr Sgall,
Ondr?ej Bojar, Silvie Cinkova?, Eva Fuc???kova?, Marie
Mikulova?, Petr Pajas, Jan Popelka, Jir??? Semecky?,
Jana S?indlerova?, Jan S?te?pa?nek, Josef Toman, Zden?ka
Ures?ova?, and Zdene?k Z?abokrtsky?. 2012. Announc-
ing prague czech-english dependency treebank 2.0.
In Nicoletta Calzolari (Conference Chair), Khalid
Choukri, Thierry Declerck, Mehmet Ug?ur Dog?an,
Bente Maegaard, Joseph Mariani, Jan Odijk, and Ste-
lios Piperidis, editors, Proceedings of the Eight In-
ternational Conference on Language Resources and
Evaluation (LREC?12), Istanbul, Turkey, May. Euro-
pean Language Resources Association (ELRA).
Shan He and Daniel Gildea. 2006. Self-training and
co-training for semantic role labeling: Primary report.
Technical report, University of Rochester.
Jena D. Hwang, Rodney D. Nielsen, and Martha Palmer.
2010. Towards a domain independent semantics:
Enhancing semantic representation with construction
grammar. In Proceedings of the NAACL HLT Work-
shop on Extracting and Using Constructions in Com-
putational Linguistics, pages 1?8, Los Angeles, Cali-
fornia, June. Association for Computational Linguis-
tics.
Seokhwan Kim, Minwoo Jeong, Jonghoon Lee, and
Gary Geunbae Lee. 2010. A cross-lingual annota-
tion projection approach for relation detection. In Pro-
ceedings of the 23rd International Conference on Com-
putational Linguistics, COLING ?10, pages 564?571,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Philipp Koehn. 2005. Europarl: A Parallel Corpus
for Statistical Machine Translation. In Conference
Proceedings: the tenth Machine Translation Summit,
pages 79?86, Phuket, Thailand. AAMT, AAMT.
Joel Lang and Mirella Lapata. 2010. Unsupervised in-
duction of semantic roles. In Human Language Tech-
326
nologies: The 2010 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 939?947, Los Angeles, Cal-
ifornia, June. Association for Computational Linguis-
tics.
Joel Lang and Mirella Lapata. 2011. Unsupervised se-
mantic role induction via split-merge clustering. In
Proc. of Annual Meeting of the Association for Com-
putational Linguistics (ACL).
Adam Lopez, Daniel Zeman, Michael Nossal, Philip
Resnik, and Rebecca Hwa. 2008. Cross-Language
Parser Adaptation between Related Languages. In
IJCNLP-08 Workshop on NLP for Less Privileged
Languages, pages 35?42, Hyderabad, India, January.
Alejandra Lorenzo and Christophe Cerisara. 2012. Un-
supervised frame based semantic role induction: ap-
plication to french and english. In Proceedings of the
ACL 2012 Joint Workshop on Statistical Parsing and
Semantic Processing of Morphologically Rich Lan-
guages, pages 30?35, Jeju, Republic of Korea, July 12.
Association for Computational Linguistics.
Ryan McDonald, Slav Petrov, and Keith Hall. 2011.
Multi-source transfer of delexicalized dependency
parsers. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
EMNLP ?11, pages 62?72, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
Paola Merlo and Matthias Leybold. 2001. Automatic
distinction of arguments and modifiers: the case of
prepositional phrases. In Proceedings of the Fifth
Computational Natural Language Learning Workshop
(CoNLL-2001), pages 121?128, Toulouse, France.
Tahira Naseem, Regina Barzilay, and Amir Globerson.
2012. Selective sharing for multilingual dependency
parsing. In Proceedings of the 50th Annual Meeting
of the Association for Computational Linguistics (Vol-
ume 1: Long Papers), pages 629?637, Jeju Island, Ko-
rea, July. Association for Computational Linguistics.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1).
Sebastian Pado? and Mirella Lapata. 2005. Cross-
linguistic projection of role-semantic information. In
Proceedings of Human Language Technology Confer-
ence and Conference on Empirical Methods in Natu-
ral Language Processing, pages 859?866, Vancouver,
British Columbia, Canada.
Sebastian Pado? and Mirella Lapata. 2009. Cross-lingual
annotation projection for semantic roles. Journal of
Artificial Intelligence Research, 36:307?340.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005a. The Proposition Bank: An annotated corpus
of semantic roles. Computational Linguistics, 31:71?
105.
Martha Palmer, Nianwen Xue, Olga Babko-Malaya, Jiny-
ing Chen, and Benjamin Snyder. 2005b. A parallel
Proposition Bank II for Chinese and English. In Pro-
ceedings of the Workshop on Frontiers in Corpus An-
notations II: Pie in the Sky, CorpusAnno ?05, pages
61?67, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Sameer S. Pradhan, Wayne Ward, and James H. Martin.
2008. Towards robust semantic role labeling. Compu-
tational Linguistics, 34(2):289?310.
Dan Roth and Wen-tau Yih. 2005. Integer linear pro-
gramming inference for conditional random fields. In
ICML, pages 736?743.
Anders S?gaard. 2011. Data point selection for cross-
language adaptation of dependency parsers. In Pro-
ceedings of the 49th Annual Meeting of the Associa-
tion for Computational Linguistics: Human Language
Technologies: short papers - Volume 2, HLT ?11,
pages 682?686, Stroudsburg, PA, USA. Association
for Computational Linguistics.
David Sontag, Amir Globerson, and Tommi Jaakkola.
2011. Introduction to dual decomposition for in-
ference. In Suvrit Sra, Sebastian Nowozin, and
Stephen J. Wright, editors, Optimization for Machine
Learning. MIT Press.
Ivan Titov and Alexandre Klementiev. 2012a. A
Bayesian approach to unsupervised semantic role in-
duction. In Proc. of European Chapter of the Associa-
tion for Computational Linguistics (EACL).
Ivan Titov and Alexandre Klementiev. 2012b. Crosslin-
gual induction of semantic roles. In Proceedings of
the 50th Annual Meeting of the Association for Com-
putational Linguistics, Jeju Island, South Korea, July.
Association for Computational Linguistics.
Lonneke van der Plas, Paola Merlo, and James Hender-
son. 2011. Scaling up automatic cross-lingual seman-
tic role annotation. In Proceedings of the 49th Annual
Meeting of the Association for Computational Linguis-
tics: Human Language Technologies: short papers -
Volume 2, HLT ?11, pages 299?304, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Tao Zhuang and Chengqing Zong. 2010. Joint inference
for bilingual semantic role labeling. In Proceedings of
the 2010 Conference on Empirical Methods in Natural
Language Processing, EMNLP ?10, pages 304?314,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
327
CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 178?182
Manchester, August 2008
A Latent Variable Model of Synchronous Parsing
for Syntactic and Semantic Dependencies
James Henderson
Dept Computer Science
Univ Geneva
james.henderson@
cui.unige.ch
Paola Merlo
Dept Linguistics
Univ Geneva
merlo@
lettres.unige.ch
Gabriele Musillo
Depts Linguistics
and Computer Science
Univ Geneva
musillo@
lettres.unige.ch
Ivan Titov
?
Dept Computer Science
Univ Illinois at U-C
titov@uiuc.edu
Abstract
We propose a solution to the challenge
of the CoNLL 2008 shared task that uses
a generative history-based latent variable
model to predict the most likely derivation
of a synchronous dependency parser for
both syntactic and semantic dependencies.
The submitted model yields 79.1% macro-
average F1 performance, for the joint task,
86.9% syntactic dependencies LAS and
71.0% semantic dependencies F1. A larger
model trained after the deadline achieves
80.5% macro-average F1, 87.6% syntac-
tic dependencies LAS, and 73.1% seman-
tic dependencies F1.
1 Introduction
Successes in syntactic tasks, such as statistical
parsing and tagging, have recently paved the way
to statistical learning techniques for levels of se-
mantic representation, such as recovering the log-
ical form of a sentence for information extraction
and question-answering applications (e.g. (Wong
and Mooney, 2007)) or jointly learning the syntac-
tic structure of the sentence and the propositional
argument-structure of its main predicates (Musillo
and Merlo, 2006; Merlo and Musillo, 2008). In
this vein, the CoNLL 2008 shared task sets the
challenge of learning jointly both syntactic depen-
dencies (extracted from the Penn Treebank (Mar-
cus et al, 1993) ) and semantic dependencies (ex-
tracted both from PropBank (Palmer et al, 2005)
?
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
0
Authors in alphabetical order.
and NomBank (Meyers et al, 2004) under a uni-
fied representation.
We propose a solution that uses a generative
history-based model to predict the most likely
derivation of a synchronous dependency parser for
both syntactic and semantic dependencies. Our
probabilistic model is based on Incremental Sig-
moid Belief Networks (ISBNs), a recently pro-
posed latent variable model for syntactic struc-
ture prediction, which has shown very good be-
haviour for both constituency (Titov and Hender-
son, 2007a) and dependency parsing (Titov and
Henderson, 2007b). The ability of ISBNs to in-
duce their features automatically enables us to ex-
tend this architecture to learning a synchronous
parse of syntax and semantics without modifica-
tion of the main architecture. By solving the
problem with synchronous parsing, a probabilistic
model is learnt which maximises the joint proba-
bility of the syntactic and semantic dependencies
and thereby guarantees that the output structure is
globally coherent, while at the same time building
the two structures separately. This extension of the
ISBN architecture is therefore applicable to other
problems where two independent, but related, lev-
els of representation are being learnt, such as sta-
tistical machine translation.
Currently the largest model we have trained
achieves 80.5% macro-average F1 performance for
the joint task, 87.6% syntactic dependencies LAS,
and 73.1% semantic dependencies F1.
2 The Probability Model
Our probability model is a joint generative model
of syntactic and semantic dependencies. The
two dependency structures are specified as the se-
quence of actions for a synchronous parser, which
requires each dependency structure to be projec-
178
tivised separately.
2.1 Synchronous derivations
The derivations for syntactic dependency trees are
the same as specified in (Titov and Henderson,
2007b), which are based on the shift-reduce style
parser of (Nivre et al, 2006). The derivations use a
stack and an input queue. There are actions for cre-
ating a leftward or rightward arc between the top of
the stack and the front of the queue, for popping a
word from the stack, and for shifting a word from
the queue to the stack. The derivations for seman-
tic dependency graphs use virtually the same set
of actions, but impose fewer constraints on when
they can be applied, due to the fact that a word in
a semantic dependency graph can have more than
one parent. An additional action predicate
s
was
introduced to label a predicate with sense s.
Let T
d
be a syntactic dependency tree with
derivation D
1
d
, ..., D
m
d
d
, and T
s
be a semantic de-
pendency graph with derivation D
1
s
, ..., D
m
s
s
. To
define derivations for the joint structure T
d
, T
s
,
we need to specify how the two derivations are
synchronised, and in particular make the impor-
tant choice of the granularity of the synchronisa-
tion step. Linguistic intuition would perhaps sug-
gest that syntax and semantics are connected at the
clause level ? a big step size ? while a fully in-
tegrated system would synchronise at each pars-
ing decision, thereby providing the most commu-
nication between these two levels. We choose to
synchronise the construction of the two structures
at every word ? an intermediate step size. This
choice is simpler, as it is based on the natural to-
tal order of the input, and it avoids the problems
of the more linguistically motivated choice, where
chunks corresponding to different semantic propo-
sitions would be overlapping.
We divide the two derivations into the chunks
between shifting each word onto the stack,
c
t
d
= D
b
t
d
d
, ..., D
e
t
d
d
and c
t
s
= D
b
t
s
s
, ..., D
e
t
s
s
,
where D
b
t
d
?1
d
= D
b
t
s
?1
s
= shift
t?1
and
D
e
t
d
+1
d
= D
e
t
s
+1
s
= shift
t
. Then the actions of
the synchronous derivations consist of quadruples
C
t
= (c
t
d
, switch, c
t
s
, shift
t
), where switch means
switching from syntactic to semantic mode. This
gives us the following joint probability model,
where n is the number of words in the input.
P (T
d
, T
s
) = P (C
1
, . . . , C
n
)
=
?
t
P (C
t
|C
1
, . . . , C
t?1
)
(1)
The probability of each synchronous derivation
chunk C
t
is the product of four factors, related to
the syntactic level, the semantic level and the two
synchronising steps.
P (C
t
|C
1
, . . . , C
t?1
) =
P (c
t
d
|C
1
, . . . , C
t?1
)?
P (switch|c
t
d
, C
1
, . . . , C
t?1
)?
P (c
t
s
|switch, c
t
d
, C
1
, . . . , C
t?1
)?
P (shift
t
|c
t
d
, c
t
s
, C
1
, . . . , C
t?1
)
(2)
These synchronous derivations C
1
, . . . , C
n
only
require a single input queue, since the shift opera-
tions are synchronised, but they require two sepa-
rate stacks, one for the syntactic derivation and one
for the semantic derivation.
The probability of c
t
d
is decomposed into deriva-
tion action D
i
probabilities, and likewise for c
t
s
.
P (c
t
d
|C
1
, . . . , C
t?1
)
=
?
i
P (D
i
d
|D
b
t
d
d
,. . ., D
i?1
d
, C
1
,. . ., C
t?1
)
(3)
The actions are also sometimes split into a se-
quence of elementary decisions D
i
= d
i
1
, . . . , d
i
n
,
as discussed in (Titov and Henderson, 2007a).
2.2 Projectivisation of dependencies
These derivations can only specify projective
syntactic or semantic dependency graphs. Ex-
ploratory data analysis indicates that many in-
stances of non-projectivity in the complete graph
are due to crossings of the syntactic and seman-
tic graphs. The amount of non-projectivity of the
joint syntactic-semantic graph is approximately
7.5% non-projective arcs, while summing the non-
projectivity within the two separate graphs results
in only roughly 3% non-projective arcs.
Because our synchronous derivations use two
different stacks for the syntactic and semantic de-
pendencies, respectively, we only require each in-
dividual graph to be projective. As with many de-
pendency parsers (Nivre et al, 2006; Titov and
Henderson, 2007b), we handle non-projective (i.e.
crossing) arcs by transforming them into non-
crossing arcs with augmented labels.
1
Because
our syntactic derivations are equivalent to those of
(Nivre et al, 2006), we use their HEAD methods
to projectivise the syntactic dependencies.
Although our semantic derivations use the same
set of actions as the syntactic derivations, they dif-
fer in that the graph of semantic dependencies need
1
During testing, these projectivised structures are then
transformed back to the original format for evaluation.
179
not form a tree. The only constraints we place on
the set of semantic dependencies are imposed by
the use of a stack, which excludes crossing arcs.
Given two crossing arcs, we try to uncross them
by changing an endpoint of one of the arcs. The
arc (p, a), where p is a predicate and a is an argu-
ment, is changed to (p, h), where h is the syntactic
head of argument a. Its label r is then changed to
r/d where d is the syntactic dependency of a on
h. This transformation may need to be repeated
before the arcs become uncrossed. The choice of
which arc to transform is done using a greedy al-
gorithm and a number of heuristics, without doing
any global optimisation across the data.
This projectivisation method is similar to the
HEAD method of (Nivre et al, 2006), but has two
interesting new characteristics. First, syntactic de-
pendencies are used to projectivise the semantic
dependencies. Because the graph of semantic roles
is disconnected, moving across semantic arcs is of-
ten not possible. This would cause a large number
of roles to be moved to ROOT. Second, our method
changes the semantic argument of a given pred-
icate, whereas syntactic dependency projectivisa-
tion changes the head of a given dependent. This
difference is motivated by a predicate-centred view
of semantic dependencies, as it avoids changing a
predicate to a node which is not a predicate.
3 The Learning Architecture
The synchronous derivations described above are
modelled with an Incremental Sigmoid Belief Net-
work (ISBN) (Titov and Henderson, 2007a). IS-
BNs are dynamic Bayesian Networks which incre-
mentally specify their model structure based on the
partial structure being built by a derivation. They
have previously been applied to constituency and
dependency parsing. In both cases the derivations
were based on a push-down automaton, but ISBNs
can be directly applied to any automaton. We suc-
cessfully apply ISBNs to a two-stack automaton,
without changing the machine learning methods.
3.1 The Incremental Sigmoid Belief Networks
ISBNs use vectors of latent variables to represent
properties of parsing history relevant to the next
decisions. Latent variables do not need to be anno-
tated in the training data, but instead get induced
during learning. As illustrated by the vectors S
i
in figure 1, the latent feature vectors are used to
estimate the probabilities of derivation actions D
i
.
s
SS
DD
S
i?c
i?c i?1
i?1
i
ij
Di dki
Figure 1: An ISBN for estimating
P (d
i
k
|history(i, k)) ? one of the elementary
decisions. Variables whose values are given in
history(i, k) are shaded, and latent and current
decision variables are unshaded.
Latent variable vectors are connected to variables
from previous positions via a pattern of edges de-
termined by the previous decisions. Our ISBN
model distinguishes two types of latent states: syn-
tactic states, when syntactic decisions are consid-
ered, and semantic states, when semantic decision
are made. Different patterns of interconnections
are used for different types of states. We use the
neural network approximation (Titov and Hender-
son, 2007a) to perform inference in our model.
As also illustrated in figure 1, the induced latent
variables S
i
at state i are statistically dependent on
both pre-defined features of the derivation history
D
1
, . . . , D
i?1
and the latent variables for a finite
set of relevant previous states S
i
?
, i
?
< i. Choos-
ing this set of relevant previous states is one of the
main design decisions in building an ISBN model.
By connecting to a previous state, we place that
state in the local context of the current decision.
This specification of the domain of locality deter-
mines the inductive bias of learning with ISBNs.
Thus, we need to choose the set of local (i.e. con-
nected) states in accordance with our prior knowl-
edge about which previous decisions are likely to
be particularly relevant to the current decision.
3.2 Layers and features
To choose previous relevant decisions, we make
use of the partial syntactic and semantic depen-
dency structures which have been decided so far
in the parse. Specifically, the current latent state
vector is connected to the most recent previous la-
tent state vectors (if they exist) whose configura-
tion shares a node with the current configuration,
as specified in Table 1. The nodes are chosen be-
cause their properties are thought to be relevant to
the current decision. Each row of the table indi-
cates which nodes need to be identical, while each
180
Closest Current Syn-Syn Srl-Srl Syn-Srl
Input Input + + +
Top Top + + +
RDT Top + +
LDT Top + +
HT Top + +
LDN Top + +
Input Top +
Table 1: Latent-to-latent variable connections. In-
put= input queue; Top= top of stack; RDT= right-
most right dependent of top; LDT= leftmost left
dependent of top; HT= Head of top; LDN= left-
most dependent of next (front of input).
column indicates whether the latent state vectors
are for the syntactic or semantic derivations. For
example, the first row indicates edges between the
current state and a state which had the same in-
put as the current state. The three columns indi-
cate that this edge holds within syntactic states,
within semantic states, and from syntactic to se-
mantic states. The fourth cell of the third row, for
example, indicates that there is an edge between
the current semantic state on top of the stack and
the most recent semantic state where the rightmost
dependent of the current top of the semantic stack
was at the top of the semantic stack.
Each of these relations has a distinct weight ma-
trix for the resulting edges in the ISBN, but the
same weight matrix is used at each position where
the relation applies. Training and testing times
scale linearly with the number of relations.
The pre-defined features of the parse history
which also influence the current decision are spec-
ified in table 2. The model distinguishes argument
roles of nominal predicates from argument roles of
verbal predicates.
3.3 Decoding
Given a trained ISBN as our probability esti-
mator, we search for the most probable joint
syntactic-semantic dependency structure using a
beam search. Most pruning is done just after each
shift operation (when the next word is predicted).
Global constraints (such as label uniqueness) are
not enforced by decoding, but can be learnt.
For the system whose results we submitted, we
then do a second step to improve on the choice
of syntactic dependency structure. Because of the
lack of edges in the graphical model from seman-
tic to syntactic states, it is easy to marginalise out
the semantic structure, giving us the most proba-
ble syntactic dependency structure. This syntactic
structure is then combined with the semantic struc-
State Stack Syntactic step features
LEX POS DEP
Input + +
Top syn + +
Top - 1 syn +
HT syn +
RDT syn +
LDT syn +
LDN syn +
State Stack Semantic step features
LEX POS DEP SENSE
Input + + +
Top sem + + +
Top - 1 sem + +
HT sem + +
RDT sem +
LDT sem +
LDN sem +
A0-A5 of Top sem +
A0-A5 of Input sem +
Table 2: Pre-defined features. syn=syntactic stack;
sem=semantic stack. Input= input queue; Top=
top of stack; RDT= rightmost dependent of top;
LDT= leftmost dependent of Top; HT= Head of
top; LDN= leftmost dependent of next (front of
input); A0-A5 of Top/Input= arguments of top of
stack / input.
ture from the first stage, to get our submitted re-
sults. This second stage does not maximise perfor-
mance on the joint syntactic-semantic dependency
structure, but it better fits the evaluation measure
used to rank systems.
4 Experiments and Discussion
The experimental set-up common for all the teams
is described in the introduction (Surdeanu et al,
2008). The submitted model has latent variable
vectors of 60 units, and a word frequency cut-off
of 100, resulting in a small vocabulary of 1083
words. We used a beam of size 15 to prune deriva-
tions after each shift operation to obtain the joint
structure, and a beam of size 40 when perform-
ing the marginalisation. Training took approxi-
mately 2.5 days on a standard PC with 3.0 GHz
Pentium4 CPU. It took approximately 2 hours to
parse the entire testing set (2,824 sentences) and
an additional 3 hours to perform syntactic parsing
when marginalising out the semantic structures.
2
Shortly after the submission deadline, we trained a
?large? model with a latent variable vector of size
80, a word frequency cut-off of 20, and additional
latent-to-latent connections from semantics to syn-
tax of the same configuration as the last column
2
A multifold speed-up with a small decrease in accuracy
can be achieved by using a small beam.
181
Syn Semantic Overall
LAS P R F1 P R F1
Submitted
D 86.1 78.8 64.7 71.1 82.5 75.4 78.8
W 87.8 79.6 66.2 72.3 83.7 77.0 80.2
B 80.0 66.6 55.3 60.4 73.3 67.6 70.3
WB 86.9 78.2 65.0 71.0 82.5 76.0 79.1
Joint inference
D 85.5 78.8 64.7 71.1 82.2 75.1 78.5
Large, joint inference
D 86.5 79.9 67.5 73.2 83.2 77.0 80.0
W 88.5 80.4 69.2 74.4 84.4 78.8 81.5
B 81.0 68.3 57.7 62.6 74.7 69.4 71.9
WB 87.6 79.1 67.9 73.1 83.4 77.8 80.5
Table 3: Scores on the development set and the
final testing sets (percentages). D= development
set; W=WSJ; B=Brown; WB=WSJ+Brown;
of table 1. This model took about 50% longer in
training and testing.
In table 3, we report results for the marginalised
inference (?submitted?) and joint inference for the
submitted model, and the results for joint inference
with the ?large? model. The larger model improves
on the submitted results by almost 1.5%, a signifi-
cant improvement. If completed earlier, this model
would have been fifth overall, second for syntactic
LAS, and fifth for semantic F1.
To explore the relationship between the two
components of the model, we removed the edges
between the syntax and the semantics in the sub-
mitted model. This model?s performance drops by
about 3.5% for semantic role labelling, thereby in-
dicating that the latent annotation of parsing states
helps semantic role labelling. However, it also
indicates that there is much room for improve-
ment in developing useful semantic-specific fea-
tures, which was not done for these experiments
simply due to constraints on development time.
To test whether joint learning degrades the ac-
curacy of the syntactic parsing model, we trained a
syntactic parsing model with the same features and
the same pattern of interconnections as used for the
syntactic states in our joint model. The resulting
labelled attachment score was non-significantly
lower (0.2%) than the score for the marginalised
inference with the joint model. This result sug-
gests that, though the latent variables associated
with syntactic states in the joint model were trained
to be useful in semantic role labelling, this did not
have a negative effect on syntactic parsing accu-
racy, and may even have helped.
Finally, an analysis of the errors on the develop-
ment set for the submitted model paints a coherent
picture. We find attachment of adjuncts particu-
larly hard. For dependency labels, we make the
most mistakes on modification labels, while for se-
mantic labels, we find TMP, ADV, LOC, and PRN
particularly hard. NomBank arcs are not learnt as
well as PropBank arcs: we identify PropBank SRL
arguments at F1 70.8% while Nombank arguments
reach 58.1%, and predicates at accuracy 87.9% for
PropBank and 74.9% for NomBank.
5 Conclusions
While still preliminary, these results indicate that
synchronous parsing is an effective way of build-
ing joint models on separate structures. The gen-
erality of the ISBN design used so far suggests
that ISBN?s latent feature induction extends well to
estimating very complex probability models, with
little need for feature engineering. Nonetheless,
performance could be improved by task-specific
features, which we plan for future work.
Acknowledgements
This work was partly funded by European Community FP7
grant 216594 (CLASSiC, www.classic-project.org), Swiss
NSF grant 114044, and Swiss NSF fellowships PBGE2-
117146 and PBGE22-119276. Part of this work was done
when G. Musillo was visiting MIT/CSAIL, hosted by Prof.
Michael Collins.
References
Marcus, M., B. Santorini, and M.A. Marcinkiewicz. 1993.
Building a large annotated corpus of English: the Penn
Treebank. Computational Linguistics, 19:313?330.
Merlo, P. and G. Musillo. 2008. Semantic parsing for high-
precision semantic role labelling. In Procs of CoNLL
2008, Manchester, UK.
Meyers, A., R. Reeves, C. Macleod, R. Szekely, V. Zielin-
ska, B. Young, and R. Grishman. 2004. The nombank
project: An interim report. In Meyers, A., editor, HLT-
NAACL 2004 Workshop: Frontiers in Corpus Annotation,
24?31, Boston, MA.
Musillo, G. and P. Merlo. 2006. Accurate semantic parsing
of the Proposition Bank. In Procs of NAACL 2006, New
York, NY.
Nivre, J., J. Hall, J. Nilsson, G. Eryigit, and S. Marinov. 2006.
Pseudo-projective dependency parsing with support vector
machines. In Proc. of CoNNL, 221?225, New York, USA.
Palmer, M., D. Gildea, and P. Kingsbury. 2005. The Propo-
sition Bank: An annotated corpus of semantic roles. Com-
putational Linguistics, 31:71?105.
Surdeanu, M., R. Johansson, A. Meyers, L. M`arquez, and J.
Nivre. 2008. The CoNLL-2008 shared task on joint pars-
ing of syntactic and semantic dependencies. In Procs of
CoNLL-2008, Manchester,UK.
Titov, I. and J. Henderson. 2007a. Constituent parsing with
incremental sigmoid belief networks. In Procs of ACL?07,
pages 632?639, Prague, Czech Republic.
Titov, I. and J. Henderson. 2007b. A latent variable model
for generative dependency parsing. In Procs of IWPT?07,
Prague, Czech Republic.
Wong, Y.W. and R. Mooney. 2007. Learning synchronous
grammars for semantic parsing with lambda calculus. In
Procs of ACL?07, 960?967, Prague, Czech Republic.
182
NAACL-HLT Workshop on the Induction of Linguistic Structure, pages 1?7,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Unsupervised Induction of Frame-Semantic Representations
Ashutosh Modi Ivan Titov Alexandre Klementiev
Saarland University
Saarbru?cken, Germany
{amodi|titov|aklement}@mmci.uni-saarland.de
Abstract
The frame-semantic parsing task is challeng-
ing for supervised techniques, even for those
few languages where relatively large amounts
of labeled data are available. In this prelim-
inary work, we consider unsupervised induc-
tion of frame-semantic representations. An
existing state-of-the-art Bayesian model for
PropBank-style unsupervised semantic role
induction (Titov and Klementiev, 2012) is ex-
tended to jointly induce semantic frames and
their roles. We evaluate the model perfor-
mance both quantitatively and qualitatively by
comparing the induced representation against
FrameNet annotations.
1 Introduction
Shallow representations of meaning, and semantic
role labels in particular, have a long history in lin-
guistics (Fillmore, 1968). In this paper we focus on
frame-semantic representations: a semantic frame is
a conceptual structure describing a situation (or an
entity) and its participants (or its properties). Par-
ticipants and properties are associated with seman-
tic roles (also called frame elements). For example,
following the FrameNet annotation guidelines (Rup-
penhofer et al, 2006), in the following sentences:
(a) [COOK Mary] cooks [FOOD the broccoli]
[CONTAINER in a small pan].
(b) Sautee [FOOD the onions] [MANNER gently ]
[TEMP SETTING on low heat].
the same semantic frame Apply Heat is evoked
by verbs cook and sautee, and roles COOK and
FOOD in the sentence (a) are filled by Mary and
the broccoli, respectively. Note that roles are spe-
cific to the frame, not to the individual lexical units
(verbs cook and sautee, in the example).1
Most approaches to predicting these representa-
tions, called semantic role labeling (SRL), have re-
lied on large annotated datasets (Gildea and Juraf-
sky, 2002; Carreras and Ma`rquez, 2005; Surdeanu
et al, 2008; Hajic? et al, 2009). By far, most of
this work has focused on PropBank-style represen-
tations (Palmer et al, 2005) where roles are defined
for each individual verb, or even individual senses of
a verb. The only exceptions are modifiers and roles
A0 and A1 which correspond to proto-agent (a doer,
or initiator of the action) and proto-patient (an af-
fected entity), respectively. However, the SRL task
is known to be especially hard for the FrameNet-
style representations for a number of reasons, in-
cluding, the lack of cross-frame correspondence for
most roles, fine-grain definitions of roles and frames
in FrameNet, and relatively small amounts of statis-
tically representative data (Erk and Pado, 2006; Das
et al, 2010; Palmer and Sporleder, 2010; Das and
Smith, 2011). Another reason for reduced interest in
predicting FrameNet representations is the lack of
annotated resources for most languages, with anno-
tated corpora available or being developed only for
English (Ruppenhofer et al, 2006), German (Bur-
chardt et al, 2006), Spanish (Subirats, 2009) and
Japanese (Ohara et al, 2004).
Due to scarcity of labeled data, purely unsuper-
vised set-ups recently started to receive considerable
attention (Swier and Stevenson, 2004; Grenager and
Manning, 2006; Lang and Lapata, 2010; Lang and
1More accurately, FrameNet distinguishes core and non-
core roles with non-core roles mostly corresponding to mod-
ifiers, e.g., MANNER in sentence (b). Non-core roles are
expected to generalize across frames.
1
cooks
Mary
the broccoli in a small  pan  
CONTAINER
COOK FOOD
Apply_Heat
Figure 1: An example of a semantic dependency graph.
Lapata, 2011a; Lang and Lapata, 2011b; Titov and
Klementiev, 2012). However, all these approaches
have focused on PropBank-style representations.
This may seem somewhat unnatural as FrameNet
representations, though arguably more powerful, are
harder to learn in the supervised setting, harder to
annotate, and annotated data is available for a con-
siderably fewer languages. This is the gap which we
address in this preliminary study.
More specifically, we extend an existing state-
of-the-art Bayesian model for unsupervised seman-
tic role labeling and apply it to support FrameNet-
style semantics. In other words, our method jointly
induces both frames and frame-specific semantic
roles. We experiment only with verbal predicates
and evaluate the performance of the model with re-
spect to some natural baselines. Though the scores
for frame induction are not high, we argue that this is
primarily due to very high granularity of FrameNet
frames which is hard to reproduce for unsupervised
systems, as the implicit supervision signal is not ca-
pable of providing these distinctions.
2 Task Definition
In this work, we use dependency representations
of frame semantics. Dependency representations
for SRL (Johansson and Nugues, 2008) were made
popular by CoNLL-2008 and CoNLL-2009 shared
tasks (Surdeanu et al, 2008; Hajic? et al, 2009), but
for English were limited to PropBank. Recently,
English FrameNet was also released in the depen-
dency format (Bauer et al, 2012). Instead of pre-
dicting argument spans, in dependency representa-
tion the goal is, roughly, to predict the syntactic head
of the argument. The semantic dependency repre-
sentation for sentence (a) is shown in Figure 1, la-
bels on edges denote roles and labels on words de-
note frames. Note that in practice the structures can
be more complex, as, for example, arguments can
evoke their own frames or the same arguments can
be shared by multiple predicates, as in right node
raising constructions.
The SRL task, or more specifically frame-
semantic parsing task consists, at least conceptually,
of four stages: (1) identification of frame-evoking
elements(FEE), (2) identification of arguments, (3)
frame labeling and (4) role labeling. In this work,
we focus only on the frame labeling and role label-
ing stages, relying on gold standard (i.e. the oracle)
for FEEs and role identification. In other words, our
goal is to label (or cluster) edges and nodes in the
dependency graph, Figure 1. Since we focus in this
study on verbal predicates only, the first stage would
be trivial and the second stage could be handled with
heuristics as in much of previous work on unsuper-
vised SRL (Lang and Lapata, 2011a; Titov and Kle-
mentiev, 2012).
Additionally to considering only verbal predi-
cates, we also assume that every verb belongs to
a single frame. This assumption, though restric-
tive, may be reasonable in practice as (a) the dis-
tributions across frames (i.e. senses) are gener-
ally highly skewed, (b) current state-of-the-art tech-
niques for word-sense induction hardly beat most-
frequent-sense baselines in accuracy metrics (Man-
andhar et al, 2010). This assumption, or its minor
relaxations, is relatively standard in work on unsu-
pervised semantic parsing tasks (Poon and Domin-
gos, 2009; Poon and Domingos, 2010; Titov and
Klementiev, 2011). From the modeling prospective,
there are no major obstacles to relaxing this assump-
tion, but it would lead to a major explosion of the
search space and, as a result, slow inference.
3 Model and Inference
We follow previous work on unsupervised seman-
tic role labeling (Lang and Lapata, 2011a; Titov
and Klementiev, 2012) and associate arguments with
their frame specific syntactic signatures which we
refer to as argument keys:
? Active or passive verb voice (ACT/PASS).
? Argument position relative to predicate
(LEFT/RIGHT).
? Syntactic relation to its governor.
? Preposition used for argument realization.
Semantic roles are then represented as clusters of
argument keys instead of individual argument occur-
rences. This representation aids our models in in-
ducing high purity clusters (of argument keys) while
2
reducing their granularity. Thus, if an argument key
k is assigned to a role r (k ? r), all of its occurrences
are labeled r.
3.1 A model for frame-semantic parsing
Our approach is similar to the models of Titov and
Klementiev (2012; 2011). Please, see Section 5 for
a discussion of the differences.
Our model encodes three assumptions about
frames and semantic roles. First, we assume that
the distribution of lexical units (verbal predicates)
is sparse for each semantic frame. Second, we en-
force the selectional restriction assumption: we as-
sume that the distribution over potential argument
fillers is sparse for every role, implying that ?peaky?
distributions of arguments for each role r are pre-
ferred to flat distributions. Third, each role normally
appears at most once per predicate occurrence. Our
inference will search for a frame and role clustering
which meets the above requirements to the maximal
extent.
Our model associates three distributions with each
frame. The first one (?) models the selection of lex-
ical units, the second (?) governs the selection of ar-
gument fillers for each semantic role, and the third
(?) models (and penalizes) duplicate occurrence of
roles. Each frame occurrence is generated indepen-
dently given these distributions. Let us describe the
model by first defining how the set of model param-
eters and an argument key clustering are drawn, and
then explaining the generation of individual frame
instances. The generative story is formally presented
in Figure 2.
For each frame, we begin by drawing a dis-
tribution of its lexical units from a DP prior
DP (?, H(P )) with a small concentration parame-
ter ?, and a base distribution H(P ), pre-computed as
normalized counts of all verbs in our dataset. Next,
we generate a partition of argument keys Bf from
CRP(?) with each subset r ? Bf representing a sin-
gle frame specific semantic role. The crucial part
of the model is the set of selectional preference pa-
rameters ?f,r, the distributions of arguments x for
each role r of frame f . We represent arguments by
lemmas of their syntactic heads.2 In order to encode
2For prepositional phrases, we take as head the head noun of
the object noun phrase as it encodes crucial lexical information.
However, the preposition is not ignored but rather encoded in
the assumption about sparseness of the distributions
?f,r, we draw them from the DP prior DP (?, H(A))
with a small concentration parameter ?, the base
probability distribution H(A) is just the normalized
frequencies of arguments in the corpus. Finally,
the geometric distribution ?f,r is used to model the
number of times a role r appears with a given frame
occurrence. The decision whether to generate at
least one role r is drawn from the uniform Bernoulli
distribution. If 0 is drawn then the semantic role is
not realized for the given occurrence, otherwise the
number of additional roles r is drawn from the ge-
ometric distribution Geom(?f,r). The Beta priors
over ? indicate the preference towards generating at
most one argument for each role.
Now, when parameters and argument key cluster-
ings are chosen, we can summarize the remainder of
the generative story as follows. We begin by inde-
pendently drawing occurrences for each frame. For
each frame occurrence, we first draw its lexical unit.
Then for each role we independently decide on the
number of role occurrences. Then we generate each
of the arguments (seeGenArgument in Figure 2) by
generating an argument key kf,r uniformly from the
set of argument keys assigned to the cluster r, and fi-
nally choosing its filler xf,r, where the filler is either
a lemma or the syntactic head of the argument.
3.2 Inference
We use a simple approximate inference algo-
rithm based on greedy search for the maximum a-
posteriori clustering of lexical units and argument
keys. We begin by assigning each verbal predi-
cate to its own frame, and then iteratively choose
a pair of frames and merge them. Note that each
merge involves inducing a new set of roles, i.e. a
re-clustering of argument keys, for the new merged
frame. We use the search procedure proposed in
(Titov and Klementiev, 2012), in order to cluster ar-
gument keys for each frame.
Our search procedure chooses a pair of frames to
merge based on the largest incremental change to the
objective due to the merge. Computing the change
involves re-clustering of argument keys, so consider-
ing all pairs of initial frames containing single verbal
predicates is computationally expensive. Instead, we
the corresponding argument key.
3
Parameters:
for each frame f = 1, 2, . . . :
?f ? DP (?, H(P )) [distrib of lexical units]
Bf ? CRP (?) [partition of arg keys]
for each role r ? Bf :
?f,r ? DP (?, H(A)) [distrib of arg fillers]
?f,r ? Beta(?0, ?1) [geom distr for dup roles]
Data Generation:
for each frame f = 1, 2, . . . :
for each occurrence of frame f :
p ? ?f [draw a lexical unit]
for every role r ? Bf :
if [n ? Unif(0, 1)] = 1: [role appears at least once]
GenArgument(f, r) [draw one arg]
while [n ? ?f,r] = 1: [continue generation]
GenArgument(f, r) [draw more args]
GenArgument(f, r):
kf,r ? Unif(1, . . . , |r|) [draw arg key]
xf,r ? ?f,r [draw arg filler]
Figure 2: Generative story for the frame-semantic parsing
model.
prune the space of possible pairs of verbs using a
simple but effective pre-processing step. Each verb
is associated with a vector of normalized aggregate
corpus counts of syntactic dependents of the verb
(ignoring the type of dependency relation). Cosine
similarity of these vectors are then used to prune the
pairs of verbs so that only verbs which are distribu-
tionally similar enough are considered for a merge.
Finally, the search terminates when no additional
merges result in a positive change to the objective.
4 Experimental Evaluation
4.1 Data
We used the dependency representation of the
FrameNet corpus (Bauer et al, 2012). The corpus is
automatically annotated with syntactic dependency
trees produced by the Stanford parser. The data con-
sists of 158,048 sentences with 3,474 unique verbal
predicates and 722 gold frames.
4.2 Evaluation Metrics
We cannot use supervised metrics to evaluate our
models, since we do not have an alignment between
gold labels and clusters induced in the unsupervised
setup. Instead, we use the standard purity (PU) and
collocation (CO) metrics as well as their harmonic
mean (F1) to measure the quality of the resulting
clusters. Purity measures the degree to which each
cluster contains arguments (verbs) sharing the same
gold role (gold frame) and collocation evaluates the
degree to which arguments (verbs) with the same
gold roles (gold frame) are assigned to a single clus-
ter, see (Lang and Lapata, 2010). As in previous
work, for role induction, the scores are first com-
puted for individual predicates and then averaged
with the weights proportional to the total number oc-
currences of roles for each predicate.
4.3 Model Parameters
The model parameters were tuned coarsely by visual
inspection: ? = 1.e-5, ? = 1.e-4, ? = 1, ?0 = 100,
?1 = 1.e-10. Only a single model was evaluated
quantitatively to avoid overfitting to the evaluation
set.
4.4 Qualitative Evaluation
Our model induced 128 multi-verb frames from the
dataset. Out of 78,039 predicate occurrences in the
data, these correspond to 18,963 verb occurrences
(or, approximately, 25%). Some examples of the
induced multi-verb frames are shown in Table 1.
As we can observe from the table, our model clus-
ters semantically related verbs into a single frame,
even though they may not correspond to the same
gold frame in FrameNet. Consider, for example, the
frame (ratify::sign::accede): the verbs are semanti-
cally related and hence they should go into a single
frame, as they all denote a similar action.
Another result worth noting is that the model of-
ten clusters antonyms together as they are often used
in similar context. For example, consider the frame
(cool::heat::warm), the verbs cool, heat and warm,
all denote a change in temperature. This agrees well
with annotation in FrameNet. Similarly, we clus-
ter sell and purchase together. This contrasts with
FrameNet annotation as FrameNet treats them not
as antonyms but as different views on same situation
and according to their guidelines, different frames
are assigned to different views.
Often frames in FrameNet correspond to more
fine-grained meanings of the verbs, as we can see
in the example for (plait::braid::dye). The three de-
scribe a similar activity involving hair but FrameNet
4
Induced frames FrameNet frames corresponding to the verbs
(rush::dash::tiptoe) rush : [Self motion](150) [Fluidic motion](19)
dash : [Self motion](100)
tiptoe : [Self motion](114)
(ratify::sign::accede) ratify : [Ratification](41)
sign : [Sign agreement](81) [Hiring](18) [Text Creation](1)
accede : [Sign Agreement](31)
(crane::lean::bustle) crane : [Body movement](26)
lean: [Change posture](70) [Placing](22) [Posture](12)
bustle : [Self motion](55)
(cool::heat::warm) cool : [Cause temperature change](27)
heat: [Cause temperature change](52)
warm: [Cause temperature change](41) [Inchoative change of temperature](16)
(want::fib::dare) want : [Desiring](105) [Possession](44)
fib : [Prevarication](9)
dare : [Daring](21)
(encourage::intimidate::confuse) encourage : [Stimulus focus](49)
intimidate : [Stimulus focus](26)
confuse: [Stimulus focus](45)
(happen::transpire::teach) happen : [Event](38) [Coincidence](21) [Eventive affecting](1)
transpire : [Event](15)
teach : [Education teaching](7)
(do::understand::hope) do : [Intentionally affect](6) [Intentionally act](56)
understand : [Grasp](74) [Awareness](57) [Categorization](15)
hope : [Desiring](77)
(frighten::vary::reassure) frighten : [Emotion directed](44)
vary : [Diversity](24)
reassure : [Stimulus focus](35)
(plait::braid::dye) plait : [Hair configuration](11) [Grooming](12)
braid : [Hair configuration](7) [Clothing parts](6) [Rope manipulation](4)
dye : [Processing materials](18)
(sell::purchase) sell : [Commerce sell](107)
purchase : [Commerce buy](93)
(glisten::sparkle::gleam) glisten : [Location of light](52) [Light movement](1)
sparkle : [Location of light](23) [Light movement](3)
gleam : [Location of light](77) [Light movement](4)
(forestall::shush) forestall : [Thwarting](12)
shush : [Silencing](6)
Table 1: Examples of the induced multi-verb frames. The left column shows the induced verb clusters and the right
column lists the gold frames corresponding to each verb and the number in the parentheses are their occurrence counts.
gives them a finer distinction. Arguably, implicit su-
pervision signal present in the unlabeled data is not
sufficient to provide such fine-grained distinctions.
The model does not distinguish verb senses, i.e. it
always assigns a single frame to each verb, so there
is an upper bound on our clustering performance.
4.5 Quantitative Evaluation
Nowwe turn to quantitative evaluation of both frame
and role induction.
Frame Labeling. In this section, we evaluate how
well the induced frames correspond to the gold stan-
dard annotation. Because of the lack of relevant
previous work, we use only a trivial baseline which
places each verb in a separate cluster (NoCluster-
ing). The results are summarized in Table 3.
As we can see from the results, our model
achieves a small, but probably significant, improve-
ment in the F1-score. Though the scores are
fairly low, note that, as discussed in Section 4.4,
the model is severely penalized even for induc-
ing semantically plausible frames such as the frame
(plait::braid::dye).
Role Labeling. In this section, we evaluate how
well the induced roles correspond to the gold stan-
dard annotation. We use two baselines: one is
the syntactic baseline SyntF, which simply clus-
ters arguments according to the dependency rela-
5
PU CO F1
Our approach 78.9 71.0 74.8
NoFrameInduction 79.2 70.7 74.7
SyntF 69.9 73.3 71.6
Table 2: Role labeling performance.
tion to their head, as described in (Lang and La-
pata, 2010), and the other one is a version of our
model which does not attempt to cluster verbs and
only induces roles (NoFrameInduction). Note that
the NoFrameInduction baseline is equivalent to the
factoredmodel of Titov and Klementiev (2012). The
results are summarized in Table 2.
First, observe that both our full model and its sim-
plified version NoFrameInduction significantly out-
perform the syntactic baseline. It is important to
note that the syntactic baseline is not trivial to beat
in the unsupervised setting (Lang and Lapata, 2010).
Though there is a minor improvement from inducing
frames, it is small and may not be significant.3
Another observation is that the absolute scores
of all the systems, including the baselines, are sig-
nificantly below the results reported in Titov and
Klementiev (Titov and Klementiev, 2012) on the
CoNLL-08 version of PropBank in a comparable
setting (auto parses, gold argument identification):
73.9 % and 77.9 % F1 for SyntF and NoFrameIn-
duction, respectively. We believe that the main rea-
son for this discrepancy is the difference in the syn-
tactic representations. The CoNLL-08 dependencies
include function tags (e.g., TMP, LOC), and, there-
fore, modifiers do not need to be predicted, whereas
the Stanford syntactic dependencies do not provide
this information and the model needs to induce it.
It is clear from these results, and also from the
previous observation that only 25% of verb occur-
rences belong to multi-verb clusters, that the model
does not induce sufficiently rich clustering of verbs.
Arguably, this is largely due to the relatively small
size of FrameNet, as it may not provide enough evi-
dence for clustering. Given that our method is quite
efficient, a single experiment was taking around 8
hours on a single CPU, and the procedure is highly
parallelizable, the next step would be to use a much
larger and statistically representative corpus to in-
duce the representations.
3There is no well-established methodology for testing statis-
tical significance when comparing two clustering methods.
PU CO F1
Our approach 77.9 31.4 44.7
NoClustering 80.8 29.0 42.7
Table 3: Frame labeling performance.
Additional visual inspection suggest that the data
is quite noisy primarily due to mistakes in parsing.
The large proportion of mistakes can probably be ex-
plained by the domain shift: the parser is trained on
the WSJ newswire data and tested on more general
BNC texts.
5 Related Work
The space constraints do not permit us to pro-
vide a comprehensive overview of related work.
Aside from the original model of Titov and Klemen-
tiev (2012), the most related previous method is the
Bayesian method of Titov and Klementiev (2011).
In that work, along with predicate-argument struc-
ture, they also induce clusterings of dependency
tree fragments (not necessarily verbs). However,
their approach uses a different model for argument
generation, a different inference procedure, and it
has only been applied and evaluated on biomedi-
cal data. The same shallow semantic parsing task
has also been considered in the work of Poon and
Domingos (2009; 2010), but using a MLN model
and, again, only on the biomedical domain. An-
other closely related vein of research is on semi-
supervised frame-semantic parsing (Fu?rstenau and
Lapata, 2009; Das and Smith, 2011).
6 Conclusions
This work is the first to consider the task of unsuper-
vised frame-semantic parsing. Though the quantita-
tive results are mixed, we showed that meaningful
semantic frames are induced. In the future work, we
intend to consider much larger corpora and to focus
on a more general set-up by relaxing the assumption
that frames are evoked only by verbal predicates.
Acknowledgements
The authors acknowledge the support of the MMCI Clus-
ter of Excellence, and thank Caroline Sporleder, Alexis
Palmer and the anonymous reviewers for their sugges-
tions.
6
References
Daniel Bauer, Hagen Fu?rstenau, and Owen Rambow.
2012. The dependency-parsed framenet corpus. In
International conference on Language Resources and
Evaluation (LREC), Istanbul, Turkey.
A. Burchardt, K. Erk, A. Frank, A. Kowalski, S. Pado,
and M. Pinkal. 2006. The SALSA corpus: a german
corpus resource for lexical semantics. In LREC.
Xavier Carreras and Llu??s Ma`rquez. 2005. Introduction
to the CoNLL-2005 Shared Task: Semantic Role La-
beling. In CoNLL.
D. Das and N.A. Smith. 2011. Semi-supervised frame-
semantic parsing for unknown predicates. In Pro-
ceedings of the 49th Annual Meeting of the Associa-
tion for Computational Linguistics: Human Language
Technologies-Volume 1, pages 1435?1444. Associa-
tion for Computational Linguistics.
D. Das, N. Schneider, D. Chen, and N.A. Smith. 2010.
Probabilistic frame-semantic parsing. In Human Lan-
guage Technologies: The 2010 Annual Conference of
the North American Chapter of the Association for
Computational Linguistics, pages 948?956. Associa-
tion for Computational Linguistics.
K. Erk and S. Pado. 2006. Shalmaneser?a toolchain for
shallow semantic parsing. In Proceedings of LREC,
volume 6. Citeseer.
Charles J. Fillmore. 1968. The case for case. In Bach
E. and Harms R.T., editors, Universals in Linguistic
Theory, pages 1?88. Holt, Rinehart, andWinston, New
York.
Hagen Fu?rstenau and Mirella Lapata. 2009. Graph align-
ment for semi-supervised semantic role labeling. In
EMNLP.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic la-
belling of semantic roles. Computational Linguistics,
28(3):245?288.
Trond Grenager and Christoph Manning. 2006. Un-
supervised discovery of a statistical verb lexicon. In
EMNLP.
Jan Hajic?, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Anto`nia Mart??, Llu??s
Ma`rquez, Adam Meyers, Joakim Nivre, Sebastian
Pado?, Jan S?te?pa?nek, Pavel Stran?a?k, Mihai Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The CoNLL-
2009 shared task: Syntactic and semantic dependen-
cies in multiple languages. In Proceedings of the
13th Conference on Computational Natural Language
Learning (CoNLL-2009), June 4-5.
Richard Johansson and Pierre Nugues. 2008.
Dependency-based semantic role labeling of Prop-
Bank. In EMNLP.
Joel Lang and Mirella Lapata. 2010. Unsupervised in-
duction of semantic roles. In ACL.
Joel Lang and Mirella Lapata. 2011a. Unsupervised se-
mantic role induction via split-merge clustering. In
ACL.
Joel Lang and Mirella Lapata. 2011b. Unsupervised
semantic role induction with graph partitioning. In
EMNLP.
Suresh Manandhar, Ioannis P. Klapaftis, Dmitriy Dli-
gach, and Sameer S. Pradhan. 2010. Semeval-2010
task 14: Word sense induction and disambiguation. In
Proceedings of the 5th International Workshop on Se-
mantic Evaluation.
K.H. Ohara, S. Fujii, T. Ohori, R. Suzuki, H. Saito, and
S. Ishizaki. 2004. The japanese framenet project:
An introduction. In Proceedings of LREC-04 Satellite
Workshop Building Lexical Resources from Semanti-
cally Annotated Corpora(LREC 2004), pages 9?11.
Alexis Palmer and Caroline Sporleder. 2010. Evaluating
FrameNet-style semantic parsing: the role of coverage
gaps in FrameNet. In COLING.
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The
proposition bank: An annotated corpus of semantic
roles. Computational Linguistics, 31(1):71?106.
Hoifung Poon and Pedro Domingos. 2009. Unsuper-
vised semantic parsing. In EMNLP.
H. Poon and P. Domingos. 2010. Unsupervised ontol-
ogy induction from text. In Proceedings of the 48th
Annual Meeting of the Association for Computational
Linguistics, pages 296?305. Association for Computa-
tional Linguistics.
Josef Ruppenhofer, Michael Ellsworth, Miriam
R. L. Petruck, Christopher R. Johnson, and
Jan Scheffczyk. 2006. Framenet ii: Ex-
tended theory and practice. available at http:
//framenet.icsi.berkeley.edu/index.
php?option=com_wrapper&Itemid=126.
C. Subirats. 2009. Spanish framenet: A frame-semantic
analysis of the spanish lexicon. Berlin/New York:
Mouton de Gruyter, pages 135?162.
Mihai Surdeanu, Adam Meyers Richard Johansson, Llu??s
Ma`rquez, and Joakim Nivre. 2008. The CoNLL-2008
shared task on joint parsing of syntactic and semantic
dependencies. In CoNLL 2008: Shared Task.
Richard Swier and Suzanne Stevenson. 2004. Unsuper-
vised semantic role labelling. In EMNLP.
Ivan Titov and Alexandre Klementiev. 2011. A Bayesian
model for unsupervised semantic parsing. In ACL.
Ivan Titov and Alexandre Klementiev. 2012. A bayesian
approach to semantic role induction. In Proc. EACL,
Avignon, France.
7
Proceedings of the Eighteenth Conference on Computational Language Learning, pages 49?57,
Baltimore, Maryland USA, June 26-27 2014.
c?2014 Association for Computational Linguistics
Inducing Neural Models of Script Knowledge
Ashutosh Modi
MMCI,
Saarland University, Germany
amodi@mmci.uni-saarland.de
Ivan Titov
ILLC,
University of Amsterdam, Netherlands
titov@uva.nl
Abstract
Induction of common sense knowledge
about prototypical sequence of events has
recently received much attention (e.g.,
Chambers and Jurafsky (2008); Regneri
et al. (2010)). Instead of inducing this
knowledge in the form of graphs, as in
much of the previous work, in our method,
distributed representations of event real-
izations are computed based on distributed
representations of predicates and their ar-
guments, and then these representations
are used to predict prototypical event or-
derings. The parameters of the composi-
tional process for computing the event rep-
resentations and the ranking component
of the model are jointly estimated. We
show that this approach results in a sub-
stantial boost in performance on the event
ordering task with respect to the previous
approaches, both on natural and crowd-
sourced texts.
1 Introduction
It is generally believed that natural language un-
derstanding systems would benefit from incorpo-
rating common-sense knowledge about prototyp-
ical sequences of events and their participants.
Early work focused on structured representations
of this knowledge (called scripts (Schank and
Abelson, 1977)) and manual construction of script
knowledge bases. However, these approaches do
not scale to complex domains (Mueller, 1998;
Gordon, 2001). More recently, automatic induc-
tion of script knowledge from text have started
to attract attention: these methods exploit ei-
ther natural texts (Chambers and Jurafsky, 2008,
2009) or crowdsourced data (Regneri et al., 2010),
and, consequently, do not require expensive ex-
pert annotation. Given a text corpus, they ex-
tract structured representations (i.e. graphs), for
example chains (Chambers and Jurafsky, 2008)
or more general directed acyclic graphs (Regneri
et al., 2010). These graphs are scenario-specific,
nodes in them correspond to events (and associ-
ated with sets of potential event mentions) and arcs
encode the temporal precedence relation. These
graphs can then be used to inform NLP applica-
tions (e.g., question answering) by providing in-
formation whether one event is likely to precede
or succeed another. Note that these graphs en-
code common-sense knowledge about prototypi-
cal ordering of events rather than temporal order
of events as described in a given text.
Though representing the script knowledge as
graphs is attractive from the human interpretability
perspective, it may not be optimal from the appli-
cation point of view. More specifically, these rep-
resentations (1) require a model designer to choose
an appropriate granularity of event mentions (e.g.,
whether nodes in the graph should be associated
with verbs, or also their arguments); (2) do not
provide a mechanism for deciding which scenario
applies in a given discourse context and (3) often
do not associate confidence levels with informa-
tion encoded in the graph (e.g., the precedence re-
lation in Regneri et al. (2010)).
Instead of constructing a graph and using it to
provide information (e.g., prototypical event or-
dering) to NLP applications, in this work we ad-
vocate for constructing a statistical model which is
capable to ?answer? at least some of the questions
these graphs can be used to answer, but doing this
without explicitly representing the knowledge as a
graph. In our method, the distributed representa-
tions (i.e. vectors of real numbers) of event real-
izations are computed based on distributed repre-
sentations of predicates and their arguments, and
then the event representations are used in a ranker
to predict the prototypical ordering of events. Both
the parameters of the compositional process for
computing the event representation and the rank-
49
ing component of the model are estimated from
texts (either relying on unambiguous discourse
clues or natural ordering in text). In this way we
build on recent research on compositional distri-
butional semantics (Baroni and Zamparelli, 2011;
Socher et al., 2012), though our approach specif-
ically focuses on embedding predicate-argument
structures rather than arbitrary phrases, and learn-
ing these representation to be especially informa-
tive for prototypical event ordering.
In order to get an intuition why the embedding
approach may be attractive, consider a situation
where a prototypical ordering of events the bus
disembarked passengers and the bus drove away
needs to be predicted. An approach based on fre-
quency of predicate pairs (Chambers and Jurafsky,
2008) (henceforth CJ08), is unlikely to make a
right prediction as driving usually precedes disem-
barking. Similarly, an approach which treats the
whole predicate-argument structure as an atomic
unit (Regneri et al., 2010) will probably fail as
well, as such a sparse model is unlikely to be ef-
fectively learnable even from large amounts of un-
labeled data. However, our embedding method
would be expected to capture relevant features of
the verb frames, namely, the transitive use for the
predicate disembark and the effect of the particle
away, and these features will then be used by the
ranking component to make the correct prediction.
In previous work on learning inference
rules (Berant et al., 2011), it has been shown
that enforcing transitivity constraints on the
inference rules results in significantly improved
performance. The same is likely to be true for
the event ordering task, as scripts have largely
linear structure, and observing that a ? b and
b ? c is likely to imply a ? c. Interestingly, in
our approach we learn the model which satisfies
transitivity constraints, without the need for any
explicit global optimization on a graph. This
results in a significant boost of performance when
using embeddings of just predicates (i.e. ignoring
arguments) with respect to using frequencies of
ordered verb pairs, as in CJ08 (76% vs. 61% on
the natural data).
Our model is solely focusing on the ordering
task, and admittedly does not represent all the in-
formation encoded by a script graph structure. For
example, it cannot be directly used to predict a
missing event given a set of events (the narrative
cloze task (Chambers and Jurafsky, 2009)). Nev-
disembarked passengersbus
predicate embedding 
event embedding
arg embedding
Ta
1
Rp
Ta
2
f(e)
a
1
= C(bus) a
2
= C(passenger)p = C(disembark)
arg embedding
hidden layer
h
Ah
Figure 1: Computation of an event representation
for a predicate with two arguments (the bus disem-
barked passengers), an arbitrary number of argu-
ments is supported by our approach.
ertheless, we believe that the framework (a proba-
bilistic model using event embeddings as its com-
ponent) can be extended to represent other aspects
of script knowledge by modifying the learning ob-
jective, but we leave this for future work. In this
paper, we show how our model can be used to pre-
dict if two event mentions are likely paraphrases
of the same event.
The approach is evaluated in two set-ups. First,
we consider the crowdsourced dataset of Regneri
et al. (2010) and demonstrate that using our model
results in the 13.5% absolute improvement in F1
on event ordering with respect to their graph in-
duction method (84.1% vs. 70.6%). Secondly,
we derive an event ordering dataset from the Gi-
gaword corpus, where we also show that the em-
bedding method beats the frequency-based base-
line (i.e. reimplementation of the scoring compo-
nent of CJ08) by 22.8% in accuracy (83.5% vs.
60.7%).
2 Model
In this section we describe the model we use for
computing event representations as well as the
ranking component of our model.
2.1 Event Representation
Learning and exploiting distributed word repre-
sentations (i.e. vectors of real values, also known
as embeddings) have been shown to be benefi-
cial in many NLP applications (Bengio et al.,
2001; Turian et al., 2010; Collobert et al., 2011).
These representations encode semantic and syn-
tactic properties of a word, and are normally
50
learned in the language modeling setting (i.e.
learned to be predictive of local word context),
though they can also be specialized by learning
in the context of other NLP applications such as
PoS tagging or semantic role labeling (Collobert
et al., 2011). More recently, the area of dis-
tributional compositional semantics have started
to emerge (Baroni and Zamparelli, 2011; Socher
et al., 2012), they focus on inducing represen-
tations of phrases by learning a compositional
model. Such a model would compute a represen-
tation of a phrase by starting with embeddings of
individual words in the phrase, often this composi-
tion process is recursive and guided by some form
of syntactic structure.
In our work, we use a simple compositional
model for representing semantics of a verb frame
e (i.e. the predicate and its arguments). We will
refer to such verb frames as events. The model is
shown in Figure 1. Each word c
i
in the vocabu-
lary is mapped to a real vector based on the cor-
responding lemma (the embedding function C).
The hidden layer is computed by summing linearly
transformed predicate and argument
1
embeddings
and passing it through the logistic sigmoid func-
tion. We use different transformation matrices for
arguments and predicates, T and R, respectively.
The event representation f(e) is then obtained by
applying another linear transform (matrix A) fol-
lowed by another application of the sigmoid func-
tion. Another point to note in here is that, as in
previous work on script induction, we use lemmas
for predicates and specifically filter out any tense
markers as our goal is to induce common-sense
knowledge about an event rather than properties
predictive of temporal order in a specific discourse
context.
We leave exploration of more complex and
linguistically-motivated models for future work.
2
These event representations are learned in the con-
text of event ranking: the transformation parame-
ters as well as representations of words are forced
to be predictive of the temporal order of events.
In our experiments, we also consider initialization
of predicate and arguments with the SENNA word
embeddings (Collobert et al., 2011).
1
Only syntactic heads of arguments are used in this work.
If an argument is a coffee maker, we will use only the word
maker.
2
In this study, we apply our model in two very differ-
ent settings, learning from crowdsourced and natural texts.
Crowdsourced collections are relatively small and require not
over-expressive models.
2.2 Learning to Order
The task of learning stereotyped order of events
naturally corresponds to the standard ranking set-
ting. We assume that we are provided with se-
quences of events, and our goal is to capture this
order. We discuss how we obtain this learning ma-
terial in the next section. We learn a linear ranker
(characterized by a vectorw) which takes an event
representation and returns a ranking score. Events
are then ordered according to the score to yield
the model prediction. Note that during the learn-
ing stage we estimate not only w but also the
event representation parameters, i.e. matrices T ,
R and A, and the word embedding C. Note that
by casting the event ordering task as a global rank-
ing problem we ensure that the model implicitly
exploits transitivity of the relation, the property
which is crucial for successful learning from finite
amount of data, as we argued in the introduction
and will confirm in our experiments.
At training time, we assume that each training
example k is a list of events e
(k)
1
, . . . , e
(k)
n
(k)
pro-
vided in the stereotypical order (i.e. e
(k)
i
? e
(k)
j
if
i < j), n
(k)
is the length of the list k. We mini-
mize the L
2
-regularized ranking hinge loss:
?
k
?
i<j?n
(k)
max(0, 1?w
T
f(e
(k)
i
;?)+w
T
f(e
(k)
j
;?))
+ ?(?w?
2
+ ???
2
),
where f(e;?) is the embedding computed
for event e, ? are all embedding parame-
ters corresponding to elements of the matrices
{R,C, T,A}. We use stochastic gradient descent,
gradients w.r.t. ? are computed using back propa-
gation.
3 Experiments
We evaluate our approach in two different set-ups.
First, we induce the model from the crowdsourced
data specifically collected for script induction by
Regneri et al. (2010), secondly, we consider an
arguably more challenging set-up of learning the
model from news data (Gigaword (Parker et al.,
2011)), in the latter case we use a learning sce-
nario inspired by Chambers and Jurafsky (2008).
3
3
Details about downloading the data and models are at:
http://www.coli.uni-saarland.de/projects/smile/docs/nmReadme.txt
51
Precision (%) Recall (%) F1 (%)
BL EE
verb
MSA BS EE BL EE
verb
MSA BS EE BL EE
verb
MSA BS EE
Bus 70.1 81.9 80.0 76.0 85.1 71.3 75.8 80.0 76.0 91.9 70.7 78.8 80.0 76.0 88.4
Coffee 70.1 73.7 70.0 68.0 69.5 72.6 75.1 78.0 57.0 71.0 71.3 74.4 74.0 62.0 70.2
Fastfood 69.9 81.0 53.0 97.0 90.0 65.1 79.1 81.0 65.0 87.9 67.4 80.0 64.0 78.0 88.9
Return 74.0 94.1 48.0 87.0 92.4 68.6 91.4 75.0 72.0 89.7 71.0 92.8 58.0 79.0 91.0
Iron 73.4 80.1 78.0 87.0 86.9 67.3 69.8 72.0 69.0 80.2 70.2 69.8 75.0 77.0 83.4
Microw. 72.6 79.2 47.0 91.0 82.9 63.4 62.8 83.0 74.0 90.3 67.7 70.0 60.0 82.0 86.4
Eggs 72.7 71.4 67.0 77.0 80.7 68.0 67.7 64.0 59.0 76.9 70.3 69.5 66.0 67.0 78.7
Shower 62.2 76.2 48.0 85.0 80.0 62.5 80.0 82.0 84.0 84.3 62.3 78.1 61.0 85.0 82.1
Phone 67.6 87.8 83.0 92.0 87.5 62.8 87.9 86.0 87.0 89.0 65.1 87.8 84.0 89.0 88.2
Vending 66.4 87.3 84.0 90.0 84.2 60.6 87.6 85.0 74.0 81.9 63.3 84.9 84.0 81.0 88.2
Average 69.9 81.3 65.8 85.0 83.9 66.2 77.2 78.6 71.7 84.3 68.0 79.1 70.6 77.6 84.1
Table 1: Results on the crowdsourced data for the verb-frequency baseline (BL), the verb-only embed-
ding model (EE
verb
), Regneri et al. (2010) (MSA), Frermann et al. (2014)(BS) and the full model (EE).
3.1 Learning from Crowdsourced Data
3.1.1 Data and task
Regneri et al. (2010) collected descriptions (called
event sequence descriptions, ESDs) of various
types of human activities (e.g., going to a restau-
rant, ironing clothes) using crowdsourcing (Ama-
zon Mechanical Turk), this dataset was also com-
plemented by descriptions provided in the OMICS
corpus (Gupta and Kochenderfer, 2004). The
datasets are fairly small, containing 30 ESDs per
activity type in average (we will refer to different
activities as scenarios), but in principle the col-
lection can easily be extended given the low cost
of crowdsourcing. The ESDs list events forming
the scenario and are written in a bullet-point style.
The annotators were asked to follow the prototyp-
ical event order in writing. As an example, con-
sider a ESD for the scenario prepare coffee :
{go to coffee maker} ? {fill water in coffee
maker} ? {place the filter in holder} ? {place
coffee in filter} ? {place holder in coffee maker}
? {turn on coffee maker}
Regneri et al. also automatically extracted pred-
icates and heads of arguments for each event, as
needed for their MSA system and our composi-
tional model.
Though individual ESDs may seem simple, the
learning task is challenging because of the limited
amount of training data, variability in the used vo-
cabulary, optionality of events (e.g., going to the
coffee machine may not be mentioned in a ESD),
different granularity of events and variability in
the ordering (e.g., coffee may be put in the filter
before placing it in the coffee maker). Unlike our
work, Regneri et al. (2010) relies on WordNet to
provide extra signal when using the Multiple Se-
quence Alignment (MSA) algorithm. As in their
work, each description was preprocessed to extract
a predicate and heads of argument noun phrases to
be used in the model.
The methods are evaluated on human anno-
tated scenario-specific tests: the goal is to classify
event pairs as appearing in a stereotypical order or
not (Regneri et al., 2010).
4
The model was estimated as explained in Sec-
tion 2.2 with the order of events in ESDs treated
as gold standard. We used 4 held-out scenarios
to choose model parameters, no scenario-specific
tuning was performed, and the 10 test scripts were
not used to perform model selection. The selected
model used the dimensionality of 10 for event and
word embeddings. The initial learning rate and the
regularization parameter were set to 0.005 and 1.0,
respectively and both parameters were reduced by
the factor of 1.2 every epoch the error function
went up. We used 2000 epochs of stochastic gradi-
ent descent. Dropout (Hinton et al., 2012) with the
rate of 20% was used for the hidden layers in all
our experiments. When testing, we predicted that
the event pair (e
1
,e
2
) is in the stereotypical order
(e
1
? e
2
) if the ranking score for e
1
exceeded the
ranking score for e
2
.
3.1.2 Results and discussion
We evaluated our event embedding model (EE)
against baseline systems (BL , MSA and BS). MSA
is the system of Regneri et al. (2010). BS is a
hierarchical Bayesian model by Frermann et al.
(2014). BL chooses the order of events based on
the preferred order of the corresponding verbs in
the training set: (e
1
, e
2
) is predicted to be in the
4
The event pairs are not coming from the same ESDs
making the task harder as the events may not be in any tem-
poral relation.
52
stereotypical order if the number of times the cor-
responding verbs v
1
and v
2
appear in this order
in the training ESDs exceeds the number of times
they appear in the opposite order (not necessary at
adjacent positions); a coin is tossed to break ties
(or if v
1
and v
2
are the same verb). This frequency
counting method was previously used in CJ08.
5
We also compare to the version of our model
which uses only verbs (EE
verbs
). Note that
EE
verbs
is conceptually very similar to BL, as it es-
sentially induces an ordering over verbs. However,
this ordering can benefit from the implicit transi-
tivity assumption used in EE
verbs
(and EE), as we
discussed in the introduction. The results are pre-
sented in Table 1.
The first observation is that the full model im-
proves substantially over the baseline and the pre-
vious method (MSA) in F1 (13.5% improvement
over MSA and 6.5% improvement over BS). Note
also that this improvement is consistent across sce-
narios: EE outperforms MSA and BS on 9 scenar-
ios out of 10 and 8 out of 10 scenarios in case of
BS. Unlike MSA and BS, no external knowledge
(i.e. WordNet) was exploited in our method.
We also observe a substantial improvement in
all metrics from using transitivity, as seen by com-
paring the results of BL and EE
verb
(11% improve-
ment in F1). This simple approach already sub-
stantially outperforms the pipelined MSA system.
These results seem to support our hypothesis in
the introduction that inducing graph representa-
tions from scripts may not be an optimal strategy
from the practical perspective.
We performed additional experiments using the
SENNA embeddings (Collobert et al., 2011). In-
stead of randomly initializing arguments and pred-
icate embeddings (vectors), we initialized them
with pre-trained SENNA embeddings. We have
not observed any significant boost in performance
from using the initialization (average F1 of 84.0%
for EE). We attribute the lack of significant im-
provement to the following three factors. First
of all, the SENNA embeddings tend to place
antonyms / opposites near each other (e.g., come
and go, or end and start). However, ?opposite?
predicates appear in very different positions in
scripts. Additionally, the SENNA embeddings
have dimensionality of 50 which appears to be
5
They scored permutations of several events by summing
the logarithmed differences of the frequencies of ordered verb
pairs. However, when applied to event pairs, their approach
would yield exactly the same prediction rule as BL.
too high for small crowd-sourced datasets, as it
forces us to use larger matrices T and R. More-
over, the SENNA embeddings are estimated from
Wikipedia, and the activities in our crowdsourced
domain are perhaps underrepresented there.
3.1.3 Paraphrasing
Regneri et al. (2010) additionally measure para-
phrasing performance of the MSA system by com-
paring it to human annotation they obtained: a sys-
tem needs to predict if a pair of event mentions are
paraphrases or not. The dataset contains 527 event
pairs for the 10 test scenarios. Each pair consists
of events from the same scenario. The dataset is
fairly balanced containing from 47 to 60 examples
per scenario.
This task does not directly map to any statisti-
cal inference problem with our model. Instead we
use an approach inspired by the interval algebra of
Allen (1983).
Our ranking model maps event mentions to po-
sitions on the time line (see Figure 2). However,
it would be more natural to assume that events are
intervals rather than points. In principle, these in-
tervals can be overlapping to encode a rich set of
temporal relations (see (Allen, 1983)). However,
we make a simplifying assumption that the inter-
vals do not overlap and every real number belongs
to an interval. In other words, our goal is to induce
a segmentation of the line: event mentions corre-
sponding to the same interval are then regarded as
paraphrases.
One natural constraint on this segmentation is
the following: if two event mentions are from the
same training ESD, they cannot be assigned to the
same interval (as events in ESD are not supposed
to be paraphrases). In Figure 2 arcs link event
mentions from the same ESD. We look for a seg-
mentation which produces the minimal number of
segments and satisfy the above constraint for event
mentions appearing in training data.
Though inducing intervals given a set of tem-
poral constraints is known to be NP-hard in gen-
eral (see, e.g., (Golumbic and Shamir, 1993)), for
our constraints a simple greedy algorithm finds an
optimal solution. We trace the line from the left
maintaining a set of event mentions in the current
unfinished interval and create a boundary when the
constraint is violated; we repeat the process un-
til we processed all mentions. In Figure 2, we
would create the first boundary between arrive
in a restaurant and order beverages: order bev-
53
En
t
e
r
 
a
r
e
s
t
a
u
r
a
n
t
A
r
r
i
v
e
 
i
n
 
a
 
r
e
s
t
a
u
r
a
n
t
...
O
r
d
e
r
 
b
e
v
e
r
a
g
e
s
B
r
o
w
s
e
 
a
 
m
e
n
u
R
e
v
i
e
w
 
o
p
t
i
o
n
s
 
i
n
 
a
 
m
e
n
u
Figure 2: Events on the time line, dotted arcs link
events from the same ESD.
erages and enter a restaurant are from the same
ESD and continuing the interval would violate
the constraint. It is not hard to see that this re-
sults in an optimal segmentation. First, the seg-
mentation satisfies the constraint by construction.
Secondly, the number of segments is minimal as
the arcs which caused boundary creation are non-
overlapping, each of these arcs needs to be cut and
our algorithm cuts each arc exactly once.
This algorithm prefers to introduce a bound-
ary as late as possible. For example, it would
introduce a boundary between browse a menu
and review options in a menu even though the
corresponding points are very close on the line.
We modify the algorithm by moving the bound-
aries left as long as this move does not result
in new constraint violations and increases mar-
gin at boundaries. In our example, the boundary
would be moved to be between order beverages
and browse a menu, as desired.
The resulting performance is reported in Ta-
ble 2. We report results of our method, as well as
results for MSA, BS and a simple all-paraphrase
baseline which predict that all mention pairs in a
test set are paraphrases (APBL).
6
We can see that
interval induction technique results in a lower F1
than that of MSA or BS. This might be partially
due to not using external knowledge (WordNet) in
our method.
We performed extra analyses on the develop-
ment scenario doorbell. The analyses revealed that
the interval induction approach is not very robust
to noise: removing a single noisy ESD results in a
dramatic change in the interval structure induced
and in a significant increase of F1. Consequently,
soft versions of the constraint would be beneficial.
Alternatively, event embeddings (i.e. continuous
vectors) can be clustered directly. We leave this
6
The results for the random baseline are lower: F1 of
40.6% in average.
Scenario F1 (%)
APBL MSA BS EE
Take bus 53.7 74.0 47.0 63.5
Make coffee 42.1 65.0 52.0 63.5
Order fastfood 37.0 59.0 80.0 62.6
Return food back 64.8 71.0 67.0 81.1
Iron clothes 43.3 67.0 60.0 56.7
Microwave cooking
43.2 75.0 82.0 57.8
Scrambled eggs
57.6 69.0 76.0 53.0
Take shower 42.1 78.0 67.0 55.7
Answer telephone 71.0 89.0 81.0 79.4
Vending machine
56.1 69.0 77.0 69.3
Average 51.1 71.6 68.9 64.5
Table 2: Paraphrasing results on the crowdsourced
data for Regneri et al. (2010) (MSA), Frermann
et al. (2014)(BS) and the all-paraphrase baseline
(APBL) and using intervals induced from our
model (EE).
investigation for future work.
3.2 Learning from Natural Text
In the second set of experiments we consider a
more challenging problem, inducing knowledge
about the stereotyped ordering of events from nat-
ural texts. In this work, we are largely inspired
by the scenario of CJ08. The overall strategy is
the following: we process the Gigaword corpus
with a high precision rule-based temporal classi-
fier relying on explicit clues (e.g., ?then?, ?after?)
to get ordered pairs of events and then we train
our model on these pairs (note that clues used by
the classifier are removed from the examples, so
the model has to rely on verbs and their argu-
ments). Conceptually, the difference between our
approach and CJ08 is in using a different tempo-
ral classifier, not enforcing that event pairs have
the same protagonist, and learning an event em-
bedding model instead of scoring event sequences
based on verb-pair frequencies.
We also evaluate our system on examples ex-
tracted using the same temporal classifier (but val-
idated manually) which allows us to use much
larger tests set, and, consequently, provide more
detailed and reliable error analysis.
3.2.1 Data and task
The Gigaword corpus consists of news data from
different news agencies and newspapers. For test-
ing and development we took the AFP (Agence
France-Presse) section, as it appeared most differ-
ent from the rest when comparing sets of extracted
event pairs (other sections correspond mostly to
US agencies). The AFP section was not used for
54
Accuracy (%)
BL 60.7
CJ08 60.1
EE
verb
75.9
EE 83.5
Table 3: Results on the Gigaword data for the
verb-frequency baseline (BL), the verb-only em-
bedding model (EE
verb
), the full model (EE) and
CJ08 rules.
training. This selection strategy was chosen to cre-
ate a negative bias for our model which is more
expressive than the baseline methods and, conse-
quently, better at memorizing examples.
As a rule-based temporal classifier, we used
high precision ?happens-before? rules from the
VerbOcean system (Chklovski and Pantel, 2004).
Consider ?to ?verb-x? and then ?verb-y?? as one
example of such rule. We used predicted collapsed
Stanford dependencies (de Marneffe et al., 2006)
to extract arguments of the verbs, and used only
a subset of dependents of a verb.
7
This prepro-
cessing ensured that (1) clues which form part of
a pattern are not observable by our model both at
train and test time; (2) there is no systematic dif-
ference between both events (e.g., for collapsed
dependencies, the noun subject is attached to both
verbs even if the verbs are conjoined); (3) no in-
formation about the order of events in text is avail-
able to the models. Applying these rules resulted
in 22,446 event pairs for training, and we split
additional 1,015 pairs from the AFP section into
812 for final testing and 203 for development. We
manually validated random 50 examples and all 50
of them followed the correct temporal order, so we
chose not to hand correct the test set.
We largely followed the same training and eval-
uation regime as for the crowdsourced data. We
set the regularization parameter and the learning
rate to 0.01 and 5.e ? 4 respectively. The model
was trained for 600 epochs. The embedding sizes
were 30 and 50 dimensions for words and events,
respectively.
3.2.2 Results and discussion
In our experiments, as before, we use BL as a
baseline, and EE
verb
as a verb-only simplified
version of our approach. We used another baseline
7
The list of dependencies not considered: aux, auxpass,
attr, appos, cc, conj, complm, cop, dep, det, punct, mwe.
consisting of the verb pair ordering counts pro-
vided by Chambers and Jurafsky (2008).
8
We re-
fer this baseline as CJ08. Note also that BL can be
regarded as a reimplementation of CJ08 but with
a different temporal classifier. We report results in
Table 3.
The observations are largerly the same as be-
fore: (1) the full model substantially outperforms
all other approaches (p-level < 0.001 with the per-
mutation test); (2) enforcing transitivity is very
helpful (75.9 % for EE
verb
vs. 60.1% for BL).
Surprisingly CJ08 rules produce as good results
as BL, suggesting that maybe our learning set-ups
are not that different.
However, an interesting question is in which sit-
uations using a more expressive model, EE, is ben-
eficial. If these accuracy gains have to do with
memorizing the data, it may not generalize well
to other domains or datasets. In order to test this
hypothesis we divided the test examples in three
frequency bands according to the frequency of the
corresponding verb pairs in the training set (to-
tal, in both orders). There are 513, 249 and 50
event pairs in the bands corresponding to unseen
pairs of verbs, frequency ? 10 and frequency >
10, respectively. These counts emphasize that cor-
rect predictions on unseen pairs are crucial and
these are exactly where BL would be equivalent
to a random guess. Also, this suggest, even before
looking into the results, that memorization is irrel-
evant. The results for BL, CJ08, EE
verb
and EE
are shown in Figure 3.
One observation is that most gains for EE and
EE
verb
are due to an improvement on unseen pairs.
This is fairly natural, as both transitivity and in-
formation about arguments are the only sources
of information available. In this context it is im-
portant to note that some of the verbs are light,
in the sense that they have little semantic content
of their own (e.g., take, get) and the event seman-
tics can only be derived from analyzing their argu-
ments (e.g., take an exam vs. take a detour). On
the high frequency verb pairs all systems perform
equally well, except for CJ08 as it was estimated
from somewhat different data.
In order to understand how transitivity works,
we considered a few unseen predicate pairs where
the EE
verb
model was correctly predicting their
order. For many of these pairs there were no infer-
8
These verb pair frequency counts are available at
www.usna.edu/Users/cs/nchamber/data/schemas/acl09/verb-
pair-orders.gz
55
??
??
??
??
??
??
???
?????? ?????? ????
50.0
57.2
71.0
82.4
62.7
77.8
81.8
83.1
81.2
96.0
94.1
96.0
CJ08
BL
EEverb
EE
Figure 3: Results for different frequency bands:
unseen, medium frequency (between 1 and 10)
and high frequency (> 10) verb pairs.
ence chains of length 2 (e.g., chain of length 2 was
found for the pair accept ? carry: accept ? get
and get ? carry but not many other pairs). This
observation suggest that our model captures some
non-trivial transitivity rules.
4 Related Work
Additionally to the work on script induction
discussed above (Chambers and Jurafsky, 2008,
2009; Regneri et al., 2010), other methods for
unsupervised learning of event semantics have
been proposed. These methods include unsu-
pervised frame induction techniques (O?Connor,
2012; Modi et al., 2012). Frames encode situa-
tions (or objects) along with their participants and
properties (Fillmore, 1976). Events in these un-
supervised approaches are represented with cate-
gorical latent variables, and they are induced rely-
ing primarily on the selectional preferences? sig-
nal. The very recent work of Cheung et al. (2013)
can be regarded as their extension but Cheung et
al. also model transitions between events with
Markov models. However, neither of these ap-
proaches considers (or directly optimizes) the dis-
criminative objective of learning to order events,
and neither of them uses distributed representa-
tions to encode semantic properties of events.
As we pointed out before, our embedding ap-
proach is similar (or, in fact, a simplification of)
the phrase embedding methods studied in the re-
cent work on distributional compositional seman-
tics (Baroni and Zamparelli, 2011; Socher et al.,
2012). However, they have not specifically looked
into representing script information. Approaches
which study embeddings of relations in knowledge
bases (e.g., Riedel et al. (2013)) bear some similar-
ity to the methods proposed in this work but they
are mostly limited to binary relations and deal with
predicting missing relations rather than with tem-
poral reasoning of any kind.
Identification of temporal relations within a text
is a challenging problem and an active area of re-
search (see, e.g., the TempEval task (UzZaman
et al., 2013)). Many rule-based and supervised ap-
proaches have been proposed in the past. How-
ever, integration of common sense knowledge in-
duced from large-scale unannotated resources still
remains a challenge. We believe that our approach
will provide a powerful signal complementary to
information exploited by most existing methods.
5 Conclusions
We have developed a statistical model for rep-
resenting common sense knowledge about proto-
typical event orderings. Our model induces dis-
tributed representations of events by composing
predicate and argument representations. These
representations capture properties relevant to pre-
dicting stereotyped orderings of events. We learn
these representations and the ordering component
from unannotated data. We evaluated our model
in two different settings: from crowdsourced data
and natural news texts. In both set-ups our method
outperformed baselines and previously proposed
systems by a large margin. This boost in perfor-
mance is primarily caused by exploiting transitiv-
ity of temporal relations and capturing information
encoded by predicate arguments.
The primary area of future work is to exploit
our method in applications such as question an-
swering. Another obvious applications is discov-
ery of temporal relations within documents (Uz-
Zaman et al., 2013) where common sense knowl-
edge implicit in script information, induced from
large unannotated corpora, should be highly ben-
eficial. Our current model uses a fairly naive se-
mantic composition component, we plan to extend
it with more powerful recursive embedding meth-
ods which should be especially beneficial when
considering very large text collections.
6 Acknowledgements
Thanks to Lea Frermann, Michaela Regneri and
Manfred Pinkal for suggestions and help with the
data. This work is partially supported by the
MMCI Cluster of Excellence at the Saarland Uni-
versity.
56
References
James F Allen. 1983. Maintaining knowledge
about temporal intervals. Communications of
the ACM, 26(11):832?843.
Marco Baroni and Robert Zamparelli. 2011.
Nouns are vectors, adjectives are matrices: Rep-
resenting adjective-noun constructions in se-
mantic space. In Proceedings of EMNLP.
Yoshua Bengio, R?ejean Ducharme, and Pascal
Vincent. 2001. A neural probabilistic language
model. In Proceedings of NIPS.
Jonathan Berant, Ido Dagan, and Jacob Gold-
berger. 2011. Global learning of typed entail-
ment rules. In Proceedings of ACL.
Nathanael Chambers and Dan Jurafsky. 2009. Un-
supervised learning of narrative schemas and
their participants. In Proceedings of ACL.
Nathanael Chambers and Daniel Jurafsky. 2008.
Unsupervised learning of narrative event chains.
In Proceedings of ACL.
Jackie Chi Kit Cheung, Hoifung Poon, and Lucy
Vanderwende. 2013. Probabilistic frame induc-
tion. In Proceedings of NAACL.
Timothy Chklovski and Patrick Pantel. 2004. Ver-
bocean: Mining the web for fine-grained se-
mantic verb relations. In Proceedings of
EMNLP.
R. Collobert, J. Weston, L. Bottou, M. Karlen,
K. Kavukcuoglu, and P. Kuksa. 2011. Natural
language processing (almost) from scratch.
Journal of Machine Learning Research,
12:2493?2537.
Marie-Catherine de Marneffe, Bill MacCartney,
and Christopher D. Manning. 2006. Generating
typed dependency parses from phrase structure
parses. In Proceedings of LREC.
Charles Fillmore. 1976. Frame semantics and the
nature of language. Annals of the New York
Academy of Sciences, 280(1):20?32.
Lea Frermann, Ivan Titov, and Manfred Pinkal.
2014. A hierarchical bayesian model for un-
supervised induction of script knowledge. In
EACL, Gothenberg, Sweden.
Martin Charles Golumbic and Ron Shamir. 1993.
Complexity and algorithms for reasoning about
time: A graph-theoretic approach. Journal of
ACM, 40(5):1108?1133.
Andrew Gordon. 2001. Browsing image collec-
tions with representations of common-sense ac-
tivities. JAIST, 52(11).
Rakesh Gupta and Mykel J. Kochenderfer. 2004.
Common sense data acquisition for indoor mo-
bile robots. In Proceedings of AAAI.
Geoffrey E. Hinton, Nitish Srivastava, Alex
Krizhevsky, Ilya Sutskever, and Ruslan
Salakhutdinov. 2012. Improving neural net-
works by preventing co-adaptation of feature
detectors. arXiv: CoRR, abs/1207.0580.
Ashutosh Modi, Ivan Titov, and Alexandre Kle-
mentiev. 2012. Unsupervised induction of
frame-semantic representations. In Proceedings
of the NAACL-HLT Workshop on Inducing Lin-
guistic Structure. Montreal, Canada.
Erik T. Mueller. 1998. Natural Language Process-
ing with Thought Treasure. Signiform.
Brendan O?Connor. 2012. Learning frames from
text with an unsupervised latent variable model.
CMU Technical Report.
Robert Parker, David Graff, Junbo Kong,
Ke Chen, and Kazuaki Maeda. 2011. En-
glish gigaword fifth edition. Linguistic Data
Consortium.
Michaela Regneri, Alexander Koller, and Manfred
Pinkal. 2010. Learning script knowledge with
web experiments. In Proceedings of ACL.
Sebastian Riedel, Limin Yao, Andrew McCal-
lum, and Benjamin Marlin. 2013. Relation ex-
traction with matrix factorization and universal
schemas. TACL.
R. C Schank and R. P Abelson. 1977. Scripts,
Plans, Goals, and Understanding. Lawrence
Erlbaum Associates, Potomac, Maryland.
Richard Socher, Brody Huval, Christopher D.
Manning, and Andrew Y. Ng. 2012. Seman-
tic compositionality through recursive matrix-
vector spaces. In Proceedings of EMNLP.
Joseph Turian, Lev Ratinov, and Yoshua Bengio.
2010. Word representations: A simple and gen-
eral method for semi-supervised learning. In
Proceedings of ACL.
Naushad UzZaman, Hector Llorens, Leon Der-
czynski, James Allen, Marc Verhagen, and
James Pustejovsky. 2013. Semeval-2013 task
1: Tempeval-3: Evaluating time expressions,
events, and temporal relations. In Proceedings
of SemEval.
57
