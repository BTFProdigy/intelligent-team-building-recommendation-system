Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 214?220,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Learning to Differentiate Better from Worse Translations
Francisco Guzm
?
an Shafiq Joty Llu??s M
`
arquez
Alessandro Moschitti Preslav Nakov Massimo Nicosia
ALT Research Group
Qatar Computing Research Institute ? Qatar Foundation
{fguzman,sjoty,lmarquez,amoschitti,pnakov,mnicosia}@qf.org.qa
Abstract
We present a pairwise learning-to-rank
approach to machine translation evalua-
tion that learns to differentiate better from
worse translations in the context of a given
reference. We integrate several layers
of linguistic information encapsulated in
tree-based structures, making use of both
the reference and the system output simul-
taneously, thus bringing our ranking closer
to how humans evaluate translations. Most
importantly, instead of deciding upfront
which types of features are important, we
use the learning framework of preference
re-ranking kernels to learn the features au-
tomatically. The evaluation results show
that learning in the proposed framework
yields better correlation with humans than
computing the direct similarity over the
same type of structures. Also, we show
our structural kernel learning (SKL) can
be a general framework for MT evaluation,
in which syntactic and semantic informa-
tion can be naturally incorporated.
1 Introduction
We have seen in recent years fast improvement
in the overall quality of machine translation (MT)
systems. This was only possible because of the
use of automatic metrics for MT evaluation, such
as BLEU (Papineni et al., 2002), which is the de-
facto standard; and more recently: TER (Snover et
al., 2006) and METEOR (Lavie and Denkowski,
2009), among other emerging MT evaluation met-
rics. These automatic metrics provide fast and in-
expensive means to compare the output of differ-
ent MT systems, without the need to ask for hu-
man judgments each time the MT system has been
changed.
As a result, this has enabled rapid develop-
ment in the field of statistical machine translation
(SMT), by allowing to train and tune systems as
well as to track progress in a way that highly cor-
relates with human judgments.
Today, MT evaluation is an active field of re-
search, and modern metrics perform analysis at
various levels, e.g., lexical (Papineni et al., 2002;
Snover et al., 2006), including synonymy and
paraphrasing (Lavie and Denkowski, 2009); syn-
tactic (Gim?enez and M`arquez, 2007; Popovi?c
and Ney, 2007; Liu and Gildea, 2005); semantic
(Gim?enez and M`arquez, 2007; Lo et al., 2012);
and discourse (Comelles et al., 2010; Wong and
Kit, 2012; Guzm?an et al., 2014; Joty et al., 2014).
Automatic MT evaluation metrics compare the
output of a system to one or more human ref-
erences in order to produce a similarity score.
The quality of such a metric is typically judged
in terms of correlation of the scores it produces
with scores given by human judges. As a result,
some evaluation metrics have been trained to re-
produce the scores assigned by humans as closely
as possible (Albrecht and Hwa, 2008). Unfortu-
nately, humans have a hard time assigning an ab-
solute score to a translation. Hence, direct hu-
man evaluation scores such as adequacy and flu-
ency, which were widely used in the past, are
now discontinued in favor of ranking-based eval-
uations, where judges are asked to rank the out-
put of 2 to 5 systems instead. It has been shown
that using such ranking-based assessments yields
much higher inter-annotator agreement (Callison-
Burch et al., 2007).
While evaluation metrics still produce numeri-
cal scores, in part because MT evaluation shared
tasks at NIST and WMT ask for it, there has also
been work on a ranking formulation of the MT
evaluation task for a given set of outputs. This
was shown to yield higher correlation with human
judgments (Duh, 2008; Song and Cohn, 2011).
214
Learning automatic metrics in a pairwise set-
ting, i.e., learning to distinguish between two al-
ternative translations and to decide which of the
two is better (which is arguably one of the easiest
ways to produce a ranking), emulates closely how
human judges perform evaluation assessments in
reality. Instead of learning a similarity function
between a translation and the reference, they learn
how to differentiate a better from a worse trans-
lation given a corresponding reference. While the
pairwise setting does not provide an absolute qual-
ity scoring metric, it is useful for most evaluation
and MT development scenarios.
In this paper, we propose a pairwise learning
setting similar to that of Duh (2008), but we extend
it to a new level, both in terms of feature represen-
tation and learning framework. First, we integrate
several layers of linguistic information encapsu-
lated in tree-based structures; Duh (2008) only
used lexical and POS matches as features. Second,
we use information about both the reference and
two alternative translations simultaneously, thus
bringing our ranking closer to how humans rank
translations. Finally, instead of deciding upfront
which types of features between hypotheses and
references are important, we use a our structural
kernel learning (SKL) framework to generate and
select them automatically.
The structural kernel learning (SKL) framework
we propose consists in: (i) designing a struc-
tural representation, e.g., using syntactic and dis-
course trees of translation hypotheses and a refer-
ences; and (ii) applying structural kernels (Mos-
chitti, 2006; Moschitti, 2008), to such representa-
tions in order to automatically inject structural fea-
tures in the preference re-ranking algorithm. We
use this method with translation-reference pairs
to directly learn the features themselves, instead
of learning the importance of a predetermined set
of features. A similar learning framework has
been proven to be effective for question answer-
ing (Moschitti et al., 2007), and textual entailment
recognition (Zanzotto and Moschitti, 2006).
Our goals are twofold: (i) in the short term, to
demonstrate that structural kernel learning is suit-
able for this task, and can effectively learn to rank
hypotheses at the segment-level; and (ii) in the
long term, to show that this approach provides a
unified framework that allows to integrate several
layers of linguistic analysis and information and to
improve over the state-of-the-art.
Below we report the results of some initial ex-
periments using syntactic and discourse structures.
We show that learning in the proposed framework
yields better correlation with humans than apply-
ing the traditional translation?reference similarity
metrics using the same type of structures. We
also show that the contributions of syntax and dis-
course information are cumulative. Finally, de-
spite the limited information we use, we achieve
correlation at the segment level that outperforms
BLEU and other metrics at WMT12, e.g., our met-
ric would have been ranked higher in terms of cor-
relation with human judgments compared to TER,
NIST, and BLEU in the WMT12 Metrics shared
task (Callison-Burch et al., 2012).
2 Kernel-based Learning from Linguistic
Structures
In our pairwise setting, each sentence s in
the source language is represented by a tuple
?t
1
, t
2
, r?, where t
1
and t
2
are two alternative
translations and r is a reference translation. Our
goal is to develop a classifier of such tuples that
decides whether t
1
is a better translation than t
2
given the reference r.
Engineering features for deciding whether t
1
is
a better translation than t
2
is a difficult task. Thus,
we rely on the automatic feature extraction en-
abled by the SKL framework, and our task is re-
duced to choosing: (i) a meaningful structural rep-
resentation for ?t
1
, t
2
, r?, and (ii) a feature func-
tion ?
mt
that maps such structures to substruc-
tures, i.e., our feature space. Since the design
of ?
mt
is complex, we use tree kernels applied
to two simpler structural mappings ?
M
(t
1
, r) and
?
M
(t
2
, r). The latter generate the tree representa-
tions for the translation-reference pairs (t
1
, r) and
(t
2
, r). The next section shows such mappings.
2.1 Representations
To represent a translation-reference pair (t, r), we
adopt shallow syntactic trees combined with RST-
style discourse trees. Shallow trees have been
successfully used for question answering (Severyn
and Moschitti, 2012) and semantic textual sim-
ilarity (Severyn et al., 2013b); while discourse
information has proved useful in MT evaluation
(Guzm?an et al., 2014; Joty et al., 2014). Com-
bined shallow syntax and discourse trees worked
well for concept segmentation and labeling (Saleh
et al., 2014a).
215
DIS:ELABORATION
EDU:NUCLEUS EDU:SATELLITE-REL
VP NP-REL NP VP-REL o-REL o-REL
RB TO-REL VB-REL PRP-REL DT NN-REL TO-REL VB-REL .-REL ''-REL
not to give them the time to think . "
VP NP-REL NP VP-REL o-REL o-REL
TO-REL `` VB-REL PRP-REL DT NN-REL TO-REL VB-REL .-REL ''-REL
to " give them no time to think . "
a) Hypothesis
b) Reference DIS:ELABORATION
EDU:NUCLEUS EDU:SATELLITE-REL
Bag-of-words relations 
rela
tion
 pro
pag
atio
n di
rect
ion
Figure 1: Hypothesis and reference trees combining discourse, shallow syntax and POS.
Figure 1 shows two example trees combining
discourse, shallow syntax and POS: one for a
translation hypothesis (top) and the other one for
the reference (bottom). To build such structures,
we used the Stanford POS tagger (Toutanova et
al., 2003), the Illinois chunker (Punyakanok and
Roth, 2001), and the discourse parser
1
of (Joty et
al., 2012; Joty et al., 2013).
The lexical items constitute the leaves of the
tree. The words are connected to their respec-
tive POS tags, which are in turn grouped into
chunks. Then, the chunks are grouped into el-
ementary discourse units (EDU), to which the
nuclearity status is attached (i.e., NUCLEUS or
SATELLITE). Finally, EDUs and higher-order dis-
course units are connected by discourse relations
(e.g., DIS:ELABORATION).
2.2 Kernels-based modeling
In the SKL framework, the learning objects are
pairs of translations ?t
1
, t
2
?. Our objective is to
automatically learn which pair features are impor-
tant, independently of the source sentence. We
achieve this by using kernel machines (KMs) over
two learning objects ?t
1
, t
2
?, ?t
?
1
, t
?
2
?, along with
an explicit and structural representation of the
pairs (see Fig. 1).
1
The discourse parser can be downloaded from
http://alt.qcri.org/tools/
More specifically, KMs carry out learning using
the scalar product
K
mt
(?t
1
, t
2
?, ?t
?
1
, t
?
2
?) = ?
mt
(t
1
, t
2
) ??
mt
(t
?
1
, t
?
2
),
where ?
mt
maps pairs into the feature space.
Considering that our task is to decide whether
t
1
is better than t
2
, we can conveniently rep-
resent the vector for the pair in terms of the
difference between the two translation vectors,
i.e., ?
mt
(t
1
, t
2
) = ?
K
(t
1
) ? ?
K
(t
2
). We can
approximate K
mt
with a preference kernel PK to
compute this difference in the kernel space K:
PK(?t
1
, t
2
?, ?t
?
1
, t
?
2
?) (1)
= K(t
1
)? ?
K
(t
2
)) ? (?
K
(t
?
1
)? ?
K
(t
?
2
))
= K(t
1
, t
?
1
) +K(t
2
, t
?
2
)?K(t
1
, t
?
2
)?K(t
2
, t
?
1
)
The advantage of this is that now K(t
i
, t
?
j
) =
?
K
(t
i
) ? ?
K
(t
?
j
) is defined between two transla-
tions only, and not between two pairs of transla-
tions. This simplification enables us to map trans-
lations into simple trees, e.g., those in Figure 1,
and then to apply them tree kernels, e.g., the Par-
tial Tree Kernel (Moschitti, 2006), which carry out
a scalar product in the subtree space.
We can further enrich the representation ?
K
, if
we consider all the information available to the
human judges when they are ranking translations.
That is, the two alternative translations along with
their corresponding reference.
216
In particular, let r and r
?
be the references for
the pairs ?t
1
, t
2
? and ?t
?
1
, t
?
2
?, we can redefine all
the members of Eq. 1, e.g., K(t
1
, t
?
1
) becomes
K(?t
1
, r?, ?t
?
1
, r
?
?) = PTK(?
M
(t
1
, r), ?
M
(t
?
1
, r
?
))
+ PTK(?
M
(r, t
1
), ?
M
(r
?
, t
?
1
)),
where ?
M
maps a pair of texts to a single tree.
There are several options to produce the bitext-
to-tree mapping for ?
M
. A simple approach is
to only use the tree corresponding to the first ar-
gument of ?
M
. This leads to the basic model
K(?t
1
, r?, ?t
?
1
, r
?
?) = PTK(?
M
(t
1
), ?
M
(t
?
1
)) +
PTK(?
M
(r), ?
M
(r
?
)), i.e., the sum of two tree
kernels applied to the trees constructed by ?
M
(we
previously informally mentioned it).
However, this simple mapping may be ineffec-
tive since the trees within a pair, e.g., (t
1
, r), are
treated independently, and no meaningful features
connecting t
1
and r can be derived from their
tree fragments. Therefore, we model ?
M
(r, t
1
) by
using word-matching relations between t
1
and r,
such that connections between words and con-
stituents of the two trees are established using
position-independent word matching. For exam-
ple, in Figure 1, the thin dashed arrows show the
links connecting the matching words between t
1
and r. The propagation of these relations works
from the bottom up. Thus, if all children in a con-
stituent have a link, their parent is also linked.
The use of such connections is essential as it en-
ables the comparison of the structural properties
and relations between two translation-reference
pairs. For example, the tree fragment [ELABORA-
TION [SATELLITE]] from the translation is con-
nected to [ELABORATION [SATELLITE]] in the
reference, indicating a link between two entire dis-
course units (drawn with a thicker arrow), and pro-
viding some reliability to the translation
2
.
Note that the use of connections yields a graph
representation instead of a tree. This is problem-
atic as effective models for graph kernels, which
would be a natural fit to this problem, are not cur-
rently available for exploiting linguistic informa-
tion. Thus, we simply use K, as defined above,
where the mapping ?
M
(t
1
, r) only produces a tree
for t
1
annotated with the marker REL represent-
ing the connections to r. This marker is placed on
all node labels of the tree generated from t
1
that
match labels from the tree generated from r.
2
Note that a non-pairwise model, i.e., K(t
1
, r), could
also be used to match the structural information above, but
it would not learn to compare it to a second pair (t
2
, r).
In other words, we only consider the trees en-
riched by markers separately, and ignore the edges
connecting both trees.
3 Experiments and Discussion
We experimented with datasets of segment-level
human rankings of system outputs from the
WMT11 and the WMT12 Metrics shared tasks
(Callison-Burch et al., 2011; Callison-Burch et al.,
2012): we used the WMT11 dataset for training
and the WMT12 dataset for testing. We focused
on translating into English only, for which the
datasets can be split by source language: Czech
(cs), German (de), Spanish (es), and French (fr).
There were about 10,000 non-tied human judg-
ments per language pair per dataset. We scored
our pairwise system predictions with respect to
the WMT12 human judgments using the Kendall?s
Tau (? ), which was official at WMT12.
Table 1 presents the ? scores for all metric vari-
ants introduced in this paper: for the individual
language pairs and overall. The left-hand side of
the table shows the results when using as sim-
ilarity the direct kernel calculation between the
corresponding structures of the candidate transla-
tion and the reference
3
, e.g., as in (Guzm?an et al.,
2014; Joty et al., 2014). The right-hand side con-
tains the results for structured kernel learning.
We can make the following observations:
(i) The overall results for all SKL-trained metrics
are higher than the ones when applying direct sim-
ilarity, showing that learning tree structures is bet-
ter than just calculating similarity.
(ii) Regarding the linguistic representation, we see
that, when learning tree structures, syntactic and
discourse-based trees yield similar improvements
with a slight advantage for the former. More in-
terestingly, when both structures are put together
in a combined tree, the improvement is cumula-
tive and yields the best results by a sizable margin.
This provides positive evidence towards our goal
of a unified tree-based representation with multi-
ple layers of linguistic information.
(iii) Comparing to the best evaluation metrics
that participated in the WMT12 Metrics shared
task, we find that our approach is competitive and
would have been ranked among the top 3 partici-
pants.
3
Applying tree kernels between the members of a pair to
generate one feature (for each different kernel function) has
become a standard practice in text similarity tasks (Severyn et
al., 2013b) and in question answering (Severyn et al., 2013a).
217
Similarity Structured Kernel Learning
Structure cs-en de-en es-en fr-en all cs-en de-en es-en fr-en all
1 SYN 0.169 0.188 0.203 0.222 0.195 0.190 0.244 0.198 0.158 0.198
2 DIS 0.130 0.174 0.188 0.169 0.165 0.176 0.235 0.166 0.160 0.184
3 DIS+POS 0.135 0.186 0.190 0.178 0.172 0.167 0.232 0.202 0.133 0.183
4 DIS+SYN 0.156 0.205 0.206 0.203 0.192 0.210 0.251 0.240 0.223 0.231
Table 1: Kendall?s (? ) correlation with human judgements on WMT12 for each language pair.
Furthermore, our result (0.237) is ahead of the
correlation obtained by popular metrics such as
TER (0.217), NIST (0.214) and BLEU (0.185) at
WMT12. This is very encouraging and shows the
potential of our new proposal.
In this paper, we have presented only the first
exploratory results. Our approach can be easily
extended with richer linguistic structures and fur-
ther combined with some of the already existing
strong evaluation metrics.
Testing
Train cs-en de-en es-en fr-en all
1 cs-en 0.210 0.204 0.217 0.204 0.209
2 de-en 0.196 0.251 0.203 0.202 0.213
3 es-en 0.218 0.204 0.240 0.223 0.221
4 fr-en 0.203 0.218 0.224 0.223 0.217
5 all 0.231 0.258 0.226 0.232 0.237
Table 2: Kendall?s (? ) on WMT12 for cross-
language training with DIS+SYN.
Note that the results in Table 1 were for train-
ing on WMT11 and testing on WMT12 for each
language pair in isolation. Next, we study the im-
pact of the choice of training language pair. Ta-
ble 2 shows cross-language evaluation results for
DIS+SYN: lines 1-4 show results when training on
WMT11 for one language pair, and then testing for
each language pair of WMT12.
We can see that the overall differences in perfor-
mance (see the last column: all) when training on
different source languages are rather small, rang-
ing from 0.209 to 0.221, which suggests that our
approach is quite independent of the source lan-
guage used for training. Still, looking at individ-
ual test languages, we can see that for de-en and
es-en, it is best to train on the same language; this
also holds for fr-en, but there it is equally good
to train on es-en. Interestingly, training on es-en
improves a bit for cs-en.
These somewhat mixed results have motivated
us to try tuning on the full WMT11 dataset; as line
5 shows, this yielded improvements for all lan-
guage pairs except for es-en. Comparing to line
4 in Table 1, we see that the overall Tau improved
from 0.231 to 0.237.
4 Conclusions and Future Work
We have presented a pairwise learning-to-rank ap-
proach to MT evaluation, which learns to differen-
tiate good from bad translations in the context of
a given reference. We have integrated several lay-
ers of linguistic information (lexical, syntactic and
discourse) in tree-based structures, and we have
used the structured kernel learning to identify rel-
evant features and learn pairwise rankers.
The evaluation results have shown that learning
in the proposed SKL framework is possible, yield-
ing better correlation (Kendall?s ? ) with human
judgments than computing the direct kernel sim-
ilarity between translation and reference, over the
same type of structures. We have also shown that
the contributions of syntax and discourse informa-
tion are cumulative, indicating that this learning
framework can be appropriate for the combination
of different sources of information. Finally, de-
spite the limited information we used, we achieved
better correlation at the segment level than BLEU
and other metrics in the WMT12 Metrics task.
In the future, we plan to work towards our long-
term goal, i.e., including more linguistic informa-
tion in the SKL framework and showing that this
can help. This would also include more semantic
information, e.g., in the form of Brown clusters or
using semantic similarity between the words com-
posing the structure calculated with latent seman-
tic analysis (Saleh et al., 2014b).
We further want to show that the proposed
framework is flexible and can include information
in the form of quality scores predicted by other
evaluation metrics, for which a vector of features
would be combined with the structured kernel.
Acknowledgments
This research is part of the Interactive sYstems
for Answer Search (Iyas) project, conducted by
the Arabic Language Technologies (ALT) group
at Qatar Computing Research Institute (QCRI)
within the Qatar Foundation.
218
References
Joshua Albrecht and Rebecca Hwa. 2008. Regression
for machine translation evaluation at the sentence
level. Machine Translation, 22(1-2):1?27.
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz, and Josh Schroeder. 2007.
(Meta-) evaluation of machine translation. In Pro-
ceedings of the Second Workshop on Statistical
Machine Translation, WMT ?07, pages 136?158,
Prague, Czech Republic.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Omar Zaidan. 2011. Findings of the 2011
workshop on statistical machine translation. In Pro-
ceedings of the Sixth Workshop on Statistical Ma-
chine Translation, WMT ?11, pages 22?64, Edin-
burgh, Scotland, UK.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 workshop on statistical ma-
chine translation. In Proceedings of the Seventh
Workshop on Statistical Machine Translation, WMT
?12, pages 10?51, Montr?eal, Canada.
Elisabet Comelles, Jes?us Gim?enez, Llu??s M`arquez,
Irene Castell?on, and Victoria Arranz. 2010.
Document-level automatic MT evaluation based on
discourse representations. In Proceedings of the
Joint Fifth Workshop on Statistical Machine Trans-
lation and MetricsMATR, WMT ?10, pages 333?
338, Uppsala, Sweden.
Kevin Duh. 2008. Ranking vs. regression in machine
translation evaluation. In Proceedings of the Third
Workshop on Statistical Machine Translation, WMT
?08, pages 191?194, Columbus, Ohio, USA.
Jes?us Gim?enez and Llu??s M`arquez. 2007. Linguis-
tic features for automatic evaluation of heterogenous
MT systems. In Proceedings of the Second Work-
shop on Statistical Machine Translation, WMT ?07,
pages 256?264, Prague, Czech Republic.
Francisco Guzm?an, Shafiq Joty, Llu??s M`arquez, and
Preslav Nakov. 2014. Using discourse structure
improves machine translation evaluation. In Pro-
ceedings of 52nd Annual Meeting of the Association
for Computational Linguistics, ACL ?14, pages 687?
698, Baltimore, Maryland, USA.
Shafiq Joty, Giuseppe Carenini, and Raymond Ng.
2012. A Novel Discriminative Framework for
Sentence-Level Discourse Analysis. In Proceedings
of the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, EMNLP-CoNLL ?12,
pages 904?915, Jeju Island, Korea.
Shafiq Joty, Giuseppe Carenini, Raymond Ng, and
Yashar Mehdad. 2013. Combining Intra- and
Multi-sentential Rhetorical Parsing for Document-
level Discourse Analysis. In Proceedings of the
51st Annual Meeting of the Association for Compu-
tational Linguistics, ACL ?13, pages 486?496, Sofia,
Bulgaria.
Shafiq Joty, Francisco Guzm?an, Llu??s M`arquez, and
Preslav Nakov. 2014. DiscoTK: Using discourse
structure for machine translation evaluation. In Pro-
ceedings of the Ninth Workshop on Statistical Ma-
chine Translation, WMT ?14, pages 402?408, Balti-
more, Maryland, USA.
Alon Lavie and Michael Denkowski. 2009. The ME-
TEOR metric for automatic evaluation of machine
translation. Machine Translation, 23(2?3):105?115.
Ding Liu and Daniel Gildea. 2005. Syntactic fea-
tures for evaluation of machine translation. In Pro-
ceedings of the ACL Workshop on Intrinsic and Ex-
trinsic Evaluation Measures for Machine Transla-
tion and/or Summarization, pages 25?32, Ann Ar-
bor, Michigan, USA.
Chi-kiu Lo, Anand Karthik Tumuluru, and Dekai Wu.
2012. Fully automatic semantic MT evaluation. In
Proceedings of the Seventh Workshop on Statisti-
cal Machine Translation, WMT ?12, pages 243?252,
Montr?eal, Canada.
Alessandro Moschitti, Silvia Quarteroni, Roberto
Basili, and Suresh Manandhar. 2007. Exploiting
syntactic and shallow semantic kernels for ques-
tion answer classification. In Proceedings of the
45th Annual Meeting of the Association of Computa-
tional Linguistics, ACL ?07, pages 776?783, Prague,
Czech Republic.
Alessandro Moschitti. 2006. Efficient convolution ker-
nels for dependency and constituent syntactic trees.
In Proceedings of 17th European Conference on Ma-
chine Learning and the 10th European Conference
on Principles and Practice of Knowledge Discovery
in Databases, ECML/PKDD ?06, pages 318?329,
Berlin, Germany.
Alessandro Moschitti. 2008. Kernel methods, syn-
tax and semantics for relational text categorization.
In Proceedings of the 17th ACM Conference on In-
formation and Knowledge Management, CIKM ?08,
pages 253?262, Napa Valley, California, USA.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings
of 40th Annual Meting of the Association for Com-
putational Linguistics, ACL ?02, pages 311?318,
Philadelphia, Pennsylvania, USA.
Maja Popovi?c and Hermann Ney. 2007. Word error
rates: Decomposition over POS classes and applica-
tions for error analysis. In Proceedings of the Sec-
ond Workshop on Statistical Machine Translation,
WMT ?07, pages 48?55, Prague, Czech Republic.
Vasin Punyakanok and Dan Roth. 2001. The use of
classifiers in sequential inference. In Advances in
Neural Information Processing Systems 14, NIPS
?01, pages 995?1001, Vancouver, Canada.
219
Iman Saleh, Scott Cyphers, Jim Glass, Shafiq Joty,
Llu??s M`arquez, Alessandro Moschitti, and Preslav
Nakov. 2014a. A study of using syntactic and se-
mantic structures for concept segmentation and la-
beling. In Proceedings of the 25th International
Conference on Computational Linguistics, COLING
?14, pages 193?202, Dublin, Ireland.
Iman Saleh, Alessandro Moschitti, Preslav Nakov,
Llu??s M`arquez, and Shafiq Joty. 2014b. Semantic
kernels for semantic parsing. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP ?14, Doha, Qatar.
Aliaksei Severyn and Alessandro Moschitti. 2012.
Structural relationships for large-scale learning of
answer re-ranking. In Proceedings of the 35th Inter-
national ACM SIGIR Conference on Research and
Development in Information Retrieval, SIGIR ?12,
pages 741?750, Portland, Oregon, USA.
Aliaksei Severyn, Massimo Nicosia, and Alessandro
Moschitti. 2013a. Learning adaptable patterns for
passage reranking. In Proceedings of the Seven-
teenth Conference on Computational Natural Lan-
guage Learning, CoNLL ?13, pages 75?83, Sofia,
Bulgaria.
Aliaksei Severyn, Massimo Nicosia, and Alessandro
Moschitti. 2013b. Learning semantic textual sim-
ilarity with structural representations. In Proceed-
ings of the 51st Annual Meeting of the Association
for Computational Linguistics (Volume 2: Short Pa-
pers), ACL ?13, pages 714?718, Sofia, Bulgaria.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of the 7th Biennial Conference of the
Association for Machine Translation in the Ameri-
cas, AMTA ?06, Cambridge, Massachusetts, USA.
Xingyi Song and Trevor Cohn. 2011. Regression and
ranking based optimisation for sentence-level MT
evaluation. In Proceedings of the Sixth Workshop
on Statistical Machine Translation, WMT ?11, pages
123?129, Edinburgh, Scotland, UK.
Kristina Toutanova, Dan Klein, Christopher Manning,
and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology
- Volume 1, HLT-NAACL ?03, pages 173?180, Ed-
monton, Canada.
Billy Wong and Chunyu Kit. 2012. Extending ma-
chine translation evaluation metrics with lexical co-
hesion to document level. In Proceedings of the
2012 Joint Conference on Empirical Methods in
Natural Language Processing and Computational
Natural Language Learning, EMNLP-CoNLL ?12,
pages 1060?1068, Jeju Island, Korea.
Fabio Massimo Zanzotto and Alessandro Moschitti.
2006. Automatic learning of textual entailments
with cross-pair similarities. In Proceedings of the
21st International Conference on Computational
Linguistics and 44th Annual Meeting of the Associ-
ation for Computational Linguistics, COLING-ACL
?06, pages 401?408, Sydney, Australia.
220
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 714?718,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Learning Semantic Textual Similarity with Structural Representations
Aliaksei Severyn(1) and Massimo Nicosia(1) and Alessandro Moschitti1,2
(1)DISI, University of Trento, 38123 Povo (TN), Italy
{severyn,m.nicosia,moschitti}@disi.unitn.it
(2)QCRI, Qatar Foundation, Doha, Qatar
amoschitti@qf.org.qa
Abstract
Measuring semantic textual similarity
(STS) is at the cornerstone of many NLP
applications. Different from the major-
ity of approaches, where a large number
of pairwise similarity features are used to
represent a text pair, our model features
the following: (i) it directly encodes input
texts into relational syntactic structures;
(ii) relies on tree kernels to handle feature
engineering automatically; (iii) combines
both structural and feature vector repre-
sentations in a single scoring model, i.e.,
in Support Vector Regression (SVR); and
(iv) delivers significant improvement over
the best STS systems.
1 Introduction
In STS the goal is to learn a scoring model that
given a pair of two short texts returns a similar-
ity score that correlates with human judgement.
Hence, the key aspect of having an accurate STS
framework is the design of features that can ade-
quately represent various aspects of the similarity
between texts, e.g., using lexical, syntactic and se-
mantic similarity metrics.
The majority of approaches treat input text pairs
as feature vectors where each feature is a score
corresponding to a certain type of similarity. This
approach is conceptually easy to implement and
the STS shared task at SemEval 2012 (Agirre et
al., 2012) (STS-2012) has shown that the best sys-
tems were built following this idea, i.e., a num-
ber of features encoding similarity of an input text
pair were combined in a single scoring model, e.g.,
SVR. Nevertheless, one limitation of using only
similarity features to represent a text pair is that of
low representation power.
The novelty of our approach is that we treat the
input text pairs as structural objects and rely on the
power of kernel learning to extract relevant struc-
tures. To link the documents in a pair we mark the
nodes in the related structures with a special rela-
tional tag. This way effective structural relational
patterns are implicitly encoded in the trees and
can be automatically learned by the kernel-based
machines. We combine our relational structural
model with the features from two best systems of
STS-2012. Finally, we use the approach of classi-
fier stacking to combine several structural models
into the feature vector representation.
The contribution of this paper is as follows: (i) it
provides a convincing evidence that adding struc-
tural features automatically extracted by structural
kernels yields a significant improvement in accu-
racy; (ii) we define a combination kernel that inte-
grates both structural and feature vector represen-
tations within a single scoring model, e.g., Sup-
port Vector Regression; (iii) we provide a sim-
ple way to construct relational structural models
that can be built using off-the-shelf NLP tools;
(iv) we experiment with four structural representa-
tions and show that constituency and dependency
trees represent the best source for learning struc-
tural relationships; and (v) using a classifier stack-
ing approach, structural models can be easily com-
bined and integrated into existing feature-based
STS models.
2 Structural Relational Similarity
The approach of relating pairs of input struc-
tures by learning predictable syntactic transforma-
tions has shown to deliver state-of-the-art results
in question answering, recognizing textual entail-
ment, and paraphrase detection, e.g. (Wang et al,
2007; Wang and Manning, 2010; Heilman and
Smith, 2010). Previous work relied on fairly com-
plex approaches, e.g. applying quasi-synchronous
grammar formalism and variations of tree edit dis-
tance alignments, to extract syntactic patterns re-
lating pairs of input structures. Our approach
is conceptually simpler, as it regards the prob-
lem within the kernel learning framework, where
we first encode salient syntactic/semantic proper-
714
ties of the input text pairs into tree structures and
rely on tree kernels to automatically generate rich
feature spaces. This work extends in several di-
rections our earlier work in question answering,
e.g., (Moschitti et al, 2007; Moschitti and Quar-
teroni, 2008), in textual entailment recognition,
e.g., (Moschitti and Zanzotto, 2007), and more in
general in relational text categorization (Moschitti,
2008; Severyn and Moschitti, 2012).
In this section we describe: (i) a kernel frame-
work to combine structural and vector models; (ii)
structural kernels to handle feature engineering;
and (iii) suitable structural representations for re-
lational learning.
2.1 Structural Kernel Learning
In supervised learning, given labeled data
{(xi, y i)}ni=1, the goal is to estimate a decision
function h(x) = y that maps input examples to
their targets. A conventional approach is to rep-
resent a pair of texts as a set of similarity fea-
tures {fi}, s.t. the predictions are computed as
h(x) = w ? x = ?iwifi, where w is the model
weight vector. Hence, the learning problem boils
down to estimating individual weights of each of
the similarity features fi. One downside of such
approach is that a great deal of similarity infor-
mation encoded in a given text pair is lost when
modeled by single real-valued scores.
A more versatile approach in terms of the input
representation relies on kernels. In a typical kernel
learning approach, e.g., SVM, the prediction func-
tion for a test input x takes on the following form
h(x) =
?
i ?iyiK(x,xi), where ?i are the model
parameters estimated from the training data, yi are
target variables, xi are support vectors, andK(?, ?)
is a kernel function.
To encode both structural representation and
similarity feature vectors of a given text pair in a
single model we define each document in a pair
to be composed of a tree and a vector: ?t, v?.
To compute a kernel between two text pairs xi
and xj we define the following all-vs-all kernel,
where all possible combinations of components,
x(1) and x(2), from each text pair are consid-
ered: K(xi,xj) = K(x(1)i ,x(1)j )+K(x(1)i ,x(2)j )+
K(x(2)i ,x
(1)
j ) + K(x
(2)
i ,x
(2)
j ). Each of the ker-
nel computations K can be broken down into
the following: K(x(1),x(2)) = KTK(t(1), t(2)) +
Kfvec(v(1), v(2)), where KTK computes a struc-
tural kernel and Kfvec is a kernel over feature vec-
tors, e.g., linear, polynomial or RBF, etc. Further
in the text we refer to structural tree kernel models
as TK and explicit feature vector representation as
fvec.
Having defined a way to jointly model text pairs
using structural TK representations along with the
similarity features fvec, we next briefly review
tree kernels and our relational structures.
2.2 Tree Kernels
We use tree structures as our base representation
since they provide sufficient flexibility in repre-
sentation and allow for easier feature extraction
than, for example, graph structures. Hence, we
rely on tree kernels to compute KTK(?, ?). Given
two trees it evaluates the number of substructures
(or fragments) they have in common, i.e., it is a
measure of their overlap. Different TK functions
are characterized by alternative fragment defini-
tions. In particular, we focus on the Syntactic Tree
kernel (STK) (Collins and Duffy, 2002) and a Par-
tial Tree Kernel (PTK) (Moschitti, 2006).
STK generates all possible substructures rooted in
each node of the tree with the constraint that pro-
duction rules can not be broken (i.e., any node in a
tree fragment must include either all or none of its
children).
PTK can be more effectively applied to both con-
stituency and dependency parse trees. It general-
izes STK as the fragments it generates can contain
any subset of nodes, i.e., PTK allows for breaking
the production rules and generating an extremely
rich feature space, which results in higher gener-
alization ability.
2.3 Structural representations
In this paper, we define simple-to-build relational
structures based on: (i) a shallow syntactic tree,
(ii) constituency, (iii) dependency and (iv) phrase-
dependency trees.
Shallow tree is a two-level syntactic hierarchy
built from word lemmas (leaves), part-of-speech
tags (preterminals) that are further organized into
chunks. It was shown to significantly outperform
feature vector baselines for modeling relationships
between question answer pairs (Severyn and Mos-
chitti, 2012).
Constituency tree. While shallow syntactic pars-
ing is very fast, here we consider using con-
stituency structures as a potentially richer source
of syntactic/semantic information.
Dependency tree. We propose to use depen-
dency relations between words to derive an alter-
native structural representation. In particular, de-
715
Figure 1: A phrase dependency-based structural representation of a text pair (s1, s2): A woman with
a knife is slicing a pepper (s1) vs. A women slicing green pepper (s2) with a high semantic similarity
(human judgement score 4.0 out of 5.0). Related tree fragments are linked with a REL tag.
pendency relations are used to link words in a way
that they are always at the leaf level. This reorder-
ing of the nodes helps to avoid the situation where
nodes with words tend to form long chains. This
is essential for PTK to extract meaningful frag-
ments. We also plug part-of-speech tags between
the word nodes and nodes carrying their grammat-
ical role.
Phrase-dependency tree. We explore a phrase-
dependency tree similar to the one defined in (Wu
et al, 2009). It represents an alternative struc-
ture derived from the dependency tree, where the
dependency relations between words belonging to
the same phrase (chunk) are collapsed in a unified
node. Different from (Wu et al, 2009), the col-
lapsed nodes are stored as a shallow subtree rooted
at the unified node. This node organization is par-
ticularly suitable for PTK that effectively runs a
sequence kernel on the tree fragments inside each
chunk subtree. Fig 1 gives an example of our vari-
ation of a phrase dependency tree.
As a final consideration, if a document contains
multiple sentences they are merged in a single tree
with a common root. To encode the structural
relationships between documents in a pair a spe-
cial REL tag is used to link the related structures.
We adopt a simple strategy to establish such links:
words from two documents that have a common
lemma get their parents (POS tags) and grandpar-
ents, non-terminals, marked with a REL tag.
3 Pairwise similarity features.
Along with the direct representation of input text
pairs as structural objects our framework is also
capable of encoding pairwise similarity feature
vectors (fvec), which we describe below.
Baseline features. (base) We adopt similar-
ity features from two best performing systems
of STS-2012, which were publicly released1:
namely, the Takelab2 system (S?aric? et al, 2012)
and the UKP Lab?s system3 (Bar et al, 2012).
Both systems represent input texts with similarity
features combining multiple text similarity mea-
sures of varying complexity.
UKP (U) provides metrics based on match-
ing of character, word n-grams and common
subsequences. It also includes features derived
from Explicit Semantic Analysis (Gabrilovich and
Markovitch, 2007) and aggregation of word sim-
ilarity based on lexical-semantic resources, e.g.,
WordNet. In total it provides 18 features.
Takelab (T) includes n-gram matching of vary-
ing size, weighted word matching, length differ-
ence, WordNet similarity and vector space simi-
larity where pairs of input sentences are mapped
into Latent Semantic Analysis (LSA) space. The
features are computed over several sentence rep-
resentations where stop words are removed and/or
lemmas are used in place of raw tokens. The total
number of Takelab?s features is 21. The combined
system consists of 39 features.
Additional features. We also augment the U and
T feature sets, with an additional set of features (A)
which includes: a cosine similarity scores com-
puted over (i) n-grams of part-of-speech tags (up
to 4-grams), (ii) SuperSense tags (Ciaramita and
1Note that only a subset of the features used in the fi-
nal evaluation was released, which results in lower accuracy
when compared to the official rankings.
2http://takelab.fer.hr/sts/
3https://code.google.com/p/dkpro-similarity-
asl/wiki/SemEval2013
716
Altun, 2006), (iii) named entities, (iv) dependency
triplets, and (v) PTK syntactic similarity scores
computed between documents in a pair, where as
input representations we use raw dependency and
constituency trees. To alleviate the problem of do-
main adaptation, where datasets used for training
and testing are drawn from different sources, we
include additional features to represent the com-
bined text of a pair: (i) bags (B) of lemmas, de-
pendency triplets, production rules (from the con-
stituency parse tree) and a normalized length of
the entire pair; and (ii) a manually encoded cor-
pus type (M), where we use a binary feature with
a non-zero entry corresponding to a dataset type.
This helps the learning algorithm to learn implic-
itly the individual properties of each dataset.
Stacking. To integrate multiple TK representa-
tions into a single model we apply a classifier
stacking approach (Fast and Jensen, 2008). Each
of the learned TK models is used to generate pre-
dictions which are then plugged as features into
the final fvec representation, s.t. the final model
uses only explicit feature vector representation. To
obtain prediction scores, we apply 5-fold cross-
validation scheme, s.t. for each of the held-out
folds we obtain independent predictions.
4 Experiments
We present the results of our model tested on the
data from the Core STS task at SemEval 2012.
4.1 Setup
Data. To compare with the best systems of the
STS-2012 we followed the same setup used in
the final evaluation, where 3 datasets (MSRpar,
MSRvid and SMTeuroparl) are used for training
and 5 for testing (two ?surprise? datasets were
added: OnWN and SMTnews). We use the entire
training data to obtain a single model for making
predictions on each test set.
Software. To encode TK models along with the
similarity feature vectors into a single regression
scoring model, we use an SVR framework imple-
mented in SVM-Light-TK4. We use the follow-
ing parameter settings -t 5 -F 1 -W A -C
+, which specifies a combination of trees and fea-
ture vectors (-C +), STK over trees (-F 1) (-F
3 for PTK) computed in all-vs-all mode (-W A)
and polynomial kernel of degree 3 for the feature
vector (active by default).
4http://disi.unitn.it/moschitti/Tree-Kernel.htm
Metrics. We report the following metrics em-
ployed in the final evaluation: Pearson correlation
for individual test sets5 and Mean ? an average
score weighted by the test set size.
4.2 Results
Table 1 summarizes the results of combining TK
models with a strong feature vector model. We
test structures defined in Sec. 2.3 when using STK
and PTK. The results show that: (i) combining
all three features sets (U, T, A) provides a strong
baseline system that we attempt to further improve
with our relational structures; (ii) the generality of
PTK provides an advantage over STK for learn-
ing more versatile models; (iii) constituency and
dependency representations seem to perform bet-
ter than shallow and phrase-dependency trees; (iv)
using structures with no relational linking does not
work; (v) TK models provide a far superior source
of structural similarity than U + T + A that already
includes PTK similarity scores as features, and fi-
nally (vi) the domain adaptation problem can be
addressed by including corpus specific features,
which leads to a large improvement over the pre-
vious best system.
5 Conclusions and Future Work
We have presented an approach where text pairs
are directly treated as structural objects. This pro-
vides a much richer representation for the learning
algorithm to extract useful syntactic and shallow
semantic patterns. We have provided an exten-
sive experimental study of four different structural
representations, e.g. shallow, constituency, de-
pendency and phrase-dependency trees using STK
and PTK. The novelty of our approach is that it
goes beyond a simple combination of tree kernels
with feature vectors as: (i) it directly encodes input
text pairs into relationally linked structures; (ii) the
learned structural models are used to obtain pre-
diction scores thus making it easy to plug into ex-
isting feature-based models, e.g. via stacking; (iii)
to our knowledge, this work is the first to apply
structural kernels and combinations in a regres-
sion setting; and (iv) our model achieves the state
of the art in STS largely improving the best pre-
vious systems. Our structural learning approach
to STS is conceptually simple and does not re-
quire additional linguistic sources other than off-
the-shelf syntactic parsers. It is particularly suit-
able for NLP tasks where the input domain comes
5we also report the results for a concatenation of all five
test sets (ALL)
717
Experiment U T A S C D P STK PTK B M ALL Mean MSRp MSRv SMTe OnWN SMTn
fvec
model
? .7060 .6087 .6080 .8390 .2540 .6820 .4470
? .7589 .6863 .6814 .8637 .4950 .7091 .5395
? ? .8079 .7161 .7134 .8837 .5519 .7343 .5607
? ? ? .8187 .7137 .7157 .8833 .5131 .7355 .5809
TK
models
with STK
and PTK
? ? ? ? ? .8261 .6982 .7026 .8870 .4807 .7258 .5333
? ? ? ? ? .8326 .6970 .7020 .8925 .4826 .7190 .5253
? ? ? ? ? .8341 .7024 .7086 .8921 .4671 .7319 .5495
? ? ? ? ? .8211 .6693 .6994 .8903 .2980 .7035 .5603
? ? ? ? ? .8362 .7026 .6927 .8896 .5282 .7144 .5485
? ? ? ? ? .8458 .7047 .6935 .8953 .5080 .7101 .5834
? ? ? ? ? .8468 .6954 .6717 .8902 .4652 .7089 .6133
? ? ? ? ? .8326 .6693 .7108 .8879 .4922 .7215 .5156
REL tag ? ? ? ? .8218 .6899 .6644 .8726 .4846 .7228 .5684? ? ? ? .8250 .7000 .6806 .8822 .5171 .7145 .5769
domain
adaptation
? ? ? ? ? .8539 .7132 .6993 .9005 .4772 .7189 .6481
? ? ? ? ? .8529 .7249 .7080 .8984 .5142 .7263 .6700
? ? ? ? ? ? .8546 .7156 .6989 .8979 .4884 .7181 .6609
? ? ? ? ? ? .8810 .7416 .7210 .8971 .5912 .7328 .6778
UKP (best system of STS-2012) .8239 .6773 .6830 .8739 .5280 .6641 .4937
Table 1: Results on STS-2012. First set of experiments studies the combination of fvec models from
UKP (U), Takelab (T) and (A). Next we show results for four structural representations: shallow (S),
constituency (C), dependency (D) and phrase-dependency (P) trees with STK and PTK; next row set
demonstrates the necessity of relational linking for two best structures, i.e. C and D (empty circle denotes
a structures with no relational linking.); finally, domain adaptation via bags of features (B) of the entire
pair and (M) manually encoded dataset type show the state of the art results.
as pairs of objects, e.g., question answering, para-
phrasing and recognizing textual entailment.
6 Acknowledgements
This research is supported by the EU?s Seventh
Framework Program (FP7/2007-2013) under the
#288024 LIMOSINE project.
References
Eneko Agirre, Daniel Cer, Mona Diab, and Gonzalez-
Agirre. 2012. Semeval-2012 task 6: A pilot on se-
mantic textual similarity. In *SEM.
Daniel Bar, Chris Biemann, Iryna Gurevych, and
Torsten Zesch. 2012. Ukp: Computing seman-
tic textual similarity by combining multiple content
similarity measures. In SemEval.
Massimiliano Ciaramita and Yasemin Altun. 2006.
Broad-coverage sense disambiguation and informa-
tion extraction with a supersense sequence tagger. In
EMNLP.
Michael Collins and Nigel Duffy. 2002. New Ranking
Algorithms for Parsing and Tagging: Kernels over
Discrete Structures, and the Voted Perceptron. In
ACL.
Andrew S. Fast and David Jensen. 2008. Why stacked
models perform effective collective classification.
In ICDM.
Evgeniy Gabrilovich and Shaul Markovitch. 2007.
Computing semantic relatedness using wikipedia-
based explicit semantic analysis. In IJCAI.
Michael Heilman and Noah A. Smith. 2010. Tree edit
models for recognizing textual entailments, para-
phrases, and answers to questions. In NAACL.
Alessandro Moschitti and Silvia Quarteroni. 2008.
Kernels on linguistic structures for answer extrac-
tion. In ACL.
Alessandro Moschitti and Fabio Massimo Zanzotto.
2007. Fast and effective kernels for relational learn-
ing from texts. In ICML.
Alessandro Moschitti, Silvia Quarteroni, Roberto
Basili, and Suresh Manandhar. 2007. Exploit-
ing syntactic and shallow semantic kernels for ques-
tion/answer classification. In ACL.
Alessandro Moschitti. 2006. Efficient convolution ker-
nels for dependency and constituent syntactic trees.
In ECML.
Alessandro Moschitti. 2008. Kernel methods, syntax
and semantics for relational text categorization. In
CIKM.
Aliaksei Severyn and Alessandro Moschitti. 2012.
Structural relationships for large-scale learning of
answer re-ranking. In SIGIR.
Frane S?aric?, Goran Glavas?, Mladen Karan, Jan S?najder,
and Bojana Dalbelo Bas?ic?. 2012. Takelab: Systems
for measuring semantic text similarity. In SemEval.
Mengqiu Wang and Christopher D. Manning. 2010.
Probabilistic tree-edit models with structured latent
variables for textual entailment and question answer-
ing. In ACL.
Mengqiu Wang, Noah A. Smith, and Teruko Mitaura.
2007. What is the jeopardy model? a quasi-
synchronous grammar for qa. In EMNLP.
Yuanbin Wu, Qi Zhang, Xuanjing Huang, and Lide Wu.
2009. Phrase dependency parsing for opinion min-
ing. In EMNLP.
718
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
and the Shared Task, pages 53?58, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational Linguistics
iKernels-Core: Tree Kernel Learning for Textual Similarity
Aliaksei Severyn1 and Massimo Nicosia1 and Alessandro Moschitti1,2
1University of Trento, DISI, 38123 Povo (TN), Italy
{severyn,m.nicosia,moschitti}@disi.unitn.it
2Qatar Foundation, QCRI, Doha, Qatar
{amoschitti}@qf.org.qa
Abstract
This paper describes the participation of iKer-
nels system in the Semantic Textual Similar-
ity (STS) shared task at *SEM 2013. Different
from the majority of approaches, where a large
number of pairwise similarity features are
used to learn a regression model, our model
directly encodes the input texts into syntac-
tic/semantic structures. Our systems rely on
tree kernels to automatically extract a rich set
of syntactic patterns to learn a similarity score
correlated with human judgements. We ex-
periment with different structural representa-
tions derived from constituency and depen-
dency trees. While showing large improve-
ments over the top results from the previous
year task (STS-2012), our best system ranks
21st out of total 88 participated in the STS-
2013 task. Nevertheless, a slight refinement to
our model makes it rank 4th.
1 Introduction
Comparing textual data to establish the degree of se-
mantic similarity is of key importance in many Nat-
ural Language Processing (NLP) tasks ranging from
document categorization to textual entailment and
summarization. The key aspect of having an accu-
rate STS framework is the design of features that can
adequately represent various aspects of the similar-
ity between texts, e.g. using lexical, syntactic and
semantic similarity metrics.
The majority of approaches to semantic textual
similarity treat the input text pairs as feature vec-
tors where each feature is a score corresponding to a
certain type of similarity. This approach is concep-
tually easy to implement and STS-2012 (Agirre et
al., 2012) has shown that the best systems were built
following this idea, i.e. a number of features encod-
ing similarity of an input text pair were combined in
a single scoring model, such as Linear Regression
or Support Vector Regression (SVR). One potential
limitation of using only similarity features to repre-
sent a text pair is that of low representation power.
The novelty of our approach is that we encode the
input text pairs directly into structural objects, e.g.
trees, and rely on the power of kernel learning to ex-
tract relevant structures. This completely different
from (Croce et al, ), where tree kernels where used
to establish syntactic similarity and then plugged as
similarity features. To link the documents in a pair
we mark the nodes in the related structures with a
special relational tag. In this way effective struc-
tural relational patterns are implicitly encoded in the
trees and can be automatically learned by the kernel-
based machine learning methods. We build our sys-
tems on top of the features used by two best systems
from STS-2012 and combine them with the tree ker-
nel models within the Support Vector Regression to
derive a single scoring model. Since the test data
used for evaluation in STS-2013 (Agirre et al, 2013)
is different from the 2012 data provided for the sys-
tem development, domain adaptation represents an
additional challenge. To address this problem we
augment our feature vector representation with fea-
tures extracted from a text pair as a whole to capture
individual properties of each dataset. Additionally,
we experiment with a corpus type classifier and in-
clude its prediction score as additional features. Fi-
nally, we use stacking to combine several structural
models into the feature vector representation.
53
In the following sections we describe our ap-
proach to combine structural representations with
the pairwise similarity features in a single SVR
learning framework. We then report results on both
STS-2012 and 2013 tasks.
2 Structural Relational Similarity
In this section we first describe the kernel framework
to combine structural and vector models, then we
explain how to construct the tree models and briefly
describe tree kernels we use to automatically extract
the features.
2.1 Structural Kernel Learning
In supervised learning, given the labeled data
{(xi, y i)}ni=1, the goal is to estimate a decision func-
tion h(x) = y that maps input examples to the tar-
get variables. A conventional approach is to rep-
resent a pair of texts as a set of similarity features
{fi}, s.t. the predictions are computed as h(x) =
w ? x =
?
iwifi, wherew is the model weight vec-
tor. Hence, the learning problem boils down to es-
timating the individual weight of each of the sim-
ilarity feature fi. One downside of such approach
is that a great deal of similarity information carried
by a given text pair is lost when modeled by single
real-valued scores.
A more versatile approach in terms of the input
representation relies on kernels. In a typical ker-
nel machine, e.g. SVM, the prediction function for
a test input x takes on the following form h(x) =
?
i ?iyiK(x,xi), where ?i are the model parame-
ters estimated from the training data, yi - target vari-
ables, xi are support vectors, and K(?, ?) is a kernel
function.
To encode both structural representation and sim-
ilarity feature vectors of input text pairs xi in a sin-
gle model, we treat it as the following tuple: xi =
?xai ,x
b
i? = ?(t
a
i , v
a
i ), (t
b
i , v
b
i)?, where x
a
i x
b
i are the
first and the second document of xi, and t and v de-
note tree and vector representations respectively.
To compute a kernel between two text pairs xi
and xj we define the following all-vs-all kernel,
where all possible combinations of documents from
each pair are considered: K(xi,xj) = K(xai ,x
a
j ) +
K(xai ,x
b
j) + K(x
b
i ,x
a
j ) + K(x
b
i ,x
b
j). Each of the
kernel computations K between two documents xa
and xb can be broken down into the following:
K(xa,xb) = KTK(ta, tb) + Kfvec(va, vb), where
KTK computes a tree kernel and Kfvec is a kernel
over feature vectors, e.g. linear, polynomial or RBF,
etc. Further in the text we refer to structural tree
kernel models as TK and explicit feature vector rep-
resentation as fvec.
Having defined a way to jointly model text pairs
using structural TK representations along with the
similarity features fvec, we next briefly review tree
kernels and our relational structures derived from
constituency and dependency trees.
2.2 Tree Kernels
We use tree structures as our base representation
since they provide sufficient flexibility in represen-
tation and allow for easier feature extraction than,
for example, graph structures. We use a Partial Tree
Kernel (PTK) (Moschitti, 2006) to take care of auto-
matic feature extraction and compute KTK(?, ?).
PTK is a tree kernel function that can be ef-
fectively applied to both constituency and depen-
dency parse trees. It generalizes a subset tree ker-
nel (STK) (Collins and Duffy, 2002) that maps a
tree into the space of all possible tree fragments con-
strained by the rule that the sibling nodes from their
parents cannot be separated. Different from STK
where the nodes in the generated tree fragments are
constrained to include none or all of their direct chil-
dren, PTK fragments can contain any subset of the
features, i.e. PTK allows for breaking the production
rules. Consequently, PTK generalizes STK generat-
ing an extremely rich feature space, which results in
higher generalization ability.
2.3 Relational Structures
The idea of using relational structures to jointly
model text pairs was previously proposed in (Sev-
eryn and Moschitti, 2012), where shallow syntactic
structures derived from chunks and part-of-speech
tags were used to represent question/answer pairs.
In this paper, we define novel relational structures
based on: (i) constituency and (ii) dependency trees.
Constituency tree. Each document in a given text
pair is represented by its constituency parse tree.
If a document contains multiple sentences they are
merged in a single tree with a common root. To
encode the structural relationships between docu-
54
Figure 1: A dependency-based structural representation of a text pair. REL tag links related fragments.
ments in a pair a special REL tag is used to link
the related structures. We adopt a simple strategy
to establish such links: words from two documents
that have a common lemma get their parents (POS
tags) and grandparents, non-terminals, marked with
a REL tag.
Dependency tree. We propose to use dependency
relations between words to derive an alternative
structural representation. In particular, dependency
relations are used to link words in a way that words
are always at the leaf level. This reordering of the
nodes helps to avoid the situation where nodes with
words tend to form long chains. This is essential
for PTK to extract meaningful fragments. We also
plug part-of-speech tags between the word nodes
and nodes carrying their grammatical role. Again
a special REL tag is used to establish relations be-
tween tree fragments. Fig. 1 gives an example of
a dependency-based structure taken from STS-2013
headlines dataset.
3 Pairwise similarity features.
Along with the direct representation of input text
pairs as structural objects our framework also en-
codes feature vectors (base), which we describe
below.
3.1 Baseline features
We adopt similarity features from two best perform-
ing systems of STS-2012, which were publicly re-
leased: namely, the Takelab1 system (S?aric? et al,
2012) and the UKP Lab?s system2 (Bar et al, 2012).
Both systems represent input texts with similar-
1http://takelab.fer.hr/sts/
2https://code.google.com/p/dkpro-similarity-
asl/wiki/SemEval2013
ity features which combine multiple text similarity
measures of varying complexity.
UKP provides metrics based on matching of char-
acter, word n-grams and common subsequences. It
also includes features derived from Explicit Seman-
tic Analysis vector comparisons and aggregation of
word similarity based on lexical-semantic resources,
e.g. WordNet. In total it provides 18 features.
Takelab includes n-gram matching of varying size,
weighted word matching, length difference, Word-
Net similarity and vector space similarity where
pairs of input sentences are mapped into Latent Se-
mantic Analysis (LSA) space (Turney and Pantel,
2010). The features are computed over several sen-
tence representations where stop words are removed
and/or lemmas are used in place of raw tokens.
The total number of Takelab?s features is 21. Even
though some of the UKP and Takelab features over-
lap we include all of them in a combined system with
the total of 39 features.
3.2 iKernels features
Here we describe our additional features added to
the fvec representation. First, we note that word
frequencies used to compute weighted word match-
ings and the word-vector mappings to compute LSA
similarities required by Takelab features are pro-
vided only for the vocabulary extracted from 2012
data. Hence, we use both STS-2012 and 2013 data to
obtain the word counts and re-estimate LSA vector
representations. For the former we extract unigram
counts from Google Books Ngrams3, while for the
latter we use additional corpora as described below.
LSA similarity. To construct LSA word-vector
mappings we use the following three sources: (i)
3http://storage.googleapis.com/books/ngrams/books/datasetsv2.html
55
Aquaint4, which consists of more than 1 million
newswire documents, (ii) ukWaC (Baroni et al,
2009) - a 2 billion word corpus constructed from
the Web, and (iii) and a collection of documents
extracted from Wikipedia dump5. To extract LSA
topics we use GenSim6 software. We preprocess
the data by lowercasing, removing stopwords and
words with frequency lower than 5. Finally, we ap-
ply tf-idf weighting. For all representations we fix
the number of dimensions to 250. For all corpora
we use document-level representation, except for
Wikipedia we also experimented with a sentence-
level document representation, which typically pro-
vides a more restricted context for estimating word-
document distributions.
Brown Clusters. In addition to vector represen-
tations derived from LSA, we extract word-vector
mappings using Brown word clusters7 (Turian et al,
2010), where words are organized into a hierarchy
and each word is represented as a bit-string. We
encode each word by a feature vector where each
entry corresponds to a prefix extracted from its bit-
string. We use prefix lengths in the following range:
k = {4, 8, 12, 16, 20}. Finally, the document is rep-
resented as a feature vector composed by the indi-
vidual word vectors.
Term-overlap features. In addition to the word
overlap features computed by UKP and Takelab
systems we also compute a cosine similarity over
the following representations: (i) n-grams of part-
of-speech tags (up to 4-grams), (ii) SuperSense
tags (Ciaramita and Altun, 2006), (iii) named enti-
ties, and (iv) dependency triplets.
PTK similarity. We use PTK to provide a syn-
tactic similarity score between documents in a pair:
PTK(a, b) = PTK(a, b), where as input represen-
tations we use dependency and constituency trees.
Explicit Semantic Analysis (ESA) similarity.
ESA (Gabrilovich and Markovitch, 2007) represents
input documents as vectors of Wikipedia concepts.
To compute ESA features we use Lucene8 to in-
dex documents extracted from a Wikipedia dump.
Given a text pair we retrieve k top documents (i.e.
4http://www.ldc.upenn.edu/Catalog/docs/LDC2002T31/
5http://dumps.wikimedia.org/
6http://radimrehurek.com/gensim/
7http://metaoptimize.com/projects/wordreprs/
8http://lucene.apache.org/
Wikipedia concepts) and compute the metric by
looking at the overlap of the concepts between the
documents: esak(a, b) =
|Wa
?
Wb|
k , where Wa is
the set of concepts retrieved for document a. We
compute esa features with k ? {10, 25, 50, 100}.
3.3 Corpus type features
Here we describe two complementary approaches
(corpus) in an attempt to alleviate the problem of
domain adaptation, where the datasets used for train-
ing and testing are drawn from different sources.
Pair representation. We treat each pair of texts as a
whole and extract the following sets of corpus fea-
tures: plain bag-of-words, dependency triplets, pro-
duction rules of the syntactic parse tree and a length
feature, i.e. a log-normalized length of the combined
text. Each feature set is normalized and added to the
fvec model.
Corpus classifier. We use the above set of features
to train a multi-class classifier to predict for each in-
stance its most likely corpus type. Our categories
correspond to five dataset types of STS-2012. Pre-
diction scores for each of the dataset categories are
then plugged as features into the final fvec repre-
sentation. Our multi-class classifier is a one-vs-all
binary SVM trained on the merged data from STS-
2012. We apply 5-fold cross-validation scheme, s.t.
for each of the held-out folds we obtain independent
predictions. The accuracy (averaged over 5-folds)
on the STS-2012 data is 92.0%.
3.4 Stacking
To integrate multiple TK models into a single model
we apply a classifier stacking approach (Fast and
Jensen, 2008). Each of the learned TK models is
used to generate predictions which are then plugged
as features into the final fvec representation, s.t.
the final model uses only explicit feature vector
representation. We apply a 5-fold cross-validation
scheme to obtain prediction scores in the same man-
ner as described above.
4 Experimental Evaluation
4.1 Experimental setup
To encode TK models along with the similarity fea-
ture vectors into a single regression scoring model,
56
base corpus TK
U T I B O M C D ALL Mean MSRp MSRv SMTe OnWN SMTn
? 0.7060 0.6087 0.6080 0.8390 0.2540 0.6820 0.4470
? 0.7589 0.6863 0.6814 0.8637 0.4950 0.7091 0.5395
? ? 0.8079 0.7161 0.7134 0.8837 0.5519 0.7343 0.5607
? ? ? 0.8187 0.7137 0.7157 0.8833 0.5131 0.7355 0.5809
? ? ? ? 0.8458 0.7047 0.6935 0.8953 0.5080 0.7101 0.5834
? ? ? ? 0.8468 0.6954 0.6717 0.8902 0.4652 0.7089 0.6133
? ? ? ? ? 0.8539 0.7132 0.6993 0.9005 0.4772 0.7189 0.6481
? ? ? ? ? 0.8529 0.7249 0.7080 0.8984 0.5142 0.7263 0.6700
Sys1 ? ? ? ? ? ? 0.8546 0.7156 0.6989 0.8979 0.4884 0.7181 0.6609
Sys3 ? ? ? ? ? ? 0.8810 0.7416 0.7210 0.8971 0.5912 0.7328 0.6778
Sys2 ? ? ? ? ? ? 0.8705 0.7339 0.7039 0.9012 0.5629 0.7376 0.6656
UKPbest 0.8239 0.6773 0.6830 0.8739 0.5280 0.6641 0.4937
Table 1: System configurations and results on STS-2012. Column set base lists 3 feature sets : UKP (U), Takelab
(T) and iKernels (I); corpus type features (corpus) include plain features (B), corpus classifier (O), and manually
encoded dataset category (M); TK contains constituency (C) and dependency-based (D) models. UKPbest is the best
system of STS-2012. First column shows configuration of our three system runs submitted to STS-2013.
we use an SVR framework implemented in SVM-
Light-TK9. We use the following parameter settings
-t 5 -F 3 -W A -C +, which specifies to use
a combination of trees and feature vectors (-C +),
PTK over trees (-F 3) computed in all-vs-all mode
(-W A) and using polynomial kernel of degree 3 for
the feature vector (active by default).
We report the following metrics employed in the
final evaluation: Pearson correlation for individual
test sets10 and Mean ? an average score weighted by
the test set size.
4.2 STS-2012
For STS-2013 task the entire data from STS-2012
was provided for the system development. To com-
pare with the best systems of the previous year we
followed the same setup, where 3 datasets (MSRp,
MSRv and SMTe) are used for training and 5 for test-
ing (two ?surprise? datasets were added: OnWN and
SMTn). We use the entire training data to obtain a
single model.
Table 1 summarizes the results using structural
models (TK), pairwise similarity (base) and corpus
type features (corpus). We first note, that com-
bining all three features sets (U, T and I) provides
a good match to the best system UKPbest. Next,
adding TK models results in a large improvement
beating the top results in STS-2012. Furthermore,
using corpus features results in even greater im-
9http://disi.unitn.it/moschitti/Tree-Kernel.htm
10for STS-2012 we also report the results for a concatenation
of all five test sets (ALL)
provement with the Mean = 0.7416 and Pearson
ALL = 0.8810.
4.3 STS-2013
Below we specify the configuration for each of the
submitted runs (also shown in Table 1) and report the
results on the STS-2013 test sets: headlines (head),
OnWN, FNWN, and SMT:
Sys1: combines base features (U, T and I), TK
models (C and D) and plain corpus type features (B).
We use STS-2012 data to train a single model.
Sys2: different from Sys1 where a single model
trained on the entire data is used to make predictions,
we adopt a different training/test setup to account for
the different nature of the data used for training and
testing. After performing manual analysis of the test
data we came up with the following strategy to split
the training data into two sets to learn two differ-
ent models: STMe and OnWN (model1) and MSRp,
SMTn and STMe (model2); model1 is then used to
get predictions for OnWN, FNWN, while model2 is
used for SMT and headlines.
Sys3: same as Sys1 + a corpus type classifier O as
described in Sec. 3.3.
Table 2 shows the resulting performance of our
systems and the best UMBC system published in the
final ranking. Sys2 appears the most accurate among
our systems, which ranked 21st out of 88. Compar-
ing to the best system across four datasets we ob-
serve that it performs reasonably well on the head-
lines dataset (it is 5th best), while completely fails
on the OnWN and FNWN test sets. After performing
57
error analysis, we found that TK models underper-
form on FNWN and OnWN sets, which appear un-
derrepresented in the training data from STS-2012.
We build a new system (Sys?2), which is based on
Sys2, by making two adjustments in the setup: (i)
we exclude SMTe from training to obtain predictions
on SMT and head and (ii) we remove all TK features
to train a model for FNWN and OnWN. This is mo-
tivated by the observation that text pairs from STS-
2012 yield a paraphrase model, since the texts are
syntactically very similar. Yet, two datasets from
STS-2013 FNWN, and OnWN contain text pairs
where documents exhibit completely different struc-
tures. This is misleading for our syntactic similarity
model learned on the STS-2012.
System head OnWN FNWN SMT Mean Rank
UMBC 0.7642 0.7529 0.5818 0.3804 0.6181 1
Sys2 0.7465 0.5572 0.3875 0.3409 0.5339 21
Sys1 0.7352 0.5432 0.3842 0.3180 0.5188 28
Sys3 0.7395 0.4228 0.3596 0.3294 0.4919 40
Sys?2 0.7538 0.6872 0.4478 0.3391 0.5732 4*
Table 2: Results on STS-2013.
5 Conclusions and Future Work
We have described our participation in STS-2013
task. Our approach treats text pairs as structural
objects which provides much richer representation
for the learning algorithm to extract useful patterns.
We experiment with structures derived from con-
stituency and dependency trees where related frag-
ments are linked with a special tag. Such struc-
tures are then used to learn tree kernel models which
can be efficiently combined with the a feature vector
representation in a single scoring model. Our ap-
proach ranks 1st with a large margin w.r.t. to the
best systems in STS-2012 task, while it is 21st ac-
cording to the final rankings of STS-2013. Never-
theless, a small change in the system setup makes
it rank 4th. Clearly, domain adaptation represents a
big challenge in STS-2013 task. We plan to address
this issue in our future work.
6 Acknowledgements
This research has been supported by the Euro-
pean Community?s Seventh Framework Program
(FP7/2007-2013) under the #288024 LIMOSINE
project.
References
Eneko Agirre, Daniel Cer, Mona Diab, and Gonzalez-
Agirre. 2012. Semeval-2012 task 6: A pilot on se-
mantic textual similarity. In First Joint Conference on
Lexical and Computational Semantics (*SEM).
Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-
Agirre, and Weiwei Guo. 2013. *sem 2013 shared
task: Semantic textual similarity, including a pilot on
typed-similarity. In *SEM 2013: The Second Joint
Conference on Lexical and Computational Semantics.
Daniel Bar, Chris Biemann, Iryna Gurevych, and Torsten
Zesch. 2012. Ukp: Computing semantic textual sim-
ilarity by combining multiple content similarity mea-
sures. In Proceedings of the Sixth International Work-
shop on Semantic Evaluation (SemEval 2012).
Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and
Eros Zanchetta. 2009. The wacky wide web: a
collection of very large linguistically processed web-
crawled corpora. Language Resources and Evalua-
tion, 43(3):209?226.
Massimiliano Ciaramita and Yasemin Altun. 2006.
Broad-coverage sense disambiguation and informa-
tion extraction with a supersense sequence tagger. In
EMNLP.
Michael Collins and Nigel Duffy. 2002. New Ranking
Algorithms for Parsing and Tagging: Kernels over Dis-
crete Structures, and the Voted Perceptron. In ACL.
Danilo Croce, Paolo Annesi, Valerio Storch, and Roberto
Basili. Unitor: Combining semantic text similarity
functions through sv regression. In SemEval 2012.
Andrew S. Fast and David Jensen. 2008. Why stacked
models perform effective collective classification. In
ICDM.
Evgeniy Gabrilovich and Shaul Markovitch. 2007. Com-
puting semantic relatedness using wikipedia-based ex-
plicit semantic analysis. In IJCAI.
A. Moschitti. 2006. Efficient convolution kernels for
dependency and constituent syntactic trees. In ECML.
Aliaksei Severyn and Alessandro Moschitti. 2012.
Structural relationships for large-scale learning of an-
swer re-ranking. In SIGIR.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method for
semi-supervised learning. In ACL.
Peter D. Turney and Patrick Pantel. 2010. From fre-
quency to meaning: vector space models of semantics.
J. Artif. Int. Res., 37(1):141?188.
Frane S?aric?, Goran Glavas?, Mladen Karan, Jan S?najder,
and Bojana Dalbelo Bas?ic?. 2012. Takelab: Systems
for measuring semantic text similarity. In Proceedings
of the Sixth International Workshop on Semantic Eval-
uation (SemEval 2012).
58
Proceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 75?83,
Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational Linguistics
Learning Adaptable Patterns for Passage Reranking
Aliaksei Severyn(1) and Massimo Nicosia(1) and Alessandro Moschitti1,2
(1)DISI, University of Trento, 38123 Povo (TN), Italy
{severyn,m.nicosia,moschitti}@disi.unitn.it
(2)QCRI, Qatar Foundation, 5825 Doha, Qatar
amoschitti@qf.org.qa
Abstract
This paper proposes passage reranking
models that (i) do not require manual fea-
ture engineering and (ii) greatly preserve
accuracy, when changing application do-
main. Their main characteristic is the
use of relational semantic structures rep-
resenting questions and their answer pas-
sages. The relations are established us-
ing information from automatic classifiers,
i.e., question category (QC) and focus
classifiers (FC) and Named Entity Recog-
nizers (NER). This way (i) effective struc-
tural relational patterns can be automati-
cally learned with kernel machines; and
(ii) structures are more invariant w.r.t. dif-
ferent domains, thus fostering adaptability.
1 Introduction
A critical issue for implementing Question An-
swering (QA) systems is the need of designing
answer search and extraction modules specific to
the target application domain. These modules en-
code handcrafted rules based on syntactic patterns
that detect the relations between a question and its
candidate answers in text fragments. Such rules
are triggered when patterns in the question and the
passage are found. For example, given a ques-
tion1:
What is Mark Twain?s real name?
and a relevant passage, e.g., retrieved by a search
engine:
Samuel Langhorne Clemens, better
known as Mark Twain.
the QA engineers typically apply a syntactic parser
to obtain the parse trees of the above two sen-
tences, from which, they extract rules like:
1We use this question/answer pair from TREC QA as a
running example in the rest of the paper.
if the pattern ?What is NP2?s ADJ
name? is in the question and the pat-
tern ?NP1 better known as NP2?
is in the answer passage then associate
the passage with a high score2.
Machine learning has made easier the task of
QA engineering by enabling the automatic learn-
ing of answer extraction modules. However, new
features and training data have to be typically de-
veloped when porting a QA system from a domain
to another. This is even more critical considering
that effective features tend to be as much complex
and similar as traditional handcrafted rules.
To reduce the burden of manual feature engi-
neering for QA, we proposed structural models
based on kernel methods, (Moschitti et al, 2007;
Moschitti and Quarteroni, 2008; Moschitti, 2008)
with passages limited to one sentence. Their main
idea is to: (i) generate question and passage pairs,
where the text passages are retrieved by a search
engine; (ii) assuming those containing the correct
answer as positive instance pairs and all the oth-
ers as negative ones; (iii) represent such pairs with
two syntactic trees; and (ii) learn to rank answer
passages by means of structural kernels applied to
two trees. This enables the automatic engineering
of structural/lexical semantic patterns.
More recently, we showed that such models can
be learned for passages constituted by multiple
sentences on very large-scale (Severyn and Mos-
chitti, 2012). For this purpose, we designed a shal-
low syntactic representation of entire paragraphs
by also improving the pair representation using re-
lational tags.
In this paper, we firstly use our model in (Sev-
eryn and Moschitti, 2012) as the current baseline
and compare it with more advanced structures de-
rived from dependency trees.
2If the point-wise answer is needed rather than the entire
passage, the rule could end with: returns NP1
75
Search Engine
Kernel-based 
reranker
Reranked
answers
Candidate 
answers
Query
Evaluation
UIMA pipeline
NLP 
annotators
Focus and 
Question 
classifiers
syntactic/semantic 
graph
q/a similarity 
features
train/test 
data
Figure 1: Kernel-based Answer Passage Reranking system
Secondly, we enrich the semantic representa-
tion of QA pairs with the categorical informa-
tion provided by automatic classifiers, i.e., ques-
tion category (QC) and focus classifiers (FC) and
Named Entity Recognizers (NER). FC determines
the constituent of the question to be linked to the
named entities (NEs) of the answer passage. The
target NEs are selected based on their compatibil-
ity with the category of the question, e.g., an NE
of type PERSON is compatible with a category of
a question asking for a human (HUM).
Thirdly, we tested our models in a cross-domain
setting since we believe that: (i) the enriched rep-
resentation is supposed to increase the capability
of learning effective structural relational patterns
through kernel machines; and (ii) such structural
features are more invariant with respect to differ-
ent domains, fostering their adaptability.
Finally, the results show that our methods
greatly improve on IR baseline, e.g., BM25, by
40%, and on previous reranking models, up to
10%. In particular, differently from our previous
work such models can effectively use NERs and
the output of different automatic modules.
The rest of the paper is organized as follows,
Sec. 2 describes our kernel-based reranker, Sec. 3
illustrates our question/answer relational struc-
tures; Sec. 5 briefly describes the feature vectors,
and finally Sec. 6 reports the experimental results
on TREC and Answerbag data.
2 Learning to rank with kernels
2.1 QA system
Our QA system is based on a rather simple rerank-
ing framework as displayed in Figure 1: given a
query question a search engine retrieves a list of
candidate passages ranked by their relevancy. Var-
ious NLP components embedded in the pipeline as
UIMA3 annotators are then used to analyze each
question together with its candidate answers, e.g.,
part-of-speech tagging, chunking, named entity
recognition, constituency and dependency pars-
ing, etc. These annotations are then used to
produce structural models (described in Sec. 3),
which are further used by a question focus detector
and question type classifiers to establish relational
links for a given question/answer pair. The result-
ing tree pairs are then used to train a kernel-based
reranker, which outputs the model to refine the ini-
tial ordering of the retrieved answer passages.
2.2 Tree kernels
We use tree structures as our base representation
since they provide sufficient flexibility in repre-
sentation and allow for easier feature extraction
than, for example, graph structures. We rely on
the Partial Tree Kernel (PTK) (Moschitti, 2006) to
handle feature engineering over the structural rep-
resentations. The choice of PTK is motivated by
its ability to generate rich feature spaces over both
constituency and dependency parse trees. It gen-
eralizes a subset tree kernel (STK) (Collins and
Duffy, 2002) that maps a tree into the space of
all possible tree fragments constrained by the rule
that the sibling nodes from their parents cannot be
separated. Different from STK where the nodes
in the generated tree fragments are constrained to
include none or all of their direct children, PTK
fragments can contain any subset of the features,
i.e., PTK allows for breaking the production rules.
Consequently, PTK generalizes STK, thus gener-
ating an extremely rich feature space, which re-
sults in higher generalization ability.
2.3 Preference reranking with kernels
To enable the use of kernels for learning to
rank with SVMs, we use preference reranking
(Joachims, 2002), which reduces the task to bi-
nary classification. More specifically, the problem
of learning to pick the correct candidate hi from
a candidate set {h1, . . . , hk} is reduced to a bi-
nary classification problem by creating pairs: pos-
itive training instances ?h1, h2?, . . . , ?h1, hk? and
negative instances ?h2, h1?, . . . , ?hk, h1?. This set
can then be used to train a binary classifier. At
classification time the standard one-versus-all bi-
narization method is applied to form all possible
3http://uima.apache.org/
76
pairs of hypotheses. These are ranked according
to the number of classifier votes they receive: a
positive classification of ?hk, hi? gives a vote to
hk whereas a negative one votes for hi.
A vectorial representation of such pairs is the
difference between the vectors representing the
hypotheses in a pair. However, this assumes that
features are explicit and already available whereas
we aim at automatically generating implicit pat-
terns with kernel methods. Thus, for keeping im-
plicit the difference between such vectors we use
the following preference kernel:
PK(?h1, h2?, ?h?1, h?2?) = K(h1, h?1)+
K(h2, h?2)?K(h1, h?2)?K(h2, h?1),
(1)
where hi and h?i refer to two sets of hypothe-
ses associated with two rankings and K is a ker-
nel applied to pairs of hypotheses. We represent
the latter as pairs of question and answer passage
trees. More formally, given two hypotheses, hi =
?hi(q), hi(a)? and hi = ?h?i(q), h?i(a)?, whose
members are the question and answer passage
trees, we define K(hi, h?i) as TK(hi(q), h?i(q)) +
TK(hi(a), h?i(a)), where TK can be any tree ker-
nel function, e.g., STK or PTK.
To enable traditional feature vectors it is enough
to add the product (~xh1 ? ~xh2) ? (~xh?1 ? ~xh?2) tothe structural kernel PK , where ~xh is the feature
vector associated with the hypothesis h.
We opted for a simple kernel sum over a prod-
uct, since the latter rarely works in practice. Al-
though in (Moschitti, 2004) the kernel product has
been shown to provide some improvement when
applied to tree kernels over a subcategorization
frame structure, in general, it seems to work well
only when the tree structures are small and derived
rather accurately (Giordani and Moschitti, 2009;
Giordani and Moschitti, 2012).
3 Structural models of Q/A pairs
First, we briefly describe a shallow tree represen-
tation that we use as our baseline model and then
propose a new dependency-based representation.
3.1 Shallow tree structures
In a shallow syntactic representation first explored
for QA in (Severyn and Moschitti, 2012) each
question and its candidate answer are encoded into
a tree where part-of-speech tags are found at the
pre-terminal level and word lemmas at the leaf
level. To encode structural relationships for a
given q/a pair a special REL tag is used to link
the related structures. The authors adopt a sim-
ple strategy to establish such links: lemmas shared
between a question and and answer get their par-
ents (POS tags) and grandparents (chunk labels)
marked by a REL tag.
3.2 Dependency-based structures
Given the ability of PTK to generate a rich set
of structural features from a relatively flat shal-
low tree representation, we propose to use depen-
dency relations between words to derive an al-
ternative structural model. In particular, we use
a variation of the dependency tree, where depen-
dency relations are altered in such a way that the
words are always at the leaf level. This reorder-
ing of the nodes in the dependency tree, s.t. words
do not form long chains, which is typical in the
standard dependency tree representation, is essen-
tial for PTK to extract meaningful fragments. We
also add part-of-speech tags between the words
and the nodes encoding their grammatical roles
(provided by the original dependency parse tree).
Again a special REL tag is used in the same man-
ner as in the shallow representation to establish
structural links between a question and an answer.
Fig. 2 (top) gives an example of a dependency-
based structure for our example q/a pair.
4 Relational Linking
The use of a special tag to mark the related frag-
ments in the question and answer tree represen-
tations has been shown to yield more accurate
relational models (Severyn and Moschitti, 2012).
However, previous approach was based on a na??ve
hard matching between word lemmas.
Below we propose a novel strategy to estab-
lish relational links using named entities extracted
from the answer along with question focus and
category classifiers. In particular, we use a ques-
tion category to link the focus word of a question
with the named entities extracted from the candi-
date answer. For this purpose, we first introduce
our tree kernel-based models for building a ques-
tion focus and category classifiers.
4.1 Question focus detection
The question focus is typically a simple noun rep-
resenting the entity or property being sought by
the question (Prager, 2006). It can be used to
search for semantically compatible candidate an-
77
NER: Person NER: Personfocus
Figure 2: Dependency-based structure DEP (top) for the q/a pair. Q: What is Mark Twain?s real name? A: Samuel Langhorne
Clemens, better known as Mark Twain. Arrows indicate the tree fragments in the question and its answer passage linked by the
relational REL tag. Shallow tree structure CH (bottom) with a typed relation tag REL-FOCUS-HUM to link a question focus
word name with the named entities of type Person corresponding to the question category (HUM).
swers in document passages, thus greatly reduc-
ing the search space (Pinchak, 2006). While sev-
eral machine learning approaches based on man-
ual features and syntactic structures have been
recently explored, e.g. (Quarteroni et al, 2012;
Damljanovic et al, 2010; Bunescu and Huang,
2010), we opt for the latter approach where tree
kernels handle automatic feature engineering.
In particular, to detect the question focus word
we train a binary SVM classifier with tree ker-
nels applied to the constituency tree representa-
tion. For each given question we generate a set
of candidate trees where the parent (node with the
POS tag) of each candidate focus word is anno-
tated with a special FOCUS tag. Trees with the
correctly tagged focus word constitute a positive
example, while the others are negative examples.
To detect the focus for an unseen question we clas-
sify the trees obtained after tagging each candidate
focus word. The tree yielding the highest classifi-
cation score reveals the target focus word.
4.2 Question classification
Question classification is the task of assigning a
question to one of the pre-specified categories. We
use the coarse-grain classes described in (Li and
Roth, 2002): six non-overlapping classes: Abbre-
viations (ABBR), Descriptions (DESC, e.g. def-
initions or explanations), Entity (ENTY, e.g. an-
imal, body or color), Human (HUM, e.g. group
or individual), Location (LOC, e.g. cities or coun-
tries) and Numeric (NUM, e.g. amounts or dates).
These categories can be used to determine the Ex-
pected Answer Type for a given question and find
the appropriate entities found in the candidate an-
swers. Imposing such constraints on the potential
answer keys greatly reduces the search space.
Previous work in Question Classification re-
veals the power of syntactic/semantic tree repre-
sentations coupled with tree kernels to train the
state-of-the-art models (Bloehdorn and Moschitti,
2007). Hence, we opt for an SVM multi-classifier
using tree kernels to automatically extract the
question class. To build a multi-class classifier
we train a binary SVM for each of the classes and
apply a one-vs-all strategy to obtain the predicted
class. We use constituency trees as our input rep-
resentation.
4.3 Linking focus word with named entities
using question class
Question focus captures the target information
need posed by a question, but to make this piece
of information effective, the focus word needs to
be linked to the target candidate answer. The focus
word can be lexically matched with words present
in an answer, or the match can be established us-
ing semantic information. Clearly, the latter ap-
proach is more appealing since it helps to allevi-
ate the lexical gap problem which makes the na?ive
string matching of words between a question and
its answer less reliable.
Hence, we propose to exploit a question cate-
gory (automatically identified by a question type
classifier) along with named entities found in the
answer to establish relational links between the
tree structures of a given q/a pair. In particu-
lar, once the question focus and question category
78
Table 1: Question classes ? named entity types.
Question Category Named Entity types
HUM Person
LOC Location
NUM Date, Time, Money, Percentage
ENTY Organization, Person
are determined, we link the focus word wfocus in
the question, with all the named entities whose
type matches the question class. Table 1 provides
the correspondence between question classes and
named entity types. We perform tagging at the
chunk level and use two types of relational tags:
plain REL-FOCUS and a tag typed with a ques-
tion class, e.g., REL-FOCUS-HUM. Fig. 2 (bot-
tom) shows an example q/a pair where the typed
relational tag is used in the shallow syntactic tree
representation to link the chunk containing the
question focus name with the named entities of the
corresponding type Person (according to the map-
ping defined in Table 1), i.e. samuel langhorne
clemens and mark twain.
5 Feature vector representation
While the primary focus of our study is on the
structural representations and relations between
q/a pairs we also include basic features widely
used in QA:
Term-overlap features. A cosine similarity be-
tween a question and an answer: simCOS(q, a),
where the input vectors are composed of: (i) n-
grams (up to tri-grams) of word lemmas and part-
of-speech tags, and (ii) dependency triplets. For
the latter, we simply hash the string value of the
predicate defining the triple together with its argu-
ment, e.g. poss(name, twain).
PTK score. For the structural representations we
also define a similarity based on the PTK score:
simPTK(q, a) = PTK(q, a), where the input
trees can be both dependency trees and shallow
chunk trees. Note that this similarity is computed
between the members of a q/a pair, thus, it is very
different from the one defined in Eq. 1.
NER relatedness represents a match between a
question category and the related named entity
types extracted from the candidate answer. It
counts the proportion of named entities in the an-
swer that correspond to the question type returned
by the question classifier.
In our study feature vectors serve a complemen-
tary purpose, while the main focus is to study the
virtue of structural representations for reranking.
The effect of a more extensive number of pairwise
similarity features in QA has been studied else-
where, e.g., (Surdeanu et al, 2008).
6 Experiments
We report the results on two QA collections: fac-
toid open-domain QA corpus from TREC and a
community QA corpus Answerbag. Since we fo-
cus on passage reranking we do not carry out an-
swer extraction. The goal is to rank the passage
containing the right answer in the top position.
6.1 Corpora
TREC QA. In the TREC QA tasks, answer pas-
sages containing correct information nuggets, i.e.
answer keys, have to be extracted from a given text
corpus, typically a large corpus from newswire.
In our experiments, we opted for questions from
2002 and 2003 years, which totals to 824 ques-
tions. AQUAINT newswire corpus4 is used for
searching the supporting answers.
Answerbag is a community-driven QA collection
that contains a large portion of questions that have
?professionally researched? answers. Such an-
swers are provided by the website moderators and
allow for training high quality models. From the
original corpus containing 180k question/answer
pairs, we use 1k randomly sampled questions for
testing and 10k for training.
Question Focus. We use 3 datasets for train-
ing and evaluating the performance of our fo-
cus detector: SeCo-600 (Quarteroni et al, 2012),
Mooney GeoQuery (Damljanovic et al, 2010) and
the dataset from (Bunescu and Huang, 2010). The
SeCo dataset contains 600 questions from which
we discarded a subset of multi-focus questions
and non-interrogative queries. The Mooney Geo-
Query contains 250 question targeted at geograph-
ical information in the U.S. The first two datasets
are very domain specific, so we also carried out
experiments with the dataset from (Bunescu and
Huang, 2010), which contains the first 2000 ques-
tions from the answer type dataset from Li and
Roth annotated with focus words. We removed
questions with implicit and multiple focuses.
Question Classification. We used the UIUIC
dataset (Li and Roth, 2002)5 which contains 5952
4http://www.ldc.upenn.edu/Catalog/docs/LDC2002T31/
5although the QC dataset from (Li and Roth, 2002) in-
cludes additional 50 fine grain classes we opted for using only
6 coarse classes that are sufficient to capture the coarse se-
mantic answer type of the candidate answer. This choice also
results in a more accurate multi-class classifier.
79
factoid questions from different sources (USC,
TREC 8, TREC 9, TREC 10). For training the
classifiers we excluded questions from TREC 8 to
ensure there is no overlap with the data used for
testing models trained on TREC QA.
6.2 Models and Metrics
Our models are built applying a kernel-based
reranker to the output of a search engine.
6.2.1 BM25
We use Terrier6 search engine, which provides
BM25 scoring model for indexing and retrieval.
For the TREC QA 2002 and 2003 task we index
AQUAINT corpus treating paragraphs as docu-
ments. The resulting index contains about 12 mil-
lion items. For the Answerbag we index the entire
collection of 180k answers. We retrieve a list of
top 50 candidate answers for each question.
6.2.2 Reranking models
To train our reranking models we used SVM-light-
TK7, which encodes structural kernels in SVM-
light (Joachims, 2002) solver. In particular, we
use PTK on the relational tree structures combined
with the polynomial kernel of degree 3 applied to
the feature vectors. Therefore, different represen-
tations lead to different models described below.
CH - our basic shallow chunk tree (Severyn and
Moschitti, 2012) used as a baseline structural
reranking model.
DEP - dependency tree augmented with POS tags
and reorganized relations suitable for PTK.
V - reranker model using similarity features de-
fined in Sec. 5.
DEP+V, CH+V - a combination of tree structures
and similarity feature vectors.
+FC+QC - relational linking of the question focus
word and named entities of the corresponding type
using Focus and Question classifiers.
+TFC+QC - a typed relational link refined a ques-
tion category.8
6.2.3 Metrics
We report the following metrics, most commonly
used in QA: Precision at rank 1 (P@1), i.e.,
6http://terrier.org/
7http://disi.unitn.it/moschitti/Tree-Kernel.htm
8? is used for showing the results of DEP, DEP+V and
CH+V structural representations that are significantly better
than the baseline model CH, while ? indicates improvement
of +QC+FC and +QC+TFC tagging applied to basic struc-
tural representations, e.g. CH+V and DEP+V.
Table 2: Structural representations on TREC QA.
MODELS MAP MRR P@1
BM25 0.22 28.02 18.17
V 0.22 28.40 18.54
STRUCTURAL REPRESENTATIONS
CH (S&M, 2012) 0.28 35.63 24.88
CH+V 0.30? 37.45? 27.91?
DEP 0.30? 37.87? 28.05?
DEP+V 0.30? 37.64? 28.05?
REFINED RELATIONAL TAG
CH+V+QC+FC 0.32? 39.48? 29.63?
CH+V+QC+TFC 0.32? 39.49? 30.00?
DEP+V+QC+FC 0.31? 37.49 28.56
DEP+V+QC+TFC 0.31? 38.05? 28.93?
the percentage of questions with a correct an-
swer at rank 1, Mean Reciprocal Rank (MRR),
and Mean Average Precision (MAP). The reported
metrics are averages after performing a 5-fold
cross-validation. We used a paired t-test at 95%
confidence to compare the performance of our
models to a baseline.
6.3 Passage Reranking Results
We first evaluate the impact of two different syn-
tactic representations using shallow and depen-
dency trees. Then, we evaluate the accuracy boost
when such structures are enriched with automati-
cally derived tags, e.g., question focus and ques-
tion category and NEs found in the answer pas-
sage.
6.3.1 Structural representations
Table 2 reveals that using V model results in a
small improvement over BM25 baseline. Indeed,
similarity scores that are most often based on
word-overlap measures even when computed over
various q/a representations are fairly redundant to
the search engine similarity score. Instead, using
the structural representations, CH and DEP, gives
a bigger boost in the performance. Interestingly,
having more features in the CH+V model results
in further improvement while DEP+V seems to re-
main insensitive to additional features provided by
the V model.
6.3.2 Semantically Enriched Structures
In the following set of experiments we explore an-
other strategy for linking structures for a given
q/a pair. We automatically detect the question
focus word and link it to the related named en-
tities in the answer, selected accordingly to the
question category identified by the question clas-
sifier (QC+FC). Further refining the relational link
80
Table 3: Accuracy (%) of focus classifiers.
DATASET ST STK STK+BOW PTK
MOONEY 73.0 81.9 81.5 80.5
SECO-600 90.0 94.5 94.5 90.0
BUNESCU 89.7 98.3 98.2 96.9
Table 4: Accuracy (%) of question classifiers.
DATASET STK+BOW PTK
LI & ROTH 86.1 82.2
TREC TEST 79.3 78.1
with the question category yields QC+TFC model.
First, we report the results of training our question
focus detector and question category classifier.
Focus classifier results. Table 3 displays the ac-
curacies obtained by the question focus detector
on 3 datasets using different kernels: the ST (sub-
tree kernel where fragments contain full subtrees
including leaves), STK, STK+bow (bag-of-words
feature vector is added) and PTK. As we can see,
using STK model yields the best accuracy and we
use it in our pipeline to automatically detect the
focus.
Question classifier results. Table 4 contains the
accuracies of the question classifier on the UIUIC
dataset and the TREC questions that we also use
for testing our reranker models. STK+bow per-
forms better than PTK, since here the input rep-
resentation is a plain constituency tree, for which
STK is particularly suited. Hence, we use this
model to predict the question category.
Ranking results. Table 2 (bottom) summarizes
the performance of the CH+V and DEP+V models
when coupled with QC+FC and QC+TFC strate-
gies to establish the links between the structures
in a given q/a pair. CH structural representation
with QC+FC yields an interesting improvement,
while further refining the relational tag by adding
a question category (QC+TFC) gives slightly bet-
ter results.
Integrating the refined relational tag into the
DEP based structures results more problematic,
since the dependency tree is less suitable for repre-
senting multi-word expressions, named entities in
our case. Hence, using the relational tag to mark
the nodes spanning such multi-word entities in the
dependency structure may result in less meaning-
ful features than in CH model, where words in a
phrase are naturally grouped under a chunk node.
A more detailed discussion on the merits of each
model is provided in the Sec. 6.5.
Table 5: Cross-domain experiment: training on Answerbag
and testing on TREC QA.
MODELS MAP MRR P@1
BM25 0.22 27.91 18.08
V 0.23 28.86 18.90
BASIC STRUCTURAL REPRESENTATIONS
CH (S&M, 2012) 0.24 30.25 20.42
CH+V 0.25? 31.31? 21.28?
DEP+V 0.26? 33.26? 22.21?
REFINED RELATIONAL TAG
CH+V+QC+TFC 0.27? 33.53? 22.81?
DEP+V+QC+TFC 0.29? 34.25? 23.45?
6.4 Learning cross-domain pairwise
structural relationships
To test the robustness of the syntactic patterns au-
tomatically learned by our structural models, we
conduct a cross-domain experiment, i.e. we train
a model on Answerbag data and test it on TREC. It
should be noted that unlike TREC data, where the
answers are simply passages containing the cor-
rect answer phrase, answers in Answerbag specif-
ically address a given question and are generated
by humans. Additionally, TREC QA contains only
factoid questions, while Answerbag is a commu-
nity QA corpus with a large portion of non-factoid
questions. Interestingly, the results demonstrate
the robustness of our syntactic relational model
which captures patterns shared across different do-
mains, e.g. TREC and Answerbag data.
Table 5 shows that: (i) models based on depen-
dency structures result in a better generalization
ability extracting more robust syntactic patterns;
and (ii) the strategy to link the question focus with
the related named entities in the answer provides
an interesting improvement over the basic struc-
tural representations.
6.5 Error Analysis
Consider our running example q/a pair from
Sec. 1. As the first candidate answer, the
search engine retrieves the following incorrect
passage: ?The autobiography of Mark Twain?,
Mark Twain. It is relatively short and mentions the
keywords {Mark, Twain} twice, which apparently
results in a high score for the BM25 model. In-
stead, the search engine ranks the correct answer at
position 34. After reranking using the basic CH+V
model the correct answer is promoted by 20 posi-
tions. While using the CH+V+QC+FC model the
correct answer advances to position 6. Below, we
provide the intuition behind the merits of QC+FC
and QC+TFC encoding question focus and ques-
81
tion category into the basic models.
The model learned by the reranker represents a
collection of q/a pairs from the training set (sup-
port vectors) which are matched against each can-
didate q/a pair. We isolated the following pair
from the model that has a high structural similarity
with our running example:
Q: What is Barbie?s full name?
A: The toy is called after Barbie Millicent
Roberts from Willows.
Despite differences in the surface forms of
the words, PTK extracts matching patterns,
e.g. [S NP [VP VBN] [PP IN] REL-NP],
which yields a high similarity score boosting the
rank of the correct candidate. However, we
note that at the same time an incorrect candi-
date answer, e.g. Mark Twain was accused of
racist language., exhibits similar patterns and also
gets a high rank. The basic structural repre-
sentation is not able to encode essential differ-
ences from the correct answer candidate. This
poses a certain limitation on the discriminative
power of CH and DEP representations. Intro-
ducing a focus tag changes the structural repre-
sentation of both q/a pairs, s.t. the correct q/a
pair preserves the pattern (after identifying word
name as focus and question category as HUM,
it is transformed to [S REL-FOCUS-NP [VP
VBN] [PP IN] REL-FOCUS-NP]), while it
is absent in the incorrect candidate. Thus, linking
the focus word with the related NEs in the answer
helps to discriminate between structurally similar
yet semantically different candidates.
Another step towards a more fine-grained struc-
tural representation is to specialize the relational
focus tag (QC+TFC model). We propose to aug-
ment the focus tag with the question category to
avoid matches with other structurally similar but
semantically different candidates. For example, a
q/a pair found in the list of support vectors:
Q: What is Mark Twain?s place of birth?
A: Mark Twain was raised in Hannibal Missouri.
would exhibit high structural similarity even when
relational focus is used (since the relational tag
does not incorporate the question class LOC), but
refining the focus tag with the question class elim-
inates such cases.
7 Related Work
Previous studies similar to ours carry out pas-
sage reranking by exploiting structural informa-
tion, e.g. using subject-verb-object relations (At-
tardi et al, 2001; Katz and Lin, 2003). Un-
fortunately, the large variability of natural lan-
guage makes such triples rather sparse thus dif-
ferent methods explore soft matching (i.e., lexical
similarity) based on answer types and named en-
tity types (Aktolga et al, 2011). Passage reranking
using classifiers of question and answer pairs were
proposed in (Radlinski and Joachims, 2006; Jeon
et al, 2005).
Regarding kernel methods, our work in (Mos-
chitti et al, 2007; Severyn and Moschitti, 2012)
was the first to exploit tree kernels for modeling
answer reranking. However, such method lacks
the use of important relational information be-
tween a question and a candidate answer, which
is essential to learn accurate relational patterns. In
contrast, this paper relies on shallow and depen-
dency trees encoding the output of question and
focus classifiers to connect focus word and NEs of
the answer passage. This provides more effective
relational information, which allows our model to
significantly improve on previous rerankers.
8 Conclusions
This paper shows a viable research direction in
the automatic QA engineering. One of its main
characteristics is the use of structural kernel tech-
nology to induce features from structural seman-
tic representations of question and answer pas-
sage pairs. The same technology is also used to
construct question and focus classifiers, which are
used to derive relational structures.
An interesting result of this paper is that to de-
sign an answer passage reranker for a new do-
main, we can use off-the-shelf syntactic parsers
and NERs along with little training data for the
QC and FC classifiers. This is due to the fact
that: (i) the kernel technology is able to automat-
ically extract effective structural patterns; and (ii)
the extracted patterns are rather robust, e.g., mod-
els learned on Answerbag improve accuracy on
TREC test data.
Acknowledgements
This research is partially supported by the EU?s 7th
Framework Program (FP7/2007-2013) (#288024
LIMOSINE project) and an Open Collaborative
Research (OCR) award from IBM Research.
82
References
Elif Aktolga, James Allan, and David A. Smith. 2011.
Passage reranking for question answering using syn-
tactic structures and answer types. In ECIR.
Giuseppe Attardi, Antonio Cisternino, Francesco
Formica, Maria Simi, and Ro Tommasi. 2001.
Piqasso: Pisa question answering system. In TREC,
pages 599?607.
Stephan Bloehdorn and Alessandro Moschitti. 2007.
Combined syntactic and semantic kernels for text
classification. In ECIR.
Razvan Bunescu and Yunfeng Huang. 2010. Towards
a general model of answer typing: Question focus
identification. In CICLing.
Michael Collins and Nigel Duffy. 2002. New Ranking
Algorithms for Parsing and Tagging: Kernels over
Discrete Structures, and the Voted Perceptron. In
ACL.
Danica Damljanovic, Milan Agatonovic, and Hamish
Cunningham. 2010. Identification of the question
focus: Combining syntactic analysis and ontology-
based lookup through the user interaction. In LREC.
Alessandra Giordani and Alessandro Moschitti. 2009.
Syntactic structural kernels for natural language in-
terfaces to databases. In Proceedings of ECML
PKDD, ECML PKDD ?09. Springer-Verlag.
Alessandra Giordani and Alessandro Moschitti. 2012.
Translating questions to sql queries with generative
parsers discriminatively reranked. In Proceedings
of The 24rd International Conference on Computa-
tional Linguistics, India. Coling 2012.
Jiwoon Jeon, W. Bruce Croft, and Joon Ho Lee. 2005.
Finding similar questions in large question and an-
swer archives. In CIKM.
T. Joachims. 2002. Optimizing search engines using
clickthrough data. In ACM SIGKDD Conference
on Knowledge Discovery and Data Mining (KDD),
pages 133?142.
Boris Katz and Jimmy Lin. 2003. Selectively using re-
lations to improve precision in question answering.
Xin Li and Dan Roth. 2002. Learning question classi-
fiers. In COLING.
Alessandro Moschitti and Silvia Quarteroni. 2008.
Kernels on linguistic structures for answer extrac-
tion. In ACL.
Alessandro Moschitti, Silvia Quarteroni, Roberto
Basili, and Suresh Manandhar. 2007. Exploit-
ing syntactic and shallow semantic kernels for ques-
tion/answer classification. In ACL.
Alessandro Moschitti. 2004. A study on convolution
kernels for shallow statistic parsing. In Proceedings
of the 42nd Meeting of the Association for Compu-
tational Linguistics (ACL?04), Main Volume, pages
335?342, Barcelona, Spain, July.
Alessandro Moschitti. 2006. Efficient convolution ker-
nels for dependency and constituent syntactic trees.
In ECML.
Alessandro Moschitti. 2008. Kernel methods, syntax
and semantics for relational text categorization. In
CIKM.
Christopher Pinchak. 2006. A probabilistic answer
type model. In In EACL.
John M. Prager. 2006. Open-domain question-
answering. Foundations and Trends in Information
Retrieval, 1(2):91?231.
Silvia Quarteroni, Vincenzo Guerrisi, and Pietro La
Torre. 2012. Evaluating multi-focus natural lan-
guage queries over data services. In LREC.
Filip Radlinski and Thorsten Joachims. 2006. Query
chains: Learning to rank from implicit feedback.
CoRR.
Aliaksei Severyn and Alessandro Moschitti. 2012.
Structural relationships for large-scale learning of
answer re-ranking. In SIGIR.
M. Surdeanu, M. Ciaramita, and H. Zaragoza. 2008.
Learning to rank answers on large online QA collec-
tions. In Proceedings of ACL-HLT.
83
Proceedings of the Eighteenth Conference on Computational Language Learning, pages 39?48,
Baltimore, Maryland USA, June 26-27 2014.
c
?2014 Association for Computational Linguistics
Learning to Rank Answer Candidates
for Automatic Resolution of Crossword Puzzles
Gianni Barlacchi
University of Trento
38123 Povo (TN), Italy
gianni.barlacchi@gmail.com
Massimo Nicosia and Alessandro Moschitti
Qatar Computing Research Institute
5825 Doha, Qatar
m.nicosia@gmail.com, amoschitti@qf.org.qa
Abstract
In this paper, we study the impact of rela-
tional and syntactic representations for an
interesting and challenging task: the au-
tomatic resolution of crossword puzzles.
Automatic solvers are typically based on
two answer retrieval modules: (i) a web
search engine, e.g., Google, Bing, etc. and
(ii) a database (DB) system for access-
ing previously resolved crossword puz-
zles. We show that learning to rank models
based on relational syntactic structures de-
fined between the clues and the answer can
improve both modules above. In particu-
lar, our approach accesses the DB using
a search engine and reranks its output by
modeling paraphrasing. This improves on
the MRR of previous system up to 53% in
ranking answer candidates and greatly im-
pacts on the resolution accuracy of cross-
word puzzles up to 15%.
1 Introduction
Crossword puzzles (CPs) are probably the most
popular language games played around the world.
It is very challenging for human intelligence as it
requires high level of general knowledge, logical
thinking, intuition and the ability to deal with am-
biguities and puns. CPs normally have the form
of a square or rectangular grid of white and black
shaded squares. The white squares on the border
of the grid or adjacent to the black ones are associ-
ated with clues. The goal of the game is to fill the
sequences of white squares with words answering
the clues.
There have been many attempts to build auto-
matic CP solving systems, which have also par-
ticipated in competitions such as The American
Crossword Puzzle Tournament (ACPT). This is
the oldest and largest CP tournament for cross-
word experts held in the United States. The goal
of such systems is to outperform human players
in solving crosswords more accurately and in less
time.
Automatic CP solvers have been mainly tar-
geted by the artificial intelligence (AI) community,
who has mostly focused on AI techniques for fill-
ing the puzzle grid, given a set of answer candi-
dates for each clue. The basic idea is to optimize
the overall probability of correctly filling the entire
grid by exploiting the likelihood of each candidate
answer, fulfilling at the same time the grid con-
straints. After several failures in approaching the
human expert performance, it has become clear
that designing more accurate solvers would not
have provided a winning system. In contrast, the
Precision and Recall of the answer candidates are
obviously a key factor: a very high value for both
of them would enable the solver to quickly find the
correct solution.
This basically suggests that, similarly to the
Jeopardy! challenge case (Ferrucci et al., 2010b),
the solution relies on Question Answering (QA)
research. However, although some CP clues are
rather similar to standard questions, as for ex-
ample, in the clue/answer pair:

What keeps a
camera rolling?: dolly

, some specific differences
hold: (i) clues can be in interrogative form or not,
e.g.,

Capital of USA: Washington

; (ii) they can
contain riddles or be deliberately ambiguous and
misleading (e.g.,

It?s green at first: orange

);
(iii) the exact length of the answer keyword is
known in advance; and (vi) the confidence in the
answers is an extremely important input for the CP
solver.
In this paper, we study methods for improving
the quality of automatic extraction of answer can-
didate lists for automatic CP resolution. For this
purpose, we designed learning to rank models for
reordering the answers produced with two differ-
ent techniques typically used in CP systems: (i)
searching the Web with clue representations, e.g.,
39
exploiting Bing search engine
1
; and (ii) querying
the DB of previously resolved CP clues, e.g., using
standard SQL techniques.
We rerank the text snippets returned by Bing by
means of SVM preference ranking (Herbrich et al.,
2000) for improving the first technique. One in-
teresting contribution is that our model exploits a
syntactic representation of clues to improve Web
search. More in detail, we use structural kernels
(e.g., see (Moschitti, 2006; Moschitti, 2008)) in
SVMs applied to our syntactic representation of
pairs, formed by clues with their candidate snip-
pets. Regarding the DB approach, we provide a
completely novel solution by substituting it and
the SQL function with a search engine for retriev-
ing clues similar to the target one. Then, we rerank
the retrieved clues by applying SVMs and struc-
tural kernels to the syntactic representation of clue
pairs. This way, SVMs learn to choose the best
candidate among similar clues that are available in
the DB. The syntactic representation captures clue
paraphrasing properties.
In order to carry out our study, we created two
different corpora, one for each task: (i) a snip-
pets reranking dataset and (ii) a clue similarity
dataset. The first includes 21,000 clues, each asso-
ciated with 150 candidate snippets whereas the lat-
ter comprises 794,190 clues. These datasets con-
stitute interesting resources that we made available
to the research community
2
.
We compare our methods with one of the best
systems for automatic CP resolution, WebCrow
(Ernandes et al., 2005). Such system does use
the two approaches mentioned before. Regarding
snippet reranking, our structural models improve
on the basic approach of WebCrow based on Bing
by more than 4 absolute percent points in MRR,
for a relative improvement of 23%. Concerning
the similar clues retrieval, our methods improve
on the one used by WebCrow, based on DBs, by
25% absolute, i.e., about 53% of error reduction
whereas the answer accuracy at first position im-
proves up to 70%.
Given such promising results, we used our clue
reranking method in WebCrow, and obtained an
average improvement of 15% in resolving com-
plete CPs. This demonstrates that advanced QA
methods such as those based on syntactic struc-
tures and learning to rank methods can help to win
1
https://www.bing.com/
2
http://projects.disi.unitn.it/
iKernels/projects/webcrow/
the CP resolution challenge.
In the reminder of this paper, Sec. 2 introduces
the automatic CP resolution task in the context
of the related work, Sec. 3 introduces WebCrow,
Sec. 4 illustrates our models for snippets rerank-
ing and similar clue retrieval using kernel meth-
ods, syntactic structures, and traditional feature
vectors, Sec. 5 describes our experiments, and fi-
nally, Sec. 6 derives the conclusions.
2 Related Work
Proverb (Littman et al., 2002) was the first sys-
tem for the automatic resolution of CPs. It in-
cludes several modules for generating lists of can-
didate answers. These lists are merged and used to
solve a Probabilistic-Constraint Satisfaction Prob-
lem. Proverb relies on a very large crossword
database as well as several expert modules, each of
them mainly based on domain-specific databases
(e.g., movies, writers and geography). In addition,
it employs generic-word list generators and clue-
specific modules to find solutions for particular
kinds of clues like

Tel (4): aviv

. Proverb?s
modules use many knowledge sources: databases
of clues, encyclopedias and Web documents. Dur-
ing the 1998 ACPT, Proverb placed 109th out of
251 contestants.
WebCrow (Ernandes et al., 2005) is based on
Proverb. It incorporates additional knowledge
sources, provides a solver for the Italian language
and improves the clues retrieval model from DB.
In particular, it enables partial matching to re-
trieve clues that do not perfectly overlap with the
query. WebCrow carries out basic linguistic anal-
ysis such as Part-Of-Speech tagging and lemma-
tization. It takes advantage of semantic relations
contained in WordNet, dictionaries and gazetteers.
Its Web module is constituted by a search en-
gine, which can retrieve text snippets or docu-
ments related to the clue. Answer candidates
and their confidence scores are generated from
this content. WebCrow uses a WA* algorithm
(Pohl, 1970) for Probabilistic-Constraint Satisfac-
tion Problems, adapted for CP resolution. The
solver fills the grid entries for which no solution
was found by the previous modules. It tries com-
binations of letters that satisfy the crossword con-
straints, where the letters are derived from words
found in dictionaries or in the generated candidate
lists. WebCrow participated in international com-
petitions with good results.
40
Figure 1: Overview of WebCrow?s architecture.
Dr. Fill (Ginsberg, 2011) targets the crossword
filling task with a Weighted-Constraint Satisfac-
tion Problem. Constraint violations are weighted
and can be tolerated. It heavily relies on huge
databases of clues. It was placed 92nd out of more
than 600 opponents in the 2013 ACPT.
Specifically for QA using syntactic structures,
a referring work for our research is the IBM Wat-
son system (Ferrucci et al., 2010a). This is an ad-
vanced QA pipeline based on deep linguistic pro-
cessing and semantic resources. It demonstrated
that automatic methods can be more accurate than
human experts in answering complex questions.
More traditional studies on passage reranking,
exploiting structural information, were carried out
in (Katz and Lin, 2003), whereas other meth-
ods explored soft matching (i.e., lexical similarity)
based on answer and named entity types (Aktolga
et al., 2011). (Radlinski and Joachims, 2006; Jeon
et al., 2005) applied question and answer classi-
fiers for passage reranking. In this context, sev-
eral approaches focused on reranking the answers
to definition/description questions, e.g., (Shen and
Lapata, 2007; Moschitti et al., 2007; Surdeanu et
al., 2008; Severyn and Moschitti, 2012; Severyn
et al., 2013b).
3 WebCrow Architecture
Our research focuses on the generation of accurate
answer candidate lists, which, when used in a CP
resolution systems, can improve the overall solu-
tion accuracy. Therefore, the quality of our mod-
ules can be assessed by testing them within such
systems. For this purpose, we selected WebCrow
as it is rather modular, accurate and it was kindly
made available by the authors. Its architecture is
illustrated in Figure 1.
The solving process is divided in two phases:
in the first phase, the coordinator module forwards
the clues of an input CP to a set of modules for
the generation of several candidate answer lists.
Each module returns a list of possible solutions
for each clue. Such individual clue lists are then
merged by a specific Merger component, which
uses list confidence values and the probabilities of
correctness of each candidate in the lists. Eventu-
ally, a single list of candidate-probability pairs is
generated for each input clue. During the second
phase WebCrow fills the crossword grid by solving
a constraint-satisfaction problem. WebCrow se-
lects a single answer from each candidate merged
list, trying to satisfy the imposed constraints. The
goal of this phase is to find an admissible solution
maximizing the number of correct inserted words.
In this paper, we focus on two essential modules
of WebCrow: the Web and the DB modules, de-
scribed in the next sections.
3.1 WebSearch Module (WSM)
WSM carries out four different tasks: (i) the re-
trieval of useful text snippets (TS) and web docu-
ments, (ii) the extraction of the answer candidates
from such text, (iii) the scoring/filtering of the can-
didates, and (iv) the estimation of the list confi-
dence. The retrieval of TS is performed by the
Bing search engine by simply providing it the clue
through its APIs. Then, the latter again are used
to access the retrieved TS. The word list gener-
ator extracts possible candidate answers from TS
or Web documents by picking the terms (also mul-
tiwords) of the correct length. The generated lists
are merged and sorted using the candidate confi-
dence computed by two filters: the statistical filter
and the morphological filter. The score associated
with each candidate word w is given by the fol-
lowing heuristic formula:
p(w,C) = k(score
sf
(w,C)? score
mf
(w,C)),
where (i) C is the target clue, (ii) k is a
constant tuned on a validation set such that
?
n
i=0
p(w
n
, C) = 1, (iii) score
sf
(w,C) is com-
puted using statistical information extracted from
the text, e.g., the classical TF?IDF, and (iv)
score
mf
(w,C) is computed using morphological
features of w.
41
S
REL-NP REL-NP VP NP
REL-NNP REL-POS TO VB CC RB TO VB NN
hamlet 's to be or not to be addressee
S
NP VP REL-NP REL-NP VP VP NP VP PP NP ADVP PP NP PP NP
DT RBS JJ NN VBZ REL-NNP REL-POS TO VB CC RB TO VB DT NN VBZVBN IN DT NN RB TO PRP TO DT NN
the most obvious example be hamlet 's to be or not to be the monologue be address by a character either to himself to the audience
Figure 2: Shallow syntactic trees of clue (upper) and snippet (lower) and their relational links.
3.2 Database module (CWDB)
The knowledge about previous CPs is essential for
solving new ones. Indeed, clues often repeat in
different CPs, thus the availability of a large DB
of clue-answer pairs allows for easily finding the
answers to previously used clues. In order to ex-
ploit the database of clue-answer pairs, WebCrow
uses three different modules:
CWDB-EXACT, which simply checks for an
exact matching between the target clue and those
in the DB. The score of the match is computed
using the number of occurrences of the matched
clue.
CWDB-PARTIAL, which employs MySQL?s
partial matching function, query expansion and
positional term distances to compute clue-
similarity scores, along with the Full-Text search
functions.
CWDB-DICTIO, which simply returns the full
list of words of correct length, ranked by their
number of occurrences in the initial list.
We improve WSM and CWDB by applying
learning-to-rank algorithms based on SVMs and
tree kernels applied to structural representations.
We describe our models in detail in the next sec-
tion.
4 Learning to rank with kernels
The basic architecture of our reranking framework
is relatively simple: it uses a standard preference
kernel reranking approach (e.g., see (Shen and
Joshi, 2005; Moschitti et al., 2006)). The struc-
tural kernel reranking framework is a specializa-
tion of the one we proposed in (Severyn and Mos-
chitti, 2012; Severyn et al., 2013b; Severyn et al.,
2013a). However, to tackle the novelty of the task,
especially for clue DB retrieval, we modeled inno-
vative kernels. In the following, we first describe
the general framework and then we instantiate it
for the two reranking tasks studied in this paper.
4.1 Kernel framework
The framework takes a textual query and retrieves
a list of related text candidates using a search en-
gine (applied to the Web or a DB), according to
some similarity criteria. Then, the query and can-
didates are processed by an NLP pipeline. The
pipeline is based on the UIMA framework (Fer-
rucci and Lally, 2004) and contains many text
analysis components. The latter used for our spe-
cific tasks are: the tokenizer
3
, sentence detector
1
,
lemmatizer
1
, part-of-speech (POS) tagger
1
, chun-
ker
4
and stopword marker
5
.
The annotations produced by such processors
are used by additional components to produce
structural models representing clues and TS. The
structure component converts the text fragments
into trees. We use both trees and feature vectors
to represent pairs of clues and TS, which are em-
ployed to train kernel-based rerankers for reorder-
ing the candidate lists provided by a search engine.
Since the syntactic parsing accuracy can impact
the quality of our structure and thus the accuracy
of our learning to rank algorithms, we preferred
to use shallow syntactic trees over full syntactic
representations. In the next section, we first de-
scribe the structures we used in our kernels, then
the tree kernels used as building blocks for our
models. Finally, we show the reranking models
for both tasks, TS and clue reranking.
3
http://nlp.stanford.edu/software/
corenlp.shtml
4
http://cogcomp.cs.illinois.edu/page/
software_view/13
5
Based on a standard stoplist.
42
Rank Clue Answer
1 Kind of support for a computer user tech
2 Kind of computer connection wifi
3 Computer connection port
4 Comb users bees
5 Traveling bag grip
Table 1: Clue ranking for the query: Kind of con-
nection for traveling computer users (wifi)
4.2 Relational shallow tree representation
The structures we adopt are similar to those de-
fined in (Severyn et al., 2013b). They are essen-
tially shallow syntactic trees built from POS tags
grouped into chunks. Each clue and its answer
candidate (either a TS or clue) are encoded into
a tree having word lemmas at the leaves and POS
tags as pre-terminals. The higher tree level orga-
nizes POS tags into chunks. For example, the up-
per tree of Figure 2, shows a shallow tree for the
clue: Hamlet?s ?To be, or not to be? addressee,
whereas the lower tree represents a retrieved TS
containing the answer, himself : The most obvious
example is Hamlet?s ?To be or not to be ... the
monologue is addressed by a character either to
himself or to the audience.
Additionally, we use a special REL tag to link
the clue/snippet trees above such that structural re-
lations will be captured by tree fragments. The
links are established as follows: words from a
clue and a snippet sharing a lemma get their par-
ents (POS tags) and grandparents, i.e., chunk la-
bels, marked by a prepending REL tag. We build
such structural representations for both snippet
and similar clue reranking tasks.
4.3 Tree kernels
We briefly report the different types of kernels
(see, e.g., (Moschitti, 2006) for more details).
Syntactic Tree Kernel (STK), also known as a
subset tree kernel (Collins and Duffy, 2002), maps
objects in the space of all possible tree fragments
constrained by the rule that the sibling nodes from
their parents cannot be separated. In other words,
substructures are composed by atomic building
blocks corresponding to nodes along with all of
their direct children. These, in case of a syntac-
tic parse tree, are complete production rules of the
associated parser grammar.
STK
b
extends STK by allowing leaf nodes to be
part of the feature space. Leaf in syntactic trees are
words, from this the subscript b (bag-of-words).
Subtree Kernel (SbtK) is one of the simplest tree
kernels as it only generates complete subtrees, i.e.,
tree fragments that, given any arbitrary starting
node, necessarily include all its descendants.
Partial Tree Kernel (PTK) (Moschitti, 2006) can
be effectively applied to both constituency and de-
pendency parse trees. It generates all possible
connected tree fragments, e.g., sibling nodes can
also be separated and be part of different tree frag-
ments. In other words, a fragment is any possible
tree path, from whose nodes other tree paths can
depart. Thus, it can generate a very rich feature
space resulting in higher generalization ability.
4.4 Snippet reranking
The task of snippet reranking consists in reorder-
ing the list of snippets retrieved from the search
engine such that those containing the correct an-
swer can be pushed at the top of the list. For this
purpose, we transform the target clue in a search
query and retrieve candidate text snippets. In our
training set, these candidate text snippets are con-
sidered as positive examples if they contain the an-
swer to the target clue.
We rerank snippets using preference reranking
approach (see, e.g., (Shen and Joshi, 2005)). This
means that two snippets are compared to derive
which one is the best, i.e., which snippet contains
the answer with higher probability. Since we aim
at using kernel methods, we apply the following
preference kernel:
P
K
(?s
1
, s
2
?, ?s
?
1
, s
?
2
?) = K(s
1
, s
?
1
)+
+K(s
2
, s
?
2
)?K(s
1
, s
?
2
)?K(s
2
, s
?
1
),
where s
r
and s
?
r
refer to two sets of candidates
associated with two rankings and K is a kernel
applied to pairs of candidates. We represent the
latter as pairs of clue and snippet trees. More for-
mally, given two candidates, s
i
= ?s
i
(c), s
i
(s)?
and s
?
i
= ?s
?
i
(c), s
?
i
(s)?, whose members are the
clue and snippet trees, we define
K(s
i
, s
?
i
) = TK(s
i
(c), s
?
i
(c))+TK(s
i
(s), s
?
i
(s)),
where TK can be any tree kernel function, e.g.,
STK or PTK. Finally, it should be noted that, to
add traditional feature vectors to the reranker, it is
enough to add the product (~x
s
1
?~x
s
2
) ?(~x
s
?
1
?~x
s
?
2
)
to the structural kernel P
K
, where ~x
s
is the feature
vector associated with the snippet s.
43
S
REL-NP PP REL-NP PP REL-NP
REL-NNP IN REL-NN IN VBG REL-NN NNS
kind of connection for travel computer user
S
REL-NP PP REL-NP
REL-NNP IN REL-NN REL-NN
kind of computer connection
Figure 3: Two similar clues leading to the same answer.
4.5 Similar clue reranking
WebCrow creates answer lists by retrieving clues
from the DB of previously solved crosswords. It
simply uses the classical SQL operator and full-
text search. We instead verified the hypothesis
that a search engine could achieve a better re-
sult. Thus we opted for indexing the DB clues
and their answers with the open source search en-
gine Lucene (McCandless et al., 2010), using the
state-of-the-art BM25 retrieval model. This alone
significantly improved the quality of the retrieved
clue list, which could be further refined by apply-
ing reranking. The latter consists in (i) retrieving
a list of similar clues using a search engine and
(ii) moving those more similar, which more prob-
ably contain the same answer to the clue query,
at the top. For example, Table 1 shows the first
five clues, retrieved for a query built from the clue:
Kind of connection for traveling computer users.
The search engine retrieves the wrong clue, Kind
of connection for traveling computer users, at the
top since it overlaps more with the query.
To solve these kinds of problems by also en-
hancing the generalization power of our reranking
algorithm, we use a structural representation
similar to the one for TS that we illustrated in
the previous section. The main difference with
the previous models is that the reranking pair is
only constituted by clues. For example, Fig. 3
shows the representation of the pairs constituted
by the query clue and the correct clue ranked
in the second position (see Table 1). The rela-
tional arrows suggest a syntactic transformation
from connection for
*
computer to
computer connection, which can be used
by the reranker to prefer the correct clue to
the wrong one. Note that such transformation
corresponds to the pair of tree fragments: [S
[REL-NP[REL-NN]][PP][NP[VBG][REL-NN]]]
? [S [REL-NP[REL-NN][REL-NN]]], where the
node pairs, ?REL-NN,REL-NN? define the arguments
of the syntactic transformation. Such fragments
can be generated by PTK, which can thus be used
for learning clue paraphrasing.
To build the reranking training set, we used
the training clues for querying the search engine,
which draws candidates from the indexed clues.
We stress the fact that this set of clues is disjoint
from the clues in the training and test sets. Thus,
identical clues are not present across sets. At clas-
sification time, the new clue is used as a search
query. Similar candidate clues are retrieved and
used to form pairs.
4.6 Feature Vectors
In addition to structural representations, we also
used features for capturing the degrees of similar-
ity between clues within a pair.
DKPro Similarity. We used similarity features
from a top performing system in the Semantic
Textual Similarity (STS) task, namely DKPro
from the UKP Lab (B?ar et al., 2013). These
features were effective in predicting the degree
of similarity between two sentences. DKPro in-
cludes the following syntactic similarity metrics,
operating on string sequences, and more advanced
semantic similarities:
? Longest common substring measure (Gusfield,
1997). It determines the length of the longest
substring shared by two text segments.
? Longest common subsequence measure (Allison
and Dix, 1986). It extends the notion of substrings
to word subsequences by applying word insertions
or deletions to the original input text pairs.
? Running-Karp-Rabin Greedy String Tiling
(Wise, 1996). It provides a similarity between two
sentences by counting the number of shuffles in
their subparts.
? Resnik similarity (Resnik, 1995). The WordNet
hypernymy hierarchy is used to compute a mea-
sure of semantic relatedness between concepts
expressed in the text. The aggregation algorithm
by Mihalcea et al. (Mihalcea et al., 2006) is
applied to extend the measure from words to
sentences.
? Explicit Semantic Analysis (ESA) similarity
44
(Gabrilovich and Markovitch, 2007). It represents
documents as weighted vectors of concepts
learned from Wikipedia, WordNet and Wik-
tionary.
? Lexical Substitution (Biemann, 2013). A super-
vised word sense disambiguation system is used
to substitute a wide selection of high-frequency
English nouns with generalizations. Resnik
and ESA features are then computed on the
transformed text.
New features. Hereafter, we describe new fea-
tures that we designed for CP reranking tasks.
? Feasible Candidate. It is a binary feature sig-
naling the presence or absence of words with the
same length of the clue answer (only used for snip-
pet reranking).
? Term overlap features. They compute the co-
sine similarity of text pairs encoded into sets of n-
grams extracted from different text features: sur-
face forms of words, lemmas and POS-tags. They
are computed keeping and removing stop-words.
They complement DKPro features.
? Kernel similarities. These are computed using
(i) string kernels applied to sentences, or PTK ap-
plied to structural representations with and with-
out embedded relational information (REL). This
similarity is computed between the members of a
?clue, snippet? or a ?clue, clue? pair.
5 Experiments
Our experiments aim at demonstrating the effec-
tiveness of our models on two different tasks: (i)
Snippet Reranking and (ii) Similar Clue Retrieval
(SCR). Additionally, we measured the impact of
our best model for SCR in the WebCrow system
by comparing with it. Our referring database of
clues is composed by 1,158,202 clues, which be-
long to eight different crossword editors (down-
loaded from the Web
6
). We use the latter to create
one dataset for snippet reranking and one dataset
for clues retrieval.
5.1 Experimental Setup
To train our models, we adopted SVM-light-TK
7
,
which enables the use of structural kernels (Mos-
chitti, 2006) in SVM-light (Joachims, 2002), with
default parameters. We applied a polynomial ker-
nel of degree 3 to the explicit feature vectors,
6
http://www.crosswordgiant.com
7
http://disi.unitn.it/moschitti/
Tree-Kernel.htm
Model MAP MRR AvgRec REC@1 REC@5
Bing 16.00 18.09 69.00 12.50 24.80
V 18.00 19.88 76.00 14.20 26.10
SbtK 17.00 19.6 75.00 13.80 26.40
STK 18.00 20.44 76.00 15.10 27.00
STK
b
18.00 20.68 76.00 15.30 27.40
PTK 19.00 21.65 77.00 16.10 28.70
V+SbtK 20.00 22.39 80.00 17.20 29.10
V+STK 19.00 20.82 78.00 14.90 27.90
STK
b
19.00 21.20 79.00 15.60 28.40
V+PTK 19.00 21.68 79.00 16.00 29.40
V+DK 18.00 20.48 77.00 14.60 26.80
V+DK+SbtK 20.00 22.29 80.00 16.90 28.70
V+DK+STK 19.00 21.47 79.00 15.50 28.30
V+DK+STK
b
19.00 21.58 79.00 15.4 28.60
V+DK+PTK 20.00 22.24 80.00 16.80 29.30
Table 2: Snippet reranking
Model MAP MRR AvgRec REC@1 REC@5
MB25 69.00 73.78 80.00 62.11 81.23
WebCrow - 53.22 58.00 39.60 62.85
SbtK 52.00 54.72 69.00 36.50 64.05
STK 63.00 68.21 77.00 54.57 76.11
STK
b
63.00 67.68 77.00 53.85 75.63
PTK 65.00 70.12 78.00 57.39 77.65
V+SbtK 68.00 73.26 80.00 60.95 81.28
V+STK 71.00 76.01 82.00 64.58 83.95
V+STK
b
70.00 75.68 82.00 63.95 83.77
V+PTK 71.00 76.67 82.00 65.67 84.07
V+DK 71.00 76.76 81.00 65.55 84.29
V+DK+SbtK 72.00 76.91 82.00 65.87 84.51
V+DK+STK 73.00 78.37 84.00 67.83 85.87
V+DK+STK
b
73.00 78.29 84.00 67.71 85.77
V+DK+PTK 73.00 78.13 83.00 67.39 85.75
Table 3: Reranking of similar clues.
as we believe feature combinations can be valu-
able. To measure the impact of the rerankers as
well as the baselines, we used well known met-
rics for assessing the accuracy of QA and re-
trieval systems, i.e.: Recall at rank 1 (R@1 and
5), Mean Reciprocal Rank (MRR), Mean Average
Precision (MAP), the average Recall (AvgRec).
R@k is the percentage of questions with a cor-
rect answer ranked at the first position. MRR is
computed as follows: MRR =
1
|Q|
?
|Q|
q=1
1
rank(q)
,
where rank(q) is the position of the first correct an-
swer in the candidate list. For a set of queries Q,
MAP is the mean over the average precision scores
for each query:
1
Q
?
Q
q=1
AveP (q). AvgRec and
all the measures are evaluated on the first 10 re-
trieved snippets/clues. For training and testing the
reranker, only the first 10 snippets/clues retrieved
by the search engine are used.
5.2 Snippet Reranking
The retrieval from the Web is affected by a sig-
nificant query processing delay, which prevents us
45
to use entire documents. Thus, we only consid-
ered the text from Bing snippets. Moreover, since
our reranking approach does not include the treat-
ment of special clues such as anagrams or linguis-
tic games, e.g., fill-in-the blank clues, we have ex-
cluded them by our dataset. We crawled the lat-
ter from the Web. We converted each clue into a
query and downloaded the first 10 snippets as re-
sult of a Bing query. In order to reduce noise from
the data, we created a black list containing URLs
that must not be considered in the download phase,
e.g., crossword websites. The training set is com-
posed by 20,000 clues while the test set comprises
1,000 clues.
We implemented and compared many models
for reranking the correct snippets higher, i.e., con-
taining the answer to the clue. The compared sys-
tems are listed on the first column of Table 2,
where: V is the approach using the vector only
constituted by the new feature set (see Sec. 4.6);
DK is the model using the features made available
by DKPro; the systems ending in TK are described
in Sec. 4.3; and the plus operator indicates models
obtained by summing the related kernels.
Depending on the target measure they suggest
slightly different findings. Hereafter, we comment
on MRR as it is the most interesting from a rank-
ing viewpoint. We note that: (i) Bing is improved
by the reranker based on the new feature vector by
2 absolute points; (ii) DK+V improves on V by
just half point; (iii) PTK provides the highest re-
sult among individual systems; (iv) combinations
improve on the individual systems; and (v) over-
all, our reranking improves on the ranking of para-
graphs of Bing by 4 points in MRR and 5 points
in accuracy on the first candidate (REC@1), cor-
responding to about 20% and 50% of relative im-
provement and error reduction, respectively.
5.3 Similar clue retrieval
We compiled a crossword database of 794,190
unique pairs of clue-answer. Using the clues con-
tained in this set, we created three different sets:
training and test sets and the database of clues.
The database of clues can be indexed for retriev-
ing similar clues. It contains 700,000 unique clue-
answer pairs. The training set contains 39,504
clues whose answer may be found in database. Us-
ing the same approach, we created a test set con-
taining 5,060 clues that (i) are not in the training
set and (ii) have at least an answer in the database.
Model MRR REC@1 REC@5 REC@10
WebCrow 41.00 33.00 51.00 58.00
Our Model 46.00 39.00 56.00 59.00
Table 4: Performance on the word list candidates
averaged over the clues of 10 entire CPs
Model %Correct words %Correct letters
WebCrow 34.45 49.72
Our Model 39.69 54.30
Table 5: Performance given in terms of correct
words and letters averaged on the 10 CPs
We experimented with all models, as in the
previous section, trained for the similar clue re-
trieval task. However, since WebCrow includes a
database module, in Tab. 3, we have an extra row
indicating its accuracy. We note that: (i) BM25
shows a very accurate MRR, 73.78%. It largely
improves on WebCrow by about 20.5 absolute per-
cent points, demonstrating the superiority of an IR
approach over DB methods. (ii) All TK types do
not improve alone on BM25, this happens since
they do not exploit the initial rank provided by
BM25. (iii) All the feature vector and TK combi-
nations achieve high MRR, up to 4.5 absolute per-
cent points of improvement over BM25 and thus
25 points more than WebCrow, corresponding to
53% of error reduction. Finally, (iv) the relative
improvement on REC@1 is up to 71% (28.23%
absolute). This high result is promising in the light
of improving WebCrow for the end task of solving
complete CPs.
5.4 Impact on WebCrow
In these experiments, we used our reranking
model of similar clues (more specifically, the
V+DK+STK model) using 10 complete CPs (for
a total of 760 clues) from the New York Times
and Washington Post. This way, we could mea-
sure the impact of our model on the complete task
carried out by WebCrow. More specifically, we
give our reranked list of answers to WebCrow in
place of the list it would have extracted with the
CWDB module. It should be noted that to evalu-
ate the impact of our list, we disabled WebCrow
access to other lists, e.g., dictionaries. This means
that the absolute resolution accuracy of WebCrow
using our and its own lists can be higher (see (Er-
nandes et al., 2008) for more details).
46
The first result that we derive is the accuracy
of the answer list produced from the new data,
i.e., constituted by the 10 entire CPs. The results
are reported in Tab. 4. We note that the improve-
ment of our model is lower than before as a non-
negligible percentage of clues are not solved us-
ing the clue DB. However, when we compute the
accuracy in solving the complete CPs, the impact
is still remarkable as reported by Tab. 5. Indeed,
the results show that when the lists reordered by
our reranker are used by WebCrow, the latter im-
proves by more than 5 absolute percent points in
both word and character accuracy.
6 Conclusions
In this paper, we improve automatic CP resolution
by modeling two innovative reranking tasks for:
(i) CP answer list derived from Web search and
(ii) CP clue retrieval from clue DBs.
Our rankers are based on SVMs and structural
kernels, where the latter are applied to robust shal-
low syntactic structures. Our model applied to
clue reranking is very interesting as it allows us
to learn clue paraphrasing by exploiting relational
syntactic structures representing pairs of clues.
For our study, we created two different corpora
for Snippet Reranking Dataset and Clue Similarity
Dataset on which we tested our methods. The lat-
ter improve on the lists generated by WebCrow by
25 absolute percent points in MRR (about 53% of
relative improvement). When such improved lists
are used in WebCrow, its resolution accuracy in-
creases by 15%, demonstrating that there is a large
room for improvement in automatic CP resolution.
In the future, we would like to add more seman-
tic information to our rerankers and include an an-
swer extraction component in the pipeline.
Acknowledgments
We are deeply in debt with Marco Gori and
Marco Ernandes for making available WebCrow,
for helping us with their system and for the useful
technical discussion regarding research directions.
This research has been partially supported by the
EC?s Seventh Framework Programme (FP7/2007-
2013) under the grants #288024: LIMOSINE
? Linguistically Motivated Semantic aggregation
engiNes. Many thanks to the anonymous review-
ers for their valuable work.
References
Elif Aktolga, James Allan, and David A. Smith. 2011.
Passage reranking for question answering using syn-
tactic structures and answer types. In ECIR.
L Allison and T I Dix. 1986. A bit-string longest-
common-subsequence algorithm. Inf. Process. Lett.,
23(6):305?310, December.
Daniel B?ar, Torsten Zesch, and Iryna Gurevych. 2013.
Dkpro similarity: An open source framework for
text similarity. In Proceedings of the 51st An-
nual Meeting of the Association for Computational
Linguistics (System Demonstrations) (ACL 2013),
pages 121?126, Stroudsburg, PA, USA, August. As-
sociation for Computational Linguistics.
Chris Biemann. 2013. Creating a system for lexi-
cal substitutions from scratch using crowdsourcing.
Lang. Resour. Eval., 47(1):97?122, March.
Michael Collins and Nigel Duffy. 2002. New rank-
ing algorithms for parsing and tagging: Kernels over
discrete structures, and the voted perceptron. In Pro-
ceedings of the 40th Annual Meeting on Association
for Computational Linguistics, ACL ?02, pages 263?
270, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Marco Ernandes, Giovanni Angelini, and Marco Gori.
2005. Webcrow: A web-based system for crossword
solving. In In Proc. of AAAI 05, pages 1412?1417.
Menlo Park, Calif., AAAI Press.
Marco Ernandes, Giovanni Angelini, and Marco Gori.
2008. A web-based agent challenges human experts
on crosswords. AI Magazine, 29(1).
David Ferrucci and Adam Lally. 2004. Uima: An
architectural approach to unstructured information
processing in the corporate research environment.
Nat. Lang. Eng., 10(3-4):327?348, September.
David Ferrucci, Eric Brown, Jennifer Chu-Carroll,
James Fan, David Gondek, Aditya Kalyanpur, Adam
Lally, J. William Murdock, Eric Nyberg, John
Prager, Nico Schlaefer, and Chris Welty. 2010a.
Building watson: An overview of the deepqa
project. AI Magazine, 31(3).
David A. Ferrucci, Eric W. Brown, Jennifer Chu-
Carroll, James Fan, David Gondek, Aditya Kalyan-
pur, Adam Lally, J. William Murdock, Eric Nyberg,
John M. Prager, Nico Schlaefer, and Christopher A.
Welty. 2010b. Building watson: An overview of the
deepqa project. AI Magazine, 31(3):59?79.
Evgeniy Gabrilovich and Shaul Markovitch. 2007.
Computing semantic relatedness using wikipedia-
based explicit semantic analysis. In Proceedings of
the 20th International Joint Conference on Artifical
Intelligence, IJCAI?07, pages 1606?1611, San Fran-
cisco, CA, USA. Morgan Kaufmann Publishers Inc.
47
Matthew L. Ginsberg. 2011. Dr.fill: Crosswords and
an implemented solver for singly weighted csps. J.
Artif. Int. Res., 42(1):851?886, September.
Dan Gusfield. 1997. Algorithms on Strings, Trees, and
Sequences: Computer Science and Computational
Biology. Cambridge University Press, New York,
NY, USA.
R Herbrich, T Graepel, and K Obermayer. 2000. Large
margin rank boundaries for ordinal regression. In
A.J. Smola, P.L. Bartlett, B. Sch?olkopf, and D. Schu-
urmans, editors, Advances in Large Margin Classi-
fiers, pages 115?132, Cambridge, MA. MIT Press.
Jiwoon Jeon, W. Bruce Croft, and Joon Ho Lee. 2005.
Finding similar questions in large question and an-
swer archives. In CIKM.
Thorsten Joachims. 2002. Optimizing search en-
gines using clickthrough data. In Proceedings of the
Eighth ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining, KDD ?02,
pages 133?142, New York, NY, USA. ACM.
Boris Katz and Jimmy Lin. 2003. Selectively using re-
lations to improve precision in question answering.
Michael L. Littman, Greg A. Keim, and Noam Shazeer.
2002. A probabilistic approach to solving crossword
puzzles. Artificial Intelligence, 134(12):23 ? 55.
Michael McCandless, Erik Hatcher, and Otis Gospod-
netic. 2010. Lucene in Action, Second Edition:
Covers Apache Lucene 3.0. Manning Publications
Co., Greenwich, CT, USA.
Rada Mihalcea, Courtney Corley, and Carlo Strappa-
rava. 2006. Corpus-based and knowledge-based
measures of text semantic similarity. In Proceedings
of the 21st National Conference on Artificial Intelli-
gence - Volume 1, AAAI?06, pages 775?780. AAAI
Press.
Alessandro Moschitti, Daniele Pighin, and Roberto
Basili. 2006. Semantic role labeling via tree kernel
joint inference. In Proceedings of CoNLL-X, New
York City.
Alessandro Moschitti, Silvia Quarteroni, Roberto
Basili, and Suresh Manandhar. 2007. Exploit-
ing syntactic and shallow semantic kernels for ques-
tion/answer classification. In ACL.
Alessandro Moschitti. 2006. Efficient convolution ker-
nels for dependency and constituent syntactic trees.
In ECML, pages 318?329.
Alessandro Moschitti. 2008. Kernel methods, syntax
and semantics for relational text categorization. In
CIKM.
Ira Pohl. 1970. Heuristic search viewed as path finding
in a graph. Artificial Intelligence, 1(34):193 ? 204.
Filip Radlinski and Thorsten Joachims. 2006. Query
chains: Learning to rank from implicit feedback.
CoRR.
Philip Resnik. 1995. Using information content to
evaluate semantic similarity in a taxonomy. In
Proceedings of the 14th International Joint Confer-
ence on Artificial Intelligence - Volume 1, IJCAI?95,
pages 448?453, San Francisco, CA, USA. Morgan
Kaufmann Publishers Inc.
Aliaksei Severyn and Alessandro Moschitti. 2012.
Structural relationships for large-scale learning of
answer re-ranking. In Proceedings of the 35th in-
ternational ACM SIGIR conference on Research and
development in information retrieval (SIGIR), pages
741?750. ACM.
Aliaksei Severyn, Massimo Nicosia, and Alessandro
Moschitti. 2013a. Building structures from clas-
sifiers for passage reranking. In CIKM, pages 969?
978.
Aliaksei Severyn, Massimo Nicosia, and Alessandro
Moschitti. 2013b. Learning adaptable patterns for
passage reranking. In Proceedings of the Seven-
teenth Conference on Computational Natural Lan-
guage Learning, pages 75?83, Sofia, Bulgaria, Au-
gust. Association for Computational Linguistics.
Libin Shen and Aravind K. Joshi. 2005. Ranking
and reranking with perceptron. Machine Learning,
60(1-3):73?96.
D. Shen and M. Lapata. 2007. Using semantic roles to
improve question answering. In EMNLP-CoNLL.
M. Surdeanu, M. Ciaramita, and H. Zaragoza. 2008.
Learning to rank answers on large online QA collec-
tions. In Proceedings of ACL-HLT.
Michael J. Wise. 1996. Yap3: Improved detection
of similarities in computer program and other texts.
In Proceedings of the Twenty-seventh SIGCSE Tech-
nical Symposium on Computer Science Education,
SIGCSE ?96, pages 130?134, New York, NY, USA.
ACM.
48
