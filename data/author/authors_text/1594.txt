Quantitative and Qualitative Evaluation of the OntoLearn Ontology Learning
System
Roberto NAVIGLI, Paola VELARDI
Dipartimento di Informatica
Universit? ?La Sapienza?
via Salaria 113
Roma, Italy, 00198
{velardi,navigli}@di.uniroma1.it
Alessandro CUCCHIARELLI, Francesca NERI
DIIGA
Universit? Politecnica delle Marche
via Brecce Bianche 12
Ancona, Italy, 60131
{cucchiarelli, neri}@diiga.univpm.it
Abstract
Ontology evaluation is a critical task, even more so
when the ontology is the output of an automatic
system, rather than the result of a conceptualisation
effort produced by a team of domain specialists
and knowledge engineers. This paper provides an
evaluation of the OntoLearn ontology learning
system. The proposed evaluation strategy is
twofold: first, we provide a detailed quantitative
analysis of the ontology learning algorithms, in
order to compute the accuracy of OntoLearn under
different learning circumstances. Second, we
automatically generate natural language
descriptions of formal concept specifications, in
order to facilitate per-concept qualitative analysis
by domain specialists.
1 Evaluating ontologies
Automatic methods for ontology learning and
population have been proposed in recent literature
(e.g. ECAI-2002 and KCAP-2003 workshops1) but
a co-related issue then becomes the evaluation of
such automatically generated ontologies, not only
with the goal of comparing the different
approaches (Hovy, 2001) and ontology-based tools
(Angele and Sure, 2002), but also to verify whether
an automatic process may actually compete with
the typically human process of converging on an
agreed conceptualization of a given domain.
Ontology construction, apart from the technical
aspects of a knowledge representation task (i.e.
choice of representation languages, consistency
and correctness with respect to axioms, etc.), is a
consensus building process, one that implies long
and often harsh discussions among the specialists
of a given domain. Can an automatic method
simulate this process? Can we provide domain
specialists with a means to measure the adequacy
of a specific set of concepts as a model of a given
                                                       
1ECAI-2002 http://www-sop.inria.fr/acacia/WORKSHOPS/
ECAI2002-OLT/accepted-papers.html
KCAP-2003 http://km.aifb.uni-karlsruhe.de/ws/semannot
2003/papers.html
domain?, Specialists are often unable to evaluate
the formal content of a computational ontology
(e.g. the denotational theory, the formal notation,
the knowledge representation system capabilities
like property inheritance, consistency, etc.).
Evaluation of the formal content is rather tackled
by computational scientists, or by automatic
verification systems. The role of the specialists is
instead to compare their intuition of a domain with
the description of this domain, as provided by the
ontology concepts. To facilitate one such
qualitative per-concept evaluation, we devised a
method for automatic generation of textual
explanations (glosses) of automatically learned
concepts. Glosses provide a description, in natural
language, of the formal specifications assigned to
the learned concepts. An expert can easily compare
his intuition with these natural language
descriptions.
The objective of the gloss-based evaluation is, as
previously remarked, to obtain a judgement, by
domain specialists, concerning the adequacy of an
automatically derived domain conceptualisation.
On the computational side, an ontology learning
tool is based on a battery of software programs
aimed at extracting and formalising domain
knowledge, usually starting from unstructured
data. Therefore, it is equally important to produce a
detailed evaluation of these programs, on a
quantitative ground, in order to gain insight on the
internal and external contingencies that may affect
the result of an ontology learning process.
In what follows, we firstly provide a quantitative
evaluation of the OntoLearn ontology learning
system, under different learning circumstances.
Secondly, we describe the gloss-based per-concept
evaluation method. Both evaluation strategies are
experimented in two application domains: Tourism
and Economy.
The subsequent section provides a sketchy
description of the OntoLearn algorithms. Details
are found in (Navigli and Velardi, 2004) and
(Navigli, Velardi and Gangemi, 2003). Sections 3
and 4 are dedicated to the quantitative and
qualitative analyses of OntoLearn.
2 Summary of the OntoLearn system
OntoLearn is an ontology population method
based on text mining and machine learning
techniques. OntoLearn starts with an existing
generic ontology (we use WordNet, though other
choices are possible) and a set of documents in a
given domain, and produces a domain extended
and trimmed version of the initial ontology. The
ontology generated by OntoLearn is anchored to
texts, it can be therefore classified as a linguistic
ontology (G?mez-P?rez et al 2004).
OntoLearn has been applied to different domains
(tourism, computer networks, economy) and in
several European projects2.
Concept learning is achieved in the following
three phases:
1) Terminology Extraction: A list of domain
multi-word expressions (MWE hereafter) is
extracted from a set of documents that are
judged representative of a given domain.
MWEs are extracted using natural language
processing and statistical techniques.
Contrastive corpora and glossaries in different
domains are used to prune terminology that is
not domain-specific. Domain MWEs are
selected also on the basis of an entropy-based
measure that simulates specialist consensus on
concepts choice: in words, the probability
distribution of a ?good? domain MWE must be
uniform across the individual documents of the
domain corpus.
2) Semantic interpretation of MWEs: Semantic
interpretation is based on a principle,
compositional interpretation, and on a novel
algorithm, called structural semantic
interconnections (SSI) .  Composi t ional
interpretation signifies that the meaning of a
multi-word expression (MWE) can be derived
compositionally from its components3, e.g. the
meaning of business plan is derived first, by
associating the appropriate concept identifier,
with reference to the initial top ontology, to the
component terms (i.e. sense #2 of business and
sense #1 of plan in WordNet), and then, by
identifying the semantic relations holding
among the involved concepts (e.g.
                                                       
2
 E.g. : Harmonize IST-2000-29329  and the INTEROP network of
excellence, started on december 2003.
3
 In the literature, multi word expressions are classified as
compositional, idiosyncratically compositional and non-
compositional.  In mid-technical domains, compositional MWEs
cover about 60-70% of MWE (we cannot support with data our
statitics for sake of space)
plan#1 topic? ? ? ? business# 2 ).
3) Extending and trimming the initial
o n t o l o g y : Once the terms have been
semantically interpreted, they are organized in
sub-trees, and appended under the appropriate
node  o f  t he  i n i t i a l  on to logy,
e.g. business _ plan# 1 kind _ of? ? ? ? ? ? plan# 1 .
Furthermore, certain upper and lower nodes of
the initial ontology are pruned to create a
domain-view  of the ontology. The final
ontology is output in OWL language.
SSI lies in the area of syntactic pattern matching
algorithms (Bunke and Sanfeliu, 1990). It is a word
sense disambiguation algorithm used to determine
the correct sense (with reference to the initial
ontology) for each component of a complex MWE.
The algorithm is based on building a graph
representation for alternative senses of each MWE
component4, and then selecting the appropriate
senses on the basis of detected s e m a n t i c
interconnection patterns between graph pairs. The
SSI algorithm seeks for semantic interconnections
among the words of a context T. Contexts Ti are
generated from groups of partially overlapping
complex MWEs (extracted during phase 1 of the
OntoLearn procedure) sharing the same syntactic
head . For example, given the list of complex
MWEs securities portfolio, investment portfolio,
real-estate portfolio, junk-bond portfolio,
diversified portfolio, stock portfolio, bond
portfolio, loan portfolio, the following list of term
components is created:
T=[security, investment, real-estate, estate, bond,
junk-bond, diversified, stock, portfolio, loan ]
Relevant pattern types  are described by a
context free grammar G. An example of rule in G
is the following (S1 S2 and S are concepts, i.e.
synsets in WordNet):
Rule Name:gloss+hyperonymy/meronymy (S1,S2):
Def: :SynsetsG?? S1
gloss
? ? ? ?  S
 and there is a
hyperonymy/meronymy path between S and S2
For instance, in railways company, the gloss of
railway#1 contains the word organization, and
there is an hyperonymy path of length 2 between
company#1 and organization#1. That is:
railway#1 gloss? ? ? ?  o r g a n i z a t i o n # 1 ,  a n d :
company#1
  
kind _of
? ? ? ? ? ? institution#1 
  
kind _of
? ? ? ? ? ? 
organization#1. This pattern (an instance of the
gloss+hypeonymyr/meronymy rule) cumulates
                                                       
4
 We remark again that a detailed description of the SSI algorithm
is in (Navigli & Velardi, 2004) and (Navigli, Velardi and Gangemi,
2003). Graphs are generated on the basis of lexico-semantic
information in WordNet and in a variety of on-line resources, see the
mentioned papers for details.
evidence for senses #1 of both ra i lway and
company.
In SSI, the correct sense St for a term t?T is
selected depending upon the number and weight of
patterns matching with rules in G. The weights of
patterns are automatically learned using a
perceptron5 model. The weight function is given
by:
)
_
1
()()1(
j
jjj patternlength
patternweight ?? +=
where ? j  is the weight of rule j in G, and the
second addend is a smoothing parameter inversely
proportional to the length of the matching pattern
(e.g. 2 in the previous example, since 2 is the
minimal length of the rule, and the actual length of
the pattern is 3). The perceptron has been trained
on the SemCor6 semantically annotated corpus.
In order to complete the semantic interpretation
process, OntoLearn then attempts to determine the
semantic relations that hold between the
components of a complex concept. In order to do
this, it was first necessary to select an inventory of
semantic relations. We examined several
proposals, like EuroWordnet (Vossen, 1999),
DOLCE (Masolo et al, 2002), FrameNet
(Ruppenhofer Fillmore & Baker, 2002) and others.
 As also remarked in (Hovy, 2001), no
systematic methods are available in literature to
compare the different sets of relations. Since our
objective was to define an automatic method for
semantic relation extraction, our final choice was
to use a reduced set of FrameNet relations, which
seemed general enough to cover our application
domains (tourism, computer networks, economy).
The choice of FrameNet is motivated by the
availability of a sufficiently large set of annotated
examples of conceptual relations7, that we used to
train an available machine learning algorithm,
TiMBL (Daelemans et al, 2002). The relations
used are: Material, Purpose, Use, Topic, Product,
Constituent Parts, Attribute8. Examples for each
relation are the following:
net # 1 attribute? ? ? ? ? ? loss# 3
takeover# 2 topic? ? ? ? proposal# 1
sand# 1 material? ? ? ? ? ? beach# 1
merger# 1 purpose? ? ? ? ? ? agreement# 1
                                                       
5
 http://www.cs.waikato.ac.nz/ml/weka/
6
 http://www.cs.unt.edu/~rada/downloads.html#semcor
7
 The choice of FrameNet was motivated more by availability than
appropriateness.
8
 The relation Attribute is not in FrameNet, however it was a
useful relation for terminological strings of the adjective_noun type.
meeting# 1 use? ? ? ? ro om# 1
bond# 2 const _ part? ? ? ? ? ? ? market# 1
c om puter# 1 product? ? ? ? ?  com pany# 1
We represented training instances as pairs of
concepts annotated with the appropriate conceptual
relation, e.g.:
[(computer#1,maker#3),Product]
Each concept is in turn represented by a feature-
vec to r  where attributes are the concept?s
hyperonyms in WordNet, e.g.:
(computer#1,maker#3):
((computer#1,machine#1,device#1
,instrumentality#3),(maker#3,business
#1,enterprise#2,organization#1))
3 Quantitative Evaluation of OntoLearn
This section provides a quantitative evaluation of
OntoLearn s main algorithms. We believe that a
quantitative evaluation is particularly important in
complex learning systems, where errors can be
produced at almost any stage. Even though some
of these errors (e.g. subtle sense distinctions) may
not have a percievable effect on the final ontology,
as shown by the results of the qualitative
evaluation in Section 4.2, it is nevertheless
important to gain insight on the actual system
capabilities, as well as on the pararmeters and
external circumstances that may positively or
negatively influence the final performance.
3.1 Evaluating the MWE extraction
algorithm
The terminology extraction algorithm has been
evaluated in the context of the European project
Harmonise on Tourism interoperability. We first
collected a corpus of about 1 million words of
tourism documents, mainly descriptions of travel
and tourism sites. From this corpus, a syntactic
parser extracted an initial list of 14,383 candidate
complex MWEs from which the statistical filters
selected a list of 3,840 domain-relevant complex
MWEs, that were submitted to the domain
specialists. The Harmonise ontology partners were
not skilled to evaluate the OntoLearn semantic
interpretation of MWEs, therefore we let them
evaluate only the domain appropriateness of the
terms. The gloss generation method described in
Section 4 was subsequently concieved to overcome
this limitation.
We obtained a precision ranging from 72.9% to
about 80% and a recall of 52.74%. The precision
shift is due to the well-known fact that experts may
have different intuitions about the relevance of a
concept. The recall estimate was produced by
manually inspecting 6,000 of the initial 14,383
candidate MWEs, asking the experts to mark all
the MWEs judged as ?good? domain MWEs, and
comparing the obtained list with the list of terms
automatically filtered by OntoLearn.
We ran similar experiments on an Economy
corpus and a Computer Network corpus, but in this
case the evaluation was performed by the authors.
Overall, the performance of the MWE extraction
task appears to be influenced by the dimension and
the focus of the starting corpus (e.g. ?generic
tourism? vs. ?hotel accomodation descriptions?).
Small and unfocused corpora do not favor the
efficacy of statistical analysis. However, the
availability of sufficiently large and focused
corpora seems a realistic requirement for most
applications.
3.2 Evaluating the ontology learning
algorithms
The distinctive task performed by OntoLearn is
semantic disambiguation. The performance of the
SSI algorithm critically depends upon two factors:
the first is the ability to detect semantic
interrelations among concepts associated to the
words of complex MWEs, the second is the
dimension of the context T available to start the
disambiguation process.
As for the first factor, there are two possible
ways of enhancing reliable identification of
semantic interconnections: one is to tune at best the
weight of individual rules in G (e.g. formula (1) in
Section 2), the second is to enrich the semantic
information associated to alternative word senses.
The latter is an on-going research activity.
As far as the context T is concerned, the
intuition is that, with a larger T , there are higher
chances of detecting semantic patterns among the
?correct? senses of the terms in T. However, the
dimension of contexts Ti is an external
contingency, it depends upon the available corpus.
Accordingly, we evaluated the SSI algorithm
using as parameters the dimension of T, T , and
the weights associated to rules in G. We ran several
experiments over the full terminology extracted
from the Economy and Tourism corpora, but
performances are computed only on, respectively,
453 and 638 manually disambiguated terms. This
means that in a context Ti including, e.g. k terms,
we evaluate OntoLearn?s sense choices only for
the fragment of j ? k terms, for which the ?true?
sense has been manually assigned.
Table 1 shows the performance of SSI (precision
and recall) when using only patterns whose weight,
computed with formula (1) is over a threshold ? .
The ?Core? column in Table 1 shows the
performance of SSI when accepting only these core
patterns, while the third column refers to all
matching patterns. With ? = 0,7  a subset of 7-9
rules9 in G (over a total of 20) are used by the
algorithm. Interestingly enough, these rules have a
high probability of being hired, as shown by the
relatively low difference in recall. The Baseline
tower in Table 1 is computed selecting always the
first sense (senses in WordNet are ordered by
probability in everyday language).
Table 2 shows that performance of SSI is indeed
affected by the dimension of T. Large T , as
expected, improves the performance, however,
overly large contexts (>80 terms) may favor the
detection of non-relevant patterns.
In general, both experiments show that the
Economy corpus performs better than the Tourism,
since the latter is less technical (the baseline is
quite high), rather unfocused, and contexts Ti are
much less populated.
Table 1. Performances as a function of pattern?s
weight
Table 2. Performances as a function of |T|
We remark that SSI performs better than
standard WSD (word sense disambiguation) tasks
but this is also motivated by the fact that context
words in T are more interrelated than co-occurring
words in generic sentences. The SSI algorithm, by
                                                       
9
 in formula (1), ? , that depends upon the rule, has a much
higher influence than ? , that depends upon the matching pattern)
86.40%
57.17% 53.76%
59.72%
80.21%76.48%
81.77%
85.48%
71.30% 67.33%
0%
20%
40%
60%
80%
100%
Baseline Core All Rules Baseline Core All Rules
Prec.
Recall Finance Tourism
78.57% 81.82% 82.84% 80.29% 80%
58.51% 63.28%
73.16%
55.46%
73.20%
0%
20%
40%
60%
80%
100%
|T| < 35 35 <= |T| < 65 |T| >= 65 |T| < 35 |T| >= 35
Prec. Recall
Finance Tourism
its very nature, is favored by focused and large
contexts. In any case, it is worth mentioning that
SSI received the second best score in the latest
SenSeval-310, gloss disambiguation exercise,
placed about 1% below the first and about 11%
before the third participant.
3.3 Evaluating the semantic annotation
algorithm
To test the semantic relation annotation task, we
used a learning set (including selected annotated
examples from FrameNet (FN), Tourism (Tour),
and Economy (Econ)), and a test set with a
distribution of examples shown in Table 3.
Table 3. Distribution of examples in the learning
and test set for the semantic annotation task
Learning Set Test Set
Sem_Rel FN Tour Econ Tot FN Tour Econ Tot
MATERIAL 8 3 0 11 5 2 0 7
USE 9 32 2 43 6 20 1 27
TOPIC 52 79 100 231 29 43 50 122
C_PART 3 7 12 22 2 4 6 12
PURPOSE 26 64 22 112 14 34 11 59
PRODUCT 3 1 6 10 1 1 4 6
Total 101 186 142 429 57 104 72 233
Notice that the relation Attribute is generated
whenever the term associated to one of the
concepts is an adjective. Therefore, this semantic
relation is not included in the evaluation
experiment, since it would artificially increase
performances. We then tested the learner on test
sets for individual domains11, leading to the
results shown in Table 4 a and b.
Table 4 Performance of the semantic annotation
task on a) Tourism b) Economy
d<=10% d<=30% d<=100%
Precision MACRO 0,958 0,875 0,847
Recall MACRO 0,283 0,636 0,793
F1 MACRO 0,437 0,737 0,819
Precision micro 0,900 0,857 0,798
Recall micro 0,087 0,635 0,798
F1 micro 0,158 0,721 0,798
d<=10% d<=30% d<=100%
Precision MACRO 1,000 0,804 0,651
Recall MACRO 0,015 0,403 0,455
F1 MACRO 0,030 0,537 0,536
Precision micro 1,000 0,758 0,750
Recall micro 0,042 0,653 0,750
F1 micro 0,080 0,701 0,750
The performance measures are those adopted in
TREC competitions12. The parameter d  in the
above Tables is a confidence factor defined in the
TiMBL algorithm. This parameter can be used to
                                                       
10
 SensEval?3   http://www.senseval.org/senseval3
11
 This of course penalised the results (the performance over a test
set composed by examples of all the three domains is much higher),
but provides a more realistic test bed of the generality of the approach.
12
 http://trec.nist.gov/
increase system?s robustness in the following way:
whenever the confidence associated by TiMBL to
the classification of a new instance is lower than a
given threshold, we output a ?generic? conceptual
relation, named Relatedness. We experimentally
fixed the threshold for d  around 30% (central
column of Table 4).
Table 4 demonstrates rather good performances,
however the main problem with semantic relation
annotation is the unavailability of an agreed set of
conceptual relations, and a sufficiently large and
balanced training set. Consequently, we need to
update the set of used relations whenever we
analyse a new domain, and re-run the training
phase enriching the training corpus with manually
tagged examples from the new domain (as for in
Table 2).
4  Qualitative evaluation: Evaluating the
generated ontology on a per-concept basis
The lesson learned during the Harmonise EC
project was that the domain specialists, tourism
operators in our case, can hardly evaluate the
formal aspects of a computational ontology. When
presented with the domain extended and trimmed
version of WordNet (OntoLearn?s phase 3 in
Section 2), they were only able to express a generic
judgment on each node of the hierarchy, based on
the concept label. These judgments were used to
evaluate the terminology extraction task, but the
experiment suggested that, indeed, it was necessary
to provide a better description for the learned
concepts.
4.1 Gloss generation grammar
To help human evaluation on a per-concept
basis, we decided to enhance OntoLearn with a
gloss generation algorithm. The idea is to generate
glosses in a way that closely reflects the key
aspects of the concept learning process, i.e.
semantic disambiguation and annotation with a
conceptual relation.
The gloss generation algorithm is based on the
definition of a grammar with distinct generation
rules for each type of semantic relation.
Let Sih
sem _ rel
? ? ? ? ? ? Sj
k
 be the complex concept
associated to a complex term whw k (e.g. jazz
festival, or long-term debt), and let:
<H>= the syntactic head of whwk (e.g. festival,
debt)
<M> = the syntactic modifier of whwk (e.g. jazz,
long-term)
<GNC>= be the gloss of the new complex concept
Shk
<HYP>= the selected sense of <H>(e.g.
respectively, festival#1 and debt#1).
<MSGHYP>= the main sentence13 of the
WordNet gloss of <HYP>
<MSGM>= the main sentence of the WordNet
gloss of the selected sense for <M>
Here we provide two examples of rules for
generating GNCs:
If sem_rel=Topic, <GNC>:: = a kind of <HYP>,
<MSGHYP>, relating to the <M>, <MSGM>.
e,g.: GNC(jazz festival): a kind of festival,  a
day or period of time set aside for feasting and
celebration,  relating to the jazz, a style of dance
music popular in the 1920.
If sem_rel=Attribute, <GNC>:= a kind of <HYP>,
<MSGHYP>, <MSGM>.
e.g.:GNC(long term debt)= a kind of debt, the
state of owing something (especially money),
relating to or extending over a relatively long time.
4.2 Per-concept evaluation experiment
To verify the utility of gloss generation, the
automatically generated glosses were submitted for
evaluation to two human experts, a tourism
specialist from ECCA14, and an economist from
the University of Ancona. The specialists were not
aware of the method used to generate glosses; they
have been presented with a list of concept-gloss
pairs and asked to fill in an evaluation form (see
Appendix) as follows: vote 1 means
?unsatisfactory definition?, vote 2 means ?the
definition is helpful?, vote 3 means ?the definition
is fully acceptable?. Whenever he was not fully
happy with a definition (vote 2 or 1), the specialist
was asked to provide a brief explanation. For
comparison, Appendix 2 shows also glossary
definitions extracted from the web for the same
MWEs, that were not shown to the specialists.
Table 5 provides a summary of the
evaluation..
Table 5. Evaluation of glosses by domain
specialists.
vote =1 vote=2 vote=3 uncertai
n
average
Tourism
total
(97)
33
(34.7)
14
(14.4)
45
(46.3)
5 (5.1) 2,13
Ecomo
my total
(134)
52
(38.8)
16
(11.9)
66
(49.2)
- 2.10
The following conclusions can be drawn from
this experiment:
1 .  Overall, the two domain specialists fully
accepted the system?s choices in 45-49% of the
cases, and were reasonably satisfied in 12-14%
                                                       
13
 The main sentence is the gloss pruned of subordinates,
examples, etc.
14
 ECCA ? eTourism Competence Center Austria.
of the cases. The average vote is above 2 in
both cases.
2. As expected, if a MWE is compositional, the
generated definition is more often accepted or
fully accepted (e.g. examples 25_E and 14_T
in Appendix 2). When a compositional
interpretation is not accepted (vote=1), this is
motivated either by an OntoLearn
interpretation error (wrong sense or wrong
conceptual relations) or by the unavailability
of a correct sense in WordNet, despite the fact
that the sense is not idiosyncratic. OntoLearn
errors for compositional MWEs are 7 (5%) in
Economy and 12 (13%) in Tourism.  Examples
of OntoLearn errors and core ontology
?misses? are the definitions 14_T (wrong sense
of form) and 19_E (no good sense for bilateral
in WordNet), respectively.
3.  Sometimes the specialists found it acceptable
also an idiosyncratic or non compositional
definition. This happens in 16 cases for the
Tourism domain (16%) and in 19 cases for the
Economy domain (13%). Examples are the
MWEs 45_E and 76_E, both idiosyncratically
decomposable, in Appendix 2.
One of the specialists is particularly involved in
ontology building projects, therefore we report his
valuable comment: ?some of the descriptions
would not be appropriate to take them over in a
tourism ontology just as they are. But most of them
are quite helpful as basis for building the ontology.
The most important problem from my point of view
is the too detailed descriptions of the components
itself instead of the meaning of the overall term in
this context. Best example is the term ?bed tax?.
Nobody would expect a definition of a bed or a
tax.? In other terms, he found disturbing the fact
that a definition extensively reports the definitions
of its components. On the other side, our objective
is not only to produce concept definitions, but also
to organize concepts in hierarchies. Showing the
definitions of individual components is a ?natural?
mean to verify that the correct senses have been
selected (e.g. the correct senses of bed and tax).
This is clearly the case, since, for example in
definition 14_T (booking form), the specialist was
immediately able to diagnose a sense
disambiguation error for form , though he was
unaware of the OntoLearn methodology.
5 Concluding remarks
This paper presented an in-depth evaluation of
the Ontolearn ontology learning system. The three
basic algorithms (terminology extraction, sense
disambiguation and annotation with semantic
relation) have been individually evaluated in two
domains, under different parametrizations, to
obtain a realistic and comprehensible picture of
system?s capabilities. The critical algorithm, SSI,
has very good performances that are favored by the
fact that word sense disambiguation is applied to
group of words (domain MWEs) that are strongly
semantically related, unlike for generic WSD tasks
(e.g. Senseval). The performance of the SSI
algorithm can be further improved through an
extension of the grammar G, which is an on-going
research activity.
6 Acknowledgements
Our thanks go to Dr. Wolfram H?pken, from
ECCA ? eTourism Competence Center Austria
(wolfram@hoepken.org ) and Dr. Renato
Iacobucci, from the University of Ancona, who
gave up their precious time to evaluate our glosses.
This work has been in part supported by the
INTEROP Network of Excellence IST-2003-
508011
References
J. Angele and Y. Sure (2002) ?Whitepaper:
Evaluation of Ontology-based Tools?, Workshop
on evaluation of ontology-based tools
(EON2002), at the 13th Int. EKAW 2002,
Sigueza (Spain), September 2002.
H. Bunke and A. Sanfeliu (editors) (1990).
Syntactic and Structural pattern Recognition:
Theory and Applications, World Scientific,
Series in Computer Science vol. 7, 1990.
Daelemans,W. Zavrel, J. Van den Sloot, K. & Van
den Bosch, A. (2002). TiMBL: Tilburg Memory
Based Learner. Version 4.3 Reference Guide.
Tilburg University.
G?mez-P?rez, A., Fern?ndez-Lopez M. and
Corcho O. (2004). Ontological Engineering,
Springer Verlag, London, 2004.
Hovy, E. (2001). Comparing Sets of Semantic
relations in Ontologies. In R. Geen, C.A. Bean
and S. Myaeng Semantic of relations. Kluwer.
Masolo, C., Borgo, S., Gangemi, A., Guarino, N.
Oltramari, A. & Schneider, L. (2002).
Sweetening Ontologies with DOLCE.
Proceedings of the 13th International Conference
on Knowledge Engineering and Knowledge
Management. Ontologies and the Semantic Web.
Navigli, R. & Velardi, P. (2004). Learning Domain
Ontologies from Document Warehouses and
Dedicated Web Sites. Computational Linguistics,
MIT press, (50)2.
Navigli, R., Velardi, P. Gangemi, A. (2003).
Corpus Driven Ontology Learning: a Method
and its Application to Automated Terminology
Translation. IEEE Intelligent Systems (18)1.22-
31.
Ruppenhofer, J., Fillmore, C.J. & Baker, C.F.
(2002). Collocational Information in the
FrameNet Database. In Braasch, A. and Povlsen,
C. (eds.), Proceedings of the Tenth Euralex
International Congress. Copenhagen, Denmark.
Vol. I: 359--369, 2002.
Vossen, P. (1999). EuroWordNet: General
D o c u m e n t .  V e r s i o n  3  F i n a l .
http://www.hum.uva.nl/~ewn
APPENDIX: Excerpt of the per-concept evaluation form
Concept #: 25_E Term: business_plan Synt: N-N Rel<w1,w2>: Topic
Gloss: a kind of plan, a series of steps to be carried out or goals to be accomplished, relating to the business, the activity
of providing goods and services involving financial and commercial and industrial aspects.
Specialist vote: 3
Comment by Specialist: none
Diagnose: none
Glossary definition: a written report that states what a company (or a part of a company) aims to do increase sales,
develop new products, etc. within a certain period, and how it will obtain the necessary finances and resources.
Concept #: 2_T Term: affiliated_hotel Synt: Agg-N Rel<w1,w2>: Attribute
Gloss: a kind of hotel, a building where travellers can pay for lodging and meals and other services, being joined in close
association.
Specialist vote: 3
Comment by Specialist: none
Diagnose: none
Glossary definition: a hotel that is a member of a chain, franchise, or referral system. Membership provides special
advantages, particularly a national reservation system.
Concept #: 14_T Term: booking_form Synt: N-N Rel<w1,w2>: Purpose
Gloss: a kind of form, alternative names for the body of a human being, for booking, the act of reserving (a place or
passage) or engaging the services of (a person or group).
Specialist vote: 1
Comment by Specialist: definition of form  wrong in this context
Diagnose: OntoLearn disambiguation error for form
Glossary definition: a document which purchasers of tours must complete to give the operator full particulars about who
is buying the tour.
Concept #: 19_E Term: bilateral_aid Synt: Agg-N Rel<w1,w2>: Attribute
Gloss: a kind of aid, the activity of contributing to the fulfillment of a need or furtherance of an effort or purpose, having
identical parts on each side of an axis.
Specialist vote: 1
Comment by Specialist: Fully wrong definition.
Diagnose: WordNet gloss of bilateral  is not adequate to domain (no better definition is available in WordNet).
Glossary definition:  assistance given by one country to another.
Concept #: 45_E Term: cyclical_uneployment Synt: Agg-N Rel<w1,w2>: Attribute
Gloss: a kind of unemployment, the state of being unemployed or not having a job, recurring in cycles.
Specialist vote: 3
Comment by Specialist: none
Diagnose: none
Glossary definition: workers are without a job because of a lack of aggregate demand due to a down turn in economic
activity.
Concept #: 76_E Term: foreign_aid Synt: Agg-N Rel<w1,w2>: Attribute
Gloss: a kind of aid, the activity of contributing to the fulfillment of a need or furtherance of an effort or purpose, of
concern to or concerning the affairs of other nations .
Specialist vote: 3
Comment by Specialist: none
Diagnose: none
Glossary defonition: the international transfer of public and private funds in the form of loans or grants from donor
countries to recipient countries.
Squibs and Discussions 
Unsupervised Named Entity Recognition 
Using Syntactic and Semantic Contextual 
Evidence 
Alessandro Cucchiarelli* 
Universita di Ancona 
Paola Velardi t
Universit~i di Roma 'La Sapienza' 
Proper nouns form an open class, making the incompleteness of manually or automatically earned 
classification rules an obvious problem. The purpose of this paper is twofold:first, o suggest the use 
of a complementary "backup" method to increase the robustness of any hand-crafted ormachine- 
learning-based NE tagger; and second, to explore the effectiveness of using more fine-grained 
evidence--namely, s ntactic and semantic ontextual knowledge--in classifying NEs. 
1. Proper Noun Classification 
In this paper we present a corpus-driven statistical technique that uses a learning 
corpus to acquire contextual classification cues, and then uses the results of this 
phase to classify unrecognized proper nouns (PN) in an unlabeled corpus. Training 
examples of proper nouns are obtained using any available named entity (NE) recog- 
nizer (in our experiments we used a rule-based recognizer and a machine-learning- 
based recognizer). The contextual model of PN categories i learned without supervi- 
sion. 
The approach described in this paper is complementary to current methods for 
NE recognition: our objective is to improve, without additional manual effort, the 
robustness of any available NE system through the use of more "fine-grained" con- 
textual knowledge, best exploited at a relatively late stage of analysis. The method is 
particularly useful when an available NE system must be rapidly adapted to another 
language or to another domain, provided the shift is not dramatic. 
Furthermore, our study provides experimental evidence relating to two issues 
still under debate: i) the effectiveness, in practical NLP applications, of using syntactic 
relations (most systems use plain collocations and morphological features), and ii) 
context expansion based on thesauri. While we do not provide a definitive argument 
in favor of syntactic ontexts and semantic expansion for word sense disambiguation 
tasks in general, we do show that they can be successfully used for unknown proper 
noun classification. Proper nouns have particular characteristics, such as low or zero 
ambiguity, which makes it easier to characterize their contexts. 
2. Description of the U_PN Classification Method 
In this section we briefly summarize the corpus-based tagging technique for the classi- 
fication of unknown proper nouns (for more details, see Cucchiarelli, Luzi, and Velardi 
\[1998\]). 
* Istituto di Informatica, Via Brecce Bianche 1-60131 Ancona, Italy. E-mail: alex@inform.unian.it 
t Dipartimento di Scienze dell'Informazione, Via Salaria 113, 1-00198 Roma, Italy. E-mail: velardi@ 
dsi.uniromal.it 
Computational Linguistics Volume 27, Number 1 
2.1 Learning Contextual Sense Indicators 
Our method proceeds as follows: first, by means of any available NE recognition 
technique (which we will call an early NE classifier), at least some examples of PNs in 
each category are detected. Second, through an unsupervised corpus-based technique, 
typical PN syntactic and semantic contexts are learned. Syntactic and semantic cues can 
then be used to extend the coverage of the early NE classifier, increasing its robustness 
to the limitations of the gazetteers (PN dictionaries) and domain shifts. 
In phase one, a learning corpus in the application domain is morphologically 
processed. The gazetteer lookup and the early NE classifier are then used to detect 
PNs. At the end of this phase, "some" PNs are recognized and classified, depending 
upon the size of the gazetteer and the actual performance (in the domain) of the NE 
classifier. 
In phase two, the objective is to learn a contextual model of each PN category, 
augmented with syntactic and semantic features. Since the algorithm is unsupervised, 
statistical techniques are applied to smooth the weight of acquired examples as a 
function of semantic and syntactic ambiguity. 1 
Syntactic processing is applied over the corpus. A shallow parser (see details in 
Basili, Pazienza, and Velardi \[1994\]) extracts from the learning corpus elementary syn- 
tactic relations such as Subject-Object, Noun-Preposition-Noun, etc. 2 An elementary 
syntactic link (esl) is represented as: 
esl(wi, mod( typei, Wk ) ) 
where wj is the headword, Wk is the modifier, and type i is the type of syntactic relation 
(e.g. Prepositional Phrase, Subject-Verb, Verb-Direct-Object, e c.). For example, esl(close 
mod(G_N_V_Act Xerox)) reads: Xerox is the modifier of the head close in a Subject-Verb 
(G_N_V_Act) syntactic relation. 
In our study, the context of a word w in a sentence S is represented by the esls 
including w as one of its arguments (wj or Wk). The esls that include semantically 
classified PNs as one of their arguments are grouped in a database, called PN_esl. 
This database provides contextual evidence for assigning a category to unknown PNs. 
2.2 Tagging Unknown PNs 
A corpus-driven algorithm is used to classify unknown proper nouns recognized as 
such, but not semantically classified by the early NE recognizer. 3 
? Let U_PN be an unknown proper noun, i.e., a single word or a complex 
nominal. Let Cpn = (Cp~l, Cpn2 . . . . .  CpnN) be the set of semantic ategories 
for proper nouns (e.g. Person, Organization, Product, etc.). Finally, let 
ESL be the set of esls (often more than one in a text) that include U_PN 
as one of their arguments. 
? For each esli in ESL let: 
esli( wj, mod( typei, Wk )) = esli( x, U_PN) 
1 We say the algorithm is unsupervised because neither the NE items detected by the early recognizer 
nor the extracted syntactic ontexts are inspected for correctness. 
2 Shallow, or partial parsers are a well-established technique for corpus parsing. Several partial parsers 
are readily available---for example, the freely downloadable LINK parser. 
3 A standard POS tagger augmented with simple heuristics is used to detect possible instances of PNs. 
Errors are originated only by ambiguous entence beginners, as "Owens Illinois" or "Boots Plc" 
causing partial recognition. 
124 
Cucchiarelli and Velardi Unsupervised Named Entity Recognition 
where x = w\] or x = Wk and U-PN=wk or wj (the unknown PN can be 
either the head or the modifier), type i is the syntactic type of esl (e.g. 
N-of-N, NAN, V-for-N, etc.), and furthermore l t: 
pl(esli(x, U_PN) ) 
be the plausibi l i ty of a detected esl. Plausibility is a measure of the 
statistical evidence of a detected syntactic relation (Basili, Marziali, and 
Pazienza 1994; Grishman and Sterling 1994) that depends upon local 
(i.e., sentence-level) syntactic ambiguity and global corpus evidence. The 
plausibility accounts for the uncertainty arising from syntactic ambiguity. 
,. Finally, let: 
- -  ESLA be a set of esls in PN_esl (the previously learned 
contextual model) defined as follows: for each esli(x, Uff)N) in 
ESL, put in ESLA the set of eslj(x, PNj) with typej = type i, x in 
the same position as esli, and PNj a known proper noun, in 
the same position as U_PN in esli. 
ESLB be a set of esls in PN_esl defined as follows: for each 
esli(x, U_PN) in ESL put in ESLB the set of eslj(w, PNj) with 
type\] -- type i, w in the same position as x in esli, Sim(w,x) > 6, 
and PNj a known proper noun, in the same position as U_PN 
in esli. Sim(w, x) is a similarity measure between x and w. In 
our experiments, Sim(w,x) > ~ iff w and x have a common 
hyperonym H in WordNet. The generality of H (i.e., the 
number of levels from x to H) is made parametric, to analyze 
the effect of generalization. 
? For each semantic ategory Cp,j compute vidence(Cp,j) as: 
E weightq (x)D(x, C(PNj)) 
esliC ESLA,C( PNj)=Cpn j 
evidence(Cp~j) = a + 
E weight~j (x)D(x, C(PNj)) 
esliEESLA 
E weightq (x)D(x, C(PNj)) 
esli E ESLB,C( PNj) =Cpn j fl 
E weightiy(x)D(x'C(PNJ )) 
esli6 ESLB 
where: 
weightq(x) = weight q( esli(x, PNj) ) = pl( esli(x, PNj) ) ? (1 - ~(~)-1~_1 , 
u weightij(w ) = weightij(esli(w, PNj) ) = pl(esli(w, PNj)). (1 - amb(w)-l~k_\] 2 
pl(esli(x, PNj)) is the plausibility and arab(x) is the ambiguity 
of x in esli 
k is a constant factor used to incrementally reduce the influence 
of ambiguous words. The smoothing is tuned to be higher in 
ESLB 
a and fl are parametric, and can be used to study the evidence 
provided by ESLA and ESLB 
125 
Computational Linguistics Volume 27, Number 1 
D(x, C(PNj)) is a discrimination factor used to determine the 
saliency (Yarowsky 1992) of a context esli(x, _) for a category 
C(PNj), i.e., how good a context is at discriminating between 
C(PNj)and the other categories. 4 
The selected category for U~N is 
C = argmax(evidence(Cp~k)) 
When grouping all the evidence of a U_PN in a text, the underlying hypothesis i
that, in a given linguistic domain (finance, medicine, etc.), a PN has a unique sense. This 
is a reasonable restriction for Proper Nouns, supported by empirical evidence, though 
we would be more skeptical about the applicability of the one-sense-per-discourse 
paradigm (Gale, Church, and Yarowsky 1992) to generic words. We believe that it is 
precisely this restriction that makes the use of syntactic and semantic ontexts effective 
for PNs. 
Notice that the formula of the evidence has several smoothing factors that work to- 
gether to reduce the influence of unreliable or uninformative contexts. The formula also 
has parameters (k, ~, fl), estimated by running systematic experiments. Standard sta- 
tistical techniques have been used to balance xperimental conditions and the sources 
of variance. 
3. Using WordNet for Context Generalization 
One of the stated objectives of this paper is to investigate the effect of context gen- 
eralization (the addend ESLB in the formula of the evidence) on our sense tagging 
task. 
The use of on-line thesauri for context generalization has already been investigated 
with limited success (Hearst and Schuetze 1993; Brill and Resnik 1994; Resnik 1997; 
Agirre and Rigau 1996). Though the idea of using thesauri for context expansion is 
quite common, there are no clear indications that this is actually useful in terms of 
performance. However, studying the effect of context expansion for a PN tagging task 
in particular is relevant because: 
PNs may be hypothesized to have a unique sense in a text, and even in a 
domain corpus. Therefore, we can reliably consider as potential sense 
indicators all the contexts in which a PN appears. The only source of 
ambiguity is then the word wi co-occurring in a syntactic ontext with a 
PN, esli(wi, U_PN), but since in ESLB we group several contexts, 
hopefully spurious hyperonyms of wi will gain lower evidence. For 
example, consider the context "division of Americand3randsdnc". Division 
is a highly ambiguous word, but, when generalizing it, the majority of 
its senses appearing in the same type of syntactic relation with a Proper 
Noun (e.g. branch of Drexel_ Burnhamd,ambert_Group dnc, part of Nationale_ 
Nederlanden_Group) are indeed pertinent senses. 
4 For example, a Subject_Verb phrase with the verb make (e.g., Ace made acontract) isfound with almost 
equal probability with Person and Organization names. We used a simple conditional probability 
model for D(x, c(PNj)), but we believe that more refined measures could improve performance. 
126 
Cucchiarelli and Velardi Unsupervised Named Entity Recognition 
? PN categories (e.g., Person, Location, Product) exhibit a more stable and 
less ambiguous contextual behavior than other more vague categories, 
such as psychological_feature. 5 
? We can study the degree of generalization at which an opt imum 
performance is achieved. 
4. Experimental Discussion 
The purpose of experimental evaluation is twofold: 
To test the improvement in robustness of a state-of-the-art NE recognizer. 
To study the effectiveness of syntactic ontexts and of a "cautious" 
context generalization on the performance of the U_PN tagger, analyzed 
in isolation. The effect of generalization is studied by gradually relaxing 
the notion of similarity in the formula of evidence and by tuning, 
through the factors a and fl, the contribution of generalized contexts to 
the formula of evidence. 
In our experiment, we used the Italian Sole24Ore half-million-word corpus on 
financial news, the one-mill ion-word Wall Street Journal corpus, and WordNet, as stan- 
dard on-line available resources, as well as a series of computational tools made avail- 
able for our research: 
? the VIE system (Humphreys et al 1996) for initial detection of Proper 
Nouns from the learning corpus; for the same purpose we also used a 
machine learning method based on decision lists, described in Paliouras, 
Karkaletsis, and Spyropolous (1998). 
? the SSA shallow syntactic analyzer (Basili, Pazienza, and Velardi 1994) 
for surface corpus parsing. 6
? the tool described in Cucchiarelli and Velardi (1998) for corpus-driven 
WordNet pruning. 7
4.1 Experiment 1: Improving Robustness of NE Recognizers 
The objective of Experiment 1 is to verify the improvement in robustness of existing 
NE recognizers, through the use of our tagger. In Figure 1, three testing experiments 
are shown. The table measures the local performance of the NE tagging task achieved 
by the early NE recognizer, by our untrained tagger, and finally, the joint performance 
of the two methods. 
In the first test, we used the Italian Sole24Ore corpus. Due to the unavailability of 
WordNet in Italian, we used a dictionary of strict synonyms for context expansion. In 
this test, we "loosely" adapted the English VIE system (as used in MUC-6) to Italian. 
5 In Velardi and Cucchiarelli (2000) we formally studied the relation between category type and 
learnability of contextual cues for WSD. 
6 We also used the GATE partial parser. We were not as successful with this parser because it is not 
designed for high-performance VP3?P and NP-PP detection, but prepositional contexts are often the 
most informative indicators. 
7 This method produces a20-30% reduction of the initial WordNet ambiguity, depending on the specific 
corpus. 
127 
Computational Linguistics Volume 27, Number 1 
A B C D E F G H I J K L 
Test 1 239 355 67.32% 339 70.50% 60 83 72.29% 75 80.00% 84.23% 88.20% 
Test 2 650 793 81.90% 759 85.63% 67 83 80.72% 80 83.75% 90.42% 94.47% 
Test 3 3,040 4,168 72.94% 3,233 94.03% 585 935 62.57% 810 72.22% 86.97% 89.66% 
Legend 
A: PNs correctly tagged by the early NE recognizer 
B: Total PNs in the Test Corpus 
C: Local Recall of the early NE recognizer (A/B) 
D: Total PNs detected by the early NE recognizer (D = A + A1 (errors) + G(unknown) 
E: Local Precision of the early NE recognizer (A/D) 
F: UPNs correctly tagged by the UPN tagger in the Test Corpus 
G: Total UPNs not detected by the early NE recognizer 
H: Local recall of UPN tagger (Phase2) (F/G) 
I: Total UPNs for which a decision was possible by the UPN tagger 
\]: Local precision of the UPN tagger 
K: Joint Recall of the two methods (A + F)/B 
L: Joint Precision of the two methods (A+F)/D 
Figure 1 
Outline of results on the Sole24Ore corpus. 
We used the English gazetteer as it was and we appl ied simple " language porting" to 
the NE grammar  (e.g., replacing English words and preposit ions with corresponding 
Italian words, and little more), s This justifies the low performance of the rule-based 
classifier. Note that our context-based tagger produces a considerable improvement  in
performance (around 18%), therefore the global performance (column K and L) turns 
out to be comparable with state-of-the-art systems, without a significant readaptation 
effort. 
In the second test, we used again VIE, on the English Wall Street Journal corpus. 
We used a version of VIE that was designed to detect NE in a management  succession 
domain (we are testing the effect of a domain shift here). Local performance was 
somewhat  lower than in MUC-6. Again, we measured a 9% improvement  using our 
tagger, and very high global performance. 
The third test was the most demanding. Here, we used only half of the named 
entity gazetteer used in previous experiments. The purpose of this test was also to 
verify the effect on performance of a poorly populated gazetteer. In this test, rather than 
using LASIE, we used a machine learning method described in Paliouras, Karkaletsis 
and Spyropolous (1998). This method uses as a training set the available half of the 
gazetteer to learn a context-based decision list for NE classification. 
As shown in Test 3, column B, the initial number  of PNs in the test corpus is now 
considerably higher. The decision-list classifier is tuned to classify with high precision 
and lower recall. Therefore, only the "hardest" cases are submitted to our untrained 
classifier. In fact, local performance of our classifier is around 10% lower than for pre- 
vious tests, but nevertheless, global performance (in terms of joint precision and recall) 
shows an improvement.  Finally, we observe that the performance figures reported in 
Figure 1 say nothing about the various sources of errors. Errors and misses occur both 
during the off-line learning phase (as we said, NE instances and syntactic contexts 
8 Most location and company names known worldwide (e.g., NewYork, IBM) are in fact mentioned in 
economic journals regardless of the language. 
128 
Cucchiarelli and Velardi Unsupervised Named Entity Recognition 
are not inspected for correctness, therefore the contextual knowledge base is error 
prone) and prior to the U_PN tagging phase: a compound PN may be incompletely 
recognized uring POS tagging, causing the generation of an uninformative syntactic 
context (e.g., "Owens Illinois" at the beginning of a sentence is recognized as "owens 
Illinois", causing a spurious NdN(owen,Illinois) context o be generated). 
Because all these "external" sources of noise are not filtered out, we may then 
reliably conclude that our tagger is effective at improving the robustness of proper 
noun classification, though clearly the amount of improvement depends upon the 
baseline performances of the early method used for PN classification. 
Although the classification evidence provided by syntactic ontexts is somewhat 
noise prone, it proves to be useful as a "backup," when other "simpler" contextual 
evidence does not allow a reliable decision. 
4.2 Effectiveness of Syntactic and Semantic Cues for Semantic Classification 
In a second experiment, we used the experimental set up of Test 2 (WSJ+VIE described 
above) to evaluate the effectiveness of context expansion on system performance. We 
applied a pruning method on WordNet (Cucchiarelli and Velardi 1998) to reduce initial 
ambiguity of contexts. This pruning method allowed an average of 27% reduction in 
the initial ambiguity of the total number of the 13,428 common nouns in the Wall 
Street Journal corpus. The objective of this experiment was to allow a more detailed 
evaluation of our method, with respect o several parameters. 
We built four test sets with the same distribution of PN categories and frequency 
distribution as in the application corpus. We selected four frequency ranges (1, 2, 3-9, 
> 10) and in each range we selected 100 PNs, reflecting the frequency distribution 
in the corpus of the three main PN semantic ategories--Person, Organization, and 
Location. We then built another test set, called TSAll, with 400 PNs again reflecting the 
frequency and category distribution of the corpus. The 400 PNs were then removed 
from the set of 37,018 esls extracted by our parser and from the gazetteer (whenever 
included). 
In this experiment, we wanted to measure the performance of the U_PN tagger 
over the 400 words in the test set, in terms of F-measure, according to several varying 
factors: 
? the category type; 
? the amount of initial contextual evidence (i.e., the frequency range, 
reflected by the different test sets); 
? the factors oe and fl, i.e., the influence of local and generalized contexts; 
? the level of generalization L. 
Figures 2 summarizes the results of the experiment. Figure 2(a) shows the increase 
in performance as a function of the values of oe and fl and the generalization level. N 
means no generalization, only the evidence provided by ESLA is computed; 0 means 
that ESLB collects the evidence provided by contexts in which w is a strict synonym of 
x according to WordNet; 1, 2, and 3 refer to incremental levels of generalization in the 
(pruned) WordNet hierarchy. The figure shows that context generalization produces up 
to 7% improvement in performance. Best results are obtained with L = 2 and ~ = 0.7, 
fl = 0.3. Further generalization may cause a drop in performance. High ambiguity is 
the cause of this behavior, despite WordNet pruning (without WordNet pruning, we 
observed a performance inversion at level 1; this experiment is not reported ue to 
129 
Computational Linguistics Volume 27, Number 1 
hi1% i . . , "  . ,"  "/ /( l=o ?, \]3 0 3 
~% 4 o 3 I~ 41 7 
76% 
9S~,  
I 
N " Leve l  of  Gen~ra l i za t lon  
(a )  
Figure 2 
Evaluation of the effectiveness of context expansion. 
f:2 
t~ 
(b)  
limitations of space). Figure 2(b) illustrates the influence of initial contextual evidence. 
Recognition of singleton PNs remains almost constant as the contribution of gener- 
alized and nongeneralized contexts varies. Looking more in detail, we observe that 
recall increases with fl -- (1 -  c~), but precision decreases. Generalization on the basis of 
a unique context does not allow any filtering of spurious senses, while when grouping 
several contexts, spurious senses gain lower evidence (as anticipated in Section 3). 
Finally, we designed an experiment to evaluate the influence of the test set com- 
position on the U_PN tagger performances. We performed an analysis of variance 
(ANOVA test \[Hoel 1971\]) on the results obtained by processing nine different est 
sets of 400 PNs each, selected randomly. In all our experiments the details of which 
we omit, for lack of space), we found that the U-PN tagging method performances 
were independent of the variations of the test set. 
References 
Agirre, Eneko and German Rigau. 1996. 
Word Sense Disambiguation using 
Conceptual Density. In Proceedings ofthe 
16th International Conference on 
Computational Linguistics (COLING '96), 
Copenhagen, Denmark. 
Basili, Roberto, Alessandro Marziali, and 
Maria Teresa Pazienza. 1994. Modelling 
syntax uncertainty in lexical acquisition 
from texts. Journal of Quantitative 
Linguistics, 1(1). 
Basili, Roberto, Maria Teresa Pazienza, and 
Paola Velardi. 1994. A (not-so) shallow 
parser for collocational nalysis. In 
Proceedings ofthe 15th International 
Conference on Computational Linguistics 
(COLING '94), Kyoto, Japan. 
Brill, Erik and Philip Resnik. 1994. A 
transformation-based approach to 
prepositional phrase attachment 
disambiguation. I  Proceedings ofthe 15th 
International Conference on Computational 
Linguistics (COLING '94), Kyoto, Japan. 
Cucchiarelli, Alessandro, Danilo Luzi, and 
Paola Velardi. 1998. Automatic semantic 
tagging of unknown proper names. In 
COLING-ACL "98: 36th Annual Meeting of 
the Association for Computational Linguistics 
and I7th International Conference on 
Computational Linguistics, Montreal, 
Canada. 
Cucchiarelli, Alessandro and Paola Velardi. 
1998. Finding a domain-appropriate sense 
inventory for semantically tagging a 
corpus. International Journal on Natural 
Language Engineering, December. 
Gale, William, Kenneth Church, and David 
Yarowsky. 1992. One sense per discourse. 
In Proceedings ofthe DARPA Speech and 
Natural Language Workshop. Harriman, NY. 
Grishrnan, Ralph and John Sterling. 1994. 
Generalizing automatically generated 
selectional patterns. Proceedings ofthe 15th 
International Conference on Computational 
Linguistics (COLING "94), Kyoto, Japan. 
Hearst, Marti and Hinrich Schuetze. 1993. 
Customizing a lexicon to better suite a 
computational task. In Proceedings of
ACL-SIGLEX Workshop on Lexical 
Acquisition from Text. Columbus, OH. 
Hoel, Paul Gerhard. 1971. Introduction to 
130 
Cucchiarelli and Velardi Unsupervised Named Entity Recognition 
Mathematical Statistics. John Wiley & Sons 
Inc., New York. 
Humphreys, Kevin, Robert Gaizauskas, 
Hamish Cunningam, and Sheila Azzan. 
1996. Technical Specifications, 1996/10/1815. 
ILASH, University of Sheffield, UK. 
Paliouras, George, Vangelis Karkaletsis, and 
Constantine Spyropolous. 1998. Results 
from the named entity recognition task. In 
Deliverable 3.2.1 of the European project 
ECRAN LE 2110. Available at: http://  
www2.echo.lu/langeng/en/lel/ecran/ 
ecran.html. 
Resnik, Philip. 1997. Selectional reference 
and sense disambiguation. I  Proceedings 
of the ACL Workshop Tagging Text with 
Lexical Semantics: Why, What, and How? 
Washington, DC. 
Velardi, Paola and Alessandro Cucchiarelli. 
2000. A theoretical nalysis of 
contextual-based l arning algorithms for 
word sense disambiguation. I  Proceedings 
of ECA12000, Berlin, Germany. (To 
appear.) 
Yarowsky, David. 1992 Word-sense 
disambiguation using statistical models of 
Roget's categories trained on large 
corpora. In Proceedings ofthe 14th 
International Conference on Computational 
Linguistics (COLING "92), Nantes, France. 
131 
