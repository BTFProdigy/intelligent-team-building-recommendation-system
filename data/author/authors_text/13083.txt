Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 357?368,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Grounding Strategic Conversation:
Using negotiation dialogues to predict trades in a win-lose game
Ana??s Cadilhac
IRIT
Univ. Toulouse, France
cadilhac@irit.fr
Nicholas Asher
IRIT, CNRS
Toulouse, France
asher@irit.fr
Farah Benamara
IRIT
Univ. Toulouse, France
benamara@irit.fr
Alex Lascarides
School of Informatics
Univ. Edinburgh, UK
alex@inf.ed.ac.uk
Abstract
This paper describes a method that predicts
which trades players execute during a win-
lose game. Our method uses data collected
from chat negotiations of the game The Set-
tlers of Catan and exploits the conversation
to construct dynamically a partial model of
each player?s preferences. This in turn yields
equilibrium trading moves via principles from
game theory. We compare our method against
four baselines and show that tracking how
preferences evolve through the dialogue and
reasoning about equilibrium moves are both
crucial to success.
1 Introduction
Rational agents act so as to maximise their expected
utilities?an optimal trade off between what they
prefer and what they believe they can achieve (Sav-
age, 1954). Solving a game problem involves find-
ing equilibrium strategies: an optimal action for
each player that maximises his expected utility, as-
suming that the other players perform their speci-
fied action (Shoham and Leyton-Brown, 2009). Cal-
culating equilibria thus requires knowledge of the
other players? preferences but almost all bargaining
games occur under the handicap of imperfect infor-
mation about this (Osborne and Rubinstein, 1994).
Players therefore try to extract their opponents? pref-
erences from what they say, likewise revealing their
own preferences in their own utterances. These
elicited preferences guide an agent?s decisions, like
choosing to make such and such a bargain with such
and such a person. Tracking preferences through
dialogue is thus crucial for analyzing the agents?
strategic reasoning in real game scenarios.
In this paper, we design a model that maps what
people say in a win-lose game into a prediction of
exactly which players, if any, trade with each other,
and exactly what resources they exchange. We use
both statistics and logic: we use a corpus of nego-
tiation dialogues to learn classifiers that map each
utterance to its speech act and to other acts perti-
nent to bargaining; and we develop a symbolic al-
gorithm that, from the classifiers? output, dynami-
cally constructs a model of each player?s preferences
as the conversation proceeds (for instance, the pref-
erence to receive a certain resource, or to accept a
certain trade). This preference model uses CP-nets
(Boutilier et al, 2004), a representation of prefer-
ences for which algorithms for computing equilib-
rium strategies exist. We adapt those algorithms to
predict the trades executed in the game.
The algorithm for construcing CP-nets uses only
the output of our classifiers, which in turn rely en-
tirely on shallow features in the raw text and robust
parsers. Together they provide an end to end model,
from raw text to a prediction of which trade, if any,
occurred. We evaluate the various components of
this (pipeline) algorithm separately, as well as the
end to end model.
Our study exploits a corpus of negotiation dia-
logues from an online version of the win lose game
The Settlers of Catan. Sections 2 and 3 describe
the corpus and its annotation. Section 4 introduces
our method for constructing the agents? preferences
from the dialogues. We use this in Section 5 to pre-
dict whether a trade is executed as a result of the
357
players? negotiations, and if so we predict who took
part in the trade, and what they exchanged. Our
method shows promising results, beating baselines
that don?t adequately track or reason about prefer-
ences. We compare our model to related work in
Section 6 and point to future work in Section 7.
2 The game
The Settlers of Catan (www.catan.com) is a win-
lose game that involves negotiations over restricted
resources. Each player (three or more) acquires re-
sources (of 5 types: ore, wood, wheat, clay, sheep),
which they use in different combinations to build
roads, settlements and cities, which in turn earns
them points towards winning. The first player to
10 points wins. Players acquire resources in sev-
eral ways, in particular through agreed trades with
other players. Some methods (e.g., robbing) are hid-
den from view, so players lack complete information
about their opponents? resources.
Our corpus contains conversations of humans
playing an online version of Settlers (Afantenos et
al., 2012). Players must converse in a chat inter-
face to carry out trades. Each game contains several
dozen self-contained bargaining dialogues. Our ex-
periments use 10 Settlers games, consisting of more
than 2000 individual dialogue turns (see Section 3).
Table 1 is a sample dialogue from the corpus. The
sentences in the corpus have a relatively simple syn-
tax, though many also exhibit long distance depen-
dencies. However, these conversations are pragmat-
ically complex. They exhibit complex anaphoric de-
pendences (e.g., utterance ID 4 in Table 1). Other
pragmatic inferences, which are dependent on rea-
soning about intentions, speech acts and discourse
structure, are also ubiquitous. For example, the
question Have you got any ore? implies an offer for
the speaker to receive ore in exchange for something
from someone unspecified, and its response I?ve got
wheat not only implies a willingness to exchange
wheat for something, but as a response to the ques-
tion it also implies a refusal to give any ore.
More generally, a dialogue turn in our corpus can
express an offer, a counteroffer, an acceptance or re-
jection of an offer, or a commentary on the above
or on moves in the game. All except the last pro-
vide clues about preferences: e.g., which players
a speaker wants to execute a trade with; or what
resources to exchange. For instance, the utterance
Anybody have any sheep for wheat? conveys sev-
eral preferences. First, it conveys the speaker?s pref-
erence to trade with someone unspecified. Other
informative but underspecified preferences include:
the speaker?s preference to acquire some sheep over
alternatives; and in a context where she receives
sheep, a preference to give away some of her wheat
over the alternatives. Crucially, it does not convey
a preference to give away wheat in a context where
she receives nothing or something other than sheep.
In line with a non-cooperative bargaining game,
the preferences and offers that a speaker reveals are
less specific than an executable trade requires, where
the trading partners and the type of resources offered
and received must all be defined. Such general dia-
logue moves are essentially information seeking?
evidence that humans playing Settlers have imper-
fect information about their opponents? preferences.
In fact, many offers to trade result in no trade be-
ing agreed to and executed. While observed negoti-
ation failure would be puzzling in a bargaining game
with perfect information (Osborne and Rubinstein,
1994), it occurs relatively frequently in Settlers.
3 Annotation
We have a multi-layered dialogue annotation
scheme that includes: (1) a pre-annotation that seg-
ments the dialogue into turns which are further seg-
mented into Elementary Discourse Units (EDUs)
with the author of each turn automatically given;
(2) a characterization of each EDU in terms of ba-
sic speech acts (assertion, question, request) as well
as dialogue acts that are specific to bargaining (of-
fers, counteroffers, etc.); and (3) associated infor-
mation about the givable and/or receivable resources
that EDUs express.
Two annotators received training on 77 dialogues,
totaling 699 EDUs. They then both annotated the
remaining dialogues independently (2741 EDUs and
511 dialogues in total). Kappas for inter-annotator
agreement are given below.
3.1 Dialogue act annotation (Kappa=0.79)
Each turn logs what a player enters in the chat win-
dow and also aspects of the game state at the time:
358
ID Dialogue Act Text Speaker Addressee Resource
1 Offer i need clay, any1 have? Rainbow All Receivable (clay, ?)
2 Refusal Nope, sorry inca Rainbow
3 Refusal Not at the moment, unfortunately. ariachiba Rainbow
4 Refusal need mine sorry Kittles Rainbow Not givable (Anaphoric, ?)
Anaphora Link:(mine , clay )
5 Offer no one has ore to giv? Rainbow All Receivable (ore, ?)
6 Accept oh yeah me Kittles Rainbow
7 Counteroffer ore for wheat again? Kittles Rainbow Givable (ore, ?) Receivable (wheat, ?)
8 Accept ya Rainbow Kittles
9 Accept ok Kittles Rainbow
Table 1: Example of an annotated negotiation dialogue.
his resources, the state of the game board and a time
stamp. The pre-annotation divides each turn into
EDUs. The annotators then have to specify the dia-
logue act of each EDU: Offer, Counteroffer, Accept
or Refusal (of an offer addressed to the emitter), and
Other. Other labels units that either comment on
strategic moves in the game or are not directly perti-
nent to bargaining. Annotators also specify the ad-
dressee of the EDU and its surface type: Question,
Request or Assertion.
3.2 Resource type annotation (Kappa=0.80)
Annotators also specify for each EDU and its dia-
logue act an associated feature structure, which cap-
tures (partial) information that the EDU expresses
about the type and quantity of resources that are of
the following four attributes: Givable, Not Givable,
Receivable or Not Receivable. These attributes can
take Boolean combinations of resources as values
via two operators AND and OR, that respectively
stand for conjunction (the agent expresses two pref-
erences and he prefers to achieve one of them if he
cannot have both, such as I need clay and wood) and
disjunction (free choice) of preferences (e.g., I can
give you clay or wood). We allow attributes to have
unknown values: the annotation tool inserts a ? in
these cases. We also insist that the annotators re-
solve anaphoric dependencies when specifying val-
ues to attributes, as shown in EDU (4) in Table 1.
4 Dialogue act and resource prediction
Predicting the executed trades from the dialogues
starts with three sub-tasks: automatically identify-
ing each EDU?s dialogue act; detecting the EDU?s
resources; and specifying the attributes of those re-
sources (i.e., Givable, Receivable, etc.).
4.1 Identifying dialogue acts
As is well established, one EDU?s dialogue act
depends on previous dialogue acts (Stolcke et al,
2000). In our corpus, Accept or Reject frequently
follow Offer and Counteroffer. Since labeling is se-
quential, we use Conditional Random Fields (CRFs)
to learn dialogue acts. CRFs have been shown to
yield better results in dialogue act classification on
online chat than HMM-SVN and Naive Bayes (Kim
et al, 2012).
We use three types of features: lexical, syntactic
and semantic. And we exploit them as unigrams and
bigrams: unigrams associate the value of the feature
with the current output class (level 0); bigrams take
account of the value of the feature associated with
a combination of the current output class and pre-
vious output class (level -1). 6 features were used
exclusively as unigrams: the EDU?s position in the
dialogue, its first and last words, its subject lemma,
a boolean feature to indicate if the current speaker is
the one that initiates the dialogue and the position of
the speaker?s first turn in the dialogue.
We have 15 unigram and bigram features (at lev-
els 0 and -1), as well as templates that combine
feature values for the two levels. These include
14 boolean features that indicate if the EDU con-
tains: bargaining verbs (e.g. trade, offer), refer-
ences to another player (e.g. you), resource tokens
as encoded in a task dedicated lexicon (e.g. wheat,
clay), quantifiers (e.g. one, none), anaphoric pro-
nouns, occurrences of ?for? prepositional phrases
(e.g. wheat for clay), acceptance words (e.g. OK),
negation words, emoticons, opinion words (from
(Benamara et al, 2011)), words of politeness, ex-
clamation marks, questions, and finally whether the
EDU?s speaker has talked previously in the dialogue.
359
The last feature gives the EDU speaker lemma. In
addition, 3 unigram and bigram booleans indicate
whether the current EDU contains the most frequent
tokens, couple of tokens and syntactic patterns in our
corpus. Finally, we use 2 composed bigram features
that encode whether the EDU contains an accep-
tance or refusal word, given that the previous EDU
is a question.
To assign sequential tags of dialogue acts within
a negotiation dialogue, we use the CRF++ tool
(crfpp.googlecode.com). Our data consists of
2741 EDUs in 511 dialogues. Each EDU is asso-
ciated with a dialogue act resulting in 410 Offer,
197 Counteroffer, 179 Accept, 398 Refusal and 1557
Other. We use 10-fold cross-validation to evalu-
ate our model, computing precision, recall and F-
score for each class and global accuracy from the
total number of true positives, false positives, false
negatives and true negatives obtained by summing
over all fold decisions. The results (in percent) are
given in Table 2 (MaF is the average of F-scores
of all the classes). Our model significantly out-
performs the frequency-based baseline (MaF=14.5;
Accuracy=56.8), with the best F-score achieved for
Other. The least good results are for the two least
frequent classes in our data. In addition to the fre-
quency problem, the lower score for Counteroffer is
mainly due to the model confusing it with Offer. Er-
rors in the Accept class were often due to misspelling
or to chat style conversation; e.g., kk, yup.
Dialogue act Precision Recall F-score
Other 87.4 93.1 90.1
Offer 80.0 81.0 80.5
Counterof. 64.8 53.3 58.5
Accept 65.1 53.1 58.5
Refusal 81.7 73.9 77.6
Macro-averaged F-score (MaF) 73.0
Accuracy 83.0
Table 2: Results for dialogue act classification.
4.2 Finding resource text spans
Since the resource vocabulary in The Settlers of
Catan is a closed set composed of words denoting
specific resources (e.g., clay, wood) and their syn-
onyms (brick), we use a simple rule to detect them:
a noun phrase (NP) is a resource text span if and
only if it contains a lemma from our resource lexi-
con. A closed set resource vocabulary is common to
many different types of negotiation dialogues. We
used the Stanford parser (Klein and Manning, 2003)
to obtain the NPs: there are 4361 NPs, where (by the
gold standard annotations) 21% are resources and
79% are not. We obtain an F-score of 96.9% and ac-
curacy of 97.9%, clearly beating both the frequency
and random baselines for this task.
4.3 Recognizing the type of resources
Recall that each resource within an EDU can be the
value of four types of attributes: Givable, Receiv-
able, Not Givable or Not Receivable (cf. Section
3.2). We predict these attributes using CRFs with the
following features. 8 features are used as unigram at
the current and the previous EDU level: the speaker,
the EDU?s subject, the dialogue act, and (if present)
the lemma of a bargaining verb, and 4 boolean fea-
tures indicate if the EDU contains an opinion word,
a reference to another speaker, if the resource comes
after a ?for? and if it contains a refusal word. These
features also serve as bigrams at the current EDU
level. Additionally, we have a set of unigram and
bigram boolean features that indicate if the current
EDU contains the most frequent verbs in the corpus.
And finally, we use a feature that encodes the com-
bination subject/bargaining verb in the current EDU.
We used CRF++ to implement our classifier. Our
corpus data consists of 1077 Resources, split into
510 Receivable, 432 Givable, 116 Not Givable and
19 Not Receivable. We use again 10-fold cross-
validation to evaluate our model and compute the
results by summing over all fold decisions. We
present them (in percent) in Table 3. They beat
the frequency-based baseline (MaF=16.1; Accu-
racy=47.4), although performance on the Not Re-
ceivable class is poor probably due to its low fre-
quency in the data.
Ambiguities make this task challenging. For in-
stance, anyone wheat for clay? can mean that the
speaker wants to receive wheat and give clay or the
opposite, and resolving which meaning is intended
involves reasoning not only with the previous and/or
the following EDU, but also sometimes EDUs with
long distance attachments, which are not supported
by our classifier and require a full discourse parser.
360
Res. type Precision Recall F-score
Receivable 66.8 71.4 69.0
Givable 62.6 59.7 61.1
Not Giv. 88.1 89.7 88.9
Not Rec. 0 0 0
Macro-averaged F-score (MaF) 54.8
Accuracy 67.4
Table 3: Results for resource type classification.
5 Predicting Players? Strategic Actions
We aim to capture the evolution of commitments to
certain preferences as the dialogue proceeds so as
to predict the agents? bargaining behavior. In other
words, we wish to predict which of the 61 possi-
ble trade actions is executed at the end of each dia-
logue. The possible trades vary over which partner
the player whose turn it is trades with (3 options in a
4 player game), the resources exchanged (assuming
each partner gives one type of resource and receives
another type yields 5?4 = 20 possibilities), or there
is no trade; i.e., (3 ? 20) + 1 = 61 possible actions
in the hypothesis space (we predict the types of re-
sources that are exchanged, but not their quantity).
We predict the executed action by identifying the
equilibrium trade entailed by the model of the play-
ers? preferences, which in turn we construct dynam-
ically from the output of the classifiers in Section 4.
We use the attributes of resources in the EDUs (Giv-
able, etc.) to identify the preference that a speaker
conveys in the EDU, and we use the dialogue acts
(Offer, Accept, etc.) to update a model of the pref-
erences expressed so far in the dialogue with this
new preference (see Section 5.2). Our model of
preferences consists of a set of partial CP-nets, one
for each player (see Section 5.1 for details). The
resulting CP-nets are then used to infer the exe-
cuted trading action (if any) automatically, via well-
understood principles from game theory for identi-
fying rational behavior (Bonzon, 2007).
5.1 CP-Nets
Following Cadilhac et al (2011), we use CP-nets
(Boutilier et al, 2004) to model preferences and
their dependencies. CP-nets are compatible with the
kind of partial information about preferences that ut-
terances reveal, and inference with CP-nets is com-
putationally efficient.
Just as Bayesian nets are a graphical model that
exploits probabilistic conditional independence to
provide a compact representation of a joint probabil-
ity distribution (Pearl, 1988), CP-nets are a graphi-
cal model that exploits conditional preferential in-
dependence to provide a compact representation of
the preference order over all outcomes. The CP-
net structures the decision maker?s preferences un-
der a ceteris paribus assumption: outcomes are com-
pared, other things being equal.
More formally, let V be a finite set of variables
whose combination of values determine all out-
comes O. Then a preference relation  over O is
a reflexive and transitive binary relation with strict
preference  defined as: o  o? and o? 6 o. Indif-
ference, written o ? o?, means o  o? and o?  o.
Definition 1 defines conditional preference indepen-
dence and Definition 2 defines CP-nets: the graphi-
cal component G of a CP-net specifies for each vari-
able X ? V its parent variables Pa(X) that affect
the agent?s preferences over the values of X , such
thatX is conditionally preferentially independent of
V \ ({X} ? Pa(X)) given Pa(X).
Definition 1 Let V be a set of variables, each vari-
able Xi with a domain D(Xi). Let {X,Y, Z} be
a partition of V . X is conditionally preferentially
independent of Y givenZ if and only if ?z ? D(Z),
?x1, x2 ? D(X) and ?y1, y2 ? D(Y ), x1y1z 
x2y1z iff x1y2z  x2y2z.
Definition 2 NV = ?G, T ? is a CP-net on variables
V , where G is a directed graph over V , and T is a
set of Conditional Preference Tables (CPTs). That
is, T = {CPT(Xj): Xj ? V }, where CPT(Xj)
specifies for each combination p of values of the par-
ent variables Pa(Xj) either p : xj  xj , p : xj 
xj or p : xj ? xj where the ?? symbol sets the vari-
able to false.
We discuss below how a CP-net predicts rational
action, but first we describe how CP-nets are con-
structed from the dialogues. In the Settlers cor-
pus, preferences involve a quadruplet (o, a, <r,q>)
where: o is the preference owner, a is the ad-
dressee, r is the resource and q is its quantity. So
each variable in the CP-nets we construct is such a
quadruplet, and for each variable the possibles val-
ues are Givable (Giv), Not Givable (Giv), Receiv-
361
able (Rcv) and Not Receivable (Rcv).
For example, the utterance Anyone want to give
me a wheat for a clay? expresses two prefer-
ences: one for receiving wheat, represented by the
variable Pw = (A,All,<wheat,1>); and given this
preference, another for giving clay, represented by
Pc = (A,All,<clay,1>) (where A is the name of the
speaker). The corresponding CP-Net is Figure 1.
Pw
Pc
CPT(Pw) = Rcv  Rcv
CPT(Pc) = Rcv Pw : Giv  Giv
Figure 1: An example CP-net
5.2 Modeling players? preferences
As stated above, we first automatically acquire a CP-
net from each EDU by using the EDU?s dialogue act
and the attributes (Givable, etc.) of its resources.
We then apply the rules presented in (Cadilhac et al,
2011) to dynamically construct a preference model
of the dialogue overall: this uses an equivalence
between their coherence relations and our dialogue
acts. Our CP-nets reasoning model handles uncer-
tain information and noise because it use as input
only the outputs of the statistical models described
in Section 4, and these prior models handle uncer-
tain information and noise. The symbolic rules for
constructing CP-nets have complete coverage over
any possible combination of classes that are output
by the statistical models, and so they are robust. We
give our rules below where pii stands for EDU ID i.
Offers. Because an Offer may specify or refine an
existing preference or offer, we must model how the
preferences expressed in an EDU that?s an Offer up-
dates the prior declared preferences. So, while our
annotations treat Offer as a property of EDUs, we
treat them here as binary relations: Offer(pi1, pi2),
where the second term, pi2, is the actual EDU whose
dialogue act is Offer and pi1 is the set of EDUs oc-
curring between pi2 and the last EDU uttered by
the same speaker. Offers then have a similar effect
on the CP-net as the coherence relation Elaboration
presented in (Cadilhac et al, 2011). That is, to auto-
matically update the CP-net constructed so far with
a current EDU that?s an Offer, the two step rule for
Offer(pi1, pi2) is:
1. to update the speaker?s CP-net according to the
preferences expressed in pi1, and
2. if pi2 expresses preferences, to enrich the CP-
net with these new preferences so that each
variable in pi2 depends on each variable in pi1.
Counteroffers. They specify or modify the terms
of a previous Offer or Counteroffer. Their purpose
is to give new information to refine the negotiation.
Like Offers they must also receive a contextually de-
pendent interpretation. The rule is quite similar to
that for Offer; however, Counteroffer can modify or
correct elements in a previously introduced offer. So
for Counteroffer(pi1, pi2), the rule is :
1. to partially update the speaker?s CP-net accord-
ing to the preferences expressed in pi1 which do
not have the same resource type (Givable, Re-
ceivable) than the ones in pi2.
2. same as step 2 Offer rule.
Accepts and Refusals. As they are answers to
Offers and Counteroffers, they behave like question
answer pairs (QAPs) presented in (Cadilhac et al,
2011). Because we are not doing full discourse pars-
ing, we once again approximate its effects by mak-
ing Accepts and Refusals respond to the set of EDUs
between the current EDU and the speaker?s last turn.
Accepts are positive responses to Offers or Coun-
teroffers and are de facto similar to QAP(pi1, pi2)
where pi2 is Yes. Thus, the rule is, as for Offer, to
update and enrich the CP-net.
Refusals are instead negative responses and be-
have like QAP(pi1, pi2) where pi2 is No. For
Refusal(pi1, pi2), there is no update of the prefer-
ences expressed in pi1. Instead, we enrich the CP-net
with the Non Givable and Non Receivable informa-
tion obtained from the negation of the preferences
expressed in the previous Offer or Counteroffer. We
then enrich the CP-net based on any new preferences
expressed in pi2. If there is a conflict between the
value of a variable to be updated and the current
value in the CP-net, we apply the Correction rule:
all occurrences of the old value are replaced by the
new value in pi2.
Other. This category pertains to content that does
not directly relate to trading in the game, and so we
choose to ignore resources expressed in the EDUs
with this dialogue act.
At the end of the negotiation dialogue, to predict
exactly what trade is executed (if any), the method
362
checks if there are complete and reciprocal prefer-
ences expressed in the CP-nets that respectively rep-
resent the declared preferences of two agents A and
B. This is done in two steps. First, we use the logic
of CP-nets to determine each agent?s best outcome
bestOA and bestOB from their respective CP-nets
(we?ll discuss how shortly). Secondly, we compare
these best outcomes: if they correspond to the same
trade, we predict that this trade was executed; if
not, we predict no trade is executed. Specifically,
bestOA (resp. bestOB) corresponds to a prefer-
ence for receiving a resource r1 from an agent B
(or from all the agents indifferently) and for giving
a resource r2 to this (or these) agent(s). We predict
that A gives B r2 and B gives A r1 if and only if:
bestOA = Rcv(A, B, r1) ? Giv(A, B, r2) and
bestOB = Rcv(B, A, r2) ?Giv(B, A, r1).
The first step?computing each agent?s best out-
come from his CP-net?can be found in linear time
using the forward sweep algorithm (Boutilier et al,
2004): sweep through the CP-net?s graph from top to
bottom, instantiating each variable with its preferred
value, given the values that are (already) assigned to
its parents. This algorithm is sound with respect to
the semantics of CP-nets.
Example. We apply this method for constructing
CP-nets and determining the executed trade to the
negotiation dialogue presented in Table 1.
pi1 The EDU is an Offer, so Rainbow?s CP-net is
updated according to pi1?s content.
CPT(R,All,<clay,?>) = Rcv  Rcv
pi2 It?s a Refusal, so we update inca?s CP-net with
the negation of the preferences expressed in Rain-
bow?s offer.
CPT(I,R,<clay,?>) = Giv  Giv
pi3 Idem for ariachiba.
CPT(A,R,<clay,?>) = Giv  Giv
pi4 Idem for Kittles where the preferences ex-
pressed in this EDU are redundant with the negation
of the preferences in Rainbow?s offer.
CPT(K,R,<clay,?>) = Giv  Giv
pi5 It?s an Offer, so Rainbow?s CP-net is first up-
dated according to previous EDUs (pi2 to pi4 until his
last speaking), then according to the content of pi5.
CPT(R,All,<clay,?>) = Rcv  Rcv (inactive)
CPT(R,I,<clay,?>) = Rcv  Rcv
CPT(R,A,<clay,?>) = Rcv  Rcv
CPT(R,K,<clay,?>) = Rcv  Rcv
CPT(R,All,<ore,?>) = Rcv(R,I,<clay,?>) ? Rcv(R,A,
<clay,?>) ? Rcv(R,K,<clay,?>): Rcv  Rcv
The introduction of the preference to receive ore
conflicts with the prior one for receiving clay. So
the method adds to the associated CPT the label ?in-
active? to indicate that this is older and should be
ignored if the preference about ore is satisfied.
pi6 The EDU is an Accept, so Kittles?s CP-net is
updated according to previous EDUs (only pi5).1
CPT(K,R,<ore,?>) = Giv(K,R,<clay,?>): Giv  Giv
pi7 The EDU is a Counteroffer. Since she is the
last speaker, her CP-net gets updated only according
to the content of the current EDU, to obtain:
CPT(K,R,<ore,?>) = Giv(K,R,<clay,?>): Giv  Giv
CPT(K,R,<wheat,?>) = Giv(K,R,<clay,?>) ?
Giv(K,R, <ore,?>) : Rcv  Rcv
pi8 The EDU is an Accept, so Rainbow?s CP-net
is updated according to previous EDUs (pi6 and pi7):
CPT(R,K,<ore,?>) = Rcv(R,I,<clay,?>) ? Rcv(R,A,
<clay,?>) ? Rcv(R,K,<clay,?>) : Rcv  Rcv
CPT(R,K,<wheat,?>) =Rcv(R,I,<clay,?>) ?Rcv(R,A,
<clay,?>) ? Rcv(R,K,<clay,?>) ? Rcv(R,K,<ore,?>) :
Giv  Giv
pi9 It?s an Accept with nothing new to update.
At the end of the dialogue, these agents? CP-nets
(correctly) predict that Kittles gave ore to Rainbow
in exchange for wheat.
5.3 Evaluation and results
We compare our model against four baselines. Since
none of these baselines support reasoning about
equilibrium moves, they all rely on the presence of
an Accept act to predict there was a trade, and its
absence to predict there wasn?t. The baselines dif-
fer, however, in how they identify the trading part-
ners and resources in an executed trade. The first
baseline predicts a trade according to the first Of-
fer and the last person to Accept, and if the Offer
doesn?t specify one of the resources then it is chosen
randomly (similar random choices complete all par-
tial predictions in all the models we consider here):
e.g., for Table 1 this would predict that Kittles gave
clay to Rainbow (which is incorrect) in exchange for
1Due to lack of space, in the following CP-nets, we do not
copy the inactive CPTs and CPTs about Not Givable or Not Re-
ceivable resources.
363
something that?s chosen randomly (which will prob-
ably be incorrect). The second baseline uses the
last Offer and the last person to Accept: e.g., for
Table 1 this predicts that Kittles gave ore to Rain-
bow (correct) for something random (probably in-
correct). The third baseline uses the last Offer or
Counteroffer, whichever is latest, and the last per-
son to Accept: e.g., for Table 1 this correctly pre-
dicts that Kittles gave ore to Rainbow in exchange
for wheat. And the fourth baseline, uses default
unification between the prior Offers or Counterof-
fers and the current one to resolve any of the cur-
rent offer?s elided parts and to replace specific val-
ues in prior offers with conflicting specific values in
the current offer (Ehlen and Johnston, 2013). One
then takes the executed trade to be the result of this
unification process at the point where the last Accept
occurs. This makes the same predictions as the third
baseline for Table 1, but outperforms it in the corpus
example (1) by predicting the correct and complete
trade (i.e., Rainbow gave Kittles sheep for wheat,
rather than for something random):
(1) Rainbow: i need clay ore or wheat
Kittles: i got wheat
Rainbow: i cn giv sheep
Kittles: ok
We performed the evaluation on the data pre-
sented in Sections 3 and 4: 254 dialogues in total
since we ignore dialogues that contain only Others.
90 of these dialogues end with a trade being exe-
cuted and 2 of them end with 2 trades. A random
baseline would give 1.6% accuracy (given the 61
possible trading actions) and a frequency baseline
(always choose no trade) gives 64.1% accuracy.
Table 4 presents the accuracy figures for all the
models when calculated from the gold standard la-
bels rather than the classifiers? predicted labels from
Section 4, so that we can compare the models in
isolation of the classifiers? errors. McNemar?s test
shows that our model significantly outperforms all
the baselines (p < 0.05). A predicted trade counts as
correct only if it specifies the right participants and
the correct type of resources offered and received
(we ignore their quantity). True Positives (TP) are
thus examples where the model correctly predicts
not only that a trade happened, but also the correct
partners and resources; Wrong Positives (WP), on
the other hand, constitute a correct prediction that
there was a trade but errors on the partners and/or
resources involved (so WPs undermine accuracy).
True Negatives (TN) are examples where the model
correctly predicts there was no trade (so TPs and
TNs contribute to accuracy). False Positives (FP)
and False Negatives (FN) are respectively incorrect
predictions that there was a trade, or that there was
no trade.
While Table 4 does not reflect this, the first three
baselines tend to predict incomplete information
about the trade even when what they do predict is
correct: that is, they predict the correct addressee
and the owner but resort to random choice for a re-
source that?s missing from the Offer or Counterof-
fer that predicts which trade occurred. For the first
baseline 34 examples are like this; for the second
and third baselines it?s 32. In contrast, this prob-
lem occurs only once with the fourth baseline, and
all the trades predicted by our method are complete,
making random choice unnecessary. Moreover, the
first three baselines often make incorrect predictions
about the addressee or resources exchanged because
in contrast to our model and the fourth baseline, they
don?t track how potential trades evolve through a se-
quence of offers and counteroffers.
Even though the fourth baseline, which uses de-
fault unification to track the content of the current
offer, is smart and gives good results, it has statis-
tically significant lower accuracy than our model.
One major problem with the fourth baseline is that,
in contrast to our model, it does not track each
player?s attitude towards the current offer. Instead,
like all our baselines, it relies on the presence of an
Accept act to predict that there?s a trade.2 But sev-
eral corpus examples are like (2), in which a trade
is executed but there?s no Accept act, thus yielding a
False Negative (FN) for all four baselines:
(2) Joel: anyone have sheep or wheat
Cardlinger: neither :(
Joel: will give clay or ore
Euan: not just now
Jon: got a wheat for a clay
(Joel gives clay to Jon and receives wheat)
2We tried a baseline that doesn?t rely on the presence of an
Accept act, but rather predicts a trade whenever default unifica-
tion yields a complete offer. It performed worse than the fourth
baseline.
364
So overall, our analysis shows that using CP-nets
significantly outperforms all baselines that don?t
model how preferences evolve in the dialogue, and
error analysis yields evidence that our model outper-
forms the fourth baseline because our model sup-
ports reasoning about player preferences, rational
behavior and equilibrium strategies.
1st baseline: first Offer/last Accept
TP FP FN TN WP Accuracy
24 14 30 150 38 68.0
2nd baseline: last Offer/last Accept
TP FP FN TN WP Accuracy
29 6 32 158 31 73.0
3rd baseline: last (Counter)Offer/last Accept
TP FP FN TN WP Accuracy
39 4 23 160 30 77.7
4th baseline: default unification
TP FP FN TN WP Accuracy
64 4 23 160 5 87.5
Our method
TP FP FN TN WP Accuracy
75 4 15 160 2 91.8
Table 4: Results for trade prediction. TP, FP, FN, TN
and WP are the True and False Positives, False and True
Negatives and Wrong Positives.
Table 5 presents the results for the end to end
evaluation, where trade predictions are made from
the classifiers? output from Section 4 rather than the
gold standard labels. As expected, performance de-
creases due to the classifiers? errors, mainly on the
type of resources (Givable, etc.). But our method
still significantly outperforms all the baselines with
an accuracy of 73.4% when the baselines obtain val-
ues between 60.9% and 68.4%.
4th baseline: default unification
TP FP FN TN WP Accuracy
23 12 37 152 32 68.4
Our method
TP FP FN TN WP Accuracy
34 10 43 154 15 73.4
Table 5: Results for the end to end trade prediction.
6 Related Work
6.1 Dialogue act modeling
Most work on dialogue act modeling focuses on spo-
ken dialogue (Stolcke et al, 2000; Ferna?ndez et al,
2005; Keizer et al, 2002). But live chats introduce
specific complications (Kim et al, 2012): ill-formed
data, abbreviations and acronyms, emotional indi-
cators and entanglement (especially for multi-party
chat). Among related work in this emerging field,
Joty et al (2011) use unsupervised learning to model
dialogue acts in Twitter, Ivanovic (2008) and Kim et
al. (2010) analyze one-to-one online chat in a cus-
tomer service domain, and Wu et al (2002) and Kim
et al (2012) predict dialogue acts in a multi-party
setting. We used a similar classifier to predict dia-
logue acts as the one reported in (Kim et al, 2012)
and evaluation yields similar results.
This paper proposes an approach to dialogue act
identification in online chat that aims to predict
strategic actions like bargaining. Compared to (Sid-
ner, 1994) and DAMSL (Core and Allen, 1997), our
domain level annotation is much more detailed: we
not only predict moves like Accept but also features
like the Givable and Receivable resources. Our gen-
eral speech act typology of EDUs lacks intentional
descriptions of speech acts, however. This reflects
a conscious choice to specify the semantics of each
act purely by the public commitments made to offer
or to receive goods.
6.2 Preference extraction
While preference extraction from non-linguistic ac-
tions is well studied (Chen and Pu, 2004; Fu?rnkranz
and Hu?llermeier, 2011), their extraction from spon-
taneous conversation has received little attention. To
our knowledge, the only existing work is (Asher
et al, 2010; Cadilhac et al, 2011; Cadilhac et al,
2012) which we build on. Cadilhac et al (2011)
compute CP-nets from coherence relations, found in
the annotation of the Verbmobil corpus (Baldridge
and Lascarides, 2005). Here we adapt their algo-
rithm from coherence relations to unary dialogue
acts. Further, while they assume that preferences are
given, here we apply versions of the NLP techniques
from Cadilhac et al(2012) to estimate the prefer-
ences of EDUs automatically. And we go further
than any of these works by using the elicited pref-
365
erences to infer the domain-level actions that result
from information exchanged in the conversation.
In this respect, our work relates to models for
grounding language, where semantic parsing tech-
niques are used to automatically map linguistic in-
structions to domain-level actions (Artzi and Zettle-
moyer, 2013; Kim and Mooney, 2013). Our do-
main of application is more challenging, however:
to our knowledge, this is the first attempt to map
non-cooperative dialogues into predictions about
domain-level actions. We can tackle these strategic
scenarios because we exploit a logic of preferences
as part of our model, yielding inferences about ratio-
nal action even when agents? preferences conflict.
Compared to previous work, our task is new. Our
aim is not to predict what dialogue act to perform
next, but what non verbal action should be per-
formed, mapping dialogue acts to non verbal ac-
tions. The difference between our work and other
work on grounding is that we are grounding non-
cooperative dialogue rather than instructions in a co-
operative setting. There is no prior work of which
we?re aware that maps a non-cooperative dialogue
into a prediction about which joint non-verbal ac-
tion the agents will do as a result of what they?ve
learned about their opponent through conversation.
Furthermore, both the CP-net and the fourth base-
line, whose accuracy is quite high (making it a hard
baseline to beat), use the dialogue history as they in-
crementally build up the preference model.
6.3 Predicting strategic actions
Modeling player behavior in real-time strategy
games is a growing research area in AI. These mod-
els can be used to identify common strategic states,
discover new strategies as they emerge or predict
an opponents future actions and so help players to
optimize their choices. For example, Schadd et
al. (2007) develop a hierarchical opponent model in
the game Spring, Dereszynski et al (2011) reason
about strategic behavior in StarCraft using hidden
Markov models and Amato and Shani (2010) use re-
inforcement learning to acquire a policy for switch-
ing among high-level strategies in Civilization IV.
In comparison, we propose a novel approach for
predicting strategic action based on the symbolically
formalized preferences that each agent commits to in
spontaneous conversation. Our approach thus deals
with imperfect information by exploiting the agents?
declared preferences. By predicting what bargain (if
any) will take place, we are able to verify the cor-
rectness of our preference descriptions. Our task is
a subtask of learning a strategy over an entire game
space, but our approach yields good predictive re-
sults on relatively little data?an advantage of ex-
ploiting CP-nets and the symbolic rules that guide
their evolution from observable evidence.
7 Conclusion
We have proposed a linguistic approach to strategy
prediction in spontaneous conversation, exploiting
dialogue acts to build a partial model of the agents?
declared preferences. Our method tracks how pref-
erences evolve during the dialogue, which we use to
infer their bargaining behavior, i.e. what resources,
if any, are exchanged, and by whom.
We based our study on a corpus collected using an
online version of The Settlers of Catan. Negotiations
in this game mirror complex real life negotiations
and provide a fruitful arena to study strategic con-
versation. Evaluation shows that our approach pro-
vides more accurate and complete information about
trades than baselines that don?t track how an offer
evolves through the dialogue, and we also argued
that game-theoretic reasoning about rational behav-
ior has advantages over relying on the presence or
absence of an Accept act to make predictions.
Our approach, however, does not exploit dis-
course structure, which is needed to properly handle
long distance dependencies of offers on prior mate-
rial. We will exploit this in future work to improve
our results. We also plan to investigate other aspects
of strategic reasoning on a larger dataset.
We have proposed a method that relies on a typol-
ogy of dialogue acts that is domain sensitive. How-
ever, in other work we have shown how to adapt
our algorithms to several domains (Cadilhac et al,
2012). In future work, we plan to link our prefer-
ence extraction algorithms to an automatically ac-
quired discourse structure for a given text. This will
provide a domain independent means for extracting
preferences from dialogue.
Acknowledgments
This work is supported by ERC grant 269427 STAC.
366
References
Stergos Afantenos, Nicholas Asher, Farah Benamara,
Ana??s Cadilhac, Ce?dric De?gremont, Pascal Denis,
Markus Guhe, Simon Keizer, Alex Lascarides, Oliver
Lemon, Philippe Muller, Soumya Paul, Verena Rieser,
and Laure Vieu. 2012. Developing a corpus of strate-
gic conversation in the settlers of catan. In Pro-
ceedings of the 1st Workshop on Games and NLP
(GAMNLP-12).
Christopher Amato and Guy Shani. 2010. High-
level reinforcement learning in strategy games. In
Proceedings of the 9th International Conference on
Autonomous Agents and Multiagent Systems (AA-
MAS?10), pages 75?82.
Yoav Artzi and Luke Zettlemoyer. 2013. Weakly su-
pervised learning of semantic parsers for mapping in-
structions to actions. Transactions of the Association
for Computational Linguistics, 1:49?62.
Nicholas Asher, Elise Bonzon, and Alex Lascarides.
2010. Extracting and modelling preferences from dia-
logue. In IPMU, pages 542?553.
Jason Baldridge and Alex Lascarides. 2005. Annotating
discourse structures for robust semantic interpretation.
In Proceedings of the 6th IWCS.
Farah Benamara, Baptiste Chardon, Yannick Mathieu,
and Vladimir Popescu. 2011. Towards context-based
subjectivity analysis. In Proceedings of 5th Interna-
tional Joint Conference on Natural Language Process-
ing, pages 1180?1188, Chiang Mai, Thailand.
Elise Bonzon. 2007. Mode?lisation des interactions en-
tre agents rationnels : les jeux boole?ens. PhD thesis,
Universite? Paul Sabatier, Toulouse.
Craig Boutilier, Craig Brafman, Carmel Domshlak, Hol-
ger H. Hoos, and David Poole. 2004. Cp-nets: A tool
for representing and reasoning with conditional ceteris
paribus preference statements. Journal of Artificial In-
telligence Research, 21:135?191.
Ana??s Cadilhac, Nicholas Asher, Farah Benamara, and
Alex Lascarides. 2011. Commitments to preferences
in dialogue. In Proceedings of SIGDIAL, pages 204?
215. ACL.
Ana??s Cadilhac, Nicholas Asher, Farah Benamara,
Vladimir Popescu, and Mohamadou Seck. 2012. Pref-
erence extraction from negotiation dialogues. In Eu-
ropean Conference on Artificial Intelligence (ECAI),
pages 211?216. IOS Press.
Li Chen and Pearl Pu. 2004. Survey of preference elici-
tation methods. Technical report.
Mark G. Core and James F. Allen. 1997. Coding di-
alogs with the DAMSL annotation scheme. In Work-
ing Notes of the AAAI Fall Symposium on Communica-
tive Action in Humans and Machines.
Ethan W. Dereszynski, Jesse Hostetler, Alan Fern,
Thomas G. Dietterich, Thao-Trang Hoang, and Mark
Udarbe. 2011. Learning probabilistic behavior mod-
els in real-time strategy games. In AIIDE.
Patrick Ehlen and Michael Johnston. 2013. A multi-
modal dialogue interface for mobile local search. In
IUI Companion, pages 63?64.
Raquel Ferna?ndez, Jonathan Ginzburg, and Shalom Lap-
pin. 2005. Using machine learning for non-sentential
utterance classification. In Proceedings of the 6th SIG-
dial Workshop on Discourse and Dialogue, pages 77?
86.
Johannes Fu?rnkranz and Eyke Hu?llermeier, editors.
2011. Preference Learning. Springer.
Edward Ivanovic. 2008. Automatic instant messaging
dialogue using statistical models and dialogue acts. In
Masters thesis, The University of Melbourne.
Shafiq R. Joty, Giuseppe Carenini, and Chin-Yew Lin.
2011. Unsupervised modeling of dialog acts in asyn-
chronous conversations. In Proceedings of the 22nd
International Joint Conference on Artificial Intelli-
gence, pages 1807?1813.
Simon Keizer, Rieks op den Akker, and Anton Nijholt.
2002. Dialogue act recognition with bayesian net-
works for dutch dialogues. In Proceedings of the 3rd
SIGdial Workshop on Discourse and Dialogue, pages
88?94. Association for Computational Linguistics.
Joohyun Kim and Raymond J. Mooney. 2013. Adapting
discriminative reranking to grounded language learn-
ing. In Proceedings of the 51st Annual Meeting of
the Association for Computational Linguistics (ACL-
2013), Sofia, Bulgaria.
Su Nam Kim, Lawrence Cavedon, and Timothy Baldwin.
2010. Classifying dialogue acts in 1-to-1 live chats.
In Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, pages 862?
871.
Su Nam Kim, Lawrence Cavedon, and Timothy Bald-
win. 2012. Classifying dialogue acts in multi-party
live chats. In 26th Pacific Asia Conference on Lan-
guage,Information and Computation, pages 463?472.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In Proceedings of the 41st
Meeting of the Association for Computational Linguis-
tics, pages 423?430.
Martin Osborne and Ariel Rubinstein. 1994. A Course in
Game Theory. MIT Press.
Judea Pearl. 1988. Probabilistic Reasoning in Intelli-
gent Systems: Networks of Plausible Inference. Mor-
gan Kauffmann.
Leonard Savage. 1954. The Foundations of Statistics.
John Wiley.
367
Frederik Schadd, Sander Bakkes, and Pieter Spronck.
2007. Opponent modeling in real-time strategy games.
In Games and Simulation GAMEON, pages 61?68.
Yoav Shoham and Kevin Leyton-Brown. 2009. Multia-
gent Systems: Algorithmic, Game-Theoretic and Logi-
cal Foundations. Cambridge University Press.
Candace Sidner. 1994. An artificial discourse language
for collaborative negotiation. In AAAI, volume 1,
pages 814?819. MIT Press, Cambridge.
Andreas Stolcke, Klaus Ries, Noah Coccaro, Elizabeth
Shriberg, Rebecca Bates, Daniel Jurafsky, Paul Tay-
lor, Rachel Martin, Carol V. Ess-dykema, and Marie
Meteer. 2000. Dialogue act modeling for automatic
tagging and recognition of conversational speech. In
Computational Linguistics, pages 26:339?373.
Tianhao Wu, Faisal M. Khan, Todd A. Fisher, Lori A.
Shuler, and William M. Pottenger. 2002. Posting
act tagging using transformation-based learning. In
Foundations of Data Mining and knowledge Discov-
ery, pages 319?331. Springer.
368
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 105?113,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Annotating Preferences in Negotiation Dialogues
Ana??s Cadilhac, Nicholas Asher and Farah Benamara
IRIT, CNRS and University of Toulouse
118, route de Narbonne
31062 Toulouse, France
{cadilhac, asher, benamara}@irit.fr
Abstract
Modeling user preferences is crucial in many
real-life problems, ranging from individual
and collective decision-making to strategic in-
teractions between agents and game theory.
Since agents do not come with their prefer-
ences transparently given in advance, we have
only two means to determine what they are if
we wish to exploit them in reasoning: we can
infer them from what an agent says or from
his nonlinguistic actions. In this paper, we an-
alyze how to infer preferences from dialogue
moves in actual conversations that involve bar-
gaining or negotiation. To this end, we pro-
pose a new annotation scheme to study how
preferences are linguistically expressed in two
different corpus genres. This paper describes
the annotation methodology and details the
inter-annotator agreement study on each cor-
pus genre. Our results show that preferences
can be easily annotated by humans.
1 Introduction
Modeling user preferences is crucial in many real-
life problems, ranging from individual and collec-
tive decision-making (Arora and Allenby, 1999)
to strategic interactions between agents (Brainov,
2000) and game theory (Hausman, 2000). A web-
based recommender system can, for example, help
a user to identify (among an optimal ranking) the
product item that best fits his preferences (Burke,
2000). Modeling preferences can also help to find
some compromise or consensus between two or
more agents having different goals during a nego-
tiation (Meyer and Foo, 2004).
Working with preferences involves three subtasks
(Brafman and Domshlak, 2009): preference acquisi-
tion, which extracts preferences from users, prefer-
ence modeling where a model of users? preferences
is built using a preference representation language
and preference reasoning which aims at computing
the set of optimal outcomes. We focus in this paper
on the first task.
Handling preferences is not easy. First, specifying
an ordering over acceptable outcomes is not trivial
especially when multiple aspects of an outcome mat-
ter. For instance, choosing a new camera to buy may
depend on several criteria (e.g. battery life, weight,
etc.), hence, ordering even two outcomes (cameras)
can be cognitively difficult because of the need to
consider trade-offs and dependencies between the
criteria. Second, users often lack complete infor-
mation about preferences initially. They build a
partial description of agents? preferences that typi-
cally changes over time. Indeed, users often learn
about the domain, each others? preferences and even
their own preferences during a decision-making pro-
cess. Since agents don?t come with their preferences
transparently given in advance, we have only two
means to determine what they are if we wish to ex-
ploit them in reasoning: we can infer them from
what an agent says or from his nonlinguistic actions.
In this paper, we analyze how to infer preferences
from dialogue moves in actual conversations that in-
volve bargaining or negotiation.
Within the Artificial Intelligence community,
preference acquisition from nonlinguistic actions
has been performed using a variety of specific
tasks, including preference learning (Fu?rnkranz and
105
Hu?llermeier, 2011) and preference elicitation meth-
ods (Chen and Pu, 2004) (such as query learning
(Blum et al, 2004), collaborative filtering (Su and
Khoshgoftaar, 2009) and qualitative graphical rep-
resentation of preferences (Boutilier et al, 1997)).
However, these tasks don?t occur in actual conver-
sations about negotiation. We are interested in how
agents learn about preferences from actual conver-
sational turns in real dialogue (Edwards and Barron,
1994), using NLP techniques.
To this end, we propose a new annotation scheme
to study how preferences are linguistically expressed
in dialogues. The annotation study is performed
on two different corpus genres: the Verbmobil cor-
pus (Wahlster, 2000) and a booking corpus, built
by ourselves. This paper describes the annotation
methodology and details the inter-annotator agree-
ment study on each corpus genre. Our results show
that preferences can be easily annotated by humans.
2 Background
2.1 What are preferences?
A preference is commonly understood as an order-
ing by an agent over outcomes, which are under-
stood as actions that the agent can perform or goal
states that are the direct result of an action of the
agent. For instance, an agent?s preferences may be
defined over actions like buy a new car or by its end
result like have a new car. The outcomes over which
a preference is defined will depend on the domain or
task.
Among these outcomes, some are acceptable for
the agent, i.e. the agent is ready to act in such a
way as to realize them, and some outcomes are not.
Among the acceptable outcomes, the agent will typ-
ically prefer some to others. Our aim is not to de-
termine the most preferred outcome of an agent but
follows rather the evolution of their commitments to
certain preferences as the dialogue proceeds. To give
an example, if an agent proposes to meet on a certain
day X and at a certain time Y, we learn that among
the agent?s acceptable outcomes is a meeting on X
at Y, even if this is not his most preferred outcome.
We are interested in an ordinal definition of prefer-
ences, which consists in imposing a ranking over all
(relevant) possible outcomes and not a cardinal defi-
nition which is based on numerical values that allow
comparisons.
More formally, let ? be a set of possible
outcomes. A preference relation, written , is a
reflexive and transitive binary relation over elements
of ?. The preference orderings are not necessarily
complete, since some candidates may not be com-
parable by a given agent. Given the two outcomes
o1 and o2, o1  o2 means that outcome o1 is equally
or more preferred to the decision maker than o2.
Strict preference o1  o2 holds iff o1  o2 and not
o2  o1. The associated indifference relation is
o1 ? o2 if o1  o2 and o2  o1.
2.2 Preferences vs. opinions
It is important to distinguish preferences from opin-
ions. While opinions are defined as a point of view, a
belief, a sentiment or a judgment that an agent may
have about an object or a person, preferences, as
we have defined them, involve an ordering on be-
half of an agent and thus are relational and com-
parative. Hence, opinions concern absolute judg-
ments towards objects or persons (positive, negative
or neutral), while preferences concern relative judg-
ments towards actions (preferring them or not over
others). The following examples illustrate this:
(a) The movie is not bad.
(b) The scenario of the first season is better than the
second one.
(c) I would like to go to the cinema. Let?s go and see
Madagascar 2.
(a) expresses a direct positive opinion towards the
movie but we do not know if this movie is the most
preferred. (b) expresses a comparative opinion be-
tween two movies with respect to their shared fea-
tures (scenarios) (Ganapathibhotla and Liu, 2008).
If actions involving these movies (e.g. seeing them)
are clear in the context, such a comparative opin-
ion will imply a preference, ordering the first season
scenario over the second. Finally, (c) expresses two
preferences, one depending on the other. The first
is that the speaker prefers to go to the cinema over
other alternative actions; the second is, given that
preference, that he wants to see Madagascar 2 over
other possible movies.
Reasoning about preferences is also distinct from
reasoning about opinions. An agent?s preferences
106
determine an order over outcomes that predicts how
the agent, if he is rational, will act. This is not true
for opinions. Opinions have at best an indirect link
to action: I may hate what I?m doing, but do it any-
way because I prefer that outcome to any of the al-
ternatives.
3 Data
Our data come from two corpora: one already-
existing, Verbmobil (CV ), and one that we cre-
ated, Booking (CB).
The first corpus is composed of 35 dialogues ran-
domly chosen from the existing corpus Verbmobil
(Wahlster, 2000), where two agents discuss on when
and where to set up a meeting. Here is a typical frag-
ment:
pi1 A: Shall we meet sometime in the next week?
pi2 A: What days are good for you?
pi3 B: I have some free time on almost every day
except Fridays.
pi4 B: Fridays are bad.
pi5 B: In fact, I?m busy on Thursday too.
pi6 A: Next week I am out of town Tuesday, Wednes-
day and Thursday.
pi7 A: So perhaps Monday?
The second corpus was built from various En-
glish language learning resources, available on the
Web (e.g., www.bbc.co.uk/worldservice/
learningenglish). It contains 21 randomly se-
lected dialogues, in which one agent (the customer)
calls a service to book a room, a flight, a taxi, etc.
Here is a typical fragment:
pi1 A: Northwind Airways, good morning. May I
help you?
pi2 B: Yes, do you have any flights to Sydney next
Tuesday?
pi3 A: Yes, there?s a flight at 16:45 and one at 18:00.
pi4 A: Economy, business class or first class ticket?
pi5 B: Economy, please.
Our approach to preference acquisition exploits
discourse structure and aims to study the impact
of discourse for extracting and reasoning on prefer-
ences. Cadilhac et al (2011) show how to compute
automatically preference representations for a whole
stretch of dialogue from the preference representa-
tions for elementary discourse units. Our annota-
tion here concentrates on the commitments to pref-
erences expressed in elementary discourse units or
EDUs. We analyze how the outcomes and the depen-
dencies between them are linguistically expressed
by performing, on each corpus, a two-level anno-
tation. First, we perform a segmentation of the di-
alogue into EDUs. Second, we annotate preferences
expressed by the EDUs.
The examples above show the effects of segmen-
tation. Each EDU is associated with a label pii.
For Verbmobil, we rely on the already avail-
able discourse annotation of Baldridge and Las-
carides (2005). For Booking, the segmentation
was made by consensus.
We detail, in the next section, our preference an-
notation scheme.
4 Preference annotation scheme
To analyze how preferences are linguistically ex-
pressed in each EDU, we must: (1) identify the set
? of outcomes, on which the agent?s preferences
are expressed, and (2) identify the dependencies be-
tween the elements of ? by using a set of specific
operators, i.e. identifying the agent?s preferences on
the stated outcomes. Consider the segment ?Let?s
meet Thursday or Friday?. We have ? = {meet
Thursday, meet Friday} where outcomes are linked
by a disjunction that means the agent is ready to act
for one of these outcomes, preferring them equally.
Within an EDU, preferences can be expressed in
different ways. They can be atomic preference state-
ments or complex preference statements.
4.1 Atomic preferences
Atomic preference statements are of the form ?I pre-
fer X?, ?Let?s X?, or ?We need X?, where X de-
scribes an outcome. X may be a definite noun phrase
(?Monday?, ?next week?, ?almost every day?), a
prepositional phrase (?at my office?) or a verb
phrase (?to meet?). They can be expressed within
comparatives and/or superlatives (?a cheaper room?
or ?the cheapest flight?).
Preferences can also be expressed in an indirect
way using questions. Although not all questions
entail that their author commits to a preference, in
many cases they do. That is, if A asks ?can we meet
next week?? he implicates a preference for meeting.
For negative and wh-interrogatives, the implication
107
is even stronger. Expressions of sentiment or polite-
ness can also be used to indirectly introduce prefer-
ences. In Booking, the segment ?economy please?
indicates the agent?s preference to be in an economy
class.
EDUs can also express preferences via free-choice
modalities; ?I am free on Thursday? or ?I can meet
on Thursday? tells us that Thursday is a possible day
to meet, it is an acceptable outcome.
A negative preference expresses an unacceptable
outcome, i.e. what the agent does not prefer. Neg-
ative preference can be expressed explicitly with
negation words (?I don?t want to meet on Friday?)
or inferred from the context (?I am busy on Mon-
day?).
While the logical form of an atomic preference
statement is something of the form Pref(X), we
abbreviate this in the annotation language, using just
the outcome expression X to denote that the agent
prefers X to the alternatives, i.e. X  X . If X is
an unacceptable outcome, we use the non-boolean
operator not to denote that the agent prefers not X to
other alternatives, i.e. X  X . In our Verbmobil
annotation, X is typically an NP denoting a time or
place; X as an outcome is thus shorthand for meet
on X or meet at X . For Booking, X is short for
reserve or book X .
4.2 Complex preferences
Preference statements can also be complex, express-
ing dependencies between outcomes. Borrowing
from the language of conditional preference net-
works or CP-nets (Boutilier et al, 2004), we rec-
ognize that some preferences may depend on an-
other action. For instance, given that I have cho-
sen to eat fish, I will prefer to have white wine
over red wine?something which we express as
eat fish : drink white wine  drink red wine.
Among the possible combinations, we find con-
junctions, disjunctions and conditionals. We exam-
ine these conjunctive, disjunctive and conditional
operations over outcomes and suppose a language
with non-boolean operators &,5 and 7? taking out-
come expressions as arguments.
With conjunctions of preferences, as in ?Could
I have a breakfast and a vegetarian meal?? or in
?Mondays and Fridays are not good??, the agent ex-
presses two preferences (respectively over the ac-
ceptable outcomes breakfast and vegetarian meal
and the non acceptable outcomes not Mondays and
not Fridays) that he wants to satisfy and he prefers
to have one of them if he can not have both. Hence
o1 & o2 means o1  o1 and o2  o2.
The semantics of a disjunctive preference is a free
choice one. For example in ?either Monday or Tues-
day is fine for me? or in ?I am free Monday and
Tuesday?, the agent states that either Monday or
Tuesday is an acceptable outcome and he is indif-
ferent between the choice of the outcomes. Hence
o1 5 o2 means o2 : o1 ? o1, o2 : o1  o1 and
o1 : o2 ? o2, o1 : o2  o2.
Finally, some EDUs express conditional among
preferences. For example, in the sentence ?What
about Monday, in the afternoon??, there are two
preferences: one for the day Monday, and, given the
Monday preference, one for the time afternoon (of
Monday), at least for one syntactic reading of the
utterance. Hence o1 7? o2 means o1  o1 and
o1 : o2  o2.
For each EDU, annotators identify how outcomes
are expressed and then indicate if the outcomes are
acceptable, or not, using the operator not and how
the preferences on these outcomes are linked using
the operators &,5 and 7?.
4.3 Example
We give below an example of how some EDUs are
annotated. <o> i indicates that o is the outcome
number i in the EDU, the symbol // is used to sepa-
rate the two annotation levels and brackets indicate
how outcomes are attached.
pi1 : <Tuesday the sixteenth> 1 I got class<from nine
to twelve> 2? // 1 7? not 2
pi2 : What about <Friday afternoon> 1, <at two
thirty> 2 or <three> 3, // 1 7? (25 3)
pi3 : <The room with balcony> 1 should be equipped
<with a queen size bed> 2, <the other one> 3
<with twin beds> 4, please. // (1 7? 2) & (3 7?
4)
In pi1, the annotation tells us that we have two out-
comes and that the agent prefers outcome 1 over any
other alternatives and given that, he does not pre-
fer outcome 2. In pi2, the annotation tells us that
the agent prefers to have one of outcome 2 and out-
come 3 satisfied given that he prefers outcome 1. In
this example, the free choice between outcome 2 and
108
outcome 3 is lexicalized by the coordinating con-
junction ?or?. On the contrary, pi3 is a more complex
example where there is no discursive marker to find
that the preference operator between the couples of
outcomes 1 and 2 on one hand, and 3 and 4 on the
other hand, is the conjunctive operator &.
5 Inter-annotator agreements
Our two corpora (Verbmobil and Booking)
were annotated by two annotators using the pre-
viously described annotation scheme. We per-
formed an intermediate analysis of agreement and
disagreement between the two annotators on two
Verbmobil dialogues. Annotators were thus
trained only for Verbmobil. The aim is to study to
what extent our annotation scheme is genre depen-
dent. The training allowed each annotator to under-
stand the reason of some annotation choices. After
this step, the dialogues of our corpora have been an-
notated separately, discarding those two dialogues.
Table 1 presents some statistics about the annotated
data in the gold standard.
CV CB
No. of dialogues 35 21
No. of outcomes 1081 275
No. of EDUs with outcomes 776 182
% with 1 outcome 71% 70%
% with 2 outcomes 22% 19%
% with 3 or more outcomes 8% 11%
No. of unacceptable outcomes (not) 266 9
No. of conjunctions (&) 56 31
No. of disjunctions (5) 75 29
No. of conditionals (7?) 184 37
Table 1: Statistics for the two corpora.
We compute four inter-annotator agreements: on
outcome identification, on outcome acceptance, on
outcome attachment and finally on operator identifi-
cation. Table 2 summarizes our results.
5.1 Agreements on outcome identification
Two inter-annotator agreements were computed us-
ing Cohen?s Kappa. One based on an exact matching
between two outcome annotations (i.e. their corre-
sponding text spans), and the other based on a le-
CV CB
Outcome identification (Kappa) exact : 0.66
lenient : 0.85
Outcome acceptance (Kappa) 0.90 0.95
Outcome attachment (F-measure) 93% 82%
Operator identification (Kappa) 0.93 0.75
Table 2: Inter-annotator agreements for the two corpora.
nient match between annotations (i.e. there is an
overlap between their text spans as in ?2p.m? and
?around 2p.m?). This approach is similar to the one
used by Wiebe, Wilson and Cardie (2005) to com-
pute agreement when annotating opinions in news
corpora. We obtained an exact agreement of 0.66
and a lenient agreement of 0.85 for both corpus gen-
res.
We made the gold standard after discussing cases
of disagreement. We observed four cases. The first
one concerns redundant preferences which we de-
cided not to keep in the gold standard. In such cases,
the second EDU pi2 does not introduce a new prefer-
ence, neither does it correct the preferences stated in
pi1; rather, the agent just wants to insist by repeat-
ing already stated preferences, as in the following
example:
pi1 A: Thursday, Friday, and Saturday I am out.
pi2 A: So those days are all out for me,
The second case of disagreement comes from
anaphora which are often used to introduce new, to
make more precise or to accept preferences. Hence,
we decided to annotate them in the gold standard.
Here is an example:
pi1 A: One p.m. on the seventeenth?
pi2 B: That sounds fantastic.
The third case of disagreement concerns prefer-
ence explanation. We chose not to annotate these
expressions in the gold standard because they are
used to explain already stated preferences. In the
following example, one judge annotated ?from nine
to twelve? to be expressions of preferences while the
other did not :
pi1 A: Monday is really not good,
pi2 A: I have got class from nine to twelve.
109
Finally, the last case of disagreement comes from
preferences that are not directly related to the action
of fixing a date to meet but to other actions, such as
having lunch, choosing a place to meet, etc. Even
though those preferences were often missed by an-
notators, we decided to keep them, when relevant.
5.2 Agreements on outcome acceptance
The aim here is to compute the agreement on the not
operator, that is if an outcome is acceptable, as in
?<Mondays> 1 are good // 1?, or unacceptable, as
in ?<Mondays> 1 are not good // not 1?. We get a
Cohen?s Kappa of 0.9 for Verbmobil and 0.95 for
Booking. The main case of disagreement concerns
anaphoric negations that are inferred from the con-
text, as in pi2 below where annotators sometimes fail
to consider ?in the morning? as unacceptable out-
comes:
pi1 A: Tuesday is kind of out,
pi2 A: Same reason in the morning
Same case of disagreement in this example where
?Monday? is an unacceptable outcome:
pi1 well, I am, busy <in the afternoon of the twenty
sixth> 1, // not 1
pi2 that is <Monday> 1 // not 1
5.3 Agreements on outcome attachment
Since this task involves structure building, we com-
pute the agreement using the F-score measure. The
agreement was computed on the previously built
gold standard once annotators discussed cases of
outcome identification disagreements. We compare
how each outcome is attached to the others within
the same EDU. This agreement concerns EDUs
that contain at least three outcomes, that is 8% of
EDUs from Verbmobil and 11% of EDUs from
Booking. When comparing annotations for the ex-
ample pi1 below, there is three errors, one for out-
come 2, one for 3 and one for 4.
pi1 <for the next week> 1 the only days I have
open are <Monday> 2 or <Tuesday> 3 <in the
morning> 4.
? Annotation 1 : 1 7? (25 (3 7? 4))
? Annotation 2 : 1 7? ((25 3) 7? 4)
We obtain an agreement of 93% for Verbmobil
and 82% for Booking.
5.4 Agreements on outcome dependencies
Finally, we compute the agreements for each couple
of outcomes on which annotators agreed about how
they are attached.
In Verbmobil, the most frequently used binary
operator is 7?. Because the main purpose of the
agents in this corpus is to schedule an appointment,
the preferences expressed by the agents are mainly
focused on concepts of time and there are many con-
ditional preferences since it is common that prefer-
ences on specific concepts depend on more broad
temporal concepts. For example, preferences on
hours are generally conditional on preferences on
days. In Booking, there are almost as many & as
7? because independent and dependent preferences
are more balanced in this corpus. The agents dis-
cuss preferences about various criteria that are in-
dependent. For example, to book a hotel, the agent
express his preferences towards the size of the bed
(single or double), the quality of the room (smoker
or nonsmoker), the presence of certain conveniences
(TV, bathtub), the possibility to have breakfast in
his room, etc. Within an EDU, such preferences are
often expressed in different sentences (compared to
Verbmobil where segments? lengths are smaller)
which lead annotators to link those preferences with
the operator &. Conditionals between preferences
hold when decision criteria are dependent. For ex-
ample, the preference for having a vegetarian meal
is conditional on the preference for having lunch.
There also are conditionals between temporal con-
cepts, for example, to choose the time of a flight.
Table 3 shows the Kappa for each operator on
each corpus genre. The Cohen?s Kappa, averaged
over all the operators, is 0.93 for Verbmobil and
0.75 for Booking. We observe two main cases of
disagreement: between 5 and &, and between &
and 7?. These cases are more frequent for Booking
mainly because annotators were not trained on this
corpus. This is why the Kappa was lower than for
Verbmobil. We discuss below the main two cases
of disagreement.
Confusion between 5 and &. The same lin-
guistic realizations do not always lead to the same
operator. For instance, in ?<Monday> 1 and
<Wednesday> 2 are good? we have 15 2 whereas
in ?<Monday> 1 and <Wednesday> 2 are not
110
CV CB
& 0.90 0.66
5 0.97 0.89
7? 0.92 0.71
Table 3: Agreements on binary operators.
good? or in ?I would like a <single room> 1 and
a <taxi> 2? we have respectively not 1 & not 2
and 1 & 2.
The coordinating conjunction ?or? is a strong pre-
dictor for recognizing a disjunction of preferences,
at least when the ?or? is clearly outside of the scope
of a negation1, as in the examples below (in pi1, the
negation is part of the wh-question, and not boolean
over the preference):
pi1 Why don?t we <meet, either Thursday the first> 1,
or <Thursday the eighth> 2 // 15 2
pi2 Would you like <a single> 1 or <a double> 2? //
15 2
The coordinating conjunction ?and? is also a
strong indication, especially when it is used to link
two acceptable outcomes that are both of a single
type (e.g., day of the week, time of day, place,
type of room, etc.) between which an agent wants
to choose a single realization. For example, in
Verbmobil, agents want to fix a single appoint-
ment so if there is a conjunction ?and? between two
temporal concepts of the same level, it is a disjunc-
tion of preference (see pi3 below). It is also the case
in Booking when an agent wants to book a single
plane flight (see pi4).
pi3 <Monday> 1 and <Tuesday> 2 are good for me
// 15 2
pi4 You could <travel at 10am.> 1, <noon> 2 and
<2pm> 3 // 15 (25 3)
The acceptability modality distributes across
the conjoined NPs to deliver something like
3(meet Monday) ? 3(meet Tuesday) in modal
logic (clearly acceptability is an existential
rather than universal modality), and as is
known from studies of free choice modality
1When there is a propositional negation over the disjunction
as in ?I don?t want sheep or wheat?, which occurs frequently
in a corpus in preparation, we no longer have a disjunction of
preferences.
(Schulz, 2007), such a conjunction translates to
3(meet Monday ? meet Tuesday), which ex-
presses our free choice disjunction of preferences,
o1 5 o2.
On the other hand, when the conjunction ?and?
links two outcomes referring to a single concept
that are not acceptable, it gives a conjunction of
preferences, as in pi5. Once again thinking in
terms of modality is helpful. The ?not accept-
able? modality distributes across the conjunction,
this gives something like 2?o1 ? 2?o2 (where ?
is truth conditional negation) which is equivalent to
2(?o1 ? ?o2), i.e. not o1 & not o2 and not equiv-
alent to 2(?o1 ? ?o2), i.e. not o1 5 not o2.
The connector ?and? also involves a conjunction
of preferences when it links two independent out-
comes that the agent wants to satisfy simultaneously.
For example, in pi6, the agent wants to book two ho-
tel rooms, and so the outcomes are independent. In
pi7, the agent expresses his preferences on two differ-
ent features he wants for the hotel room he is book-
ing.
pi5 <Thursday the thirtieth> 1, and <Wednesday the
twenty ninth> 2 are, booked up // not 1 & not 2
pi6 Can I have one room< with balcony> 1 and <one
without balcony> 2? // 1 & 2
pi7 <Queen> 1 and <nonsmoking> 2 // 1 & 2
Confusion between & and 7?. In this case, dis-
agreements are mainly due to the difficulty for an-
notators to decide if preferences are dependent, or
not. For example, in ?I have a meeting <starting
at three> 1, but I could meet <at one o?clock> 2?,
one annotator put not 1 7? 2 meaning that the
agent is ready to meet at one o?clock because he
can not meet at three, while the other annotated
not 1 & 2 meaning that the agent is ready to meet
at one o?clock independently of what it will do at
three.
Some connectors introduce contrast between the
preferences expressed in a segment as ?but?,
?although? and ?unless?. In the annotation, we can
model it thanks to the operator 7?. When it is used
between two conflicting values, it represents a cor-
rection. Thus, the annotation o1 7? not o1 means we
need to replace in our model of preferences o1  o1
by o1  o1. And vice versa for not o1 7? o1.
pi8 I have class <on Monday> 1, but, <any time, after
one or two> 2 I am free. // not 1 7? (1 7? 2)
111
pi9 <Friday> 1 is a little full, although there is some
possibility, <before lunch> 2 // not 1 7? (1 7? 2)
pi10 we?re full <on the 22nd> 1, unless you want <a
smoking room> 2 // not 1 7? (1 7? 2)
However, it is important to note that the coordi-
nating conjunction ?but? does not always introduce
contrast, as in the example below, where it intro-
duces a conjunction of preferences.
pi11 I am busy <on Monday> 1, but <Tuesday
afternoon> 2, sounds good // not 1 & 2
The subordinating conjunctions ?if?, ?because?
and ?so? are indications for detecting conditional
preferences. The preferences in the main clause de-
pend on the preferences in the subordinate clause
(if-clause, because-clause, so-clause), as in the ex-
amples below.
pi12 so if we are going to be able to meet <that, last
week in January> 1, it is going have to be <the,
twenty fifth> 2 // 1 7? 2
pi13 <the twenty eighth> 1 I am free, <all day> 2, if
you want to go for <a Sunday meeting> 3 // 3 7?
(2 7? 1)
pi14 it is going to have to be <Wednesday the third> 1
because, I am busy <Tuesday> 2 // not 2 7? 1
pi15 I have a meeting <from eleven to one> 1, so
we could, meet <in the morning from nine to
eleven> 2, or,<in the afternoon after one> 3 // not
1 7? (25 3)
Whether or not there are some discursive markers
between two outcomes, to find the appropriate oper-
ator, we need to answer some questions : does the
agent want to satisfy the two outcomes at the same
time ? Are the preferences on the outcomes depen-
dent or independent ?
We have shown in this section that it is difficult to
answer the second question and there is quite some
ambiguity between the operators & et 7?. This am-
biguity can be explained by the fact that both opera-
tors model the same optimal preference. Indeed, we
saw in section 4.2 that for two outcomes o1 and o2
linked by a conjunction of preferences (o1 & o2), we
have o1  o1 and o2  o2. For two outcomes o1 and
o2 where o2 is linked to o1 by a conditional prefer-
ence (o1 7? o2), we have o1  o1 and o1 : o2  o2.
In both cases, the best possible world for the agent
is the one where o1 and o2 are both satisfied at the
same time.
6 Conclusion and Future Work
In this paper, we proposed a linguistic approach
to preference aquisition that aims to infer prefer-
ences from dialogue moves in actual conversations
that involve bargaining or negotiation. We stud-
ied how preferences are linguistically expressed in
elementary discourse units on two different cor-
pus genres: one already available, the Verbmobil
corpus and the Booking corpus purposely built
for this project. Annotators were trained only for
Verbmobil. The aim is to study to what extent
our annotation scheme is genre dependent.
Our preference annotation scheme requires two
steps: identify the set of acceptable and non accept-
able outcomes on which the agents preferences are
expressed, and then identify the dependencies be-
tween these outcomes by using a set of specific non-
boolean operators expressing conjunctions, disjunc-
tions and conditionals. The inter-annotator agree-
ment study shows good results on each corpus genre
for outcome identification, outcome acceptance and
outcome attachment. The results for outcome de-
pendencies are also good but they are better for
Verbmobil. The difficulties concern the confu-
sion between disjunctions and conjunctions mainly
because the same linguistic realizations do not al-
ways lead to the same operator. In addition, anno-
tators often fail to decide if the preferences on the
outcomes are dependent or independent.
This work shows that preference acquisition from
linguistic actions is feasible for humans. The next
step is to automate the process of preference extrac-
tion using NLP methods. We plan to do it using an
hybrid approach combining both machine learning
techniques (for outcome extraction and outcome ac-
ceptance) and rule-based approaches (for outcome
attachment and outcome dependencies).
References
Neeraj Arora and Greg M. Allenby. 1999. Measur-
ing the influence of individual preference structures
in group decision making. Journal of Marketing Re-
search, 36:476?487.
Jason Baldridge and Alex Lascarides. 2005. Annotating
discourse structures for robust semantic interpretation.
In Proceedings of the 6th IWCS.
Avrim Blum, Jeffrey Jackson, Tuomas Sandholm, and
112
Martin Zinkevich. 2004. Preference elicitation and
query learning. Journal of Machine Learning Re-
search, 5:649?667.
Craig Boutilier, Ronen Brafman, Chris Geib, and David
Poole. 1997. A constraint-based approach to prefer-
ence elicitation and decision making. In AAAI Spring
Symposium on Qualitative Decision Theory, pages 19?
28.
Craig Boutilier, Craig Brafman, Carmel Domshlak, Hol-
ger H. Hoos, and David Poole. 2004. Cp-nets: A tool
for representing and reasoning with conditional ceteris
paribus preference statements. Journal of Artificial In-
telligence Research, 21:135?191.
Ronen I. Brafman and Carmel Domshlak. 2009. Prefer-
ence handling - an introductory tutorial. AI Magazine,
30(1):58?86.
Sviatoslav Brainov. 2000. The role and the impact of
preferences on multiagent interaction. In Proceedings
of ATAL, pages 349?363. Springer-Verlag.
Robin Burke. 2000. Knowledge-based recommender
systems. In Encyclopedia of Library and Information
Science, volume 69, pages 180?200. Marcel Dekker.
Ana??s Cadilhac, Nicholas Asher, Farah Benamara, and
Alex Lascarides. 2011. Commitments to preferences
in dialogue. In Proceedings of SIGDIAL, pages 204?
215. ACL.
Li Chen and Pearl Pu. 2004. Survey of preference elici-
tation methods. Technical report.
Ward Edwards and F. Hutton Barron. 1994. Smarts
and smarter: Improved simple methods for multiat-
tribute utility measurement. Organizational Behavior
and Human Decision Processes, 60(3):306?325.
Johannes Fu?rnkranz and Eyke Hu?llermeier, editors.
2011. Preference Learning. Springer.
Murthy Ganapathibhotla and Bing Liu. 2008. Mining
opinions in comparative sentences. In Proceedings of
the 22nd International Conference on Computational
Linguistics - Volume 1, COLING ?08, pages 241?248,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Daniel M. Hausman. 2000. Revealed preference, be-
lief, and game theory. Economics and Philosophy,
16(01):99?115.
Thomas Meyer and Norman Foo. 2004. Logical founda-
tions of negotiation: Strategies and preferences. In In
Proceedings of the Ninth International Conference on
Principles of Knowledge Representation and Reason-
ing (KR04, pages 311?318.
Katrin Schulz. 2007. Minimal Models in Semantics and
Pragmatics: Free Choice, Exhaustivity, and Condi-
tionals. PhD thesis, ILLC.
Xiaoyuan Su and Taghi M. Khoshgoftaar. 2009. A sur-
vey of collaborative filtering techniques. Advances in
Artificial Intelligence, 2009:1?20.
Wolfgang Wahlster, editor. 2000. Verbmobil: Founda-
tions of Speech-to-Speech Translation. Springer.
Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005.
Annotating expressions of opinions and emotions in
language. Language Resources and Evaluation, 39(2-
3):165?210.
113
Proceedings of the 6th Workshop on Ontologies and Lexical Resources (Ontolex 2010), pages 77?86,
Beijing, August 2010
Ontolexical resources for feature based opinion mining :                 
a case-study 
 
Ana?s Cadilhac 
IRIT 
Toulouse University 
cadilhac@irit.fr  
Farah Benamara 
IRIT 
Toulouse University 
benamara@irit.fr 
Nathalie Aussenac-Gilles 
IRIT 
Toulouse University 
aussenac@irit.fr 
 
Abstract 
Opinion mining is a growing research 
area both at the natural language proc-
essing and the information retrieval 
communities.  Companies, politicians, 
as well as customers need powerful 
tools to track opinions, sentiments, 
judgments and beliefs that people may 
express in blogs, reviews, audios and 
videos data regarding a prod-
uct/service/person/organisation/etc. This 
work describes our contribution to fea-
ture based opinion mining where opin-
ions expressed towards each feature of 
an object or a product are extracted and 
summarized. The state of the art has 
shown that the hierarchical organization 
of features is a key step. In this context, 
our goal is to study the role of a domain 
ontology to structure and extract object 
features as well as to produce a compre-
hensive summary. This paper presents 
the developed system and the experi-
ments we carried out on a case study: 
French restaurant reviews. Our results 
show that our approach outperforms 
standard baselines. 
1 Introduction  
Opinion mining is a growing research area both 
in natural language processing and information 
retrieval communities. Companies, politicians, 
as well as customers need powerful tools to 
track opinions, sentiments, judgments and be-
liefs that people may express in blogs, reviews, 
audios and videos data regarding a prod-
uct/service/person/organisation/etc. The impor-
tance of emotion-oriented computing in the 
Web 2.0 has encouraged the creation of new 
search engines (like Tweetfeel 
(www.tweetfeel.com)) as well as the creation of 
a new research group within the W3C, namely 
the Emotion Markup Language, that aims to 
develop a representation language of the emo-
tional states of a user or the emotional states to 
be simulated by a user interface. In addition, 
most information retrieval evaluation campaigns 
(TREC, NTCI, etc.) have already integrated an 
opinion track.  
Computational approaches to sentiment analysis 
focus on extracting the affective content of a 
text from the detection of expressions of ?bag of 
sentiment words? at different levels of granular-
ity. These expressions are assigned a positive or 
a negative scalar value, representing a positive, 
a negative or neutral sentiment towards some 
topic. Roughly, research in this field can be 
grouped in four main categories (which are not 
exclusive):  
? Development of linguistic and cognitive 
models of opinion/sentiment where already 
existing psycholinguistic theories of emo-
tions are used to analyse how opinions are 
lexically expressed in texts (Wiebe et al 
2005; Read et al 2007; Asher et al 2009) 
? Elaboration of linguistic resources where 
corpus based and dictionary based ap-
proaches are used to automatically or semi-
automatically extract opinion bearing  
terms/expressions as well as their sentiment 
orientation (Strapparava et al, 2004; Turney 
and Littman, 2002) 
? Opinion extraction/analysis at the document 
(Pang et al, 2002; Turney, 2002), at the 
sentence or at the clause level (Kim et 
al., 2006; Choi et al, 2005) where local 
77
opinions are aggregated in order to compute 
the overall orientation of a docu-
ment/sentence/clause. 
? Feature based opinion mining (Hu and Liu, 
2004; Popescu and Etzioni, 2005; Carenini 
et al, 2005; Cheng and Xu, 2008) where 
opinions expressed towards the features of 
an object or a product are exacted and 
summarized. 
The work described in this paper feats into the 
last category. The aim is not to compute the 
general orientation of a document or a sentence, 
since a positive sentiment towards an object 
does not imply a positive sentiment towards all 
the aspects of this object, as in: I like this res-
taurant even if the service is slow. In feature 
based opinion mining, a holder (the person who 
posts the review) expresses a positive/negative 
or neutral opinions towards a main topic (the 
object or the product on which the holder ex-
presses his opinions) and its associated features. 
As defined in (Hu and Liu, 2004), a feature can 
be a ?part-of? of a topic (such as the screen of a 
camera) or a property of the ?part-of? of the 
topic (such as the size of the screen).  The ex-
pressed opinion can be explicit, as in ?the 
screen of this camera is great?, or implicit, as in 
?the camera is heavy?, that expresses a negative 
opinion towards the weight of the camera. Same 
features can also be expressed differently, for 
example, ?drink? and ?beverage? refer to the 
same restaurant feature. 
Having, for an object/product, the set of its as-
sociated features F={f1,?fn}, research in fea-
ture based opinion mining mostly focus on 
extracting the set F from reviews, and then, for 
each feature fi of F, extract the set of its associ-
ated opinion expressions OE={OE1,?OEj}. 
Once the set of couples (fi, OE) were extracted, 
a summary of the review is generally produced. 
During this process, the key questions are: how 
the set F of features can be obtained? How they 
are linguistically expressed? How they are re-
lated to each other ? Which knowledge repre-
sentation model can be used to better organize 
product features and to produce a comprehen-
sive summary?  
To answer these questions, we propose in this 
paper to study the role of an ontology in feature 
based opinion mining. More precisely, our aim 
is to study how a domain ontology can be used 
to: 
? structure features: we show that an ontol-
ogy is more suitable than a simple hierarchy 
where features are grouped using only the 
?is-a? relation (Carenini et al, 2005; Blair-
Goldensohn et al, 2008) 
? extract explicit and implicit features from 
texts: we show how the lexical component 
as well as the set of properties of the ontol-
ogy can help to extract, for each feature, the 
set of the associated opinion expressions.  
? produce a discourse based summary of the 
review: we show how the ontology can 
guide the process of identifying the most 
relevant discourse relations that may hold 
between elementary discourse  units.  
The paper is organised as follows. We give in 
section 2, a state of the art of the main ap-
proaches used in the field as well as the motiva-
tions of our work. We present in the next sec-
tion, our approach. Finally, in section 4, we de-
scribe the experiments we carried out on a case 
study: French restaurant reviews 
  
2 Feature based Opinion mining 
2.1 Related Works 
Overall, two main families of work stand out: 
those that extract a simple list of features and 
those that organize them into a hierarchy using 
taxonomies or ontologys. The feature extraction 
process mainly concerns explicit features. 
 
Works without knowledge representation 
models : The pioneer work in feature based 
opinion mining is probably the one of Hu and 
Liu (2004) that applies association rule mining 
algorithm to discover product features (nouns 
and noun-phrases). Heuristics (frequency of 
occurrence, proximity with opinion words, 
etc...) can eliminate irrelevant candidates. Opin-
ion expressions (only adjective phrases) which 
are the closest to these features are extracted. A 
summary is then produced and displays, for 
each feature, both positive and negative phrases 
and the total number of these two categories. 
To improve the feature extraction phase, Pope-
scu and Etzioni (2005) suggest in their system 
78
OPINE, to extract only nominal groups whose 
frequency is above a threshold determined ex-
perimentally using the calculation of PMI 
(Point-wise Mutual Information) between each 
of these nouns and meronymy expressions asso-
ciated with the product. No summary is pro-
duced.  
The main limitation of these approaches is that 
there are a great many extracted features and 
there is a lack of organization. Thus, similar 
features are not grouped together (for example, 
in restaurant domain, ?atmosphere? and ?ambi-
ence?), and possible relationships between fea-
tures of an object are not recognized (for exam-
ple, ?coffee? is a specific term for ?drink?). In 
addition, polarity analysis (positive, negative or 
neutral) of the document is done by assigning 
the dominant polarity of opinion words it con-
tains (usually adjectives), regardless of polari-
ties individually associated to each feature. 
 
Works using feature taxonomies. Following 
works have a different approach: they do not 
look for a ?basic list? of features but rather a list 
hierarchically organized through the use of tax-
onomies. We recall that a taxonomy is a list of 
terms organized hierarchically through speciali-
zation relationship type ?is a sort of?.  
Carenini et al (2005) use predefined taxono-
mies and semantic similarity measures to auto-
matically extract classic features of a product 
and calculate how close to predefined concepts 
in the taxonomy they are. This is reviewed by 
the user in order to insert missing concepts in 
the right place while avoiding duplication. The 
steps of identifying opinions and their polarity 
and the production of a summary are not de-
tailed. This method was evaluated on the prod-
uct review corpus of Hu and Liu (2004) and 
resulted in a significant reduction in the number 
of extracted features. However, this method is 
very dependent on the effectiveness of similar-
ity measures used.  
In their system PULSE, Gamon et al (2005) 
analyze a large amount of text contained in a 
database. A taxonomy, including brands and 
models of cars, is automatically extracted from 
the database. Coupled with a classification 
technique, sentences corresponding to each leaf 
of the taxonomy are extracted. At the end of the 
process, a summary which can be more or less 
detailed is produced. 
The system described in (Blair-Goldensohn et 
al., 2008) extracts information about services, 
aggregates the sentiments expressed on every 
aspect and produces a summary. The automatic 
feature extraction combines a dynamic method, 
where the different aspects of services are the 
most common nouns, and a static method, 
where a taxonomy grouping the concepts con-
sidered to be the most relevant by the user is 
used to manually annotate sentences. The re-
sults also showed that the use of a hierarchy 
significantly improves the quality of extracted 
features. 
 
Works using ontologys. These works aim at 
organizing features using a more elaborated 
model of representation: an ontology Unlike 
taxonomy, ontology is not restricted to a hierar-
chical relationship between concepts, but can 
describe other types of paradigmatic relations 
such as synonymy, or more complex relation-
ships such as composition relationship or space 
relationship.  
Overall, extracted features correspond exclu-
sively to terms contained in the ontology. The 
feature extraction phase is guided by a domain 
ontology, built manually (Zhao and Li, 2009), 
or semi-automatically (Feiguina, 2006; Cheng 
and Xu, 2008), which is then enriched by an 
automatic process of extraction / clustering of 
terms which corresponds to new feature identi-
fication.  
To extract terms, Feiguina (2006) uses pattern 
extraction coupled to a terminology extractor 
trained over a set of features related to a product 
and identified manually in a few reviews. Same 
features are grouped together using semantic 
similarity measures. The system OMINE 
(Cheng and Xu, 2008) proposes a mechanism 
for ontology enrichment using a domain glos-
sary which includes specific terms such as 
words of jargon, abbreviations and acronyms. 
Zhao and Li (2009) add to their ontology con-
cepts using a corpus based method: sentences 
containing a combination of conjunction word 
and already recognized concept are extracted. 
This process is repeated iteratively until no new 
concepts are found. 
Ontologys have also been used to support polar-
ity mining. For example, (Chaovalit and Zhou, 
2008) manually built an ontology for movie re-
views and incorporated it into the polarity clas-
79
sification task which significantly improve per-
formance over standard baseline. 
 
2.2 Towards an ontology based opinion 
mining 
Most of the researchers actually argue that 
the use of a hierarchy of features improves the 
performance of feature based opinion mining 
systems.  However, works that actually use a 
domain ontology (cf. last section) exploit the 
ontology as a taxonomy using only the is-a rela-
tion between concepts. They do not really use 
all data stored in an ontology, such as the lexical 
components and other types of relations. In ad-
dition, in our knowledge, no work has investi-
gated the use of an ontology to produce com-
prehensive summaries. 
 We think there is still room for improvement 
in the field of feature based sentiment analysis. 
To get an accurate appraisal of opinion in texts, 
it is important for NLP systems to go beyond 
explicit features and to propose a fine-grained 
analysis of opinions expressed towards each 
feature. Our intuition is that the full use of on-
tology would have several advantages in the 
domain of opinion mining to:  
Structure features: ontologys are tools that 
provide a lot of semantic information. They help 
to define concepts, relationships and entities 
that describe a domain with unlimited number 
of terms. This set of terms can be a significant 
and valuable lexical resource for extracting ex-
plicit and implicit features. For example, in the 
following restaurant review: cold and not tasty 
the negative opinion not tasty is ambiguous 
since it is not associated to any lexicalised fea-
ture. However, if the term cold is stored in the 
ontology as a lexical realization of the concept 
quality of the cuisine, the opinion not tasty can 
be easily associated to the feature cuisine of the 
restaurant (note that the conjunction and plays 
an important role in the desambiguisation proc-
ess). We discuss this point at the last section of 
the paper.   
 
Extract features: ontologys provide structure 
for these features through their concept hierar-
chy but also their ability to define many rela-
tions linking these concepts. This is also a valu-
able resource for structuring the knowledge ob-
tained during feature extraction task. In addi-
tion, the relations between concepts and lexical 
information can be used to extract implicit fea-
tures. For example, if the concept customer is 
linked to the concept restaurant by the relation 
to eat in,  a positive opinion towards the restau-
rant can be extracted from the review: we eat 
well. Similarly, if the concept restaurant is 
linked to the concept landscape with the rela-
tion to view, a positive opinion can be extracted 
towards the look out of the restaurant from the 
following review:  very good restaurant where 
you can savour excellent Gratin Dauphinois 
and admire the most beautiful peak of the Pyr?-
n?es 
 
Produce summaries. Finally, we also believe 
that ontologys can play a fundamental role to 
produce well organised summary and discursive 
representation of the review. We further detail 
this point at the last section of the paper. 
3 Our approach 
Our feature based opinion mining system needs 
three basic components: a lexical resource L of 
opinion expressions, a lexical ontology O where 
each concept and each property is associated to 
a set of labels that correspond to their linguistic 
realizations and a review R.   
Following the idea described in (Asher et al 
2009), a review R is composed of a set of ele-
mentary discourse units (EDU). Using the dis-
course theory SDRT (Asher and Lascarides 
2003) as our formal framework, an EDU is a 
clause containing at least one elementary opin-
ion unit (EOU) or a sequence of clauses that 
together bear a rhetorical relation to a segment 
expressing an opinion. An EOU is an explicit 
opinion expression composed of a noun, an ad-
jective or a verb with its possible modifiers (ac-
tually negation and adverb) as described in our 
lexicon L. 
We have segmented conjoined NPs or APs 
into separate clauses?for instance, the film is 
beautiful and powerful is taken to express two 
segments: the film is beautiful and the film is 
powerful. Segments are then connected to each 
other using a small subset of ?veridical? dis-
course relations, namely: 
? Contrast (a,b), implies that a and b are both 
true but there is some defeasible implication 
80
of one that is contradicted by the other. Pos-
sible markers can be although, but. 
? Result(a,b) indicated by markers like so, as 
a result, indicates that the EDU b  is a con-
sequence or result of the EDU a.  
? Continuation(a,b ) corresponds to a series 
of speeches in which there are no time con-
straints and where segments form part of a 
larger thematic. For example, "The average 
life expectancy in France is 81 years. In 
Andorra, it reaches over 83 years. In Swazi-
land it does not exceed 85 years." 
? Elaboration(a,b) describes global informa-
tion that was stated previously with more 
specific information. For example, "Yester-
day, I spent a wonderful day. I lounged in 
the sun all morning. I ate in a nice little res-
taurant. Then at night,  I met my friend Emi-
ly." 
 
 In a review R, an opinion holder h comments 
on a subset S of the features of an ob-
ject/product using some opinion expressions. 
Each feature corresponds to the set of linguistic 
realizations of a concept or a property of the 
domain ontology O. For example, in the follow-
ing product review, EDUs are between square 
brackets, EOUs are between embraces whereas 
object features are underlined. There is a 
contrast relation between the EDUb and EDUc 
which makes up the opinion expressed within 
the EDUd. 
[I bought the product yesterday]
 a. [Even if the 
product is {excellent}]b, [the design and the size 
are  {very basic}]
 c, [which is {disappointing}  
in this brand]
 c.  
 
The figure below gives an overview of our sys-
tem. First, each review R is parsed using the 
French syntactic parser Cordial 1 , which pro-
vides, for each sentence, its POS tagging and 
the set of dependency relations. The review is 
then segmented in EDUs using the discourse 
parser described in (Afantenos and al, 2010).  
 
For each EDU, the system : 
1. Extracts EOUs using a rule based approach  
2. Extracts features that correspond to the 
process of term extraction using the domain 
ontology 
                                                 
1
 http://www.synapse-fr.com/Cordial_Analyseur/ 
 
Figure 1 Overview of our system. 
 
 
3. Associates, for each feature within an EDU, 
the set of opinion expressions 
4. Produces a discourse based summary. 
 
Since the summarization module is not done 
yet, we detail below the three first steps.  
 
3.1 Extracting Elementary Opinion Units 
We recall that an EOU is the smallest opinion 
unit within an EDU. It is composed of one and 
only one opinion word (a noun, an adjective or a 
verb) possibly associated with some modifiers 
like negation words and adverbs. For example, 
?really not good? is an EOU.  An EOU can also 
be simply an adverb as in too spicy. Adverbs are 
also used to update our opinion lexicon, as in 
too chic where the opinion word chic is added.  
Finally, we also extract expressions of recom-
mendation, such as : go to this restaurant, you 
will not regret it, which are very frequent in 
reviews. 
3.2 Extracting features  
This step aims at extracting for the review all 
the labels of the ontology. Since each concept 
and its associated lexical realizations corre-
spond to explicit features, we simply project the 
lexical component of the ontology in the review 
in order to get, for each EDU, the set of features 
F. Of course, since our lexical ontology does not 
81
cover all the linguistic realizations of concepts 
and properties in a given domain, many terms in 
the review can be missed. We show, in the next 
section, that linking features to opinion expres-
sions can partially solve this problem. 
To extract implicit features, ontology proper-
ties are used. We recall that these properties 
define relations between concepts of the ontol-
ogy. For example, the property ?look at? links 
?customer? and ?design? concepts.  
3.3 Associating opinions expressions to 
extracted features  
In this step, the extracted opinion expressions in 
step 1 have to be linked to the features extracted 
in step 2 i.e. we have to associate to each EDUi 
the set of couples (fi, OEi). During this step, we 
distinguish the following cases : 
 
Case 1. Known features and known opinion 
words. For example, if the lexicon contains the 
words really, good and excellent and the ontol-
ogy contains the terms eating place and food  as 
a linguistic realization of the concepts restau-
rant and food, then this step allows the extrac-
tion from the EDU ?really good restaurant with 
excellent food?? the couples (restaurant, really 
good) and (food, excellent). This example is 
quite simple but in many cases, features and 
opinion words are not close to each other which 
make the link difficult to find. Actually, our 
system deals with conjunctions (including co-
mas) as in: ?I recommend pizzas and ice 
creams?, ?very good restaurant but very expen-
sive?  
 
Case 2. Known features and unknown opinion 
expressions, as in the EDU ?acceptable prices? 
where the opinion word acceptable has not been 
extracted in step 1 (cf. section 3.1). In this case, 
the opinion lexicon can be automatically up-
dated with the retrieved opinion word. 
 
Case 3. Unknown features and known opinion 
expressions, as in the EDU ?old fashion restau-
rant? where the features fashion has not been 
extracted in step 2 (cf. section 3.2). In this case, 
the domain ontology can be updated by adding a 
new label to an existing concept or property or 
by adding a new concept or a new property in 
the right place to the ontology. However, since a 
user may express an opinion on different objects 
within a review, this step has to be done care-
fully. To avoid errors, we propose to manually 
update the ontology.  
 
Case 4. Opinion expressions alone, as in the 
EDU ?It?s slow, cold and not good?. This kind 
of EDU expresses an implicit feature. In this 
case, we use the ontology properties in order to 
retrieve the associated concept in the ontology. 
For example, in the sentence ?we eat very well?, 
the property ?eat? of the ontology which links 
?customer? and ?food? will allow the system to 
determine that ?very well? refers to ?food?. 
 
Case 5. Features alone, as in the EDU: ?Nice 
surrounding on sunny days with terrace?, even 
if the feature ?terrace? is not associated to any 
opinion word, it is important to extract this in-
formation because it gives a positive opinion 
towards the restaurant. An EDU with features 
alone can also be an indicator of the presence of 
an implicit opinion expression towards the fea-
ture as in this restaurant is a nest of tourists 
 
Actually, our system deals with all these cases 
except the last one.  
4 Case study : mining restaurant re-
views 
In this section, we present the experiments we 
carried out on a case study: French restaurant 
reviews.  
4.1 Corpus 
For our experiments, we use a corpus of 58 
restaurant reviews (40 positive reviews and 18 
negatives reviews, for a total of 4000 words) 
extracted from the web site Qype2. Each review 
contains around 70 words and is composed of 
free comments on restaurants (but also on other 
objects like pubs, cinemas, etc.) with a lot of 
typos and syntactic errors. Each review appears 
in the web site with additional information such 
as the date of the review, the user name of the 
holder and a global rate from 1 (bad review) to 
5 (very good review). In this experiment, we 
only use the textual comments posted. Figure 2 
shows an example of a review form our corpus. 
 
                                                 
2
 http://www.qype.fr 
82
  
Figure 2. Example of a restaurant review 
 
4.2 Ontology 
Since our aim is to study the role of a domain 
ontology to feature based opinion mining, we 
choose to reuse an existing ontology. However, 
for the restaurant domain, we do not find any 
public available ontology for French. We thus 
use a pre-existent ontology 3  for English as a 
basis coupled with additional information that 
we gather from several web sites 4 . We first 
translate the existing ontology to French and 
then adapt it to our application by manually re-
organize, add and delete concepts in order to 
describe important restaurant features. Dispari-
ties between our ontology and the one we found 
in the web mainly come from cultural consid-
erations. For example, we do not found in the 
English ontology concepts like terrace. 
Our domain ontology has been implemented 
under Prot?g?5 and actually contains 239 con-
cepts (from which we have 14 concepts directly 
related to the superclass owl:think), 36 object 
properties and 703 labels (646 labels for con-
cepts and 57 labels for properties). The left part 
of figure 3 shows an extract of our restaurant 
domain ontology.  
4.3 Opinion Lexicon 
Our lexicon contains a list of opinion terms 
where each lexical entry is of the form:  
[POS, opinion category, polarity, strength] 
where POS is the part of speech tagging of the 
term, opinion category can be a judgment, a 
sentiment or an advice (see (Asher et al 2009) 
for a detailed description of these categories), 
polarity and strength corresponds respectively 
to the opinion orientation (positive, negative 
and neutral) and the opinion strength (a score 
between 0 and 2). For example, we have the 
following entry for the term good: [Adj, judg-
ment, +, 1]. 
                                                 
3
 http://gaia.fdi.ucm.es/ontologies/restaurant.owl 
4
 http://www.kelrestaurant.com/dept/31/ and 
http://www.resto.fr/default.cfm 
5
 http://protege.stanford.edu/  
 The lexicon actually contains 222 adjectives, 
152 nouns, 157 verbs. It is automatically built 
following the algorithm described in (Chardon, 
2010). We then add manually to this lexicon 98 
adverbs and 15 expressions of negation.  
 
 
    
 
Figure 3. Extract of the restaurant domain 
ontology : Left - hierarchy of concepts and 
labels of ?decoration? concept. Right ? in-
formation about a particular object property. 
 
4.4 Experiments 
We conduct three types of experiment: the 
evaluation of the extraction of elementary opin-
ion units (cf. section 3.1), the evaluation of the 
features extraction step (cf. section 3.2) and fi-
nally, the evaluation of the link between the re-
trieved opinion expressions and the retrieved 
object features (cf. section 3.3).  
These experiments are carried out using   
GATE 6  toolkit.  To evaluate our system, we 
create a gold standard by manually annotate in 
the corpus implicit and explicit elementary 
opinion units, implicit and explicit object fea-
tures as well as for each opinion expression its 
associated feature.  
 
Evaluation of the EOU extraction step. 
The table below shows our results. Our system 
misses some EOU for two main reasons. The 
first one is due to missed opinion words in the 
lexicon and to implicit opinion expressions, 
such as breathtaking, since our extraction rules 
do not manage these cases (note that implicit 
opinion detection is still an open research prob-
lem in opinion mining).  
                                                 
6
 http://gate.ac.uk/ 
83
The second reason is the errors that come from 
the syntactic parser mainly because of typos and 
dependency link errors. Concerning precision, 
false positives are mainly due to some opinion 
words that are in our lexicon but they do not 
express opinions in the restaurant domain. In 
addition, some of our extraction rules, espe-
cially those that extract expression of recom-
mendations, do not perform very well which 
imply a loss of precision.  
 
Precision 0,7486 
Recall 0,8535 
F-measure 0,7976 
 
Table 1. Evaluation of EOU extraction 
 
 
Evaluation of the features extraction step. 
Since the corpus is in the restaurant domain, the 
precision of this task is very good because most 
of the extracted features are relevant. However, 
recall is not as good as precision because the set 
of ontology labels do not totally cover the terms 
of the corpus. Another limitation of our system 
is that we do not take into account the cases 
where a term can be a linguistic realization of 
many concepts (ex. caf? can be a drink or a 
place to drink).  
Figure 4 shows an example of the result we ob-
tain for this step. 
 
 
 
Figure 4. Result of EOU (blue) and 
ontological term (pink) extraction 
 
Evaluation of the link between EOU and fea-
tures. 
The figure below shows our result on a sample. 
In this example, the system is able to extract 
opinion expressions which do not contain words 
present in the lexicon. It is the case with ?sympa 
(nice)? which has been correctly associated to 
?resto (restaurant)? and ?deco (interior de-
sign)? even if the word nice was not in the lexi-
con.  
In order to evaluate the added value of using an 
ontology to feature based opinion mining, we 
compare our system to the well known ap-
proaches of Hu and Liu and Popescu and Etzi-
oni (cf. section 2.1) that do not use any knowl-
edge representation. We have also compared 
our approach to those that use taxonomies of 
concepts by deleting the properties of our do-
main ontology. The results are shown in table 2. 
 
 
 
Figure 5. Result of linking EOU to extracted 
features 
 
 
 Precision Recall  F-measure 
Our sys-
tem 
0,7692 0,7733  0,7712 
Hu and 
Liu 
0,6737 
   
0,7653 0,7166 
Popescu 
and al 
0,7328   0,7387 0,7357 
Taxon-
omy 
0,7717   0,7573 0,7644 
 
Table 2. Evaluation of our system and its 
comparison to existing approaches 
 
In the Hu and Liu approach, features are nomi-
nal groups. We first extract all frequent features 
from our corpus that appear in more than 1% of 
the sentences. Then we extract EOU from those 
sentences (note that contrary to Hu and Liu, we 
do not extract only adjectives, but also nouns, 
verbs and adverbs). Non frequent features are 
finally removed as described in (Hu and Liu, 
2004). In order to improve the extraction of 
relevant features, we extract features that have a 
good point mutual information value with the 
word restaurant, as described in (Popescu and 
Etzioni, 2005). The precision of our system is 
better compared to the approach of Hu and Liu 
that extracts too many irrelevant features (such 
as any doubt, whole word). Our system is also 
better compared to the PMI approach even if it 
performs better than Hu and Liu?s approach. 
Recall is also better because our system can ex-
tract implicit features such as well eating,  lot of 
noise,  thanks to the use of ontology properties.   
Finally, when using only taxonomy of concepts 
instead of the ontology, we observe that the F-
measure is slightly better because actually fea-
84
tures related to object properties represent only 
1,6% of feature cases in our corpus. Using, the 
ontology, our approach is able to extract from 
sentences like "we eat good and healthy" the 
couples (eat, good) and (eat, healthy) and then 
to link the opinion expressions to the concept 
dish whereas when using only the taxonomy, 
these opinion expressions are related to any fea-
ture. 
5 Conclusion and prospects 
5.1 Contribution of our system 
Our method is promising because the use of the 
ontology allows to improve the feature extrac-
tion and the association between an opinion ex-
pressions and object features. On the one hand, 
the ontology is useful thanks to its concept list 
which brings a lot of semantic data in the sys-
tem. Using concept labels the ontology allows 
to recognize terms which refer to the same con-
cepts and brings some hierarchy between these 
concepts. On the other hand, the ontology is 
useful thanks to its list of properties between 
concepts which allows recognizing some opin-
ions expressed about implicit features.  
 
5.2 Prospects 
Opinion lexicon improvement.  
The opinion extraction we achieved is naive 
because we use a simple opinion word lexicon 
which is not perfectly adapted to the domain. To 
improve this part of the treatment, it would be 
interesting to use opinion ontology. As illus-
trated in section 2.2, constructing a domain on-
tology for the purpose of opinion mining poses 
several interesting questions in term of knowl-
edge representation, such as: what are the fron-
tiers between knowledge, where concepts are 
domain dependent, and opinion, where expres-
sions can be at the same time dependent (the 
term long can be positive for a battery life but 
negative if it refers to a the service of a restau-
rant) and independent (the term good is posi-
tive) from a domain. Our intuition is that the 
two levels have to be separated as possible.  
 
Natural Language processing (NLP) rules 
improvement.  
Our system is limited by some current NLP 
problems. For example, the system does not 
treat the anaphora. For example, in the sentence 
?Pizzas are great. They are tasty, original and 
generous?, it does not recognize that the three 
last adjectives refer to ?pizzas?.  There is also 
the problem of conditional proposition. For ex-
ample, in the sentence ?affordable prices if you 
have a fat wallet?, the system is not able to de-
termine that ?affordable prices? is subject to a 
condition. 
  
Ontology and lexicon enrichment. 
 Thanks to the ability to link opinion expression 
and ontological term extractions, our system is 
able to extract some missing opinion words and 
labels of the ontology. We think it could be in-
teresting to implement a module which allows 
the user to easily enrich opinion word lexicon 
and ontology. Furthermore, it will be interesting 
to evaluate the benefit of this method in both 
opinion mining and ontological domains.   
 
Towards a discourse based summary.  
The last step of the system is to produce a sum-
mary of the review that presents to the user all 
the opinion expressions associated to the main 
topic and all its features. This summary does not 
pretend to aggregate opinions for each feature 
or for the global topic. Instead, the aim is to or-
ganize the opinions of several reviews about 
one restaurant in order to allow the user to 
choose what feature is important or not for him. 
In addition to this kind of summarization, we 
want to investigate how the domain ontology 
can be used to guide the process of identifying 
the most relevant discourse relations between 
elementary discourse units (EDU).  Actually, 
the automatic identification of discourse rela-
tions that hold between EDUs is still an open 
research problem. Our idea is that there is con-
tinuation relation between EDU that contain 
terms that refer to concepts which are at the 
same level of the ontology hierarchy, and there 
is an elaboration relation when EDU contains 
more specific concepts than those of the previ-
ous clause. 
References 
Afantenos Stergos, Denis Pascal, Muller Philippe, 
Danlos Laurence. Learning Recursive Segments 
for Discourse Parsing. LREC 2010 
85
Asher, Nicholas, Farah Benamara, and Yvette Y. 
Mathieu. 2009. Appraisal of Opinion Expressions 
in Discourse. Lingvistic? Investigationes, John 
Benjamins Publishing Company, Amsterdam, 
Vol. 32:2. 
Asher Nicholas and Lascarides Alex. Logics of Con-
versation. Cambridge University Press, 2003 
BlairGoldensohn, Sasha, Kerry Hannan, Ryan 
McDonald, Tyler Neylon, George A. Reis, and 
Jeff Reynar. 2008. Building a Sentiment Summar-
izer for Local Service Reviews. WWW2008 
Workshop : Natural Language Processing Chal-
lenges in the Information Explosion Era (NLPIX 
2008). 
Carenini, Giuseppe, Raymond T. Ng, and Ed Zwart. 
2005. Extracting Knowledge from Evaluative 
Text. In Proceedings of the 3rd international con-
ference on Knowledge captur. 
Chardon Baptiste. Cat?gorisation automatique d?adjectifs 
d?opinion ? partir d?une ressource linguistique g?n?ri-
que. In proceedings of RECITAL 2010, Montreal, Ca-
nada 
Pimwadee Chaovalit, Lina Zhou: Movie Review 
Mining: a Comparison between Supervised and 
Unsupervised Classification Approaches. HICSS 
2005 
Cheng, Xiwen, and Feiyu Xu. 2008. Fine-grained 
Opinion Topic and Polarity Identification. In Pro-
ceedings of the Sixth International Language Re-
sources and Evaluation (LREC' 08), Marrakech, 
Morocco. 
Feiguina, Olga. 2006. R?sum? automatique des 
commentaires de Consommateurs. M?moire pr?-
sent? ? la Facult? des ?tudes sup?rieures en vue de 
l?obtention du grade de M.Sc. en informatique, 
D?partement d?informatique et de recherche op?-
rationnelle, Universit? de Montr?al. 
Gamon, Michael, Anthony Aue, Simon Corston-
Oliver, and Eric Ringger. 2005. Pulse: Mining 
Customer Opinions from Free Text. In Proceed-
ings of International symposium on intelligent 
data analysis N?6, Madrid. 
Hu, Minqing, and Bing Liu. 2004. Mining and Sum-
marizing Customer Reviews. In Proceedings of the 
10th ACM SIGKDD international conference on 
Knowledge discovery and data mining. 
Kim, Soo-Min, and Eduard Hovy. 2006. Extracting 
Opinions, Opinion Holders, and Topics Expressed 
in Online News Media Text. In Proceedings of 
ACL/COLING Workshop on Sentiment and Sub-
jectivity in Text, Sydney, Australia. 
Pang, Bo, Lillian Lee, and Shivakumar Vaithyana-
than. 2002. Thumbs up? Sentiment Classification 
using Machine Learning Techniques. Proceedings 
of EMNLP 2002. 
Popescu, Ana-Maria, and Oren Etzioni. 2005. Ex-
tracting Product Features and Opinions from Re-
views. In Proceedings of the conference on Hu-
man Language Technology and Empirical Meth-
ods in Natural Language Processing.  
Read, Jonathon,  David Hope, and John Carroll. 
2007.  Annotating Expressions of Appraisal in 
English. The Linguistic Annotation Workshop, 
ACL 2007. 
Strapparava, Carlo, and Alessandro Valitutti. 2004. 
WordNet-Affect: an Affective Extension of Word-
Net. Proceedings of LREC 04. 
Turney, Peter D. 2002. Thumbs Up or Thumbs 
Down? Semantic Orientation Applied to Unsuper-
vised Classification of Reviews. Proceedings of 
2006 International Conference on Intelligent User 
Interfaces (IUI06). 
Turney, Peter D., and Michael L. Littman. 2002. 
Unsupervised Learning of Semantic Orientation 
from a Hundred-Billion-Word Corpus. National 
Research Council, Institute for Information Tech-
nology, Technical Report ERB-1094. (NRC 
#44929) 
Wiebe, Janyce, Theresa Wilson, and Claire Cardie. 
2005. Annotating Expressions of Opinions and 
Emotions in Language. Language Resources and 
Evaluation 1(2). 
Zhao, Lili, and Chunping Li. 2009. Ontology Based 
Opinion Mining for Movie Reviews. In Proceed-
ings of the 3rd International Conference on 
Knowledge Science, Engineering and Manage-
ment. 
 
 
 
 
 
86
Proceedings of the SIGDIAL 2011: the 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 204?215,
Portland, Oregon, June 17-18, 2011. c?2011 Association for Computational Linguistics
Commitments to Preferences in Dialogue
Anais Cadilhac*, Nicholas Asher*, Farah Benamara*, Alex Lascarides**
*IRIT, University of Toulouse, **School of Informatics, University of Edinburgh
Abstract
We propose a method for modelling how dialogue
moves influence and are influenced by the agents?
preferences. We extract constraints on preferences
and dependencies among them, even when they are
expressed indirectly, by exploiting discourse struc-
ture. Our method relies on a study of 20 dia-
logues chosen at random from the Verbmobil cor-
pus. We then test the algorithms predictions against
the judgements of naive annotators on 3 random un-
seen dialogues. The average annotator-algorithm
agreement and the average inter-annotator agree-
ment show that our method is reliable.
1 Introduction
Dialogues are structured by various moves that the
participants make?e.g., answering questions, asking
follow-up questions, elaborating prior claims, and so
on. Such moves come with commitments to certain at-
titudes such as intentions and preferences. While map-
ping utterances to their underlying intentions is well
studied through the application of plan recognition tech-
niques (e.g., Grosz and Sidner (1990), Allen and Litman
(1987)), game-theoretic models of rationality generally
suggest that intentions result from a deliberation to find
the optimal tradeoff between one?s preferences and one?s
beliefs about possible outcomes (Rasmusen, 2007). So
mapping dialogue moves to preferences is an important
task: for instance, they are vital in decisions on how to
re-plan and repair should the agents? current plan fail, for
they inform the agents about the relative importance of
their various goals. Classical game theory, however, de-
mands a complete and cardinal representation of prefer-
ences for the optimal intention to be defined. This is not
realistic for modelling dialogue because agents often lack
complete information about preferences prior to talking:
they learn about the domain, each other?s preferences and
even their own preferences through dialogue exchange.
For instance, utterance (1) implies that the speaker wants
to go to the mall given that he wants to eat, but we do not
know his preferences over ?go to the mall? if he does not
want to eat.
(1) I want to go to the mall to eat something.
Existing formal models of dialogue content either do not
formalise a link between utterances and preferences (e.g.,
Ginzburg (to appear)), or they encode such links in a
typed feature structure, where desire is represented as a
feature that takes conjunctions of values as arguments
(e.g., Poesio and Traum (1998)), making the language
too restricted to express dependencies among preferences
of the kind we just described. Existing implemented
dialogue systems likewise typically represent goals as
simple combinations of values on certain information
?slots? (e.g., He and Young (2005), Lemon and Pietquin
(2007)); thus (1) yields a conjunction of preferences, to
go to the mall and to eat something. But such a system
could lead to suboptimal dialogue moves?e.g., to help
the speaker go to the mall even if he has already received
food.
What?s required, then, is a method for extracting par-
tial information about preferences and the dependencies
among them that are expressed in dialogue, perhaps indi-
rectly, and a method for exploiting that partial informa-
tion to identify the next optimal action. This paper pro-
poses a method for achieving these tasks by exploiting
discourse structure.
We exploited the corpus of Baldridge and Lascarides
(2005a), who annotated 100 randomly chosen sponta-
neous face-to-face dialogues from the Verbmobil cor-
pus (Wahlster, 2000) with their discourse structure ac-
cording to Segmented Discourse Representation Theory
(SDRT, Asher and Lascarides (2003))?these structures
represent the types of (relational) speech acts that the
agents perform. Here?s a typical fragment:
(2) a. A: Shall we meet sometime in the next
week?
b. A: What days are good for you?
c. B: Well, I have some free time on almost
every day except Fridays.
204
d. B: In fact, I?m busy on Thursday too.
e. A: So perhaps Monday?
Across the corpus, more than 30% of the discourse units
are either questions or assertions that help to elaborate a
plan to achieve the preferences revealed by a prior part
of the dialogue?these are marked respectively with the
discourse relations Q-Elab and Plan-Elab in SDRT, and
utterances (2b) and (2e) and the segments (2c) and (2d)
invoke these relations (see Section 2). Moreover, 10% of
the moves revise or correct prior preferences (like (2d)).
We will model the interaction between dialogue con-
tent and preferences in two steps. The first maps ut-
terances and their rhetorical connections into a partial
description of the agents? preferences. The mapping is
compositional and monotonic over the dialogue?s logi-
cal form (i.e., the description of preferences for an ex-
tended segment is defined in terms of and always sub-
sumes those for its subsegments): it exploits recursion
over discourse structure. The descriptions partially de-
scribe ceteris paribus preference nets or CP-nets with
Boolean variables (Boutilier et al, 2004). We chose CP-
nets over alternative logics of preferences, because they
provide a compact, computationally efficient, qualitative
and relational representation of preferences and their de-
pendencies, making them compatible with the kind of
partial information about preferences that utterances re-
veal. Our mapping from the logical form of dialogue
to partial descriptions of Boolean CP-nets proceeds in a
purely linguistic or domain independent way (e.g., it ig-
nores information such as Monday and Tuesday cannot
co-refer) and will therefore apply to dialogue generally
and not just Verbmobil.
In a second stage, we ?compress? and refine our descrip-
tion making use of constraints proper to CP-nets (e.g.,
that preference is transitive) and constraints provided by
the domain?in this case constraints about times and
places, as well as constraints from deep semantics. This
second step reduces the complexity of inferring which
CP-net(s) satisfy the partial description and allows us to
identify the minimal CP-net that satisfies the domain-
dependent description of preferences. We can thus ex-
ploit dependencies between dialogue moves and mental
states in a compact, efficient and intuitive way.
We start by motivating and describing the semantic repre-
sentation of dialogue from which our CP-net descriptions
and then our CP-nets will be constructed.
2 The Logical Form of Dialogue
Our starting point for representing dialogue con-
tent is SDRT. Like Hobbs et al (1993) and
Mann and Thompson (1987), it structures discourse
into units that are linked together with rhetorical re-
lations such as Explanation, Question Answer Pair
(QAP), Q-Elab, Plan-Elab, and so on. Logical forms
in SDRT consist of Segmented Discourse Representation
Structures (SDRSs). As defined in Asher and Lascarides
(2003), an SDRS is a set of labels representing discourse
units, and a mapping from each label to an SDRS-formula
representing its content?these formulas are based on
those for representing clauses or elementary discourse
units (EDUs) plus rhetorical relation symbols between
labels. Lascarides and Asher (2009) argue that to make
accurate predictions about acceptance and denial, both
of which can be implicated rather than linguistically
explicit, the logical form of dialogue should track each
agent?s commitments to content, including rhetorical
connections. They represent a dialogue turn (where turn
boundaries occur whenever the speaker changes) as a
set of SDRSs?one for each agent representing all his
current commitments, from the beginning of the dialogue
to the end of that turn. The representation of the dialogue
overall?a Dialogue SDRS or DSDRS?is that of each of
its turns. Each agent constructs the SDRSs for all other
agents as well as his own. For instance, (2) is assigned
the DSDRS in Table 1, with the content of the EDUs
omitted for reasons of space (see Lascarides and Asher
(2009) for details). We adopt a convention of indexing
the root label of the nth turn, spoken by agent d, as
nd; and pi : ? means that ? describes pi?s content (we?ll
sometimes also write ?pi to identify this description).
We now return to our example (2). Intuitively, (2a) com-
mits A to a preference for meeting next week but it does
so indirectly: the preference is not asserted, or equiva-
lently entailed at the level of content from the semantics
of Q-Elab(a,b). Accordingly, responding with "I do too"
(meaning "I want to meet next week too") is correctly pre-
dicted to be highly anomalous. A?s SDRS for turn 1 in Ta-
ble 1 commits him to the questions (2a) and (2b) because
Q-Elab is veridical: i.e. Q-Elab(a,b) entails the dynamic
conjunction ?a ??b. Since intuitively (2a) commits A to
the implicature that he prefers next week, our algorithm
for eliciting preferences from dialogue must ascribe this
preference to A on the basis of his move Q-Elab(a,b).
Furthermore,Q-Elab(a,b) entails that any answer to (2b)
must elaborate a plan to achieve the preference revealed
by (2a); this makes ?b paraphrasable as ?What days next
week are good for you??, which does not add new prefer-
ences.
B?s contribution in the second turn attaches to (2b) with
QAP and also Plan-Elab?he answers with a non-empty
extension for what days. Lascarides and Asher (2009) ar-
gue that this means that B is also committed to the illo-
cutionary contribution of (2b), as shown in Table 1 by
the addition of Q-Elab(a,b) to B?s SDRS. This addition
commits B also to the preference of meeting next week,
with his answer making the preferencemore precise: (2c)
reveals that B prefers any day except Friday; by linking
(2d) with Plan-Correction he retracts the preference for
Thursday. This compels A to revise his inferences about
205
Turn A?s SDRS B?s SDRS
1 pi1A : Q-Elab(a,b) /0
2 pi1A : Q-Elab(a,b) pi2B : Q-Elab(a,b)?QAP(b,pi)?Plan-Elab(b,pi)
pi : Plan-Correction(c,d)
3 pi3A : Q-Elab(a,b)?QAP(b,pi)?Plan-Elab(b,pi)? pi2B : Q-Elab(a,b)?QAP(b,pi)?Plan-Elab(b,pi)
Plan-Elab(pi,e) pi : Plan-Correction(c,d)
Table 1: The DSDRS for Dialogue (2).
B?s preference for meeting on Thursday. A?s Plan-Elab
move (2e) in the third turn reveals another preference for
Monday. This may not match his preferred day when the
dialogue started: perhaps that was Friday. He may con-
tinue to prefer that day. But engaging in dialogue can
compel agents to revise their commitments to preferences
as they learn about the domain and each other.
The above discussion of (2) exhibits how different types
of rhetorical relations between utterances rather than
Searle-like speech acts like question, construed as a prop-
erty of an utterance, are useful for encoding how pref-
erences evolve in a dialogue and how they relate to
one another. While the Grounding Acts dialogue model
(Poesio and Traum, 1998) and the Question Under Dis-
cussion (QUD) model (Ginzburg, to appear) both have
many attractive features, they do not encode as fine-
grained a taxonomy of types of speech acts and their se-
mantic effects as SDRT: in SDRT each rhetorical relation
is a different kind of (relational) speech act, so that, for
instance, the speech act of questioning is divided into the
distinct types Q-Elab, Plan-Correction, and others. For
the QUD model to encode such relations would require
implicit questions of all sorts of different types to be in-
cluded in the taxonomy, in which case the result may be
equivalent to the SDRT taxonomy of dialogue moves. We
have not explored this eventual equivalence here.
3 CP-nets and CP-net descriptions
A preference is standardly understood as an ordering by
an agent over outcomes; at the very least it entails a com-
parison between one entity and another (outcomes being
one sort of entity among others). As indicated in the in-
troduction, we are interested in an ordinal definition of
preferences, which consists in imposing an ordering over
all (relevant) possible outcomes. Among these outcomes,
some are acceptable for the agent, in the sense that the
agent is ready to act in such a way as to realize them;
and some outcomes are not acceptable. Amongst the ac-
ceptable outcomes, the agent will typically prefer some
to others. Our method does not try to determine the most
preferred outcome of an agent but follows rather the evo-
lution of their commitments to certain preferences as the
dialogue proceeds. To give an example, if an agent pro-
poses to meet on a certain day X and at a certain time Y,
we infer that among the agent?s acceptable outcomes is a
meeting on X at Y, even if this is not his most preferred
outcome (see earlier discussion of (2e)).
A CP-net (Boutilier et al, 2004) offers a compact rep-
resentation of preferences. It is a graphical model that
exploits conditional preferential independence so as to
structure the decision maker?s preferences under a ceteris
paribus assumption.
Although CP-nets generally consider variables with a fi-
nite range of values, to define the mapping from dialogue
turns to descriptions of CP-nets in a domain indepen-
dent and compositional way, we use Boolean proposi-
tional variables: each variable describes an action that an
agent can choose to perform, or not. We will then refine
the CP-net description by using domain-specific informa-
tion, transforming CP-nets with binary valued variables
to CP-nets with multiple valued variables. This reduces
the complexity of the evaluation of the CP-net by a large
factor.
More formally, let V be a finite set of propositional vari-
ables and LV the description language built from V via
Boolean connectives and the constants ? (true) and ?
(false). Formulas of LV are denoted by ?,?, etc. 2V is the
set of interpretations for V , and as usual for M ? 2V and
x?V , M gives the value true to x if x?M and false other-
wise. Where X ?V , let 2X be the set of X-interpretations.
X-interpretations are denoted by listing all variables of
X , with a ? symbol when the variable is set to false: e.g.,
where X = {a,b,d}, the X-interpretation M = {a,d} is
expressed as abd.
A preference relation  is a reflexive and transitive bi-
nary relation on 2V with strict preference ? defined in
the usual way (i.e., M  M? but M? 6 M). Note that
preference orderings are not necessarily complete, since
some candidates may not be comparable by a given agent.
An agent is said to be indifferent between two options
M,M? ? 2V , written M ?M?, if M M? and M? M.
As we stated earlier, CP-nets exploit conditional pref-
erential independence to compute a preferential ranking
over outcomes:
Definition 1 Let V be a set of propositional variables
and {X ,Y,Z} a partition of V . X is conditionally pref-
erentially independent of Y given Z if and only if ?z ?
2Z , ?x1,x2 ? 2X and ?y1,y2 ? 2Y we have: x1y1z 
206
x2y1z iff x1y2z x2y2z.
For each variable X , the agent specifies a set of parent
variables Pa(X) that can affect his preferences over the
values of X . Formally, X is conditionally preferentially
independent of V \ ({X}?Pa(X)). This is then used to
create the CP-net.
Definition 2 Let V be a set of propositional variables.
N = ?G ,T ? is a CP-net on V , where G is a directed
graph over V , and T is a set of Conditional Preference
Tables (CPTs) with indifference. That is, T = {CPT(X j):
X j ? V}, where CPT(X j) specifies for each instantiation
p ? 2Pa(X j) either x j ?p x j, x j ?p x j or x j ?p x j.
The following simple example illustrates these defini-
tions. Suppose our agent prefers to go from Paris to
Hong Kong by day rather than overnight. If he takes an
overnight trip, he prefers a non stop flight, but if he goes
by day, he prefers a flight with a stop. Figure 1 shows the
associated CP-net. The variable T stands for the prefer-
ence over the period of travel. Its values are Td for a day
trip and Tn for a night one. The variable St stands for the
preference over stops. Its values are S for a trip with stops
and S without.
T
St
CPT(T) = Td ? Tn
CPT(St) = Td : S ? S
Tn : S? S
Figure 1: Travel CP-net
With CP-nets defined, we proceed to a description lan-
guage for them. The description language formula w ?
y(CPT ) describes a CP-net where a CPT contains an en-
try of the form w ?p y for some possibly empty list of
parent variables p. A CP-net description is a set of such
formulas. The CP-net N |= x1, . . .xn : w? y(CPT ) iff the
CP-net N ?s CPT T contains an entry w?~u y?also writ-
ten~u :w? y?where x1, . . .xn figure in~u. Satisfaction of a
description formula by a CP-net yields a notion of logical
consequence between a CP-net descriptionD N and a de-
scription formula in the obvious way. Dialogue turns also
sometimes inform us that certain variables enter into pref-
erence statements. We?ll express the fact that the vari-
ables x1, . . . ,xn are associated with discourse constituent
pi by the formula x1, . . . ,xn(P(pi)), where P(pi) refers to
the partial description of the preferences expressed by the
discourse unit pi (see Section 4).
The description language allows us to impose constraints
on the CP-nets that agents commit to without specifying
the CP-net completely, as is required for utterances like
(1). In section 6, we describe how to construct a min-
imal CP-net from a satisfiable CP-net description. One
can then use the forward sweep procedure for outcome
optimisation (Boutilier et al, 2004). This is a proce-
dure of linear complexity, which consists in instantiating
variables following an order compatible with the graph,
choosing for each variable (one of) its preferred values
given the value of the parents.
4 From EDUs to Preferences
EDUs are described in SDRT using essentially Boolean
formulas over labels (Asher and Lascarides, 2003); thus
?(pi)??(pi) means that ? and ? describe aspects of pi?s
content. Not(pi1,pi)? ?(pi1) means that the logical form
of the EDU pi is of the form ?pi1 and that pi1 is described
by ?; so pi has the content ??. Our task is to map such
descriptions of content into descriptions of preferences.
Our preference descriptions will use Boolean connectives
and operators over preference entries (e.g., of the form
x ? y): namely, &,?, 7?, and a modal operator ?. The
rules below explain the semantics of preference opera-
tors (they are in effect defined in terms of the semantics
of buletic attitudes and Boolean connectives) and how
to recursively calculate preference descriptions from the
EDU?s logical structure.
Simple EDUs can provide atomic preference statements
(e.g., I want X or We need X). This means that with this
EDU the speaker commits to a preference for X . X will
typically involve a Boolean variable and a preference en-
try for its CPT. P(pi) is the label of the preference descrip-
tion associated with discourse unit pi. Hence for a sim-
ple EDU pi, we have X(P(pi)) as its description. Simple
EDUs also sometimes express preferences in an indirect
way (see (2a)).
More generally, P recursively exploits the logical struc-
ture of an EDU?s logical form to produce an EDU pref-
erence representation (EDUPR). For instance, since the
logical form of the EDU I want fish and wine features
conjunction, likewise so does its preference description:
?&?(P(pi)) means that among the preferences included
in pi, the agent prefers to have both ? and ? and prefers ei-
ther one if he can?t have both.1 We also have disjunctions
(let?s meet Thursday or Friday), and negations (I don?t
want to meet on Friday), whose preferences we?ll express
respectively as Thurs?Fri(P(pi)) and ?Fri(P(pi)).
Some EDUs express commitments to dependencies
among preferences. For example, in the sentence What
about Monday, in the afternoon?, there are two prefer-
ences: one for the day Monday, and, given the Monday
preference, one for the time afternoon (of Monday), at
least on one syntactic disambiguation. We represent this
dependency as Mon 7? Aft(P(pi)). Note that 7? is not
expressible with just Boolean operators. Finally, EDUs
can express commitment to preferences via free choice
1The full set of rules also includes a stronger conjunction ???(P(pi))
(the agent prefers both ? and ?, but is indifferent if he can?t have both).
207
modalities; I am free on Thursday, or?Thurs(P(pi)), tells
us that Thursday is a possible day to meet. ?? says that ?
is an acceptable outcome (as described earlier, this means
the agent is ready to act so as to realize an outcome that
entails ?). Thus, ??(pi) entails ?(pi), and ?-embedded
preferences obey reduction axioms permitting ? to be
eliminated when combined with other preference oper-
ators. But a ? preference statement does affect a prefer-
ence description when is is conjoined in Boolean fashion
with another ? preference statement in an EDU or com-
bined via a discourse relation like Continuation. This is
because ? is a free choice modality and obeys the equiv-
alence (3) below, which in turn yields a disjunctive pref-
erence ???(P(pi)) from what appeared to be a conjunc-
tion.2
(3) (??(P(pi))???(P(pi)))??(???)(P(pi))
The variables introduced by a discourse segment pi are
integrated into the CP-net description D N via the oper-
ation Commit(pi,D N ). The following seven rules cover
the different possible logical structures for the EDU pref-
erence representation. In the following, X ,Y,Z,W denote
propositional variables and ?, ? propositional formulas
from EDUPR. Var(?) are the variables in ?, and ?X
the preference relation describing CPT (X). Sat(?) (or
non-Sat(?)) is a conjunction of literals from Var(?) that
satisfy (or do not satisfy) ?. Sat(?)?X is the formula that
results from removing the conjunct with X from Sat(?).
1. Where X(P(pi)) (X is a variable of P(pi), e.g., I want
X), Commit(pi,D N ) adds the description D N |=
X ? X(CPT (X)).3
2. Where ?&?(P(pi)), Commit(pi,D N ) adds descrip-
tions as follows:
? For each X ?Var(?), addVar(?) to Pa(X) and
modifyCPT (X) as follows:
If Sati(?), Sat j(?) ? X (resp. X), then Sati(?),
Sat j(?)?X : X ? X (resp. X ? X), for all sat-
isfiers i and j.
? Similarly for each Y ?Var(?).
If ? and ? are literals X and Y we get: D N |= Y ?
Y (CPT (Y )) and D N |= X ? X(CPT (X)). Graph-
ically, this yields the following preference relation
(where one way arrows denote preference, two way
2We provide here the reduction axioms over preference descriptions
1. ?(?&?)(P(pi))? (?&?)(P(pi))
2. ?(? 7? ?)(P(pi))? (? 7? ?)(P(pi))
3. ?(???)? (?? ?)(P(pi))
4. ???(P(pi))???(P(pi))
3Given our description language semantics, this means that any
CP-net which satisfies the description D N contains a preference ta-
ble CPT (X) with an entry X ? X with at least one instantiation of the
variables in Pa(X).
arrows denote indifference or equal preference, and
no arrow means the options are incomparable):
XY
XY XY
XY
3. Where ?? ?(P(pi)) (the agent prefers to have at
least one of ? and ? satisfied). If ? and ? are X
and Y , we get:
? Var(X) ? Pa(Var(Y )) and D N |= X : Y ?
Y (CPT (Y )), D N |= X : Y ? Y (CPT (Y )).
? Var(Y ) ? Pa(Var(X)) and D N |= Y : X ?
X(CPT (X)), D N |= Y : X ? X(CPT (X)).
This corresponds to the following preference rela-
tion:
XY
XY XY XY
As before, the use of indifference allows us to find
the best outcomes (XY , XY and XY ) easily.
4. Where ? 7? ?(P(pi)) (the agent prefers that ? is sat-
isfied and if so that ? is also satisfied. If ? is not
satisfied, it is not possible to define preferences on
?). If ? and ? are X and Y , we get:
? D N |= X ? X(CPT (X))
? Var(X) ? Pa(Var(Y)) and
D N |= X : Y ? Y (CPT (Y )).
Note that this description is also produced by
Elab(pii,pi j) below where X(P(pii)) and Y (P(pi j))
(see rule 8). Thus the implication symbol 7? is a
"shortcut" in that it represents elaborations whose
arguments are in the same EDU.
5. Where ??(P(pi)) (the agent prefers a free choice of
?). Given the behaviour of?, this reduces to treating
?(P(pi)).
6. Where ??(P(pi)). We can apply rules 1-5 by con-
verting ?? into conjunctive normal form.
7. Where ?(P(pi))??(P(pi)), with ? and ? nonmodal,
we simply apply the rule for ? and that for ?.
5 From Discourse Structure to Preferences
We must now define how the agents? preferences, repre-
sented as a partial description of a CP-net, are built com-
positionally from the discourse structure over EDUs. The
constraints are different for different discourse relations,
reflecting the fact that the semantics of connections be-
tween segments influences how their preferences relate
to one another.
We will add rules for defining Commit over la-
bels pi whose content ?pi express rhetorical relations
R(pii,pi j)?indeed, we overload the notation and write
Commit(R(pii,pi j),D N ). Since Commit applies com-
positionally, starting with the EDUs and working up
208
the discourse structure towards the unique root la-
bel of the SDRS, we can assume in our definition of
Commit(R(pii,pi j),D N ) that the EDUPRs are already de-
fined. We give rules for all the relations in the Verbmobil
corpus, though we will be very brief with those that are
less prevalent. A complete example using our rules is in
appendix A.
IExplanation, Elab, Plan-Elab, Q-elab
IExplanation(pii, pi j): i.e., pi j?s preferences explain pii?s
(e.g., see (1), where P(pii) would be going to the mall
and P(pi j) is eating something). With Elab(pii, pi j) a
preference in pii is elaborated on or developed in pi j,
as in: I want wine. I want white wine. That is, a
preference for white wine depends on a preference for
wine. Plan-Elab(pii,pi j) means that pi j describes a plan
for achieving the preferences expressed by pii, and with
Q-Elab we have a similar dependence between prefer-
ences, but the second constituent is a question (so often
in practice this means preference commitments from pii
transfer from one agent to another).
Plan-Elab(pi j,pii), Elab(pi j,pii) and IExplanation(pii,pi j)
all follow the same two-step rule, and so from the point
of view of preference updates they are equivalent:
8. i Firstly, preference description D N is up-
dated according to P(pi j) by applying
Commit(pi j,D N ), if pi j expresses a new
preference. If not go to step (ii).
ii. Secondly, description D N is modified so that
each variable in P(pii) depends on each vari-
able in P(pi j): i.e., ?X ? Var(P(pii)), ?Y ?
Var(P(pi j)), Y ? Pa(X). Then, D N is enriched
according to P(pii), if pii expresses a preference.
If it does not, then end.
We now give some details concerning step (ii) above. To
this end, let ? denote a formula with SDRS description
predicates, ?? its corresponding boolean (preference) for-
mula and ?? its negation. Then for ?=Y , we define ??=Y
and ?? = Y ; for ? = Y 7? Z we define ?? = Y ? Z and
?? = Y ? Z; and for ? = Y ? Z and ? = Y&Z, we have
?? = Y ?Z and ?? = Y ?Z.
a. X(P(pii)) and ?(P(pi j)). The agent explains his pref-
erences on X by ?. So, if no preferences on X are
already defined, ? is a reason to prefer X . That is,
D N |= ??: X ? X(CPT (X)). However, it is not pos-
sible to define preferences on X if ? is false. If, on
the other hand, preferences on X are already defined,
the agent prefers X if ? is satisfied, and does not
modify his preferences otherwise?i.e.,?X ,??= X ?
X , ?X ,??=?X .4
4If we have ?X such that Z: X ? X , Z: X ? X , ?X ,?? represents
preferences defined by Z??? and Z???, whereas ?X ,?? represents pref-
erences defined by Z??? and Z???.
For ? = Y , if ?X is not already defined, we obtain
the following preference relation (no information on
the preference for X if Y is false makes XY and XY
incomparable):
XY
XYXY
XY
b. X?Z(P(pii)) and ?(P(pi j)). The agent explains his
preferences X?Z by ?: he wants to satisfy X or Z
if ? is satisfied.
First, we set Var(Z) ? Pa(Var(X)), Var(X) ?
Pa(Var(Z)). If ?X is not already defined, we have:
D N |= ?? ? Z: X ? X(CPT (X)), D N |= ?? ? Z:
X ? X(CPT (X)).
Otherwise, ?X ,??,Z= X ? X , ?X ,??,Z= X ? X ,
?X ,??,Z= ?X ,??,Z= ?X .
CPT (Z) is defined asCPT (X) by inverting X and Z.
For ? =Y , if ?X and?Z are not already defined, we
obtain the following preference relation (again, the
lack of preference information on X and Z when Y
is false yields incomparability among states whereY
is false):
XYZXYZ
XYZ
XYZ
XYZ
XYZ
XYZXYZ
c. X&Z(P(pii)) and ?(P(pi j)). The agent explains his
preferences on X&Z by ?.
? If ?X is not already defined, we have: D N |=
?? : X ? X(CPT (X)).
Otherwise, ?X ,??= X ? X , ?X ,??= ?X ,
? CPT (Z) is defined as CPT (X) by replacing X
by Z.
d. X 7? Z(P(pii)) and ?(P(pi j)). The agent explains his
preferences on X 7? Z by ?: he wants to satisfy X
and after Z if ? is satisfied.
If ?X is not already defined, we have D N |= ?? :
X ? X(CPT (X)) and we setVar(X)? Pa(Var(Z)).5
If ?Z is not yet defined, we have : D N |= ?? ?X :
Z ? Z(CPT (Z)), D N |= ???X : Z ? Z(CPT (Z)).
Else, ?Z,(???X)= Z ? Z, ?Z,(???X)= Z ? Z,
?Z,(???X)= ?Z,(???X)=?Z.
e. ?(P(pii)) and ?(P(pi j)). We can apply rules 8 by
decomposing ?.
5Otherwise, there is no need to modify ?X . This is what we call a
?partial elaboration?. Variables that were evoked since preferences on
X were introduced are parents of Z but not of X . For example, if an
agent commits to a preference for Monday then Afternoon, and later in
the discourse he commits to 2oclock, then Afternoon is 2oclock?s parent
but not Monday?s.
209
f. ?(?)(P(pii)) and ?(?)(P(pi j)). We treat this like a
free choice EDU (see rule 5).
g. ?(?)(P(pii)) and ?(P(pi j)), where ? is non modal.
We treat this like ?(P(pii)) and ?(P(pi j)) (see rule
8.e)
Let?s briefly look at how the rule changes for
Q-elabA(pi1,pi2) (where the subscript A identifies the
speaker of pi2):
9. Q-ElabA(pi1,pi2) implies that we update A?s CP-
net description D N by applying the rule for
Elab(pi1,pi2), where if pi2 expresses no preferences
on their own, we simply make the P(pi2) description
equal to the P(pi1) description. Thus A?s CP-net de-
scription is updated with the preferences expressed
by utterance pi1, regardless of who said pi1.
QAPAnswers to questions affect preferences in complex
ways:
10. The first case concerns yes/no questions and there
are two cases, depending on whether B replies yes
or no:
Yes QAPB(pi1,pi2) where pi2 is yes. B?s pref-
erence descriptions are updated by apply-
ing Commit(ElabB(pi1,pi2),D N ) (and so B?s
preference description include preferences ex-
pressed by pi1 and pi2).
No QAPB(pi1,pi2) where pi2 is no. If P(pi1)
and P(pi2) are consistent, then B?s pref-
erence descriptions are updated by ap-
plying CommitB(ElabB(pi1,pi2),D N );
otherwise, they are updated by applying
Commit(Correction(pi1,pi2),D N ) (see rule
13).
11. When pi1 is a wh-question and QAPB(pi1,pi2), B?s
preferences over variables in pi1 and pi2 are ex-
actly the same as the ones defined for a yes/no
question where the answer is yes. Variables in pi2
will refine preferences over variables in pi1. So,
B?s preference descriptions are updated by applying
CommitB(ElabB(pi1,pi2),D N ).
In previous rules, it is relatively clear how to update the
preference commitments. However, in some cases it?s not
clear what the answer in a QAP targets: in Could we meet
the 25 in the morning? No, I can?t., we do not know if
No is about the 25 and the morning, or only about the
morning. So, we define the following rule for managing
cases where the target is unknown :
12. If we know the target, we can change the description
of the CP-net. Otherwise, we wait to learn more.
Correction and Plan-Correction allow a speaker to rec-
tify a prior commitment to preferences. Self-corrections
also occur in the corpus: I could do it on the 27th. No I
can not make it on the 27th, sorry I have a seminar. Cor-
rection and Plan-Correction can have several effects on
the preferences. For instance, they can correct preference
entries. That is, given Correction(pi1,pi2), some variables
in P(pi1) are replaced by variables in P(pi2) (in the self-
correction example, every occurrence of 27 in P(pi1) is
replaced with 27 and vice versa). We have a set of rules
of the form X ?{Y1, . . . ,Ym}, which means that the vari-
able X ? Var(P(pi1)) is replaced by the set of variables
{Y1, . . . ,Ym} ? Var(P(pi2)). We assume that X can?t de-
pend on {Y1, . . . ,Ym} before the Correction is performed.
Then replacement proceeds as follows:
13. If Pa(X) = /0, we add the description D N |= Yk ?
Y k(CPT (Yk)) for all k ? {1, . . . ,m} and remove X ?
X(CPT (X)) (or X ? X(CPT (X))). Otherwise, we
replace every description ofCPT (X) with an equiv-
alent statement using Yk (to describe CPT (Yk)), for
all k ? {1, . . .m}.
The specific target of the correction behaves similarly to
the target of a QAP. In some cases we don?t know the
target, in which case we apply rule 12.
Plan-Correction can also lead to the modification of an
agent?s own plan because of other agent?s proposals. In
this case it corrects the list of parent variables on which
a preference depends. We call that list of variables the
operative variables. Once the operative variables are
changed, Plan-Correction can elaborate a plan if some
new preferences are expressed. For example, all agents
have agreed to meet next week, so in their CP-net descrip-
tion, there is the entry Week1?Week1. Then discussion
shows that their availabilities are not compatible and one
of them says "okay, that week is not going to work.". That
does not mean the agent prefersWeek1 toWeek1 because
both agreed on Week1 as preferable. Rather, Week1 has
been removed as an operative variable in the following
discourse segments. This leads us to the following rule:
14. For Plan-Correction(pi1,pi2) which corrects
the list of parent variables, the operative vari-
able list becomes the intersection of all Pa(X)
where X ? Var(P(pi1)). We can now apply
Commit(Plan-Elab(pi1,pi2),D N ), if P(pi2) contains
some new preferences ?. If the CPT affected by a
rule has no entry for the current operative variable
list O , then O : ? has to be added to D N .
Continuation, Contrast and Q-Cont pattern with the
rule for Elab. Alternation patterns with rule 8.b.6 Expla-
nation, Explanation*,Result, Qclar (clarification ques-
tion), Commentary, Summary and Acknowledgment
6The rule for Alternative questions like Do you want fish or chicken?
is a special case yielding ???(P(pi)), but we don?t offer details here.
210
either do nothing or have the same effect on preference
elicitation as Elab. Sometimes, adding these preferences
via the Elab rule may yield an unsatisfiable CP-net de-
scription, because an implicit correction is involved. If an
evaluation of the CP-net (see next section) is performed
after a processing of one of these rules shows that the
CP-net description is not satisfiable, then we apply the
rule 13, associated with Correction.
6 From Descriptions to Models
Each dialogue turn adds constraints monotonically to the
descriptions of the CP-nets to which the dialogue partic-
ipants commit. We have interpreted each new declared
variable in our rules as independent, which allows us to
give a domain independent description of preference elic-
itation. However, when it comes to evaluating a CP-net
description for satisfiability, we need to take into account
various axioms about preference (irreflexivity and transi-
tivity), and axioms for the domain of conversation: in our
case, temporal designations (Wednesdays are not Tues-
days and so on). This typically adds dependencies among
the variables in the description. In the case of the Verb-
mobil domain, since the variable Monday means essen-
tially "to meet on Monday", Monday implies Meet , and
this must be reflected via a dependency in the CP-net: we
must view the variable Meet as filling a hidden slot in
the variable Monday in the preference description, Meet :
Mon?Mon. This likewise allows us to fill in the negative
clauses of the CP-net description: we can now infer that
Meet : Mon ? Mon. These axioms also predict certain
preference descriptions to be unsatisfiable. For instance,
if we have Mon ?Mon, our axioms imply Mon ? Tues,
Mon ?Wed, etc. At this point we can calculate, ceteris
paribus, inconsistencies on afternoons and mornings of
particular days.
Domain knowledge also allows us to collapse Boolean
valued variables that all denote, say, days or times of the
day into multiple valued variables. So for instance, our
domain independent algorithm from dialogue moves to
preference descriptions might yield:
(4) Meet?31.01?30.01?02.02: am? am
Domain knowledge collapses all Boolean variables for
distinct days into one variable with values for days to get:
(5) Meet?02.02: am? pm
This leads to a sizeable reduction in the set of variables
that are used in the CP-net.
We can test any CP-net description for satisfiability by
turning the description formulas into CP-net entries. Our
description automatically produces a directed graph over
the parent variables. We have to check that the ? state-
ments form an irreflexive and transitive relation and that
each variable introduced into the CP-net has a preference
entry consistent given these constraints. If the description
does not yield a preference entry for a given variable X ,
we will add the indifference formula X ? X as the entry.
If our CP-net description meets these requirements, this
procedure yields a minimal CP-net. Testing for satisfia-
bility is useful in eliciting preferences from several dis-
course moves like Explanation, Qclar or Result, since in
the case of unsatisfiability, we will exploit the Correction
rule 13 with these moves.
7 Evaluation of the proposed method
We evaluate our method by testing it against the judg-
ments of three annotators on three randomly chosen un-
seen test dialogues from the Verbmobil corpus. The
test corpus contains 75 EDUs and the proportion of dis-
course relations is the same as in the corpus overall. The
three annotators were naive in the sense that they were
not familiar with preference representations and prefer-
ence reasoning strategies. For each dialogue segment,
we checked if the judges had the same intuitions that we
did on: (i) how commitments to preferences are extracted
from EDUs, and (ii) how preferences evolve through dia-
logue exchange.
The judges were given a manual with all the instructions
and definitions needed to make the annotations. For ex-
ample, the manual defined preference to be "a notion of
comparison between one thing at least one other". The
manual also instructs annotators to label each EDU with
the following four bits of information: (1) preferences
(if any) expressed in the EDU; (2) dependencies between
preferences expressed in the EDU; (3) dependencies be-
tween preferences in the current EDU and previous ones;
and (4) preference evolution (namely, the appearance of
a new factor that affects preferred outcomes, update to
preferences over values for an existing factor, and so on).
For each of these four components, example dialogues
were given for each type of decision they would need to
make, and instructions were given on the format in which
to code their judgements. Appendix A shows an example
of an annotated dialogue.
Table 2 presents results of the evaluation of (i). For each
EDU, we asked the annotator to list the preferences ex-
pressed in the EDU and we compared the preferences ex-
tracted by each judge with those extracted by our algo-
rithm. The triple (a, b, c) respectively indicates the pro-
portion of common preferences (two preference sets ?i
and ? j are common if (?i = ? j) or (?x ? ?i,y ? ? j ,x?
y)?for example, the preference MeetBefore2?MeetAt2
implies MeetAt2 ? MeetAt2), the proportion of prefer-
ences that one judge extracts and the other judge or our al-
gorithm misses and the proportion of preferences missed
by one judge and extracted by the other judge or by our
algorithm. The average annotator-algorithm agreement
(AAA) is 75.6% and the average inter-annotator agree-
211
Our algorithm J1 J2 J3 % of EDUs that commit to preferences
Our algorithm (83, 4, 13) (91, 0, 9) (91, 0, 9) 76%
J1 (83, 13, 4) (85, 7, 8) (91, 4, 5) 80%
J2 (91, 9, 0) (85, 8, 7) (92, 4, 4) 86%
J3 (91, 9, 0) (91, 5, 4) (92, 4, 4) 84%
Table 2: Evaluating how preferences are extracted from EDUs.
Our algorithm J1 J2 J3
Our algorithm (85, 71) (96, 100) (93, 86)
J1 (85, 71) (89, 71) (91, 86)
J2 (96, 100) (89, 71) (98, 86)
J3 (93, 86) (91, 86) (98, 86)
Table 3: Evaluating how preferences evolve through dialogue.
ment (IAA) is 77.9%; this shows that our method for ex-
tracting preferences from EDUs is reliable.
The evaluation (ii) proceeds as follows. For each EDU, we
ask the judge if the segment introduces new preferences
or if it updates, corrects or deletes preferences commited
in previous turns. As in (i), judges have to justify their
choices. Table 3 presents the preliminary results where
the couple (a,b) indicates respectively the proportion of
common elaborations (preference updates or new prefer-
ences) and the proportion of common corrections. Since
elaboration is also applied in case of other discourse re-
lations (e.g., Q-Elab), the measure a evaluates the rules
8, 9, 10 (yes) and 11. Similarly, the measure b evalu-
ates the rules 10 (no), 13 and 14. We obtain AAA=91%
IAA=92.7% for elaboration and AAA=85.7% IAA=81%
for correction.
8 Conclusion
We have proposed a compositional method for elicit-
ing preferences from dialogue consisting of a domain-
independent algorithm for constructing a partial CP-net
description of preferences, followed by a domain-specific
method for identifying the minimal CP-net satisfying the
partial description and domain constraints. The method
supports qualitative and partial information about prefer-
ences, with CP-nets benefiting from linear algorithms for
computing the optimal outcome from a set of preferences
and their dependencies. The need to compute intentions
from partially defined preferences is crucial in dialogue,
since preferences are acquired and change through dia-
logue exchange.
Our work partially confirms that CP-nets have a certain
naturalness, as the map from dialogue moves to prefer-
ences using the CP-net formalism is relatively intuitive.
The next step is to implement our method. This depends
on extracting discourse structure from text, which, though
difficult, is becoming increasingly tractable for simple
domains (Baldridge and Lascarides, 2005b). We plan to
extract CP-net descriptions from EDUs and to evaluate
these descriptions using "multi-valued variables" auto-
matically. We will then evaluate our method on a large
number of dialogues.
Our work here is also and more generally a first step to-
wards modelling the complex interaction between what
agents say, what their preferences are, and what they take
the preferences of other dialogue agents to be. It leads
to a conception of dialogue that?s more general than one
based purely on Gricean cooperative principles (Grice,
1975). On a purely Gricean approach, conversation is
cooperative in at least two ways: a basic level concern-
ing the conventions that govern linguistic meaning (ba-
sic cooperativity); and a level concerning shared attitudes
towards what is said, including shared intentions (con-
tent cooperativity). While basic cooperation is needed
for communication to work at all, content cooperativ-
ity involves strongly cooperative axioms like Coopera-
tivity (interlocutors normally adopt the speaker?s inten-
tions) (Allen and Litman, 1987, Grosz and Sidner, 1990,
Lochbaum, 1998). Our approach allows for divergent
preferences and divergent intentions, i.e. conversations
that aren?t based on content cooperativity. This will al-
low us to exploit information about conflicting agents?
preferences and game-theoretic techniques that are inher-
ent in the logics of CP-nets for computing optimal moves
(Bonzon, 2007). And in contrast to Franke et al (2009),
who analyse conversations where content cooperativity
doesn?t hold using a game-theoretic framework, our ap-
proach allows for partial and qualitative representations
of preferences rather than demanding complete and quan-
titative representations of them.
212
References
J. Allen and D. Litman. A plan recognition model for
subdialogues in conversations. Cognitive Science, 11
(2):163?200, 1987.
N. Asher and A. Lascarides. Logics of Conversation.
Cambridge University Press, 2003.
J. Baldridge and A. Lascarides. Annotating discourse
structures for robust semantic interpretation. In Pro-
ceedings of the Sixth International Workshop on Com-
putational Semantics (IWCS), Tilburg, The Nether-
lands, 2005a.
J. Baldridge and A. Lascarides. Probabilistic head-driven
parsing for discourse structure. In Proceedings of
the Ninth Conference on Computational Natural Lan-
guage Learning (CoNLL), pages 96?103, 2005b.
E. Bonzon. Mod?lisation des Interactions entre Agents
Rationnels: les Jeux Bool?ens. PhD thesis, Universit?
Paul Sabatier, Toulouse, 2007.
C. Boutilier, R.I. Brafman, C. Domshlak, H.H. Hoos, and
David Poole. Cp-nets: A tool for representing and
reasoning with conditional ceteris paribus preference
statements. Journal of Artificial Intelligence Research,
21:135?191, 2004.
M. Franke, T. de Jager, and R. van Rooij. Relevance in
cooperation and conflict. Journal of Logic and Lan-
guage, 2009.
J. Ginzburg. The Interactive Stance: Meaning for Con-
versation. CSLI Publications, to appear.
H. P. Grice. Logic and conversation. In P. Cole and
J. L. Morgan, editors, Syntax and Semantics Volume
3: Speech Acts, pages 41?58. Academic Press, 1975.
B. Grosz and C. Sidner. Plans for discourse. In J. Mor-
gan P. R. Cohen and M. Pollack, editors, Intentions in
Communication, pages 365?388. MIT Press, 1990.
Y. He and S. Young. Spoken language understsanding
using the hidden vector state model. Speech Commu-
nication, 48(3-4):262?275, 2005.
J. R. Hobbs, M. Stickel, D. Appelt, and P. Martin. Inter-
pretation as abduction. Artificial Intelligence, 63(1?2):
69?142, 1993.
A. Lascarides and N. Asher. Agreement, disputes and
commitment in dialogue. Journal of Semantics, 26(2):
109?158, 2009.
O. Lemon and O. Pietquin. Machine learning for spoken
dialogue systems. In Interspeech, 2007.
K. E. Lochbaum. A collaborative planning model of in-
tentional structure. Computational Linguistics, 24(4):
525?572, 1998.
W. C. Mann and S. A. Thompson. Rhetorical structure
theory: A framework for the analysis of texts. Interna-
tional Pragmatics Association Papers in Pragmatics,
1:79?105, 1987.
M. Poesio and D. Traum. Towards an axiomatisation of
dialogue acts. In J. Hulstijn and A. Nijholt, editors,
Proceedings of the Twente Workshop on the Formal Se-
mantics and Pragmatics of Dialogue. 1998.
E. Rasmusen. Games and Information: An Introduction
to Game Theory. Blackwell Publishing, 2007.
W. Wahlster, editor. Verbmobil: Foundations of Speech-
to-Speech Translation. Springer, 2000.
213
Appendix A : Treatment of an example
We illustrate in this section how our rules work on an
example. Since this dialogue was also evaluated by our
judges (cf section 7), we give where relevant some details
on those annotations. The example is as follows:
(6) pi1. A: so, I guess we should have another meet-
ing
pi2. A: how long do you think it should be for.
pi3. B: well, I think we have quite a bit to talk
about.
pi4. B: maybe, two hours?
pi5. B: how does that sound.
pi6. A: deadly,
pi7. A: but, let us do it anyways.
pi8. B: okay, do you have any time next week?
pi9. B: I have got, afternoons on Tuesday and
Thursday.
pi10. A: I am out of Tuesday Wednesday Thurs-
day,
pi11. A: so, how about Monday or Friday
Table 4 is the DSDRS associated with (6).
Relation(pii, [pi j ? pik]) indicates that a rhetorical re-
lation holds between the segment pii and a segment
consisting of pi j, pi j+1, . . . , pik
pi1 provides an atomic preference. We apply the rule
1 and so CommitA(pi1,D N A) adds the description
D N A |=M ?M(CPT (M)) where M means Meet.
pi2 We have Q-Elab(pi1, pi2). A continues to commit to
M in pi2 and no new preferences are introduced by
pi2. We apply rule 9, which makes the P(pi2) de-
scription the same as P(pi1)?s.
pi3 is linked to pi2 with QAP. B accepts A?s preference
and we apply the rule 11 since pi2 is a wh-question.
Thus CommitB(ElabB(pi2,pi3),D N B) adds the de-
scription D N B |= M ?M(CPT (M)). It is interest-
ing to note that some judges consider that agent?s
utterance in pi3 indicates a preference towards "talk-
ing a long time" while other judges consider, as our
method predicts, that this segment does not convey
any preference.
pi4 is linked to pi3 by Q-Elab. B commits to a new
preference. We apply rule 9, rule 8 and then rule
8.a. The preference on the hour is now dependent
on the preference on meeting; i.e., D N B |= M :
2h ? 2h(CPT (2h)), where the variable 2h means
two hours.
pi5 is related to pi4 with the Q-Cont relation. We
then follow the same rule as the continued relation,
namely Q-Elab. We apply rule 9 which does not
change the CP-net description of B because pi5 does
not convey any preference.
pi6 is related to pi5 with QAP relation. In this case, it?s
not clear what is the QAP target and so we apply
rule 12: we wait to learn more and we do not change
B?s CP-net description.
All the Judges indicated that segments pi5 and pi6
are ambiguous and therefore hesitated to say if they
commit to preferences. For example in pi6, do we
have a preference for meeting more than 2 hours
or less than 2 hours? This indecision is compatible
with the predictions of rule 12.
pi7 A accepts B?s preference. We apply rule 9 and then
rule 8 to obtain:
D N A |=M ?M(CPT (M)),
D N A |=M : 2h? 2h(CPT (2h)).
pi8 is linked to pi7 by Q-Elab. B introduces a new pref-
erence for meeting next week.
We apply rule 9 and then 8 to obtain:
D N B |=M ?M(CPT (M)),
D N B |=M : 2h? 2h(CPT (2h)),
D N B |=M?2h :NW ?NW (CPT (NW ))where the
variable NW means next week.
pi9 is linked to pi8 by Plan-Elab. pi9 expresses com-
mitments to preference that already involve a
CP-net description. B introduces three prefer-
ences: one for meeting on Tuesday, the other
for meeting on Thursday and given the conjunc-
tion of preferences Tues ? Thurs, one for time
afternoon (of Tuesday and Thursday). That is,
((?(Tues)??(Thurs)) 7? Aft)(P(pi9)). We apply
the equivalence (3) and obtain :
(?(Tues?Thurs)? Aft)(P(pi9)).
Then, we apply rules 8.g, 8.b and 8.d. The CP-net
description of B is thus updated as follows:
D N B |= M ? 2h ? NW ? Tues : Thurs ?
Thurs(CPT (Thurs)),
D N B |= M ? 2h ? NW ? Tues : Thurs ?
Thurs(CPT (Thurs)),
D N B |= M ? 2h ? NW ? Thurs : Tues ?
Tues(CPT (Tues)),
D N B |= M ? 2h ? NW ? Thurs : Tues ?
Tues(CPT (Tues)),
D N B |= M ? 2h ? NW ? (Thurs ? Tues) : Aft ?
Aft(CPT (Aft)).
Most judges express here a preference ranking over
outcomes. For instance, if B elaborates by adding
the preference "I have got Monday morning too"
(as it is in the test corpus), some consider the rank-
ing "(Tuesday or Thursday afternoons) ? (Monday
214
Turn A?s SDRS B?s SDRS
1 pi1A : Q-Elab(pi1,pi2) /0
2 pi1A:is the same as in turn 1 pi2B : Q-Elab(pi1, [pi2?pi5])?QAP(pi2, [pi3?pi5])?
Q-Elab(pi3,pi)
pi : Q-Cont(pi4,pi5)
3 pi3A : Q-Elab(pi1, [pi2?pi7])?QAP(pi2, [pi3?pi7])? pi2B: is the same as in turn 2
Q-Elab(pi3, [pi4,pi7])?QAP(pi,pi?)
pi : Q-Cont(pi4,pi5),pi? : Contrast(pi6,pi7)
4 pi3A: is the same as in turn 3 pi4B : Q-Elab(pi1, [pi2?pi9])?QAP(pi2, [pi3?pi9])?
Q-Elab(pi3, [pi4?pi9])?QAP(pi, [pi6?pi9])?
Q-Elab(pi?,pi??)
pi : Q-Cont(pi4,pi5),pi? : Contrast(pi6,pi7)
pi?? : Plan-Elab(pi8,pi9)
5 pi5A : Q-Elab(pi1, [pi2?pi11])?QAP(pi2, [pi3?pi11])? pi4B: is the same as in turn 4
Q-Elab(pi3, [pi4?pi11])?QAP(pi, [pi6?pi11])?
Q-Elab(pi?, [pi8?pi11])?QAP(pi??,pi???)
pi : Q-Cont(pi4,pi5),pi? : Contrast(pi6,pi7)
pi?? : Plan-Elab(pi8,pi9),pi??? : Q-Elab(pi10,pi11)
Table 4: The DSDRS for Dialogue (6).
morning)? (other days)", while others consider the
ranking "(Tuesday or Thursday afternoon) or (Mon-
day morning)? (other days)". We did not treat such
preference ranking.
pi10 is related to pi9 by QAP where A answers no to B?s
question asked in pi8. We apply rule 10 (no). Since
Tues&Weds&Thurs(P(pi10)) is not consistent with
((?(Tues) ??(Thurs)) 7? Aft)(P(pi9)), we apply
CommitA(Correction(pi9,pi10),D N A), which adds
the preference Weds to A?s description and then
the rule 13 where Tues and Thurs are respectively
replaced by Tues and Thurs :
D N A |=M?2h?NW : Tues? Tues(CPT (Tues)),
D N A |= M ? 2h ? NW : Thurs ?
Thurs(CPT (Thurs)),
D N A |= M ? 2h ? NW : Weds ?
Weds(CPT (Weds)).
pi11 Finally, this segment is linked to pi10 with Q-Elab
where Mond?Fri(P(pi11)). We apply rules 9 and
8.b and update A?s CP-net description as follows:
D N A |=M?2h?NW ?Tues?Thurs?Weds?Fri :
Mond ?Mond(CPT (Mond)),
D N A |=M?2h?NW ?Tues?Thurs?Weds?Fri :
Mond ?Mond(CPT (Mond)),
D N A |= M ? 2h ? NW ? Tues ? Thurs ?Weds ?
Mond : Fri? Fri(CPT (Fri)),
D N A |= M ? 2h ? NW ? Tues ? Thurs ?Weds ?
Mond : Fri? Fri(CPT (Fri)).
The evaluation of this dialogue also reveals to what extent
naive annotators reason with binary (Monday preferred
to not Monday) or multi-valued variables (Monday pre-
ferred to Tuesday). Most judges use multi-valued vari-
ables to express the preference extracted from an EDU,
and the way in which our method exploits domain knowl-
edge to yield the minimal CP-net satisfying the descrip-
tion reflects this. In addition, some judges use a small
set of variables (for example the variable time of meeting
that groups together the notion of week, day, hours, etc.)
while others use a distinct variable for each preference.
Finally, we also noticed that judges do not describe the
same preference dependencies. For example, in:
(7) We could have lunch together and then have the
meeting from one to three?
some consider that the preference on having lunch is in-
dependent from the preference on the meeting (in this
case, they consider that the preference on the period one
to three is independent from the preference on meeting)
while others consider that the two preferences are depen-
dent.
215
