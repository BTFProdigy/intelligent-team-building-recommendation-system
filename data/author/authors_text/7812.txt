Interpreting Vague Utterances in Context
David DeVault and Matthew Stone
Department of Computer Science
Rutgers University
Piscataway NJ 08854-8019
David.DeVault@rutgers.edu, Matthew.Stone@rutgers.edu
Abstract
We use the interpretation of vague scalar predi-
cates like small as an illustration of how system-
atic semantic models of dialogue context en-
able the derivation of useful, fine-grained utter-
ance interpretations from radically underspeci-
fied semantic forms. Because dialogue context
suffices to determine salient alternative scales
and relevant distinctions along these scales,
we can infer implicit standards of comparison
for vague scalar predicates through completely
general pragmatics, yet closely constrain the in-
tended meaning to within a natural range.
1 Introduction
Modeling context and its effects on interpretation
may once have seemed to call for an open-ended in-
vestigation of people?s knowledge of the common-
sense world (Hobbs et al, 1993). But research on
the semantics of practical dialogue (Allen et al,
2001) now approaches dimensions of context sys-
tematically, through increasingly lightweight, fac-
tored models. The evolving state of real-world ac-
tivity proceeds predictably according to background
plans and principles of coordination (Rich et al,
2001). The status of the dialogue itself is defined
by circumscribed obligations to ground prior ut-
terances, follow up open issues, and advance real-
world negotiation (Larsson and Traum, 2000). Fi-
nally, the evolving state of the linguistic context
is a direct outgrowth of the linguistic forms inter-
locutors use and the linguistic relationships among
successive utterances (Ginzburg and Cooper, 2001;
Asher and Lascarides, 2003). These compatible
models combine directly to characterize an aggre-
gate information state that provides a general back-
ground for interpretation (Bunt, 2000).
We argue in this paper that such integrated mod-
els enable systems to calculate useful, fine-grained
utterance interpretations from radically underspec-
ified semantic forms. We focus in particular on
vague scalar predicates like small or long. These
predicates typify qualitative linguistic expression of
quantitative information, and are thus both chal-
lenging and commonplace. Building on a mul-
tidimensional treatment of dialogue context, we
develop and implement a theoretically-motivated
model of vagueness which is unique in treating
vague predicates as genuinely vague and genuinely
context-sensitive, yet amenable to general processes
of contextual and interpretive inference.
1.1 Semantic insights
We pursue our argument in the context of an imple-
mented drawing application, FIGLET, which allows
users to give English instructions to draw a carica-
ture of an expressive face. Figure 1 shows a repre-
sentative interaction with FIGLET; the user gives the
successive instructions in (1):
(1) a. Make two small circles.
b. Draw a long line underneath.
Like Di Eugenio and Webber (1996), we empha-
size that understanding such instructions requires
contextual inference combining linguistic, task and
domain knowledge. For example, consider the re-
sponse to (1a) of placing circles so as to form the
eyes of a new face. To recognize the possibility
of drawing eyes exploits knowledge of the ongoing
drawing task. To put the eyes where they belong
in the upper part of the new face exploits domain
knowledge. The response to (1b) adds the linguis-
tic context as another ingredient. To identify where
the line goes, the user uses the objects mentioned
recently in the interaction as the understood spatial
landmark for underneath. Figure 1 highlights the
importance of using multidimensional representa-
tions of dialogue context in understanding instruc-
tions for quantitative domains.
We leverage this background context in our com-
putational approach to vagueness. We model a
vague utterance like draw a long line as though it
meant draw a line with, you know, length. In this
approach, vague predicates are completely under-
specified; linguistic knowledge says nothing about
how long something long is. Instead, vague lan-
guage explicitly draws on the background knowl-
Initial blank figure state. After the user utters (1a):
Make two small circles.
After the user utters (1b):
Draw a long line underneath.
Figure 1: Motivating interaction: Vague instructions to draw a face.
edge already being applied in utterance interpre-
tation. The user?s motivation in using long is to
differentiate an intended interpretation, here an in-
tended action, from alternative possibilities in con-
text. Background knowledge already sets out the
relevant ways to draw a line; drawing a long line
means singling out some of them by the length of
that new line. This model recalls dynamic theo-
ries of vague scalar predicates, such as the seman-
tics of Kyburg and Morreau (2000), Barker (2002),
or Kennedy (2003), but it is exactly implemented
in FIGLET. The implementation capitalizes on the
richness of current models of context to recover
content for the you know of vagueness.
1.2 Overview
In Section 2, we motivate approaches to the seman-
tics of vague scalar predicates that associate them
with a presupposed standard of comparison. We il-
lustrate how context can be understood to supply
possible standards, and how pragmatic reasoning
from utterances allows interlocutors to infer them.
In Section 3, we establish a bridge to the general
treatment of practical dialogue, by showing how
multiple dimensions of context generally contribute
to recognizing possible interpretations for under-
specified utterances. Section 4 builds on Sections 2
and 3 to show how FIGLET exploits a rich model of
utterance context to respond cooperatively to vague
utterances like (1a) and (1b), while Section 5 de-
tails FIGLET?s actual implementation. We conclude
in Section 6 by suggesting further challenges that
vagueness still poses for computational semantics.
2 Vague standards in context
We adopt a view of vague predicates motivated by
linguistic theory, particularly Kennedy?s approach
(1999; 2003). We assume that gradable adjectives
are associated with measurement functions mapping
individuals to degrees on a scale. In FIGLET?s draw-
ing domain, the relevant measurements pertain to
spatial properties. For long, for example, the mea-
surement maps individuals to their spatial lengths;
for small, it maps individuals to degrees on an in-
verted scale of size.
Positive gradable adjectives compare measured
degrees against a standard on the scale which is de-
rived from context. For example, long says that an
object?s length exceeds the threshold set by the cur-
rent standard for length. Other forms, such as com-
parative adjectives or adjectives with explicit mea-
sure phrases, compare degrees differently.
Importantly, grammar says nothing about how
standards for positive gradable adjectives are de-
rived. In other words, contra Cresswell (1977) and
others, the interpretation of adjectives is not com-
puted relative to a grammatically-specified ?com-
parison class? of related objects. And, contra Oates
et al (2000) and Roy and Pentland (2002), the in-
terpretation of adjectives need not require statistical
knowledge about how objects with different mea-
surements on a scale tend to be described. Instead,
standards are derived directly from an evolving con-
text by the general principles that govern pragmatic
resolution of context dependence.
Kennedy synthesizes a range of evidence for this
claim. Here we go further, and provide a formal,
implemented model. We can sketch the evidence
and our model by considering two key examples.
First, we illustrate that vagueness depends di-
rectly on specific contextually-relevant distinctions.
Consider the session with FIGLET shown in Fig-
ure 2. The user has elected to draw two objects side-
by-side. The initial context just contains a square.
The user utters (2).
(2) Make a small circle.
To interpret (2) it doesn?t seem to help to appeal to
general knowledge about how big circles tend to be.
(It?s quite convoluted to even frame the idea in a
sensible way.) Graff (2000) observes that standards
often implicitly appeal to what we expect about par-
ticular individuals, not just what we know about
similar individuals. In context, here, the user just
seems to be asking for a circle vaguely smaller than
the square. This is the interpretation FIGLET builds;
to comply, FIGLET draws the circle an arbitrary but
representative possible size. The point is that salient
objects and actions inevitably set up meaningful dis-
Initial figure state. After the user utters (2). Initial figure state. After the user utters (3).
Figure 2: Taking standards from context in (2):
Make a small circle.
Figure 3: Disambiguating contextual standards
in (3): Make the small square a circle.
tinctions in the context. Interlocutors exploit these
distinctions in using vague adjectives.
Figure 3 illustrates that understanding vagueness
is part of a general problem of understanding utter-
ances. Figure 3 shows FIGLET?s action in a more
complex context, containing two squares of differ-
ent sizes. We consider the user?s instruction (3):
(3) Make the small square a circle.
FIGLET?s action changes the smaller of the two
squares. The standard behind this interpretation
is implicitly set to differentiate the contextually-
salient objects from one another; the natural reso-
lution of (3) does not require that either square be
definitely small (Kyburg and Morreau, 2000). In
Figure 3, for example, there are different potential
standards that would admit either both squares or
neither square as small. However, we can rule out
these candidate standards in interpreting (3). The
user?s communicative intention must explain how
a unique square from the context can be identified
from (3) using a presupposed small standard. If that
standard is too big, both squares will fit. If that stan-
dard is too small, neither square will fit. Only when
that standard falls between the sizes of the squares
does (3) identify a unique square.
The examples in Figures 2 and 3 show two ways
new standards can be established. Once established,
however, standards become part of the evolving
context (Barker, 2002). Old standards serve as de-
faults in interpreting subsequent utterances. Only
if no better interpretation is found will FIGLET go
back and reconsider its standard. This too is general
pragmatic reasoning (Stone and Thomason, 2003).
3 Dimensions of context in interpretation
To cash out our account of contextual reasoning
with vagueness, we need to characterize the con-
text for practical dialogue. Our account presupposes
a context comprising domain and situation knowl-
edge, task context and linguistic context. In this sec-
tion, we survey each of these dimensions of context,
and show how they converge in the resolution of un-
derspecification across a wide range utterances.
Domain and situation knowledge describes the
commonsense structure of the real-world objects
and actions under discussion. Practical dialogue re-
stricts this otherwise open-ended specification to the
circumscribed facts that are directly relevant to an
ongoing collaboration. For example, in our drawing
domain, individuals are categorized by a few types:
types of shape such as circles and squares; and types
of depiction such as eyes and heads. These types
come with corresponding constraints on individuals.
For example, the shape of a mouth may be a line, an
ellipse, or a rectangle, while the shape of a head can
only be an ellipse. These constraints contribute to
interpretation. For instance, a head can never be de-
scribed as a line, for example, since heads cannot
have this shape.
Task context tracks collaborators? evolving com-
mitment to shared goals and plans during joint ac-
tivity. In FIGLET?s drawing domain, available ac-
tions allow users to build figure parts by introducing
shapes and revising them. Our experience is that
users? domain plans organize these actions hierar-
chically into strategic patterns. For example, users
tend to complete the structures they begin drawing
before drawing elsewhere; and once they are satis-
fied with what they have, they proceed in natural
sequence to a new part nearby. Task context plays
a powerful role in finding natural utterance inter-
pretations. By recording a plan representation and
keeping track of progress in carrying it out, FIGLET
has access to a set of candidate next actions at each
point in an interaction. Matching the user?s utter-
ance against this candidate set restricts the interpre-
tation of instructions based on the drawing already
created and the user?s focus of attention within it.
For example, if the user has just drawn the right eye
onto an empty face, they are likely to turn to the left
eye next. This context suggests making a winking
left eye in response to draw a line, an interpretation
that might not otherwise be salient.
Linguistic context records the evolving status
of pragmatic distinctions triggered by grammatical
conventions. One role of the linguistic context is its
contribution to distinguishing the prominent entities
Initial figure state. After the user utters (4):
Draw a line underneath.
Figure 4: Context in instructions.
that can serve as the referents of pronouns and other
reduced expressions. To see this, note that, as far
as domain knowledge and task context go, the in-
struction make it bigger could apply to any object
currently being created. If the figure is hierarchi-
cal, there will be many possibilities. Yet we typi-
cally understand it to refer specifically to an object
mentioned saliently in the previous utterance. The
linguistic context helps disambiguate it.
Figure 4 illustrates how the three different dimen-
sions of context work together. It illustrates an inter-
action with FIGLET where the user has just issued an
instruction to create two eyes, resulting in the figure
state shown at the left in Figure 4. The user?s next
instruction is (4):
(4) Draw a line underneath.
We focus on how the context constrains the position
and orientation of the line.
Linguistic context indicates that underneath
should be understood as underneath the eyes. This
provides one constraint on the placement of the line.
Task context makes drawing the mouth a plausible
candidate next action. Domain knowledge shows
that the mouth can be a line, but only if further
constraints on position, orientation and length are
met. In understanding the instruction, FIGLET ap-
plies all these contextual constraints simultaneously.
The set of consistent solutions?drawing a horizon-
tal line at a range of plausible mouth positions be-
low the eyes?constitutes the utterance interpreta-
tion. FIGLET acts to create the result in Figure 4 by
choosing a representative action from this set.
4 Interpreting vague utterances in context
In our approach, the linguistic context stores agreed
standards for vague predicates. Candidate standards
are determined using information available from do-
main knowledge and the current task context. In
FIGLET?s drawing domain, possibilities include the
actual measurements of objects that have already
been drawn. They also include the default domain
measurements for new objects that task context says
could be added. Setting standards by a measure-
ment is our shorthand for adopting an implicit range
of compatible standards; these standards remain
vague, especially since many options are normally
available (Graff, 2000).
We treat the use of new candidate standards in in-
terpretation as a case of presupposition accommo-
dation (Bos, 2003). In presupposition accommo-
dation, the interpretation of an utterance must be
constructed using a context that differs from the ac-
tual context. When speakers use an utterance which
requires accommodation, they typically expect that
interlocutors will update the dialogue context to in-
clude the additional presumptions the utterance re-
quires. We assume that all accommodation is sub-
ject to two Gricean constraints. First, we assume
whenever possible that an utterance should have a
uniquely identifiable intended interpretation in the
context in which it is to be interpreted. Second, we
assume that when interpretations in alternative con-
texts are available, the speaker is committed to the
strongest one?compare Dalrymple et al (1998).
Inferring standards for vague predicates is a special
case of this general Gricean reasoning.
The principles articulated thus far in Sections 2?4
allow us to offer a precise explanation of FIGLET?s
behavior as depicted in Figure 1. The user starts
drawing a face with an empty figure. In this domain
and task context, make two circles fits a number of
possible actions. For example, it fits the action of
drawing a round head and its gaping mouth. How-
ever, in (1a), what the user actually says is make
two small circles. The interpretation for (1a) must
accommodate a standard for small and select from
the continuum of size possibilities two new circles
that meet this standard.
The standards in this context are associated with
the size distinctions among potential new objects.
The different qualitative behavior of these standards
in interpretation can be illustrated by the standards
set from possible new circular objects that are con-
sistent with the face-drawing task. We can set the
standard from the default size of an eye, from the
default size of a mouth (larger), or from the default
size of a head (larger still).1 Because each stan-
dard allows all smaller objects to be created next,
these standards lead to 1, 3, and 6 interpretations,
respectively. So we recover the standard from the
eye, which results in a unique interpretation.2
1Since the default sizes of new objects reflect the relative
dimensions of any other objects already in the figure, FIGLET?s
default sizes are not generally equivalent to static comparison
classes.
2Note that there are many potential sources of standards for
small that FIGLET does not currently pursue. E.g. the average
size of all objects already in the figure. We believe that general
In tandem with its response, FIGLET tracks the
changes to the context. The task context is updated
to note that the user has drawn the eyes and must
continue with the process of creating and revising
the features of the face. The linguistic context is
updated to include the new small standard, and to
place the eyes in focus.
This updated context provides the background
for (1b), the user?s next instruction draw a long
line underneath. In this context, as we saw with
Figure 4, context makes it clear that any response
to draw a line underneath must draw the mouth.
Thus, unlike in (1a), all the interpretations here have
the same qualitative form. Nevertheless, FIGLET?s
Gricean reasoning can still adjust the standard for
length to differentiate interpretations quantitatively,
and thereby motivate the user?s use of the word long
in the instruction. FIGLET bases its possible stan-
dards for length on both actual and potential ob-
jects. It can set the standard from an actual eye or
from the two eyes together; and it can set the stan-
dard from the default mouth or head. The mouth, of
course, must fit inside the head; the largest standard
is ruled out. All the other standards lead to unique
interpretations. Since the length of the two eyes to-
gether is the strictest of the remaining standards, it
is adopted. This interpretation leads FIGLET to the
response illustrated at the right in Figure 1.
5 Implementation
We have implemented FIGLET in Prolog using
CLP(R) real constraints (Jaffar and Lassez, 1987)
for metric and spatial reasoning. This section
presents a necessarily brief overview of this imple-
mentation; we highlight how FIGLET is able to ex-
actly implement the semantic representations and
pragmatic reasoning presented in Sections 2?4. We
offer a detailed description of our system and dis-
cuss some of the challenges of building it in DeVault
and Stone (2003).
5.1 Semantic representation
In FIGLET, we record the semantics of user instruc-
tions using constraints, or logical conjunctions of
open atomic formulas, to represent the contextual
requirements that utterances impose; we view these
constraints as presuppositions that speakers make in
using the utterance. We assume matches take the
form of instances that supply particular domain rep-
resentations as suitable values for variables. Stone
(2003) motivates this framework in detail.
methods for specifying domain knowledge will help provide
the meaningful task distinctions that serve as candidate stan-
dards for vague predicates on our approach, but pursuing this
hypothesis is beyond the scope of this paper.
In (5a-d), we show the presuppositions FIGLET
assigns to an utterance of Make two small circles,
arranged to show the contributions of each individ-
ual word. In (5e), we show the contribution made
by the utterance to an evolving dialogue; the effect
is to propose that an action be carried out.
(5) a. simple(A)? target(A,X)?fits plan(A)?
holds(result(A,now),visible(X))?
holds(now, invisible(X))?
b. number(X ,2)?
c. standard(small,S)?
holds(result(A,now),small(X ,S))?
d. number(X ,multiple)?
holds(result(A,now),shape(X ,circle))
e. propose(A)
We formulate these constraints in an expressive
ontology. We have terms and variables for ac-
tions, such as A; for situations, such as now and
result(A,now); for objects, such as X ; for stan-
dards for gradable vague predicates (scale-threshold
pairs), such as S; and for quantitative points and in-
tervals of varying dimensionality, as necessary.
5.2 Pragmatic reasoning
Constraint networks such as (5a-e) provide a uni-
form venue for describing the various contextual
dependencies required to arrive at natural utterance
interpretations. Thus, the contextual representation
and reasoning outlined in Sections 3 and 4 is real-
ized by a uniform mechanism in FIGLET: specifica-
tions of how to reason from context to find solutions
to these constraints.
For example, Section 3 described domain knowl-
edge that links particular object types like eyes and
heads with type-specific constraints. In our imple-
mentation, we specify real and finite constraints that
individuals of each type must satisfy. In order for an
individual e of type t to serve as part of a solution
to a constraint network like (5a-e), e must addition-
ally meet the constraints associated with type t. In
this way, FIGLET requires utterance interpretations
to respect domain knowledge.
Solving many of the constraints appearing in (5a-
e) requires contextual reasoning about domain ac-
tions and their consequences. Some constraints
characterize actions directly; thus simple(A) means
that A is a natural domain action rather than
an abstruse one. Constraints can describe the
effects of actions by reference to the state of
the visual display in hypothetical situations; thus
holds(result(A,now),shape(X ,circle)) means that
the individual X has a circular shape once action
A is carried out. Constraints can additionally char-
acterize causal relationships in the domain; thus
target(A,X) means that action A directly affects X ,
and the constraints of (5a-d) together mean that car-
rying out action A in the current situation causes two
small circles to become visible. These constraints
are proved in FIGLET by what is in effect a planner
that can find complex actions that achieve specified
effects via a repertoire of basic domain actions.
Task context is brought to bear on interpretation
through the fits plan(A) constraint of (5a). FIGLET
uses a standard hierarchical, partially ordered plan
representation to record the structure of a user?s
task. We specify the solutions to fits plan(A) to
be just those actions A that are possible next steps
given the user?s current state in achieving the task.
Since these task-appropriate actions can factor addi-
tional constraints into interpretation, enforcing the
fits plan(A) constraint can help FIGLET identify a
natural interpretation.
As discussed in Section 4, FIGLET records a
list of current standards for vague scalar adjec-
tives in the linguistic context. The constraint
standard(small,S) of (5c) connects the overall ut-
terance interpretation to the available standards for
small in the linguistic context. FIGLET interprets ut-
terances carrying semantic constraints of the form
standard(vague-predicate,S) in one or two stages.
In the first stage, the constraint is solved just in
case S is the prevailing standard for vague-predicate
in the linguistic context. If there is no prevailing
standard for an evoked vague property, or if this
stage does not yield a unique utterance interpreta-
tion, then FIGLET moves to a second stage in which
the constraint is solved for any standard that cap-
tures a relevant distinction for vague-predicate in
the context. If there is a strongest standard that re-
sults in a unique interpretation, it is adopted and in-
tegrated into the new linguistic context.
5.3 Parsing and Interpretation
Language understanding in FIGLET is mediated by a
bottom-up chart parser written in Prolog. As usual,
chart edges indicate the presence of recognized par-
tial constituents within the input sequence. In ad-
dition, edges now carry constraint networks that
specify the contextual reasoning required for under-
standing. In addition to finite instances (Schuler,
2001), these networks include real constraints that
formalize metric and spatial relationships. Interpre-
tation of these networks is carried out incremen-
tally, during parsing; each edge thus records a set
of associated candidate interpretations. Since do-
main reasoning can be somewhat time-intensive in
our current implementation, we adopt a strategy
of delaying the solution of certain constraints until
enough lexical material has accrued that the asso-
ciated problem-solving is judged tractable (DeVault
and Stone, 2003).
6 Assessment and Conclusion
In our approach, we specify a genuinely vague se-
mantics: vague words evoke a domain-specific scale
that can differentiate alternative domain individuals.
To find a unique interpretation for a vague utter-
ance, we leverage ordinary inference about the do-
main, task, and linguistic context to recover implicit
thresholds on this scale.
We believe that further methodological advances
will be required to evaluate treatments of vagueness
in indefinite reference, such as that considered here.
For example, obviously the very idea of a ?gold
standard? for resolution of vagueness is problem-
atic. We believe that the best argument for a theory
of vagueness in a language interface would show
that naive users of the interface are, on the whole,
likely to accept its vague interpretations and un-
likely to renegotiate them through clarification. But
the experiment would have to rule out confounding
factors such as poorly-modeled lexical representa-
tion and context tracking as sources for system in-
terpretations that users reject.
We intend to take up the methodological chal-
lenges necessary to construct such an argument in
future work. In the meantime, while our current im-
plementation of FIGLET exhibits the promising be-
havior discussed in this paper and illustrated in Fig-
ures 1?4, some minor engineering unrelated to lan-
guage understanding remains before a fruitful eval-
uation can take place. As alluded to above, the tight
integration of contextual reasoning and interpreta-
tion that FIGLET carries out can be expensive if not
pursued efficiently. While our initial implementa-
tion achieves a level of performance that we accept
as researchers (interpretation times of between one
and a few tens of seconds), evaluation requires us to
improve FIGLET?s performance to levels that exper-
imental participants will accept as volunteers. Our
analysis of FIGLET indicates that this performance
can in fact be achieved with better-regimented do-
main problem-solving.
Nevertheless, we emphasize the empirical and
computational arguments we already have in sup-
port of our model. Our close links with the linguis-
tic literature mean that major empirical errors would
be surprising and important across the language sci-
ences. Indeed, limited evaluations of treatments of
vague definite reference using standards of differ-
entiation or very similar ideas have been promising
(Gorniak and Roy, In Press). The computational ap-
peal is that all the expensive infrastructure required
to pursue the account is independently necessary.
Once this infrastructure is in place the account is
readily implemented with small penalty of perfor-
mance and development time. It is particularly at-
tractive that the approach requires minimal lexical
knowledge and training data. This means adding
new vague words to an interface is a snap.
Overall, our new model offers three contribu-
tions. Most importantly, of course, we have devel-
oped a computational model of vagueness in terms
of underspecified quantitative constraints. But we
have also presented a new demonstration of the im-
portance and the feasibility of using multidimen-
sional representations of dialogue context in under-
standing descriptions of quantitative domains. And
we have introduced an architecture for resolving un-
derspecification through uniform pragmatic mech-
anisms based on context-dependent collaboration.
Together, these developments allow us to circum-
scribe possible resolutions for underspecified utter-
ances, to zero in on those that the speaker and hearer
could adopt consistently and collaboratively, and
so to constrain the speaker?s intended meaning to
within a natural range.
Acknowledgments
We thank Kees van Deemter and our anonymous re-
viewers for valuable comments. This work was sup-
ported by NSF grant HLC 0308121.
References
J. Allen, D. Byron, M. Dzikovska, G. Ferguson,
L. Galescu, and A. Stent. 2001. Towards conver-
sational human-computer interaction. AI Maga-
zine, 22(4):27?37.
N. Asher and A. Lascarides. 2003. Logics of Con-
versation. Cambridge.
C. Barker. 2002. The dynamics of vagueness. Lin-
guistics and Philosophy, 25(1):1?36.
J. Bos. 2003. Implementing the binding and
accommodation theory for anaphora resolution
and presupposition. Computational Linguistics,
29(2):179?210.
H. Bunt. 2000. Dialogue pragmatics and context
specification. In H. Bunt and W. Black, editors,
Abduction, Belief and Context in Dialogue, pages
81?150. Benjamin.
M. Cresswell. 1977. The semantics of degree. In
B. H. Partee, editor, Montague Grammar, pages
261?292. Academic.
M. Dalrymple, M. Kanazawa, Y. Kim, S. Mchombo,
and S. Peters. 1998. Reciprocal expressions and
the concept of reciprocity. Linguistics and Phi-
losophy, 21(2):159?210.
D. DeVault and M. Stone. 2003. Domain inference
in incremental interpretation. In Proc. ICoS.
B. Di Eugenio and B. Webber. 1996. Pragmatic
overloading in natural language instructions. Int.
Journal of Expert Systems, 9(2):53?84.
J. Ginzburg and R. Cooper. 2001. Resolving ellip-
sis in clarification. In Proc. ACL.
P. Gorniak and D. Roy. In Press. Grounded seman-
tic composition for visual scenes. Journal of Ar-
tificial Intelligence Research.
D. Graff. 2000. Shifting sands: An interest-
relative theory of vagueness. Philosophical Top-
ics, 28(1):45?81.
J. Hobbs, M. Stickel, D. Appelt, and P. Martin.
1993. Interpretation as abduction. Artificial In-
telligence, 63:69?142.
J. Jaffar and J.-L. Lassez. 1987. Constraint logic
programming. In Proc. POPL, pages 111?119.
C. Kennedy. 1999. Projecting the adjective: The
syntax and semantics of gradability and compar-
ison. Garland.
C. Kennedy. 2003. Towards a grammar of vague-
ness. Manuscript, Northwestern.
A. Kyburg and M. Morreau. 2000. Fitting words:
Vague words in context. Linguistics and Philos-
ophy, 23(6):577?597.
S. Larsson and D. Traum. 2000. Information state
and dialogue management in the TRINDI dia-
logue move engine toolkit. Natural Language
Engineering, 6:323?340.
T. Oates, M. D. Schmill, and P. R. Cohen. 2000.
Toward natural language interfaces for robotic
agents. In Proc. Agents, pages 227?228.
C. Rich, C. L. Sidner, and N. Lesh. 2001. COL-
LAGEN: applying collaborative discourse the-
ory to human-computer interaction. AI Maga-
zine, 22(4):15?26.
D. Roy and A. Pentland. 2002. Learning words
from sights and sounds: A computational model.
Cognitive Science, 26(1):113?146.
W. Schuler. 2001. Computational properties of
environment-based disambiguation. In Proc.
ACL, pages 466?473.
M. Stone and R. H. Thomason. 2003. Coordinat-
ing understanding and generation in an abductive
approach to interpretation. In Proc. DiaBruck,
pages 131?138.
M. Stone. 2003. Knowledge representation for lan-
guage engineering. In A. Farghaly, editor, A
Handbook for Language Engineers, pages 299?
366. CSLI.
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 184?192,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Learning to Interpret Utterances Using Dialogue History
David DeVault
Institute for Creative Technologies
University of Southern California
Marina del Rey, CA 90292
devault@ict.usc.edu
Matthew Stone
Department of Computer Science
Rutgers University
Piscataway, NJ 08845-8019
Matthew.Stone@rutgers.edu
Abstract
We describe a methodology for learning a
disambiguation model for deep pragmatic
interpretations in the context of situated
task-oriented dialogue. The system accu-
mulates training examples for ambiguity
resolution by tracking the fates of alter-
native interpretations across dialogue, in-
cluding subsequent clarificatory episodes
initiated by the system itself. We illus-
trate with a case study building maxi-
mum entropy models over abductive in-
terpretations in a referential communica-
tion task. The resulting model correctly re-
solves 81% of ambiguities left unresolved
by an initial handcrafted baseline. A key
innovation is that our method draws exclu-
sively on a system?s own skills and experi-
ence and requires no human annotation.
1 Introduction
In dialogue, the basic problem of interpretation is
to identify the contribution a speaker is making to
the conversation. There is much to recognize: the
domain objects and properties the speaker is refer-
ring to; the kind of action that the speaker is per-
forming; the presuppositions and implicatures that
relate that action to the ongoing task. Neverthe-
less, since the seminal work of Hobbs et al (1993),
it has been possible to conceptualize pragmatic in-
terpretation as a unified reasoning process that se-
lects a representation of the speaker?s contribution
that is most preferred according to a background
model of how speakers tend to behave.
In principle, the problem of pragmatic interpre-
tation is qualitatively no different from the many
problems that have been tackled successfully by
data-driven models in NLP. However, while re-
searchers have shown that it is sometimes possi-
ble to annotate corpora that capture features of in-
terpretation, to provide empirical support for the-
ories, as in (Eugenio et al, 2000), or to build
classifiers that assist in dialogue reasoning, as in
(Jordan and Walker, 2005), it is rarely feasible
to fully annotate the interpretations themselves.
The distinctions that must be encoded are subtle,
theoretically-loaded and task-specific?and they
are not always signaled unambiguously by the
speaker. See (Poesio and Vieira, 1998; Poesio
and Artstein, 2005), for example, for an overview
of problems of vagueness, underspecification and
ambiguity in reference annotation.
As an alternative to annotation, we argue here
that dialogue systems can and should prepare
their own training data by inference from under-
specified models, which provide sets of candi-
date meanings, and from skilled engagement with
their interlocutors, who know which meanings are
right. Our specific approach is based on contribu-
tion tracking (DeVault, 2008), a framework which
casts linguistic inference in situated, task-oriented
dialogue in probabilistic terms. In contribution
tracking, ambiguous utterances may result in alter-
native possible contexts. As subsequent utterances
are interpreted in those contexts, ambiguities may
ramify, cascade, or disappear, giving new insight
into the pattern of activity that the interlocutor is
engaged in. For example, consider what happens
if the system initiates clarification. The interlocu-
tor?s answer may indicate not only what they mean
now but also what they must have meant earlier
when they used the original ambiguous utterance.
Contribution tracking allows a system to accu-
mulate training examples for ambiguity resolution
by tracking the fates of alternative interpretations
across dialogue. The system can use these ex-
amples to improve its models of pragmatic inter-
pretation. To demonstrate the feasibility of this
approach in realistic situations, we present a sys-
tem that tracks contributions to a referential com-
munication task using an abductive interpretation
184
model: see Section 2. A user study with this sys-
tem, described in Section 3, shows that this sys-
tem can, in the course of interacting with its users,
discover the correct interpretations of many poten-
tially ambiguous utterances. The system thereby
automatically acquires a body of training data in
its native representations. We use this data to build
a maximum entropy model of pragmatic interpre-
tation in our referential communication task. After
training, we correctly resolve 81% of the ambigu-
ities left open in our handcrafted baseline.
2 Contribution tracking
We continue a tradition of research that uses sim-
ple referential communication tasks to explore the
organization and processing of human?computer
and mediated human?human conversation, includ-
ing recently (DeVault and Stone, 2007; Gergle
et al, 2007; Healey and Mills, 2006; Schlangen
and Ferna?ndez, 2007). Our specific task is a two-
player object-identification game adapted from the
experiments of Clark and Wilkes-Gibbs (1986)
and Brennan and Clark (1996); see Section 2.1.
To play this game, our agent, COREF, inter-
prets utterances as performing sequences of task-
specific problem-solving acts using a combination
of grammar-based constraint inference and abduc-
tive plan recognition; see Section 2.2. Crucially,
COREF?s capabilities also include the ambiguity
management skills described in Section 2.3, in-
cluding policies for asking and answering clarifi-
cation questions.
2.1 A referential communication task
The game plays out in a special-purpose graphical
interface, which can support either human?human
or human?agent interactions. Two players work
together to create a specific configuration of ob-
jects, or a scene, by adding objects into the scene
one at a time. Their interfaces display the same set
of candidate objects (geometric objects that differ
in shape, color and pattern), but their locations are
shuffled. The shuffling undermines the use of spa-
tial expressions such as ?the object at bottom left?.
Figures 1 and 2 illustrate the different views.1
1Note that in a human?human game, there are literally
two versions of the graphical interface on the separate com-
puters the human participants are using. In a human?agent
interaction, COREF does not literally use the graphical inter-
face, but the information that COREF is provided is limited
to the information the graphical interface would provide to a
human participant. For example, COREF is not aware of the
locations of objects on its partner?s screen.
Present: [c4, Agent], Active: [] Skip this objectContinue (next object) or You (c4:) 
c4: brown diamond
c4: yesHistory  
Candidate Objects    Your scene    
Figure 1: A human user plays an object identifi-
cation game with COREF. The figure shows the
perspective of the user (denoted c4). The user is
playing the role of director, and trying to identify
the diamond at upper right (indicated to the user
by the blue arrow) to COREF.
Present: [c4, Agent], Active: [] Skip this object or You (Agent:) 
c4: brown diamond
c4: yesHistory  
Candidate Objects    Your scene    
Figure 2: The conversation of Figure 1 from
COREF?s perspective. COREF is playing the role
of matcher, and trying to determine which object
the user wants COREF to identify.
As in the experiments of Clark and Wilkes-
Gibbs (1986) and Brennan and Clark (1996), one
of the players, who plays the role of director,
instructs the other player, who plays the role of
matcher, which object is to be added next to the
scene. As the game proceeds, the next target ob-
ject is automatically determined by the interface
and privately indicated to the director with a blue
arrow, as shown in Figure 1. (Note that the corre-
sponding matcher?s perspective, shown in Figure
2, does not include the blue arrow.) The director?s
job is then to get the matcher to click on (their ver-
sion of) this target object.
To achieve agreement about the target, the two
players can exchange text through an instant-
messaging modality. (This is the only communi-
185
cation channel.) Each player?s interface provides
a real-time indication that their partner is ?Active?
while their partner is composing an utterance, but
the interface does not show in real-time what is
being typed. Once the Enter key is pressed, the
utterance appears to both players at the bottom of
a scrollable display which provides full access to
all the previous utterances in the dialogue.
When the matcher clicks on an object they be-
lieve is the target, their version of that object is pri-
vately moved into their scene. The director has no
visible indication that the matcher has clicked on
an object. However, the director needs to click the
Continue (next object) button (see Fig-
ure 1) in order to move the current target into the
director?s scene, and move on to the next target
object. This means that the players need to discuss
not just what the target object is, but also whether
the matcher has added it, so that they can coordi-
nate on the right moment to move on to the next
object. If this coordination succeeds, then after
the director and matcher have completed a series
of objects, they will have created the exact same
scene in their separate interfaces.
2.2 Interpreting user utterances
COREF treats interpretation broadly as a prob-
lem of abductive intention recognition (Hobbs et
al., 1993).2 We give a brief sketch here to high-
light the content of COREF?s representations, the
sources of information that COREF uses to con-
struct them, and the demands they place on disam-
biguation. See DeVault (2008) for full details.
COREF?s utterance interpretations take the
form of action sequences that it believes would
constitute coherent contributions to the dialogue
task in the current context. Interpretations are con-
structed abductively in that the initial actions in
the sequence need not be directly tied to observ-
able events; they may be tacit in the terminology
of Thomason et al (2006). Examples of such tacit
actions include clicking an object, initiating a clar-
ification, or abandoning a previous question. As
a concrete example, consider utterance (1b) from
the dialogue of Figure 1, repeated here as (1):
(1) a. COREF: is the target round?
b. c4: brown diamond
c. COREF: do you mean dark brown?
d. c4: yes
2In fact, the same reasoning interprets utterances, button
presses and the other actions COREF observes!
In interpreting (1b), COREF hypothesizes that the
user has tacitly abandoned the agent?s question in
(1a). In fact, COREF identifies two possible inter-
pretations for (1b):
i2,1= ? c4:tacitAbandonTasks[2],
c4:addcr[t7,rhombus(t7)],
c4:setPrag[inFocus(t7)],
c4:addcr[t7,saddlebrown(t7)]?
i2,2= ? c4:tacitAbandonTasks[2],
c4:addcr[t7,rhombus(t7)],
c4:setPrag[inFocus(t7)],
c4:addcr[t7,sandybrown(t7)]?
Both interpretations begin by assuming that
user c4 has tacitly abandoned the previous ques-
tion, and then further analyze the utterance as per-
forming three additional dialogue acts. When a di-
alogue act is preceded by tacit actions in an inter-
pretation, the speaker of the utterance implicates
that the earlier tacit actions have taken place (De-
Vault, 2008). These implicatures are an important
part of the interlocutors? coordination in COREF?s
dialogues, but they are a major obstacle to annotat-
ing interpretations by hand.
Action sequences such as i2,1 and i2,2 are coher-
ent only when they match the state of the ongoing
referential communication game and the seman-
tic and pragmatic status of information in the dia-
logue. COREF tracks these connections by main-
taining a probability distribution over a set of di-
alogue states, each of which represents a possi-
ble thread that resolves the ambiguities in the di-
alogue history. For performance reasons, COREF
entertains up to three alternative threads of inter-
pretation; COREF strategically drops down to the
single most probable thread at the moment each
object is completed. Each dialogue state repre-
sents the stack of processes underway in the ref-
erential communication game; constituent activi-
ties include problem-solving interactions such as
identifying an object, information-seeking interac-
tions such as question?answer pairs, and ground-
ing processes such as acknowledgment and clari-
fication. Dialogue states also represent pragmatic
information including recent utterances and refer-
ents which are salient or in focus.
COREF abductively recognizes the intention I
of an actor in three steps. First, for each dia-
logue state sk, COREF builds a horizon graph of
possible tacit action sequences that could be as-
sumed coherently, given the pending tasks (De-
Vault, 2008).
Second, COREF uses the horizon graph and
other resources to solve any constraints associ-
186
ated with the observed action. This step instanti-
ates any free parameters associated with the action
to contextually relevant values. For utterances,
the relevant constraints are identified by parsing
the utterance using a hand-built, lexicalized tree-
adjoining grammar. In interpreting (1b), the parse
yields an ambiguity in the dialogue act associated
with the word ?brown?, which may mean either
of the two shades of brown in Figure 1, which
COREF distinguishes using its saddlebrown
and sandybrown concepts.
Once COREF has identified a set of interpre-
tations {it,1, ..., it,n} for an utterance o at time t,
the last step is to assign a probability to each. In
general, we conceive of this following Hobbs et
al. (1993): the agent should weigh the different
assumptions that went into constructing each in-
terpretation.3 Ultimately, this process should be
made sensitive to the rich range of factors that
are available from COREF?s deep representation
of the dialogue state and the input utterance?this
is our project in this paper. However, in our initial
implemented prototype, COREF assigned these
probabilities using a simple hand-built model con-
sidering only NT, the number of tacit actions ab-
ductively assumed to occur in an interpretation:
P (I = it,j |o, St = sk) ?
1
NT(it,j) + 1
(1)
In effect, this is a ?null hypothesis? that assigns
relatively uniform weights to different abductive
hypotheses.
2.3 Interactive disambiguation
COREF uses its probabilistic model of context in
order to tolerate ambiguity as it moves forward
with its dialogues and to resolve ambiguity over
time. We have put particular effort into COREF?s
skills with three kinds of ambiguity: word-sense
ambiguities, where COREF finds multiple resolu-
tions for the domain concept evoked by the use
of a lexical item, as in the interaction (1) of Fig-
ure 1; referential ambiguities, where COREF takes
a noun phrase to be compatible with multiple ob-
jects from the display; and speech act ambiguities,
where alternative interpretations communicate or
implicate different kinds of contributions to the
ongoing task.
The resolution of ambiguity may involve some
combination of asking questions of the user, ag-
3Though note that Hobbs et al do not explicitly construe
their weights in terms of probabilities.
Figure 3: A human user plays the director role in
an object identification game with COREF. The
figure shows the perspective of the user.
gregating information provided by the user across
multiple turns of dialogue, and strategically drop-
ping threads of interpretation. For example,
COREF represents the context resulting from (1b)
in terms of two states: one from interpretation i2,1
and one from i2,2. COREF asks a clarification
question (1c); the user?s explicit answer yes allows
COREF to discard one of the possible states and
allocate all its probability mass to the other one.
The dialogue in (2) shows an alternative case.
(2) a. c1: color is orange
b. c1: shape is diamond
c. COREF: ok
The example is taken from the setting shown in
Figure 3. In this case, COREF finds two colors on
the screen it thinks the user could intend to evoke
with the word orange; the peachy orange of the di-
amond and circle on the top row and the brighter
orange of the solid and empty squares in the mid-
dle column. COREF responds to the ambiguity by
introducing two states which track the alternative
colors. Immediately COREF gets an additional
description from the user, and adds the constraint
that the object is a diamond. As there is no bright
orange diamond, there is no way to interpret the
user?s utterance in the bright orange state; COREF
discards this state and allocates all its probability
mass to the other one.
3 Inferring the fates of interpretations
Our approach is based on the observation that
COREF?s contribution tracking can be viewed as
assigning a fate to every dialogue state it enter-
tains as part of some thread of interpretation. In
187
particular, if we consider the agent?s contribution
tracking retrospectively, every dialogue state can
be assigned a fate of correct or incorrect, where a
state is viewed as correct if it or some of its descen-
dants eventually capture all the probability mass
that COREF is distributing across the viable sur-
viving states, and incorrect otherwise.
In general, there are two ways that a state can
end up with fate incorrect. One way is that the
state and all of its descendants are eventually de-
nied any probability mass due to a failure to in-
terpret a subsequent utterance or action as a co-
herent contribution from any of those states. In
this case, we say that the incorrect state was elimi-
nated. The second way a state can end up incorrect
is if COREF makes a strategic decision to drop the
state, or all of its surviving descendants, at a time
when the state or its descendants were assigned
nonzero probability mass. In this case we say that
the incorrect state was dropped. Meanwhile, be-
cause COREF drops all states but one after each
object is completed, there is a single hypothesized
state at each time t whose descendants will ulti-
mately capture all of COREF?s probability mass.
Thus, for each time t, COREF will retrospectively
classify exactly one state as correct.
Of course, we really want to classify interpre-
tations. Because we seek to estimate P (I =
it,j |o, St = sk), which conditions the probability
assigned to I = it,j on the correctness of state
sk, we consider only those interpretations arising
in states that are retrospectively identified as cor-
rect. For each such interpretation, we start from
the state where that interpretation is adopted and
trace forward to a correct state or to its last surviv-
ing descendant. We classify the interpretation the
same way as that final state, either correct, elimi-
nated, or dropped.
We harvested a training set using this method-
ology from the transcripts of a previous evaluation
experiment designed to exercise COREF?s ambi-
guity management skills. The data comes from
20 subjects?most of them undergraduates par-
ticipating for course credit?who interacted with
COREF over the web in three rounds of the ref-
erential communication each. The number of ob-
jects increased from 4 to 9 to 16 across rounds;
the roles of director and matcher alternated in each
round, with the initial role assigned at random.
Of the 3275 sensory events that COREF in-
terpreted in these dialogues, from the (retrospec-
N Percentage N Percentage
0 10.53 5 0.21
1 79.76 6 0.12
2 7.79 7 0.09
3 0.85 8 0.06
4 0.58 9 0.0
Figure 4: Distribution of degree of ambiguity in
training set. The table lists percentage of events
that had a specific number N of candidate inter-
pretations constructed from the correct state.
tively) correct state, COREF hypothesized 0 inter-
pretations for 345 events, 1 interpretation for 2612
events, and more than one interpretation for 318
events. The overall distribution in the number of
interpretations hypothesized from the correct state
is given in Figure 4.
4 Learning pragmatic interpretation
We capture the fate of each interpretation it,j in a
discrete variable F whose value is correct, elimi-
nated, or dropped. We also represent each inten-
tion it,j , observation o, and state sk in terms of
features. We seek to learn a function
P (F = correct | features(it,j),
features(o),
features(sk))
from a set of training examples E = {e1, ..., en}
where, for l = 1..n, we have:
el = ( F = fate(it,j), features(it,j),
features(o), features(sk)).
We chose to train maximum entropy models
(Berger et al, 1996). Our learning framework is
described in Section 4.1; the results in Section 4.2.
4.1 Learning setup
We defined a range of potentially useful features,
which we list in Figures 5, 6, and 7. These fea-
tures formalize pragmatic distinctions that plau-
sibly provide evidence of the correct interpreta-
tion for a user utterance or action. You might
annotate any of these features by hand, but com-
puting them automatically lets us easily explore a
much larger range of possibilities. To allow these
various kinds of features (integer-valued, binary-
valued, and string-valued) to interface to the max-
imum entropy model, these features were con-
verted into a much broader class of indicator fea-
tures taking on a value of either 0.0 or 1.0.
188
feature set description
NumTacitActions The number of tacit actions in it,j .
TaskActions These features represent the action type (function symbol) of
each action ak in it,j = ?A1 : a1, A2 : a2, ..., An : an?, as a
string.
ActorDoesTaskAction For each Ak : ak in it,j = ?A1 : a1, A2 : a2, ..., An : an?, a
feature indicates that Ak (represented as string ?Agent? or
?User?) has performed action ak (represented as a string
action type, as in the TaskActions features).
Presuppositions If o is an utterance, we include a string representation of each
presupposition assigned to o by it,j . The predicate/argument
structure is captured in the string, but any gensym identifiers
within the string (e.g. target12) are replaced with
exemplars for that identifier type (e.g. target).
Assertions If o is an utterance, we include a string representation of each
dialogue act assigned to o by it,j . Gensym identifiers are
filtered as in the Presuppositions features.
Syntax If o is an utterance, we include a string representation of the
bracketed phrase structure of the syntactic analysis assigned to
o by it,j . This includes the categories of all non-terminals in
the structure.
FlexiTaskIntentionActors Given it,j = ?A1 : a1, A2 : a2, ..., An : an?, we include a single
string feature capturing the actor sequence ?A1, A2, ..., An? in
it,j (e.g. ?User, Agent, Agent?).
Figure 5: The interpretation features, features(it,j), available for selection in our learned model.
feature set description
Words If o is an utterance, we include features that indicate the
presence of each word that occurs in the utterance.
Figure 6: The observation features, features(o), available for selection in our learned model.
feature set description
NumTasksUnderway The number of tasks underway in sk.
TasksUnderway The name, stack depth, and current task state for each task
underway in sk.
NumRemainingReferents The number of objects yet to be identified in sk.
TabulatedFacts String features representing each proposition in the
conversational record in sk (with filtered gensym identifiers).
CurrentTargetConstraints String features for each positive and negative constraint on the
current target in sk (with filtered gensym identifiers). E.g.
?positive: squareFigureObject(target)? or
?negative: solidFigureObject(target)?.
UsefulProperties String features for each property instantiated in the experiment
interface in sk. E.g. ?squareFigureObject?,
?solidFigureObject?, etc.
Figure 7: The dialogue state features, features(sk), available for selection in our learned model.
189
We used the MALLET maximum entropy clas-
sifier (McCallum, 2002) as an off-the-shelf, train-
able maximum entropy model. Each run involved
two steps. First, we applied MALLET?s feature
selection algorithm, which incrementally selects
features (as well as conjunctions of features) that
maximize an exponential gain function which rep-
resents the value of the feature in predicting in-
terpretation fates. Based on manual experimenta-
tion, we chose to have MALLET select about 300
features for each learned model. In the second
step, the selected features were used to train the
model to estimate probabilities. We used MAL-
LET?s implementation of Limited-Memory BFGS
(Nocedal, 1980).
4.2 Evaluation
We are generally interested in whether COREF?s
experience with previous subjects can be lever-
aged to improve its interactions with new sub-
jects. Therefore, to evaluate our approach, while
making maximal use of our available data set, we
performed a hold-one-subject-out cross-validation
using our 20 human subjects H = {h1, ..., h20}.
That is, for each subject hi, we trained a model
on the training examples associated with subjects
H \ {hi}, and then tested the model on the exam-
ples associated with subject hi.
To quantify the performance of the learned
model in comparison to our baseline, we adapt
the mean reciprocal rank statistic commonly used
for evaluation in information retrieval (Vorhees,
1999). We expect that a system will use the prob-
abilities calculated by a disambiguation model to
decide which interpretations to pursue and how to
follow them up through the most efficient interac-
tion. What matters is not the absolute probability
of the correct interpretation but its rank with re-
spect to competing interpretations. Thus, we con-
sider each utterance as a query; the disambigua-
tion model produces a ranked list of responses for
this query (candidate interpretations), ordered by
probability. We find the rank r of the correct in-
terpretation in this list and measure the outcome
of the query as 1r . Because of its weak assump-
tions, our baseline disambiguation model actually
leaves many ties. So in fact we must compute an
expected reciprocal rank (ERR) statistic that aver-
ages 1r over all ways of ordering the correct inter-
pretation against competitors of equal probability.
Figure 8 shows a histogram of ERR across
ERR range Hand-built
model
Learned
models
1 20.75% 81.76%
[12 , 1) 74.21% 16.35%
[13 ,
1
2) 3.46% 1.26%
[0, 13) 1.57% 0.63%
mean(ERR) 0.77 0.92
var(ERR) 0.02 0.03
Figure 8: For the 318 ambiguous sensory events,
the distribution of the expected reciprocal of rank
of the correct interpretation, for the initial, hand-
built model and the learned models in aggregate.
the ambiguous utterances from the corpus. The
learned models correctly resolve almost 82%,
while the baseline model correctly resolves about
21%. In fact, the learned models get much of this
improvement by learning weights to break the ties
in our baseline model. The overall performance
measure for a disambiguation model is the mean
expected reciprocal rank across all examples in the
corpus. The learned model improves this metric to
0.92 from a baseline of 0.77. The difference is un-
ambiguously significant (Wilcoxon rank sum test
W = 23743.5, p < 10?15).
4.3 Selected features
Feature selection during training identified a vari-
ety of syntactic, semantic, and pragmatic features
as useful in disambiguating correct interpretations.
Selections were made from every feature set in
Figures 5, 6, and 7. It was often possible to iden-
tify relevant features as playing a role in successful
disambiguation by the learned models. For exam-
ple, the learned model trained on H \ {c4} deliv-
ered the following probabilities for the two inter-
pretations COREF found for c4?s utterance (1b):
P (I = i2,1|o, S2 = s8923) = 0.665
P (I = i2,2|o, S2 = s8923) = 0.335
The correct interpretation, i2,1, hypothesizes that
the user means saddlebrown, the darker of the
two shades of brown in the display. Among the
features selected in this model is a Presupposi-
tions feature (see Figure 5) which is present just
in case the word ?brown? is interpreted as mean-
ing saddlebrown rather than some other shade.
This feature allows the learned model to prefer
to interpret c4?s use of ?brown? as meaning this
190
darker shade of brown, based on the observed lin-
guistic behavior of other users.
5 Results in context
Our work adds to a body of research learning deep
models of language from evidence implicit in an
agent?s interactions with its environment. It shares
much of its motivation with co-training (Blum and
Mitchell, 1998) in improving initial models by
leveraging additional data that is easy to obtain.
However, as the examples of Section 2.3 illustrate,
COREF?s interactions with its users offer substan-
tially more information about interpretation than
the raw text generally used for co-training. Closer
in spirit is AI research on learning vocabulary
items by connecting user vocabulary to the agent?s
perceptual representations at the time of utterance
(Oates et al, 2000; Roy and Pentland, 2002; Co-
hen et al, 2002; Yu and Ballard, 2004; Steels
and Belpaeme, 2005). Our framework augments
this information about utterance context with ad-
ditional evidence about meaning from linguistic
interaction. In general, dialogue coherence is an
important source of evidence for all aspects of lan-
guage, for both human language learning (Saxton
et al, 2005) as well as machine models. For exam-
ple, Bohus et al (2008) use users? confirmations
of their spoken requests in a multi-modal interface
to tune the system?s ASR rankings for recognizing
subsequent utterances.
Our work to date has a number of limitations.
First, although 318 ambiguous interpretations did
occur, this user study provided a relatively small
number of ambiguous interpretations, in machine
learning terms; and most (80.2%) of those that did
occur were 2-way ambiguities. A richer domain
would require both more data and a generative ap-
proach to model-building and search.
Second, this learning experiment has been per-
formed after the fact, and we have not yet inves-
tigated the performance of the learned model in a
follow-up experiment in which COREF uses the
learned model in interactions with its users.
A third limitation lies in the detection of
?correct? interpretations. Our scheme some-
times conflates the user?s actual intentions with
COREF?s subsequent assumptions about them. If
COREF decides to strategically drop the user?s
actual intended interpretation, our scheme may
mark another interpretation as ?correct?. Alterna-
tive approaches may do better at harvesting mean-
ingful examples of correct and incorrect interpre-
tations from an agent?s dialogue experience. Our
approach also depends on having clear evidence
about what an interlocutor has said and whether
the system has interpreted it correctly?evidence
that is often unavailable with spoken input or
information-seeking tasks. Thus, even when spo-
ken language interfaces use probabilistic inference
for dialogue management (Williams and Young,
2007), new techniques may be needed to mine
their experience for correct interpretations.
6 Conclusion
We have implemented a system COREF that
makes productive use of its dialogue experience by
learning to rank new interpretations based on fea-
tures it has historically associated with correct ut-
terance interpretations. We present these results as
a proof-of-concept that contribution tracking pro-
vides a source of information that an agent can
use to improve its statistical interpretation process.
Further work is required to scale these techniques
to richer dialogue systems, and to understand the
best architecture for extracting evidence from an
agent?s interpretive experience and modeling that
evidence for future language use. Nevertheless,
we believe that these results showcase how judi-
cious system-building efforts can lead to dialogue
capabilities that defuse some of the bottlenecks to
learning rich pragmatic interpretation. In particu-
lar, a focus on improving our agents? basic abilities
to tolerate and resolve ambiguities as a dialogue
proceeds may prove to be a valuable technique for
improving the overall dialogue competence of the
agents we build.
Acknowledgments
This work was sponsored in part by NSF CCF-
0541185 and HSD-0624191, and by the U.S.
Army Research, Development, and Engineering
Command (RDECOM). Statements and opinions
expressed do not necessarily reflect the position or
the policy of the Government, and no official en-
dorsement should be inferred. Thanks to our re-
viewers, Rich Thomason, David Traum and Jason
Williams.
191
References
Adam L. Berger, Stephen Della Pietra, and Vincent
J. Della Pietra. 1996. A maximum entropy ap-
proach to natural language processing. Computa-
tional Linguistics, 22(1):39?71.
Avrim Blum and Tom Mitchell. 1998. Combining la-
beled and unlabeled data with co-training. In Pro-
ceedings of the 11th Annual Conference on Compu-
tational Learning Theory, pages 92?100.
Dan Bohus, Xiao Li, Patrick Nguyen, and Geoffrey
Zweig. 2008. Learning n-best correction models
from implicit user feedback in a multi-modal local
search application. In The 9th SIGdial Workshop on
Discourse and Dialogue.
Susan E. Brennan and Herbert H. Clark. 1996. Con-
ceptual pacts and lexical choice in conversation.
Journal of Experimental Psychology, 22(6):1482?
1493.
Herbert H. Clark and DeannaWilkes-Gibbs. 1986. Re-
ferring as a collaborative process. In Philip R. Co-
hen, Jerry Morgan, and Martha E. Pollack, editors,
Intentions in Communication, pages 463?493. MIT
Press, Cambridge, Massachusetts, 1990.
Paul R. Cohen, Tim Oates, Carole R. Beal, and Niall
Adams. 2002. Contentful mental states for robot
baby. In Eighteenth national conference on Artifi-
cial intelligence, pages 126?131, Menlo Park, CA,
USA. American Association for Artificial Intelli-
gence.
David DeVault and Matthew Stone. 2007. Managing
ambiguities across utterances in dialogue. In Pro-
ceedings of the 11th Workshop on the Semantics and
Pragmatics of Dialogue (Decalog 2007), pages 49?
56.
David DeVault. 2008. Contribution Tracking: Par-
ticipating in Task-Oriented Dialogue under Uncer-
tainty. Ph.D. thesis, Department of Computer Sci-
ence, Rutgers, The State University of New Jersey,
New Brunswick, NJ.
Barbara Di Eugenio, Pamela W. Jordan, Richmond H.
Thomason, and Johanna D. Moore. 2000. The
agreement process: An empirical investigation of
human-human computer-mediated collaborative di-
alogue. International Journal of Human-Computer
Studies, 53:1017?1076.
Darren Gergle, Carolyn P. Rose?, and Robert E. Kraut.
2007. Modeling the impact of shared visual infor-
mation on collaborative reference. InCHI 2007 Pro-
ceedings, pages 1543?1552.
Patrick G. T. Healey and Greg J. Mills. 2006. Partic-
ipation, precedence and co-ordination in dialogue.
In Proceedings of Cognitive Science, pages 1470?
1475.
Jerry R. Hobbs, Mark Stickel, Douglas Appelt, and
Paul Martin. 1993. Interpretation as abduction. Ar-
tificial Intelligence, 63:69?142.
Pamela W. Jordan and Marilyn A. Walker. 2005.
Learning content selection rules for generating ob-
ject descriptions in dialogue. JAIR, 24:157?194.
Andrew McCallum. 2002. MALLET: A
MAchine learning for LanguagE toolkit.
http://mallet.cs.umass.edu.
Jorge Nocedal. 1980. Updating quasi-newton matrices
with limited storage. Mathematics of Computation,
35(151):773?782.
Tim Oates, Zachary Eyler-Walker, and Paul R. Co-
hen. 2000. Toward natural language interfaces for
robotic agents. In Proc. Agents, pages 227?228.
Massimo Poesio and Ron Artstein. 2005. Annotating
(anaphoric) ambiguity. In Proceedings of the Cor-
pus Linguistics Conference.
Massimo Poesio and Renata Vieira. 1998. A corpus-
based investigation of definite description use. Com-
putational Linguistics, 24(2):183?216.
Deb Roy and Alex Pentland. 2002. Learning words
from sights and sounds: A computational model.
Cognitive Science, 26(1):113?146.
Matthew Saxton, Carmel Houston-Price, and Natasha
Dawson. 2005. The prompt hypothesis: clarifica-
tion requests as corrective input for grammatical er-
rors. Applied Psycholinguistics, 26(3):393?414.
David Schlangen and Raquel Ferna?ndez. 2007. Speak-
ing through a noisy channel: Experiments on in-
ducing clarification behaviour in human?human di-
alogue. In Proceedings of Interspeech 2007.
Luc Steels and Tony Belpaeme. 2005. Coordinating
perceptually grounded categories through language.
a case study for colour. Behavioral and Brain Sci-
ences, 28(4):469?529.
Richmond H. Thomason, Matthew Stone, and David
DeVault. 2006. Enlightened update: A
computational architecture for presupposition and
other pragmatic phenomena. For the Ohio
State Pragmatics Initiative, 2006, available at
http://www.research.rutgers.edu/?ddevault/.
Ellen M. Vorhees. 1999. The TREC-8 question an-
swering track report. In Proceedings of the 8th Text
Retrieval Conference, pages 77?82.
Jason Williams and Steve Young. 2007. Partially
observable markov decision processes for spoken
dialog systems. Computer Speech and Language,
21(2):393?422.
Chen Yu and Dana H. Ballard. 2004. A multimodal
learning interface for grounding spoken language in
sensory perceptions. ACM Transactions on Applied
Perception, 1:57?80.
192
c? 2003 Association for Computational Linguistics
Anaphora and Discourse Structure
Bonnie Webber? Matthew Stone?
Edinburgh University Rutgers University
Aravind Joshi? Alistair Knott?
University of Pennsylvania University of Otago
We argue in this article that many common adverbial phrases generally taken to signal a discourse
relation between syntactically connected units within discourse structure instead work anaphor-
ically to contribute relational meaning, with only indirect dependence on discourse structure.
This allows a simpler discourse structure to provide scaffolding for compositional semantics and
reveals multiple ways in which the relational meaning conveyed by adverbial connectives can
interact with that associated with discourse structure. We conclude by sketching out a lexicalized
grammar for discourse that facilitates discourse interpretation as a product of compositional rules,
anaphor resolution, and inference.
1. Introduction
It is a truism that a text means more than the sum of its component sentences. One
source of additional meaning are relations taken to hold between adjacent sentences
?syntactically? connected within a larger discourse structure. It has been very difficult,
however, to say what discourse relations there are, either theoretically (Mann and
Thompson 1988; Kehler 2002; Asher and Lascarides 2003) or empirically (Knott 1996).
Knott?s empirical attempt to identify and characterize cue phrases as evidence
for discourse relations illustrates some of the difficulties. Knott used the following
theory-neutral test to identify cue phrases: For a potential cue phrase ? in naturally
occurring text, consider in isolation the clause in which it appears. If the clause ap-
pears incomplete without an adjacent left context, whereas it appears complete if ? is
removed, then ? is a cue phrase. Knott?s test produced a nonexhaustive list of about
two hundred different phrases from 226 pages of text. He then attempted to charac-
terize the discourse relation(s) conveyed by each phrase by identifying when (always,
sometimes, never) one phrase could substitute for another in a way that preserved
meaning. He showed how these substitution patterns could be a consequence of a set
of semantic features and their values. Roughly speaking, one cue phrase could always
substitute for another if it had the same set of features and values, sometimes do so if
it was less specific than the other in terms of its feature values, and never do so if their
values conflicted for one or more features.
? School of Informatics, University of Edinburgh, 2 Buccleuch Place, Edinburgh, EH8 9LW, UK. E-mail:
bonnie@inf.ed.ac.uk.
? Department of Computer Science, Rutgers Universtiy, 110 Frelinghuysen Road, Piscataway, NJ
08854-8019. E-mail: mdstone@cs.rutgers.edu.
? Department of Computer & Information Science, University of Pennsylvania, 200 South 33rd Street,
Philadelphia, PA 19104-6389. E-mail: joshi@linc.cis.upenn.edu.
? Department of Computer Science, University of Otago, P.O. Box 56, DUNEDIN 9015, New Zealand.
E-mail: alik@cs.otago.ac.nz.
546
Computational Linguistics Volume 29, Number 4
By assuming that cue phrases contribute meaning in a uniform way, Knott was
led to a set of surprisingly complex directed acyclic graphs relating cue phrases in
terms of features and their values, each graph loosely corresponding to some family of
discourse relations. But what if the relational meaning conveyed by cue phrases could
in fact interact with discourse meaning in multiple ways? Then Knott?s substitution
patterns among cue phrases may have reflected these complex interactions, as well as
the meanings of individual cue phrases themselves.
This article argues that cue phrases do depend on another mechanism for convey-
ing extrasentential meaning?specifically, anaphora. One early hint that adverbial cue
phrases (called here discourse connectives) might be anaphoric can be found in an
ACL workshop paper in which Janyce Wiebe (1993) used the following example to
question the adequacy of tree structures for discourse:
(1) a. The car was finally coming toward him.
b. He [Chee] finished his diagnostic tests,
c. feeling relief.
d. But then the car started to turn right.
The problem Wiebe noted was that the discourse connectives but and then appear to
link clause (1d) to two different things: then to clause (1b) in a sequence relation (i.e.,
the car?s starting to turn right being the next relevant event after Chee?s finishing his
tests) and but to a grouping of clauses (1a) and (1c) (i.e., reporting a contrast between,
on the one hand, Chee?s attitude toward the car coming toward him and his feeling
of relief and, on the other hand, his seeing the car turning right). (Wiebe doesn?t give
a name to the relation she posits between (1d) and the grouping of (1a) and (1c), but
it appears to be some form of contrast.)
If these relations are taken to be the basis for discourse structure, some possible
discourse structures for this example are given in Figure 1. Such structures might seem
advantageous in allowing the semantics of the example to be computed directly by
compositional rules and defeasible inference. However, both structures are directed
acyclic graphs (DAGs), with acyclicity the only constraint on what nodes can be con-
nected. Viewed syntactically, arbitrary DAGs are completely unconstrained systems.
They substantially complicate interpretive rules for discourse, in order for those rules
to account for the relative scope of unrelated operators and the contribution of syn-
tactic nodes with arbitrarily many parents.1
We are not committed to trees as the limiting case of discourse structure. For
example, we agree, by and large, with the analysis that Bateman (1999) gives of
(2) (vi) The first to do that were the German jewellers, (vii) in particular Klaus
Burie. (viii) And Morris followed very quickly after, (ix) using a lacquetry
technique to make the brooch, (x) and using acrylics, (xi) and exploring
the use of colour, (xii) and colour is another thing that was new at that
time.
1 A reviewer has suggested an alternative analysis of (1) in which clause (1a) is elaborated by clause
(1b), which is in turn elaborated by (1c), and clause (1d) stands in both a sequence relation and a
contrast relation to the segment as a whole. Although this might address Wiebe?s problem, the result is
still a DAG, and such a fix will not address the additional examples we present in section 2, in which a
purely structural account still requires DAGs with crossing arcs.
547
Webber et al Anaphora and Discourse Structure
b
seq
d
contrast
c
cb
elaboration
elaboration
a seq
a d
seq contrast
(i)
(ii)
Figure 1
Possible discourse structure for example (1). Each root and internal node is labeled by the type
of relation that Wiebe takes to hold between the daughters of that node. (i) uses an n-ary
branching sequence relation, whereas in (ii), sequence is binary branching.
(ix)(vi)
succession manner
(viii)
Figure 2
Simple multiparent structure.
in which clause (ix) stands in a manner relation with clause (viii), which in turn stands
in a succession (i.e., sequence) relation with clause (vi). This is illustrated in Figure 2,
which shows a DAG (rather than a tree), but without crossing dependencies.
So it is the cost of moving to arbitrary DAGs for discourse structure that we feel is
too great to be taken lightly. This is what has led us to look for another explanation for
these and other examples of apparent complex and crossing dependencies in discourse.
The position we argue for in this article, is that whereas adjacency and explicit
conjunction (coordinating conjunctions such as and, or, so, and but; subordinating con-
junctions such as although, whereas, and when) imply discourse relations between (the
interpretation of) adjacent or conjoined discourse units, discourse adverbials such as
then, otherwise, nevertheless, and instead are anaphors, signaling a relation between the
interpretation of their matrix clause and an entity in or derived from the discourse
context. This position has four advantages:
1. Understanding discourse adverbials as anaphors recognizes their
behavioral similarity to the pronouns and definite noun phrases (NPs)
that are the bread and butter of previous work on anaphora. This is
discussed in section 2.
2. By understanding and exploring the full range of phenomena for which
an anaphoric account is appropriate, we can better characterize anaphors
and devise more accurate algorithms for resolving them. This is explored
in section 3.
3. Any theory of discourse must still provide an account of how a sequence
of adjacent discourse units (clauses, sentences, and the larger units that
they can comprise) means more than just the sum of its component
548
Computational Linguistics Volume 29, Number 4
units. This is a goal that researchers have been pursuing for some time,
using both compositional rules and defeasible inference to determine
these additional aspects of meaning (Asher and Lascarides 1999; Gardent
1997; Hobbs et al 1993; Kehler 2002; Polanyi and van den Berg 1996;
Scha and Polanyi 1988; Schilder 1997a, 1997b; van den Berg 1996) If that
portion of discourse semantics that can be handled by mechanisms
already needed for resolving other forms of anaphora and deixis is
factored out, there is less need to stretch and possibly distort
compositional rules and defeasible inference to handle everything.2
Moreover, recognizing the possibility of two separate relations (one
derived anaphorically and one associated with adjacency and/or a
structural connective) admits additional richness to discourse semantics.
Both points are discussed further in section 4.
4. Understanding discourse adverbials as anaphors allows us to see more
clearly how a lexicalized approach to the computation of clausal syntax
and semantics extends naturally to the computation of discourse syntax
and semantics, providing a single syntactic and semantic matrix with
which to associate speaker intentions and other aspects of pragmatics
(section 5.)
The account we provide here is meant to be compatible with current approaches
to discourse semantics such as DRT (Kamp and Reyle 1993; van Eijck and Kamp
1997), dynamic semantics (Stokhof and Groenendijk 1999), and even SDRT (Asher
1993; Asher and Lascarides 2003), understood as a representational scheme rather
than an interpretive mechanism. It is also meant to be compatible with more detailed
analyses of the meaning and use of individual discourse adverbials, such as Jayes
and Rossari (1998a, 1998b) and Traugott, (1995, 1997). It provides what we believe to
be a more coherent account of how discourse meaning is computed, rather than an
alternative account of what that meaning is or what speaker intentions it is being used
to achieve.
2. Discourse Adverbials as Anaphors
2.1 Discourse Adverbials Do Not Behave like Structural Connectives
We take the building blocks of the most basic level of discourse structure to be explicit
structural connectives between adjacent discourse units (i.e., coordinating and subor-
dinating conjunctions and ?paired? conjunctions such as not only . . . but also, and on the
one hand . . . on the other (hand) and inferred relations between adjacent discourse units
(in the absense of an explicit structural connective). Here, adjacency is what triggers
the inference. Consider the following example:
(3) You shouldn?t trust John. He never returns what he borrows.
Adjacency leads the hearer to hypothesize that a discourse relation of something like
explanation holds between the two clauses. Placing the subordinate conjunction (struc-
tural connective) because between the two clauses provides more evidence for this rela-
2 There is an analogous situation at the sentence level, where the relationship between syntactic structure
and compositional semantics is simplified by factoring away intersentential anaphoric relations. Here
the factorization is so obvious that one does not even think about any other possibility.
549
Webber et al Anaphora and Discourse Structure
tion. Our goal in this section is to convince the reader that many discourse adverbials,
including then, also, otherwise, nevertheless, and instead, do not behave in this way.
Structural connectives and discourse adverbials do have one thing in common:
Like verbs, they can both be seen as heading a predicate-argument construction; unlike
verbs, their arguments are independent clauses. For example, both the subordinate
conjunction after and the adverbial then (in its temporal sense) can be seen as binary
predicates (e.g., sequence) whose arguments are clausally derived events, with the
earlier event in first position and the succeeding event in second.
But that is the only thing that discourse adverbials and structural connectives have
in common. As we have pointed out in earlier papers (Webber, Knott, and Joshi 2001;
Webber et al, 1999a, 1999b), structural connectives have two relevant properties: (1)
they admit stretching of predicate-argument dependencies; and (2) they do not admit
crossing of those dependencies. This is most obvious in the case of preposed subor-
dinate conjunctions (example (4)) or ?paired? coordinate conjunctions (example (5)).
With such connectives, the initial predicate signals that its two arguments will follow.
(4) Although John is generous, he is hard to find.
(5) On the one hand, Fred likes beans. On the other hand, he?s allergic to them.
Like verbs, structural connectives allow the distance between the predicate and its
arguments to be ?stretched? over embedded material, without loss of the dependency
between them. For the verb like and an object argument apples, such stretching without
loss of dependency is illustrated in example (6b).
(6) a. Apples John likes.
b. Apples Bill thinks he heard Fred say John likes.
That this also happens with structural connectives and their arguments is illustrated
in example (7) (in which the first clause of example (4) is elaborated by another pre-
posed subordinate-main clause construction embedded within it) and in example (8)
(in which the first conjunct of example (5) is elaborated by another paired-conjunction
construction embedded within it). Possible discourse structures for these examples are
given in Figure 3.
(7) a. Although John is very generous?
b. if you need some money,
c. you only have to ask him for it?
d. he?s very hard to find.
(8) a. On the one hand, Fred likes beans.
b. Not only does he eat them for dinner.
c. But he also eats them for breakfast and snacks.
d. On the other hand, he?s allergic to them.
But, as already noted, structural connectives do not admit crossing of predicate-
argument dependencies. If we admit crossing dependencies in examples (7) and (8),
we get
(9) a. Although John is very generous?
b. if you need some money?
550
Computational Linguistics Volume 29, Number 4
(i) (ii)
a
b
d
contrast[one/other]
elaboration
comparison[not only/but also]
a
b
d
elaboration
concession[although]
cc
condition[if]
Figure 3
Discourse structures associated with (i) example (7) and (ii) Example (8).
a cb
concession[although] condition[if]
elaboration
a b
elaboration
contrast[one/other] comparison[not only...]
(i) (ii)
dcd
Figure 4
(Impossible) discourse structures that would have to be associated with (i) Example (9) and
(ii) example (10).
c. he?s very hard to find?
d. you only have to ask him for it.
(10) a. On the one hand, Fred likes beans.
b. Not only does he eat them for dinner.
c. On the other hand, he?s allergic to them.
d. But he also eats them for breakfast and snacks.
Possible discourse structures for these (impossible) discourses are given in Figure 4.
Even if the reader finds no problem with these crossed versions, they clearly do not
mean the same thing as their uncrossed counterparts: In (10), but now appears to link
(10d) with (10c), conveying that despite being allergic to beans, Fred eats them for
breakfast and snacks. And although this might be inferred from (8), it is certainly not
conveyed directly. As a consequence, we stipulate that structural connectives do not
admit crossing of their predicate-argument dependencies.3
That is not all. Since we take the basic level of discourse structure to be a conse-
quence of (1) relations associated with explicit structural connectives and (2) relations
3 A reviewer has asked how much stretching is possible in discourse without losing its thread or having
to rephrase later material in light of the intervening material. One could ask a similar question about
the apparently unbounded dependencies of sentence-level syntax, which inattentive speakers are prone
to lose track of and ?fracture.? Neither question seems answerable on theoretical grounds alone, with
both demanding substantial amounts of empirical data from both written and spoken discourse. The
point we are trying to make is simply that there is a difference in discourse between any amount of
stretching and even the smallest amount of crossing.
551
Webber et al Anaphora and Discourse Structure
whose defeasible inference is triggered by adjacency, we stipulate that discourse struc-
ture itself does not admit crossing structural dependencies. (In this sense, discourse structure
may be truly simpler than sentence structure. To verify this, one might examine the
discourse structure of languages such as Dutch that allow crossing dependencies in
sentence-level syntax. Initial cursory examination does not give any evidence of cross-
ing dependencies in Dutch discourse.)
If we now consider the corresponding properties of discourse adverbials, we see
that they do admit crossing of predicate-argument dependencies, as shown in exam-
ples (11)?(13).
(11) a. John loves Barolo.
b. So he ordered three cases of the ?97.
c. But he had to cancel the order
d. because then he discovered he was broke.
(12) a. High heels are fine for going to the theater.
b. But wear comfortable shoes
c. if instead you plan to go to the zoo.
(13) a. Because Fred is ill
b. you will have to stay home
c. whereas otherwise the two of you could have gone to the zoo.
Consider first the discourse adverbial then in clause (11d). For it to get its first
argument from (11b) (i.e., the event that the discovery in (d) is ?after?), it must cross
the structural connection between clauses (c) and (d) associated with because). This
crossing dependency is illustrated in Figure 5(i). Now consider the discourse adverbial
instead) in clause (12c). For it to get its first argument from (12a) (i.e., going to the zoo is
an alternative to going to the theater), it must cross the structural connection between
clauses (12b) and (12c) associated with if. This crossing dependency is illustrated in
Figure 5(ii). Example (13) is its mirror image: For the discourse adverbial otherwise in
(13c) to get its first argument from (13a) (i.e., alternatives to the state/condition of
Fred being ill), it must cross the structural connection associated with because. This is
illustrated in Figure 5(iii).
Crossing dependencies are not unusual in discourse when one considers anaphora
(e.g., pronouns and definite NPs), as for example in
b
conseq[so]
contrast[but]
dc
seq[then]
b
explanation[because]
conditional[if]
a
contrast[but]
alt[instead]
a
c ba
contrast[whereas]
c
explanation?[because]
alt[otherwise]
(i) (ii) (iii)
Figure 5
Discourse structures for examples (11)?(13). Structural dependencies are indicated by solid
lines and dependencies associate with discourse adverbials are indicated by broken lines.
(explanation? is the inverse of explanation?i.e., with its arguments in reverse order. Such
relations are used to maintain the given linear order of clauses.)
552
Computational Linguistics Volume 29, Number 4
(14) Every mani tells every womanj hei meets that shej reminds himi of hisi
mother.
(15) Suei drives an Alfa Romeo. Shei drives too fast. Maryj races heri on week-
ends. Shej often beats heri. (Strube 1998)
This suggests that in examples (11)?(13), the relationship between the discourse ad-
verbial and its (initial) argument from the previous discourse might usefully be taken
to be anaphoric as well.4
2.2 Discourse Adverbials Do Behave like Anaphors
There is additional evidence to suggest that otherwise, then, and other discourse adver-
bials are anaphors. First, anaphors in the form of definite and demonstrative NPs can
take implicit material as their referents. For example, in
(16) Stack five blocks on top of one another. Now close your eyes and try
knocking {the tower, this tower} over with your nose.
both NPs refer to the structure which is the implicit result of the block stacking.
(Further discussion of such examples can be found in Isard [1975]; Dale [1992]; and
Webber and Baldwin [1992].) The same is true of discourse adverbials. In
(17) Do you want an apple? Otherwise you can have a pear.
the situation in which you can have a pear is one in which you don?t want an apple?
that is, one in which your answer to the question is ?no.? But this answer isn?t there
structurally: It is only inferred. Although it appears natural to resolve an anaphor to an
inferred entity, it would be much more difficult to establish such links through purely
structural connections: To do so would require complex transformations that introduce
invisible elements into discourse syntax with no deeper motivation. For example, in
(17), we would need a rule that takes a discourse unit consisting solely of a yes/no
question P? and replaces it with a complex segment consisting of P? and the clause
it is possible that P, with the two related by something like elaboration. Then and only
then could we account for the interpretation of the subsequent otherwise structurally,
by a syntactic link to the covert material (i.e., to the possibility that P holds, which
otherwise introduces an alterative to).
Second, discourse adverbials have a wider range of options with respect to their
initial argument than do structural connectives (i.e., coordinating and subordinating
conjunctions). The latter are constrained to linking a discourse unit on the right frontier
of the evolving discourse (i.e., the clause, sentence and larger discourse units to its
immediate left). Discourse adverbials are not so constrained. To see this, consider the
following example:
4 We are aware that crossing examples such as (11)?(13) are rare in naturally occurring discourse. We
believe that this is because they are only possible when, as here, strong constraints from the discourse
adverbial and from context prevent the adverbial from relating to the closest (leftmost) eventuality or
an eventuality coerced from that one. But rarity doesn?t necessarily mean ill-formedness or marginality,
as readers can see for themselves if they use Google to search the Web for strings such as because then, if
instead, and whereas otherwise, and consider (1) whether the hundreds, even thousands, of texts in which
these strings occur are ill-formed, and (2) what then, instead, and otherwise are relating in these texts.
One must look at rare events if one is studying complex linguistic phenomena in detail. Thus it is not
the case that only common things in language are real or worth further study.
553
Webber et al Anaphora and Discourse Structure
(18) If the light is red, stop. Otherwise you?ll get a ticket.
(If you do something other than stop, you?ll get a ticket.)
This can be paraphrased using the conjunction or:
If the light is red, stop, or you?ll get a ticket.
Here or links its right argument to a unit on the right frontier of the evolving discourse?
in this case, the clause stop on its immediate left. Now consider the related example
(19) If the light is red, stop. Otherwise go straight on.
(If the light is not red, go straight on.)
This cannot be paraphrased with or, as in
(20) If the light is red, stop, or go straight on.
even though both stop and If the light is red, stop are on the right frontier of the evolving
discourse structure. This is because otherwise is accessing something else, so that (20)
means something quite different from either (18) or (19). What otherwise is accessing,
which or cannot, is the interpretation of the condition alone.5 Thus discourse adver-
bials, like other anaphors, have access to material that is not available to structural
connectives.
Finally, discourse adverbials, like other anaphors, may require semantic represen-
tations in which their arguments are bound variables ranging over discourse entities.
That is, whereas it might be possible to represent Although P, Q using a binary modal
operator
(21) although(p, q)
where formulas p and q translate the sentences P and Q that although combines, we
cannot represent P . . .Nevertheless, Q this way. We need something more like
(22) p ? nevertheless(e, q)
The motivation for the variable e in this representation is that discourse adverbials,
like pronouns, can appear intrasententially in an analog of donkey sentences. Donkey
sentences such as example (23) are a special kind of bound-variable reading:
(23) Every farmer who owns a donkey feeds it rutabagas.
In donkey sentences, anaphors are interpreted as covarying with their antecedents:
The it that is being fed in (23) varies with the farmer who feeds it. These anaphors,
however, appear in a structural and interpretive environment in which a direct syn-
tactic relationship between anaphor and antecedent is normally impossible, so they
cannot be a reflex of true binding in the syntax-semantics interface. Rather, donkey
sentences show that discourse semantics has to provide variables to translate pronouns,
5 This was independently pointed out by several people when this work was presented at ESSLLI?01 in
Helsinki in August 2001. The authors would like to thank Natalia Modjeska, Lauri Karttunen, Mark
Steedman, Robin Cooper, and David Traum for bringing it to their attention.
554
Computational Linguistics Volume 29, Number 4
and that discourse mechanisms must interpret these variables as bound?even though
the pronouns appear ?free? by syntactic criteria.
Thus, it is significant that discourse adverbials can appear in their own version of
donkey sentences, as in
(24) a. Anyone who has developed innovative new software has then had to
hire a lawyer to protect his/her interests. (i.e., after developing innovative
new software)
b. Several people who have developed innovative new software have
nevertheless failed to profit from it. (i.e., despite having developed innovative
new software)
c. Every person selling ?The Big Issue? might otherwise be asking for
spare change. (i.e., if he/she weren?t selling ?The Big Issue?)
The examples in (24) involve binding in the interpretation of discourse adverbials.
In (24a), the temporal use of then locates each hiring event after the corresponding
software development. Likewise in (24b), the adversative use of nevertheless signals
each developer?s unexpected failure to turn the corresponding profit. And in (24c),
otherwise envisions each person?s begging if that person weren?t selling ?The Big Issue?.
Such bound interpretations require variables in the semantic representations and
alternative values for them in some model?hence the representation given in (22).
Indeed, it is clear that the binding here has to be the discourse kind, not the syntac-
tic kind, for the same reason as in (23), although we cannot imagine anyone arguing
otherwise, since discourse adverbials have always been treated as elements of dis-
course interpretation. So the variables must be the discourse variables usually used to
translate other kinds of discourse anaphors.6
These arguments have been directed at the behavioral similarity between discourse
adverbials and what we normally take to be discourse anaphors. But this isn?t the only
reason to recognize discourse adverbials as anaphors: In the next section, we suggest
a framework for anaphora that is broad enough to include discourse adverbials as
well as definite and demonstrative pronouns and NPs, along with other discourse
phenomena that have been argued to be anaphoric, such as VP ellipsis (Hardt 1992,
1999; Kehler 2002), tense (Partee 1984; Webber 1988) and modality (Kibble 1995; Frank
and Kamp 1997; Stone and Hardt 1999).
3. A Framework for Anaphora
Here we show how only a single extension to a general framework for discourse
anaphora is needed to cover discourse adverbials. The general framework is presented
in Section 3.1, and the extension in Section 3.2.
3.1 Discourse Referents and Anaphor Interpretation
The simplest discourse anaphors are coreferential: definite pronouns and definite NPs
that denote one (or more) discourse referents in focus within the current discourse
6 Although rhetorical structure theory (RST) (Mann and Thompson 1998) was developed as an account
of the relation between adjacent units within a text, Marcu?s guide to RST annotation (Marcu 1999) has
added an ?embedded? version of each RST relation in order to handle examples such as (24). Although
this importantly recognizes that material in an embedded clause can bear a semantic relation to its
matrix clause, it does not contribute to understanding the nature of the phenomenon.
555
Webber et al Anaphora and Discourse Structure
context. (Under coreference we include split reference, in which a plural anaphor such
as the companies denotes all the separately mentioned companies in focus within the
discourse context.) Much has been written about the factors affecting what discourse
referents are taken to be in focus. For a recent review by Andrew Kehler, see chap-
ter 18 of Jurafsky and Martin (2000). For the effect of different types of quantifiers on
discourse referents and focus, see Kibble (1995).
Somewhat more complex than coreference is indirect anaphora (Hellman and
Fraurud 1996) (also called partial anaphora [Luperfoy 1992], textual ellipsis [Hahn,
Markert, and Strube 1996], associative anaphora [Cosse 1996] bridging anaphora
[Clark 1975; Clark and Marshall 1981; Not, Tovena, and Zancanaro 1999], and in-
ferrables [Prince 1992]), in which the anaphor (usually a definite NP) denotes a dis-
course referent associated with one (or more) discourse referents in the current discourse
context; for example,
(25) Myra darted to a phone and picked up the receiver.
Here the receiver denotes the receiver associated with (by virtue of being part of) the
already-mentioned phone Myra darted to.
Coreference and indirect anaphora can be uniformly modeled by saying that the
discourse referent e? denoted by an anaphoric expression ? is either equal to or asso-
ciated with an existing discourse referent er, that is, e?=er or e? ?assoc(er). But coref-
erence and associative anaphora do not exhaust the space of constructs that derive
all or part of their sense from the discourse context and are thus anaphoric. Consider
?other NPs? (Bierner 2001a; Bierner and Webber 2000; Modjeska 2001, 2002), as in:
(26) Sue grabbed one phone, as Tom darted to the other phone.
Although ?other NPs? are clearly anaphoric, should the referent of the other phone
(e?)?the phone other than the one Sue grabbed (er)?simply be considered a case of
e? ? assoc(er)? Here are two reasons why they should not.
First, in all cases of associative anaphora discussed in the literature, possible as-
sociations have depended only on the antecedent er and not on the anaphor. For
example, only antecedents that have parts participate in whole-part associations (e.g.,
phone ? receiver). Only antecedents with functional schemata participate in schema-
based associations (e.g., lock ? key). In (26), the relationship between e?, the referent
of the other phone, and its antecedent, er, depends in part on the anaphor, and not just
on the antecedent?in particular, on the presence of the word other.
Second, we also have examples such as
(27) Sue lifted the receiver as Tom darted to the other phone.7
in which the referent of the other phone (e?) is the phone other than the phone associated
with the receiver that Sue lifted. Together, these two points argue for a third possibility,
in which an anaphoric element can convey a specific function f? that is idiosyncratic
to the anaphor, which may be applied to either er or an associate of er. The result of
that application is e?. For want of a better name, we will call these lexically specified
anaphors.
Other lexically specified anaphors include noun phrases headed by other (exam-
ple (28)), NPs with such but no postmodifying as phrase (example (29)), comparative
7 Modjeska (2001) discovered such examples in the British National Corpus.
556
Computational Linguistics Volume 29, Number 4
NPs with no postmodifying than phrase (example (30)), and the pronoun elsewhere
(example (31)) (Bierner 2001b)
(28) Some dogs are constantly on the move. Others lie around until you call
them.
(29) I saw a 2kg lobster in the fish store yesterday. The fishmonger said it takes
about five years to grow to such a size.
(30) Terriers are very nervous. Larger dogs tend to have calmer dispositions.
(31) I don?t like sitting in this room. Can we move elsewhere?
To summarize the situation with anaphors so far, we have coreference when e?=er,
indirect anaphora when e? ?assoc(er), and lexically specified anaphora when e?=f?(ei)
where ei = er or ei ?assoc(er).
3.2 Discourse Adverbials as Lexical Anaphors
There is nothing in this generalized approach to discourse anaphora that requires that
the source of er be an NP, or that the anaphor be a pronoun or NP. For example, the
antecedent er of a singular demonstrative pronoun (in English, this or that) is often
an eventuality that derives from a clause, a sentence, or a larger unit in the recent
discourse (Asher 1993; Byron 2002; Eckert and Strube 2000; Webber 1991). We will
show that this is the case with discourse adverbials as well.
The extension we make to the general framework presented above in order to
include discourse adverbials as discourse anaphors is to allow more general functions
f? to be associated with lexically specified anaphors. In particular, for the discourse
adverbials considered in this article, the function associated with an adverbial maps its
anaphoric argument?an eventuality derived from the current discourse context?to
a function that applies to the interpretation of the adverbial?s matrix clause (itself an
eventuality). The result is a binary relation that holds between the two eventualities
and is added to the discourse context. For example, in
(32) John loves Barolo. So he ordered three cases of the ?97. But he had to
cancel the order because he then discovered he was broke.
then, roughly speaking, contributes the fact that its matrix clause event (John?s find-
ing he was broke) is after the anaphorically derived event of his ordering the wine.8
Similarly, in
(33) John didn?t have enough money to buy a mango. Instead, he bought a
guava.
instead contributes the fact that its matrix clause event (buying a guava) is an alternative
to the anaphorically derived event of buying a mango. The relation between the two
sentences is something like result, as in So instead, he bought a guava.
8 Words and phrases that function as discourse adverbials usually have other roles as well: For example,
otherwise also serves as an adjectival modifier, as in I was otherwise occupied with grading exams. This
overloading of closed-class lexico-syntactic items is not unusual in English, and any ambiguities that
arise must be handled as part of the normal ambiguity resolution process.
557
Webber et al Anaphora and Discourse Structure
Note that our only concern here is with the compositional and anaphoric mech-
anisms by which adverbials contribute meaning. For detailed analysis of the lexical
semantics of adverbials (but no attention to mechanism), the reader is referred to Jayes
and Rossari (1998a, 1998b, Lagerwerf (1998), Traugott (1995, 1997), and others.
Formally, we represent the function that a discourse adverbial ? contributes as a
?-expression involving a binary relation R? that is idiosyncratic to ?, one of whose
arguments (represented here by the variable EV) is resolved anaphorically:
?x . R?(x, EV)
R? gets its other argument compositionally, when this ?-expression is applied to ??s
matrix clause S interpreted as an eventuality ?, that is,
[?x . R?(x, EV)]? ? R?(?, EV)
The result of both function application and resolving EV to some eventuality ei derived
from the discourse context either directly or by association is the proposition R?(?, ei),
one of whose arguments (ei) has been supplied by the discourse context and the other
(?) compositionally from syntax.
Note that this is a formal model, meant to have no implications for how pro-
cessing takes place. We have not tried at this stage to instantiate our view of how
discourse adverbials are resolved in the context of (simultaneous) sentence-level and
discourse-level processing. Our basic view is that resolution is initiated when the dis-
course adverbial (?) is encountered. As ??s matrix clause S is incrementally parsed and
interpreted, producing eventuality ?, the resolution process polls the discourse context
and either finds an appropriate eventuality ei (or creates one by a bridging inference,
as illustrated in the next section) such that R?(?, ei) makes sense with respect to the
discourse so far. As is the case with resolving a discourse deictic (Asher 1993; Byron
2002; Eckert and Strube 2000; Webber 1991) this resolution process would use syn-
tactic and semantic constraints that it accumulates as the incremental sentence-level
parser/interpreter processes S. As with discourse deixis, this is best seen as a con-
straint satisfaction problem that involves finding or deriving an eventuality from the
current discourse context that meets the constraints of the adverbial with respect to the
eventuality interpretation of the matrix clause. (Examples of this are given throughout
the rest of the article.)
3.3 A Logical Form for Eventualities
Before using this generalized view of anaphora to show what discourse adverbials
contribute to discourse and how they interact with discourse relations that arise from
adjacency or explicit discourse connectives, we briefly describe how we represent
clausal interpretations in logical form (LF).
Essentially, we follow Hobbs (1985) in using a rich ontology and a representation
scheme that makes explicit all the individuals and abstract objects (i.e., propositions,
facts/beliefs, and eventualities) (Asher 1993) involved in the LF interpretation of an
utterance. We do so because we want to make intuitions about individuals, eventual-
ities, lexical meaning, and anaphora as clear as possible. But certainly, other forms of
representation are possible.
In this LF representation scheme, each clause and each relation between clauses
is indexed by the label of its associated abstract object. So, for example, the LF inter-
pretation of the sentence
(34) John left because Mary left.
558
Computational Linguistics Volume 29, Number 4
would be written
e1:left(j) ? john(j) ? e2:left(m) ? mary(m) ? e3:because(e1,e2)
where the first argument of the asymmetric binary predicate because is the consequent
and the second is the eventuality leading to this consequent. Thus when because occurs
sentence-medially, as in the above example, the eventuality arguments are in the same
order as their corresponding clauses occur in the text. When because occurs sentence-
initially (as in Because Mary left, John did), the interpretation of the second clause (John
[left]) will appear as the first argument and the interpretation of the first clause (Mary
left) will appear as the second.9
The set of available discourse referents includes both individuals like j and m, and
also abstract objects like e1 and e2. We then represent resolved anaphors by reusing
these discourse referents. So, for example, the LF interpretation of the follow-on sen-
tence
(35) This upset Sue.
would be written
e4:upset(DPRO, s) ? sue(s)
where DPRO is the anaphoric variable contributed by the demonstrative pronoun this.
Since the subject of upset could be either the eventuality of John?s leaving or the fact
that he left because Mary left, DPRO could be resolved to either e1 or e3, that is,
a. e4:upset(e1, s) ? sue(s)
b. e4:upset(e3, s) ? sue(s)
depending on whether one took Sue to have been upset by (1) John?s leaving or (2)
that he left because Mary left.
3.4 The Contribution of Discourse Adverbials to Discourse Semantics
Here we step through some examples of discourse adverbials and demonstrate how
they make their semantic contribution to the discourse context. We start with exam-
ple (32), repeated here as (36):
(36) a. John loves Barolo.
b. So he ordered three cases of the ?97.
c. But he had to cancel the order
d. because he then discovered he was broke.
9 We are not claiming to give a detailed semantics of discourse connectives except insofar as they may
affect how discourse adverbials are resolved. Thus, for example, we are not bothering to distinguish
among different senses of because (epistemic vs. nonepistemic), while (temporal vs. concessive), since
(temporal vs. causal), etc. Of course, these distinctions are important to discourse interpretation, but
they are independent of and orthogonal to the points made in this article. Similarly, Asher (1993)
argues that a simple ontology of eventualities is too coarse-grained, and that discourse representations
need to distinguish different kinds of abstract objects, including actions, propositions, and facts as well
as eventualities. Different discourse connectives will require different kinds of abstract objects as
arguments. This distinction is also orthogonal to the points made in this article, because we can
understand these abstract referents to be associates of the corresponding Hobbsian eventualities and
leave the appropriate choice to the lexical semantics of discourse connectives. Byron (2002) advocates a
similar approach to resolving discourse anaphora.
559
Webber et al Anaphora and Discourse Structure
Using the above LF representation scheme and our notation from Section 3.2, namely,
? ? = the anaphoric expression (here, the discourse adverbial)
? R? = the relation name linked with ?
? S = the matrix clause/sentence containing ?
? ? = the interpretation of S as an abstract object
and ignoring, for now, the conjunction because (discussed in section 4), the relevant
elements of (36d) can be represented as:
? = then
R? = after
S = he [John] discovered he was broke
? = e4:find(j,e5), where e5:broke(j)
This means that the unresolved interpretation of (36d) is
[?x . R?(x,EV)]? ? [?x . after(x,EV)]e4 ? after(e4, EV)
The anaphoric argument EV is resolved to the eventuality e2, derived from (36b)?
e2:order(j, c1).
after(e4,EV) ? after(e4,e2)
That is, the eventuality of John?s finding he was broke is after that of John?s ordering
three cases of the ?97 Barolo. The resulting proposition after(e4,e2) would be given its
own index, e6, and added to the discourse context.
When then is understood temporally, as it is above, as opposed to logically, it
requires a culminated eventuality from the discourse context as its first argument
(which Vendler (1967) calls an achievement or an accomplishment). The ordering
event in (36b) is such a Vendlerian accomplishment. In example (37), though, there
is no culminated eventuality in the discourse context for then), to take as its first
argument.
(37) a. Go west on Lancaster Avenue.
b. Then turn right on County Line.
How does (37b) get its interpretation?
As with (36d), the relevant elements of (37b) can be represented as
? = then
R? = after
S = turn right on County Line
? = e3:turn-right(you, county line)
and the unresolved interpretation of (37b) is thus
[? x . after(x, EV)]e3 ? after(e3, EV)
560
Computational Linguistics Volume 29, Number 4
As for resolving EV, in a well-known article, Moens and Steedman (1988) discuss
several ways in which an eventuality of one type (e.g., a process) can be coerced into
an eventuality of another type (e.g., an accomplishment, which Moens and Steedman
call a culminated process). In this case, the matrix argument of then (the eventuality of
turning right on County Line) can be used to coerce the process eventuality in (37b) into
a culminated process of going west on Lancaster Avenue until County Line. We treat this
coercion as a type of associative or bridging inference, as in the examples discussed
in section 3.1. That is,
e2 = culmination(e1)?assoc(e1), where e1:go-west(you, lancaster ave)
Taking this e2 as the anaphoric argument EV of then yields the proposition
after(e3, e2)
That is, the eventuality of turning right onto County Line is after that of going west
on Lancaster Avenue to County Line. This proposition would be indexed and added
to the discourse context.
It is important to stress here that the level of representation we are concerned
with is essentially an LF for discourse. Any reasoning that might then have to be done
on the content of LFs might then require making explicit the different modal and
temporal contexts involved, their accessibility relations, the status of abstract objects
as facts, propositions or eventualities, and so on. But as our goal here is primarily
to capture the mechanism by means of which discourse adverbials are involved in
discourse structure and discourse semantics, we will continue to assume for as long
as possible that an LF representation will suffice.
Now it may appear as if there is no difference between treating adverbials as
anaphors and treating them as structural connectives, especially in cases like (37) in
which the antecedent comes from the immediately left-adjacent context, and in which
the only obvious semantic relation between the adjacent sentences appears to be the
one expressed by the discourse adverbial. (Of course, there may also be a separate
intentional relation between the two sentences [Moore and Pollack 1992], independent
of the relation conveyed by the discourse adverbial.)
One must distinguish, however, between whether a theory allows a distinction
to be made and whether that distinction needs to be made in a particular case. It
is clear that there are many examples in which the two approaches (i.e., a purely
structural treatment of all connectives, versus one that treats adverbials as linking into
the discourse context anaphorically) appear to make the same prediction. We have
already, however, demonstrated cases in which a purely structural account makes the
wrong prediction, and in the next section, we will demonstrate the additional power
of an account that allows for two relations between an adverbial?s matrix clause or
sentence and the previous discourse: one arising from the anaphoric connection and
the other inferred from adjacency or conveyed explicitly by a structural connective.
Before closing this section, we want to step through examples (19)?(20), repeated
here as examples (38)?(39).
(38) If the light is red, stop. Otherwise you?ll get a ticket.
(39) If the light is red, stop. Otherwise go straight on.
561
Webber et al Anaphora and Discourse Structure
Roughly speaking, otherwise conveys that the complement of its anaphorically derived
argument serves as the condition under which the interpretation of its structural ar-
gument holds. (This complement must be with respect to some contextually relevant
set.)10
If we represent a conditional relation between two eventualities with the asym-
metric relation if(e1,e2), where e1 is derived from the antecedent and e2 from the conse-
quent, and we approximate a single contextually relevant alternative e2 to an eventu-
ality e1 using a symmetric complement relation, complement(e1, e2), then we can represent
the interpretation of otherwise as
? x . if(VE, x), where complement(VE, EV)
where variable EV is resolved anaphorically to an eventuality in the current discourse
context that admits a complement. That is, otherwise requires a contextually relevant
complement to its antecedent and asserts that if that complement holds, the argument
to the ?-expression will as well. The resulting ?-expression applies to the interpretation
of the matrix clause of otherwise, resulting in the conditional?s being added to the
discourse context:
[?x . if(VE,x)] ? ? if(VE,?), where complement(VE,EV)
Here the relevant elements of (38b) and (39b) can be represented as
? = otherwise
R? = if
S38 = you get a ticket
?38 = e3, where e3:get ticket(you)
S39 = go straight on
?39 = e3? , where e3? :go straight(you)
The unresolved interpretations of (38b) and (39b) are thus:
[?x . if(VE38,x)] e3 ? if(VE38,e3), where complement(VE38,EV38)
[?x . if(VE39,x)] e3? ? if(VE39,e3? ), where complement(VE39,EV39)
As we showed in section 2.2, different ways of resolving the anaphoric argument lead
to different interpretations. In (38), the anaphoric argument is resolved to e2:stop(you),
10 Kruijff-Korbayova? and Webber (2001a) demonstrate that the information structure of sentences in the
previous discourse (theme-rheme partitioning, as well as focus within theme and within rheme
[Steedman 2000a]) can influence what eventualities er are available for resolving the anaphorically
derived argument of otherwise. This then correctly predicts different interpretations for ?otherwise? in
(i) and (ii):
(i) Q. How should I transport the dog?
A. You should carry the dog. Otherwise you might get hurt.
(ii) Q. What should I carry?
A. You should carry the dog. Otherwise you might get hurt.
In both (i) and (ii), the questions constrain the theme/rheme partition of the answer. Small capitals
represent focus within the rheme. In (i), the otherwise clause will be interpreted as warning the hearer
(H) that H might get hurt if he/she transports the dog in some way other than carrying it (e.g., H might
get tangled up in its lead). In (ii), the otherwise clause warns H that he/she might get hurt if what she
is carrying is not the dog (e.g., H might be walking past fanatical members of the Royal Kennel Club).
562
Computational Linguistics Volume 29, Number 4
whereas in (39), it is resolved to e1:red(light1). Thus the resulting interpretations of
(38b) and (39b) are, respectively,
if(e4,e3), where complement(e2,e4) and e2:stop(you)
(If you do something other than stop, you?ll get a ticket.)
if(e4? , e3? ), where complement(e1,e4? ) and e1:red(light)
(If the light is not red, go straight on.)
We have not been specific about how the anaphoric argument of otherwise (or
of any other discourse adverbial) is resolved, other than having it treated as a con-
straint satisfaction problem. This is the subject of current and future work, exploring
the empirical properties of resolution algorithms with data drawn from appropriately
annotated corpora and from psycholinguistic studies of human discourse interpreta-
tion. To this end, Creswell et al (2002) report on a preliminary annotation study of
discourse adverbials and the location and type of their antecedents. This initial ef-
fort involves nine discourse adverbials?three each from the classes of concessive,
result, and reinforcing (additive) conjuncts given in Quirk et al (1972). Meanwhile,
Venditti et al (2002) present a preliminary report on the use of a constraint satisfac-
tion model of interpretation, crucially combining anaphoric and structural reasoning
about discourse relations, to predict subjects? on-line interpretation of discourses in-
volving stressed pronouns. In addition, two proposals have recently been submitted to
construct a larger and more extensively annotated corpus, covering more adverbials,
based on what we have learned from this initial effort. This more extensive study
would be an adequate basis for developing resolution algorithms.11
3.5 Summary
In this section, we have presented a general framework for anaphora with the follow-
ing features:
? Anaphors can access one or more discourse referents or entities
associated with them through bridging inferences. These are sufficient
for interpreting anaphoric pronouns, definite NPs and demonstrative
NPs, allowing entities to be evoked by NPs or by clauses. In the case of
clauses, this may be on an as-needed basis, as in Eckert and Strube
(2000).
? A type of anaphor ? that we call lexically specified can also contribute
additional meaning through a function f? that is idiosyncratic to ?, that
can be applied to either an existing discourse referent or an entity
associated with it through a bridging inference. In the case of the
premodifier other, f? applied to its argument produces contextually
11 With respect to how many discourse adverbials there are, Quirk et al (1972) discuss 60 conjunctions
and discourse adverbials under the overall heading time relations and 123 under the overall heading
conjuncts. Some entries appear under several headings, so that the total number of conjunctions and
discourse adverbials they present is closer to 160. In another enumeration of discourse adverbials,
Forbes and Webber (2002) start with all annotations of sentence-level adverbials in the Penn Treebank,
then filter them systematically to determine which draw part of their meaning from the preceding
discourse and how they do so. What we understand from both of these studies is that there are fewer
than 200 adverbials to be considered, many of which are minor variations of one another (in contrast, by
contrast, by way of contrast, in comparison, by comparison, by way of comparison that are unlikely to differ in
their anaphoric properties, and some of which, such as contrariwise, hitherto, and to cap it all, will occur
only rarely in a corpus of modern English.
563
Webber et al Anaphora and Discourse Structure
relevant alternatives to that argument. In the case of the premodifier
such, it yields a set of entities that are similar to its argument in a
contextually relevant way.
? Discourse adverbials are lexically specified anaphors whose meaning
function f? is a ?-expression involving a binary relation R? that is
idiosyncratic to ?, one of whose arguments is resolved anaphorically and
the other is provided compositionally, when the ?-expression is applied
to ??s matrix clause interpreted as an eventuality ?.
In the next section, we move on to consider how the presence of both a semantic rela-
tion associated with a discourse adverbial and a semantic relation associated with the
adjacency of two clauses or a structural connective between them allows for interesting
interactions between the two.
4. Patterns of Anaphoric Relations and Structural/Inferred Relations
Prior to the current work, researchers have treated both explicit structural connec-
tives (coordinating and subordinating conjunctions, and ?paired? conjunctions) and
discourse adverbials simply as evidence for a particular structural relation holding
between adjacent units. For example, Kehler (2002) takes but as evidence of a contrast
relation between adjacent units, in general as evidence of a generalization relation,
in other words as evidence of an elaboration relation, therefore as evidence of a result
relation, because as evidence of an explanation relation, and even though as evidence
of a denial of preventer relation (Kehler 2002, Section 2.1). Here Kehler has probably
correctly identified the type of relation that holds between elements, but not which
elements it holds between.
In one respect, we follow previous researchers, in that we accept that when clauses,
sentences, or larger discourse units are placed adjacent to one another, listeners infer a
relation between the two, and that the structural connective (coordinate or subordinate
conjunction) gives evidence for the relation that is intended to hold between them.
Because we take discourse adverbials to contribute meaning through an anaphoric
connection with the previous discourse, however, this means that there may be two
relations on offer and opens up the possibility that the relation contributed by the
discourse adverbial can interact in more than one way with the relation conveyed
by a structural connective or inferred through adjacency. Below we show that this
prediction is correct.
We start from the idea that, in the absence of an explicit structural connective, de-
feasible inference correlates with structural attachment of adjacent discourse segments
in discourse structure, relating their interpretations. The most basic relation is that the
following segment in some way describes the same object or eventuality as the one it
abuts (elaboration). But evidence in the segments can lead (via defeasible inference) to
a more specific relation, such as one of the resemblance relations (e.g., parallel, contrast,
exemplification, generalisation), or cause-effect relations (result, explanation, violated expecta-
tion), or contiguity relations (narration) described in Hobbs (1990) and Kehler (2002). If
nothing more specific can be inferred, the relation will remain simply elaboration. What
explicit structural connectives can do is convey relations that are not easy to convey
by defeasible inference (e.g., if, conveying condition, and or, conveying disjunction) or
provide nondefeasible evidence for an inferrable relation (e.g., yet, so, and because).
Discourse adverbials can interact with structural connectives, with adjacency-
triggered defeasible inference, and with each other. To describe the ways in which we
564
Computational Linguistics Volume 29, Number 4
have so far observed discourse adverbials to interact with relations conveyed struc-
turally, we extend the notation used in the previous section:
? ? = discourse adverbial
? R? = the name of the relation associated with ?
? S = the matrix clause/sentence of ?
? ? = the logical form (LF) interpretation of S
adding the following:
? D = the discourse unit that is left-adjacent to S, to which a relationship
holds by either inference or a structural connective
? ? = the LF interpretation of D
? R = the name of the relation that holds with ?
Although ? is one argument of R, we show below that the other argument of R may
be one of at least two different abstract objects.
Case 1: ? separately serves as an argument to both R? and R. This is the case that
holds in example (36) (repeated here):
(36) a. John loves Barolo.
b. So he ordered three cases of the ?97.
c. But he had to cancel the order
d. because he then discovered he was broke.
We have already seen that the interpretation of the clause in (36d) following because
involves
R? = after
? = e4:discover(j,e5), where e5:broke(j)
[?x . after(x,EV)]e4 ? after(e4, EV)
where EV is resolved to e2:order(j, c1), and the proposition after(e4, e2) is added to the
discourse context?that is, John?s discovering he was broke is after his ordering the
wine.
Now consider the explanation relation R associated with because in (36d). It relates
e4, John?s finding he was broke, to the intepretation of (36c), e3:cancel(j,o1)?that is,
explanation(e4,e3). Clause 36d thus adds both explanation(e4,e3) and after(e4, e2) to the
discourse. Although these two propositions share an argument (e4), they are neverthe-
less distinct.12
12 Because eventuality e4, John?s finding he was broke, both explains the canceling and follows the ordering, it
follows that the canceling is after the ordering.
565
Webber et al Anaphora and Discourse Structure
Case 2: R?(?, ei) is an argument of R. In case 1, it is the interpretation of the ad-
verbial?s matrix clause ? that serves as one argument to the discourse relation R. In
contrast, in case 2, that argument is filled by the relation contributed by the discourse
adverbial (itself an abstract object available for subsequent reference). In both cases,
the other argument to R is ?.
One configuration in which case 2 holds is with the discourse adverbial otherwise.
Recall from section 3.4 that the interpretation of otherwise involves a conditional relation
between the complement of its anaphoric argument and the interpretation ? of its
matrix clause:
[?x . if(VE,x)] ? ? if(VE,?), where complement(VE,EV)
With variable EV resolved to an eventuality in the discourse context, it is the resulting
relation (viewed as an abstract object) that serves as one argument to R, with ? serving
as the other. We can see this most clearly by considering variants of examples (38) and
(39) that contain an explicit connective between the clauses. In (38), the conjunction
because is made explicit (example (40)), and in (39), the connective is simply and or but
(example (41)).
(40) If the light is red, stop, because otherwise you?ll get a ticket.
R? = if
?38 = e3:get ticket(you)
(41) If the light is red, stop, and/but otherwise go straight on.
R? = if
?39 = e3? :go straight(you)
In the case of (40), resolving otherwise contributes the relation
e6: if(e4,e3), where complement(e4,e2) and e2:stop(you)
(If you do something other than stop, you?ll get a ticket.)
At the level of LF, the abstract object e6 that is associated with the conditional relation
serves as one argument to the explanation relation contributed by because, with e2 being
the other. That is, because and otherwise together end up contributing explanation(e2,e6)
(i.e., your needing to stop is explained by the fact that if you do something other than
stop, you?ll get a ticket).
In the case of (41), resolving otherwise contributes the relation
e6? :if(e4? , e3? ), where complement(e4? ,e1) and e1:red(light)
(If the light is not red, go straight on.)
What is the discourse relation to which otherwise contributes this abstract object e6??
Whether the connective is and or but, both its conjuncts describe (elaborate) alternative
specializations of the same situation e0 introduced earlier in the discourse (e.g., e0 could
be associated with the first sentence of Go another mile and you?ll get to a bridge. If the
light is red, stop. Otherwise go straight on.) If the connective is and, what is added to
context might simply be elaboration(e6? ,e0). (Note that without otherwise, the relation
elaboration(e5,e0) would have been added to context, where e5 is the abstract object
associated with the interpretation of If the light is red, stop.) If the connective is but, then
one might also possibly add contrast(e6? ,e5)?that is, the situation that (if the light is
566
Computational Linguistics Volume 29, Number 4
red) you should stop is in contrast to the situation that if the light is not red, you
should go straight on.13
As is clear from the original pair of examples (38) and (39), interpretations can
arise through adjacency-triggered inference that are similar to those that arise with an
explicit connective. In either case, the above treatment demonstrates that there is no
need for a separate otherwise relation, as proposed in rhetorical structure theory (Mann
and Thompson 1988). We are not, however, entirely clear at this point when case 1
holds and when case 2 does. A more careful analysis is clearly required.
Case 3: R? is parasitic on R. Case 3 appears to apply with discourse adverbials such
as for example and for instance. The interpretation of such adverbials appears to be
parasitic on the relation associated with a structural connective or discourse adverbial
to their left, or on an inferred relation triggered by adjacency. The way to understand
this is to first consider intraclausal for example, where it follows the verb, as in
(42) Q. What does this box contain?
A. It contains, for example, some hematite.
The interpretation of for example here involves abstracting the meaning of its matrix
structure with respect to the material to its right, then making an assertion with respect
to this abstraction. That is, if the LF contributed by the matrix clause of (42A) is,
roughly,
i. contain(box1,hematite1)
then the LF resulting from the addition of for example can be written either with set
notation (as in (ii)), taking an entity to exemplify a set, or with ?-notation (as in (iii)),
taking an entity to exemplify a property:
ii. exemplify(hematite1, {X | contain(box1,X)})
iii. exemplify(hematite1, ?X . contain(box1,X))
Both express the fact that hematite is an example of what is contained in the box.14 Since
one can derive (i) logically from either (ii) or (iii), one might choose to retain only (ii) or
(iii) and derive (i) if and when it is needed. In the remainder of the article, we use the
? notation given in (iii). Note that from the perspective of compositional semantics, for
example resembles a quantifier, in that the scope of its interpretation is not isomorphic
to its syntactic position. Thus producing an interpretation for for example will require
techniques similar to those that have long been used in interpreting quantifiers (Woods,
1978; Barwise and Cooper 1981). We take this up again in section 5.
If we look at the comparable situation in discourse, such as (43)?(44), where for
example occurs to the right of a discourse connective, it can also be seen as abstracting
13 A much finer-grained treatment of the semantics of otherwise in terms of context-update potential is
given in Kruijff-Korbayova? and Webber (2001b). Here we are just concerned with its interaction with
structural connectives and adjacency-triggered relations.
14 The material to the right of for example can be any kind of constituent, including such strange ones as
John gave, for example, a flower to a nurse.
Here, a flower to a nurse would be an example of the set of object-recipient pairs within John?s givings.
Such nonstandard constituents are also found with coordination, which was one motivation for
combinatory categorial grammar (Steedman 1996). This just illustrates another case in which such
nonstandard constituents are needed.
567
Webber et al Anaphora and Discourse Structure
the interpretation of its discourse-level matrix structure, with respect to the material
to its right:
(43) John just broke his arm. So, for example, he can?t cycle to work now.
(44) You shouldn?t trust John because, for example, he never returns what he
borrows.
In (43), the connective so leads to
result(?,?)
being added to the discourse, where ? is the interpretation of John can?t cycle to work
now, and ? is the interpretation of John just broke his arm. For example then abstracts this
relation with respect to the material to its right (i.e., ?), thereby contributing
exemplify(?, ?X . result(X, ?))
That is, John can?t cycle to work is an example of what results from John?s breaking his
arm. Similarly, because in (44) leads to
explanation(?,?)
being added to the discourse, where ? is the interpretation of he never returns what he
borrows, ? is the interpretation of you shouldn?t trust John, and for example adds
exemplify(?, ?X . explanation(X,?))
that is, that ? is an example of the reasons for not trusting John.
For example interacts with discourse adverbials in the same way:
(45) Shall we go to the Lincoln Memorial? Then, for example, we can go to the
White House.
(46) As a money manager and a grass-roots environmentalist, I was very dis-
appointed to read in the premiere issue of Garbage that The Wall Street
Journal uses 220,000 metric tons of newsprint each year, but that only 1.4%
of it comes from recycled paper. By contrast, the Los Angeles Times, for ex-
ample, uses 83% recycled paper. [WSJ, from Penn Treebank /02/wsj-0269]
In example (45), the resolved discourse adverbial then leads to after(?,?) being added
to the discourse context, where ? is the interpretation of we can go to the White House, ?
is the interpretation of we shall go to the Lincoln Memorial, and for example adds
exemplify(?, ?X . after(X,?))
that is, that ? is an example of the events that [can] follow going to the Lincoln
Memorial. (As already noted, we are being fairly fast and loose regarding tense and
modality, in the interests of focusing on the types of interactions.)
568
Computational Linguistics Volume 29, Number 4
In example (46), the resolved discourse anaphor by contrast contributes contrast(?,?),
where ? is the interpretation of the Los Angeles Times?s using 83% recycled paper and ? is
the intepretation of only 1.4% of it [newsprint used by the WSJ] comes from recycled paper.
For example then contributes
exemplify(?, ?X . contrast(X,?))
that is, that ? is one example of contrasts with the WSJ?s minimal use of recycled
paper.
What occurs with discourse connectives and adverbials can also occur with rela-
tions added through adjacency-triggered defeasible inference, as in
(47) You shouldn?t trust John. For example, he never returns what he borrows.
explanation(?,?)
exemplify(?, ?X . explanation(?,X))
Here, as in (44), the relation provided by adjacency-triggered inference is R = explana-
tion, which is then used by for example.
But what about the many cases in which only exemplify seems present, as in
(48) In some respects they [hypertext books] are clearly superior to normal
books, for example they have database cross-referencing facilities ordinary
volumes lack. (British National Corpus, CBX 1087)
(49) He [James Bellows] and his successor, Mary Anne Dolan, restored respect
for the editorial product, and though in recent years the paper had been
limping along on limited resources, its accomplishments were notable. For
example, the Herald consistently beat its much-larger rival on disclosures
about Los Angeles Mayor Tom Bradley?s financial dealings.
There are at least two explanations: One is that for example simply provides direct
nondefeasible evidence for exemplify, which is the only relation that holds. The other
explanation follows the same pattern as the examples given above, but with no further
relation than elaboration(?,?). That is, we understand in (48) that having database cross-
referencing facilities elaborates the respects in which hypertext books are superior to
normal books, whereas in (49), we understand that the Herald?s [newspaper] consistently
beating its much-larger rival elaborates the claim that its accomplishments were notable. This
elaboration relation is then abstracted (in response to for example) to produce:
exemplify(?, ?X . elaboration(X, ?))
that is, that this is one example of many possible elaborations. Because this is more
specific than elaboration and seems to mean the same as exemplify(?,?), one might
simply take it to be the only relation that holds. Given that so many naturally occuring
instances of for example occur with elaboration, it is probably useful to persist with the
above shorthand. But it shouldn?t obscure the regular pattern that appears to hold.
Before going on to case 4, we should comment on an ambiguity associated with for
example. When for example occurs after an NP, a PP, or a clause that can be interpreted
as a general concept or a set, it can contribute a relation between the general concept
or set and an instance, rather than being parasitic on another relation. For example,
in:
569
Webber et al Anaphora and Discourse Structure
(50) In the case of the managed funds they will be denominated in a leading
currency, for example US dollar, . . . (BNC CBX 1590)
for example relates the general concept denoted by a leading currency to a specific in-
stance, U.S. dollars. (In British English, the BNC shows that most such examples occur
with such as?i.e., in the construction such as for example. This paraphrase does not work
with the predicate-abstracting for example that is of primary concern here, as in exam-
ple (42).)
But for example occurring after an NP, a PP, or a clause can, alternatively, contribute
a more subtle parasitic relationship to the previous clause, as in
(51) All the children are ill, so Andrew, for example, can?t help out in the shop.
This differs from both (43) and (50). That is, one cannot paraphrase (51) as (52) as in
(43) where for example follows so:
(52) All the children are ill, so for example Andrew can?t help out in the shop.
Example (52) simply specifies an example consequence of all the children being ill, as
does
(53) All the children are ill, so for example one of us has to be at home at all
times.
In contrast, (51) specifies an example consequence for Andrew, as one of the children.
Support for this comes from the fact that in (52), Andrew doesn?t have to be one of
the children: He could be their nanny or child minder, now stuck with dealing with a
lot of sick kids. But (51) is not felicitous if Andrew is not one of the children.
We suspect here the involvement of information structure (Steedman 2000a):
Whereas the interpretation conveyed by for example is parasitic on the adjacency rela-
tion (result in example (51)), its position after the NP Andrew in (51) may indicate a
contrastive theme with respect to the previous clause, according to which Andrew in
contrast to the other children suffers this particular consequence. But more work needs
to be done on this to gain a full understanding of what is going on.
Case 4: R? is a defeasible rule that incorporates R. Case 4 occurs with discourse
adverbials that carry the same presupposition as the discourse connectives although
and the concessive sense of while (Lagerwerf 1998). Case 4 shares one feature with
case 1, in that the discourse relation R conveyed by a structural connective or inferred
from adjacency holds between ? (the interpretation of the adverbial?s matrix clause)
and ? (the interpretation of the left-adjacent discourse unit). Where it differs is that
the result is then incorporated into the presupposition of the discourse adverbial. This
presupposition, according to Lagerwerf (1998), has the nature of a presupposed (or
conventionally implicated) defeasible rule that fails to hold in the current situation.
He gives as an example
(54) Although Greta Garbo was called the yardstick of beauty, she never mar-
ried.
This asserts both that Greta Garbo was called the yardstick of beauty and that she
never married. The first implies that Greta Garbo was beautiful. The example also
570
Computational Linguistics Volume 29, Number 4
presupposes that, in general, if a woman is beautiful, she will marry. If such a pre-
supposition can be accommodated, it will simply be added to the discourse context.
If not, the hearer will find the utterance confusing or possibly even insulting.
We argue here that the same thing happens with the discourse adverbials never-
theless and though. The difference is that, with discourse adverbials, the antecedent to
the rule derives anaphorically from the previous discourse, whereas the consequent
derives from the adverbial?s matrix clause. (With the conjunctions although and con-
cessive while, both arguments are provided structurally.)
We first illustrate case 4 with two examples in which nevertheless occurs in the
main clause of a sentence containing a preposed subordinate clause. The subordinate
conjunction helps clarify the relation between the clauses that forms the basis for the
presupposed defeasible rule. After these, we give a further example in which the
relation between the adjacent clauses comes through inference.
(55) While John is discussing politics, he is nevertheless thinking about his fish.
In (55), the conjunction while conveys a temporal relation R between the two clauses
it connects:
during(e2, e1), where e1:discuss(john,politics) and e2:think about(john,fish)
What nevertheless contributes to (55) is a defeasible rule based on this relation, which
we will write informally as
during(X,E) ? E:discuss(Y,politics)) > ?X:think about(Y,fish))
Normally, whatever one does during the time one is discussing politics, it is
not thinking about one?s fish.
This rule uses Asher and Morreau?s (1991) defeasible implication operator (>) and
abstracts over the individual (John), which seems appropriate for the general statement
conveyed by the present tense of the utterance.
Similarly, in
(56) Even after John has had three glasses of wine, he is nevertheless able to
solve difficult math problems.
the conjunction after contributes a relation between the two clauses it connects:
after(e2, e1), where e1:drink(john,wine) and e2:solve(john,hard problems)
What nevertheless contributes to this example is a defeasible rule that we will again
write informally as
after(X,E) ? E:drink(Y,wine)) > ?X:solve(Y,hard problems))
Normally, whatever one is able to do after one has had three glasses of wine, it
is not solving difficult algebra problems.
571
Webber et al Anaphora and Discourse Structure
Again, we have abstracted over the individual, as the presupposed defeasible rule
associated with the present-tense sentence appears to be more general than a statement
about a particular individual.15
On the other hand, in the following example illustrating a presupposed defeasi-
ble rule and a discourse relation associated with adjacency, it seems possible for the
presupposed defeasible rule to be about John himself:
(57) John is discussing politics. Nevertheless, he is thinking about his fish.
Here the discourse relation between the two clauses, each of which denotes a specific
event, is
during(e2, e1), where e1:discuss(john,politics) and e2:think about(john,fish)
(Note that our LF representation isn?t sufficiently rich to express the difference between
(55) and (57).) What nevertheless contributes here is the presupposed defeasible rule
during(X,e1) > ?X = e2
Normally what occurs during John?s discussing politics is not John?s thinking
about his fish.
Lagerwerf (1998) does not discuss how specific or general will be the presup-
posed defeasible rule that is accommodated or what factors affect the choice. Kruijff-
Korbayova? and Webber (2001a) also punt on the question, when considering the effect
of information structure on what presupposed defeasible rule is associated with al-
though. Again, this seems to be a topic for future work.
Summary
We have indicated four ways in which we have found the relation associated with a
discourse adverbial to interact with a relation R triggered by adjacency or conveyed
by structural connectives or, in some cases, by another relational anaphor:
1. ? separately serves as an argument to both R? and R.
2. R?(?, ei) is an argument of R.
3. R? is parasitic on R.
4. R? is a defeasible rule that incorporates R.
We do not know whether this list is exhaustive or whether a discourse adverbial
always behaves the same way vis-a`-vis other relations. Moreover, in the process of
setting down the four cases we discuss, we have identified several problems that we
have not addressed, on which further work is needed. Still, we hope that we have
convinced the reader of our main thesis: that by recognizing discourse adverbials
as doing something different from simply signaling the discourse relation between
adjacent discourse units and by considering their contribution as relations in their own
right, one can begin to characterize different ways in which anaphoric and structural
relations may themselves interact.
15 We speculate that the reason examples such as (55) and (56) sound more natural with the focus particle
even applied to the subordinate clause is that even conveys an even greater likelihood that the
defeasible rule holds, so nevertheless emphasizes its failure to do so.
572
Computational Linguistics Volume 29, Number 4
5. Lexicalized Grammar for Discourse Syntax and Semantics
The question we consider in this section is how the treatment we have presented of
discourse adverbials and structural connectives can be incorporated into a general
approach to discourse interpretation. There are three possible ways.
The first possibility is simply to incorporate our treatment of adverbials and con-
nectives into a sentence-level grammar, since such grammars already cover the syntax
of sentence-level conjunction (both coordinate and subordinate) and the syntax of
adverbials of all types. The problem with this approach is that sentence-level gram-
mars, whether phrasal or lexicalized, stop at explicit sentence-level conjunction and do
not provide any mechanism for forming the meaning of multiclausal units that cross
sentence-level punctuation. Moreover, as we have already shown in section 3, the
interpretation of discourse adverbials can interact with the implicit relation between
adjacent sentences, as well as with an explicitly signaled relation, so that a syntax and
compositional semantics that stops at the sentence will not provide all the structures
and associated semantics needed to build the structures and interpretations of interest.
The second possibility is to have a completely different approach to discourse-
level syntax and semantics than to sentence-level syntax and semantics, combining
(for example) a definite clause grammar with rhetorical structure theory. But as we
and others have already noted, this requires discourse semantics reaching further and
further into sentence-level syntax and semantics to handle relations between main and
embedded clauses, and between embedded clauses themselves, as in example (58).
(58) If they?re drunk and they?re meant to be on parade and you go to their
room and they?re lying in a pool of piss, then you lock them up for a day.
(The Independent, June 17, 1997)
Thus it becomes harder and harder to distinguish the scope of discourse-level syntax
and semantics from that at the sentence-level.
The third possibility is to recognize the overlapping scope and similar mechanisms
and simply extend a sentence-level grammar and its associated semantic mechanisms
to discourse. The additional responsibilities of the grammer would be to account for
the formation of larger units of discourse from smaller units; the projection of the
interpretation of smaller discourse units onto the interpretation of the larger discourse
units they participate in; and the effect of discourse unit interpretation on the evolv-
ing discourse model. There are two styles of grammar one could use for this: (1) a
phrase structure grammar (PSG) extended to discourse, as in Figure 6, or (2) a lexi-
calized grammar that extends to discourse, a sentence-level lexicalized grammar such
as tree-adjoining grammar (Joshi, 1987; XTAG-Group 2001) or combinatory categorial
grammar (CCG) (Steedman 1996, 2000b).
Whereas Polanyi and van den Berg (1996) extend a PSG to discourse, we argue
for extending a lexicalized grammar, even though TAG and CCG are weakly context-
sensitive (CS) and the power needed for a discourse grammar with no crossing de-
pendencies is only context-free (section 2.1). Our argument is based on our desire to
use a discourse grammar in natural language generation (NLG). It is well-known that
context-free PSGs (CF PSGs) set up a complex search space for NLG. A discourse
grammar specified in terms of phrase structure rules such as those shown in Figure 6
doesn?t provide sufficient guidance when reversed for use in generating discourse.
For example, one might end up having to guess randomly how many sentences and
connectives one had, in what order, before being able to fill in the sentences and con-
nectives with any content. More generally, trying to generate exactly a given semantics
573
Webber et al Anaphora and Discourse Structure
Seg := SPunct Seg | Seg SPunct | SPunct |
on the one hand Seg on the other hand Seg |
not only Seg but also Seg
SPunct := S Punctuation
Punctuation := . | ; | : | ? | !
S := S Coord S | S Subord S | Subord S S | Sadv S |
NP Sadv VP | S Sadv | . . .
Coord := and | or | but | so
Subord := although | after | because | before | ...
Sadv := DAdv | SimpleAdv
DAdv := instead | otherwise | for example | meanwhile | ...
SimpleAdv := yesterday | today | surprisingly | hopefully | ...
Figure 6
PS rules for a discourse grammar.
when semantics underspecifies syntactic dependency (as discourse semantics must, on
our account) is known to be intractable (Koller and Striegnitz 2002). An effective so-
lution is to generate semantics and syntax simultaneously, which is straightforward
with a lexicalized grammar (Stone et al 2001).
Given the importance of various types of inference in discourse understanding,
there is a second argument for using a lexicalized discourse grammar that derives from
the role of implicature in discourse. Gricean reasoning about implicatures requires a
hearer be able to infer the meaningful alternatives that a speaker had in composing a
sentence. With lexicalization, these alternatives can be given by a grammar, allowing
the hearer, for example, to ask sensible questions like ?Why did the speaker say ?in-
stead? here instead of nothing at all?? and draw implicatures from this. A CF PSG, on
the other hand, might suggest questions like ?Why did the speaker say two sentences
rather than one here?? which seem empirically not to lead to any real implicatures.
(On the contrast between choices, which seem to lead to implicatures, and mere alter-
native linguistic formulations, which do not seem to, see, for example, Dale and Reiter
[1995] and Levison [2000].)
In several previous papers (Webber, Knott, and Joshi, 2001; Webber et al, 1999a,
1999b), we described how our approach fits into the framework of tree-adjoining gram-
mar. This led to the initial version of a discourse parser (Forbes et al 2001) in which
the same parser that builds trees for individual clauses using clause-level LTAG trees
then combines them using discourse-level LTAG trees. Here we simply outline the
grammar, called DLTAG (section 5.1), then show how it supports the approach to
structural and anaphoric discourse connectives presented earlier (section 5.2).
(Of course, one still needs to account for how speakers realize their intentions
through text and how what is achieved through a single unit of text contributes to
what a speaker hopes to achieve through any larger unit in which it is embedded.
Preliminary accounts are given in Grosz and Sidner [1990] and Moser and Moore
[1996]. Given the complex relation between individual sentences and speaker inten-
tions, however, it is unlikely that the relation between multisentence discourse and
speaker intentions can be modeled in a straightforward way similar to the basically
monotonic compositional process that we have discussed in this article for discourse
semantics.)
574
Computational Linguistics Volume 29, Number 4
Dc
DcDc
subconj
(a)
Dc
Dc Dc
subconj
(b)
?:subconj_mid ?: subconj_pre
Figure 7
Initial trees for a subordinate conjunction: (a) postposed; (b) preposed. Dc stands for discourse
clause, ? indicates a substitution site, and subconj stands for the particular subordinate
conjunction that anchors the tree.
5.1 DLTAG and Discourse Syntax
A lexicalized TAG begins with the notion of a lexical anchor, which can have one
or more associated tree structures. For example, the verb likes anchors one tree corre-
sponding to John likes apples, another corresponding to the topicalized Apples John likes,
a third corresponding to the passive Apples are liked by John, and others as well. That
is, there is a tree for each minimal syntactic construction in which likes can appear, all
sharing the same predicate-argument structure. This syntactic/semantic encapsulation
is possible because of the extended domain of locality of LTAG.
A lexicalized TAG contains two kinds of elementary trees: initial trees that reflect
basic functor-argument dependencies and auxiliary trees that introduce recursion and
allow elementary trees to be modified and/or elaborated. Unlike the wide variety of
trees needed at the clause level, we have found that extending a lexicalized TAG to
discourse requires only a few elementary tree structures, possibly because clause-level
syntax exploits structural variation in ways that discourse doesn?t.
5.1.1 Initial Trees. DLTAG has initial trees associated with subordinate conjunctions,
with parallel constructions, and with some coordinate conjuctions. We describe each
in turn.
In the large LTAG developed by the XTAG project (XTAG-Group 2001) subordi-
nate clauses are seen as adjuncts to sentences or verb phrases (i.e., as auxiliary trees)
because they are outside the domain of locality of the verb. In DLTAG, however, it
is predicates on clausal arguments (such as coordinate and subordinate conjunctions)
that define the domain of locality. Thus, at this level, these predicates anchor initial
trees into which clauses substitute as arguments. Figure 7 shows the initial trees for (a)
postposed subordinate clauses and (b) preposed subordinate clauses.16 At both leaves
and root is a discourse clause (Dc): a clause or a structure composed of discourse
clauses.
One reason for taking something to be an initial tree is that its local dependencies
can be stretched long distance. At the sentence level, the dependency between apples
and likes in Apples John likes is localized in all the trees for likes. This dependency can
be stretched long distance, as in Apples, Bill thinks John may like. In discourse, as we
noted in section 2, local dependencies can be stretched long distance as well, as in
(59) a. Although John is generous, he?s hard to find.
16 Although in an earlier paper (Webber and Joshi 1998), we discuss reasons for taking the lexical anchors
of the initial trees in Figures 7 and 8 to be feature structures, following the analysis in Knott (1996) and
Knott and Mellish (1996), here we just take them to be specific lexical items.
575
Webber et al Anaphora and Discourse Structure
Dc
On the
one hand
On the
other
Dc Dc
?:contrast
Figure 8
An initial tree for parallel constructions. This particular tree is for a contrastive construction
anchored by on the one hand and on the other hand.
b. Although John is generous?for example, he gives money to anyone
who asks him for it?he?s hard to find.
(60) a. On the one hand, John is generous. On the other hand, he?s hard to
find.
b. On the one hand, John is generous. For example, suppose you needed
some money: You?d only have to ask him for it. On the other hand,
he?s hard to find.
Thus DLTAG also contains initial trees for parallel constructions as in (60). Such an
initial tree is shown in Figure 8. Like some initial trees in XTAG (XTAG-Group 2001),
such trees can have a pair of anchors. Since there are different ways in which dis-
course units can be parallel, we assume a different initial tree for contrast (on the one
hand. . . on the other (hand). . . ), disjunction (either. . . or. . . ), addition (not only. . . but also. . . ),
and concession (admittedly. . . but. . . ).
Finally, there are initial trees for structural connectives between adjacent sentences
or clauses that convey a particular relation between the connected units. One clear
example is so, conveying result. Its initial tree is shown in Figure 9. We will have a
better sense of what other connectives to treat as structural as a result of annotation
efforts of the sort described in Creswell et al (2002).17
5.1.2 Auxiliary Trees. DLTAG uses auxiliary trees in two ways: (1) for discourse units
that continue a description in some way, and (2) for discourse adverbials. Again we
describe each in turn.
17 For example, one might also have initial trees for marked uses of and and or that have a specific
meaning beyond simple conjunction or disjunction, as in
(61) a. Throw another spit ball and you?ll regret it.
b. Eat your spinach or you won?t get dessert.
These differ from the more frequent, simple coordinate uses of and and or in that the second conjunct
in these marked cases bears a discourse relation to the first conjunct (result in both (61a) and (61b)).
With simple coordinate uses of and and or, all conjuncts (disjuncts) bear the same relation to the same
immediately left-adjacent discourse unit. For example, in (62), each conjunct is a separate explanation
for not trusting John, wheras in (63), each disjunct conveys an alternative result of John?s good fortune:
(62) You shouldn?t trust John. He never returns what he borrows, and he bad-mouths his associates
behind their backs.
(63) John just won the lottery. So he will quit his job, or he will at least stop working overtime.
For simple coordinate uses of and and or, we have auxiliary trees (section 5.1.2).
576
Computational Linguistics Volume 29, Number 4
Dc
DcDc
?:so
so
Figure 9
Initial tree for coordinate conjunction so.
Dc
Dc Dc
?
.
Dc
Dc Dc
? and ?
S
S
then
(a) (b) (c)
?: punct1 ?: and ?: then
Figure 10
Auxiliary trees for basic elaboration. These particular trees are anchored by (a) the
punctuation mark ?period? and (b) and. The symbol ? indicates the foot node of the auxiliary
tree, which has the same label as its root. (c) Auxiliary tree for the discourse adverbial then.
?: punct1
?: punct1
3
?2
*
.
T1
T2
T1 T2
.
?1
0
Figure 11
TAG derivation of example (64).
First, auxiliary trees anchored by punctuation (e.g., period, comma, semicolon.) (Fig-
ure 10a) or by simple coordination (Figure 10b) are used to provide further description
of a situation or of one or more entities (objects, events, situations, states, etc.) within
the situation.18 The additional information is conveyed by the discourse clause that fills
its substitution site. Such auxiliary trees are used in the derivation of simple discourses
such as
(64) a. John went to the zoo.
b. He took his cell phone with him.
Figure 11 shows the DLTAG derivation of example (64), starting from LTAG deriva-
tions of the individual sentences.19 To the left of the horizontal arrow are the elemen-
tary trees to be combined: T1 stands for the LTAG tree for clause (64a), T2 for clause
18 The latter use of an auxiliary tree is related to dominant topic chaining in Scha and Polanyi (1988) and
entity chains in Knott et al (2001).
19 We comment on left-to-right incremental construction of DLTAG structures in parallel with
sentence-level LTAG structures at the end of Section 5.2.
577
Webber et al Anaphora and Discourse Structure
(64b), and ?:punct1 for the auxiliary tree assocated with the period after (64a). In the
derivation, the foot node of ?:punct1 is adjoined to the root of T1 and its substitution
site filled by T2, resulting in the tree to the right of the horizontal arrow. (A standard
way of indicating TAG derivations is shown under the horizontal arrow, where bro-
ken lines indicate adjunction and solid lines, substitution. Each line is labeled with the
address of the argument at which the operation occurs. ?1 is the derivation tree for
T1 and ?2, the derivation tree for T2.)
The other auxiliary trees used in the lexicalized discourse grammar are those for
discourse adverbials, which are simply auxiliary trees in a sentence-level LTAG (XTAG-
Group 2001), but with an interpretation that projects up to the discourse level. An
example is shown in Figure 10c. Adjoining such an adverbial to a clausal/sentential
structure contributes to how information conveyed by that structure relates to the
previous discourse.
There is some lexical ambiguity in this grammar, but no more than serious con-
sideration of adverbials and conjunctions demands. First, as already noted, discourse
adverbials have other uses that may not be anaphoric (65a?b) and may not be clausal
(65a?c):
(65) a. John ate an apple instead of a pear.
b. In contrast with Sue, Fred was tired.
c. Mary was otherwise occupied.
Second, many of the adverbials found in second position in parallel constructions
(e.g., on the other hand, at the same time, nevertheless) can also serve as simple adverbial
discourse connectives on their own. In the first case, they will be one of the two anchors
of an initial tree (Figure 8), and in the second, they will anchor a simple auxiliary tree
(Figure 10c). These lexical ambiguities correlate with structural ambiguity.
5.2 Example Derivations
It should be clear by now that our approach aims to explain discourse semantics in
terms of a product of the same three interpretive mechanisms that operate within
clause-level semantics:
? compositional rules on syntactic structure (here, discourse structure)
? anaphor resolution
? inference triggered by adjacency and structural connection
For the compositional part of semantics in DLTAG (in particular, computing interpre-
tations on derivation trees), we follow Joshi and Vijay-Shanker (2001). Roughly, they
compute interpretations on the derivation tree using a bottom-up procedure. At each
level, function application is used to assemble the interpretation of the tree from the
interpretation of its root node and its subtrees. Where multiple subtrees have function
types, the interpretation procedure is potentially nondeterministic: The resulting am-
biguities in interpretation may be admitted as genuine, or they may be eliminated by a
lexical specification. Multicomponent TAG tree sets are used to provide an appropriate
compositional treatment for quantifiers, which we borrow for interpreting for example
(examples (66c?d)).
In showing how DLTAG and an interpretative process on its derivations operate,
we must, of necessity, gloss over how inference triggered by adjacency or associated
with a structural connective provides the intended relation between adjacent discourse
578
Computational Linguistics Volume 29, Number 4
units: It may be a matter simply of statistical inference, as in Marcu and Echihabi
(2002), or of more complex inference, as in Hobbs et al (1993). As we noted, our view
is that there are three mechanisms at work in discourse semantics, just as there are in
clause-level semantics: Inference isn?t the only process involved. Thus the focus of our
presentation here is on how compositional rules and anaphor resolution (which itself
often appears to require inference) operate together with inference to yield discourse
semantics.
We start with previous examples (44) (here (66c)) and (47) (here (66d)) and two
somewhat simpler variants (66a?b):
(66) a. You shouldn?t trust John because he never returns what he borrows.
b. You shouldn?t trust John. He never returns what he borrows.
c. You shouldn?t trust John because, for example, he never returns what
he borrows.
d. You shouldn?t trust John. For example, he never returns what he bor-
rows.
This allows us to show how (66a?b) and (66c?d) receive similar interpretations, despite
having somewhat different derivations, and how the discourse adverbial for example
contributes both syntactically and semantically to those interpretations.
We let T1 stand for the LTAG parse tree for you shouldn?t trust John, ?1, for its
derivation tree, and interp(T1), for the eventuality associated with its interpretation.
Similarly, we let T2 stand for the LTAG parse tree for he never returns what he bor-
rows, ?2, for its derivation tree, and interp(T2), for the eventuality associated with its
interpretation.
Example (66a) involves an initial tree (?:because-mid) anchored by because (Fig-
ure 12). Its derived tree comes from T1 substituting at the left-hand substitution
site of ?:because-mid (index 1) and T2 at its right-hand substitution site (index 3).
Compositional interpretation of the resulting derivation tree yields explanation(interp
(T2),interp(T1)). (A more precise interpretation would distinguish between the direct
and epistemic causality senses of because, but the derivation would proceed in the
same way.)
In contrast with (66a), example (66b) employs an auxiliary tree (?:punct1) anchored
by a period (Figure 13). Its derived tree comes from T2 substituting at the right-hand
substitution site (index 3) of ?:punct1, and ?:punct1 adjoining at the root of T1 (index 0).
Compositional interpretation of the derivation tree yields merely that T2 continues the
description of the situation associated with T1, that is, elaboration(interp(T2),interp(T1)).
Further inference triggered by adjacency and structural connection leads to a con-
?:because_mid
?:because_mid
31
T2
T1
?1 ?2because
because
T1 T2
Figure 12
Derivation of example (66a). The derivation tree is shown below the arrow, and the derived
tree, to its right. (Node labels Dc have been omitted for simplicity.)
579
Webber et al Anaphora and Discourse Structure
?: punct1
?: punct1
*
T2 . .
T1
T1 T2
3
?2
?1
0
Figure 13
Derivation of example (66b).
D
D  *
cD{ c
c
}
for-ex2
?: for-ex1
0
3
?:?1
0
because_mid
?2
?:
1
T2
T1
?: for-ex1
because_mid?:T1
T2 because
?: for-ex2
because
for example
?
for example
Figure 14
Derivation of example (66c).
clusion of causality between them, that is, explanation(interp(T2),interp(T1)), but this
conclusion is defeasible because it can be denied without a contradiction: for example,
(67) You shouldn?t trust John. He never returns what he borrows. But that?s
not why you shouldn?t trust him.
Example (66c) differs from (66a) in containing for example in its second clause. As
noted earlier, for example resembles a quantifier with respect to its semantics, as its
interpretation takes wider scope than would be explained by its syntactic position. We
handle this in the same way that quantifiers are handled in Joshi and Vijay-Shanker
(2001) by associating with for example a two-element TAG tree set (Figure 14). Both
trees in the tree set participate in the derivation: The auxiliary tree ?:for ex1 adjoins
at the root of T2, whereas the auxiliary tree ?:for ex2 adjoins at the root of the higher
discourse unit. Since we saw from example (66a) that the interpretation of this higher
discourse unit is explanation(interp(T2),interp(T1)), the interpretation associated with
the adjoined ?:for ex2 node both embeds and abstracts this interpretation, yielding
exemplification(interp(T2), ?X . explanation(X,interp(T1))
That is, John?s never returning what he borrows is one instance of a set of explanations.
Similarly, example (66d) differs from (66b) in containing for example in its second
sentence (Figure 15). As in example (66b), an inferred relation is triggered between
the interpretations of T2 and T1, namely, explanation(interp(T2),interp(T1)). Then, as
a result of ?:for ex1 adjoining at T2 and ?:for ex2 adjoining at the root of the higher
580
Computational Linguistics Volume 29, Number 4
D
D
* .
.
D* }{ for-ex1?: for-ex2
0
punct1
?23
0
?:
?1
?:
0
T1
T2
?: for-ex2
?:
punct1
for example
T2
T1
for example ?
for-ex1?:
Figure 15
Derivation of example (66d).
discourse unit, for example again contributes the interpretation
exemplification(interp(T2), ?X . explanation(X,interp(T1))
Thus (66c) and (66d) differ only in the derivation of the interpretation that for example
then abstracts over.
The next example we will walk through is example (11) (repeated here as exam-
ple (68)):
(68) John loves Barolo. So he ordered three cases of the ?97. But he had to
cancel the order because then he discovered he was broke.
As shown in Figure 16, this example involves two initial trees (?:so, ?:because mid) for
the structural connectives so and because; an auxiliary tree for the structural connective
but (?:but), since but functions as a simple conjunction to continue the description of
the situation under discussion; an auxiliary tree (?:then) for the discourse adverbial
then; and initial trees for the four individual clauses T1?T4. As can be seen from the
derivation tree, T1 and T2 substitute into ?:so as its first and third arguments, and ?:but
root-adjoins to the result. The substitution argument of ?:but is filled by ?:because mid,
with T3 and T4 substituted in as its first and third arguments, and ?:then is root-
adjoined to T4. The interpretation contributed by then, after its anaphoric argument is
resolved to interp(T2), is
?4: after(interp(T4), interp(T2))
The interpretations derived compositionally from the structural connectives so, because,
and but are
?1: result(interp(T2), interp(T1))
?2: explanation(interp(T4), interp(S3))
?3: elaboration(?2,?1)
Further inference may then refine elaboration to contrast, based on how but is being
used.
Finally, we want to point out one more way in which texts that seem to be close
paraphrases get their interpretations in different ways. Consider the two texts in ex-
ample (69):
581
Webber et al Anaphora and Discourse Structure
*
so
?:then
*then
because
?:because_mid
?3
T1 ?1
?4T3
T2
T4
?: but
?: so
?: but
?:because_mid
?:
?2
3
1 3
0
?3 ?4
then
?1 ?2
31 0
because
thenT3
T4
T2
so
T1
but
?:
but
so
Figure 16
Derivation of example (68).
(69) a. You should eliminate part 2 before part 3 because part 2 is more sus-
ceptible to damage.
b. You should eliminate part 2 before part 3. This is because part 2 is
more susceptible to damage.
Example (69b) is a simpler version of an example in Moser and Moore (1995), in which
This is because is treated as an unanalyzed cue phrase, no different from because in (69a).
We show here that this isn?t necessary: One can analyze (69b) using compositional
semantics and anaphor resolution and achieve the same results.
First consider (69a). Given the interpretations of its two component clauses, its
overall interpretation follows in the same way as (66a), shown in Figure 12. Now
consider (69b) and the derivation shown in Figure 17. Here the initial tree ?:because mid
T1
T2
TB
?:because_mid
because
?: punct1
*
.
.
because
T2 TB
?: punct1
?:because_mid
31
?2 ??
?1
0
3
T1
Figure 17
Derivation of example (69b).
582
Computational Linguistics Volume 29, Number 4
has its two arguments filled by T2, the TAG analysis of this is and TB, the TAG analysis
of part 2 is more susceptible to damage. The overall derived tree for (69b) comes from
?:punct1 root-adjoining to T1 (the TAG analysis of You should eliminate part 2 before
part 3), with the subsitution site of ?:punct1 filled by the ?:because mid derivation.
The compositional interpretation of the derivation tree yields the interpretation of the
?:because mid tree (i1) as an elaboration of the interpretation of T1:
i1: explanation(interp(TB),interp(T2))
i2: elaboration(i1,interp(T1))
But this is not all. The pronoun this in T2 is resolved anaphorically to the nearest con-
sistent eventuality (Eckert and Strube 2000; Byron 2002) which in this case is interp(T1).
Taking this as the interpretation of T2 and substituting, we get
i1: explanation(interp(TB),interp(T1))
i2: elaboration(i1,interp(T1))
Notice that i1 is also the interpretation of (69a). To this, i2 adds the somewhat redun-
dant information that i1 serves to elaborate the advice in T1. Thus (69a) and (69b)
receive similar interpretations but by different means. This treatment has the added
advantage that one does not have to treat This is not because as a separate cue phrase.
Rather, negation simply produces
i1: ?explanation(interp(TB),interp(T1))
i2: elaboration(i1,interp(T1))
That is, T1 is elaborated by a denial of a (possible) explanation. Presumably, the text
would go on to provide the actual explanation.
Finally, we want to comment on the Holy Grail of discourse parsing: a realistic
model that is computed in parallel with incremental sentence-level parsing. Neither
the analyses given in this section nor the discourse parsing described in Forbes et
al. (2001) is done in a left-to-right incremental fashion, in parallel with incremental
left-to-right sentence-level parsing.
What would an integrated incremental method of sentence-discourse processing
require? At minimum, we believe it would involve:
? A left-to-right parser that would simultaneously compute increments to
sentence-level syntactic structure, sentence-level semantics,
discourse-level syntactic structure, and discourse-level semantics.
Increments to the latter two would occur only at clause boundaries and
with discourse adverbials and structural connectives.
? An incremental anaphor resolution mechanism, similar to that in Strube
(1998), but extended both to deictic pronouns, as in Eckert and Strube
(2000) and Byron (2002) and to the anaphoric argument of discourse
adverbials.
? Incremental computation of discourse structure in terms of elaboration
relations and further nondefeasible reasoning to more specific relations,
where possible.
A left-to-right parser that simultaneously produces sentence-level syntactic and
semantic analyses already exists for combinatory categorial grammar (Steedman 1996,
583
Webber et al Anaphora and Discourse Structure
2000b; Hockenmaier, Bierner, and Baldridge, forthcoming), and it would seem straight-
forward to extend such a parser to computing discourse-level syntax and semantics
as well. Similarly, it seems straightforward to produce an incremental version of any
of the current generation of anaphor resolution mechanisms, extended to deictic pro-
nouns, although current approaches attempt to resolve this and that only with the
interpretation of a single clause, not with that of any larger discourse unit. As these
approaches are also not very accurate as yet, incremental anaphor resolution awaits
improvements to anaphor resolution in general. Moreover, as we better understand
the specific anaphoric properties of discourse adverbials through empirical analysis
such as Creswell et al (2000), such anaphor resolution mechanisms can be extended
to include them as well.
As for building discourse structure incrementally in parallel with syntactic struc-
ture, there is no working prototype yet that will do what is needed. But we have no
doubt that as psycholinguistics and computation together develop a better understand-
ing of incremental semantic processing, researchers? desire for a working prototype
will eventually result in the development of one.
6. Conclusion
In this article, we have argued that discourse adverbials make an anaphoric, rather than
a structural, connection with the previous discourse (section 2), and we have provided
a general view of anaphora in which it makes sense to talk of discourse adverbials as
being anaphoric (section 3). We have then shown that this view of discourse adverbials
allows us to characterize a range of ways in which the relation contributed by a
discourse adverbial can interact with the relation conveyed by a structural connective
or inferred through adjacency (section 4), and then illustrated how discourse syntax
and semantics can be treated as an extension of sentence-level syntax and semantics,
using a lexicalized discourse grammar (section 5).
We are clearly not the first to have proposed a grammatical treatment of low-level
aspects of discourse semantics (Asher and Lascarides 1999; Gardent 1997; Polanyi and
van den Berg 1996; Scha and Polanyi 1988; Schilder 1997a, 1997b; van den Berg 1996),
but we are the first to have recognized that the key to avoiding problems of maintain-
ing a compositional semantics for discourse lies in recognizing discourse adverbials as
anaphors and not trying to shoehorn everything into a single class of discourse connec-
tives. Although we are not yet able to propose a solution to the problem of correctly
resolving discourse adverbials or a way of achieving the Holy Grail of computing
discourse syntax and semantics in parallel with incremental sentence processing, the
proposed approach does simplify issues of discourse structure and discourse semantics
in ways that have not before been possible.
Acknowledgments
The authors would like to thank Kate
Forbes, Katja Markert, Natalia Modjeska,
Rashmi Prasad, Eleni Miltsakaki, Cassandra
Creswell, Mark Steedman, members of the
University of Edinburgh Dialogue Systems
Group, and participants at ESSLLI?01 for
helpful criticism as the ideas in the article
were being developed. We would also like
to thank our three anonymous reviewers.
We believe that in addressing their
criticisms and suggestions, both the article?s
arguments and its presentation have
become clearer. This work has been funded
in part by EPSRC grant GR/M75129
(Webber), NSF grant CISE CDA 9818322
(Stone), and NSF grants NSF-STC SBR
8920230 and NSF-EIA02-24417 (Joshi).
References
Asher, Nicholas. 1993. Reference to Abstract
Objects in Discourse. Kluwer, Boston.
Asher, Nicholas and Alex Lascarides. 1999.
The semantics and pragmatics of
584
Computational Linguistics Volume 29, Number 4
presupposition. Journal of Semantics,
15(3):239?300.
Asher, Nicholas and Alex Lascarides. 2003.
Logics of Conversation. Cambridge
University Press, Cambridge, England.
Asher, Nicholas and Michael Morreau. 1991.
Commonsense entailment. In Proceedings
of the Ninth International Joint Conference on
Artificial Intelligence IJCAI?91, pages
387?392, Sydney, Australia.
Barwise, Jon and Robin Cooper. 1981.
Generalized quantifiers and natural
language. Linguistics and Philosophy,
4:159?219.
Bateman, John. 1999. The dynamics of
?surfacing?: An initial exploration. In
Proceedings of International Workshop on
Levels of Representation in Discourse
(LORID?99), pages 127?133, Edinburgh.
Bierner, Gann. 2001a. Alternative phrases
and natural language information
retrieval. In Proceedings of the 39th Annual
Conference of the Association for
Computational Linguistics, Toulouse,
France, July.
Bierner, Gann. 2001b. Alternative Phrases:
Theoretical Analysis and Practical Application.
Ph.D. thesis, University of Edinburgh.
Bierner, Gann and Bonnie Webber. 2000.
Inference through alternative set
semantics. Journal of Language and
Computation, 1(2):259?274.
Byron, Donna. 2002. Resolving pronominal
reference to abstract entities. In
Proceedings of the 40th Annual Meeting,
Association for Computational Linguistics,
pages 80?87, University of Pennsylvania.
Clark, Herbert. 1975. Bridging. In
Proceedings of Theoretical Issues in Natural
Language Processing (TINLAP-1), pages
169?174, Cambridge, MA.
Clark, Herbert and Catherine Marshall.
1981. Definite reference and mutual
knowledge. In Aravind Joshi, Bonnie
Webber, and Ivan Sag, editors, Elements of
Discourse Understanding. Cambridge
University Press, Cambridge, England,
pages 10?63.
Cosse, Michel. 1996. Indefinite associative
anaphora in French. In Proceedings of the
IndiAna Workshop on Indirect Anaphora,
University of Lancaster, Lancaster,
England.
Creswell, Cassandre, Kate Forbes, Eleni
Miltsakaki, Rashmi Prasad, Aravind Joshi,
and Bonnie Webber. 2002. The discourse
anaphoric properties of connectives. In
Proceedings of the Discourse Anaphora and
Anaphor Resolution Colloquium, Lisbon,
Portugal.
Dale, Robert. 1992. Generating Referring
Expressions. MIT Press, Cambridge, MA.
Dale, Robert and Ehud Reiter. 1995.
Computational interpretations of the
Gricean maxims in the generation of
referring expressions. Cognitive Science,
18:233?263.
Eckert, Miriam and Michael Strube. 2000.
Synchronising units and anaphora
resolution. Journal of Semantics, 17:51?89.
Forbes, Katherine, Eleni Miltsakaki, Rashmi
Prasad, Anoop Sarkar, Aravind Joshi, and
Bonnie Webber. 2001. D-LTAG system:
Discourse parsing with a lexicalized
tree-adjoining grammar. In ESSLLI?2001
Workshop on Information Structure, Discourse
Structure and Discourse Semantics, Helsinki,
Finland.
Forbes, Kate and Bonnie Webber. 2002. A
semantic account of adverbials as
discourse connectives. In Proceedings of
Third SIGDial Workshop, pages 27?36,
Philadelphia, PA.
Frank, Anette and Hans Kamp. 1997. On
context dependence in modal
constructions. In SALT-97, Stanford, CA.
Gardent, Claire. 1997. Discourse tree
adjoining grammars. Claus Report no. 89,
University of the Saarland, Saarbru?cken,
Germany.
Grosz, Barbara and Candace Sidner. 1990.
Plans for discourse. In Philip Cohen, Jerry
Morgan, and Martha Pollack, editors,
Intentions in Communication. MIT Press,
Cambridge, MA, pages 417?444.
Hahn, Udo, Katja Markert, and Michael
Strube. 1996. A conceptual reasoning
approach to textual ellipsis. In Proceedings
of the 12th European Conference on Artificial
Intelligence, pages 572?576, Budapest,
Hungary.
Hardt, Dan. 1992. VP ellipsis and contextual
interpretation. In Proceedings of
International Conference on Computational
Linguistics(COLING-92), pages 303?309,
Nantes.
Hardt, Dan. 1999. Dynamic interpretation of
verb phrase ellipsis. Linguistics and
Philosophy, 22:187?221.
Hellman, Christina and Kari Fraurud. 1996.
Proceedings of the IndiAna Workshop on
Indirect Anaphora. University of Lancaster,
Lancaster, England.
Hobbs, Jerry. 1985. Ontological promiscuity.
In Proceedings of the 23rd Annual Meeting of
the Association for Computational Linguistics,
pages 61?69, Palo Alto, CA. Morgan
Kaufmann.
Hobbs, Jerry. 1990. Literature and Cognition.
Volume 21 of CSLI Lecture Notes. Center
for the Study of Language and
Information, Stanford, CA.
585
Webber et al Anaphora and Discourse Structure
Hobbs, Jerry, Mark Stickel, Paul Martin, and
Douglas Edwards. 1993. Interpretation as
abduction. Artificial Intelligence,
63(1?2):69?142.
Hockenmaier, Julia, Gann Bierner, and Jason
Baldridge. Forthcoming. Providing
robustness for a CCG system. Journal of
Language and Computation.
Isard, Stephen. 1975. Changing the context.
In Edward Keenan, editor, Formal
Semantics of Natural Language. Cambridge
University Press, Cambridge, England,
pages 287?296.
Jayez, Jacques and Corinne Rossari. 1998a.
Pragmatic connectives as predicates. In
Patrick Saint-Dizier, editor, Predicative
Structures in Natural Language and Lexical
Knowledge Bases. Kluwer Academic,
Dordrecht, the Netherlands, pages
306?340.
Jayez, Jacques and Corinne Rossari. 1998b.
The semantics of pragmatic connectives
in TAG: The French donc example. In
Anne Abeille? and Owen Rambow, editors,
Proceedings of the TAG+4 Conference. CSLI
Publications, Stanford, CA.
Joshi, Aravind. 1987. An introduction to tree
adjoining grammar. In Alexis
Manaster-Ramer, editor, Mathematics of
Language. John Benjamins, Amsterdam,
pages 87?114.
Joshi, Aravind and K. Vijay-Shanker. 2001.
Compositional semantics with lexicalized
tree-adjoining grammar (LTAG): How
much underspecification is necessary? In
Harry Bunt, Reinhard Muskens, and Elias
Thijsse, editors, Computing Meaning,
Volume 2, Kluwer, Dordrecht, the
Netherlands, pages 147?163.
Jurafsky, Dan and James Martin. 2000.
Speech and Language Processing.
Prentice-Hall, Englewood Cliffs, NJ.
Kamp, Hans and Uwe Reyle. 1993. From
Discourse to Logic. Kluwer, Dordrecht, the
Netherlands.
Kehler, Andrew. 2002. Coherence, Reference
and the Theory of Grammar. CSLI
Publications, Stanford, CA.
Kibble, Rodger. 1995. Modal
insubordination. In Empirical Issues in
Formal Syntax and Semantics, Selected Papers
from the Colloque de Syntaxe et de Se?mantique
de Paris, pages 317?332.
Knott, Alistair. 1996. A Data-Driven
Methodology for Motivating a Set of Coherence
Relations. Ph.D. thesis, Department of
Artificial Intelligence, University of
Edinburgh.
Knott, Alistair and Chris Mellish. 1996. A
feature-based account of the relations
signalled by sentence and clause
connectives. Language and Speech,
39(2?3):143?183.
Knott, Alistair, Jon Oberlander, Mick
O?Donnell, and Chris Mellish. 2001.
Beyond elaboration: The interaction of
relations and focus in coherent text. In
T. Sanders, J. Schilperoord, and
W. Spooren, editors, Text Representation:
Linguistic and Psycholinguistic Aspects. John
Benjamins, Amsterdam, pages 181?196.
Koller, Alexander and Kristina Striegnitz.
2002. Generation as dependency parsing.
In Proceedings of the 40th Annual Meeting of
the Association for Computational Linguistics,
pages 17?24, Philadelphia, PA.
Kruijff-Korbayova?, Ivana and Bonnie
Webber. 2001a. Concession, implicature
and alternative sets. In Fourth International
Workshop on Computational Semantics,
Tilburg, the Netherlands.
Kruijff-Korbayova?, Ivana and Bonnie
Webber. 2001b. Information structure and
the semantics of ?otherwise.? In
ESSLLI?2001 Workshop on Information
Structure, Discourse Structure and Discourse
Semantics, pages 61?78, Helsinki, Finland.
Lagerwerf, Luuk. 1998. Causal Connectives
Have Presuppositions. Holland Academic
Graphics, The Hague, the Netherlands.
Levinson, Stephen. 2000. Presumptive
Meanings: The Theory of Generalized
Conversational Implicature. MIT Press,
Cambridge, MA.
Luperfoy, Susann. 1992. The representation
of multimodal user interface dialogues
using discourse pegs. In Proceedings of the
30th Annual Meeting of the Association for
Computational Linguistics (ACL), pages
22?31, University of Delaware, Newark.
Mann, William and Sandra Thompson.
1988. Rhetorical structure theory: Toward
a functional theory of text organization.
Text, 8(3):243?281.
Marcu, Daniel. 1999. Instructions for
manually annotating the discourse
structure of texts. Available from
http://www.isi.edu/?marcu.
Marcu, Daniel and Abdessamad Echihabi.
2002. An unsupervised approach to
recognizing discourse relations. In
Proceedings of the 40th Annual Meeting,
Association for Computational Linguistics,
pages 368?375, University of
Pennsylvania, Philadelphia.
Modjeska, Natalia Nygren. 2001. Towards a
resolution of comparative anaphora: A
corpus study of ??other.? In PAPACOL,
Italy.
Modjeska, Natalia Nygren. 2002. Lexical
and grammatical role constraints in
resolving other-anaphora. In Proceedings of
586
Computational Linguistics Volume 29, Number 4
the Discourse Anaphora and Anaphor
Resolution Colloquium, Lisbon, Portugal.
Moens, Marc and Mark Steedman. 1988.
Temporal ontology and temporal
reference. Computational Linguistics,
14(1):15?28.
Moore, Johanna and Martha Pollack. 1992.
A problem for RST: The need for
multi-level discouse analysis.
Computational Linguistics, 18(4):537?544.
Moser, Megan and Johanna Moore. 1995.
Investigating cue selection and placement
in tutorial discourse. In Proceedings of the
33rd Annual Meeting, Association for
Computational Linguistics, pages 130?135,
MIT, Cambridge, MA.
Moser, Megan and Johanna Moore. 1996.
Toward a synthesis of two accounts of
discourse structure. Computational
Linguistics, 22(3):409?419.
Not, Elena, Lucia Tovena, and Massimo
Zancanaro. 1999. Positing and resolving
bridging anaphora in deverbal NPs. In
ACL?99 Workshop on the Relationship between
Discourse/Dialogue Structure and Reference,
College Park, MD.
Partee, Barbara. 1984. Nominal and
temporal anaphora. Linguistics and
Philosophy, 7(3):287?324.
Polanyi, Livia and Martin H. van den Berg.
1996. Discourse structure and discourse
interpretation. In P. Dekker and
M. Stokhof, editors, Proceedings of the Tenth
Amsterdam Colloquium, pages 113?131,
University of Amsterdam.
Prince, Ellen. 1992. The ZPG letter: Subjects,
definiteness and information-status. In
Susan Thompson and William Mann,
editors, Discourse Description: Diverse
Analyses of a Fundraising Text. John
Benjamins, Amsterdam, pages 295?325.
Quirk, Randolph, Sidney Greenbaum,
Geoffrey Leech, and Jan Svartik. 1972. A
Grammar of Contemporary English.
Longman, Harlow, England.
Scha, Remko and Livia Polanyi. 1988. An
augmented context free grammar for
discourse. In Proceedings of the 12th
International Conference on Computational
Linguistics (COLING?88), pages 573?577,
Budapest, Hungary, August.
Schilder, Frank. 1997a. Towards a theory of
discourse processing: Flashback sequences
described by D-trees. In Proceedings of the
Formal Grammar Conference (ESSLLI?97),
Aix-en-Provence, France, August.
Schilder, Frank. 1997b. Tree discourse
grammar, or how to get attached to a
discourse. In Proceedings of the Second
International Workshop on Computational
Semantics, Tilburg, the Netherlands,
January.
Steedman, Mark. 1996. Surface Structure and
Interpretation. Volume 30 of Linguistic
Inquiry Monograph, 5, MIT Press,
Cambridge, MA.
Steedman, Mark. 2000a. Information
structure and the syntax-phonology
interface. Linguistic Inquiry, 34:649?689.
Steedman, Mark. 2000b. The Syntactic
Process. MIT Press, Cambridge, MA.
Stokhof, Martin and Jeroen Groenendijk.
1999. Dynamic semantics. In Robert
Wilson and Frank Keil, editors, MIT
Encyclopedia of Cognitive Science. MIT Press.
Cambridge, MA, pages 247?249
Stone, Matthew, Christine Doran, Bonnie
Webber, Tonia Bleam, and Martha Palmer.
2001. Microplanning from communicative
intentions: Sentence planning using
descriptions (SPUD). Technical Report no.
RUCCS TR68, Department of Cognitive
Science, Rutgers University, New
Brunswick, NJ.
Stone, Matthew and Daniel Hardt. 1999.
Dynamic discourse referents for tense and
modals. In Harry Bunt, editor,
Computational Semantics. Kluwer,
Dordrecht, the Netherlands, pages
287?299.
Strube, Michael. 1998. Never look back: An
alternative to centering. In Proceedings,
COLING/ACL?98, pages 1251?1257,
Montreal, Quebec, Canada.
Traugott, Elizabeth. 1995. The role of the
development of discourse markers in a
theory of grammaticalization. Paper
presented at ICHL XII, Manchester,
England. Revised version of (1997)
available at http://www.stanford.
edu/traugott/ect-papersonline.html.
Traugott, Elizabeth. 1997. The discourse
connective after all: A historical
pragmatic account. Paper presented at
ICL, Paris. Available at http://www.
stanford.edu/traugott/ect-
papersonline.html.
van den Berg, Martin H. 1996. Discourse
grammar and dynamic logic. In P. Dekker
and M. Stokhof, editors, Proceedings of the
Tenth Amsterdam Colloquium, pages 93?111,
ILLC/Department of Philosophy,
University of Amsterdam.
van Eijck, Jan and Hans Kamp. 1997.
Representing discourse in context. In Jan
van Benthem and Alice ter Meulen,
editors, Handbook of Logic and Language.
Elsevier Science B.V., Amsterdam, pages
181?237.
Venditti, Jennifer J., Matthew Stone,
Preetham Nanda, and Paul Tepper. 2002.
Discourse constraints on the
587
Webber et al Anaphora and Discourse Structure
interpretation of nuclear-accented
pronouns. In Proceedings of Symposium on
Speech Prosody, Aix-en-Provence, France.
Available at http://www.lpl.
univ-aix.fr/sp2002/papers.htm.
Vendler, Zeno. 1967. Linguistics in Philosophy.
Cornell University Press, Ithaca, NY.
Webber, Bonnie. 1988. Tense as discourse
anaphor. Computational Linguistics,
14(2):61?73.
Webber, Bonnie. 1991. Structure and
ostension in the interpretation of
discourse deixis. Language and Cognitive
Processes, 6(2):107?135.
Webber, Bonnie and Breck Baldwin. 1992.
Accommodating context change. In
Proceedings of the 30th Annual Meeting of the
Association for Computational Linguistics
(ACL), pages 96?103, University of
Delaware, Newark.
Webber, Bonnie and Aravind Joshi. 1998.
Anchoring a lexicalized tree-adjoining
grammar for discourse. In COLING/ACL
Workshop on Discourse Relations and
Discourse Markers, pages 86?92, Montreal,
Quebec, Canada.
Webber, Bonnie, Alistair Knott, and Aravind
Joshi. 2001. Multiple discourse
connectives in a lexicalized grammar for
discourse. In Harry Bunt, Reinhard
Muskens, and Elias Thijsse, editors,
Computing Meaning, volume 2. Kluwer,
Dordrecht, the Netherlands, pages
229?249.
Webber, Bonnie, Alistair Knott, Matthew
Stone, and Aravind Joshi. 1999a.
Discourse relations: A structural and
presuppositional account using lexicalised
TAG. In Proceedings of the 36th Annual
Meeting of the Association for Computational
Linguistics, pages 41?48, College Park,
MD.
Webber, Bonnie, Alistair Knott, Matthew
Stone, and Aravind Joshi. 1999b. What
are little trees made of: A structural and
presuppositional account using lexicalised
TAG. In Proceedings of International
Workshop on Levels of Representation in
Discourse (LORID?99), pages 151?156,
Edinburgh.
Wiebe, Janyce. 1993. Issues in linguistic
segmentation. In Workshop on Intentionality
and Structure in Discourse Relations,
Association for Computational Linguistics,
pages 148?151, Ohio State University.
Woods, William. 1978. Semantics and
quantification in natural language
question answering. In Marshall C. Yovits,
editor, Advances in Computers, volume 17.
Academic Press, New York, pages 1?87.
XTAG-Group. 2001. A lexicalized tree
adjoining grammar for English. Technical
Report no. IRCS 01-03, University
of Pennsylvania, Philadelphia. Available at
ftp://ftp.cis.upenn.edu/pub/ircs/technical-
reports/01-03.
Proceedings of the ACL Interactive Poster and Demonstration Sessions,
pages 1?4, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
An Information-State Approach to Collaborative Reference
David DeVault1 Natalia Kariaeva2 Anubha Kothari2 Iris Oved3 and Matthew Stone1
1Computer Science 2Linguistics 3Philosophy and Center for Cognitive Science
Rutgers University
Piscataway NJ 08845-8020
Firstname.Lastname@Rutgers.Edu
Abstract
We describe a dialogue system that works
with its interlocutor to identify objects.
Our contributions include a concise, mod-
ular architecture with reversible pro-
cesses of understanding and generation,
an information-state model of reference,
and flexible links between semantics and
collaborative problem solving.
1 Introduction
People work together to make sure they understand
one another. For example, when identifying an ob-
ject, speakers are prepared to give many alternative
descriptions, and listeners not only show whether
they understand each description but often help the
speaker find one they do understand (Clark and
Wilkes-Gibbs, 1986). This natural collaboration is
part of what makes human communication so robust
to failure. We aim both to explain this ability and to
reproduce it.
In this paper, we present a novel model of collab-
oration in referential linguistic communication, and
we describe and illustrate its implementation. As we
argue in Section 2, our approach is unique in com-
bining a concise abstraction of the dynamics of joint
activity with a reversible grammar-driven model of
referential language. In the new information-state
model of reference we present in Section 3, inter-
locutors work together over multiple turns to asso-
ciate an entity with an agreed set of concepts that
characterize it. On our approach, utterance planning
and understanding involves reasoning about how
domain-independent linguistic forms can be used
in context to contribute to the task; see Section 4.
Our system reduces to four modules: understanding,
update, deliberation and generation, together with
some supporting infrastructure; see Section 5. This
design derives the efficiency and flexibility of refer-
ential communication from carefully-designed rep-
resentation and reasoning in this simple architecture;
see Section 6. With this proof-of-concept implemen-
tation, then, we provide a jumping-off point for more
detailed investigation of knowledge and processes in
conversation.
2 Overview and Related Work
Our demonstration system plays a referential com-
munication game, much like the one that pairs of
human subjects play in the experiments of Clark and
Wilkes-Gibbs (1986). We describe each episode in
this game as an activity involving the coordinated
action of two participants: a director D who knows
the referent R of a target variable T and a matcher
M whose task is to identify R. Our system can play
either role, D or M, using virtual objects in a graph-
ical display as candidate targets and distractors, and
using text as its input and output. Our system uses
the same task knowledge and the same grammar
whichever role it plays. Of course, the system also
draws on private knowledge to decide how best to
carry out its role; for now it describes objects using
the domain-specific iteration proposed by Dale and
Reiter (1995). The knowledge we have formalized is
targeted to a proof-of-concept implementation, but
we see no methodological obstacle in adding to the
1
system?s resources.
We exemplify what our system does in (1).
(1) a. S: This one is a square.
b. U: Um-hm...
c. S: It?s light brown.
d. U: You mean like tan?
e. S: Yeah.
f. S: It?s solid.
g. U: Got it.
The system (S) and user (U) exchange seven utter-
ances in the course of identifying a tan solid square.
We achieve this interaction using the information-
state approach to dialogue system design (Larsson
and Traum, 2000). This approach describes dialogue
as a coordinated effort to maintain an agreed record
of the state of the conversation. Our model contrasts
with traditional plan-based models, as exemplified
by Heeman and Hirst?s model of goals and beliefs
in collaborative reference (1995). Our approach ab-
stracts away from such details of individuals? men-
tal states and cognitive processes, for principled rea-
sons (Stone, 2004a). We are able to capture these
details implicitly in the dynamics of conversation,
whereas plan-based models must represent them ex-
plicitly. Our representations are simpler than Hee-
man and Hirst?s but support more flexible dialogue.
For example, their approach to (1) would have in-
terlocutors coordinating on goals and beliefs about
a syntactic representation for the tan solid square;
for us, this description and the interlocutors? com-
mitment to it are abstract results of the underlying
collaborative activity.
Another important antecedent to our work is
Purver?s (2004) characterization of clarification of
names for objects and properties. We extend this
work to develop a treatment of referential descriptive
clarification. When we describe things, our descrip-
tions grow incrementally and can specify as much
detail as needed. Clarification becomes correspond-
ingly cumulative and open-ended. Our revised in-
formation state includes a model of cumulative and
open-ended collaborative activity, similar to that ad-
vocated by Rich et al (2001). We also benefit from
a reversible goal-directed perspective on descriptive
language (Stone et al, 2003).
3 Information State
Our information state (IS) models the ongoing col-
laboration using a stack of tasks. For a task of col-
laborative reference, the IS tracks how interlocutors
together set up and solve a constraint-satisfaction
problem to identify a target object. In any state, D
and M have agreed on a target variable T and a set of
constraints that the value of T must satisfy. When M
recognizes that these constraints identify R, the task
ends successfully. Until then, D can take actions
that contribute new constraints on R. Importantly,
what D says adds to what is already known about R,
so that the identification of R can be accomplished
across multiple sentences with heterogeneous syn-
tactic structure.
Our IS also allows subtasks of questioning or clar-
ification that interlocutors can use to maintain align-
ment. The same constraint-satisfaction model is
used not only for referring to displayed objects but
also for referring to abstract entities, such as actions
or properties. Our IS tracks the salience of entity
and property referents and, like Purver?s, maintains
the previous utterance for reference in clarification
questions. Note, however, that we do not factor
updates to the IS through an abstract taxonomy of
speech acts. Instead, utterances directly make do-
main moves, such as adding a constraint, so our ar-
chitecture allows utterances to trigger an open-ended
range of domain-specific updates.
4 Linguistic Representations
The way utterances signal task contributions is
through a collection of presupposed constraints. To
understand an utterance, we solve the utterance?s
grammatically-specified semantic constraints. An
interpretation is only feasible if it represents a
contextually-appropriate contribution to the ongoing
task. Symmetrically, to generate an utterance, we
use the grammar to formulate a set of constraints;
these constraints must identify the contribution the
system intends to make. We view interpreted lin-
guistic structures as representing communicative in-
tentions; see (Stone et al, 2003) or (Stone, 2004b).
As in (DeVault et al, 2004), a knowledge in-
terface mediates between domain-general meanings
and the domain-specific ontology supported in a par-
ticular application. This allows us to build inter-
2
pretations using domain-specific representations for
referents, for task moves, and for the domain prop-
erties that characterize referents.
5 Architecture
Our system is implemented in Java. A set of in-
terface types describes the flow of information and
control through the architecture. The representation
and reasoning outlined in Sections 3 and 4 is ac-
complished by implementations of these interfaces
that realize our approach. Modules in the architec-
ture exchange messages about events and their in-
terpretations. (1) Deliberation responds to changes
in the IS by proposing task moves. (2) Generation
constructs collaborative intentions to accomplish the
planned task moves. (3) Understanding infers col-
laborative intentions behind user actions. Genera-
tion and understanding share code to construct inten-
tions for utterances, and both carry out a form of in-
ference to the best explanation. (4) Update advances
the IS symmetrically in response to intentions sig-
naled by the system or recognized from the user;
the symmetric architecture frees the designer from
programming complementary updates in a symmet-
rical way. Additional supporting infrastructure han-
dles the recognition of input actions, the realization
of output actions, and interfacing between domain
knowledge and linguistic resources.
Our system is designed not just for users to inter-
act with, but also for demonstrating and debugging
the system?s underlying models. Processing can be
paused at any point to allow inspection of the sys-
tem?s representations using a range of visualization
tools. You can interactively explore the IS, including
the present state of the world, the agreed direction
of the ongoing task, and the representation of lin-
guistic distinctions in salience and information sta-
tus. You can test the grammar and other interpretive
resources. And you can visualize the search space
for understanding and generation.
6 Example
Let us return to dialogue (1). Here the system rep-
resents its moves as successively constraining the
shape, color and pattern of the target object. In gen-
erating (1c), the system iteratively elaborates its de-
scription from brown to light brown in an attempt
to identify the object?s color unambiguously. The
user?s clarification request at (1d) marks this de-
scription of color as problematic and so triggers a
nested instance of the collaborative reference task.
At (1e) the system adds the user?s proposed con-
straint and (we assume) solves this nested subtask.
The system returns to the main task at (1f) having
grounded the color constraint and continues by iden-
tifying the pattern of the target object.
Let us explore utterance (1c) in more detail. The
IS records the status of the identification process.
The system is the director; the user is the matcher.
The target is represented provisionally by a dis-
course referent t1, and what has been agreed so far
is that the current target is a square of the rele-
vant sort for this task, represented in the agent as
square-figure-object(t1). In addition, the system has
privately recorded that square o1 is the referent it
must identify. For this IS, it is expected that the
director will propose an additional constraint iden-
tifying t1. The discourse state represents t1 as being
in-focus, or available for pronominal reference.
Deliberation now gives the generator a specific
move for the system to achieve:
(2) add-constraint(t1,color-sandybrown(t1))
The content of the move in (2) is that the system
should update the collaborative reference task to in-
clude the constraint that the target is drawn in a par-
ticular, domain-specific color (RGB value F4-A4-60,
or XHTML standard ?sandy brown?). The system
finds an utterance that achieves this by exploring
head-first derivations in its grammar; it arrives at the
derivation of it?s light brown in (3).
(3)
brown [present predicative adjective]



H
HH
HH
it [subject] light [color degree adverb]
A set of presuppositions connect this linguistic
structure to a task domain; they are given in (4a).
The relevant instances in this task are shown in (4b).
(4) a. predication(M)?brown(C)? light(C)
b. predication(add-constraint)?
brown(color-sandybrown)?
light(color-sandybrown)
3
The utterance also uses it to describe a referent
X so presupposes that in-focus(X) holds. The
move effected by the utterance is schematized as
M(X ,C(X)). Given the range of possible task moves
in the current context, the constraints specified by
the grammar for (3) are modeled as determining the
instantiation in (2). The system realizes the utter-
ance and assumes, provisionally, that the utterance
achieves its intended effect and records the new con-
straint on t1.
Because the generation process incorporates en-
tirely declarative reasoning, it is normally reversible.
Normally, the interlocutor would be able to identify
the speaker?s intended derivation, associate it with
the same semantic constraints, resolve those con-
straints to the intended instances, and thereby dis-
cover the intended task move. In our example, this
is not what happens. Recognition of the user?s clari-
fication request is triggered as in (Purver, 2004). The
system fails to interpret utterance (1d) as an appro-
priate move in the main reference task. As an alter-
native, the system ?downdates? the context to record
the fact that the system?s intended move may be the
subject of explicit grounding. This involves push-
ing a new collaborative reference task on the stack
of ongoing activities. The system remains the direc-
tor, the new target is the variable C in interpretation
and the referent to be identified is the property color-
sandybrown. Interpretation of (1d) now succeeds.
7 Discussion
Our work bridges research on collaborative dialogue
in AI (Rich et al, 2001) and research on pragmat-
ics in computational linguistics (Stone et al, 2003).
The two traditions have a lot to gain from reconcil-
ing their assumptions, if as Clark (1996) suggests,
people?s language use is coextensive with their joint
activity. There are implications both ways.
For pragmatics, our model suggests that language
use requires collaboration in part because reaching
agreement about content involves substantive social
knowledge and coordination. Indeed, we suspect
that collaborative reference is only one of many rel-
evant social processes. For collaborative dialogue
systems, adopting rich declarative linguistic repre-
sentations enables us to directly interface the core
modules of a collaborative system with one another.
In language understanding, for example, we can col-
lapse together notional subprocesses like semantic
reconstruction, reference resolution, and intention
recognition and solve them in a uniform way.
Our declarative, reversible approach supports an
analysis of how the system?s specifications drive its
input-output behavior. The architecture of this sys-
tem thus provides the groundwork for further in-
vestigations into the interaction of social, linguis-
tic, cognitive and even perceptual and developmen-
tal processes in meaningful communication.
Acknowledgements
Supported in part by NSF HLC 0308121. Thanks to
Paul Tepper.
References
H. H. Clark and D. Wilkes-Gibbs. 1986. Referring as a
collaborative process. Cognition, 22:1?39.
H. H. Clark. 1996. Using Language. Cambridge.
R. Dale and E. Reiter. 1995. Computational interpreta-
tions of the Gricean maxims in the generation of refer-
ring expressions. Cognitive Science, 18:233?263.
D. DeVault, C. Rich, and C. L. Sidner. 2004. Natural
language generation and discourse context: Comput-
ing distractor sets from the focus stack. In FLAIRS.
P. Heeman and G. Hirst. 1995. Collaborating on refer-
ring expressions. Comp. Ling., 21(3):351?382.
S. Larsson and D. Traum. 2000. Information state and
dialogue management in the TRINDI dialogue move
engine toolkit. Natural Language Eng., 6:323?340.
M. Purver. 2004. The Theory and Use of Clarification
Requests in Dialogue. Ph.D. thesis, Univ. of London.
C. Rich, C. L. Sidner, and N. Lesh. 2001. COL-
LAGEN: applying collaborative discourse theory to
human-computer interaction. AI Magazine, 22:15?25.
M. Stone, C. Doran, B. Webber, T. Bleam, and M. Palmer.
2003. Microplanning with communicative intentions.
Comp. Intelligence, 19(4):311?381.
M. Stone. 2004a. Communicative intentions and conver-
sational processes. In J. Trueswell and M. K. Tanen-
haus, editors, Approaches to Studying World-Situated
Language Use, pages 39?70. MIT.
M. Stone. 2004b. Intention, interpretation and the com-
putational structure of language. Cognitive Science,
28(5):781?809.
4
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 336?343,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Sentence generation as a planning problem
Alexander Koller
Center for Computational Learning Systems
Columbia University
koller@cs.columbia.edu
Matthew Stone
Computer Science
Rutgers University
Matthew.Stone@rutgers.edu
Abstract
We translate sentence generation from TAG
grammars with semantic and pragmatic in-
formation into a planning problem by encod-
ing the contribution of each word declara-
tively and explicitly. This allows us to ex-
ploit the performance of off-the-shelf plan-
ners. It also opens up new perspectives on
referring expression generation and the rela-
tionship between language and action.
1 Introduction
Systems that produce natural language must synthe-
size the primitives of linguistic structure into well-
formed utterances that make desired contributions to
discourse. This is fundamentally a planning prob-
lem: Each linguistic primitive makes certain con-
tributions while potentially introducing new goals.
In this paper, we make this perspective explicit by
translating the sentence generation problem of TAG
grammars with semantic and pragmatic information
into a planning problem stated in the widely used
Planning Domain Definition Language (PDDL, Mc-
Dermott (2000)). The encoding provides a clean
separation between computation and linguistic mod-
elling and is open to future extensions. It also allows
us to benefit from the past and ongoing advances in
the performance of off-the-shelf planners (Blum and
Furst, 1997; Kautz and Selman, 1998; Hoffmann
and Nebel, 2001).
While there have been previous systems that en-
code generation as planning (Cohen and Perrault,
1979; Appelt, 1985; Heeman and Hirst, 1995), our
approach is distinguished from these systems by its
focus on the grammatically specified contributions
of each individual word (and the TAG tree it an-
chors) to syntax, semantics, and local pragmatics
(Hobbs et al, 1993). For example, words directly
achieve content goals by adding a corresponding se-
mantic primitive to the conversational record. We
deliberately avoid reasoning about utterances as co-
ordinated rational behavior, as earlier systems did;
this allows us to get by with a much simpler logic.
The problem we solve encompasses the genera-
tion of referring expressions (REs) as a special case.
Unlike some approaches (Dale and Reiter, 1995;
Heeman and Hirst, 1995), we do not have to dis-
tinguish between generating NPs and expressions of
other syntactic categories. We develop a new per-
spective on the lifecycle of a distractor, which allows
us to generate more succinct REs by taking the rest
of the utterance into account. More generally, we do
not split the process of sentence generation into two
separate steps of sentence planning and realization,
as most other systems do, but solve the joint prob-
lem in a single integrated step. This can potentially
allow us to generate higher-quality sentences. We
share these advantages with systems such as SPUD
(Stone et al, 2003).
Crucially, however, our approach describes the
dynamics of interpretation explicitly and declara-
tively. We do not need to assume extra machin-
ery beyond the encoding of words as PDDL plan-
ning operators; for example, our planning opera-
tors give a self-contained description of how each
individual word contributes to resolving references.
This makes our encoding more direct and transpar-
ent than those in work like Thomason and Hobbs
(1997) and Stone et al (2003).
We present our encoding in a sequence of steps,
each of which adds more linguistic information to
336
the planning operators. After a brief review of LTAG
and PDDL, we first focus on syntax alone and show
how to cast the problem of generating grammatically
well-formed LTAG trees as a planning problem in
Section 2. In Section 3, we add semantics to the ele-
mentary trees and add goals to communicate specific
content (this corresponds to surface realization). We
complete the account by modeling referring expres-
sions and go through an example. Finally, we assess
the practical efficiency of our approach and discuss
future work in Section 4.
2 Grammaticality as planning
We start by reviewing the LTAG grammar formal-
ism and giving an intuition of how LTAG gen-
eration is planning. We then add semantic roles
to the LTAG elementary trees in order to distin-
guish different substitution nodes. Finally, we re-
view the PDDL planning specification language and
show how LTAG grammaticality can be encoded as
a PDDL problem and how we can reconstruct an
LTAG derivation from the plan.
2.1 Tree-adjoining grammars
The grammar formalism we use here is that of lex-
icalized tree-adjoining grammars (LTAG; Joshi and
Schabes (1997)). An LTAG grammar consists of a
finite set of lexicalized elementary trees as shown in
Fig. 1a. Each elementary tree contains exactly one
anchor node, which is labelled by a word. Elemen-
tary trees can contain substitution nodes, which are
marked by down arrows (?). Those elementary trees
that are auxiliary trees also contain exactly one foot
node, which is marked with an asterisk (?). Trees
that are not auxiliary trees are called initial trees.
Elementary trees can be combined by substitution
and adjunction to form larger trees. Substitution
is the operation of replacing a substitution node of
some tree by another initial tree with the same root
label. Adjunction is the operation of splicing an aux-
iliary tree into some node v of a tree, in such a way
that the root of the auxiliary tree becomes the child
of v?s parent, and the foot node becomes the parent
of v?s children. If a node carries a null adjunction
constraint (indicated by no-adjoin), no adjunction is
allowed at this node; if it carries an obligatory ad-
junction constraint (indicated by adjoin!), an auxil-
S
NP ? 
VP
V
likes
NP
the
NP * 
PN
Mary
N
white N * 
Mary
likes 
rabbit
white
(a)
(b)
(c)
NP ? 
N
rabbit
NP
adjoin!
NP
S
NP VP
V
likes
NP 
PN
Mary
NP
the
N
white N
rabbit
the
no-adjoin
Figure 1: Building a derived (b) and a derivation tree
(c) by combining elementary trees (a).
iary tree must be adjoined there.
In Fig. 1a, we have combined some ele-
mentary trees by substitution (indicated by the
dashed/magenta arrows) and adjunction (dotted/blue
arrows). The result of these operations is the derived
tree in Fig. 1b. The derivation tree in Fig. 1c rep-
resents the tree combination operations we used by
having one node per elementary tree and drawing a
solid edge if we combined the two trees by substitu-
tion, and a dashed edge for adjunctions.
2.2 The basic idea
Consider the process of constructing a derivation
tree top-down. To build the tree in Fig. 1c, say, we
start with the empty derivation tree and an obligation
to generate an expression of category S. We satisfy
this obligation by adding the tree for ?likes? as the
root of the derivation; but in doing so, we have in-
troduced new unfilled substitution nodes of category
NP, i.e. the derivation tree is not complete. We use
the NP tree for ?Mary? to fill one substitution node
and the NP tree for ?rabbit? to fill the other. This
fills both substitution nodes, but the ?rabbit? tree in-
troduces an obligatory adjunction constraint, which
we must satisfy by adjoining the auxiliary tree for
?the?. We now have a grammatical derivation tree,
but we are free to continue by adding more auxiliary
trees, such as the one for ?white?.
As we have just presented it, the generation of
derivation trees is essentially a planning problem.
A planning problem involves states and actions that
can move from one state to another. The task is to
find a sequence of actions that moves us from the
337
initial state to a state that satisfies all the goals. In
our case, the states are defined by the unfilled sub-
stitution nodes, the unsatisfied obligatory adjunction
constraints, and the nodes that are available for ad-
junction in some (possibly incomplete) derivation
tree. Each action adds a single elementary tree to the
derivation, removing some of these ?open nodes?
while introducing new ones. The initial state is asso-
ciated with the empty derivation tree and a require-
ment to generate an expression for the given root cat-
egory. The goal is for the current derivation tree to
be grammatically complete.
2.3 Semantic roles
Formalizing this intuition requires unique names for
each node in the derived tree. Such names are nec-
essary to distinguish the different open substitution
nodes that still need to be filled, or the different
available adjunction sites; in the example, the plan-
ner needed to be aware that ?likes? introduces two
separate NP substitution nodes to fill.
There are many ways to assign these names. One
that works particularly well in the context of PDDL
(as we will see below) is to assume that each node
in an elementary tree, except for ones with null ad-
junction constraints, is marked with a semantic role,
and that all substitution nodes are marked with dif-
ferent roles. Nothing hinges on the particular role in-
ventory; here we assume an inventory including the
roles ag for ?agent? and pat for ?patient?. We also
assume one special role self, which must be used for
the root of each elementary tree and must never be
used for substitution nodes.
We can now assign a unique name to every sub-
stitution node in a derived tree by assigning arbitrary
but distinct indices to each use of an elementary tree,
and giving the substitution node with role r in the el-
ementary tree with index i the identity i.r. In the ex-
ample, let?s say the ?likes? tree has index 1 and the
semantic roles for the substitution nodes were ag and
pat, respectively. The planner action that adds this
tree would then require substitution of one NP with
identity 1.ag and another NP with identity 1.pat; the
?Mary? tree would satisfy the first requirement and
the ?rabbit? tree the second. If we assume that no
elementary tree contains two internal nodes with the
same category and role, we can refer to adjunction
opportunities in a similar way.
Action S-likes-1(u). Precond: subst(S,u),step(1)
Effect: ?subst(S,u),subst(NP,1.ag),
subst(NP,1.pat),?step(1),step(2)
Action NP-Mary-2(u). Precond: subst(NP,u),step(2)
Effect: ?subst(NP,u),?step(2),step(3)
Action NP-rabbit-3(u). Precond: subst(NP,u),step(3)
Effect: ?subst(NP,u),canadjoin(NP,u),
mustadjoin(NP,u),?step(3),step(4)
Action NP-the-4(u). Precond: canadjoin(NP,u),step(4)
Effect: ?mustadjoin(NP,u),?step(4),step(5)
Figure 2: Some actions for the grammar in Fig. 1.
2.4 Encoding in PDDL
Now we are ready to encode the problem of generat-
ing grammatical LTAG derivation trees into PDDL.
PDDL (McDermott, 2000) is the standard input lan-
guage for modern planning systems. It is based on
the well-known STRIPS language (Fikes and Nils-
son, 1971). In this paradigm, a planning state is
defined as a finite set of ground atoms of predicate
logic that are true in this state; all other atoms are as-
sumed to be false. Actions have a number of param-
eters, as well as a precondition and effect, both of
which are logical formulas. When a planner tries to
apply an action, it will first create an action instance
by binding all parameters to constants from the do-
main. It must then verify that the precondition of the
action instance is satisfied in the current state. If so,
the action can be applied, in which case the effect is
processed in order to change the state. In STRIPS,
the precondition and effect both had to be conjunc-
tions of atoms or negated atoms; positive effects are
interpreted as making the atom true in the new state,
and negative ones as making it false. PDDL per-
mits numerous extensions to the formulas that can
be used as preconditions and effects.
Each action in our planning problem encodes the
effect of adding some elementary tree to the deriva-
tion tree. An initial tree with root category A trans-
lates to an action with a parameter u for the iden-
tity of the node that the current tree is substituted
into. The action carries the precondition subst(A,u),
and so can only be applied if u is an open substi-
tution node in the current derivation with the cor-
rect category A. Auxiliary trees are analogous, but
carry the precondition canadjoin(A,u). The effect
of an initial tree is to remove the subst condition
from the planning state (to record that the substitu-
338
S-likes-1
 (1.self)
subst(S,1.self)
subst(NP,1.ag)
NP-Mary-2
   (1.ag)
subst(NP,1.pat)
NP-rabbit-3
  (1.pat)
mustadjoin(NP,1.pat)
NP-the-4
(1.pat)
canadjoin(NP,1.pat)
subst(NP,1.pat)
canadjoin(NP,1.pat)
step(1)
step(2)
step(3)
step(4)
step(5)
Figure 3: A plan for the actions in Fig. 2.
tion node u is now filled); an auxiliary tree has an
effect ?mustadjoin(A,u) to indicate that any oblig-
atory adjunction constraint is satisfied but leaves the
canadjoin condition in place to allow multiple ad-
junctions into the same node. In both cases, effects
add subst, canadjoin and mustadjoin atoms repre-
senting the substitution nodes and adjunction sites
that are introduced by the new elementary tree.
One remaining complication is that an action must
assign new identities to the nodes it introduces; thus
it must have access to a tree index that was not used
in the derivation tree so far. We use the number of
the current plan step as the index. We add an atom
step(1) to the initial state of the planning problem,
and we introduce k different copies of the actions for
each elementary tree, where k is some upper limit
on the plan size. These actions are identical, except
that the i-th copy has an extra precondition step(i)
and effects ?step(i) and step(i+1). It is no restric-
tion to assume an upper limit on the plan size, as
most modern planners search for plans smaller than
a given maximum length anyway.
Fig. 2 shows some of the actions into which the
grammar in Fig. 1 translates. We display only one
copy of each action and have left out most of the
canadjoin effects. In addition, we use an initial state
containing the atoms subst(S,1.self) and step(1)
and a final state consisting of the following goal:
?A,u.?subst(A,u)??A,u.?mustadjoin(A,u).
We can then send the actions and the initial state
and goal specifications to any off-the-shelf planner
and obtain the plan in Fig. 3. The straight arrows in
the picture link the actions to their preconditions and
(positive) effects; the curved arrows indicate atoms
that carry over from one state to the next without
being changed by the action. Atoms are printed in
boldface iff they contradict the goal.
This plan can be read as a derivation tree that has
one node for each action instance in the plan, and an
edge from node u to node v if u establishes a subst
or canadjoin fact that is a precondition of v. These
causal links are drawn as bold edges in Fig. 3. The
mapping is unique for substitution edges because
subst atoms are removed by every action that has
them as their precondition. There may be multiple
action instances in the plan that introduce the same
atom canadjoin(A,u). In this case, we can freely
choose one of these instances as the parent.
3 Sentence generation as planning
Now we extend this encoding to deal with semantics
and referring expressions.
3.1 Communicative goals
In order to use the planner as a surface realiza-
tion algorithm for TAG along the lines of Koller
and Striegnitz (2002), we attach semantic content to
each elementary tree and require that the sentence
achieves a certain communicative goal. We also use
a knowledge base that specifies the speaker?s knowl-
edge, and require that we can only use trees that ex-
press information in this knowledge base.
We follow Stone et al (2003) in formalizing the
semantic content of a lexicalized elementary tree t as
a finite set of atoms; but unlike in earlier approaches,
we use the semantic roles in t as the arguments of
these atoms. For instance, the semantic content of
the ?likes? tree in Fig. 1 is {like(self,ag,pat)} (see
also the semcon entries in Fig. 4). The knowledge
base is some finite set of ground atoms; in the exam-
ple, it could contain such entries as like(e,m,r) and
rabbit(r). Finally, the communicative goal is some
subset of the knowledge base, such as {like(e,m,r)}.
We implement unsatisfied communicative goals
as flaws that the plan must remedy. To this end,
we add an atom cg(P,a1, . . . ,an) for each element
P(a1, . . . ,an) of the communicative goal to the ini-
tial state, and we add a corresponding conjunct
?P,x1, . . . ,xn.?cg(P,x1, . . . ,xn) to the goal. In ad-
dition, we add an atom skb(P,a1, . . . ,an) to the
initial state for each element P(a1, . . . ,an) of the
(speaker?s) knowledge base.
339
We then add parameters x1, . . . ,xn to each action
with n semantic roles (including self). These new
parameters are intended to be bound to individual
constants in the knowledge base by the planner. For
each elementary tree t and possible step index i, we
establish the relationship between these parameters
and the roles in two steps. First we fix a function id
that maps the semantic roles of t to node identities.
It maps self to u and each other role r to i.r. Second,
we fix a function ref that maps the outputs of id bi-
jectively to the parameters x1, . . . ,xn, in such a way
that ref(u) = x1.
We can then capture the contribution of the i-th
action for t to the communicative goal by giving it
an effect ?cg(P, ref(id(r1)), . . . , ref(id(rn))) for each
element P(r1, . . . ,rn) of the elementary tree?s seman-
tic content. We restrict ourselves to only expressing
true statements by giving the action a precondition
skb(P, ref(id(r1)), . . . , ref(id(rn))) for each element
of the semantic content.
In order to keep track of the connection between
node identities and individuals for future reference,
each action gets an effect referent(id(r), ref(id(r)))
for each semantic role r except self. We enforce the
connection between u and x1 by adding a precondi-
tion referent(u,x1).
In the example, the most interesting action in this
respect is the one for the elementary tree for ?likes?.
This action looks as follows:
Action S-likes-1(u,x1,x2,x3).
Precond: subst(S,u),step(1), referent(u,x1),
skb(like,x1,x2,x3)
Effect: ?subst(S,u),subst(NP,1.ag),subst(NP,1.pat),
?step(1),step(2),
referent(1.ag,x2), referent(1.pat,x3),
?cg(like,x1,x2,x3)
We can run a planner and interpret the plan as
above; the main difference is that complete plans not
only correspond to grammatical derivation trees, but
also express all communicative goals. Notice that
this encoding models some aspects of lexical choice:
The semantic content sets of the elementary trees
need not be singletons, and so there may be multiple
ways of partitioning the communicative goal into the
content sets of various elementary trees.
3.2 Referring expressions
Finally, we extend the system to deal with the gen-
eration of referring expressions. While this prob-
lem is typically taken to require the generation of a
noun phrase that refers uniquely to some individual,
we don?t need to make any assumptions about the
syntactic category here. Moreover, we consider the
problem in the wider context of generating referring
expressions within a sentence, which can allow us to
generate more succinct expressions.
Because a referring expression must allow the
hearer to identify the intended referent uniquely,
we keep track of the hearer?s knowledge base sep-
arately. We use atoms hkb(P,a1, . . . ,an), as with
skb above. In addition, we assume pragmatic
information of the form pkb(P,a1, . . . ,an). The
three pragmatic predicates that we will use here are
hearer-new, indicating that the hearer does not know
about the existence of an individual and can?t infer it
(Stone et al, 2003), hearer-old for the opposite, and
contextset. The context set of an intended referent is
the set of all individuals that the hearer might possi-
bly confuse it with (DeVault et al, 2004). It is empty
for hearer-new individuals. To say that b is in a?s
context set, we put the atom pkb(contextset,a,b)
into the initial state.
In addition to the semantic content, we equip ev-
ery elementary tree in the grammar with a seman-
tic requirement and a pragmatic condition (Stone
et al, 2003). The semantic requirement is a set of
atoms spelling out presuppositions of an elementary
tree that can help the hearer identify what its argu-
ments refer to. For instance, ?likes? has the selec-
tional restriction that its agent must be animate; thus
the hearer will not consider inanimate individuals as
distractors for the referring expression in agent posi-
tion. The pragmatic condition is a set of atoms over
the predicates in the pragmatic knowledge base.
In our setting, every substitution node that is in-
troduced during the derivation introduces a new re-
ferring expression. This means that we can dis-
tinguish the referring expressions by the identity
of the substitution node that introduced them. For
each referring expression u (where u is a node iden-
tity), we keep track of the distractors in atoms
of the form distractor(u,x). The presence of an
atom distractor(u,a) in some planning state repre-
sents the fact that the current derivation tree is not
yet informative enough to allow the hearer to iden-
tify the intended referent for u uniquely; a is an-
other individual that is not the intended referent,
340
but consistent with the partial referring expression
we have constructed so far. We enforce uniqueness
of all referring expressions by adding the conjunct
?u,x?distractor(u,x) to the planning goal.
Now whenever an action introduces a new substi-
tution node u, it will also introduce some distractor
atoms to record the initial distractors for the refer-
ring expression at u. An individual a is in the initial
distractor set for the substitution node with role r
if (a) it is not the intended referent, (b) it is in the
context set of the intended referent, and (c) there
is a choice of individuals for the other parameters
of the action that satisfies the semantic requirement
together with a. This is expressed by adding the
following effect for each substitution node; the con-
junction is over the elements P(r1, . . . ,rn) of the se-
mantic requirement, and there is one universal quan-
tifier for y and for each parameter x j of the action
except for ref(id(r)).
?y,x1, . . . ,xn
(y 6= ref(id(r))?pkb(contextset, ref(id(r)),y)?
V
hkb(P, ref(id(r1)), . . . , ref(id(rn)))[y/ref(id(r))])
? distractor(id(r),y)
On the other hand, a distractor a for a referring ex-
pression introduced at u is removed when we substi-
tute or adjoin an elementary tree into u which rules
a out. For instance, the elementary tree for ?rabbit?
will remove all non-rabbits from the distractor set of
the substitution node into which it is substituted. We
achieve this by adding the following effect to each
action; here the conjunction is over all elements of
the semantic content.
?y.(?
V
hkb(P, ref(id(r1)), . . . , ref(id(rn))))[y/x1]
??distractor(u,y),
Finally, each action gets its pragmatic condition
as a precondition.
3.3 The example
By way of example, Fig. 5 shows the full versions
of the actions from Fig. 2, for the extended gram-
mar in Fig. 4. Let?s say that the hearer knows
about two rabbits r (which is white) and r? (which
is not), about a person m with the name Mary, and
about an event e, and that the context set of r is
{r,r?,m,e}. Let?s also say that our communicative
goal is {like(e,m,r)}. In this case, the first action
instance in Fig. 3, S-likes-1(1.self,e,m,r), intro-
duces a substitution node with identity 1.pat. The
S:self
NP:ag ? 
VP:self
V:self
likes
NP:self 
the NP:self *
NP:self
a NP:self *
NP:self
PN:self
Mary
N:self
rabbit
N:self
white N:self * 
semcon: {like(self,ag,pat)}
semreq: {animate(ag)}
semcon: { }
semreq: { }
pragcon: {hearer-old(self)}
semcon: { }
semreq: { }
pragcon: {hearer-new(self)}
semcon: {white(self)}
semcon: {name(self, mary)}
semcon: {rabbit(self)}
NP:pat ? 
adjoin!
NP:self
Figure 4: The extended example grammar.
initial distractor set of this node is {r?,m} ? the set
of all individuals in r?s context set except for inan-
imate objects (which violate the semantic require-
ment) and r itself. The NP-rabbit-3 action removes
m from the distractor set, but at the end of the plan in
Fig. 3, r? is still a distractor, i.e. we have not reached
a goal state. We can complete the plan by perform-
ing a final action NP-white-5(1.pat,r), which will
remove this distractor and achieve the planning goal.
We can still reconstruct a derivation tree from the
complete plan literally as described in Section 2.
Now let?s say that the hearer did not know about
the existence of the individual r before the utterance
we are generating. We model this by marking r as
hearer-new in the pragmatic knowledge base and as-
signing it an empty context set. In this case, the re-
ferring expression 1.pat would be initialized with an
empty distractor set. This entitles us to use the action
NP-a-4 and generate the four-step plan correspond-
ing to the sentence ?Mary likes a rabbit.?
4 Discussion and future work
In conclusion, let?s look in more detail at computa-
tional issues and the role of mutually constraining
referring expressions.
341
Action S-likes-1(u,x1,x2,x3).
Precond: referent(u,x1),skb(like,x1,x2,x3),subst(S,u),step(1)
Effect: ?cg(like,x1,x2,x3),?subst(S,u),?step(1),step(2),subst(NP,1.ag),subst(NP,1.pat),
?y.?hkb(like,y,x2,x3) ??distractor(u,y),
?y,x1,x3.x2 6= y?pkb(contextset,x2,y)?animate(y) ? distractor(1.ag,y),
?y,x1,x2.x3 6= y?pkb(contextset,x3,y) ? distractor(1.pat,y)
Action NP-Mary-2(u,x1).
Precond: referent(u,x1),skb(name,x1,mary),
subst(NP,u),step(2)
Effect: ?cg(name,x1,mary),?subst(NP,u),
?step(2),step(3),
?y.?hkb(name,y,mary) ??distractor(u,y)
Action NP-rabbit-3(u,x1).
Precond: referent(u,x1),skb(rabbit,x1),
subst(N,u),step(3)
Effect: ?cg(rabbit,x1),?subst(N,u),?step(3),step(4),
canadjoin(NP,u),mustadjoin(NP,u),
?y.?hkb(rabbit,y) ??distractor(u,y)
Action NP-the-4(u,x1).
Precond: referent(u,x1),canadjoin(NP,u),step(4),
pkb(hearer-old,x1)
Effect: ?mustadjoin(NP,u),?step(4),step(5)
Action NP-a-4(u,x1).
Precond: referent(u,x1),canadjoin(NP,u),step(4),
pkb(hearer-new,x1)
Effect: ?mustadjoin(NP,u),?step(4),step(5)
Action NP-white-5(u,x1).
Precond: referent(u,x1),skb(white,x1),canadjoin(NP,u),step(5)
Effect: ?cg(white,x1),?mustadjoin(NP,u),?step(5),step(6),
?y.?hkb(white,y) ??distractor(u,y)
Figure 5: Some of the actions corresponding to the grammar in Fig. 4.
4.1 Computational issues
We lack the space to present the formal definition
of the sentence generation problem we encode into
PDDL. However, this problem is NP-complete, by
reduction of Hamiltonian Cycle ? unsurprisingly,
given that it encompasses realization, and the very
similar realization problem in Koller and Striegnitz
(2002) is NP-hard. So any algorithm for our prob-
lem must be prepared for exponential runtimes.
We have implemented the translation described in
this paper and experimented with a number of differ-
ent grammars, knowledge bases, and planners. The
FF planner (Hoffmann and Nebel, 2001) can com-
pute the plans in Section 3.3 in under 100 ms us-
ing the grammar in Fig. 4. If we add 10 more lex-
icon entries to the grammar, the runtime grows to
190 ms; and for 20 more entries, to 360 ms. The
runtime also grows with the plan length: It takes
410 ms to generate a sentence ?Mary likes the Adj
. . . Adj rabbit? with four adjectives and 890 ms for
six adjectives, corresponding to a plan length of 10.
We compared these results against a planning-based
reimplementation of SPUD?s greedy search heuris-
tic (Stone et al, 2003). This system is faster than FF
for small inputs (360 ms for four adjectives), but be-
comes slower as inputs grow larger (1000 ms for six
adjectives); but notice that while FF is also a heuris-
tic planner, it is guaranteed to find a solution if one
exists, unlike SPUD.
Planners have made tremendous progress in effi-
ciency in the past decade, and by encoding sentence
generation as a planning problem, we are set to profit
from any future improvements; it is an advantage
of the planning approach that we can compare very
different search strategies like FF?s and SPUD?s in
the same framework. However, our PDDL problems
are challenging for modern planners because most
planners start by computing all instances of atoms
and actions. In our experiments, FF generally spent
only about 10% of the runtime on search and the
rest on computing the instances; that is, there is a lot
of room for optimization. For larger grammars and
knowledge bases, the number of instances can easily
grow into the billions. In future work, we will there-
fore collaborate with experts on planning systems to
compute action instances only by need.
4.2 Referring expressions
In our analysis of referring expressions, the tree t
that introduces the new substitution nodes typically
initializes the distractor sets with proper subsets of
the entire domain. This allows us to generate suc-
cinct descriptions by encoding t?s presuppositions
as semantic requirements, and localizes the inter-
actions between the referring expressions generated
for different substitution nodes within t?s action.
342
However, an important detail in the encoding of
referring expressions above is that an individual a
counts as a distractor for the role r if there is any
tuple of values that satisfies the semantic require-
ment and has a in the r-component. This is correct,
but can sometimes lead to overly complicated refer-
ring expressions. An example is the construction ?X
takes Y from Z?, which presupposes that Y is in Z.
In a scenario that involves multiple rabbits, multiple
hats, and multiple individuals that are inside other
individuals, but only one pair of a rabbit r inside a
hat h, the expression ?X takes the rabbit from the
hat? is sufficient to refer uniquely to r and h (Stone
and Webber, 1998). Our system would try to gen-
erate an expression for Y that suffices by itself to
distinguish r from all distractors, and similarly for
Z. We will explore this issue further in future work.
5 Conclusion
In this paper, we have shown how sentence gener-
ation with TAG grammars and semantic and prag-
matic information can be encoded into PDDL. Our
encoding is declarative in that it can be used with
any correct planning algorithm, and explicit in that
the actions capture the complete effect of a word on
the syntactic, semantic, and local pragmatic goals.
In terms of expressive power, it captures the core of
SPUD, except for its inference capabilities.
This work is practically relevant because it opens
up the possibility of using efficient planners to make
generators faster and more flexible. Conversely, our
PDDL problems are a challenge for current plan-
ners and open up NLG as an application domain that
planning research itself can target.
Theoretically, our encoding provides a new
framework for understanding and exploring the gen-
eral relationships between language and action. It
suggests new ways of going beyond SPUD?s expres-
sive power, to formulate utterances that describe and
disambiguate concurrent real-world actions or ex-
ploit the dynamics of linguistic context within and
across sentences.
Acknowledgments. This work was funded by a DFG re-
search fellowship and the NSF grants HLC 0308121, IGERT
0549115, and HSD 0624191. We are indebted to Henry Kautz
for his advice on planning systems, and to Owen Rambow, Bon-
nie Webber, and the anonymous reviewers for feedback.
References
D. Appelt. 1985. Planning English Sentences. Cam-
bridge University Press, Cambridge England.
A. Blum and M. Furst. 1997. Fast planning through
graph analysis. Artificial Intelligence, 90:281?300.
P. R. Cohen and C. R. Perrault. 1979. Elements of a
plan-based theory of speech acts. Cognitive Science,
3(3):177?212.
R. Dale and E. Reiter. 1995. Computational interpreta-
tions of the Gricean maxims in the generation of refer-
ring expressions. Cognitive Science, 19.
D. DeVault, C. Rich, and C. Sidner. 2004. Natural lan-
guage generation and discourse context: Computing
distractor sets from the focus stack. In Proc. FLAIRS.
R. Fikes and N. Nilsson. 1971. STRIPS: A new approach
in the application of theorem proving to problem solv-
ing. Artificial Intelligence, 2:189?208.
P. Heeman and G. Hirst. 1995. Collaborating on
referring expressions. Computational Linguistics,
21(3):351?382.
J. Hobbs, M. Stickel, D. Appelt, and P. Martin. 1993.
Interpretation as abduction. Artificial Intelligence,
63:69?142.
J. Hoffmann and B. Nebel. 2001. The FF planning
system: Fast plan generation through heuristic search.
Journal of Artificial Intelligence Research, 14.
A. Joshi and Y. Schabes. 1997. Tree-Adjoining Gram-
mars. In G. Rozenberg and A. Salomaa, editors,
Handbook of Formal Languages, chapter 2, pages 69?
123. Springer-Verlag, Berlin.
H. Kautz and B. Selman. 1998. Blackbox: A new ap-
proach to the application of theorem proving to prob-
lem solving. In Workshop Planning as Combinatorial
Search, AIPS-98.
A. Koller and K. Striegnitz. 2002. Generation as depen-
dency parsing. In Proc. 40th ACL, Philadelphia.
D. V. McDermott. 2000. The 1998 AI Planning Systems
Competition. AI Magazine, 21(2):35?55.
M. Stone and B. Webber. 1998. Textual economy
through close coupling of syntax and semantics. In
Proc. INLG.
M. Stone, C. Doran, B.Webber, T. Bleam, andM. Palmer.
2003. Microplanning with communicative inten-
tions: The SPUD system. Computational Intelligence,
19(4):311?381.
R. Thomason and J. Hobbs. 1997. Interrelating interpre-
tation and generation in an abductive framework. In
AAAI Fall Symposium on Communicative Action.
343
On Identifying Sets 
Matthew Stone 
Department of Computer Science & 
,Center for Cognitive Science 
RutgersUn ivers i ty  
110 Frelinghuysen Road, Piscataway NJ 08854-8019 
mdstone @cs. rutgers, edu 
Abstract 
A range of research as explored the problem of 
generating referring expressions that uniquely iden- 
tify a single entity from the shared context. But 
what about expressions that identify sets of enti- 
ties? In this paper, I adapt recent semantic re- 
search on plural descriptions--using covers to ab- 
stract collective and distributive readings and us- 
ing sets of assignments o represent dependencies 
among references--to describe asearch problem for 
set-identifying expressions that largely mirrors the 
search problem for singular referring expressions. 
By structuring the search space only in terms of 
the words that can be added to the description, the 
proposal defuses potential combinatorial explosions 
that might otherwise arise with reference to sets. 
1 Introduction 
Natural anguage interaction lends itself to tasks like 
generalization, abstraction, comparison, and sum- 
marization which call for SETS of objects to be 
picked out using definite referring expressions. 
For example, consider the concrete function of 
describing the elements of a figure. In (1 b), we find 
a pair of elements from (1 a); imagine that some no- 
table relationship leads to an intention to identify 
and comment on just THOSE elements. 
? . . , . " 
I 
(1)a . . . . . . . . . . .  
? . . . 
The intersecting dotted segments. 
116 
As (lc) records, these are the intersecting dotted 
segments of (1 a), and can be designated as such. 
Or again, we find distinguished in (2b) five ele- 
ments of (2a), which might hold some independent 
interest. So we can and should identify these ele- 
ments, and (2c), the squares clustered at the lower 
left, will do the trick. 
(2) a 
b 
c 
\[\] \[\] 
0    
0 \[\] O0  
\ [30  \[\] 00 \ [  3 
\[\] \[\] \[\] \[\] 
\[\] 
\[\] \[\] 
\[\] \[\] 
The squares clustered at the lower left. 
Concrete problems like those of (1) and (2) cast 
into relief the potential difficulty of identifying sets. 
The world provides sets with embarrassing abun- 
dance, yet we are able to call attention to any of these 
sets at will, and to describe it at will both by prop- 
erties its members .have on  their .own---,the mem- 
bers individually may be square or dotted--and by 
properties or relationships that the set enjoys as a 
collection--the set as a whole may be intersecting, 
or clustered in a certain region? 
Reference to sets is more general than picturesque 
examples like (1) and (2) might suggest? Linguistic 
research suggests that covert reference to SETS OF 
SALIENT ALTERNATIVES plays a pervasive and fun- 
damental role in mediating between the meanings of 
sentences and their interpretations in context. Con- 
sider (3), for'example. 
(3) a 
b 
C 
Only \[Mary\] F passed. 
Well, \[I\]F passed. 
Another student passed. 
According to (Rooth, 1992), the inte!rpretation of fo- 
cusingadverbs such as ~anly~relates,an.'instance ~-to 
a set of alternatives C: the adverb describes a prop- 
erty that makes ~ unique in C. Thus in (3a) Mary is 
unique among some set C of individuals in passing. 
Likewise, scalar implicature, as illustrated in (3b), 
depends on distinguishing one claim--my passing, 
say--as the strongest claim that can be supported 
among some salient set of claims C--students in the 
class passing, for (3b). Rooth formalizes the focus 
marking in these xamples as contributing a presup- 
position that helps to identify these alternative sets. 
More generally, a range of lexical items, including 
the morpheme other from (3c), carry discourse pre- 
suppositions that relate their referent o salient al- 
ternatives from the context--like the students we 
accommodate in understanding (3c) (Bierner and 
Webber, 2000). 
Overtly, all the examples in (3) involve singu- 
lar noun phrases that specify isolated individuals. 
Nevertheless, representing and reasoning about ref- 
erence to sets is required for faithful account of how 
such sentences are interpreted, and thus how such 
sentences can achieve the communicative goals of a 
system for natural anguage generation (NLG). 
So how are expressions that refer to sets to be con- 
structed? In this paper, I will argue that identifying 
sets of individuals is not as forbidding as it may at 
first appear. The extensive literature in NLG on sin- 
gular references starting with (Dale and Haddock, 
1991) tells us what to do. We must use the INTER- 
PRETATION o f  provisional descriptions in context 
to assemble a combination of descriptive lements 
which identifies the intended target. Take (2), where 
we used the descriptive lements quare, clustered 
somewhere, and at thel lower :left:. Tracking the in- 
cremental interpretation of these descriptors should 
lead to a sequence like that in (4). 
\[\] \[\] 
O0 
(4) a 
b The squares. 
\[\] 
\[\] \[\] \[\] 
\[\] \[\] \[\] \[\] 
117 
C 
d 
\[\] 
O0  
\[\] 
\[\] \[\] \[\] 
O 0 \[\] 
? \[\] 
? k ? 
The squares clustered somewhere. 
\[\] 
\[\] \[\] 
e \[\] \[\] 
f The squares clustered at the lower left. 
This high-level story leaves us on familiar ground. 
The project of this paper is to realize this high- 
level story in formal terms. I begin in Section 2 by 
framing the problem of singular noun phrase gener- 
ation more precisely. The sequel extends this frame- 
work with a formal account of plural interpreta- 
tion and generation. Section 3 introduces the two 
independently-motivated observations from formal 
semantics which form the basis of this account. 
The ASSIGNMENT-SET semantics for reference 
to plurals provides away to evoke and describe 
collections with variables that range only over 
individuals (van den Berg, 1993; van den Berg, 
1996). By using the assignment-set semantics, 
we can dispense with explicit collections in for- 
malizing an interpretation such as that schema- 
tized in (4c); we represent only the individuals 
involved. 
The COVER semantics for predications about 
pluralities provides a simple scheme of im- 
plicit quantification to abstract collective and 
distributive predication (Gill?n, 1987; Verkuyl 
and van der Does, 1991; Schwarzschild, 1994; 
Schwarzschild;. 1996). :T.he cover semantics of- 
fers an elegant, and convenient, definition of 
what it means for the set distinguished in(4c) to 
be characterized asthe squares clustered some- 
where. 
Section 4 presents the computational model of plu- 
ral descriptions based on these principles. In keep- 
ing with (4), this model simply and naturally extends 
," the models used to generate-singular references. In 
particular, as (4) suggests, this model continues to 
structure the search space for generation in terms of 
the words that can be added to the description and to 
arrive at corresponding interpretations by constraint 
satisfaction over individuals. In so doing, the pro- 
posal defuses the potential combinatorial explosions 
that might otherwise arise with reference to sets. 
? , .  . :  
2 Background 
At a high level, we can characterize generation pro- 
cedures like that of (Dale and Haddock, 1991 ) or its 
successors a  manipulating linguistic data structures 
that link together FORMS, MEANINGS and INTER- 
PRETATIONS. (5) illustrates uch a data structure, 
as it might be entertained in identifying a uniquely 
identifiable element of (2a). 
(5) a F: /the square in the upper left/ 
b M: {square(x), in(x, r), upper-left(r) } 
g3x 
. . . . . . . . .  r 
c I: 
(5a) proposes the form the square in the upper left-- 
a syntactic structure represented to some degree of 
abstraction. (5b) records the semantics for the de- 
scription as a set of constraints--each constraint is
an atomic formula with free variables that speci- 
fies the requirement that some lexical meaning con- 
tributes to the description; the variables are place- 
holders for the discourse ntities that the descrip- 
tion identifies. And (5c) anticipates how the hearer 
could process the description, by outlining the pos- 
sible candidate referents for it; in (5c) we find the 
element of the figure which x must represent, along 
with the corresponding (vaguely delimited) region r
in the upper left where x is located. 
Any data structure linking form, meaning and 
interpretation combines two kinds of information. 
Form and meaning are related by_the generator's 
model of linguistic resources. In the concrete case, 
this model is a grammar; LEXICAL semantics de- 
termines the separate constraints hat can go into a 
description and COMPOSITIONAL semantics deter- 
mines how these constraints can share variables and 
so describe common objects. Meaning and interpre- 
tation, meanwhile, are related by a model of the con- 
text in which the form is to be uttered. In interpreting 
referring expressions,we appeal "to a CONTEXT SET 
enumerating the salient individuals at some point in 
the discourse and a COMMON GROUND listing the 
118 
instances of constraints hat can be presumed to be 
mutually known at that point. To determine the in- 
terpretation from the meaning, we must instantiate 
the free variables to individuals in the context set 
and match the instantiated constraints against the el- 
ements:~of the1 common.ground., In, practice, con- 
straint satisfaction heuristics (Mackworth, 1987) are 
required to accomplish the process of instantiation 
and matching with any hope of efficiency. 
With an understanding of what data structures 
such as (5) represent and how to carry out reasoning 
over them, solving descriptive problems becomes a
matter of search. In practice, this search is typi- 
cally managed quite simply: for example, (Dale and 
Haddock, 1991) select ransitions among states ac- 
cording to a greedy heuristic, while (Dale and Re- 
iter, 1995) select alternatives by exploring differ- 
ent kinds of constraints in a fixed order. In any 
case, the search starts with a structure defining an 
empty description, which means nothing and could 
refer to anything. Structures are then extended and 
considered in turn until the interpretation satisfies 
the system's goals (for example because it allows 
only a specified value, the intended referent, for a 
particular variable). The process of extension sim- 
ply consists of deriving a more elaborate form with 
a richer meaning using the generator's linguistic 
resources--it is useful to think of obtaining this by 
carrying out a step of derivation in a lexicalized 
grammar (Stone and Doran, 1997)--and then con- 
sulting the model of the context to obtain an updated 
interpretation. 
To extend these data structures to sets, we cannot 
introduce set variables and maintain the alternative 
candidate set values those variables might ultimately 
refer to--for one thing, there are just too many sets 
to represent an interpretation this way. 
3 An intuit ion and some semantics 
Here. is a suggestion: REINTERPRET_data.structures 
like (5) as compatible with descriptions of collec- 
tions as well as singletons. This should have some 
intuitive appeal. After all, we always thought hat 
a form like (5a) abstracted out details of syntax and 
morphology; there's no difficulty then in seeing it 
as short for a family of singular and plural expres- 
sions like/the square(s) in the upper left~. Similarly, 
the interpretation is already defined in terms of a set 
ofinstances that satisfy the description; why not use 
this as THE set that the description refers to? 
The problem is the meaning. We have to allow 
for both DISTRIBUTIVE predicates, which character- team, so both the collective and distributive readings 
ize collections based on properties of the individuals are false. 
involved, and COLLECTIVE predicates, which de- We will follow Schwarzschild's proposal most 
scribe collections that jointly participate in some re- closely. Schwarzschild argues that we establish that 
lation. If we have collective predicates, how can we a linguistic predicate applies to a plural argument 
get away without explicit set variables which cou ld . ,  ~ by reeoyeringa.salient~cover of:~that:~gumentf~r0m ~,- 
take on any set as a possible value? the context. A cover here means a set of plurali- 
Van den Berg's treatmen t of dependent plurals in 
dynamic semantics provides the first half of the an- 
swer(van den Berg, 1993; van den Berg, 1996). Van 
den Berg's starting observation is that discourse can 
set up and maintain dependencies between the indi- 
viduals in one set and the individuals in another. 
(6) a Every man loves a woman. 
b They prove this by giving them flowers. 
In (6) for example, the first sentence introduces a set 
of men and a set of women, where each man in the 
one set is related to a woman in the other set (by 
love); the second sentence builds on that relation- 
ship, indicating another connection (of giving) be- 
tween each man and the corresponding woman. 
For van den Berg, data like (6) show that dis- 
courses describe sets of CASES generally. Each case 
involves a sequence of entities that stand in vari- 
ous relationships toone another, sometimes directly 
as individuals and sometimes indirectly through 
their membership in larger, related groups. Some 
sentences in discourse aggregate cases together, to 
express relationships that hold collectively among 
groups. Other sentences, like (6), zoom in on in- 
dividual cases, and describe distributive properties 
which hold of isolated individuals. In zooming in on 
cases, rather than individuals, these sentences main- 
tain and extend the dependencies and other relation- 
ships that define a case. 
The second half of the answer derives from 
the observation, made in (Gillon, 1987; Verkuyl 
and van der Does, 1991; Schwarzschild, 1994; 
Schwarzschi ld, 1996),,-that :the collective and dis- 
tributive readings of plurals represent only the ex- 
tremes in a larger space of readings. Take (7): 
(7) Rogers, Hammerstein and Hart wrote 
musicals. 
This sentence is true, but only in virtue of the joint 
action of Rogers and Hammerstein writing some 
musicals and thezioint--ac~fion..of.Rogers andHart in , . 
writing other musicals. As a matter of fact, the three 
never wrote a musical individually or as a single 
ties whose union or sum is the overall plural argu- 
ment. Given the cover, the overall plural predication 
holds just in case the basic property denoted by the 
predicate is true (collectively)of each of the sets (or 
CELLS) in the cover. For example, the sets consist- 
ing of Rogers and Hammerstein and of Rogers and 
Hart form the salient cover of Roger, Hammerstein 
and Hart in (7); the example is true because ach of 
the cells in this cover directly enjoys the property of 
having written a musical. 
Schwarzschild's covering proposal and van den 
Berg's assignment-set proposal are perfectly com- 
patible. Following van den Berg, we interpret dis- 
courses in terms of sets of cases, where these cases 
spell out dependencies among related individuals. 
But now, following Schwarzschild, we zoom in on 
those cases flexibly, by covering them. Sometimes 
we consider all the cases together and describe rela- 
tionships among aggregated groups; sometimes we 
consider cases separately and describe individuals 
distributively; and sometimes, as in (7), we take an 
intermediate step and cluster the cases into some 
salient subgroups. 
Now let us return to (4c), repeated as (8a), and 
consider informally what this proposal amounts to: 
\[\] 
~ \ [ \ ]  
\[\] 
\[\] \[\] \[\] 
(8) a \[\] \[\] \[\] \[\] 
b The squares clustered.somewhere. 
The assignment-set cover semantics fits the descrip- 
tion to the figure this way. As in (5c), the figure 
schematizes a set of cases; here each case involves 
two entities, a square and the location of the cluster 
to which the square belongs. The description applies 
because we can look at the individual cases to see 
that we have squares, and we can group the cases to- 
gether by. regioninto a cover so,that ineach cell the 
squares are indeed clustered at the location. 
At this point, some formalism is required to pro- 
119 
ceed with the development. We'll use assignment 
variables like g to range over cases; gx is the value 
of g for variable x.t Interpretations are defined in 
terms of sets of cases, naturally; we'll use F to range 
over a set of cases and write F(x) for {gx:g E F}. 
Most constraints will involve several variables; we 
can abstract this in terms of a sequence Of {,~iable~'x ....
and the tuple of collections that those variables take 
on across a set of cases, F(x). (We can define this 
explicitly as F(x) = G where Gxi = F(xi).) 
Now, consider an atomic constraint F(x). In gen- 
eral, F(x) will have multiple known instances, and 
each instance will relate collections of individuals 
to one another. Thus the common ground will asso- 
ciate F(x) with a set of tuples of sets, which we write 
as ~F(x)~. An interpretation F will fit one of those 
instances directly iff F(x) E ~F(x)~. In this case we 
say F(x) DESCRmES F. 
For example, consider the constraint 
clustered(x,r). Let us say a set X is clustered 
around R if R is a singleton spatial ocation {r} and 
X is a group of sufficient cardinality and density 
located together at r. Then we might find three 
tuples of \[\[clustered(x, r)\]\] in the explicit depiction 
of (8). If we define Fl as in (9) then clustered(x,r) 
describes Fi. 
(9) { (x, r) : x a square in the lower left region r} 
Of course, we are principally interested in the 
ability to zoom in to particular cases, using covers. 
We represent a cover using a reflexive binary rela- 
tion that links each assignment to any assignment 
in its cell. Given such a relation C, the constraint 
@cP- - read  "covered by C, p"--says that p is true 
on each of the cells of the cover specified by C. We 
will only consider the case where p is an atomic on- 
straint F(x). 
(10) @cF(x )  DESCRIBES F ifffor all g E F, 
Then with C defined as in (12), @cclustered(x, r) 
describes Fi U F2 U F3. 
(12) C=(F ,  xr,)u(r2xr2)u(r3xr3). 
Observe the close connection between this formal 
.jiidgment'and 'the-info,mat-disenssion' of"(8) pre- 
sented earlier. We have a set of cases involving a 
square and the location of the cluster to which the 
square belongs; we cover the cases together by re- 
gion and find that the resulting roups define a spec- 
ified cluster at a specified location. 
Schwarzschild's proposal is that the salient cover 
C is supplied from context. In the case of definite 
reference to tuples F, we can regard the tuples in any 
predicate as defining the appropriate salient cover 
for plural predication; any tuples that help to iden- 
tify F must be prominent parts of the shared context. 
Meanings of referring expressions should therefore 
appeal to a condition @ p which describes F iff there 
is a C for which @cP describes F. 
Clearly, if @p describes F and @p describes F' 
then @p describes FU I '~. This in turn entails that 
any condition ?p describes a maximal set of cases 
from the current context; the same goes for any con- 
junction of conditions of this form. We can treat 
this set of cases as the interpretation f a description. 
In particular, consider a description L that consists 
of a list of constraints @Li(x) formulated in terms 
of a tuple of variables x and atomic conditions on 
those variables Li(x). Assume a context set D defin- 
ing a domain of salient individuals, so that candidate 
cases to interpret L are given by I" := {a : ai E D}. 
The development thus far leads us to define the IN- 
TERPRETATION of I_,---l(L)--as: 
(13) I(L) := maxrcrVi :  @Li(x) describes F
Drawing on our running discussion, we 
can apply this definition to the description 
F(x) describes {h E F :  C(g,h)}. 
Continuing from (9), define F2 and F3 in (1 ta) and 
(11 b) respectively. 
( l l )a  {(x.r) : x a square in the center top r} 
b {(x,r) :x a square in the lower right r} 
I I adopt the notation throughout that v is a tuple and v i is 
component i of  v, where components may be indexed equiva- 
. . . . . . . . . . . . . . . .  L . ~_ . .{@square(x) ,@clustered(x,r)  } and the 
context schematized by (2a). Of course, we find 
I (L)  = Fi U F2 U F3. The fully distributive cover 
shows that the square condition is satisfied; the 
cover of (12) shows that the clustered condition 
is satisfied. Meanwhile, no further cases can be 
considered without adding either a circle or the 
unclustered square. 
The reader will already have recognized I(L) = 
lently by variables or numbers..Lower case Roman letters are 
for ordinary individuals and tuples thereof; upl~er case Ronl.an FI U:F2I'3 F 3 as~the set o f  cases:that-goes'with (8a). 
letters are for sets of individuals and tuples thereof; upper case Thus, we have reconciled the informal picture of (4) 
Greek letters are for sets of tuples, with the concrete data structures of form, meaning 
120 
and interpretation that NLG demands. For (8) we 
can now read (14). 
(14)a F: /the squares(s) clustered __ / 
b M: {(~) square(x), (~) cluster(x, r)} 
\] . . . . .  
c I: 
\[\] 
\[\] \[\] 
\[\] \[\] 
\ [ \ ]  \[\]" 
\[\] 
\[\] \[\] 
4 Computing referring expressions 
At this point, we have an understanding of what 
kinds of representations wecan use to describe the 
derivation of plural referring expressions. But we 
still must devise appropriate r asoning methods for 
these representations. The problem is the subject of 
this section. 
4.1 Collective Constraints 
The first step is to formulate a constraint-satisfaction 
heuristic that accounts for cover-constraints on col- 
lections. In general, constraint-satisfaction heuris- 
tics provide a technique for approximating the inter- 
pretation of a description. The key notion is that of a 
CONSTRAINT NETWORK for a description L, which 
determines a tuple C of CONSTRAINED VALUES. 
This tuple specifies a generous et of possible val- 
ues Ci for each variable xi in x; it is obtained by con- 
servatively eliminating values that are determined to
be inconsistent with L according to heuristic tests. 
For example, the usual arc-consistency heuristic for 
a constraint over individuals K(x) is to eliminate a
value v for variable z unless some g E ~K(x)~ has 
g: -- v and gk E Ck for all k. 
We will adapt his to the case of cover constraints 
with the following test of consistency. An individ- 
ual value v for a variable xi maintains its member- 
ship in C i in the.presence~oLa,collective,constraint 
?Lj(x) whenever v belongs to a SUBSET Gi of Ci 
which participates directly in the relation denoted by 
Lj(x) with sets of possible values for the other vari- 
ables. This criterion is spelled out formally in the 
definition in (15). 
(15) Value v for variable xi is 
COVER-CONSISTENT (C-CONSISTENT) 
with constraint (~)Lj(x)~under, constrained 
values C if there is an G E ~Lj(x)\]\] with 
v E Gi and Gt. C Ck for all k. 
121 
All values of xi 
with constraint QLj(x)  may be deleted from Ci, 
as they will not satisfy the constraint. Doing so 
makes Ci ARC c-consistent with respect to (~)Lj(x), 
and provides the basic step in a network-based arc- 
.consistency .constraint-satisfier. As with. ordinary 
constraint satisfaction, we arrive at a final tuple of 
values for x by starting with an initial tuple Co of 
values---often an assignment Dx giving each vari- 
able D--and a queue of arcs linking each Ci with any 
QLj (x)  that constrains it. Until the queue is empty, 
we select an arc and enforce the arc c-consistency 
by pruning Ci; if Ci changes we requeue all arcs 
that might no longer be arc c-consistent after the 
deletion. I will refer to the final tuple of values as 
P(L; Co), for the PLURAL constraint etwork on de- 
scription L and domains Co. 
The properties of this algorithm are in line with 
ordinary constraint satisfaction. The output will 
not provide all and only solutions to the constraints 
without further assumptions about the constraints. 
However, we can show, as usual, that the network 
converges on consistent values for variables in the 
ordinary linguistic ase where the constraint graph 
is a tree--a semantic property, that there are no dis- 
joint sets of constraints that overlap on the same 
two variables, that follows under plausible assump- 
tions about he derivation of semantics from a tree- 
structured syntax. We can show further that these 
values, together with the tuples in \[\[Li(x)~ that cover 
these values, determine precisely the collection of 
assignments I(L). 
4.2 Search for Referring Expressions 
The second step is to formalize the task of construct- 
ing a description as a state-space s arch task. Sup- 
pressing details of form for exposition, each state is 
a tuple E as set out in (16). 
(16) E---- (L,r ,R,x,P(L;R,  Dx),P(L;Dr,Dx)) 
The state represents: 
(17)a a description L; 
b a tuple r of distinguished free variables in 
the description for which we must identify 
specific intended values; 
c a tuple R of sets describing the value Ri 
which we intend for the corresponding 
-var iable r i ;  . . . .  . ? - 
d the remaining free variables of the 
description x;
in Cithat are-not c-consistent_ 
e a constraint network P(L; R, Dx) describing 
the values for all the free variables in the 
description, on the assumption that the 
distinguished variables take on the values we 
intend; and 
f a constraint network p(L; Dr, Dx) 
describing the values for all the free 
variables in the description, on the 
assumption that the distinguished variables 
may, like other variables, take on any values 
from the context set. 
The distinction between the variables whose in- 
tended reference is fixed and those for which it is de- 
rived as a byproduct of the search process is due to 
Horacek (Horacek, 1995; Horacek, 1996); the dis- 
tinction derives increased importance when relating 
one collection to another as the choice of collections 
need not give rise to explicit branching in search. 
The initial state involves an empty description and 
so has the form given in (18). 
(18) Y~= (?,r,R, 0 ,P (Q;R) ,P (Q;Dr) )  
A state such as (16) represents a final state that suc- 
cessfully resolves the generation task when each 
variable x from r and x is associated with the same 
set Cx in both P(L;R,Dx) and P(L;Dr,Dx).  This 
simply means that the hearer's interpretation of the 
referring expression matches the speaker's intended 
interpretation. 
At any state Z, the grammar defines a set of con- 
straints of the form (~) L(rx; y) that could potentially 
be added to the description to obtain L~--L is some 
domain relation, r and x name the old variables 
from L while y names fresh variables. Of course, 
we want to restrict our attention to constraints that 
are compatible with our intended interpretation. To 
achieve this restriction, we begin by computing the 
new constraint network C ~ = P(L~;R;Dxy). We 
check, whenever R assigns avalue to x, that Rx C C' x. 
If this test admits .the_new constraint, he newstate 
obtained from state E is computed as in (19). 
(19) (L', r, R, xy, P(L'; R, Dxy), P(L'; Dr, Dxy)} 
4.3 An Example 
I return to (1) to provide an illustration of the final 
scheme; the goal is to identify the segments in (20f), 
R, from among those in (20a). I use figures and ref- 
erences to figures, .in .place-of=.eonstraint, networks; 
the description uses the variable r. The states pro- 
ceed, perhaps, thus: 
(20)a 
b 
C 
? ? ? ? ? ? ? 
(o, r,g, l), (20a), (20f)) 
({ Qsegment(r) . ;r,R, 0, (20/1), (20ff)) 
" ? , . 
* ~ % ' .  
? ? ? ?  ? 
? ? ? ? 
d 
e ({Qsegment(r), Qintersecting(r)}, 
r,R, (), (20d), (20f)) 
? , . . . 
? ? ? 
g ({Qsegment(r),Qintersecting(r), 
Qdotted(r) }, r, R, {), (20f), (20f) } 
5 Closing thoughts 
Descriptions of sets obviously have much in com- 
mon with expressions that describe a single entity 
from the shared context? In particular, adopting the 
standard view of NLG as goal-directed activity (Ap- 
pelt, 1985; Dale, 1992; Moore, 1994; Moore and 
Paris, 1993), singular and plural descriptions agree 
both in the kinds of intentions that they can achieve 
and the stages of generation at which they can be for- 
mulated. We cannot expect asingle process to be re- 
sponsible for set descriptions across all intentions or 
stages of NLG. 
For example, as with a singular description, a
description of a set may appeal to properties that 
play a role in the argument the speaker is trying to 
make, and may therefore address goals above and 
beyond simple identification of discourse ntities. 
(Se e .(Donellan, ..! 966;: Kx~0nfeld, 1986) on the dis- .- 
tinction.) (Green et al, 1998a; Green et al, 1998b) 
show how such descriptions may be represented and 
formulated in NLG at a high-level process of con- 
tent or rhetorical planning. At the same time, plu- 
rals and singulars are alike in offering resources for 
reference--such aspronouns, one-anaphora or ag- 
gregated expressions--that bypass explicit descrip- 
tion altogether? The use of these resources may be 
.... ~quite-closety dependent onthe  surface 'form being 
generated and so could reflect a relatively late deci- 
sion in the generation process (Dale and Haddock, 
122 
1991; Reiter, 1994; Dalianis, 1996). 
These complexities notwithstanding, we can ex- 
pect many descriptions of sets, like descriptions 
of individuals, to be formulated from scratch to 
achieve purely referential goals during the SEN- 
TENCE PLANNING. plaase: of  .NLG, io:.he:tween ~gon=. 
tent planning and surface realization (Rainbow and 
Korelsky, 1992; Reiter, 1994). I have shown that 
using covers to abstract collective and distributive 
readings--and using sets of assignments o repre- 
sent plural references--yields a search space for 
this problem which largely mirrors that for singu- 
lars, and which avoids computation and search over 
sets of collections. Although sets proliferate xplo- 
sively, it is no surprise that the search space for plu- 
rals set up by (19) is, like that for singulars, ulti- 
mately defined by the sequences of elements that 
make up descriptions. NLG involves search to use 
words effectively--choices of words should be the 
only decisions areferring expression generation sys- 
tem has to make. 
Acknowledgments 
This paper benefits from the comments of anonymous 
referees and from discussions with Kees van Deemter, 
Roger Schwarzschild, Bonnie Webber, the Edinburgh 
generation group, and the participants of the GNOME 
workshop where a preliminary version was presented; it
was supported by a postdoctoral fellowship from RuCCS. 
References  
Douglas Appelt. 1985. Planning English Sentences. 
Cambridge University Press, Cambridge England. 
Gann Bierner and Bonnie Webber. 2000. Inference 
through alternative-set semantics. Journal of Lan- 
guage and Computation. 
Robert Dale and Nicholas Haddock. 1991. Content de- 
termination i the generation ofreferring expressions. 
Computational Intelligence, 7(4): 252-265. 
Robert Dale and Ehud Reiter. 1995. Computational in- 
terpretations of the Gricean maxims in the generation 
of referring expressions. Cognitive Science, 18:233- 
263. 
Robert Dale. 1992. Generating Referring E~pressions: 
Constructing Descriptions in a Domain of Objects and 
Processes. MIT Press, Cambridge MA. 
Hercules Dalianis. 1996. Concise Natural Language 
Generation .from Formal Specifications. Ph.D. thesis, 
Royal Institute of Technology. Stockholm. 
K. Donellan. 1966. Reference and definite description. 
Philosophical Review, 75:281-304. 
Brendan-Gillon. 1987. The readings of plural noun 
phrases in english. Linguistics and Philosophy, 
10(2): 199-299. 
123 
Nancy Green, Giuseppe Carenini, Stephan Kerpedjiev, 
Steven Roth, and Johanna Moore. 1998a. A media- 
independent content language for integrated text and 
graphics generation. In CVIR '98- Workshop on Con- 
tent Visualization and lntermedia Representations. 
Nancy Green, Giuseppe Carenini, and Johanna Moore. 
...... ~ v~ t998b~ k,p~ineipted~representafiort . o  at ributive de- 
scriptions for generating integrated text and informa- 
tion graphics presentations. In Proceedings of In- 
ternational Natural Language Generation Workshop, 
pages 18-27. 
Helmut Horacek. 1995. More on generating referring ex- 
pressions. In Proceedings of the Fifth European Work- 
shop on Natural Language Generation, pages 43-58, 
Leiden. 
Helmut Horacek. 1996. A new algorithm for generating 
referring expressions. In ECAI 8, pages 577-581. 
Amichai Kronfeld. 1986. Donellan's distinction and a 
computational model of reference. In Proceedings of 
ACL, pages 186.--191. 
Alan Mackworth. 1987. Constraint Satisfaction. In 
S.C. Shapiro, editor, Encyclopedia of Artificial Intel- 
ligence, pages 205-211. John Wiley and Sons. 
Johanna D. Moore and C6cile L. Paris. 1993. Plan- 
ning text for advisory dialogues: capturing intentional 
and rhetorical information. Computational Linguis- 
tics, 19(4):651-695. 
Johanna Moore. 1994. Participating in Explanatory Di- 
alogues. MIT Press, Cambridge MA. 
Owen Rainbow and Tanya Korelsky. 1992. Applied text 
generation. InANLP, pages 40-47. 
Ehud Reiter. 1994. Has a consensus NL generation 
architecture appeared, and is it psycholinguistically 
plausible? In Seventh International Workshop on Nat- 
ural Language Generation, pages 163-170, June. 
Mats Rooth. 1992. A theory of focus interpretation. Nat- 
ural Language Semantics, 1( 1 ): 75- I 16. 
Roger Schwarzschild. 1994. Plurals, presuppositions, 
and the sources of distributivity. Natural Language 
Semantics. 2:201-248. 
Roger Schwarzschild. 1996. Pluralities. Kluwer, Dor- 
drecht. 
Matthew Stone and Christine Doran. 1997. Sentence 
planning as description using tree-adjoining grammar. 
:: In ProceedingsofACL, pages_198-205. 
M. H. van den Berg. 1993. Full dynamic plural logic. 
In K. Bimb6 and A. Mfit6, editors, Proceedings of the 
Fourth Symposium on Logic and Language, Budapest. 
M. H. van den Berg. 1996. Generalized dynamic quanti- 
tiers. In J. van der Does and J. van Eijk, editors, Quan- 
tifiers, Logic and Language. CSLI. 
Henk Verkuyl and Jaap van der Does. 1991. The seman- 
tics of plural noun phrases. Preprint, ITLI. Amster- 
dam. 
Coordination and context-dependence in the generation of embodied 
conversation 
Justine Cassell* 
*Media Laboratory 
MIT 
E15-315 
20 Ames, Cambridge MA 
{justine, yanhao}@media.mit, edu 
Matthew Stone t Hao Yan* 
tDepartment of Computer Science & 
Center for Cognitive Science 
Rutgers University 
110 Frelinghuysen, Piscataway NJ 08854-8019 
mdstone@cs, rutgers, edu 
Abstract 
We describe the generation of communicative ac- 
tions in an implemented embodied conversational 
agent. Our agent plans each utterance so that mul- 
tiple communicative goals may be realized oppor- 
tunistically by a composite action including not only 
speech but also coverbal gesture that fits the con- 
text and the ongoing speech in ways representative 
of natural human conversation. We accomplish this 
by reasoning from a grammar which describes ges- 
ture declaratively in terms of its discourse function, 
semantics and synchrony with speech. 
1 Introduction 
When we are face-to-face with another human, no 
matter what our language, cultural background, or 
age, we virtually all use our faces and hands as an in- 
tegral part of our dialogue with others. Research on 
embod ied  conversat iona l  agents  aims to imbue in- 
teractive dialogue systems with the same nonverbal 
skills and behaviors (Cassell, 2000a). 
There is good reason to think that nonverbal be- 
havior will play an important role in evoking from 
users the kinds of communicative dialogue behav- 
iors they use with other humans, and thus allow 
them to use the computer with the same kind of ef- 
ficiency and smoothness that characterizes their di- 
alogues with other people. For example, (Cassell 
and Th6risson, 1999) show that humans are more 
likely to consider computers lifelike, and to rate their 
language skills more highly, when those computers 
display not only speech but appropriate nonverbal 
communicative behavior. This argument akes on 
particular importance given that users repeat hem- 
selves needlessly, mistake when it is their turn to 
speak, and so forth when interacting with voice di- 
alogue systems (Oviatt, 1995): tn -life; noisy situa- 
tions like these provoke the non-verbal modalities to 
come into play (Rogers, 1978). 
In this paper, we describe the generation of com- 
municative actions in an implemented embodied 
conversational gent. Our generation framework 
adopts a goal-directed view of generation and casts 
knowledge about communicative action in the form 
of a grammar that specifies how forms combine, 
what interpretive ffects they impart and in what 
contexts they are appropriate (Appelt, 1985; Moore, 
1994; Dale, 1992; Stone and Doran, 1997). We ex- 
pand this framework to take into account findings, 
by ourselves and others, on the relationship between 
spontaneous coverbal hand gestures and speech. In 
particular, our agent plans each utterance so that 
multiple communicative goals may be realized op- 
portunistically by a composite action including not 
only speech but also coverbal gesture. By describing 
gesture declaratively in terms of its discourse func- 
tion, semantics and synchrony with speech, we en- 
sure that coverbal gesture fits the context and the on- 
going speech in ways representative of natural hu- 
man conversation. The result is a streamlined imple- 
mentation that instantiates important theoretical in- 
sights into the relationship between speech and ges- 
ture in human-human conversation. 
2 Explor ing the relat ionship between 
-speech and gesture 
To generate mbodied communicative action re- 
quires an architecture for embodied conversation; 
ours is provided by the agent REA ("Real Estate 
Agent"), a computer-generated humanoid that has 
an articulated graphical body, can sense the user 
passively through cameras and audio input, and 
supports communicative actions realized in speech 
with intonation, facial display, and animated ges- 
ture. REA currently offers the reasoning and dis- 
play capabilities to act as a real estate agent showing 
..... ~users'the--features"o~ vm i-o-wsmodels"of howsesthat ~ 
appear on-screen behind her. We use existing fea- 
tures of kEA here as a resem'ch platform for imple- 
171 
menting models of the relationship between speech 
and spontaneous hand gestures during conversation. 
For more details about the functionality of REA see 
(Cassell, 2000a). 
Evidence from many sources uggests that this re- 
.lationship is aclose one..About three,quarters of al! 
clauses in narrative discourse are accompanied by 
gestures of one kind or another (McNeill, 1992), and 
within those clauses, the most effortful part of ges- 
tures tends to co-occur with or just before the phono- 
logically most prominent syllable of the accompany- 
ing speech (Kendon, 1974). 
Of course, communication is still possible with- 
out gesture. But it has been shown that when speech 
is ambiguous (Thompson and Massaro, 1986) or in 
a speech situation with some noise (Rogers, 1978), 
listeners do rely on gestural cues (and, the higher the 
noise-to-signal ratio, the more facilitation by ges- 
ture). Similarly, Cassell et al (1999) established that 
listeners rely on information conveyed only in ges- 
ture as they try to comprehend a story. 
Most interesting in terms of building interactive 
dialogue systems i the semantic and pragmatic rela- 
tionship between gesture and speech. The two chan- 
nels do not always manifest he same information, 
but what they convey is virtually always compati- 
ble. Semantically, speech and gesture give a con- 
sistent view of an overall situation. For example, 
gesture may depict he way in which an action was 
carried out when this aspect of meaning is not de- 
picted in speech. Pragmatically, speech and ges- 
ture mark information about his meaning as advanc- 
ing the purposes of the conversation i a consistent 
way. Indeed, gesture often emphasizes information 
that is also focused pragmatically by mechanisms 
like prosody in speech (Cassell, 2000b). The seman- 
tic and pragmatic ompatibility seen in the gesture- 
speech relationship recalls the interaction of words 
and graphics in multimodal presentations (Feiner 
and McKeown, 1991; Green et al, 1998; Wahlster 
et al, 1991 ). In fact, some suggest (McNeill, 1992), 
that gesture and speech arise together f om an under- 
lying representation that has both visual and linguis- 
tic aspects, and so the relationship between gesture 
and speech is essential to the production of meaning 
and to its comprehension. 
This theoretical perspective on speech and gesture 
involves two key claims with computational import: 
that gesture and speech ref lectacommon concep- 
tual source; and that the content and form of a ges- 
ture is tuned to the communicative context and the 
172 
actor's communicative intentions. We believe that 
these characteristics of the use of gesture are uni- 
versal, and see the key contribution of this work as 
providing ageneral framework for building dialogue 
systems in accord with them. However, a concrete 
!mplementationrequires " more thanJustgeneralities 
behind its operation; we also need an understanding 
of the precise ways gesture and speech are used to- 
gether in a particular task and setting. 
To this end, we collected a sample of real-estate 
descriptions in line with what REA might be asked 
to provide. To elicit each description, we asked one 
subject o study a video and floor plan of a partic- 
ular house, and then to describe the house to a sec- 
ond subject (who did not know the house and had not 
seen the video). During the conversation, the video 
and floor plan were not available to either subject; 
the listener was free to interrupt and ask questions. 
The collected conversations were transcribed, 
yielding 328 utterances and 134 referential gestures, 
and coded to describe the general communicative 
goals of the speaker and the kinds of semantic fea- 
tures realized in speech and gesture. 
Analysis of the data revealed that for roughly 
50% of the gesture-accompanied utterances, gestu- 
ral content was redundant with speech; for the other 
50% gesture contributed content hat was different, 
but complementary, to that contributed by speech. 
In addition, the relationship between content of ges- 
ture, content of speech and general communicative 
functions in house descriptions could be captured by 
a small number or rules; these rules are informed by 
and accord with our two key claims about speech 
and gesture. For example, one rule describes di- 
alogue contributions whose general function was 
what we call presentation, to advance the descrip- 
tion of the house by introducing a single new ob- 
ject.. These contributions tended to be made up of 
a sentence that asserted the existence of an object 
of some type, accompanied by a non-redundant ges- 
? ture that elaborated theshape or location of  the ob- 
ject. Our approach casts this extended escription of 
a new entity, mediated by two compatible modali- 
ties, as the speaker's expression of one overall func- 
tion of presentation. 
( I ) is a representative example. 
(1) It has \[a nice garden\]. (right hand, held flat, 
traces a circle, indicating location of the 
garden sunounding the house) 
Six rules account for 60% of the gestures in the 
Figure 1" Interacting with REA 
transcriptions (recall) and apply with an accuracy of 
96% (precision). These patterns provide a concrete 
specification for the main communicative strategies 
and communicative r sources required for REA. m 
full discussion of the experimental methods and 
analysis, and the resulting rules, can be found in 
(Yan, 2000). 
3 F raming  the generat ion problem 
In REA, requests for the generation of speech and 
gesture are formulated within the dialogue manage- 
ment module. REA'S utterances reflect a coordina- 
tion of multiple kinds of processing in the dialogue 
manager- the system recognizes that it has the floor, 
derives the appropriate communicative context for 
a response and an appropriate set of communicative 
goals, triggers the generation process, and realizes 
the resulting speech and gesture. The dialogue man- 
ager is only one component in a multithreaded ar- 
chitecture that carries out hardwired reactions to in- 
put as well as deliberative processing. The diver- 
sity is required in order to exhibit appropriate inter- 
actional and propositional conversational behaviors 
at a range of time scales, from tracking the user's 
movements with gaze and providing nods and other 
feedback as the user speaks, to participating in rou- 
tine exchanges and generating principled responses 
to user's queries. See (Cassell, 2000a) for descrip- 
tion and motivation of the architecture, aswell as the 
conversational functions and behaviors it supports. 
REA'S design and capabilities reflect our research 
focus on allying conversational content with conver- 
sation management, and allying nonverbal modali- 
ties with speech: how can anembodiedagent use'all 
its communicative modalities to contribute new con- 
tent when needed (propositional function), to signal 
the state of the dialogue, and to regulate the over- 
all process of conversation (interactional function)? 
Within this focus, REA's talk is firmly delimited. 
REA'S utterances take a question-answer fo mat, in 
which the user asks about (and REA describes) a
single house .at.a. time. REA'S .sentences ,ate short; 
generally, they contribute just a few new semantic 
features about particular ooms or features of the 
house (in speech and gesture), and flesh this contri- 
bution out with a handful of meaningful e ements (in 
speech and gesture) that ground the contribution i
shared context of the conversation. 
Despite the apparent simplicity, the dialogue 
manager must contribute a wealth of information 
about he domain and the conversation torepresent 
the communicative context. This detail is needed for 
REA tO achieve atheoretically-motivated realization 
of the common patterns of speech and gesture we ob- 
served in human conversation. For example, a vari- 
ety of changing features determine whether marked 
forms in speech and gesture are appropriate in the 
context. REA'S dialogue manager t acks the chang- 
ing status of such features as: 
e Attentionalprominence, r presented (as usual 
in natural language generation) by setting up a 
context set for each entity (Dale, 1992). Our 
model of prominence is a simple local one sim- 
ilar to (Strube, 1998). 
o Cognitive status, including whether an entity is 
hearer-old or hearer-new (Prince, 1992), and 
whether an entity is in-focus or not (Gundel 
et al, 1993). We can assume that houses and 
their rooms are hearer-new until REA describes 
them; and that just those entities mentioned in
the prior sentence are in-focus. 
Information structure, including the open 
propositions or, following (Steedman, 1991 ), 
themes, which describe the salient questions 
currently at issue in the discourse (Prince, 
1986). In REA'S dialogue, open questions are 
always general questions about some entity 
raised by a recent urn; although in principle 
such an open question ought o be formalized 
as theme(XP.Pe), REA can use the simpler 
theme(e). 
In fact, both speech and gesture depend on the same 
? " kinds of'feamresi;-andaccessthem in the same way; " 
this specification of the dialogue state crosscuts dis- 
tinctions of communicative modality. 
173 
Another component of context is provided by a 
domain knowledge base, consisting of facts explic- 
itly labeled with the kind of information they repre- 
sent. This defines the common ground in the con- 
versation in terms of sources of information that 
lation of goals and tightly fits the context specified 
by the dialogue manager. 
4 Generation and linguistic representation 
speaker and hearer share. Modeling the discourse as We model REA'S communicative actions as com- 
a shared source of information means that new ~e "'-':''~'lmsed~?f:a~c~rHeeti?n'?f'at?mie'etementsiqnclndiiag 
both lexical items in speech and clusters of seman- mantic features REA imparts are added to the com- 
mon ground as the dialogue proceeds. Following re- 
sults from (Kelly et al, 1999) which show that infor- 
mation from both speech and gesture is used to pro- 
vide context for ongoing talk, our common ground 
may be updated by both speech and gesture. 
The structured omain knowledge also provides 
a resource for specifying communicative strategies. 
Recall that REA'S communicative strategies are for- 
mulated in terms of functions which are common 
in naturally-occurring dialogues (such as "presenta- 
tion") and which lead to distinctive bundles of con- 
tent in gesture and speech. The knowledge base's 
kinds of information provide a mechanism for spec- 
ifying and reasoning about such functions. The 
knowledge base is structured to describe the rela- 
tionship between the system's private information 
and the questions of interest hat that information 
can be used to settle. Once the user's words have 
been interpreted, a layer of production rules con- 
structs obligations for response (Traum and Allen, 
1994); then, a second layer plans to meet hese obli- 
gations by deciding to present a specified kind of 
information about a specified object. This deter- 
mines some concrete communicative goals--facts 
of this kind that a contribution to dialogue could 
make. Both speech and gesture can access the 
whole structured database inrealizing these concrete 
communicative goals. For example, a variety of 
facts that bear on where a residence is--which city, 
which neighborhood or, if appropriate, where in a 
building--all provide the same kind of information, 
and would therefore fit the obligation to specify the 
location of a residence. Or, to implement the rule 
for presentation described in connection with ( 1 ), we 
can associate an obligation of presentation with a 
cluster of facts describing an object's type, its loca- 
tion in a house, and its size, shape or quality. 
The communicative context and concrete com- 
municative goals provide a common source for gen- 
erating speech and gesture in REA. The utterance 
generation problem ,in REa,.then, is to construct a- 
complex communicative action, made up of speech 
and coverbal gesture, that achieves a given constel- 
tic features expressed as gestures; ince we assume 
that any such item usually conveys a specific piece 
of content, we refer to these elements generally as 
lexicalized escriptors. The generation task in REA 
thus involves selecting a number of such lexical- 
ized descriptors and organizing them into a gram- 
matical whole that manifests the right semantic and 
pragmatic coordination between speech and gesture. 
The information conveyed must be enough that the 
hearer can identify the entity in each domain ref- 
erence from among its context set. Moreover, the 
descriptors must provide a source which allows the 
hearer to recover any needed new domain proposi- 
tion, either explicitly or by inference. 
We use the SPUD generator ("Sentence Planning 
Using Description") introduced in (Stone and Do- 
ran, 1997) to carry out this task for REA. SPUD 
builds the utterance lement-by-element; at each 
stage of construction, SPUD'S representation f the 
current, incomplete utterance specifies its syntax, 
semantics, interpretation a d fit to context. This rep- 
resentation both allows SPUD to determine which 
lexicalized escriptors are available at each stage to 
extend the utterance, and to assess the progress to- 
wards its communicative goals which each exten- 
sion would bring about. At each stage, then, SPUD 
selects the available option that offers the best im- 
mediate advance toward completing the utterance 
successfully. (We have developed a suite of guide- 
lines for the design of syntactic structures, eman- 
tic and pragmatic representations, and the interface 
between them so that SPUD'S greedy search, which 
is necessary for real-time performance, succeeds in 
finding concise and effective Utterances described 
by the grammar (Stone et al, 2000).) 
As part of the development of REA, we have con- 
structed a new inventory of lexicalized escriptors. 
REA'S descriptors consist of entries that contribute 
to coverbal gestures, as well as revised entries for 
spoken words that allow for their coordination with 
gesture under appropriate discourse conditions. The 
:-organization f'these ntries assures'that--rasing the 
same mechanism as with speech--REA'S gestures 
draw on the single available conceptual representa- 
174 
tion and that both REA'S  gesture and the relation- 
ship between gesture and speech-vary as a function 
of pragmatic context in the same way as natural ges- 
tures and speech do. More abstractly, these entries 
enable SPUD to realize the concrete goals tied to 
common communicative functions with same dis- 
tribution of speech and gestiire bbse~ed:iffn/lttl'ral- 
conversations. 
To explain how these entries work, we need to 
consider SPUD's representation of lexicalized de- 
scriptors in more detail. Each entry is specified 
in three parts. The first part--the syntax of the 
elemenv--sets out what words or other actions the 
element contributes to its utterance. The syn- 
tax is a hierarchical structure, formalized using 
Feature-Based Lexicalized Tree Adjoining Gram- 
mar (LTAG) (Joshi et al, 1975; Schabes, 1990). 
Syntactic structures are also associated with referen- 
tial indices that specify the entities in the discourse 
that the entry refers to. For the entry to apply at a 
particular stage, its syntactic structure must combine 
by LTAG operations with the syntax of the ongoing 
utterance. 
REA'S syntactic entries combine typical phrase- 
structure analyses of linguistic constructions with 
annotations that describe the occurrence of gestures 
in coordination with linguistic phrases. Our device 
for this is a construction SYNC which pairs a descrip- 
tion of a gesture G with the syntactic structure of a 
spoken constituent c:
SYNC 
(2) G C 
The temporal interpretation of (2) mirrors the rules 
for surface synchrony between speech and gesture 
presented in (Cassell et al, 1994). That is, the 
preparatory phase of gesture G is set to begin before 
the time constituent c begins; the stroke of gesture 
G (the most effortful part) co-occurs with the most 
phonologically prominent syllable in c; and, except 
in cases of coarticulation between successive ges- 
tures, by the time the constituent c is complete, the 
speaker must be relaxing and bringing the hands out 
of gesture space (while the generator specifies yn- 
chrony as described, in practice the synchronization 
of synthesized speech with graphics is an ongoing 
challenge in the REA~projeet).-Jn. sum; 'the produc- 
tion of gesture G is synchronized with the produc- 
tion of speech c. (Our representation f synchrony 
175 
in a single tree conveniently allows modules dowrL- 
stream to describe mbodied communicative actions 
as marked-up text.) 
The syntactic description of the gesture itself in- 
dicates the choices the generator must make to pro- 
duce a gesture, but does not analyze a ,gesture lit- 
er~i|y--~is '~/ hier~chy :i~f ~+p~a~e  m~=~fi~s~-'~f~?:"= .... 
stead, these choices specify independent semantic 
features which we can associate with aspects of a 
gesture (such as handshape and trajectory through 
space). Our current grammar does not undertake the 
final step of associating semantic features to choice 
of particular handshapes and movements, orgesture 
morphology; we reserve this problem for later in 
the research program. We allow gesture to accom- 
pany alternative constituents by introducing alterna- 
tive syntactic entries; these entries take on different 
pragmatic requirements (as described below) to cap- 
ture their respective discourse functions. 
So much for syntax. The second part--the seman- 
tics of the element--is a formula that specifies the 
content hat the element carries. Before the entry 
can be used, SPUD must establish that the semantics 
holds of the entities the entry describes. If the se- 
mantics already follows from the common ground, 
SPUD assumes that the hearer can use it to help iden- 
tify the entities described. If the semantics i merely 
part of the system's private knowledge, SPUD treats 
it as new information for the hearer. 
Finally, the third part--the pragmatics of the 
element--is also a formula that SPUD looks to prove 
before using the entry. Unlike the semantics, how- 
ever, the pragmatics does not achieve specific com- 
municative goals like identifying referents. Instead, 
the pragmatics establishes a general fit between the 
entry and the context. 
The entry schematized in (3) illustrates these three 
components; the entry also suggests how these com- 
ponents can define coordinated actions of speech 
and gesture that respond coherently to the context. 
(3) a syntax: s 
NP VP 
NP:o V SYNC 
/have/  G:x I NP:x 
b semantics: have(o,x) 
c 'pragmaties:"heardr-n-ew(x) A'theme{O) . . . . .  
(3) describes the use of have to introduce a new fea- 
ture of (a house) o. The feature, indicated through- 
out the entry by the variable x,.is realized as the ob- 
ject NP of the verb have, but x can also form the ba- 
sis of a gesture G coordinated with the noun phrase 
(as indicated by the SYNC constituent). The entry as- 
serts that o has x. 
(3) is a presentational Construction; in other 
words, it coordinates non-redundant paired speech 
and gesture in the same way as demonstrated by our 
house description data. To represent this constraint 
on its use, the entry carries two pragmatic require- 
ments: first, x must be new to the hearer; moreover, 
o must link up with the open question in the dis- 
course that the sentence responds to. 
The pragmatic onditions of (3) help support 
our theory of the discourse function of gesture and 
speech. A similar kind of sentence could be used 
to address other open questions in the discourse-- 
for example, to answer which house has a garden? 
This would not be a presentational function, and 
(3) would be infelicitous here. In that case, gesture 
would naturally coordinate with and elaborate on the 
answering information--in this case the house. So 
the different information structure would activate a
different entry, where the gesture would coordinate 
with the subject and describe o. 
Meanwhile, alternative ntries like (4a) and 
(4b)---two entries that both convey (4c) and that 
both could combine with (3) by LTAG operations-- 
underlie our claim that our implementation allows 
gesture and speech to draw on a single conceptual 
source and fulfill similar communicative intentions. 
(4) a syntax: G:x 
circular-trajectory RS:x l
b syntax: NP 
NP.:x VP 
V NP:p j 
I 
surrounding 
c semantics: urround(x.p) 
(4a) provides astructure that could substitute for the 
G node in (3) to produce semantically and pragmat- 
ically coordinated speech and gesture. (4a) speci- 
fies a right hand gestnre:in.wlhieh.~the hand. traces 
out a circular trajectory; a further decision must de- 
termine the correct handshape (node RS, as a func- 
176 
tion of the entity x that the gesture describes). We 
pair (4a) with the semantics in (4c), and thereby 
model that the gesture indicates that one object, x, 
surrounds another, p. Since p cannot be further de- 
scribed, p must be identified by an additional pre- 
supposition ofthe gesture which.picks up~a reference 
frame from the sliared context. 
Similarly, (4b) describes how we could modify 
the vP introduced by (3) (using the LTAG operation 
of adjunction), to produce an utterance such as It 
has a garden surrounding it. By pairing (4b) with 
the same semantics (4c), we ensure that SPUD will 
treat he communicative contribution of the alterna- 
tive constructions of (4) in a parallel fashion. Both 
are triggered by accessing background knowledge 
and both are recognized as directly communicating 
specified facts. 
5 Solving the generat ion problem 
We now sketch how entries uch as these combine 
together to account for REA'S utterances. Our exam- 
ple is the dialogue in (5): 
(5) a User: Tell me more about he house. 
b REA: It has \[a nice garden\]. (right hand, held 
fiat, traces a circle) 
REA's response indicates both that the house has a 
nice garden and that it surrounds the house. 
As we have seen, (5b) represents a common pat- 
tern of description; this particular example is moti- 
vated by an exchange two human subjects had in our 
study, cf. (1). (5b) represents a solution to a gen- 
eration problem that arises as follows within REA'S 
overall architecture. The user's directive is inter- 
preted and classified as a directive requiring adelib- 
erative response. The dialogue manager recognizes 
an obligation to respond to the directive, and con- 
cludes that to fulfill the function of presenting the 
garden would discharge this obligation. The presen- 
tational function grounds out in the communicative 
goal to convey a collection of facts about he garden 
(type, quality, location relative to the house). Along 
with these goals, the dialogue manager supplies its 
communicative context, which represents he cen- 
trality of the house in attentional prominence, cog- 
nitive status and information structure. 
In producing (5b) in response to this NLG prob- 
lem, SPUD both calculates the applicability of and 
. ~determines a preference,for-theqexiOatized descrip- 
tors involved. Initially, (3) is applicable; the system 
knows the house has the garden, and represents he 
garden as new and the house as questioned. The en- 
try can be selected over potential alternatives based 
on its interpretation--it achieves a communicative 
goal, refers to a prominent entity, and makes a rel- 
atively specific connection to facts in the context. 
and what its role might be. Likewise, we need a 
model of the communicative effects of spontaneous 
coverbal gesture--one that allows us to reason at- 
urally about he multiple goals speakers have in pro- 
ducing each utterance. 
_Similarly, in the .second, stage, SPUD evaluates .and 
selects (4a) because it Communicates a needed fact 7 
in a way that helps flesh out a concise, balanced 
communicative act by supplying a gesture that by 
using (3) SPUD has already realized belongs here. 
Choices of remaining elements--the words garden 
and nice, the semantic features to represent the gar- 
den in the gesture--proceed similarly. Thus SPUD 
arrives at the response in (5b) just by reasoning from 
the declarative specification ofthe meaning and con- 
text of communicative actions. 
6 Re la ted  Work  
The interpretation of speech and gesture has been 
investigated since the pioneering work of (Bolt, 
1980) on deictic gesture; recent work includes 
(Koons et al, 1993; Bolt and Herranz, 1992). Sys- 
tems have also attempted generation of gesture in 
conjunction with speech. Lester et al (1998) gener- 
ate deictic gestures and choose referring expressions 
as a function of the potential ambiguity of objects re- 
ferred to, and their proximity to the animated agent. 
Rickel and Johnson (1999)'s pedagogical gent pro- 
duces a deictic gesture at the beginning of explana- 
tions about objects in the virtual world. Andr6 et 
al. (1999) generate pointing estures as a sub-action 
of the rhetorical action of labeling, in turn a sub- 
action of elaborating. 
Missing from these prior systems, however, is a 
representation f communicative action that treats 
the different modalities on a par. Such representa- 
tions have been explored in research on combining 
linguistic and graphical interaction. For example, 
multimodal managers have been described to allo- 
cate an underlying content representation for gen- 
eration of text and graphics (Wahlster et al, 1991; 
Green et al, 1998). Meanwhile, (Johnston et al, 
1997; Johnston, 1998) describe a formalism for 
tightly-coupled interpretation which uses a gram- 
mar and semantic onstraints oanalyze input from 
speech and pen. While many insights from these 
formalisms are relevant in embodied conversation, 
spontaneous gesture requires adistinct analysis with 
different emphasis:For example;-we need some no- 
tion of discourse pragmatics that would allow us to 
predict where gesture occurs with respect to speech, 
Conclusion . . . . .  
Research on the robustness of human conversation 
suggests that a dialogue agent capable of acting 
as a conversational partner would provide for effi- 
cient and natural collaborative dialogue. But human 
conversational partners display gestures that derive 
from the same underlying conceptual source as their 
speech, and which relate appropriately to their com- 
municative intent. In this paper, we have summa- 
rized the evidence for this view of human conver- 
sation, and shown how it informs the generation 
of communicative action in our artificial embodied 
conversational agent, REA. REA has a working im- 
plementation, which includes the modules described 
in this paper, and can engage in a variety of interac- 
tions including that in (5). Experiments are under- 
way to investigate the extent o which REA 'S conver- 
sational capacities share the strengths of the human 
capacities they are modeled on. 
Acknowledgments  
The research reported here was supported by NSF (award 
IIS-9618939), Deutsche Telekom, AT&T, and the other 
generous sponsors of the MIT Media Lab, and a postdoc- 
toral fellowship from RUCCS. Hannes Vilhj~ilmsson as- 
sisted with the implementation of REA'S discourse man- 
ager. We thank Nancy Green. James Lester, Jeff Rickel, 
Candy Sidner, and anonymous reviewers for comments 
on this and earlier drafts. 
References 
Elisabeth Andr6, Thomas Rist, and Jochen Mailer. 1999. 
Employing AI methods to control the behavior of ani- 
mated interface agents. AppliedArtificial Intelligence, 
13:415-448. 
Douglas Appelt. 1985. Planning English Sentences. 
Cambridge University Press, Canlbridge England. 
R. A. Bolt and E. Herranz. 1992. Two-handed gesture in 
multi-modal natural dialog. In UIST92: Fifth Ammal 
Symposium on User Interface Software and Technol- 
ogy. 
R. A. Bolt. 1980. Put-that-there: voice and gesture at the 
graphics interface. Computer Graphics. 14(3):262- 
270. 
J. Cassell and K. Th6risson. 1999. The power of a nod 
and a glance: Envelope vs. emotional feedback in an- 
imatefd conversational agents. AppliedArt(ficial Intel- 
ligence, 13(3). 
177 
Justine Casseil, Catherine Pelachaud, Norm Badler, 
Mark Steedman, Brett Achorn, Tripp Becket, Brett 
Douville, Scott Prevost, and Matthew Stone. 1994. 
Animated conversation: Rule-based generation of fa- 
cial expression, gesture and spoken intonation for mul- 
tiple conversational gents. In SIGGRAPH, pages 
Patrick FitzGerald. 1998. Deictic and emotive com- 
munication i  animated pedagogical gents. In Work- 
shop on Embodied Conversational Characters. 
David McNeill. 1992. Hand and Mind: What Gestures 
Reveal about Thought. University of Chicago Press, 
Chicago. 
413-420. 
J. Cassell, D. McNeill, and K. E. McCullough. 1999. 
Speech-gesture mismatches: evidence for one under- 
lying representation f linguistic and nonlinguistic n- 
formation. Pragmatics and Cognition, 6(2). 
Justine Cassell. 2000a. Embodied conversational inter- 
face agents. Communications of the ACM, 43(4):70- 
78. 
Justine Cassell. 2000b. Nudge nudge wink wink: Ele- 
ments of face-to-face conversation for embodied con- 
versational gents. In J. Cassell, J. Sullivan, S. Pre- 
vost, and E. Churchill, editors, Embodied Conversa- 
tional Agents, pages 1-28. MIT Press, Cambridge, 
MA. 
Robert Dale. 1992. Generating Referring Expressions: 
Constructing Descriptions ina Domain of Objects and 
Processes. MIT Press, Cambridge MA. 
S. Feiner and K. McKeown. 1991. Automating the gen- 
eration of coordinated multimedia explanations. IEEE 
Computer, 24(10): 33-41. 
Nancy Green, Giuseppe Carenini, Stephan Kerpedjiev, 
Steven Roth, and Johanna Moore. 1998. A media- 
independent content language for integrated text and 
graphics generation. In CVIR '98- Workshop on Con- 
tent Visualization and Intermedia Representations. 
Jeanette K. Gundel, Nancy Hedberg, and Ron Zacharski. 
1993. Cognitive status and the form of referring ex- 
pressions in discourse. Language, 69(2):274--307. 
M. Johnston, P. R. Cohen, D. McGee, J. Pittman, S. L. 
Oviatt, and I. Smith. 1997. Unification-based multi- 
modal integration. In ACL/EACL 97: Proceedings of 
the Annual Meeting of the Assocation for Computa- 
tional Linguistics. 
Michael Johnston. 1998. Unification-based naultimodal 
parsing. In COLING/ACL. 
Aravind K. Joshi, L. Levy, and M. Takahashi. 1975. Tree 
adjunct grammars. Journal of the Computer and Sys- 
tem Sciences, 10:136--- 163. 
S. D. Kelly, J. D. Barr, R. B. Church, and K. Lynch. 1999. 
Offering a hand to pragmatic understanding: The role 
of speech and gesture in comprehension a d memory. 
Journal of Memory and Language, 40:577-592. 
A. Kendon. 1974. Movement coordination i social in- 
teraction: somem examples described. In S. Weitz, ed- 
itor, Nonverbal Communication. Oxford, New York. 
D. B. Koons. C. J. Sparrell, and K. R. Th6risson. 1993. 
Integrating simultaneous input from speech, gaze and 
hand gestures. In M. T. Maybtiry,'-editor~-lntel/igent 
Multi-media Interfaces. MIT Press, Cambridge. 
James Lester, Stuart Towns. Charles Calloway, and 
. . . . . . . .  - ~ .....  :.~,, ~Joharma:Moore; ~,~994,,P, articipating in Explanatory Di- 
alogues. MIT Press, Cambridge MA. 
S. L. Oviatt. 1995. Predicting spoken language disflu- 
encies during human-computer interaction. Computer 
Speech and Language, 9( l ): 19-35. 
Ellen Prince. 1986. On the syntactic marking of pre- 
supposed open propositions. In Proceedings of the 
22nd Annual Meeting of the Chicago Linguistic Soci- 
ety, pages 208-222, Chicago. CLS. 
Ellen F. Prince. 1992. The ZPG letter: Subjects, definite- 
ness and information status. In William C. Mann and 
Sandra A. Thompson, editors, Discourse Description: 
Diverse Analyses of a Fund-raising Text, pages 295- 
325. John Benjamins, Philadelphia. 
Jeff Rickel and W. Lewis Johnson. 1999. Animated 
agents for procedural training in virtual reality: Per- 
ception, cognition and motor control. Applied Artifi- 
cial Intelligence, 13:343-382. 
W. T. Rogers. 1978. The contribution of kinesic illus- 
trators towards the comprehension f verbal behavior 
within utterances. Human Communication Research, 
5:54--62. 
Yves Schabes. 1990. Mathematical nd Computational 
Aspects of Lexicalized Grammars. Ph.D. thesis, Com- 
puter Science Department, University of Pennsylva- 
nia. 
Mark Steedman. 1991. Structure and intonation. Lan- 
guage, 67:260-296. 
Matthew Stone and Christine Doran. 1997. Sentence 
planning as description using tree-adjoining grammar. 
In Proceedings of ACL, pages 198-205. 
Matthew Stone, Tonia Bleam, Christine Doran, and 
Martha Palmer. 2000. Lexicalized grammar and the 
description of motion events. In TAG+: Workshop on 
Tree-Adjoining Grammar and Related Forntalisms. 
Michael Strube. 1998. Never look back: An alternative 
to centering. In Proceedings of COLING-ACL. 
L. A. Thompson and D. W. Massaro. 1986. Evaluation 
and integration of speech and pointing gestures dur- 
ing referential understanding. Journal of Experimen- 
tal Child Psychology, 42:144-168. 
David R. Traum and James F. Allen. 1994. Discourse 
obligations in dialogue processing. In ACL, pages 1- 
8. 
W. Wahlster, E. Andr6, W. Graf, and T. Rist. 1991. 
Designing illustrated texts. In Proceedings of EACL, 
pages 8-14. 
Hao Yan.~ 20,00. Paired speech~and gesture generation i
embodied conversational agents~ Master's thesis, Me- 
dia Lab, MIT. 
178 
Lexicalized Grammar 101
Matthew Stone
Department of Computer Science and Center for Cognitive Science
Rutgers, the State University of New Jersey
Piscataway NJ 08854-8019 USA
http://www.cs.rutgers.edu/?mdstone
mdstone@cs.rutgers.edu
Abstract
This paper presents a simple and ver-
satile tree-rewriting lexicalized grammar
formalism, TAGLET, that provides an ef-
fective scaffold for introducing advanced
topics in a survey course on natural lan-
guage processing (NLP). Students who
implement a strong competence TAGLET
parser and generator simultaneously get
experience with central computer science
ideas and develop an effective starting
point for their own subsequent projects in
data-intensive and interactive NLP.
1 Introduction
This paper is particularly addressed to readers at in-
stitutions whose resources and organization rule out
extensive formal course-work in natural language
processing (NLP). This is typical at universities in
North America. In such places, NLP teaching must
be ambitious but focused; courses must quickly ac-
quaint a broad range of students to the essential
concepts of the field and sell them on its current
research opportunities and challenges. This paper
presents one resource that may help. Specifically,
I outline a simple and versatile lexicalized formal-
ism for natural language syntax, semantics and prag-
matics, called TAGLET, and draw on my experi-
ence with CS 533 (NLP) at Rutgers to motivate the
potential role for TAGLET in a broad NLP class
whose emphasis is to introduce topics of current re-
search. Notes, assignments and implementations for
TAGLET are available on the web.
I begin in Section 2 by describing CS 533?
situating the course within the university and outlin-
ing its topics, audience and goals. I then describe the
specific goals for teaching and implementing gram-
mar formalisms within such a course, in Section 3.
Section 4 gives an informal overview of TAGLET,
and the algorithms, specifications and assignments
that fit TAGLET into a broad general NLP class.
In brief, TAGLET is a context-free tree-rewriting
formalism, defined by the usual complementation
operation and the simplest imaginable modifica-
tion operation. By implementing a strong compe-
tence TAGLET parser and generator students simul-
taneously get experience with central computer sci-
ence ideas?data structures, unification, recursion
and abstraction?and develop an effective starting
point for their own subsequent projects. Two note-
worthy directions are the construction of interac-
tive applications, where TAGLET?s relatively scal-
able and reversible processing lets students easily
explore cutting-edge issues in dialogue semantics
and pragmatics, and the development of linguistic
specifications, where TAGLET?s ability to lexical-
ize tree-bank parses introduces a modern perspec-
tive of linguistic intuitions and annotations as pro-
grams. Section 5 briefly summarizes the advantages
of TAGLET over the many alternative formalisms
that are available; an appendix to the paper provides
more extensive technical details.
2 CS 533
NLP at Rutgers is taught as part of the graduate ar-
tificial intelligence (AI) sequence in the computer
science department. As a prerequisite, computer sci-
ence students are expected to be familiar with prob-
                     July 2002, pp. 77-84.  Association for Computational Linguistics.
              Natural Language Processing and Computational Linguistics, Philadelphia,
         Proceedings of the Workshop on Effective Tools and Methodologies for Teaching
abilistic and decision-theoretic modeling (including
statistical classification, hidden Markov models and
Markov decision processes) from the graduate-level
AI foundations class. They might take NLP as a pre-
liminary to research in dialogue systems or in learn-
ing for language and information?or simply to ful-
fill the breadth requirement of MS and PhD degrees.
Students from a number of other departments fre-
quently get involved in natural language research,
however, and are also welcome in 533; on average,
only about half the students in 533 come from com-
puter science. Students from the linguistics depart-
ment frequently undertake computational work as
a way of exploring practical learnability as a con-
straint on universal grammar, or practical reasoning
as a constraint on formal semantics and pragmatics.
The course also attracts students from Rutgers?s li-
brary and information science department, its pri-
mary locus for research in information retrieval and
human-computer interaction. Ambitious undergrad-
uates can also take 533 their senior year; most par-
ticipate in the interdisciplinary cognitive science un-
dergraduate major. 533 is the only computational
course in natural language at Rutgers.
Overall, the course is structured into three mod-
ules, each of which represents about fifteen hours of
in-class lecture time.
The first module gives a general overview of lan-
guage use and dialogue applications. Lectures fol-
low (Clark, 1996), but instill the practical method-
ology for specifying and constructing knowledge-
based systems, in the style of (Brachman et al,
1990), into the treatment of communication. Con-
currently, students explore precise descriptions of
their intuitions about language and communication
through a series of short homework exercises.
The second module focuses on general techniques
for linguistic representation and implementation, us-
ing TAGLET. With an extended TAGLET project,
conveniently implemented in stages, we use basic
tree operations to introduce Prolog programming,
including data structures, recursion and abstraction
much as outlined in (Sterling and Shapiro, 1994);
then we write a simple chart parser with incremental
interpretation, and a simple communicative-intent
generator scaled down after (Stone et al, 2001).
The third module explores the distinctive prob-
lems of specific applications in NLP, including spo-
ken dialogue systems, information retrieval and text
classification, spelling correction and shallow tag-
ging applications, and machine translation. Jurafsky
and Martin (2000) is our source-book. Concurrently,
students pursue a final project, singly or in cross-
disciplinary teams, involving a more substantial and
potentially innovative implementation.
In its overall structure, the course seems quite
successful. The initial emphasis on clarifying in-
tuitions about communication puts students on an
even footing, as it highlights important ideas about
language use without too much dependence on spe-
cialized training in language or computation. By the
end of the class, students are able to build on the
more specifically computational material to come up
with substantial and interesting final projects. In
Spring 2002 (the first time this version of 533 was
taught), some students looked at utterance interpre-
tation, response generation and graphics generation
in dialogue interaction; explored statistical methods
for word-sense disambiguation, summarization and
generation; and quantified the potential impact of
NLP techniques on information tasks. Many of these
results represented fruitful collaborations between
students from different departments.
Naturally, there is always room for improvement,
and the course is evolving. My presentation of
TAGLET here, for example, represents as much a
project for the next run of 533 as a report of this
year?s materials; in many respects, TAGLET actu-
ally emerged during the semester as a dynamic reac-
tion to the requirements and opportunities of a six-
week module on general techniques for linguistic
representation and implementation.
3 Language and Computation in NLP
In a survey course for a broad, research-oriented au-
dience, like CS 533 at Rutgers, a module on linguis-
tic representation must orient itself to central ideas
about computation. 533 may be the first and last
place linguistics or information science students en-
counter concepts of specification, abstraction, com-
plexity and search in class-work. The students who
attack interdisciplinary research with success will be
the ones who internalize and draw on these concepts,
not those who merely hack proficiently. At the same
time, computer scientists also can benefit from an
emphasis on computational fundamentals; it means
that they are building on and reinforcing their ex-
pertise in computation in exploring its application to
language. Nevertheless, NLP is not compiler con-
struction. Programming assignments should always
underline a worthwhile linguistic lesson, not indulge
in implementation for its own sake.
This perspective suggests a number of desiderata
for the grammar formalism for a survey course in
NLP.
Tree rewriting. Students need to master recur-
sive data-structures and programming. NLP directs
our attention to the recursive structures of linguistic
syntax. In fact, by adopting a grammar formalism
whose primitives operate on these structures as first-
class objects, we can introduce a rich set of relatively
straightforward operations to implement, and moti-
vate them by their role in subsequent programs.
Lexicalization. Students need to distinguish be-
tween specification and implementation, and to un-
derstand the barriers of abstraction that underlie
the distinction. Lexicalized grammars come with a
ready notion of abstraction. From the outside, ab-
stractly, a lexicalized grammar analyzes each sen-
tence as a simple combination of atomic elements
from a lexicon of options. Simultaneously, a con-
crete implementation can assign complex structures
to the atomic elements (elementary trees) and imple-
ment complex combinatory operations.
Strong competence implementation. Students
need to understand how natural language must and
does respond to the practical logic of physical re-
alization, like all AI (Agre, 1997). Mechanisms that
use grammars face inherent computational problems
and natural grammars in particular must respond to
these problems: students should undertake imple-
mentations which directly realize the operations of
the grammar in parsing and generation. But these
must be effective programs that students can build
on?our time and interest is too scarce for extensive
reimplementations.
Simplicity. Where possible, linguistic proposals
should translate readily to the formalism. At the
same time, students should be able to adapt aspects
of the formalism to explore their own judgments
and ideas. Where possible, students should get in-
tuitive and satisfying results from straightforward
algorithms implemented with minimal bookkeeping
and case analysis. At the same time, there is no rea-
son why the formalism should not offer opportuni-
ties for meaningful optimization.
We cannot expect any formalism to fare perfectly
by all these criteria?if any does, it is a deep fact
about natural language! Still, it is worth remark-
ing just how badly these criteria judge traditional
unification-based context-free grammars (CFGs), as
presented in say (Pereira and Shieber, 1987). Data-
structures are an afterthought in CFGs; CFGs can-
not in principle be lexicalized; and, whatever their
merits in parsing or recognition, CFGs set up a pos-
itively abysmal search space for meaningful genera-
tion tasks.
4 TAGLET
TAGLET1 is my response to the objectives mo-
tivated in Section 2 and outlined in Section 3.
TAGLET represents my way of distilling the essen-
tial linguistic and computational insights of lexical-
ized tree-adjoining grammar?LTAG (Joshi et al,
1975; Schabes, 1990)?into a form that students can
easily realize in end-to-end implementations.
4.1 Overview
Like LTAG, TAGLET analyzes sentences as a com-
plex of atomic elements combined by two kinds of
operations, complementation and modification. Ab-
stractly, complementation combines a head with an
argument which is syntactically obligatory and se-
mantically dependent on the head. Abstractly, mod-
ification combines a head with an adjunct which is
syntactically optional and need not involve any spe-
cial semantic dependence. Crucially for generation,
in a derivation, modification and complementation
operations can apply to a head in any order, often
yielding identical structures in surface syntax. This
means the generator can provide required material
first, then elaborate it, enabling use of grammar in
high-level tasks such as the planning of referring ex-
pressions or the ?aggregation? of related semantic
material into a single complex sentence.
Concretely, TAGLET operations are implemented
by operations that rewrite trees. Each lexical el-
ement is associated with a fragmentary phrase-
1If the acronym must stand for something, ?Tree Assembly
Grammar for LExicalized Teaching? will do.
CT +
T?
C
)
T?
C
T
Figure 1: Substitution (complementation).
C
T
+
T?
C?
C*
)
T?
C?
C
T
Figure 2: Forward sister-adjunction (modification.)
structure tree containing a distinguished word called
the anchor. For complementation, TAGLET adopts
TAG?s substitution operation; substitution replaces
a leaf node in the head tree with the phrase struc-
ture tree associated with the complement. See Fig-
ure 1. For modification, TAGLET adopts the the
sister-adjunction operation defined in (Rambow et
al., 1995); sister-adjunction just adds the modifier
subtree as a child of an existing node in the head
tree?either on the left of the head (forward sister-
adjunction) as in Figure 2, or on the right of the head
(backward sister-adjunction). I describe TAGLET
formally in Appendix A.
TAGLET is equivalent in weak generative power
to context-free grammar. That is, any language de-
fined by a TAGLET also has a CFG, and any lan-
guage defined by a CFG also has a TAGLET. On the
other hand context-free languages can have deriva-
tions in which all lexical items are arbitrarily far
from the root; TAGLET derived structures always
have an anchor whose path to the root of the sen-
tence has a fixed length given by a grammatical ele-
ment. See Appendix B. The restriction seems of lit-
tle linguistic significance, since any tree-bank parse
induces a unique TAGLET grammar once you la-
bel which child of each node is the head, which are
complements and which are modifiers. Indeed, since
TAGLET thus induces bigram dependency struc-
tures from trees, this invites the estimation of proba-
bility distributions on TAGLET derivations based on
NP
Chris
S


H
H
NP VP


H
H
V
loves
NP
NP
Sandy
VP*
n
ADVP
madly
Figure 3: Parallel analysis in TAGLET and TAG.
observed bigram dependencies; see (Chiang, 2000).
To implement an effective TAGLET generator,
you can perform a greedy head-first search of deriva-
tions guided by heuristic progress toward achieving
communicative goals (Stone et al, 2001). Mean-
while, because TAGLET is context-free, you can
easily write a CKY-style dynamic programming
parser that stores structures recognized for spans of
text in a chart, and iteratively combines structures
in adjacent spans until the analyses span the entire
sentence. (More complexity would be required for
multiply-anchored trees, as they induce discontinu-
ous constituents.) The simple requirement that op-
erations never apply inside complements or modi-
fiers, and apply left-to-right within a head, suffices
to avoid spurious ambiguity. See Appendix C.
4.2 Examples
With TAGLET, two kinds of examples are instruc-
tive: those where TAGLET can mirror TAG, and
those where it cannot. For the first case, consider
an analysis of Chris loves Sandy madly by the trees
of Figure 3. The final structure is:
S




H
H
H
H
NP
Chris
VP




H
H
H
H
V
loves
NP
Sandy
ADVP
madly
For the second case, consider the embedded ques-
tion who Chris thinks Sandy likes. The usual TAG
analysis uses the full power of adjunction. TAGLET
requires the use of one of the familiar context-free
filler-gap analyses, as perhaps that suggested by the
trees in Figure 4, and their composition:
Q


H
H
NP
who
S/NP
NP
Chris
S/NP



H
H
H
NP VP/NP


H
H
V
thinks
S/NP
NP
Sandy
S/NP


H
H
NP VP/NP
V
likes
Figure 4: TAGLET requires a gap-threading analy-
sis of extraction (or another context-free analysis).
Q




H
H
H
H
NP
who
S/NP




H
H
H
H
NP
Chris
VP/NP



H
H
H
V
thinks
S/NP


H
H
NP
Sandy
VP/NP
V
likes
The use of syntactic features amounts to an in-
termediate case. In TAGLET derivations (unlike in
TAG) nodes accrete children during the course of a
derivation but are never rewritten or split. Thus, we
can decorate any TAGLET node with a single set
of syntactic features that is preserved throughout the
derivation. Consider the trees for he knows below:
NP
[
NM SG
CS X
]
/he/
S




H
H
H
H
NP
[
NM Y
CS N
]
VP
V
[
NM Y
]
/know/
When these trees combine, we can immediately
unify the number Y of the verb with the pronoun?s
singular; we can immediately unify the case X of the
pronoun with the nominative assigned by the verb:
S




H
H
H
H
NP
[
NM SG
CS N
]
/he/
VP
V
[
NM SG
]
/know/
The feature values will be preserved by further steps
of derivation.
4.3 Building on TAGLET
Semantics and pragmatics are crucial to NLP.
TAGLET lets students explore meaty issues in se-
mantics and pragmatics, using the unification-based
semantics proposed in (Stone and Doran, 1997). We
view constituents as referential, or better, indexical;
we link elementary trees with constraints on these
indices and conjoin the constraints in the meaning
of a compound structure. This example shows how
the strategy depends on a rich ontology:
S:e




H
H
H
H
NP:c
Chris
VP:e





H
H
H
H
H
V:e
loves
NP:s
Sandy
ADVP:e
madly
chris(c)^ sandy(s)^ love(e,c,s)^mad(e)
The example also shows how the strategy lets us
quickly implement, say, the constraint-satisfaction
approaches to reference resolution or the plan-
recognition approaches to discourse integration de-
scribed in (Stone and Webber, 1998).
4.4 Lectures and Assignments
Here is a plan for a six-week TAGLET module. The
first two weeks introduce data structures and recur-
sive programming in Prolog, with examples drawn
from phrase structure trees and syntactic combi-
nation; and discuss dynamic-programming parsers,
with an aside on convenient implementation using
Prolog assertion. As homework, students implement
simple tree operations, and build up to definitions of
substitution and modification for parsing and gener-
ation; they use these combinatory operations to write
a CKY TAGLET parser.
The next two weeks begin with lectures on the
lexicon, emphasizing abstraction on the computa-
tional side and the idiosyncrasy of lexical syntax and
the indexicality of lexical semantics on the linguis-
tic side; and continue with lectures on semantics and
interpretation. Meanwhile, students add reference
resolution to the parser, and implement routines to
construct grammars from tree-bank parses.
The final two weeks cover generation as problem-
solving, and search through the grammar. Students
reuse the grammar and interpretation model they al-
ready have to construct a generator.
5 Conclusion
Important as they are, lexicalized grammars can
be forbidding. Versions of TAG and combinatory
categorial grammars (CCG) (Steedman, 2000), as
presented in the literature, require complex book-
keeping for effective computation. When I wrote
a CCG parser as an undergraduate, it took me a
whole semester to get an implemented handle on
the metatheory that governs the interaction of (cross-
ing) composition or type-raising with spurious am-
biguity; I still have never written a TAG parser or a
CCG generator. Variants of TAG like TIG (Schabes
and Waters, 1995) or D-Tree grammars (Rambow
et al, 1995) are motivated by linguistic or formal
considerations rather than pedagogical or computa-
tional ones. Other formalisms come with linguistic
assumptions that are hard to manage. Link gram-
mar (Sleator and Temperley, 1993) and other pure
dependency formalisms can make it difficult to ex-
plore rich hierarchical syntax and the flexibility of
modification; HPSG (Pollard and Sag, 1994) comes
with a commitment to its complex, rather bewilder-
ing regime for formalizing linguistic information as
feature structures. Of course, you probably could
refine any of these theories to a simple core?and
would get something very like TAGLET.
I strongly believe that this distillation is worth
the trouble, because lexicalization ties grammar for-
malisms so closely to the motivations for studying
language in the first place. For linguistics, this phi-
losophy invites a fine-grained description of sen-
tence syntax, in which researchers document the di-
versity of linguistic constructions within and across
languages, and at the same time uncover impor-
tant generalizations among them. For computation,
this philosophy suggests a particularly concrete ap-
proach to language processing, in which the infor-
mation a system maintains and the decisions it takes
ultimately always just concern words. In taking
TAGLET as a starting point for teaching implemen-
tation in NLP, I aim to expose a broad range of stu-
dents to a lexicalized approach to the cognitive sci-
ence of human language that respects and integrates
both linguistic and computational advantages.
Acknowledgments
Thanks to the students of CS 533 and four anony-
mous reviewers for helping to disabuse me of nu-
merous preconceptions.
References
Philip E. Agre. 1997. Computation and Human Experi-
ence. Cambridge.
Ronald Brachman, Deborah McGuinness, Peter Pa-
tel Schneider, Lori Alperin Resnick, and Alexander
Borgida. 1990. Living with CLASSIC: when and how
to use a KL-ONE-like language. In J. Sowa, editor,
Principles of Semantic Networks. Morgan Kaufmann.
David Chiang. 2000. Statistical parsing with an
automatically-extracted tree adjoining grammar. In
ACL, pages 456?463.
Herbert H. Clark. 1996. Using Language. Cambridge
University Press, Cambridge, UK.
John E. Hopcroft, Rajeev Motwani, and Jeffrey D. Ull-
man. 2000. Introduction to automata theory, lan-
guages and computation. Addison-Wesley, second
edition.
Aravind K. Joshi, L. Levy, and M. Takahashi. 1975. Tree
adjunct grammars. Journal of the Computer and Sys-
tem Sciences, 10:136?163.
Daniel Jurafsky and James H. Martin. 2000. Speech
and Language Processing: An introduction to nat-
ural language processing, computational linguistics
and speech recognition. Prentice-Hall.
Fernando C. N. Pereira and Stuart M. Shieber. 1987.
Prolog and Natural Language Analysis. CSLI, Stan-
ford CA.
Carl Pollard and Ivan A. Sag. 1994. Head-Driven Phrase
Structure Grammar. University of Chicago Press,
Chicago.
Owen Rambow, K. Vijay-Shanker, and David Weir.
1995. D-Tree grammars. In ACL, pages 151?158.
Yves Schabes and Richard C. Waters. 1995. Tree-
insertion grammar: A cubic-time parsable formalism
that lexicalizes context-free grammar without chang-
ing the trees produced. Computational Linguistics,
21:479?513.
Yves Schabes. 1990. Mathematical and Computational
Aspects of Lexicalized Grammars. Ph.D. thesis, Com-
puter Science Department, University of Pennsylva-
nia.
Daniel Sleator and Davy Temperley. 1993. Parsing
English with a link grammar. In Third International
Workshop on Parsing Technologies.
Mark Steedman. 2000. The Syntactic Process. MIT.
Leon Sterling and Ehud Shapiro. 1994. The Art of Pro-
log. MIT, second edition.
Matthew Stone and Christine Doran. 1997. Sentence
planning as description using tree-adjoining grammar.
In Proceedings of ACL, pages 198?205.
Matthew Stone and Bonnie Webber. 1998. Textual
economy through close coupling of syntax and seman-
tics. In Proceedings of International Natural Lan-
guage Generation Workshop, pages 178?187.
Matthew Stone, Christine Doran, Bonnie Webber, Tonia
Bleam, and Martha Palmer. 2001. Microplanning
with communicative intentions: The SPUD system.
Under review.
A Definitions
I define TAGLET in terms of primitive trees. The
definitions require a set VT of terminal categories,
corresponding to our lexical items, and a disjoint set
VN of nonterminal categories, corresponding to con-
stituent categories. TAGLET uses trees labeled by
these categories both as representations of the syn-
tactic structure of sentences and as representations
of the grammatical properties of words:
 A syntactic tree is a tree whose nodes are each
assigned a unique label in VN [VT , such that
only leaf nodes are assigned a label in VT .
 A lexical tree is a syntactic tree in which ex-
actly one node, called the anchor, is assigned a
label in VT . The path through such a tree from
the root to the anchor is called the spine.
A primitive tree is lexical tree in which every leaf is
the child of a node on the spine. See Figures 3 and 4.
A TAGLET element is a pair hT,Oi consisting of
primitive tree together with the specification of the
operation for the tree; the allowable operations are
complementation, indicated by ?; premodification
at a specified category C 2 VN , indicated by ?!(C)
and postmodification at a specified category C 2VN ,
indicated by ? (C).
Formally, then, a TAGLET grammar is a tuple
G = hVT ,VN ,?i where VT gives the set of termi-
nal categories, VN gives the set of nonterminal cat-
egories, and ? gives a set of TAGLET elements for
VT and VN . Given a TAGLET grammar G, the set
of derived trees for G is defined as the smallest set
closed under the following operations:
 (Initial) Suppose hT,Oi 2 ?. Then hT,Oi is a
derived tree for G.
 (Substitution) Suppose hT,Oi is a derived tree
for G where T contains leaf node n with label
C 2VN ; and suppose hT 0,?i is a derived tree for
G where the root of T 0 also has label C. Then
hT 00,Oi is a derived tree for G where T 00 is ob-
tained from T by identifying node n with the
root of T 0.
 (Premodification) Suppose hT,Oi is a derived
tree for G where T contains node n with label
C 2 VN , and suppose hT 0,?!(C)i is a derived
tree for G. Then hT 00,Oi is a derived tree for G
where T 00 is obtained from T by adding T 0 as
the first child of node n.
 (Postmodification) Suppose hT,Oi is a derived
tree for G where T contains node n with label
C 2 VN , and suppose hT 0,? (C)i is a derived
tree for G. Then hT 00,Oi is a derived tree for G
where T 00 is obtained from T by adding T 0 as
the last child of node n.
A derivation for G is a derived tree hT,?i for G, in
which all the leaves of T are elements of VT . The
yield of a derivation hT,?i is the string consisting of
the leaves of T in order. A string ? is in the language
generated by G just in case ? is the yield of some
derivation for G.
B Properties
Each node in a TAGLET derived tree T is first con-
tributed by a specific TAGLET element, and so in-
directly by a particular anchor. Accordingly, we can
construct a lexicalized derivation tree corresponding
to T . Nodes in the derivation tree are labeled by the
elements used in deriving T . An edge leads from
parent E to child E 0 if T includes a step of deriva-
tion in which E 0 is substituted or sister-adjoined at a
node first contributed by E . To make the derivation
unambiguous, we record the address of the node in
E at which the operation applies, and we order the
edges in the derivation tree in the same order that
the corresponding operations are applied in T . For
Figure 3, we have:
?2:loves








H
H
H
H
H
H
H
H
?1:Chris (0) ?3:Sandy (1.1) ? 4 :madly (1.1)
Let L be a CFL. Then there is a grammar G for
L in Greibach normal form (Hopcroft et al, 2000),
where each production has the form
A! xB1 . . .Bn
where x2VT and Bi 2VN . For each such production,
create the TAGLET element which allows comple-
mentation with a tree as below:
A


H
H
x B1 Bn
An easy induction transforms any derivation in G
to a derivation in this TAGLET grammar, and vice
versa. So both generate the same language L.
Conversely, we can build a CFG for a TAGLET by
creating nonterminals and productions for each node
in a TAGLET elementary structure, taking into ac-
count the possibilities for optional premodification
and postmodification as well as complementation.
C Parsing
Suppose we make a bottom-up traversal of a
TAGLET derivation tree to construct the derived
tree. After we finish with each node (and all its chil-
dren), we obtain a subtree of the final derived tree.
This subtree represents a complete constituent that
must appear as a subsequence of the final sentence.
A CKY TAGLET parser just reproduces this hier-
archical discovery of constituents, by adding com-
pleted constituents for complements and modifiers
into an open constituent for a head.
The only trick is to preserve linear order; this
means adding each new complement and modifier at
a possible ?next place?, without skipping past miss-
ing complements or slipping under existing modi-
fiers. To do that, we only apply operations that add
completed constituents T2 along what is known as
the frontier of the head tree T1, further away from
the head than previously incorporated material. This
concept, though complex, is essential in any account
of incremental structure-building. To avoid spuri-
ous ambiguities, we also require that operations to
the left frontier must precede operations to the right
frontier. This gives a relation COMBINE(T1,T2,T3).
The parser analyses a string of length N using a
dynamic-programming procedure to enumerate all
the analyses that span contiguous substrings, short-
est substrings first. We write T 2 (i, j) to indicate
that object T spans position i to j. The start of the
string is position 0; the end is position N. So we
have:
for word w 2 (i, i + 1), T with anchor w
add T 2 (i, i + 1)
for k 2 up to N
for i k?2 down to 0
for j i + 1 up to k?1
for T1 2 (i, j) and T2 2 ( j,k)
for T3 with COMBINE(T1,T2,T3)
add T3 2 (i,k)
Now, any parser that delivers possible analyses ex-
haustively will be prohibitively expensive in the
worst-case; analyses of ambiguities multiply expo-
nentially. At the cost of a strong-competence imple-
mentation, one can imagine avoiding the complexity
by maintaining TAGLET derivation forests. This en-
ables O(N3) recognition, since TAGLET parsing op-
erations apply within spans of the spine of single ele-
mentary trees and therefore the number of COMBINE
results for T1 and T2 is independent of N.
Proceedings of the Second ACL Workshop on Effective Tools and Methodologies for Teaching NLP and CL, pages 9?14,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Teaching Dialogue to Interdisciplinary Teams through Toolkits
Justine Cassell
Technology and Social Behavior
Northwestern University
justine@northwestern.edu
Matthew Stone
Computer Science and Cognitive Science
Rutgers University
matthew.stone@rutgers.edu
Abstract
We present some lessons we have learned
from using software infrastructure to
support coursework in natural language
dialogue and embodied conversational
agents. We have a new appreciation
for the differences between coursework
and research infrastructure?supporting
teaching may be harder, because students
require a broader spectrum of implemen-
tation, a faster learning curve and the abil-
ity to explore mistaken ideas as well as
promising ones. We outline the collabo-
rative discussion and effort we think is re-
quired to create better teaching infrastruc-
ture in the future.
1 Introduction
Hands-on interaction with dialogue systems is a nec-
essary component of a course on computational lin-
guistics and natural language technology. And yet, it
is clearly impracticable to have students in a quarter-
long or semester-long course build a dialogue sys-
tem from scratch. For this reason, instructors of
these courses have experimented with various op-
tions to allow students to view the code of a work-
ing dialogue system, tweak code, or build their own
application using a dialogue system toolkit. Some
popular options include the NLTK (Loper and Bird,
2002), CSLU (Cole, 1999), Trindi (Larsson and
Traum, 2000) and Regulus (Rayner et al, 2003)
toolkits. However, each of these options has turned
out to have disadvantages. Some of the toolkits re-
quire too much knowledge of linguistics for the av-
erage computer science student, and vice-versa, oth-
ers require too much programming for the average
linguist. What is needed is an extensible dialogue
toolkit that allows easy application building for be-
ginning students, and more sophisticated access to,
and tweakability of, the models of discourse for ad-
vanced students.
In addition, as computational linguists become in-
creasingly interested in the role of non-verbal be-
havior in discourse and dialogue, more of us would
like to give our students exposure to models of the
interaction between language and nonverbal behav-
iors such as eye gaze, head nods and hand gestures.
However, the available dialogue system toolkits ei-
ther have no graphical body or if they do have (part
of) a body?as in the case of the CSLU toolkit?the
toolkit does not allow the implementation of alterna-
tive models of body?language interaction.
We feel, therefore, that there is a need for a
toolkit that allows the beginning graduate student?
who may have some computer science or some lin-
guistics background, but not both?to implement a
working embodied dialogue system, as a way to ex-
periment with models of discourse, dialogue, collab-
orative conversation and the interaction between ver-
bal and nonverbal behavior in conversation. We be-
lieve the community as a whole must be engaged in
the design, implementation and fielding of this kind
of educational software. In this paper, we survey
the experience that has led us to these conclusions
and frame the broader discussion we hope the TNLP
workshop will help to further.
9
2 Our Courses
Our perspective in this paper draws on more than
fifteen course offerings at the graduate level in dis-
course and dialogue over the years. Justine Cassell?s
course Theories and Technologies of Human Com-
munication is documented on the web here:
http://www.soc.northwestern.edu/justine/discourse
Matthew Stone?s courses Natural Language Pro-
cessing and Meaning Machines1 are documented
here:
http://www.cs.rutgers.edu/?mdstone/class/533-spring-03/
http://www.cs.rutgers.edu/?mdstone/class/672
These courses are similar in perspective. All ad-
dress an extremely diverse and interdisciplinary au-
dience of students from computer science, linguis-
tics, cognitive science, information science, commu-
nication, and education. The typical student is a first
or second-year PhD student with a serious interest in
doing a dissertation on human-computer communi-
cation or in enriching their dissertation research with
results from the theory or practice of discourse and
dialogue. All are project courses, but no program-
ming is required; projects may involve evaluation of
existing implementations or the prospective design
of new implementations based on ongoing empir-
ical research. Nevertheless, the courses retain the
dual goals that students should not only understand
discourse and the theory of pragmatics, but should
also understand how the theory is implemented, ei-
ther well enough to talk intelligently about the im-
plementation or, if they are computer scientists, to
actually carry it out.
As befits our dual goals, our courses all involve
a mix of instruction in human-human dialogue and
human-computer dialogue. For example, Cassell be-
gins her course with a homework where students
collect, transcribe and analyze their own recordings
of face-to-face conversation. Students are asked to
discuss what constitutes a sufficient record of dis-
course, and to speculate on what the most challeng-
ing processing issues would be to allow a computer
to replace one of the participants. Computer sci-
entists definitely have difficulty with this aspect of
1The catchy title is the inspiration of Deb Roy at MIT.
the course?only fair, since they are at the advan-
tage when it comes to implementation. But com-
puter scientists see the value in the exercise: even
if they do not believe that interfaces should be de-
signed to act like people, they still recognize that
well-designed interactive systems must be ready to
handle the kinds of behaviors people actually carry
out. And hands-on experience convinces them that
behavior in human conversation is both rich and sur-
prising. The computer scientists agree?after turn-
ing in impoverished and uninformed ?analyses? of
their discourse for a brutal critique?that they will
never look at conversation the same way again.
Our experience suggests that we should be try-
ing to give students outside computer science the
same kind of eye-opening hands-on experience with
technology. For example, we have found that lin-
guists are just as challenged and excited by the dis-
cipline of technology as computer scientists are by
the discipline of empirical observations. Linguists
in our classes typically report that successful en-
gagement with technology ?exposes a lot of de-
tails that were missing from my theoretical under-
standing that I never would have considered with-
out working through the code?. Nothing is better at
bringing out the assumptions you bring to an anal-
ysis of human-human conversation than the thought
experiment of replacing one of the participants by
something that has to struggle consciously to un-
derstand it?a space alien, perhaps, or, more real-
istically, an AI system. We are frustrated that no
succinct assignment, comparable to our transcrip-
tion homework, yet exists that can reliably deliver
this insight to students outside computer science.
3 Framing the Problem
Our courses are not typical NLP classes. Our treat-
ment of parsing is marginal, and for the most part
we ignore the mainstays of statistical language pro-
cessing courses: the low-level technology such as
finite-state methods; the specific language process-
ing challenges for machine learning methods; and
?applied? subproblems like named entity extraction,
or phrase chunking. Our focus is almost exclu-
sively on high-level and interactional issues, such
as the structure of discourse and dialogue, informa-
tion structure, intentions, turn-taking, collaboration,
10
reference and clarification. Context is central, and
under that umbrella we explicitly discuss both the
perceptual environment in which conversation takes
place and the non-verbal actions that contribute to
the management of conversation and participants?
real-world collaborations.
Our unusual focus means that we can not readily
take advantage of software toolkits such as NLTK
(Loper and Bird, 2002) or Regulus (Rayner et al,
2003). These toolkits are great at helping students
implement and visualize the fundamentals of natu-
ral language processing?lexicon, morphology, syn-
tax. They make it easy to experiment with machine
learning or with specific models for a small scale,
short course assignment in a specific NLP module.
You can think of this as a ?horizontal? approach, al-
lowing students to systematically develop a compre-
hensive approach to a single processing task. But
what we need is a ?vertical? approach, which allows
students to follow a specific choice about the rep-
resentation of communicative behaviors or commu-
nicative functions all the way through an end-to-end
dialogue system. We have not succeeded in concep-
tualizing how a carefully modularized toolkit would
support this kind of student experience.
Still, we have not met with success with alterna-
tive approaches, either. As we describe in Section
3.1, our own research systems may allow the kinds
of experiments we want students to carry out. But
they demand too much expertise of students for a
one-semester course. In fact, as we describe in Sec-
tion 3.2, even broad research systems that come with
specific support for students to carry out a range of
tasks may not enable the specific directions that re-
ally turn students on to the challenge of discourse
and dialogue. However, our experience with im-
plementing dedicated modules for teaching, as de-
scribed in Section 3.3, is that the lack of synergy
with ongoing research can result in impoverished
tools that fail to engage students. We don?t have the
tools we want?but our experience argues that we
think the tools we really want will be developed only
through a collaborative effort shared across multiple
sites and broadly engaged with a range of research
issues as well as with pedagogical challenges.
3.1 Difficulties with REA and BEAT
Cassell has experimented with the use of her re-
search platforms REA (Cassell et al, 1999) and
BEAT (Cassell et al, 2001) for course projects in
discourse and dialogue. REA is an embodied con-
versational agent that interacts with a user in a real
estate agent domain. It includes an end-to-end dia-
logue architecture; it supports speech input, stereo
vision input, conversational process including pres-
ence and turn-taking, content planning, the context-
sensitive generation of communicative action and
the animated realization of multimodal communica-
tive actions. BEAT (the behavior expression anima-
tion toolkit), on the other hand, is a module that fits
into animation systems. It marks up text to describe
appropriate synchronized nonverbal behaviors and
speech to realize on a humanoid talking character.
In teaching dialogue at MIT, Cassell invited stu-
dents to adapt her existing REA and BEAT system
to explore aspects of the theory and practice of dis-
course and dialogue. This led to a range of interest-
ing projects. For example, students were able to ex-
plore hypothetical differences among characters?
from virtual ?Italians? with profuse gesture, to vir-
tual children whose marked use of a large gesture
space contrasted with typical adults, to characters
who showed new and interesting behavior such as
the repeated foot-tap of frustrated condescension.
However, we think we can serve students much bet-
ter. Many of these projects were accomplished only
with substantial help from the instructor and TAs,
who were already extremely familiar with the over-
all system. Students did not have time to learn how
to make these changes entirely on their own.
The foot-tapping agent is a good example of this.
To add foot-tapping is a paradigmatic ?vertical?
modification. It requires adding suitable context to
the discourse state to represent uncooperative user
behavior; it requires extending the process for gener-
ating communicative actions to detect this new state
and schedule an appropriate behavioral response;
and then it requires extending the animation plat-
form to be able to show this behavior. BEAT makes
the second step easy?as it should be?even for lin-
guistics students. To handle the first and third steps,
you would hope that an interdisciplinary team con-
taining a communication student and a computer sci-
11
ence student would be able to bring the expertise to
design the new dialogue state and the new animated
behavior. But that wasn?t exactly true. In order to
add the behavior to REA, students needed not only
background in the relevant technology?like what a
computer scientist would learn in a general human
animation class. To add the behavior, students also
needed to know how this technology was realized
in our particular research platform. This proved too
much for one semester.
We think this is a general problem with new re-
search systems. For example, we think many of the
same issues would arise in asking students to build a
dialogue system on top of the Trindi toolkit in a one
semester course.
3.2 Difficulties with the CSLU toolkit
In Fall 2004, Cassell experimented with using the
CSLU dialogue toolkit (Cole, 1999) as a resource
for class projects. This is a broad toolkit to support
research and teaching in spoken language technol-
ogy. A particular strength of the toolkit is its sup-
port for the design of finite-state dialogue models.
Even students outside computer science appreciated
the toolkit?s drag-and-drop interface for scripting di-
alogue flow. For example, with this interface, you
can add a repair sequence to a dialogue flow in one
easy step. However, the indirection the toolkit places
between students and the actual constructs of dia-
logue theory can by quite challenging. For example,
the finite-state architecture of the CSLU toolkit al-
lows students to look at floor management and at di-
alogue initiative only indirectly: specific transition
networks encode specific strategies for taking turns
or managing problem solving by scheduling specific
communicative functions and behaviors.
The way we see it, the CSLU toolkit is more heav-
ily geared towards the rapid construction of particu-
lar kinds of research prototypes than we would like
in a teaching toolkit. Its dialogue models provide an
instructive perspective on actions in discourse, one
that nicely complements the perspective of DAMSL
(Core and Allen, 1997) in seeing utterances as the
combined realization of a specific, constrained range
of communicative functions. But we would like to
be able to explore a range of other metaphors for
organizing the information in dialogue. We would
like students to be able to realize models of face-to-
face dialogue (Cassell et al, 2000), the information-
state approach to domain-independent practical di-
alogue (Larsson and Traum, 2000), or approaches
that emphasize the grounding of conversation in the
specifics of a particular ongoing collaboration (Rich
et al, 2001). The integration of a talking head into
the CSLU toolkit epitomizes these limitations with
the platform. The toolkit allows for the automatic
realization of text with an animated spoken deliv-
ery, but does not expose the model to programmers,
making it impossible for programmers adapt or con-
trol the behavior of the face and head.
We think this is a general problem with platforms
that are primarily designed to streamline a particular
research methodology. For example, we think many
of the same issues would arise in asking students to
build a multimodal behavior realization system on
top of a general-purpose speech synthesis platform
like Festival (Black and Taylor, 1997).
3.3 Difficulties with TAGLET
At this point, the right solution might seem to be
to devise resources explicitly for teaching. In fact,
Stone advocated more or less this at the 2002 TNLP
workshop (2002). There, Stone motivated the poten-
tial role for a simple lexicalized formalism for nat-
ural language syntax, semantics and pragmatics in
a broad NLP class whose emphasis is to introduce
topics of current research.
The system, TAGLET, is a context-free tree-
rewriting formalism, defined by the usual comple-
mentation operation and the simplest imaginable
modification operation. This formalism may in fact
be a good way to present computational linguistics
to technically-minded cognitive science students?
those rare students who come with interest and ex-
perience in the science of language as well as a solid
ability to program. By implementing a strong com-
petence TAGLET parser and generator students si-
multaneously get experience with central computer
science ideas?data structures, unification, recur-
sion and abstraction?and develop an effective start-
ing point for their own subsequent projects.
However, in retrospect, TAGLET does not serve
to introduce students outside computer science to the
distinctive insights that come from a computational
approach to language use. For one thing, to reach
a broad audience, it is a mistake to focus on repre-
12
sentations that programmers can easily build at the
expense of representations that other students can
easily understand. These other students need visu-
alization; they need to be able to see what the sys-
tem computes and how it computes it. Moreover,
these other students can tolerate substantial com-
plexity in the underlying algorithms if the system
can be understood clearly and mechanistically in ab-
stract terms. You wouldn?t ask a computer scientist
to implement a parser for full tree-adjoining gram-
mar but that doesn?t change the fact that it?s still a
perfectly natural, and comprehensible, algorithmic
abstraction for characterizing linguistic structure.
Another set of representations and algorithms
might avoid some of these problems. But a new
approach could not avoid another problem that we
think applies generally to platforms that are de-
signed exclusively for teaching: there is no synergy
with ongoing research efforts. Rich resources are so
crucial to any computational treatment of dialogue:
annotated corpora, wide-coverage grammars, plan-
recognizers, context models, and the rest. We can?t
afford to start from scratch. We have found this con-
cretely in our work. What got linguists involved in
the computational exploration of dialogue semantics
at Rutgers was not the special teaching resources
Stone created. It was hooking students up with the
systems that were being actively developed in ongo-
ing research (DeVault et al, 2005). These research
efforts made it practical to provide students with the
visualizations, task and context models, and interac-
tive architecture they needed to explore substantive
issues in dialogue semantics. Whatever we do will
have to closely connect teaching and our ongoing re-
search.
4 Looking ahead
Our experience teaching dialogue to interdisci-
plinary teams through toolkits has been humbling.
We have a new appreciation for the differences
between coursework and research infrastructure?
supporting teaching may be harder, because stu-
dents require a broader spectrum of implementa-
tion, a faster learning curve and the ability to ex-
plore mistaken ideas as well as promising ones.
But we increasingly think the community can and
should come together to foster more broadly useful
resources for teaching.
We have reframed our ongoing activities so that
we can find new synergies between research and
teaching. For example, we are currently working
to expand the repertoire of animated action in our
freely-available talking head RUTH (DeCarlo et al,
2004). In our next release, we expect to make dif-
ferent kinds of resources available than in the initial
release. Originally, we distributed only the model
we created. The next version will again provide that
model, along with a broader and more useful inven-
tory of facial expressions for it, but we also want
the new RUTH to be more easily extensible than the
last one. To do that, we have ported our model to a
general-purpose animation environment (Alias Re-
search?s Maya) and created software tools that can
output edited models into the collection of files that
RUTH needs to run. This helps achieve our ob-
jective of quickly-learned extensibility. We expect
that students with a background in human anima-
tion will bring experience with Maya to a dialogue
course. (Anyway, learning Maya is much more gen-
eral than learning RUTH!) Computer science stu-
dents will thus find it easier to assist a team of com-
munication and linguistics students in adding new
expressions to an animated character.
Creating such resources to span a general system
for face-to-face dialogue would be an enormous un-
dertaking. It could happen only with broad input
from those who teach discourse and dialogue, as we
do, through a mix of theory and practice. We hope
the TNLP workshop will spark this kind of process.
We close with the questions we?d like to consider
further. What kinds of classes on dialogue and dis-
course pragmatics are currently being offered? What
kinds of audiences do others reach, what goals do
they bring, and what do they teach them? What are
the scientific and technological principles that oth-
ers would use toolkits to teach and illustrate? In
short, what would your dialogue toolkit make possi-
ble? And how can we work together to realize both
our visions?
5 Acknowledgments
Thanks to Doug DeCarlo, NSF HLC 0308121.
13
References
Alan Black and Paul Taylor. 1997. Festi-
val speech synthesis system. Technical Report
HCRC/TR-83, Human Communication Research Cen-
ter. http://www.cstr.ed.ac.uk/projects/festival/.
J. Cassell, T. Bickmore, M. Billinghurst, L. Campbell,
K. Chang, H. Vilhja?lmsson, and H. Yan. 1999. Em-
bodiment in conversational characters: Rea. In CHI
99, pages 520?527.
Justine Cassell, Tim Bickmore, Lee Campbell, Hannes
Vilhjalmsson, and Hao Yan. 2000. Human conver-
sation as a system framework. In J. Cassell, J. Sul-
livan, S. Prevost, and E. Churchill, editors, Embod-
ied Conversational Agents, pages 29?63. MIT Press,
Cambridge, MA.
Justine Cassell, Hannes Vilhja?lmsson, and Tim Bick-
more. 2001. BEAT: the behavioral expression ani-
mation toolkit. In SIGGRAPH, pages 477?486.
Ron Cole. 1999. Tools for research and ed-
ucation in speech science. In Proceedings of
the International Conference of Phonetic Sciences.
http://cslu.cse.ogi.edu/toolkit/.
Mark G. Core and James F. Allen. 1997. Cod-
ing dialogs with the DAMSL annotation scheme.
In Working Notes of AAAI Fall Symposium on
Communicative Action in Humans and Machines.
http://www.cs.rochester.edu/research/cisd/resources/damsl/.
Douglas DeCarlo, Corey Revilla, Matthew Stone, and
Jennifer Venditti. 2004. Specifying and animating fa-
cial signals for discourse in embodied conversational
agents. Journal of Visualization and Computer Ani-
mation. http://www.cs.rutgers.edu/?village/ruth/.
David DeVault, Anubha Kothari, Natalia Kariaeva,
Iris Oved, and Matthew Stone. 2005. An
information-state approach to collaborative ref-
erence. In ACL Proceedings Companion Vol-
ume (interactive poster and demonstration track).
http://www.cs.rutgers.edu/?mdstone/pointers/collabref.html.
Staffan Larsson and David Traum. 2000. In-
formation state and dialogue management in
the TRINDI dialogue move engine toolkit.
Natural Language Engineering, 6:323?340.
http://www.ling.gu.se/projekt/trindi/.
Edward Loper and Steven Bird. 2002. NLTK: the natu-
ral language toolkit. In Proceedings of the ACL Work-
shop on Effective Tools and Methodologies for Teach-
ing Natural Language Processing and Computational
Linguistics. http://nltk.sourceforge.net.
Manny Rayner, Beth Ann Hockey, and John Dowd-
ing. 2003. An open source environment for com-
piling typed unification grammars into speech recog-
nisers. In Proceedings of the 10th Conference of the
European Chapter of the Association for Computa-
tion Linguistics (interactive poster and demo track).
http://sourceforge.net/projects/regulus.
C. Rich, C. L. Sidner, and N. Lesh. 2001. COL-
LAGEN: applying collaborative discourse theory to
human-computer interaction. AI Magazine, 22:15?25.
Matthew Stone. 2002. Lexicalized grammar 101.
In ACL Workshop on Effective Tools and Method-
ologies for Teaching NLP and CL, pages 76?83.
http://www.cs.rutgers.edu/?mdstone/class/taglet/.
14
Proceedings of the Third Workshop on Issues in Teaching Computational Linguistics (TeachCL-08), pages 129?136,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Support Collaboration by Teaching Fundamentals
Matthew Stone
Computer Science and Cognitive Science
Rutgers, The State University of New Jersey
110 Frelinghuysen Road, Piscataway NJ 08854-8019
Matthew.Stone@Rutgers.EDU
Abstract
This paper argues for teaching computer sci-
ence to linguists through a general course at
the introductory graduate level whose goal is
to prepare students of all backgrounds for col-
laborative computational research, especially
in the sciences. We describe our work over
the past three years in creating a model course
in the area, called Computational Thinking.
What makes this course distinctive is its com-
bined emphasis on the formulation and solu-
tion of computational problems, strategies for
interdisciplinary communication, and critical
thinking about computational explanations.
1 Introduction
The central long-term challenge of computational
linguistics is meaningfulness. I want to build situ-
ated, embodied interactive agents that can work with
people through language to achieve a shared under-
standing of the world. We have an increasing toolkit
to approach such problems. Linguistics gives us
powerful resources for representing utterance struc-
ture and interpretation, for example through the fam-
ily of formalisms and models that have grown up
around dynamic semantics and discourse representa-
tion theory. Supervised machine learning has proved
to be a profoundly successful software engineering
methodology for scaling representations and mod-
els from isolated instances to systematic wide cov-
erage. Nevertheless, talking robots are a long way
off. This is not a problem that is likely to be solved
by writing down a corpus of interpretations for sen-
tences (whatever that might mean) and training up
the right kind of synchronous grammar. Nor is
it likely to be solved by some one lone genius?
half Aravind Joshi, half Richard Stallman?driven
to learn and implement solo all of linguistics, artifi-
cial intelligence and cognitive science. Progress will
come through teamwork, as groups from disparate
backgrounds come together to share their discov-
eries, perspectives, and technical skills on concrete
projects of mutual interest. In the course such col-
laborations, I expect research to unlock fundamental
new insights about the nature of meaning, about its
dependence on perception, action, linguistic knowl-
edge and social relationships, and about the archi-
tecture of systems that can pick up on, create, and
generalize meanings in their experience. This pa-
per offers an interim summary of my reflections on
preparing the next generation of scientists for this
endeavor.
My efforts are anchored to the specific commu-
nity where I work. Semantics at Rutgers involves a
core group of eight faculty from linguistics, philoso-
phy and computer science, with a committed group
of about twice that many PhD students. That?s three
or four students a year: not much if you?re think-
ing of running a class for them, but pretty big if the
aim is to place graduates, as we successfully have
recently, in positions where they can continue to do
semantics (that is, in academic research and tenure-
track faculty jobs). Interdisciplinary interaction is
the norm for our group; it means that each seman-
tics project inevitably introduces the team to ques-
tions, concepts and methodologies that lie outside
the background expertise its members bring to the
project as individuals. My own work is a good ex-
129
ample: papers like (DeVault et al, 2006) or (Lep-
ore and Stone, 2007) reveal interconnections be-
tween computational ideas and philosophical analy-
sis that my colleagues and I discovered in attempting
to bridge our different perspectives on meaning and
meaningfulness.
In my experience, what makes it possible for these
efforts to take up sophisticated computational ideas
is not getting everyone up to speed with a spe-
cific programming environment or linguistic formal-
ism. The key step is to get outsiders to appreciate
the arguments that computer scientists make, and
why they make them. Jeannette Wing (2006) calls
this Computational Thinking. Wing argues that you
should have a course where you teach first-year col-
lege students to think like computer scientists. But
her arguments apply just as cogently to graduate stu-
dents in the sciences, and to linguists in particu-
lar. Computation as a framework for data collec-
tion, data analysis, inference, and explanation has
become the norm in the physical and life sciences,
and is rapidly transforming the behavioral sciences
and especially now the environmental sciences. The
situation is not so different in the cultural fields of
media, arts and entertainment either?as video game
designers are quick to remind us. A wide swath
of researchers in any university are now interested
in supporting exploratory and innovative interdisci-
plinary computing research, and specifically in train-
ing future faculty to pursue and mentor such collab-
orations. We decided to make common cause with
them at Rutgers, since computational linguistics is
such a small group. So our computer science depart-
ment offers a general course called Computational
Thinking at the introductory graduate level, aimed
at preparing researchers across fields to work on
collaborative projects involving computational re-
search. You have an opportunity to do the same.
2 Overview of the Course
We hold Computational Thinking in three hour
blocks once a week. This responds to Rutgers?s
quirky geography, with philosophy, linguistics and
computer science each on different campuses along
a five-mile stretch of the main local thoroughfare,
route 18. Elsewhere, it might make more sense to
meet in more frequent, shorter sessions.
Each meeting is divided so that students spend
about half of each lecture session (and half of
each week?s work) on technical material drawn
from the standard computer science curriculum.
As outlined in Section 2.1, the technical mate-
rial mixes programming practice, problem-solving
techniques and theoretical tools, and aims to pro-
vide the key elements that are needed to appre-
ciate the computational considerations of an inter-
disciplinary research project. The typical format
of these sessions is live interactive literate pro-
gramming. We work in Scheme, supported by
the DrScheme system available at http://www.plt-
scheme.org/software/drscheme/. I beam an image of
the Scheme development environment in front of the
class and write, run, analyze and debug short pro-
grams on the fly. Students follow along on their lap-
tops, doing exercises, asking questions, and seeing
results as they go.
The remainder of each class meeting (and the as-
sociated outside coursework) explicitly focuses on
the interpretive effort and people skills required to
reframe the ideas and methodologies of another field
in computational terms. Partly, as outlined in Sec-
tion 2.2, that involves developing a shared under-
standing of how computers accomplish the represen-
tation, processing and problem solving they do, so
that students become comfortable at viewing com-
putational systems abstractly as manipulating gen-
erative scientific models and knowledge. Funda-
mentally this understanding is what enables an in-
terdisciplinary team to reconcile the principles of an
outside field with the practice of computer science.
In addition, as outlined in Section 2.3, we offer ex-
plicit discussion of the conversational strategies and
interactive skills involved in bridging the different
perspectives of an interdisciplinary team, and over-
coming the divides of disjoint academic cultures, the
stresses of work and deadlines, and the many possi-
bilities for misunderstanding.
Homework mixes short benchmark problems,
which allow students to assess their progress against
objective standards, with open-ended collaborative
assignments that let students apply their domain ex-
pertise and challenge their developing skills and pro-
gramming, problem solving and teamwork. This
year students worked individually on a set of exer-
cises on list processing, matching of recursive struc-
130
tures, and interpreting programming languages de-
signed to give some general competence in Scheme.
Then they worked in teams of three to four to de-
velop a web site using DrScheme?s Scheme servlet
architecture. Finally, they explored the possibilities
for computational research in their home field in a
brief speculative paper.
The course has been offered twice, with about a
dozen students participating each session. Three or
four each year?the expected number?come from
linguistics and the philosophy of language. The
small numbers nevertheless add up. Already more
than half the students in this spring?s dissertation
reading group in the philosophy of language had
taken Computational Thinking. The group?s focus
was context, and the related problems of common
ground, presupposition, anaphora and accommoda-
tion. You could feel the difference Computational
Thinking made for many of the students, philoso-
phers included, who succeeded not only in framing
computational arguments about context and context
change, but also in synthesizing computational con-
cerns with philosophical ones in explaining linguis-
tic interpretation in terms of context.
2.1 Technical ideas
The technical goal of the course is to give stu-
dents greater facility in stating problems in compu-
tational terms and understanding and building so-
lutions to computational problems. The perspec-
tive aligns with the online textbook How to Design
Programs (Felleisen et al, 2001), which accompa-
nies the Dr Scheme distribution, but we emphasize
its continuity with the general mathematical prob-
lem solving that students have been doing since el-
ementary school (Polya, 1945). Indeed, following
Wing (2006), we see computational thinking as or-
dinary and pervasive. ?It?s not just the software and
hardware artifacts we produce that will be physically
present everywhere and touch our lives all the time,
it will be the computational concepts we use to ap-
proach and solve problems, manage our daily lives,
and communicate and interact with other people?
(Wing, 2006, p. 35).
On our view, the main challenge of learning to
think like a computer scientist?or to argue with
one?is the abstraction and flexibility you need.
For example, modern machine learning techniques
amount to finding a solution to a problem that is par-
tially specified in advance but partially determined
by empirical evidence that is available to the system
but not to the programmer. Thus we teach compu-
tational problem solving through case studies whose
input and output gets progressively more and more
abstract and remote from the programmer. The pro-
gression is suggested by the following examples,
which we cover either by developing solutions in in-
class literate programming demonstrations or by as-
signing them as programming projects.
? Answer a determinate mathematical question,
but one whose size or complexity invites the use
of an automatic tool in obtaining the results. The
sieve of Eratosthenes is a representative case: list
the prime numbers less than 100.
? Answer a mathematical question parameterized
by an arbitrary and potentially open-ended input.
Prototypical example: given a list of numbers de-
termine its maximum element.
? Answer a mathematical question where the in-
put needs to be understood as a generative, compo-
sitional representation. Given the abstract syntax of
a formula of propositional logic as a recursive list
structure and an interpretation assigning truth val-
ues to the atomic proposition letters, determine the
truth value of the whole formula.
? Answer a question where the input needs to
be understood as the specification of a computation,
and thus fundamentally similar in kind to the so-
lution. Write an interpreter for a simple program-
ming language (a functional language, like a frag-
ment of scheme; an imperative language involving
action and state; or a logical language involving the
construction of answer representations as in a pro-
duction rule shell).
? Answer a mathematical question where the out-
put may best be understood as the specification of a
computation, depending on input programs or data.
A familiar case is taking the derivative of an input
function, represented as a Scheme list. A richer
example that helps to suggest the optimization per-
spective of machine learning algorithms is Huffman
coding. Given a sequence of input symbols, come
up with programs that encode each symbol as a se-
quence of bits and decode bit sequences as symbol
sequences in such a way that the encoded sequence
131
is as short as possible.
? Answer a question where both input and output
need to be understood as generative compositional
representations with a computational interpretation.
Reinforcement learning epitomizes this case. Given
training data of a set of histories of action in the
world including traces of perceptual inputs, outputs
selected and reward achieved, compute a policy?
a suitable function from perception to action?that
acts to maximize expected reward if the environment
continues as patterned in the training data.
We go slowly, spending a couple weeks on each
case, and treat each case as an opportunity to teach a
range of important ideas. Students see several useful
data structures, including association lists (needed
for assignments of values to variables in logical for-
mulas and program environments), queues (as an
abstraction of data-dependent control in production
rules for example), and heaps (part of the infrastruc-
ture for Huffman coding). They get an introduction
to classic patterns for the design of functional pro-
grams, such as mapping a function over the elements
of a list, traversing a tree, accumulating results, and
writing helper functions. They get some basic the-
oretical tools for thinking about the results, such as
machine models of computation, the notion of com-
putability, and measures of asymptotic complexity.
Finally, they see lots of different kinds of represen-
tations through which Scheme programs can encode
knowledge about the world, including mathemati-
cal expressions, HTML pages, logical knowledge
bases, probabilistic models and of course Scheme
programs themselves.
The goal is to have enough examples that stu-
dents get a sense that it?s useful and powerful to
think about computation in a more abstract way.
Nevertheless, it?s clear that the abstraction involved
in these cases eventually becomes very difficult.
There?s no getting around this. When these stu-
dents are working successfully on interdisciplinary
teams, we don?t want them struggling across dis-
ciplines to encode specific facts on a case-by-case
basis. We want them to be working collaboratively
to design tools that will let team members express
themselves directly in computational terms and ex-
plore their own computational questions.
2.2 Interdisciplinary Readings
There is a rich literature in cognitive science which
reflects on representation and computation as expla-
nations of complex behavior. We read extensively
from this literature throughout the course. Engaging
with these primary sources helps students see how
their empirical expertise connects with the mathe-
matical principles that we?re covering in our techni-
cal sessions. It energizes our discussions of knowl-
edge, representation and algorithms with provoca-
tive examples of real-world processes and a dynamic
understanding of the scientific questions involved in
explaining these processes as computations.
For example, we read Newell and Simon?s fa-
mous discussions of knowledge and problem solv-
ing in intelligent behavior (Newell and Simon, 1976;
Newell, 1982). But Todd and Gigerenzer (2007)
have much better examples of heuristic problem
solving from real human behavior, and much bet-
ter arguments about how computational thinking and
empirical investigation must be combined together
to understand the problems that intelligent agents
have to solve in the real world. Indeed, students
should expect to do science to find out what repre-
sentations and computations the brain uses?that?s
why interdisciplinary teamwork is so important. We
read Gallistel?s survey (2008) to get a sense of the in-
creasing behavioral evidence from a range of species
for powerful and general computational mechanisms
in cognition. But we also read Brooks (1990) and his
critics, especially Kirsh (1991), as a reminder that
the final explanations may be surprising.
We also spend a fair amount of time consider-
ing how representations might be implemented in
intelligent hardware?whether that hardware takes
the form of silicon, neurons, or even the hydraulic
pipes, tinkertoys, dominoes and legos described by
Hillis (1999). Hardware examples like Agre?s net-
work models of prioritized argumentation for prob-
lem solving and decision making (1997) demystify
computation, and help to show why the knowledge
level or symbol level is just an abstract, functional
characterization of a system. Similarly, readings
from connectionism such as (Hinton et al, 1986)
dramatize the particular ways that network mod-
els of parallel representation and computation an-
ticipate possible explanations in cognitive neuro-
132
science. However, we also explore arguments that
symbolic representations, even in a finite brain, may
not be best thought of as a prewired inventory of fi-
nite possibilities (Pylyshyn, 1984). Computational
cognitive science like Hofstadter?s (1979)?which
emphasizes the creativity that inevitably accompa-
nies compositional representations and general com-
putational capacity?is particularly instructive. In
emphasizing the paradoxes of self-reference and the
generality of Turing machines, it tells a plausible
but challenging story that?s diametrically opposed to
the ?modular? Zeitgeist of domain-specific adaptive
cognitive mechanisms.
2.3 Communication
Another tack to motivate course material and keep
students engaged is to focus explicitly on interdis-
ciplinary collaboration as a goal and challenge for
work in the course. We read descriptions of more or
less successful interdisciplinary projects, such as Si-
mon?s description of Logic Theorist (1996) and Cas-
sell?s account of interdisciplinary work on embod-
ied conversational agents (2007). We try to find our
own generalizations about what allowed these teams
to work together as well as they did, and what we
could do differently.
In tandem, we survey social science research
about what allows diverse groups to succeed in
bridging their perspectives and communicating ef-
fectively with one another. Our sourcebook is Diffi-
cult Conversations (Stone et al, 1999), a guidebook
for conflict resolution developed by the Harvard Ne-
gotiation Project. It can be a bit awkward to teach
such personal material in a technical class, but many
students are fascinated to explore suggestions about
interaction that work just as well for roommates and
significant others as for interdisciplinary colleagues.
Anyway, the practices of Difficult Conversations do
fit with the broader themes of the class; they play out
directly in the joint projects and collaborative dis-
cussions that students must undertake to complete
the class.
I think it?s crucial to take collaboration seriously.
For many years, we offered a graduate computer sci-
ence course on computational linguistics as a first
interdisciplinary experience. We welcomed scien-
tists from the linguistics, philosophy and library and
information science departments, as well as engi-
neers from the computer science and electrical and
computer engineering departments, without expect-
ing anyone to bring any special background to the
course. Nevertheless, we encouraged both individ-
ualized projects and team projects, and worked to
support interdisciplinary teams in particular.
We were unsatisfied with this model based on its
results. We discovered that we hadn?t empowered
science students to contribute their expertise effec-
tively to joint projects, nor had we primed com-
puter science students to anticipate and welcome
their contributions. So joint projects found computer
scientists doing too much translating and not enough
enabling for their linguist partners. Linguists felt
like they weren?t pulling their weight or engaging
with the real issues in the field. Computer scientists
grew frustrated with the distance of their work from
specific practical problems.
Reading and reflecting on about generally-
accessible examples goes a long way to bridge the
divide. One case study that works well is the history
of Logic Theorist?the first implemented software
system in the history of AI, for building proofs in
the propositional logic of Whitehead and Russell?s
Principia Mathematica (1910). In 1955?56, when
Herb Simon, Allen Newell and Cliff Shaw wrote
it, they were actually an interdisciplinary team. Si-
mon was a social scientist trained at the Univer-
sity of Chicago, now a full professor of business,
at what seemed like the peak of a distinguished ca-
reer studying human decisions in the management
of organizations. Newell and Shaw were whiz-kid
hackers?Newell was a Carnegie Tech grad student
interested in software; Shaw was RAND corpora-
tion staff and a builder of prototype research com-
puters. Their work together is documented in two
fun chapters of Simon?s memoir Models of My Life
(1996). The story shows how computational col-
laboration demands modest but real technical exper-
tise and communication skills of all its practitioners.
Reading the story early on helps students appreciate
the goal of the computational thinking class from
the beginning: to instill these key shared concepts,
experiences, attitudes and practices, and thereby to
scaffold interdisciplinary technical research.
To work together, Simon, Newell and Shaw
needed to share a fairly specific understanding of
the concept of a representation (Newell and Si-
133
mon, 1976). Their work together consisted of tak-
ing knowledge about their domain and regiment-
ing it into formal structures and manipulations that
they could actually go on to implement. The frame-
work they developed for conceptualizing this pro-
cess rested on representations as symbolic struc-
tures: formal objects which they could understand
as invested with meaning and encoding knowledge,
but which they could also realize in computer sys-
tems and use to define concrete computational op-
erations. In effect, then, the concept of representa-
tion defined their project together, and they all had
to master it.
Simon, Newell and Shaw also needed a shared un-
derstanding of the computational methodology that
would integrate their different contributions into the
final program. Their work centered around the de-
velopment of a high-level programming language
that allowed Simon, Newell and Shaw to coordinate
their efforts together in a particularly transparent
way. Simon worked in the programming language,
using its abstract resources to specify formulas and
rules of inference in intuitive but precise terms; on
his own, he could think through the effects of these
programs. Newell and Shaw worked to build the
programming language, by developing the underly-
ing machinery to realize the abstract computations
that Simon was working with. The programming
language was a product of their effort together; its
features were negotiated based on Simon?s evolving
conceptual understanding of heuristic proof search
and Newell and Shaw?s engagement with the prac-
tical demands of implementation. The language is
in effect a fulcrum where both domain expertise and
computational constraints exercise their leverage on
one another. This perspective on language design
comes as a surprise both to scientists, who are used
to thinking of programming paradigms as remote
and arcane, and to computer scientists, who are used
to thinking of them solely in terms of their software
engineering patterns, but it remains extremely pow-
erful. To make it work, everyone involved in the re-
search has to understand how their judicious collab-
orative exploration of new techniques for specifica-
tion and programming can knit their work together.
In the course of developing their language, Si-
mon, Newell and Shaw also came to share a set
of principles for discussing the computational fea-
sibility of alternative design decisions. Proof, like
most useful computational processes, is most natu-
rally characterized as a search problem. Inevitably,
this meant that the development of Logic Theorist
ran up against the possibility of combinatorial explo-
sions and the need for heuristics and approximations
to overcome them. The solutions Simon, Newell and
Shaw developed reflected the team?s combined in-
sight in constructing representations for proof search
that made the right information explicit and afforded
the right symbolic manipulations. Many in the class,
especially computer scientists, will have seen such
ideas in introductory AI courses, so it?s challeng-
ing and exciting for them to engage with Simon?s
presentation of these ideas in their original interdis-
ciplinary context as new, computational principles
governing psychological explanations.
Finally?and crucially?this joint effort reflected
the team?s social engagement with each other, not
just their intellectual relationships. In their decades
of work together, Simon and Newell cultivated and
perfected a specific set of practices for engaging and
supporting each other in collaborative work. Simon
particularly emphasizes their practice of open dis-
cussion. Their talk didn?t always aim directly at
problem-solving or design. In the first instance, the
two just worked towards understanding?distilling
potential insights into mutually-satisfying formula-
tions. They put forward vague and speculative ideas,
and engaged with them constructively, not critically.
Simon?s memoirs also bring out the respect
the teammates had for each others? expertise and
work styles, especially when different?as Newell?s
brash, hands-on, late-night scheming was for
Simon?and the shared commitment they brought
to making their work together fun. Their good re-
lationship as people may have been just as impor-
tant to their success at interdisciplinary research as
the shared interests, ideas and intellectual techniques
they developed together.
These kinds of case studies allow students to
make sense of the goals and methods of the course
in advance of the technical and interpretive details.
Not much has changed since Logic Theorist. Effec-
tive computational teamwork still involves develop-
ing a conceptual toolbox that allows all participants
on the project to formulate precise representations
and engage with those representations in computa-
134
tional terms. And it still requires a more nuanced ap-
proach to communication, interaction and collabora-
tion than more homogeneous efforts?one focused
not just on solving problems and getting work done
but on fostering teammates? learning and commu-
nication, by addressing phenomena from multiple
perspectives, building shared vocabulary, and find-
ing shared values and satisfaction. These skills are
abstract and largely domain independent. The class
allows students to explore them.
3 Interim Assessment
The resources for creating our Computational
Thinking class came from the award of a train-
ing grant designed to crossfertilize vision research
between psychology and computer science. The
course has now become a general resource for our
cognitive science community. It attracts psychol-
ogists from across the cognitive areas, linguists,
philosophers, and information scientists. We also
make sure that there is a critical mass of computer
scientists to afford everyone meaningful collabora-
tive experiences across disciplines. For example,
participation is required for training grant partici-
pants from computer science, and other interdisci-
plinary projects invite their computer science stu-
dents to build community.
One sign of the success of the course is that stu-
dents take responsibility for shaping the course ma-
terial to facilitate their own joint projects. Our ini-
tial version of the course emphasized the technical
ideas and programming techniques described in Sec-
tion 2.1. Students asked for more opportunities for
collaboration; we added it right away in year one.
Students also asked for more reading and discussion
to get a sense of what computation brings to inter-
disciplinary research, and what it requires of it. We
added that in year two, providing much of the ma-
terials now summarized in Sections 2.2 and 2.3. In
general, we found concrete and creative discussions
aimed at an interdisciplinary audience more helpful
than the general philosophical statements that com-
puter scientists offer of the significance of computa-
tion as a methodology. We will continue to broaden
the reading list with down-to-earth materials cover-
ing rich examples.
From student feedback with the second running
of the class, the course could go further to get stu-
dents learning from each other and working together
early on. We plan to respond by giving an initial
pretest to get a sense of the skills students bring
to the class and pair people with partners of dif-
fering skills for an initial project. As always this
project will provide a setting where all students ac-
quire a core proficiency in thinking precisely about
processes and representations. But by connecting
more experienced programmers with novices from
the beginning, we hope to allow students to ramp up
quickly into hands-on exploration of specification,
program design and collaborative computational re-
search. Possible initial projects include writing a
production rule shell and using it to encode knowl-
edge in an application of identifying visual objects,
recognizing language structure, diagnosing causes
for observed phenomena or planning goal-directed
activity; or writing an interpreter to evaluate math-
ematical expressions and visualize the shapes of
mathematical objects or probabilistic state spaces.
Anecdotally, we can point to a number of cases
where Computational Thinking has empowered stu-
dents to leverage computational methods in their
own research. Students have written programs to
model experimental manipulations, analyze data, or
work through the consequences of a theory, where
otherwise they would have counted on pencil-and-
paper inference or an off-the-shelf tool. However, as
yet, we have only a preliminary sense of how well
the course is doing at its goal of promoting com-
putational research and collaboration in the cogni-
tive science community here. Next year we will get
our first detailed assessment, however, with the first
offering of a new follow-on course called ?Inter-
disciplinary Methods in Perceptual Science?. This
course explicitly requires students to team up in
extended projects that combine psychological and
computational methods for visual interaction. We
will be watching students? experience in the new
class closely to see whether our curriculum supports
them in developing the concepts, experiences, atti-
tudes and practices they need to work together.
4 Conclusion
Teamwork in computational linguistics often starts
by endowing machine learning methods with mod-
135
els or features informed by the principles and re-
sults of linguistic theory. Teams can also work
together to formalize linguistic knowledge and in-
terpretation for applications, through grammar de-
velopment and corpus annotation, in ways that fit
into larger system-building efforts. More generally,
we need to bridge the science of conversation and
software architecture to program interactive systems
that exhibit more natural linguistic behavior. And
we can even bring computation and linguistics to-
gether outside of system building: pursuing compu-
tational theories as an integral part of the explanation
of human linguistic knowledge and behavior.
To work on such teams, researchers do have to
master a range of specific intellectual connections.
But they need the fundamentals first. They have
to appreciate the exploratory nature of interdisci-
plinary research, and understand how such work can
be fostered by sharing representational insight, de-
signing new high-level languages and thinking crit-
ically about computation. Computational Thinking
is our attempt to teach the fundamentals directly.
You should be find it easy to make a case for this
course at your institution. In these days of declin-
ing enrollments and interdisciplinary fervor, most
departments will welcome a serious effort to culti-
vate the place of CS as a bridge discipline for re-
search projects across the university. Computational
Thinking is a means to get more students taking our
classes and drawing on our concepts and discoveries
to work more effectively with us! As the course sta-
bilizes, we plan to reach out to other departments
with ongoing computational collaborations, espe-
cially economics and the life and environmental sci-
ences departments. You could design the course
from the start for the full spectrum of computational
collaborations already underway at your university.
Acknowledgments
Supported by IGERT 0549115. Thanks to the stu-
dents in 198:503 and reviewers for the workshop.
References
Philip E. Agre. 1997. Computation and Human Experi-
ence. Cambridge.
Rodney A. Brooks. 1990. Elephants don?t play chess.
Robotics and Autonomous Systems, 6:3?15.
Justine Cassell. 2007. Body language: Lessons from
the near-human. In J. Riskin, editor, Genesis Redux:
Essays in the History and Philosophy of Artificial In-
telligence, pages 346?374. Chicago.
David DeVault, Iris Oved, and Matthew Stone. 2006. So-
cietal grounding is essential to meaningful language
use. In Proceedings of AAAI, pages 747?754.
Matthias Felleisen, Robert Bruce Findler, Matthew Flatt,
and Shriram Krishnamurthi. 2001. How to Design
Programs: An Introduction to Computing and Pro-
gramming. MIT.
C. R. Gallistel. 2008. Learning and representation. In
John H. Byrne, editor, Learning and Memory: A Com-
prehensive Reference. Elsevier.
W. Daniel Hillis. 1999. The Pattern on the Stone. Basic
Books.
Geoffrey E. Hinton, David E. Rumelhart, and James L.
McClelland. 1986. Distributed representations. In
Parallel Distributed Processing: Explorations in the
Microstructure of Cognition, Volume 1: Foundations,
pages 77?109. MIT.
Douglas Hofstadter. 1979. Go?del, Escher, Bach: An
Eternal Golden Braid. Basic Books.
David Kirsh. 1991. Today the earwig, tomorrow man?
Artificial Intelligence, pages 161?184.
Ernest Lepore and Matthew Stone. 2007. Logic and se-
mantic analysis. In Dale Jacquette, editor, Handbook
of the Philosophy of Logic, pages 173?204. Elsevier.
Allen Newell and Herbert A. Simon. 1976. Computer
science as empirical inquiry: Symbols and search.
Communications of the ACM, 19(3):113?126.
Allen Newell. 1982. The knowledge level. Artificial
Intelligence, 18:87?127.
G. Polya. 1945. How to Solve it. Princeton.
Zenon Pylyshyn. 1984. Computation and Cognition: To-
ward a Foundation for Cognitive Science. MIT.
Herbert A. Simon, 1996. Models of My Life, chapter
Roots of Artificial Intelligence and Artificial Intelli-
gence Achieved, pages 189?214. MIT.
Douglas Stone, Bruce Patton, and Sheila Heen. 1999.
Difficult Conversations: How to Discuss What Matters
Most. Penguin.
Peter M. Todd and Gerd Gigerenzer. 2007. Environ-
ments that make us smart: Ecological rationality. Cur-
rent Directions in Psych. Science, 16(3):170?174.
Alfred North Whitehead and Bertrand Russell. 1910.
Principia Mathematica Volume 1. Cambridge.
Jeannette M. Wing. 2006. Computational thinking.
Communications of the ACM, 49(3):33?35.
136
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 80?84,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Towards a Flexible Semantics:
Colour Terms in Collaborative Reference Tasks
Bert Baumgaertner
University of California, Davis
bbaum@ucdavis.edu
Raquel Ferna?ndez
University of Amsterdam
raquel.fernandez@uva.nl
Matthew Stone
Rutgers University
matthew.stone@rutgers.edu
Abstract
We report ongoing work on the development
of agents that can implicitly coordinate with
their partners in referential tasks, taking as a
case study colour terms. We describe algo-
rithms for generation and resolution of colour
descriptions and report results of experiments
on how humans use colour terms for reference
in production and comprehension.
1 Introduction
Speakers do not always share identical semantic rep-
resentations nor identical lexicons. For instance, a
subject may refer to a shape as a diamond while
another subject may call that same shape a square
(which just happens to be tilted sidewise); or some-
one may refer to a particular colour with ?light pink?
while a different speaker may refer to it as ?salmon?.
Regardless of these differences, which seem com-
mon place, speakers in dialogue are able to com-
municate successfully most of the time. Success-
ful communication exploits interlocutors? abilities to
negotiate referring expressions interactively through
grounding (Clark and Wilkes-Gibbs, 1986; Clark
and Schaefer, 1989), but in many cases interlocutors
can already make a good guess at their partners? in-
tentions by relaxing the interpretation of their utter-
ances and looking for the referent that best matches
this looser interpretation. We are interested in mod-
elling this second kind of behaviour computation-
ally, to get a better understanding of it and to con-
tribute to the development of dialogue systems that
are able to better coordinate with their human part-
ners.
In this paper we focus on collaborative referen-
tial tasks (akin to the classic matching tasks intro-
duced by Krauss and Weinheimer (1966) and Clark
and Wilkes-Gibbs (1986)) and take as a case study
colour terms. Our focus here is not on the explicit
joint negotiation of effective terms, but rather on the
deployment of flexible semantic representations that
can adapt to the constraints imposed by the context
and to the dialogue partner?s language use.
We start by describing our algorithms for genera-
tion and resolution of colour descriptions in the next
section. In sections 3 and 4, we present results of
experiments that investigate how humans use colour
terms for reference in production and comprehen-
sion. Section 5 compares our model against the ex-
perimental data we have collected so far and dis-
cusses some directions for future work. We end with
a short conclusion in section 6.
2 Reference to Colours: Our Model
Our view of how colour terms are used in referential
tasks follows the basic tenets of Gricean pragmat-
ics (Grice, 1975) and collaborative reference theo-
ries (Clark and Wilkes-Gibbs, 1986), according to
which speakers and addressees tend to maximize the
success of their joint task while minimizing costs.
In the domain of colour terms, we take this to
mean that speakers tend use a basic colour term (e.g.,
?red? or ?blue?) whenever this is enough to iden-
tify the target object and resort to an alternative,
more specific or complex term (e.g., ?bordeaux? or
?navy blue?) in other contexts where the basic term
is deemed insufficient. Non-basic terms can be con-
sidered more costly because they are less frequent
and thus more difficult to retrieve.
Similar ideas are at the core of models for the
generation of referring expressions that build on the
seminal work of Dale and Reiter (1995). These ap-
80
proaches, however, rely on a lexicon or database
where the properties of potential target objects are
associated with specific, predefined terms.1 Our aim
is to develop dialogue agents that employ more flex-
ible semantic representations, allowing them to (a)
refer to target colours with different terms in differ-
ent contexts, and (b) resolve the reference of colour
terms produced by the dialogue partner by picking
up targets that are not rigidly linked to the term in
the agent?s lexicon.
2.1 Algorithms
Data. To develop the generation and resolution al-
gorithms of our agent, we used a publicly avail-
able database of RGB codes and colour terms gen-
erated from a colour naming survey created by Ran-
dall Monroe (author of the webcomic xkcd.com)
and taken by around two hundred thousand par-
ticipants.2 This database contains a total of 954
colour terms (corresponding to the colour terms
most frequently used by the participants) paired with
a unique RGB code corresponding to the location in
the RGB colour space which was most frequently
named with the colour term in question.
We use this database as the default lexicon of our
agent. Amongst the colour terms in the lexicon,
we distinguish between basic and non-basic colours.
We selected the following as our basic colours: red,
purple, pink, magenta, brown, orange, yellow, green,
teal, blue, and grey. This selection takes into account
the high frequency of these terms in English and is in
line with the literature on basic colour terms (Berlin
and Kay, 1967; Berlin and Kay, 1991).
Resolution Algorithm. ALIN (ALgorithm for IN-
terpretation) is given as input a scene of coloured
squares and a colour term. Its output is the square it
takes to be the intended target, generated as follows.
Assuming the input term is in the lexicon, ALIN
compares every colour in the scene to the RGB value
of the input (the anchor). ALIN considers a colour
c the intended target if, (a) c is nearest the anchor
within a certain distance threshold, and (b) for any
other colour c? in the scene within the given distance
1See, however, van Deemter (2006) for an attempt to deal
with vague properties such as size within this framework.
2For further details visit http://blog.xkcd.com/
2010/05/03/color-survey-results/.
Figure 1: Two scenes with the brown square (top left in
both scenes) as the target; no competitors (left scene) and
one potential competitor (right scene).
threshold of the anchor, c? is far enough away from
both the anchor and c. We say more about distance
thresholds below.
Generation Algorithm. Unless there are competi-
tors (colours relatively close to the target), GENA
(GENeration Algorithm) is disposed to output a ba-
sic colour term if the target is acceptably close to a
basic colour (if not, it selects the default term asso-
ciated with the RGB code in the lexicon). In case
there are competitor colours in the scene, if the tar-
get is a basic colour, GENA will attempt to select a
non-basic colour term closest to the target but still
further away from the competitor(s). If the target is
not a basic colour, GENA simply selects the default
term in the lexicon.
Measuring Colour Distance. We treat colours
in our model as points in a conceptual space
(Ga?rdenfors, 2000; Ja?ger, 2009). As a first approx-
imation, we measure colour proximity in terms of
Euclidean distances between RGB values.3 Three
variables were used to set the thresholds required by
ALIN and GENA: i) bc is the maximum range to
search for basic colours; ii) min is the minimum dis-
tance required between two colours to be considered
minimally different; and iii) max is the maximum
range of allowable search for alternative colours. We
conducted two pilot studies to establish reasonable
values for these variables, which we then set as: bc
= 100; min = 25; max = 75.4
3 Experimental Methodology
We conducted two small experiments to collect data
about how speakers and addressees use colour terms
in referential tasks.
3We recognize Euclidean distances between RGB values as-
sumes colour space is uniform, which is not the case in human
vision (Wyszecki and Stiles, 2000). See section 5.
4RGB codes scaled at 0?255.
81
brow
n
choc
olate
 brow
n
dark 
brow
n
earth
y bro
wn
poop bro
wn
same
 as m
ud
basic colour w/o competitors
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
blueb
erry brow
n
choc
olate
 brow
n
colou
r of m
ud
dark 
brow
n
basic colour with competitors
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
dark 
pink
dusty
 rose mage
nta mauv
e pink red rose
rose 
pink salm
on
salm
on pi
nk
non-basic colour w/o competitors
0.0
0.1
0.2
0.3
0.4
0.5
brigh
t pink
dull l
ight f
uchs
ia
dull s
almo
n pin
k
dusty
 rose
light 
mauv
e
light 
pink light 
red
light 
salm
on
lighti
sh pi
nk
mage
nta mauv
e
medi
um p
ink
orang
ish p
ink
paste
l pink pink red rose rose 
pink salm
on
salm
on pi
nk
terra
 cotta
non-basic colour with competitors
0.0
0.1
0.2
0.3
0.4
0.5
Figure 2: Sample of results from ExpA, for a basic and a non-basic colour.
Materials & Setup. We created 12 different
scenes, each consisting of four solid coloured
squares, one of them the target (see Figure 1 for
sample scenes). Scenes were designed to take into
account two parameters: basic and non-basic target
colours, and without or with a competitor ? a colour
at a distance threshold from the target.5 The target
basic colours used were ?brown? and ?magenta? and
the non-basic ones, ?rose? and ?sea blue?.6 Each tar-
get colour appeared at least in one scene where there
were no competitors.
We run a generation experiment (ExpA) and a res-
olution experiment (ExpB). In ExpA, participants
were shown our 12 scenes and were asked to refer
to the target with a colour term that would allow a
potential addressee to identify it in the current con-
text, but without reference to the other colours in
the scene (to avoid comparatives such as ?the bluer
square?). In ExpB, participants were shown a scene
and a colour term and were asked to pick up the in-
tended referent. The colour terms used in this sec-
ond experiment were selected from those produced
in ExpA ? 29 scene-term pairs in total. Each scene
appeared at least twice, once with a term with high
occurrence frequency in ExpA, and once or twice
with one or two terms that had been produced with
low frequency. To minimize chances that subjects
recognize the same scene more than once, we ro-
tated and dispersed them evenly throughout.
5Any colour within a Euclidean distance of 125 from the
target was considered a competitor.
6Compositional phrases may introduce more sophisticated
effects. However, the data on which our lexicon is based ab-
stracted away from such details, treating them as simples.
Participants. A total of 36 native-English partici-
pants took part in the experiments: 19 in ExpA and
17 in ExpB. Subjects for both experiments included
undergraduate students, graduates students, and uni-
versity faculty. Both experiments were run online.
4 Experimental Results
ExpA Generation. ExpA revealed there is high
variability in the terms produced to refer to a sin-
gle colour. As expected, variability of terms gener-
ated for non-basic colours was higher than for ba-
sic colours. For non-basic colours, variability of
terms in scenes with competitors was higher. Fig-
ure 2 shows the different terms produced for a basic
colour (?brown?) and a non-basic colour (?rose?) in
scenes without and with competitors, together with
the proportional frequency of each term.
For the brown square target in a scene with-
out competitors, the basic-colour term ?brown? was
used with high frequency (72% of the time) while
any other terms were used 1 or 2 times only. In
scenes with competitors, ?dark brown? had high-
est frequency with ?brown? almost as much (43%
vs. 40%). For the rose square target in a scene with-
out competitors, there was also one term that stood
out as the most frequent, ?pink?, although its fre-
quency (30%) is substantially lower to that of the
basic-colour ?brown?. In scenes with competitors
there is an explosion in variation, with ?pink? still
standing out but only with a proportional frequency
of 21%.
Overall, ExpA showed that speakers attempt to
adapt their colour descriptions to the context and that
82
there is high variability in the terms they choose to
do this.
ExpB: Resolution. ExpB showed that reference
resolution is almost always successful despite the
variation in colour terms observed in ExpA. For the
basic colours in scenes with no competitors, partici-
pants successfully identified the targets in all cases,
while in scenes with competitors they did so 98%
of the time. This was the case for both terms with
proportionally high and low frequency.
For the non-basic colours in scenes with no com-
petitors, the success rate in identifying the target
was again 100% for both high and low frequency
terms. For scenes with competitors, there were dif-
ferences depending on the frequency of the terms
used: for high frequency terms there were once more
no resolution errors, while the resolution success
rate dropped to 78% where we used terms with low
proportional frequency scores. A summary of these
results is shown in Table 1, together with the success
rate of our resolution algorithm ALIN.
Basic Colours Non-basic Colours
high freq. low freq. high freq. low freq.
nc c nc c nc c nc c
ExpB 1 0.98 1 0.98 1 1 1 0.78
ALIN 1 0.71 1 0.71 0.5 1 0.75 0.71
Table 1: Resolution success rate by human participants
and ALIN in scenes without and with competitors (nc/c).
5 Discussion
The data we collected allows us to make informa-
tive comparisons between humans and our model in
collaborative reference tasks. Although we do not
believe the data is sufficient for an evaluation, the
comparison illuminates how the model can be re-
fined and the setup required for a proper evaluation.
Regarding resolution, we note that an algorithm
that rigidly associates colours and terms would have
successfully resolved only 4 of the 29 cases, 3 of
which were basic colours with no distractors ? a
7.25% success rate. In our scenarios with four po-
tential targets, a random algorithm would have an
average success rate of 25%. ALIN is closer to our
human data (see Table 1), though anomalies exist.
One problem is the lack of compositional semantics
in our current model. ALIN failed to resolve com-
plex phrases like ?dull salmon pink? and ?deep gray
blue?, which were terms produced by humans for
non-basic colours with competitors, simply because
the terms were not in the agent?s lexicon. Other
anomalies seem to be consequences of taking Eu-
clidean distances over RGB values, which may be
too crude. In the future, our intent is to convert RGB
values to Lab values and then use Delta-E values to
measure distances. First, however, we need a more
sophisticated analysis of the thresholds that we used
for ALIN and GENA.
As for generation, given the amount of variation
observed in the terms produced by our subjects, it is
not clear how human performance ought to be com-
pared to GENA?s. For instance, in scenes with com-
petitors, GENA produced ?reddish brown? for the
basic colour ?brown? and ?coral? for the non-basic
colour ?rose?. These did not appear in our human-
generated data but still seem to our lights reasonable
descriptions. GENA also produced ?gray? to refer to
?rose? in a different scene, which seems less appro-
priate and may be due to our current way of calcu-
lating colour distances and setting up the thresholds.
We believe that instead of comparing GENA?s
output to human output, it makes more sense to eval-
uate GENA by testing how well humans can resolve
terms produced by it. We intend to carry out this
evaluation in the future.
6 Conclusions
We have focused on the specific case of colours
where speakers differ in the referring expressions
they generate, but addressees are nevertheless able
to relax the interpretations of the expressions in or-
der to coordinate. We believe this implicit adapt-
ability is part of our semantic representation more
broadly. The case of colour provides us with a start-
ing point for studying and modelling computation-
ally this flexibility we possess.
Acknowledgements
This work has been partially supported by grant
632.002.001 from the Netherlands Organisation for
Scientic Research (NWO) and by grant IIS-1017811
from the National Science Foundation (NSF).
83
References
Brent Berlin and Paul Kay. 1967. Universality and evo-
lution of basic color terms. Laboratory for Language-
Behavior Research.
Brent Berlin and Paul Kay. 1991. Basic color terms:
Their universality and evolution. Univ of California
Pr.
Herbert H. Clark and Edward F. Schaefer. 1989. Con-
tributing to discourse. Cognitive Science, 13(2):259?
294.
Herbert H. Clark and Donna Wilkes-Gibbs. 1986. Refer-
ring as a collaborative process. Cognition, 22:1?39.
Robert Dale and Ehud Reiter. 1995. Computational in-
terpretations of the Gricean Maxims in the Generation
of Referring Expressions. Cognitive Science, 18:233?
266.
Peter Ga?rdenfors. 2000. Conceptual Spaces. MIT Press,
Cambridge.
Paul Grice. 1975. Logic and conversation. In D. David-
son and G. Harman, editors, The Logic of Grammar,
pages 64?75. Dickenson, Encino, California.
Gerhard Ja?ger. 2009. Natural color categories are convex
sets. In Logic, Language and Meaning: 17th Amster-
dam Colloquium, Amsterdam, The Netherlands, De-
cember 16-18, 2009, Revised Selected Papers, pages
11?20. Springer.
Robert Krauss and Sidney Weinheimer. 1966. Concur-
rent feedback, confirmation, and the encoding of refer-
ents in verbal communication. Journal of Personality
and Social Psychology, 4(3):343?346.
Kees van Deemter. 2006. Generating referring expres-
sions that involve gradable properties. Computational
Lingustics, 32(2):195?222.
Gu?nter Wyszecki and Walter S. Stiles. 2000. Color sci-
ence: concepts and methods, quantitative data and
formulae. Wiley Classics Library.
84
Language, Embodiment and Social Intelligence
Matthew Stone
Computer Science and Cognitive Science
Rutgers, The State University of New Jersey
110 Frelinghuysen Road, Piscataway NJ 08854-8019
Matthew.Stone@Rutgers.EDU
Abstract
It is an honor to have this chance to tie together
themes from my recent research, and to sketch
some challenges and opportunities for NLG in
face-to-face conversational interaction.
Communication reflects our general involvement
in one anothers? lives. Through the choices we man-
ifest with one another, we share our thoughts and
feelings, strengthen our relationships and further our
joint projects. We rely not only on words to artic-
ulate our perspectives, but also on a heterogeneous
array of accompanying efforts: embodied deixis, ex-
pressive movement, presentation of iconic imagery
and instrumental action in the world. Words show-
case the distinctive linguistic knowledge which hu-
man communication exploits. But people?s diverse
choices in conversation in fact come together to re-
veal multifaceted, interrelated meanings, in which
all our actions, verbal and nonverbal, fit the situation
and further social purposes. In the best case, they let
interlocutors understand not just each other?s words,
but each other.
As NLG researchers, I argue, we have good rea-
son to work towards models of social cognition that
embrace the breadth of conversation. Scientifically,
it connects us to an emerging consensus in favor of
a general human pragmatic competence, rooted in
capacities for engagement, coordination, shared in-
tentionality and extended relationships. Technically,
it lets us position ourselves as part of an emerging
revolution in integrative Artificial Intelligence, char-
acterized by research challenges like human?robot
interaction and the design of virtual humans, and
applications in assistive and educational technology
and interactive entertainment.
Researchers are already hard at work to place our
accounts of embodied action in conversation in con-
tact with pragmatic theories derived from text dis-
course and spoken dialogue. In my own experi-
ence, such work proves both illuminating and ex-
citing. For example, it challenges us to support and
refine theories of discourse coherence by accounting
for the discourse relations and default inference that
determine the joint interpretation of coverbal gesture
and its accompanying speech (Lascarides and Stone,
2008). And it challenges us to show how speak-
ers work across modalities to engage with, disam-
biguate, and (on acceptance) recapitulate each oth-
ers? communicative actions, to ground their mean-
ings (Lascarides and Stone, In Preparation). The
closer we look at conversation, the more we can fit
all its behaviors into a unitary framework?inviting
us to implement behavioral control for embodied so-
cial agents through a pervasive analogy to NLG.
We can already pursue such implementations eas-
ily. Computationally, motion is just sequence data,
and we can manipulate it in parallel ways to the
speech data we already use in spoken language gen-
eration (Stone et al, 2004). At a higher level, we
can represent an embodied performance through a
matrix of discrete actions selected and synchronized
to an abstract time-line, as in our RUTH system (De-
Carlo et al, 2004; Stone and Oh, 2008). This lets us
use any NLG method that manipulates structured se-
lections of discrete actions as an architecture for the
production of embodied behavior. Templates, as in
(Stone and DeCarlo, 2003; Stone et al, 2004), offer
5
a good illustration.
Nevertheless, face-to-face dialogue does demand
qualitatively new capabilities. In fact, people?s
choices and meanings in interactive conversation are
profoundly informed by their social settings. We
are a long way from general models that could al-
low NLG systems to recognize and exploit these
connections in the words and other behaviors they
use. In my experience, even the simplest social prac-
tices, such as interlocutors? cooperation on an on-
going practical task, require new models of linguis-
tic meaning and discourse context. For example,
systems must be creative to evoke the distinctions
that matter for their ongoing task, and use mean-
ings that are not programmed or learned but invented
on the fly (DeVault and Stone, 2004). They must
count on their interlocutors to recognize the back-
ground knowledge they presuppose by general infer-
ence from the logic of their behavior as a coopera-
tive contribution to the task (Thomason et al, 2006).
Such reasoning becomes particularly important in
problematic cases, such as when systems must fine-
tune the form and meaning of a clarification request
so that the response is more likely to resolve a pend-
ing task ambiguity (DeVault and Stone, 2007). I ex-
pect many further exciting developments in our un-
derstanding of meaning and interpretation as we en-
rich the social intelligence of NLG.
Modeling efforts will remain crucial to the explo-
ration of these new capabilities. When we build and
assemble models of actions and interpretations, we
get systems that can plan their own behavior simply
by exploiting what they know about communication.
These systems give new evidence about the informa-
tion and problem-solving that?s involved. The chal-
lenge is that these models must describe semantics
and pragmatics, as well as syntax and behavior. My
own slow progress (Cassell et al, 2000; Stone et al,
2003; Koller and Stone, 2007) shows that there?s
still lots of hard work needed to develop suitable
techniques. I keep going because of the method-
ological payoffs I see on the horizon. Modeling lets
us take social intelligence seriously as a general im-
plementation principle, and thus to aim for systems
whose multimodal behavior matches the flexibility
and coordination that distinguishes our own embod-
ied meanings. More generally, modeling replaces
programming with data fitting, and a good model of
action and interpretation in particular would let an
agent?s own experience in conversational interaction
determine the repertoire of behaviors and meanings
it uses to make itself understood.
Acknowledgments
To colleagues and coauthors, especially David DeVault
and the organizers of INLG 2008, and to NSF IGERT
0549115, CCF 0541185 and HSD 0624191.
References
J. Cassell, M. Stone, and H. Yan. 2000. Coordination
and context-dependence in the generation of embodied
conversation. In INLG, pages 171?178.
D. DeCarlo, M. Stone, C. Revilla, and J. J. Venditti.
2004. Specifying and animating facial signals for dis-
course in embodied conversational agents. Computer
Animation and Virtual Worlds, 15(1):27?38.
D. DeVault and M. Stone. 2004. Interpreting vague ut-
terances in context. In COLING, pages 1247?1253.
D. DeVault and M. Stone. 2007. Managing ambiguities
across utterances in dialogue. In DECALOG: Work-
shop on the Semantics and Pragmatics of Dialogue.
A. Koller and M. Stone. 2007. Sentence generation as a
planning problem. In ACL, pages 336?343.
A. Lascarides and M. Stone. 2008. Discourse coherence
and gesture interpretation. Ms, Edinburgh?Rutgers.
A. Lascarides and M. Stone. In Preparation. Grounding
and gesture. Ms, Edinburgh?Rutgers.
M. Stone and D. DeCarlo. 2003. Crafting the illusion
of meaning: Template-based generation of embodied
conversational behavior. In Computer Animation and
Social Agents (CASA), pages 11?16.
M. Stone and I. Oh. 2008. Modeling facial ex-
pression of uncertainty in conversational animation.
In I. Wachsmuth and G. Knoblich, editors, Model-
ing Communication with Robots and Virtual Humans,
pages 57?76. Springer.
M. Stone, C. Doran, B. Webber, T. Bleam, and M. Palmer.
2003. Microplanning with communicative inten-
tions: The SPUD system. Computational Intelligence,
19(4):311?381.
M. Stone, D. DeCarlo, I. Oh, C. Rodriguez, A. Stere,
A. Lees, and C. Bregler. 2004. Speaking with
hands: Creating animated conversational characters
from recordings of human performance. ACM Trans-
actions on Graphics, 23(3):506?513.
R. Thomason, M. Stone, and D. DeVault. 2006. Enlight-
ened update: a computational architecture for presup-
position accommodation. Ms, Michigan?Rutgers.
6
Proceedings of the SIGDIAL 2013 Conference, pages 31?40,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
Training an Integrated Sentence Planner on User Dialogue
Brian McMahan
Computer Science
Rutgers University
brian.mcmahan@rutgers.edu
Matthew Stone
Computer Science
Rutgers University
matthew.stone@rutgers.edu
Abstract
An appealing methodology for natural lan-
guage generation in dialogue systems is to
train the system to match a target corpus.
We show how users can provide such a
corpus as a natural side effect of interact-
ing with a prototype system, when the sys-
tem uses mixed-initiative interaction and a
reversible architecture to cover a domain
familiar to users. We experiment with
integrated problems of sentence planning
and realization in a referential communi-
cation task. Our model learns general and
context-sensitive patterns to choose de-
scriptive content, vocabulary, syntax and
function words, and improves string match
with user utterances to 85.8% from a hand-
crafted baseline of 54.4%.
1 Introduction
Natural language generation (NLG) in dialogue
involves a complex array of choices. It?s appeal-
ing to scale up NLG by training systems to make
these choices with models derived from empirical
data. Sometimes, these choices have a measurable
effect on the flow of the interaction. Systems can
plan such choices with a model of dialogue dy-
namics that predicts which utterances will fulfill
communicative goals successfully and efficiently
(Lemon, 2011; Janarthanam et al, 2011; Garoufi
and Koller, 2011).
Other times, a wide variety of utterances work
well (Belz and Gatt, 2008). In these cases, systems
can instead be designed simply to choose those ut-
terances that most closely resemble specified tar-
get behavior. This paper describes and evaluates
a new data-driven methodology for training sen-
tence planning and realization in interactive dia-
logue systems this way. Our work is particularly
inspired by Walker et al (2002), who train a di-
alogue sentence planner by annotating its possi-
ble outputs for quality; and Jordan and Walker
(2005), who train a referring expression generator
to match annotated human?human dialogue.
In text generation, researchers have been able
to exploit automatic analysis of existing resources
on such tasks as ordering words more naturally
(Langkilde and Knight, 1998) and identifying
named entities in line with attested mentions (Sid-
dharthan and Copestake, 2004). However, previ-
ous work on training dialogue generation has in-
volved the acquisition or annotation of relevant
data ad hoc, for example by collecting human?
human dialogue, running Wizard of Oz experi-
ments, or rating system outputs. Our work is dif-
ferent: we use a bootstrapping approach that auto-
matically mines interactions with a running proto-
type to adapt NLG to match users.
As described in Section 2, our work builds on
the COREF system of DeVault and Stone (2009).
COREF and its users chat together to identify
simple objects in a visual scene. COREF is de-
signed with reversible models of language and
dialogue?it tracks users? utterances and its own
utterances with the same data structures and rep-
resents them as updating the conversational state
in parallel ways. Because of this symmetry,
COREF?s understanding of each user utterance
determines an input?output pair that the system
could take as a target for NLG. We explain the sig-
nificance of learning from such data in Section 3.
However, we argue in Sections 4 and 5 that this
learning will yield significant results only if sys-
tem and user do in fact turn out to make similar
contributions to dialogue.
Our main experiment therefore uses data col-
lected with a new version of COREF with more
flexible strategies for taking initiative, as described
in Section 6. We use the system?s understand-
ing of user utterances in the experiment, along
with its productive capacity to generate alterna-
31
tive paraphrases of those utterances, to build an
automatically labeled training set of good and bad
NLG examples. We learn a model of the differ-
ence and evaluate its use in choosing novel utter-
ances. As documented in Section 7, the learned
model leads to improvements in naturalness over
COREF?s handcrafted baseline generator; our ex-
periments document these improvements qualita-
tively and quantitatively.
Our work suggests new ways to design dialogue
systems to adhere to formal models with guaran-
teed behavior (Paek and Pieraccini, 2008) while
reaping the benefits of data-driven approaches
(Rieser and Lemon, 2011) by improving them-
selves through ongoing interactions with users.
Our experiments suggest that engaging with user
expertise is a key factor in enabling such new de-
sign strategies. Our technique crucially exploits
synergies in our domain between the architecture
of the dialogue system, the specific dialogue pol-
icy that the system implements, and users? abilities
to contribute to domain problem solving.
2 Background
COREF, short for ?collaborative reference?, com-
municates with users through a text-chat window
for human?computer dialogue. A graphical inter-
face provides task context and realizes domain ac-
tions; it orchestrates a basic referential communi-
cation task like those studied by Clark and Wilkes-
Gibbs (1986) or Brennan and Clark (1996). In
each round of interaction, the participants in the
conversation are presented with a set of simple
geometric shapes that they must talk about; the
shapes are displayed on screen to human users
and described as a knowledge base to the COREF
agent. As the dialogue proceeds, one participant,
assigned to work as the director, gets an indication
of which object to describe next. The other partic-
ipant, assigned to work as the matcher, must move
this target object to its final disposition. Figure 1
is a snapshot of the interface in a session where the
user works as matcher. Experimental sessions nor-
mally involve multiple rounds where participants
alternate serving as director and as matcher.
COREF?s architecture factors its reasoning into
three integrative problem-solving modules, as
shown in Figure 2. The modules use different
algorithms and control flow, but are linked to-
gether by common representations and knowledge
bases. One shared resource is COREF?s prob-
Figure 1: User?s view of the chat interface in an
interaction with COREF acting as director.
abilistic context model, which tracks the likely
state of ongoing activity, maintains a linguistic
context describing what has probably been said
and what should be salient as a result, and repre-
sents the information available through the inter-
face as grounded in interlocutors? perception. An-
other shared resource is COREF?s tree-adjoining
grammar (TAG; Joshi and Schabes (1997)), which
assigns syntactic structures and semantic repre-
sentations to utterances, and predicts what utter-
ances will refer to in context and what dialogue
moves they will contribute. Finally, both under-
standing and generation use a common represen-
tation of the interpretation of utterances, utterance
plans, which associate specific strings of words
with the updates that they are predicted to achieve
via grammar and context.
The dialogue manager handles interaction with
the user, coordinates understanding and genera-
tion, tracks updates to the context, and selects up-
dates that COREF should contribute to the conver-
sation. In case of ambiguity, the dialogue man-
ager propagates uncertainty forward in time and
works to resolve it through interaction. (COREF
has general mechanisms for engaging in clarifica-
tion subdialogues.) In fact, by the time each ob-
ject has been identified, COREF has committed
retrospectively, in light of what has happened, to
a single most likely interpretation for everything
the user has said about it. COREF has evidence
that other interpretations it originally entertained
were not what the user intended. This links each
user utterance with a corresponding utterance plan
that can be used for subsequent learning (DeVault
and Stone, 2009).
The understanding module parses utterances us-
ing the grammar and resolves them using the con-
32
Figure 2: COREF system architecture, showing representations and knowledge shared across modules:
utterance plans show how each agent?s contributions follow from the system?s representations of gram-
mar and context; update rules map out consistent contextual effects for each agent?s contributions.
text model to recognize the possible utterance
plans behind them. The generator, meanwhile,
uses the grammar and the context model to syn-
thesize an utterance plan for a grammatical expres-
sion that is predicted to achieve some desired up-
dates unambiguously, as in SPUD (Stone et al,
2003). A range of choices are folded together
by this integrated problem-solving process. For
example, the grammar specifies alternative real-
izations involving different syntactic frames and
functional items, as in the paraphrases ?the target
is a square?, ?a square? and ?square?. The gram-
mar also specifies lexical paraphrases, as in the
equivalents ?dark blue? and ?navy blue? or ?beige?
and ?tan?. SPUD?s problem solving also creates
choices about how much descriptive content to in-
clude in a reference, as ?the square? versus ?the
blue square?, and what kind of descriptive content
to include, as in ?the blue square? versus ?the solid
square?. Full utterances involve all these choices,
potentially in overlapping combinations, as in ?the
target is the light brown object? versus ?the solid
square?. See the Appendix for examples of NLG
search, and DeVault (2008) for full details about
COREF?s design and implementation.
COREF?s handcrafted NLG search heuristics
draw on ideas from Stone et al (2003) and Dale
and Reiter (1995) to prioritize efficient, specific ut-
terances which use preferred descriptive attributes
and respect built-in preferences for certain words
and constructions. When we implemented these
heuristics, we had no intention of revising the
model using learning. However, COREF?s strat-
egy never generates human-like overspecification,
its lexical and syntactic choices are determined
by hand-coded logical constraints, and it offers
few tools to discriminate among comparable para-
phrases. In principle, a system like COREF ought
to be able to find out how people tend to make such
choices in interacting with it, and learn to speak
the same way. This is the central problem we ad-
dress in this paper.
3 Related Work
Our key contribution is demonstrating that a di-
alogue system can bootstrap an integrated NLG
strategy from interactions with a prototype system
by training a model to imitate user utterances. This
complements DeVault and Stone (2009), who train
an interpretation model in a similar way. Boot-
strapping NLG for dialogue requires new insights,
and require us to synthesize of a number of trends
in dialogue, in NLG and in social learning.
A number of researchers have trained genera-
tors for dialogue based on human specifications of
desired output. For example, Walker et al (2002)
and Stent et al (2004) optimize sentence plans
based on expert ratings of candidate output utter-
ances. Jordan and Walker (2005) learn rules for
predicting the content of referring expressions to
match patterns found in corpora of human descrip-
tions in context. Garoufi and Koller (2011) tune
the referential strategies of a general-purpose sen-
tence planner based on metrics of utterance effec-
tiveness mined from human?human interactions.
Our work involves a new domain and for the first
time involves integrated training of all these di-
mensions of NLG, but we draw closely on the ar-
chitectures, features and learning techniques de-
veloped by these researchers. The key difference
that they use data collected, and to some degree
hand-annotated, specifically to train NLG.
At the same time, a range of research has
explored the way existing data sets can im-
33
prove NLG results. For example, Langkilde and
Knight (1998) n-gram statistics to bias a non-
deterministic realization system towards frequent
utterances. Siddharthan and Copestake (2004) use
references in corpora to bootstrap a generator for
named entities in text. Such methods, however,
have generally focused on offline text generation
applications. Our research shows that specific in-
frastructure must be in place to tune NLG to a di-
alogue system?s own experience.
In addition, our work finds echoes in work
across AI on learning by imitation. Interactive
robots can learn in new ways by modeling their
behavior on competent humans (Breazeal et al,
2005). Other domains require agents to develop
cooperative relationships and elicit meaningful be-
havior from one another before they can learn to
act effectively together (Zinkevich et al, 2011).
Our work helps to establish the connections of
these ideas to dialogue.
Finally, we note that our work is orthogonal to
a range of other research that aims to extend and
improve NLG in dialogue through learning. Given
specified target utterances, knowledge acquisition
techniques can be used to induce new resources
that describe those utterances for NLG as well as
to optimize the use of those resources to match the
corpus (Higashinaka et al, 2006; DeVault et al,
2008). Moreover, given a model of the differen-
tial effects of utterances on the conversation, rein-
forcement learning can be used to identify utter-
ances with the best outcomes (Lemon, 2011; Ja-
narthanam et al, 2011). We see no reason not to
combine these techniques with imitation learning
in the development of future systems.
4 Training COREF
Our method for mining COREF?s dialogue experi-
ence involves three steps. First, we compile train-
ing data: positive instances are derived from user
utterances and negative instances are derived from
the generator?s alternative realizations of commu-
nicative goals inferred from user utterances. Next,
we build a machine learning model to distinguish
positive from negative instances, using features
describing the utterance itself, the current state of
the conversation and relevant facts from the dia-
logue history. Finally, we apply the learned model
on new NLG problems by collecting candidate
paraphrases and finding the one rated most likely
to be natural by the learned model.
4.1 Data Analysis
Each user utterance in COREF?s interaction logs
is associated with a particular state of the dialogue
and with the utterance plan ultimately identified
as its best interpretation. Our method extracts the
task moves in the utterance plan as candidate com-
municative goals for the utterance. It swaps the
role of the user and the system, so as to realize
an NLG problem instance to plan a contribution
with the utterance?s inferred communicative goals,
given the user?s role in the dialogue and their re-
constructed dialogue state. It then calls a revised
version of the generator that?s non-deterministic
and accumulates a range of plausible solutions.1
This process automatically creates a representa-
tion of the NLG problem faced by the user and the
set of possible solutions to that problem implic-
itly determined by COREF?s models of language
in context. Our method partitions the training in-
stances based on how the user chose to solve the
NLG problem. If the NLG output string matches
what the user actually said here, it becomes a pos-
itive training example. If it differs from what the
user actually said, it becomes a negative one.
4.2 Machine Learning
We can now build a machine learning model of
this data set. Given an unlabeled candidate solu-
tion to an NLG problem, we want to build a model
of the probability that the solution is representa-
tive of human behavior in our transcripts. We train
a maximum entropy model (Berger et al, 1996) to
make the prediction, using the MALLET software
package (McCallum, 2002). Given that the gener-
ator ultimately wants to choose the best utterance,
we could explore approaches to learn rankings di-
rectly, such as RankBoost (Freund et al, 2003).
Formally, the machine learning model charac-
terizes an input?output pair for NLG with a set of
features that would be available to a generator in
assessing a candidate output. Each training exam-
ple pairs an inventory of features with an observed
value indicating whether the instance does or does
not match the utterance produced by the human
user. Given a training set, MALLET selects a set
1Our specific approach was to capture all the successful
utterances that differ from the preferred NLG path by any
three derivation steps of the lexicalized generation grammar.
This heuristic was easy to implement with COREF?s existing
infrastructure for look-ahead search, and we found empiri-
cally that more comprehensive search was expensive to carry
out and tended primarily to add unnaturally verbose and re-
dundant utterances. See the Appendix for examples.
34
of features to use and fits numerical weights for
the features for logistic regression by maximum
entropy. That is, the features determine the pre-
dicted probability that candidate output j for prob-
lem t (utterance ut, j) is good (a match with a hy-
pothetical user utterance), as a logistic function
of the sum of the feature weights describing the
instance?formally,
P(ut, j = Good | features(ut, j)) =
1/(1+ exp(?w0??i features(ut, j)i ?wi))
This model can then be applied to unlabeled in-
stances with features derived from novel NLG
problem instances and candidate outputs.
The features we use in our experiments are de-
scribed in full in Tables 4 and 5 in the Appendix.
Most are from DeVault and Stone (2009). We have
features describing the form of the output utter-
ance: what phrase structure it has and what lexical
items are used. We have features describing what
task moves are achieved by the utterance and what
links the utterance has to context. For complete-
ness, we also add DeVault and Stone?s features
describing the context itself, including the conver-
sational tasks underway, the facts on the conversa-
tional record, and the properties relevant to ongo-
ing problem solving.2
In designing features for learning, we also draw
on the experience of Jordan and Walker (2005)
in predicting the form of referring expressions.
Many of their features closely align with those
we inherit from DeVault and Stone (2009). One
kind that doesn?t is Jordan and Walker?s concep-
tual pacts feature set. These features are de-
signed to capture utterance choices that are con-
tingent on other participants? previous choices
in interaction?entrainment (Brennan and Clark,
1996). We make it possible for the learner to de-
tect entrainment by introducing a new set of his-
tory features, which list the presuppositions of re-
cent utterances.
We do not need Jordan and Walker?s distrac-
tor features, however. Unlike them, we do not try
to learn the difference between distinguishing de-
scriptions and ambiguous ones. Our architecture,
2If these context features were shared across all outputs
for a given input, they would not affect what option for NLG
was best. But this is not always the case in COREF, because
contexts can be uncertain and because COREF can trigger ac-
commodation that changes the context as part of NLG. More-
over, including these features might allow us to capture pos-
sible variability in NLG, since the model can then predict that
otherwise marked utterances work naturally in some contexts.
like that of Garoufi and Koller (2011), doesn?t
even consider a candidate utterance unless it?s un-
ambiguous on a standard reference model (Dale
and Reiter, 1995). Garoufi and Koller (2011) pro-
vide evidence for the effectiveness of this kind of
factorization of modeling and learning.
4.3 Assessing the Model
To use the trained model, we start from the NLG
problem of generating an utterance to achieve
specified communicative goals in context. Our
NLG model constructs its space of candidate ut-
terances. Each candidate input?output pair is ana-
lyzed in terms of its features, and then the learned
model assigns it a probability score. We pick our
output via the candidate with the highest score.
In evaluating how well this works, we are in-
terested in how well the learned model predicts
the utterances of new subjects given data from
other subjects. We assess this by reporting cross-
validation results, predicting the choices of one,
held-out subject given a model trained on the data
from all other users in an experiment. We report
an exact match error measure. In a more complex
generation task, we could measure error based on
edit distance to give partial credit to NLG results
that are closer to user utterances. As a baseline, we
report comparable measures for COREF?s original
NLG implementation.
5 Pilot: The Need for Reciprocity
We applied our NLG training methodology to the
data set reported by DeVault and Stone (2009)
with 20 subjects interacting with COREF. The re-
sults were not compelling.
Analysis of this data set transforms human sub-
jects? utterances into 889 problem instances for
NLG. In 247 of these instances, the user?s utter-
ance is not in the NLG search space, usually be-
cause it is interpreted by robust methods rather
than COREF?s grammar. Of the remaining 642 ut-
terances, our baseline generator already matches
the user utterance 308 times (48%); it differs on
the other 334 instances (52%). After learning,
a model-based generator trained on the other 19
users? data now matches the utterance of a held-
out user on 546 instances (85%) across cross-
validation runs. This sounds promising, but in fact
almost all of the model successes (534 instances)
are due to just five utterance types that fulfill sim-
ple dialogue-management functions: ?yes?, ?no?,
35
?click continue?, ?done? and ?ok?.
There is in fact quite little evidence in this data
about how COREF should make its typical genera-
tion decisions. Looking under the hood, the prob-
lem is that COREF?s dialogue management pol-
icy did not exploit the symmetry and reciprocity
of its dialogue models and NL representations.
COREF took the initiative in object-identification
dialogues when it was the director, offering de-
scriptions of the target object, but it also took the
initiative when it was the matcher, asking the user
to confirm or reject its suggestions about the iden-
tity and properties of the target objects.
System builders often make such design choices
to foster task success. Giving the system the ini-
tiative generally means that user utterances are un-
derstood more reliably, which helps keep the di-
alogue on track. However, in settings where the
system can potentially improve its behavior, we
may have to design the system to take more risks
so it can acquire the data it needs; we may even
want to sacrifice short-term task success to enable
long-term improvement. Such trade-offs of explo-
ration and exploitation are endemic in reinforce-
ment learning, but learning by imitation gives the
problem a distinctively social dimension: getting
the right data may mean not only trying new ac-
tions in new situations, but actively creating the
right relationship with the user.
6 Collecting Mixed-initiative Data
We revised COREF?s dialogue strategy to better
reflect users? interactive competence using sim-
ple statistics about dialogue outcomes. For each
class of dialogue move by the agent in DeVault
and Stone?s evaluation data, we tabulated the num-
ber of subsequent utterances required to identify
the object. These measures give COREF?s planned
utterance an empirical score quantifying its antic-
ipated effect in dialogue. For example, after ask-
ing if a particular object was the target, the sub-
dialogue finished in 6.0 more turns on average.
Analogous measures give a comparable score to
the most effective kind of contribution that?s po-
tentially available to the user at each point in the
dialogue. For example, after saying that a particu-
lar object was the target, the subdialogue finished
in 3.2 more turns on average. Our new dialogue
policy compares COREF?s planned move with the
user?s best option. COREF proceeds with its ut-
terance if its score is better but waits for the user
if its score is worse. This analysis gives our re-
vised version of COREF an empirical threshold
for taking initiative in the dialogue based on the
strengths of the contributions COREF and the user
could make next in context. In practice, the re-
vised strategy lets user directors drive the dialogue
much more often than DeVault and Stone?s origi-
nal handcrafted policy. For example, COREF now
waits for the user to propose a description rather
than asking about a candidate object.
We had 42 subjects interact with the revised
COREF in a protocol of 29 object identification
tasks, grouped in blocks of 4, 9 and 16 as in De-
Vault and Stone (2009). Subjects were recruited
by advertisement and word of mouth from our in-
stitution and were paid for their participation. The
data was collected as part of an independently-
motivated assessment of COREF?s trade-offs be-
tween asking for clarification and proceeding un-
der uncertainty with its best interpretation, so
COREF varied these choices across the dialogues.
Analysis of our new data set induces 2006 NLG
problem instances corresponding to human utter-
ances, including 1382 cases where the user?s ut-
terance is (1) completely described by COREF?s
grammar, (2) found in the NLG search space, and
(3) represented as unambiguous by the underly-
ing NLG model. To confirm the diversity of utter-
ances in this set, we automatically partitioned the
utterances into four classes based on surface form
and communicative goals achieved: acknowledg-
ments that coordinate on the current state of the
dialogue (569 instances), task instructions (23 in-
stances), yes/no answers (434 instances) and other
dialogue contributions with explicit descriptive
content (356 instances). Thus, this data set con-
tains substantial evidence about human strategies
in COREF?s domain. We continue to perform
analyses of utterances by category to document the
results of our learning experiment.
7 Results
Table 1 compares the aggregate performance
of the learned NLG module in comparison to
COREF?s baseline generator across all cross-
validation runs (training on 41 users and testing on
data from one held-out user). Except in the small
category of task instructions, where the baseline is
already good, the learned model offers a substan-
tial improvement in rate of exact match to user ut-
terance across all categories. These differences in
36
Table 1: Comparison of learned model and baseline generator.
System Descriptive Acknowledgments Yes/No Instructions Total
Baseline 170356 = 47.8%
349
569 = 61.3%
210
434 = 48.4%
23
23 = 100%
752
1382 = 54.4%
Model 259356 = 72.8%
477
569 = 83.8%
427
434 = 98.4%
23
23 = 100%
1186
1382 = 85.8%
Evaluation of exact match to user utterances across hold-one-user-out cross-validation runs. We report
number of matching instances out of number of instances with the user utterance in the NLG search
space, along with percentage match, broken down by form and communicative goal of the utterance.
Table 2: Comparison of accuracy by item.
Baseline
Model
Match Mismatch
Match 720 466
Mismatch 32 164
(a) Counts of NLG problem instances of all types,
comparing matches in the baseline generator
against matches in the learned model.
Baseline
Model
Match Mismatch
Match 152 107
Mismatch 18 79
(b) Counts of NLG problem instances with sub-
stantive contributions and explicit descriptive ma-
terial, comparing matches in the baseline genera-
tor against matches in the learned model.
rates are all statistically significant (p < .005 by
Fisher?s exact test).
Table 2 breaks down overall results (Table 2a)
and results on descriptive utterances (Table 2b), to
explore associations between the performance of
the baseline generator and the performance of the
learned model on individual items. We find a clear
link between the two methods: when the model
gets an utterance wrong, the baseline method is
much more likely to have gotten the utterance
wrong as well (p < .001 by Fisher?s exact test).
We conclude that the model is not just improv-
ing on the baseline generator in aggregate, but has
learned to correct specific choices in the baseline
system that are not representative of user behavior.
The breakdown in Table 1 gives a sense of the
range of cases covered by the learned model. The
?yes/no? cases mostly involve training COREF to
say ?yes? rather than ?yeah?. The acknowledg-
ment cases involve understanding the subtle ways
that people trade off alternatives such as ?ok?,
?done? and ?I added it??a difficult problem but
one where we have little choice but to trust ma-
chine learning results.
Descriptive utterances are more substantial. To
understand these cases better, we built an overall
model with data from all 42 users and looked at the
features selected by MALLET and the weights fit
for them in the maximum entropy model. Table 3
shows a sample of the MALLET output. We think
of these features as establishing a network of prior-
itized defaults; lower-weighted features must con-
spire together to override higher-weighted ones.
Syntax is the strongest effect; for example, the
contrast between [S DET N] and [S NP IS DET N]
gives a preference of 1.27 to the simpler struc-
ture. Lexical features encode more natural items
(?brown? versus ?beige?) but also implicitly en-
code natural descriptive patterns (as with the color
modifier ?light?). Presupposition features, mean-
while, help ensure that words have their most natu-
ral meanings. On this analysis, the model contents
corroborate our hypothesis that user data gives ev-
idence to refine a wide variety of NLG choices.
8 Discussion
In this paper, we show how users? utterances can
give a dialogue system consistent and reliable in-
dicators not only of how to solve its NLU prob-
lems, as in DeVault and Stone (2009), but also
how to solve its NLG problems. Thus, we can
now design dialogue systems to learn to imitate
their human users in certain cases. To do so, the
system needs to work in a domain where users are
prepared to offer the same kind of contributions
37
Table 3: Sample features used to identify user tu-
ples and their weights in an overall model.
Syntax Features:
Fits [S DET N] 2.29
Fits [S COLOR N] 2.09
Fits [S DET COLOR N] 1.86
Fits [S NP IS DET N] 1.12
Lexical Features:
Includes word light 0.87
Includes word dark 0.60
Includes word brown 0.22
Includes word beige 0.005
Presupposition Features:
Uses square for square object 2.05
Uses diamond for rhombus 2.09
Uses pink for pale red-purple 1.70
Describes light blue as light 0.92
as the system, the system needs to represent those
contributions symmetrically, and the system needs
to be able to actually elicit, analyze and learn from
relevant user utterances.
Our approach, like that of Garoufi and Koller
(2011), is to combine a symbolic account of ut-
terance interpretation with a learned model of ut-
terance quality. Thus, on our approach, system
utterances always come with formal guarantees
that they fulfill specified communicative goals and
have a unique interpretation in context. That may
help underwrite the guarantees that Paek and Pier-
accini (2008) emphasize, that data-driven systems
must respect the coherence of dialogue and must
continue to do so even as they learn to improve
dialogue efficiency and naturalness.
Our work suggests some natural followups. It
would be interesting to refine the NLG model
based on the disambiguation strategy learned in
DeVault and Stone (2009). If the system discov-
ers that utterances are not as ambiguous as the ini-
tial model suggests, it opens up new possibilities
for tuning NLG to match what users say. Scal-
ing up the ideas, meanwhile, invites us to build
factored models that describe NLG decisions in a
more compositional way, as well as finding more
powerful and generalizable features.
Further work is also required to use these tech-
niques in a broader range of settings. Our tech-
nique requires the system to give users the op-
portunity to say the same kinds of things it says,
so it is most appropriate for collaborative prob-
lem solving. Further research is required to use
the methodology for asymmetric situations such
as information seeking. Use in spoken dialogue
systems, meanwhile, would challenge the limits
of mixed-initiative interaction and would require
techniques to discount users? errors and disfluen-
cies. Although these limitations make our tech-
niques difficult to use in many current applica-
tions, we are optimistic that our methods will
apply quite naturally to emerging open-domain
settings such as human?robot interaction, where
users and systems meet on a more equal footing.
Acknowledgments
This authors were supported by NSF DGE
0549115 (IGERT) and IIS 1017811. Thanks to the
reviewers, and to David DeVault and Lara Martin
for discussion and assistance.
References
Anja Belz and Albert Gatt. 2008. Intrinsic vs. extrinsic
evaluation measures for referring expression gener-
ation. In Proceedings of ACL, pages 197?200.
Adam L. Berger, Stephen Della Pietra, and Vincent
J. Della Pietra. 1996. A maximum entropy ap-
proach to natural language processing. Computa-
tional Linguistics, 22(1):39?71.
Cynthia Breazeal, Daphna Buchsbaum, Jesse Gray, and
Bruce Blumberg. 2005. Learning from and about
others: Towards using imitation to bootstrap the so-
cial understanding of others by robots. Artificial
Life, 11(1?2):1?32.
Susan E. Brennan and Herbert H. Clark. 1996. Con-
ceptual pacts and lexical choice in conversation. J.
Experimental Psychology, 22(6):1482?1493.
Herbert H. Clark and Deanna Wilkes-Gibbs. 1986.
Referring as a collaborative process. Cognition,
22(1):1?39.
Robert Dale and Ehud Reiter. 1995. Computational
interpretations of the Gricean maxims in the gener-
ation of referring expressions. Cognitive Science,
18:233?263.
David DeVault and Matthew Stone. 2009. Learning to
interpret utterances using dialogue history. In Pro-
ceedings of EACL, pages 184?192.
David DeVault, David Traum, and Ron Artstein. 2008.
Practical grammar-based NLG from examples. In
Proceedings of INLG, pages 78?85.
David DeVault. 2008. Contribution Tracking: Par-
ticipating in Task-Oriented Dialogue under Uncer-
tainty. Ph.D. thesis, Rutgers.
38
Yoav Freund, Raj Iyer, Robert E. Schapire, and Yoram
Singer. 2003. An efficient boosting algorithm for
combining preferences. J. Machine Learning Re-
search, 4:933?969.
Konstantina Garoufi and Alexander Koller. 2011.
Combining symbolic and corpus-based approaches
for the generation of successful referring expres-
sions. In Proceedings of EWNLG, pages 121?131.
Ryuichiro Higashinaka, Rashmi Prasad, and Mari-
lyn A. Walker. 2006. Learning to generate natu-
ralistic utterances using reviews in spoken dialogue
systems. In Proceedings of ICCL?ACL, pages 265?
272.
Srinivasan Janarthanam, Helen Hastie, Oliver Lemon,
and Xingkun Liu. 2011. ?The day after the day after
tomorrow?? a machine learning approach to adap-
tive temporal expression generation: training and
evaluation with real users. In Proceedings of SIG-
DIAL, pages 142?151.
Pamela W. Jordan and Marilyn A. Walker. 2005.
Learning content selection rules for generating ob-
ject descriptions in dialogue. J. Artif. Intell. Res.
(JAIR), 24:157?194.
Aravind K. Joshi and Yves Schabes. 1997. Tree-
adjoining grammars. In Grzegorz Rozenberg and
Arto Salomaa, editors, Handbook of Formal Lan-
guages, pages 69?123. Springer.
Irene Langkilde and Kevin Knight. 1998. Generation
that exploits corpus-based statistical knowledge. In
Proceedings of COLING?ACL, pages 704?710.
Oliver Lemon. 2011. Learning what to say and how to
say it: Joint optimisation of spoken dialogue man-
agement and natural language generation. Com-
puter Speech & Language, 25(2):210?221.
Andrew McCallum. 2002. MALLET: A
MAchine learning for LanguagE toolkit.
http://mallet.cs.umass.edu.
Tim Paek and Roberto Pieraccini. 2008. Automating
spoken dialogue management design using machine
learning: An industry perspective. Speech Commu-
nication, 50(8?9):716?729.
Verena Rieser and Oliver Lemon. 2011. Reinforce-
ment Learning for Adaptive Dialogue Systems: A
Data-driven Methodology for Dialogue Manage-
ment and Natural Language Generation. Springer.
Advaith Siddharthan and Ann A. Copestake. 2004.
Generating referring expressions in open domains.
In Proceedings of ACL, pages 407?414.
Amanda Stent, Rashmi Prasad, and Marilyn A. Walker.
2004. Trainable sentence planning for complex in-
formation presentations in spoken dialog systems.
In Proceedings of ACL, pages 79?86.
Matthew Stone, Christine Doran, Bonnie Webber, To-
nia Bleam, and Martha Palmer. 2003. Microplan-
ning with communicative intentions: the SPUD sys-
tem. Computational Intelligence, 19(4):314?381.
Marilyn A. Walker, Owen Rambow, and Monica Ro-
gati. 2002. Training a sentence planner for spoken
dialogue using boosting. Computer Speech & Lan-
guage, 16(3?4):409?433.
Martin Zinkevich, Michael H. Bowling, and Michael
Wunder. 2011. The lemonade stand game com-
petition: solving unsolvable games. SIGecom Ex-
changes, 10(1):35?38.
Appendix: NLG Search and Features
User utterance pink square
Goal(s) found 1. Target is pink
2. Target is square, or
3. Target is both pink and square
Baseline 1. the target is pink
2. the target is square
3. pink square
Model 1. pink square
2. square
3. pink square
Candidates a box, a fuschia box, a fuschia
fuschia box, a fuschia fuschia
square, a fuschia pink box,
a fuschia pink square, a fuschia
purple box, a fuschia purple
square, a fuschia square, a like
fuschia box, a like fuschia square,
a like pink box, a like pink
square, a like purple box, a like
purple square, a pink box, a pink
fuschia box, a pink fuschia
square, a pink pink box, a pink
pink square, a pink purple box,
a pink purple square, a pink
square, a purple box, a purple
fuschia box, a purple fuschia
square, a purple pink box,
a purple pink square, a purple
purple box, a purple purple
square, a purple square, a square,
box, fuschia box, fuschia square,
pink box, pink square, purple
box, purple square, square, the
target is fuschia, the target is
pink, the target is purple, the
target is square
Model confirms baseline vocabulary, learns to
overspecify color goal (1) for more natural syn-
tax. COREF can?t spell ?fuchsia?.
39
Table 4: Features derived from the current state of the dialogue (st).
feature set description
NumTasksUnderway The number of tasks underway in the state st .
TasksUnderway
For any task that is underway in state st , a feature includes its
name, its depth on the task stack, and its current status in its
formal task network.
NumRemainingReferents The number of targets that remain to be identified in state st .
TabulatedFacts
For any fact on the conversational record at state st there is a
corresponding string feature?a formula with any unique ref-
erence symbols anonymized (e.g. X34 becomes some-object).
CurrentTargetConstraints For any positive or negative constraint on the current target instate st , there is a corresponding string feature.
UsefulProperties For any property instantiated in the display in state st there is acorresponding feature.
History Each assertion and presupposition on the conversational recordin state st is represented as a string feature.
Table 5: Features derived from the proposed utterance (ut, j).
feature set description
Presuppositions
Each of the atomic presuppositions of the utterance ut, j is rep-
resented as a string feature. The string captures predicate?
argument structure but anonymizes references to individuals
(e.g. target12 becomes sometarget).
Assertions
Each of the dialogue moves that the utterance contributes cor-
responds to a feature. This string also captures predicate?
argument structure but anonymizes references to individuals.
Syntax A string representation of the bracketed phrase structure, in-cluding non-terminal categories, of the utterance.
Words We represent each word that occurs in the utterance as a fea-ture.
User utterance the light blue diamond
Goal(s) found Target is specified object
Baseline the blue object
Model the light blue diamond
Candidates the blue blue diamond,
the blue blue object, the blue
blue rhombus, the blue
diamond, the blue diamond
outline, the blue object,
the blue object outline,
the blue rhombus, the blue
rhombus outline, the empty
blue diamond, the empty blue
object, the empty blue
rhombus, the hollow blue
diamond, the hollow blue
object, the hollow blue
rhombus, (continued)
Candidates the light blue diamond,
the light blue object, the light
blue rhombus, the lighter blue
diamond, the lighter blue
object, the lighter blue
rhombus, the like blue
diamond, the like blue object,
the like blue rhombus,
the outline blue diamond,
the outline blue object,
the outline blue rhombus,
the sky blue diamond, the sky
blue object, the sky blue
rhombus
Model confirms baseline pattern of color and type
reference but learns to overspecify color as light
blue and to use basic type diamond.
40
