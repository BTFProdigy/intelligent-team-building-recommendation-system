Determining the Sentiment of Opinions 
Soo-Min Kim 
Information Sciences Institute 
University of Southern California 
4676 Admiralty Way 
Marina del Rey, CA 90292-6695 
skim@isi.edu 
Eduard Hovy 
Information Sciences Institute 
University of Southern California 
4676 Admiralty Way 
Marina del Rey, CA 90292-6695 
hovy@isi.edu 
 
Abstract 
Identifying sentiments (the affective parts 
of opinions) is a challenging problem.  We 
present a system that, given a topic, 
automatically finds the people who hold 
opinions about that topic and the sentiment 
of each opinion.  The system contains a 
module for determining word sentiment 
and another for combining sentiments 
within a sentence.  We experiment with 
various models of classifying and 
combining sentiment at word and sentence 
levels, with promising results.   
1 Introduction 
What is an opinion?   
The many opinions on opinions are reflected 
in a considerable literature (Aristotle 1954; 
Perelman 1970; Toulmin et al 1979; Wallace 
1975; Toulmin 2003).  Recent computational 
work either focuses on sentence ?subjectivity? 
(Wiebe et al 2002; Riloff et al 2003), 
concentrates just on explicit statements of 
evaluation, such as of films (Turney 2002; Pang 
et al 2002),  or focuses on just one aspect of 
opinion, e.g., (Hatzivassiloglou and McKeown 
1997) on adjectives.  We wish to study opinion 
in general; our work most closely resembles 
that of (Yu and Hatzivassiloglou 2003).   
Since an analytic definition of opinion is 
probably impossible anyway, we will not 
summarize past discussion or try to define 
formally what is and what is not an opinion.  
For our purposes, we describe an opinion as a 
quadruple [Topic, Holder, Claim, Sentiment] in 
which the Holder believes a Claim about the 
Topic, and in many cases associates a 
Sentiment, such as good or bad, with the belief.  
For example, the following opinions contain 
Claims but no Sentiments:  
?I believe the world is flat?  
?The Gap is likely to go bankrupt? 
?Bin Laden is hiding in Pakistan?  
?Water always flushes anti-clockwise in 
the southern hemisphere?  
Like Yu and Hatzivassiloglou (2003), we 
want to automatically identify Sentiments, 
which in this work we define as an explicit or 
implicit expression in text of the Holder?s 
positive, negative, or neutral regard toward the 
Claim about the Topic.  (Other sentiments we 
plan to study later.)  Sentiments always involve 
the Holder?s emotions or desires, and may be 
present explicitly or only implicitly:  
 ?I think that attacking Iraq would put the 
US in a difficult position? (implicit)  
?The US attack on Iraq is wrong? 
(explicit)  
?I like Ike? (explicit) 
?We should decrease our dependence on 
oil? (implicit)  
 ?Reps. Tom Petri and William F. 
Goodling asserted that counting illegal aliens 
violates citizens? basic right to equal 
representation?  (implicit)  
In this paper we address the following 
challenge problem.  Given a Topic (e.g., 
?Should abortion be banned??) and a set of 
texts about the topic, find the Sentiments 
expressed about (claims about) the Topic (but 
not its supporting subtopics) in each text, and 
identify the people who hold each sentiment.  
To avoid the problem of differentiating 
between shades of sentiments, we simplify the 
problem to: identify just expressions of 
positive, negative, or neutral sentiments, 
together with their holders.  In addition, for 
sentences that do not express a sentiment but 
simply state that some sentiment(s) exist(s), 
return these sentences in a separate set.  For 
example, given the topic ?What should be done 
with Medicare?? the sentence ?After years of 
empty promises, Congress has rolled out two 
Medicare prescription plans, one from House 
Republicans and the other from the Democratic  
Sentence
POS Tagger
verbs nounsAdjectives
Adjective Senti ment
classifier 
sentiment sentiment
Sentence sentiment classifier
Opinion region + polarity + holder
Holder finder
Named Entity 
Tagger
Sentence
Sentence
texts + topic
sentiment sentiment sentiment
V rbs
Verb Senti ment
classifier 
Nouns
Noun Senti ment
classifier 
WordNet
Sentence :
Figure 1: System architecture.  
Sens. Bob Graham of Florida and Zell Miller of 
Georgia? should be returned in the separate set.   
We approach the problem in stages, starting 
with words and moving on to sentences.  We 
take as unit sentiment carrier a single word, and 
first classify each adjective, verb, and noun by 
its sentiment.  We experimented with several 
classifier models.  But combining sentiments 
requires additional care, as Table 1 shows.   
California Supreme Court agreed that the state?s 
new term-limit law was constitutional. 
California Supreme Court disagreed that the 
state?s new term-limit law was constitutional. 
California Supreme Court agreed that the state?s 
new term-limit law was unconstitutional. 
California Supreme Court disagreed that the 
state?s new term-limit law was unconstitutional. 
Table 1: Combining sentiments.  
A sentence might even express opinions of 
different people.  When combining word-level 
sentiments, we therefore first determine for 
each Holder a relevant region within the 
sentence and then experiment with various 
models for combining word sentiments.     
We describe our models and algorithm in 
Section 2, system experiments and discussion 
in Section 3, and conclude in Section 4.   
2 Algorithm  
Given a topic and a set of texts, the system 
operates in four steps.  First it selects sentences 
that contain both the topic phrase and holder 
candidates.  Next, the holder-based regions of 
opinion are delimited.  Then the sentence 
sentiment classifier calculates the polarity of all 
sentiment-bearing words individually. Finally, 
the system combines them to produce the 
holder?s sentiment for the whole sentence.  
Figure 1 shows the overall system architecture.  
Section 2.1 describes the word sentiment 
classifier and Section 2.2 describes the sentence 
sentiment classifier.   
2.1 Word Sentiment Classifier 
2.1.1 Word Classification Models 
For word sentiment classification we 
developed two models.  The basic approach is 
to assemble a small amount of seed words by 
hand, sorted by polarity into two lists?positive 
and negative?and then to grow this by adding 
words obtained from WordNet (Miller et al 
1993; Fellbaum et al 1993).  We assume 
synonyms of positive words are mostly positive 
and antonyms mostly negative, e.g., the 
positive word ?good? has synonyms ?virtuous, 
honorable, righteous? and antonyms ?evil, 
disreputable, unrighteous?.  Antonyms of 
negative words are added to the positive list, 
and synonyms to the negative one.   
To start the seed lists we selected verbs (23 
positive and 21 negative) and adjectives (15 
positive and 19 negative), adding nouns later.   
Since adjectives and verbs are structured 
differently in WordNet, we obtained from it 
synonyms and antonyms for adjectives but only 
synonyms for verbs.  For each seed word, we 
extracted from WordNet its expansions and 
added them back into the appropriate seed lists.  
Using these expanded lists, we extracted an 
additional cycle of words from WordNet, to 
obtain finally 5880 positive adjectives, 6233 
negative adjectives, 2840 positive verbs, and 
3239 negative verbs.   
However, not all synonyms and antonyms 
could be used: some had opposite sentiment or 
were neutral.  In addition, some common words 
such as ?great?, ?strong?, ?take?, and ?get? 
occurred many times in both positive and 
negative categories.  This indicated the need to 
develop a measure of strength of sentiment 
polarity (the alternative was simply to discard 
such ambiguous words)?to determine how 
strongly a word is positive and also how 
strongly it is negative.  This would enable us to 
discard sentiment-ambiguous words but retain 
those with strengths over some threshold.   
Armed with such a measure, we can also 
assign strength of sentiment polarity to as yet 
unseen words.  Given a new word, we use 
WordNet again to obtain a synonym set of the 
unseen word to determine how it interacts with 
our sentiment seed lists.  That is, we compute  
(1)                 ).....,|(maxarg
)|(maxarg
21 n
c
c
synsynsyncP
wcP
?
where c is a sentiment category (positive or 
negative), w is the unseen word, and synn are the 
WordNet synonyms of w.  To compute 
Equation (1), we tried two different models:  
(2)   )|()(maxarg
)|()(maxarg
)|()(maxarg)|(maxarg
1
))(,(
 ...3 2 1
?
=
=
=
=
m
k
wsynsetfcount
k
c
n
c
cc
kcfPcP
csynsynsynsynPcP
cwPcPwcP
where fk is the kth feature (list word) of 
sentiment class c which is also a member of the 
synonym set of w, and count(fk,synset(w)) is the 
total number of occurrences of fk in the 
synonym set of w.  P(c) is the number of words 
in class c divided by the total number of words 
considered.  This model derives from document 
classification.  We used the synonym and 
antonym lists obtained from Wordnet instead of 
learning word sets from a corpus, since the 
former is simpler and does not require 
manually annotated data for training.   
Equation (3) shows the second model for a 
word sentiment classifier.   
(3)        
)(
),(
)(maxarg
)|()(maxarg)|(maxarg
1
ccount
csyncount
cP
cwPcPwcP
n
i
i
c
cc
?
==
=
 
To compute the probability P(w|c) of word w 
given a sentiment class c, we count the 
occurrence of w?s synonyms in the list of c.  
The intuition is that the more synonyms 
occuring in c, the more likely the word belongs.   
We computed both positive and negative 
sentiment strengths for each word and 
compared their relative magnitudes.  Table 2 
shows several examples of the system output, 
computed with Equation (2), in which ?+? 
represents positive category strength and ?-? 
negative.  The word ?amusing?, for example, 
was classified as carrying primarily positive 
sentiment, and ?blame? as primarily negative.  
The absolute value of each category represents 
the strength of its sentiment polarity.  For 
instance, ?afraid? with strength -0.99 represents 
strong negavitity while ?abysmal? with strength 
-0.61 represents weaker negativity.   
abysmal : NEGATIVE   
[+ : 0.3811][- : 0.6188] 
adequate : POSITIVE    
[+ : 0.9999][- : 0.0484e-11] 
afraid : NEGATIVE     
[+ : 0.0212e-04][- : 0.9999] 
ailing : NEGATIVE     
[+ : 0.0467e-8][- : 0.9999] 
amusing : POSITIVE    
[+ : 0.9999][- : 0.0593e-07] 
answerable : POSITIVE   
[+ : 0.8655][- : 0.1344] 
apprehensible: POSITIVE   
[+ : 0.9999][- : 0.0227e-07] 
averse : NEGATIVE      
[+ : 0.0454e-05][- : 0.9999] 
blame : NEGATIVE      
[+ : 0.2530][- : 0.7469] 
Table 2: Sample output of word sentiment 
classifier.  
2.2 Sentence Sentiment Classifier 
As shows in Table 1, combining sentiments 
in a sentence can be tricky.  We are interested 
in the sentiments of the Holder about the 
Claim.  Manual analysis showed that such 
sentiments can be found most reliably close to 
the Holder; without either Holder or 
Topic/Claim nearby as anchor points, even 
humans sometimes have trouble reliably 
determining the source of a sentiment.  We 
therefore included in the algorithm steps to 
identify the Topic (through direct matching, 
since we took it as given) and any likely 
opinion Holders (see Section 2.2.1).  Near each 
Holder we then identified a region in which 
sentiments would be considered; any 
sentiments outside such a region we take to be 
of undetermined origin and ignore (Section 
2.2.2).  We then defined several models for 
combining the sentiments expressed within a 
region (Section 2.2.3).   
2.2.1 Holder Identification 
We used BBN?s named entity tagger 
IdentiFinder to identify potential holders of an 
opinion.  We considered PERSON and 
ORGANIZATION as the only possible opinion 
holders.  For sentences with more than one 
Holder, we chose the one closest to the Topic 
phrase, for simplicity.  This is a very crude step.  
A more sophisticated approach would employ a 
parser to identify syntactic relationships 
between each Holder and all dependent 
expressions of sentiment.   
2.2.2 Sentiment Region 
Lacking a parse of the sentence, we were 
faced with a dilemma: How large should a 
region be?  We therefore defined the sentiment 
region in various ways (see Table 3) and 
experimented with their effectiveness, as 
reported in Section 3.   
Window1: full sentence 
Window2: words between Holder and Topic 
Window3: window2 ? 2 words 
Window4: window2 to the end of sentence 
Table 3: Four variations of region size. 
2.2.3 Classification Models 
We built three models to assign a sentiment 
category to a given sentence, each combining 
the individual sentiments of sentiment-bearing 
words, as described above, in a different way.   
Model 0 simply considers the polarities of 
the sentiments, not the strengths:  
Model 0: ? (signs in region) 
The intuition here is something like 
?negatives cancel one another out?.  Here the 
system assigns the same sentiment to both ?the 
California Supreme Court agreed that the 
state?s new term-limit law was constitutional? 
and ?the California Supreme Court disagreed 
that the state?s new term-limit law was 
unconstitutional?.  For this model, we also 
included negation words such as not and never 
to reverse the sentiment polarity.   
Model 1 is the harmonic mean (average) of 
the sentiment strengths in the region:  
Model 1: 
cwcp
wcp
cn
scP
ij
n
i
i
=
= ?
=
)|(argmax if
 ,)|(
)(
1)|(
j
1
 
Here n(c) is the number of words in the region 
whose sentiment category is c.  If a region 
contains more and stronger positive than 
negative words, the sentiment will be positive.   
Model 2 is the geometric mean:  
Model 2: 
cwcpif
wcpscP
ij
n
i
i
cn
=
?= ?
=
?
)|(argmax 
,)|(10)|(
j
1
1)(
 
2.2.4 Examples 
The following are two example outputs.   
 
Public officials throughout California have 
condemned a U.S. Senate vote Thursday to 
exclude illegal aliens from the 1990 census, 
saying the action will shortchange California in 
Congress and possibly deprive the state of 
millions of dollars of federal aid for medical 
emergency services and other programs for poor 
people. 
TOPIC : illegal alien 
HOLDER : U.S. Senate 
OPINION REGION: vote/NN Thursday/NNP     
to/TO exclude/VB illegal/JJ aliens/NNS from/IN 
the/DT 1990/CD census,/NN  
SENTIMENT_POLARITY: negative  
For that reason and others, the Constitutional 
Convention unanimously rejected term limits 
and the First Congress soundly defeated two 
subsequent term-limit proposals. 
TOPIC : term limit 
HOLDER : First Congress 
OPINION REGION: soundly/RB defeated/VBD 
two/CD subsequent/JJ term-limit/JJ 
proposals./NN 
SENTIMENT_POLARITY: negative 
3 Experiments 
The first experiment examines the two word 
sentiment classifier models and the second the 
three sentence sentiment classifier models.   
3.1 Word Sentiment Classifier 
For test material, we asked three humans to 
classify data.  We started with a basic English 
word list for foreign students preparing for the 
TOEFL test and intersected it with an adjective 
list containing 19748 English adjectives and a 
verb list of 8011 verbs to obtain common 
adjectives and verbs.  From this we randomly 
selected 462 adjectives and 502 verbs for 
human classification.  Human1 and human2 
each classified 462 adjectives, and human2 and 
human3 502 verbs.   
The classification task is defined as assigning 
each word to one of three categories: positive, 
negative, and neutral.  
3.1.1 Human?Human Agreement  
 Adjectives Verbs 
 Human1 : Human2 Human1 : Human3
Strict 76.19% 62.35% 
Lenient 88.96% 85.06% 
Table 4: Inter-human classification 
agreement. 
Table 4 shows inter-human agreement.  The 
strict measure is defined over all three 
categories, whereas the lenient measure is taken 
over only two categories, where positive and 
neutral have been merged, should we choose to 
focus only on differentiating words of negative 
sentiment.   
3.1.2 Human?Machine Agreement 
Table 5 shows results, using Equation (2) of 
Section 2.1.1, compared against a baseline that 
randomly assigns a sentiment category to each 
word (averaged over 10 iterations).  The system 
achieves lower agreement than humans but 
higher than the random process.   
Of the test data, the algorithm classified 
93.07% of adjectives and 83.27% of verbs as 
either positive and negative.  The remainder of 
adjectives and verbs failed to be classified, 
since they did not overlap with the synonym set 
of adjectives and verbs.   
In Table 5, the seed list included just a few 
manually selected seed words (23 positive and 
21 negative verbs and 15 and 19 adjectives, 
repectively).  We decided to investigate the 
effect of more seed words.   After collecting the 
annotated data, we added half of it (231 
adjectives and 251 verbs) to the training set, 
retaining the other half for the test.  As Table 6 
shows, agreement of both adjectives and verbs 
with humans improves.  Recall is also 
improved.  
Adjective 
(Train: 231  Test : 231) 
Verb 
(Train: 251  Test : 251) 
Lenient agreement Lenient agreement 
H1:M H2:M 
recall 
H1:M H3:M 
recall 
75.66% 77.88% 97.84% 81.20% 79.06% 93.23%
Table 6: Results including manual data.   
3.2 Sentence Sentiment Classifier 
3.2.1 Data 
100 sentences were selected from the DUC 
2001 corpus with the topics ?illegal alien?, 
?term limits?, ?gun control?, and ?NAFTA?.  
Two humans annotated the 100 sentences with 
three categories (positive, negative, and N/A).  
To measure the agreement between humans, we 
used the Kappa statistic (Siegel and Castellan 
Jr. 1988).  The Kappa value for the annotation 
task of 100 sentences was 0.91, which is 
considered to be reliable.   
3.2.2 Test on Human Annotated Data 
We experimented on Section 2.2.3?s 3 
models of sentiment classifiers, using the 4 
different window definitions and 4 variations of 
word-level classifiers (the two word sentiment 
equations introduced in Section 2.1.1, first with 
and then without normalization, to compare 
performance).   
Since Model 0 considers not probabilities of 
words but only their polarities, the two word- 
level classifier equations yield the same results. 
Consequently, Model 0 has 8 combinations and 
Models 1 and 2 have 16 each.    
To test the identification of opinion Holder, 
we first ran models with holders that were 
annotated by humans then ran the same models 
with the automatic holder finding strategies.  
The results appear in Figures 2 and 3. The 
models are numbered as follows: m0 through 
m4 represent 4 sentence classifier models,
Table 5. Agreement between humans and system.  
 Adjective  (test: 231 adjectives) Verb (test : 251 verbs) 
Lenient agreement Lenient agreement 
 
H1:M H2:M 
recall 
H1:M H3:M 
recall  
Random selection 
(average of 10 iterations) 59.35% 57.81% 100% 59.02% 56.59% 100% 
Basic method 68.37% 68.60% 93.07% 75.84% 72.72% 83.27% 
p1/p2 and p3/p4 represent the word classifier 
models in Equation (2) and Equation (3) with 
normalization and without normalization 
respectively. 
0.3
0.4
0.5
0.6
0.7
0.8
0.9
m0p1 m0p3 m1p1 m1p2 m1p3 m1p4 m2p1 m2p2 m2p3 m2p4
ac
cu
ra
cy
Window 1 Window 2 Window 3 Window 4
0.3
0.4
0.5
0.6
0.7
0.8
0.9
m0p1 m0p3 m1p1 m1p2 m1p3 m1p4 m2p1 m2p2 m2p3 m2p4
ac
cu
rac
y
Window 1 Window 2 Window 3 Window 4
Human 1 : Machine
Human 2 : Machine
Figure 2: Results with manually annotated 
Holder. 
0.3
0.4
0.5
0.6
0.7
0.8
0.9
m0p1 m0p3 m1p1 m1p2 m1p3 m1p4 m2p1 m2p2 m2p3 m2p4
ac
cu
rac
y
Window 1 Window 2 Window 3 Window 4
0.3
0.4
0.5
0.6
0.7
0.8
0.9
m0p1 m0p3 m1p1 m1p2 m1p3 m1p4 m2p1 m2p2 m2p3 m2p4
ac
cu
rac
y
Window 1 Window 2 Window 3 Window 4
Human 1 : Machine
Human 2 : Machine
Figure 3: Results with automatic Holder 
detection. 
Correctness of an opinion is determined 
when the system finds both a correct holder and 
the appropriate sentiment within the sentence.  
Since human1 classified 33 sentences positive 
and 33 negative, random classification gives 33 
out of 66 sentences.  Similarly, since human2 
classified 29 positive and 34 negative, random 
classification gives 34 out of 63 when the 
system blindly marks all sentences as negative 
and 29 out of 63 when it marks all as positive.  
The system?s best model performed at 81% 
accuracy with the manually provided holder 
and at 67% accuracy with automatic holder 
detection.   
3.3 Problems 
3.3.1 Word Sentiment Classification 
As mentioned, some words have both strong 
positive and negative sentiment.  For these 
words, it is difficult to pick one sentiment 
category without considering context.  Second, 
a unigram model is not sufficient: common 
words without much sentiment alone can 
combine to produce reliable sentiment.  For 
example, in ??Term limits really hit at 
democracy,? says Prof. Fenno?, the common 
and multi-meaning word ?hit? was used to 
express a negative point of view about term 
limits.  If such combinations occur adjacently, 
we can use bigrams or trigrams in the seed 
word list.  When they occur at a distance, 
however, it is more difficult to identify the 
sentiment correctly, especially if one of the 
words falls outside the sentiment region.   
3.3.2 Sentence Sentiment Classification 
Even in a single sentence, a holder might 
express two different opinions. Our system 
only detects the closest one.   
Another difficult problem is that the models 
cannot infer sentiments from facts in a 
sentence.  ?She thinks term limits will give 
women more opportunities in politics? 
expresses a positive opinion about term limits 
but the absence of adjective, verb, and noun 
sentiment-words prevents a classification.   
Although relatively easy task for people, 
detecting an opinion holder is not simple either.  
As a result, our system sometimes picks a 
wrong holder when there are multiple plausible 
opinion holder candidates present.   Employing 
a parser to delimit opinion regions and more 
accurately associate them with potential holders 
should help.   
3.4 Discussion 
Which combination of models is best? 
The best overall performance is provided by 
Model 0.  Apparently, the mere presence of 
negative words is more important than 
sentiment strength.  For manually tagged holder 
and topic, Model 0 has the highest single 
performance, though Model 1 averages best.   
Which is better, a sentence or a region?  
With manually identified topic and holder, 
the region window4 (from Holder to sentence 
end) performs better than other regions.   
How do scores differ from manual to 
automatic holder identification? 
Table 7 compares the average results with 
automatic holder identification to manually 
annotated holders in 40 different models.  
Around 7 more sentences (around 11%) were 
misclassified by the automatic detection 
method.    
 positive negative total 
Human1 5.394 1.667 7.060 
Human2 4.984 1.714 6.698 
Table 7: Average difference between 
manual and automatic holder detection. 
How does adding the neutral sentiment as a 
separate category affect the score? 
It is very confusing even for humans to 
distinguish between a neutral opinion and non-
opinion bearing sentences.  In previous 
research, we built a sentence subjectivity 
classifier.  Unfortunately, in most cases it 
classifies neutral and weak sentiment sentences 
as non-opinion bearing sentences.   
4 Conclusion 
Sentiment recognition is a challenging and 
difficult part of understanding opinions.  We 
plan to extend our work to more difficult cases 
such as sentences with weak-opinion-bearing 
words or sentences with multiple opinions 
about a topic.  To improve identification of the 
Holder, we plan to use a parser to associate 
regions more reliably with holders.  We plan to 
explore other learning techniques, such as 
decision lists or SVMs.   
Nonetheless, as the experiments show, 
encouraging results can be obtained even with 
relatively simple models and only a small 
amount of manual seeding effort.     
References  
Aristotle. The Rhetorics and Poetics (trans. W. 
Rhys Roberts), Modern Library, 1954. 
Fellbaum, C., D. Gross, and K. Miller. 1993. 
Adjectives in WordNet.  http://www.cosgi. 
princeton.edu/~wn. 
Hatzivassiloglou, V. and K. McKeown 1997. 
Predicting the Semantic Orientation of 
Adjectives. Proceedings of the 35th ACL 
conference, 174?181. 
Miller, G.A., R. Beckwith, C. Fellbaum, D. 
Gross, and K. Miller. 1993. Introduction to 
WordNet: An On-Line Lexical Database.  
http://www.cosgi.princeton.edu/~wn. 
Pang, B. L. Lee, and S. Vaithyanathan, 2002.  
Thumbs up? Sentiment classification using 
Machine Learning Techniques. Proceedings 
of the EMNLP conference.  
Perelman, C. 1970. The New Rhetoric: A 
Theory of Practical Reasoning. In The Great 
Ideas Today. Chicago: Encyclopedia 
Britannica.    
Riloff, E., J. Wiebe, and T. Wilson 2003. 
Learning Subjective Nouns Using Extraction 
Pattern Bootstrapping. Proceedings of the 
CoNLL-03 conference.   
Siegel, S. and N.J. Castellan Jr. 1988. 
Nonparametric Statistics for the Behavioral 
Sciences. McGraw-Hill.  
Toulmin, S.E., R. Rieke, and A. Janik. 1979. 
An Introduction to Reasoning. Macmillan, 
New York.   
Toulmin, S.E. 2003. The Uses of Argument.  
Cambridge University Press.  
Turney, P. 2002. Thumbs Up or Thumbs 
Down? Semantic Orientation Applied to 
Unsupervised Classification of Reviews. 
Proceedings of the 40th Annual Meeting of 
the ACL, Philadelphia, 417?424. 
Wallace, K. 1975. Topoi and the Problem of 
Invention. In W. Ross Winterowd (ed), 
Contemporary Rhetoric. Harcourt Brace 
Jovanovich.  
Wiebe, J. et al 2002. NRRC summer study Jan 
Wiebe and group (University of Pittsburgh) 
on ?subjective? statements.  
Yu, H. and V. Hatzivassiloglou. 2003. Towards 
Answering Opinion Questions: Separating 
Facts from Opinions and Identifying the 
Polarity of Opinion Sentences. Proceedings 
of the EMNLP conference. 
Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 1056?1064, Prague, June 2007. c?2007 Association for Computational Linguistics
Crystal: Analyzing Predictive Opinions on the Web 
 
 
Soo-Min Kim and Eduard Hovy 
USC Information Sciences Institute 
4676 Admiralty Way, Marina del Rey, CA 90292 
{skim,hovy}@ISI.EDU 
 
  
 
Abstract 
In this paper, we present an election predic-
tion system (Crystal) based on web users? 
opinions posted on an election prediction 
website. Given a prediction message, Crys-
tal first identifies which party the message 
predicts to win and then aggregates predic-
tion analysis results of a large amount of 
opinions to project the election results. We 
collect past election prediction messages 
from the Web and automatically build a 
gold standard. We focus on capturing lexi-
cal patterns that people frequently use 
when they express their predictive opinions 
about a coming election. To predict elec-
tion results, we apply SVM-based super-
vised learning. To improve performance, 
we propose a novel technique which gener-
alizes n-gram feature patterns. Experimen-
tal results show that Crystal significantly 
outperforms several baselines as well as a 
non-generalized n-gram approach. Crystal 
predicts future elections with 81.68% accu-
racy. 
1 Introduction 
As a growing number of people use the Web as a 
medium for expressing their opinions, the Web is 
becoming a rich source of various opinions in the 
form of product reviews, travel advice, social issue 
discussions, consumer complaints, stock market 
predictions, real estate market predictions, etc. 
At least two categories of opinions can be iden-
tified. One consists of opinions such as ?I 
like/dislike it?, and the other consists of opinions 
like ?It is likely/unlikely to happen.? We call the 
first category Judgment Opinions and the second 
(those discussing the future) Predictive Opinions. 
Judgment opinions express positive or negative 
sentiment about a topic such as, for example, re-
views about cameras, movies, books, or hotels, and 
discussions about topics like abortion and war. In 
contrast, predictive opinions express a person's 
opinion about the future of a topic or event such as 
the housing market, a popular sports match, and 
national election, based on his or her belief and 
knowledge. 
Due to the different nature of these two catego-
ries of opinion, each has different valences. Judg-
ment opinions have core valences of positive and 
negative. For example, ?liking a product? and 
?supporting abortion? have the valence ?positive? 
toward each topic (namely ?a product? and ?abor-
tion?). Predictive opinions have the core valence of 
likely or unlikely predicated on the event. For ex-
ample, a sentence ?Housing prices will go down 
soon? carries the valence of ?likely? for the event 
of ?housing prices go down?.  
The two types of opinions can co-appear. The 
sentence ?I like Democrats but I think they are not 
likely to win considering the war issue? contains 
both types of opinion: ?positive? valence towards 
Democrats and ?unlikely? valence towards the 
event of ?Democrats wins?. In order to accurately 
identify and analyze each type of opinion, different 
approaches are desirable. 
Note that our work is different from predictive 
data mining which models a data mining system 
using statistical approaches in order to forecast the 
future or trace a pattern of interest (Rickel and Por-
ter, 1997; Rodionov and Martin, 1996). Example 
domains of predictive data mining include earth-
quake prediction, air temperature prediction, for-
eign exchange prediction, and energy price predic-
1056
tion. However, predictive data mining is only fea-
sible when a large amount of structured numerical 
data (e.g., in a database) is available. Unlike this 
research area which analyzes numeric values, our 
study mines unstructured text using NLP tech-
niques and it can potentially extend the reach of 
numeric techniques.  
Despite the vast amount of predictive opinions 
and their potential applications such as identifica-
tion and analysis of people's opinions about the 
real estate market or a specific country's economic 
future, studies on predictive opinions have been 
neglected in Computational Linguistics, where 
most previous work focuses on judgment opinions 
(see Section 2). In this paper, we concentrate on 
identifying predictive opinion with its valence.  
Among many prediction domains on the Web, 
we focus on election prediction and introduce 
Crystal, a system to predict election results using 
the public's written viewpoints. To build our sys-
tem, we collect opinions about past elections 
posted on an election prediction project website 
before the election day, and build a corpus1. We 
then use this corpus to train our system for analyz-
ing predictive opinion messages and, using this, to 
predict the election outcome. Due to the availabil-
ity of actual results of the past elections, we can 
not only evaluate how accurately Crystal analyzes 
prediction messages (by checking agreement with 
the gold standard), but also objectively measure the 
prediction accuracy of our system. 
The main contributions of this work are as fol-
lows: 
? an NLP technique for analyzing predictive 
opinions in the electoral domain; 
? a method of automatically building a corpus 
of predictive opinions for a supervised 
learning approach; and 
? a feature generalization technique that out-
performs all the baselines on the task of 
identifying a predicted winning party given 
a predictive opinion. 
The rest of this paper is structured as follows. 
Section 2 surveys previous work. Section 3 for-
mally defines our task and describes our data set. 
Section 4 describes our system Crystal with pro-
posed feature generalization algorithm. Section 5 
                                                 
1 The resulting corpus is available at  
http://www.isi.edu/ ~skim/Download/Data/predictive.htm 
reports empirical evidence that Crystal outper-
forms several baseline systems. Finally, Section 6 
concludes with a description of the impact of this 
work. 
2 Related Work 
This work is closely related to opinion analysis and 
text classification. Most research on opinion analy-
sis in computational linguistics has focused on sen-
timent analysis, subjectivity detection, and review 
mining. Pang et al (2002) and Turney (2002) clas-
sified sentiment polarity of reviews at the docu-
ment level. Wiebe et al (1999) classified sentence 
level subjectivity using syntactic classes such as 
adjectives, pronouns and modal verbs as features. 
Riloff and Wiebe (2003) extracted subjective ex-
pressions from sentences using a bootstrapping 
pattern learning process. Wiebe et. al (2004) and 
Riloff et. al (2005) adopted pattern learning with 
lexical feature generalization for subjective expres-
sion detection. Dave et. al (2003) and Jindal and 
Liu (2006) also learned patterns of opinion expres-
sion in product reviews. Yu and Hatzivassiloglou 
(2003) identified the polarity of opinion sentences 
using semantically oriented words. These tech-
niques were applied and examined in different do-
mains, such as customer reviews (Hu and Liu 
2004; Popescu et al, 2005) and news articles (Kim 
and Hovy, 2004; Wilson et al, 2005). 
In text classification, systems typically use bag-
of-words models, mostly with supervised learning 
algorithms using Naive Bayes or Support Vector 
Machines (Joachims, 1998) to classify documents 
into several categories such as sports, art, politics, 
and religion. Liu et al (2004) and Gliozzo et al 
(2005) address the difficulty of obtaining training 
corpora for supervised learning and propose unsu-
pervised learning approaches. Another recent re-
lated classification task focuses on academic and 
commercial efforts to detect email spam messages. 
For an SVM-based approach, see (Drucker et al, 
1999). In our study, we explore the use of general-
ized lexical features for predictive opinion analysis 
and compare it with the bag-of-words approach. 
3 Modeling Prediction 
In this section, we define the task of analyzing pre-
dictive opinions in the electoral domain. 
1057
3.1 Task Definition 
We model predictive opinions in an election as 
follows: 
Valence) (Party,i ni onedicti onOpElecti onPr =  
where Party is a political party running for an elec-
tion (e.g., Democrats and Republicans) and Va-
lence is the valence of a predictive opinion which 
can be either ?likely to win? (WIN) or ?unlikely to 
win? (LOSE). Values for Party vary depending on 
in which year (e.g., 1996 and 2006) and where an 
election takes place (e.g., United States, France, or 
Japan). The unit of a predictive opinion is an un-
structured textual document such as an article in a 
personal blog or a message posted on a news group 
discussion board about the topic of ?Which party 
do you think will win/lose in this election??. 
Figure 1 illustrates an overview of our election 
prediction system Crystal in action. Given each 
document posted on blogs or message boards (e.g., 
www.election prediction.org) as seen in Figure 1.a, 
a system can determine a Party that the author of a 
document thinks to win or lose (Valence), Figure 
1.b. For the example document starting with the 
sentence ?I think this riding will stay NDP as it has 
for the past 11 years.? in Figure 1.a, our predictive 
opinion analysis system aims to recognize NDP as 
Party and WIN as Valence. After aggregating the 
predictive opinion analysis results of all docu-
ments, we project the election results in Figure 1.c. 
The following section describes how we obtain our 
data set and the subsequent sections describe Crys-
tal. 
3.2 Automatically Labeled Data 
We collected messages posted on an election pre-
diction project page, www.electionprediction. org. 
The website contains various election prediction 
projects (e.g., provincial election, federal election, 
and general election) of different countries (e.g., 
Canada and United Kingdom) from 1999 to 2006. 
For our data set, we downloaded Canadian federal 
election prediction data for 2004 and 2006. The 
Canadian federal electoral system is based on 308 
Figure 1. Our election prediction system. Public
opinions are collected from message boards (a)
and our system determines for each the election
prediction ?Party? and ?Valence? (b). The output
of the system is a prediction of the election out-
come (c). 
Message text Predicted  winning party Riding Year
??? ??? ??? ??? 
Message_1457 Party_3 Riding_206 2004
Message_1458 Party_2 Riding_206 2004
Message_1459 Party_2 Riding_189 2006
Message_1460 Party_1 Riding_189 2006
Message_1461 Party_2 Riding_189 2006
Message_1462 Party_1 Riding_46 2006
??? ??? ??? ??? 
Table 1. A snapshot of the processed data 
Riding name Party Candidate name 
 NDP Noreen Johns 
Blackstrap Liberal J. Wayne Zimmer
 PC Lynne Yelich 
Table 2. An example of our Party-Candidate 
listing for a riding (PC: Progressive Conserva-
tive) 
1058
ridings (electoral districts). The website contains 
308 separate html files of messages corresponding 
to the 308 ridings for different years. In total, we 
collected 4858 and 4680 messages for the 2004 
and 2006 federal elections respectively. On aver-
age, a message consists of 98.8 words. 
To train and evaluate our system, we require a 
gold standard for each message (i.e., which party 
does an author of a message predict to win?). One 
option is to hire human annotators to build the gold 
standard. Instead, we used an online party logo 
image file that the author of each message already 
labeled for the message. Note that authors only 
select parties they think will win, which means our 
gold standard only contains a party with WIN va-
lence of each message. However, we leverage this 
information to build a system which is able to de-
termine a party even with LOSE valence. We de-
scribe this idea in detail in Section 4. 
Finally, we pre-processed the data by converting 
the downloaded html source files into a structured 
format with the following fields: message, party, 
riding, and year, where message is a text, party is a 
winning party predicted in the text, riding is one of 
the 308 ridings, and year is either 2004 or 2006. 
Table 1 shows a snapshot of the processed data set 
that we used for our system training and evalua-
tion. An additional piece of information consisting 
of a candidate's name for each party for each riding 
was also stored in our data set. With this informa-
tion, the system can infer opinions about a party 
based on opinions about candidates who run for the 
party. Table 2 shows an example of a riding. 
4 Analyzing Predictions 
In this section we describe Crystal. One simple 
approach could be a system (see NGR system in 
Section 5) trained by a machine learning technique 
using n-gram features and classifying a message 
into multiple classes (e.g., NDP, Liberal, or Pro-
gressive). However, we develop a more sophisti-
cated algorithm and compare its result with several 
baselines, including the simple n-gram method2. 
Experimental results in Section 5 show that Crystal 
outperforms all the baselines. 
Our approach consists of three steps: feature 
generalization, classification using SVMs, and 
                                                 
2 N-gram approach is often unbeatable (and therefore great) in 
many text classification tasks. 
SVM result integration3. Crystal generates general-
ized sentences in the feature generalization step. 
Then it classifies each sentence using generalized 
lexical features in order to determine Valence of 
Party in a sentence. Finally, it combines results of 
sentences to determine Valence and Party of a 
message. Note that the classification using SVM is 
an intermediate step conducting a binary classifica-
tion (i.e., WIN or LOSE) for the final multi-class 
classification in result integration. The following 
sections describe each step. 
4.1 Feature Generalization 
In the feature generalization step, we generalize 
patterns of words used in predictive opinions. For 
example, instead of using three different trigrams 
like ?Liberals will win?, ?NDP will win?, and 
?Conservatives will win?, we generalize these to 
?PARTY will win?. The assumption is that the 
generalized patterns can represent better the rela-
tionship among Party, Valence, and words sur-
rounding Party (e.g., will win) than pure lexical 
patterns. For this algorithm, we first substitute a 
candidate's name (both the first name and the last 
name) with the political party name that the candi-
date belongs to (see Table 2). We then break each 
message into sentences4.   
Table 3 outlines the feature generalization algo-
rithm. Here, our approach is that if a message pre-
                                                 
3 ?feature? indicates n-grams in our corpus that we use in the 
SVM classification step. 
4 The sentence breaker that we used is available at 
http://search.cpan.org/ ~shlomoy/Lingua-EN-sentence -
0.25/lib/Lingua/EN/Sentence.pm. 
1 for each message M with a party that M predicts to win, Pw 
2   for each sentence Si in a message M 
3      for each party Pj in Si 
4         valence Vj = +1 if Pj = Pw 
5         valence Vj = -1  Otherwise 
6         Generate S'ij by substituting Pj with  PARTY 
7         and all other parties in Si with OTHER
8          Return (Pj, Vj, S'ij) 
Table 3. Feature generalization algorithm 
1059
dicts a particular party to win, sentences which 
mention that party in the message also imply that it 
will win. Conversely all other parties are assumed 
to be in sentences that imply they will lose. As 
shown in Section 3.2, a message (M) in our corpus 
has a label of a party (Pw) that the author of M pre-
dicts to win. After breaking sentences in M, we 
duplicate a sentence by the number of unique par-
ties in the sentence and modify the duplicated sen-
tences by substituting the party names with 
PARTY and OTHER in order to generalize fea-
tures. 
Consider the following sentence: 
 ?Dockrill will barely take this riding from 
Rodger Cuzner?  
which gets re-written as: 
?NDP will barely take this riding from Liberal?  
because Dockrill is an NDP candidate and Rodger 
Cuzner is a Liberal candidate. Since the sentence 
contains two parties (i.e., NDP and Liberal), the 
algorithm duplicates the sentence twice, once for 
each party (see Lines 4?8 in Table 3)5. For NDP, 
the algorithm determines its Valence as -1 because 
NDP is not equal to the predicted winning party 
(i.e., Liberal) of the message (see Lines 4?5 in Ta-
                                                 
5 In the feature generalization algorithm, we represent 
WIN and LOSE valence as +1 and -1. 
ble 3). Then it generates a generalized sentence by 
substituting NDP with PARTY and Liberal with 
OTHER (Lines 6?7). It returns (NDP, -1, ?PARTY 
will barely take this riding from OTHER?). For 
Liberal, on the other hand, the algorithm deter-
mines its Valence as +1 since Liberal is the same 
as the predicted winning party of the message. Af-
ter similar generalization, it returns (Liberal, +1, 
?OTHER will barely take this riding from 
PARTY?).  
Note that the final result of the feature generali-
zation algorithm is a set of triplets: (Party, Va-
lence, Generalized Sentence). Among a triplet, we 
use (Valence, Generalized Sentence) to produce 
feature vectors for a machine learning algorithm 
(see Section 4.2) and (Party, Valence) to integrate 
system results of each sentence for the final deci-
sion of Party and Valence of a message (see Sec-
tion 4.3). Figure 2 shows an example of the algo-
rithm. 
4.2 Classification Using SVMs 
In this step, we use Support Vector Machines 
(SVMs) to train our system using the generalized 
features described in Section 4.1. After we ob-
tained examples of (Valence, Generalized Sen-
tence) in the feature generalization step, we mod-
eled a subtask of classifying a Generalized Sen-
tence into Valence towards our final goal of deter-
mining (Valence, Party) of a message. This subtask 
is a binary classification since Valence has only 2 
classes: +1 and -16. Given a generalized sentence 
?OTHER will barely take this riding from 
PARTY? in Figure 2, for example, the goal of our 
system is to learn WIN valence for PARTY. Fea-
tures for SVMs are extracted from generalized sen-
tences. We implemented our SVM learning model 
using the SVMlight package7. 
4.3 SVM Result Integration 
In this step, we combine the valence of each sen-
tence predicted by SVMs to determine the final 
valence and predicted party of a message. For each 
party mentioned in a message, we calculate the 
sum of the party's valences of each sentence and 
                                                 
6 However, the final evaluation of the system and all the base-
lines is equally performed on the multi-classification results of 
messages. 
7 SVMlight is available from http://svmlight.joachims. 
org/ 
Figure 2. An example of feature generalization 
of a message 
1060
pick a party that has the maximum value. This in-
tegration algorithm can be represented as follows: 
?
=
m
k
k
p
pValence
0
)(max arg
 
where p is one of parties mentioned in a message, 
m is the number of sentences that contains party p 
in a message, and Valencek(p) is the valence of p in 
the kth sentence that contains p. Given the example 
in Figure 2, the Liberal party appears twice in sen-
tence S0 and S1 and its total valence score is +2, 
whereas the NDP party appears once in sentence 
S1 and its valence sum is -1. As a result, our algo-
rithm picks liberal as the winning party that the 
message predicts. 
5 Experiments and Results 
This section reports our experimental results show-
ing empirical evidence that Crystal outperforms 
several baseline systems. 
5.1 Experimental Setup 
Our corpus consists of 4858 and 4680 messages 
from 2004 and 2006 Canadian federal election pre-
diction data respectively described in detail in Sec-
tion 3.2. We split our pre-processed corpus into 10 
folds for cross-validation. We implemented the 
following five systems to compare with Crystal 8. 
? NGR: In this algorithm, we train the system us-
ing SVM with n-gram features without the gener-
alization step described in Section 4.19. The re-
placement of each candidate's first and last name 
by his or her party name was still applied. 
? FRQ: This system picks the most frequently 
mentioned party in a message as the predicted 
winning party. Party name substitution is also ap-
plied. For example, given a message ?This riding 
will go liberal. Dockrill will barely take this riding 
from Rodger Cuzner.?, all candidates' names are 
replaced by party names (i.e., ?This riding will go 
Liberal. NDP will barely take this riding from Lib-
eral.?). After name replacement, the system picks 
Liberal as an answer because Liberal appears twice 
whereas NDP appears only once. Note that, unlike 
Crystal, this system does not consider the valence 
of each party (as done in our sentence duplication 
                                                 
8 In our experiments using SVM, we used the linear kernel for 
all Crystal, NGR, and JDG. 
9 This system is exactly like Crystal without the feature gener-
alization and result integration steps. 
step of the feature generalization algorithm). In-
stead, it blindly picks the party that appeared most 
in a message. 
? MJR: This system marks all messages with the 
most dominant predicted party in the entire data 
set. In our corpus, Conservatives was the majority 
party (3480 messages) followed closely by Liberal 
(3473 messages). 
? INC: This system chooses the incumbent party 
as the predicted winning party of a message. (This 
is a strong baseline since incumbents often win in 
Canadian politics). For example, since the incum-
bent party of the riding ?Blackstrap? in 2004 was 
Conservative, all the messages about Blackstrap in 
2004 were marked Conservative as their predicted 
winning party by this system.  
? JDG: This system uses judgment opinion words 
as its features for SVM. For our list of judgment 
opinion words, we use General Inquirer which is a 
publicly available list of 1635 positive and nega-
tive sentiment words (e.g., love, hate, wise, dumb, 
etc.)10. 
5.2 Experimental Results 
We measure the system performance with its accu-
racy in two different ways: accuracy per message 
(Accmessage) and accuracy per riding (Accriding). Both 
accuracies are represented as follows: 
set test ain  messages of # Total
labledcorrectly  system  themessages of #=messageAcc  
set test ain  ridings of # Total
predictedcorrectly  system  theridings of #=ridingAcc  
We first report the results with Accmessage in 
Evaluation1 and then report with Accriding in 
Evaluation2. 
Evaluation1: Table 4 shows accuracies of base-
lines and Crystal. We calculated accuracy for each 
test set in 10-fold data sets and averaged it. Among 
the baselines, MJR performed worst (36.48%). 
Both FRQ and INC performed around 50% 
(54.82% and 53.29% respectively). NGR achieved 
its best score (62.02%) when using unigram, bi-
gram, and trigram features together (uni+bi+tri). 
We also experimented with other feature combina-
tions (see Table 5). Our system achieved 73.07% 
which is 11% higher than NGR and around 20% 
                                                 
10 Available at http://www.wjh.harvard.edu/~inquirer 
/homecat.htm 
1061
higher than FRQ and INC. The best accuracy of 
our system was also obtained with the combination 
of unigram, bigram, and trigram features. 
The JDG system, which uses positive and nega-
tive sentiment word features, had 66.23% accu-
racy. This is about 7% lower than Crystal. Since 
the lower performance of JDG might be related to 
the number of features it uses, we also experi-
mented with the reduced number of features of 
Crystal based on the tfidf scores11. With the same 
number of features (i.e., 1635), Crystal performed 
70.62% which is 4.4% higher than JDG. An inter-
esting finding was that NGR with 1635 features 
performed only 54.60% which is significantly 
                                                 
11 The total number of all features of Crystal is 689,642. 
lower than both systems. This indicates that the 
1635 pure n-gram features are not as good as the 
same number of sentiment words carefully chosen 
from a dictionary but the generalized features of 
Crystal represent the predictive opinions better 
than JDG features. 
Table 5 illustrates the comparison of NGR 
(without feature generalization) and Crystal (with 
feature generalization) in different feature combi-
nations. uni, bi, tri, and four correspond to uni-
gram, bigram, trigram, and fourgram. Our pro-
posed technique Crystal performed always better 
than the pure n-gram system (NGR). Both systems 
performed best (62.02% and 73.07%) with the 
combination of unigram, bigram, and trigram 
(uni+bi+tri). The second best scores (61.96% and 
73.01%) are achieved with the combinations of all 
grams (uni+bi+tri+four) in both systems. Using 
fourgrams alone performed worst since the system 
overfitted to the training examples. 
Table 6 presents several examples of frequent n-
gram features in both WIN and LOSE classes. As 
shown in Table 6, lexical patterns in the WIN class 
express optimistic sentiments about PARTY (e.g., 
PARTY_will_win and go_ PARTY_again) 
whereas patterns in the LOSE class express pessi-
mistic sentiments (e.g., PARTY_don't_have) and 
optimistic ones about OTHER (e.g., 
want_OTHER). 
Evaluation2: In this evaluation, we use Accriding 
computed as the number of ridings that a system 
correctly predicted, divided by the total number of 
ridings. For each riding R, systems pick a party 
that obtains the majority prediction votes from 
messages in R as the winning party of R. For ex-
Patterns in WIN class Patterns in LOSE class 
PARTY_will_win want_OTHER 
PARTY_hold PARTY_don?t_have 
PARTY_will_win_this OTHER_and 
PARTY_win the_PARTY 
will_go_PARTY OTHER_will_win 
PARTY_will_take OTHER_is 
PARTY_will_take_this to_the_OTHER 
PARTY_is and_OTHER 
safest_PARTY results_OTHER 
PARTY_has OTHER_has 
go_PARTY_again to_OTHER 
Table 6. Examples of frequent features in 
WIN and LOSE classes. 
system Accmessage (%) Accriding (%) 
FRQ 54.82 63.14 
MJR 36.48 36.63 
INC 53.29 78.03 
NGR (uni+bi+tri) 62.02 79.65 
JDG 66.23 78.68 
Crystal (uni+bi+tri) 73.07 81.68 
Table 4. System performance with accuracy 
per message (Accmessage ) and accuracy per 
riding (Accriding): FRQ, MJR, INC, NGR, 
JDG, and Crystal. 
Accmessage (%) Features 
NGR Crystal 
uni 60.49 72.03 
bi 58.79 71.81 
tri 54.04 69.57 
four 47.25 67.64 
uni + bi 61.54 72.93 
uni + tri 61.36 72.20 
uni + four 60.70 72.84 
bi + tri 58.68 72.26 
bi + four 58.54 72.17 
uni + bi + tri 62.02 73.07 
uni + bi + four 61.75 72.30 
uni + tri + four 61.34 72.30 
bi + tri + four 58.42 72.62 
uni + bi + tri + four 61.96 73.01 
Table 5. System performance with different 
features: Pure n-gram (NGR) and General-
ized n-gram Crystal. 
1062
ample, if Crystal identified 9 messages predicting 
for Conservative Party, 3 messages for NDP, and 1 
message for Liberal among 13 messages in the rid-
ing ?Blackstrap?, the system will predict that the 
Conservative Party would win in ?Blackstrap?. 
Table 4 shows the system performance with Ac-
criding. Note that people who write messages on a 
particular web site are not a random sample for 
prediction. So we introduce a measure of confi-
dence (ConfidenceScore) of each system and use 
the prediction results when the ConfidenceScore is 
higher than a threshold. Otherwise, we use a de-
fault party (i.e., the incumbent party) as the win-
ning party. ConfidenceScore of a riding R is calcu-
lated as follows: 
ConfidenceScore =  countmessage(Pfirst) ?  countmes-
sage(Psecond) 
where countmessage(Px) is the number of messages 
that predict a party Px to win, Pfirst is the party that 
the most number of messages predict to win, and 
Psecond is the party that the second most number of 
messages predict to win. 
We used 62 ridings to tune the ConfidenceScore 
parameter arriving at the value of 4. As shown in 
Table 4, the system which just considers the in-
cumbent party (INC) performed fairly well 
(78.03% accuracy) because incumbents are often 
re-elected in Canadian elections. The upper bound 
of this prediction task is 88.85% accuracy which is 
the prediction result using numerical values of a 
prediction survey. FRQ and MJR performed 
63.14% and 36.63% respectively. Similarly to 
Evaluation1, JDG which only uses judgment word 
features performed worse than both Crystal and 
NGR. Also, Crystal with our feature generalization 
algorithm performed better than NGR with non-
generalized n-gram features. The accuracy of Crys-
tal (81.68%) is comparable to the upper bound 
88.85%. 
6 Discussion 
In this section, we discuss possible extensions and 
improvements of this work. 
Our experiment focuses on investigating aspects 
of predictive opinions by learning lexical patterns 
and comparing them with judgment opinions. 
However, this work can be extended to investigat-
ing how those two types of opinions are related to 
each other and whether lexical features of one 
(e.g., judgment opinion) can help identify the other 
(e.g., predictive opinion). Combining two types of 
opinion features and testing on each domain can 
examine this issue. 
In our experiment, we used General Inquirer 
words as judgment opinion indicators for JDG 
baseline system. It might be interesting to employ 
different resources for judgment words such as the 
polarity lexicon by Wilson et al (2005) and the 
recently released SentiWordNet12. 
 Our work is an initial step towards analyzing a 
new type of opinion. In the future, we plan to in-
corporate more features such as priors like incum-
bent party in addition to the lexical features to im-
prove the system performance. 
7 Conclusions 
In this paper, we proposed a framework for work-
ing with predictive opinion. Previously, research-
ers in opinion analysis mostly focused on judgment 
opinions which express positive or negative senti-
ment about a topic, as in product reviews and pol-
icy discussions. Unlike judgment opinions, predic-
tive opinions express a person's opinion about the 
future of a topic or event such as the housing mar-
ket, a popular sports match, and election results, 
based on his or her belief and knowledge. Among 
these many kinds of predictive opinions, we fo-
cused on election prediction. 
We collected past election prediction data from 
an election prediction project site and automati-
cally built a gold standard. Using this data, we 
modeled the election prediction task using a super-
vised learning approach, SVM. We proposed a 
novel technique which generalized n-gram feature 
patterns. Experimental results showed that this ap-
proach outperforms several baselines as well as a 
non-generalized n-gram approach. This is signifi-
cant because an n-gram model without generaliza-
tion is often extremely competitive in many text 
classification tasks.  
This work adopts NLP techniques for predictive 
opinions and it sets the foundation for exploring a 
whole new subclass of the opinion analysis prob-
lems. Potential applications of this work are sys-
tems that analyze various kinds of election predic-
tions by monitoring texts in discussion boards and 
personal blogs. In the future, we would like to 
                                                 
12 http://sentiwordnet.isti.cnr.it/ 
1063
model predictive opinions in other domains such as 
the real estate market and the stock market which 
would require further exploration of system design 
and data collection.  
Reference 
Engelmore, R., and Morgan, A. eds. 1986. Blackboard 
Systems. Reading, Mass.: Addison-Wesley. 
Dave, K., Lawrence, S. and Pennock, D. M.  2003. Min-
ing the peanut gallery: Opinion extraction and se-
mantic classification of product reviews. Proc. of 
World Wide Web Conference 2003 
Drucker, H., Wu, D. and Vapnik, V. 1999. Support vec-
tor machines for spam categorization. IEEE Trans. 
Neural Netw., 10, pp 1048?1054. 
Gliozzo, A., Strapparava C. and Dagan, I. 2005. Investi-
gating Unsupervised Learning for Text Categoriza-
tion Bootstrapping, Proc. of EMNLP 2005. Vancou-
ver, B.C., Canada 
Hu, M. and Liu, B. 2004. Mining and summarizing cus-
tomer reviews. Proc. Of KDD-2004, Seattle, Wash-
ington, USA. 
Jindal, N. and Liu, B. 2006. Mining Comprative Sen-
tences and Relations. Proc. of 21st National Confer-
ence on Artificial Intellgience (AAAI-2006). 2006. 
Boston, Massachusetts, USA 
Joachims, T. 1998. Text categorization with support 
vector machines: Learning with many relevant fea-
tures, Proc. of ECML, p. 137?142.  
Kim, S-M. and Hovy, E. 2004. Determining the Senti-
ment of Opinions. Proc. of COLING 2004. 
Liu, B., Li, X., Lee, W. S. and Yu, P. S. Text Classifica-
tion by Labeling Words Proc. of AAAI-2004, San 
Jose, USA. 
Pang, B, Lee, L. and Vaithyanathan, S. 2002. Thumbs 
up? Sentiment Classification using Machine Learning 
Techniques. Proc. of EMNLP 2002. 
Popescu, A-M. and Etzioni, O. 2005. Extracting Product 
Features and Opinions from Reviews, Proc. of HLT-
EMNLP 2005. 
Rickel, J. and Porter, B. 1997. Automated Modeling of 
Complex Systems to Answer Prediction Questions, 
Artificial Intelligence Journal, volume 93, numbers 
1-2, pp. 201?260 
Riloff, E., Wiebe, J., and Phillips, W. 2005. Exploiting 
Subjectivity Classification to Improve Information 
Extraction, Proc. of the 20th National Conference on 
Artificial Intelligence (AAAI-05) . 
Riloff, E., Wiebe, J. and Wilson, T. 2003. Learning Sub-
jective Nouns Using Extraction Pattern Bootstrap-
ping. Proc. of CoNLL 2003. pp 25?32. 
Rodionov, S. and Martin, J. H. 1996. A Knowledge-
Based System for the Diagnosis and Prediction of 
Short-Term Climatic Changes in the North Atlantic, 
Journal of Climate, 9(8)  
Turney, P. 2002. Thumbs Up or Thumbs Down? Se-
mantic Orientation Applied to Unsupervised Classifi-
cation of Reviews. Proc. of ACL 2002, pp 417?424. 
Wiebe, J., Bruce, R. and O?Hara, T. 1999. Development 
and use of a gold standard data set for subjectivity 
classifications. Proc. of ACL 1999, pp 246?253. 
Wiebe, J., Wilson, T. , Bruce, R. , Bell , M. and Martin, 
M. Learning Subjective Language. 2004. Computa-
tional Linguistics 
Wilson, T., Wiebe, J. and Hoffmann, P. 2005. Recog-
nizing Contextual Polarity in Phrase-Level Sentiment 
Analysis. Proc. of HLT/EMNLP 2005. 
Yu, H. and Hatzivassiloglou, V. 2003. Towards An-
swering Opinion Questions: Separating Facts from 
Opinions and Identifying the Polarity of Opinion 
Sentences. Proc. of EMNLP 2003. 
1064
Automatic Detection of Opinion Bearing Words and Sentences 
Soo-Min Kim and Eduard Hovy 
Information Sciences Institute 
University of Southern California 
4676 Admiralty Way 
Marina del Rey, CA 90292-6695 
{skim, hovy}@isi.edu 
Abstract 
We describe a sentence-level opinion 
detection system. We first define what 
an opinion means in our research and 
introduce an effective method for ob-
taining opinion-bearing and non-
opinion-bearing words. Then we de-
scribe recognizing opinion-bearing sen-
tences using these words We test the 
system on 3 different test sets: MPQA 
data, an internal corpus, and the TREC-
2003 Novelty track data. We show that 
our automatic method for obtaining 
opinion-bearing words can be used ef-
fectively to identify opinion-bearing 
sentences. 
1 Introduction 
Sophisticated language processing in recent 
years has made possible increasingly complex 
challenges for text analysis. One such challenge 
is recognizing, classifying, and understanding 
opinionated text. This ability is desirable for 
various tasks, including filtering advertisements, 
separating the arguments in online debate or 
discussions, and ranking web documents cited as 
authorities on contentious topics.   
The challenge is made very difficult by a 
general inability to define opinion. Our prelimi-
nary reading of a small selection of the available 
literature (Aristotle, 1954; Toulmin et al, 1979; 
Perelman, 1970; Wallace, 1975), as well as our 
own text analysis, indicates that a profitable ap-
proach to opinion requires a system to know 
and/or identify at least the following elements: 
the topic (T), the opinion holder (H), the belief 
(B), and the opinion valence (V). For the pur-
poses of the various interested communities, 
neutral-valence opinions (such as we believe the 
sun will rise tomorrow; Susan believes that John 
has three children) is of less interest; more rele-
vant are opinions in which the valence is posi-
tive or negative. Such valence often falls 
together with the actual belief, as in ?going to 
Mars is a waste of money?; in which the word 
waste signifies both the belief a lot [of money] 
and the valence bad/undesirable, but need not 
always do so: ?Smith[the holder] believes that 
abortion should be permissible[the topic] al-
though he thinks that is a bad thing[the va-
lence]?.   
As the core first step of our research, we 
would like an automated system to identify, 
given an opinionated text, all instances of the 
[Holder/Topic/Valence] opinion triads it con-
tains1. Exploratory manual work has shown this 
to be a difficult task. We therefore simplify the 
task as follows.  We build a classifier that sim-
ply identifies in a text all the sentences express-
ing a valence. Such a two-way classification is 
simple to set up and evaluate, since enough test-
ing data has been created.   
As primary indicators, we note from newspa-
per editorials and online exhortatory text that 
certain modal verbs (should, must) and adjec-
tives and adverbs (better, best, unfair, ugly, nice, 
desirable, nicely, luckily) are strong markers of 
opinion. Section 3 describes our construction of 
a series of increasingly large collections of such 
marker words. Section 4 describes our methods 
for organizing and combining them and using 
them to identify valence-bearing sentences.  The 
evaluation is reported in Section 5.   
2 Past Computational Studies 
There has been a spate of research on identify-
ing sentence-level subjectivity in general and 
opinion in particular. The Novelty track 
                                                          
1 In the remainder of the paper, we will mostly use ?opin-
ion? in place of ?valence?.  We will no longer discuss Be-
lief, Holder, or Topic. 
61
(Soboroff and Harman, 2003) of the TREC-2003 
competition included a task of recognizing opin-
ion-bearing sentences (see Section 5.2).   
Wilson and Wiebe (2003) developed an anno-
tation scheme for so-called subjective sentences 
(opinions and other private states) as part of a 
U.S. government-sponsored project (ARDA 
AQUAINT NRRC) in 2002. They created a cor-
pus, MPQA, containing news articles manually 
annotated. Several other approaches have been 
applied for learning words and phrases that sig-
nal subjectivity. Turney (2002) and Wiebe 
(2000) focused on learning adjectives and adjec-
tival phrases and Wiebe et al (2001) focused on 
nouns. Riloff et al (2003) extracted nouns and 
Riloff and Wiebe (2003) extracted patterns for 
subjective expressions using a bootstrapping 
process.  
3 Data Sources 
We developed several collections of opinion-
bearing and non-opinion-bearing words. One is 
accurate but small; another is large but relatively 
inaccurate. We combined them to obtain a more 
reliable list. We obtained an additional list from 
Columbia University. 
3.1 Collection 1: Using WordNet 
In pursuit of accuracy, we first manually col-
lected a set of opinion-bearing words (34 adjec-
tives and 44 verbs). Early classification trials 
showed that precision was very high (the system 
found only opinion-bearing sentences), but since 
the list was so small, recall was very low (it 
missed many). We therefore used this list as 
seed words for expansion using WordNet. Our 
assumption was that synonyms and antonyms of 
an opinion-bearing word could be opinion-
bearing as well, as for example ?nice, virtuous, 
pleasing, well-behaved, gracious, honorable, 
righteous? as synonyms for ?good?, or ?bad, evil, 
disreputable, unrighteous? as antonyms. How-
ever, not all synonyms and antonyms could be 
used: some such words seemed to exhibit both 
opinion-bearing and non-opinion-bearing senses, 
such as ?solid, hot, full, ample? for ?good?.  
This indicated the need for a scale of valence 
strength. If we can measure the ?opinion-based 
closeness? of a synonym or antonym to a known 
opinion bearer, then we can determine whether 
to include it in the expanded set.  
To develop such a scale, we first created a 
non-opinion-bearing word list manually and 
produced related words for it using WordNet.  
To avoid collecting uncommon words, we 
started with a basic/common English word list 
compiled for foreign students preparing for the 
TOEFL test. From this we randomly selected 
462 adjectives and 502 verbs for human annota-
tion. Human1 and human2 annotated 462 adjec-
tives and human3 and human2 annotated 502 
verbs, labeling each word as either opinion-
bearing or non-opinion-bearing. 
O P N o n O P
w o r d
S y n o n y m  se t  o f  O P S y n o n y m  s e t  o f  N o n O P
S y n o n y m  s e t  o f  
a  g iv e n  w o r d
O P         :  O p in io n -b e a r in g  w o r d s
N o n O P : N o n -O p in io n -b e a r in g  
w o r d s  
Figure 1. Automatic word expansion using WordNet 
Now, to obtain a measure of opinion/non-
opinion strength, we measured the WordNet 
distance of a target (synonym or antonym) word 
to the two sets of manually selected seed words 
plus their current expansion words (see Figure 
1). We assigned the new word to the closer 
category. The following equation represents this 
approach: 
(1)          ).....,|(maxarg
)|(maxarg
21 n
c
c
synsynsyncP
wcP
?  
where c is a category (opinion-bearing or non-
opinion-bearing), w is the target word, and synn 
is the synonyms or antonyms of the given word 
by WordNet. To compute equation (1), we built 
a classification model, equation (2): 
(2)   )|()(maxarg
)|()(maxarg
)|()(maxarg)|(maxarg
1
))(,(
 ...3 2 1
?
=
=
=
=
m
k
wsynsetfcount
k
c
n
c
cc
kcfPcP
csynsynsynsynPcP
cwPcPwcP
where kf  is the kth feature of category c which is 
also a member of the synonym set of the target 
word w, and count(fk, synset(w)) means the total 
number of occurrences of fk in the synonym set 
of w. The motivation for this model is document 
classification.  (Although we used the synonym 
set of seed words achieved by WordNet, we 
could instead have obtained word features from 
a corpus.) After expansion, we obtained 2682 
62
opinion-bearing and 2548 non-opinion-bearing 
adjectives, and 1329 opinion-bearing and 1760 
non-opinion-bearing verbs, with strength values. 
By using these words as features we built a Na-
ive bayesian classifier and we finally classified 
32373 words. 
3.2 Collection 2: WSJ Data 
Experiments with the above set did not provide 
very satisfactory results on arbitrary text. For 
one reason, WordNet?s synonym connections 
are simply not extensive enough. However, if 
we know the relative frequency of a word in 
opinion-bearing texts compared to non-opinion-
bearing text, we can use the statistical informa-
tion instead of lexical information. For this, we 
collected a huge amount of data in order to make 
up for the limitations of collection 1. 
Following the insight of Yu and Hatzivassi-
loglou (2003), we made the basic and rough as-
sumption that words that appear more often in 
newspaper editorials and letters to the editor 
than in non-editorial news articles could be po-
tential opinion-bearing words (even though edi-
torials contain sentences about factual events as 
well). We used the TREC collection to collect 
data, extracting and classifying all Wall Street 
Journal documents from it either as Editorial or 
nonEditorial based on the occurrence of the 
keywords ?Letters to the Editor?, ?Letter to the 
Editor? or ?Editorial? present in its headline.  
This produced in total 7053 editorial documents 
and 166025 non-editorial documents.   
We separated out opinion from non-opinion 
words by considering their relative frequency in 
the two collections, expressed as a probability, 
using SRILM, SRI?s language modeling toolkit 
(http://www.speech.sri.com/projects/srilm/). For 
every word W occurring in either of the docu-
ment sets, we computed the followings: 
documents Editorialin   wordstotal
documents Editorialin W #)(Pr =WobEditorial
docs alnonEditoriin   wordstotal
docs alnonEditoriin W #)(Pr =WobalnonEditori
  
We used Kneser-Ney smoothing (Kneser and 
Ney, 1995) to handle unknown/rare words.  
Having obtained the above probabilities we cal-
culated the score of W as the following ratio: 
alProb(W)nonEditori
rob(W)EditorialP )( =WScore
 
Score(W) gives an indication of the bias of 
each word towards editorial or non-editorial 
texts. We computed scores for 86,674,738 word 
tokens. Naturally, words with scores close to 1 
were untrustworthy markers of valence.  To 
eliminate these words we applied a simple filter 
as follows. We divided the Editorial and the 
non-Editorial collections each into 3 subsets. For 
each word in each {Editorial, non-Editorial} 
subset pair we calculated Score(W). We retained 
only those words for which the scores in all 
three subset pairs were all greater than 1 or all 
less than 1.  In other words, we only kept words 
with a repeated bias towards Editorial or non-
Editorial. This procedure helped eliminate some 
of the noisy words, resulting in 15568 words.  
3.3 Collection 3: With Columbia Wordlist 
Simply partitioning WSJ articles into Edito-
rial/non-Editorial is a very crude differentiation.  
In order to compare the effectiveness of our im-
plementation of this idea with the implementa-
tion by Yu and Hatzivassiloglou of Columbia 
University, we requested their word list, which 
they kindly provided. Their list contained 
167020 adjectives, 72352 verbs, 168614 nouns, 
and 9884 adverbs. However, this figure is sig-
nificantly inflated due to redundant counting of 
words with variations in capitalization and a 
punctuation.We merged this list and ours to ob-
tain collection 4. Among these words, we only 
took top 2000 opinion bearing words and top 
2000 non-opinion-bearing words for the final 
word list. 
3.4 Collection 4: Final Merger  
So far, we have classified words as either opin-
ion-bearing or non-opinion-bearing by two dif-
ferent methods.  The first method calculates the 
degrees of closeness to manually chosen sets of 
opinion-bearing and non-opinion-bearing words 
in WordNet and decides its class and strength. 
When the word is equally close to both classes, 
it is hard to decide its subjectivity, and when 
WordNet doesn?t contain a word or its syno-
nyms, such as the word ?antihomosexsual?, we 
fail to classify it.   
The second method, classification of words 
using WSJ texts, is less reliable than the lexical 
method.  However, it does for example success-
fully handle ?antihomosexual?.  Therefore, we 
combined the results of the two methods (collec-
tions 1 and 2), since their different characteris-
63
tics compensate for each other. Later we also 
combine 4000 words from the Columbia word 
list to our final 43700 word list. Since all three 
lists include a strength between 0 and 1, we 
simply averaged them, and normalized the va-
lence strengths to the range from -1 to +1, with 
greater opinion valence closer to 1 (see Table 1).  
Obviously, words that had a high valence 
strength in all three collections had a high over-
all positive strength. When there was a conflict 
vote among three for a word, it aotomatically 
got weak strength. Table 2 shows the distribu-
tion of words according to their sources: Collec-
tion1(C1), Collection2(C2) and Collection3(C3). 
4 Measuring Sentence Valence 
4.1   Two Models 
We are now ready to automatically identify 
opinion-bearing sentences.  We defined several 
models, combining valence scores in different 
ways, and eventually kept two:  
Model 1: Total valence score of all words in a 
sentence  
Model 2: Presence of a single strong valence 
word  
The intuition underlying Model 1 is that sen-
tences in which opinion-bearing words dominate 
tend to be opinion-bearing, while Model 2 re-
flects the idea that even one strong valence word 
is enough.  After experimenting with these mod-
els, we decided to use Model 2.   
How strong is ?strong enough??  To deter-
mine the cutoff threshold (?) on the opinion-
bearing valence strength of words, we experi-
mented on human annotated data.  
4.2   Gold Standard Annotation  
We built two sets of human annotated sentence 
subjectivity data.  Test set A contains 50 sen-
tences about welfare reform, of which 24 sen-
tences are opinion-bearing.  Test set B contains 
124 sentences on two topics (illegal aliens and 
term limits), of which 53 sentences are opinion-
bearing. Three humans classified the sentences 
as either opinion or non-opinion bearing.  We 
calculated agreement for each pair of humans 
and for all three together.  Simple pairwise 
agreement averaged at 0.73, but the kappa score 
was only 0.49.  
Table 3 shows the results of experimenting 
with different combinations of Model 1, Model 
2, and several cutoff values. Recall, precision, F-
score, and accuracy are defined in the normal 
way. Generally, as the cutoff threshold in-
creases, fewer opinion markers are included in 
the lists, and precision increases while recall 
drops. The best F-core is obtained on Test set A, 
Model 2, with ?=0.1 or 0.2 (i.e., being rather 
liberal). 
Table 1. Examples of opinion-bearing/non-opinion-
bearing words  
Adjectives Final score Verbs Final score 
Careless 0.63749 Harm 0.61715 
wasteful 0.49999 Hate 0.53847 
Unpleasant 0.15263 Yearn 0.50000 
Southern -0.2746 Enter -0.4870 
Vertical -0.4999 Crack -0.4999 
Scored -0.5874 combine -0.5852 
Table 2. Distribution of words 
 C1 C2 C3 # words % 
 ?   25605 58.60 
  ?  8202 18.77 
   ? 2291 5.24 
 ? ?  5893 13.49 
  ? ? 834 1.90 
 ?  ? 236 0.54 
 ? ? ? 639 1.46 
Total # 32373 15568 4000 43700 100 
Table 3. Determining ? and performance for various models on gold standard data  
[?: cutoff parameter, R: recall, P: precision, F: F-score, A: accuracy] 
Development Test set A Development Test set B 
 
Model1 Model2 Model1 Model2 
? R P F A R P F A R P F A R P F A 
0.1 0.54 0.61 0.57 0.62 0.91 0.55 0.69 0.6 0.43 0.36 0.39 0.43 0.94 0.45 0.61 0.48 
0.2 0.54 0.61 0.57 0.62 0.91 0.56 0.69 0.62 0.39 0.35 0.37 0.42 0.86 0.45 0.59 0.49 
0.3 0.58 0.6 0.59 0.62 0.83 0.55 0.66 0.6 0.43 0.39 0.41 0.47 0.77 0.45 0.57 0.05 
0.4 0.33 0.8 0.47 0.64 0.33 0.8 0.47 0.64 0.45 0.36 0.4 0.42 0.45 0.36 0.4 0.42 
0.5 0.16 0.8 0.27 0.58 0.16 0.8 0.27 0.58 0.32 0.3 0.31 0.4 0.32 0.3 0.31 0.4 
0.6 0.16 0.8 0.27 0.58 0.16 0.8 0.27 0.58 0.2 0.22 0.21 0.35 0.2 0.22 0.21 0.35 
64
Table 4. Test on MPQA data 
 Accuracy Precision Recall 
 C Ours All C Ours All C Ours All
t=1 0.55 0.63 0.59 0.55 0.61 0.58 0.97 0.85 0.91
t=2 0.57 0.65 0.63 0.56 0.70 0.63 0.92 0.62 0.75
t=3 0.58 0.61 0.62 0.58 0.77 0.69 0.84 0.40 0.56
t=4 0.59 0.55 0.60 0.60 0.83 0.74 0.74 0.22 0.39
t=5 0.59 0.51 0.55 0.62 0.87 0.78 0.63 0.12 0.25
t=6 0.58 0.48 0.52 0.64 0.91 0.82 0.53 0.06 0.15
random 0.50 0.54 0.50 
C: Columbia word list(top 10682 words),  Ours : C1+C2 (top 
10682 words), All: C+Ours (top 19947 words) 
5 Results 
We tested our system on three different data sets.  
First, we ran the system on MPQA data pro-
vided by ARDA. Second, we participated in the 
novelty track of TREC 2003. Third, we ran it on 
our own test data described in Section 4.2. 
5.1  MPQA Test 
The MPQA corpus contains news articles manu-
ally annotated using an annotation scheme for 
subjectivity (opinions and other private states 
that cannot be directly observed or verified. 
(Quirk et al, 1985), such as beliefs, emotions, 
sentiment, speculation, etc.). This corpus was 
collected and annotated as part of the summer 
2002 NRRC Workshop on Multi-Perspective 
Question Answering (MPQA) (Wiebe et al, 
2003) sponsored by ARDA. It contains 535 
documents and 10,657 sentences.   
The annotation scheme contains two main 
components: a type of explicit private state and 
speech event, and a type of expressive subjec-
tive element. Several detailed attributes and 
strengths are annotated as well. More details are 
provided in (Riloff et al, 2003).   
Subjective sentences are defined according to 
their attributes and strength. In order to apply 
our system at the sentence level, we followed 
their definition of subjective sentences. The an-
notation GATE_on is used to mark speech 
events and direct expressions of private states.  
The onlyfactive attribute is used to indicate 
whether the source of the private state or speech 
event is indeed expressing an emotion, opinion 
or other private state. GATE_expressive-
subjectivity annotation marks words and phrases 
that indirectly express a private state. 
In our experiments, our system performed 
relatively well in both precision and recall. We 
interpret our opinion markers as coinciding with 
(enough of) the ?subjective? words of MPQA.  
In order to see the relationship between the 
number of opinion-bearing words in a sentence 
and its classification by MPQA as subjective, 
we varied the threshold number of opinion-
bearing words required for subjectivity. Table 4 
shows accuracy, precision, and recall according 
to the list used and the threshold value t. 
The random row shows the average of ten 
runs of randomly assigning sentences as either 
subjective or objective. As we can see from Ta-
ble 4, our word list which is the combination of 
the Collection1 and Collection2, achieved 
higher accuracy and precision than the Colum-
bia list. However, the Columbia list achieved 
higher recall than ours. For a fair comparison, 
we took top 10682 opinion-bearing words from 
each side and ran the same sentence classifier 
system.2  
5.2 TREC data 
Opinion sentence recognition was a part of the 
novelty track of TREC 2003 (Soboroff and Har-
man, 2003). The task was as follows.  Given a 
TREC topic and an ordered list of 25 documents 
relevant to the topic, find all the opinion-bearing 
sentences. No definition of opinion was pro-
vided by TREC; their assessor?s intuitions were 
considered final. In 2003, there were 22 opinion 
topics containing 21115 sentences in total.  The 
opinion topics generally related to the pros and 
cons of some controversial subject, such as, 
?partial birth abortion ban?, ?Microsoft antitrust 
charges?, ?Cuban child refugee Elian Gonzalez?, 
?marijuana legalization?, ?Clinton relationship 
with Lewinsky?, ?death penalty?, ?adoption 
same-sex partners, and etc. For the opinion top-
ics, a sentence is relevant if it contains an opin-
ion about that subject, as decided by the assessor.  
There was no categorizing of polarity of opinion 
or ranking of sentences by likelihood that they 
contain an opinion. F-score was used to measure 
system performance. 
We submitted 5 separate runs, using different 
models. Our best model among the five was 
Model 2. It performed the second best of the 55 
runs in the task, submitted by 14 participating 
                                                          
2 In comparison, the HP-Subj (height precision subjectivity 
classifier) (Riloff, 2003) produced recall 40.1 and precision 
90.2 on test data using text patterns, and recall 32.9 and 
precision 91.3 without patterns.  These figures are compa-
rable with ours. 
65
institutions. (Interestingly, and perhaps disturb-
ingly, RUN3, which simply returned every sen-
tence as opinion-bearing, fared extremely well, 
coming in 11th.  This model now provides a 
baseline for future research.) After the TREC 
evaluation data was made available, we tested 
Model 1 and Model 2 further. Table 5 shows the 
performance of each model with the two best-
performing cutoff values. 
Table 5. System performance with different models and 
cutoff values on TREC 2003 data 
Model System Parameter ? F-score 
0.2 0.398 Model1 
0.3 0.425 
0.2 0.514 
Model2 
0.3 0.464 
5.3 Test with Our Data 
Section 4.2 described our manual data annota-
tion by 3 humans. Here we used the work of one 
human as development test data for parameter 
tuning.  The other set with 62 sentences on the 
topic of gun control we used as blind test data. 
Although the TREC and MPQA data sets are lar-
ger and provide comparisons with others? work, 
and despite the low kappa agreement values, we 
decided to obtain cutoff values on this data too. 
The graphs in Figure 3 show the performance of 
Models 1 and 2 with different values. 
6 Conclusions and Future Work 
In this paper, we described an efficient auto-
matic algorithm to produce opinion-bearing 
words by combining two methods. The first 
method used only a small set of human-
annotated data. We showed that one can find 
productive synonyms and antonyms of an opin-
ion-bearing word through automatic expansion 
in WordNet and use them as feature sets of a 
classifier. To determine a word?s closeness to 
opinion-bearing or non-opinion-bearing synoym 
set, we also used all synonyms of a given word 
as well as the word itself. An additional method, 
harvesting words from WSJ, can compensate the 
first method.   
Using the resulting list, we experimented with 
different cutoff thresholds in the opinion/non-
opinion sentence classification on 3 different 
test data sets.  Especially on the TREC 2003 
Novelty Track, the system performed well. We 
plan in future work to pursue the automated 
analysis of exhortatory text in order to produce 
detailed argument graphs reflecting their au-
thors? argumentation. 
References 
Aristotle. The Rhetorics and Poetics (trans. W. Rhys Rob-
erts, Modern Library, 1954). 
Fellbaum, C., D. Gross, and K. Miller. 1993. Adjectives  in 
WordNet.  http://www.cosgi.princeton.edu/~wn. 
Kneser, R. and H. Ney. 1995. Improved Backing-off for n-
gram Language Modeling. Proceedings of ICASSP, vol. 
1, 181?184.  
Miller, G.A., R. Beckwith, C. Fellbaum, D. Gross, and K. 
Miller. 1993. Introduction to WordNet: An On-Line 
Lexical Database. http://www.cosgi.princeton. edu/~wn. 
Pang, B. L. Lee, and S. Vaithyanathan, 2002.  Thumbs up? 
Sentiment classification using Machine Learning Tech-
niques. Proceedings of the EMNLP conference. 
Perelman, C. 1970. The New Rhetoric: A Theory of Practi-
cal Reasoning. In The Great Ideas Today. Chicago: En-
cyclopedia Britannica.    
Riloff , E. and J. Wiebe. 2003. Learning Extraction Pat-
terns for Opinion-bearing Expressions. Proceedings of 
the EMNLP-03. 
Riloff, E., J. Wiebe, and T. Wilson 2003. Learning Subjec-
tive Nouns Using Extraction Pattern Bootstrapping. 
Proceedings of CoNLL-03 
Soboroff, I. and D. Harman. 2003. Overview of the TREC 
2003 Novelty Track. Proceedings of TREC-2003. 
Toulmin, S.E., R. Rieke, and A. Janik. 1979. An Introduc-
tion to Reasoning. Macmillan, New York 
Turney, P. 2002. Thumbs Up or Thumbs Down? Semantic 
Orientation Applied to Unsupervised Classification of 
Reviews. Proceedings of the 40th Annual Meeting of the 
ACL, Philadelphia, 417?424. 
Wallace, K. 1975. Topoi and the Problem of Invention. In 
W. Ross Winterowd (ed), Contemporary Rhetoric. Har-
court Brace Jovanovich.  
Wilson, T. and J. Wiebe. 2003. Annotating Opinions in the 
World Press. Proceedings of the ACL SIGDIAL-03. 
Yu, H. and V. Hatzivassiloglou. 2003. Towards Answering 
Opinion Questions: Separating Facts from Opinions and 
Identifying the Polarity of Opinion Sentences. Proceed-
ings of EMNLP-2003. 
Model1
0
0.2
0.4
0.6
0.8
1
0.1 0.2 0.3 0.4 0.5 0.6
cutoff parameter
Recall
Precision
fscore
Accuracy
Model2
0
0.2
0.4
0.6
0.8
1
0.1 0.2 0.3 0.4 0.5 0.6
cutoff parameter
Recall
Precision
fscore
Accuracy
Figure 3. Test on human-annotated sentences 
66
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 200?207,
New York, June 2006. c?2006 Association for Computational Linguistics
Identifying and Analyzing Judgment Opinions 
 
Soo-Min Kim and Eduard Hovy 
USC Information Sciences Institute 
4676 Admiralty Way, Marina del Rey, CA 90292 
{skim, hovy}@ISI.EDU 
Abstract 
In this paper, we introduce a methodology 
for analyzing judgment opinions. We de-
fine a judgment opinion as consisting of a 
valence, a holder, and a topic. We decom-
pose the task of opinion analysis into four 
parts: 1) recognizing the opinion; 2) iden-
tifying the valence; 3) identifying the 
holder; and 4) identifying the topic. In this 
paper, we address the first three parts and 
evaluate our methodology using both in-
trinsic and extrinsic measures. 
1 Introduction 
Recently, many researchers and companies have 
explored the area of opinion detection and analysis. 
With the increased immersion of Internet users has 
come a proliferation of opinions available on the 
web. Not only do we read more opinions from the 
web, such as in daily news editorials, but also we 
post more opinions through mechanisms such as 
governmental web sites, product review sites, news 
group message boards and personal blogs. This 
phenomenon has opened the door for massive 
opinion collection, which has potential impact on 
various applications such as public opinion moni-
toring and product review summary systems. 
   Although in its infancy, many researchers have 
worked in various facets of opinion analysis. Pang 
et al (2002) and Turney (2002) classified senti-
ment polarity of reviews at the document level.  
Wiebe et al (1999) classified sentence level sub-
jectivity using syntactic classes such as adjectives, 
pronouns and modal verbs as features. Riloff and 
Wiebe (2003) extracted subjective expressions 
from sentences using a bootstrapping pattern learn-
ing process. Yu and Hatzivassiloglou (2003) iden-
tified the polarity of opinion sentences using 
semantically oriented words. These techniques 
were applied and examined in different domains, 
such as customer reviews (Hu and Liu 2004) and 
news articles1. These researchers use lists of opin-
ion-bearing clue words and phrases, and then apply 
various additional techniques and refinements. 
Along with many opinion researchers, we par-
ticipated in a large pilot study, sponsored by NIST, 
which concluded that it is very difficult to define 
what an opinion is in general. Moreover, an ex-
pression that is considered as an opinion in one 
domain might not be an opinion in another. For 
example, the statement ?The screen is very big? 
might be a positive review for a wide screen desk-
top review, but it could be a mere fact in general 
newspaper text. This implies that it is hard to apply 
opinion bearing words collected from one domain 
to an application for another domain. One might 
therefore need to collect opinion clues within indi-
vidual domains. In case we cannot simply find 
training data from existing sources, such as news 
article analysis, we need to manually annotate data 
first. 
Most opinions are of two kinds: 1) beliefs about 
the world, with values such as true, false, possible, 
unlikely, etc.; and 2) judgments about the world, 
with values such as good, bad, neutral, wise, fool-
ish, virtuous, etc. Statements like ?I believe that he 
is smart? and ?Stock prices will rise soon? are ex-
amples of beliefs whereas ?I like the new policy on 
social security? and ?Unfortunately this really was 
his year: despite a stagnant economy, he still won 
his re-election? are examples of judgment opinions. 
However, judgment opinions and beliefs are not 
necessarily mutually exclusive. For example, ?I 
think it is an outrage? or ?I believe that he is 
smart? carry both a belief and a judgment. 
In the NIST pilot study, it was apparent that 
human annotators often disagreed on whether a 
belief statement was or was not an opinion. How-
ever, high annotator agreement was seen on judg-
                                                          
1 TREC novelty track 2003 and 2004 
200
ment opinions. In this paper, we therefore focus 
our analysis on judgment opinions only. We hope 
that future work yields a more precise definition of 
belief opinions on which human annotators can 
agree. 
We define a judgment opinion as consisting of 
three elements: a valence, a holder, and a topic. 
The valence, which applies specifically to judg-
ment opinions and not beliefs, is the value of the 
judgment. In our framework, we consider the fol-
lowing valences: positive, negative, and neutral. 
The holder of an opinion is the person, organiza-
tion or group whose opinion is expressed. Finally, 
the topic is the event or entity about which the 
opinion is held. 
In previous work, Choi et al (2005) identify 
opinion holders (sources) using Conditional Ran-
dom Fields (CRF) and extraction patterns. They 
define the opinion holder identification problem as 
a sequence tagging task: given a sequence of words 
( nxxx L21 ) in a sentence, they generate a se-
quence of labels ( nyyy L21 ) indicating whether 
the word is a holder or not. However, there are 
many cases where multiple opinions are expressed 
in a sentence each with its own holder. In those 
cases, finding opinion holders for each individual 
expression is necessary. In the corpus they used, 
48.5% of the sentences which contain an opinion 
have more than one opinion expression with multi-
ple opinion holders. This implies that multiple 
opinion expressions in a sentence occur signifi-
cantly often. A major challenge of our work is 
therefore not only to focus on sentence with only 
one opinion, but also to identify opinion holders 
when there is more than one opinion expressed in a 
sentence. For example, consider the sentence ?In 
relation to Bush?s axis of evil remarks, the German 
Foreign Minister also said, Allies are not satellites, 
and the French Foreign Minister caustically criti-
cized that the United States? unilateral, simplistic 
worldview poses a new threat to the world?. Here, 
?the German Foreign Minister? should be the 
holder for the opinion ?Allies are not satellites? 
and ?the French Foreign Minister? should be the 
holder for ?caustically criticized?. 
In this paper, we introduce a methodology for 
analyzing judgment opinions. We decompose the 
task into four parts: 1) recognizing the opinion; 2) 
identifying the valence; 3) identifying the holder; 
and 4) identifying the topic. For the purposes of 
this paper, we address the first three parts and 
leave the last for future work. Opinions can be ex-
tracted from various granularities such as a word, a 
sentence, a text, or even multiple texts. Each is 
important, but we focus our attention on word-
level opinion detection (Section 2.1) and the detec-
tion of opinions in short emails (Section 3). We 
evaluate our methodology using intrinsic and ex-
trinsic measures. 
The remainder of the paper is organized as fol-
lows. In the next section, we describe our method-
ology addressing the three steps described above, 
and in Section 4 we present our experimental re-
sults. We conclude with a discussion of future 
work. 
2 Analysis of Judgment Opinions  
In this section, we first describe our methodology 
for detecting opinion bearing words and for identi-
fying their valence, which is described in Section 
2.1. Then, in Section 2.2, we describe our algo-
rithm for identifying opinion holders. In Section 3, 
we show how to use our methodology for detecting 
opinions in short emails. 
2.1 Detecting Opinion-Bearing Words 
and Identifying Valence 
We introduce an algorithm to classify a word as 
being positive, negative, or neutral classes. This 
classifier can be used for any set of words of inter-
est and the resulting words with their valence tags 
can help in developing new applications such as a 
public opinion monitoring system. We define an 
opinion-bearing word as a word that carries a posi-
tive or negative sentiment directly such as ?good?, 
?bad?, ?foolish?, ?virtuous?, etc. In other words, 
this is the smallest unit of opinion that can thereaf-
ter be used as a clue for sentence-level or text-level 
opinion detection.  
We treat word sentiment classification into Posi-
tive, Negative, and Neutral as a three-way classifi-
cation problem instead of a two-way classification 
problem of Positive and Negative. By adding the 
third class, Neutral, we can prevent the classifier 
from assigning either positive or negative senti-
ment to weak opinion-bearing words. For example, 
the word ?central? that Hatzivassiloglou and 
McKeown (1997) included as a positive adjective 
is not classified as positive in our system. Instead 
201
we mark it as ?neutral? since it is a weak clue for 
an opinion. If an unknown word has a strong rela-
tionship with the neutral class, we can therefore 
classify it as neutral even if it has some small con-
notation of Positive or Negative as well.  
Approach: We built a word sentiment classifier 
using WordNet and three sets of positive, negative, 
and neutral words tagged by hand. Our insight is 
that synonyms of positive words tend to have posi-
tive sentiment. We expanded those manually se-
lected seed words of each sentiment class by 
collecting synonyms from WordNet. However, we 
cannot simply assume that all the synonyms of 
positive words are positive since most words could 
have synonym relationships with all three senti-
ment classes. This requires us to calculate the 
closeness of a given word to each category and 
determine the most probable class. The following 
formula describes our model for determining the 
category of a word: 
(1)            ).....,|(maxarg)|(maxarg 21 n
cc
synsynsyncPwcP ?  
where c is a category (Positive, Negative, or Neu-
tral) and w is a given word; synn is a WordNet 
synonym of the word w. We calculate this close-
ness as follows; 
(2)   )|()(maxarg
)|()(maxarg
)|()(maxarg)|(maxarg
1
))(,(
 ...3 2 1
?
=
=
=
=
m
k
wsynsetfcount
k
c
n
c
cc
kcfPcP
csynsynsynsynPcP
cwPcPwcP  
where kf  is the kth feature of class c which is also a 
member of the synonym set of the given word w. 
count(fk ,synset(w)) is the total number of occur-
rences of the word feature fk in the synonym set of 
word w. In section 4.1, we describe our manually 
annotated dataset which we used for seed words 
and for our evaluation. 
2.2 Identifying Opinion Holders 
Despite successes in identifying opinion expres-
sions and subjective words/phrases (See Section 
1), there has been less achievement on the factors 
closely related to subjectivity and polarity, such as 
identifying the opinion holder. However, our re-
search indicates that without this information, it is 
difficult, if not impossible, to define ?opinion? ac-
curately enough to obtain reasonable inter-
annotator agreement. Since these factors co-occur 
and mutually reinforce each other, the question 
?Who is the holder of this opinion?? is as impor-
tant as ?Is this an opinion?? or ?What kind of opin-
ion is expressed here??. 
In this section, we describe the automated iden-
tification for opinion holders. We define an opin-
ion holder as an entity (person, organization, 
country, or special group of people) who expresses 
explicitly or implicitly the opinion contained in the 
sentence.  
Previous work that is related to opinion holder 
identification is (Bethard et al 2004) who identify 
opinion propositions and holders. However, their 
opinion is restricted to propositional opinion and 
mostly to verbs. Another related work is (Choi et al 
2005) who use the MPQA corpus2 to learn patterns 
of opinion sources using a graphical model and 
extraction pattern learning. However, they have a 
different task definition from ours. They define the 
task as identifying opinion sources (holders) given 
a sentence, whereas we define it as identifying 
opinion sources given an opinion expression in a 
sentence. We discussed their work in Section 1. 
Data: As training data, we used the MPQA cor-
pus (Wilson and Wiebe, 2003), which contains 
news articles manually annotated by 5 trained an-
notators. They annotated 10657 sentences from 
535 documents, in four different aspects: agent, 
expressive-subjectivity, on, and inside. Expressive-
subjectivity marks words and phrases that indi-
rectly express a private state that is defined as a 
term for opinions, evaluations, emotions, and 
speculations. The on annotation is used to mark 
speech events and direct expressions of private 
states. As for the holder, we use the agent of the 
selected private states or speech events. While 
there are many possible ways to define what opin-
ion means, intuitively, given an opinion, it is clear 
what the opinion holder means. Table 1 shows an 
example of the annotation. In this example, we 
consider the expression ?the U.S. government ?is 
the source of evil? in the world? with an expres-
                                                          
2 http://www.cs.pitt.edu/~wiebe/pubs/ardasummer02/ 
Sentence 
Iraqi Vice President Taha Yassin Rama-
dan, responding to Bush?s ?axis of evil? 
remark, said the U.S. government ?is the 
source of evil? in the world. 
Expressive 
subjectivity
the U.S. government ?is the source of evil? 
in the world 
Strength Extreme 
Source Iraqi Vice President Taha Yassin Ramadan
Table 1: Annotation example 
202
sive-subjectivity tag as an opinion of the holder 
?Iraqi Vice President Taha Yassin Ramadan?.  
Approach: Since more than one opinion may be 
expressed in a sentence, we have to find an opinion 
holder for each opinion expression. For example, 
in a sentence ?A thinks B?s criticism of T is 
wrong?, B is the holder of ?the criticism of T?, 
whereas A is the person who has an opinion that 
B?s criticism is wrong. Therefore, we define our 
task as finding an opinion holder, given an opinion 
expression. Our earlier work (ref suppressed) fo-
cused on identifying opinion expressions within 
text. We employ that system in tandem with the 
one described here. 
To learn opinion holders automatically, we use a 
Maximum Entropy model. Maximum Entropy 
models implement the intuition that the best model 
is the one that is consistent with the set of con-
straints imposed by the evidence but otherwise is 
as uniform as possible (Berger et al 1996). There 
are two ways to model the problem with ME: clas-
sification and ranking. Classification allocates each 
holder candidate to one of a set of predefined 
classes while ranking selects a single candidate as 
answer. This means that classification modeling3 
can select many candidates as answers as long as 
they are marked as true, and does not select any 
candidate if every one is marked as false. In con-
trast, ranking always selects the most probable 
candidate as an answer, which suits our task better. 
Our earlier experiments showed poor performance 
with classification modeling, an experience also 
reported for Question Answering (Ravichandran et 
al. 2003).   
We modeled the problem to choose the most 
probable candidate that maximizes a given condi-
tional probability distribution, given a set of holder 
candidates h
1
h
2
. . . h
N
and opinion expression e. 
The conditional probability P h h
1
h
2
. . . h
N
, e  
can be calculated based on K feature func-
tions f
k
h , h
1
h
2
. ..h
N
, e . We write a decision rule 
for the ranking as follows: 
{ }
{ } ]e),hhh(h,f?[=
e)],hhh|[P(hh
K
=k
Nkk
h
N
h
?
=
1
21
21
...argmax  
...argmax
 
Each k?  is a model parameter indicating the 
weight of its feature function. 
                                                          
3 In our task, there are two classes: holder and non-holder. 
Figure 1 illustrates our holder identification sys-
tem. First, the system generates all possible holder 
candidates, given a sentence and an opinion ex-
pression <E>. After parsing the sentence, it ex-
tracts features such as the syntactic path 
information between each candidate <H> and the 
expression <E> and a distance between <H> and 
<E>. Then it ranks holder candidates according to 
the score obtained by the ME ranking model. Fi-
nally the system picks the candidate with the high-
est score. Below, we describe in turn how to select 
holder candidates and how to select features for the 
training model. 
Holder Candidate Selection: Intuitively, one 
would expect most opinion holders to be named 
entities (PERSON or ORGANIZATION)4. However, 
other common noun phrases can often be opinion 
holders, such as ?the leader?, ?three nations?, and 
?the Arab and Islamic world?. Sometimes, pro-
nouns like he, she, and they that refer to a PERSON, 
or it that refers to an ORGANIZATION or country, 
can be an opinion holder. In our study, we consider 
all noun phrases, including common noun phrases, 
named entities, and pronouns, as holder candidates. 
Feature Selection: Our hypothesis is that there 
exists a structural relation between a holder <H> 
and an expression <E> that can help to identify 
opinion holders. This relation may be represented 
by lexical-level patterns between <H> and <E>, 
but anchoring on surface words might run into the 
data sparseness problem. For example, if we see 
the lexical pattern ?<H> recently criticized <E>? in 
the training data, it is impossible to match the ex-
pression ?<H> yesterday condemned <E>?. These, 
however, have the same syntactic features in our 
                                                          
4 We use BBN?s named entity tagger IdentiFinder to collect 
named entities. 
Sentence             :   w1 w2 w3 w4 w5 w6 w7 w8 w9 ? wn
Opinion expression  <E>  :                  w6 w7 w8
? w2 ... w4 w5 w6 w7 w8 ? w11 w12 w13 ? w18 ? w23 w24 w25 ..
      C1         C2            <E>                       C3                 C4                  C5 
given
Candidate 
holder 
selection
Feature 
extraction:
Parsing
C1            C2    <E>          C3          C4          C5
Rank the candidates by 
ME model 1.C1   2. C5   3.C3  4.C2  5.C4  
Pick the best candidate as a holder C1    
 
Figure 1: Overall system architecture 
203
model. We therefore selected structural features 
from a deep parse, using the Charniak parser. 
After parsing the sentence, we search for the 
lowest common parent node of the words in <H> 
and <E> respectively (<H> and <E> are mostly 
expressed with multiple words). A lowest common 
parent node is a non-terminal node in a parse tree 
that covers all the words in <H> and <E>. Figure 2 
shows a parsed example of a sentence with the 
holder ?China?s official Xinhua news agency? and 
the opinion expression ?accusing?. In this example, 
the lowest common parent of words in <H> is the 
bold NP and the lowest common parent of <E> is 
the bold VBG. We name these nodes Hhead and 
Ehead respectively. After finding these nodes, we 
label them by subscript (e.g., NPH and VBGE) to 
indicate they cover <H> and <E>. In order to see 
how Hhead and Ehead are related to each other in 
the parse tree, we define another node, HEhead, 
which covers both Hhead and Ehead. In the exam-
ple, HEhead is S at the top of the parse tree since it 
covers both NPH and VBGE.  We also label S by 
subscript as SHE. 
To express tree structure for ME training, we 
extract path information between <H> and <E>.  In 
the example, the complete path from Hhead to 
Ehead is ?<H> NP S VP S S VP VBG <E>?.  How-
ever, representing each complete path as a single 
feature produces so many different paths with low 
frequencies that the ME system would learn 
poorly. Therefore, we split the path into three 
parts: HEpath, Hpath an Epath. HEpath is defined 
as a path from HEhead to its left and right child 
nodes that are also parents of Hhead and Ehead. 
Hpath is a path from Hhead and one of its ancestor 
nodes that is a child of HEhead. Similarly, Epath is 
defined as a path from Ehead to one of its ances-
tors that is also a child of HEhead. With this split-
ting, the system can work when any of HEpath, 
Hpath or Epath appeared in the training data, even 
if the entire path from <H> to <E> is unseen. Table 
2 summarizes these concepts with two holder can-
didate examples in the parse tree of Figure 2. 
We also include two non-structural features. The 
first is the type of the candidate, with values NP, 
PERSON, ORGANIZATION, and LOCATION. The 
second feature is the distance between <H> and 
<E>, counted in parse tree words. This is moti-
vated by the intuition that holder candidates tend to 
lie closer to their opinion expression. All features 
are listed in Table 3. We describe the performance 
of the system in Section 4. 
Candidate 1 Candidate 2 
 China?s official Xinu-
hua news agency Bush 
Hhead NPH  NNPH 
Ehead VBGE VBGE 
HEhead SHE VPHE 
Hpath NPH NNPH NPH NPH 
NPH PPH 
Epath VBGE VPE SE SE VPE VBGE VPE SE SE  
HEpath SHE NPH VPE VPHE  PPH SE 
Table 2: Heads and paths for the Figure 2 example  
Features Description 
F1 Type of <H> 
F2 HEpath 
F3 Hpath 
F4 Epath 
F5 Distance between <H> and <E> 
Table 3: Features for ME training 
NP ADVP VP
S
.
NP JJ
NNP NNNN
NNP POS
RB
VBD PP
PP
,
NPIN
NNP
NPIN
PPNP
NNNP NPIN
S
S
official
China ?s
Xinhua news agency
also
weighed
in
sunday
on
NNP POS
choice
Bush ?s
of NNS
words
VP
VBG
PPNP
accusing
the
DT NN
IN S
president
of VP
VBG NP PP
orchestrating
public opinion
JJ NN In advance of possible 
strikes against the three 
countries in an expansion of 
the war against terrorism  
Figure 2: A parsing example 
 
204
Model 1 
? Translate a German email to English  
? Apply English opinion-bearing words 
Model 2 
? Translate English opinion-bearing words to 
German 
 
? Analyze a German email using the German 
opinion-bearing words. 
Table 4: Two models of German Email opinion 
analysis system 
3 Applying our Methodology to German 
Emails 
In this section, we describe a German email analy-
sis system into which we included the opinion-
bearing words from Section 2.1 to detect opinions 
expressed in emails. This system is part of a col-
laboration with the EU-funded project QUALEG 
(Quality of Service and Legitimacy in eGovern-
ment) which aims at enabling local governments to 
manage their policies in a transparent and trustable 
way5. For this purpose, local governments should 
be able to measure the performance of the services 
they offer, by assessing the satisfaction of its citi-
zens. This need makes a system that can monitor 
and analyze citizens? emails essential. The goal of 
our system is to classify emails as neutral or as 
bearing a positive or negative opinion. 
To generate opinion bearing words, we ran the 
word sentiment classifier from Section 2.1 on 8011 
verbs to classify them into 807 positive, 785 nega-
tive, and 6149 neutral. For 19748 adjectives, the 
system classified them into 3254 positive, 303 
negative, and 16191 neutral. Since our opinion-
bearing words are in English and our target system 
is in German, we also applied a statistical word 
alignment technique, GIZA++ 6  (Och and Ney 
2000). Running it on version two of the European 
Parliament corpus, we obtained statistics for 
678,340 German-English word pairs and 577,362 
English-German word pairs. Obtaining these two 
lists of translation pairs allows us to convert Eng-
lish words to German, and German to English, 
without a full document translation system. To util-
ize our English opinion-bearing words in a German 
opinion analysis system, we developed two models, 
                                                          
5 http://www.qualeg.eupm.net/my_spip/index.php 
6 http://www.fjoch.com/GIZA++.html 
outlined in Table 4, each of which is triggered at 
different points in the system.  
In both models, however, we still need to decide 
how to apply opinion-bearing words as clues to 
determine the sentiment of a whole email. Our 
previous work on sentence level sentiment classifi-
cation (ref suppressed) shows that the presence of 
any negative words is a reasonable indication of a 
negative sentence. Since our emails are mostly 
short (the average number of words in each email 
is 19.2) and we avoided collecting weak negative 
opinion clue words, we hypothesize that our previ-
ous sentence sentiment classification study works 
on the email sentiment analysis. This implies that 
an email is negative if it contains more than certain 
number of strong negative words. We tune this 
parameter using our training data. Conversely, if 
an email contains mostly positive opinion-bearing 
words, we classify it as a positive email. We assign 
neutral if an email does not contain any strong 
opinion-bearing words.   
Manually annotated email data was provided by 
our joint research site. This data contains 71 emails 
from citizens regarding a German festival. 26 of 
them contained negative complaints, for example, 
the lack of parking space, and 24 of them were 
positive with complimentary comments to the or-
ganization. The rest of them were marked as 
?questions? such as how to buy festival tickets, 
?only text? of simple comments, ?fuzzy?, and ?dif-
ficult?. So, we carried system experiments on posi-
tive and negative emails with precision and recall. 
We report system results in Section 4. 
4 Experiment Results  
In this section, we evaluate the three systems de-
scribed in Sections 2 and 3: detecting opinion-
bearing words and identifying valence, identifying 
opinion holders, and the German email opinion 
analysis system. 
4.1 Detecting Opinion-bearing Words   
We described a word classification system to de-
tect opinion-bearing words in Section 2.1. To ex-
amine its effectiveness, we annotated 2011 verbs 
and 1860 adjectives, which served as a gold stan-
dard7. These words were randomly selected from a  
                                                          
7 Although nouns and adverbs may also be opinion-bearing, 
we focus only on verbs and adjectives for this study. 
205
collection of 8011 English verbs and 19748 Eng-
lish adjectives. We use training data as seed words 
for the WordNet expansion part of our algorithm 
(described in Section 2.1). Table 5 shows the dis-
tribution of each semantic class. In both verb and 
adjective annotation, neutral class has much more 
words than the positive or negative classes. 
We measured the precision, recall, and F-score 
of our system using 10-fold cross validation. Table 
6 shows the results with 95% confidence bounds. 
Overall (combining positive, neutral and negative), 
our system achieved 77.7% ? 1.2% accuracy on 
verbs and 69.1% ? 2.1% accuracy on adjectives. 
The system has very high precision in the neutral 
category for both verbs (97.2%) and adjectives 
(89.5%), which we interpret to mean that our sys-
tem is really good at filtering non-opinion bearing 
words. Recall is high in all cases but precision var-
ies; very high for neutral and relatively high for 
negative but low for positive. 
4.2 Opinion Holder Identification  
We conducted experiments on 2822 <sentence; 
opinion expression; holder> triples and divided the 
data set into 10 <training; test> sets for cross vali-
dation. For evaluation, we consider to match either 
fully or partially with the holder marked in the test 
data. The holder matches fully if it is a single entity 
(e.g., ?Bush?). The holder matches partially when 
it is part of the multiple entities that make up the 
marked holder. For example, given a marked 
holder ?Michel Sidibe, Director of the Country and  
Regional Support Department of UNAIDS?, we  
consider both ?Michel Sidibe? and ?Director of the 
Country and Regional Support Department of 
UNAIDS? as acceptable answers. 
Our experiments consist of two parts based on 
the candidate selection method. Besides the selec-
tion method we described in Section 2.2, we also 
conducted a separate experiment by excluding pro-
nouns from the candidate list. With the second 
method, the system always produces a non-
pronoun holder as an answer. This selection 
method is useful in some Information Extraction 
application that only cares non-pronoun holders. 
We report accuracy (the percentage of correct 
answers the system found in the test set) to evalu-
ate our system. We also report how many correct 
answers were found within the top2 and top3 sys-
tem answers. Tables 7 and 8 show the system accu-
racy with and without considering pronouns as 
alias candidates, respectively. Table 8 mostly 
shows lower accuracies than Table 7 because test 
data often has only a non-pronoun entity as a 
holder and the system picks a pronoun as its an-
swer. Even if the pronoun refers the same entity 
marked in the test data, the evaluation system 
counts it as wrong because it does not match the 
hand annotated holder. 
To evaluate the effectiveness of our system, we 
set the baseline as a system choosing the closest 
candidate to the expression as a holder without the 
Maximum Entropy decision. The baseline system 
had an accuracy of only 21.3% for candidate selec-
tion over all noun phrases and 23.2% for candidate 
selection excluding pronouns.  
The results show that detecting opinion holders 
is a hard problem, but adopting syntactic features 
(F2, F3, and F4) helps to improve the system. A 
promising avenue of future work is to investigate 
the use of semantic features to eliminate noun 
 Positive Negative Neutral Total 
Verb 69 151 1791 2011 
Adjective 199 304 1357 1860 
Table 5: Word distribution in our gold standard
 Precision Recall F-score 
V 20.5% ? 3.5% 82.4% ? 7.5% 32.3% ? 4.6% 
P 
A 32.4% ? 3.8% 75.5% ? 6.1% 45.1% ? 4.4% 
V 97.2% ? 0.6% 77.6% ? 1.4% 86.3% ? 0.7% 
X 
A 89.5% ? 1.7% 67.1% ? 2.7% 76.6% ? 2.1% 
V 37.8% ? 4.9% 76.2% ? 8.0% 50.1% ? 5.6% 
N 
A 60.0% ? 4.1% 78.5% ? 4.9% 67.8% ? 3.8% 
Table 6: Precision, recall, and F-score on word va-
lence categorization for Positive (P), Negative (N) 
and Neutral (X) verbs (V) and adjectives (A) (with 
95% confidence intervals) 
 Baseline F5 F15 F234 F12345 
Top1 23.2% 21.8% 41.6% 50.8% 52.7%
Top2 39.7% 61.9% 66.3% 67.9%
Top3 52.2% 72.5% 77.1% 77.8%
Table 7: Opinion holder identification results 
(excluding pronouns from candidates) 
 Baseline F5 F15 F234 F12345 
Top1 21.3% 18.9% 41.8% 47.9% 50.6%
Top2 37.9% 61.6% 64.8% 66.7%
Top3 51.2% 72.3% 75.3% 76.0%
Table 8: Opinion holder identification results (All 
noun phrases as candidates) 
206
phrases such as ?cheap energy subsidies? or ?pos-
sible strikes? from the candidate set before we run 
our ME model, since they are less likely to be an 
opinion holder than noun phrases like ?three na-
tions? or ?Palestine people.?  
4.3 German Emails  
For our experiment, we performed 7-fold cross 
validation on a set of 71 emails. Table 9 shows the 
average precision, recall, and F-score. Results 
show that our system identifies negative emails 
(complaints) better than praise. When we chose a 
system parameter for the focus, we intended to find 
negative emails rather than positive emails because 
officials who receive these emails need to act to 
solve problems when people complain but they 
have less need to react to compliments. By high-
lighting high recall of negative emails, we may 
misclassify a neutral email as negative but there is 
also less chance to neglect complaints.  
Category  Model1 Model2 
Precision 0.72 0.55 
Recall 0.40 0.65 
Positive 
(P) 
F-score 0.51 0.60 
Precision 0.55 0.61 
Recall 0.80 0.42 
Negative 
(N) 
F-score 0.65 0.50 
Table 9: German email opinion analysis system results 
5 Conclusion and Future Work 
In this paper, we presented a methodology for ana-
lyzing judgment opinions, which we define as 
opinions consisting of a valence, a holder, and a 
topic. We presented models for recognizing sen-
tences containing judgment opinions, identifying 
the valence of the opinion, and identifying the 
holder of the opinion. Remaining is to also finally 
identify the topic of the opinion. Past tests with 
human annotators indicate that the accuracy of 
identifying valence, holder and topic is much in-
creased when all three are being done simultane-
ously. We plan to investigate a joint model to 
verify this intuition. 
Our past work indicated that, for newspaper 
texts, it is feasible for annotators to identify judg-
ment opinion sentences and for them to identify 
their holders and judgment valences. It is encour-
aging to see that we achieved good results on a 
new genre ? emails sent from citizens to a city co- 
unsel ? and in a new language, German. 
This paper presents a computational framework 
for analyzing judgment opinions. Even though 
these are the most common opinions, it is a pity 
that the research community remains unable to de-
fine belief opinions (i.e., those opinions that have 
values such as true, false, possible, unlikely, etc.) 
with high enough inter-annotator agreement. Only 
once we properly define belief opinion will we be 
capable of building a complete opinion analysis 
system.  
References  
Berger, A, S. Della Pietra, and V. Della Pietra. 1996. A Maximum 
Entropy Approach to Natural Language. Computational Linguis-
tics 22(1). 
Bethard, S., H. Yu, A. Thornton, V. Hatzivassiloglou, and D. Jurafsky. 
2004. Automatic Extraction of Opinion Propositions and their 
Holders. AAAI Spring Symposium on Exploring Attitude and Affect 
in Text. 
Charniak, E. 2000. A Maximum-Entropy-Inspired Parser.  Proc. of 
NAACL-2000.  
Choi, Y., C. Cardie, E. Riloff, and S. Patwardhan. 2005. Identifying 
Sources of Opinions with Conditional Random Fields and Extrac-
tion Patterns. Proc. of Human Language Technology Confer-
ence/Conference on Empirical Methods in Natural Language 
Processing (HLT-EMNLP 2005). 
Esuli, A.  and F. Sebastiani. 2005. Determining the semantic orienta-
tion of terms through gloss classification. Proc. of CIKM-05, 14th 
ACM International Conference on Information and Knowledge 
Management. 
Hatzivassiloglou, V. and McKeown, K. (1997). Predicting the seman-
tic orientation of adjectives. Proc. 35th Annual Meeting of the 
Assoc. for Computational Linguistics (ACL-EACL 97. 
Hu, M. and Liu, B. 2004. Mining and summarizing customer reviews. 
Proc. of KDD?04. pp.168 - 177 
Och, F.J. 2002. Yet Another MaxEnt Toolkit: YASMET 
http://wasserstoff.informatik.rwth-aachen.de/Colleag ues/och/ 
Och, F.J and  Ney, H. 2000. Improved statistical alignment models. 
Proc. of ACL-2000, pp. 440?447, Hong Kong, China. 
Pang, B, L. Lee, and S. Vaithyanathan. 2002. Thumbs up? Sentiment 
Classification using Machine Learning Techniques.  Proc. of 
EMNLP 2002. 
Ravichandran, D., E. Hovy, and F.J. Och. 2003. Statistical QA - clas-
sifier vs re-ranker: What?s the difference? Proc. of the ACL Work-
shop on Multilingual Summarization and Question Answering. 
Riloff, E. and J. Wiebe. 2003. Learning Extraction Patterns for Sub-
jective Expressions. Proc. of EMNLP-03. 
Turney, P. 2002. Thumbs Up or Thumbs Down? Semantic Orientation 
Applied to Unsupervised Classification of Reviews. Proc. of the 
40th Annual Meeting of the ACL, 417?424. 
Wiebe, J, R. Bruce, and T. O?Hara. 1999. Development and use of a 
gold standard data set for subjectivity classifications. Proc. of the 
37th Annual Meeting of the Association for Computational Linguis-
tics (ACL-99), 246?253. 
Wilson, T. and J. Wiebe. 2003. Annotating Opinions in the World 
Press. Proc. of  ACL SIGDIAL-03. 
Yu, H. and V. Hatzivassiloglou. 2003. Towards Answering Opinion 
Questions: Separating Facts from Opinions and Identifying the Po-
larity of Opinion Sentences. Proc. of EMNLP. 
207
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 483?490,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Automatic Identification of Pro and Con Reasons in Online Reviews 
 
Soo-Min Kim and Eduard Hovy 
USC Information Sciences Institute 
4676 Admiralty Way 
Marina del Rey, CA 90292-6695 
{skim, hovy}@ISI.EDU 
 
  
 
Abstract 
In this paper, we present a system that 
automatically extracts the pros and cons 
from online reviews. Although many ap-
proaches have been developed for ex-
tracting opinions from text, our focus 
here is on extracting the reasons of the 
opinions, which may themselves be in the 
form of either fact or opinion. Leveraging 
online review sites with author-generated 
pros and cons, we propose a system for 
aligning the pros and cons to their sen-
tences in review texts. A maximum en-
tropy model is then trained on the result-
ing labeled set to subsequently extract 
pros and cons from online review sites 
that do not explicitly provide them. Our 
experimental results show that our result-
ing system identifies pros and cons with 
66% precision and 76% recall. 
1 Introduction  
Many opinions are being expressed on the Web 
in such settings as product reviews, personal 
blogs, and news group message boards. People 
increasingly participate to express their opinions 
online. This trend has raised many interesting 
and challenging research topics such as subjec-
tivity detection, semantic orientation classifica-
tion, and review classification. 
Subjectivity detection is the task of identifying 
subjective words, expressions, and sentences. 
(Wiebe et al, 1999; Hatzivassiloglou and Wiebe, 
2000; Riloff et al 2003). Identifying subjectivity 
helps separate opinions from fact, which may be 
useful in question answering, summarization, etc. 
Semantic orientation classification is a task of 
determining positive or negative sentiment of 
words (Hatzivassiloglou and McKeown, 1997; 
Turney, 2002; Esuli and Sebastiani, 2005). Sen-
timent of phrases and sentences has also been 
studied in (Kim and Hovy, 2004; Wilson et al, 
2005). Document level sentiment classification is 
mostly applied to reviews, where systems assign 
a positive or negative sentiment for a whole re-
view document (Pang et al, 2002; Turney, 
2002).  
Building on this work, more sophisticated 
problems in the opinion domain have been stud-
ied by many researchers. (Bethard et al, 2004; 
Choi et al, 2005; Kim and Hovy, 2006) identi-
fied the holder (source) of opinions expressed in 
sentences using various techniques. (Wilson et 
al., 2004) focused on the strength of opinion 
clauses, finding strong and weak opinions. 
(Chklovski, 2006) presented a system that aggre-
gates and quantifies degree assessment of opin-
ions scattered throughout web pages. 
 Beyond document level sentiment classifica-
tion in online product reviews, (Hu and Liu, 
2004; Popescu and Etzioni, 2005) concentrated 
on mining and summarizing reviews by extract-
ing opinion sentences regarding product features. 
In this paper, we focus on another challenging 
yet critical problem of opinion analysis, identify-
ing reasons for opinions, especially for opinions 
in online product reviews. The opinion reason 
identification problem in online reviews seeks to 
answer the question ?What are the reasons that 
the author of this review likes or dislikes the 
product?? For example, in hotel reviews, infor-
mation such as ?found 189 positive reviews and 
65 negative reviews? may not fully satisfy the 
information needs of different users. More useful 
information would be ?This hotel is great for 
families with young infants? or ?Elevators are 
grouped according to floors, which makes the 
wait short?. 
This work differs in important ways from 
studies in (Hu and Liu, 2004) and (Popescu and 
Etzioni, 2005). These approaches extract features 
483
of products and identify sentences that contain 
opinions about those features by using opinion 
words and phrases. Here, we focus on extracting 
pros and cons which include not only sentences 
that contain opinion-bearing expressions about 
products and features but also sentences with 
reasons why an author of a review writes the re-
view. Following are examples identified by our 
system. 
 
It creates duplicate files. 
Video drains battery. 
It won't play music from all 
music stores 
 
 Even though finding reasons in opinion-
bearing texts is a critical part of in-depth opinion 
assessment, no study has been done in this par-
ticular vein partly because there is no annotated 
data. Labeling each sentence is a time-
consuming and costly task. In this paper, we pro-
pose a framework for automatically identifying 
reasons in online reviews and introduce a novel 
technique to automatically label training data for 
this task. We assume reasons in an online review 
document are closely related to pros and cons 
represented in the text. We leverage the fact that 
reviews on some websites such as epinions.com 
already contain pros and cons written by the 
same author as the reviews. We use those pros 
and cons to automatically label sentences in the 
reviews on which we subsequently train our clas-
sification system. We then apply the resulting 
system to extract pros and cons from reviews in 
other websites which do not have specified pros 
and cons. 
This paper is organized as follows: Section 2 
describes a definition of reasons in online re-
views in terms of pros and cons. Section 3 pre-
sents our approach to identify them and Section 4 
explains our automatic data labeling process. 
Section 5 describes experimental and results and 
finally, in Section 6, we conclude with future 
work. 
2 Pros and Cons in Online Reviews 
This section describes how we define reasons in 
online reviews for our study. First, we take a 
look at how researchers in Computational Lin-
guistics define an opinion for their studies. It is 
difficult to define what an opinion means in a 
computational model because of the difficulty of 
determining the unit of an opinion. In general, 
researchers study opinion at three different lev-
els: word level, sentence level, and document 
level.  
Word level opinion analysis includes word 
sentiment classification, which views single lexi-
cal items (such as good or bad) as sentiment car-
riers, allowing one to classify words into positive 
and negative semantic categories. Studies in sen-
tence level opinion regard the sentence as a mini-
mum unit of opinion. Researchers try to identify 
opinion-bearing sentences, classify their senti-
ment, and identify opinion holders and topics of 
opinion sentences. Document level opinion 
analysis has been mostly applied to review clas-
sification, in which a whole document written for 
a review is judged as carrying either positive or 
negative sentiment. Many researchers, however, 
consider a whole document as the unit of an 
opinion to be too coarse. 
In our study, we take the approach that a re-
view text has a main opinion (recommendation 
or not) about a given product, but also includes 
various reasons for recommendation or non-
recommendation, which are valuable to identify. 
Therefore, we focus on detecting those reasons in 
online product review. We also assume that rea-
sons in a review are closely related to pros and 
cons expressed in the review. Pros in a product 
review are sentences that describe reasons why 
an author of the review likes the product. Cons 
are reasons why the author doesn?t like the prod-
uct. Based on our observation in online reviews, 
most reviews have both pros and cons even if 
sometimes one of them dominates. 
3 Finding Pros and Cons 
This section describes our approach for find-
ing pro and con sentences given a review text. 
We first collect data from epinions.com and 
automatically label each sentences in the data set. 
We then model our system using one of the ma-
chine learning techniques that have been success-
fully applied to various problems in Natural 
Language Processing. This section also describes 
features we used for our model.   
3.1 Automatically Labeling Pro and Con 
Sentences 
Among many web sites that have product re-
views such as amazon.com and epinions.com, 
some of them (e.g. epinions.com) explicitly state 
pros and cons phrases in their respective catego-
ries by each review?s author along with the re-
view text. First, we collected a large set of <re-
view text, pros, cons> triplets from epin-
484
ions.com.  A review document in epinions.com 
consists of a topic (a product model, restaurant 
name, travel destination, etc.), pros and cons 
(mostly a few keywords but sometimes complete 
sentences), and the review text. Our automatic 
labeling system first collects phrases in pro and 
con fields and then searches the main review text 
in order to collect sentences corresponding to 
those phrases. Figure 1 illustrates the automatic 
labeling process. 
 
Figure 1. The automatic labeling process of 
pros and cons sentences in a review. 
The system first extracts comma-delimited 
phrases from each pro and con field, generating 
two sets of phrases: {P1, P2, ?, Pn} for pros 
and {C1, C2, ?, Cm} for cons. In the example in 
Figure 1, ?beautiful display? can be Pi and ?not 
something you want to drop? can be Cj. Then the 
system compares these phrases to the sentences 
in the text in the ?Full Review?. For each phrase 
in {P1, P2, ?, Pn} and {C1, C2, ?, Cm}, the 
system checks each sentence to find a sentence 
that covers most of the words in the phrase. Then 
the system annotates this sentence with the ap-
propriate ?pro? or ?con? label. All remaining 
sentences with neither label are marked as ?nei-
ther?. After labeling all the epinion data, we use 
it to train our pro and con sentence recognition 
system. 
3.2 Modeling with Maximum Entropy 
Classification 
We use Maximum Entropy classification for the 
task of finding pro and con sentences in a given 
review. Maximum Entropy classification has 
been successfully applied in many tasks in natu-
ral language processing, such as Semantic Role 
labeling, Question Answering, and Information 
Extraction. 
Maximum Entropy models implement the in-
tuition that the best model is the one that is con-
sistent with the set of constraints imposed by the 
evidence but otherwise is as uniform as possible 
(Berger et al, 1996). We modeled the condi-
tional probability of a class c  given a feature 
vector x  as follows: 
)),(exp(
1
)|( ?=
i
ii
x
xcf
Z
xcp ?  
where xZ  is a normalization factor which can be 
calculated by the following: 
 ? ?=
c i
iix xcfZ )),(exp( ?  
In the first equation, ),( xcfi  is a feature func-
tion which has a binary value, 0 or 1. i?  is a 
weight parameter for the feature function 
),( xcfi  and higher value of the weight indicates 
that ),( xcfi  is an important feature for a class 
c . For our system development, we used 
MegaM toolkit 1  which implements the above 
intuition.  
In order to build an efficient model, we sepa-
rated the task of finding pro and con sentences 
into two phases, each being a binary classifica-
tion. The first is an identification phase and the 
second is a classification phase. For this 2-phase 
model, we defined the 3 classes of c  listed in 
Table 1. The identification task separates pro and 
con candidate sentences (CR and PR in Table 1) 
from sentences irrelevant to either of them (NR). 
The classification task then classifies candidates 
into pros (PR) and cons (CR). Section 5 reports 
system results of both phases. 
                                                 
1 http://www.isi.edu/~hdaume/megam/index.html 
Table 1: Classes defined for the classification 
tasks. 
Class 
symbol Description 
PR Sentences related to pros in a review 
CR Sentences related to cons in a review 
NR Sentences related to neither PR nor CR 
 
485
3.3 Features 
The classification uses three types of features: 
lexical features, positional features, and opinion-
bearing word features.  
For lexical features, we use unigrams, bi-
grams, and trigrams collected from the training 
set. They investigate the intuition that there are 
certain words that are frequently used in pro and 
con sentences which are likely to represent rea-
sons why an author writes a review. Examples of 
such words and phrases are: ?because? and 
?that?s why?. 
 For positional features, we first find para-
graph boundaries in review texts using html tags 
such as <br> and <p>. After finding paragraph 
boundaries, we add features indicating the first, 
the second, the last, and the second last sentence 
in a paragraph. These features test the intuition 
used in document summarization that important 
sentences that contain topics in a text have cer-
tain positional patterns in a paragraph (Lin and 
Hovy, 1997), which may apply because reasons 
like pros and cons in a review document are most 
important sentences that summarize the whole 
point of the review.   
For opinion-bearing word features, we used 
pre-selected opinion-bearing words produced by 
a combination of two methods. The first method 
derived a list of opinion-bearing words from a 
large news corpus by separating opinion articles 
such as letters or editorials from news articles 
which simply reported news or events. The sec-
ond method calculated semantic orientations of 
words based on WordNet2 synonyms. In our pre-
vious work (Kim and Hovy, 2005), we demon-
strated that the list of words produced by a com-
bination of those two methods performed very 
well in detecting opinion bearing sentences. Both 
algorithms are described in that paper.  
The motivation for including the list of opin-
ion-bearing words as one of our features is that 
pro and con sentences are quite likely to contain 
opinion-bearing expressions (even though some 
of them are only facts), such as ?The waiting 
time was horrible? and ?Their portion size of 
food was extremely generous!? in restaurant re-
views. We presumed pro and con sentences con-
taining only facts, such as ?The battery lasted 3 
hours, not 5 hours like they advertised?, would 
be captured by lexical or positional features. 
In Section 5, we report experimental results 
with different combinations of these features. 
                                                 
2 http://wordnet.princeton.edu/ 
Table 2 summarizes the features we used for our 
model and the symbols we will use in the rest of 
this paper. 
4 Data 
We collected data from two different sources: 
epinions.com and complaints.com3 (see Section 
3.1 for details about review data in epinion.com). 
Data from epinions.com is mostly used to train 
the system whereas data from complaints.com is 
to test how the trained model performs on new 
data. 
Complaints.com includes a large database of 
publicized consumer complaints about diverse 
products, services, and companies collected for 
over 6 years. Interestingly, reviews in com-
plaint.com are somewhat different from many 
other web sites which are directly or indirectly 
linked to Internet shopping malls such as ama-
zon.com and epinions.com. The purpose of re-
views in complaints.com is to share consumers? 
mostly negative experiences and alert businesses 
to customers feedback. However, many reviews 
in Internet shopping mall related reviews are 
positive and sometimes encourage people to buy 
more products or to use more services.  
Despite its significance, however, there is no 
hand-annotated data that we can use to build a 
system to identify reasons of complaints.com. In 
order to solve this problem, we assume that rea-
sons in complaints reviews are similar to cons in 
other reviews and therefore if we are, somehow, 
able to build a system that can identify cons from 
                                                 
3 http://www.complaints.com/ 
Table 2: Feature summary. 
Feature 
category Description Symbol
Lexical 
Features 
unigrams  
bigrams 
trigrams  
Lex 
Positional 
Features 
the first, the second, 
the last, the second 
to last sentence in a 
paragraph 
Pos 
Opinion-
bearing 
word  
features 
pre-selected opin-
ion-bearing words Op 
 
486
reviews, we can apply it to identify reasons in 
complaints reviews. Based on this assumption, 
we learn a system using the data from epin-
ions.com, to which we can apply our automatic 
data labeling technique, and employ the resulting 
system to identify reasons from reviews in com-
plaint.com. The following sections describe each 
data set. 
4.1 Dataset 1: Automatically Labeled Data 
We collected two different domains of reviews 
from epinions.com: product reviews and restau-
rant reviews. As for the product reviews, we col-
lected 3241 reviews (115029 sentences) about 
mp3 players made by various manufacturers such 
as Apple, iRiver, Creative Lab, and Samsung. 
We also collected 7524 reviews (194393 sen-
tences) about various types of restaurants such as 
family restaurants, Mexican restaurants, fast food 
chains, steak houses, and Asian restaurants. The 
average numbers of sentences in a review docu-
ment are 35.49 and 25.89 respectively.     
The purpose of selecting one of electronics 
products and restaurants as topics of reviews for 
our study is to test our approach in two ex-
tremely different situations. Reasons why con-
sumers like or dislike a product in electronics? 
reviews are mostly about specific and tangible 
features. Also, there are somewhat a fixed set of 
features of a specific type of product, for exam-
ple, ease of use, durability, battery life, photo 
quality, and shutter lag for digital cameras. Con-
sequently, we can expect that reasons in electron-
ics? reviews may share those product feature 
words and words that describe aspects of features 
such as short or long for battery life. This fact 
might make the reason identification task easy.  
 On the other hand, restaurant reviewers talk 
about very diverse aspects and abstract features 
as reasons. For example, reasons such as ?You 
feel like you are in a train station or a busy 
amusement park that is ill-staffed to meet de-
mand!?, ?preferential treatment given to large 
groups?, and ?they don't offer salads of any 
kind? are hard to predict. Also, they seem rarely 
share common keyword features. 
We first automatically labeled each sentence 
in those reviews collected from each domain 
with the features described in Section 3.1. We 
divided the data for training and testing. We then 
trained our model using the training set and 
tested it to see if the system can successfully la-
bel sentences in the test set. 
4.2 Dataset 2: Complaints.com Data 
From the database 4  in complaints.com, we 
searched for the same topics of reviews as Data-
set 1: 59 complaints reviews about mp3 players 
and 322 reviews about restaurants5. We tested 
our system on this dataset and compare the re-
sults against human judges? annotation results. 
Subsection 5.2 reports the evaluation results. 
5 Experiments and Results 
We describe two goals in our experiments in this 
section. The first is to investigate how well our 
pro and con detection model with different fea-
ture combinations performs on the data we col-
lected from epinions.com. The second is to see 
how well the trained model performs on new 
data from a different source, complaint.com.  
For both datasets, we carried out two separate 
sets of experiments, for the domains of mp3 
players and restaurant reviews. We divided data 
into 80% for training, 10% for development, and 
10% for test for our experiments. 
5.1 Experiments on Dataset 1 
Identification step: Table 3 and 4 show pros and 
cons sentences identification results of our sys-
tem for mp3 player and restaurant reviews re-
spectively. The first column indicates which 
combination of features was used for our model 
(see Table 2 for the meaning of Op, Lex, and Pos 
feature categories). We measure the performance 
with accuracy (Acc), precision (Prec), recall 
(Recl), and F-score 6. 
The baseline system assigned all sentences as 
reason and achieved 57.75% and 54.82% of ac-
curacy. The system performed well when it only 
used lexical features in mp3 player reviews 
(76.27% of accuracy in Lex), whereas it per-
formed well with the combination of lexical and 
opinion features in restaurant reviews (Lex+Op 
row in Table 4). 
It was very interesting to see that the system 
achieved a very low score when it only used 
opinion word features. We can interpret this phe-
nomenon as supporting our hypothesis that pro 
and con sentences in reviews are often purely 
                                                 
4 At the time (December 2005), there were total 42593 
complaint reviews available in the database. 
5 Average numbers of sentences in a complaint is 
19.57 for mp3 player reviews and 21.38 for restaurant 
reviews. 
6 We calculated F-score by 
Recall Precision 
Recall Precision   2
+
??  
487
factual. However, opinion features improved 
both precision and recall when combined with 
lexical features in restaurant reviews. It was also 
interesting that experiments on mp3 players re-
views achieved mostly higher scores than restau-
rants. Like the observation we described in Sub-
section 4.1, frequently mentioned keywords of 
product features (e.g. durability) may have 
helped performance, especially with lexical fea-
tures. Another interesting observation is that the 
positional features that helped in topic sentence 
identification did not help much for our task.        
Classification step: Tables 5 and 6 show the 
system results of the pro and con classification 
task. The baseline system marked all sentences 
as pros and achieved 53.87% and 50.71% accu-
racy for each domain. All features performed 
better than the baseline but the results are not as 
good as in the identification task. Unlike the 
identification task, opinion words by themselves 
achieved the best accuracy in both mp3 player 
and restaurant domains. We think opinion words 
played more important roles in classifying pros 
and cons than identifying them. Position features 
helped recognizing con sentences in mp3 player 
reviews.  
5.2 Experiments on Dataset 2 
This subsection reports the evaluation results of 
our system on Dataset 2. Since Dataset 2 from 
complaints.com has no training data, we trained 
a system on Dataset 1 and applied it to Dataset 2. 
Table 3: Pros and cons sentences identification 
results on mp3 player reviews. 
Features 
used 
Acc 
(%) 
Prec 
(%) 
Recl 
(%) 
F-score
(%) 
Op 60.15 65.84 57.31 61.28 
Lex 76.27 66.18 76.42 70.93 
Lex+Pos 63.10 71.14 60.72 65.52 
Lex+Op 62.75 70.64 60.07 64.93 
Lex+Pos+Op 62.23 70.58 59.35 64.48 
Baseline 57.75    
 
Table 4: Reason sentence identification results 
on restaurant reviews. 
Features 
used 
Acc 
(%) 
Prec 
(%) 
Recl 
(%) 
F-score
(%) 
Op 61.64 60.76 47.48 53.31 
Lex 63.77 67.10 51.20 58.08 
Lex+Pos 63.89 67.62 51.70 58.60 
Lex+Op 61.66 69.13 54.30 60.83 
Lex+Pos+Op 63.13 66.80 50.41 57.46 
Baseline 54.82    
 
Table 5: Pros and cons sentences classification results for mp3 player reviews. 
Cons  Pros Features 
used 
Acc 
(%) Prec 
(%) 
Recl 
(%) 
F-score 
(%) 
Prec 
(%) 
Recl 
(%) 
F-score 
(%) 
Op 57.18 54.43 67.10 60.10 61.18 48.00 53.80 
Lex 55.88 55.49 67.45 60.89 56.52 43.88 49.40 
Lex+Pos 55.62 55.26 68.12 61.02 56.24 42.62 48.49 
Lex+Op 55.60 55.46 64.63 59.70 55.81 46.26 50.59 
Lex+Pos+Op 56.68 56.70 62.45 59.44 56.65 50.71 53.52 
baseline 53.87      (mark all as pros) 
 
Table 6: Pros and cons sentences classification results for restaurant reviews. 
Cons Pros Features 
used 
Acc 
(%) Prec 
(%) 
Recl 
(%) 
F-score 
(%) 
Prec 
(%) 
Recl 
(%) 
F-score 
(%) 
Op 57.32 54.78 51.62 53.15 59.32 62.35 60.80 
Lex 55.76 55.94 52.52 54.18 55.60 58.97 57.24 
Lex+Pos 56.07 56.20 53.33 54.73 55.94 58.78 57.33 
Lex+Op 55.88 56.10 52.39 54.18 55.68 59.34 57.45 
Lex+Pos+Op 55.79 55.89 53.17 54.50 55.70 58.38 57.01 
baseline 50.71      (mark all as pros) 
488
A tough question, however, is how to evaluate 
the system results. Since it seemed impossible to 
evaluate the system without involving a human 
judge, we annotated a small set of data manually 
for evaluation purposes. 
Gold Standard Annotation: Four humans 
annotated 3 sets of test sets: Testset 1 with 5 
complaints (73 sentences), Testset 2 with 7 com-
plaints (105 sentences), and Testset 3 with 6 
complaints (85 sentences). Testset 1 and 2 are 
from mp3 player complaints and Testset 3 is 
from restaurant reviews. Annotators marked sen-
tences if they describe specific reasons of the 
complaint. Each test set was annotated by 2 hu-
mans. The average pair-wise human agreement 
was 82.1%7. 
System Performance: Like the human anno-
tators, our system also labeled reason sentences. 
Since our goal is to identify reason sentences in 
complaints, we applied a system modeled as in 
the identification phase described in Subsection 
3.2 instead of the classification phase8. Table 7 
reports the accuracy, precision, and recall of the 
system on each test set. We calculated numbers 
in each A and B column by assuming each anno-
tator?s answers separately as a gold standard.  
 
    
In Table 7, accuracies indicate the agreement 
between the system and human annotators. The 
average accuracy 68.0% is comparable with the 
pair-wise human agreement 82.1% even if there 
is still a lot of room for improvement9. It was 
interesting to see that Testset 3, which was from 
restaurant complaints, achieved higher accuracy 
and recall than the other test sets from mp3 
player complaints, suggesting that it would be 
interesting to further investigate the performance 
                                                 
7 The kappa value was 0.63. 
8 In complaints reviews, we believe that it is more 
important to identify reason sentences than to classify 
because most reasons in complaints are likely to be 
cons. 
9 The baseline system which assigned the majority 
class to each sentence achieved 59.9% of average 
accuracy. 
of reason identification in various other review 
domains such as travel and beauty products in 
future work. Also, even though we were some-
what able to measure reason sentence identifica-
tion in complaint reviews, we agree that we need 
more data annotation for more precise evalua-
tion. 
Finally, the followings are examples of sen-
tences that our system identified as reasons of 
complaints. 
(1) Unfortunately, I find that 
I am no longer comfortable in 
your establishment because of 
the unprofessional, rude, ob-
noxious, and unsanitary treat-
ment from the employees.  
(2) They never get my order 
right the first time and what 
really disgusts me is how they 
handle the food. 
(3) The kids play area at 
Braum's in The Colony, Texas is 
very dirty. 
(4) The only complaint that I 
have is that the French fries 
are usually cold. 
(5) The cashier there had short 
changed me on the payment of my 
bill. 
 
As we can see from the examples, our system 
was able to detect con sentences which contained 
opinion-bearing expressions such as in (1), (2), 
and (3) as well as reason sentences that mostly 
described mere facts as in (4) and (5).      
6 Conclusions and Future work 
This paper proposes a framework for identifying 
one of the critical elements of online product re-
views to answer the question, ?What are reasons 
that the author of a review likes or dislikes the 
product?? We believe that pro and con sentences 
in reviews can be answers for this question. We 
present a novel technique that automatically la-
bels a large set of pro and con sentences in online 
reviews using clue phrases for pros and cons in 
epinions.com in order to train our system. We 
applied it to label sentences both on epin-
ions.com and complaints.com. To investigate the 
reliability of our system, we tested it on two ex-
tremely different review domains, mp3 player 
reviews and restaurant reviews. Our system with 
the best feature selection performs 71% F-score 
in the reason identification task and 61% F-score 
in the reason classification task. 
Table 7: System results on Complaint.com 
reviews (A, B: The first and the second anno-
tator of each set) 
 Testset 1 Testset 2 Testset 3 
 A B A B A B 
Avg 
Acc(%) 65.8 63.0 67.6 61.0 77.6 72.9 68.0 
Prec(%) 50.0 60.7 68.6 62.9 67.9 60.7 61.8 
Recl(%) 56.0 51.5 51.1 44.0 65.5 58.6 54.5 
 
489
The experimental results further show that pro 
and con sentences are a mixture of opinions and 
facts, making identifying them in online reviews 
a distinct problem from opinion sentence identi-
fication. Finally, we also apply the resulting sys-
tem to another review data in complaints.com in 
order to analyze reasons of consumers? com-
plaints.  
In the future, we plan to extend our pro and 
con identification system on other sorts of opin-
ion texts, such as debates about political and so-
cial agenda that we can find on blogs or news 
group discussions, to analyze why people sup-
port a specific agenda and why people are 
against it. 
Reference  
Berger, Adam L., Stephen Della Pietra, and Vin-
cent Della Pietra. 1996. A maximum entropy ap-
proach to natural language processing, Computa-
tional Linguistics, (22-1).  
Bethard, Steven, Hong Yu, Ashley Thornton, Va-
sileios Hatzivassiloglou, and Dan Jurafsky. 
2004. Automatic Extraction of Opinion Proposi-
tions and their Holders, AAAI Spring Symposium 
on Exploring Attitude and Affect in Text: Theo-
ries and Applications. 
Chklovski, Timothy. 2006. Deriving Quantitative 
Overviews of Free Text Assessments on the 
Web. Proceedings of 2006 International Confer-
ence on Intelligent User Interfaces (IUI06). 
Sydney, Australia. 
Choi, Y., Cardie, C., Riloff, E., and Patwardhan, S. 
2005. Identifying Sources of Opinions with 
Conditional Random Fields and Extraction Pat-
terns. Proceedings of HLT/EMNLP-05. 
Esuli, Andrea and Fabrizio Sebastiani. 2005. De-
termining the semantic orientation of terms 
through gloss classification. Proceedings of 
CIKM-05, 14th ACM International Conference 
on Information and Knowledge Management, 
Bremen, DE, pp. 617-624.  
Hatzivassiloglou, Vasileios and Kathleen McKe-
own. 1997. Predicting the Semantic Orientation 
of Adjectives. Proceedings of 35th Annual Meet-
ing of the Assoc. for Computational Linguistics 
(ACL-97): 174-181 
Hatzivassiloglou, Vasileios and Janyce Wiebe. 
2000. Effects of Adjective Orientation and 
Gradability on Sentence Subjectivity. Proceed-
ings of International Conference on Computa-
tional Linguistics (COLING-2000). Saarbr?cken, 
Germany. 
Hu, Minqing and Bing Liu. 2004. Mining and 
summarizing customer reviews". Proceedings of 
the ACM SIGKDD International Conference on 
Knowledge Discovery & Data Mining (KDD-
2004), Seattle, Washington, USA. 
Kim, Soo-Min and Eduard Hovy. 2004. Determin-
ing the Sentiment of Opinions. Proceedings of 
COLING-04. pp. 1367-1373. Geneva, Switzer-
land. 
Kim, Soo-Min and Eduard Hovy. 2005. Automatic 
Detection of Opinion Bearing Words and Sen-
tences. In the Companion Volume of the Pro-
ceedings of IJCNLP-05, Jeju Island, Republic of 
Korea. 
Kim, Soo-Min and Eduard Hovy. 2006. Identifying 
and Analyzing Judgment Opinions. Proceedings 
of HLT/NAACL-2006, New York City, NY. 
Lin, Chin-Yew and Eduard Hovy. 1997. 
Identifying Topics by Position. Proceedings of 
the 5th Conference on Applied Natural Lan-
guage Processing (ANLP97). Washington, D.C. 
Pang, Bo, Lillian Lee, and Shivakumar Vaithyana-
than. 2002. Thumbs up? Sentiment Classifica-
tion using Machine Learning Techniques, Pro-
ceedings of EMNLP 2002. 
Popescu, Ana-Maria, and Oren Etzioni. 2005. 
Extracting Product Features and Opinions from 
Reviews , Proceedings of HLT-EMNLP 2005. 
Riloff, Ellen, Janyce Wiebe, and Theresa Wilson. 
2003. Learning Subjective Nouns Using Extrac-
tion Pattern Bootstrapping. Proceedings of Sev-
enth Conference on Natural Language Learning 
(CoNLL-03). ACL SIGNLL. Pages 25-32. 
Turney, Peter D. 2002. Thumbs up or thumbs 
down? Semantic orientation applied to unsuper-
vised classification of reviews, Proceedings of 
ACL-02, Philadelphia, Pennsylvania, 417-424 
Wiebe, Janyce M., Bruce, Rebecca F., and O'Hara, 
Thomas P. 1999. Development and use of a gold 
standard data set for subjectivity classifications. 
Proceedings of ACL-99. University of Maryland, 
June, pp. 246-253. 
Wilson, Theresa, Janyce Wiebe, and Paul Hoff-
mann. 2005. Recognizing Contextual Polarity in 
Phrase-Level Sentiment Analysis. Proceedings 
of HLT/EMNLP 2005, Vancouver, Canada 
Wilson, Theresa, Janyce Wiebe, and Rebecca Hwa. 
2004. Just how mad are you? Finding strong and 
weak opinion clauses. Proceedings of 19th Na-
tional Conference on Artificial Intelligence 
(AAAI-2004). 
490
Proceedings of the Workshop on Sentiment and Subjectivity in Text, pages 1?8,
Sydney, July 2006. c?2006 Association for Computational Linguistics
 Extracting Opinions, Opinion Holders, and Topics Expressed in 
Online News Media Text 
Soo-Min Kim and Eduard Hovy 
USC Information Sciences Institute 
4676 Admiralty Way 
Marina del Rey, CA 90292-6695 
{skim, hovy}@ISI.EDU 
 
  
 
Abstract 
This paper presents a method for identi-
fying an opinion with its holder and 
topic, given a sentence from online news 
media texts. We introduce an approach of 
exploiting the semantic structure of a 
sentence, anchored to an opinion bearing 
verb or adjective. This method uses se-
mantic role labeling as an intermediate 
step to label an opinion holder and topic 
using data from FrameNet. We decom-
pose our task into three phases: identify-
ing an opinion-bearing word, labeling 
semantic roles related to the word in the 
sentence, and then finding the holder and 
the topic of the opinion word among the 
labeled semantic roles. For a broader 
coverage, we also employ a clustering 
technique to predict the most probable 
frame for a word which is not defined in 
FrameNet. Our experimental results show 
that our system performs significantly 
better than the baseline. 
1 Introduction   
The challenge of automatically identifying opin-
ions in text automatically has been the focus of 
attention in recent years in many different do-
mains such as news articles and product reviews. 
Various approaches have been adopted in subjec-
tivity detection, semantic orientation detection, 
review classification and review mining. Despite 
the successes in identifying opinion expressions 
and subjective words/phrases, there has been less 
achievement on the factors closely related to sub-
jectivity and polarity, such as opinion holder, 
topic of opinion, and inter-topic/inter-opinion 
relationships. This paper addresses the problem 
of identifying not only opinions in text but also 
holders and topics of opinions from online news 
articles. 
Identifying opinion holders is important espe-
cially in news articles. Unlike product reviews in 
which most opinions expressed in a review are 
likely to be opinions of the author of the review, 
news articles contain different opinions of differ-
ent opinion holders (e.g. people, organizations, 
and countries). By grouping opinion holders of 
different stance on diverse social and political 
issues, we can have a better understanding of the 
relationships among countries or among organi-
zations. 
An opinion topic can be considered as an ob-
ject an opinion is about. In product reviews, for 
example, opinion topics are often the product 
itself or its specific features, such as design and 
quality (e.g. ?I like the design of iPod video?, 
?The sound quality is amazing?). In news arti-
cles, opinion topics can be social issues, gov-
ernment?s acts, new events, or someone?s opin-
ions. (e.g., ?Democrats in Congress accused vice 
president Dick Cheney?s shooting accident.?, 
?Shiite leaders accused Sunnis of a mass killing 
of Shiites in Madaen, south of Baghdad.?)  
As for opinion topic identification, little re-
search has been conducted, and only in a very 
limited domain, product reviews. In most ap-
proaches in product review mining, given a 
product (e.g. mp3 player), its frequently men-
tioned features (e.g. sound, screen, and design) 
are first collected and then used as anchor points. 
In this study, we extract opinion topics from 
news articles. Also, we do not pre-limit topics in 
advance. We first identify an opinion and then 
find its holder and topic. We define holder as an 
entity who holds an opinion, and topic, as what 
the opinion is about.   
In this paper, we propose a novel method that 
employs Semantic Role Labeling, a task of iden-
tifying semantic roles given a sentence. We de-
1
compose the overall task into the following 
steps: 
? Identify opinions. 
? Label semantic roles related to the opin-
ions. 
? Find holders and topics of opinions 
among the identified semantic roles. 
? Store <opinion, holder, topic> triples 
into a database. 
In this paper, we focus on the first three subtasks. 
The main contribution of this paper is to pre-
sent a method that identifies not only opinion 
holders but also opinion topics. To achieve this 
goal, we utilize FrameNet data by mapping target 
words to opinion-bearing words and mapping 
semantic roles to holders and topics, and then use 
them for system training. We demonstrate that 
investigating semantic relations between an opin-
ion and its holder and topic is crucial in opinion 
holder and topic identification. 
This paper is organized as follows: Section 2 
briefly introduces related work both in sentiment 
analysis and semantic role labeling. Section 3 
describes our approach for identifying opinions 
and labeling holders and topics by utilizing Fra-
meNet1 data for our task. Section 4 reports our 
experiments and results with discussions and 
finally Section 5 concludes. 
2 Related Work 
This section reviews previous works in both 
sentiment detection and semantic role labeling.  
2.1 Subjectivity and Sentiment Detection 
Subjectivity detection is the task of identifying 
subjective words, expressions, and sentences 
(Wiebe et al, 1999; Hatzivassiloglou and Wiebe, 
2000; Riloff et al, 2003). Identifying subjectiv-
ity helps separate opinions from fact, which may 
be useful in question answering, summarization, 
etc. Sentiment detection is the task of determin-
ing positive or negative sentiment of words (Hat-
zivassiloglou and McKeown, 1997; Turney, 
2002; Esuli and Sebastiani, 2005), phrases and 
sentences (Kim and Hovy, 2004; Wilson et al, 
2005), or documents (Pang et al, 2002; Turney, 
2002).  
Building on this work, more sophisticated 
problems such as opinion holder identification 
have also been studied. (Bethard et al, 2004) 
identify opinion propositions and holders. Their 
                                                 
1 http://framenet.icsi.berkeley.edu/ 
work is similar to ours but different because their 
opinion is restricted to propositional opinion and 
mostly to verbs. Another related works are (Choi 
et al, 2005; Kim and Hovy, 2005). Both of them 
use the MPQA corpus 2  but they only identify 
opinion holders, not topics. 
As for opinion topic identification, little re-
search has been conducted, and only in a very 
limited domain, product reviews. (Hu and Liu, 
2004; Popescu and Etzioni, 2005) present prod-
uct mining algorithms with extracting certain 
product features given specific product types. 
Our paper aims at extracting topics of opinion in 
general news media text. 
2.2 Semantic Role Labeling 
Semantic role labeling is the task of identifying 
semantic roles such as Agent, Patient, Speaker, 
or Topic, in a sentence. A statistical approach for 
semantic role labeling was introduced by (Gildea 
and Jurafsky, 2002). Their system learned se-
mantic relationship among constituents in a sen-
tence from FrameNet, a large corpus of semanti-
cally hand-annotated data. The FrameNet annota-
tion scheme is based on Frame Semantics (Fill-
more, 1976). Frames are defined as ?schematic 
representations of situations involving various 
frame elements such as participants, props, and 
other conceptual roles.? For example, given a 
sentence ?Jack built a new house out of bricks?, 
a semantic role labeling system should identify 
the roles for the verb built such as ?[Agent Jack] 
built [Created_entity  a new house] [Component out of 
bricks]?3. In our study, we build a semantic role 
labeling system as an intermediate step to label 
opinion holders and topics by training it on opin-
ion-bearing frames and their frame elements in 
FrameNet. 
3 Finding Opinions and Their Holders 
and Topics 
For the goal of this study, extracting opinions 
from news media texts with their holders and 
topics, we utilize FrameNet data. The basic idea 
of our approach is to explore how an opinion 
holder and a topic are semantically related to an 
opinion bearing word in a sentence. Given a sen-
tence and an opinion bearing word, our method 
identifies frame elements in the sentence and 
                                                 
2 http://www.cs.pitt.edu/~wiebe/pubs/ardasummer02/ 
3 The verb ?build? is defined under the frame ?Build-
ing? in which Agent, Created_entity, and Components 
are defined as frame elements. 
2
searches which frame element corresponds to the 
opinion holder and which to the topic. The ex-
ample in Figure 1 shows the intuition of our al-
gorithm. 
We decompose our task in 3 subtasks: (1) col-
lect opinion words and opinion-related frames, 
(2) semantic role labeling for those frames, and 
(3) finally map semantic roles to holder and 
topic. Following subsections describe each sub-
task. 
3.1 Opinion Words and Related Frames 
We describe the subtask of collecting opinion 
words and related frames in 3 phases. 
Phase 1: Collect Opinion Words 
In this study, we consider an opinion-bearing 
(positive/negative) word is a key indicator of an 
opinion. Therefore, we first identify opinion-
bearing word from a given sentence and extract 
its holder and topic. Since previous studies indi-
cate that opinion-bearing verbs and adjectives are 
especially efficient for opinion identification, we 
focus on creating a set of opinion-bearing verbs 
and adjectives. We annotated 1860 adjectives 
and 2011 verbs4 by classifying them into posi-
tive, negative, and neutral classes. Words in the 
positive class carry positive valence whereas 
                                                 
4 These were randomly selected from 8011 English 
verbs and 19748 English adjectives. 
those in negative class carry negative valence. 
Words that are not opinion-bearing are classified 
as neutral.  
Note that in our study we treat word sentiment 
classification as a three-way classification prob-
lem instead of a two-way classification problem 
(i.e. positive and negative). By adding the third 
class, neutral, we can prevent the classifier as-
signing either positive or negative sentiment to 
weak opinion-bearing word. For example, the 
word ?central? that Hatzivassiloglou and McKe-
own (1997) marked as a positive adjective is not 
classified as positive by our system. Instead we 
mark it as ?neutral?, since it is a weak clue for an 
opinion. For the same reason, we did not con-
sider ?able? classified as a positive word by Gen-
eral Inquirer5 , a sentiment word lexicon, as a 
positive opinion indicator. Finally, we collected 
69 positive and 151 negative verbs and 199 posi-
tive and 304 negative adjectives. 
Phase 2: Find Opinion-related Frames 
We collected frames related to opinion words 
from the FrameNet corpus. We used FrameNet II 
(Baker et al, 2003) which contains 450 semantic 
frames and more than 3000 frame elements (FE). 
A frame consists of lexical items, called Lexical 
Unit (LU), and related frame elements. For in-
stance, LUs in ATTACK frame are verbs such as 
assail, assault, and attack, and nouns such as in-
vasion, raid, and strike. FrameNet II contains 
                                                 
5 http://www.wjh.harvard.edu/~inquirer/homecat.htm 
Table 1: Example of opinion related frames 
and lexical units 
Frame 
name Lexical units Frame elements 
Desiring
want, wish, hope, 
eager, desire, 
interested, 
Event, 
Experiencer, 
Location_of_event
Emotion
_directed
agitated, amused, 
anguish, ashamed, 
angry, annoyed, 
Event, Topic 
Experiencer, 
Expressor, 
Mental 
_property
absurd, brilliant, 
careless, crazy, 
cunning, foolish 
Behavior, 
Protagonist, 
Domain, Degree 
Subject 
_stimulus
delightful, amazing, 
annoying, amusing, 
aggravating, 
Stimulus, Degree
Experiencer, 
Circumstances, 
  
Figure 1: An overview of our algorithm 
 
 
3
approximately 7500 lexical units and over 
100,000 annotated sentences. 
For each word in our opinion word set de-
scribed in Phase 1, we find a frame to which the 
word belongs. 49 frames for verbs and 43 frames 
for adjectives are collected. Table 1 shows ex-
amples of selected frames with some of the lexi-
cal units those frames cover. For example, our 
system found the frame Desiring from opinion-
bearing words want, wish, hope, etc. Finally, we 
collected 8256 and 11877 sentences related to 
selected opinion bearing frames for verbs and 
adjectives respectively. 
Phase 3: FrameNet expansion  
Even though Phase 2 searches for a correlated 
frame for each verb and adjective in our opinion-
bearing word list, not all of them are defined in 
FrameNet data. Some words such as criticize and 
harass in our list have associated frames (Case 
1), whereas others such as vilify and maltreat do 
not have those (Case 2). For a word in Case 2, 
we use a clustering algorithms CBC (Clustering 
By Committee) to predict the closest (most rea-
sonable) frame of undefined word from existing 
frames. CBC (Pantel and Lin, 2002) was devel-
oped based on the distributional hypothesis (Har-
ris, 1954) that words which occur in the same 
contexts tend to be similar. Using CBC, for ex-
ample, our clustering module computes lexical 
similarity between the word vilify in Case 2 and 
all words in Case 1. Then it picks criticize as a 
similar word, so that we can use for vilify the 
frame Judgment_communication to which criti-
cize belongs and all frame elements defined un-
der Judgment_ communication. 
3.2 Semantic Role Labeling 
To find a potential holder and topic of an opinion 
word in a sentence, we first label semantic roles 
in a sentence.  
Modeling: We follow the statistical ap-
proaches for semantic role labeling (Gildea and 
Jurafsky, 2002; Fleischman et. al, 2003) which 
separate the task into two steps: identify candi-
dates of frame elements (Step 1) and assign se-
mantic roles for those candidates (Step 2). Like 
their intuition, we treated both steps as classifica-
tion problems. We first collected all constituents 
of the given sentence by parsing it using the 
Charniak parser. Then, in Step 1, we classified 
candidate constituents of frame elements from 
non-candidates. In Step 2, each selected candi-
date was thus classified into one of frame ele-
ment types (e.g. Stimulus, Degree, Experiencer, 
etc.). As a learning algorithm for our classifica-
tion model, we used Maximum Entropy (Berger 
et al, 1996). For system development, we used 
MEGA model optimization package6, an imple-
mentation of ME models. 
Data: We collected 8256 and 11877 sentences 
which were associated to opinion bearing frames 
for verbs and adjectives from FrameNet annota-
tion data. Each sentence in our dataset contained 
a frame name, a target predicate (a word whose 
meaning represents aspects of the frame), and 
frame elements labeled with element types. We 
divided the data into 90% for training and 10% 
for test.  
Features used: Table 2 describes features that 
we used for our classification model. The target 
word is an opinion-bearing verb or adjective 
which is associated to a frame. We used the 
Charniak parser to get a phrase type feature of a 
frame element and the parse tree path feature. 
We determined a head word of a phrase by an 
algorithm using a tree head table7, position fea-
ture by the order of surface words of a frame 
element and the target word, and the voice fea-
ture by a simple pattern. Frame name for a target 
                                                 
6 http://www.isi.edu/~hdaume/megam/index.html 
7 http://people.csail.mit.edu/mcollins/papers/heads 
Table 2: Features used for our semantic role 
labeling model. 
Feature Description 
target word 
A predicate whose meaning 
represents the frame (a verb 
or an adjective in our task) 
phrase type Syntactic type of the frame element (e.g. NP, PP) 
head word Syntactic head of the frame element phrase 
parse tree 
path 
A path between the frame 
element and target word in 
the parse tree 
position 
Whether the element phrase 
occurs before or after the tar-
get word 
voice The voice of the sentence (active or passive) 
frame name one of our opinion-related frames 
 
4
word was selected by methods described in 
Phase 2 and Phase 3 in Subsection 3.1.   
3.3 Map Semantic Roles to Holder and 
Topic 
After identifying frame elements in a sentence, 
our system finally selects holder and topic from 
those frame elements. In the example in Table 1, 
the frame ?Desiring? has frame elements such as 
Event (?The change that the Experiencer would 
like to see?), Experiencer (?the person or sentient 
being who wishes for the Event to occur?), Loca-
tion_of_event (?the place involved in the desired 
Event?), Focal_participant (?entity that the Ex-
periencer wishes to be affected by some Event?). 
Among these FEs, we can consider that Experi-
encer can be a holder and Focal_participant can 
be a topic (if any exists in a sentence). We 
manually built a mapping table to map FEs to 
holder or topic using as support the FE defini-
tions in each opinion related frame and the anno-
tated sample sentences. 
4 Experimental Results 
The goal of our experiment is first, to see how 
our holder and topic labeling system works on 
the FrameNet data, and second, to examine how 
it performs on online news media text. The first 
data set (Testset 1) consists of 10% of data de-
scribed in Subsection 3.2 and the second (Testset 
2) is manually annotated by 2 humans. (see Sub-
section 4.2). We report experimental results for 
both test sets. 
4.1 Experiments on Testset 1 
Gold Standard: In total, Testset 1 contains 2028 
annotated sentences collected from FrameNet 
data set. (834 from frames related to opinion 
verb and 1194 from opinion adjectives) We 
measure the system performance using precision 
(the percentage of correct holders/topics among 
system?s labeling results), recall (the percentage 
of correct holders/topics that system retrieved), 
and F-score.  
Baseline: For the baseline system, we applied 
two different algorithms for sentences which 
have opinion-bearing verbs as target words and 
for those that have opinion-bearing adjectives as 
target words. For verbs, baseline system labeled 
a subject of a verb as a holder and an object as a 
topic. (e.g. ?[holder He] condemned [topic the law-
yer].?) For adjectives, the baseline marked the 
subject of a predicate adjective as a holder (e.g. 
?[holder I] was happy?). For the topics of adjec-
tives, the baseline picks a modified word if the 
target adjective is a modifier (e.g. ?That was a 
stupid [topic mistake]?.) and a subject word if the 
adjective is a predicate. ([topic The view] is 
breathtaking in January.) 
Result: Table 3 and 4 show evaluation results 
of our system and the baseline system respec-
tively. Our system performed much better than 
the baseline system in identifying topic and 
holder for both sets of sentences with verb target 
words and those with adjectives. Especially in 
recognizing topics of target opinion-bearing 
words, our system improved F-score from 30.4% 
to 66.5% for verb target words and from 38.2% 
to 70.3% for adjectives. It was interesting to see 
that the intuition that ?A subject of opinion-
bearing verb is a holder and an object is a topic? 
which we applied for the baseline achieved rela-
tively good F-score (56.9%). However, our sys-
tem obtained much higher F-score (78.7%). 
Holder identification task achieved higher F-
score than topic identification which implies that 
identifying topics of opinion is a harder task. 
We believe that there are many complicated 
semantic relations between opinion-bearing 
words and their holders and topics that simple 
relations such as subject and object relations are 
not able to capture. For example, in a sentence 
?Her letter upset me?, simply looking for the 
subjective and objective of the verb upset is not 
enough to recognize the holder and topic. It is 
necessary to see a deeper level of semantic rela-
Table 3. Precision (P), Recall (R), and F-
score (F) of Topic and Holder identification 
for opinion verbs (V) and adjectives (A) on 
Testset 1. 
 Topic  Holder  
 P (%) R (%) F (%) P (%) R (%) F (%)
V  69.1 64.0 66.5 81.9 75.7 78.7 
A  67.5 73.4 70.3 66.2 77.9 71.6 
 
Table 4. Baseline system on Testset 1. 
 Topic  Holder  
 P (%) R (%) F (%) P (%) R (%) F (%)
V 85.5 18.5 30.4 73.7 46.4 56.9 
A  68.2 26.5 38.2 12.0 49.1 19.3 
 
5
tions: ?Her letter? is a stimulus and ?me? is an 
experiencer of the verb upset.  
4.2 Experiments on Testset 2 
Gold Standard: Two humans 8  annotated 100 
sentences randomly selected from news media 
texts. Those news data is collected from online 
news sources such as The New York Times, UN 
Office for the Coordination of Humanitarian Af-
fairs, and BBC News 9 , which contain articles 
about various international affaires. Annotators 
identified opinion-bearing sentences with mark-
ing opinion word with its holder and topic if they 
existed. The inter-annotator agreement in identi-
fying opinion sentences was 82%.  
Baseline: In order to identify opinion-bearing 
sentences for our baseline system, we used the 
opinion-bearing word set introduced in Phase 1 
in Subsection 3.1. If a sentence contains an opin-
ion-bearing verb or adjective, the baseline sys-
tem started looking for its holder and topic. For 
holder and topic identification, we applied the 
                                                 
8 We refer them as Human1 and Human2 for the rest of this 
paper. 
9 www.nytimes.com, www.irinnews.org, and 
www.bbc.co.uk  
 
same baseline algorithm as described in Subsec-
tion 4.1 to Testset 2.  
Result: Note that Testset 1 was collected from 
sentences of opinion-related frames in FrameNet 
and therefore all sentences in the set contained 
either opinion-bearing verb or adjective. (i.e. All 
sentences are opinion-bearing) However, sen-
tences in Testset 2 were randomly collected from 
online news media pages and therefore not all of 
them are opinion-bearing. We first evaluated the 
task of opinion-bearing sentence identification. 
Table 5 shows the system results. When we mark 
all sentences as opinion-bearing, it achieved 43% 
and 38% of accuracy for the annotation result of 
Human1 and Human2 respectively. Our system 
performance (64% and 55%) is comparable with 
the unique assignment.  
We measured the holder and topic identifica-
tion system with precision, recall, and F-score. 
As we can see from Table 6, our system achieved 
much higher precision than the baseline system 
for both Topic and Holder identification tasks. 
However, we admit that there is still a lot of 
room for improvement. 
The system achieved higher precision for topic 
identification, whereas it achieved higher recall 
for holder identification. In overall, our system 
attained higher F-score in holder identification 
task, including the baseline system. Based on F-
score, we believe that identifying topics of opin-
ion is much more difficult than identifying hold-
ers. It was interesting to see the same phenome-
non that the baseline system mainly assuming 
that subject and object of a sentence are likely to 
be opinion holder and topic, achieved lower 
scores for both holder and topic identification 
tasks in Testset 2 as in Testset 1. This implies 
that more sophisticated analysis of the relation-
ship between opinion words (e.g. verbs and ad-
jectives) and their topics and holders is crucial.  
4.3 Difficulties in evaluation 
We observed several difficulties in evaluating 
holder and topic identification. First, the bound-
ary of an entity of holder or topic can be flexible. 
For example, in sentence ?Senator Titus Olupitan 
who sponsored the bill wants the permission.?, 
not only ?Senator Titus Olupitan? but also 
?Senator Titus Olupitan who sponsored the bill? 
is an eligible answer. Second, some correct hold-
ers and topics which our system found were 
evaluated wrong even if they referred the same 
entities in the gold standard because human an-
notators marked only one of them as an answer.  
Table 5. Opinion-bearing sentence identifica-
tion on Testset 2. (P: precision, R: recall, F: 
F-score, A: Accuracy, H1: Human1, H2: 
Human2) 
 P (%) R (%) F (%) A (%) 
H1 56.9 67.4 61.7 64.0 
H2 43.1 57.9 49.4 55.0 
 
 
Table 6: Results of Topic and Holder identi-
fication on Testset 2. (Sys: our system, BL: 
baseline) 
Topic Holder 
 
P(%) R(%) F(%) P(%) R(%) F(%)
H1 64.7 20.8 31.5 47.9 34.0 39.8
Sys 
H2 58.8 7.1 12.7 36.6 26.2 30.5
H1 12.5 9.4 10.7 20.0 28.3 23.4
BL 
H2 23.2 7.1 10.9 14.0 19.0 16.1
 
6
In the future, we need more annotated data for 
improved evaluation.   
5 Conclusion and Future Work 
This paper presented a methodology to identify 
an opinion with its holder and topic given a sen-
tence in online news media texts. We introduced 
an approach of exploiting semantic structure of a 
sentence, anchored to an opinion bearing verb or 
adjective. This method uses semantic role label-
ing as an intermediate step to label an opinion 
holder and topic using FrameNet data. Our 
method first identifies an opinion-bearing word, 
labels semantic roles related to the word in the 
sentence, and then finds a holder and a topic of 
the opinion word among labeled semantic roles. 
There has been little previous study in identi-
fying opinion holders and topics partly because it 
requires a great amount of annotated data. To 
overcome this barrier, we utilized FrameNet data 
by mapping target words to opinion-bearing 
words and mapping semantic roles to holders and 
topics. However, FrameNet has a limited number 
of words in its annotated corpus. For a broader 
coverage, we used a clustering technique to pre-
dict a most probable frame for an unseen word.  
Our experimental results showed that our sys-
tem performs significantly better than the base-
line. The baseline system results imply that opin-
ion holder and topic identification is a hard task. 
We believe that there are many complicated se-
mantic relations between opinion-bearing words 
and their holders and topics which simple rela-
tions such as subject and object relations are not 
able to capture. 
In the future, we plan to extend our list of 
opinion-bearing verbs and adjectives so that we 
can discover and apply more opinion-related 
frames. Also, it would be interesting to see how 
other types of part of speech such as adverbs and 
nouns affect the performance of the system. 
Reference 
Baker, Collin F. and Hiroaki Sato. 2003. The Frame-
Net Data and Software. Poster and Demonstration 
at Association for Computational Linguistics. Sap-
poro, Japan. 
Berger, Adam, Stephen Della Pietra, and Vincent 
Della Pietra. 1996. A maximum entropy approach 
to natural language processing, Computational Lin-
guistics, (22-1).  
Bethard, Steven, Hong Yu, Ashley Thornton, Va-
sileios Hatzivassiloglou, and Dan Jurafsky. 2004. 
Automatic Extraction of Opinion Propositions and 
their Holders, AAAI Spring Symposium on Explor-
ing Attitude and Affect in Text: Theories and Ap-
plications. 
Choi, Y., Cardie, C., Riloff, E., and Patwardhan, S. 
2005. Identifying Sources of Opinions with Condi-
tional Random Fields and Extraction Patterns. Pro-
ceedings of HLT/EMNLP-05. 
Esuli, Andrea and Fabrizio Sebastiani. 2005. Deter-
mining the semantic orientation of terms through 
gloss classification. Proceedings of CIKM-05, 14th 
ACM International Conference on Information and 
Knowledge Management, Bremen, DE, pp. 617-
624.  
Fillmore, C. Frame semantics and the nature of lan-
guage. 1976. In Annals of the New York Academy 
of Sciences: Conferences on the Origin and Devel-
opment of Language and Speech, Volume 280: 20-
32. 
Fleischman, Michael, Namhee Kwon, and Eduard 
Hovy. 2003. Maximum Entropy Models for Fra-
meNet Classification. Proceedings of EMNLP, 
Sapporo, Japan.  
Gildea, D. and Jurafsky, D. Automatic Labeling of 
semantic roles. 2002. In Computational Linguis-
tics. 28(3), 245-288. 
Harris, Zellig, 1954. Distributional structure. Word, 
10(23) :146--162. 
Hatzivassiloglou, Vasileios and Kathleen McKeown. 
1997. Predicting the Semantic Orientation of Ad-
jectives. Proceedings of 35th Annual Meeting of 
the Assoc. for Computational Linguistics (ACL-
97): 174-181 
Hatzivassiloglou, Vasileios and Wiebe, Janyce. 2000. 
Effects of Adjective Orientation and Gradability on 
Sentence Subjectivity. Proceedings of Interna-
tional Conference on Computational Linguistics 
(COLING-2000). Saarbr?cken, Germany. 
Hu, Minqing and Bing Liu. 2004. Mining and summa-
rizing customer reviews". Proceedings of the ACM 
SIGKDD International Conference on Knowledge 
Discovery & Data Mining (KDD-2004), Seattle, 
Washington, USA. 
Kim, Soo-Min and Eduard Hovy. 2004. Determining 
the Sentiment of Opinions. Proceedings of COL-
ING-04. pp. 1367-1373. Geneva, Switzerland. 
Kim, Soo-Min and Eduard Hovy. 2005. Identifying 
Opinion Holders for Question Answering in Opin-
ion Texts. Proceedings of AAAI-05 Workshop on 
Question Answering in Restricted Domains 
Pang, Bo, Lillian Lee, and Shivakumar Vaithyana-
than. 2002. Thumbs up? Sentiment Classification 
using Machine Learning Techniques, Proceedings 
of EMNLP-2002. 
7
Pantel, Patrick and Dekang Lin. 2002. Discovering 
Word Senses from Text. Proceedings of ACM Con-
ference on Knowledge Discovery and Data Mining. 
(KDD-02). pp. 613-619. Edmonton, Canada. 
Popescu, Ana-Maria and Oren Etzioni. 2005. 
Extracting Product Features and Opinions from 
Reviews , Proceedings of HLT-EMNLP 2005. 
Riloff, Ellen, Janyce Wiebe, and Theresa Wilson. 
2003. Learning Subjective Nouns Using Extraction 
Pattern Bootstrapping. Proceedings of Seventh 
Conference on Natural Language Learning 
(CoNLL-03). ACL SIGNLL. Pages 25-32. 
Turney, Peter D. 2002. Thumbs up or thumbs down? 
Semantic orientation applied to unsupervised clas-
sification of reviews, Proceedings of ACL-02, 
Philadelphia, Pennsylvania, 417-424 
Wiebe, Janyce, Bruce M., Rebecca F., and Thomas P. 
O'Hara. 1999. Development and use of a gold stan-
dard data set for subjectivity classifications. Pro-
ceedings of ACL-99. University of Maryland, June, 
pp. 246-253. 
Wilson, Theresa, Janyce Wiebe, and Paul Hoffmann. 
2005. Recognizing Contextual Polarity in Phrase-
Level Sentiment Analysis. Proceedings of 
HLT/EMNLP 2005, Vancouver, Canada 
 
8
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 423?430,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Automatically Assessing Review Helpfulness 
 
Soo-Min Kim?, Patrick Pantel?, Tim Chklovski?, Marco Pennacchiotti? 
?Information Sciences Institute 
University of Southern California 
4676 Admiralty Way 
Marina del Rey, CA  90292 
{skim,pantel,timc}@isi.edu 
?ART Group - DISP 
University of Rome ?Tor Vergata? 
Viale del Politecnico 1 
Rome, Italy 
pennacchiotti@info.uniroma2.it
 
 
 
Abstract 
User-supplied reviews are widely and 
increasingly used to enhance e-
commerce and other websites. Because 
reviews can be numerous and varying in 
quality, it is important to assess how 
helpful each review is. While review 
helpfulness is currently assessed manu-
ally, in this paper we consider the task 
of automatically assessing it. Experi-
ments using SVM regression on a vari-
ety of features over Amazon.com 
product reviews show promising results, 
with rank correlations of up to 0.66. We 
found that the most useful features in-
clude the length of the review, its uni-
grams, and its product rating. 
1 Introduction 
Unbiased user-supplied reviews are solicited 
ubiquitously by online retailers like Ama-
zon.com, Overstock.com, Apple.com and Epin-
ions.com, movie sites like imdb.com, traveling 
sites like citysearch.com, open source software 
distributors like cpanratings.perl.org, and count-
less others. Because reviews can be numerous 
and varying in quality, it is important to rank 
them to enhance customer experience. 
In contrast with ranking search results, assess-
ing relevance when ranking reviews is of little 
importance because reviews are directly associ-
ated with the relevant product or service. Instead, 
a key challenge when ranking reviews is to de-
termine which reviews the customers will find 
helpful. 
Most websites currently rank reviews by their 
recency or product rating (e.g., number of stars 
in Amazon.com reviews). Recently, more sophis-
ticated ranking schemes measure reviews by their 
helpfulness, which is typically estimated by hav-
ing users manually assess it. For example, on 
Amazon.com, an interface allows customers to 
vote whether a particular review is helpful or not. 
Unfortunately, newly written reviews and re-
views with few votes cannot be ranked as several 
assessments are required in order to properly es-
timate helpfulness. For example, for all MP3 
player products on Amazon.com, 38% of the 
20,919 reviews received three or fewer helpful-
ness votes. Another problem is that low-traffic 
items may never gather enough votes. Among the 
MP3 player reviews that were authored at least 
three months ago on Amazon.com, still only 31% 
had three or fewer helpfulness votes. 
It would be useful to assess review helpfulness 
automatically, as soon as the review is written. 
This would accelerate determining a review?s 
ranking and allow a website to provide rapid 
feedback to review authors. 
In this paper, we investigate the task of auto-
matically predicting review helpfulness using a 
machine learning approach. Our main contribu-
tions are: 
? A system for automatically ranking reviews 
according to helpfulness; using state of the art 
SVM regression, we empirically evaluate our 
system on a real world dataset collected from 
Amazon.com on the task of reconstructing the 
helpfulness ranking; and 
? An analysis of different classes of features 
most important to capture review helpful-
ness; including structural (e.g., html tags, 
punctuation, review length), lexical (e.g., n-
grams), syntactic (e.g., percentage of verbs and 
nouns), semantic (e.g., product feature men-
tions), and meta-data (e.g., star rating). 
2 Relevant Work 
The task of automatically assessing product re-
view helpfulness is related to these broader areas 
423
of research: automatic analysis of product re-
views, opinion and sentiment analysis, and text 
classification. 
In the thriving area of research on automatic 
analysis and processing of product reviews (Hu 
and Liu 2004; Turney 2002; Pang and Lee 2005), 
little attention has been paid to the important task 
studied here ? assessing review helpfulness. Pang 
and Lee (2005) have studied prediction of prod-
uct ratings, which may be particularly relevant 
due to the correlation we find between product 
rating and the helpfulness of the review (dis-
cussed in Section 5). However, a user?s overall 
rating for the product is often already available. 
Helpfulness, on the other hand, is valuable to 
assess because it is not explicitly known in cur-
rent approaches until many users vote on the 
helpfulness of a review.  
In opinion and sentiment analysis, the focus is 
on distinguishing between statements of fact vs. 
opinion, and on detecting the polarity of senti-
ments being expressed. Many researchers have 
worked in various facets of opinion analysis. 
Pang et al (2002) and Turney (2002) classified 
sentiment polarity of reviews at the document 
level.  Wiebe et al (1999) classified sentence 
level subjectivity using syntactic classes such as 
adjectives, pronouns and modal verbs as features.  
Riloff and Wiebe (2003) extracted subjective 
expressions from sentences using a bootstrapping 
pattern learning process. Yu and Hatzivassi-
loglou (2003) identified the polarity of opinion 
sentences using semantically oriented words. 
These techniques were applied and examined in 
different domains, such as customer reviews (Hu 
and Liu 2004) and news articles (TREC novelty 
track 2003 and 2004).  
In text classification, systems typically use 
bag-of-words models, although there is some 
evidence of benefits when introducing relevant 
semantic knowledge (Gabrilovich and Mark-
ovitch, 2005). In this paper, we explore the use of 
some semantic features for review helpfulness 
ranking. Another potential relevant classification 
task is academic and commercial efforts on de-
tecting email spam messages1, which aim to cap-
ture a much broader notion of helpfulness. For an 
SVM-based approach, see (Drucker et al 1999).  
Finally, a related area is work on automatic es-
say scoring, which seeks to rate the quality of an 
essay (Attali and Burstein 2006; Burstein et al 
2004). The task is important for reducing the 
human effort required in scoring large numbers 
                                                     
1 See http://www.ceas.cc/, http://spamconference.org/  
of student essays regularly written for standard 
tests such as the GRE. The exact scoring ap-
proaches developed in commercial systems are 
often not disclosed. However, more recent work 
on one of the major systems, e-rater 2.0, has fo-
cused on systematizing and simplifying the set of 
features used (Attali and Burstein 2006). Our 
choice of features to test was partially influenced 
by the features discussed by Attali and Burstein. 
At the same time, due to differences in the tasks, 
we did not use features aimed at assessing essay 
structure such as discourse structure analysis fea-
tures. Our observations suggest that even helpful 
reviews vary widely in their discourse structure. 
We present the features which we have used be-
low, in Section 3.2. 
3 Modeling Review Helpfulness 
In this section, we formally define the learning 
task and we investigate several features for as-
sessing review helpfulness. 
3.1 Task Definition 
Formally, given a set of reviews R for a particu-
lar product, our task is to rank the reviews ac-
cording to their helpfulness. We define a review 
helpfulness function, h, as: 
 ( ) ( )( ) ( )rratingrrating
rrating
Rrh
?+
+
+=?  (1) 
where rating+(r) is the number of people that will 
find a review helpful and rating-(r) is the number 
of people that will find the review unhelpful. For 
evaluation, we resort to estimates of h from man-
ual review assessments on websites like Ama-
zon.com, as described in Section 4. 
3.2 Features 
One aim of this paper is to investigate how well 
different classes of features capture the helpful-
ness of a review. We experimented with various 
features organized in five classes: Structural, 
Lexical, Syntactic, Semantic, and Meta-data. Be-
low we describe each feature class in turn. 
Structural Features 
Structural features are observations of the docu-
ment structure and formatting. Properties such as 
review length and average sentence length are 
hypothesized to relate structural complexity to 
helpfulness. Also, HTML formatting tags could 
help in making a review more readable, and con-
sequently more helpful. We experimented with 
the following features: 
424
? Length (LEN): The total number of tokens in a 
syntactic analysis2 of the review. 
? Sentential (SEN): Observations of the sen-
tences, including the number of sentences, the 
average sentence length, the percentage of 
question sentences, and the number of excla-
mation marks. 
? HTML (HTM): Two features for the number of 
bold tags <b> and line breaks <br>. 
Lexical Features 
Lexical features capture the words observed in 
the reviews. We experimented with two sets of 
features: 
? Unigram (UGR): The tf-idf statistic of each 
word occurring in a review. 
? Bigram (BGR): The tf-idf statistic of each bi-
gram occurring in a review. 
For both unigrams and bigrams, we used lemma-
tized words from a syntactic analysis of the re-
views and computed the tf-idf statistic (Salton 
and McGill 1983) using the following formula: 
 ( )
N
idftf
idftf
log?=  
where N is the number of tokens in the review. 
Syntactic Features 
Syntactic features aim to capture the linguistic 
properties of the review. We grouped them into 
the following feature set: 
? Syntax (SYN): Includes the percentage of 
parsed tokens that are open-class (i.e., nouns, 
verbs, adjectives and adverbs), the percentage 
of tokens that are nouns, the percentage of to-
kens that are verbs, the percentage of tokens 
that are verbs conjugated in the first person, 
and the percentage of tokens that are adjectives 
or adverbs. 
Semantic Features 
Most online reviews are fairly short; their spar-
sity suggests that bigram features will not per-
form well (which is supported by our 
experiments described in Section 5.3). Although 
semantic features have rarely been effective in 
many text classification problems (Moschitti and 
Basili 2004), there is reason here to hypothesize 
that a specialized vocabulary of important words 
might help with the sparsity. We hypothesized 
                                                     
2  Reviews are analyzed using the Minipar dependency 
parser (Lin 1994). 
that good reviews will often contain: i) refer-
ences to the features of a product (e.g., the LCD 
and resolution of a digital camera), and ii) men-
tions of sentiment words (i.e., words that express 
an opinion such as ?great screen?). Below we 
describe two families of features that capture 
these semantic observations within the reviews: 
? Product-Feature (PRF): The features of prod-
ucts that occur in the review, e.g., capacity of 
MP3 players and zoom of a digital camera. 
This feature counts the number of lexical 
matches that occur in the review for each prod-
uct feature. There is no trivial way of obtaining 
a list of all the features of a product. In Section 
5.1 we describe a method for automatically ex-
tracting product features from Pro/Con listings 
from Epinions.com. Our assumption is that 
pro/cons are the features that are important for 
customers (and hence should be part of a help-
ful review). 
? General-Inquirer (GIW): Positive and negative 
sentiment words describing products or prod-
uct features (e.g., ?amazing sound quality? and 
?weak zoom?). The intuition is that reviews 
that analyze product features are more helpful 
than those that do not. We try to capture this 
analysis by extracting sentiment words using 
the publicly available list of positive and nega-
tive sentiment words from the General Inquirer 
Dictionaries3. 
Meta-Data Features 
Unlike the previous four feature classes, meta-
data features capture observations which are in-
dependent of the text (i.e., unrelated with linguis-
tic features). We consider the following feature: 
? Stars (STR): Most websites require reviewers 
to include an overall rating for the products 
that they review (e.g., star ratings in Ama-
zon.com). This feature set includes the rating 
score (STR1) as well as the absolute value of 
the difference between the rating score and the 
average rating score given by all reviewers 
(STR2). 
We differentiate meta-data features from seman-
tic features since they require external knowl-
edge that may not be available from certain 
review sites. Nowadays, however, most sites that 
collect user reviews also collect some form of 
product rating (e.g., Amazon.com, Over-
stock.com, and Apple.com). 
                                                     
3 http://www.wjh.harvard.edu/~inquirer/homecat.htm 
425
4 Ranking System 
In this paper, we estimate the helpfulness func-
tion in Equation 1 using user ratings extracted 
from Amazon.com, where rating+(r) is the num-
ber of unique users that rated the review r as 
helpful and rating-(r) is the number of unique 
users that rated r as unhelpful. 
Reviews from Amazon.com form a gold stan-
dard labeled dataset of {review, h(review)} pairs 
that can be used to train a supervised machine 
learning algorithm. In this paper, we applied an 
SVM (Vapnik 1995) package on the features ex-
tracted from reviews to learn the function h. 
Two natural options for learning helpfulness 
according to Equation 1 are SVM Regression and 
SVM Ranking (Joachims 2002). Though learning 
to rank according to helpfulness requires only 
SVM Ranking, the helpfulness function provides 
non-uniform differences between ranks in the 
training set. Also, in practice, many products 
have only one review, which can serve as train-
ing data for SVM Regression but not SVM Rank-
ing. Furthermore, in large sites such as 
Amazon.com, when new reviews are written it is 
inefficient to re-rank all previously ranked re-
views. We therefore choose SVM Regression in 
this paper. We describe the exact implementation 
in Section 5.1. 
After the SVM is trained, for a given product 
and its set of reviews R, we rank the reviews of R 
in decreasing order of h(r), r ? R. 
Table 1 shows four sample reviews for the 
iPod Photo 20GB product from Amazon.com, 
their total number of helpful and unhelpful votes, 
as well as their rank according to the helpfulness 
score h from both the gold standard from Ama-
zon.com and using the SVM prediction of our 
best performing system described in Section 5.2. 
5 Experimental Results 
We empirically evaluate our review model and 
ranking system, described in Section 3 and Sec-
tion 4, by comparing the performance of various 
feature combinations on products mined from 
Amazon.com. Below, we describe our experi-
mental setup, present our results, and analyze 
system performance. 
5.1 Experimental Setup 
We describe below the datasets that we extracted 
from Amazon.com, the implementation of our 
SVM system, and the method we used for ex-
tracting features of reviews. 
Extraction and Preprocessing of Datasets 
We focused our experiments on two products 
from Amazon.com: MP3 Players and Digital 
Cameras. 
Using Amazon Web Services API, we col-
lected reviews associated with all products in the 
MP3 Players and Digital Cameras categories. 
For MP3 Players, we collected 821 products and 
33,016 reviews; for Digital Cameras, we col-
lected 1,104 products and 26,189 reviews. 
In most retailer websites like Amazon.com, 
duplicate reviews, which are quite frequent, skew 
statistics and can greatly affect a learning algo-
rithm. Looking for exact string matches between 
reviews is not a sufficient filter since authors of 
duplicated reviews often make small changes to 
the reviews to avoid detection. We built a simple 
filter that compares the distribution of word bi-
grams across each pair of reviews. A pair is 
deemed a duplicate if more than 80% of their 
bigrams match. 
Also, whole products can be duplicated. For 
different product versions, such as iPods that can 
come in black or white models, reviews on Ama-
zon.com are duplicated between them. We filter 
Table 1. Sample of 4 out of 43 reviews for the iPod Photo 20GB product from Ama-
zon.com along with their ratings as well as their helpfulness ranks (from both the gold 
standard from Amazon.com and the SVM prediction of our best performing system de-
scribed in Section 5.2). 
RANK(h) 
REVIEW TITLE 
HELPFUL 
VOTES 
UNHELPFUL 
VOTES GOLD 
STANDARD 
SVM 
PREDICTION 
?iPod Moves to All-color Line-up? 215 11 7 1 
?iPod: It's NOT Music to My Ears? 11 13 25 30 
?The best thing I ever bought? 22 32 26 27 
?VERY disappointing? 1 18 40 40 
 
426
out complete products where each of its reviews 
is detected as a duplicate of another product (i.e., 
only one iPod version is retained). 
The filtering of duplicate products and dupli-
cate reviews discarded 85 products and 12,097 
reviews for MP3 Players and 38 products and 
3,692 reviews for Digital Cameras. 
In order to have accurate estimates for the 
helpfulness function in Equation 1, we filtered 
out any review that did not receive at least five 
user ratings (i.e., reviews where less than five 
users voted it as helpful or unhelpful are filtered 
out). This filtering was performed before dupli-
cate detection and discarded 45.7% of the MP3 
Players reviews and 32.7% of the Digital Cam-
eras reviews. 
Table 2 describes statistics for the final data-
sets after the filtering steps. 10% of products for 
both datasets were withheld as development cor-
pora and the remaining 90% were randomly 
sorted into 10 sets for 10-fold cross validation. 
SVM Regression 
For our regression model, we deployed the state 
of the art SVM regression tool SVMlight 
(Joachims 1999). We tested on the development 
sets various kernels including linear, polynomial 
(degrees 2, 3, and 4), and radial basis function 
(RBF). The best performing kernel was RBF and 
we report only these results in this paper (per-
formance was measured using Spearman?s corre-
lation coefficient, described in Section 5.2). 
We tuned the RBF kernel parameters C (the 
penalty parameter) and ? (the kernel width hy-
perparameter) performing full grid search over 
the 110 combinations of exponentially spaced 
parameter pairs (C,?) following (Hsu et al 2003). 
Feature Extraction 
To extract the features described in Section 3.2, 
we preprocessed each review using the Minipar 
dependency parser (Lin 1994). We used the 
parser tokenization, sentence breaker, and syn-
tactic categorizations to generate the Length, 
Sentential, Unigram, Bigram, and Syntax feature 
sets. 
In order to count the occurrences of product 
features for the Product-Feature set, we devel-
oped an automatic way of mining references to 
product features from Epinions.com. On this 
website, user-generated product reviews include 
explicit lists of pros and cons, describing the best 
and worst aspects of a product. For example, for 
MP3 players, we found the pro ?belt clip? and 
the con ?Useless FM tuner?. Our assumption is 
that the pro/con lists tend to contain references to 
the product features that are important to cus-
tomers, and hence their occurrence in a review 
may correlate with review helpfulness. We fil-
tered out all single-word entries which were in-
frequently seen (e.g., hold, ever). After splitting 
and filtering the pro/con lists, we were left with a 
total of 9,110 unique features for MP3 Players 
and 13,991 unique features for Digital Cameras. 
The Stars feature set was created directly from 
the star ratings given by each author of an Ama-
zon.com review. 
For each feature measurement f, we applied 
the following standard transformation: 
 ( )1ln +f  
and then scaled each feature between [0, 1] as 
suggested in (Hsu et al 2003). 
We experimented with various combinations 
of feature sets. Our results tables use the abbre-
viations presented in Section 3.2. For brevity, we 
report the combinations which contributed to our 
best performing system and those that help assess 
the power of the different feature classes in cap-
turing helpfulness. 
5.2 Ranking Performance 
Evaluating the quality of a particular ranking is 
difficult since certain ranking intervals can be 
more important than others (e.g., top-10 versus 
bottom-10) We adopt the Spearman correlation 
coefficient ? (Spearman 1904) since it is the 
most commonly used measure of correlation be-
tween two sets of ranked data points4. 
For each fold in our 10-fold cross-validation 
experiments, we trained our SVM system using 9 
folds. For the remaining test fold, we ranked each 
product?s reviews according to the SVM predic-
tion (described in Section 4) and computed the ? 
                                                     
4 We used the version of Spearman?s correlation coeffi-
cient that allows for ties in rankings. See Siegel and Cas-
tellan (1988) for more on alternate rank statistics such as 
Kendall?s tau. 
Table 2. Overview of filtered datasets extracted 
from Amazon.com. 
 MP3 PLAYERS 
DIGITAL 
CAMERAS 
Total Products 736 1066 
Total Reviews 11,374 14,467 
Average Reviews/Product 15.4 13.6 
Min/MaxReviews/Product 1 / 375 1 / 168 
 
427
correlation between the ranking and the gold 
standard ranking from the test fold5. 
Although our task definition is to learn review 
rankings according to helpfulness, as an interme-
diate step the SVM system learns to predict the 
absolute helpfulness score for each review. To 
test the correlation of this score against the gold 
standard, we computed the standard Pearson cor-
relation coefficient. 
Results show that the highest performing fea-
ture combination consisted of the Length, the 
Unigram, and the Stars feature sets. Table 3 re-
ports the evaluation results for every combination 
of these features with 95% confidence bounds. 
Of the three features alone, neither was statisti-
cally more significant than the others. Examining 
each pair combination, only the combination of 
length with stars outperformed the others. Sur-
prisingly, adding unigram features to this combi-
nation had little effect for the MP3 Players. 
Given our list of features defined in Section 
3.2, helpfulness of reviews is best captured with 
a combination of the Length and Stars features. 
Training an RBF-kernel SVM regression model 
does not necessarily make clear the exact rela-
tionship between input and output variables. To 
investigate this relationship between length and 
helpfulness, we inspected their Pearson correla-
tion coefficient, which was 0.45. Users indeed 
tend to find short reviews less helpful than longer 
ones: out of the 5,247 reviews for MP3 Players 
that contained more than 1000 characters, the 
average gold standard helpfulness score was 
82%; the 204 reviews with fewer than 100 char-
acters had on average a score of 23%. The ex-
plicit product rating, such as Stars is also an 
                                                     
5 Recall that the gold standard is extracted directly from 
user helpfulness votes on Amazon.com (see Section 4). 
indicator of review helpfulness, with a Pearson 
correlation coefficient of 0.48. 
The low Pearson correlations of Table 3 com-
pared to the Spearman correlations suggest that 
we can learn the ranking without perfectly learn-
ing the function itself. To investigate this, we 
tested the ability of SVM regression to recover 
the target helpfulness score, given the score itself 
as the only feature. The Spearman correlation for 
this test was a perfect 1.0. Interestingly, the Pear-
son correlation was only 0.798, suggesting that 
the RBF kernel does learn the helpfulness rank-
ing without learning the function exactly. 
5.3 Results Analysis 
Table 3 shows only the feature combinations of 
our highest performing system. In Table 4, we 
report several other feature combinations to show 
why we selected certain features and what was 
the effect of our five feature classes presented in 
Section 3.2. 
In the first block of six feature combinations in 
Table 4, we show that the unigram features out-
perform the bigram features, which seem to be 
suffering from the data sparsity of the short re-
views. Also, unigram features seem to subsume 
the information carried in our semantic features 
Product-Feature (PRF) and General-Inquirer 
(GIW). Although both PRF and GIW perform 
well as standalone features, when combined with 
unigrams there is little performance difference 
(for MP3 Players we see a small but insignificant 
decrease in performance whereas for Digital 
Cameras we see a small but insignificant im-
provement). Recall that PRF and GIW are simply 
subsets of review words that are found to be 
product features or sentiment words. The learn-
ing algorithm seems to discover on its own which 
Table 3. Evaluation of the feature combinations that make up our best performing system 
(in bold), for ranking reviews of Amazon.com MP3 Players and Digital Cameras accord-
ing to helpfulness. 
MP3 PLAYERS DIGITAL CAMERAS 
FEATURE COMBINATIONS 
SPEARMAN? PEARSON? SPEARMAN? PEARSON? 
LEN 0.575 ? 0.037 0.391 ? 0.038 0.521 ? 0.029 0.357 ? 0.029 
UGR 0.593 ? 0.036 0.398 ? 0.038 0.499 ? 0.025 0.328 ? 0.029 
STR1 0.589 ? 0.034 0.326 ? 0.038 0.507 ? 0.029 0.266 ? 0.030 
UGR+STR1 0.644 ? 0.033 0.436 ? 0.038 0.490 ? 0.032 0.324 ? 0.032 
LEN+UGR 0.582 ? 0.036 0.401 ? 0.038 0.553 ? 0.028 0.394 ? 0.029 
LEN+STR1 0.652 ? 0.033 0.470 ? 0.038 0.577 ? 0.029 0.423 ? 0.031 
LEN+UGR+STR1 0.656 ? 0.033 0.476 ? 0.038 0.595 ? 0.028 0.442 ? 0.031 
LEN=Length; UGR=Unigram; STR=Stars 
?95% confidence bounds are calculated using 10-fold cross-validation. 
428
words are most important in a review and does 
not use additional knowledge about the meaning 
of the words (at least not the semantics contained 
in PRF and GIW). 
We tested two different versions of the Stars 
feature: i) the number of star ratings, STR1; and 
ii) the difference between the star rating and the 
average rating of the review, STR2. The second 
block of feature combinations in Table 4 shows 
that neither is significantly better than the other 
so we chose STR1 for our best performing sys-
tem. 
Our experiments also revealed that our struc-
tural features Sentential and HTML, as well as 
our syntactic features, Syntax, did not show any 
significant improvement in system performance. 
In the last block of feature combinations in Table 
4, we report the performance of our best per-
forming features (Length, Unigram, and Stars) 
along with these other features. Though none of 
the features cause a performance deterioration, 
neither of them significantly improves perform-
ance. 
5.4 Discussion 
In this section, we discuss the broader implica-
tions and potential impacts of our work, and pos-
sible connections with other research directions. 
The usefulness of the Stars feature for deter-
mining review helpfulness suggests the need for 
developing automatic methods for assessing pro-
duct ratings, e.g., (Pang and Lee 2005).  
Our findings focus on predictors of helpful-
ness of reviews of tangible consumer products 
(consumer electronics). Helpfulness is also solic-
ited and tracked for reviews of many other types 
of entities: restaurants (citysearch.com), films 
(imdb.com), reviews of open-source software 
modules (cpanratings.perl.org), and countless 
others. Our findings of the importance of Length, 
Unigrams, and Stars may provide the basis of 
comparison for assessing helpfulness of reviews 
of other entity types. 
Our work represents an initial step in assessing 
helpfulness. In the future, we plan to investigate 
other possible indicators of helpfulness such as a 
reviewer?s reputation, the use of comparatives 
(e.g., more and better than), and references to 
other products. 
Taken further, this work may have interesting 
connections to work on personalization, social 
networks, and recommender systems, for in-
stance by identifying the reviews that a particular 
user would find helpful.  
Our work on helpfulness of reviews also has 
potential applications to work on automatic gen-
Table 4. Performance evaluation of various feature combinations for ranking reviews of MP3 Players 
and Digital Cameras on Amazon.com according to helpfulness. The first six lines suggest that uni-
grams subsume the semantic features; the next two support the use of the raw counts of product ratings 
(stars) rather than the distance of this count from the average rating; the final six investigate the impor-
tance of auxiliary feature sets.  
MP3 PLAYERS DIGITAL CAMERAS 
FEATURE COMBINATIONS 
SPEARMAN? PEARSON? SPEARMAN? PEARSON? 
UGR 0.593 ? 0.036 0.398 ? 0.038 0.499 ? 0.025 0.328 ? 0.029 
BGR 0.499 ? 0.040 0.293 ? 0.038 0.434 ? 0.032 0.242 ? 0.029 
PRF 0.591? 0.037 0.400 ? 0.039 0.527 ? 0.030 0.316 ? 0.028 
GIW 0.571 ? 0.036 0.381 ? 0.038 0.524 ? 0.030 0.333 ? 0.028 
UGR+PRF 0.570 ? 0.037 0.375 ? 0.038 0.546 ? 0.029 0.348 ? 0.028 
UGR+GIW 0.554 ? 0.037 0.358 ? 0.038 0.568 ? 0.031 0.324 ? 0.029 
STR1 0.589 ? 0.034 0.326 ? 0.038 0.507 ? 0.029 0.266 ? 0.030 
STR2 0.556 ? 0.032 0.303 ? 0.038 0.504 ? 0.027 0.229 ? 0.027 
LEN+UGR+STR1 0.656 ? 0.033 0.476 ? 0.038 0.595 ? 0.028 0.442 ? 0.031 
LEN+UGR+STR1+SEN 0.653 ? 0.033 0.470 ? 0.038 0.599 ? 0.028 0.448 ? 0.030 
LEN+UGR+STR1+HTM 0.640 ? 0.035 0.459 ? 0.039 0.594 ? 0.028 0.442 ? 0.031 
LEN+UGR+STR1+SYN 0.645 ? 0.034 0.469 ? 0.039 0.595 ? 0.028 0.447 ? 0.030 
LEN+UGR+STR1+SEN+HTM+SYN 0.631 ? 0.035 0.453 ? 0.039 0.600 ? 0.028 0.452 ? 0.030 
LEN+UGR+STR1+SEN+HTM+SYN+PRF+GIW 0.601 ? 0.035 0.396 ? 0.038 0.604 ? 0.027 0.460 ? 0.030 
LEN=Length; SEN=Sentential; HTM=HTML; UGR=Unigram; BGR=Bigram; 
SYN=Syntax; PRF=Product-Feature; GIW=General-Inquirer; STR=Stars 
?95% confidence bounds are calculated using 10-fold cross-validation. 
429
eration of review information, by providing a 
way to assess helpfulness of automatically gener-
ated reviews. Work on generation of reviews in-
cludes review summarization and extraction of 
useful reviews from blogs and other mixed texts. 
6 Conclusions 
Ranking reviews according to user helpfulness is 
an important problem for many online sites such 
as Amazon.com and Ebay.com. To date, most 
websites measure helpfulness by having users 
manually assess how helpful each review is to 
them. In this paper, we proposed an algorithm for 
automatically assessing helpfulness and ranking 
reviews according to it. Exploiting the multitude 
of user-rated reviews on Amazon.com, we 
trained an SVM regression system to learn a 
helpfulness function and then applied it to rank 
unlabeled reviews. Our best system achieved 
Spearman correlation coefficient scores of 0.656 
and 0.604 against a gold standard for MP3 play-
ers and digital cameras. 
We also performed a detailed analysis of dif-
ferent features to study the importance of several 
feature classes in capturing helpfulness. We 
found that the most useful features were the 
length of the review, its unigrams, and its product 
rating. Semantic features like mentions of prod-
uct features and sentiment words seemed to be 
subsumed by the simple unigram features. Struc-
tural features (other than length) and syntactic 
features had no significant impact. 
It is our hope through this work to shed some 
light onto what people find helpful in user-
supplied reviews and, by automatically ranking 
them, to ultimately enhance user experience. 
References 
Attali, Y. and Burstein, J. 2006. Automated Essay 
Scoring With e-rater? V.2. Journal of Technology, 
Learning, and Assessment, 4(3). 
Burstein, J., Chodorow, M., and Leacock, C. 2004. 
Automated essay evaluation: the criterion online 
writing service. AI Magazine. 25(3), pp 27?36.  
Drucker,H., Wu,D. and Vapnik,V. 1999. Support vector 
machines for spam categorization. IEEE Trans. 
Neural Netw., 10, 1048?1054. 
Gabrilovich, E. and Markovitch, S. 2005. 
Feature Generation for Text Categorization Using 
World Knowledge. In Proceedings of IJCAI-2005. 
Hsu, C.-W.; Chang, C.-C.; and Lin, C.-J. 2003. A 
practical guide to SVM classification. Technical 
report, Department of Computer Science and 
Information Technology, National Taiwan University. 
Hu, M. and Liu, B. 2004. Mining and summarizing 
customer reviews. KDD?04. pp.168 ? 177 
Kim, S. and Hovy, E. 2004. Determining the Sentiment 
of Opinions. Proceedings of COLING-04. 
Joachims, T. 1999. Making Large-Scale SVM Learning 
Practical. In B. Sch?lkopf, C. Burges, and A. Smola 
(eds), Advances in Kernel Methods: Support Vector 
Learning. MIT Press. Cambridge, MA. 
Joachims, T. 2002. Optimizing Search Engines Using 
Clickthrough Data. In Proceedings of ACM KDD-02.  
Moschitti, A. and Basili R. 2004. Complex Linguistic 
Features for Text Classification: A Comprehensive 
Study. In Proceedings of ECIR 2004. Sunderland, 
U.K. 
Pang, B, L. Lee, and S. Vaithyanathan. 2001. Thumbs 
up? Sentiment Classification using Machine Learning 
Techniques.  Proceedings of EMNLP 2002. 
Pang, B. and Lee, L. 2005. Seeing stars: Exploiting class 
relationships for sentiment categorization with respect 
to rating scales. In Proceedings of the ACL, 2005. 
Riloff , E. and J. Wiebe. 2003. Learning Extraction 
Patterns for Subjective Expressions. In Proc. of 
EMNLP-03. 
Riloff, E., J. Wiebe, and T. Wilson. 2003. Learning 
Subjective Nouns Using Extraction Pattern 
Bootstrapping. Proceedings of CoNLL-03 
Rose, C., Roque, A., Bhembe, D., and Vanlehn, K. 2003. 
A Hybrid Text Classification Approach for Analysis 
of Student Essays. In Proc. of the HLT-NAACL, 2003. 
Salton, G. and McGill, M. J. 1983. Introduction to 
Modern Information Retrieval. McGraw Hill. 
Siegel, S. and Castellan, N.J. Jr. 1988. Nonparametric 
Statistics for the Behavioral Sciences. McGraw-Hill. 
Spearman C. 1904. The Proof and Measurement of 
Association Between Two Things. American Journal 
of Psychology, 15:72?101. 
Turney, P. 2002. Thumbs Up or Thumbs Down? 
Semantic Orientation Applied to Unsupervised 
Classification of Reviews. Proceedings of the 40th 
Annual Meeting of the ACL, Philadelphia, 417?424. 
Vapnik, V.N. 1995. The Nature of Statistical Learning 
Theory. Springer.  
Wiebe, J, R. Bruce, and T. O?Hara. 1999. Development 
and use of a gold standard data set for subjectivity 
classifications. Proc. of the 37th Annual Meeting of the 
Association for Computational Linguistics(ACL-99), 
246?253. 
Yu, H. and Hatzivassiloglou, V. 2003. Towards 
Answering Opinion Questions: Separating Facts from 
Opinions and Identifying the Polarity of Opinion 
Sentences. Proceedings of EMNLP 2003.
  
430
