Semantic Role Labeling with
Boosting, SVMs, Maximum Entropy, SNOW, and Decision Lists
Grace NGAI?1 , Dekai WU?2
Marine CARPUAT? , Chi-Shing WANG? , Chi-Yung WANG?
? Dept. of Computing
HK Polytechnic University
Hong Kong
? HKUST, Dept of Computer Science
Human Language Technology Center
Hong Kong
csgngai@polyu.edu.hk, dekai@cs.ust.hk
marine@cs.ust.hk, wcsshing@netvigator.com, cscywang@comp.polyu.edu.hk
Abstract
This paper describes the HKPolyU-HKUST sys-
tems which were entered into the Semantic Role La-
beling task in Senseval-3. Results show that these
systems, which are based upon common machine
learning algorithms, all manage to achieve good
performances on the non-restricted Semantic Role
Labeling task.
1 Introduction
This paper describes the HKPolyU-HKUST sys-
tems which participated in the Senseval-3 Semantic
Role Labeling task. The systems represent a diverse
array of machine learning algorithms, from decision
lists to SVMs to Winnow-type networks.
Semantic Role Labeling (SRL) is a task that
has recently received a lot of attention in the NLP
community. The SRL task in Senseval-3 used
the Framenet (Baker et al, 1998) corpus: given a
sentence instance from the corpus, a system?s job
would be to identify the phrase constituents and
their corresponding role.
The Senseval-3 task was divided into restricted
and non-restricted subtasks. In the non-restricted
subtask, any and all of the gold standard annotations
contained in the FrameNet corpus could be used.
Since this includes information on the boundaries
of the parse constituents which correspond to some
frame element, this effectively maps the SRL task
to that of a role-labeling classification task: given a
constituent parse, identify the frame element that it
belongs to.
Due to the lack of time and resources, we chose to
participate only in the non-restricted subtask. This
enabled our systems to take the classification ap-
proach mentioned in the previous paragraph.
1The author would like to thank the Hong Kong Polytechnic
University for supporting this research in part through research
grants A-PE37 and 4-Z03S.
2The author would like to thank the Hong Kong Research
Grants Council (RGC) for supporting this research in part
through research grants RGC6083/99E, RGC6256/00E, and
DAG03/04.EG09.
2 Experimental Features
This section describes the features that were used
for the SRL task. Since the non-restricted SRL task
is essentially a classification task, each parse con-
stituent that was known to correspond to a frame
element was considered to be a sample.
The features that we used for each sample have
been previously shown to be helpful for the SRL
task (Gildea and Jurafsky, 2002). Some of these
features can be obtained directly from the Framenet
annotations:
? The name of the frame.
? The lexical unit of the sentence ? i.e. the lex-
ical identity of the target word in the sentence.
? The general part-of-speech tag of the target
word.
? The ?phrase type? of the constituent ? i.e. the
syntactic category (e.g. NP, VP) that the con-
stituent falls into.
? The ?grammatical function? (e.g. subject, ob-
ject, modifier, etc) of the constituent, with re-
spect to the target word.
? The position (e.g. before, after) of the con-
stituent, with respect to the target word.
In addition to the above features, we also ex-
tracted a set of features which required the use of
some statistical NLP tools:
? Transitivity and voice of the target word ?
The sentence was first part-of-speech tagged
and chunked with the fnTBL transformation-
based learning tools (Ngai and Florian, 2001).
Simple heuristics were then used to deduce the
transitivity voice of the target word.
? Head word (and its part-of-speech tag) of the
constituent ? After POS tagging, a syntactic
parser (Collins, 1997) was then used to ob-
tain the parse tree for the sentence. The head
word (and the POS tag of the head word) of
                                             Association for Computational Linguistics
                        for the Semantic Analysis of Text, Barcelona, Spain, July 2004
                 SENSEVAL-3: Third International Workshop on the Evaluation of Systems
the syntactic parse constituent whose span cor-
responded most closely to the candidate con-
stituent was then assumed to be the head word
of the candidate constituent.
The resulting training data set consisted of 51,366
constituent samples with a total of 151 frame ele-
ment types. These ranged from ?Descriptor? (3520
constituents) to ?Baggage? and ?Carrier? (1 con-
stituent each). This training data was randomly par-
titioned into a 80/20 ?development training? and
?validation? set.
3 Methodology
The previous section described the features that
were extracted for each constituent. This section
will describe the experiment methodology as well
as the learning systems used to construct the mod-
els.
Our systems had originally been trained on the
entire development training (devtrain) set, gener-
ating one global model per system. However, on
closer examination of the task, it quickly became
evident that distinguishing between 151 possible
outcomes was a difficult task for any system. It
was also not clear that there was going to be a
lot of information that could be generalized across
frame types. We therefore partitioned the data by
frame, so that one model would be trained for each
frame. (This was also the approach taken by (Gildea
and Jurafsky, 2002).) Some of our individual sys-
tems tried both approaches; the results are com-
pared in the following subsections. For compar-
ison purposes, a baseline model was constructed
by simply classifying all constituents with the most
frequently-seen (in the training set) frame element
for the frame.
In total, five individual systems were trained for
the SRL task, and four ensemble models were gen-
erated by using various combinations of the indi-
vidual systems. With one exception, all of the indi-
vidual systems were constructed using off-the-shelf
machine learning software. The following subsec-
tions describe each system; however, it should be
noted that some of the individual systems were not
officially entered as competing systems; therefore,
their scores are not listed in the final rankings.
3.1 Boosting
The most successful of our individual systems is
based on boosting, a powerful machine learning
algorithm which has been shown to achieve good
results on NLP problems in the past. Our sys-
tem was constructed around the Boostexter soft-
ware (Schapire and Singer, 2000), which imple-
Model Prec. Recall Attempted
Single Model 0.891 0.795 89.2%
Frame Separated 0.894 0.798 89.2%
Baseline 0.444 0.396 89.2%
Table 1: Boosting Models: Validation Set Results
ments boosting on top of decision stumps (decision
trees of one level), and was originally designed for
text classification. The same system also partici-
pated in the Senseval-3 lexical sample tasks for Chi-
nese and English, as well as the Multilingual lexical
sample task (Carpuat et al, 2004).
Table 1 compares the results of training one sin-
gle overall boosting model (Single) versus training
separate models for each frame (Frame). It can be
seen that training frame-specific models produces
a small improvement over the single model. The
frame-specific model was used in all of the ensem-
ble systems, and was also entered into the competi-
tion as an individual system (hkpust-boost).
3.2 Support Vector Machines
The second of our individual systems was based
on support vector machines, and implemented using
the TinySVM software package (Boser et al, 1992).
Since SVMs are binary classifiers, we used a
one-against-all method to reduce the SRL task to
a binary classification problem. One model is con-
structed for each possible frame element and the
task of the model is to decide, for a given con-
stituent, whether it should be classified with that
frame element. Since it is possible for all the bi-
nary classifiers to decide on ?NOT-<element>?, the
model is effectively allowed to pass on samples that
it is not confident about. This results in a very pre-
cise model, but unfortunately at a significant hit to
recall.
A number of kernel parameter settings were in-
vestigated, and the best performance was achieved
with a polynomial kernel of degree 4. The rest of
the parameters were left at the default values. Table
2 shows the results of the best SVM model on the
validation set. This model participated in the all of
the ensemble systems, and was also entered into the
competition as an individual system.
System Prec. Recall Attempted
SVM 0.945 0.669 70.8%
Baseline 0.444 0.396 89.2%
Table 2: SVM Models: Validation Set Results
3.3 Maximum Entropy
The third of our individual systems was based on
the maximum entropy model, and implemented on
top of the YASMET package (Och, 2002). Like the
boosting model, the maximum entropy system also
participated in the Senseval-3 lexical sample tasks
for Chinese and English, as well as the Multilingual
lexical sample task (Carpuat et al, 2004).
Our maximum entropy models can be classi-
fied into two main approaches. Both approaches
used the frame-partitioned data. The more conven-
tional approach (?multi?) then trained one model
per frame; that model would be responsible for clas-
sifying a constituent belonging to that frame with
one of several possible frame elements. The second
approach (binary) used the same approach as the
SVM models, and trained one binary one-against-
all classifier for each frame type-frame element
combination. (Unlike the boosting models, a single
maximum entropy model could not be trained for all
possible frame types and elements, since YASMET
crashed on the sheer size of the feature space.)
System Prec. Recall Attempted
multi 0.856 0.764 89.2%
binary 0.956 0.539 56.4%
Baseline 0.444 0.396 89.2%
Table 3: Maximum Entropy Models: Validation Set
Results
Table 3 shows the results for the maximum en-
tropy models. As would have been expected, the
binary model achieves very high levels of precision,
but at considerable expense of recall. Both systems
were eventually used in the some of the ensemble
models but were not submitted as individual contes-
tants.
3.4 SNOW
The fourth of our individual systems is based on
SNOW ? Sparse Network Of Winnows (Mun?oz et
al., 1999).
The development approach for the SNOW mod-
els was similar to that of the boosting models. Two
main model types were generated: one which gener-
ated a single overall model for all the possible frame
elements, and one which generated one model per
frame type. Due to a bug in the coding which was
not discovered until the last minute, however, the
results for the frame-separated model were invali-
dated. The single model system was eventually used
in some of the ensemble systems, but not entered as
an official contestant. Table 4 shows the results.
System Prec. Recall Attempted
Single Model 0.764 0.682 89.2%
Baseline 0.444 0.396 89.2%
Table 4: SNOW Models: Validation Set Results
3.5 Decision Lists
The final individual system was a decision list im-
plementation contributed from the Swarthmore Col-
lege team (Wicentowski et al, 2004), which partic-
ipated in some of the lexical sample tasks.
The Swarthmore team followed the frame-
separated approach in building the decision list
models. Table 5 shows the result on the validation
set. This system participated in some of the final
ensemble systems as well as being an official par-
ticipant (hkpust-swat-dl).
System Prec. Recall Attempted
DL 0.837 0.747 89.2%
Baseline 0.444 0.396 89.2%
Table 5: Decision List Models: Validation Set Re-
sults
3.6 Ensemble Systems
Classifier combination, where the results of differ-
ent models are combined in some way to make a
new model, has been well studied in the literature.
A successful combined classifier can result in the
combined model outperforming the best base mod-
els, as the advantages of one model make up for the
shortcomings of another.
Classifier combination is most successful when
the base models are biased differently. That condi-
tion applies to our set of base models, and it was
reasonable to make an attempt at combining them.
Since the performances of our systems spanned
a large range, we did not want to use a simple ma-
jority vote in creating the combined system. Rather,
we used a set of heuristics which trusted the most
precise systems (the SVM and the binary maximum
entropy) when they made a prediction, or a combi-
nation of the others when they did not.
Table 6 shows the results of the top-scoring com-
bined systems which were entered as official con-
testants. As expected, the best of our combined sys-
tems outperformed the best base model.
4 Test Set Results
Table 7 shows the test set results for all systems
which participated in some way in the official com-
petition, either as part of a combined system or as
an individual contestant.
Model Prec. Recall Attempted
svm, boosting, maxent (binary) (hkpolyust-all(a)) 0.874 0.867 99.2%
boosting (hkpolyust-boost) 0.859 0.852 0.846%
svm, boosting, maxent (binary), DL (hkpolyust-swat(a)) 0.902 0.849 94.1%
svm, boosting, maxent (binary), DL, snow (hkpolyust-swat(b)) 0.908 0.846 93.2%
svm, boosting, maxent (multi), DL, snow (hkpolyust-all(b)) 0.905 0.846 93.5%
decision list (hkpolyust-swat-dl) 0.819 0.812 99.2%
maxent (multi) 0.827 0.735 88.8%
svm (hkpolyust-svm) 0.926 0.725 76.1%
snow 0.713 0.499 70.0%
maxent (binary) 0.935 0.454 48.6%
Baseline 0.438 0.388 88.6%
Table 7: Test set results for all our official systems, as well as the base models used in the ensemble system.
Base Models Prec. Recall Attempted
svm, boosting,
maxent (bin)
0.901 0.803 89.2%
svm, boosting,
maxent (bin), snow
0.938 0.8 85.2%
svm, boosting,
maxent (bin), DL
0.926 0.783 84.6%
svm, boosting,
maxent (multi),
DL, snow
0.935 0.797 85.2%
Baseline 0.444 0.396 89.2%
Table 6: Combined Models: Validation Set Results
The top-performing system is the combined sys-
tem that uses the SVM, boosting and the binary im-
plementation of maximum entropy. Of the individ-
ual systems, boosting performs the best, even out-
performing 3 of the combined systems. The SVM
suffers from its high-precision approach, as does the
binary implementation of maximum entropy. The
rest of the systems fall somewhere in between.
5 Conclusion
This paper presented the HKPolyU-HKUST sys-
tems for the non-restricted Semantic Role Labeling
task for Senseval-3. We mapped the task to that
of a simple classification task, and used features
and systems which were easily extracted and con-
structed. Our systems achieved good performance
on the SRL task, easily beating the baseline.
6 Acknowledgments
The ?hkpolyust-swat-*? systems are the result of
joint work between our team and Richard Wicen-
towski?s team at Swarthmore College. The authors
would like to express their immense gratitude to the
Swarthmore team for providing their decision list
system as one of our models.
References
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley FrameNet project. In Chris-
tian Boitet and Pete Whitelock, editors, Proceedings
of the Thirty-Sixth Annual Meeting of the Associa-
tion for Computational Linguistics and Seventeenth
International Conference on Computational Linguis-
tics, pages 86?90, San Francisco, California. Morgan
Kaufmann Publishers.
Bernhard E. Boser, Isabelle Guyon, and Vladimir Vap-
nik. 1992. A training algorithm for optimal margin
classifiers. In Computational Learing Theory, pages
144?152.
Marine Carpuat, Weifeng Su, and Dekai Wu. 2004.
Augmenting Ensemble Classification for Word Sense
Disambiguation with a Kernel PCA Model. In Pro-
ceedings of Senseval-3, Barcelona.
Michael Collins. 1997. Three generative, lexicalised
models for statistical parsing. In Proceedings of the
35th Annual Meeting of the ACL (jointly with the 8th
Conference of the EACL), Madrid.
Daniel Gildea and Dan Jurafsky. 2002. Automatic la-
beling of semantic roles. Computational Linguistics,
28(3):256?288.
Marcia Mun?oz, Vasin Punyakanok, Dan Roth, and Dav
Zimak. 1999. A learning approach to shallow pars-
ing. In Proceedings of EMNLP-WVLC?99, pages
168?178, College Park. Association for Computa-
tional Linguistics.
G. Ngai and R. Florian. 2001. Transformation-based
learning in the fast lane. In Proceedings of the 39th
Conference of the Association for Comp utational Lin-
guistics, Pittsburgh, PA.
Franz Josef Och. 2002. Yet Another Small Max-
ent Toolkit: Yasmet. http://www-i6.informatik.rwth-
aachen.de/Colleagues/och.
Robert E. Schapire and Yoram Singer. 2000. Boostex-
ter: A boosting-based system for text categorization.
Machine Learning, 39(2/3):135?168.
Richard Wicentowski, Emily Thomforde, and Adrian
Packel. 2004. The Swarthmore College Senseval-3
system. In Proceedings of Senseval-3, Barcelona.
Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 40?47,
Sydney, July 2006. c?2006 Association for Computational Linguistics
A Clustering Approach for  Unsupervised Chinese Coreference Resolution 
Chi-shing Wang Grace NGAI Department of Computing Hong Kong Polytechnic University Kowloon, HONG KONG {cscswang, csgngai}@comp.polyu.edu.hk   Abstract Coreference resolution is the process of identifying expressions that refer to the same entity. This paper presents a clustering algo-rithm for unsupervised Chinese coreference resolution. We investigate why Chinese coreference is hard and demonstrate that techniques used in coreference resolution for English can be extended to Chinese. The proposed system exploits clustering as it has advantages over traditional classification methods, such as the fact that no training data is required and it is easily extended to accommodate additional features. We con-duct a set of experiments to investigate how noun phrase identification and feature selec-tion can contribute to coreference resolution performance. Our system is evaluated on an annotated version of the TDT3 corpus using the MUC-7 scorer, and obtains comparable performance. We believe that this is the first attempt at an unsupervised approach to Chi-nese noun phrase coreference resolution. 1 INTRODUCTION  Noun phrase coreference resolution is the proc-ess of detecting noun phrases (NPs) in a docu-ment and determining whether the NPs refer to the same entity, where an entity is defined as ?a construct that represents an abstract identity?. The NPs that refer to the entity are known as mentions. Mentions can be antecedents or ana-phors. An anaphor is an expression that refers back to a previous expression in a discourse. In Figure 1, ????? (President Clinton) refers to ??? (Clinton) and is described as an ana-
phoric reference to??? (Clinton). ????? (President Clinton) is described as the antece-dent of ? (he). ??? (Clinton), ????? (President Clinton) and the second ? (he) are all mentions of the same entity that refers to former U.S. president Bill Clinton. 
NP coreference resolution is an important sub-task in natural language processing (NLP) appli-cations such as text summarization, information extraction, data mining and question answering. This task has attracted much attention in recent years (Cardie and Wagstaff, 1999; Harabagiu et al, 2001; Soon et al, 2001; Ng and Cardie, 2002; Yang et al, 2004; Florian et al, 2004; Zhou et al, 2005), and has been included as a subtask in the MUC (Message Understanding Conferences) and ACE (Automatic Content Extraction) competitions.  Coreference resolution is a difficult task for various reasons. Firstly, a list of features can play a role to support coreference resolution such as 
 
Figure 1: An excerpt from the text, with core-ferring noun phrases annotated. English trans-lation in italics. 
[???1]???????????[??2]??????[???3]?[???1]????????[?3]???[?????1]???????[?1]?????[??2]????????	 [Clinton1] said that Washington would progres-sively follow through on economic aid to [Ko-rea2]. [Kim Dae-Jung3] applauded [Clinton1]?s speech. [He1] said, ?[President Clinton1] reiter-ated in the talks that [he1] would provide solid support for [Korea2] to shake off the economic crisis. 
40
gender agreement, number agreement, head noun matches, semantic class, positional information, contextual information, appositive, abbreviation etc. Ng and Cardie (2002) found 53 features which are useful for this problem. However, no single feature is completely reliable since there are always exceptions: e.g. the number agree-ment test returns false when ????  (this army, singular) is matched against ??? (army members, plural), despite the two phrases being coreferential. Secondly, identifying features automatically and accurately is hard. Features such as semantic class come from named entity recognition (NER) systems and ontologies and gazetteers, but they are not always accurate, es-pecially where new terms are concerned. Thirdly, coreference resolution subsumes the pronoun resolution problem, which is already difficult since pronouns carry limited lexical and semantic information. In addition to the aforementioned, Chinese coreference resolution is also made more diffi-cult due to the lack of morphological and ortho-graphic clues. Chinese words contain less exte-rior information than words in many Indoeuro-pean languages. For example, in English, number agreement can be detected through word inflec-tions and part-of-speech (POS) tags, but there are no simple rules in Chinese to distinguish whether a word is singular or plural. Proper name and abbreviations are identified by capitalization in English, but Chinese does not use capitalization. Moreover, written Chinese does not have word boundaries, so word segmentation is a crucial problem, as we cannot get the true meaning of the sentence based on characters alone. A simple sentence can be segmented in several different ways to get different meanings.  This characteris-tic affects the performance of all parts and leads to irrecoverable errors. In addition, there are very few Chinese coreference data sets available for research purposes (none of them freely available) and as a result, no easily obtainable benchmark-ing dataset for training and measuring perform-ance. Building a reasonably large coreference corpus is a labor-consuming task. To our knowledge, there have only been two Chinese coreference systems in previously pub-lished work: Florian et al (2004), which presents a statistical framework and reports experiment results on Chinese texts; and Zhou et al (2005), which proposed a unified transformation based learning framework for Chinese entity detection and tracking. It consists of two models: the de-tection model locates possibly coreferring NPs 
and the tracking model links the coreference re-lations. This paper presents research performed on Chinese noun phrase coreference resolution.  Since there are no freely available Chinese coreference resources, we used an unsupervised method that partially borrows from Cardie and Wagstaff?s (1999) clustering-based technique, with features that are specially designed for Chi-nese. In addition, we perform and present the results of experiments designed to investigate the contribution of each feature. 2 Experiment Setup Identifying coreferent NPs in an unannotated document actually involves two tasks: mention detection, which identifies the anaphors and an-tecedents in a document, followed by noun phrase coreference resolution. In order to reduce the complexity of the final system, we follow the usual approach in handling these two phases separately.  2.1 Corpus Even though we are using an unsupervised ap-proach, a gold standard corpus is still needed for experiment evaluation. Since we did not have access to the ACE multilingual entity tracking corpus, we created our own corpus by selecting 30 documents from the TDT3 Chinese corpus. This resulted in a corpus of approximately 36K Chinese characters, about the same size as the MUC dryrun test sets. We then had our corpus annotated by a native Chinese speaker following the MUC-7 (Hirschman and Chinchor, 1997) and ACE Chinese entity guidelines (LDC, 2004) by picking out noun phrase mentions corresponding to one of the following nine types of entities: Person, Organization, Location, Geo-Political Entity (GPE), Facility, Vehicle, Weapon, Date and Money, and for each pair of mentions, decid-ing whether they refer to the same entity follow-ing MUC-7 definitions.  According to the guide-lines, each mention participates in exactly one entity, and all mentions in the same entity are coreferent. The NPs that are marked include proper nouns, nominal nouns and pronouns and the entity types are a superset of those used in the MUC and ACE competitions. The resulting cor-pus includes 1640 mentions, referring to 410 en-tities.  Once our corpus had been determined, the first step was to determine the possible mentions in a plain text. We first used a dictionary-based word 
41
segmentation system (Lancashire, 2005) to seg-ment the Chinese characters into words. The segmented words are then labeled with POS tags by a statistical POS tagging system (Fung et al, 2004).   3 Mention Detection After the corpus has been preprocessed, mention detection involves the identification of NPs in the corpus that refer to some entity. Most of these NPs correspond to non-recursive NPs, which makes this task simpler as most syntactic parsers identify NPs as part of the parsing proc-ess. This approach, however, suffers from two problems: firstly, the parser itself is unlikely to be 100% accurate; and secondly, the boundaries of the NPs identified by the parser may not cor-respond exactly with those of the entities identi-fied by the human annotator. Another approach is simply to use heuristics based on the POS tag sequence to identify poten-tial NPs of interest. The advantage of this method is that the NPs thus extracted should be closer to the human-annotated entities since the heuristics will be constructed specifically for this task. To investigate the effect of different ap-proaches on the result of the coreference resolu-tion, we applied both methods separately to our corpus. The corpus was parsed with a state-of-the-art multilingual statistical parser (Bikel 2004), which is trained on the Chinese Penn Treebank. After parsing, we extracted all non-recursive NP chunks tagged by the parser as pos-sible mentions.  For the heuristic-based approach, we applied a few simple heuristics, which had been previ-ously developed during unrelated work for Eng-lish named-entity resolution (i.e. they were not written with foreknowledge of the gold standard entities) and which are based on the part-of-speech tags of the words.  Some examples of our heuristics were to look for pronouns, or to extract all noun sequences, or sequences of determiners followed by adjectives and nouns.  Table 1 shows the performance of the pars-ing-based approach versus the heuristic-based approach. The parser-based approach suffers 
mainly because the NPs that it extracts tend to be on the long side, resulting in recall errors when the boundaries of the parser-identified NPs mis-match with the human-annotated entities. In ad-dition, the parser also tends to extract more NPs than needed, which results in a hit to precision. 4 Coreference Resolution The final step after the mention detection phase is to determine which of the extracted phrases refer to the same entity, or are coreferent.  The small size of our corpus made it quite ob-vious that we would not be able to perform su-pervised learning, as there would not be enough data for generalization purposes.  Therefore we chose to use an unsupervised clustering approach for this step.  Clustering is a natural choice as it partitions the data into groups; used on corefer-ence resolution, we expect to gather coreferrent NPs into the same cluster.  Furthermore, most clustering methods can easily incorporate both context-dependent and independent constraints into their features. 4.1 Features Our features use both lexical and syntactic in-formation designed to capture both the content of the phrase and its role within the sentence. With the exception of the last three features, which are defined with respect to a noun phrase pair, all our features describe various aspects of a single noun phrase:  Lexical String ? This is just simply the string of words in the phrase. Head Noun ? The head noun in a phrase is the noun that is not a modifier for another noun. Sentence Position ? This measures the position of the phrase within the document. Gender ? For each phrase, we use a gazetteer to assign it a gender. The possible values are male (e.g. ??, mister), female (e.g. ??, miss), ei-ther (e.g. ??, leader) and neither (e.g. ??, factory).  Number ? A phrase can be either singular (e.g. ???, one cat), plural (e.g. ???, two dogs), either (e.g. ??, product) or neither (e.g. ??, safety). 
 Recall Precision F-Measure Heuristics 83 59.3 69.2 Parser-Based 62.7 28.7 39.4 Table 1: Mention Detection Results  
42
Semantic Class ? To give the system more in-formation on each phrase, we generated our own gazetteer from a combination of gazetteers com-piled from web sources and heuristics.  Our gaz-etteer consists of 4700 entries, each of which is labeled with one of the following semantic classes: person, organization, location, facility, GPE, date, money, vehicle and weapon. Phrases in the corpus that are found in the gazetteer are given the same semantic class label; phrases not in the gazetteer are marked as UNKNOWN.  Proper Name ? The part-of-speech tag ?NR? and a list of common proper names were used to label each noun phrase as to whether it is a proper name (values: true/false). Pronoun ? Determined by the part-of-speech ?PN?. Values: true/false.  Demonstrative Noun Phrase ? A demonstrative noun phrase is a phrase that consists of a noun phrases preceded by one of the characters [???] (this/that/some).  Appositive ? Two noun phrases are in apposition when the first phrase is headed by a common noun while the second one is a proper name with no space or punctuation between them. e.g. [?
??? ][??? ]????????? ([US president] [Clinton] visited Pyongyang last week.) This differs from English where two nouns are considered to be in apposition when one of them is an anaphor and separated by a comma from the other phrase, which is the most immediate proper name. (e.g. ?Bill Gates, the chairman of Microsoft Corp?) Abbreviation ? A noun phrase is an abbrevia-tion when it is formed by using part of another noun phrase, e.g. ??????? (Pyongyang Central Communications Office) is commonly abbreviated as ???. Since name abbreviations in Chinese are often given in an ad-hoc manner, it would be infeasible to generate a list of nam and abbreviations in advance. We therefore use the following heuristic: given two phrases, we test if one is an abbreviation of another by ex-tracting each successive character from the shorter phrase and testing to see if it is included in the corresponding word from the longer phrase. Intuitively, we know that this is a com-mon way of abbreviating terms; empirically, it usually gives us a correct result. Edit Distance ? Abbreviations and nicknames Feature f Function Noun Phrase Match -1 if the string of NPi matches the string of NPj; else 0 Head Noun Match -1 if head noun of NPi matches the head noun of NPj; else 0 
Sentence Distance 0 if NPi and NPj are in the same sentence;  For non-pronouns: 1/10 if they are one sentence apart; and so on with maximum value 1;  For pronouns: if more than two sentences apart, then 1 Gender Agreement 1 if they do not match in gender; else 0 Number Agreement 1 if they do not match in number; else 0 Semantic Agreement 1 if they do not match in semantic class or unknown; else 0 
Proper Name Agreement 1 if both are proper names, but mismatch on every word; else 0 
Pronoun Agreement 1 if either NPi or NPj is pronoun and mismatch in gender or number; else 0 Demonstrative Noun Phrase -1 if NPi is demonstrative and NPi contains NPj; else 0 Appositive -1 if NPi and NPj are in an appositive relationship; else 0 Abbreviation -1 if NPi and NPj are in an abbreviative relationship; else 0 Edit Distance 0 if NPi and NPj are the same, 1/(length of longer string) if one edit is needed to transform one to another, and so on. Table 2: Features and functions used in clustering algorithm  
43
are very commonly used in Chinese and even though the previous feature will work on most of them, there are some common exceptions. To make sure that we catch those as well, we intro-duced a Chinese-specific feature as a further test. Since abbreviations and nicknames are not usu-ally substrings of the original strings, but will still share some common characters, we measure the Levenshtein distance, defined as the number of character insertions, deletions and substitu-tions, between every potential antecedent-anaphor pair. 4.2 Distance Metric In order for the clustering algorithm to be able to group instances together by similarity, we need to determine a distance metric between two in-stances ? in our case, two noun phrases. For our system, we borrowed a simple distance metric from Cardie and Wagstaff (1999) that sums up the results of a series of functions over the two phrases: 
( , ) ( , )
i j f i j
f F
dist NP NP function NP NP
?
=
?
 Table 2 presents the features and the correspond-ing functions that were used in our system. Each function calculates a distance between the two phrases that is an indicator of the degree of in-compatibility between the two phrases with re-spect to a particular feature. The NOUN PHRASE, HEAD NOUN, DEMONSTRATIVE, APPOSITIVE and ABBREVIATIVE functions test for compatibility and return a negative value when the two phrases are compatible for that term?s feature. The reason for the negative value returned is that if the two phrases match on this particular feature, then it is a strong indicator of coreference. Therefore, we reduce the distance between two phrases, making it more likely that they will be clustered together into the same en-tity.  When there is a mismatch, however, it does not necessarily indicate that the two NPs are non-coreferential, so we leave the distance between the NPs unchanged. Conversely, there are some features where a mismatch would indicate that the two NPs are absolutely non-compatible and will definitely not refer to the same entity.  The DISTANCE, GENDER, NUMBER, SEMANTIC, PROPER NAME, PRONOUN and EDIT_DISTANCE functions return a positive value when the two phrases mismatch on that particular feature. A positive value results in a greater distance be-tween two phrases, which makes it less likely for them to be clustered together.  
4.3 Clustering Algorithm Most of the previous work in clustering-based noun phrase coreference resolution has centered around the use of bottom-up clustering methods, where each noun phrase is initially assigned to a singleton cluster by itself, and clusters which are ?close enough? to each other are merged (Cardie & Wagstaff, 1999; Angheluta et al, 2004). In our system, we use a method called modi-fied k-means clustering (Wilpon & Rabiner 1985), which takes the opposite approach and uses a top-down approach to split clusters, inter-leaved with a k-means iterative phase.  Modified k-means clustering has been successfully applied to speech recognition and it has the advantage of always being able to come to the optimal cluster-ing (i.e. it is not dependent upon the starting state or merging order).   Modified k-means starts off with all the in-stances in one big cluster.  The system then itera-tively performs the following steps: 1. For each cluster, find its centroid, de-fined as the instance which is the closest to all other instances in the same cluster. 2. For each instance: a. Calculate its distance to all the centroids.   b. Find the centroid with the mini-mum distance, and join its clus-ter. 3. Iterate 1-2 until instances stop moving between clusters. 4. Find the cluster with the largest intra-cluster distance.  (Call this Clustermax and its centroid, Centroidmax.) If this distance is smaller than some threshold r, stop. 5. From the instances inside Clustermax, find the pair that are the furthest apart from each other. a. Add the pair of instances to the list of centroids and remove Centroidmax from the list.  b. Repeat from Step 2. The algorithm thus alternates traditional k-means clustering with a step that adds new clusters to the pool of existing ones.  Used for coreference resolution, it splits up the instances into clusters in which the instances are more similar to each other than to instances in other clusters.  The only thing left to do is to determine a suit-able threshold. As functions that check for com-patibility return negative values while positive distances indicate incompatibility, a threshold of 0 would separate compatible and incompatible 
44
elements.  However, since the feature extraction will not be totally accurate, (especially for the GENDER and NUMBER features which test for incompatibility) we chose to be more lenient with deciding whether two phrases should be clustered together, and used a threshold of r = 1 to allow for possible errors. 5 Evaluation Evaluation of coreference resolution systems has traditionally been performed with precision and recall.  The MUC competition defines recall as follows (Vilain et al, 1995): 
(| | | ( ) |)
(| | 1)
i i
i
C p C
R
C
?
=
?
?
?
 
Each Ci is a gold standard cluster (i.e. a set of phrases which we know refer to the same entity), and p(Ci) is the partitioning of Ci by the auto-matically-generated clusters. For precision, the role of the automatic and gold standard clusters are reversed. Our results were evaluated using the MUC scoring program which reports recall, precision and F-measure, where the F-measure is defined as the harmonic mean of precision and recall: 
? 
F =
2PR
P + R
 Table 3 presents the results of our coreference resolution system on the outputs of both the pars-ing-based and heuristic-based entity detection systems, as measured by the MUC-7 scoring program. For the purposes of comparison, we also present results of our clustering algorithm on the gold standard entities.  This gives us a sense of the upper bound that we could poten-tially achieve if we got 100% accuracy on our mention detection phase.  An additional baseline is generated by implementing a system that as-sumes that all phrases refer to the same entity ? i.e. it takes all the heuristically-generated phrases and puts them into one big cluster. This gives us an upper bound on the recall of the system. Yet another baseline, to see how easy the task is, is to merge mentions together if the ?Noun Phrase Match? function tests true.   
From the results, it can be seen that our system achieves a performance gain of over 10 F-Measure points over the simplest baseline, and over 8 F-Measure points over the more sophisti-cated baseline. Unfortunately, due to corpus dif-ferences, we cannot conduct a comparison with results found in previous work. An interesting observation is the fact that the heuristic-based entity recognizer achieves better performance than the one based on statistical parsing.  The parser is trained on the Chinese Penn Treebank, which tends to have relatively longer noun phrases, and as result, the phrases generated by the parser also tend to be on the long side. This causes errors at the entity recog-nition phase, which results in a performance hit for the overall system.  6 Analysis One interesting question to ask about the results is the contribution of any given individual fea-ture to the result of the overall system. We have already investigated the effect of entity recogni-tion, and in this section, we take a look at the features for the clustering algorithm. Error! Reference source not found. presents the results of a series of experiments in which one feature at a time was removed from the clustering algo-rithm. The last entry in the table shows the re-sults of the full system; the drop in performance when a feature is removed is indicative of its contribution. Judging from the results, the 3 fea-tures that contribute the most to performance are the NOUN PHRASE MATCH, SEMANTIC AGREEMENT and EDIT DISTANCE features. Two out of the three, NOUN PHRASE and EDIT DISTANCE, operate on lexical informa-tion.  The importance of string matching to coreference resolution is consistent with findings in previous work (Yang et al 2004), which ar-rived at the same conclusion for English.  In addition, we note that the two Chinese-specific features that were introduced, ABBRE-VIATION and EDIT DISTANCE, both contrib-ute significantly (as measured by a student?s t-test) to the performance of the final system. 
 Recall Precision F-Measure Gold Standard Entities 78 88.5 82.9 Baseline (Heuristic-based Entities) 80.9 44.1 57.1 Baseline (Noun Phrase Match Only) 50.9 77.2 61.3 Heuristic-Based Entity Recognition 62.9 77.1 69.3 Parsing-Based Entity Recognition 42.5 62.9 50.7 Table 3: Coreference Resolution Performance  
45
Of our features, those that contribute the least to the overall performance are the GENDER, NUMBER and DEMONSTRATIVE NOUN PHRASE features. For DEMONSTRATIVE NOUN PHRASE, the reason is because of data sparsity ? there are just simply not enough ex-amples that it would make any significant im-pact.  For the GENDER and NUMBER features, we find that the problem is mostly with errors in feature generation. To our knowledge, this is the first published result on unsupervised Chinese coreference reso-lution. Due to differences in data, it is not possi-ble to conduct a comparison of our work with previous results.  7 Related Work Coreference resolution has attracted much atten-tion in recent years, especially as a result of the MUC and ACE competitions. The approaches taken have exhibited a shift from knowledge-based approaches to learning-based approaches. Many of the learning-based approaches recast coreference resolution as a binary classification task, which, given a pair of NPs, uses a trained classifier to determine whether they are corefer-ent. Soon et al (2001) used this approach with a 12-feature decision tree-based classifier and Ng and Cardie (2002) extended this approach with extra machine learning frameworks and a larger set of features. Yang et al (2004) extended this approach into an NP-cluster based approach, which considers the relationships between phrases and coreferential clusters.  In addition, several unsupervised approaches have been proposed. Cardie and Wagstaff (1999) re-cast the problem as a clustering task which applied a set of incompatibility functions and 
weights in the distance metric. Bean and Riloff (2004) used information extraction patterns to identify contextual clues that would determine the compatibility between NPs.  All of the previously mentioned work has been for English. There has been relatively little work in Chinese: Florian et al (2004) provides results using a language-independent framework on the Entity Detection and Tracking task (EDT). They formulate the detection subtask as a classification problem using a Robust Risk Minimization clas-sifier combined with a Maximum Entropy classi-fier. Their system performs significantly well on English, Chinese and Arabic, however, the sys-tem suffers from small amount of training data (90K characters for Chinese, in contrast with 340K words for English). Their system obtained an ACE value of 58.8 on the ACE evaluation data on Chinese.  Finally, Zhou et al (2005) pro-posed a unified Transformation-Based Learning framework on Chinese EDT. The TBL tracking model looks at pairs of NPs at a time and classi-fies them as being coreferent or not based on the values of six features. They report an ACE score of 63.3 on their dataset. 8 Conclusions and Future Work In this paper, we have presented an unsupervised approach to Chinese coreference resolution. Our approach performs resolution by clustering, with the advantage that no annotated training data is needed. We evaluated our approach using a cor-pus which we developed using standard annota-tion schemes, and find that our system achieves an error reduction rate of almost 30% over the baseline. We also analyze the performance of our system by investigating the contribution of indi-vidual features to our system. The analysis illus-
Removed feature Recall Precision F-measure Noun Phrase Match 59.8 75.9 66.9 Head Noun Match 60.4 76.2 67.4 Sentence Distance 63.2 73.3 67.8 Gender Agreement 62.9 76.3 68.9 Number Agreement 63.2 75.9 69 Semantic Agreement 60.5 73 66.2 Proper Name Agreement 63 76.2 69 Pronoun Agreement 61.3 76.9 68.2 Demonstrative Noun Phrase 62.2 77.9 69.2 Appositive 60.1 76.9 67.5 Abbreviation 61.6 77 68.4 Edit Distance 62.4 72.8 67.2 None (All Features) 62.9 77.1 69.3 Table 4: Contribution of individual features to overall performance.  
46
trates the contribution of the new language-specific features. While the results produced by our system are impressive, it should be noted that all our fea-tures consider only the mention phrase itself. We consider this to be a rather simplistic and incom-plete. In future work, we plan to investigate the use of more sophisticated features, including contextual features, to improve the performance of our system. References ANGHELUTA R., JEUNIAUX P., RUDRADEB M., MOENS M.F. 2004. Clustering Algorithms for Noun Phrase Coreference Resolution. Proceedings of the 7th International Conference on the Statisti-cal Analysis of Textual Data. BEAN, D. and RILOFF, E. 2004. Unsupervised learn-ing of contextual role knowledge for coreference resolution. In Proc. of HLT/NAACL, pages 297?304. BIKEL, D. M. 2004. A Distributional Analysis of a Lexicalized Statistical Parsing Model. In Proceed-ings of EMNLP, Barcelona CARDIE, C. and WAGSTAFF, K. 1999. Noun phrase coreference as clustering. In Proceedings of the 1999 Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora, pages 82-89. FLORIAN, R., HASSAN, H., ITTYCHERIAH, A., JING, H., KAMBHATLA, N., LUO, X., NICOLOV, N., and ROUKOS, S. 2004. Statistical Model for Multilingual Entity Detection and Tracking. In Proceedings of 2004 annual meeting of the North American Chapter of the Association for Computational Linguistics (HLT-NAACL 2004). FUNG, P., NGAI, G., YANG, Y. S., and CHEN, B.F. 2004. A maximum-entropy Chinese parser aug-mented by Transformation-Based Learning. ACM Transactions on Asian Language Information Processing (TALIP), 3(2), pp 159-168. GAO J.F., LI M. and HUANG C.N. 2003. Improved souce-channel model for Chinese wordsegmenta-tion. In Proc. of ACL2003. HARABAGIU, S., BUNESCU, R.,and MAIORANO, S. 2001. Text and Knowledge Mining for Corefer-ence Resolution, in Proceedings of the 2nd Meet-ing of the North American Chapter of the Associa-tion of Computational Linguistics (NAACL-2001). HIRSCHMAN, L. and CHINCHOR, N. 1997. MUC7 Coreference Task Definition, http://www.itl.nist.gov/iaui/894.02/related_projects/muc/proceedings/co_task.html. 
LANCASHIRE, D. 2005. Adsotrans Chinese-English annotation. http://www.adsotrans.com/. LDC. 2004. Chinese Annotation Guidelines for Entity Detection and Tracking. http://www.ldc.upenn.edu/Projects/ACE/Data. MUC-7. 1998. Proceedings of the Seventh Message Understanding Conference (MUC-7). Morgan Kaufmann, San Francisco, CA. NG V. 2005. Machine learning for coreference resolution: From local classification to global ranking. In Proceedings of the 43rd Annual Meet-ing of the Association for Computational Linguis-tics (ACL), 2005. NG, V. and CARDIE, C. 2002. Improving machine learning approaches to coreference resolution. In Proceedings of the 40rd Annual Meeting of the As-sociation for Computational Linguistics, Pages 104-111. NG V. and CARDIE C. 2003. Bootstrapping Corefer-ence Classifiers with Multiple Machine Learning Algorithms. Proceedings of the 2003 Conference on Empirical Methods in Natural Language Proc-essing (EMNLP-2003), Association for Computa-tional Linguistics, 2003. SOON, W., NG, H., and LIM, D. 2001. A machine learning approach to coreference resolution of noun phrases. Computational Linguistics, 27(4):521-544. VILAIN, M., BURGER, J., ABERDEEN, J., CON-NOLLY, D., and HIRSCHMAN, L. 1995. A model-theoretic coreference scoring scheme. In Proceedings of the Sixth Message Understanding Conference (MUC-6), pages 45?52, San Francisco, CA. Morgan Kaufmann. WILPON, J., AND RABINER, L. 1985. A modified K-means clustering algorithm for use in isolated word recognition. In IEEE Transactions on Acous-tics, Speech, Signal Processing. ASSP-33(3), 587-594. YANG, X., ZHOU, G., SU, J., and TAN, C. L. 2004. An NP-Cluster Based Approach to Coreference Resolution. Proceedings of the 20th International Conference on Computational Linguistics (COL-ING2004). ZHOU Y., HUANG C., GAO J., WU L. 2005. Trans-formation Based Chinese Entity Detection and Tracking. Proceedings of the Second International Joint Conference on Natural Language Processing. 
47
