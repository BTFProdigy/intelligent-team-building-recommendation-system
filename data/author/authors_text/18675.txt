Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 31?36,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
DISSECT - DIStributional SEmantics Composition Toolkit
Georgiana Dinu and Nghia The Pham and Marco Baroni
Center for Mind/Brain Sciences (University of Trento, Italy)
(georgiana.dinu|thenghia.pham|marco.baroni)@unitn.it
Abstract
We introduce DISSECT, a toolkit to
build and explore computational models
of word, phrase and sentence meaning
based on the principles of distributional
semantics. The toolkit focuses in partic-
ular on compositional meaning, and im-
plements a number of composition meth-
ods that have been proposed in the litera-
ture. Furthermore, DISSECT can be use-
ful to researchers and practitioners who
need models of word meaning (without
composition) as well, as it supports var-
ious methods to construct distributional
semantic spaces, assessing similarity and
even evaluating against benchmarks, that
are independent of the composition infras-
tructure.
1 Introduction
Distributional methods for meaning similarity are
based on the observation that similar words oc-
cur in similar contexts and measure similarity
based on patterns of word occurrence in large cor-
pora (Clark, 2012; Erk, 2012; Turney and Pan-
tel, 2010). More precisely, they represent words,
or any other target linguistic elements, as high-
dimensional vectors, where the dimensions repre-
sent context features. Semantic relatedness is as-
sessed by comparing vectors, leading, for exam-
ple, to determine that car and vehicle are very sim-
ilar in meaning, since they have similar contextual
distributions. Despite the appeal of these meth-
ods, modeling words in isolation has limited ap-
plications and ideally we want to model semantics
beyond word level by representing the meaning of
phrases or sentences. These combinations are in-
finite and compositional methods are called for to
derive the meaning of a larger construction from
the meaning of its parts. For this reason, the ques-
tion of compositionality within the distributional
paradigm has received a lot of attention in recent
years and a number of compositional frameworks
have been proposed in the distributional seman-
tic literature, see, e.g., Coecke et al (2010) and
Mitchell and Lapata (2010). For example, in such
frameworks, the distributional representations of
red and car may be combined, through various op-
erations, in order to obtain a vector for red car.
The DISSECT toolkit (http://clic.
cimec.unitn.it/composes/toolkit)
is, to the best of our knowledge, the first to
provide an easy-to-use implementation of many
compositional methods proposed in the literature.
As such, we hope that it will foster further work
on compositional distributional semantics, as well
as making the relevant techniques easily available
to those interested in their many potential applica-
tions, e.g., to context-based polysemy resolution,
recognizing textual entailment or paraphrase
detection. Moreover, the DISSECT tools to
construct distributional semantic spaces from
raw co-occurrence counts, to measure similarity
and to evaluate these spaces might also be of
use to researchers who are not interested in the
compositional framework. DISSECT is freely
available under the GNU General Public License.
2 Building and composing distributional
semantic representations
The pipeline from corpora to compositional mod-
els of meaning can be roughly summarized as con-
sisting of three stages:1
1. Extraction of co-occurrence counts from cor-
pora In this stage, an input corpus is used to ex-
tract counts of target elements co-occurring with
some contextual features. The target elements
can vary from words (for lexical similarity), to
pairs of words (e.g., for relation categorization),
1See Turney and Pantel (2010) for a technical overview of
distributional methods for semantics.
31
to paths in syntactic trees (for unsupervised para-
phrasing). Context features can also vary from
shallow window-based collocates to syntactic de-
pendencies.
2. Transformation of the raw counts This
stage may involve the application of weighting
schemes such as Pointwise Mutual Information,
feature selection, dimensionality reduction meth-
ods such as Singular Value Decomposition, etc.
The goal is to eliminate the biases that typically
affect raw counts and to produce vectors which
better approximate similarity in meaning.
3. Application of composition functions
Once meaningful representations have been
constructed for the atomic target elements of
interest (typically, words), various methods, such
as vector addition or multiplication, can be used
for combining them to derive context-sensitive
representations or for constructing representations
for larger phrases or even entire sentences.
DISSECT can be used for the second and
third stages of this pipeline, as well as to measure
similarity among the resulting word or phrase vec-
tors. The first step is highly language-, task- and
corpus-annotation-dependent. We do not attempt
to implement all the corpus pre-processing and
co-occurrence extraction routines that it would
require to be of general use, and expect instead as
input a matrix of raw target-context co-occurrence
counts.2 DISSECT provides various methods to
re-weight the counts with association measures,
dimensionality reduction methods as well as the
composition functions proposed by Mitchell and
Lapata (2010) (Additive, Multiplicative and Dila-
tion), Baroni and Zamparelli (2010)/Coecke et al
(2010) (Lexfunc) and Guevara (2010)/Zanzotto et
al. (2010) (Fulladd). In DISSECT we define and
implement these in a unified framework and in a
computationally efficient manner. The focus of
DISSECT is to provide an intuitive interface for
researchers and to allow easy extension by adding
other composition methods.
3 DISSECT overview
DISSECT is written in Python. We provide many
standard functionalities through a set of power-
2These counts can be read from a text file containing two
strings (the target and context items) and a number (the corre-
sponding count) on each line (e.g., maggot food 15) or
from a matrix in format word freq1 freq2 ...
#create a semantic space from counts in
#dense format("dm"): word freq1 freq2 ..
ss = Space.build(data="counts.txt",
format="dm")
#apply transformations
ss = ss.apply(PpmiWeighting())
ss = ss.apply(Svd(300))
#retrieve the vector of a target element
print ss.get_row("car")
Figure 1: Creating a semantic space.
ful command-line tools, however users with ba-
sic Python familiarity are encouraged to use the
Python interface that DISSECT provides. This
section focuses on this interface (see the online
documentation on how to perform the same oper-
ations with the command-line tools), that consists
of the following top-level packages:
#DISSECT packages
composes.matrix
composes.semantic_space
composes.transformation
composes.similarity
composes.composition
composes.utils
Semantic spaces and transforma-
tions The concept of a semantic space
(composes.semantic space) is at the
core of the DISSECT toolkit. A semantic
space consists of co-occurrence values, stored
as a matrix, together with strings associated to
the rows of this matrix (by design, the target
linguistic elements) and a (potentially empty)
list of strings associated to the columns (the
context features). A number of transforma-
tions (composes.transformation) can
be applied to semantic spaces. We implement
weighting schemes such as positive Pointwise
Mutual Information (ppmi) and Local Mu-
tual Information, feature selection methods,
dimensionality reduction (Singular Value De-
composition (SVD) and Nonnegative Matrix
Factorization (NMF)), and new methods can
be easily added.3 Going from raw counts to a
transformed space is accomplished in just a few
lines of code (Figure 1).
3The complete list of transformations currently sup-
ported can be found at http://clic.cimec.unitn.
it/composes/toolkit/spacetrans.html#
spacetrans.
32
#load a previously saved space
ss = io_utils.load("ss.pkl")
#compute cosine similarity
print ss.get_sim("car", "book",
CosSimilarity())
#the two nearest neighbours of "car"
print ss.get_neighbours("car", 2,
CosSimilarity())
Figure 2: Similarity queries in a semantic space.
Furthermore DISSECT allows the pos-
sibility of adding new data to a seman-
tic space in an online manner (using the
semantic space.peripheral space
functionality). This can be used as a way to effi-
ciently expand a co-occurrence matrix with new
rows, without re-applying the transformations to
the entire space. In some other cases, the user may
want to represent phrases that are specialization
of words already existing in the space (e.g.,
slimy maggot and maggot), without distorting the
computation of association measures by counting
the same context twice. In this case, adding slimy
maggot as a ?peripheral? row to a semantic space
that already contains maggot implements the
desired behaviour.
Similarity queries Semantic spaces are used for
the computation of similarity scores. DISSECT
provides a series of similarity measures such as co-
sine, inverse Euclidean distance and Lin similarity,
implemented in the composes.similarity
package. Similarity of two elements can be com-
puted within one semantic space or across two
spaces that have the same dimensionality. Figure
2 exemplifies (word) similarity computations with
DISSECT.
Composition functions Composition functions
in DISSECT (composes.composition) take
as arguments a list of element pairs to be com-
posed, and one or two spaces where the elements
to be composed are represented. They return a se-
mantic space containing the distributional repre-
sentations of the composed items, which can be
further transformed, used for similarity queries, or
used as inputs to another round of composition,
thus scaling up beyond binary composition. Fig-
ure 3 shows a Multiplicative composition exam-
ple. See Table 1 for the currently available com-
position models, their definitions and parameters.
Model Composition function Parameters
Add. w1~u+ w2~v w1(= 1), w2(= 1)
Mult. ~u ~v -
Dilation ||~u||22~v + (?? 1)?~u,~v?~u ?(= 2)
Fulladd W1~u+W2~v W1,W2 ? Rm?m
Lexfunc Au~v Au ? Rm?m
Table 1: Currently implemented composition
functions of inputs (u, v) together with parame-
ters and their default values in parenthesis, where
defined. Note that in Lexfunc each functor word
corresponds to a separate matrix or tensor Au (Ba-
roni and Zamparelli, 2010).
Parameter estimation All composition models
except Multiplicative have parameters to be esti-
mated. For simple models with few parameters,
such as as Additive, the parameters can be passed
by hand. However, DISSECT supports automated
parameter estimation from training examples. In
particular, we extend to all composition methods
the idea originally proposed by Baroni and Zam-
parelli (2010) for Lexfunc and Guevara (2010) for
Fulladd, namely to use corpus-extracted example
vectors of both the input (typically, words) and
output elements (typically, phrases) in order to op-
timize the composition operation parameters. The
problem can be generally stated as:
?? = arg min
?
||P ? fcomp?(U, V )||F
where U, V and P are matrices containing input
and output vectors respectively. For example U
may contain adjective vectors such as red, blue,
V noun vectors such as car, sky and P corpus-
extracted vectors for the corresponding phrases
red car, blue sky. fcomp? is a composition func-
tion and ? stands for a list of parameters that this
composition function is associated with.4 We im-
plement standard least-squares estimation meth-
ods as well as Ridge regression with the option
for generalized cross-validation, but other meth-
ods such as partial least-squares regression can be
easily added. Figure 4 exemplifies the Fulladd
model.
Composition output examples DISSECT pro-
vides functions to evaluate (compositional) distri-
butional semantic spaces against benchmarks in
the composes.utils package. However, as a
more qualitatively interesting example of what can
be done with DISSECT, Table 2 shows the nearest
4Details on the extended corpus-extracted vector estima-
tion method in DISSECT can be found in Dinu et al (2013).
33
#instantiate a multiplicative model
mult_model = Multiplicative()
#use the model to compose words from input space input_space
comp_space = mult_model.compose([("red", "book", "my_red_book"),
("red", "car", "my_red_car")],
input_space)
#compute similarity of: 1) two composed phrases and 2) a composed phrase and a word
print comp_space.get_sim("my_red_book", "my_red_car", CosSimilarity())
print comp_space.get_sim("my_red_book", "book", CosSimilarity(), input_space)
Figure 3: Creating and using Multiplicative phrase vectors.
#training data for learning an adjective-noun phrase model
train_data = [("red","book","red_book"), ("blue","car","blue_car")]
#train a fulladd model
fa_model = FullAdditive()
fa_model.train(train_data, input_space, phrase_space)
#use the model to compose a phrase from new words and retrieve its nearest neighb.
comp_space = fa_model.compose([("yellow", "table", "my_yellow_table")], input_space)
print comp_space.get_neighbours("my_yellow_table", 10, CosSimilarity())
Figure 4: Estimating a Fulladd model and using it to create phrase vectors.
Target Method Neighbours
florist Corpus Harrod, wholesaler, stockist
flora + -ist
Fulladd flora, fauna, ecologist
Lexfunc ornithologist, naturalist, botanist
Additive flora, fauna, ecosystem
Table 3: Compositional models for morphol-
ogy. Top 3 neighbours of florist using its (low-
frequency) corpus-extracted vector, and when the
vector is obtained through composition of flora
and -ist with Fulladd, Lexfunc and Additive.
neighbours of false belief obtained through com-
position with the Fulladd, Lexfunc and Additive
models. In Table 3, we exemplify a less typical ap-
plication of compositional models to derivational
morphology, namely obtaining a representation of
florist compositionally from distributional repre-
sentations of flora and -ist (Lazaridou et al, 2013).
4 Main features
Support for dense and sparse representations
Co-occurrence matrices, as extracted from text,
tend to be very sparse structures, especially when
using detailed context features which include syn-
tactic information, for example. On the other
hand, dimensionality reduction operations, which
are often used in distributional models, lead to
smaller, dense structures, for which sparse rep-
resentations are not optimal. This is our motiva-
tion for supporting both dense and sparse repre-
sentations. The choice of dense vs. sparse is ini-
tially determined by the input format, if a space
is created from co-occurrence counts. By default,
DISSECT switches to dense representations af-
ter dimensionality reduction, however the user can
freely switch from one representation to the other,
in order to optimize computations. For this pur-
pose DISSECT provides wrappers around matrix
operations, as well as around common linear alge-
bra operations, in the composes.matrix pack-
age. The underlying Python functionality is pro-
vided by numpy.array and scipy.sparse.
Efficient computations DISSECT is optimized
for speed since most operations are cast as matrix
operations, that are very efficiently implemented
in Python?s numpy and scipy modules5. Ta-
bles 4 and 5 show running times for typical DIS-
SECT operations: application of the ppmi weight-
ing scheme, nearest neighbour queries and estima-
tion of composition function parameters (on a 2.1
5For SVD on sparse structures, we use sparsesvd
(https://pypi.python.org/pypi/sparsesvd/).
For NMF, we adapted http://www.csie.ntu.edu.
tw/?cjlin/nmf/ (Lin, 2007).
34
Target Method Neighbours
belief Corpus moral, dogma, worldview, religion, world-view, morality, theism, tenet, agnosticism, dogmatic
false belief
Fulladd pantheist, belief, agnosticism, religiosity, dogmatism, pantheism, theist, fatalism, deism, mind-set
Lexfunc self-deception, untruth, credulity, obfuscation, misapprehension, deceiver, disservice, falsehood
Additive belief, assertion, falsity, falsehood, truth, credence, dogma, supposition, hearsay, denial
Table 2: Top nearest neighbours of belief and of false belief obtained through composition with the
Fulladd, Lexfunc and Additive models.
Method Fulladd Lexfunc Add. Dilation
Time (s.) 2864 787 46 68
Table 4: Composition model parameter estimation
times (in seconds) for 1 million training points in
300-dimensional space.
Matrix size (nnz) Ppmi Query
100Kx300 (30M) 5.8 0.5
100Kx100K (250M) 52.6 9.5
Table 5: Running times (in seconds) for 1) appli-
cation of ppmi weighting and 2) querying for the
top neighbours of a word (cosine similarity) for
different matrix sizes (nnz: number of non-zero
entries, in millions).
GHz machine). The price to pay for fast computa-
tions is that data must be stored in main memory.
We do not think that this is a major inconvenience.
For example, a typical symmetric co-occurrence
matrix extracted from a corpus of several billion
words, defining context in terms of 5-word win-
dows and considering the top 100K?100K most
frequent words, contains? 250 million entries and
requires only 2GB of memory for (double preci-
sion) storage.
Simple design We have opted for a very simple
and intuitive design as the classes interact in
very natural ways: A semantic space stores
the actual data matrix and structures to index
its rows and columns, and supports similarity
queries and transformations. Transformations
take one semantic space as input to return
another, transformed, space. Composition func-
tions take one or more input spaces and yield
a composed-elements space, which can further
undergo transformations and be used for similarity
queries. In fact, DISSECT semantic spaces also
support higher-order tensor representations, not
just vectors. Higher-order representations are
used, for example, to represent transitive verbs
and other multi-argument functors by Coecke
et al (2010) and Grefenstette et al (2013).
See http://clic.cimec.unitn.it/
composes/toolkit/composing.html for
an example of using DISSECT for estimating
such tensors.
Extensive documentation The DISSECT
documentation can be found at http://clic.
cimec.unitn.it/composes/toolkit.
We provide a tutorial which guides the user
through the creation of some toy semantic spaces,
estimation of the parameters of composition
models and similarity computations in semantic
spaces. We also provide a full-scale example
of intransitive verb-subject composition. We
show how to go from co-occurrence counts to
composed representations and make the data used
in the examples available for download.
Comparison to existing software In terms of
design choices, DISSECT most resembles the
Gensim toolkit (R?ehu?r?ek and Sojka, 2010). How-
ever Gensim is intended for topic modeling, and
therefore diverges considerably from DISSECT in
its functionality. The SSpace package of Jurgens
and Stevens (2010) also overlaps to some degree
with DISSECT in terms of its intended use, how-
ever, like Gensim, it does not support composi-
tional operations that, as far as we know, are an
unique feature of DISSECT.
5 Future extensions
We implemented and are currently testing DIS-
SECT functions supporting other composition
methods, including the one proposed by Socher
et al (2012). Adding further methods is our top-
priority goal. In particular, several distributional
models of word meaning in context share impor-
tant similarities with composition models, and we
plan to add them to DISSECT. Dinu et al (2012)
show, for example, that well-performing, simpli-
fied variants of the method in Thater et al (2010),
Thater et al (2011) and Erk and Pado? (2008) can
be reduced to relatively simple matrix operations,
making them particularly suitable for a DISSECT
implementation.
35
DISSECT is currently optimized for the compo-
sition of many phrases of the same type. This is in
line with most of the current evaluations of com-
positional models, which focus on specific phe-
nomena, such as adjectival modification, noun-
noun compounds or intransitive verbs, to name a
few. In the future we plan to provide a module for
composing entire sentences, taking syntactic trees
as input and returning composed representations
for each node in the input trees.
Finally, we intend to make use of the exist-
ing Python plotting libraries to add a visualization
module to DISSECT.
6 Acknowledgments
We thank Angeliki Lazaridou for helpful dis-
cussions. This research was supported by the
ERC 2011 Starting Independent Research Grant
n. 283554 (COMPOSES).
References
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Proceedings of EMNLP, pages 1183?1193, Boston,
MA.
Stephen Clark. 2012. Vector space models of lexical
meaning. In Shalom Lappin and Chris Fox, editors,
Handbook of Contemporary Semantics, 2nd edition.
Blackwell, Malden, MA. In press.
Bob Coecke, Mehrnoosh Sadrzadeh, and Stephen
Clark. 2010. Mathematical foundations for a com-
positional distributional model of meaning. Linguis-
tic Analysis, 36:345?384.
Georgiana Dinu, Stefan Thater, and So?ren Laue. 2012.
A comparison of models of word meaning in con-
text. In Proceedings of NAACL HLT, pages 611?
615, Montreal, Canada.
Georgiana Dinu, Nghia The Pham, and Marco Baroni.
2013. A general framework for the estimation of
distributional composition functions. In Proceed-
ings of ACL Workshop on Continuous Vector Space
Models and their Compositionality, Sofia, Bulgaria.
In press.
Katrin Erk and Sebastian Pado?. 2008. A structured
vector space model for word meaning in context. In
Proceedings of EMNLP, pages 897?906, Honolulu,
HI.
Katrin Erk. 2012. Vector space models of word mean-
ing and phrase meaning: A survey. Language and
Linguistics Compass, 6(10):635?653.
Edward Grefenstette, Georgiana Dinu, Yao-Zhong
Zhang, Mehrnoosh Sadrzadeh, and Marco Baroni.
2013. Multi-step regression learning for composi-
tional distributional semantics. In Proceedings of
IWCS, pages 131?142, Potsdam, Germany.
Emiliano Guevara. 2010. A regression model of
adjective-noun compositionality in distributional se-
mantics. In Proceedings of GEMS, pages 33?37,
Uppsala, Sweden.
David Jurgens and Keith Stevens. 2010. The S-Space
package: an open source package for word space
models. In Proceedings of the ACL 2010 System
Demonstrations, pages 30?35, Uppsala, Sweden.
Angeliki Lazaridou, Marco Marelli, Roberto Zampar-
elli, and Marco Baroni. 2013. Compositional-ly
derived representations of morphologically complex
words in distributional semantics. In Proceedings of
ACL, Sofia, Bulgaria. In press.
Chih-Jen Lin. 2007. Projected gradient methods for
Nonnegative Matrix Factorization. Neural Compu-
tation, 19(10):2756?2779.
Jeff Mitchell and Mirella Lapata. 2010. Composition
in distributional models of semantics. Cognitive Sci-
ence, 34(8):1388?1429.
Radim R?ehu?r?ek and Petr Sojka. 2010. Software frame-
work for topic modelling with large corpora. In Pro-
ceedings of the LREC 2010 Workshop on New Chal-
lenges for NLP Frameworks, pages 45?50, Valletta,
Malta.
Richard Socher, Brody Huval, Christopher Manning,
and Andrew Ng. 2012. Semantic compositionality
through recursive matrix-vector spaces. In Proceed-
ings of EMNLP, pages 1201?1211, Jeju Island, Ko-
rea.
Stefan Thater, Hagen Fu?rstenau, and Manfred Pinkal.
2010. Contextualizing semantic representations us-
ing syntactically enriched vector models. In Pro-
ceedings of ACL, pages 948?957, Uppsala, Sweden.
Stefan Thater, Hagen Fu?rstenau, and Manfred Pinkal.
2011. Word meaning in context: A simple and ef-
fective vector model. In Proceedings of IJCNLP,
pages 1134?1143, Chiang Mai, Thailand.
Peter Turney and Patrick Pantel. 2010. From fre-
quency to meaning: Vector space models of se-
mantics. Journal of Artificial Intelligence Research,
37:141?188.
Fabio Zanzotto, Ioannis Korkontzelos, Francesca
Falucchi, and Suresh Manandhar. 2010. Estimat-
ing linear models for compositional distributional
semantics. In Proceedings of COLING, pages 1263?
1271, Beijing, China.
36
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 90?99,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
A practical and linguistically-motivated approach
to compositional distributional semantics
Denis Paperno and Nghia The Pham and Marco Baroni
Center for Mind/Brain Sciences (University of Trento, Italy)
(denis.paperno|thenghia.pham|marco.baroni)@unitn.it
Abstract
Distributional semantic methods to ap-
proximate word meaning with context
vectors have been very successful empir-
ically, and the last years have seen a surge
of interest in their compositional exten-
sion to phrases and sentences. We present
here a new model that, like those of Co-
ecke et al (2010) and Baroni and Zam-
parelli (2010), closely mimics the standard
Montagovian semantic treatment of com-
position in distributional terms. However,
our approach avoids a number of issues
that have prevented the application of the
earlier linguistically-motivated models to
full-fledged, real-life sentences. We test
the model on a variety of empirical tasks,
showing that it consistently outperforms a
set of competitive rivals.
1 Compositional distributional semantics
The research of the last two decades has estab-
lished empirically that distributional vectors for
words obtained from corpus statistics can be used
to represent word meaning in a variety of tasks
(Turney and Pantel, 2010). If distributional vec-
tors encode certain aspects of word meaning, it is
natural to expect that similar aspects of sentence
meaning can also receive vector representations,
obtained compositionally from word vectors. De-
veloping a practical model of compositionality is
still an open issue, which we address in this pa-
per. One approach is to use simple, parameter-
free models that perform operations such as point-
wise multiplication or summing (Mitchell and La-
pata, 2008). Such models turn out to be sur-
prisingly effective in practice (Blacoe and Lap-
ata, 2012), but they have obvious limitations. For
instance, symmetric operations like vector addi-
tion are insensitive to syntactic structure, there-
fore meaning differences encoded in word order
are lost in composition: pandas eat bamboo is
identical to bamboo eats pandas. Guevara (2010),
Mitchell and Lapata (2010), Socher et al (2011)
and Zanzotto et al (2010) generalize the simple
additive model by applying structure-encoding op-
erators to the vectors of two sister nodes before
addition, thus breaking the inherent symmetry of
the simple additive model. A related approach
(Socher et al, 2012) assumes richer lexical rep-
resentations where each word is represented with
a vector and a matrix that encodes its interaction
with its syntactic sister. The training proposed in
this model estimates the parameters in a super-
vised setting. Despite positive empirical evalua-
tion, this approach is hardly practical for general-
purpose semantic language processing, since it re-
quires computationally expensive approximate pa-
rameter optimization techniques, and it assumes
task-specific parameter learning whose results are
not meant to generalize across tasks.
1.1 The lexical function model
None of the proposals mentioned above, from sim-
ple to elaborate, incorporates in its architecture the
intuitive idea (standard in theoretical linguistics)
that semantic composition is more than a weighted
combination of words. Generally one of the com-
ponents of a phrase, e.g., an adjective, acts as
a function affecting the other component (e.g., a
noun). This underlying intuition, adopted from
formal semantics of natural language, motivated
the creation of the lexical function model of com-
position (lf ) (Baroni and Zamparelli, 2010; Co-
ecke et al, 2010). The lf model can be seen as a
projection of the symbolic Montagovian approach
to semantic composition in natural language onto
the domain of vector spaces and linear operations
on them (Baroni et al, 2013). In lf, arguments
are vectors and functions taking arguments (e.g.,
adjectives that combine with nouns) are tensors,
with the number of arguments (n) determining the
90
order of tensor (n+1). For example, adjectives, as
unary functors, are modeled with 2-way tensors, or
matrices. Tensor by vector multiplication formal-
izes function application and serves as the general
composition method.
Baroni and Zamparelli (2010) propose a practi-
cal and empirically effective way to estimate ma-
trices representing adjectival modifiers of nouns
by linear regression from corpus-extracted exam-
ples of noun and adjective-noun vectors. Un-
like the neural network approach of Socher et
al. (2011; 2012), the Baroni and Zamparelli
method does not require manually labeled data nor
costly iterative estimation procedures, as it relies
on automatically extracted phrase vectors and on
the analytical solution of the least-squares-error
problem.
The same method was later applied to matrix
representations of intransitive verbs and determin-
ers (Bernardi et al, 2013; Dinu et al, 2013), al-
ways with good empirical results.
The full range of semantic types required for
natural language processing, including those of
adverbs and transitive verbs, has to include, how-
ever, tensors of greater rank. The estimation
method originally proposed by Baroni and Zam-
parelli has been extended to 3-way tensors rep-
resenting transitive verbs by Grefenstette et al
(2013) with preliminary success. Grefenstette et
al.?s method works in two steps. First, one esti-
mates matrices of verb-object phrases from sub-
ject and subject-verb-object vectors; next, transi-
tive verb tensors are estimated from verb-object
matrices and object vectors.
1.2 Problems with the extension of the lexical
function model to sentences
With all the advantages of lf, scaling it up to ar-
bitrary sentences, however, leads to several issues.
In particular, it is desirable for all practical pur-
poses to limit representation size. For example,
if noun meanings are encoded in vectors of 300
dimensions, adjectives become matrices of 300
2
cells, and transitive verbs are represented as ten-
sors with 300
3
=27, 000, 000 dimensions.
Estimating tensors of this size runs into data
sparseness issues already for less common tran-
sitive verbs. Indeed, in order to train a transitive
verb tensor (e.g., eat), the method of Grefenstette
et al (2013) requires a sufficient number of dis-
tinct verb object phrases with that verb (e.g., eat
cake, eat fruits), each attested in combination with
a certain number of subject nouns with sufficient
frequency to extract sensible vectors. It is not fea-
sible to obtain enough data points for all verbs in
such a training design.
Things get even worse for other categories.
Adverbs like quickly that modify intransitive
verbs have to be represented with 300
2
2
=
8, 100, 000, 000 dimensions. Modifiers of transi-
tive verbs would have even greater representation
size, which may not be possible to store and learn
efficiently.
Another issue is that the same or similar items
that occur in different syntactic contexts are as-
signed different semantic types with incompara-
ble representations. For example, verbs like eat
can be used in transitive or intransitive construc-
tions (children eat meat/children eat), or in passive
(meat is eaten). Since predicate arity is encoded
in the order of the corresponding tensor, eat and
the like have to be assigned different representa-
tions (matrix or tensor) depending on the context.
Deverbal nouns like demolition, often used with-
out mention of who demolished what, would have
to get vector representations while the correspond-
ing verbs (demolish) would become tensors, which
makes immediately related verbs and nouns in-
comparable. Nouns in general would oscillate be-
tween vector and matrix representations depend-
ing on argument vs. predicate vs. modifier posi-
tion (an animal runs vs. this is an animal vs. an-
imal shelter). Prepositions are the hardest, as the
syntactic positions in which they occur are most
diverse (park in the dark vs. play in the dark vs.
be in the dark vs. a light glowing in the dark).
In all those cases, the same word has to be
mapped to tensors of different orders. Since each
of these tensors must be learned from examples
individually, their obvious relation is missed. Be-
sides losing the comparability of the semantic con-
tribution of a word across syntactic contexts, we
also worsen the data sparseness issues.
The last, and related, point is that for the ten-
sor calculus to work, one needs to model, for each
word, each of the constructions in the corpus that
the word is attested in. In its pure form lf does
not include an emergency backoff strategy when
unknown words or constructions are encountered.
For example, if we only observe transitive usages
of to eat in the training corpus, and encounter an
intransitive or passive example of it in testing data,
91
the system would not be able to compose a sen-
tence vector at all. This issue is unavoidable since
we don?t expect to find all words in all possible
constructions even in the largest corpus.
2 The practical lexical function model
As follows from section 1.2, it would be desirable
to have a compositional distributional model that
encodes function-argument relations but avoids
the troublesome high-order tensor representations
of the pure lexical function model, with all the
practical problems that come with them. We may
still want to represent word meanings in differ-
ent syntactic contexts differently, but at the same
time we need to incorporate a formal connection
between those representations, e.g., between the
transitive and the intransitive instantiations of the
verb to eat. Last but not least, all items need to
include a common aspect of their representation
(e.g., a vector) to allow comparison across cate-
gories (the case of demolish and demolition).
To this end, we propose a new model of compo-
sition that maintains the idea of function applica-
tion, while avoiding the complications and rigidity
of lf. We call our proposal practical lexical func-
tion model, or plf. In plf, a functional word is not
represented by a single tensor of arity-dependent
order, but by a vector plus an ordered set of matri-
ces, with one matrix for each argument the func-
tion takes. After applying the matrices to the cor-
responding argument vectors, a single representa-
tion is obtained by summing across all resulting
vectors.
2.1 Word meaning representation
In plf, all words are represented by a vector, and
functional words, such as predicates and modi-
fiers, are also assigned one or more matrices. The
general form of a semantic representation for a
linguistic unit is an ordered tuple of a vector and
n ? N matrices:
1
?
~x,
2
1
x , . . . ,
2
n
x
?
The number of matrices in the representation
encodes the arity of a linguistic unit, i.e., the num-
ber of other units to which it applies as a function.
Each matrix corresponds to a function-argument
relation, and words have as many matrices as
many arguments they take: none for (most) nouns,
1
Matrices associated with term x are symbolized
2
x.
dog
~
dog
run ~run,
2
run
chase
~
chase,
2
s
chase,
2
o
chase
give
~
give,
2
s
give,
2
o
give,
2
io
give
big
~
big,
2
big
very ~very,
2
n
very,
2
a
very
quickly
~
quickly,
2
s
quickly,
2
v
quickly
Table 1: Examples of word representations. Sub-
scripts encode, just for mnemonic purposes, the
constituent whose vector the matrix combines
with: subject, object, indirect object, noun,
adjective, verb phrase.
one for adjectives and intransitive verbs, two for
transitives, etc. The matrices formalize argument
slot saturation, operating on an argument vector
representation through matrix by vector multipli-
cation, as described in the next section.
Modifiers of n-ary functors are represented by
n+1-ary structures. For instance, we treat adjec-
tives that modify nouns (0-ary) as unary functions,
encoded in a vector-matrix pair. Adverbs have dif-
ferent semantic types depending on their syntac-
tic role. Sentential adverbs are unary, while ad-
verbs that modify adjectives (very) or verb phrases
(quickly) are encoded as binary functions, repre-
sented by a vector and two matrices. The form of
semantic representations we are using is shown in
Table 1.
2
2.2 Semantic composition
Our system incorporates semantic composition via
two composition rules, one for combining struc-
tures of different arity and the other for symmet-
ric composition of structures with the same ar-
ity. These rules incorporate insights of two em-
pirically successful models, lexical function and
the simple additive approach, used as the default
structure merging strategy.
The first rule is function application, illustrated
in Figure 1. Table 2 illustrates simple cases of
function application. For transitive verbs seman-
tic composition applies iteratively as shown in the
derivation of Figure 2. For ternary predicates such
2
To determine the number and ordering of matrices rep-
resenting the word in the current syntactic context, our plf
implementation relies on the syntactic type assigned to the
word in the categorial grammar parse of the sentence.
92
?~x+
2
n+k
x ? ~y,
2
1
x +
2
1
y , . . . ,
2
n
x +
2
n
y , . . .
?
?
~x,
2
1
x , . . . ,
2
n
x , . . . ,
2
n+k
x
? ?
~y,
2
1
y , . . . ,
2
n
y
?
Figure 1: Function application: If two syntactic
sisters have different arity, treat the higher-arity
sister as the functor. Compose by multiplying the
last matrix in the functor tuple by the argument
vector and summing the result to the functor vec-
tor. Unsaturated matrices are carried up to the
composed node, summing across sisters if needed.
dogs
~
dogs
run ~run,
2
run
dogs run ~run +
2
run?
~
dog
house
~
house
big
~
big,
2
big
big house
~
big +
2
big ?
~
house
Table 2: Examples of function application.
as give in a ditransitive construction, the first step
in the derivation absorbs the innermost argument
by multiplying its vector by the third give matrix,
and then composition proceeds like for transitives.
The second composition rule, symmetric com-
position applies when two syntactic sisters are of
the same arity (e.g., two vectors, or two vector-
matrix pairs). Symmetric composition simply
sums the objects in the two tuples: vector with
vector, n-th matrix with n-th matrix.
Symmetric composition is reserved for struc-
tures in which the function-argument distinction
is problematic. Some candidates for such treat-
ment are coordination and nominal compounds,
although we recognize that the headless analysis is
2
s
chase?
~
dogs+
~
chase+
2
o
chase?
~
cats
~
dogs
?
~
chase+
2
o
chase?
~
cats,
2
s
chase
?
?
~
chase,
2
s
chase,
2
o
chase
?
~
cats
Figure 2: Applying function application twice to
derive the representation of a transitive sentence.
sing:
~
sing,
2
sing dance:
~
dance,
2
dance
sing and dance:
~
sing +
~
dance,
2
sing +
2
dance
rice:
~
rice cake:
~
cake
rice cake
~
rice+
~
cake
Table 3: Examples of symmetric composition.
not the only possible one here. See two examples
of Symmetric Composition application in Table 3.
Note that the sing and dance composition in Ta-
ble 3 skips the conjunction. Our current plf im-
plementation treats most grammatical words, in-
cluding conjunctions, as ?empty? elements, that
do not project into semantics. This choice leads
to some interesting ?serendipitous? treatments of
various constructions. For example, since the cop-
ula is empty, a sentence with a predicative adjec-
tive (cars are red) is treated in the same way as a
phrase with the same adjective in attributive posi-
tion (red cars) ? although the latter, being a phrase
and not a full sentence, will later be embedded as
argument in a larger construction. Similarly, leav-
ing the relative pronoun empty makes cars that
run identical to cars run, although, again, the for-
mer will be embedded in a larger construction later
in the derivation.
We conclude our brief exposition of plf with an
alternative intuition for it: the plf model is also
a more sophisticated version of the additive ap-
proach, where argument words are adapted by ma-
trices that encode the relation to their functors be-
fore the sentence vector is derived by summing.
2.3 Satisfying the desiderata
Let us now outline how plf addresses the short-
comings of lf listed in Section 1.2. First, all is-
sues caused by representation size disappear. An
n-ary predicate is no longer encoded as an n+1-
way tensor; instead we have a sequence of n ma-
trices. The representation size grows linearly, not
exponentially, for higher semantic types, allowing
for simpler and more efficient parameter estima-
tion, storage, and computation.
As a consequence of our architecture, we no
longer need to perform the complicated step-by-
step estimation for elements of higher arity. In-
deed, one can estimate each matrix of a com-
plex representation individually using the simple
method of Baroni and Zamparelli (2010). For in-
stance, for transitive verbs we estimate the verb-
subject combination matrix from subject and verb-
93
boys
~
boys
eat (intrans.)
~
eat,
2
s
eat
boys eat
2
s
eat?
~
boys+
~
eat
meat
~
meat
eat (trans.)
~
eat,
2
s
eat,
2
o
eat
boys eat meat
2
s
eat?
~
boys+
~
eat+
2
o
eat?
~
meat
(is) eaten (pass.)
~
eat,
2
o
eat
meat is eaten
~
eat+
2
o
eat?
~
meat
Table 4: The verb to eat associated to different sets
of matrices in different syntactic contexts.
subject vectors, the verb-object combination ma-
trix from object and verb-object vectors. We ex-
pect a reasonably large corpus to feature many oc-
currences of a verb with a variety of subjects and
a variety of objects (but not necessarily a variety
of subjects with each of the objects as required by
Grefenstette et al?s training), allowing us to avoid
the data sparseness issue.
The semantic representations we propose in-
clude a semantic vector for constituents of any se-
mantic type, thus enabling semantic comparison
for words of different parts of speech (the case of
demolition vs. demolish).
Finally, the fact that we represent the predicate
interaction with each of its arguments in a sepa-
rate matrix allows for a natural and intuitive treat-
ment of argument alternations. For instance, as
shown in Table 4, one can distinguish the transi-
tive and intransitive usages of the verb to eat by
the presence of the object-oriented matrix of the
verb while keeping the rest of the representation
intact. To model passive usages, we insert the ob-
ject matrix of the verb only, which will be multi-
plied by the syntactic subject vector, capturing the
similarity between eat meat and meat is eaten.
So keeping the verb?s interaction with subject
and object encoded in distinct matrices not only
solves the issues of representation size for arbi-
trary semantic types, but also provides a sensible
built-in strategy for handling a word?s occurrence
in multiple constructions. Indeed, if we encounter
a verb used intransitively which was only attested
as transitive in the training corpus, we can simply
omit the object matrix to obtain a type-appropriate
representation. On the other hand, if the verb oc-
curs with more arguments than usual in testing
materials, we can add a default diagonal identity
matrix to its representation, signaling agnosticism
about how the verb relates to the unexpected argu-
ment. This flexibility makes our model suitable to
compute vector representations of sentences with-
out stumbling at unseen syntactic usages of words.
To summarize, plf is an extension of the lexi-
cal function model that inherits its strengths and
overcomes its weaknesses. We still employ a
linguistically-motivated notion of semantic com-
position as function application and use distinct
kinds of representations for different semantic
types. At the same time, we avoid high order ten-
sor representations, produce semantic vectors for
all syntactic constituents, and allow for an elegant
and transparent correspondence between different
syntactic usages of a lexeme, such as the transi-
tive, the intransitive, and the passive usages of the
verb to eat. Last but not least, our implementation
is suitable for realistic language processing since
it allows to produce vectors for sentences of arbi-
trary size, including those containing novel syn-
tactic configurations.
3 Evaluation
3.1 Evaluation materials
We consider 5 different benchmarks that focus on
different aspects of sentence-level semantic com-
position. The first data set, created by Edward
Grefenstette and Mehrnoosh Sadrzadeh and in-
troduced in Kartsaklis et al (2013), features 200
sentence pairs that were rated for similarity by
43 annotators. In this data set, sentences have
fixed adjective-noun-verb-adjective-noun (anvan)
structure, and they were built in order to cru-
cially require context-based verb disambiguation
(e.g., young woman filed long nails is paired with
both young woman smoothed long nails and young
woman registered long nails). We also consider a
similar data set introduced by Grefenstette (2013),
comprising 200 sentence pairs rated by 50 anno-
tators. We will call these benchmarks anvan1 and
anvan2, respectively. Evaluation is carried out by
computing the Spearman correlation between the
annotator similarity ratings for the sentence pairs
and the cosines of the vectors produced by the var-
ious systems for the same sentence pairs.
The benchmark introduced by The Pham et al
(2013) at the TFDS workshop (tfds below) was
specifically designed to test compositional meth-
ods for their sensitivity to word order and the se-
mantic effect of determiners. The tfds benchmark
contains 157 target sentences that are matched
with a set of (approximate) paraphrases (8 on av-
94
erage), and a set of ?foils? (17 on average). The
foils have high lexical overlap with the targets but
very different meanings, due to different determin-
ers and/or word order. For example, the target
A man plays an acoustic guitar is matched with
paraphrases such as A man plays guitar and The
man plays the guitar, and foils such as The man
plays no guitar and A guitar plays a man. A
good system should return higher similarities for
the comparison with the paraphrases with respect
to that with the foils. Performance is assessed
through the t-standardized cross-target average of
the difference between mean cosine with para-
phrases and mean cosine with foils (Pham and col-
leagues, equivalently, reported non-standardized
average and standard deviations).
The two remaining data sets are larger and more
?natural?, as they were not constructed by linguists
under controlled conditions to focus on specific
phenomena. They are aimed at evaluating sys-
tems on the sort of free-form sentences one en-
counters in real-life applications. The msrvid data
set from the SemEval-2012 Semantic Textual Sim-
ilarity (STS) task (Agirre et al, 2012) consists of
750 sentence pairs that describe brief videos. Sen-
tence pairs were scored for similarity by 5 subjects
each. Following standard practice in paraphrase
detection studies (e.g., Blacoe and Lapata (2012)),
we use cosine similarity between sentence pairs as
computed by one of our systems together with two
shallow similarity cues: word overlap between the
two sentences and difference in sentence length.
We obtain a final similarity score by weighted ad-
dition of the 3 cues, with the optimal weights de-
termined by linear regression on separate msrvid
train data that were also provided by the SemEval
task organizers (before combining, we checked
that the collinearity between cues was low). Sys-
tem scores are evaluated by their Pearson correla-
tion with the human ratings.
The final set we use is onwn, from the *SEM-
2013 STS shared task (Agirre et al, 2013). This
set contains 561 pairs of glosses (from the Word-
Net and OntoNotes databases), rated by 5 judges
for similarity. Our main interest in this set stems
from the fact that glosses are rarely well-formed
full sentences (consider, e.g., cause something to
pass or lead somewhere; coerce by violence, fill
with terror). For this reason, they are very chal-
lenging for standard parsers. Indeed, we estimated
from a sample of 40 onwn glosses that the C&C
parser (see below) has only 45% accuracy on this
set. Since plf needs syntactic information to con-
struct sentence vectors compositionally, we test it
on onwn to make sure that it is not overly sensi-
tive to parser noise. Evaluation proceeds as with
msrvid (cue weights are determined by 10-fold
cross-validation).
3
3.2 Semantic space construction and
composition model implementation
Our source corpus was given by the concatena-
tion of ukWaC (wacky.sslmit.unibo.it),
a mid-2009 dump of the English Wikipedia (en.
wikipedia.org) and the British National Cor-
pus (www.natcorp.ox.ac.uk), for a total of
about 2.8 billion words.
We collected a 30K-by-30K matrix by counting
co-occurrence of the 30K most frequent content
lemmas (nouns, adjectives and verbs) within a 3-
word window. The raw count vectors were trans-
formed into positive Pointwise Mutual Informa-
tion scores and reduced to 300 dimensions by the
Singular Value Decomposition. All vectors were
normalized to length 1. This setup was picked
without tuning, as we found it effective in previ-
ous, unrelated experiments.
4
We consider four composition models. The add
(additive) model produces the vector of a sentence
by summing the vectors of all content words in it.
Similarly, mult uses component-wise multiplica-
tion of vectors for composition. While these mod-
els are very simple, a long experimental tradition
has proven their effectiveness (Landauer and Du-
mais, 1997; Mitchell and Lapata, 2008; Mitchell
and Lapata, 2010; Blacoe and Lapata, 2012).
For the lf (lexical function) model, we construct
functional matrix representations of adjectives, de-
terminers and intransitive verbs. These are trained
using Ridge regression with generalized cross-
validation from corpus-extracted vectors of nouns,
3
We did not evaluate on other STS benchmarks since they
have characteristics, such as high density of named entities,
that would require embedding our compositional models into
more complex systems, obfuscating their impact on the over-
all performance.
4
With the multiplicative composition model we also tried
Nonnegative Matrix Factorization instead of Singular Value
Decomposition, because the negative values produced by
SVD are potentially problematic for mult. In addition, we re-
peated the evaluation for the multiplicative and additive mod-
els without any form of dimensionality reduction. The over-
all pattern of results did not change significantly, and thus for
consistency we report all models? performance only for the
SVD-reduced space.
95
as input, and phrases including those nouns as out-
put (e.g., the matrix for red is trained from corpus-
extracted ?noun, red-noun? vector pairs). Transi-
tive verb tensors are estimated using the two-step
regression procedure outlined by Grefenstette et
al. (2013). We did not attempt to train a lf model
for the larger and more varied msrvid and onwn
data sets, as this would have been extremely time
consuming and impractical for all the reasons we
discussed in Section 1.2 above.
Training plf (practical lexical function) pro-
ceeds similarly, but we also build preposition
matrices (from ?noun, preposition-noun? vector
pairs), and for verbs we prepare separate subject
and object matrices.
Since syntax guides lf and plf composition, we
supplied all test sentences with categorial gram-
mar parses. Every sentence in the anvan1 and
anvan2 datasets has the form (subject) Adjective
+ Noun + Transitive Verb + (object) Adjective +
Noun, so parsing them is trivial. All sentences in
tfds have a predictable structure that allows per-
fect parsing with simple finite state rules. In all
these cases, applying a general-purpose parser to
the data would have, at best, had no impact and,
at worst, introduced parsing errors. For msrvid
and onwn, we used the output of the C&C parser
(Clark and Curran, 2007).
3.3 Results
Table 5 summarizes the performance of our mod-
els on the chosen tasks, and compares it to the state
of the art reported in previous work, as well as to
various strong baselines.
The plf model performs very well on both an-
van benchmarks, outperforming not only add and
mult, but also the full-fledged lf model. Given
that these data sets contain, systematically, transi-
tive verbs, the major difference between plf and lf
lies in their representation of the latter. Evidently,
the separately-trained subject and object matrices
of plf, being less affected by data sparseness than
the 3-way tensors of lf, are better able to capture
how verbs interact with their arguments. For an-
van1, plf is just below the state of the art, which
is based on disambiguating the verb vector in con-
text (Kartsaklis and Sadrzadeh, 2013), and lf out-
performs the baseline, which consists in using the
verb vector only as a proxy to sentence similar-
ity.
5
On anvan2, plf outperforms the best model
5
We report state of the art from Kartsaklis and Sadrzadeh
models anvan anvan tfds msr onwn
1 2 vid
add 8 22 -0.2 78 66
mult 8 -4 -2.3 77 55
lf 15 30 5.90 NA NA
plf 20 36 2.7 79 67
soa 22 27 11.4 87 75
baseline 8 22 7.9 77 55
Table 5: Performance of composition models on
all evaluation sets. Figures of merit follow previ-
ous art on each set and are: percentage Spearman
coefficients for anvan1 and anvan2, t-standardized
average difference between mean cosines with
paraphrases and with foils for tfds, percentage
Pearson coefficients for msrvid and onwn. State-
of-the-art (soa) references: anvan1: Kartsaklis and
Sadrzadeh (2013); anvan2: Grefenstette (2013);
tfds: The Pham et al (2013); msrvid: B?ar et
al. (2012); onwn: Han et al (2013). Baselines:
anvan1/anvan2: verb vectors only; tfds: word
overlap; msrvid/onwn: word overlap + sentence
length.
reported by Grefenstette (2013) (an implementa-
tion of the lexical function ideas along the lines of
Grefenstette and Sadrzadeh (2011a; 2011b)). And
lf is, again, the only model, besides plf, that per-
forms better than the baseline.
In the tfds task, not surprisingly the add and
mult models, lacking determiner representations
and being order-insensitive, fail to distinguish be-
tween true paraphrases and foils (indeed, for the
mult model foils are significantly closer to the tar-
gets than the paraphrases, probably because the
latter have lower content word overlap than the
foils, that often differ in word order and determin-
ers only). Our plf approach is able to handle deter-
miners and word order correctly, as demonstrated
by a highly significant (p < 0.01) difference be-
tween paraphrase and foil similarity (average dif-
ference in cosine .017, standard deviation .077). In
this case, however, the traditional lf model (aver-
age difference .044, standard deviation .092) out-
performs plf. Since determiners are handled iden-
tically under the two approaches, the culprit must
be word order. We conjecture that the lf 3-way
tensor representation of transitive verbs leads to
a stronger asymmetry between sentences with in-
(2013) rather than Kartsaklis et al (2013), since only the for-
mer used a source corpus that is comparable to ours.
96
verted arguments, and thus makes this model par-
ticularly sensitive to word order differences. In-
deed, if we limit evaluation to those foils charac-
terized by word order changes only, lf discrim-
inates between paraphrases and foils even more
clearly, whereas the plf difference, while still sig-
nificant, decreases slightly.
The state-of-the-art row for tfds reports the lf
implementation by The Pham et al (2013), which
outperforms ours. The main difference is that
Pham and colleagues do not normalize vectors like
we do. If we don?t normalize, we do get larger dif-
ferences for our models as well, but consistently
lower performance in all other tasks. More wor-
ryingly, the simple word overlap baseline reported
in the table sports a larger difference than our best
model. Clearly, this baseline is exploiting the sys-
tematic determiner differences in the foils and, in-
deed, when it is evaluated on foils where only
word order changes its performance is no longer
significant.
On msrvid, the plf approach outperforms add
and mult, although the difference between the
three is not big. Our result stands in contrast with
Blacoe and Lapata (2012), the only study we are
aware of that compared a sophisticated composi-
tion model (Socher et al?s 2011 model) to add
and mult on realistic sentences, which attained the
top performance with the simple models for both
figures of merit they used.
6
The best 2012 STS
system (B?ar et al, 2012), obtained 0.87 correla-
tion, but with many more and considerably more
complex features than the ones we used here. In-
deed, our simple system would have obtained a re-
spectable 25/89 ranking in the STS 2012 msrvid
task. Still, we must also stress the impressive per-
formance of our baseline, given by the combina-
tion of the word overlap and sentence length cues.
This suggests that the msrvid benchmark lacks the
lexical and syntactic variety we would like to test
our systems on.
Our plf model is again the best on the onwn
set (albeit by a small margin over add). This
is a very positive result, in the light of the fact
that the parser has very low performance on the
onwn glosses, thus suggesting that plf can pro-
duce sensible semantic vectors from noisy syntac-
6
We refer here to the results reported in the er-
ratum available at http://homepages.inf.ed.ac.
uk/s1066731/pdf/emnlp2012erratum.pdf. The
add/mult advantage was even more marked in the original pa-
per.
tic representations. Here the overlap+length base-
line does not perform so well, and again the best
STS 2013 system (Han et al, 2013) uses consider-
ably richer knowledge sources and algorithms than
ours. Our plf-based method would have reached a
respectable 20/90 rank in the STS 2013 onwn task.
As a final remark, in all experiments the running
time of plf was only slightly larger than for the
simpler models, but orders of magnitude smaller
than lf, confirming another practical side of our
approach.
4 Conclusion
We introduced an approach to compositional dis-
tributional semantics based on a linguistically-
motivated syntax-to-semantics type mapping, but
simple and flexible enough that it can produce rep-
resentations of English sentences of arbitrary size
and structure.
We showed that our approach is competitive
against the more complex lexical function model
when evaluated on the simple constructions the
latter can be applied to, and it outperforms the ad-
ditive and multiplicative compositionality models
when tested on more realistic benchmarks (where
the full-fledged lexical function approach is dif-
ficult or impossible to use), even in presence of
strong noise in its syntactic input. While our re-
sults are encouraging, no current benchmark com-
bines large-scale, real-life data with the syntactic
variety on which a syntax-driven approach to se-
mantics such as ours could truly prove its worth.
The recently announced SemEval 2014 Task 1
7
is
filling exactly this gap, and we look forward to ap-
ply our method to this new benchmark, as soon as
it becomes available.
One of the strengths of our framework is that
it allows for incremental improvement focused on
specific constructions. For example, one could add
representations for different conjunctions (and vs.
or), train matrices for verb arguments other than
subject and direct object, or include new types of
modifiers into the model, etc.
While there is potential for local improvements,
our framework, which extends and improves on
existing compositional semantic vector models,
has demonstrated its ability to account for full sen-
tences in a principled and elegant way. Our imple-
mentation of the model relies on simple and effi-
7
http://alt.qcri.org/semeval2014/
task1/
97
cient training, works fast, and shows good empiri-
cal results.
Acknowledgements
We thank Roberto Zamparelli and the COM-
POSES team for helpful discussions. This re-
search was supported by the ERC 2011 Starting
Independent Research Grant n. 283554 (COM-
POSES).
References
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor
Gonzalez-Agirre. 2012. SemEval-2012 Task 6: a
pilot on semantic textual similarity. In Proceedings
of *SEM, pages 385?393, Montreal, Canada.
Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-
Agirre, and Weiwei Guo. 2013. *SEM 2013 shared
task: Semantic Textual Similarity. In Proceedings
of *SEM, pages 32?43, Atlanta, GA.
Daniel B?ar, Chris Biemann, Iryna Gurevych, and
Torsten Zesch. 2012. UKP: Computing seman-
tic textual similarity by combining multiple content
similarity measures. In Proceedings of *SEM, pages
435?440, Montreal, Canada.
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Proceedings of EMNLP, pages 1183?1193, Boston,
MA.
Marco Baroni, Raffaella Bernardi, and Roberto Zam-
parelli. 2013. Frege in space: A program for
compositional distributional semantics. Linguistic
Issues in Language Technology. In press; http:
//clic.cimec.unitn.it/composes/
materials/frege-in-space.pdf.
Raffaella Bernardi, Georgiana Dinu, Marco Marelli,
and Marco Baroni. 2013. A relatedness benchmark
to test the role of determiners in compositional dis-
tributional semantics. In Proceedings of ACL (Short
Papers), pages 53?57, Sofia, Bulgaria.
William Blacoe and Mirella Lapata. 2012. A com-
parison of vector-based representations for seman-
tic composition. In Proceedings of EMNLP, pages
546?556, Jeju Island, Korea.
Stephen Clark and James Curran. 2007. Wide-
coverage efficient statistical parsing with CCG
and log-linear models. Computational Linguistics,
33(4):493?552.
Bob Coecke, Mehrnoosh Sadrzadeh, and Stephen
Clark. 2010. Mathematical foundations for a com-
positional distributional model of meaning. Linguis-
tic Analysis, 36:345?384.
Georgiana Dinu, Nghia The Pham, and Marco Baroni.
2013. General estimation and evaluation of com-
positional distributional semantic models. In Pro-
ceedings of ACL Workshop on Continuous Vector
Space Models and their Compositionality, pages 50?
58, Sofia, Bulgaria.
Edward Grefenstette and Mehrnoosh Sadrzadeh.
2011a. Experimental support for a categorical com-
positional distributional model of meaning. In Pro-
ceedings of EMNLP, pages 1394?1404, Edinburgh,
UK.
Edward Grefenstette and Mehrnoosh Sadrzadeh.
2011b. Experimenting with transitive verbs in a Dis-
CoCat. In Proceedings of GEMS, pages 62?66, Ed-
inburgh, UK.
Edward Grefenstette, Georgiana Dinu, Yao-Zhong
Zhang, Mehrnoosh Sadrzadeh, and Marco Baroni.
2013. Multi-step regression learning for composi-
tional distributional semantics. In Proceedings of
IWCS, pages 131?142, Potsdam, Germany.
Edward Grefenstette. 2013. Category-Theoretic
Quantitative Compositional Distributional Models
of Natural Language Semantics. PhD thesis, Uni-
versity of Oxford Essex.
Emiliano Guevara. 2010. A regression model of
adjective-noun compositionality in distributional se-
mantics. In Proceedings of GEMS, pages 33?37,
Uppsala, Sweden.
Lushan Han, Abhay Kashyap, Tim Finin,
James Mayfield, and Jonathan Weese. 2013.
UMBC EBIQUITY-CORE: Semantic textual sim-
ilarity systems. In Proceedings of *SEM, pages
44?52, Atlanta, GA.
Dimitri Kartsaklis and Mehrnoosh Sadrzadeh. 2013.
Prior disambiguation of word tensors for construct-
ing sentence vectors. In Proceedings of EMNLP,
pages 1590?1601, Seattle, WA.
Dimitri Kartsaklis, Mehrnoosh Sadrzadeh, and Stephen
Pulman. 2013. Separating disambiguation from
composition in distributional semantics. In Pro-
ceedings of CoNLL, pages 114?123, Sofia, Bulgaria.
Thomas Landauer and Susan Dumais. 1997. A solu-
tion to Plato?s problem: The latent semantic analysis
theory of acquisition, induction, and representation
of knowledge. Psychological Review, 104(2):211?
240.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings of
ACL, pages 236?244, Columbus, OH.
Jeff Mitchell and Mirella Lapata. 2010. Composition
in distributional models of semantics. Cognitive Sci-
ence, 34(8):1388?1429.
98
Richard Socher, Eric Huang, Jeffrey Pennin, Andrew
Ng, and Christopher Manning. 2011. Dynamic
pooling and unfolding recursive autoencoders for
paraphrase detection. In Proceedings of NIPS, pages
801?809, Granada, Spain.
Richard Socher, Brody Huval, Christopher Manning,
and Andrew Ng. 2012. Semantic compositionality
through recursive matrix-vector spaces. In Proceed-
ings of EMNLP, pages 1201?1211, Jeju Island, Ko-
rea.
Nghia The Pham, Raffaella Bernardi, Yao-Zhong
Zhang, and Marco Baroni. 2013. Sentence para-
phrase detection: When determiners and word or-
der make the difference. In Proceedings of the To-
wards a Formal Distributional Semantics Workshop
at IWCS 2013, pages 21?29, Potsdam, Germany.
Peter Turney and Patrick Pantel. 2010. From fre-
quency to meaning: Vector space models of se-
mantics. Journal of Artificial Intelligence Research,
37:141?188.
Fabio Zanzotto, Ioannis Korkontzelos, Francesca
Falucchi, and Suresh Manandhar. 2010. Estimat-
ing linear models for compositional distributional
semantics. In Proceedings of COLING, pages 1263?
1271, Beijing, China.
99
Proceedings of the Third Joint Conference on Lexical and Computational Semantics (*SEM 2014), pages 93?98,
Dublin, Ireland, August 23-24 2014.
Compositional Distributional Semantics Models
in Chunk-based Smoothed Tree Kernels
Nghia The Pham
University of Trento
thenghia.pham@unitn.it
Lorenzo Ferrone
University of Rome ?Tor Vergata?
lorenzo.ferrone@gmail.com
Fabio Massimo Zanzotto
University of Rome ?Tor Vergata?
fabio.massimo.zanzotto@uniroma2.it
Abstract
The field of compositional distributional
semantics has proposed very interesting
and reliable models for accounting the
distributional meaning of simple phrases.
These models however tend to disregard
the syntactic structures when they are ap-
plied to larger sentences. In this paper we
propose the chunk-based smoothed tree
kernels (CSTKs) as a way to exploit the
syntactic structures as well as the reliabil-
ity of these compositional models for sim-
ple phrases. We experiment with the rec-
ognizing textual entailment datasets. Our
experiments show that our CSTKs per-
form better than basic compositional dis-
tributional semantic models (CDSMs) re-
cursively applied at the sentence level, and
also better than syntactic tree kernels.
1 Introduction
A clear interaction between syntactic and semantic
interpretations for sentences is important for many
high-level NLP tasks, such as question-answering,
textual entailment recognition, and semantic tex-
tual similarity. Systems and models for these tasks
often use classifiers or regressors that exploit con-
volution kernels (Haussler, 1999) to model both
interpretations.
Convolution kernels are naturally defined on
spaces where there exists a similarity function be-
tween terminal nodes. This feature has been used
to integrate distributional semantics within tree
kernels. This class of kernels is often referred to as
smoothed tree kernels (Mehdad et al., 2010; Croce
et al., 2011), yet, these models only use distribu-
tional vectors for words.
Compositional distributional semantics models
(CDSMs) on the other hand are functions map-
ping text fragments to vectors (or higher-order ten-
sors) which then provide a distributional meaning
for simple phrases or sentences. Many CDSMs
have been proposed for simple phrases like non-
recursive noun phrases or verbal phrases (Mitchell
and Lapata, 2008; Baroni and Zamparelli, 2010;
Clark et al., 2008; Grefenstette and Sadrzadeh,
2011; Zanzotto et al., 2010). Non-recursive
phrases are often referred to as chunks (Abney,
1996), and thus, CDSMs are good and reliable
models for chunks.
In this paper, we present the chunk-based
smoothed tree kernels (CSTK) as a way to merge
the two approaches: the smoothed tree kernels
and the models for compositional distributional se-
mantics. Our approach overcomes the limitation
of the smoothed tree kernels which only use vec-
tors for words by exploiting reliable CDSMs over
chunks. CSTKs are defined over a chunk-based
syntactic subtrees where terminal nodes are words
or word sequences. We experimented with CSTKs
on data from the recognizing textual entailment
challenge (Dagan et al., 2006) and we compared
our CSTKs with other standard tree kernels and
standard recursive CDSMs. Experiments show
that our CSTKs perform better than basic compo-
sitional distributional semantic models (CDSMs)
recursively applied at the sentence level and better
than syntactic tree kernels.
The rest of the paper is organized as follows.
Section 2 describes the CSTKs. Section 3 re-
ports on the experimental setting and on the re-
sults. Finally, Section 4 draws the conclusions and
sketches the future work.
2 Chunk-based Smoothed Tree Kernels
This section describes the new class of kernels.
We first introduce the notion of the chunk-based
syntactic subtree. Then, we describe the recursive
formulation of the class of kernels. Finally, we in-
troduce the basic CDSMs we use and we introduce
two instances of the class of kernels.
93
2.1 Notation and preliminaries
S
h
h
h
h
h
(
(
(
(
(
NP
X
X
X



DT
the:d
NN
rock:n
NN
band:n
VP
X
X
X



VBZ
holds:v
NP
X
X
X
X




PRP
its:p
JJ
final:j
NN
concert:n
Figure 1: Sample Syntactic Tree
A Chunk-based Syntactic Sub-Tree is a subtree
of a syntactic tree where each non-terminal node
dominating a contiguous word sequence is col-
lapsed into a chunk and, as usual in chunks (Ab-
ney, 1996), the internal structure is disregarded.
For example, Figure 2 reports some chunk-based
syntactic subtrees of the tree in Figure 1. Chunks
are represented with a pre-terminal node dominat-
ing a triangle that covers a word sequence. The
first subtree represents the chunk covering the sec-
ond NP and the node dominates the word sequence
its:d final:n concert:n. The second subtree repre-
sents the structure of the whole sentence and one
chunk, that is the first NP dominating the word
sequence the:d rock:n band:n. The third subtree
again represents the structure of the whole sen-
tence split into two chunks without the verb.
NP
`
`
`
`
 
 
 
 
its:p final:j concert:n
S
X
X
X



NP
X
X
X
X




the:d rock:n band:n
VP
ZProceedings of the Workshop on Continuous Vector Space Models and their Compositionality, pages 50?58,
Sofia, Bulgaria, August 9 2013. c?2013 Association for Computational Linguistics
General estimation and evaluation
of compositional distributional semantic models
Georgiana Dinu and Nghia The Pham and Marco Baroni
Center for Mind/Brain Sciences (University of Trento, Italy)
(georgiana.dinu|thenghia.pham|marco.baroni)@unitn.it
Abstract
In recent years, there has been widespread
interest in compositional distributional
semantic models (cDSMs), that derive
meaning representations for phrases from
their parts. We present an evaluation of al-
ternative cDSMs under truly comparable
conditions. In particular, we extend the
idea of Baroni and Zamparelli (2010) and
Guevara (2010) to use corpus-extracted
examples of the target phrases for param-
eter estimation to the other models pro-
posed in the literature, so that all models
can be tested under the same training con-
ditions. The linguistically motivated func-
tional model of Baroni and Zamparelli
(2010) and Coecke et al (2010) emerges
as the winner in all our tests.
1 Introduction
The need to assess similarity in meaning is cen-
tral to many language technology applications,
and distributional methods are the most robust ap-
proach to the task. These methods measure word
similarity based on patterns of occurrence in large
corpora, following the intuition that similar words
occur in similar contexts. More precisely, vector
space models, the most widely used distributional
models, represent words as high-dimensional vec-
tors, where the dimensions represent (functions
of) context features, such as co-occurring context
words. The relatedness of two words is assessed
by comparing their vector representations.
The question of assessing meaning similarity
above the word level within the distributional
paradigm has received a lot of attention in re-
cent years. A number of compositional frame-
works have been proposed in the literature, each
of these defining operations to combine word vec-
tors into representations for phrases or even en-
tire sentences. These range from simple but ro-
bust methods such as vector addition to more ad-
vanced methods, such as learning function words
as tensors and composing constituents through in-
ner product operations. Empirical evaluations in
which alternative methods are tested in compara-
ble settings are thus called for. This is compli-
cated by the fact that the proposed compositional
frameworks package together a number of choices
that are conceptually distinct, but difficult to disen-
tangle. Broadly, these concern (i) the input repre-
sentations fed to composition; (ii) the composition
operation proper; (iii) the method to estimate the
parameters of the composition operation.
For example, Mitchell and Lapata in their clas-
sic 2010 study propose a set of composition op-
erations (multiplicative, additive, etc.), but they
also experiment with two different kinds of input
representations (vectors recording co-occurrence
with words vs. distributions over latent topics) and
use supervised training via a grid search over pa-
rameter settings to estimate their models. Gue-
vara (2010), to give just one further example, is
not only proposing a different composition method
with respect to Mitchell and Lapata, but he is
also adopting different input vectors (word co-
occurrences compressed via SVD) and an unsu-
pervised estimation method based on minimizing
the distance of composed vectors to their equiva-
lents directly extracted from the source corpus.
Blacoe and Lapata (2012) have recently high-
lighted the importance of teasing apart the differ-
ent aspects of a composition framework, present-
ing an evaluation in which different input vector
representations are crossed with different compo-
sition methods. However, two out of three com-
position methods they evaluate are parameter-free,
so that they can side-step the issue of fixing the pa-
rameter estimation method.
In this work, we evaluate all composition meth-
ods we know of, excluding a few that lag be-
50
hind the state of the art or are special cases of
those we consider, while keeping the estimation
method constant. This evaluation is made pos-
sible by our extension to all target composition
models of the corpus-extracted phrase approxima-
tion method originally proposed in ad-hoc settings
by Baroni and Zamparelli (2010) and Guevara
(2010). For the models for which it is feasible,
we compare the phrase approximation approach
to supervised estimation with crossvalidation, and
show that phrase approximation is competitive,
thus confirming that we are not comparing mod-
els under poor training conditions. Our tests are
conducted over three tasks that involve different
syntactic constructions and evaluation setups. Fi-
nally, we consider a range of parameter settings for
the input vector representations, to insure that our
results are not too brittle or parameter-dependent.1
2 Composition frameworks
Distributional semantic models (DSMs) approxi-
mate word meanings with vectors recording their
patterns of co-occurrence with corpus contexts
(e.g., other words). There is an extensive literature
on how to develop such models and on their eval-
uation (see, e.g., Clark (2012), Erk (2012), Tur-
ney and Pantel (2010)). We focus here on compo-
sitional DSMs (cDSMs). After discussing some
options pertaining to the input vectors, we review
all the composition operations we are aware of
(excluding only the tensor-product-based models
shown by Mitchell and Lapata (2010) to be much
worse than simpler models),2 and then methods to
estimate their parameters.
Input vectors Different studies have assumed
different distributional inputs to composition.
These include bag-of-words co-occurrence vec-
tors, possibly mapped to lower dimensionality
with SVD or other techniques (Mitchell and La-
pata (2010) and many others), vectors whose di-
1We made the software we used to construct seman-
tic models and estimate and test composition methods
available online at http://clic.cimec.unitn.it/
composes/toolkit/
2Erk and Pado? (2008) and Thater et al (2010) use in-
put vectors that have been adapted to their phrasal contexts,
but then apply straightforward composition operations such
as addition and multiplication to these contextualized vec-
tors. Their approaches are thus not alternative cDSMs, but
special ways to construct the input vectors. Grefenstette and
Sadrzadeh (2011a; 2011b) and Kartsaklis et al (2012) pro-
pose estimation techniques for the tensors in the functional
model of Coecke et al (2010). Turney (2012) does not com-
pose representations but similarity scores.
Model Composition function Parameters
Add w1~u + w2~v w1, w2
Mult ~uw1  ~vw2 w1, w2
Dil ||~u||22~v + (? ? 1)?~u,~v?~u ?
Fulladd W1~u + W2~v W1,W2 ? Rm?m
Lexfunc Au~v Au ? Rm?m
Fulllex tanh([W1,W2]
h
Au~v
Av~u
i
) W1,W2,
Au, Av ? Rm?m
Table 1: Composition functions of inputs (u, v).
mensions record the syntactic link between targets
and collocates (Erk and Pado?, 2008; Thater et al,
2010), and most recently vectors based on neural
language models (Socher et al, 2011; Socher et
al., 2012). Blacoe and Lapata (2012) compared
the three representations on phrase similarity and
paraphrase detection, concluding that ?simple is
best?, that is, the bag-of-words approach performs
at least as good or better than either syntax-based
or neural representations across the board. Here,
we take their message home and we focus on bag-
of-words representations, exploring the impact of
various parameters within this approach.
Most frameworks assume that word vectors
constitute rigid inputs fixed before composition,
often using a separate word-similarity task inde-
pendent of composition. The only exception is
Socher et al (2012), where the values in the in-
put vectors are re-estimated during composition
parameter optimization. Our re-implementation of
their method assumes rigid input vectors instead.
Composition operations Mitchell and Lapata
(2008; 2010) present a set of simple but effec-
tive models in which each component of the output
vector is a function of the corresponding compo-
nents of the inputs. Given input vectors ~u and ~v,
the weighted additive model (Add) returns their
weighted sum: ~p = w1~u + w2~v. In the dilation
model (Dil), the output vector is obtained by de-
composing one of the input vectors, say ~v, into
a vector parallel to ~u and an orthogonal vector,
and then dilating only the parallel vector by a fac-
tor ? before re-combining (formula in Table 1).
Mitchell and Lapata also propose a simple mul-
tiplicative model in which the output components
are obtained by component-wise multiplication of
the corresponding input components. We intro-
duce here its natural weighted extension (Mult),
that takes w1 and w2 powers of the components
before multiplying, such that each phrase compo-
nent pi is given by: pi = u
w1
i v
w2
i .
51
Guevara (2010) and Zanzotto et al (2010) ex-
plore a full form of the additive model (Fulladd),
where the two vectors entering a composition pro-
cess are pre-multiplied by weight matrices before
being added, so that each output component is
a weighted sum of all input components: ~p =
W1~u + W2~v.
Baroni and Zamparelli (2010) and Coecke et
al. (2010), taking inspiration from formal seman-
tics, characterize composition as function applica-
tion. For example, Baroni and Zamparelli model
adjective-noun phrases by treating the adjective
as a function from nouns onto (modified) nouns.
Given that linear functions can be expressed by
matrices and their application by matrix-by-vector
multiplication, a functor (such as the adjective) is
represented by a matrix Au to be composed with
the argument vector ~v (e.g., the noun) by multi-
plication, returning the lexical function (Lexfunc)
representation of the phrase: ~p = Au~v.
The method proposed by Socher et al (2012)
(see Socher et al (2011) for an earlier proposal
from the same team) can be seen as a combination
and non-linear extension of Fulladd and Lexfunc
(that we thus call Fulllex) in which both phrase
elements act as functors (matrices) and arguments
(vectors). Given input terms u and v represented
by (~u,Au) and (~v,Av), respectively, their com-
position vector is obtained by applying first a lin-
ear transformation and then the hyperbolic tangent
function to the concatenation of the products Au~v
and Av~u (see Table 1 for the equation). Socher
and colleagues also present a way to construct ma-
trix representations for specific phrases, needed
to scale this composition method to larger con-
stituents. We ignore it here since we focus on the
two-word case.
Estimating composition parameters If we
have manually labeled example data for a target
task, we can use supervised machine learning to
optimize parameters. Mitchell and Lapata (2008;
2010), since their models have just a few param-
eters to optimize, use a direct grid search for the
parameter setting that performs best on the train-
ing data. Socher et al (2012) train their models
using multinomial softmax classifiers.
If our goal is to develop a cDSM optimized for
a specific task, supervised methods are undoubt-
edly the most promising approach. However, ev-
ery time we face a new task, parameters must be
re-estimated from scratch, which goes against the
idea of distributional semantics as a general sim-
ilarity resource (Baroni and Lenci, 2010). More-
over, supervised methods are highly composition-
model-dependent, and for models such as Fulladd
and Lexfunc we are not aware of proposals about
how to estimate them in a supervised manner.
Socher et al (2011) propose an autoencoding
strategy. Given a decomposition function that re-
constructs the constituent vectors from a phrase
vector (e.g., it re-generates green and jacket vec-
tors from the composed green jacket vector), the
composition parameters minimize the distance be-
tween the original and reconstructed input vectors.
This method does not require hand-labeled train-
ing data, but it is restricted to cDSMs for which
an appropriate decomposition function can be de-
fined, and even in this case the learning problem
might lack a closed-form solution.
Guevara (2010) and Baroni and Zamparelli
(2010) optimize parameters using examples of
how the output vectors should look like that are
directly extracted from the corpus. To learn, say, a
Lexfunc matrix representing the adjective green,
we extract from the corpus example vectors of
?N, green N? pairs that occur with sufficient fre-
quency (?car, green car?, ?jacket, green jacket?,
?politician, green politician?, . . . ). We then use
least-squares methods to find weights for the green
matrix that minimize the distance between the
green N vectors generated by the model given the
input N and the corresponding corpus-observed
phrase vectors. This is a very general approach, it
does not require hand-labeled data, and it has the
nice property that corpus-harvested phrase vec-
tors provide direct evidence of the polysemous be-
haviour of functors (the green jacket vs. politician
contexts, for example, will be very different). In
the next section, we extend the corpus-extracted
phrase approximation method to all cDSMs de-
scribed above, with closed-form solutions for all
but the Fulllex model, for which we propose a
rapidly converging iterative estimation method.
3 Least-squares model estimation using
corpus-extracted phrase vectors3
Notation Given two matricesX,Y ? Rm?n we
denote their inner product by ?X,Y ?, (?X,Y ? =
?m
i=1
?n
j=1 xijyij). Similarly we denote by
?u, v? the dot product of two vectors u, v ? Rm?1
and by ||u|| the Euclidean norm of a vector:
3Proofs omitted due to space constraints.
52
||u|| = ?u, u?1/2. We use the following Frobe-
nius norm notation: ||X||F = ?X,X?
1/2. Vectors
are assumed to be column vectors and we use xi
to stand for the i-th (m ? 1)-dimensional column
of matrix X . We use [X,Y ] ? Rm?2n to denote
the horizontal concatenation of two matrices while[
X
Y
]
? R2m?n is their vertical concatenation.
General problem statement We assume vocab-
ularies of constituents U , V and that of resulting
phrases P . The training data consist of a set of
tuples (u, v, p) where p stands for the phrase asso-
ciated to the constituents u and v:
T = {(ui, vi, pi)|(ui, vi, pi) ? U?V?P, 1 ? i ? k}
We build the matrices U, V, P ? Rm?k by con-
catenating the vectors associated to the training
data elements as columns.4
Given the training data matrices, the general
problem can be stated as:
?? = arg min
?
||P ? fcomp?(U, V )||F
where fcomp? is a composition function and ?
stands for a list of parameters that this composition
function is associated to. The composition func-
tions are defined: fcomp? : Rm?1 ? Rm?1 ?
Rm?1 and fcomp?(U, V ) stands for their natural
extension when applied on the individual columns
of the U and V matrices.
Add The weighted additive model returns the
sum of the composing vectors which have been
re-weighted by some scalars w1 and w2: ~p =
w1~u + w2~v. The problem becomes:
w?1, w
?
2 = arg min
w1,w2?R
||P ? w1U ? w2V ||F
The optimal w1 and w2 are given by:
w?1 =
||V ||2F ?U,P ? ? ?U, V ??V, P ?
||U ||2F ||V ||
2
F ? ?U, V ?
2
(1)
w?2 =
||U ||2F ?V, P ? ? ?U, V ??U,P ?
||U ||2F ||V ||
2
F ? ?U, V ?
2
(2)
4In reality, not all composition models require u, v and p
to have the same dimensionality.
Dil Given two vectors ~u and ~v, the dilation
model computes the phrase vector ~p = ||~u||2~v +
(? ? 1)?~u,~v?~u where the parameter ? is a scalar.
The problem becomes:
?? = arg min
??R
||P ?V D||ui||2 ?UD(??1)?ui,vi?||F
where by D||ui||2 and D(??1)?ui,vi? we denote
diagonal matrices with diagonal elements (i, i)
given by ||ui||2 and (? ? 1)?ui, vi? respectively.
The solution is:
?? = 1?
?k
i=1?ui, (||ui||
2vi ? pi)??ui, vi?
?k
i=1?ui, vi?
2||ui||2
Mult Given two vectors ~u and ~v, the weighted
multiplicative model computes the phrase vector
~p = ~uw1  ~vw2 where  stands for component-
wise multiplication. We assume for this model that
U, V, P ? Rm?n++ , i.e. that the entries are strictly
larger than 0: in practice we add a small smooth-
ing constant to all elements to achieve this (Mult
performs badly on negative entries, such as those
produced by SVD). We use the w1 and w2 weights
obtained when solving the much simpler related
problem:5
w?1, w
?
2 = arg min
w1,w2?R
||log(P )?log(U.?w1V.
?w2)||F
where .? stands for the component-wise power op-
eration. The solution is the same as that for Add,
given in equations (1) and (2), with U ? log(U),
V ? log(V ) and P ? log(P ).
Fulladd The full additive model assumes the
composition of two vectors to be ~p = W1~u+W2~v
where W1,W2 ? Rm?m. The problem is:
[W1,W2]
? = arg min
[W1,W2]?Rm?2m
||P?[W1W2]
[
U
V
]
||
This is a multivariate linear regression prob-
lem (Hastie et al, 2009) for which the least
squares estimate is given by: [W1,W2] =
((XTX)?1XTY )T where we use X = [UT , V T ]
and Y = P T .
Lexfunc The lexical function composition
method learns a matrix representation for each
functor (given by U here) and defines composition
as matrix-vector multiplication. More precisely:
5In practice training Mult this way achieves similar or
lower errors in comparison to Add.
53
~p = Au~v where Au is a matrix associated to each
functor u ? U . We denote by Tu the training
data subset associated to an element u, which
contains only tuples which have u as first element.
Learning the matrix representations amounts to
solving the set of problems:
Au = arg min
Au?Rm?m
||Pu ?AuVu||
for each u ? U where Pu, Vu ? Rm?|Tu|
are the matrices corresponding to the Tu train-
ing subset. The solutions are given by: Au =
((VuV Tu )
?1VuP Tu )
T . This composition function
does not use the functor vectors.
Fulllex This model can be seen as a generaliza-
tion of Lexfunc which makes no assumption on
which of the constituents is a functor, so that both
words get a matrix and a vector representation.
The composition function is:
~p = tanh([W1,W2]
[
Au~v
Av~u
]
)
where Au and Av are the matrices associated to
constituents u and v and [W1,W2] ? Rm?2m.
The estimation problem is given in Figure 1.
This is the only composition model which does
not have a closed-form solution. We use a block
coordinate descent method, in which we fix each
of the matrix variables but one and solve the corre-
sponding least-squares linear regression problem,
for which we can use the closed-form solution.
Fixing everything but [W1,W2]:
[W ?1 ,W
?
2 ] = ((X
TX)?1XTY )T
X =
[
[Au1 ~v1, ..., Auk ~vk]
[Av1 ~u1, ..., Avk ~uk]
]T
Y = atanh(P T )
Fixing everything but Au for some element u,
the objective function becomes:
||atanh(Pu)?W1AuVu?W2[Av1~u, ..., Avk?~u]||F
where v1...vk? ? V are the elements occurring
with u in the training data and Vu the matrix result-
ing from their concatenation. The update formula
for the Au matrices becomes:
A?u = W
?1
1 ((X
TX)?1XTY )T
X = V Tu
Y = (atanh(Pu)?W2[Av1~u, ..., Avk?~u])
T
In all our experiments, Fulllex estimation con-
verges after very few passes though the matrices.
Despite the very large number of parameters of
this model, when evaluating on the test data we ob-
serve that using a higher dimensional space (such
as 200 dimensions) still performs better than a
lower dimensional one (e.g., 50 dimensions).
4 Evaluation setup and implementation
4.1 Datasets
We evaluate the composition methods on three
phrase-based benchmarks that test the models on
a variety of composition processes and similarity-
based tasks.
Intransitive sentences The first dataset, intro-
duced by Mitchell and Lapata (2008), focuses on
simple sentences consisting of intransitive verbs
and their noun subjects. It contains a total of
120 sentence pairs together with human similar-
ity judgments on a 7-point scale. For exam-
ple, conflict erupts/conflict bursts is scored 7, skin
glows/skin burns is scored 1. On average, each
pair is rated by 30 participants. Rather than eval-
uating against mean scores, we use each rating as
a separate data point, as done by Mitchell and La-
pata. We report Spearman correlations between
human-assigned scores and model cosine scores.
Adjective-noun phrases Turney (2012) intro-
duced a dataset including both noun-noun com-
pounds and adjective-noun phrases (ANs). We
focus on the latter, and we frame the task dif-
ferently from Turney?s original definition due to
data sparsity issues.6 In our version, the dataset
contains 620 ANs, each paired with a single-
noun paraphrase. Examples include: archaeolog-
ical site/dig, spousal relationship/marriage and
dangerous undertaking/adventure. We evaluate a
model by computing the cosine of all 20K nouns in
our semantic space with the target AN, and look-
ing at the rank of the correct paraphrase in this list.
The lower the rank, the better the model. We re-
port median rank across the test items.
Determiner phrases The last dataset, intro-
duced in Bernardi et al (2013), focuses on a
class of grammatical terms (rather than content
6Turney used a corpus of about 50 billion words, almost
20 times larger than ours, and we have very poor or no cov-
erage of many original items, making the ?multiple-choice?
evaluation proposed by Turney meaningless in our case.
54
W ?1 ,W
?
2 , A
?
u1 , ..., A
?
v1 , ... =arg min
Rm?m
||atanh(P T )? [W1,W2]
[
[Au1 ~v1, ..., Auk ~vk]
[Av1 ~u1, ..., Avk ~uk]
]
||F
=arg min
Rm?m
||atanh(P T )?W1[Au1 ~v1, ..., Auk ~vk]?W2[Av1 ~u1, ..., Avk ~uk]||F
Figure 1: Fulllex estimation problem.
words), namely determiners. It is a multiple-
choice test where target nouns (e.g., amnesia)
must be matched with the most closely related
determiner(-noun) phrases (DPs) (e.g., no mem-
ory). The task differs from the previous one also
because here the targets are single words, and the
related items are composite. There are 173 tar-
get nouns in total, each paired with one correct
DP response, as well as 5 foils, namely the de-
terminer (no) and noun (memory) from the correct
response and three more DPs, two of which con-
tain the same noun as the correct phrase (less mem-
ory, all memory), the third the same determiner
(no repertoire). Other examples of targets/related-
phrases are polysemy/several senses and tril-
ogy/three books. The models compute cosines be-
tween target noun and responses and are scored
based on their accuracy at ranking the correct
phrase first.
4.2 Input vectors
We extracted distributional semantic vectors us-
ing as source corpus the concatenation of ukWaC,
Wikipedia (2009 dump) and BNC, 2.8 billion to-
kens in total.7 We use a bag-of-words approach
and we count co-occurrences within sentences and
with a limit of maximally 50 words surrounding
the target word. By tuning on the MEN lexical
relatedness dataset,8 we decided to use the top
10K most frequent content lemmas as context fea-
tures (vs. top 10K inflected forms), and we experi-
mented with positive Pointwise and Local Mutual
Information (Evert, 2005) as association measures
(vs. raw counts, log transform and a probability
ratio measure) and dimensionality reduction by
Non-negative Matrix Factorization (NMF, Lee and
Seung (2000)) and Singular Value Decomposition
(SVD, Golub and Van Loan (1996)) (both outper-
forming full dimensionality vectors on MEN). For
7http://wacky.sslmit.unibo.it;
http://www.natcorp.ox.ac.uk
8http://clic.cimec.unitn.it/?elia.
bruni/MEN
both reduction techniques, we varied the number
of dimensions to be preserved from 50 to 300 in
50-unit intervals. As Local Mutual Information
performed very poorly across composition exper-
iments and other parameter choices, we dropped
it. We will thus report, for each experiment and
composition method, the distribution of the rele-
vant performance measure across 12 input settings
(NMF vs. SVD times 6 dimensionalities). How-
ever, since the Mult model, as expected, worked
very poorly when the input vectors contained neg-
ative values, as is the case with SVD, for this
model we report result distributions across the 6
NMF variations only.
4.3 Composition model estimation
Training by approximating the corpus-extracted
phrase vectors requires corpus-based examples of
input (constituent word) and output (phrase) vec-
tors for the composition processes to be learned.
In all cases, training examples are simply selected
based on corpus frequency. For the first experi-
ment, we have 42 distinct target verbs and a total
of ?20K training instances, that is, ??noun, verb?,
noun-verb? tuples (505 per verb on average). For
the second experiment, we have 479 adjectives and
?1 million ??adjective, noun?, adjective-noun?
training tuples (2K per adjective on average). In
the third, 50 determiners and 50K ??determiner,
noun?, determiner-noun? tuples (1K per deter-
miner). For all models except Lexfunc and Ful-
llex, training examples are pooled across target el-
ements to learn a single set of parameters. The
Lexfunc model takes only argument word vectors
as inputs (the functors in the three datasets are
verbs, adjectives and determiners, respectively). A
separate weight matrix is learned for each func-
tor, using the corresponding training data.9 The
Fulllex method jointly learns distinct matrix rep-
resentations for both left- and right-hand side con-
9For the Lexfunc model we have experimented with least
squeares regression with and without regularization, obtain-
ing similar results.
55
stituents. For this reason, we must train this model
on balanced datasets. More precisely, for the in-
transitive verb experiments, we use training data
containing noun-verb phrases in which the verbs
and the nouns are present in the lists of 1,500
most frequent verbs/nouns respectively, adding to
these the verbs and nouns present in our dataset.
We obtain 400K training tuples. We create the
training data similarity for the other datasets ob-
taining 440K adjective-noun and 50K determiner
phrase training tuples, respectively (we also exper-
imented with Fulllex trained on the same tuples
used for the other models, obtaining considerably
worse results than those reported). Finally, for Dil
we treat direction of stretching as a further param-
eter to be optimized, and find that for intransitives
it is better to stretch verbs, in the other datasets
nouns.
For the simple composition models for which
parameters consist of one or two scalars, namely
Add, Mult and Dil, we also tune the parame-
ters through 5-fold crossvalidation on the datasets,
directly optimizing the parameters on the target
tasks. For Add and Mult, we search w1, w2
through the crossproduct of the interval [0 : 5] in
0.2-sized steps. For Dil we use ? ? [0 : 20], again
in 0.2-sized steps.
5 Evaluation results
We begin with some remarks pertaining to the
overall quality of and motivation for corpus-
phrase-based estimation. In seven out of nine
comparisons of this unsupervised technique with
fully supervised crossvalidation (3 ?simple? mod-
els ?Add, Dil and Mult? times 3 test sets), there
was no significant difference between the two esti-
mation methods.10 Supervised estimation outper-
formed the corpus-phrase-based method only for
Dil on the intransitive sentence and AN bench-
marks, but crossvalidated Dil was outperformed
by at least one phrase-estimated simple model on
both benchmarks.
The rightmost boxes in the panels of Fig-
ure 2 depict the performance distribution for us-
ing phrase vectors directly extracted from the
corpus to tackle the various tasks. This non-
compositional approach outperforms all composi-
tional methods in two tasks over three, and it is
one of the best approaches in the third, although
10Significance assessed through Tukey Honestly Signifi-
cant Difference tests (Abdi and Williams, 2010), ? = 0.05.
in all cases even its top scores are far from the
theoretical ceiling. Still, performance is impres-
sive, especially in light of the fact that the non-
compositional approach suffers of serious data-
sparseness problems. Performance on the intran-
sitive task is above state-of-the-art despite the fact
that for almost half of the cases one test phrase
is not in the corpus, resulting in 0 vectors and
consequently 0 similarity pairs. The other bench-
marks have better corpus-phrase coverage (nearly
perfect AN coverage; for DPs, about 90% correct
phrase responses are in the corpus), but many tar-
get phrases occur only rarely, leading to unreliable
distributional vectors. We interpret these results as
a goodmotivation for corpus-phrase-based estima-
tion. On the one hand they show how good these
vectors are, and thus that they are sensible targets
of learning. On the other hand, they do not suffice,
since natural language is infinitely productive and
thus no corpus can provide full phrase coverage,
justifying the whole compositional enterprise.
The other boxes in Figure 2 report the perfor-
mance of the composition methods trained by cor-
pus phrase approximation. Nearly all models are
significantly above chance in all tasks, except for
Fulladd on intransitive sentences. To put AN me-
dian ranks into perspective, consider that a median
rank as high as 8,300 has near-0 probability to oc-
cur by chance. For DP accuracy, random guessing
gets 0.17% accuracy.
Lexfunc emerges consistently as the best model.
On intransitive constructions, it significantly out-
performs all other models except Mult, but the dif-
ference approaches significance even with respect
to the latter (p = 0.071). On this task, Lexfunc?s
median correlation (0.26) is nearly equivalent to
the best correlation across a wide range of parame-
ters reported by Erk and Pado? (2008) (0.27). In the
AN task, Lexfunc significantly outperforms Ful-
llex and Dil and, visually, its distribution is slightly
more skewed towards lower (better) ranks than any
other model. In the DP task, Lexfunc significantly
outperforms Add and Mult and, visually, most of
its distribution lies above that of the other mod-
els. Most importantly, Lexfunc is the only model
that is consistent across the three tasks, with all
other models displaying instead a brittle perfor-
mance pattern.11
Still, the top-performance range of all models
11No systematic trend emerged pertaining to the input vec-
tor parameters (SVD vs. NMF and retained dimension num-
ber).
56
Add Dil Mult Fulla
dd
Lexfu
nc Fullle
x
Corp
us
0.00
0.05
0.10
0.15
0.20
0.25
0.30 Intransitive sentences
l
l
l
l
l
Add Dil Mult Fulla
dd
Lexfu
nc Fullle
x
Corp
us1
000
800
600
400
200
ANs
l
l
Add Dil Mult Fulla
dd
Lexfu
nc Fullle
x
Corp
us
0.15
0.20
0.25
0.30
0.35
DPs
Figure 2: Boxplots displaying composition model performance distribution on three benchmarks, across
input vector settings (6 datapoints for Mult, 12 for all other models). For intransitive sentences, figure of
merit is Spearman correlation, for ANs median rank of correct paraphrase, and for DPs correct response
accuracy. The boxplots display the distribution median as a thick horizontal line within a box extending
from first to third quartile. Whiskers cover 1.5 of interquartile range in each direction from the box, and
extreme outliers outside this extended range are plotted as circles.
on the three tasks is underwhelming, and none of
them succeeds in exploiting compositionality to
do significantly better than using whatever phrase
vectors can be extracted from the corpus directly.
Clearly, much work is still needed to develop truly
successful cDSMs.
The AN results might look particularly worry-
ing, considering that even the top (lowest) median
ranks are above 100. A qualitative analysis, how-
ever, suggests that the actual performance is not
as bad as the numerical scores suggest, since of-
ten the nearest neighbours of the ANs to be para-
phrased are nouns that are as strongly related to
the ANs as the gold standard response (although
not necessarily proper paraphrases). For example,
the gold response to colorimetric analysis is col-
orimetry, whereas the Lexfunc (NMF, 300 dimen-
sions) nearest neighbour is chromatography; the
gold response to heavy particle is baryon, whereas
Lexfunc proposes muon; for melodic phrase the
gold is tune and Lexfunc has appoggiatura; for in-
door garden, the gold is hothouse but Lexfunc pro-
poses glasshouse (followed by the more sophisti-
cated orangery!), and so on and so forth.
6 Conclusion
We extended the unsupervised corpus-extracted
phrase approximation method of Guevara (2010)
and Baroni and Zamparelli (2010) to estimate
all known state-of-the-art cDSMs, using closed-
form solutions or simple iterative procedures in
all cases. Equipped with a general estimation ap-
proach, we thoroughly evaluated the cDSMs in
a comparable setting. The linguistically moti-
vated Lexfunc model of Baroni and Zamparelli
(2010) and Coecke et al (2010) was the win-
ner across three composition tasks, also outper-
forming the more complex Fulllex model, our re-
implementation of Socher et al?s (2012) compo-
sition method (of course, the composition method
is only one aspect of Socher et al?s architecture).
All other composition methods behaved inconsis-
tently.
In the near future, we want to focus on improv-
ing estimation itself. In particular, we want to
explore ways to automatically select good phrase
examples for training, beyond simple frequency
thresholds. We tested composition methods on
two-word phrase benchmarks. Another natural
next step is to apply the composition rules recur-
sively, to obtain representations of larger chunks,
up to full sentences, coming, in this way, nearer to
the ultimate goal of compositional distributional
semantics.
Acknowledgments
We acknowledge ERC 2011 Starting Independent
Research Grant n. 283554 (COMPOSES).
57
References
Herve? Abdi and Lynne Williams. 2010. Newman-
Keuls and Tukey test. In Neil Salkind, Bruce Frey,
and Dondald Dougherty, editors, Encyclopedia of
Research Design, pages 897?904. Sage, Thousand
Oaks, CA.
Marco Baroni and Alessandro Lenci. 2010. Dis-
tributional Memory: A general framework for
corpus-based semantics. Computational Linguis-
tics, 36(4):673?721.
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Proceedings of EMNLP, pages 1183?1193, Boston,
MA.
Raffaella Bernardi, Georgiana Dinu, Marco Marelli,
and Marco Baroni. 2013. A relatedness benchmark
to test the role of determiners in compositional dis-
tributional semantics. In Proceedings of ACL (Short
Papers), Sofia, Bulgaria. In press.
William Blacoe and Mirella Lapata. 2012. A com-
parison of vector-based representations for seman-
tic composition. In Proceedings of EMNLP, pages
546?556, Jeju Island, Korea.
Stephen Clark. 2012. Vector space models of lexical
meaning. In Shalom Lappin and Chris Fox, editors,
Handbook of Contemporary Semantics, 2nd edition.
Blackwell, Malden, MA. In press.
Bob Coecke, Mehrnoosh Sadrzadeh, and Stephen
Clark. 2010. Mathematical foundations for a com-
positional distributional model of meaning. Linguis-
tic Analysis, 36:345?384.
Katrin Erk and Sebastian Pado?. 2008. A structured
vector space model for word meaning in context. In
Proceedings of EMNLP, pages 897?906, Honolulu,
HI.
Katrin Erk. 2012. Vector space models of word mean-
ing and phrase meaning: A survey. Language and
Linguistics Compass, 6(10):635?653.
Stefan Evert. 2005. The Statistics of Word Cooccur-
rences. Dissertation, Stuttgart University.
Gene Golub and Charles Van Loan. 1996. Matrix
Computations (3rd ed.). JHU Press, Baltimore, MD.
Edward Grefenstette and Mehrnoosh Sadrzadeh.
2011a. Experimental support for a categorical com-
positional distributional model of meaning. In Pro-
ceedings of EMNLP, pages 1394?1404, Edinburgh,
UK.
Edward Grefenstette and Mehrnoosh Sadrzadeh.
2011b. Experimenting with transitive verbs in a Dis-
CoCat. In Proceedings of GEMS, pages 62?66, Ed-
inburgh, UK.
Emiliano Guevara. 2010. A regression model of
adjective-noun compositionality in distributional se-
mantics. In Proceedings of GEMS, pages 33?37,
Uppsala, Sweden.
Trevor Hastie, Robert Tibshirani, and Jerome Fried-
man. 2009. The Elements of Statistical Learning,
2nd ed. Springer, New York.
Dimitri Kartsaklis, Mehrnoosh Sadrzadeh, and Stephen
Pulman. 2012. A unified sentence space for
categorical distributional-compositional semantics:
Theory and experiments. In Proceedings of COL-
ING: Posters, pages 549?558, Mumbai, India.
Daniel Lee and Sebastian Seung. 2000. Algorithms for
Non-negative Matrix Factorization. In Proceedings
of NIPS, pages 556?562.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings of
ACL, pages 236?244, Columbus, OH.
Jeff Mitchell and Mirella Lapata. 2010. Composition
in distributional models of semantics. Cognitive Sci-
ence, 34(8):1388?1429.
Richard Socher, Eric Huang, Jeffrey Pennin, Andrew
Ng, and Christopher Manning. 2011. Dynamic
pooling and unfolding recursive autoencoders for
paraphrase detection. In Proceedings of NIPS, pages
801?809, Granada, Spain.
Richard Socher, Brody Huval, Christopher Manning,
and Andrew Ng. 2012. Semantic compositionality
through recursive matrix-vector spaces. In Proceed-
ings of EMNLP, pages 1201?1211, Jeju Island, Ko-
rea.
Stefan Thater, Hagen Fu?rstenau, and Manfred Pinkal.
2010. Contextualizing semantic representations us-
ing syntactically enriched vector models. In Pro-
ceedings of ACL, pages 948?957, Uppsala, Sweden.
Peter Turney and Patrick Pantel. 2010. From fre-
quency to meaning: Vector space models of se-
mantics. Journal of Artificial Intelligence Research,
37:141?188.
Peter Turney. 2012. Domain and function: A dual-
space model of semantic relations and compositions.
Journal of Artificial Intelligence Research, 44:533?
585.
Fabio Zanzotto, Ioannis Korkontzelos, Francesca
Falucchi, and Suresh Manandhar. 2010. Estimat-
ing linear models for compositional distributional
semantics. In Proceedings of COLING, pages 1263?
1271, Beijing, China.
58
