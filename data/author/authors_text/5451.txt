Automat ic  Lexical Acquisit ion Based on Statistical Distributions* 
Suzanne Stevenson  
Depar l ;ment of Coml)uter Science 
Uldvcrsity of Toronto  
6 King's College l(oa,d 
'.l.'oronto, ON Ca, lla, d~t M5S 311115 
suzanne~cs, toronto, edu 
Abstract 
We a, ui;omatically cla,ssi(y verbs into lexica,1 se- 
mantic classes, 1)ased on distributions of indices- 
tots of verb a.lterna.tions, extra.cCed froln a very 
la.rge a.nnota.ted corpus. We ~ddress a. prol)lem 
which is pa.rticuhtrly difficult 1)eca.use the verl) 
classes, a.lthough sema.ntica.lly different, show sim- 
ila.r surface syntactic 1)eha.vior, Five gra.m,na.tica.1 
fea.l;tlres ~u:e su\[\[icient to reduce error i:ate by more 
tha.n 50% over cha.nc(,: we a.chieve almost 70% 
a.ceura.cy in a 1;ask whose baseline perl'ornmn(:e is 
34%, and whose exl)ert-I)ased Ul)l)er bound we ('aJ- 
culated a.t 86.5%. We conclude l;ha.1; corl)us-driven 
exl;racl;ion of gramma.1;ical \['eaJ;ures i a. promising 
lnethodology for line-grained verb classilica.tion. 
1 I n t roduct ion  
\])eta.ileal hfforma.tion a.I)out verbs is critical to a. 
broad ra.nge of NI,I ) and l i t  1;asks, yet; ils mau 
us.1 (lel;('rmina.tion for la.rge numl)ers o\[' verl)s is 
difficult aml resource intensive. I{.esea.rch o,I tim 
a.ul,()matic, a(-quisil;ion o\[' verb-I)ased k,owl(~(Ig(, 
has succeded in gleaning sylH.a.('l;ic l)rol)erties o\[' 
verl)s such as sul)ca.tegoriza.tion frames from el> 
line resources (I}rent, \]9!)3; lh'iscoe a.nd C,a.rroll, 
1997; \])err, 1997; Ma.nning, \]993), ll,ecently, 
researchers have investiga?ed statistica.l corpus- 
ba.sed methods for lexica.l sema.ntic classitica.tion 
from synta.ctic prol)erties of verl) usage (Aone a.nd 
McKee, \]996; l,a.pa.ta, and Brew, \]999; Schulte im 
Wa.lde, :1998; Stevenson a.nd Merle. 1999; Steve.n- 
son et a l., \] 999; McCarthy, 2000). 
C, orl)us-based al)pro~mhes to lexica.l sema.ntic 
classitic~tion in pa,rticular ha.ve dra.wn on Levin's 
hypothesis (I,evin, 1993) that verbs can be classi- 
lied according to the dia.thesis aJterna.tions (a.lter- 
nations in the syntactic expressions o\[" a.rguments) 
ill which they f)articil)a.te l'or exa.mple, whether a. 
* This research was partly sponsored 1)y US NSI" grants 
#9702331 and #9818322, Swiss NSI" Mlowshlp 8210- 
d65(;9, Information Sciences (.Ollll(il of Hurters University 
and IllCS, U. of Peimsylwmia. 'l'his research was con- 
ducted wldle the tirst author was at l lutgers University. 
Pao la  Mer le  
LAT\ ] , -  \])cpa.rtment of l,inguisCics 
Univcrsil;y of ( leneva 
2 rue de C'~mdolle 
121\] Gent;re d -. Suisse 
merZo?let t res ,  un?ge, ch 
verb occurs in the dative/prepositiona.l phrase al- 
terna.tion in l';nglish. One diagnostic for dis.thesis 
a.lternations i the sulx'a.tegorization aJternatives 
of a. verb. ltowew~,r, some classes exhibit the same 
subca.tegoriza.tkm possibilities but differ in their 
a.rgument structures, i.e. tim content el' the the- 
real;it roles assigned to the arguments of l;he verb. 
rFhis gyl)e of situation consl;itutes a. pa.rticula.rly 
difficult case R)r corpus-based classification meth- 
ods. 
In this paper, we apply corpus-based lexica.l 
a.cquisition methodology 1;o distinguish classes of 
verbs which allow the same subca.tegoriza?ions, 
but differ in tlteJna.tic roles. We first a ssu me tha.t 
one ca.n a.ut;oma.tiea.lly restrict l;he choice o\[' ('lasses 
to those that; paxl;icil)a.1;e in the relewu~t subcate- 
gorizations (c\['. (l,a.p~ta. and Brew, \]999)). Our 
prOl)OSa.1 is lhen to use st.a.tistics ovel: di~Ll;hesis 
a lterna.nl,s as a, wa.y to \['urther distinguish those 
verl)s wldch allow 1;he same sul)ca.tegoriza.tions; 
achievi,g fine-grained cla.ssifica.tion within that 
S('l,. ()UI' work \['O(tllSeS oll determining tile 1)esl se- 
ma.nl, ic class for a verl) lgpc - the set of usages o1" 
a. verl) across a. document or corpus rather t;ha.n 
fl)r a single verb I, okc n in {~ single local context. 
In this way, we c~u/ exploit the broad beha.vior o1' 
the verb a.cross 1;he corl)uS to determine its most 
likely class overall. 
We investiga.te the proposed a.l)l)rOaCh in an in- 
del)th case study o\[' the three major classes of o1> 
tiona.lly inlra,nsitive w, rl)s in English: ullergative, 
unaccusa,tive, and ol)ject-drop. More specifically, 
according to l,evin's classificaJ;ion (l,evin, 1993), 
the unerga.tives are ma.nner of motion verbs, such 
as jump and march; the una.ccusa.tives are verl)s 
of cha.nge of state,, such a.s open and explode; the 
object-drop verbs a.re unexpressed object a.lCerna.- 
Lion verl)s, such as played a.nd painted. These 
classes a.ll supl)ort 1)oth tr~msitive and intra.nsi- 
1,ire sul)cal,egoriza.tions, I)ut a.re distinguished by 
the pal;tern of thema.tic role assignments i,o sub- 
jecC a.nd object position. We a.utomatica.lly cla.s- 
si(y these verbs on the basis of sta.tistical a,p- 
815 
proxilnations to syntactic indicators of the under- 
lying argunlent structures, using numerical fea- 
tures collected from a large syntactically anno- 
tated (tagged or parsed) corpus. We apply ma- 
chine learning techniques to determine whether 
the fi'equency distribntions of the features, in- 
dividually or in combination, support automatic 
classification of the verbs. To preview our re- 
sults, we demonstrate that combining only five 
numerical indicators is sufficient o reduce the er- 
ro r  rate in this classification task by more than 
50% over chance. Specifically, we achieve ahnost 
7(1% accuracy in a task whose baseline (chance) 
per\[brmance is 34%, and whose expert-based up- 
per bound is calculated at 86.5%. We conclude 
that a distribution-based method for lexical se- 
mantic verb classification is a promising avenue of 
research. 
2 The  Argument  S t ruc tures  
Our approach rests on tile hypothesis that, even in 
cases where verb classes cannot be distinguished 
by subcategorizations, the frequency distributions 
of syntactic indicators can hold clues to the under- 
lying thematic role differences. We start here then 
with a description of the subca.tegorizations and 
thematic role assignments for each of l.he three 
verb classes under investigation. 
As optionally intransitive verbs, each of 
the three classes participates in the transi- 
tive/intransitive Mternation: 
Uuergative 
(la) The horse raced past the barn. 
(1 b) The jockey raced the horse past tile barn. 
Unaccnsative 
(2a) The butter melted in the pan. 
(2b) The cook melted the butter in the pan. 
Object-drop 
(3a) The boy washed the hall. 
(3b) The boy washed. 
Unergatives are intransitive action verbs, as in (1), 
whose transitive form can be the causative coun- 
terpart of the intransitive form. In the causative 
use, the semantic argument hat appears as the 
subject of the intransitive, as in (la), surfaces 
as the object of the transitive, as in (lb) (Ilale 
and Keyser, 1993). Unaccusatives are intransitive 
change of state verbs, as in (2a); the transitive 
counterpart for these verbs exhibits the causative 
alternation, as in (2b). Object-drop verbs, as in 
(3), have a non-causative transitive/intransitive 
alternation, in which the object is simply optional. 
Subj of 
Classes Trans 
Unergative Causal Agent 
Unaccusative Causal Agent 
Object-drop Agent 
Obj of Subj of' 
rPrans Intrans 
Agent Agent 
'?heme rI'heme 
Theme Agent 
Table 1: Summary of Thematic Alternations. 
Each class is distinguished by the content of tile 
thematic roles assigned by tile verb. For object- 
drop verbs, tile subject is all Agent and the op- 
tional object is a Theme, yielding tile thematic 
assignments (Agent, Tlmme) and (Agent) for the 
transitive and intransitive alternants respectively. 
Unergatives and uuaccusatives differ \['1"o111 object- 
drop verbs in participating in the causative alter- 
nation, and also differ from each other in their core 
thematic argument. In an intransitive unerga- 
live, the subject is an Agent, and in an intran- 
sitive unaccusative, the subject is a Theme. In 
the causative transitive form of each, this core se- 
mantic argument is expressed as the direct object, 
with the addition of a Causal Agent (the causer of 
the action) as subject in bol;h cases. The thematic 
roles assigned, and their mapping to syntactic po- 
sition, are summarized in Ta.ble 1. 
3 The  features  for  C lass i f i ca t ion  
The key to any automatic lassification task is to 
determine a set of' useful fea.tures for discriminat- 
ing the itenls to be classitied. In what follows, we 
refer to the cohnnns of Table 1 to explain \]tow we 
expect the thematic distinctions to yield distri- 
butional features whose frequencies discriminate 
among the classes ~t hand. 
Considering column one of Table 1, only 
unergative and unaccusa.tive rbs assign ~ Causal 
Agent to the subject of the transitive. We hy- 
1)othesize that the causative construction is lin- 
guistically more complex than the simple argu- 
ment optionality of object-drop verbs (Stevenson 
and Merlo, 1.997). We expect then that object- 
drop verbs will be more fi:equent in the transi- 
tive than the other two classes. Furthernmre, the 
object of an unergative verb receives the Agent 
role (see the second column of Table 1.), a linguis- 
tically marked transitive construction (Stevenson 
and Merlo, 1997). We therefore xpect unerga- 
tives to be quite rare in the transitive, leading to 
a three-way distinction in transitive usage among 
the three classes. 
Second, due to the causative alternation of 
816 
'l'able 2: The l.i'ea.Cures and 'l'heir F;xpected 13ehavior 
TrmlsitiviLy Unaccusativcs and unergativcs have ~, causative transit ive, hence lower transit ive use. Fur- 
l;hc'rlnorc, unerga.tivcs ha.re a.n agent.ire object, hence very low transit ive use. 
Pa.ssivc Voice Passive implies transit ive use, hence correlated with transit ive feature. 
VBN Tag Passive implies past pa.rt;iciple use (VBN), hence correlated with transit ive (and passive). 
Causat iv i ty  () l ) ject-drop verbs do not have a. causal agent, hence low "ca.usative" use. Unergatives are 
rare in the transit ive, hence low cmlsative use. 
An imacy  Unaccusatives have a Theme subject in the intransit ive, hence lower use of animal, esubjects. 
unergatives and nnaccusatives, the l, hematic role 
of the subjec~ of the intransitiw,~ is identical to 
that of the objecl of the transitiw;, as shown in 
columns two and three of Table 1. C, iven the 
identity of thematic role mal)ped to subject and 
object positions, we expect to observe the sa.me 
noun occurring at times a.s subject of the verb, 
and at other times as object of the verb. In con- 
t ras t ,  for object-tirol) verbs, Cite thenm.tic role o\[' 
the sul)ject o17 the intransitive is identical to l;ha{, 
of the sul)ject of the transitive, not the object of 
the transitive. Thus, we expect that it will be less 
common for the same noun to occur in subject and 
object position of the same object-drop verb. We 
hypothesize that this pattern of thematic role as- 
signments will be retlected in difl'erential amount 
of u~'~age across the classes of the same nouns as 
subjects and ol)jects for a given verb. Further- 
more, since the causative is a transitive use, a.nd 
the 1,ra.nsitive use of u nerga.gives i oxpocl;ed to be 
rare., this overlap o(' subjects and ob.iects should 
primarily distinguish unaccusatives (predicted to 
have high overlap of subjects and objects) from 
the other two classes. 
Finally, considering columns one and three of 
Tal)le 1, we note that unergative and objecl;-drop 
verbs assign all agentive role to their subject in 
both the transitive and intra.nsitive, while unac- 
cusatives assign an agentive role to their subject 
only in the tr~msil, ive. Under the assutnpL ion that 
the intransitive use of' unaccusatives i  not rare, 1 
we then expect thai, unaccusatives will occur less 
often overall with an agentive subject than the 
other two verb classes. On the flu:ther assump- 
tion that Agents tend to be animate entities more 
so than Themes, we expect that unaccusatives 
will occur less freqnently with an animate subject 
compared to unergative and object-drop verbs. 
Note the importance of our use of frequency dis- 
tributions: the claim is not that only Agents can 
~This assumpl, ion is based on the linguistic conlplexity 
of the causative, and borne out in our corpus analysis. 
be animate, but rather that nouns that receive an 
Agent role will more often be animate than nouns 
that receive a Theme ,'ole. 
The above interactions between thematic roles 
and the syntactic expressions of arguments thus 
lead to three features whose distrit)utional proper- 
ties appear promising for distinguishing the verb 
classes: transitivity, causativity, and animaey of 
subject. We also investigate two additionM syn- 
tactic l'ea.l, ures, the passive voice and tile past pa.r- 
ticiple POS tag (VI3N). These features are related 
to the transitive/intransitive Mternal;ion, since a 
passive use implies a transitive use of the verb, 
and the nse of passive in turn implies the use of 
the past participle. Our hyl)ol;hesis is that these 
five features will exhibit distributional differences 
in the observed usages of the verbs, which can be 
used for classifica.tion. The features and their ex- 
pected relevance are summarized in '13ble 2. 
4 Da~a Col lec t ion  and  Ana lys i s  
We chose a set of 20 verbs from each of three 
classes. The complete list of verbs is reported in 
Appendix A. Recall that our goal is to achieve a 
fine-grained classification of verbs that exhibit the 
same subcategorization frames; thus, tile verbs 
were chosen because they do not generally show 
massive del)artures from the intended verb sense 
(and usage) in the corpus. 2 In order to simplify 
tile counting procedure, we included only tile reg- 
ular ("-ed") simple past/past participle form of 
tile verb, assuming that this would approximate 
the distribution of tile features across all forms of 
the verb. Finally, as far as we were able given 
the preceding constraints, we selected verbs that 
could occur in the transitive and in the passive. 
We counted the occurrences of each verb token 
in a transitive or intransitive use (3'RANS), ill a 
2~l~hough note that there are only 19 unaccusatives be- 
cause ripped was excluded fl'om the analysis as it occurred 
mostly in a very different use (ripped off) in the corpus 
from the intended diange of state usage. 
817 
passive or active use (PASS), in a past participle 
or simple past use (VBN), in a causative or non- 
causative use (tAgS), and with an animate subject 
or not (ANIM), as described below. The first three 
counts  (TRANS, I'ASS~ VBN) were performed on 
the LDC's 65-million word tagged ACL/DCI  cor- 
pus (Brown, and Wall Street Journal 1987-1989). 
The last two counts (CAUS and ANIM) were per- 
formed on a 29-million word parsed corpus (\gall 
Street Journal 1988, provided by Michael Collins 
(Collins, 1997)). The features were counted as 
follows: 
TaANS: The closest noun following a verb was 
considered a potential object. A verb immedi- 
ately \[bllowed by a potential object was counted 
as transitive, otherwise as intransitive. 
pass: A token tagged VBD (the tag for simple 
past) was counted as active. A token tagged VBN 
(the tag for past participle) was counted as active 
if the closest preceding auxiliary was have, and as 
passive if the closest preceding auxiliary was be. 
VBN: The counts tbr VBN/VBI )  were based on 
the POS label in the tagged corl)us. 
Each of the above counts was normalized over 
all occurrences of tim "-ed" form of the verb, yield- 
ing a single relative fi:equency measure \['or each 
verb for that feature. 
tags :  For each verl) token, the subject and ob- 
ject (it' there was one) were extracted from the 
parsed corpus, and the proportion of overlap be- 
tween subject and object nouns across all tokens 
of a verb was calculated. 
ANIM: To approximate animacy without refer- 
ence to a resource external to the corpus (such 
as WordNet), we count pronouns (other than it) 
in subject position (cf. (Aone and McKee, 1996)). 
The aSSUlnption is that the words I, we, you, .~'tze, 
he, and theft most often refer to animate entities. 
We automatically extracted all subject/verb tu- 
ples, and computed the ratio of occurrences of 
pronoun subjects to all subjects for each verb. 
The aggregate means by class resulting from the 
counts above are shown in Table 3. The distri- 
butions of each feature are indeed roughly as ex- 
pected according to the description in Section 3. 
Unergatives how a very low relative fi'equency 
of the TRANS feature, followed by unaccusatives, 
then object-drop verbs. Unaccusative verbs show 
a high frequency of the CAUS feature and a low 
frequency of the ANIM feature compared to the 
other classes. Although expected to be a redun- 
dant indicator of transitivity, pass and VBN do 
Ta.ble 3: Aggregated Relative Frequency Data \['or 
tile Five Features. E = unergatives, A = unac- 
cusatives, O = object-drol)s. 
Class 
E 
A 
O 
N MEAN I~ELATIVE FREQUENCY 
TR, ANS PASS VBN CAUS ANIM 
20 0.23 0.07 0.21 0.00 0.25 
19 0.40 0.33 0.65 0.12 0.07 
20 0.62 0.31 0.65 0.04 0.15 
not distinguish t)etween unaccusative and object- 
drop verbs, indicating that their distributions are 
sensitive to factors we have not yet investigated, a 
5 Exper iments  in C lass i f icat ion 
The frequency distributions of our  features yield 
a vector for each verb that represents the relative 
frequency wdues for the verb on eacln dimension: 
\[verb, TRANS, PASS, VBN~ CAUS, ANIM, class\] 
Example: \[opened, .69, .09, .21, .16, .36, unaec\] 
\?e use the resulting 59 vectors to train an au- 
tomatic classifier to determine, given a verb that 
exhibits transitive~intransitive sttbcategorization 
frames, which of the three major lexical semantic 
classes of English optionally intransitive verbs it 
belongs to. Note that the baseline (chance) per- 
Ibrmance in this task is 33.9%, since there are 59 
vectors and 3 possible classes, with the most coin- 
men class having 20 verbs. 
We used the C5.0 machine learning system 
(tnttp://www.rulequest.com), a newer version of 
C4.5 (Quinlan, 1992), which generates decision 
trees and corresponding rule sets from a training 
set of known classifications. We found little to no 
difference in performance between the trees and 
rule sets, and report only the rule set results. \?e 
report here on experiments using a single hold- 
out training and testing methodology. In this ap- 
proach, we hold out a single verb vector as the 
test case, and train the system on the remaining 
58 cases. We then test the resulting classifier on 
tile single hold-out case, recording tile assigned 
class for that verb. This is then repeated for each 
of the 59 verbs. This technique has the benefit 
of yielding both an overall accuracy rate (when 
the results are averaged across all 59 trials), as 
well as providing tile data necessary tbr determin- 
ing accuracy for each verb class (because we have 
the classification of each verb when it is the test 
case). This allows us to evaluate tile contribution 
aThese observations have been confirmed by t-test.s be- 
tween feature values for each pair of classes. 
818 
%d)le <1:: Percent Accuracy of Verb Clas- 
sifica,l,ion Task Using \],'eatures in Combina- 
tion. T=Tl lANS;  \])=PASS; \ /=VBN;  C=CAUS;  
An=ANIM. E=unergatives, A=unaccusatives, 
O =:ol)ject-drops 
Percent Accuracy by Class 
All l E I a I 0 \],'ca.I,1llJes 
1. '.I'P VCAn 69.5 85.0 
2. P V C An 64:.4: 80.0 
3. TV  C An 71.2 80.0 
4:. 51' P C An 61.0 65.0 
5. TPVAn 62.7 70.0 
6. 51'PV C 61.0 80.0 
63.2 
dT.d 
73.7 
68.4 
63.2 
42.1 
60.0 
65.0 
60.0 
50.0 
55.0 
60.0 
of individual feal:ures with respect to their effect 
on the perfornlance of individual classes. 
We performed experiments on the \['ull sel, of fea- 
tures, as well a.s each subsel, of fea.l,ures wil,h a. sin- 
gle f~ture  remow;d, as reported in Table d. Con- 
sider l;he first column in the ta.ble. The first line 
shows that the overall ~ccuracy for all live features 
is 69.5%, a reduction in tile error ra.te of more 
than 50% above the baseline. The removal o\[" the 
PASS lea.lure appears to improve performance (row 
3 of Ta.ble 4). However, it should be noted that 
this increase in performance results h:oln a single 
additionaJ verb being classified correctly. The re- 
ma.ining rows show thal no feal,ure is superflous 
or hm'mfld as l,he removal of any I'ealure has a. 5 
8% negative elfect on l)erR)rmance. Coral)arable. 
accuracies have been demonsl;rated vsing a more 
thorough cross-validation methodology a.nd using 
reel;hods that are, in principle, better a,t taking 
adva.nl,age of correlated lea,lures (Stevenson and 
Merle, 1999; Stevenson el. al., 1999). 
q'he single hold-out prol,ocol provides new data, 
fbr analysing the performalme on individual verbs 
and classes. The class-by-class accuracies a.re 
shown in the remaining columns of Ta.ble 4. \?e 
can see clearly thal, using all five features, l,he 
unergatives are classified with much greater ac- 
curacy (85%) than l,he UlmCCUsatives and object- 
drop verbs (63.2% and 60.0% respectively), as 
shown in the first row. The rema.ining rows show 
that this l)al,tern generally holds \['or l,he subsel,s 
of features as well, with tire excel)lion of line d. 
\?hile ful,ure work on our verb classificalion 
task will need lo focus on deterlnining features 
thal bel,ter discriminate unaccusative a.nd object- 
drop verbs, we can ah:eady exclude an explana- 
tion of the resull,s based simply on l,he wwbs' or 
tile classes' frequency. Unergatives have tile low- 
est average (log) frequency (1.3), but are the best 
classified, while unaccusatives and object-drops 
are comparable (a.verage log fi'equency = 2). If we 
group verbs by frequency, the proportion of errors 
to lhe total number of verbs remains fairly simi- 
lar (freq 1:7  errors/23 verbs; fi:eq. 2 :6  errors/24 
verbs; freq. 3 :4  errors/10 verbs). The only verb 
of frequency 0 is correctly classified, while lhe only 
one with log frequency 4 is not . In sum, we do 
not find that more frequent classes or verbs are 
more accurately classitied. 
lmlmrtantly, the experiments also enable us to 
see whether the fealures indeed contribute to dis- 
criminating the classes in the manner predicted in 
Seclion 3. The single hold-out results allow us to 
do 1;his, by comparing the individual class labels 
assigned using the full sol, of five features (TIIANS, 
PASS, VBN, CAUS, ANIM) to the class labels as- 
signed using each size four subset of features. This 
comparison indicates 1;he changes in class labels 
l,hat we can a.l,tribul,e to l,he added feature in going 
fi'om a size four subset to the full set of features. 
(The individual class labels supporling our a.naly- 
sis below a.re a.vailable from the authors.) \?e con- 
cent;rate on tile three main features: CAUS, ANIM, 
TRANS. \?e filial thai, the behaviour of lhese fea- 
l,ures generaJly does conform to our predicl;ions. 
We expected that TRANS would help make a. three- 
way distinction among the verb classes. While 
unergatives are ah:eady accurately classified with- 
Ollt TRANS, inspection of lhe change in class la.- 
bels reveals that the addition of TRANS tO tire sel; 
improves performance on unaccusatives by help- 
ing to distinguish 1;hem from object-drol)s, llow- 
ever, in this case, we also observe a loss in pre- 
cision of unerga.lives, ince some object-drops are 
now classitied a.s unergatives. Moreover, we ex- 
pected CAUS and ANIM tO be parl,icularly helpfid 
in identi\['ying unaccus~l,ives, and this is also borne 
out in our analysis of individual la.bels. We note 
that the increased accuracy from CAUS is primar- 
ily due to bel,ter disl,inguishing unergatives from 
unaccusatives, and l,he increased accura.cy from 
AN1M is primarily due go better distinguishing un- 
accusatives from objecl,-drops. \?e conclude tha.t 
the feal,ures we have devised are successful in clas- 
siting optionally 1,ra.nsil;ive verbs because they ca.p- 
lure predicted ifl'erences in underlying argument 
st ruct l r re .  4 
4 Matters are more cmnplex with the other two features 
and we arc still interpreting tile results. Our prediction 
819 
Table 5: Pair-wise Agreement (Calculated t)3' the 
Kappa Statistics) of Three Experts (El, E2, h;3) 
Compared to a Gold Standard (Levin) and to the 
Classifier (Prog). Numbers in parentheses are per- 
centage of verbs on which judges agree. 
PltOG F.I. E2 E3 
El .36 (59) 
E2 .50 (68) .59 (75) 
E3 .49 (66) .53 (70) .66 (77) 
LEWN .54 (69.5) .56 (71.) .80 (86.5) .74 (83) 
6 Compar i son  to Exper ts  
In order to evaluate the performance of the al- 
gorithm in practice, we need to compare it to the 
accuracy of classification performed by an expert, 
which gives a realistic upper bound for the task. 
In (Merle and Stevenson, 2000) we report the re- 
suits of an experiment that measures experts per- 
tbrmance and agreement on a classification task 
very similar: to the program we have described 
here. The results summarised in Table 5 illus- 
trate the performance of the progra, m. On the 
one hand, the algorithm does not perform at ex- 
pert level, as indicated by the fact that, for all ex- 
perts, the lowest agreement score is with the pro- 
gram. On the other: hand, the accuracy achieved 
by the program of 69.5% is only 1.5% less than 
one of the human experts in comparison to tire 
gold standard. In fact, if we take the best per- 
formance achieved by an expert in this task 
86.5%--as the maximum achievable accuracy in 
classification, our algorithm then reduces the er- 
ror rate over; chance by approximately 68%, a very 
respectable result. 
7 D iscuss ion  
The work here contributes both to general and 
technical issues in automatic lexical acquisition. 
Firstly, our results confirm the primary role of 
argument structure in verb classification. Our ex- 
perimental focus is particularly clear in this re- 
gard because we deM with verbs that are ~Illilli- 
was that VBN and PASS would behave similarly to TRANS. 
In fact, PASS is at best unhelpful in classification. VBN 
does appear to make the expected I.hree-way distinction. 
The change ill class labels shows that the improvement in
performance with VBN results from better distinguishing 
unergatives fi'om object-drops, and object-drops from un- 
accusatives. The latter is surprising, since analysis of the 
data found that the VnN feature values are statistically in- 
distinc~ for the object-drop and unaccusative classes as a 
whole. 
mal pairs" with respect o argument structure. 13y 
classif~ying verbs that show the same subcatego- 
rizations into different classes, we are able to elim- 
inate one of the confounds in classification work 
created by the fact that subcategorization a d ar- 
gument structure :M'e largely co-variant. We can 
infer that the accuracy in our classification is due 
to argument structure information, a.s subcatego- 
rization is the same for: all verbs, confirming that 
the con, tent of thematic roles is crucial for clas- 
sification. Secondly, our results further support 
the assumption that thematic differences such as 
these are apparent not only in differences in sub- 
categorization frames, but also in differences in 
(;heir frequencies. We thus join the many recent 
results that all seem to converge in SUl)porting 
the view that the relation between lexical syntax 
and semantics can be usefully exploited (Aone and 
McKee, 1996; l)orr, 1997; Dorr and Jones, 1996; 
Lapata and Brew, 1999; Schulte im Walde, 1998; 
Siegel, 1998), especially in a statistical franmwork. 
Finally, we observe that this information is de- 
tectable in a corpus and can be learned automat- 
ically. Thus we view corpora, especially if an- 
notated wil;h currently available tools, a.s useful 
repositories of implicit grammars. 
Technically, our N)proach extends existing 
corpus-based learning techniques 1;o a more com- 
plex lea.ruing problem, in severaJ dimensions. Our 
statistical apl)roach , which does not require ex- 
plicit negative xamples, extends ai)l)roaehes that 
encode l~evin's alternations directly, as symbolic 
properties of a verb (Dorr et al, 1995; l)orr and 
Jones, 1996; l)orr, 1997). We also extend work 
using surface indicators to approximate underly- 
ing properties. (Oishi and Matsumoto, 1997) use 
case marking particles to approximate graimnat- 
ical functions, such as subject and object. We 
improve on this approach by learning argument 
structure properties, which, unlike grammatical 
functions, are not marked lnorphologically. Oth- 
ers have tackled the problem of lexical semantic 
classification, as we have, but using only snbeate- 
gorization frequencies as input data (Lapata and 
Brew, 1.999; Sehulte im Walde, 1998). By con- 
trast, we explicitly address the definition of fea- 
tures that can tap directly into thematic role dif- 
ferences that are not reflected in "subcategoriza- 
tion distinctions. Finally, when learning of the- 
matic role assignment has been the explicit goal, 
the text has been semantically annotated (Web- 
ster and Marcus, 1989), or external semantic re- 
820 
sources ha.re I)een consulted (Ache and McI(ee, 
19!)6). We extend these results by showing that  
them;~tic informa,tion can 1)e inducexl from corpus 
COtllltS. 
The exl)er imental  results show that  our method 
is l)owerful, and suited to Cite classitica.l;i(m of lex- 
ica.1 items. However, we have not yet addressed 
the problem of verbs that  can h;~ve mult iple clas- 
sifications. We think tha.t many eases of am- 
1)iguous classification of verb types can 1)e ad- 
dressed with the notion of intersective sets in- 
t roduced by (Da.ng et al, 71998). This is an im- 
t)ortant concept tha,t l)rOl)OSes tha,t "i'egula, r" a.m- 
biguity in classifica.tion -i.e., sets of v(;rbs that  
ha.ve the same mult i -way classitications a~ccording 
to (l,evin, 1993) can be captured with a. liner- 
grained notion of lexical semant ic  classes. I~x- 
tending our work to exploit this idea. requires 
only to define the classes a.pl)ropriately; the ba- 
sic a.1)t)roac\]l will remain the same. When we turn 
to consider ambiguity,  we must a.lso address the 
l)roblem l;ha.t individual insta.nces of verl)s may 
come from diffel:ent classes. In future research we 
t)lan to extend our method to the ('\]a.ssificagion f 
a.mbiguous tokens, by exper iment ing with a. func- 
t ics  that  combines severaJ sources of information: 
a bias tbr the verb type (using the cross-corpus 
sta.l;istics we collect), as well as \[~a.tures o\[" the us- 
age of the insta.nce being classiiiod (cf. (l,apa.ta 
a,n<l I~rew, t999; Siegel, 199,q)). 
Re ferences  
Cllinatsu Aolm and Dot@as Mcl(ec. 1990. Acquiring 
predicate-argument mapping information in multilingual 
texts. In Branimir l\]oguraev and James l)ustetjovsky, ed- 
itors, Cou)us \])rocessinq.\[or l, cxieal Acq~tisition, pages 
191-202. MIq' Press. 
Michac'\] lh'ent. 1993. l"rom grammar to lexicon: UnSUl)er- 
vised learning o\[ lexical syntax. Compatational Linguis- 
tics, 1912):243 262. 
'lk:d Briscoe and ./ohn Cm'roll. 1997. Automatic extraction 
of subcategorization from corpora. In IS"ocs of the I"~Hh 
ANLP Co,@rence, pages 356-363. 
Michael John Collins. 1997. Three generative, lexicallsed 
models for statistical pro'sing. In lS"ocs of ACL '97, pages 
16-23. Madrid, Spain. 
Hot Trang Dang, Karin Kipper, Mm'tha l)almer, and 
.loseph l{osenzweig. 1998. Investigating regular sense 
exl, ensions 1oased on intersective 1,evin classes. \[n 
Procs of COI, ING-ACL '98, pages 293 299, Montreal, 
Canada. 
BOltnie 1)orr attd l)(n,g Jones. 1996. Role of word sense 
disambiguatlon i  lexical acquisition: lhedieting seman- 
tics from syntactic ues. In 15"oc. of UOL1NG'96, pages 
3;22 327, Col)enhage,l, )enmark. 
Boluiie l)orr, Joe t~larlliall, and Amy VVeinberg. 1995. 
\]~l:Oln syntactic encodillgs to thematic ro|es: l}uilding 
lexical entries for interlingual MT. Journal o\[ Machine 
!l'ran.?lation, 9(3):71- 100. 
\]\]onnie l)orr. 1997. I,m-ge-scale dictionary construction 
for foreign language tutoring and inter\]ingual machine. 
translation. Machine Translation, 12:1 55. 
K('.n l\[ale and Jay \](eyser. 1993. On argument structure 
and the lexical representation f syntactic relations. In 
I(. \]tale and .\]. l(eyser, editors, The View fl'om lJuilding 
20, pages 53-110. MIT Press. 
Maria Lapata and Chris Brew. 11999. Using subcategoriza- 
tion t,o resolve verb class ambiguity. In Frocs of Joint 
,5'IGDAT Con\[erence on Empirical Mett, ods in Natural 
Langaage, College Park, M\]). 
Beth Lcvin. 1993. English Verb Classes and Alternations. 
University of Chicago Press, Chicago, I\],. 
Christopher 1). Manning. :1993. Automatic acquisition of 
a large subcategorlzation dictionary l>om corpora. In 
l)rocs of A UL'93, pages 235 -242. Ohio State University. 
Diana McCarthy. 2000. Using semantic l)referenee Lo idcn- 
tit'y verb participation in role switchitlg alternations. In 
15"oes of NAA C1,-2000, Seattle, Washington. 
l)aola Merlo and Sllzantle Stevenson. 2000:-F, stablishing 
the upper-bound and inter-judge agremcnt in a verb 
classification task. In l)rocs of LI~EC-2000, pages 1659 
1G64. Athens, Greece. 
Akira Oishi and Yuji Matsumoto. 1997. l)etecting the or- 
ganization of semantic subclasses of Japanese verbs. In- 
ternational Journal of Corpus Linguistics, 2(1):65 89. 
,/. \]toss Quinlan. 1992. (?4.5 : l~rograms .for Maehi,ze 
Learning. Morgan l(aufmamt, San Mateo, CA. 
Sabine Sehulte im Walde. 1998. Automatic semanl.ic las- 
sification of verbs according to their alternation be- 
haviour. AIMS l\]eport 4(3), IMS, UniversitSt Stuttgart. 
I",eic V. Siegel. 1998. Linguistic Indicators .for L(mouaffc 
Undcrslandin 9 l)h.I), thesis, I)ept. of Comput(w Sci- 
ence, (Johm|l)ia University. 
Suzalllle SteveHsoll alld l)aola Merlo. 1997. l~exica\] st.ru(:- 
turc and \])ro(:essing complexity. Lanuagc and (7o\[pzilivc 
\])rocc:,~se.,% 12(1-2):3.'19 399. 
Suzanne Stevenson and \])aola Mer\]o. 1999. Verb classifi- 
cation using distributions of gt'ammatical features. In 
l)rocs of 1?,4 CL'99. Bergen, Norway. 
Suzmme Stevenson, l)aola Merlo, Natalia Kariaeva, and 
l(amin Whitehouse. 1999. Supervised learning of lexical 
semantic verb classes using fl'equency distributions. \[n 
I)rocs of Si.qLex '99, College Park, Maryland. 
Mort Webster and Mirth Mm'cus. 1989. Automatic ac- 
quisition of the lexical semanl, ics of verbs fl'om sentence 
frames. In Procs of A CL'89, pages 177-184, Vancouver, 
Canada. 
Append ix  A 
Unergatives: floated, galloped, glided, hiked, hopped, hur- 
ried, jogged , jumped, leaped, marched, paraded, raced, 
rushcd, seootcd, scurricd, skipped, tiptoed, t~vttcd, va,dtcd, 
wandered. Unaccusativcs: boiled, changcd, cleared, 
collapsed, cooled, cracked, dissolved, divided, exploded, 
flooded, .folded, fractured, hardcned, melted, opcncd, sim- 
mcred, solidified, stabilized, widened. Object-drops: bof 
rowed, eallcd, earvcd, clca~cd, danced, inheritcd, kiekcd, 
knittcd, oraniscd, packcd, paintcd, playcd, reaped, rcnted, 
skclehe.d, studied, swallowed, typed, washcd, ycllcd. 
821 
 
	 
		
	
251
252
253
254
255
256
257
258
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 620?627, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Accurate Function Parsing
Paola Merlo
Department of Linguistics
University of Geneva
1204 Geneva
Switzerland
merlo@lettres.unige.ch
Gabriele Musillo
Depts of Linguistics and Computer Science
University of Geneva
1204 Geneva
Switzerland
musillo4@etu.unige.ch
Abstract
In this paper, we extend an existing parser
to produce richer output annotated with
function labels. We obtain state-of-the-art
results both in function labelling and in
parsing, by automatically relabelling the
Penn Treebank trees. In particular, we ob-
tain the best published results on seman-
tic function labels. This suggests that cur-
rent statistical parsing methods are suffi-
ciently general to produce accurate shal-
low semantic annotation.
1 Introduction
With recent advances in speech recognition, parsing,
and information extraction, some domain-specific
interactive systems are now of practical use for
tasks such as question-answering, flight booking, or
restaurant reservation (Stallard, 2000). One of the
challenges ahead lies in moving from hand-crafted
programs of limited scope to robust systems inde-
pendent of a given domain. While this ambitious
goal will remain in the future for some time to come,
recent efforts to develop language processing sys-
tems producing richer semantic outputs will likely
be the cornerstone of many successful developments
in natural language understanding.
In this paper, we present a parser that outputs la-
bels indicating the syntactic or semantic function of
a constituent in the tree, such as NP-SBJ or PP-TMP
shown in bold face in the tree in Figure 1. These
labels indicate that the NP is the subject of the sen-
tence and that the PP conveys temporal information.
(Labels in parentheses will be explained later in the
paper.) Output annotated with such informative la-
bels underlies all domain-independent question an-
S



HH
H
HH
H
NP-SBJ
 PPP
the authority
VP




 
 
 
@
@
@
PP
PP
PP
PP
VBD
dropped
PP-TMP
 H
IN(TMP)
at
NP
NN
midnight
NP-TMP
NNP(TMP)
Tuesday
PP-DIR
 HH
TO(DIR)
to
NP
QP
 PPP
$ 2.8 trillion
Figure 1: A sample syntactic structure with function
labels.
swering (Jijkoun et al, 2004) or shallow semantic
interpretation systems (Collins and Miller, 1998; Ge
and Mooney, 2005). We test the hypothesis that a
current statistical parser can output such richer in-
formation without any degradation of the parser?s
accuracy on the original parsing task. Briefly, our
method consists in augmenting a state-of-the-art sta-
tistical parser (Henderson, 2003), whose architec-
ture and properties make it particularly adaptive to
new tasks. We achieve state-of-the-art results both
for parsing and function labelling.
Statistical parsers trained on the Penn Treebank
(PTB) (Marcus et al, 1993) produce trees annotated
with bare phrase structure labels (Collins, 1999;
Charniak, 2000). The trees of the Penn Treebank,
however, are also decorated with function labels.
Figure 1 shows the simplified tree representation
with function labels for a sample sentence from
the Penn Treebank corpus (section 00) The Gov-
ernment?s borrowing authority dropped at midnight
Tuesday to 2.8 trillion from 2.87 trillion. Table 1
illustrates the complete list of function labels in the
Penn Treebank. Unlike phrase structure labels, func-
620
Syntactic Labels Semantic Labels
DTV dative ADV adverbial
LGS logical subject BNF benefactive
PRD predicate DIR direction
PUT compl of put EXT extent
SBJ surface subject LOC locative
VOC vocative MNR manner
Miscellaneous Labels NOM nominal
CLF it-cleft PRP purpose or reason
HLN headline TMP temporal
TTL title Topic Labels
CLR closely related TPC topicalized
Table 1: Complete set of function labels in the Penn
Treebank.
tion labels are context-dependent and encode a shal-
low level of phrasal and lexical semantics, as ob-
served first in (Blaheta and Charniak, 2000).1 To a
large extent, they overlap with semantic role labels
as defined in PropBank (Palmer et al, 2005).
Current statistical parsers do not use this richer
information because performance of the parser usu-
ally decreases considerably, since a more complex
task is being solved. (Klein and Manning, 2003),
for instance report a reduction in parsing accuracy
of an unlexicalised PCFG from 77.8% to 72.9% if
using function labels in training. (Blaheta, 2004)
also reports a decrease in performance when at-
tempting to integrate his function labelling system
with a full parser. Conversely, researchers interested
in producing richer semantic outputs have concen-
trated on two-stage systems, where the semantic la-
belling task is performed on the output of a parser,
in a pipeline architecture divided in several stages
(Gildea and Jurafsky, 2002). See also the common
task of (CoNLL, 2004 2005; Senseval, 2004).
Our approach maintains state-of-the-art results in
parsing, while also reaching state-of-the-art results
in function labelling, by suitably extending a Sim-
ple Synchrony Network (SSN) parser (Henderson,
2003) into a single integrated system. This is an
interesting result, as a task combining function la-
belling and parsing is more complex than simple
parsing. While the function of a constituent and its
structural position are often correlated, they some-
1(Blaheta and Charniak, 2000) talk of function tags. We will
instead use the term function label, to indicate function identi-
fiers, as they can decorate any node in the tree. We keep the
word tag to indicate only those labels that decorate preterminal
nodes in a tree ? part-of-speech tags? as is standard use.
times diverge. For example, some nominal temporal
modifiers occupy an object position without being
objects, like Tuesday in the tree above. Moreover,
given current limited availability of annotated tree
banks, this more complex task will have to be solved
with the same overall amount of data, aggravating
the difficulty of estimating the model?s parameters
due to sparse data.
2 Method
Successfully addressing function parsing requires
accurate parsing models and training data. Under-
standing the causes and the relevance of the ob-
served results requires appropriate evaluation mea-
sures. In this section, we describe the methodology
that will be used to assess our main hypothesis.
2.1 The Basic Parsing Architecture
Our main hypothesis says that function labels can
be successfully and automatically recovered while
parsing, without affecting negatively the perfor-
mance of the parser. It is possible that attempting
to solve the function labelling and the parsing prob-
lem at the same time would require modifying ex-
isting parsing models, since their underlying inde-
pendence assumptions might no longer hold. More-
over, many more parameters are to be estimated. It is
therefore important to choose a statistical parser that
can model our augmented labelling problem. We use
a family of statistical parsers, the Simple Synchrony
Network (SSN) parsers (Henderson, 2003), which
crucially do not make any explicit independence as-
sumptions, and learn to smooth across rare feature
combinations. They are therefore likely to adapt
without much modification to the current problem.
This architecture has shown state-of-the-art perfor-
mance and is very adaptive to properties of the in-
put.
The architecture of an SSN parser comprises two
components, one which estimates the parameters
of a stochastic model for syntactic trees, and one
which searches for the most probable syntactic tree
given the parameter estimates. As with many other
statistical parsers (Collins, 1999; Charniak, 2000),
the model of parsing is history-based. Its events
are derivation moves. The set of well-formed se-
quences of derivation moves in this parser is defined
621
by a Predictive LR pushdown automaton (Nederhof,
1994), which implements a form of left-corner pars-
ing strategy.2
The probability of a phrase-structure tree is
equated to the probability of a finite (but unbounded)
sequence of derivation moves. To bound the number
of parameters, standard history-based models par-
tition the set of prefixes of well-formed sequences
of transitions into equivalence classes. While such
a partition makes the problem of searching for the
most probable parse polynomial, it introduces hard
independence assumptions: a derivation move only
depends on the equivalence class to which its history
belongs. SSN parsers, on the other hand, do not state
any explicit independence assumptions: they induce
a finite history representation of an unbounded se-
quence of moves, so that the representation of a
move i? 1 is included in the inputs to the represen-
tion of the next move i, as explained in more detail
in (Henderson, 2003). SSN parsers only impose soft
inductive biases to capture relevant properties of the
derivation, thereby exhibiting adaptivity to the in-
put. The art of designing SSN parsers consists in
selecting and introducing such biases. To this end, it
is sufficient to specify features that extract some in-
formation relevant to the next derivation move from
previous ones, or some set of nodes that are struc-
turally local to the node on top of the stack. These
features and these nodes are input to the compu-
tation of a hidden history representation of the se-
quence of previous derivation moves. Given the hid-
den representation of a derivation, a log-linear distri-
bution over possible next moves is computed. Thus,
the set D of structurally local nodes and the set f of
predefined features determine the inductive bias of
an SSN system. Unless stated otherwise, for each
of the experiments reported here, the set D that is
input to the computation of the history representa-
tion of the derivation moves d1, . . . , di?1 includes
the following nodes: topi, the node on top of the
pushdown stack before the ith move; the left-corner
ancestor of topi; the leftmost child of topi; and the
most recent child of topi, if any. The set of fea-
tures f includes the last move in the derivation, the
label or tag of topi, the tag-word pair of the most re-
2The derivation moves include: projecting a constituent with
a specified label, attaching one constituent to another, and shift-
ing a tag-word pair onto the pushdown stack.
cently shifted word, the leftmost tag-word pair that
topi dominates.
2.2 The Set of Function Labels
The bracketting guidelines for the Penn Treebank II
list 20 function labels, shown in Table 1 (Bies et al,
1995). Based on their description in the Penn Tree-
bank guidelines, we partition the set of function la-
bels into four classes, as indicated in the table. Fol-
lowing (Blaheta and Charniak, 2000), we refer to the
first class as syntactic function labels, and to the sec-
ond class as semantic function labels. In the rest
of the paper, we will ignore the other two classes,
for they do not intersect with PropBank labels, and
they do not form natural classes. Like previous work
(Blaheta and Charniak, 2000), we complete the sets
of syntactic and semantic labels by labelling con-
stituents that do not bear any function label with a
NULL label.3
2.3 Evaluation
To evaluate the performance of our function pars-
ing experiments, we will use several measures. First
of all, we apply the standard Parseval measures of
labelled recall and precision to a parser whose train-
ing data contains the Penn Treebank function labels,
to assess how well we solve the standard phrase
structure parsing problem. We call these figures
FLABEL-less figures in the tables below and we will
call the task the (simple) parsing task in the rest of
the paper. Second, we measure the accuracy of this
parser with an extension of the Parseval measures
of labelled precision and recall applied to the set of
complex labels ?the phrase structure non-terminals
augmented with function labels? to evaluate how
well the parser solves this complex parsing prob-
lem. These are the FLABEL figures in the tables be-
low. We call this task the function parsing task. Fi-
nally, we also assess function labelling performance
on its own. Note that the maximal precision or recall
score of function labelling is strictly smaller than
one-hundred percent if the precision or the recall of
3Strictly speaking, this label corresponds to two NULL la-
bels: SYN-NULL and SEM-NULL. A node bearing the SYN-
NULL label is a node that does not bear any other syntactic label.
Analogously, the SEM-NULL label completes the set of semantic
labels. Note that both the SYN-NULL label and the SEM-NULL
are necessary, since both a syntactic and a semantic label can
label a given constituent.
622
ASSIGNED LABELS
ADV BNF DIR EXT LOC MNR NOM PRP TMP SEM-NULL SUM
ADV 143 0 0 0 0 0 0 1 3 11 158
BNF 0 0 0 0 0 0 0 0 0 1 1
DIR 0 0 39 0 3 4 0 0 1 51 98
EXT 0 0 0 37 0 0 0 0 0 17 54
ACTUAL LOC 0 0 1 0 345 3 0 0 15 148 512
LABELS MNR 0 0 0 0 3 35 0 0 16 40 94
NOM 2 0 0 0 0 0 88 0 0 4 94
PRP 0 0 0 0 0 0 0 54 1 33 88
TMP 18 0 1 0 24 11 0 1 479 105 639
SEM-NULL 12 0 13 5 81 28 12 24 97 20292 20564
SUM 175 0 54 42 456 81 100 80 612 20702 22302
Table 2: Confusion matrix for simple baseline model, tested on the validation set (section 24 of PTB).
the parser is less than one-hundred percent. Follow-
ing (Blaheta and Charniak, 2000), incorrectly parsed
constituents will be ignored (roughly 11% of the to-
tal) in the evaluation of the precision and recall of
the function labels, but not in the evaluation of the
parser. Of the correctly parsed constituents, some
bear function labels, but the overwhelming majority
do not bear any label, or rather, in our notation, they
bear a NULL label. To avoid calculating excessively
optimistic scores, constituents bearing the NULL la-
bel are not taken into consideration for computing
overall recall and precision figures. NULL-labelled
constituents are only needed to calculate the preci-
sion and recall of other function labels. (In other
words, NULL-labelled constituents never contribute
to the numerators of our calculations.) For exam-
ple, consider the confusion matrix M in Table 2,
which reports scores for the semantic labels recov-
ered by the baseline model described below. Preci-
sion is computed as
?
i?{ADV???TMP} M [i,i]?
j?{ADV???TMP} M [SUM,j]
. Re-
call is computed analogously. Notice that M [n, n],
that is the [SEM-NULL,SEM-NULL] cell in the matrix, is
never taken into account.
3 Learning Function Labels
In order to assess the complexity of the task of pre-
dicting function labels while parsing, we run first the
SSN on the function parsing task, without modifica-
tions to the parser. The confusion matrix for seman-
tic function labels of this simple baseline model is
illustrated in Table 2.
It is apparent that the baseline model?s largest
cause of error is confusion between the labels and
the NULL label. These misclassfications affect recall
in particular. Consider, for example, the MNR label,
where 40 out of 94 occurrences are not given a func-
tion label. We add two augmentations to the parser
to alleviate this problem.
The simple baseline parser treats NULL labels like
other labels, and it does not distinguish subtypes of
NULL labels. Our first augmentation of the parser
is designed to discriminate among constituents with
these NULL labels. We hypothesize that the label
NULL (ie. SYN-NULL and SEM-NULL) is a mixture
of types, which will be more accurately learnt sepa-
rately. If the label NULL is learnt more precisely, the
recall of the other labels will increase. The NULL
label in the training set was automatically split into
the mutually exclusive labels CLR, OBJ and OTHER.
Constituents were assigned the OBJ label according
to the conditions stated in (Collins, 1999).4
Another striking property of the simple baseline
function parser is that the SSN tends to project NULL
labels more than any other label. Since SSNs de-
cide the label of a non-terminal at projection, this
behaviour indicates that the parser does not have
enough information at this point in the parse to
project the correct function label. We hypothesize
that finer-grained labelling will improve parsing per-
formance. This observation is consistent with results
reported in (Klein and Manning, 2003), who showed
that part-of-speech tags occurring in the Treebank
are not fine-grained enough to discriminate between
4Roughly, an OBJ non-terminal is an NP, SBAR or S whose
parent is an S, VP or SBAR. Any such non-terminal must not
bear either syntactic or semantic function labels, or the CLR la-
bel. In addition, the first child following the head of a PP is
marked with the OBJ label.
623
preterminals. For example, the tag TO labels both
the preposition to and the infinitival marker. Extend-
ing (Klein and Manning, 2003)?s technique to func-
tion labelling, we split some part-of-speech tags into
tags marked with semantic function labels. More
precisely, we concentrate on the function labels DIR,
LOC, MNR, PRP or TMP, which appear to cause the
most trouble to the parser, as illustrated in Table 2.
The label attached to a non-terminal was propa-
gated down to the pre-terminal tag of its head. The
labels in parentheses in Figure 1 illustrate the effect
of this lowering of the labels. The goal of this tag-
splitting is to indicate more clearly to the parser what
kind of label to project on reading a word-tag pair in
the input. To this end, re-labelling is applied only if
the non-terminal dominates the pre-terminal imme-
diately. This constraint guarantees that only those
non-terminals that are actual projections of the pre-
terminal are affected by this tag-splitting method.
Linguistically, we are trying to capture the notion
of maximal projection. 5 This augmented model has
a total of 188 non-terminals to represent labels of
constituents, instead of the 33 of the original SSN
parser. As a result of lowering the five function la-
bels, 83 new part-of-speech tags were introduced to
partition the original tagset of the Treebank. There
are 819 tag-word pairs in this model, while the orig-
inal SSN parser has a vocabulary size of 508 tag-
word pairs. These augmented tags as well as the
155 new non-terminals are included in the set f of
features input to parsing decisions as described in
section 2.1.
SSN parsers do not tag their input sentences. To
provide the augmented model with tagged input sen-
tences, we trained an SVM tagger whose features
and parameters are described in detail in (Gimenez
and Marquez, 2004). Trained on section 2-21, the
tagger reaches a performance of 95.8% on the test
set (section 23) of the PTB using our new tag set.
4 Experiments
In this section, we report the results of the exper-
iments testing hypotheses concerning our function
parser. All SSN function parsers were trained on
5This condition was relaxed in a few cases to
capture constructs such as coordinated PPs (e.g.
[PP-LOC[PP[INat] . . .][CCand][PP[INin] . . .] . . .] or infini-
tival clauses (e.g. [S-PRP[VP[TOto][VP[VB. . .] . . .] . . .]).
FLABEL FLABEL-less
F R P F R P
Validation Set
Base 83.4 82.8 83.9 87.7 87.1 88.2
Aug 84.6 84.0 85.2 88.1 87.5 88.7
Test Set
Aug 86.1 85.8 86.5 88.9 88.6 89.3
H03 88.6 88.3 88.9
Table 3: Percentage F-measure (F), recall (R), and
precision (P) of the SSN baseline (Base) and aug-
mented (Aug) parsers. H03 indicates the model il-
lustrated in (Henderson, 2003).
sections 2-21 from the Penn Treebank, validated on
section 24, and tested on section 23. All models are
trained on parse trees whose labels include function
labels. Both results taking function labels into ac-
count (FLABEL) and results not taking them into
account (FLABEL-less) are reported. All our mod-
els, as well as the parser described in (Henderson,
2003), are run only once. 6 These results are re-
ported in Table 3.
Our hypothesis states that we can perform func-
tion labelling and parsing at the same time, without
loss in parsing performance. For this to be an inter-
esting statement, we need to show that function la-
belling is not a straightforward extension of simple
parsing. If simple parsing could be easily applied to
function parsing, we should not have a degradation
of an SSN parser model evaluated on the complex
labels, compared to the same SSN parser evaluated
only on phrase structure labels. As the results on
the validation set indicate, our baseline model with
function labels (FLABEL) is indeed lower than the
performance of the parser when function labels are
not taken into account (FLABEL-less), indicating
that the function parsing task is more difficult than
the simple parsing task.
Since the function parsing problem is more dif-
ficult than simple parsing, it is then interesting to
observe that performance of the augmented parser
increases significantly (FLABEL column) (p <
.001) without losing accuracy on the parsing task
6This explains the little difference in performance between
our results for H03 and those cited in (Henderson, 2003), where
the best of three runs on the validation set is chosen.
624
(FLABEL-less column), compared to the initial
parsing performance (as indicated by the perfor-
mance of H03). Notice that, numerically, we do in
fact a little better than H03, but this difference is not
significant.7
Beside confirming that learning function labels
does not increase parsing errors, we can also confirm
that the nature of the errors remains the same. A sep-
arate comparison of labelled and unlabelled scores
of our complex function parser indicates that unla-
belled results are roughly 1% better than labelled re-
sults (F measure 89.8% on the validation set). The
original SSN parser exhibits the same differential.
This shows that, like other simple parsers, the func-
tion parser makes mostly node attachment mistakes
rather than labelling mistakes.
A separate experiment only discriminating NULL
labels indicates that this modification is indeed use-
ful, but not as much as introducing new tags, on
which we concentrate to explain the results. There
is converging evidence indicating that the improve-
ment in performance is due to having introduced
new tag-word pairs, and not simply new words. First
of all, of the 311 new tag-word pairs only 122 in-
troduce truly new words. The remaining pairs are
constituted by words that were already in the orig-
inal vocabulary and have been retagged, or by tags
associated to unknown words.
Second, this interpretation of the results is con-
firmed by comparing different ways of enlarging the
vocabulary size input to the SSN. (Henderson, 2003)
tested the effect of larger input vocabulary on SSN
performance by changing the frequency cut-off that
selects the input tag-word pairs. A frequency cut-
off of 200 yields a vocabulary of 508 pairs, while a
cut-off of 20 yields 4242 pairs, 3734 of which com-
prise new words. This difference in input size does
not give rise to an appreciable difference in perfor-
mance. On the contrary, we observe that introduc-
ing 122 new words and 83 new tags improves results
considerably. This leads us to conclude that the per-
formance of the augmented model is not simply due
to a larger vocabulary.
We think that our tag-word pairs are effective be-
cause they are selected by a linguistically meaning-
7Significance was measured with the randomized signifi-
cance test described in (Yeh, 2000).
Syntactic Labels Semantic Labels
F R P F R P
Validation Set
Base 95.3 93.9 96.7 73.1 70.2 76.3
Aug 95.7 95.0 96.5 80.1 77.0 83.5
Test Set
Aug 96.4 95.3 97.4 86.3 82.4 90.5
BC00 95.7 95.8 95.5 79.0 77.6 80.4
B04 FT 95.9 95.3 96.4 83.4 80.3 86.7
B04 KP 98.7 98.4 99.0 78.0 73.2 83.5
Table 4: Percentage F-measure (F), recall (R), and
precision (P) function labelling, separated for syn-
tactic and semantic labels, for our models and Bla-
heta and Charniak?s (BC00) and Blaheta?s models
(B04 FT, B04 KP). The feature trees (FT) and ker-
nel perceptrons (KP) are optimised separately for the
two different sets of labels.
ful criterion and are more informative exemplars for
the parser. Instead, simply decreasing the frequency
cut-off adds mostly types of words for which the
parser already possesses enough evidence (in gen-
eral, nouns). Our method of lowering function la-
bels acts as a finer-grained classification that parti-
tions different kinds of complements based on their
lexical semantic characteristics, yielding classes that
are relevant to constituent structure. For instance,
it is well known that lexical semantic properties of
arguments of verbs are related to the verb?s argu-
ment structure, and consequently to the parse tree
that the verb occurs in. Partitioning a verb?s comple-
ments into function classes could influence attach-
ment decisions beneficially. We also think that the
parser we use is particularly able to take advantage
of these subclasses. One of the main properties of
SSN parsers is that they do not need large vocabu-
laries, because the SSN is good at generalising item-
specific properties into an internal hidden represen-
tation of word classes.
Finally, to provide a meaningful and complete
evaluation of the parser, it is necessary to examine
the level of performance on the function labels for
those constituents that are correctly parsed accord-
ing to the usual Parseval measure, i.e. for those con-
stituents for which the phrase structure labels and
the string covered by the label have been correctly
625
Baseline Augmented
P R P R
ADV 81.7 90.5 87.9 81.0
DIR 72.2 39.8 77.0 48.5
EXT 88.1 68.5 86.8 63.5
LOC 75.7 67.4 78.9 74.6
MNR 43.2 37.2 74.0 55.7
NOM 88.0 93.6 88.7 93.1
PRP 67.5 61.4 74.4 65.9
TMP 78.3 75.0 89.6 83.7
Table 5: Percentage F-measure (F), recall (R), and
precision (P) function labelling, separated for indi-
vidual semantic labels, for validation set.
recovered. Clearly, our parsing results would be un-
interesting if our recall on function labels were very
low. In that case, we would have failed to learn the
function parsing task, and that would trivially yield
a good performance on the simple parsing task. Ta-
ble 4 reports the aggregated numbers for the base-
line and the augmented model, while Table 5 re-
ports separate figures for each semantic function la-
bel. These tables show that we also perform well
on the labelling task alone. 8 Comparison to other
researchers (last three lines of Table 4) shows that
we achieve state-of-the-art results with a single inte-
grated model that is jointly optimised for all the dif-
ferent types of function labels and for parsing, while
previous attempts are optimised separately for the
two different sets of labels. In particular, our method
performs better on semantic labels.
5 Related Work
As far as we are aware, there is no directly compa-
rable work, as nobody has so far attempted to fully
merge function labelling or semantic role labelling
into parsing. We will therefore discuss separately
those pieces of work that have made limited use
of function labels for parsing (Klein and Manning,
2003), and those that have concentrated on recover-
ing function labels as a separate task (Blaheta and
Charniak, 2000; Blaheta, 2004). We cannot discuss
here the large recent literature on semantic role la-
belling for reasons of space, apart from work that
8See also (Musillo and Merlo, 2005) for more detail and
comparisons on the labelling task.
also recovers function labels (Jijkoun and de Rijke,
2004) and work that trains a parser on Propbank la-
bels as the first stage of a semantic role labelling
pipeline (Yi and Palmer, 2005).
(Klein and Manning, 2003) and, to a much more
limited extent, (Collins, 1999) are the only re-
searchers we are aware of who used function labels
for parsing. In both cases, the aim was actually
to improve parser performance, consequently only
few carefully chosen labels were used. (Klein and
Manning, 2003) suggest the technique of tag split-
ting for the constituent bearing the label TMP. They
also speculate that locative labels could be fruitfully
percolated down the tree onto the preterminals. Re-
sults in Table 5 indicate more precisely that lower-
ing locative labels does indeed bring about some im-
provement, but not as much as the MNR and TMP
labels.
In work that predates the availability of Framenet
and Propbank, (Blaheta and Charniak, 2000) define
the task of function labelling for the first time and
highlight its relevance for NLP. Their method is in
two-steps. First, they parse the Penn Treebank us-
ing a state-of-the-art parser (Charniak, 2000). Then,
they assign function labels using features from the
local context, mostly limited to two levels up the
tree and only one next label. (Blaheta, 2004) ex-
tends on this method by developing specialised fea-
ture sets for the different subproblems of function la-
belling and slightly improves the results, as reported
in Table 4. (Jijkoun and de Rijke, 2004) approach
the problem of enriching the output of a parser in
several steps. The first step applies memory-based
learning to the output of a parser mapped to de-
pendency structures. This step learns function la-
bels. Only aggregated results for all function la-
bels, and not only for syntactic or semantic labels,
are provided. Although they cannot be compared di-
rectly to our results, it is interesting to notice that
they are slightly better in F-measure than Blaheta?s
(F=88.5%). (Yi and Palmer, 2005) share the moti-
vation of our work, although they apply it to a dif-
ferent task. Like the current work, they observe that
the distributions of semantic labels could potentially
interact with the distributions of syntactic labels and
redefine the boundaries of constituents, thus yield-
ing trees that reflect generalisations over both these
sources of information.
626
6 Conclusions
In this paper we have presented a technique to ex-
tend an existing parser to produce richer output, an-
notated with function labels. We show that both
state-of-the-art results in function labelling and in
parsing can be achieved. Application of these re-
sults are many-fold, such as information extraction
or question answering where shallow semantic an-
notation is necessary. The technique illustrated in
this paper is of wide applicability to all other se-
mantic annotation schemes available today, such as
Propbank and Framenet, and can be easily extended.
Work to extend this technique to Propbank annota-
tion is underway. Since function labels describe de-
pendence relations between the predicative head and
its complements, whether they be arguments or ad-
juncts, this paper suggests that a left-corner parser
and its probabilistic model, which are defined en-
tirely on configurational criteria, can be used to pro-
duce a dependency output. Consequences of this ob-
servation will be explored in future work.
Acknowledgments
We thank the Swiss National Science Foundation
for its support of this research under grant number
105286. We thank James Henderson for allowing us
to use his parser and for numerous helfpul discus-
sions.
References
Ann Bies, M. Ferguson, K.Katz, and Robert MacIntyre. 1995.
Bracketing guidelines for Treebank II style. Technical re-
port, University of Pennsylvania.
Don Blaheta and Eugene Charniak. 2000. Assigning function
tags to parsed text. In Procs of NAACL?00, pages 234?240,
Seattle, Washington.
Don Blaheta. 2004. Function Tagging. Ph.D. thesis, Depart-
ment of Computer Science, Brown University.
Eugene Charniak. 2000. A maximum-entropy-inspired parser.
In Procs of NAACL?00, pages 132?139, Seattle, Washington.
Michael Collins and Scott Miller. 1998. Semantic tagging us-
ing a probabilistic context-free grammar. In Procs of the
Sixth Workshop on Very Large Corpora, pages 38?48, Mon-
treal, CA.
Michael Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, Department of
Computer Science, University of Pennsylvania.
CoNLL. 2004, 2005. Conference on computational natural lan-
guage learning (conll-2004/05).
Ruifang Ge and Raymond J. Mooney. 2005. A statistical se-
mantic parser that integrates syntax and semantics. In Procs
of CONLL-05, Ann Arbor, Michigan.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic labeling
of semantic roles. Computational Linguistics, 28(3):245?
288.
Jesus Gimenez and Lluis Marquez. 2004. Svmtool: A general
POS tagger generator based on Support Vector Machines. In
Procs of LREC?04, Lisbon, Portugal.
Jamie Henderson. 2003. Inducing history representations
for broad-coverage statistical parsing. In Procs of NAACL-
HLT?03, pages 103?110, Edmonton, Canada.
Valentin Jijkoun and Maarten de Rijke. 2004. Enriching the
output of a parser using memory-based learning. In Procs of
ACL?04, pages 311?318, Barcelona,Spain.
Valentin Jijkoun, Maarten de Rijke, and Jori Mur. 2004. In-
formation extraction for question answering: Improving re-
call through syntactic patterns. In Procs of COLING-2004,
Geneva, Switzerland.
Dan Klein and Christopher D. Manning. 2003. Accurate unlex-
icalized parsing. In Procs of ACL?03, pages 423?430, Sap-
poro, Japan.
Mitch Marcus, Beatrice Santorini, and M.A. Marcinkiewicz.
1993. Building a large annotated corpus of English: the
Penn Treebank. Computational Linguistics, 19:313?330.
Gabriele Musillo and Paola Merlo. 2005. Assigning function
labels to unparsed text. In Procs of RANLP?05, Korovets,
Bulgaria.
Mark Jan Nederhof. 1994. Linguistic Parsing and Program
Transformations. Ph.D. thesis, Department of Computer
Science, University of Nijmegen.
Martha Palmer, Daniel Gildea, and Paul Kingsbury. 2005. The
Proposition Bank: An annotated corpus of semantic roles.
Computational Linguistics, 31:71?105.
Senseval. 2004. Third international workshop on the evalua-
tion of systems for the semantic analysis of text (acl 2004).
http://www.senseval.org/senseval3.
David Stallard. 2000. Talk?n?travel: A conversational system
for air travel planning. In Procs of ANLP?00, pages 68?75,
Seattle, Washington.
Alexander Yeh. 2000. More accurate tests for the statistical
significance of the result differences. In Procs of COLING
2000, pages 947?953, Saarbrucken, Germany.
Szu-ting Yi and Martha Palmer. 2005. The integration of
semantic parsing and semantic role labelling. In Procs of
CoNLL?05, Ann Arbor, Michigan.
627
Automatic Verb Classification 
Based on Statistical Distributions of 
Argument Structure 
Paola Merlo* 
University of Geneva 
Suzanne Stevenson  t 
University of Toronto 
Automatic acquisition of lexical knowledge is critical to a wide range of natural language pro- 
cessing tasks. Especially important is knowledge about verbs, which are the primary source of 
relational information in a sentence--the predicate-argument structure that relates an action 
or state to its participants (i.e., who did what to whom). In this work, we report on super- 
vised learning experiments o automatically classify three major types of English verbs, based 
on their argument structure--specifically, the thematic roles they assign to participants. We use 
linguistically-motivated statistical indicators extracted from large annotated corpora to train the 
classifier, achieving 69.8% accuracy for a task whose baseline is 34%, and whose expert-based 
upper bound we calculate at 86.5%. A detailed analysis of the performance ofthe algorithm and 
of its errors con~'rms that the proposed features capture properties related to the argument struc- 
ture of the verbs. Our results validate our hypotheses that knowledge about thematic relations 
is crucial for verb classification, and that it can be gleaned from a corpus by automatic means. 
We thus demonstrate aneffective combination of deeper linguistic knowledge with the robustness 
and scalability of statistical techniques. 
1. Introduction 
Automatic acquisition of lexical knowledge is critical to a wide range of natural an- 
guage processing (NLP) tasks (Boguraev and Pustejovsky 1996). Especially important 
is knowledge about verbs, which are the primary source of relational information in 
a sentence--the predicate-argument structure that relates an action or state to its par- 
ticipants (i.e., who did what to whom). In facing the task of automatic acquisition of 
knowledge about verbs, two basic questions must be addressed: 
What information about verbs and their relational properties needs to be 
learned? 
What information can in practice be learned through automatic means? 
In answering these questions, some approaches to lexical acquisition have focused on 
learning syntactic information about verbs, by automatically extracting subcategoriza- 
tion frames from a corpus or machine-readable dictionary (Brent 1993; Briscoe and 
Carroll 1997; Dorr 1997; Lapata 1999; Manning 1993; McCarthy and Korhonen 1998). 
* Linguistics Department; University of Geneva; 2 rue de Candolle; 1211 Geneva 4, Switzerland; merlo@ 
lettres.unige.ch 
t Department of Computer Science; University of Toronto; 6 King's College Road; Toronto, ON M5S 3H5 
Canada; suzanne@cs.toronto.edu 
Q 2001 Association for Computational Linguistics 
Computational Linguistics Volume 27, Number 3 
Table 1 
Examples of verbs from the three optionally intransitive classes. 
Unergative The horse raced past the barn. 
The jockey raced the horse past the barn. 
Unaccusative The butter melted in the pan. 
The cook melted the butter in the pan. 
Object-Drop The boy played. 
The boy played soccer. 
Other work has attempted to learn deeper semantic properties uch as selectional re- 
strictions (Resnik 1996; Riloff and Schmelzenbach 1998), verbal aspect (Klavans and 
Chodorow 1992; Siegel 1999), or lexical-semantic verb classes uch as those proposed 
by Levin (1993) (Aone and McKee 1996; McCarthy 2000; Lapata and Brew 1999; Schulte 
im Walde 2000). In this paper, we focus on argument structure--the thematic roles as- 
signed by a verb to its arguments--as the way in which the relational semantics of 
the verb is represented at the syntactic level. 
Specifically, our proposal is to automatically classify verbs based on argument 
structure properties, using statistical corpus-based methods. We address the prob- 
lem of classification because it provides a means for lexical organization which can 
effectively capture generalizations over verbs (Palmer 2000). Within the context of 
classification, the use of argument structure provides a finer discrimination among 
verbs than that induced by subcategorization frames (as we see below in our example 
classes, which allow the same subcategorizations but differ in thematic assigmnent), 
but a coarser classification than that proposed by Levin (in which classes such as 
ours are further subdivided according to more detailed semantic properties). This 
level of classification granularity appears to be appropriate for numerous language 
engineering tasks. Because knowledge of argument structure captures fundamental 
participant/event relations, it is crucial in parsing and generation (e.g., Srinivas and 
Joshi \[1999\]; Stede \[1998\]), in machine translation (Dorr 1997), and in information re- 
trieval (Klavans and Kan 1998) and extraction (Riloff and Schmelzenbach 1998). Our 
use of statistical corpus-based methods to achieve this level of classification is moti- 
vated by our hypothesis that class-based differences in argument structure are reflected 
in statistics over the usages of the component verbs, and that those statistics can be 
automatically extracted from a large annotated corpus. 
The particular classification problem within which we investigate this hypothesis 
is the task of learning the three major classes of optionally intransitive verbs in English: 
unergative, unaccusative, and object-drop verbs. (For the unergative/unaccusative dis- 
tinction, see Perlmutter \[1978\]; Burzio \[1986\]; Levin and Rappaport Hovav \[1995\]). 
Table 1 shows an example of a verb from each class in its transitive and intransitive 
usages. These three classes are motivated by theoretical linguistic properties (see dis- 
cussion and references below, and in Stevenson and Merlo \[1997b\]; Merlo and Steven- 
son \[2000b\]). Furthermore, it appears that the classes capture typological distinctions 
that are useful for machine translation (for example, causative unergatives are un- 
grammatical in many languages), as well as processing distinctions that are useful for 
generating naturally occurring language (for example, reduced relatives with unerga- 
tive verbs are awkward, but they are acceptable, and in fact often preferred to full 
relatives for unaccusative and object-drop verbs) (Stevenson and Merlo 1997b; Merlo 
and Stevenson 1998). 
374 
Merlo and Stevenson Statistical Verb Classification 
Table 2 
Summary of thematic role assignments by class. 
Transitive Intransitive 
Classes Subject Object Subject 
Unergative Agent (of Causation) Agent Agent 
Unaccusative Agent (of Causation) Theme Theme 
Object-Drop Agent Theme Agent 
The question then is what underlies these distinctions. We identify the property 
that precisely distinguishes among these three classes as that of argument structure-- 
i.e., the thematic roles assigned by the verbs. The thematic roles for each class, and 
their mapping to subject and object positions, are summarized in Table 2. Note that 
verbs across these three classes allow the same subcategorization frames (taking an NP 
object or occurring intransitively); thus, classification based on subcategorization alone 
would not distinguish them. On the other hand, each of the three classes is comprised 
of multiple Levin classes, because the latter eflect more detailed semantic distinctions 
among the verbs (Levin 1993); thus, classification based on Levin's labeling would 
miss generalizations across the three broader classes. By contrast, as shown in Table 2, 
each class has a unique pattern of thematic assignments, which categorize the verbs 
precisely into the three classes of interest. 
Although the granularity of our classification differs from Levin's, we draw on her 
hypothesis that semantic properties of verbs are reflected in their syntactic behavior. 
The behavior that Levin focuses on is the notion of diathesis alternation--an alter- 
nation in the expression of the arguments of a verb, such as the different mappings 
between transitive and intransitive that our verbs undergo. Whether a verb partici- 
pates in a particular diathesis alternation or not is a key factor in Levin's approach to 
classification. We, like others in a computational framework, have extended this idea 
by showing that statistics over the alternants of a verb effectively capture information 
about its class (Lapata 1999; McCarthy 2000; Lapata and Brew 1999). 
In our specific task, we analyze the pattern of thematic assignments given in 
Table 2 to develop statistical indicators that are able to determine the class of an op- 
tionally intransitive verb by capturing information across its transitive and intransitive 
alternants. These indicators erve as input to a machine learning algorithm, under a 
supervised training methodology, which produces an automatic lassification system 
for our three verb classes. Since we rely on patterns of behavior across multiple occur- 
rences of a verb, we begin with the problem of assigning a single class to the entire 
set of usages of a verb within the corpus. For example, we measure properties across 
all occurrences of a word, such as raced, in order to assign a single classification to 
the lexical entry for the verb race. This contrasts with work classifying individual oc- 
currences of a verb in each local context, which have typically relied on training that 
includes instances of the verbs to be classified--essentially developing a bias that is 
used in conjunction with the local context o determine the best classification for new 
instances of previously seen verbs. By contrast, our method assigns a classification 
to verbs that have not previously been seen in the training data. Thus, while we do 
not as yet assign different classes to the instances of a verb, we can assign a single 
predominant class to new verbs that have never been encountered. 
To preview our results, we demonstrate hat combining just five numerical indi- 
cators, automatically extracted from large text corpora, is sufficient to reduce the error 
375 
Computational Linguistics Volume 27, Number 3 
rate in this classification task by more than 50% over chance. Specifically, we achieve 
almost 70% accuracy in a task whose baseline (chance) performance is 34%, and whose 
expert-based upper bound is calculated at 86.5%. 
Beyond the interest for the particular classification task at hand, this work ad- 
dresses more general issues concerning verb class distinctions based in argument 
structure. We evaluate our hypothesis that such distinctions are reflected in statis- 
tics over corpora through a computational experimental methodology in which we 
investigate as indicated each of the subhypotheses below, in the context of the three 
verb classes under study: 
? Lexical features capture argument structure differences between verb 
classes. 1 
? The linguistically distinctive features exhibit distributional differences 
across the verb classes that are apparent within linguistic experience (i.e., 
they can be collected from text). 
? The statistical distributions of (some of) the features contribute to 
learning the classifications of the verbs. 
In the following sections, we show that all three hypotheses above are borne out. In 
Section 2, we describe the argument structure distinctions of our three verb classes 
in more detail. In support of the first hypothesis above, we discuss lexical correlates 
of the underlying differences in thematic assignments hat distinguish the three verb 
classes under investigation. In Section 3, we show how to approximate these features 
by simple syntactic ounts, and how to perform these counts on available corpora. We 
confirm the second hypothesis above, by showing that the differences in distribution 
predicted by the underlying argument structures are largely found in the data. In 
Section 4, in a series of machine learning experiments and a detailed analysis of errors, 
we confirm the third hypothesis by showing that the differences in the distribution of 
the extracted features are successfully used for verb classification. Section 5 evaluates 
the significance of these results by comparing the program's accuracy to an expert- 
based upper bound. We conclude the paper with a discussion of its contributions, 
comparison to related work, and suggestions for future extensions. 
2. Deriving Classification Features from Argument Structure 
Our task is to automatically build a classifier that can distinguish the three major 
classes of optionally intransitive verbs in English. As described above, these classes 
are differentiated by their argument structures. In the first subsection below, we elab- 
orate on our description of the thematic role assignments for each of the verb classes 
under investigation--unergative, unaccusative, and object-drop. This analysis yields a 
distinctive pattern of thematic assignment for each class. (For more detailed iscussion 
concerning the linguistic properties of these classes, and the behavior of their compo- 
nent verbs, please see Stevenson and Merlo \[1997b\]; Merlo and Stevenson \[2000b\].) 
Of course, the key to any automatic classification task is to determine a set of useful 
features for discriminating the items to be classified. In the second subsection below, 
1 By lexical we mean features that we think are likely stored in the lexicon, because they are properties 
of words and not of phrases or sentences. Note, however, that some lexical features may not 
necessarily be stored with individual words--indeed, the motivation for classifying verbs to capture 
generalizations within each class uggests otherwise. 
376 
Merlo and Stevenson Statistical Verb Classification 
we show how the analysis of thematic distinctions enables us to determine lexical 
properties that we hypothesize will exhibit useful, detectable frequency differences 
in our corpora, and thus serve as the machine learning features for our classification 
experiments. 
2.1 The Argument Structure Distinctions 
The verb classes are exemplified below, in sentences repeated from Table 1 for ease of 
exposition. 
Unergative: (la) The horse raced past the barn. 
(lb) The jockey raced the horse past the barn. 
Unaccusative: (2a) The butter melted in the pan. 
(2b) The cook melted the butter in the pan. 
Object-Drop: (3a) The boy played. 
(3b) The boy played soccer. 
The example sentences illustrate that all three classes participate in a diathesis alter- 
nation that relates a transitive and intransitive form of the verb. However, according 
to Levin (1993), each class exhibits a different ype of diathesis alternation, which is 
determined by the particular semantic relations of the arguments othe verb. We make 
these distinctions explicit by drawing on a standard notion of thematic role, as each 
class has a distinct pattern of thematic assignments (i.e., different argument structures). 
We assume here that a thematic role is a label taken from a fixed inventory of 
grammaticalized semantic relations; for example, an Agent is the doer of an action, 
and a Theme is the entity undergoing an event (Gruber 1965). While admitting that 
such notions as Agent and Theme lack formal definitions (in our work and in the 
literature more widely), the distinctions are clear enough to discriminate our three 
verb classes. For our purposes, these roles can simply be thought of as semantic labels 
which are non-decomposable, but there is nothing in our approach that rests on this 
assumption. Thus, our approach would also be compatible with a feature-based deft- 
nition of participant roles, as long as the features capture such general distinctions as, 
for example, the doer of an action and the entity acted upon (Dowty 1991). 
Note that in our focus on verb class distinctions we have not considered finer- 
grained features that rely on more specific semantic features, such as, for example, 
that the subject of the intransitive melt must be something that can change from solid 
to liquid. While this type of feature may be important for semantic distinctions among 
individual verbs, it thus far seems irrelevant o the level of verb classification that 
we adopt, which groups verbs more broadly according to syntactic and (somewhat 
coarser-grained) semantic properties. 
Our analysis of thematic assignment--which was summarized in Table 2, repeated 
here as Table 3--is elaborated here for each verb class. The sentences in (1) above 
illustrate the relevant alternants ofan unergative verb, race. Unergatives are intransitive 
action verbs whose transitive form, as in (lb), can be the causative counterpart of the 
intransitive form (la). The type of causative alternation that unergatives participate in 
is the "induced action alternation" according to Levin (1993). For our thematic analysis, 
we note that the subject of an intransitive activity verb is specified to be an Agent. 
The subject of the transitive form is indicated by the label Agent of Causation, which 
indicates that the thematic role assigned to the subject is marked as the role which is 
377 
Computational Linguistics Volume 27, Number 3 
Table 3 
Summary of thematic assignments. 
Transitive Intransitive 
Classes Subject Object Subject 
Unergative Agent (of Causation) Agent Agent 
Unaccusative Agent (of Causation) Theme Theme 
Object-Drop Agent Theme Agent 
introduced with the causing event. In a causative alternation, the semantic argument 
of the subject of the intransitive surfaces as the object of the transitive (Brousseau and 
Ritter 1991; Hale and Keyser 1993; Levin 1993; Levin and Rappaport Hovav 1995). For 
unergatives, this argument is an Agent and thus the alternation yields an object in 
the transitive form that receives an Agent thematic role (Cruse 1972). These thematic 
assignments are shown in the first row of Table 3. 
The sentences in (2) illustrate the corresponding forms of an unaccusative verb, 
melt. Unaccusatives are intransitive change-of-state v rbs, as in (2a); the transitive 
counterpart for these verbs also exhibits a causative alternation, as in (2b). This is 
the "causative/inchoative alternation" (Levin, 1993). Like unergatives, the subject of 
a transitive unaccusative is marked as the Agent of Causation. Unlike unergatives, 
though, the alternating argument of an unaccusative (the subject of the intransitive 
form that becomes the object of the transitive) is an entity undergoing a change of 
state, without active participation, and is therefore a Theme. The resulting pattern of 
thematic assignments i indicated in the second row of Table 3. 
The sentences in (3) use an object-drop verb, play. These are activity verbs that 
exhibit a non-causative diathesis alternation, in which the object is simply optional. 
This is dubbed "the unexpressed object alternation" (Levin 1993), and has several 
subtypes that we do not distinguish ere. The thematic assignment for these verbs is 
simply Agent for the subject (in both transitive and intransitive forms), and Theme 
for the optional object; see the last row of Table 3. 
For further details and support of this analysis, please see the discussion in Steven- 
son and Merlo (1997b) and Merlo and Stevenson (2000b). For our purposes here, the 
important fact to note is that each of the three classes can be uniquely identified by 
the pattern of thematic assignments across the two alternants of the verbs. 
2.2 Features for Automatic Classification 
Our next task then is to derive, from these thematic patterns, useful features for au- 
tomatically classifying the verbs. In what follows, we refer to the columns of Table 3 
to explain how we expect he thematic distinctions to give rise to distributional prop- 
erties, which, when appropriately approximated through corpus counts, will discrim- 
inate across the three classes. 
Transitivity Consider the first two columns of thematic roles in Table 3, which illus- 
trate the role assignment in the transitive construction. The Prague school's notion 
of linguistic markedness (Jakobson 1971; Trubetzkoy 1939) enables us to establish a 
scale of markedness of these thematic assignments and make a principled prediction 
about their frequency of occurrence. Typical tests to determine the unmarked element 
of a pair or scale are simplicity--the unmarked element is simpler, distr ibution--the 
unmarked member is more widely attested across languages, and f requency--the un- 
378 
Merlo and Stevenson Statistical Verb Classification 
marked member is more frequent (Greenberg 1966; Moravcsik and Wirth 1983). The 
claim of markedness theory is that, once an element has been identified by one test 
as the unmarked element of a scale, then all other tests will be correlated. The three 
thematic assignments appear to be ranked on a scale by the simplicity and distribu- 
tion tests, as we describe below. From this, we can conclude that frequency, as a third 
correlated test, should also be ranked by the same scale, and we can therefore make 
predictions about the expected frequencies of the three thematic assignments. 
First, we note that the specification ofan Agent of Causation for transitive unerga- 
tives (such as race) and unaccusatives (such as melt) indicates a causative construction. 
Causative constructions relate two events, the causing event and the core event de- 
scribed by the intransitive verb; the Agent of Causation is the Agent of the causing 
event. This double event structure can be considered as more complex than the sin- 
gle event hat is found in a transitive object-drop verb (such as play) (Stevenson and 
Merlo 1997b). The simplicity test thus indicates that the causative unergatives and 
unaccusatives are marked in comparison to the transitive object-drop verbs. 
We further observe that the causative transitive of an unergative verb has an Agent 
thematic role in object position which is subordinated to the Agent of Causation in 
subject position, yielding an unusual "double agentive" thematic structure. This lex- 
ical causativization f unergatives (in contrast to analytic ausativization) is a distri- 
butionally rarer phenomenon--found i  fewer languages--than lexical causatives of 
unaccusatives. In asking native speakers about our verbs, we have found that lexical 
causatives of unergative verbs are not attested in Italian, French, German, Portuguese, 
Gungbe (Kwa family), and Czech. On the other hand, the lexical causatives are pos- 
sible for unaccusative rbs (i.e., where the object is a Theme) in all these languages. 
Vietnamese appears to allow a very restricted form of causativization f unergatives 
limited to only those cases that have a comitative reading. The typological distribu- 
tion test thus indicates that unergatives are more marked than unaccusatives in the 
transitive form. 
From these observations, we can conclude that unergatives ( uch as race) have 
the most marked transitive argument structure, unaccusatives ( uch as melt) have 
an intermediately marked transitive argument structure, and object-drops (such as 
play) have the least marked transitive argument structure of the three. Under the 
assumptions of markedness theory outlined above, we then predict hat unergatives 
are the least frequent in the transitive, that unaccusatives have intermediate frequency 
in the transitive, and that object-drop verbs are the most frequent in the transitive. 
Causativity Due to the causative alternation of unergatives and unaccusatives, the 
thematic role of the subject of the intransitive is identical to that of the object of the 
transitive, as shown in the second and third columns of thematic roles in Table 3. 
Given the identity of thematic role mapped to subject and object positions across the 
two alternants, we expect o observe the same noun occurring at times as subject of 
the verb, and at other times as object of the verb. In contrast, for object-drop verbs, 
the thematic role of the subject of the intransitive is identical to that of the subject of 
the transitive, not the object of the transitive. We therefore xpect hat it will be less 
common for the same noun to occur in subject and object position across instances of 
the same object-drop verb. 
Thus, we hypothesize that this pattern of thematic role assignments will be re- 
flected in a differential amount of usage across the classes of the same nouns as sub- 
jects and objects for a given verb. Generally, we would expect hat causative verbs 
(in our case, the unergative and unaccusative v rbs) would have a greater degree of 
overlap of nouns in subject and object position than non-causative transitive verbs (in 
379 
Computational Linguistics Volume 27, Number 3 
our case, the object-drop verbs). However, since the causative is a transitive use, and 
the transitive use of unergatives i expected to be rare (see above), we do not expect 
unergatives to exhibit a high degree of detectable overlap in a corpus. Thus, this over- 
lap of subjects and objects should primarily distinguish unaccusatives (predicted to 
have high overlap of subjects and objects) from the other two classes (each of which 
is predicted to have low \[detectable\] overlap of subjects and objects). 
Animacy Finally, considering the roles in the first and last columns of thematic assign- 
ments in Table 3, we observe that unergative and object-drop verbs assign an agentive 
role to their subject in both the transitive and intransitive, while unaccusatives a sign 
an agentive role to their subject only in the transitive. Under the assumption that the 
intransitive use of unaccusatives i  not rare, we then expect that unaccusatives will 
occur less often overall with an agentive subject han will the other two verb classes. 
(The assumption that unaccusatives are not rare in the intransitive is based on the 
linguistic complexity of the causative transitive alternant, and is borne out in our cor- 
pus analysis.) On the further assumption that Agents tend to be animate ntities more 
so than Themes are, we expect hat unaccusatives will occur less frequently with an 
animate subject compared to unergative and object-drop verbs. Note the importance 
of our use of frequency distributions: the claim is not that only Agents can be animate, 
but rather that nouns that receive an Agent role will more often be animate than nouns 
that receive a Theme role. 
Additional Features The above interactions between thematic roles and the syntactic 
expressions of arguments thus lead to three features whose distributional properties 
appear promising for distinguishing unergative, unaccusative and object-drop verbs: 
transitivity, causativity, and animacy of subject. We also investigate two additional syn- 
tactic features: the use of the passive or active voice, and the use of the past participle or 
simple past part-of-speech (POS) tag (VBN or VBD, in the Penn Treebank style). These 
features are related to the transitive/intransitive alternation, since a passive use implies 
a transitive use of the verb, as well as to the use of a past participle form of the verb .  2 
Table 4 summarizes the features we derive from the thematic properties, and our 
expectations concerning their frequency of use. We hypothesize that these five features 
will exhibit distributional differences in the observed usages of the verbs that can be 
used for classification. In the next section, we describe the actual corpus counts that we 
develop to approximate the features we have identified. (Notice that the counts will 
be imperfect approximations to the thematic knowledge, beyond the inevitable rrors 
due to automatic extraction from large automatically annotated corpora. Even when 
the counts are precise, they only constitute an approximation to the actual thematic 
notions, since the features we are using are not logically implied by the knowledge 
we want to capture, but only statistically correlated.) 
3. Data Collection and Analysis 
Clearly, some of the features we've proposed are difficult (e.g., the passive use) or im- 
possible (e.g., animate subject use) to automatically extract with high accuracy from a 
2 For our sample verbs, the statistical correlation between the transitive and passive features i highly 
significant (N ----- 59, R = .44, p =- .001), as is the correlation between the transitive and past participle 
features (N = 59, R = .36, p = .005). (Since, as explained in the next section, our features are expressed 
as proportions---e.g., percent transitive use out of detected transitive and intransitive use----correlations 
of intransitivity with passive or past participle use have the same magnitude but are negative.) 
380 
Merlo and Stevenson Statistical Verb Classification 
Table 4 
The features and expected behavior. 
Expected Frequency 
Feature Pattern Explanation 
Transitivity Unerg < Unacc < ObjDrop Unaccusatives and unergatives have a causative 
transitive, hence lower transitive use. Further- 
more, unergatives have an agentive object, hence 
very low transitive use. 
Causativity Unerg, ObjDrop < Unacc Object-drop verbs do not have a causal agent, 
hence low "causative" use. Unergatives are rare 
in the transitive, hence low causative use. 
Animacy Unacc < Unerg, ObjDrop Unaccusatives have a Theme subject in the in- 
transitive, hence lower use of animate subjects. 
Passive Voice Unerg K Unacc K ObjDrop Passive implies transitive use, hence correlated 
with transitive feature. 
VBN Tag Unerg < Unacc < ObjDrop Passive implies past participle use (VBN), hence 
correlated with transitive (and passive). 
large corpus, given the current state of annotation. However, we do assume that cur- 
rently available corpora, such as the Wall Street Journal (WSJ), provide a representative, 
and large enough, sample of language from which to gather corpus counts that can ap- 
proximate the distributional patterns of the verb class alternations. Our work draws on 
two text corpora--one an automatically tagged combined corpus of 65 million words 
(primarily WSJ), the second an automatically parsed corpus of 29 million words (a sub- 
set of the WSJ text from the first corpus). Using these corpora, we develop counting 
procedures that yield relative frequency distributions for approximations to the five 
linguistic features we have determined, over a sample of verbs from our three classes. 
3.1 Materials and Method 
We chose a set of 20 verbs from each class based primarily on the classification i  Levin 
(1993). 3 The complete list of verbs appears in Table 5; the group 1/group 2 designation 
is explained below in the section on counting. As indicated in the table, unergatives 
are manner-of-motion verbs (from the "run" class in Levin), unaccusatives are change- 
of-state verbs (from several of the classes in Levin's change-of-state super-class), while 
object-drop verbs were taken from a variety of classes in Levin's classification, all of 
which undergo the unexpressed object alternation. The most frequently used classes 
are verbs of change of possession, image-creation verbs, and verbs of creation and 
transformation. The selection of verbs was based partly on our intuitive judgment 
that the verbs were likely to be used with sufficient frequency in the WSJ. Also, each 
3 We used an equal number of verbs from each class in order to have a balanced group of items. One 
potential disadvantage of this decision is that each verb class is represented qually, even though they 
may not be equally frequent in the corpora. Although we lose the relative frequency information 
among the classes that could provide a better bias for assigning a default classification (i.e., the most 
frequent one), we have the advantage that our classifier will be equally informed (in terms of number 
of exemplars) about each class. 
Note that there are only 19 unaccusative rbs because ripped, which was initially counted in the 
unaccusatives, was then excluded from the analysis as it occurred mostly in a very different usage in 
the corpus (as verb+particle, in ripped off) from the intended optionally intransitive usage. 
381 
Computational Linguistics Volume 27, Number 3 
Table 5 
Verbs used in the experiments. 
Class Name Description Selected Verbs 
Unergative manner of motion jumped, rushed, marched, leaped, floated, raced, hurried, wan- 
dered, vaulted, paraded (group 1); galloped, glided, hiked, 
hopped, jogged, scooted, scurried, skipped, tiptoed, trotted 
(group 2). 
Unaccusative change of state opened, exploded, flooded, dissolved, cracked, hardened, boiled, 
melted, fractured, solidified (group 1); collapsed, cooled, 
folded, widened, changed, cleared, divided, simmered, stabi- 
lized (group 2). 
Object-Drop unexpressed 
object alternation 
played, painted, kicked, carved, reaped, washed, danced, 
yelled, typed, knitted (group 1); borrowed, inherited, orga- 
nized, rented, sketched, cleaned, packed, studied, swallowed, 
called (group 2). 
verb presents the same form in the simple past and in the past participle (the reg- 
ular "-ed" form). In order to simplify the counting procedure, we included only the 
"-ed" form of the verb, on the assumption that counts on this single verb form would 
approximate the distribution of the features across all forms of the verb. Additionally, 
as far as we were able given the preceding constraints, we selected verbs that could 
occur in the transitive and in the passive. Finally, we aimed for a frequency cut-off 
of 10 occurrences or more for each verb, although for unergatives we had to use one 
verb (jogged) that only occurred 8 times in order to have 20 verbs that satisfied the 
other criteria above. 
In performing this kind of corpus analysis, one has to recognize the fact that 
current corpus annotations do not distinguish verb senses. In these counts, we did 
not distinguish a core sense of the verb from an extended use of the verb. So, for 
instance, the sentence Consumer spending jumped 1.7% in February after a sharp drop the 
month before (WSJ 1987) is counted as an occurrence of the manner-of-motion verb jump 
in its intransitive form. This particular sense extension has a transitive alternant, but 
not a causative transitive (i.e., Consumer spending jumped the barrier . . . .  but not Low taxes 
jumped consumer spending... ). Thus, while the possible subcategorizations remain the 
same, rates of transitivity and causativity may be different han for the literal manner- 
of-motion sense. This is an unavoidable result of using simple, automatic extraction 
methods given the current state of annotation of corpora. 
For each occurrence of each verb, we counted whether it was in a transitive or 
intransitive use (TRANS), in a passive or active use (PASS), in a past participle or simple 
past use (VBN), in a causative or non-causative use (CAUS), and with an animate subject 
or not (ANIM). 4 Note that, except for the VBN feature, for which we simply extract he 
POS tag from the corpus, all other counts are approximations to the actual linguistic 
behaviour of the verb, as we describe in detail below. 
4 One additional feature was recorded--the log frequency ofthe verb in the 65 million word 
corpus--motivated by the conjecture that the frequency ofa verb may help in predicting its class. In 
our machine l arning experiments, however, this conjecture was not borne out, as the frequency feature 
did not improve performance. This is the case for experiments onall of the verbs, as well as for 
separate experiments onthe group 1 verbs (which were matched across the classes for frequency) and 
the group 2 verbs (which were not). We therefore limit discussion here to the thematically-motivated 
features. 
382 
Merlo and Stevenson Statistical Verb Classification 
The first three counts (TRANS, PASS, VBN) were performed on the tagged ACL/DCI  
corpus available from the Linguistic Data Consortium, which includes the Brown Cor- 
pus (of one million words) and years 1987-1989 of the Wall Street Journal, a combined 
corpus in excess of 65 million words. The counts for these features proceeded as fol- 
lows: 
? TRANS: A number, a pronoun, a determiner, an adjective, or a noun were 
considered to be indication of a potential object of the verb. A verb 
occurrence preceded by forms of the verb be, or immediately followed by 
a potential object was counted as transitive; otherwise, the occurrence 
was counted as intransitive (specifically, if the verb was followed by a 
punctuation s ign- -commas,  colons, full s tops- -or  by a conjunction, a
particle, a date, or a preposition.) 
* PASS: A main verb (i.e., tagged VBD) was counted as active. A token 
with tag VBN was also counted as active if the closest preceding 
auxiliary was have, while it was counted as passive if the closest 
preceding auxil iary was be. 
? VBN: The counts for VBN/VBD were simply done based on the POS 
label within the tagged corpus. 
Each of the above three counts was normalized over all occurrences of the "-ed" form 
of the verb, yielding a single relative frequency measure for each verb for that feature; 
i.e., percent ransitive (versus intransitive) use, percent active (versus passive) use, and 
percent VBN (versus VBD) use, respectively. 
The last two counts (CAUS and ANIM) were performed on a parsed version of the 
1988 year of the Wall Street Journal, so that we could extract subjects and objects of 
the verbs more accurately. This corpus of 29 million words was provided to us by 
Michael Collins, and was automatically parsed with the parser described in Collins 
(1997). 5The counts, and their justification, are described here: 
CAUS: As discussed above, the object of a causative transitive is the same 
semantic argument of the verb as the subject of the intransitive. The 
causative feature was approximated by the following steps, intended to 
capture the degree to which the subject of a verb can also occur as its 
object. Specifically, for each verb occurrence, the subject and object (if 
there was one) were extracted from the parsed corpus. The observed 
subjects across all occurrences of the verb were placed into one multiset 
of nouns, and the observed objects into a second multiset of nouns. (A 
multiset, or bag, was used so that our representation i dicated the 
number  of times each noun was used as either subject or object.) Then, 
the proport ion of overlap between the two multisets was calculated. We 
define overlap as the largest multiset of elements belonging to both the 
5 Readers might be concerned about he portability of this method to languages for which no large 
parsed corpus is available. It is possible that using a fully parsed corpus is not necessary. Our results 
were replicated in English without he need for a fully parsed corpus (Anoop Sarkar, p.c., citing a 
project report by Wootiporn Tripasai). Our method was applied to 23 million words of the WSJ that 
were automatically tagged with Ratnaparkhi's maximum entropy tagger (Ratnaparkhi 1996) and 
chunked with the partial parser CASS (Abney 1996). The results are very similar to ours (best accuracy 
66.6%), suggesting that a more accurate tagger than the one used on our corpus might in fact be 
sufficient to overcome the fact that no full parse is available. 
383 
Computational Linguistics Volume 27, Number 3 
subject and the object multisets; e.g., the overlap between (a, a, a, b} and 
{a} is {a,a,a}. The proportion is the ratio between the cardinality of the 
overlap multiset, and the sum of the cardinality of the subject and object 
multisets. For example, for the simple sets of characters above, the ratio 
would be 3/5, yielding a value of .60 for the CAUS feature. 
ANIM: A problem with a feature like animacy is that it requires either 
manual determination f the animacy of extracted subjects, or reference 
to an on-line resource such as WordNet for determining animacy. To 
approximate animacy with a feature that can be extracted automatically, 
and without reference to a resource xternal to the corpus, we take 
advantage of the well-attested animacy hierarchy, according to which 
pronouns are the most animate (Silverstein 1976; Dixon 1994). The 
hypothesis that the words I, we, you, she, he, and they most often refer 
to animate ntities. This hypothesis was confirmed by extracting 
100 occurrences of the pronoun they, which can be either animate or 
inanimate, from our 65 million word corpus. The occurrences 
immediately preceded a verb. After eliminating repetitions, 
94 occurrences were left, which were classified by hand, yielding 71 
animate pronouns, 11 inanimate pronouns and 12 unclassified 
occurrences (for lack of sufficient context o recover the antecedent of the 
pronoun with certainty). Thus, at least 76% of usages of they were 
animate; we assume the percentage of animate usages of the other 
pronouns to be even higher. Since the hypothesis was confirmed, we 
count pronouns (other than it) in subject position (Kariaeva \[1999\]; cf. 
Aone and McKee \[1996\]). The values for the feature were determined by 
automatically extracting all subject/verb tuples including our 59 example 
verbs from the parsed corpus, and computing the ratio of occurrences of
pronoun subjects to all subjects for each verb. 
Finally, as indicated in Table 5, the verbs are designated as belonging to "group 1" 
or "group 2". All the verbs are treated equally in our data analysis and in the machine 
learning experiments, but this designation does indicate a difference in details of the 
counting procedures described above. The verbs in group I had been used in an earlier 
study in which it was important to minimize noisy data (Stevenson and Merlo 1997a), 
so they generally underwent greater manual intervention i  the counts. In adding 
group 2 for the classification experiment, we chose to minimize the intervention i  
order to demonstrate hat the classification process is robust enough to withstand the 
resulting noise in the data. 
For group 2, the transitivity, voice, and VBN counts were done automatically with- 
out any manual intervention. For group 1, these three counts were done automatically 
by regular expression patterns, and then subjected to correction, partly by hand and 
partly automatically, by one of the authors. For transitivity, the adjustments vary for 
the individual verbs. Most of the reassignments from a transitive to an intransitive 
labelling occurred when the following noun was not the direct object but rather a 
measure phrase or a date. Most of the reassignments from intransitive to transitive 
occurred when a particle or a preposition following the verb did not introduce a prepo- 
sitional phrase, but instead indicated apassive form (by) or was part of a phrasal verb. 
Some verbs were mostly used adjectivally, in which case they were excluded from the 
transitivity counts. For voice, the required adjustments included cases of coordination 
of the past participle when the verb was preceded by a conjunction, or a comma. 
384 
Merlo and Stevenson Statistical Verb Classification 
Table 6 
Aggregated relative frequency data for the five features. E = unergatives, A = unaccusatives, 
O = object-drops. 
TRANS PASS VBN CAUS ANIM 
Class N Mean SD Mean SD Mean SD Mean SD Mean SD 
E 20 0.23 0.23 0.07 0.12 0.21 0.26 0.00 0.00 0.25 0.24 
A 19 0.40 0.24 0.33 0.27 0.65 0.27 0.12 0.14 0.07 0.09 
O 20 0.62 0.25 0.31 0.26 0.65 0.23 0.04 0.07 0.15 0.14 
These were collected and classified by hand as passive or active based on intuition. 
Similarly, partial adjustments to the VBN counts were made by hand. 
For the causativity feature, subjects and objects were determined by manual  in- 
spection of the corpus for verbs belonging to group 1, while they were extracted 
automatically from the parsed corpus for group 2. The group 1 verbs were sampled 
in three ways, depending on total frequency. For verbs with less than 150 occurrences, 
all instances of the verbs were used for subject/object extraction. For verbs whose 
total frequency was greater than 150, but whose VBD frequency was in the range 
100-200, we extracted subjects and objects of the VBD occurrences only. For higher 
frequency verbs, we used only the first 100 VBD occurrences. 6 The same script for 
computing the overlap of the extracted subjects and objects was then used on the 
resulting subject/verb and verb/object  tuples for both group 1 and group 2 verbs. 
The animacy feature was calculated over subject/verb tuples extracted automati-  
cally for both groups of verbs from the parsed corpus. 
3.2 Data Analysis 
The data collection described above yields the following data points in total: TRANS: 
27403; PASS: 20481; VBN: 36297; CAt;S: 11307; ANIM: 7542. (Different features yield differ- 
ent totals because they were sampled independently, and the search patterns to extract 
some features are more imprecise than others.) The aggregate means by class of the 
normalized frequencies for all verbs are shown in Table 6; item by item distributions 
are provided in Appendix A, and raw counts are available from the authors. Note 
that aggregate means are shown for illustration purposes only--al l  machine learning 
experiments are performed on the individual normalized frequencies for each verb, as 
given in Appendix A. 
The observed istributions of each feature are indeed roughly as expected accord- 
ing to the description in Section 2. Unergatives how a very low relative frequency of 
the TRANS feature, fol lowed by unaccusatives, then object-drop verbs. Unaccusative 
verbs show a high frequency of the CAUS feature and a low frequency of the ANIM fea- 
ture compared to the other classes. Somewhat unexpectedly, object-drop verbs exhibit 
a non-zero mean CAUS value (almost half the verbs have a CAUS value greater than 
zero), leading to a three-way causative distinction among the verb classes. We suspect 
that the approximation that we used for causative use- - the overlap between subjects 
6 For this last set of high-frequency verbs (exploded, jumped, opened, played, rushed), we used the first 
100 occurrences a the simplest way to collect he sample. In response to an anonymous reviewer's 
concern, we later verified that these counts were not different from counts obtained by random 
sampling of 100 VBD occurrences. A paired t-test of the two sets of counts (first 100 sampling and 
random sampling) indicates that the two sets of counts are not statistically different (t = 1.283, DF = 4, 
p = 0.2687). 
385 
Computational Linguistics Volume 27, Number 3 
Table 7 
Manually (Man) and automatically (Aut) calculated features for a random sample of verbs. 
T -~- TRANS,  P = PASS,  V ---- VBN,  C -~ CAUS,  A = ANIM.  
Unergative Unaccusative Object-Drop 
hopped scurried folded stabilized inherited swallowed 
Man Aut Man Aut Man Aut Man Aut Man Aut Man Aut 
T 0.21 0 .21  0.00 0.00 0 .71  0.23 0.24 0.18 1.00 0.64 0.96 0.35 
P 0.00 0.00 0.00 0.00 0.44 0.33 0.19 0.13 0.39 0.13 0.54 0.44 
V 0.03 0.00 0.10 0.00 0.56 0.73 0 .71  0.92 0.56 0.60 0.64 0.79 
C 0.00 0.00 0.00 0.00 0.54 0.00 0.24 0.35 0.00 0.06 0.00 0.04 
A 0.93 1.00 0.90 0.14 0.23 0.00 0.02 0.00 0.58 0.32 0.35 0.22 
and objects for a verb--also captures a "reciprocity" effect for some object-drop verbs 
(such as call), in which subjects and objects can be similar types of entities. Finally, 
although expected to be a redundant indicator of transitivity, PASS and VBN, unlike 
TRANS, have very similar values for unaccusative and object-drop verbs, indicating 
that their distributions are sensitive to factors we have not yet investigated. 
One issue we must address is how precisely the automatic ounts reflect he actual 
linguistic behaviour of the verbs. That is, we must be assured that the patterns we note 
in the data in Table 6 are accurate reflections of the differential behaviour of the verb 
classes, and not an artifact of the way in which we estimate the features, or a result of 
inaccuracies in the counts. In order to evaluate the accuracy of our feature counts, we 
selected two verbs from each class, and determined the "true" value of each feature 
for each of those six verbs through manual counting. The six verbs were randomly 
selected from the group 2 subset of the verbs, since counts for group 2 verbs (as 
explained above) had not undergone manual correction. This allows us to determine 
the accuracy of the fully automatic ounting procedures. The selected verbs (and their 
frequencies) are: hopped (29), scurried (21), folded (189), stabilized (286), inherited (357), 
swallowed (152). For verbs that had a frequency of over 100 in the "-ed" form, we 
performed the manual counts on the first 100 occurrences. 
Table 7 shows the results of the manual counts, reported as proportions to facil- 
itate comparison to the normalized automatic ounts, shown in adjoining columns. 
We observe first that, overall, most errors in the automatic ounts occur in the unac- 
cusative and object-drop verbs. While tagging errors affect the VBN feature for all of 
the verbs somewhat, we note that TP~ANS and PaSS are consistently underestimated for 
unaccusative and object-drop verbs. These errors make the unaccusative and object- 
drop feature values more similar to each other, and therefore potentially harder to 
distinguish. Furthermore, because the TRANS and PASS values are underestimated by
the automatic ounts, and therefore lower in value, they are also closer to the values 
for the unergative verbs. For the CAUS feature, we predict the highest values for the 
unaccusative verbs, and while that prediction is confirmed, the automatic ounts for 
that class also show the most errors. Finally, although the general pattern of higher 
values for the ANIM feature of unergatives and object-drop verbs is preserved in the 
automatic ounts, the feature is underestimated for almost all the verbs, again making 
the values for that feature closer across the classes than they are in reality. 
We conclude that, although there are inaccuracies in all the counts, the general 
patterns expected based on our analysis of the verb classes hold in both the manual 
and automatic ounts. Errors in the estimating and counting procedures are therefore 
386 
Merlo and Stevenson Statistical Verb Classification 
not likely to be responsible for the pattern of data in Table 6 above, which generally 
matches our predictions. Furthermore, the errors, at least for this random sample of 
verbs, occur in a direction that makes our task of distinguishing the classes more 
difficult, and indicates that developing more accurate search patterns may possibly 
sharpen the class distinctions, and improve the classification performance. 
4. Experiments in Classification 
In this section, we turn to our computational experiments that investigate whether the 
statistical indicators of thematic properties that we have developed can in fact be used 
to classify verbs. Recall that the task we have set ourselves is that of automatically 
learning the best class for a set of usages of a verb, as opposed to classifying individual 
occurrences of the verb. The frequency distributions of our features yield a vector for 
each verb that represents the estimated values for the verb on each dimension across 
the entire corpus: 
Vector template: \[verb-name, TRANS, PASS, VBN, CAUS, ANIM, class\] 
Example: \[opened, .69, .09, .21, .16, .36, unacc\] 
The resulting set of 59 vectors constitutes the data for our machine learning experi- 
ments. We use this data to train an automatic lassifier to determine, given the feature 
values for a new verb (not from the training set), which of the three major classes of 
English optionally intransitive verbs it belongs to. 
4.1 Experimental Methodology 
In pilot experiments on a subset of the features, we investigated a number  of su- 
pervised machine learning methods that produce automatic lassifiers (decision tree 
induction, rule learning, and two types of neural networks), as well as hierarchi- 
cal clustering; see Stevenson et al (1999) for more detail. Because we achieved ap- 
proximately the same level of performance in all cases, we narrowed our further 
experimentation to the publicly available version of the C5.0 machine learning system 
(http: / /www.rulequest.com),  a newer version of C4.5 (Quinlan 1992), due to its ease 
of use and wide availability. The C5.0 system generates both decision trees and cor- 
responding rule sets from a training set of known classifications. In our experiments, 
we found little to no difference in performance between the trees and rule sets, and 
report only the rule set results. 
In the experiments below, we follow two methodologies in training and testing, 
each of which tests a subset of cases held out from the training data. Thus, in all cases, 
the results we report are on test data that was never seen in training. 7
The first training and testing methodology we follow is 10-fold cross-validation. In
this approach, the system randomly divides the data into ten parts, and runs ten times 
on a different 90%-training-data/10%-test-data split,yielding an average accuracy and 
standard error across the ten test sets. This training methodology is very useful for 
7 One anonymous reviewer aised the concern that we do not test on verbs that were unseen by the 
authors prior to finalizing the specific features to count. However, this does not reduce the generality 
of our results. The features we use are motivated by linguistic theory, and derived from the set of 
thematic properties that discriminate the verb classes. It is therefore very unlikely that they are skewed 
to the particular verbs we have chosen. Furthermore, our cross-validation experiments, described in the 
next subsection, show that our results hold across a very large number of randomly selected subsets of 
this sample of verbs. 
387 
Computational Linguistics Volume 27, Number 3 
our application, as it yields performance measures across a large number of training 
data/test data sets, avoiding the problems of outliers in a single random selection 
from a relatively small data set such as ours. 
The second methodology is a single hold-out raining and testing approach. Here, 
the system is run N times, where N is the size of the data set (i.e., the 59 verbs in 
our case), each time holding out a single data vector as the test case and using the 
remaining N-1 vectors as the training set. The single hold-out methodology yields an 
overall accuracy rate (when the results are averaged across all N trials), but also-- 
unlike cross-validation--gives us classification results on each individual data vector. 
This property enables us to analyze differential performance on the individual verbs 
and across the different verb classes. 
Under both training and testing methodologies, the baseline (chance) performance 
in this task--a three-way classification--is 33.9%. In the single hold-out methodology, 
there are 59 test cases, with 20, 19, and 20 verbs each from the unergative, unaccusative, 
and object-drop classes, respectively. Chance performance ofpicking a single class label 
as a default and assigning it to all cases would yield at most 20 out of the 59 cases 
correct, or 33.9%. For the cross-validation methodology, the determination f a baseline 
is slightly more complex, as we are testing on a random selection of 10% of the full 
data set in each run. The 33.9% figure represents he expected relative proportion of a 
test set that would be labelled correctly by assignment of a default class label to the 
entire test set. Although the precise make-up of the test cases vary, on average the test 
set will represent the class membership proportions of the entire set of verbs. Thus, 
as with the single hold-out approach, chance accuracy corresponds to a maximum of 
20/59, or 33.9%, of the test set being labelled correctly. 
The theoretical maximum accuracy for the task is, of course, 100%, although in 
Section 5 we discuss ome classification results from human experts that indicate that 
a more realistic expectation is much lower (around 87%). 
4.2 Results Using 10-Fold Cross-Validation 
We first report he results of experiments u ing a training methodology of 10-fold cross- 
validation repeated 50 times. This means that the 10-fold cross-validation procedure is
repeated for 50 different random divisions of the data. The numbers reported are the 
averages of the results over all the trials. That is, the average accuracy and standard 
error from each random division of the data (a single cross-validation run including 
10 training and test sets) are averaged across the 50 different random divisions. This 
large number of experimental trials gives us a very tight bound on the mean accuracy 
reported, enabling us to determine with high confidence the statistical significance of 
differences in results. 
Table 8 shows that performance of classification using individual features varies 
greatly, from little above the baseline to almost 22% above the baseline, or a reduction 
of a third of the error rate, a very good result for a single feature. (All reported 
accuracies in Table 8 are statistically distinct, at the p < .01 level, using an ANOVA 
\[dr = 249, F = 334.72\], with a Tukey-Kramer post test.) 
The first line of Table 9 shows that the combination of all features achieves an 
accuracy of 69.8%, which is 35.9% over the baseline, for a reduction in the error rate of 
54%. This is a rather considerable r sult, given the very low baseline (33.9%). Moreover, 
recall that our training and testing sets are always disjoint (cf., Lapata and Brew \[1999\]; 
Siegel \[1999\]); in other words, we are predicting the classification of verbs that were 
never seen in the training corpus, the hardest situation for a classification algorithm. 
The second through sixth lines of Table 9 show the accuracy achieved on each 
subset of features that results from removing asingle feature. This allows us to evaluate 
388 
Merlo and Stevenson Statistical Verb Classification 
Table 8 
Percent accuracy and standard error of the verb classification task using each feature 
individually, under a training methodology of 10-fold cross-validation repeated 50 times. 
Feature %Accuracy %SE 
CAUS 55.7 .1 
VBN 52.5 .5 
PASS 50.2 .5 
TRANS 47.1 .4 
ANIM 35.3 .5 
Table 9 
Percent accuracy and standard error of the verb classification task using features in 
combination, under a training methodology of 10-fold cross-validation repeated 50 times. 
Feature 
Features Used Not Used %Accuracy %SE 
1. TRANS PASS VBN CAUS ANIM 69.8 .5 
2. TRANS VBN CAUS ANIM PASS 69.8 .5 
3. TRANS PASS VBN ANIM CAUS 67.3 .6 
4. TRANS PASS CAUS ANIM VBN 66.5 .5 
5. TRANS PASS VBN CAUS ANIM 63.2 .6 
6. PASS VBN CAUS ANIM TRANS 61 .6  .6 
the contribution of each feature to the performance of the classification process, by 
comparing the performance of the subset without it, to the performance using the full 
set of features. We see that the removal of PASS (second line) has no effect on the results, 
while removal of the remaining features yields a 2-8% decrease in performance. (In 
Table 9, the differences between all reported accuracies are statistically significant, at 
the p < .05 level, except for between lines 1 and 2, lines 3 and 4, and lines 5 and 6, 
using an ANOVA \[dr = 299, F = 37.52\], with a Tukey-Kramer post test.) We observe 
that the behavior of the features in combination cannot be predicted by the individual 
feature behavior. For example, CAUS, which is the best individually, does not greatly 
affect accuracy when combined with the other features (compare line 3 to line 1). 
Conversely, ANIM and TRANS, which do not classify verbs accurately when used alone, 
are the most relevant in a combination of features (compare lines 5 and 6 to line 1). We 
conclude that experimentation with combinations of features is required to determine 
the relevance of individual features to the classification task. 
The general behaviour in classification based on individual features and on size 
4 and size 5 subsets of features is confirmed for all subsets. Appendix B reports the 
results for all subsets of feature combinations, in order of decreasing performance. 
Table 10 summarizes this information. In the first data column, the table illustrates 
the average accuracy across all subsets of each size. The second through sixth data 
columns report the average accuracy of all the size n subsets in which each feature 
occurs. For example, the second data cell in the second row (54.9) indicates the average 
accuracy of all subsets of size 2 that contain the feature VBN. The last row of the 
table indicates the average accuracy for each feature of all subsets containing that 
feature. 
389 
Computational Linguistics Volume 27, Number 3 
Table 10 
Average percent accuracy of feature subsets, by subset size and by sets of each size including 
each feature. 
Mean Accuracy of Subsets 
Subset Mean Accuracy that Include Each Feature 
Size by Subset Size VBN PASS TRANS ANIM CAUS 
1 48.2 52.5 50.2 47.1 35.3 55.7 
2 55.1 54.9 52.8 56.4 58.0 57.6 
3 60.5 60.1 58.5 62.3 61.1 60.5 
4 65.7 65.5 64.7 66.7 66.3 65.3 
5 69.8 69.8 69.8 69.8 69.8 69.8 
Mean Acc/Feature: 60.6 59.2 60.5 58.1 61.8 
The first observation--that more features perform better-- is confirmed overall, 
in all subsets. Looking at the first data column of Table 10, we can observe that, on 
average, larger sets of features perform better than smaller sets. Furthermore, as can be 
seen in the following individual feature columns, individual features perform better in 
a bigger set than in a smaller set, without exception. The second observation--that the 
performance of individual features is not always a predictor of their performance in 
combination-- is confirmed by comparing the average performance of each feature in 
subsets of different sizes to the average across all subsets of each size. We can observe, 
for instance, that the feature CAUS, which performs very well alone, is average in 
feature combinations of size 3 or 4. By contrast, the feature ANIM, which is the worst 
if used alone, is very effective in combination, with above average performance for all 
subsets of size 2 or greater. 
4.3 Results Using Single Hold-Out Methodology 
One of the disadvantages of the cross-validation training methodology, which aver- 
ages performance across a large number of random test sets, is that we do not have 
performance data for each verb, nor for each class of verbs. In another set of experi- 
ments, we used the same C5.0 system, but employed a single hold-out training and 
testing methodology. In this approach, we hold out a single verb vector as the test case, 
and train the system on the remaining 58 cases. We then test the resulting classifier on 
the single hold-out case, and record the assigned class for that verb. This procedure 
is repeated for each of the 59 verbs. As noted above, the single hold-out methodology 
has the benefit of yielding both classification results on each individual verb, and an 
overall accuracy rate (the average results across all 59 trials). Moreover, the results on 
individual verbs provide the data necessary for determining accuracy for each verb 
class. This allows us to determine the contribution of individual features as above, but 
with reference to their effect on the performance of individual classes. This is impor- 
tant, as it enables us to evaluate our hypotheses concerning the relation between the 
thematic features and verb class distinctions, which we turn to in Section 4.4. 
We performed single hold-out experiments on the full set of features, as well as on 
each subset of features with a single feature removed. The first line of Table 11 shows 
that the overall accuracy for all five features is almost exactly the same as that achieved 
with the 10-fold cross-validation methodology (69.5% versus 69.8%). As with the cross- 
validation results, the removal of PASS does not degrade performance-- in fact, here its 
removal appears to improve performance (see line 2 of Table 11). However, it should 
be noted that this increase in performance results from one additional verb being 
390 
Merlo and Stevenson Statistical Verb Classification 
Table 11 
Percent accuracy of the verb classification task using features in combination, under a single 
hold-out raining methodology. 
Feature %Accuracy 
Features Used Not Used on All Verbs 
1. TRANS PASS VBN CAUS ANIM 69.5 
2. TRANS VBN CAUS ANIM PASS 71.2 
3. TRANS PASS VBN ANIM CAUS 62.7 
4. TRANS PASS CAUS AN1M VBN 61.0 
5. TRANS PASS VBN CAUS ANIM 61.0 
6. PASS VBN CAUS ANIM TRANS 64.4 
Table 12 
F score of classification within each class, under a single hold-out raining methodology. 
Feature F score (%) F score (%) F score (%) 
Features Used Not Used for Unergs for Unaccs for Objdrops 
1. TRANS PASS VBN CAUS ANIM 73.9 68.6 
2. TRANS VBN CAUS ANIM PASS 76.2 75.7 
3. TRANS PASS VBN ANIM CAUS 65.1 60 .0  
4. TRANS PASS CAUS ANIM VBN 66 .7  65 .0  
5. TRANS PASS VBN CAUS AN1M 72.7 47.0 
6. PASS VBN CAUS ANIM TRANS 78.1 51.5  
64.9 
61.6 
62.8 
51.3 
60.0 
61.9 
classified correctly. The remaining lines of Table 11 show that the removal of any other 
feature has a 5-8% negative ffect on performance, again similar to the cross-validation 
results. (Although note that the precise accuracy achieved is not the same in each case 
as with 10-fold cross-validation, indicating that there is some sensitivity to the precise 
make-up of the training set when using a subset of the features.) 
Table 12 presents the results of the single hold-out experiments in terms of per- 
formance within each class, using an F measure with balanced precision and recall. 8 
The first line of the table shows clearly that, using all five features, the unergatives 
are classified with greater accuracy (F = 73.9%) than the unaccusative and object-drop 
verbs (F scores of 68.6% and 64.9%, respectively). The features appear to be better 
at distinguishing unergatives than the other two verb classes. The remaining lines of 
Table 12 show that this pattern holds for all of the subsets of features as well. Clearly, 
future work on our verb classification task will need to focus on determining features 
that better discriminate unaccusative and object-drop verbs. 
One potential explanation that we can exclude is that the pattern of results is due 
simply to the frequencies of the verbs--that is, that more frequent verbs are more ac- 
curately classified. We examined the relation between classification accuracy and log 
8 For all previous results, we reported an accuracy measure (the percentage of correct classifications out 
of all classifications). Using the terminology oftrue or false positives/negatives, thisis the same as 
truePositives/(truePositives + fal eNegafives). In the earlier esults, there are no falsePositives or 
trueNegatives, since we are only considering for each verb whether it is correctly classified 
(truePositive) ornot (falseNegative). However, when we turn to analyzing the data for each class, the 
possibility arises of having falsePositives and trueNegatives for that class. Hence, here we use the 
balanced F score, which calculates an overall measure of performance as2PR/(P + R), in which P 
(precision) is truePositives/(truePositives + fal ePositives), and R (recall) is 
truePositives/(truePositives + fal eNegatives). 
391 
Computational Linguistics Volume 27, Number 3 
frequencies of the verbs, both by class and individually. By class, unergatives have 
the lowest average log frequency (1.8), but are the best classified, while unaccusatives 
and object-drops are comparable (average log frequency = 2.4). If we group individ- 
ual verbs by frequency, the proportion of errors to the total number of verbs is not 
linearly related to frequency (log frequency K 2:7  errors/24 verbs, or 29% error; log 
frequency between 2 and 3 :7  errors/25 verbs, or 28% error; log frequency > 3 :4  
errors/10 verbs, or 40% error). Moreover, it seems that the highest-frequency verbs 
pose the most problems to the program. In addition, the only verb of log frequency 
K 1 is correctly classified, while the only one with log frequency > 4 is not. In con- 
clusion, we do not find that there is a simple mapping from frequency to accuracy. In 
particular, it is not the case that more frequent classes or verbs are more accurately 
classified. 
One factor possibly contributing to the poorer performance on unaccusatives and 
object-drops i  the greater degree of error in the automatic ounting procedures for 
these verbs, which we discussed in Section 3.2. In addition to exploration of other 
linguistic features, another area of future work is to develop better search patterns, for 
transitivity and passive in particular. Unfortunately, one limiting factor in automatic 
counting is that we inherit the inevitable rrors in POS tags in an automatically tagged 
corpus. For example, while the unergative verbs are classified highly accurately, we 
note that two of the three errors in misclassifying unergatives (galloped and paraded) are 
due to a high degree of error in tagging. 9 The verb galloped is incorrectly tagged VBN 
instead of VBD in all 12 of its uses in the corpus, and the verb paraded is incorrectly 
tagged VBN instead of VBD in 13 of its 33 uses in the corpus. After correcting only 
the VBN feature of these two verbs to reflect he actual part of speech, overall accuracy 
in classification increases by almost 10%, illustrating the importance of both accurate 
counts and accurate annotation of the corpora. 
4.4 Contribution of the Features to Classification 
We can further use the single hold-out results to determine the contribution of each 
feature to accuracy within each class. We do this by comparing the class labels as- 
signed using the full set of five features (TRANS, PASS, VBN, CAUS, ANIM) with the class 
labels assigned using each size 4 subset of features. The difference in classifications 
between each four-feature subset and the full set of features indicates the changes in 
class labels that we can attribute to the added feature in going from the four-feature 
to five-feature set. Thus, we can see whether the features indeed contribute to dis- 
criminating the classes in the manner predicted in Section 2.2, and summarized here 
in Table 13. 
We illustrate the data with a set of confusion matrices, in Tables 14 and 15, which 
show the pattern of errors according to class label for each set of features. In each 
confusion matrix, the rows indicate the actual class of incorrectly classified verbs, and 
the columns indicate the assigned class. For example, the first row of the first panel 
of Table 14 shows that one unergative was incorrectly labelled as unaccusative, and 
two unergatives as object-drop. To determine the confusability of any two classes (the 
9 The third error in classification f unergatives is the verb floated, which we conjecture is due not to 
counting errors, but to the linguistic properties of the verb itself. The verb is unusual for a 
manner-of-motion verb in that the action is inherently "uncontrolled", and thus the subject of the 
intransitive/object of he transitive isa more passive ntity than with the other unergatives (perhaps 
indicating that the inventory of thematic roles should be refined to distinguish activity verbs with less 
agentive subjects). We think that this property relates to the notion of internal and external causation 
that is an important factor in distinguishing unergative and unaccusative rbs. We refer the interested 
reader to Stevenson and Merlo (1997b), which discusses the latter issue in more detail. 
392 
Merlo and Stevenson Statistical Verb Classification 
Table 13 
Expected class discriminations for each feature. 
Feature Expected Frequency Pattern 
Transitivity Unerg < Unacc < ObjDrop 
Causativity Unerg, ObjDrop < Unacc 
Animacy Unacc < Unerg, ObjDrop 
Passive Voice Unerg < Unacc < ObjDrop 
VBN Tag Unerg < Unacc < ObjDrop 
Table 14 
Confusion matrix indicating number of errors in classification by verb class, for the full set of 
five features, compared to two of the four-feature sets. E = unergatives, A = unaccusatives, 
O = object-drops. 
Assigned Class 
All features w/o CAUS W/O ANIM 
E A O E A O E A O 
Actual E 1 2 4 2 2 2 
Class A 4 3 5 2 5 6 
O 5 3 4 5 3 5 
Table 15 
Confusion matrix indicating number of errors in classification by verb class, for the full set of 
five features and for three of the four-feature sets. E = unergatives, A = unaccusatives, 
O = object-drops. 
Assigned Class 
All features w/o TRANS W/O PASS w/o  VBN 
E A O E A O E A O E A O 
Actual E 1 2 2 2 1 3 1 5 
Class A 4 3 3 7 1 4 2 4 
O 5 3 2 5 5 3 4 6 
opposite of discriminability), we look at two cells in the matrix: the one in which 
verbs of the first class were assigned the label of the second class, and the one in 
which verbs of the second class were assigned the label of the first class. (These pairs 
of cells are those opposite the diagonal of the confusion matrix.) By examining the 
decrease (or increase) in confusability of each pair of classes in going from a four- 
feature experiment to the five-feature xperiment, we gain insight into how well (or 
how poorly) the added feature helps to discriminate ach pair of classes. 
An analysis of the confusion matrices reveals that the behavior of the features 
largely conforms to our linguistic predictions, leading us to conclude that the features 
393 
Computational Linguistics Volume 27, Number 3 
we counted worked largely for the reasons we had hypothesized. We expected CAUS 
and ANIM to be particularly helpful in identifying unaccusatives, and these predictions 
are confirmed. Compare the second to the first panel of Table 14 (the errors without 
the CAUS feature compared to the errors with the ?AUS feature added to the set). 
We see that, without the CAUS feature, the confusability between unaecusatives and 
unergatives, and between unaccusatives and object-drops, is 9 and 7 errors, respec- 
tively; but when CAUS is added to the set of features, the confusability between these 
pairs of classes drops substantially, to5 and 6 errors, respectively. On the other hand, 
the confusability between unergatives and object-drops becomes lightly worse (errors 
increasing from 6 to 7). The latter indicates that the improvement in unaccusatives is 
not simply due to an across-the-board improvement in accuracy as a result of having 
more features. We see a similar pattern with the ANIM feature. Comparing the third 
to the first panel of Table 14 (the errors without the ANIM feature compared to the 
errors with the ANIM feature added to the set), we see an even larger improvement in
discriminability of unaccusatives when the ANIM feature is added. The confusability 
of unaccusatives and unergatives drops from 7 errors to 5 errors, and of unaccusatives 
and object-drops from 11 errors to 6 errors. Again, confusability of unergatives and 
object-drops i worse, with an increase in errors of 5 to 7. 
We had predicted that the TRANS feature would make a three-way distinction 
among the verb classes, based on its predicted linear relationship between the classes 
(see the inequalities in Table 13). We had further expected that PASS and VBN would 
behave similarly, since these features are correlated to TRANS. To make a three-way dis- 
tinction among the verb classes, we would expect confusability between all three pairs 
of verb classes to decrease (i.e., discriminability would improve) with the addition of 
TRANS, PASS, or VBN. We find that these predictions are confirmed in part. 
First consider the TRANS feature. Comparing the second to the first panel of Ta- 
ble 15, we find that unergatives are already accurately classified, and the addition of 
TRANS to the set does indeed greatly reduce the confusability of unaccusatives and 
object-drops, with the number of errors dropping from 12 to 6. However, we also 
observe that the confusability of unergatives and unaccusatives is not improved, and 
the confusability of unergatives and object-drops i worsened with the addition of 
the TRANS feature, with errors in the latter case increasing from 4 to 7. We conclude 
that the expected three-way discriminability of TRANS is most apparent in the reduced 
confusion of unaccusative and object-drop verbs. 
Our initial prediction was that PASS and VBN would behave similarly to TRANS-- 
that is, also making a three-way distinction among the classes--although the aggregate 
data revealed little difference in these feature values between unaccusatives and object- 
drops. Comparing the third to the first panel of Table 15, we observe that the addition 
of the PAss feature hinders the discriminability of unergatives and unaccusatives (in- 
creasing errors from 2 to 5); it does help in discriminating the other pairs of classes, 
but only slightly (reducing the number of errors by 1 in each case). The VBN fea- 
ture shows a similar pattern, but is much more helpful at distinguishing unergatives 
from object-drops, and object-drops from unaccusatives. In comparing the fourth to 
the first panel of Table 15, we find that the confusability of unergatives and object- 
drops is reduced from 9 errors to 7, and of unaccusatives and object-drops from 10 
errors to 6. The latter result is somewhat surprising, since the aggregate VBN data 
for the unaccusative and object-drop classes are virtually identical. We conclude that 
contribution of a feature to classification is not predictable from the apparent dis- 
criminability of its numeric values across the classes. This observation emphasizes the 
importance of an experimental method to evaluating our approach to verb classifica- 
tion. 
394 
Merlo and Stevenson Statistical Verb Classification 
Table 16 
Percent agreement (%Agr) and pair-wise agreement (K) of three experts (El, E2, E3) and the 
program compared to each other and to a gold standard (Levin). 
PROGRAM E1 E2 E3 
%Agr K %Agr K %Agr K %Agr K 
E1 59% .36 
E2 68% .50 75% .59 
E3 66% .49 70% .53 77% .66 
LEVIN 69.5% .54 71% .56 86.5% .80 83% .74 
5. Establishing the Upper Bound for the Task 
In order to evaluate the performance of the algorithm in practice, we need to compare 
it to the accuracy of classification performed by an expert, which gives a realistic upper 
bound for the task. The lively theoretical debate on class membership of verbs, and the 
complex nature of the linguistic information ecessary to accomplish this task, led us to 
believe that the task is difficult and not likely to be performed at 100% accuracy even 
by experts, and is also likely to show differences in classification between experts. 
We report here the results of two experiments which measure expert accuracy in 
classifying our verbs (compared to Levin's classification as the gold standard), as well 
as inter-expert agreement. (See also Merlo and Stevenson \[2000a\] for more details.) 
To enable comparison of responses, we performed a closed-form questionnaire study, 
where the number  and types of the target classes are defined in advance, for which 
we prepared a forced-choice and a non-forced-choice variant. The forced-choice study 
provides data for a maximal ly restricted experimental situation, which corresponds 
most closely to the automatic verb classification task. However,  we are also interested 
in slightly more natural results- -provided by the non-forced-choice task- -where the 
experts can assign the verbs to an "others" category. 
We asked three experts in lexical semantics (all native speakers of English) to 
complete the forced-choice lectronic questionnaire study. Neither author was among 
the three experts, who were all professionals in computational or theoretical linguistics 
with a specialty in lexical semantics. Materials consisted of individually randomized 
lists of the same 59 verbs used for the machine learning experiments, using Levin's 
(1993) electronic index, available from Chicago University Press. The verbs were to 
be classified into the three target classes--unergative, unaccusative, and object-drop--  
which were described in the instructions. 1?(All materials and instructions are available 
at URL http: / /www.lat l .unige.ch/ lat l /personal /paola.html. )  
Table 16 shows an analysis of the results, reporting both percent agreement and 
pairwise agreement (according to the Kappa statistic) among the experts and the 
program. ~1 Assessing the percentage of verbs on which the experts agree gives us 
10 The definitions of the classes were as follows. Unergative: A verb that assigns an agent heta role to the 
subject in the intransitive. If it is able to occur transitively, it can have a causative meaning. 
Unaccusative: A verb that assigns apatient/theme theta role to the subject in the intransitive. When it 
occurs transitively, it has a causative meaning. Object-Drop: A verb that assigns an agent role to the 
subject and patient/theme role to the object, which is optional. When it occurs transitively, it does not 
have a causative meaning. 
11 In the comparison of the program to the experts, we use the results of the classifier under single 
hold-out raining--which yields an accuracy of 69.5%--because those results provide the classification 
for each of the individual verbs. 
395 
Computational Linguistics Volume 27, Number 3 
an intuitive measure. However, this measure does not take into account how much 
the experts agree over  the expected agreement by chance. The latter is provided by 
the Kappa statistic, which we calculated following Klauer (1987, 55-57) (using the z 
distribution to determine significance; p ~ 0.001 for all reported results). The Kappa 
value measures the experts', and our classifier's, degree of agreement over chance, 
with the gold standard and with each other. Expected chance agreement varies with 
the number and the relative proportions of categories used by the experts. This means 
that two given pairs of experts might reach the same percent agreement on a given 
task, but not have the same expected chance agreement, if they assigned verbs to 
classes in different proportions. The Kappa statistic ranges from 0, for no agreement 
above chance, to 1, for perfect agreement. The interpretation of the scale of agreement 
depends on the domain, like all correlations. Carletta (1996) cites the convention from 
the domain of content analysis indicating that .67 K K < .8 indicates marginal agree- 
ment, while K > .8 is an indication of good agreement. We can observe that only one 
of our agreement figures comes close to reaching what would be considered "good" 
under this interpretation. Given the very high level of expertise of our human experts, 
we suspect hen that this is too stringent a scale for our task, which is qualitatively 
quite different from content analysis. 
Evaluating the experts' performance summarized in Table 16, we can remark two 
things, which confirm our expectations. First, the task is difficult--i.e., not performed 
at 100% (or close) even by trained experts, when compared to the gold standard, with 
the highest percent agreement with Levin at 86.5%. Second, with respect to comparison 
of the experts among themselves, the rate of agreement is never very high, and the 
variability in agreement is considerable, ranging from .53 to .66. This evaluation is 
also supported by a 3-way agreement measure (Siegel and Castellan 1988). Applying 
this calculation, we find that the percentage of verbs to which the three experts gave 
the same classification (60%, K = 0.6) is smaller than any of the pairwise agreements, 
indicating that the experts do not all agree on the same subset of verbs. 
The observation that the experts often disagree on this difficult task suggests that 
a combination of expert judgments might increase the upper bound. We tried the 
simplest combination, by creating a new classification using a majority vote: each 
verb was assigned the label given by at least two experts. Only three cases did not 
have any majority label; in these cases we used the classification of the most accurate 
expert. This new classification does not improve the upper bound, reaching only 86.4% 
(K = .80) compared to the gold standard. 
The evaluation is also informative with respect to the performance of the program. 
On the one hand, we observe that if we take the best performance achieved by an 
expert in this task---86.5%--as the maximum achievable accuracy in classification, 
our algorithm then reduces the error rate over chance by approximately 68%, a very 
respectable r sult. In fact, the accuracy of 69.5% achieved by the program is only 1.5% 
less than one of the human experts in comparison to the gold standard. On the other 
hand, the algorithm still does not perform at expert level, as indicated by the fact that, 
for all experts, the lowest agreement score is with the program. 
One interesting question is whether experts and program disagree on the same 
verbs, and show similar patterns of errors. The program makes 18 errors, in total, com- 
pared to the gold standard. However, in 9 cases, at least one expert agrees with the 
classification given by the program. The program makes fewer errors on unergatives 
(3) and comparably many on unaccusatives and object-drops (7 and 8 respectively), in- 
dicating that members of the latter two classes are quite difficult to classify. This differs 
from the pattern of average agreement between the experts and Levin, who agree on 
17.7 (of 20) unergatives, 16.7 (of 19) unaccusatives, and 11.3 (of 20) object-drops. This 
396 
Merlo and Stevenson Statistical Verb Classification 
clearly indicates that the object-drop class is the most difficult for the human experts 
to define. This class is the most heterogeneous in our verb list, consisting of verbs 
from several subclasses of the "unexpressed object alternation" class in (Levin, 1993). 
We conclude that the verb classification task is likely easier for very homogeneous 
classes, and more difficult for more broadly defined classes, even when the exemplars 
share the critical syntactic behaviors. 
On the other hand, frequency does not appear to be a simple factor in explaining 
patterns of agreement between experts, or increases in accuracy. As in Section 4.3, 
we again analyze the relation between log frequency of the verbs and classification 
performance, here considering the performance of the experts. We grouped verbs in 
three log frequency classes: verbs with log frequency less than 2 (i.e., frequency less 
than 100), those with log frequency between 2 and 3 (i.e., frequency between 100 
and 1000), and those with log frequency over 3 (i.e., frequency over 1000). The low- 
frequency group had 24 verbs (14 unergatives, 5 unaccusatives, and 5 object-drop), 
the intermediate-frequency group had 25 verbs (5 unergatives, 9 unaccusatives, and 
11 object-drops), and the high-frequency group had 10 verbs (1 unergative, 5 unac- 
cusatives, and 4 object-drops). We found that verbs with high and low frequency ield 
better accuracy and agreement among the experts than the verbs with mid frequency. 
Neither the accuracy of the majority classification, or the accuracy of the expert hat 
had the best agreement with Levin, were linearly affected by frequency. For the ma- 
jority vote, verbs with frequency less than 100 yield an accuracy of 92%, K = .84; 
verbs with frequency between 100 and 1000, accuracy 80%, K = .69; and for verbs 
with frequency over 1000, accuracy 90%, K = .82. For the "best" expert, the pattern 
is similar: verbs with frequency less than 100 yield an accuracy of 87.5%, K = .74; 
verbs with frequency between 100 and 1000, accuracy 84%, K = .76; and verbs with 
frequency over 1000, accuracy 90%, K = .82. 
We can see here that different frequency groups yield different classification be- 
havior. However, the relation is not simple, and it is clearly affected by the composition 
of the frequency group: the middle group contains mostly unaccusative and object- 
drop verbs, which are the verbs with which our experts have the most difficulty. This 
confirms that the class of the verb is the predominant factor in their pattern of errors. 
Note also that the pattern of accuracy across frequency groupings is not the same as 
that of the program (see Section 4.3, which revealed the most errors by the program on 
the highest frequency verbs), again indicating qualitative differences in performance 
between the program and the experts. 
Finally, one possible shortcoming of the above analysis is that the forced-choice 
task, while maximally comparable to our computational experiments, may not be a 
natural one for human experts. To explore this issue, we asked two different experts 
in lexical semantics (one native speaker of English and one bilingual) to complete the 
non-forced-choice electronic questionnaire study; again, neither author served as one 
of the experts. In this task, in addition to the three verb classes of interest, an answer 
of "other" was allowed. Materials consisted of individually randomized lists of 119 
target and filler verbs taken from Levin's (1993) electronic index, as above. The targets 
were again the same 59 verbs used for the machine learning experiments. To avoid 
unwanted priming of target items, the 60 fillers were automatically selected from the 
set of verbs that do not share any class with any of the senses of the 59 target verbs 
in Levin's index. In this task, if we take only the target items into account, the experts 
agreed 74.6% of the time (K = 0.64) with each other, and 86% (K = 0.80) and 69% 
(K = 0.57) with the gold standard. (If we take all the verbs into consideration, they 
agreed in 67% of the cases \[K = 0.56\] with each other, and 68% \[K = 0.55\] and 60.5% 
\[K = 0.46\] with the gold standard, respectively.) These results show that the forced- 
397 
Computational Linguistics Volume 27, Number 3 
choice and non-forced-choice task are comparable in accuracy of classification and 
inter-judge agreement on the target classes, giving us confidence that the forced-choice 
results provide a reasonably stable upper bound for computational experiments. 
6. Discussion 
The work presented here contributes to some central issues in computational linguis- 
tics, by providing novel insights, data, and methodology in some cases, and by rein- 
forcing some previously established results in others. Our research stems from three 
main hypotheses: 
. 
. 
. 
Argument structure is the relevant level of representation for verb 
classification. 
Argument structure is manifested distributionally in syntactic 
alternations, giving rise to differences in subcategorization frames or the 
distributions of their usage, or in the properties of the NP arguments to 
a verb. 
This information is detectable in a corpus and can be learned 
automatically. 
We discuss the relevant debate on each of these hypotheses, and the contribution of 
our results to each, in the following subsections. 
6.1 Argument Structure and Verb Classification 
Argument structure has previously been recognized as one of the most promising 
candidates for accurate classification. For example, Basili, Pazienza, and Velardi (1996) 
argue that relational properties of verbs--their argument structure--are more infor- 
mative for classification than their definitional properties (e.g., the fact that a verb 
describes a manner of motion or a way of cooking). Their arguments rest on linguistic 
and psycholinguistic results on classification and language acquisition (in particular, 
Pinker, \[1989\]; Rosch \[1978\]). 
Our results confirm the primary role of argument structure in verb classification. 
Our experimental focus is particularly clear in this regard because we deal with verbs 
that are "minimal pairs" with respect o argument structure. By classifying verbs that 
show the same subcategorizations (transitive and intransitive) into different classes, 
we are able to eliminate one of the confounds in classification work created by the 
fact that subcategorization and argument structure are often co-variant. We can infer 
that the accuracy in our classification is due to argument structure information, as 
subcategorization is the same for all verbs. Thus, we observe that the content of the 
thematic roles assigned by a verb is crucial for classification. 
6.2 Argument Structure and Distributional Statistics 
Our results further support the assumption that thematic differences across verb 
classes are apparent not only in differences in subcategorization frames, but also in 
differences in their frequencies. This connection relies heavily on the hypothesis that 
lexical semantics and lexical syntax are correlated, following Levin (1985; 1993). How- 
ever, this position has been challenged by Basili, Pazienza, and Velardi (1996) and 
Boguraev and Briscoe (1989), among others. For example, in an attempt o assess 
the actual completeness and usefulness of the Longman Dictionary of Contemporary 
English (LDOCE) entries, Boguraev and Briscoe (1989) found that people assigned a
398 
Merlo and Stevenson Statistical Verb Classification 
"change of possession" meaning both to verbs that had dative-related subcategoriza- 
tion frames (as indicated in the LDOCE) and to verbs that did not. Conversely, they 
also found that both verbs that have a change-of-possession c mponent in their mean- 
ing and those that do not could have a dative code. They conclude that the thesis put 
forth by Levin (1985) is only partially supported. Basili, Pazienza, and Velardi (1996) 
show further isolated examples meant to illustrate that lexical syntax and semantics 
are not in a one-to-one relation. 
Many recent results, however, seem to converge in supporting the view that the 
relation between lexical syntax and semantics can be usefully exploited (Aone and 
McKee 1996; Dorr 1997; Dorr, Garman, and Weinberg 1995; Dorr and Jones 1996; La- 
pata and Brew 1999; Schulte im Walde 2000; Siegel 1998; Siegel 1999). Our work in 
particular underscores the relation between the syntactic manifestations of argument 
structure, and lexical semantic lass. In light of these recent successes, the conclusions 
in Boguraev and Briscoe (1989) are clearly too pessimistic. In fact, their results do not 
contradict he more recent ones. First of all, it is not the case that if an implication 
holds from argument structure to subcategorization (change of possession implies da- 
tive shift), the converse also holds. It comes as no surprise that verbs that do not 
have any change-of-possession component in their meaning may also show dative 
shift syntactically. Secondly, as Boguraev and Briscoe themselves note, Levin's state- 
ment should be interpreted as a statistical trend, and as such, Boguraev and Briscoe's 
results also confirm it. They claim however, that in adopting a statistical point of view, 
predictive power is lost. Our work shows that this conclusion is not appropriate ither: 
the correlation is strong enough to be useful to predict semantic lassification, at least 
for the argument structures that have been investigated. 
6.3 Detection of Argument Structure in Corpora 
Given the manifestation of argument structure in statistical distributions, we view cor- 
pora, especially if annotated with currently available tools, as repositories of implicit 
grammars, which can be exploited in automatic verb-classification tasks. Besides es- 
tablishing a relationship between syntactic alternations and underlying semantic prop- 
erties of verbs, our approach extends existing corpus-based learning techniques to the 
detection and automatic acquisition of argument structure. To date, most work in this 
area has focused on learning of subcategorization from unannotated or syntactically 
annotated text (e.g., Brent \[1993\]; Sanfilippo and Poznanski \[1992\]; Manning \[1993\]; 
Collins \[1997\]). Others have tackled the problem of lexical semantic lassification, but 
using only subcategorization frequencies as input data (Lapata and Brew 1999; Schulte 
im Walde 2000). Specifically, these researchers have not explicitly addressed the def- 
inition of features to tap directly into thematic role differences that are not reflected 
in subcategorization distinctions. On the other hand, when learning of thematic role 
assignment has been the explicit goal, the text has been semantically annotated (Web- 
ster and Marcus 1989), or external semantic resources have been consulted (Aone and 
McKee 1996; McCarthy 2000). We extend these results by showing that thematic in- 
formation can be induced from linguistically-guided counts in a corpus, without the 
use of thematic role tagging or external resources uch as WordNet. 
Finally, our results converge with the increasing agreement that corpus-based tech- 
niques are fruitful in the automatic onstruction of computational lexicons, providing 
machine readable dictionaries with complementary, reusable resources, such as fre- 
quencies of argument structures. Moreover, these techniques produce data that is eas- 
ily updated, as the information contained in corpora changes all the time, allowing 
for adaptability to new domains or usage patterns. This dynamic aspect could be ex- 
ploited if techniques uch as the one presented here are developed, which can work 
399 
Computational Linguistics Volume 27, Number 3 
on a rough collection of texts, and do not require a carefully balanced corpus or time- 
consuming semantic tagging. 
7. Related Work 
We conclude from the discussion above that our own work and work of others upport 
our hypotheses concerning the importance of the relation between classes of verbs and 
the syntactic expression of argument structure in corpora. In light of this, it is instruc- 
tive to evaluate our results in the context of other work that shares this view. Some 
related work requires either exact exemplars for acquisition, or external pre-compiled 
resources. For example, Dorr (1997) summarizes a number of automatic lassification 
experiments based on encoding Levin's alternations directly, as symbolic properties 
of a verb (Dorr, Garman, and Weinberg 1995; Dorr and Jones 1996). Each verb is rep- 
resented as the binary settings of a vector of possible alternations, acquired through 
a large corpus analysis yielding exemplars of the alternation. To cope with sparse 
data, the corpus information is supplemented by syntactic information obtained from 
the LDOCE and semantic information obtained from WordNet. This procedure clas- 
sifies 95 unknown verbs with 61% accuracy. Dorr also remarks that this result could 
be improved to 83% if missing LDOCE codes were added. While Dorr's work re- 
quires finding exact exemplars of the alternation, Oishi and Matsumoto (1997) present 
a method that, like ours, uses surface indicators to approximate underlying proper- 
ties. From a dictionary of dependency relations, they extract case-marking particles as 
indicators of the grammatical function properties of the verbs (which they call the- 
matic properties), such as subject and object. Adverbials indicate aspectual properties. 
The combination of these two orthogonal dimensions gives rise to a classification of 
Japanese verbs. 
Other work has sought o combine corpus-based extraction of verbal properties 
with statistical methods for classifying verbs. Siegel's work on automatic aspectual 
classification (1998, 1999) also reveals a close relationship between verb-related syn- 
tactic and semantic information. In this work, experiments o learn aspectual classifi- 
cation from linguistically-based numerical indicators are reported. Using combinations 
of seven statistical indicators (some morphological nd some reflecting syntactic o- 
occurrences), it is possible to learn the distinction between events and states for 739 
verb tokens with an improvement of 10% over the baseline (error rate reduction of 
74%), and to learn the distinction between culminated and non-culminated vents for 
308 verb tokens with an improvement of 11% (error rate reduction of 29%) (Siegel 
1999). 
In work on lexical semantic verb classification, Lapata and Brew (1999) further 
support he thesis of a predictive correlation between syntax and semantics in a statis- 
tical framework, showing that the frequency distributions of subcategorization frames 
within and across classes can disambiguate he usages of a verb with more than one 
known lexical semantic lass. On 306 verbs that are disambiguated by subcategoriza- 
tion frame, they achieve 91.8% accuracy on a task with a 65.7% baseline, for a 76% 
reduction in error rate. On 31 verbs that can take the same subcategorization(s) in 
different classes--more similar to our situation in that subcategorization alone cannot 
distinguish the classes--they achieve 83.9% accuracy compared to a 61.3% baseline, 
for a 58% reduction in error. Aone and McKee (1996), working with a much coarser- 
grained classification of verbs, present a technique for predicate-argument xtraction 
from multi-lingual texts. Like ours, their work goes beyond statistics over subcate- 
gorizations to include counts over the more directly semantic feature of animacy. No 
numerical evaluation of their results is provided. 
400 
Merlo and Stevenson Statistical Verb Classification 
Schulte im Walde (2000) applies two clustering methods to two types of frequency 
data for 153 verbs from 30 Levin (1993) classes. One set of experiments uses verb 
subcategorization frequencies, and the other uses subcategorization frequencies plus 
selectional preferences (a numerical measure based on an adaptation of the relative 
entropy method of Resnik \[1996\]). The best results achieved are a correct classification 
of 58 verbs out of 153, with a precision of 61% and recall of 36%, obtained using 
only subcategorization frequencies. We calculate that this corresponds to an F-score of 
45% with balanced precision and recall, n The use of selectional preference information 
decreases classification performance under either clustering algorithm. The results are 
somewhat difficult o evaluate further, as there is no description of the classes included. 
Also, the method of counting correctness entails that some "correct" classes may be 
split across distant clusters (this level of detail is not reported), so it is unclear how 
coherent the class behaviour actually is. 
McCarthy (2000) proposes a method to identify diathesis alternations. After learn- 
ing subcategorization frames, based on a parsed corpus, selectional preferences are 
acquired for slots of the subcategorization frames, using probability distributions over 
Wordnet classes. Alternations are detected by testing the hypothesis that, given any 
verb, the selectional preferences for arguments occurring in alternating slots will be 
more similar to each other than those for slots that do not alternate. For instance, given 
a verb participating in the causative alternation, its selectional preferences for the sub- 
ject in an intransitive use, and for the object in a transitive use, will be more similar 
to each other than the selectional preferences for these two slots of a verb that does 
not participate in the causative alternation. This method achieves the best accuracy 
for the causative and the conative alternations (73% and 83%, respectively), despite 
sparseness of data. McCarthy reports that a simpler measure of selectional preferences 
based simply on head words yields a lower 63% accuracy. Since this latter measure 
is very similar to our CAUS feature, we think that our results would also improve by 
adopting a similar method of abstracting from head words to classes. 
Our work extends each of these approaches in some dimension, thereby provid- 
ing additional support for the hypothesis that syntax and semantics are correlated in 
a systematic and predictive way. We extend Dorr's alternation-based automatic lassi- 
fication to a statistical setting. By using distributional pproximations of indicators of 
alternations, we solve the sparse data problem without recourse to external sources of 
knowledge, such as the LDOCE, and in addition, we are able to learn argument struc- 
ture alternations using exclusively positive examples. We improve on the approach of 
Oishi and Matsumoto (1997) by learning argument structure properties, which, unlike 
grammatical functions, are not marked morphologically, and by not relying on exter- 
nal sources of knowledge. Furthermore, in contrast o Siegel (1998) and Lapata and 
Brew (1999) our method applies successfully to previously unseen words--i.e., test 
cases that were not represented in the training set .  13 This is a very important property 
of lexical acquisition algorithms to be used for lexicon organization, as their main 
interest lies in being applied to unknown words. 
On the other hand, our approach is similar to the approaches of Siegel, and La- 
pata and Brew (1999), in attempting to learn semantic notions from distributions of 
12 A baseline of 5% is reported, based on a closest-neighbor pairing of verbs, but it is not straightforward 
to compare this task to the proposed clustering algorithm. Determining a meaningful baseline for 
unsupervised clustering isclearly achallenge, but this gives an indication that the clustering task is 
indeed ifficult. 
13 Siegel (1998) reports two experiments over verb types with disjoint raining and test sets, but the 
results were not significantly different from the baseline. 
401 
Computational Linguistics Volume 27, Number 3 
indicators that can be gleaned from a text. In our case, we are trying to learn argu- 
ment structure, a finer-grained classification than the dichotomic distinctions studied 
by Siegel. Like Lapata and Brew, three of our indicators--TRANS, VBN, PASS--are based 
on the assumption that distributional differences in subcategorization frames are re- 
lated to underlying verb class distinctions. However, we also show that other syntactic 
indicators--cAUS and ANIM--can be devised that tap directly into the argument struc- 
ture of a verb. Unlike Schulte im Walde (2000), we find the use of these semantic 
features helpful in classification--using only TRANS and its related features, VBN and 
PASS, we achieve only 55% accuracy, in comparison to 69.8% using the full set of fea- 
tures. This can perhaps be seen as support for our hypothesis that argument structure 
is the right level of representation forverb class distinctions, ince it appears that our 
features that capture thematic differences are useful in classification, while Schulte im 
Walde's electional restriction features were not. 
Aone and McKee (1996) also use features that are intended to tap into both sub- 
categorization and thematic role distinctions--frequencies of the transitive use and 
animate subject use. In our task, we show that subject animacy can be profitably ap- 
proximated solely with pronoun counts, avoiding the need for reference to external 
sources of semantic information used by Aone and McKee. In addition, our work ex- 
tends theirs in investigating much finer-grained verb classes, and in classifying verbs 
that have multiple argument structures. While Aone and McKee define ach of their 
classes according to a single argument structure, we demonstrate he usefulness of 
syntactic features that capture relations across different argument structures of a sin- 
gle verb. Furthermore, while Aone and McKee, and others, look at relative frequency 
of subcategorization frames (as with our TRANS feature), or relative frequency of a 
property of NPs within a particular grammatical function (as with our ANIM feature), 
we also look at the paradigmatic relations across a text between thematic arguments 
in different alternations (with our CAUS feature). 
McCarthy (2000) shows that a method very similar to ours can be used for identi- 
fying alternations. Her qualitative results confirm, however, what was argued in Sec- 
tion 2 above: counts that tap directly into the thematic assignments are necessary to 
fully identify a diathesis alternation. In fact, on close inspection, McCarthy's method 
does not distinguish between the induced-action alternation (which the unergatives 
exhibit) and the causative/inchoative alternation (which the unaccusatives xhibit); 
thus, her method oes not discriminate two of our classes. It is likely that a combina- 
tion of our method, which makes the necessary thematic distinctions, and her more 
sophisticated method of detecting alternations would give very good results. 
8. Limitations and Future Work 
The classification results show that our method is powerful, and suited to the clas- 
sification of unknown verbs. However, we have not yet addressed the problem of 
verbs that can have multiple classifications. We think that many cases of ambigu- 
ous classification of the lexical entry for a verb can be addressed with the notion 
of intersective sets introduced by Dang et al (1998). This is an important concept, 
which proposes that "regular" ambiguity in classification--i.e., sets of verbs that have 
the same multi-way classifications according to Levin (1993)--can be captured with 
a finer-grained notion of lexical semantic lasses. Thus, subsets of verbs that occur 
in the intersection of two or more Levin classes form in themselves a coherent se- 
mantic (sub)class. Extending our work to exploit this idea requires only defining 
the classes appropriately; the basic approach will remain the same. Given the cur- 
rent demonstration f our method on fine-grained classes that share subcategoriza- 
402 
Merlo and Stevenson Statistical Verb Classification 
tion alternations, we are optimistic regarding its future performance on intersective 
sets. 
Because we assume that thematic properties are reflected in alternations of argu- 
ment structure, our features require searching for relations across occurrences of each 
verb. This motivated our initial experimental focus on verb types. However, when 
we turn to consider ambiguity, we must also address the problem that individual in- 
stances of verbs may come from different classes, and we may (like Lapata and Brew 
\[1999\]) want to classify the individual tokens of a verb. In future research we plan 
to extend our method to the case of ambiguous tokens, by experimenting with the 
combination of several sources of information: the classification of each instance will 
be a function of a bias for the verb type (using the cross-corpus statistics we collect), 
but also of features of the usage of the instance being classified (cf., Lapata and Brew 
\[1999\]; Siegel \[1998\]). 
Finally, corpus-based learning techniques collect statistical information related to 
language use, and are a good starting point for studying human linguistic perfor- 
mance. This opens the way to investigating the relation of linguistic data in text to 
people's linguistic behaviour and use. For example, Merlo and Stevenson (1998) show 
that, contrary to the naive assumption, speakers' preferences in syntactic disambigua- 
tion are not simply directly related to frequency (i.e., a speaker's preference for one 
construction over another is not simply modelled by the frequency of the construc- 
tion, or of the words in the construction). Thus, the kind of corpus investigation we 
are advocating--founded on in-depth linguistic analysis--holds promise for building 
more natural NLP systems which go beyond the simplest assumptions, and tie to- 
gether statistical computational linguistic results with experimental psycholinguistic 
data. 
9. Conclusions 
In this paper, we have presented an in-depth case study, in which we investigate 
machine learning techniques for automatically classifying a set of verbs into classes 
determined by their argument structures. We focus on the three major classes of op- 
tionally intransitive verbs in English, which cannot be discriminated by their subcate- 
gorizations, and therefore require distinctive features that are sensitive to the thematic 
properties of the verbs. We develop such features and automatically extract hem from 
very large, syntactically annotated corpora. Results show that a small number of lin- 
guistically motivated lexical features are sufficient o achieve a 69.8% accuracy rate 
in a three-way classification task with a baseline (chance) performance of 33.9%, for 
which the best performance achieved by a human expert is 86.5%. 
Returning to our original questions of what can and need be learned about the 
relational properties of verbs, we conclude that argument structure is both a highly 
useful and learnable aspect of verb knowledge. We observe that relevant semantic 
properties of verb classes (such as causativity, or animacy of subject) may be suc- 
cessfully approximated through countable syntactic features. In spite of noisy data 
(arising from diverse sources uch as tagging errors, or limitations of our extraction 
patterns), the lexical properties of interest are reflected in the corpora robustly enough 
to positively contribute to classification. 
We remark, however, that deep linguistic analysis cannot be eliminated--in our 
approach it is embedded in the selection of the features to count. Specifically, our 
features are derived through a detailed analysis of the differences in thematic role as- 
signments across the verb classes under investigation. Thus, an important contribution 
of the work is the proposed mapping between the thematic assignment properties of 
403 
Computational Linguistics Volume 27, Number 3 
the verb classes, and the statistical distributions of their surface syntactic properties. 
We think that using such linguistically motivated features makes the approach very 
effective and easily scalable: we report a 54% reduction in error rate (a 68% reduction, 
when the human expert-based upper bound is considered), using only five features 
that are readily extractable from automatically annotated corpora. 
Acknowledgments 
We gratefully acknowledge the financial 
support of the following organizations: the 
Swiss NSF (fellowship 8210-46569 to PM); 
the United States NSF (grants #9702331 and 
#9818322 to SS); the Canadian NSERC 
(grant o SS); the University of Toronto; and 
the Information Sciences Council of Rutgers 
University. Much of this research was 
carried out while PM was a visiting scientist 
at IRCS, University of Pennsylvania, nd 
while SS was a faculty member at Rutgers 
University, both of whose generous and 
supportive nvironments were of great 
benefit o us. We thank Martha Palmer, 
Michael Collins, Natalia Kariaeva, Kamin 
Whitehouse, Julie Boland, Kiva Dickinson, 
and three anonymous reviewers, for their 
helpful comments and suggestions, and for 
their contributions to this research. We also 
greatly thank our experts for the gracious 
contribution of their time in answering our 
electronic questionnaire. 
References 
Abney, Steven. 1996. Partial parsing via 
finite-state cascades. In John Carroll, 
editor, Proceedings ofthe Workshop on Robust 
Parsing at the Eighth Summer School on 
Logic, Language and Information, umber 
435 in CSRP, pages 8-15. University of 
Sussex, Brighton. 
Aone, Chinatsu and Douglas McKee. 1996. 
Acquiring predicate-argument mapping 
information i  multilingual texts. In 
Branimir Boguraev and James 
Pustejovsky, editors, Corpus Processing for 
Lexical Acquisition. MIT Press, 
pages 191-202. 
Basili, Roberto, Maria-Teresa Pazienza, and 
Paola Velardi. 1996. A context-driven 
conceptual c ustering method for verb 
classification. In Branimir Boguraev and 
James Pustejovsky, editors, Corpus 
Processing for Lexical Acquisition. MIT 
Press, pages 117-142. 
Boguraev, Branimir and Ted Briscoe. 1989. 
Utilising the LDOCE grammar codes. In 
Branimir Boguraev and Ted Briscoe, 
editors, Computational Lexicography for 
Natural Language Processing. Longman, 
London, pages 85-116. 
Boguraev, Branimir and James Pustejovsky. 
1996. Issues in text-based lexicon 
acquisition. In Branimir Boguraev and 
James Pustejovsky, editors, Corpus 
Processing for Lexical Acquisition. MIT 
Press, pages 3-20. 
Brent, Michael. 1993. From grammar to 
lexicon: Unsupervised learning of lexical 
syntax. Computational Linguistics, 
19(2):243-262. 
Briscoe, Ted and John Carroll. 1997. 
Automatic extraction of subcategorization 
from corpora. In Proceedings ofthe Fifth 
Applied Natural Language Processing 
Conference, pages 356-363. 
Brousseau, Anne-Marie and Elizabeth Ritter. 
1991. A non-unified analysis of agentive 
verbs. In West Coast Conference on Formal 
Linguistics, number 20, pages 53-64. 
Burzio, Luigi. 1986. Italian Syntax: A 
Government-Binding Approach. Reidel: 
Dordrecht. 
Carletta, Jean. 1996. Assessing agreement on 
classification tasks: the Kappa statistics. 
Computational Linguistics, 22(2):249-254. 
Collins, Michael John. 1997. Three 
generative, lexicalised models for 
statistical parsing. In Proceedings ofthe 35th 
Annual Meeting of the ACL, pages 16-23, 
Madrid, Spain. 
Cruse, D. A. 1972. A note on English 
causatives. Linguistic Inquiry, 3(4):520-528. 
Dang, Hoa Trang, Karin Kipper, Martha 
Palmer, and Joseph Rosenzweig. 1998. 
Investigating regular sense extensions 
based on intersective Levin classes. In 
Proceedings ofthe 36th Annual Meeting of the 
ACL and the 17th International Conference on 
Computational Linguistics (COLING-ACL 
"98), pages 293-299, Montreal. Universit~ 
de Montreal. 
Dixon, Robert M. W. 1994. Ergativity. 
Cambridge University Press, Cambridge. 
Dorr, Bonnie. 1997. Large-scale dictionary 
construction for foreign language tutoring 
and interlingual machine translation. 
Machine Translation, 12(4):1-55. 
Dorr, Bonnie, Joe Garman, and Amy 
Weinberg. 1995. From syntactic encodings 
to thematic roles: Building lexical entries 
for interlingual MT. Journal of Machine 
Translation, 9(3):71-100. 
404 
Merlo and Stevenson Statistical Verb Classification 
Dorr, Bonnie and Doug Jones. 1996. Role of 
word sense disambiguation i  lexical 
acquisition: Predicting semantics from 
syntactic ues. In Proceedings ofthe 16th 
International Conference on Computational 
Linguistics, pages 322-327, Copenhagen. 
Dowty, David. 1991. Thematic proto-roles 
and argument selection. Language, 
67(3):547-619. 
Greenberg, Joseph H. 1966. Language 
Universals. Mouton, The Hague, Paris. 
Gruber, Jeffrey. 1965. Studies in Lexical 
Relation. MIT Press, Cambridge, MA. 
Hale, Ken and Jay Keyser. 1993. On 
argument structure and the lexical 
representation f syntactic relations. In K. 
Hale and J. Keyser, editors, The View from 
Building 20. MIT Press, pages 53-110. 
Jakobson, Roman. 1971. Signe Z4ro. In 
Selected Writings, volume 2, 2d ed. 
Mouton, The Hague, pages 211-219. 
Kariaeva, Natalia. 1999. Discriminating 
between unaccusative and object-drop 
verbs: Animacy factor. Ms., Rutgers 
University. New Brunswick, NJ. 
Klavans, Judith and Martin Chodorow. 
1992. Degrees of stativity: The lexical 
representation f verb aspect. In 
Proceedings ofthe Fourteenth International 
Conference on Computational Linguistics 
(COLING "92), pages 1126-1131, Nantes, 
France. 
Klavans, Judith and Min-Yen Kan. 1998. 
Role of verbs in document analysis. In 
Proceedings ofthe 36th Annual Meeting of the 
ACL and the 17th International Conference on 
Computational Linguistics (COLING-ACL 
'98), pages 680-686, Montreal. Universit4 
de Montreal. 
Lapata, Maria. 1999. Acquiring lexical 
generalizations from corpora: A case 
study for diathesis alternations. In
Proceedings ofthe 37th Annual Meeting of the 
Association for Computational Linguistics 
(ACL'99), pages 397-404, College Park, 
MD. 
Lapata, Maria and Chris Brew. 1999. Using 
subcategorization to resolve verb class 
ambiguity. In Proceedings ofJoint SIGDAT 
Conference on Empirical Methods in Natural 
Language Processing and Very Large Corpora, 
pages 266-274, College Park, MD. 
Levin, Beth. 1985. Introduction. In Beth 
Levin, editor, Lexical Semantics in Review, 
number 1 in Lexicon Project Working 
Papers. Centre for Cognitive Science, MIT, 
Cambridge, MA, pages 1-62. 
Levin, Beth. 1993. English Verb Classes and 
Alternations. University of Chicago Press, 
Chicago, IL. 
Levin, Beth and Malka Rappaport Hovav. 
1995. Unaccusativity. MIT Press, 
Cambridge, MA. 
Manning, Christopher D. 1993. Automatic 
acquisition of a large subcategorization 
dictionary from corpora. In Proceedings of
the 31st Annual Meeting of the Association for 
Computational Linguistics, pages 235-242. 
Ohio State University. 
McCarthy, Diana. 2000. Using semantic 
preferences to identify verbal 
participation i  role switching 
alternations. In Proceedings of
ANLP-NAACL 2000, pages 256-263, 
Seattle, WA. 
McCarthy, Diana and Anna Korhonen. 1998. 
Detecting verbal participation i  diathesis 
alternations. In Proceedings ofthe 36th 
Annual Meeting of the ACL and the 17th 
International Conference on Computational 
Linguistics (COLING-ACL "98), 
pages 1493-1495, Montreal, Universit4 de 
Montreal. 
Merlo, Paola and Suzanne Stevenson. 1998. 
What grammars tell us about corpora: the 
case of reduced relative clauses. In 
Proceedings ofthe Sixth Workshop on Very 
Large Corpora, pages 134-142, Montreal. 
Merlo, Paola and Suzanne Stevenson. 2000a. 
Establishing the upper-bound and 
inter-judge agreement in a verb 
classification task. In Second International 
Conference on Language Resources and 
Evaluation (LREC-2000), volume 3, 
pages 1659-1664. 
Merlo, Paola and Suzanne Stevenson. 2000b. 
Lexical syntax and parsing architecture. 
In Matthew Crocker, Martin Pickering, 
and Charles Clifton, editors, Architectures 
and Mechanisms for Language Processing. 
Cambridge University Press, Cambridge, 
pages 161-188. 
Moravcsik, Edith and Jessica Wirth. 1983. 
Markedness--an Overview. In Fred 
Eckman, Edith Moravcsik, and Jessica 
Wirth, editors, Markedness. Plenum Press, 
New York, NY, pages 1-13. 
Oishi, Akira and Yuji Matsumoto. 1997. 
Detecting the organization of semantic 
subclasses of Japanese verbs. International 
Journal of Corpus Linguistics, 2(1):65-89. 
Palmer, Martha. 2000. Consistent criteria for 
sense distinctions. Special Issue of 
Computers and the Humanities, 
SENSEVAL98: Evaluating Word Sense 
Disambiguation Systems, 34(1-2):217-222. 
Perlmutter, David. 1978. Impersonal 
passives and the unaccusative hypothesis. 
In Proceedings ofthe Annual Meeting of the 
Berkeley Linguistics Society, volume 4, 
pages 157-189. 
405 
Computational Linguistics Volume 27, Number 3 
Pinker, Steven. 1989. Learnability and 
Cognition: the Acquisition of Argument 
Structure. MIT Press, Cambridge, MA. 
Quinlan, J. Ross. 1992. C4.5: Programs for 
Machine Learning. Series in Machine 
Learning. Morgan Kaufmann, San Mateo, 
CA. 
Ratnaparkhi, Adwait. 1996. A maximum 
entropy parbof-speech tagger. In 
Proceedings of the Empirical Methods in 
Natural Language Processing Conference, 
pages 133-142, Philadelphia, PA. 
Resnik, Philip. 1996. Selectional constraints: 
an information-theoretic model and its 
computational realization. Cognition, 
61(1-2):127-160. 
Riloff, Ellen and Mark Schmelzenbach. 1998. 
An empirical approach to conceptual case 
frame acquisition. In Proceedings of the Sixth 
Workshop on Very Large Corpora, 
pages 49-56. 
Rosch, Eleanor. 1978. Principles of 
categorization. I  Cognition and 
Categorization. Lawrence Erlbaum Assoc, 
Hillsdale, NJ. 
Sanfilippo, Antonio and Victor Poznanski. 
1992. The acquisition of lexical knowledge 
from combined machine-readable 
dictionary sources. In Proceedings of the 
Third Applied Natural Language Processing 
Conference, pages 80-87, Trento, Italy. 
Schulte im Walde, Sabine. 2000. Clustering 
verbs semantically according to their 
alternation behaviour. In Proceedings of 
COLING 2000, pages 747-753, 
Saarbruecken, Germany. 
Siegel, Eric 1998. Linguistic Indicators for 
Language Understanding: Using machine 
learning methods to combine corpus-based 
indicators for aspectual c assification of clauses. 
Ph.D. thesis, Dept. of Computer Science, 
Columbia University. 
Siegel, Eric. 1999. Corpus-based linguistic 
indicators for aspectual classification. In
Proceedings of ACL'99, pages 112-119, 
College Park, MD. University of 
Maryland. 
Siegel, Sidney and John Castellan. 1988. 
Nonparametric statistics for the Behavioral 
Sciences. McGraw-Hill, New York. 
Silverstein, Michael. 1976. Hierarchy of 
features and ergativity. In Robert Dixon, 
editor, Grammatical Categories in Australian 
Languages. Australian Institute of 
Aboriginal Studies, Canberra, 
pages 112-171. 
Srinivas, Bangalore and Aravind K. Joshi. 
1999. Supertagging: An approach to 
almost parsing. Computational Linguistics, 
25(2):237-265. 
Stede, Manfred. 1998. A generative 
perspective on verb alternations. 
Computational Linguistics, 24(3):401--430. 
Stevenson, Suzanne and Paola Merlo. 1997a. 
Architecture and experience in sentence 
processing. In Proceedings of the 19th 
Annual Conference of the Cognitive Science 
Society, pages 715-720. 
Stevenson, Suzanne and Paola Merlo. 1997b. 
Lexical structure and processing 
complexity. Language and Cognitive 
Processes, 12(1-2):349-399. 
Stevenson, Suzanne, Paola Merlo, Natalia 
Kariaeva, and Kamin Whitehouse. 1999. 
Supervised learning of lexical semantic 
verb classes using frequency 
distributions. In Proceedings of SigLex99: 
Standardizing Lexical Resources (SigLex'99), 
pages 15-21, College Park, MD. 
Trubetzkoy, Nicolaj S. 1939. Grundzage der 
Phonologie. Travaux du Cercle 
Linguistique de Prague, Prague. 
Webster, Mort and Mitch Marcus. 1989. 
Automatic acquisition of the lexical 
semantics of verbs from sentence frames. 
In Proceedings of the 27th Annual Meeting of 
the Association for Computational Linguistics, 
pages 177-184, Vancouver, Canada. 
406 
Merlo and Stevenson Statistical Verb Classification 
Appendix A 
The fo l lowing three tables conta in the overall  f requency and the normal i zed  feature 
values for each of the 59 verbs in our exper imental  set. 
Unergative Verbs 
Freq VBN PASS TRANS CAUS ANIM 
Min Value 8 0.00 0.00 0.00 0.00 0.00 
Max Value 4088 1.00 0.39 0.74 0.00 1.00 
floated 176 0.43 0.26 0.74 0.00 0.17 
hurried 86 0.40 0.31 0.50 0.00 0.37 
jumped 4088 0.09 0.00 0.03 0.00 0.20 
leaped 225 0.09 0.00 0.05 0.00 0.13 
marched 238 0.10 0.01 0.09 0.00 0.12 
paraded 33 0.73 0.39 0.46 0.00 0.50 
raced 123 0.01 0.00 0.06 0.00 0.15 
rushed 467 0.22 0.12 0.20 0.00 0.10 
vaulted 54 0.00 0.00 0.41 0.00 0.03 
wandered 67 0.02 0.00 0.03 0.00 0.32 
galloped 12 1.00 0.00 0.00 0.00 0.00 
glided 14 0.00 0.00 0.08 0.00 0.50 
hiked 25 0.28 0.12 0.29 0.00 0.40 
hopped 29 0.00 0.00 0.21 0.00 1.00 
jogged 8 0.29 0.00 0.29 0.00 0.33 
scooted 10 0.00 0.00 0.43 0.00 0.00 
scurried 21 0.00 0.00 0.00 0.00 0.14 
skipped 82 0.22 0.02 0.64 0.00 0.16 
tiptoed 12 0.17 0.00 0.00 0.00 0.00 
trotted 37 0.19 0.17 0.07 0.00 0.18 
Unaccusative Verbs 
Freq VBN PASS TRANS CAUS ANIM 
Min Value 13 0.16 0.00 0.02 0.00 0.00 
Max Value 5543 0.95 0.80 0.76 0.41 0.36 
boiled 58 0.92 0.70 0.42 0.00 0.00 
cracked 175 0.61 0.19 0.76 0.02 0.14 
dissolved 226 0.51 0.58 0.71 0.05 0.11 
exploded 409 0.34 0.02 0.66 0.37 0.04 
flooded 235 0.47 0.57 0.44 0.04 0.03 
fractured 55 0.95 0.76 0.51 0.00 0.00 
hardened 123 0.92 0.55 0.56 0.12 0.00 
melted 70 0.80 0.44 0.02 0.00 0.19 
opened 3412 0.21 0.09 0.69 0.16 0.36 
solidified 34 0.65 0.21 0.68 0.00 0.12 
collapsed 950 0.16 0.00 0.16 0.01 0.02 
cooled 232 0.85 0.21 0.29 0.13 0.11 
folded 189 0.73 0.33 0.23 0.00 0.00 
widened 1155 0.18 0.02 0.13 0.41 0.01 
changed 5543 0.73 0.23 0.47 0.22 0.08 
cleared 1145 0.58 0.40 0.50 0.31 0.06 
divided 1539 0.93 0.80 0.17 0.10 0.05 
simmered 13 0.83 0.00 0.09 0.00 0.00 
stabilized 286 0.92 0.13 0.18 0.35 0.00 
407 
Computational Linguistics Volume 27, Number 3 
Object-Drop Verbs 
Freq VBN PASS TRANS CAUS ANIM 
Min Value 39 0.10 0.04 0.21 0.00 0.00 
Max Value 15063 0.95 0.99 1.00 0.24 0.42 
carved 185 0.85 0.66 0.98 0.00 0.00 
danced 88 0.22 0.14 0.37 0.00 0.00 
kicked 308 0.30 0.18 0.97 0.00 0.33 
knitted 39 0.95 0.99 0.93 0.00 0.00 
painted 506 0.72 0.18 0.71 0.00 0.38 
played 2689 0.38 0.16 0.24 0.00 0.00 
reaped 172 0.56 0.05 0.90 0.00 0.22 
typed 57 0.81 0.74 0.81 0.00 0.00 
washed 137 0.79 0.60 1.00 0.00 0.00 
yelled 74 0.10 0.04 0.38 0.00 0.00 
borrowed 1188 0.77 0.15 0.60 0.13 0.19 
inherited 357 0.60 0.13 0.64 0.06 0.32 
organized 1504 0.85 0.38 0.65 0.18 0.07 
rented 232 0.72 0.22 0.61 0.00 0.42 
sketched 44 0.67 0.17 0.44 0.00 0.20 
cleaned 160 0.83 0.47 0.21 0.05 0.21 
packed 376 0.84 0.12 0.40 0.05 0.19 
studied 901 0.66 0.17 0.57 0.05 0.11 
swallowed 152 0.79 0.44 0.35 0.04 0.22 
called 15063 0.56 0.22 0.72 0.24 0.16 
Appendix B 
Performance of all the subsets of features, in  order of decreasing accuracy. To determine 
whether  the difference between any two results is statistically signif icant, the 95% 
confidence interval  can be calculated for each of the two results, and  the two ranges 
checked to see whether  they overlap. To do this, take each accuracy p lus and  minus  
2.01 t imes its associated s tandard  error to get the 95% confidence range (dr = 49, 
t = 2.01). If the two ranges overlap, then the difference in accuracy is not  signif icant 
at the p < .05 level. 
Accuracy SE Features Accuracy SE Features 
69.8 0.5 TRANS PASS VBN CAUS ANIM 57.3 0.5 TRANS CAUS 
69.8 0.5 TRANS VBN CAUS ANIM 57.3 0.5 PASS VBN ANIM 
67.3 0.6 TRANS PASS VBN ANIM 56.7 0.5 PASS CAUS ANIM 
66.7 0.5 TRANS VBN ANIM 55.7 0.5 VBN CAUS 
66.5 0.5 TRANS PASS CAUS ANIM 55.7 0.1 CAUS 
64.4 0.5 TRANS VBN CAUS 55.4 0.4 PASS CAUS 
63.2 0.6 TRANS PASS VBN CAUS 55.0 0.6 TIKANS PASS VBN 
63.0 0.5 TRANS PASS ANIM 54.7 0.4 TRANS PASS 
62.9 0.4 TRANS CAUS AN~M 54.2 0.5 TRANS VBN 
62.1 0.5 CAUS ANIM 52.5 0.5 VBN 
61.7 0.5 TRANS PASS CAUS 50.9 0.5 PASS ANIM 
61.6 0.6 PASS VBN CAUS ANIM 50.2 0.6 PASS VBN 
60.1 0.4 VBN CAUS ANIM 50.2 0.5 PASS 
59.5 0.6 TRANS ANIM 47.1 0.4 TKANS 
59.4 0.5 VBN ANIM 35.3 0.5 ANIM 
57.4 0.6 PASS VBN CAUS 
408 
The Notion of Argument in Prepositional
Phrase Attachment
Paola Merlo?
University of Geneva
Eva Esteve Ferrer?
University of Sussex
In this article we refine the formulation of the problem of prepositional phrase (PP) attachment as
a four-way disambiguation problem. We argue that, in interpreting PPs, both knowledge about
the site of the attachment (the traditional noun?verb attachment distinction) and the nature of
the attachment (the distinction of arguments from adjuncts) are needed. We introduce a method
to learn arguments and adjuncts based on a definition of arguments as a vector of features. In
a series of supervised classification experiments, first we explore the features that enable us to
learn the distinction between arguments and adjuncts. We find that both linguistic diagnostics
of argumenthood and lexical semantic classes are useful. Second, we investigate the best method
to reach the four-way classification of potentially ambiguous prepositional phrases. We find that
whereas it is overall better to solve the problem as a single four-way classification task, verb
arguments are sometimes more precisely identified if the classification is done as a two-step
process, first choosing the attachment site and then labeling it as argument or adjunct.
1. Motivation
Incorrect attachment of prepositional phrases (PPs) often constitutes the largest single
source of errors in current parsing systems. Correct attachment of PPs is necessary to
construct a parse tree that will support the proper interpretation of constituents in the
sentence. Consider the timeworn example
(1) I saw the man with the telescope.
It is important to determine if the PP with the telescope is to be attached as a sister to
the noun the man, restricting its interpretation, or if it is to be attached to the verb,
thereby indicating the instrument of the main action described by the sentence. Based on
examples of this sort, recent approaches have formalized the problem of disambiguating
PP attachments as a binary choice, distinguishing between attachment of a PP to a given
verb or to the verb?s direct object (Hindle and Rooth 1993; Ratnaparkhi, Reynar, and
Roukos 1994; Collins and Brooks 1995; Merlo, Crocker, and Berthouzoz 1997; Stetina
and Nagao 1997; Ratnaparkhi 1997; Zhao and Lin 2004).
This is, however, a simplification of the problem, which does not take the nature
of the attachment into account. Precisely, it does not distinguish PP arguments from
? Linguistics Department, University of Geneva, 2 rue de Candolle, 1211 Gene`ve 4, Switzerland.
? Department of Informatics, University of Sussex, Falmer, Brighton BN1 9QH, UK.
Submission received: 28 November 2003; revised submission received: 22 June 2005; accepted for publication:
4 November 2005.
? 2006 Association for Computational Linguistics
Computational Linguistics Volume 32, Number 3
PP adjuncts. Consider the following example, which contains two PPs, both modifying
the verb.
(2) Put the block on the table in the morning.
The first PP is a locative PP required by the subcategorization frame of the verb
put, whereas in the morning is an optional descriptor of the time at which the ac-
tion was performed. Although both are attached to the verb, the two PPs entertain
different relationships with the verb?the first is an argument whereas the latter is
an adjunct. Analogous examples could be built for attachments to the noun. (See
examples 7a, b.)
Thus, PPs cannot only vary depending on the site to which they attach in the
structure, such as in example (1), but they can fulfill different functions in the sen-
tence, such as in example (2). In principle, then, a given PP could be four-way am-
biguous. In practice, it is difficult and moderately unnatural to construct examples
of four-way ambiguous sentences, sentences that only a good amount of linguis-
tic and extralinguistic knowledge can disambiguate among the noun-attached and
verb-attached option, with an argument or adjunct interpretation. It is, however, not
impossible.
Consider benefactive constructions, such as the sentence below.
(3) Darcy baked a cake for Elizabeth.
In this case the for is a benefactive, hence an argument of the verb bake. However, the
for-PP is optional; thus other non-argument PPs can occur in the same position.
(4) Darcy baked a cake for 5 shillings/for an hour.
Whereas in sentence (3) the PP is an argument, in (4) the PP is an adjunct, as indicated
by the different status of the corresponding passive sentences and by the ordering of the
PPs (arguments prefer to come first), as shown in (5) and (6).
(5a) Elizabeth was baked a cake by Darcy
(5b) *5 shillings/an hour were baked a cake by Darcy
(6a) Darcy baked a cake for Elizabeth for 5 shillings/for an hour
(6b) ??Darcy baked a cake for 5 shillings/for an hour for Elizabeth
This kind of ambiguity also occurs in sentences in which the for-PP is modifying
the object noun phrase. Depending on the head noun in object position, and under
the assumption that a beneficiary is an argument, as we have assumed in the sen-
tences above, the PP will be an argument or an adjunct, as in the following examples,
respectively.
(7a) Darcy baked [cakes for children]
(7b) Darcy baked [cakes for 5 shillings]
342
Merlo and Esteve Ferrer Argument in Prepositional Phrase Attachment
Modeling both the site and the nature of the attachment of a PP into the tree
structure is important. Distinguishing arguments from adjuncts is key to identifying
the elements that belong to the semantic kernel of a sentence. Extracting the kernel
of a sentence or phrase, in turn, is necessary for automatic acquisition of important
lexical knowledge, such as subcategorization frames and argument structures, which
is used in several natural language processing (NLP) tasks and applications, such
as parsing, machine translation, and information extraction (Srinivas and Joshi 1999;
Dorr 1997; Phillips and Riloff 2002). This task is fundamentally syntactic in nature
and complements the task of assigning thematic role labels (Gildea and Jurafsky 2002;
Nielsen and Pradhan 2004; Xue and Palmer 2004; Swier and Stevenson 2005). See also
the common task of CoNNL (2004, 2005) and SENSEVAL-3 (2004). Both a distinction of
arguments from adjuncts and an appropriate thematic labeling of the complements of a
predicate, verb, or noun are necessary, as confirmed by the annotations adopted by cur-
rent corpora. Framenet makes a distinction between complements and satellites (Baker,
Fillmore, and Lowe 1998). The developers of PropBank integrate the difference between
arguments and adjuncts directly into the level of specificity of their annotation. They
adopt labels that are common across verbs for adjuncts. They inherit these labels from
the Penn Treebank annotation. Arguments are annotated instead with labels specific to
each verb (Xue 2004; Palmer, Gildea, and Kingsbury 2005).
From a quantitative point of view, arguments and adjuncts have different statistical
properties. For example, Hindle and Rooth (1993) clearly indicate that their lexical
association technique performs much better for arguments than for adjuncts, whether
the attachment is to the verb or to the noun.
Researchers have abstracted away from this distinction, because identifying ar-
guments and adjuncts is a notoriously difficult task, taxing many native speakers?
intuitions. The usual expectation has been that this discrimination is not amenable to
a corpus-based treatment. In recent preliminary work, however, we have succeeded
in distinguishing arguments from adjuncts using corpus evidence (Merlo and Leybold
2001; Merlo 2003). Our method develops corpus-based statistical correlates for the
diagnostics used in linguistics to decide whether a PP is an argument or an adjunct.
A numerical vectorial representation of the notion of argumenthood is provided, which
supports automatic classification. In the current article, we expand and improve on this
work, by developing new measures and refining the previous ones. We also extend that
work to attachment to nouns. This extension enables us to explore in what way the dis-
tinction between argument and adjunct is best integrated in the traditional attachment
disambiguation problem.
We treat PP attachment as a four-way classification of PPs into noun argument
PPs, noun adjunct PPs, verb argument PPs, and verb adjunct PPs. We investigate this
new approach to PP attachment disambiguation through several sets of experiments,
testing different hypotheses on the argument/adjunct distinction of PPs and on its
interaction with the disambiguation of the PP attachment site. The two main claims
can be formulated as follows.
 Hypothesis 1: The argument/adjunct distinction can be performed based
on information collected from a minimally annotated corpus,
approximating deeper semantic information statistically.
 Hypothesis 2: The learning features developed for the notion of argument
and adjunct can be usefully integrated in a finer-grained formulation of
the problem of PP attachment as a four-way classification.
343
Computational Linguistics Volume 32, Number 3
To test these two hypotheses, we illustrate our technique to distinguish argu-
ments from adjuncts (Section 2), and we report results on this binary classification
(Sections 3 and 4). The intuition behind the technique is that we do not need to represent
the distinction between arguments and adjuncts directly, but that the distinction can
be indirectly represented as a numerical vector. The feature values in the vector are
corpus-based numerical equivalents of the grammaticality diagnostics used by linguists
to decide whether a PP is an argument or an adjunct. For example, one of the values in
the vector indicates if the PP is optional, whereas another one indicates if the PP can be
iterated. Optionality and iterability are two of the criteria used by linguists to determine
whether a PP is an argument or an adjunct. In Section 5, we show how this distinction
supports a more refined formulation of the problem of PP attachment. We compare two
methods to reach a four-way classification. One method is a two-step process that first
classifies PPs as attached to the noun or to the verb, and then refines the classification
by assigning argument or adjunct status to the disambiguated PPs. The other method
is a one-step process that performs the four-way classification directly. We find that
the latter has better overall performance, confirming our expectation (Hypothesis 2).
In Section 6 we discuss the implications of the results for a definition of the notion of
argument and compare our work to that of the few researchers who have attempted to
perform the same distinction.
2. Distinguishing Arguments from Adjuncts
Solving the four-way classification task described in the introduction crucially relies
on the ability to distinguish arguments from adjuncts, using corpus counts. The ability
to automatically make this distinction is necessary for the correct automatic acquisi-
tion of important lexical knowledge, such as subcategorization frames and argument
structures, which is used in parsing, generation, machine translation, and information
extraction (Srinivas and Joshi 1999; Stede 1998; Dorr 1997; Riloff and Schmelzenbach
1998). Yet, few attempts have been made to make this distinction automatically.
The core difficulty in this enterprise is to define the notion of argument precisely
enough that it can be used automatically. There is a consensus in linguistics that argu-
ments and adjuncts are different both with respect to their function in the sentence and
in the way they themselves are interpreted (Jackendoff 1977; Marantz 1984; Pollard and
Sag 1987; Grimshaw 1990). With respect to their function, an argument fills a role in the
relation described by its associated head, whereas an adjunct predicates a separate prop-
erty of its associate head or phrase. With respect to their interpretation, a complement
is an argument if its interpretation depends exclusively on the head with which it is
associated, whereas it is an adjunct if its interpretation remains relatively constant when
associating with different heads (Grimshaw 1990, page 108). These semantic differences
give rise to some observable distributional consequences: for a given interpretation, an
adjunct can co-occur with a relatively broad range of heads, whereas arguments are
limited to co-occurrence with a (semantically restricted) class of heads (Pollard and Sag
1987, page 136).
Restricting the discussion to PPs, these differences are illustrated in the following
examples (PP-argument in bold); see also Schu?tze (1995, page 100).
(8a) Maria is a student of physics.
(8b) Maria is a student from Phoenix.
344
Merlo and Esteve Ferrer Argument in Prepositional Phrase Attachment
In example (8a), the head student implies that a subject is being studied. The sentence
tells us only one property of Maria: that she is a student of physics. In example (8b),
the PP instead predicates a different property of the student, namely her geographical
origin, which is not implied by the head student.
(9a) Kim camps/jogs/meditates on Sunday.
(9b) Kim depended/blamed the arson on Sandy.
In example (9a) the PP on Sunday can be construed without any reference to the pre-
ceding part of the sentence, and it preserves its meaning even when combining with
different heads. This is, however, not the case for (9b). Here, the PP can only be properly
understood in connection with the rest of the sentence: Sandy is the person on whom
someone depends or the person on which the arson is blamed.
These semantic distinctions between arguments and adjuncts surface in observable
syntactic differences and can be detected automatically both by using general formal
features and by specific lexical semantic features, which group together the arguments
of a lexical head. Unfortunately, the linguistic diagnostics that are used to determine
whether a PP is an adjunct or an argument are not accurate in all circumstances, they
often partition the set of the examples differently, and they give rise to relative, and not
absolute, acceptability judgments.
We propose a methodology that retains both the linguistic insight of the grammat-
ical tests and the ability to effectively combine several gradient, partial diagnostics,
typical of automatic induction methods. Specifically, we first find countable diagnostics
for the argument?adjunct distinction, which we approximate statistically and estimate
using corpus counts. We also augment the feature vector with information encoding
the semantic classes of the input words. The diagnostics and the semantic classes are
then automatically combined in a decision tree induction algorithm. The diagnostics
are presented below.
2.1 The Diagnostics
Many diagnostics for argumenthood have been proposed in the literature (Schu?tze
1995). Some of them require complex syntactic manipulation of the sentence, such as wh-
extraction, and are therefore too difficult to apply automatically. We choose six formal
diagnostics that can be captured by simple corpus counts: head dependence, optionality,
iterativity, ordering, copular paraphrase, and deverbal nominalization. These diagnos-
tics tap into the deeper semantic properties that distinguish arguments from adjuncts,
without requiring that the distinctions be made explicit.
Head Dependence. Arguments depend on their lexical heads because they form an inte-
gral part of the phrase. Adjuncts do not. Consequently, PP-arguments can only appear
with the specific verbal head by which they are lexically selected, whereas PP-adjuncts
can co-occur with a far greater range of different heads than arguments because they
are necessary for the correct interpretation of the semantics of the verb, as illustrated in
the following example sentences.
(10a) a man/woman/dog/moppet/scarecrow with gray hair
(10b) a menu/napkin/glass/waitress/matchbook from Rosie?s
345
Computational Linguistics Volume 32, Number 3
(11a) a member/*dog/*moppet/*scarecrow of Parliament
(11b) a student/*punk/*watermelon/*Martian/*poodle/*VCR of physics
We expect an argument PP to occur with fewer heads, whereas an adjunct PP will occur
with more heads, as it is not required by a specific verb, but it can in principle adjoin to
any verb or noun head.
We capture this insight by estimating the dispersion of the distribution of the
different heads that co-occur with a given PP in a corpus. We expect adjunct PPs to have
higher dispersion than argument PPs. We use entropy as a measure of the dispersion
of the distribution, as indicated in equation (1) (h indicates the noun or verb head to
which the PP is attached, X is the random variable whose outcomes are the values
of h).
hdep(PP) ? HPP(X) = ??h?Xp(h)log2p(h) (1)
Optionality. In most cases, PP-arguments are obligatory elements of a given sentence
whose absence leads to ungrammaticality, while adjuncts do not contribute to the
semantics of any particular verb, hence they are optional, as illustrated in the following
examples (PP-argument in bold):
(12a) John put the book in the room.
(12b) ?John put the book.
(12c) John saw/read the book in the room.
(12d) John saw/read the book.
Since arguments are obligatory complements of a verb, whereas adjuncts are not, we
expect knowledge of a given verb to be more informative with respect to the probability
of existence of an argument than of an adjunct. Thus we expect that the predictive
power of a verb with regard to its complements will be greater for arguments than for
adjuncts.1 The notion of optionality can be captured by the conditional probability of a
PP given a particular verbal head, as indicated in equation (2).
opt(PP) ? P(PP|v) (2)
Iterativity and Ordering. Because they receive a semantic role from the selecting verb,
arguments of the same type cannot be iterated because verbs can only assign any given
type of role once. Moreover, in English, arguments must be adjacent to the selecting
lexical head. Neither of these two restrictions apply to adjuncts, which can be iterated,
and follow arguments in a sequence of PPs. Consequently, in a sequence of several PPs
1 Notice that this diagnostic can only be interpreted as a statistical tendency, and not as a strict test,
because not all arguments are obligatory (but all adjuncts are indeed optional). The best known
descriptive exception to the criterion of optionality is the class of so-called object-drop verbs (Levin 1993).
Here a given verb may tolerate the omission of its argument. In other words, a transitive verb, such as
kiss, can also act like an intransitive. With respect to optional PPs, it has been argued that instrumentals
are arguments (Schu?tze 1995). While keeping these exceptions in mind, we maintain optionality as a
valid diagnostic here.
346
Merlo and Esteve Ferrer Argument in Prepositional Phrase Attachment
only the first one can be an argument, whereas the others must be adjuncts, as illustrated
in the examples below.
(13a) ?Chris rented the gazebo to yuppies, to libertarians.
(13b) Kim met Sandy in Baltimore in the hotel lobby in a corner.
These two criteria combined give rise to one diagnostic. The probability of a PP
being able to iterate, and consequently being an adjunct, can be approximated as
the probability of its occurrence in second position in a sequence of PPs, as indi-
cated in equation (3), where we indicate the position of the PP in a sequence as a
subscript.
iter(PP1) ? P(PP2) (3)
Copular Paraphrase. The diagnostic of copular paraphrase is specific to the distinction of
NPs arguments and adjuncts, following Schu?tze (1995, page 103). It does not apply to
VP arguments and adjuncts, as it requires paraphrasing the PP with a relative clause.
Arguments cannot be paraphrased by a copular relative clause, as the examples in (15)
show, whereas adjuncts can, as is shown in (14):
(14) a. a man from Paris a man who was from Paris
b. the albums on the shelf the albums that were on the shelf
c. the people on the payroll the people who were on the payroll
(15) a. the destruction of the city *the destruction that was of the city
b. the weight of the cow *the weight that was of the cow
c. a member of Parliament *a member who was of Parliament
This is because a PP attached to a noun as an argument does not predicate a secondary
property of the noun, but it specifies the same property that is indicated by the head.
To be able to use the relative clause construction, there must be two properties that are
being predicated of the same entity.
Thus, the probability that a PP is able to be paraphrased, and therefore that it
is an adjunct, can be approximated by the probability of its occurrence following a
construction headed by a copular verb, be, become, appear, seem, remain (Quirk et al 1985),
as indicated in equation (4), where ? indicates linear precedence.
para(PP) ? P(vcopula ? PP) (4)
Deverbal Nouns. This diagnostic is based on the observation that PPs following a de-
verbal noun are likely to be arguments, as the noun shares the argument structure
of the verb.2 Proper counting of this feature requires identifying a deverbal noun in
the head noun position of a noun phrase. We identify deverbal nouns by inspect-
ing their morphology (Quirk et al 1985). Specifically, the suffixes that can combine
2 Doubts have been cast on the validity of this diagnostic (Schu?tze 1995), based on work in theoretical
linguistics (Grimshaw 1990). Argaman and Pearlmutter (2002), however, have shown that the argument
structures of verbs and related nouns are highly correlated. Hence, we keep deverbal noun as a valid
diagnostic here, although we show later that it is not very effective.
347
Computational Linguistics Volume 32, Number 3
with verb bases to form deverbal nouns are listed and exemplified in Figure 1 on
page 348. This diagnostic can be captured by a probability indicator function, which
assigns probability 1 of being an argument to PPs following a deverbal noun and
0 otherwise.
deverb(PP) =
{
1 if deverbal n ? PP
0 otherwise (5)
In conclusion, the diagnostics of head dependence, optionality, iterativity, ordering,
copular paraphrase, and deverbal nominalization are promising indicators of the status
of PPs as either arguments or adjuncts. In Section 3 we illustrate how they can be
quantified in a faithful way and, thanks to their simplicity, how they can be estimated
in a sufficiently large corpus by simple counts.
Another class of features is also very important for the distinction between argu-
ments and adjuncts, the lexical semantic class to which the lexical heads belong, as we
illustrate below.
Lexical Semantic Class Features. According to Levin (1993), there is a regular mapping
between the syntactic and semantic behavior of a verb. This gives rise to a lexicon where
verbs that share similar syntactic and semantic properties are organized into classes.
More specifically, it is assumed that similar underlying components of meaning give rise
to similar subcategorization frames and projections of arguments at the syntactic level.
Since an argument participates in the subcategorization frame of the verb, whereas
an adjunct does not, we expect the argument-taking properties of verbs to be also
organized around semantic classes. We expect, therefore, that knowledge of the class of
the verb will be beneficial to the acquisition of the distinction between arguments and
adjuncts for an individual verb. For example, all verbs of giving take a dative indirect
object and all benefactive verbs can take a benefactive prepositional phrase complement
(see examples 16). An analogous prediction can be made for nouns. Unlike the diagnos-
tics features, these lexical features do not have a quantitative counterpart, but they are
represented as discrete nominal values that indicate the lexical semantic class the words
belong to.
(16a) Darcy offered a gift to Elizabeth.
(16b) Darcy cooked a roast for Elizabeth.
-ANT inhabitant, contestant, informant, participant, lubricant.
-EE appointee, payee, nominee, absentee, refugee.
-ER, OR singer, writer, driver, employer, accelerator, incubator, supervisor.
-AGE breakage, coverage, drainage, leverage, shrinkage, wastage.
-AL refusal, revival, dismissal.
-ION exploration, starvation, ratification, victimization, foundation.
-SION invasion, evasion.
-ING building, opening, filling, earnings, savings, shavings, wedding.
-MENT arrangement, amazement, puzzlement, embodiment, equipment.
Figure 1
Nominal endings that indicate deverbal derivation.
348
Merlo and Esteve Ferrer Argument in Prepositional Phrase Attachment
We use all these diagnostics, which in linguistics are used as tests of the argu-
ment status of PPs, as a distributed representation of argumenthood itself. We do not
assume that the syntactic representation of arguments is different from the represen-
tation of adjuncts; for example, we do not assume they have a different attachment
configuration, rather the diagnostics themselves determine a multidimensional space
in which PPs are positioned with different degrees of argumenthood. For such an
approach to work, we need to be able to transform each diagnostic into a symbolic
or numeric feature and combine the features in a precise way. In the two following
sections we illustrate how to calculate the values of each diagnostic using corpus-based
approximations and how to combine them with widely used automatic acquisition
techniques.
3. Methodology
The diagnostics described above can be estimated by simple corpus counts. The accu-
racy of the data collection is key to the success of the classifier induction based on these
counts. We explain the details of our methodology below.
3.1 Materials
We construct two corpora comprising examples of PP sequences. A PP is approximated
as the preposition and the PP-internal head noun. For example, with very many little chil-
dren will be represented as the bigram with children. One corpus contains data encoding
information for attachment of single PPs in the form of four head words (verb, object
noun, preposition, and PP internal noun) indicating the two possible attachment sites
and the most important words in the PP for each instance of PP attachments found in
the corpus. We also create an auxiliary corpus of sequences of two PPs, where each data
item consists of verb, direct object, and the two following PPs. This corpus is only used
to estimate the feature Iterativity. All the data were extracted from the Penn Treebank
using the tgrep tools (Marcus, Santorini, and Marcinkiewicz 1993). Our goal is to create a
more comprehensive and possibly more accurate corpus than the corpora used by Merlo
and Leybold (2001), Merlo, Crocker, and Berthouzoz (1997), and Collins and Brooks
(1995), among others. To improve coverage, we extracted all cases of PPs following
transitive and intransitive verbs and following nominal phrases. We include passive
sentences and sentences containing a sentential object. To improve accuracy, we insured
that we did not extract overlapping data, contrary to practice in previous PP corpora
construction, where multiple PP sequences were extracted more than once, each time
as part of a different structural configuration. For example, in previous corpora, the
sequence using crocidolite in filters in 1956, which is a sequence of two PPs, is counted
both as an example of a two PPs sequence as well as an example of a single PP
sequence, using crocidolite in filters. This technique of using subsequences as independent
examples is used both in the corpora used in Merlo and Leybold (2001) and (Merlo,
Crocker, and Berthouzoz 1997), and to an even larger extent in the corpus used in
Collins and Brooks (1995), who would also have in their corpus the artificially con-
structed sequence using crocidolites in 1956. This method increases the number of avail-
able examples, and it is therefore hoped that it will be beneficial to the learning accuracy.
However, as shown in Merlo, Crocker, and Berthouzoz (1997), it rests on the incorrect
assumption that the probability of an attachment is independent of the position of the
PP to be disambiguated in a sequence of multiple PPs. Therefore, we have decided not
349
Computational Linguistics Volume 32, Number 3
to decompose the examples into smaller sequences. The possible grammatical config-
urations that we have taken into account to construct the corpus are exemplified in
Appendix 1.
Once the quadruples constituting the data are extracted from the text corpus,
it is necessary to separate the statistics corpus from the training and test corpus.3
Before illustrating the adopted solution to this problem, let us define the following
terms.
 The statistics corpus CSt is the part of the corpus that is used to extract the
tuples that are used to calculate the features.
 The training corpus CTr is the part of the corpus that is used to extract the
tuples that are used as training data for the classifier.
 The testing corpus CTe is the part of the corpus that used to extract the
tuples that are used as testing data to evaluate the classifier.
 The training data STr is the set of tuples in CTr, augmented with the
features calculated using CSt.
 The testing data STe is the set of tuples in CTe, augmented with the features
calculated using CSt.
Note that for the testing data STe to be an independent test set, the testing corpus
CTe must be disjoint both from CTr, the training corpus, but also it must be disjoint
from CSt, the corpus on which the statistics are calculated. One possible solution, for
example, is to equate the statistics and the training corpus, CSt = CTr, and to assign them
Sections 1?22, while CTe = Section 23, thus making the testing corpus CTe be disjoint
from both the statistics and the training corpus. The problem with this solution is that
the training data STr is no longer extracted using the same process as the testing data
STe, and is therefore not good data from which to generalize. In particular, all tuples in
the training data STr necessarily also occur in the statistics corpus CSt, and therefore no
vectors in the training data STr involve data unseen in the statistics corpus. In contrast,
the testing data STe can be expected to include tuples that did not also occur in the
statistics corpus, and so the classifier might not generalize to these tuples using the
features we calculate.
The solution we adopt is to split Sections 1 to 22 into two disjoint subcorpora.
Because the Penn Treebank is not uniformly annotated across sections, we do not
assign whole sections to either the statistical or the training corpus, but instead ran-
domly assign individual sentences to either corpus. Since data sparseness is a more
important issue when calculating the features than it is for training the decision tree, we
assigned a larger proportion of the corpus to the statistical subcorpus. We assigned 25%
of Sections 1?22 to CTr and the rest to CSt. Section 24 is used as a development corpus
and Section 23 is the testing corpus CTe. In this setting, since the statistics, training, and
testing corpora are all mutually disjoint, all issues of dependence are resolved, and the
training data are representative of the real data we are interested in, so we can expect
our classifier to be able to generalize to new data.
3 We thank Eric Joanis for his help in correctly sampling the corpus and calculating the features.
350
Merlo and Esteve Ferrer Argument in Prepositional Phrase Attachment
3.2 The Counts of the Learning Features
As we said above, accurate estimates of the values of the features are crucial for the au-
tomatic learning algorithm to be successful. We illustrate the estimation of the features
below. Often several ways of estimating the features have been implemented, mostly to
address sparseness of data.
Lexical Word Classes. As indicated in the previous section, the head word of the governor
of the PP, the noun or the verb, is very directly related to the status of the PP. For
example, all giving verbs take a PP introduced by to, which is an argument of the verb.
The lexical semantic class of the head words is therefore going to be very relevant to the
disambiguation task.
The semantic grouping has been done automatically, using the lexicographic classes
in WordNet 1.7 (Miller et al 1990). Nouns are classified in different classes, among
which, for example, are animal, artifact, attribute, body, cognition, communication, event,
feeling, food, location, motive, person, plant, process, quantity, relation, shape, and substance.
This classification required selecting the most frequent WordNet sense for those polyse-
mous nouns being classified and generalizing to its class.4
For all the features below and where appropriate, we assume the following no-
tation. Let h be the head, that is, the verb in the features related to verb attachments
and the noun in the features related to noun attachment. Let p be the preposition, and
n2 be the object of the preposition. Let hcl and n2cl be the WordNet class of h and n2,
respectively. Let C(h, p, n2) be the frequency with which p, n2 co-occurs as a preposi-
tional phrase with h. Let C(h) be the frequency of h.
Head Dependence. Head dependence of a PP on a head is approximated by estimating
the dispersion of the PP over the possible heads. In a previous attempt to capture this
notion, we approximated by simply measuring the cardinality of the set of heads that
co-occur with a given PP in a corpus, as indicated in equation (6). The expectation was
that a low number indicated argument status, whereas a high number indicated adjunct
status (Merlo and Leybold 2001).
hdep(h, p, n2) = |{h1, h2, h3, . . . , hn}p,n2| (6)
By measuring the cardinality of the set of heads, we approximate the dispersion
of the distribution of heads by its range. This is a very rough approximation, as the
range of a distribution does not give any information on the distribution?s shape. The
range of a distribution might have the same value for a uniform distribution or a very
skewed distribution. Intuitively, we would like a measure that tells us that the former
corresponds to a verb with a much lower head dependence than the latter. Entropy is
4 The automatic annotation of nouns and verbs in the corpus has been done by matching them with
the WordNet database files. Before doing the annotation, though, some preprocessing of the data
was required to maximize the matching between our corpus and WordNet. The changes made
were inspired by those described in Stetina and Nagao (1997, page 75). To lemmatize the words
we used ?morpha,? a lemmatizer developed by John A. Carroll and freely available at the address:
http://www.informatics.susx.ac.uk./research/nlp/carroll/morph.html. Upon simple observation,
it showed a better performance than the frequently used Porter Stemmer for this task.
351
Computational Linguistics Volume 32, Number 3
a more informative measure of the dispersion of a distribution, which depends both on
the range and on the shape of a distribution.
The head dependence measure based on entropy, then, is calculated as indicated
in equation (7), which calculates the entropy of the probability distribution gener-
ated by the random variable X, whose values are all the heads that co-occur with a
given PP.
hdep(h, p, n2) = Hp,n2(X) ? ??h?X
C(h, p, n2)
?iC(hi, p, n2)
log2
C(h, p, n2)
?iC(hi, p, n2)
(7)
The counts that are used to estimate this measure will depend on finding exactly
PPs with the same PP internal noun, and on attaching to exactly the same lexical
head. We can expect these measures to suffer from sparse data. We implement then
some variants of this measure, where we cluster PPs according to the semantic content
of the PP-internal nouns and we cluster nominal heads according to their class. The
semantic grouping has been done automatically, as indicated in the paragraph above,
on calculating word classes. Since WordNet has a much finer-grained top-level classifi-
cation for nouns than for verbs, we found that grouping head nouns into classes yielded
useful generalizations, but it did not do so for verbs.
Therefore, we calculate head dependency in three different variants: One measure is
based on PP-internal noun tokens, another variant is based on noun classes for the PP-
internal noun position, and another variant is based on classes for both the PP-internal
and the head noun position, as indicated in equation (8).
hdep(h, p, n2) =
?
?
?
?
?
?
?
?
?
?
?
?
?
Hp,n2(X) ? ??h?X C(h,p,n2)?iC(hi,p,n2) log2
C(h,p,n2)
?iC(hi,p,n2)
, or
Hp,n2cl (X) ? ??h?X
C(h,p,n2cl )
?iC(hi,p,n2cl )
log2
C(h,p,n2cl )
?iC(hi,p,n2cl )
, or
Hp,n2cl (Xcl) ? ??hcl?X
C(hcl,p,n2cl )
?iC(hcl,i,p,n2cl )
log2
C(hcl,p,n2cl )
?iC(hcl,i,p,n2cl )
, if h = noun.
(8)
Optionality. As explained above, we expect that the predictive power of a verbal
head?recall that optionality does not apply to noun attachments?about its com-
plements will be greater for arguments than for adjuncts. This insight can be cap-
tured by the conditional probability of a PP given a particular verb, as indicated in
equation (9).
opt(v, p, n2) ? C(v, p, n2)
C(v)
(9)
Analogously to the measure of head dependence for noun attachments, optionality
is measured in three variants. First, it is calculated as a conditional probability based
on simple word counts in the corpus of single PPs, as indicated in equation (9) above.
Second, we also implement a variant that relies on verb classes instead of individual
verbs to address the problem of sparse data. Finally, we also implement a variant that
relies on noun classes for the PP-internal noun and verb classes instead of individual
verbs. For both these measures, verbs and nouns were grouped into classes using
352
Merlo and Esteve Ferrer Argument in Prepositional Phrase Attachment
WordNet 1.7 with the same method as for head dependence. The three measures of
optionality are indicated in equation (10).
opt(v, p, n2) =
?
?
?
?
?
?
?
?
?
?
?
?
?
P(p, n2|v) ? C(v,p,n2)C(v) , or
P(p, n2|vcl ) ? C(vcl,p,n2)C(vcl ) , or
P(p, n2cl|vcl ) ? C(vcl,p,n2cl )C(vcl ) .
(10)
Iterativity and Ordering. Iterativity and ordering are approximated by collecting counts
indicating the proportion of cases in which a given PP in first position had been found
in second position in a sequence of multiple PPs over the total of occurrences in any
position, as indicated in equation (11). The problem of sparse data here is especially
serious because of the small frequencies of multiple PPs. We addressed this problem by
using a backed-off estimation, where we replace lexical items by their WordNet classes
and collect counts on this representation. Specifically, the iterativity measure has been
implemented as follows.
Let C2nd(h, p, n2) be the frequency with which p, n2 occurs as a second prepositional
phrase with h, and Cany(h, p, n2) be the frequency with which it occurs with h in any
position.5 Then:
iter(h, p, n2) ?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
C2nd(h, p, n2)
Cany(h, p, n2)
, if Cany(h, p, n2) 	= 0, or else
C2nd(h, p, n2cl)
Cany(h, p, n2cl)
, if Cany(h, p, n2cl) 	= 0, or else
C2nd(hcl, p, n2cl)
Cany(hcl, p, n2cl)
.
(11)
Copular Paraphrase. Copular paraphrase is captured by calculating the proportion of
times a given PP is found in a copular paraphrase. We approximate this diagnostic by
making the hypothesis that a PP following a nominal head is an adjunct if it is also
found following a copular verb, be, become, appear, seem, remain (Quirk et al 1985). We
calculate then the proportion of times a given PP follows a copular verb over the times
it appears following any verb. This count is an approximation because even when we
find a copular verb, it might not be part of a relative clause. Here again, we back off to
the noun classes of the PP-internal noun to address the problem of sparse data.
para(h, p, n2) ?
?
?
?
?
?
?
?
?
?
C(vcopula ? (p, n2))
?iC(vi ? (p, n2))
, if C(vcopula) 	= 0, or else
C(vcopula ? (p, n2cl))
?iC(vi ? (p, n2cl))
.
(12)
5 Note that we approximate the prepositions occurring in any position by looking only at the first two
prepositions attached to the verb phrase.
353
Computational Linguistics Volume 32, Number 3
Deverbal Nominalization. The diagnostic of deverbal nouns is implemented as a binary
feature that simply indicates if the PP follows a deverbal noun or not.
deverb(n, p, n2) =
{
1 if deverbal n ? (p,n2)
0 otherwise (13)
Deverbal nouns are identified by inspecting their morphology (Quirk et al 1985).
As our corpus is lemmatized, we are confident that all the nouns in it are in their base
forms. The suffixes that can combine with verb bases to form deverbal nouns are shown
in Figure 1.
The counts that are collected in the way described above constitute a quanti-
fied vector corresponding to a single PP exemplar. These exemplars are the input to
an automatic classifier that distinguishes arguments from adjuncts, as described in
Section 4. Before we describe the experiments, however, attention must be paid to the
method that will be used to determine the target attribute?argument or adjunct?that
will be used to train the learner in the learning phase and to evaluate the accuracy of
the learned classifier in the testing phase.
3.3 The Target Attribute
Since we are planning to use a supervised learning method, we need to label each exam-
ple with a target attribute. Deciding whether an example is an instance of an argument
or of an adjunct requires making a distinction that the Penn Treebank annotators did
not intend to make. The automatic annotation of this attribute therefore must rely on
the existing labels for the PP that have been given by the Penn Treebank annotators,
inferring from them information that was not explicitly marked. We discuss here the
motivation for our interpretation.
The PTB annotators found that consistent annotation of argument status and se-
mantic role was not possible (Marcus et al 1994). The solution adopted, then, was
to structurally distinguish arguments from adjuncts only when the distinction was
straightforward and to label only some clearly distinguishable semantic roles. Doubt-
ful cases were left untagged. In the Penn Treebank structural distinctions concerning
arguments and adjuncts have been oversimplified: All constituents attached to VP are
structurally treated as arguments, whereas all constituents attached to NP are treated
as adjuncts. The only exception are the arguments of some deverbal nouns, which are
represented as arguments. Information about the distinction between arguments and
adjuncts, then, must be gleaned from the semantic and function tags that have been
assigned to the nodes. Constituents are labeled with up to four tags (including numer-
ical indices) that account for the syntactic category of the constituent, its grammatical
function, and its semantic role (Bies et al 1995). Figure 2 illustrates the tags that involve
PP constituents.
From the description of this set of tags we can already infer some information about
the argument status of the PPs. PPs with a semantic tag (LOC, MNR, PRP, TMP) are
adjuncts, whereas labels indicating PP complements of ditransitive verbs (BNF, DTV)
or locative verbs like put are arguments. There are, though, some cases that remain am-
biguous and therefore require a deeper study. These are untagged PPs and PPs tagged
-CLR. For these cases, we will necessarily only approximate the desired distinction. We
have interpreted untagged PPs as arguments of the verb. The motivation for this choice
comes both from an overall observation of sentences and from the documentation,
354
Merlo and Esteve Ferrer Argument in Prepositional Phrase Attachment
-CLR dative object if dative shift not possible (e.g., donate); phrasal verbs;
predication adjuncts
-DTV dative object if dative shift possible (e.g., give)
-BNF benefactive (dative object of for)
-PRD non VP predicates
-PUT locative complement of put
-DIR direction and trajectory
-LOC location
-MNR manner
-PRP purpose and reason
-TMP temporal phrases
Figure 2
Grammatical function and semantic tags that involve PP constituents in the Penn Treebank.
in which it is stated that ?NPs and Ss which are clearly arguments of the verb are
unmarked by any tag? (Marcus et al 1994, page 4), and that ?Direct Object NPs and
Indirect Object NPs are all untagged? (Bies et al 1995, page 12). Although the case of
PP constituents is not specifically addressed, we have interpreted these statements as
supporting evidence for our choice.
The tag -CLR stands for ?closely related,? and its meaning varies, depending on
the element it is attached to. It indicates argument status when it labels the dative
object of ditransitive verbs that cannot undergo dative shift, such as in donate money to
the museum, and in phrasal verbs, such as pay for the horse. It indicates adjunct status
when it labels a predication adjunct as defined by Quirk et al (1985). We interpret
the tag -CLR as an argument tag in order not to lose the few cases for which the
differentiation is certain: the ditransitive verbs and some phrasal verbs. This choice
apparently misclassifies predication adjuncts as arguments. However, for some cases,
such as obligatory predication adjuncts, an argument status might in fact be more
appropriate than an adjunct status. According to Quirk et al (1985, Sections 8.27?35,
15.22, pages 16?48), there are three types of adjuncts, differentiated by the degree of
?centrality? they have in the sentence. They can be classified into predication adjuncts
and sentence adjuncts. Predication adjuncts can be obligatory or optional. Obligatory
predication adjuncts resemble objects as they are obligatory in the sentence and they
have a relatively fixed position, as in He lived in Chicago. Optional predication adjuncts
are similarly central in the sentence but are not obligatory, as in He kissed his mother
on the cheek. Sentence adjuncts, on the contrary, have a lower degree of centrality in the
sentence, as in He kissed his mother on the platform. As a conclusion, obligatory predication
adjuncts as described in Quirk et al (1985) could be interpreted as arguments, as they
are required to interpret the verb (the interpretation of lived in He lived differs from the
one in He lived in Chicago).
To recapitulate, we have labeled our examples as follows:
 Adjuncts: All PPs tagged with a semantic tag (DIR, LOC, MNR, PRP, TMP)
are adjuncts.
 Arguments: All untagged PPs or PPs tagged with CLR, EXT, PUT, DTV,
BNF, or PRD are arguments.
Validating the Target Attribute and Creating a Gold Standard. The overall mapping of
Penn Treebank function labels onto the argument?adjunct distinction is certainly too
coarse, as some function types of PPs can be both arguments or adjuncts, depending
355
Computational Linguistics Volume 32, Number 3
on the head they co-occur with. We assessed the overall validity of the mapping as
follows. First, for each of the function tags mentioned earlier, we sampled the Penn
Treebank (one example from each section) for a total of 22 examples for each tag. Then,
we manually inspected the examples to determine if the mapping onto argument or
adjunct was correct. On a first inspection, the tags PUT, DTV, and PRD are correctly
mapped as argument in the majority of cases, as well as DIR, LOC, TMP, and PRP,
which are correctly considered adjuncts. Those samples tagged MNR, CLR, BNF, or
untagged show a more mixed behavior, sometimes appearing to label arguments and
sometimes adjuncts. For these labels, we used a more elaborate procedure to determine
if the example was an argument or an adjunct. We concentrate on PPs attached to the
verb, as these cases appear to be more ambiguous.
All the test examples attached to a verb that had a CLR, PP, or MNR label were
extracted. We did not investigate BNF as there aren?t any in our test file. We constructed
test suites for each example by applying to it the typical linguistic diagnostics used to
determine argumenthood, along the lines already discussed in Section 2. Five tests were
selected, which were found to be the most discriminating in a pilot study over 224 sen-
tences: optionality, ordering, head dependence, extraction with preposition stranding,
and extraction with pied-piping, as illustrated in Figure 3. It can be noticed that we were
able to use more complex tests than those used by the algorithm; in particular we use
extraction tests (Schu?tze 1995).6 A native speaker gave binary acceptability judgments
over the 1,100 sentences thus generated. The acceptability judgments were assigned to
the sentences over the course of several days. Once the judgments to each quintuple of
sentences were collected, they were combined into a single binary-valued target feature
for each sentence by the first author. The decision was based on the relative importance
and reliability of the tests according to the linguistic literature (Schu?tze 1995), as follows:
If the optionality test is negative then the example is an argument, else if extraction can take
place then the example is an argument, else the majority label according to the outcome of
the grammaticality judgments is assigned. In a few cases, the judgment was overidden
if the negative outcome of the tests clearly derived from the fact that the V + PP was
an idiom rather than an adjunct: for example, make fools of themselves, have a ring to it,
and live up to.
In the end, we find the following correlations between the automatic labels and
those assigned based on the accurate collection of native speaker judgments. Among the
PPs that do not have a function label, 65 are adjuncts and 42 are arguments, according
to the manual annotation procedure. The label PP-CLR corresponds to 18 adjuncts and
159 arguments, according to our manual annotation procedure, whereas the label PP-
MNR corresponds to 5 adjuncts and 1 argument. Clearly, the assignments of CLR PPs to
arguments and MNR PPs to adjunct are confirmed. Prepositional phrases without any
functional labels, on the other hand, are much more evenly divided between argument
and adjunct, as we suspected, given the heterogeneous nature of the label (the label PP
6 Some of the extracted sentences had to be simplified so that the verb and prepositional phrases were in a
main clause. For example buy shares from sellers, which is generated from the sentence On days like Friday,
that means they must buy shares from sellers when no one else is willing to becomes They must buy shares from
sellers. In some cases the sentences had to be further simplified to allow extraction tests to apply, which
would be violated for reasons unrelated to the argument-adjunct distinction in the PP, such as negation,
or complex NP islands. For example, Hill democrats are particularly angry over Mr. Bush?s claim that the
capital-gains cut was part of April?s budget accord and his insistence on combining it with the deficit-reduction
legislation yields Mr. Bush combines capital gains cut with the deficit-reduction legislation, which gives rise to
the following extraction examples: What do you wonder whether Mr. Bush combines capital gains cut with?
With what do you wonder whether Mr. Bush combines capital gains cut?
356
Merlo and Esteve Ferrer Argument in Prepositional Phrase Attachment
Americans will learn more about making products [ for the Soviets ].
optionality Americans will learn more about making products.
order Americans will learn more about making products these coming years
for the Soviets.
head dependence Americans will learn more about making/selling/reading products for
the Soviets.
extraction Who do you wonder whether Americans will learn more about making
products for?
For who(m) do you wonder whether Americans will learn more about
making products?
Figure 3
Example sentence and related list of tests and test sentences.
is assigned not only to those cases that are clear arguments, but also to those cases for
which a label cannot be decided). Moreover, if the hand-annotated label is reliable, it in-
dicates that untagged PPs are somewhat more likely to be adjuncts. Our initial mapping
was incorrect. In retrospect, we must conclude that other researchers had applied what
appears to be the correct mapping for this data set (Buchholz 1999). We do not, however,
modify the label of our training set, as that would be methodologically incorrect. We
have relabeled the test set and are therefore bound to ignore any further knowledge
we have gathered in relabeling, as that would amount to tailoring our training set to
our test set. The consequence of this difference between our label and what we found
to be true for the gold standard is that all results tested on the manually labeled test
set will have to be interpreted as lower bounds of the performance to be expected on a
consistently labeled data set. Besides validating the automatically annotated test set, the
manually annotated test set serves as a gold standard. Performance measures on this set
will support comparison across methodologies.
Therefore, we conclude that the mapping we have assumed is coherent with the
judgments of a native speaker, although the agreement is not perfect. PPs without
a function tag are an exception. Thus, the automatic mapping we have defined will
provide the value of the target feature in the experiments that we illustrate in the two
following sections. When appropriate we report two performance measures, one for
the automatic label and one for the partly manual labels. We will also report some
comparative results on a test set that does not contain the noisy PPs without function
labels.
4. Distinguishing Arguments from Adjuncts
Having collected the necessary data and established the value of the target attribute
for each example, we can now perform experiments to test several different hypotheses
concerning the learning of the argument-adjunct distinction. First of all we need to show
that the distinction under discussion can be learned to a good degree. Furthermore, we
investigate the properties that, singly or in combination, lead to an improvement in
learning. We are particularly interested in comparing the baseline to a simple model,
where learning is done using only lexical heads. In this case, we investigate the rele-
vance of the simple words in detecting the difference between arguments and adjuncts.
We also verify the usefulness of knowing lexical classes on the accuracy of learning.
Finally, we show that the diagnostic features and the lexical classes developed above
bring more information to the classification than what can be gathered by simple
357
Computational Linguistics Volume 32, Number 3
lexical heads. We summarize these expectations below, where we repeat and elaborate
Hypothesis 1 formulated in the introduction.
 Hypothesis 1: The argument-adjunct distinction can be performed based
on information collected from an annotated corpus.
 Hypothesis 1?: The argument-adjunct distinction can be improved by
lexical classes and linguistic diagnostic features, (i) over a simple baseline,
but also (ii) over a model using lexical features.
Demonstration of these hypotheses requires showing that the distinction can be
learned from corpus evidence, even with a simple method (performance is better than
chance). Hypothesis 1? imposes the more stringent condition that we can considerably
improve a simple learner by using more linguistically informed statistics (performance
is better than the simple method).
4.1 The Input Data and the Classifier
In order to test the argument and adjunct attachment problem independently of
whether the PP is attached to a verb or to a noun, we create two sets of input data,
one for verb attachment and one for noun attachment.
Each input vector for verb attachment contains training features comprising the
four lexical heads and their WordNet classes (v, n1, p, n2, vcl, n1cl, and n2cl), all the
different variants of the implementation of the diagnostics for the argument-adjunct
distinction concerning PPs attached to a verb, and one binary target feature, indicating
the type of attachment, whether argument or adjunct. More specifically, the features
implementing the diagnostics for verbs consist of the variants of the measures of head
dependence (hdepv1,hdepv2), the variants of the measure of optionality (opt1, opt2, opt3),
and the measure of iterativity (iterv).
Each input vector for noun attachment contains 14 training features. They comprise
the four lexical heads and their WordNet classes, as above (v, n1, p, n2, vcl, n1cl, and
n2cl), all the different variants of the implementation of the diagnostics for PPs attached
to a noun, and one binary target feature, indicating the type of attachment. The features
implementing the diagnostics for nouns are: the variants of the measures of head de-
pendence (hdepn1, hdepn2, hdepn3), the measures of iterativity (itern), and the measures
for copular paraphrase and deverbal noun, respectively (para, deverb).
For both types of experiments?distinction of arguments from adjuncts of PPs at-
tached to the noun or PPs attached to the verb?we use the C5.0 Decision Tree Induction
Algorithm (Quinlan 1993) and Support Vector Machines (LIBSVM), version 2.71 (Chang
and Lin 2001).
4.2 Results on V Attachment Cases
We have run experiments on the automatically labeled training and test sets with
very many different feature combinations. A summary of the most interesting patterns
of results are indicated in Tables 1 and 2. Significance of results is tested using a
McNemar test.
Contribution of Lexical Items and Their Classes. In this set of experiments we use only
lexical features or features that encode the lexical semantic classes of the open class
358
Merlo and Esteve Ferrer Argument in Prepositional Phrase Attachment
Table 1
Accuracy of the argument?adjunct distinction for VP-attached PPs, using combinations of lexical
features. The training and test sets are automatically annotated.
Feature used Auto accuracy (%)
1. Chance (args) 55.8
2. Prep (baseline) 67.9
3. Lexical features (verb, prep, n2) 67.9
4. vcl, prep 73.8
5. Prep, n2cl 71.9
6. Lexical features and classes (v, vcl, p, n2, n2cl) 68.2
7. Only classes (vcl, p, n2cl) 75.0
8. Only verb classes (vcl, p, n2) 73.1
9. Only noun classes (v, p, n2cl) 70.3
10. All features 68.2
words in question. Lines 2 to 5 of Table 1 report better classification values than
line 1. These results indicate that it is possible, at different degrees of accuracy, to
leverage information encoded in lexical items to infer the semantic distinction between
arguments and adjuncts without explicit deep semantic knowledge. One interesting
fact (lines 2 and 3) is that the preposition is very informative, as informative as all
the lexical items together. An analysis of the distribution of arguments and adjuncts
by preposition indicates that whereas most prepositions are ambiguous, they have a
strong preference for either arguments or adjuncts. Only a few equibiased prepositions,
such as for, exist. An expected result (lines 4 and 5) is that the PP-internal noun class
is useful, in combination with the preposition, as well as the combination of verb
class and preposition (the difference is marginally significant). We find the best re-
sult (line 7) in the experiment that uses class combinations (difference from baseline
p < .001). We see that classes of open class items, nouns, and verbs associated with
the closed class item preposition give the best performance. Line 7 is considerably
better than line 6 (p < .001), indicating that the actual presence of individual lexical
items is disruptive. This indicates that regularities in the distinction of arguments
from adjuncts is indeed a class phenomenon and not an item-specific phenomenon.
This is expected, according to current linguistic theories, such as the one proposed by
Levin (1993).
This analysis is confirmed by observing which are the most discriminative features,
those that are closest to the root of the decision tree. The topmost feature of the best
result (line 7) is preposition, followed by the class of the verb and the class of the PP
internal noun. Predictably, the class of the object noun phrase is not used because it is
not very informative. The same features constitute the tree yielding the results of line 6.
However, the presence of lexical items makes a difference to the tree that is built, which
is much more compact, but in the end less accurate.
Combinations of All Features. Table 2 reports the accuracy in the argument-adjunct dis-
tinction of experiments that use only the most useful lexical and class features, the
preposition and the verb class, and the diagnostic-based features, using combinations
of diagnostic features. The combinations of features shown are those that yielded the
best results over a development set of tuples extracted from Section 24 of the Penn
Treebank. The results reported are calculated over a test corresponding to the tuples
359
Computational Linguistics Volume 32, Number 3
Table 2
Best results using preposition and combination of diagnostic-based features, in different
variants. The training and test sets are automatically annotated.
Feature used Auto accuracy (%)
1. vcl, prep, hdepv1, hdepv2, opt1, opt2, opt3, iterv 79.3
2. vcl, prep, hdepv1, hdepv2, opt1, opt3, iterv 79.8
3. vcl, prep, hdepv1, hdepv2, opt2, opt3, iterv 79.0
4. vcl, prep, hdepv1, opt2, opt3, iterv 78.2
in Section 23 of the Penn Treebank. The best combination, line 2, yields a 37% reduction
of the error rate over the baseline. All the differences in performance among these
configurations are significant. What all these combinations have in common is that
they are combinations of three levels of granularity, mixing lexical information, class
information, and higher level syntactic-semantic information, encoded indirectly in the
diagnostics. All the combinations that are not shown here have lower accuracies.
Table 3 shows the confusion matrix for the combination of features with the best
accuracy listed above. These figures yield a precision and recall for arguments of 80%
and 85%, respectively (F measure = 82%); and a precision and recall for adjuncts of 80%
and 73%, respectively (F measure = 76%). Clearly, although both kinds of PPs are well
identified, arguments are better identified than adjuncts, an observation already made
by several other authors, especially Hindle and Rooth (1993) in their detailed discussion
of the errors in a noun or verb PP-attachment task. In particular, we notice that more
adjuncts are misclassified as arguments than vice versa.
The results of these experiments confirm that corpus information is conducive
to learning the distinction under discussion without explicitly represented complex
semantic knowledge. They also confirm that this distinction is essentially a word class
phenomenon?and not an individual lexical-item phenomenon?as would be expected
under current theories of the syntax?semantics interface. Finally, the combination of
lexical items, classes, and linguistic diagnostics yields the best results. This indicates
that using features of different levels of granularity is beneficial, probably because
the algorithm has the option of using more specific information when reliable, while
abstracting to coarser-grained information when lexical features suffer from sparse data.
This interpretation of the results is supported by observing which features are at the top
of the tree. Interestingly, here the topmost feature is head dependence (the lexical variant,
hdepv1), on one side of which we find preposition as the second most discriminative
feature, followed by head dependence (hdepv2) again, and optionality (class variants). On
Table 3
Confusion matrix of the best classification of PPs attached to the verb. Training and test set
established automatically.
Assigned classes
Arguments Adjuncts Total
Actual classes Arguments 300 51 351
adjuncts 76 202 278
Total 376 253 629
360
Merlo and Esteve Ferrer Argument in Prepositional Phrase Attachment
the other side of the tree, we find preposition as the second most informative feature and
verb class as the third most discriminative feature.
Results on Partly Manually Labeled Set. Tables 4 and 5 report the results obtained by train-
ing the classifier on the automatically labeled training set and testing on the manually
labeled test set. They illustrate the effect of training the decision tree classifier on a
training set that has different properties from the test set. This experiment provides a
lower bound of performance across different samples and shows which are the features
with the greatest generalization ability. We can draw several conclusions. First, the
lexical features do better than chance, but do not do better than the baseline established
by using only the preposition as a feature (lines 1, 2, and 3 of Table 4). Secondly,
classes do better than the baseline (line 7 of Table 4) and so do the diagnostic features
(Table 5). Since we are using a training and a test set with different properties, these
results indicate that classes and diagnostics capture a level of generality that the lexical
features do not have and will be more useful across domains and corpora. Finally,
the rank of performance for different feature combinations holds across training and
testing methods, whether established automatically or manually, as can be confirmed
by a comparison of Tables 1 and 4 and also 2 and 5. The difference in performance with
diagnostics (line 3 of Table 5) and without, using only classes (line 7 of Table 4), is only
marginally significant, indicating that diagnostics are not useless.
Table 4
Accuracy of the argument?adjunct distinction for VP-attached PPs, using combinations
of lexical features. The training set is automatically annotated while the test set is in part
annotated by hand.
Feature used Manual accuracy (%)
1. Chance (args) 37.0
2. Prep (baseline) 61.2
3. Lexical features (verb, prep, n2) 61.2
4. vcl, prep 67.1
5. Prep, n2cl 66.8
6. Lexical features and classes (v, vcl, p, n2, n2cl) 62.8
7. Only classes (vcl, p, n2cl) 70.0
8. Only verb classes (vcl, p, n2) 67.7
9. Only noun classes (v, p, n2cl) 64.9
10. All features 66.0
Table 5
Best results using prepositions and combination of diagnostic-based features in different
variants. The training set is automatically annotated, whereas the test set is in part annotated
by hand.
Feature used Manual accuracy (%)
1. Baseline (prep) 67.3
2. vcl, prep, hdepv1, hdepv2, opt1, opt2, opt3, iter 67.2
3. vcl, prep, hdepv1, hdepv2, opt1, opt3, iter 69.0
4. vcl, prep, hdepv1, hdepv2, opt2, opt3, iter 67.6
5. vcl, prep, hdepv1, opt2, opt3, iter 65.7
361
Computational Linguistics Volume 32, Number 3
Results on Test Set without Bare PPs. The biggest discrepancy in validating the automatic
labeling was found for PPs without functional tags. The automatic labeling had classi-
fied bare PPs as argument but the manual gold standard assigns more than half of them
to the adjunct class. They are therefore a source of noise in establishing reliable results. If
we remove these PPs from the training and test set, results improve and become almost
identical across the manually and automatically labeled sets, as illustrated in Table 6.7
4.3 Results on N Attachment Cases
Experiments on learning the distinction between argument PPs and adjunct PPs at-
tached to a noun show a different pattern, probably due to a ceiling effect. The ex-
periments reported below are performed on examples of prepositional phrases whose
preposition is not of . The reason to exclude the preposition of is that it is 99.8% of
the time attached as an argument. Moreover, it accounts for approximately half of the
cases of NP attachment. Results including this preposition would therefore be overly
optimistic and not be representative of the performance of the algorithm in general.
The size of the resulting corpus?without of prepositional phrases?is 3,364 tuples.
The results illustrated in Tables 7 and 8 show that head dependence is the only feature
that improves numerically the performance above the already very high baseline that
can be obtained by using only the feature preposition. This difference is not statistically
significant, indicating that neither class nor diagnostics add useful information.
4.4 Results Using a Different Learning Algorithm
In the previous sections, we have shown that different combinations of features yield
significantly different performances. We would like to investigate, at least on a first
approximation, if these results hold across different learning algorithms. To test the
stability of the results, we compare the main performances obtained with a decision tree
to those obtained with a different learning algorithm. In recent years, a lot of attention
has been paid to large margin classifiers, in particular support vector machines (SVMs)
(Vapnik 1995). They have been shown to perform quite well in many NLP tasks. The
fact that they search for a large separating margin between classes makes them less
prone to overfitting. We expect them, then, to perform well on the task trained on
automatically labeled data and tested on manually labeled data, where a great ability
to generalize is needed. Despite being very powerful, however, SVMs are complex
algorithms, often opaque in their output. They are harder to interpret than decision
trees, where the position of the features in the tree is a clear indication of their relevance
for the classification. Finally, SVMs take a longer time to train than decision trees. For
these reasons they constitute an interestingly different learning technique from decision
trees, but not a substitute for them if clear interpretation of the induced learner is needed
and many experiments need to be run.
All the experiments reported below were performed with the LIBSVM package
(Chang and Lin 2001, version 2.71). The SVM parameters were set by a grid search on
the training set, by 10-fold cross-validation. Given the longer training times, we perform
only a few experiments. For V attachment, the baseline using only the preposition
7 The features indicated here as best are the ones used in the other experiments and kept for the sake of
comparison. But, in fact, a different feature combination found by accident gives a result of 79.3%
accuracy in classifying the automatically labeled set.
362
Merlo and Esteve Ferrer Argument in Prepositional Phrase Attachment
Table 6
Best results using preposition and combination of diagnostic-based features, in different
variants, taking out PP examples.
Feature used Manual accuracy (%) Auto accuracy (%)
1. Chance baseline (args) 67.3 37.0
2. Prep baseline 64.4 64.6
3. Classes 74.5 74.7
4. Best features combination 76.1 77.0
Table 7
Baselines and performances using lexical heads and classes for N-attached PPs.
Features used Accuracy (%)
chance (arg) 58.3
p (baseline) 93.3
n1, p, n2 93.3
n1cl, p, n2cl 93.3
n1, n1cl, p, n2, n2cl 93.3
reaches an accuracy of 62.2% on the manually labeled test set, whereas performance
using the same best combination of features as the decision tree reaches 70.6% accuracy.
There is, then, a little improvement over the 69% of the decision tree learner, as expected.
The performance on the N-attached cases, on the other hand, is surprisingly poor, with
a low 43% accuracy on testing data. This result is probably due to overfitting, since the
best accuracy on the training set is around 95%.
4.5 Conclusions
The results reported in this section show that the argument-adjunct distinction can be
learned based on information collected from an annotated corpus with good accuracy.
For verb attachment, they show in particular that using lexical features yields better
performance than the baseline, especially when we use lexical classes. For automatically
labeled data, diagnostics based on linguistic theory improve the performance even
further. Thus, the hypotheses we were testing with these experiments are confirmed.
The reported results are good enough to be practically useful. In particular, the
distinction between arguments and adjuncts attached to nouns is probably performed
as well as possible with an automatic method, even by simply using prepositions
as features. For the attachment to verbs, known to be more difficult, more room for
Table 8
Performances using some combinations of features for N-attached PPs.
Features used Accuracy (%)
n1cl, prep, n2cl, hdepn1, hdepn2, hdepn3, itern, para, deverb 93.9
prep, hdepn1, hdepn2, hdepn3, itern, para, deverb 93.9
prep, hdepn1 94.1
363
Computational Linguistics Volume 32, Number 3
improvement exists, especially in the recovery of adjuncts. The comparison of decision
trees to SVMs does not appear to indicate that one learning algorithm is consistently
better than the other.
5. Hypothesis 2: PP Attachment Disambiguation
Once we have established the fact that arguments and adjuncts can be learned from
a corpus with reasonable accuracy using cues correlated to linguistic diagnostics, we
are ready to investigate how this distinction can be integrated in the disambiguation of
ambiguously attached PPs, the PP attachment problem as usually defined.
The first question that must be asked is whether the distinction between arguments
and adjuncts is so highly correlated with the attachment site of the ambiguous PP to
be almost entirely derivative. For example, the PTB annotators have annotated all noun
attachments as adjuncts and all verb attachments as arguments. If this were the correct
representation of the linguistic facts, having established an independent procedure to
discriminate argument from adjuncts PPs would be of little value in the disambiguation
problem. In fact, there is no theoretical reason to think that the notion of argument is
closely correlated to the choice of attachment site of a PP, given that both verb and noun
attached PPs can have either an argument or an adjunct function. It might be, however,
that some distributional differences that are lexically related, or simply nonlinguistic,
exist and that they can be exploited in an automatic learning process.
We can test the independence of the distribution of arguments and adjuncts from
the distribution of noun or verb attachment with a ?2 test. The test tells us that the two
distributions are not independent (p < .001). It remains, however, to be established if
the dependence of the two distributions is sufficiently strong to improve learning of
one of the two classifications, if the other is known. This question can be investigated
empirically by augmenting the training features for a learning algorithm that solves
the usual binary attachment problem with the diagnostic features for argumenthood.
If this augmentation results in an improvement in the accuracy of the PP attachment,
then we can say that the notion of argument is, at least in part, related to the attachment
site of a PP. If no improvement is found, then this confirms that the argument status
of a PP must be established independently. Conversely, we can augment the input to
the classification into argument and adjuncts with information related to the attach-
ment site to reach analogous conclusions about the distinction between arguments
and adjuncts.
Corpora and Materials. The data for the experiments illustrated below are drawn from
the same corpora as those used in the previous experiments. In particular, recall that the
corpus from which we draw the statistics is different from the corpus from which we
draw the training examples and also from the testing corpus. In both sets of experiments
described below, we restrict our observation to those examples in the corpus that are
ambiguous between the two attachment sites, as is usual in studies of PP attachment.
The values of the learning features were calculated on all the instances in the statistics
corpus, so in practice we use both the unambiguous cases and ambiguous cases in the
estimation of the features of the ambiguous cases.
The Input Data. Each input vector represents an instance of an ambiguous PP attachment,
which could be both noun or verb attached, either as an argument or as an adjunct.
Each vector contains 20 training features. They comprise the four lexical heads, their
364
Merlo and Esteve Ferrer Argument in Prepositional Phrase Attachment
WordNet classes (v, n1, p, n2, vcl, n1cl, and n2cl), and all the different variants of the im-
plementation of the diagnostics. Finally, depending on the experiments reported below
we use either a two-valued target feature (N or V) or a four-valued target feature (Narg,
Nadj, Varg, Vadj), indicating the type of attachment. More specifically, the features
implementing the diagnostics are the variants of the measures of head dependence for
the PPs attached to verbs and nouns, respectively (hdepv1, hdepv2, and hdepn1, hdepn2,
hdepn3); the variants of the measure of optionality (opt1, opt2, opt3); the measures of
iterativity for verb-attached and noun-attached PPs, respectively (iterv, itern); and finally
the measures for copular paraphrase and deverbal noun, respectively (para, deverb).
We use the C5.0 Decision Tree Induction Algorithm (Quinlan 1993), and the imple-
mentation of SVMs provided in version 2.71 of LIBSVM (Chang and Lin 2001).
5.1 Relationship of Noun?Verb Attachment Disambiguation to the
Argument-Adjunct Distinction and Vice Versa
Here we report on results for the task of disambiguating noun or verb attachment
first, using, among other input features, those that have been established to make the
argument-adjunct distinction. The same corpora described above were used, with a
two-valued target (N or V). We report results for two sets of experiments. One set of
experiments takes all examples into account. In another set of experiments, examples
containing the preposition of were not considered, as this preposition is extremely
frequent (it adds up to almost half of the noun attachment cases) and it is almost always
attached to a noun as argument. It has therefore a very peculiar behavior. The best
combination of features reported below was established using Section 24 of the Penn
Treebank; all the tests reported here are in Section 23.
Table 9 reports the disambiguation accuracy of the comparative experiments per-
formed. The first line reports the baseline accuracy for the task, calculated by per-
forming the classification using only the feature preposition. The best result is obtained
by a combination of features in which lexical classes act as the predominant learning
feature, either in combination with the lexical items or alone (line 2 with of , line 3
without of ). We note, however, that the diagnostic features that are included in the best
diagnostic feature combination are those based in part on individual words and not
those based entirely on classes. Most importantly, those diagnostics that are meant to
directly indicate the argument or adjunct status of a PP do not help in the resolution of
PP attachment, as expected.
Table 9
Percent accuracy using combinations of features for two-way attachment disambiguation.
Best combinations for experiments with of is (opt1, hdepv1, hdepn1, para) and for
experiments without of is (opt1, opt2, hdepv1, hdepv2, hdepn1, hdepn2, para).
Features used Accuracy with of (%) Accuracy without of (%)
1. Prep (baseline) 70.9 59.5
2. Prep + classes 78.1 71.3
3. Only classes 70.2 72.3
4. Only all diagnostics 75.9 64.5
5. Prep + all diagnostics 77.1 68.2
6. Prep + best feature combination 75.8 67.8
7. Prep + classes + best feature combination 76.6 67.5
365
Computational Linguistics Volume 32, Number 3
We conclude, then, that the notion of argument and adjunct is only partially cor-
related to the classification of PPs into those that attach to the noun and those that
attach to the verb. Clearly, diagnostics are not related to the attachment site, but lexical
classes appear to be. On the one hand, this result indicates that the notion of argument
is not entirely derivative from the attachment site. On the other hand, it shows that
some features developed to distinguish arguments from adjuncts could improve the
disambiguation of the attachment site.
The same conclusion is confirmed by a simpler, much more direct experiment,
where the classification into noun or verb attachment is performed with a single input
attribute. This attribute is the feature indicating if the example is an argument or an
adjunct, and it is calculated by a binary decision tree classifier using the best feature
combination on the argument-adjunct discrimination task. In this case too the classifi-
cation accuracy is a little (2.5%) better than chance baseline.
The converse experiment does not reveal any correlations, confirming that the
interdependence between the two factors is weak. The attachment status of the am-
biguous PP, whether noun or verb attached, is input among other features, to determine
whether the PP is an argument or an adjunct. Results are shown in Table 10, where
the attachment feature is called NVstatus. Lines 1 and 2 show that NVstatus is a
better baseline than chance. Lines 3 and 4 indicate that the feature preposition offers a
good baseline over which NVstatus improves only if the preposition of is included,
as expected. Lines 5 and 6 and lines 7 and 8 show that adding NVstatus to the other
features does not improve performance. As previously, the lexical classes are the best
performing features.
The same conclusion is reached by a simple direct experiment where we classify
PPs into arguments and adjuncts using as only input feature the output of a classifier
between noun or verb attachment. This attachment classifier is trained on the best
feature combination, preposition, and word classes. We find that the attachment status
has no effect on the accuracy of the classification, as the feature is not used.
Overall, these results indicate that there is a small interdependence between the two
classification problems and therefore weakly support a view of the PP disambiguation
problem where both the site of the attachment and the argument or adjunct function
of the PP are disambiguated together in one step. In the next section, we explore this
hypothesis, while also investigating which feature combinations give the best results in
a four-way PP classification, where PPs are classified as noun arguments, noun adjuncts,
verb arguments, and verb adjuncts.
Table 10
Percent accuracy using combinations of features for argument adjunct classification, including
NV as input feature. Best features combination is (opt1, opt3, hdepv1, hdepv2, hdepn1).
Features used Accuracy with of (%) Accuracy without of (%)
1. Chance (args) 69.6 55.9
2. NVstatus 73.0 62.3
3. Prep (baseline) 81.6 82.0
4. NVstatus + prep 87.2 81.5
5. Prep + classes 89.2 84.6
6. Prep + classes + NVstatus 89.0 84.1
7. Prep + classes + best features + NVstatus 88.2 82.9
8. Prep + classes + best features 88.2 83.6
366
Merlo and Esteve Ferrer Argument in Prepositional Phrase Attachment
5.2 One- and Two-step Four-way Classification
Having shown that argumenthood of a PP is not entirely derivative of its attachment
site, but that the two tasks are weakly correlated, the task of PP attachment itself is
reformulated as a four-way classification, yielding a finer-grained and more informative
classification of PP types.
As discussed in the introduction, those applications for which the resolution of the
PP attachment ambiguity is important often need to know more about the PP than its
attachment site, in particular one might need to know whether the PP is an argument
or not. For example, the output of a parser might be used to determine the kernel of a
sentence?the predicate with its arguments?for further text processing, for translation,
or for constructing a lexicon. We redefine therefore the problem of PP attachment as a
four-way classification problem. We investigate here what features are best predictors
of this classification. Again, we report results for two sets of experiments. One set of
experiments takes all examples into account, whereas the other excludes all examples
including the preposition of . As usual, the best combination of features reported below
was established using Section 24; all the tests reported here are in Section 23.
To classify PPs into four classes, we have two options: We can construct a single
four-class classifier or we can build a sequence of binary classifiers. The discrimination
between noun and verb attachment can be performed first, and then further refined into
attachment as argument or adjunct, performing the four-way classification in two steps.
The two-step approach would be the natural way of extending current PP attachment
disambiguation methods to the more specific four-way attachment we propose here.
However, based on the previous experiments, which showed a limited amount of
dependence between the two tasks, previous work on a similar data set (Merlo 2003),
and general wisdom in machine learning, there is reason to believe that it is better
to solve the four-way classification problem directly rather than first solving a more
general problem and then specializing the classification.
To test these expectations, we performed both kinds of experiments?a direct four-
way classification experiment and a two-step classification experiment?to investigate
which of the two methods is better. The direct four-way classification uses the attributes
described above to build a single classifier. For comparability, we created a two-step
experimental setup as follows. We created three binary classifiers. The first one performs
the noun?verb attachment classification. Its learning features comprise the four lexical
heads and their WordNet classes. We also train two classifiers that learn to distinguish
arguments from adjuncts. One classifier is trained only on verb-attachment exemplars
and uses only the best verb-attachment-related features. The third classifier is trained
only on noun-attachment exemplars and utilizes only the best noun-attachment-related
features. The test data is first given to the noun?verb attachment classifier. Then, the
test examples classified as verbs are given to the verb argument-adjunct classifier, and
the test examples classified as nouns are given to the noun argument-adjunct classifier.
Thus, this cascade of classifiers performs the same task as the four-way classifier, but it
does so in two passes.
Table 11 shows that overall the one-step classification is better than the two-step
classification, confirming the intuition that the two labeling problems should be solved
at the same time.8 However, if we break down the performance, we see that recall of
8 The difference between the two results is significant (p < .05) according to the randomized test described
in Yeh (2000).
367
Computational Linguistics Volume 32, Number 3
Table 11
Percent precision, recall, and F score for the best two-step and one-step four-way classification
of PPs, including and not including the preposition of.
Two-step + of One-step + of
Prec Rec F Prec Rec F
V-arg 37.5 45.6 41.2 42.2 29.3 34.6
V-adj 56.2 52.2 54.1 59.6 60.2 59.9
N-arg 83.0 83.5 83.2 81.3 91.3 86.0
N-adj 71.2 57.5 63.6 69.5 56.2 62.1
Accuracy 68.9 72.0
Two-step ? of One-step ? of
Prec Rec F Prec Rec F
V-arg 41.3 50.0 45.3 42.2 31.4 36.0
V-adj 52.8 41.6 46.5 59.6 60.2 59.9
N-arg 67.3 70.0 68.6 65.4 80.7 71.9
N-adj 60.3 60.3 60.3 69.5 56.2 62.1
Accuracy 56.6 60.9
V-arg is lower in the one-step procedure than in the two-step procedure, in both cases,
and that the overall performance for V-arg is worse in the one-step procedure. This
might indicate that which procedure to use depends on whether precision or recall or
overall performance is most important for the application at hand.
Table 12 reports the confusion matrix of the classification that reaches the best
performance without the preposition of, which corresponds to the lower right panel
of Table 11. It can be observed that performances are reasonably good for verb adjuncts,
and noun arguments and adjuncts, but they are quite poor for the classification of prepo-
sitional phrases that are arguments of the verb. It is not clear why verb arguments are so
badly classified. We tested the hypothesis that this result is a side effect of the mapping
we have defined from the Penn Treebank label to the label we use in this classifier,
arguments or adjuncts. Recall that untagged PPs have been mapped onto the argument
label, but these are highly inconsistent labels, as we have seen in the manual validation
of the target attribute in Section 3. Then, verb arguments might be represented by noisier
training examples. This hypothesis is not confirmed. In a little experiment where the
training data did not contain untagged verb-attached PPs, the overall performance in
identifying the verb argument class did not improve. An improvement in precision was
counteracted by a loss in recall, yielding slightly worse F measures. Another observation
related to verb arguments can be drawn by comparing the experiments reported in
Section 4 to the experiments reported in the current section. This comparison shows
that the low performance in classifying verb arguments does not arise because of an
inability to distinguish verb arguments from verb adjuncts. Rather, it is the interac-
tion of making this distinction and disambiguating the attachment site as a single
classification task that creates the problem. This is also confirmed by the considerable
number of cases of noun arguments and verb arguments that are incorrectly classi-
fied, as shown in Table 12. Clearly, further study of the properties of verb arguments
is needed.
368
Merlo and Esteve Ferrer Argument in Prepositional Phrase Attachment
Table 12
Confusion matrix of the best one-step four-way classification of PPs without the preposition of .
Assigned classes
V-arg V-adj N-arg N-adj Total
Actual classes V-arg 27 17 39 3 86
V-adj 15 68 18 12 113
N-arg 19 7 121 3 150
N-adj 3 22 7 41 73
Total 64 144 185 59 422
Overall, it is interesting to notice that solving the four-way task causes only a little
degradation to the accuracy of the original disambiguation between attachment to the
noun or to the verb. On this data set, the accuracy of disambiguating the attachment
site is of 83.6% (without PPs containing of ). Accuracy decreases a little to 82.7% if the
binary attachment disambiguation result is calculated on the output of the four-way
task. This little degradation is to be expected as the four-way task is more difficult. The
accuracy of the four-way task on the simple noun or verb binary attachment distinc-
tion seems, however, acceptable, if one considers that a finer-grained discrimination
is provided.
Table 13 reports the classification accuracy of a set of comparative experiments.
Here again, the first line reports the baseline accuracy for the task, calculated by
performing the classification using only the feature preposition. We notice that in both
columns the best results are obtained by the same combination of features that includes
some lexical features, some classes, and some diagnostic features. This shows that the
distinction between arguments and adjuncts is not exclusively a syntactic phenom-
enon and lexical variability plays an important role. Similarly to the previous set of
experiments, we note that the diagnostic features that are included in the best feature
combination are those based, at least in part, on individual words, and not those based
entirely on classes. The importance of lexical classes, however, is confirmed by the
fact that the best result is only marginally better than the second best result, in which
lexical classes act as the predominant learning feature, either in combination with the
lexical items or alone (line 3 with of , line 2 without of ). We can conclude from these
observations that using various features defined at different levels of granularity allows
the learner to better use lexical information when available, and to use more abstract
Table 13
Percent accuracy using combinations of features for a one-step four-way classification of PPs.
Best combination = (vcl, n1cl, p, opt1, opt2, hdepv1, hdepv2, hdepn1, para).
Features Accuracy (%) with of Without of (%)
1. Prep (baseline) 64.2 49.5
2. Prep + classes 68.9 60.2
3. Only classes 71.5 49.3
4. All features 68.9 54.5
5. Only diagnostics 67.4 54.3
6. Best combination 72.0 60.9
369
Computational Linguistics Volume 32, Number 3
levels of generalization when finer-grained information is not available. Across the two
tasks (illustrated in Tables 9 and 13) we notice that the best diagnostics features are
almost always the same. Variants change, but the kinds of diagnostics that are useful
remain stable. This probably indicates that some diagnostics are reliably estimated,
whereas others are not, and cannot be used fruitfully.
5.3 Results Using Support Vector Machines
As mentioned above, SVMs have yielded very good results in many important applica-
tions in NLP. It is reasonable to wonder if we can improve the results for the four-way
classification, and especially the less than satisfactory performance on verb arguments,
using this learning algorithm. Table 14 shows the results to be compared to those in the
right-hand panel of Table 11.
If we consider the F-measures of this table and the right-hand panel of Table 11, the
most striking difference is that V-arguments, although still the worst cell in the table,
have improved by almost 20% (36.0% vs. 55.9%) in performance for the experiments
without of. Notice that verb arguments are now better classified than in the two-step
method. Also, the overall accuracy is significantly improved by several percentage
points, especially for the condition without the preposition of (p < .02).
5.4 Conclusion
In this section we have shown that the notion of argument is of limited help in dis-
ambiguating the attachment of ambiguous PPs, indicating that the two notions are
not strictly related and must be established independently. In a series of four-way
classification experiments, we show that the classification performances are reasonably
good for verb adjuncts, noun arguments, and noun adjuncts, but they are poor for
the classification of prepositional phrases that are arguments of the verb, if decision
trees are used. Overall performance and especially identification of verb arguments
is improved if support vector machines are used. We also show that better accuracy
is achieved by performing the four-way classification in one step. The features that
appear to be most effective are lexical classes, thus confirming current linguistic theories
that postulate that a given head?s argument structure depends on the head?s lexical
semantics, especially for verbs (Levin 1993).
Table 14
Percent precision, recall, and F-score for the best four-way classification of PPs, including
and not including the preposition of using SVMs.
One-step + of One-step ? of
Prec Rec F Prec Rec F
V-arg 59.5 47.8 53.0 60.0 52.3 55.9
V-adj 63.2 63.7 63.4 62.8 62.8 62.8
N-arg 83.6 93.4 88.2 69.9 85.3 76.9
N-adj 72.5 50.7 59.7 72.5 50.7 59.7
Accuracy 75.9 66.6
370
Merlo and Esteve Ferrer Argument in Prepositional Phrase Attachment
6. Related Work
The resolution of the attachment of ambiguous PPs is one of the staple problems in
computational linguistics. It serves as a good testing ground for new methods as it
is clearly defined and self-contained. We have argued, however, that it is somewhat
oversimplified, because knowing only the attachment site of a PP is of relatively
little value in a real application. It would be more useful to know where the PP
is attached and with what function. We review below the few pieces of work that
have tackled the problem of labeling PPs by function (arguments or adjuncts) as a
separate labeling problem. Other pieces of work have asked a similar question in
the context of acquiring high-precision subcategorization frames. We review a few of
them below.
6.1 On the Automatic Distinction of Arguments and Adjuncts
A few other pieces of work attempt to distinguish PP arguments from adjuncts auto-
matically (Buchholz 1999; Merlo and Leybold 2001; Villavicencio 2002). We extend and
modify here the preliminary work reported in Merlo and Leybold (2001) by extending
the method to noun attachment, elaborating more learning features, including cases
specifically developed for noun attachment, refining all the counting methods, thus
validating and extending the approach.
The current work on automatic binary argument-adjunct classifiers appears to
compare favorably to the only other study on this topic (Buchholz 1999). Buchholz
(1999) reports an accuracy of 77% for the argument-adjunct distinction of PPs per-
formed with a memory-based learning approach, to be compared with our 80% and
94% for verb and noun attachments, respectively. However, the comparison cannot
be very direct, as Buchholz considers all types of attachment sites, not just verbs
and nouns.
More recently, Villavicencio (2002) has explored the performance of an argu-
ment identifier, developed in the framework of a model of child language learning.
Villavicencio concentrates on locative PPs proposing that the distinction between oblig-
atory arguments, optional arguments, and adjuncts is made based on two features: a
feature derived from a semantically motivated hierarchy of prepositions and predicates
and a simple frequency cutoff of 80% of co-occurrence between the verb and the PP
that distinguishes obligatory arguments from the other two classes. She evaluates the
verbs put, come, and draw (whose locative arguments belong to the three classes above,
respectively). The approach is not directly comparable, as it is not entirely corpus-based
(the input to the algorithm is an impoverished logical form), and the evaluation is on a
smaller scale than the present work. On a test set of the occurrences of three verbs, which
is the same set inspected to develop the learning features, Villavicencio gets perfect
performance. These are very promising results, but because they are not calculated on
a previously unseen test set, the generability of the approach is not clear. Moreover,
Villavicencio applies only one diagnostic test to determine if a PP is an argument or
an adjunct, whereas our extensive validation study has shown that several tests are
necessary to reach a reliable judgment.
In a study about the automatic acquisition of lexical dependencies for lexicon build-
ing, Fabre and Bourigault (2001) discuss the relation of the problem of PP attachment
and the notion of argument and adjunct. They correctly notice that approaches such
as theirs, inspired by Hindle and Rooth (1993), are based on the assumption that high
371
Computational Linguistics Volume 32, Number 3
co-occurrence between words is an indication of a lexical argumenthood relation. As
also noticed in Merlo and Leybold (2001), this is not always the case: some adjuncts
frequently co-occur with certain heads too. Fabre and Bourigault propose a notion of
productivity that strongly resembles our notions of optionality and head dependence to
capture the two intuitions about the distribution of arguments and adjuncts. Arguments
are strongly selected by the head (the head to complement relation is not productive),
whereas adjuncts can be selected by a wide spread of heads (the complement to head
selection is highly productive). They propose, but do not test, the hypothesis that
this notion might be useful for the general problem of PP attachment. The results
in the current article show that this is not the case. In fact, we have argued that
there is no real reason to believe that the two notions should be related, other than
marginally.
6.2 On the Distinction of Argument from Adjunct PPs for
Subcategorization Acquisition
As far as we are aware, this is the first attempt to integrate the notion of argumenthood
in a more comprehensive formulation of the problem of disambiguating the attachment
of PPs. Hindle and Rooth (1993) mention the interaction between the structural and the
semantic factors in the disambiguation of a PP, indicating that verb complements are the
most difficult. We confirm their finding that noun arguments are more easily identified,
whereas verb complements (either arguments or adjuncts) are more difficult. Other
pieces of work address the current problem in the larger perspective of distinguishing
arguments from adjuncts for subcategorization acquisition (Korhonen 2002a; Aldezabal
et al 2002).
The goal of the work of Korhonen (2002a, 2002b) is to develop a semantically driven
approach to subcategorization frame hypothesis selection that can be used to improve
large-scale subcategorization frame acquisition. The main idea underlying the approach
is to leverage the well-known mapping between syntax and semantics, inspired by
Levin?s (1993) work. Korhonen uses statistics over semantic classes of verbs to smooth
a distribution of subcategorization frames and then applies a simple frequency cutoff to
select the most reliable subcategorization frames. Her work is related to ours in several
ways. First, the automatic acquisition task leverages correspondences between syntax
and semantics, particularly clear in the organization of the verb lexicon, similarly
to Merlo and Stevenson (2001). Some of our current results are also based on this
correspondence, as we assume that the notion of argument is a notion at the interface
of the syntactic and semantic levels, and participates in both, determining not only the
valency of a verb but also its subcategorization frame. Our work confirms the results
reported in Korhonen (2002a), which indicate that using word classes improves the
extraction of subcategorization frames. Differently from Korhonen, however, we do
not allow feedback between levels. In her work, syntactic similarity of verbs? subcat-
egorization sets based on an external resource (LDOCE codes) are used to determine a
semantic classification of verbs?or rather to partially reorganize Levin?s classification.
This semantic classification is then used to collect statistics that are used to smooth a
subcategorization distribution. In our work, instead, we do use WordNet in some places
to give us information on verb classes, but we never use explicit semantic information
on the set of verb subcategorization frames to determine the notion of argument
or adjunct.
Aldezabal et al (2002) is another piece of work related to our current proposal.
In this article, the distinction between arguments and adjuncts is made to determine
372
Merlo and Esteve Ferrer Argument in Prepositional Phrase Attachment
the subcategorization frames of verbs for a language, Basque, for which not many de-
veloped resources exist. This illustrates another use of the distinction between argument
and adjuncts, which is not apparent when working on English.
6.3 On Learning Abstract Notions Using Corpus-based Statistical Methods
Learning arguments and adjuncts is an example of learning simplified semantic in-
formation by using syntactic and lexical semantic correlates. We learn the target con-
cepts of arguments and adjuncts by using corpus-based indicators of their proper-
ties. It remains to be determined if we just learn correlates of a unified notion, or if
the distinction between arguments and adjuncts is a clustering of possibly unrelated
properties.
As explained in the introduction, native speakers? judgments on the argument and
adjunct status of PPs are very unstable. No explanation is usually proposed of the
fact that the tests of argumenthood are often difficult to judge or even contradict each
other. As a possible explanation for the difficulty in pinpointing exactly the properties
of arguments and adjuncts, Manning (2003) suggests that the notion of argument or
adjunct is not categorical. The different properties of argument and adjuncts are not the
reflex of a single grammatical underlying notion, but they can be ascribed to different
mechanisms. What appears as a not entirely unified behavior is in fact better explained
as separate properties.
The current article provides a representation that can support both the categor-
ical and the gradient approach to the distinction between arguments and adjuncts.
We have decomposed the notion of argument into a vector of features. The notion
of argumenthood is no longer necessarily binary, but it allows several dimensions of
variation, each potentially related to a different principle of grammar. In the current
article, we have adopted a supervised approach to the learning task and adopted
a binary classification. To pursue a line of reasoning where a gradient representa-
tion of the notion of argument is preferred, we would no longer be interested in
classifying the vectorial information according to a predetermined binary or four-
way target value, as was done in the supervised learning experiments. The appropri-
ate framework would then be unsupervised learning, where several algorithms are
available to explore the hidden regularities of a vectorial representation of clusters
of PPs.
Whether a linguistic notion should be considered categorical or gradient is both
a matter of empirical fact and of the explanatory power of the theory into which the
notion is embedded. Assessing the strengths and weaknesses of these two approaches
is beyond the scope of the current article.
7. Conclusions
We have proposed an augmentation of the problem of PP attachment as a four-way
disambiguation problem, arguing that what is needed in interpreting prepositional
phrases is knowledge about both the structural attachment site (the traditional noun?
verb attachment distinction) and the nature of the attachment (the distinction of
arguments from adjuncts). Practically, we have proposed a method to learn arguments
and adjuncts based on a definition of arguments as a vector of features. Each feature
is either a lexical element or its semantic class or it is a numerical representation of
a diagnostic that is used by linguists to determine if a PP is an argument or not. We
373
Computational Linguistics Volume 32, Number 3
have shown in particular that using lexical classes as features yields good results, and
that diagnostics based on linguistic theory improve the performance even further. We
have also argued that the notion of argument does not help much in disambiguating
the attachment site of PPs, indicating that the two notions are not closely correlated
and must be established independently. We have performed a series of four-way
classification experiments, where we classify PPs as arguments or adjuncts of a noun,
and as arguments or adjuncts of a verb. We show that the classification performances are
reasonably good for verb adjuncts, noun arguments, and noun adjuncts, independent
of the learning algorithm. Classification performances of prepositional phrases that are
arguments of the verb are poor if decision trees are used, but are greatly improved by
the use of a large margin classifier. The features that appear to be most effective are
lexical classes, thus confirming current linguistic theories that postulate that a verb?s
argument structure depends on a verb?s lexical semantics (Levin 1993). Future work
lies in further investigating the difference between arguments and adjuncts to achieve
even finer-grained classifications and to model more precisely the semantic core of
a sentence.
1. Appendix: PP Configurations
Sequence of single PP attached to a verb
Configuration Structure Example
Transitive [vp V NP PP] join board as director
Passive [vp NP PP] tracked (yield) by report
Sentential Object [vp V NP PP] continued (to slide) amid signs
Intransitive [vp V PP] talking about years
Sequence of single PP attached to a noun
Noun phrase [np NP PP] form of asbestos
Transitive [vp V [np NP PP]] have information on users
Transitive with two PPs, one attached to verb, other to noun
[vp V [np NP PP] PP] dumped sacks of material into bin
Noun phrase with two PPs attached low
[np NP [pp P [np NP PP]]] exports at end of year
Transitive verb with two PPs attached low
[vp V [np NP [pp P [np NP PP]]]] lead team of researchers from institute
Transitive with two PPs, one attached to verb, other to other PP
[vp V NP [pp P [np NP PP]]] imposed ban on all of asbestos
Intransitive with two PPs, one attached to verb, other to noun
[vp V [pp P [np NP PP]]] appear in journal of medicine
Phrasal object with two PPs
[vp V NP [pp P [np NP PP]]] continued (to surge) on rumors of buying
Passive form with two PPs
[vp V NP [pp P [np NP PP]]] approved (request) by houses of Congress
Sequence of 2 PPs attached to a verb
Intransitive [vp V PP PP] grew by billion during week
Passive [vp V NP PP PP] passed (bill) by Senate in forms
Transitive [vp V NP PP PP] giving 16 to graders at school
Sequence of 2 PPs attached to a noun
Noun [np NP PP PP] sales of buses in October
Transitive [vp V [np NP PP PP]] meet demands for products in Korea
374
Merlo and Esteve Ferrer Argument in Prepositional Phrase Attachment
Acknowledgments
Most of this research was conducted thanks
to the generous support of the Swiss
National Science Foundation, under grant
1114-065328.01, while the second author
was a master?s student at the University
of Geneva. We thank Eric Joanis for his
precious help in constituting the database of
PP tuples. We also thank Jamie Henderson
for providing native-speaker judgments.
References
Aldezabal, Izaskun, Maxux Aranzabe, Koldo
Gojenola, Kepa Sarasola, and Aitziber
Atutxa. 2002. Learning argument/adjunct
distinction for Basque. In Proceedings of the
Workshop of the ACL Special Interest Group
on the Lexicon on Unsupervised Lexical
Acquisition, pages 42?50, Philadelphia, PA.
Argaman, Vered and Neal Pearlmutter. 2002.
Lexical semantics as a basis for argument
structure frequency bias. In Paola Merlo
and Suzanne Stevenson, editors, The
Lexical Basis of Sentence Processing: Formal,
Computational and Experimental Issues. John
Benjamins, Amsterdam/Philadelphia,
pages 303?324.
Baker, Collin F., Charles J. Fillmore, and
John B. Lowe. 1998. The Berkeley
FrameNet project. In Proceedings
of the Thirty-sixth Annual Meeting
of the Association for Computational
Linguistics and Seventeenth International
Conference on Computational Linguistics
(ACL-COLING?98), pages 86?90,
Montreal, Canada.
Bies, Ann, Mark Ferguson, Karen Katz, and
Robert MacIntyre. 1995. Bracketing
guidelines for Treebank II style, Penn
Treebank Project. Technical report,
University of Pennsylvania, Philadephia.
Buchholz, Sabine. 1999. Distinguishing
complements from adjuncts using
memory-based learning. ILK,
Computational Linguistics, Tilburg
University.
Chang, Chih-Chung and Chih-Jen Lin.
2001. LIBSVM: A Library for Support
Vector Machines. Software available at
http://www.csie.ntu.edu.tw/?cjlin/
libsvm.
Collins, Michael and James Brooks. 1995.
Prepositional phrase attachment
through a backed-off model. In
Proceedings of the Third Workshop on Very
Large Corpora, pages 27?38,
Cambridge, MA.
CoNNL. 2004. Eighth Conference on
Computational Natural Language Learning
(CoNLL-2004). Boston, MA.
CoNLL. 2005. Ninth Conference on
Computational Natural Language Learning
(CoNLL-2005). Ann Arbor, MI.
Dorr, Bonnie. 1997. Large-scale dictionary
construction for foreign language tutoring
and interlingual machine translation.
Machine Translation, 12(4):1?55.
Fabre, Ce?cile and Didier Bourigault. 2001.
Linguistic clues for corpus-based
acquisition of lexical dependencies. In
Proceedings of the Corpus Linguistics
Conference, pages 176?184, Lancaster, UK.
Gildea, Daniel and Daniel Jurafsky. 2002.
Automatic labeling of semantic roles.
Computational Linguistics, 28(3):245?288.
Grimshaw, Jane. 1990. Argument Structure.
MIT Press.
Hindle, Donald and Mats Rooth. 1993.
Structural ambiguity and lexical relations.
Computational Linguistics, 19(1):103?120.
Jackendoff, Ray. 1977. X? Syntax:
A Study of Phrase Structure. MIT Press,
Cambridge, MA.
Korhonen, Anna. 2002a. Semantically
motivated subcategorization acquisition.
In Proceedings of the Workshop of the ACL
Special Interest Group on the Lexicon on
Unsupervised Lexical Acquisition,
pages 51?58, Philadelphia, PA, July.
Korhonen, Anna. 2002b. Subcategorisation
Acquisition. Ph.D. thesis, University of
Cambridge.
Levin, Beth. 1993. English Verb Classes and
Alternations. University of Chicago Press,
Chicago, IL.
Manning, Christopher. 2003. Probabilistic
syntax. In Rens Bod, Jennifer Hay, and
Stephanie Jannedy, editors, Probabilistic
Linguistics. MIT Press, pages 289?314.
Marantz, Alex. 1984. On the Nature of
Grammatical Relations. MIT Press,
Cambridge, MA.
Marcus, M., G. Kim, A. Marcinkiewicz,
R. Macintyre, A. Bies, M. Ferguson,
K. Katz, and B. Schasberger. 1994. The
Penn Treebank: Annotating predicate
argument structure. In Proceedings of the
ARPA Workshop on Human Language
Technology, pages 114?119, Plainsboro, NJ.
Marcus, Mitch, Beatrice Santorini, and
Mary Ann Marcinkiewicz. 1993. Building a
large annotated corpus of English: The
Penn Treebank. Computational Linguistics,
19:313?330.
Merlo, Paola. 2003. Generalised
PP-attachment disambiguation using
corpus-based linguistic diagnostics.
375
Computational Linguistics Volume 32, Number 3
In Proceedings of the Tenth Conference
of the European Chapter of the
Association for Computational Linguistics
(EACL?03), pages 251?258, Budapest,
Hungary.
Merlo, Paola, Matt Crocker, and Cathy
Berthouzoz. 1997. Attaching multiple
prepositional phrases: Generalized
backed-off estimation. In Proceedings
of the Second Conference on Empirical
Methods in Natural Language Processing,
pages 145?154, Providence, RI.
Merlo, Paola and Matthias Leybold. 2001.
Automatic distinction of arguments and
modifiers: The case of prepositional
phrases. In Proceedings of the Fifth
Computational Natural Language Learning
Workshop (CoNLL-2001), pages 121?128,
Toulouse, France.
Merlo, Paola and Suzanne Stevenson. 2001.
Automatic verb classification based on
statistical distributions of argument
structure. Computational Linguistics,
27(3):373?408.
Miller, George, Richard Beckwith,
Christiane Fellbaum, Derek Gross,
and Katherine Miller. 1990. Five
papers on Wordnet. Technical report,
Cognitive Science Laboratory, Princeton
University.
Nielsen, Rodney and Sameer Pradhan. 2004.
Mixing weak learners in semantic parsing.
In Proceedings of the Conference on Empirical
Methods in Natural Language Processing
(EMNLP-2004), pages 80?87, Barcelona,
Spain, July.
Palmer, Martha, Daniel Gildea, and Paul
Kingsbury. 2005. The Proposition Bank:
An annotated corpus of semantic roles.
Computational Linguistics, 31:71?105.
Phillips, William and Ellen Riloff. 2002.
Exploiting strong syntactic heuristics and
co-training to learn semantic lexicons. In
Proceedings of the 2002 Conference on
Empirical Methods in Natural Language
Processing (EMNLP 2002), pages 125?132,
Philadelphia, PA.
Pollard, Carl and Ivan Sag. 1987. An
Information-based Syntax and Semantics,
volume 13. CSLI Lecture Notes,
Stanford University.
Quinlan, J. Ross. 1993. C4.5 : Programs for
Machine Learning. Series in Machine
Learning. Morgan Kaufmann,
San Mateo, CA.
Quirk, Randolph, Sidney Greenbaum,
Geoffrey Leech, and Jan Svartvik. 1985. A
Comprehensive Grammar of the English
Language. Longman, London.
Ratnaparkhi, Adwait. 1997. A linear
observed time statistical parser based on
maximum entropy models. In Proceedings
of the Second Conference on Empirical
Methods in Natural Language Processing,
pages 1?10, Providence, RI.
Ratnaparkhi, Adwait, Jeffrey Reynar, and
Salim Roukos. 1994. A maximum entropy
model for prepositional phrase
attachment. In Proceedings of the ARPA
Workshop on Human Language Technology,
pages 250?255, Plainsboro, NJ.
Riloff, Ellen and Mark Schmelzenbach.
1998. An empirical approach to conceptual
case frame acquisition. In Proceedings
of the Sixth Workshop on Very Large Corpora,
pages 49?56, Montreal.
Schu?tze, Carson T. 1995. PP Attachment and
Argumenthood. MIT Working Papers in
Linguistics, 26:95?151.
SENSEVAL-3. 2004. Third International
Workshop on the Evaluation of Systems for the
Semantic Analysis of Text (SENSEVAL-3).
Barcelona, Spain.
Srinivas, Bangalore and Aravind K. Joshi.
1999. Supertagging: An approach to
almost parsing. Computational Linguistics,
25(2):237?265.
Stede, Manfred. 1998. A generative
perspective on verb alternations.
Computational Linguistics, 24(3):401?430.
Stetina, Jiri and Makoto Nagao. 1997.
Corpus based PP attachment ambiguity
resolution with a semantic dictionary. In
Proceedings of the Fifth Workshop on Very
Large Corpora, pages 66?80, Beijing/
Hong Kong.
Swier, Robert and Suzanne Stevenson.
2005. Exploiting a verb lexicon in
automatic semantic role labelling.
In Proceedings of the Conference on
Empirical Methods in Natural Language
Processing (EMNLP-05), pages 883?890,
Vancouver, Canada.
Vapnik, V. 1995. The Nature of Statistical
Learning Theory. Springer.
Villavicencio, Aline. 2002. Learning to
distinguish PP arguments from adjuncts.
In Proceedings of the 6th Conference on
Natural Language Learning (CoNLL-2002),
pages 84?90, Taipei, Taiwan.
Xue, Nianwen. 2004. Handling dislocated
and discontinuous constituents in Chinese
semantic role labelling. In Proceedings of the
Fourth Workshop on Asian Language
Resources (ALR04), pages 19?26, Hainan
Island, China.
Xue, Nianwen and Martha Palmer. 2004.
Calibrating features for semantic role
376
Merlo and Esteve Ferrer Argument in Prepositional Phrase Attachment
labeling. In Proceedings of the 2004
Conference on Empirical Methods in Natural
Language Processing (EMNLP-2004),
pages 88?94, Barcelona, Spain.
Yeh, Alexander. 2000. More accurate tests for
the statistical significance of result
differences. In Proceedings of the 18th
International Conference in Computational
Linguistics (COLING 2000), pages 947?953,
Saarbruecken, Germany.
Zhao, Shaojun and Dekang Lin. 2004. A
nearest-neighbor method for resolving
PP-attachment ambiguities. In The First
International Joint Conference on Natural
Language Processing (IJCNLP-04),
pages 545?554, Hainan Island, China.
377

Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 101?104,
New York, June 2006. c?2006 Association for Computational Linguistics
Accurate Parsing of the Proposition Bank
Gabriele Musillo
Depts of Linguistics and Computer Science
University of Geneva
2 Rue de Candolle
1211 Geneva 4 Switzerland
musillo4@etu.unige.ch
Paola Merlo
Department of Linguistics
University of Geneva
2 Rue de Candolle
1211 Geneva 4 Switzerland
merlo@lettres.unige.ch
Abstract
We integrate PropBank semantic role la-
bels to an existing statistical parsing
model producing richer output. We show
conclusive results on joint learning and in-
ference of syntactic and semantic repre-
sentations.
1 Introduction
Recent successes in statistical syntactic parsing
based on supervised techniques trained on a large
corpus of syntactic trees (Collins, 1999; Charniak,
2000; Henderson, 2003) have brought the hope that
the same approach could be applied to the more am-
bitious goal of recovering the propositional content
and the frame semantics of a sentence. Moving to-
wards a shallow semantic level of representation has
immediate applications in question-answering and
information extraction. For example, an automatic
flight reservation system processing the sentence I
want to book a flight from Geneva to New York will
need to know that from Geneva indicates the origin
of the flight and to New York the destination.
(Gildea and Jurafsky, 2002) define this shallow
semantic task as a classification problem where the
semantic role to be assigned to each constituent is
inferred on the basis of probability distributions of
syntactic features extracted from parse trees. They
use learning features such as phrase type, position,
voice, and parse tree path. Consider, for example,
a sentence such as The authority dropped at mid-
night Tuesday to $ 2.80 trillion (taken from section
00 of PropBank (Palmer et al, 2005)). The fact that
to $ 2.80 trillion receives a direction semantic label
is highly correlated to the fact that it is a Preposi-
tional Phrase (PP), that it follows the verb dropped,
a verb of change of state requiring an end point, that
the verb is in the active voice, and that the PP is in
a certain tree configuration with the governing verb.
All the recent systems proposed for semantic role la-
belling (SRL) follow this same assumption (CoNLL,
2005).
The assumption that syntactic distributions will
be predictive of semantic role assignments is based
on linking theory. Linking theory assumes the ex-
istence of a hierarchy of semantic roles which are
mapped by default on a hierarchy of syntactic po-
sitions. It also shows that regular mappings from
the semantic to the syntactic level can be posited
even for those verbs whose arguments can take sev-
eral syntactic positions, such as psychological verbs,
locatives, or datives, requiring a more complex the-
ory. (See (Hale and Keyser, 1993; Levin and Rappa-
port Hovav, 1995) among many others.) If the inter-
nal semantics of a predicate determines the syntactic
expressions of constituents bearing a semantic role,
it is then reasonable to expect that knowledge about
semantic roles in a sentence will be informative of its
syntactic structure, and that learning semantic role
labels at the same time as parsing will be beneficial
to parsing accuracy.
We present work to test the hypothesis that a cur-
rent statistical parser (Henderson, 2003) can output
rich information comprising both a parse tree and
semantic role labels robustly, that is without any sig-
nificant degradation of the parser?s accuracy on the
original parsing task. We achieve promising results
both on the simple parsing task, where the accuracy
of the parser is measured on the standard Parseval
measures, and also on the parsing task where more
101
complex labels comprising both syntactic labels and
semantic roles are taken into account.
These results have several consequences. First,
we show that it is possible to build a single inte-
grated system successfully. This is a meaningful
achievement, as a task combining semantic role la-
belling and parsing is more complex than simple
syntactic parsing. While the shallow semantics of
a constituent and its structural position are often
correlated, they sometimes diverge. For example,
some nominal temporal modifiers occupy an object
position without being objects, like Tuesday in the
Penn Treebank representation of the sentence above.
The indirectness of the relation is also confirmed by
the difficulty in exploiting semantic information for
parsing. Previous attempts have not been success-
ful. (Klein and Manning, 2003) report a reduction
in parsing accuracy of an unlexicalised PCFG from
77.8% to 72.9% in using Penn Treebank function la-
bels in training. The two existing systems that use
function labels sucessfully, either inherit Collins?
modelling of the notion of complement (Gabbard,
Kulick and Marcus, 2006) or model function labels
directly (Musillo and Merlo, 2005). Furthermore,
our results indicate that the proposed models are ro-
bust. To model our task accurately, additional pa-
rameters must be estimated. However, given the cur-
rent limited availability of annotated treebanks, this
more complex task will have to be solved with the
same overall amount of data, aggravating the diffi-
culty of estimating the model?s parameters due to
sparse data.
2 The Data and the Extended Parser
In this section we describe the augmentations to our
base parsing models necessary to tackle the joint
learning of parse tree and semantic role labels.
PropBank encodes propositional information by
adding a layer of argument structure annotation to
the syntactic structures of the Penn Treebank (Mar-
cus et al, 1993). Verbal predicates in the Penn Tree-
bank (PTB) receive a label REL and their arguments
are annotated with abstract semantic role labels A0-
A5 or AA for those complements of the predicative
verb that are considered arguments while those com-
plements of the verb labelled with a semantic func-
tional label in the original PTB receive the com-
posite semantic role label AM-X , where X stands
for labels such as LOC, TMP or ADV, for locative,
temporal and adverbial modifiers respectively. Prop-
Bank uses two levels of granularity in its annotation,
at least conceptually. Arguments receiving labels
A0-A5 or AA do not express consistent semantic
roles and are specific to a verb, while arguments re-
ceiving an AM-X label are supposed to be adjuncts,
and the roles they express are consistent across all
verbs.
To achieve the complex task of assigning seman-
tic role labels while parsing, we use a family of
state-of-the-art history-based statistical parsers, the
Simple Synchrony Network (SSN) parsers (Hender-
son, 2003), which use a form of left-corner parse
strategy to map parse trees to sequences of deriva-
tion steps. These parsers do not impose any a pri-
ori independence assumptions, but instead smooth
their parameters by means of the novel SSN neu-
ral network architecture. This architecture is ca-
pable of inducing a finite history representation of
an unbounded sequence of derivation steps, which
we denote h(d1, . . . , di?1). The representation
h(d1, . . . , di?1) is computed from a set f of hand-
crafted features of the derivation move di?1, and
from a finite set D of recent history representations
h(d1, . . . , dj), where j < i ? 1. Because the his-
tory representation computed for the move i ? 1
is included in the inputs to the computation of the
representation for the next move i, virtually any in-
formation about the derivation history could flow
from history representation to history representation
and be used to estimate the probability of a deriva-
tion move. In our experiments, the set D of ear-
lier history representations is modified to yield a
model that is sensitive to regularities in structurally
defined sequences of nodes bearing semantic role
labels, within and across constituents. For more
information on this technique to capture structural
domains, see (Musillo and Merlo, 2005) where the
technique was applied to function parsing. Given
the hidden history representation h(d1, ? ? ? , di?1) of
a derivation, a normalized exponential output func-
tion is computed by the SSNs to estimate a proba-
bility distribution over the possible next derivation
moves di.
To exploit the intuition that semantic role labels
are predictive of syntactic structure, we must pro-
102
vide semantic role information as early as possible
to the parser. Extending a technique presented in
(Klein and Manning, 2003) and adopted in (Merlo
and Musillo, 2005) for function labels with state-
of-the-art results, we split some part-of-speech tags
into tags marked with AM-X semantic role labels.
As a result, 240 new POS tags were introduced to
partition the original tag set which consisted of 45
tags. Our augmented model has a total of 613 non-
terminals to represent both the PTB and PropBank
labels, instead of the 33 of the original SSN parser.
The 580 newly introduced labels consist of a stan-
dard PTB label followed by one or more PropBank
semantic roles, such as PP-AM-TMP or NP-A0-A1.
These augmented tags and the new non-terminals
are included in the set f , and will influence bottom-
up projection of structure directly.
These newly introduced fine-grained labels frag-
ment our PropBank data. To alleviate this problem,
we enlarge the set f with two additional binary fea-
tures. One feature decides whether a given preter-
minal or nonterminal label is a semantic role label
belonging to the set comprising the labels A0-A5
and AA. The other feature indicates if a given la-
bel is a semantic role label of type AM-X , or oth-
erwise. These features allow the SSN to generalise
in several ways. All the constituents bearing an A0-
A5 and AA labels will have a common feature. The
same will be true for all nodes bearing an AM-X la-
bel. Thus, the SSN can generalise across these two
types of labels. Finally, all constituents that do not
bear any label will now constitute a class, the class
of the nodes for which these two features are false.
3 Experiments and Discussion
Our extended semantic role SSN parser was trained
on sections 2-21 and validated on section 24 from
the PropBank. Testing data are section 23 from the
CoNLL-2005 shared task (Carreras and Marquez,
2005).
We perform two different evaluations on our
model trained on PropBank data. We distinguish be-
tween two parsing tasks: the PropBank parsing task
and the PTB parsing task. To evaluate the former
parsing task, we compute the standard Parseval mea-
sures of labelled recall and precision of constituents,
taking into account not only the 33 original labels,
but also the newly introduced PropBank labels. This
evaluation gives us an indication of how accurately
and exhaustively we can recover this richer set of
non-terminal labels. The results, computed on the
testing data set from the PropBank, are shown in the
PropBank column of Table 1, first line. To evaluate
the PTB task, we ignore the set of PropBank seman-
tic role labels that our model assigns to constituents
(PTB column of Table 1, first line to be compared to
the third line of the same column).
To our knowledge, no results have yet been pub-
lished on parsing the PropBank.1 Accordingly, it
is not possible to draw a straightforward quantita-
tive comparison between our PropBank SSN parser
and other PropBank parsers. However, state-of-the-
art semantic role labelling systems (CoNLL, 2005)
use parse trees output by state-of-the-art parsers
(Collins, 1999; Charniak, 2000), both for training
and testing, and return partial trees annotated with
semantic role labels. An indirect way of compar-
ing our parser with semantic role labellers suggests
itself. 2 We merge the partial trees output by a se-
mantic role labeller with the output of the parser on
which it was trained, and compute PropBank parsing
performance measures on the resulting parse trees.
The third line, PropBank column of Table 1 reports
such measures summarised for the five best seman-
tic role labelling systems (Punyakanok et al, 2005b;
Haghighi et al, 2005; Pradhan et al, 2005; Mar-
quez et al, 2005; Surdeanu and Turmo, 2005) in
the CoNLL 2005 shared task. These systems all
use (Charniak, 2000)?s parse trees both for train-
ing and testing, as well as various other information
sources including sets of n-best parse trees, chunks,
or named entities. Thus, the partial trees output by
these systems were merged with the parse trees re-
turned by Charniak?s parser (second line, PropBank
column).3
These results jointly confirm our initial hypothe-
1(Shen and Joshi, 2005) use PropBank labels to extract
LTAG spinal trees to train an incremental LTAG parser, but they
do not parse PropBank. Their results on the PTB are not di-
rectly comparable to ours as calculated on dependecy relations
and obtained using gold POS.
2Current work aims at extending our parser to recovering the
argument structure for each verb, supporting a direct compari-
son to semantic role labellers.
3Because of differences in tokenisations, we retain only
2280 sentences out of the original 2416.
103
PTB PropBank
SSN+Roles model 89.0 82.8
CoNLL five best - 83.3?84.1
Henderson 03 SSN 89.1 -
Table 1: Percentage F-measure of our SSN parser on
PTB and PropBank parsing, compared to the origi-
nal SSN parser and to the best CoNLL 2005 SR la-
bellers.
sis. The performance on the parsing task (PTB col-
umn) does not appreciably deteriorate compared to
a current state-of-the-art parser, even if our learner
can output a much richer set of labels, and there-
fore solves a considerably more complex problem,
suggesting that the relationship between syntactic
PTB parsing and semantic PropBank parsing is strict
enough that an integrated approach to the problem
of semantic role labelling is beneficial. Moreover,
the results indicate that we can perform the more
complex PropBank parsing task at levels of accuracy
comparable to those achieved by the best seman-
tic role labellers (PropBank column). This indicates
that the model is robust, as it has been extended to a
richer set of labels successfully, without increase in
training data. In fact, the limited availability of data
is increased further by the high variability of the ar-
gumental labels A0-A5 whose semantics is specific
to a given verb or a given verb sense.
Methodologically, these initial results on a joint
solution to parsing and semantic role labelling pro-
vide the first direct test of whether parsing is neces-
sary for semantic role labelling (Gildea and Palmer,
2002; Punyakanok et al, 2005a). Comparing se-
mantic role labelling based on chunked input to the
better semantic role labels retrieved based on parsed
trees, (Gildea and Palmer, 2002) conclude that pars-
ing is necessary. In an extensive experimental in-
vestigation of the different learning stages usually
involved in semantic role labelling, (Punyakanok et
al., 2005a) find instead that sophisticated chunking
can achieve state-of-the-art results. Neither of these
pieces of work actually used a parser to do SRL.
Their investigation was therefore limited to estab-
lishing the usefulness of syntactic features for the
SRL task. Our results do not yet indicate that pars-
ing is beneficial to SRL, but they show that the joint
task can be performed successfully.
Acknowledgements We thank the Swiss NSF for sup-
porting this research under grant number 101411-105286/1,
James Henderson and Ivan Titov for sharing their SSN software,
and Xavier Carreras for providing the CoNLL-2005 data.
References
X. Carreras and L. Marquez. 2005. Introduction to the CoNLL-
2005 shared task: Semantic role labeling. Procs of CoNLL-
2005.
E. Charniak. 2000. A maximum-entropy-inspired parser.
Procs of NAACL?00, pages 132?139, Seattle, WA.
M. Collins. 1999. Head-Driven Statistical Models for Natural
Language Parsing. Ph.D. thesis, Pennsylvania.
CoNLL. 2005. Ninth Conference on Computational Natural
Language Learning (CoNLL-2005), Ann Arbor, MI.
R. Gabbard, S. Kulick and M. Marcus 2006. Fully parsing the
Penn Treebank. Procs of NAACL?06, New York, NY.
D. Gildea and D. Jurafsky. 2002. Automatic labeling of seman-
tic roles. Computational Linguistics, 28(3):245?288.
D. Gildea and M. Palmer. 2002. The necessity of parsing for
predicate argument recognition. Procs of ACL 2002, 239?
246, Philadelphia, PA.
A. Haghighi, K. Toutanova, and C. Manning. 2005. A joint
model for semantic role labeling. Procs of CoNLL-2005,
Ann Arbor, MI.
K. Hale and J. Keyser. 1993. On argument structure and the
lexical representation of syntactic relations. In K. Hale and
J. Keyser, editors, The View from Building 20, 53?110. MIT
Press.
J. Henderson. 2003. Inducing history representations
for broad-coverage statistical parsing. Procs of NAACL-
HLT?03, 103?110, Edmonton, Canada.
D. Klein and C. Manning. 2003. Accurate unlexicalized pars-
ing. Procs of ACL?03, 423?430, Sapporo, Japan.
B. Levin and M. Rappaport Hovav. 1995. Unaccusativity. MIT
Press, Cambridge, MA.
M. Marcus, B. Santorini, and M.A. Marcinkiewicz. 1993.
Building a large annotated corpus of English: the Penn Tree-
bank. Computational Linguistics, 19:313?330.
L. Marquez, P. Comas, J. Gimenez, and N. Catala. 2005. Se-
mantic role labeling as sequential tagging. Procs of CoNLL-
2005.
P. Merlo and G. Musillo. 2005. Accurate function parsing.
Procs of HLT/EMNLP 2005, 620?627, Vancouver, Canada.
G.Musillo and P. Merlo. 2005. Lexical and structural biases
for function parsing. Procs of IWPT?05, 83?92, Vancouver,
Canada.
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The Proposition
Bank: An annotated corpus of semantic roles. Computa-
tional Linguistics, 31:71?105.
S. Pradhan, K. Hacioglu, W. Ward, J. Martin, and D. Jurafsky.
2005. Semantic role chunking combining complementary
syntactic views. Procs of CoNLL-2005.
V. Punyakanok, D. Roth, and W. Yih. 2005a. The necessity
of syntactic parsing for semantic role labeling. Procs of IJ-
CAI?05, Edinburgh, UK.
V. Punyakanok, P. Koomen, D. Roth, and W. Yih. 2005b. Gen-
eralized inference with multiple semantic role labeling sys-
tems. Procs of CoNLL-2005.
L.Shen and A. Joshi. 2005. Incremental LTAG parsing. Procs
of HLT/EMNLP 2005, Vancouver, Canada.
M. Surdeanu and J. Turmo. 2005. Semantic role labeling using
complete syntactic analysis. Procs of CoNLL-2005.
104
Proceedings of NAACL HLT 2009: Short Papers, pages 125?128,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Domain Adaptation with Artificial Data for Semantic Parsing of Speech
Lonneke van der Plas
Department of Linguistics
University of Geneva
Geneva, Switzerland
James Henderson
Department of Computer Science
University of Geneva
Geneva, Switzerland
{Lonneke.vanderPlas,James.Henderson,Paola.Merlo}@unige.ch
Paola Merlo
Department of Linguistics
University of Geneva
Geneva, Switzerland
Abstract
We adapt a semantic role parser to the do-
main of goal-directed speech by creating an
artificial treebank from an existing text tree-
bank. We use a three-component model that
includes distributional models from both tar-
get and source domains. We show that we im-
prove the parser?s performance on utterances
collected from human-machine dialogues by
training on the artificially created data without
loss of performance on the text treebank.
1 Introduction
As the quality of natural language parsing improves
and the sophistication of natural language under-
standing applications increases, there are several do-
mains where parsing, and especially semantic pars-
ing, could be useful. This is particularly true in
adaptive systems for spoken language understand-
ing, where complex utterances need to be translated
into shallow semantic representation, such as dia-
logue acts.
The domain on which we are working is goal-
directed system-driven dialogues, where a system
helps the user to fulfil a certain goal, e.g. booking a
hotel room. Typically, users respond with short an-
swers to questions posed by the system. For exam-
ple In the South is an answer to the question Where
would you like the hotel to be? Parsing helps iden-
tifying the components (In the South is a PP) and
semantic roles identify the PP as a locative, yield-
ing the following slot-value pair for the dialogue act:
area=South. A PP such as in time is not identified as
a locative, whereas keyword-spotting techniques as
those currently used in dialogue systems may pro-
duce area=South and area=time indifferently.
Statistical syntactic and semantic parsers need
treebanks. Current available data is lacking in one or
more respects: Syntactic/semantic treebanks are de-
veloped on text, while treebanks of speech corpora
are not semantically annotated (e.g. Switchboard).
Moreover, the available human-human speech tree-
banks do not exhibit the same properties as the
system-driven speech on which we are focusing, in
particular in their proportion of non-sentential utter-
ances (NSUs), utterances that are not full sentences.
In a corpus study of a subset of the human-human
dialogues in the BNC, Ferna?ndez (2006) found that
only 9% of the total utterances are NSUs, whereas
we find 44% in our system-driven data.
We illustrate a technique to adapt an exist-
ing semantic parser trained on merged Penn Tree-
bank/PropBank data to goal-directed system-driven
dialogue by artificial data generation. Our main con-
tribution lies in the framework used to generate ar-
tificial data for domain adaptation. We mimic the
distributions over parse structures in the target do-
main by combining the text treebank data and the
artificially created NSUs, using a three-component
model. The first component is a hand-crafted model
of NSUs. The second component describes the dis-
tribution over full sentences and types of NSUs as
found in a minimally annotated subset of the target
domain. The third component describes the distribu-
tion over the internal parse structure of the generated
data and is taken from the source domain.
Our approach differs from most approaches to do-
main adaptation, which require some training on
fully annotated target data (Nivre et al, 2007),
whereas we use minimally annotated target data
only to help determine the distributions in the ar-
tificially created data. It also differs from previ-
125
ous work in domain adaptation by Foster (2007),
where similar proportions of ungrammatical and
grammatical data are combined to train a parser
on ungrammatical written text, and by Weilhammer
et al (2006), who use interpolation between two
separately trained models, one on an artificial cor-
pus of user utterances generated by a hand-coded
domain-specific grammar and one on available cor-
pora. Whereas much previous work on parsing
speech has focused on speech repairs, e.g. Charniak
and Johnson (2001), we focus on parsing NSUs.
2 The first component: a model of NSUs
To construct a model of NSUs we studied a subset of
the data under consideration: TownInfo. This small
corpus of transcribed spoken human-machine dia-
logues in the domain of hotel/restaurant/bar search
is gathered using the TownInfo tourist information
system (Lemon et al, 2006).
The NSUs we find in our data are mainly of the
type answers, according to the classification given
in Ferna?ndez (2006). More specifically, we find
short answers, plain and repeated affirmative an-
swers, plain and helpful rejections, but also greet-
ings.
Current linguistic theory provides several ap-
proaches to dealing with NSUs (Merchant, 2004;
Progovac et al, 2006; Ferna?ndez, 2006). Follow-
ing the linguistic analysis of NSUs as non-sentential
small clauses (Progovac et al, 2006) that do not have
tense or agreement functional nodes, we make the
assumption that they are phrasal projections. There-
fore, we reason, we can create an artificial data set
of NSUs by extracting phrasal projections from an
annotated treebank.
In the example given in the introduction, we saw
a PP fragment, but fragments can be NPs, APs, etc.
We define different types of NSUs based on the root
label of the phrasal projection and define rules that
allow us to extract NSUs (partial parse trees) from
the source corpus.1 Because the target corpus also
contains full sentences, we allow full sentences to
be taken without modification from the source tree-
bank.
1Not all of these rules are simple extractions of phrasal pro-
jections, as described in section 4.
3 The two distributional components
The distributional model consists of two compo-
nents. By applying the extraction rules to the source
corpus we build a large collection of both full sen-
tences and NSUs. The distributions in this collec-
tion follow the distributions of trees in the source do-
main (first distributional component). We then sam-
ple from this collection to generate our artificial cor-
pus following distributions from the target domain
(second distributional component).
The probability of an artificial tree P (fi(cj)) gen-
erated with an extraction rule fi applied to a con-
stituent from the source corpus cj is defined as
P (fi(cj)) = P (fi)P (cj |fi) ? Pt(fi)Ps(cj |fi)
The first distributional component originates from
the source domain. It is responsible for the internal
structure of the NSUs and full sentences extracted.
Ps(cj |fi) is the probability of the constituent taken
from the source treebank (cj), given that the rule fi
is applicable to that constituent.
Sampling is done according to distributions of
NSUs and full sentences found in the target corpus
(Pt(fi)). As explained in section 2, there are several
types of NSUs found in the target domain. This sec-
ond component describes the distributions of types
of NSUs (or full sentences) found in the target do-
main. It determines, for example, the proportion of
NP NSUs that will be added to the artificial corpus.
To determine the target distribution we classified
171 (approximately 5%) randomly selected utter-
ances from the TownInfo data, that were used as a
development set.2 In Table 1 we can see that 15.2 %
of the trees in the artificial corpus will be NP NSUs.3
4 Data generation
We constructed our artificial corpus from sections
2 to 21 of the Wall Street Journal (WSJ) section
of the Penn Treebank corpus (Marcus et al, 1993)
2We discarded very short utterances (yes, no, and greetings)
since they don?t need parsing. We also do not consider incom-
plete NSUs resulting from interruptions or recording problems.
3Because NSUs can be interpreted only in context, the same
NSU can correspond to several syntactic categories: South for
example, can be an noun, an adverb, or an adjective. In case of
ambiguity, we divided the score up for the several possible tags.
This accounts for the fractional counts.
126
Category # Occ. Perc. Category # Occ. Perc.
NP 19.0 15.2 RB 1.7 1.3
JJ 12.7 10.1 DT 1.0 0.8
PP 12.0 9.6 CD 1.0 0.8
NN 11.7 9.3 Total frag. 70.0 56.0
VP 11.0 8.8 Full sents 55.0 44.0
Table 1: Distribution of types of NSUs and full sentences
in the TownInfo development set.
merged with PropBank labels (Palmer et al, 2005).
We included all the sentences from this dataset in
our artificial corpus, giving us 39,832 full sentences.
In accordance with the target distribution we added
50,699 NSUs extracted from the same dataset. We
sampled NSUs according to the distribution given in
Table 1. After the extraction we added a root FRAG
node to the extracted NSUs4 and we capitalised the
first letter of each NSU to form an utterance.
There are two additional pre-processing steps.
First, for some types of NSUs maximal projections
are added. For example, in the subset from the tar-
get source we saw many occurrences of nouns with-
out determiners, such as Hotel or Bar. These types
of NSUs would be missed if we just extracted NPs
from the source data, since we assume that NSUs are
maximal projections. Therefore, we extracted single
nouns as well and we added the NP phrasal projec-
tions to these nouns in the constructed trees. Sec-
ond, not all extracted NSUs can keep their semantic
roles. Extracting part of the sentence often severs
the semantic role from the predicate of which it was
originally an argument. An exception to this are VP
NSUs and prepositional phrases that are modifiers,
such as locative PPs, which are not dependent on the
verb. Hence, we removed the semantic roles from
the generated NSUs except for VPs and modifiers.
5 Experiments
We trained three parsing models on both the original
non-augmented merged Penn Treebank/Propbank
corpus and the artificially generated augmented tree-
bank including NSUs. We ran a contrastive ex-
periment to examine the usefulness of the three-
component model by training two versions of the
4The node FRAG exists in the Penn Treebank. Our annota-
tion does not introduce new labels, but only changes their dis-
tribution.
augmented model: One with and one without the
target component.5
These models were tested on two test sets: a small
corpus of 150 transcribed utterances taken from the
TownInfo corpus, annotated with gold syntactic and
semantic annotation by two of the authors6: the
TownInfo test set. The second test set is used to
compare the performance of the parser on WSJ-style
sentences and consists of section 23 of the merged
Penn Treebank/Propbank corpus. We will refer to
this test set as the non-augmented test set.
5.1 The statistical parser
The parsing model is the one proposed in Merlo
and Musillo (2008), which extends the syntactic
parser of Henderson (2003) and Titov and Hender-
son (2007) with annotations which identify seman-
tic role labels, and has competitive performance.
The parser uses a generative history-based proba-
bility model for a binarised left-corner derivation.
The probabilities of derivation decisions are mod-
elled using the neural network approximation (Hen-
derson, 2003) to a type of dynamic Bayesian Net-
work called an Incremental Sigmoid Belief Network
(ISBN) (Titov and Henderson, 2007).
The ISBN models the derivation history with a
vector of binary latent variables. These latent vari-
ables learn to represent features of the parse history
which are useful for making the current and subse-
quent derivation decisions. Induction of these fea-
tures is biased towards features which are local in
the parse tree, but can find features which are passed
arbitrarily far through the tree. This flexible mecha-
nism for feature induction allows the model to adapt
to the parsing of NSUs without requiring any design
changes or feature engineering.
5.2 Results
In Table 2, we report labelled constituent recall, pre-
cision, and F-measure for the three trained parsers
(rows) on the two test sets (columns).7 These mea-
5The model without the target distribution has a uniform dis-
tribution over full sentences and NSUs and within NSUs a uni-
form distribution over the 8 types.
6This test set was constructed separately and is completely
different from the development set used to determine the distri-
butions in the target data.
7Statistical significance is determined using a stratified shuf-
fling method, using software available at http://www.cis.
127
Training Testing
TownInfo PTB nonaug
Rec Prec F Rec Prec F
PTB nonaug 69.4 76.7 72.9 81.4 82.1 81.7
PTB aug(+t) 81.4 77.8 79.5 81.3 82.0 81.7
PTB aug(?t) 62.6 64.3 63.4 81.2 81.9 81.6
Table 2: Recall, precision, and F-measure for the two test
sets, trained on non-augmented data and data augmented
with and without the target distribution component.
sures include both syntactic labels and semantic role
labels.
The results in the first two lines of the columns
headed TownInfo indicate the performance on the
real data to which we are trying to adapt our parser:
spoken data from human-machine dialogues. The
parser does much better when trained on the aug-
mented data. The differences between training on
newspaper text and newspaper texts augmented with
artificially created data are statistically significant
(p < 0.001) and particularly large for recall: almost
12%.
The columns headed PTB nonaug show that the
performance on parsing WSJ texts is not hurt by
training on data augmented with artificially cre-
ated NSUs (first vs. second line). The difference
in performance compared to training on the non-
augmented data is not statistically significant.
The last two rows of the TownInfo data show the
results of our contrastive experiment. It is clear
that the three-component model and in particular our
careful characterisation of the target distribution is
indispensable. The F-measure drops from 79.5% to
63.4% when we disregard the target distribution.
6 Conclusions
We have shown how a three-component model that
consists of a model of the phenomenon being stud-
ied and two distributional components, one from the
source data and one from the target data, allows
one to create data artificially for training a seman-
tic parser. Specifically, analysis and minimal anno-
tation of only a small subset of utterances from the
target domain of spoken dialogue systems suffices
to determine a model of NSUs as well as the nec-
essary target distribution. Following this framework
upenn.edu/?dbikel/software.html.
we were able to improve the performance of a statis-
tical parser on goal-directed spoken data extracted
from human-machine dialogues without degrading
the performance on full sentences.
Acknowledgements
The research leading to these results has received
funding from the EU FP7 programme (FP7/2007-
2013) under grant agreement nr 216594 (CLASSIC
project: www.classic-project.org).
References
E. Charniak and M. Johnson. 2001. Edit detection and
parsing for transcribed speech. In Procs. NAACL.
R. Ferna?ndez. 2006. Non-sentential utterances in dia-
logue: classification resolution and use. Ph.D. thesis,
University of London.
J. Foster. 2007. Treebanks gone bad: Parser evaluation
and retraining using a treebank of ungrammatical sen-
tences. International Journal of Document Analysis
and Recognition, 10:1?16.
J. Henderson. 2003. Inducing history representations for
broad-coverage statistical parsing. In Procs. NAACL-
HLT.
O. Lemon, K. Georgila, J. Henderson, and M. Stuttle.
2006. An ISU dialogue system exhibiting reinforce-
ment learning of dialogue policies: generic slot-filling
in the TALK in-car system. In Procs. EACL.
M. Marcus, B. Santorini, and M.A. Marcinkiewicz.
1993. Building a large annotated corpus of English:
the Penn Treebank. Comp. Ling., 19:313?330.
J. Merchant. 2004. Fragments and ellipsis. Linguistics
and Philosophy, 27:661?738.
P. Merlo and G. Musillo. 2008. Semantic parsing
for high-precision semantic role labelling. In Procs.
CONLL.
J. Nivre, J. Hall, S. Ku?bler, R. McDonald, J. Nilsson,
S. Riedel, and D. Yuret. 2007. The CoNLL 2007
shared task on dependency parsing. In Procs. EMNLP-
CoNLL.
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The
Proposition Bank: An annotated corpus of semantic
roles. Comp. Ling., 31:71?105.
L. Progovac, K. Paesani, E. Casielles, and E. Barton.
2006. The Syntax of Nonsententials:Multidisciplinary
Perspectives. John Benjamins.
I Titov and J Henderson. 2007. Constituent parsing with
Incremental Sigmoid Belief Networks. In Procs. ACL.
K. Weilhammer, M. Stuttle, and S. Young. 2006. Boot-
strapping language models for dialogue systems. In
Procs. Conf. on Spoken Language Processing.
128
   	

Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 213?216,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Unlexicalised Hidden Variable Models of Split Dependency Grammars?
Gabriele Antonio Musillo
Department of Computer Science
and Department of Linguistics
University of Geneva
1211 Geneva 4, Switzerland
musillo4@etu.unige.ch
Paola Merlo
Department of Linguistics
University of Geneva
1211 Geneva 4, Switzerland
merlo@lettres.unige.ch
Abstract
This paper investigates transforms of split
dependency grammars into unlexicalised
context-free grammars annotated with hidden
symbols. Our best unlexicalised grammar
achieves an accuracy of 88% on the Penn
Treebank data set, that represents a 50%
reduction in error over previously published
results on unlexicalised dependency parsing.
1 Introduction
Recent research in natural language parsing has
extensively investigated probabilistic models of
phrase-structure parse trees. As well as being the
most commonly used probabilistic models of parse
trees, probabilistic context-free grammars (PCFGs)
are the best understood. As shown in (Klein and
Manning, 2003), the ability of PCFG models to dis-
ambiguate phrases crucially depends on the expres-
siveness of the symbolic backbone they use.
Treebank-specific heuristics have commonly been
used both to alleviate inadequate independence
assumptions stipulated by naive PCFGs (Collins,
1999; Charniak, 2000). Such methods stand in sharp
contrast to partially supervised techniques that have
recently been proposed to induce hidden grammati-
cal representations that are finer-grained than those
that can be read off the parsed sentences in tree-
banks (Henderson, 2003; Matsuzaki et al, 2005;
Prescher, 2005; Petrov et al, 2006).
?Part of this work was done when Gabriele Musillo was
visiting the MIT Computer Science and Artificial Intelligence
Laboratory, funded by a grant from the Swiss NSF (PBGE2-
117146). Many thanks to Michael Collins and Xavier Carreras
for their insightful comments on the work presented here.
This paper presents extensions of such gram-
mar induction techniques to dependency grammars.
Our extensions rely on transformations of depen-
dency grammars into efficiently parsable context-
free grammars (CFG) annotated with hidden sym-
bols. Because dependency grammars are reduced to
CFGs, any learning algorithm developed for PCFGs
can be applied to them. Specifically, we use the
Inside-Outside algorithm defined in (Pereira and
Schabes, 1992) to learn transformed dependency
grammars annotated with hidden symbols. What
distinguishes our work from most previous work on
dependency parsing is that our models are not lexi-
calised. Our models are instead decorated with hid-
den symbols that are designed to capture both lex-
ical and structural information relevant to accurate
dependency parsing without having to rely on any
explicit supervision.
2 Transforms of Dependency Grammars
Contrary to phrase-structure grammars that stipulate
the existence of phrasal nodes, dependency gram-
mars assume that syntactic structures are connected
acyclic graphs consisting of vertices representing
terminal tokens related by directed edges represent-
ing dependency relations. Such terminal symbols
are most commonly assumed to be words. In our un-
lexicalised models reported below, they are instead
assumed to be part-of-speech (PoS) tags. A typical
dependency graph is illustrated in Figure 1 below.
Various projective dependency grammars exem-
plify the concept of split bilexical dependency gram-
mar (SBG) defined in (Eisner, 2000). 1 SBGs are
1An SBG is a tuple ?V,W,L,R? such that:
213
R1root
kk
]]]]]]
]]]]]]
]]]]]]
]]]]]]
]]]]]]
]]]]]]
]]]]]
R1root/R
1
V BDB
SS
R1V BDB
YYY
YYY
YYY
YYY
Y
k
1rV BDB
RR
R
ll
l
R1V BDB/R
1
IND
RR
R
R1IND
ll
l
L1V BDB
RR
0V BDB R
p
V BDB
/R1NNPC
RR
1rIND R
1
IND/R
1
NNF
RR
R
L1V BDB\L
1
NNPA
ll
1rNNPC 0IND 1
r
NNF
RR
RR
ll
ll
1lNNPA 0NNPC L
1
NNF
RR
R
0NNF
0NNPA L
1
NNF \L
1
DTE
ll
l
1lDTE
0DTE
Nica hit
uu
33 66Miles with 88the trumpet
vv
Figure 1: A projective dependency graph for the sentence Nica hit Miles with the trumpet paired with its second-order
unlexicalised derivation tree annotated with hidden variables.
closely related to CFGs as they both define struc-
tures that are rooted ordered projective trees. Such a
close relationship is clarified in this section.
It follows from the equivalence of finite au-
tomata and regular grammars that any SBG can
be transformed into an equivalent CFG. Let D =
?V,W,L,R? be a SBG and G = ?N,W,P, S? a
CFG. To transform D into G we to define the set
P of productions, the set N of non-terminals, and
the start symbol S as follows:
? For each v in W , transform the automaton Lv
into a right-linear grammar GLv whose start
symbol is L1v; by construction, GLv consists of
rules such as Lpv ? u L
q
v or L
p
v ? , where ter-
minal symbols such as u belong to W and non-
terminals such as Lpv correspond to the states of
the Lv automaton; include all -productions in
P , and, if a rule such as Lpv ? u L
q
v is in GLv ,
include the rule Lpv ? 2lu L
q
v in P .
? For each v in V , transform the automaton Rv
into a left-linear grammar GRv whose start
symbol is R1v; by construction, GRv consists
? V is a set of terminal symbols which include a distin-
guished element root;
? L is a function that, for any v ? W (= V ? { root}),
returns a finite automaton that recognises the well-formed
sequences in W ? of left dependents of v;
? R is a function that, for each v ? V , returns a finite
automaton that recognises the well-formed sequences of
right dependents in W ? for v.
of rules such as Rpv ? R
q
v u or R
p
v ? ,
where terminal symbols such as u belongs to
W and non-terminals such as Rpv correspond
to the states of the Rv automaton; include all -
productions in P , and, if a rule such as Rpv ?
Rqv u is in GRv , include the rule R
p
v ? R
q
v 2ru
in P .
? For each symbol 2lu occurring in P , include the
productions 2lu ? L
1
u 1
l
u, 1
l
u ? 0u R
1
u, and
0u ? u in P ; for each symbol 2ru in P , include
the productions 2ru ? 1
r
u R
1
u, 1
r
u ? L
1
u 0u,
and 0u ? u in P .
? Set the start symbol S to R1root.
2
Parsing CFGs resulting from such transforms
runs in O(n4). The head index v decorating non-
terminals such as 1lv, 1
r
v, 0v, L
p
v and R
q
v can be com-
puted in O(1) given the left and right indices of the
sub-string wi,j they cover. 3 Observe, however, that
if 2lv or 2
r
v derives wi,j , then v does not functionally
depend on either i or j. Because it is possible for the
head index v of 2lv or 2
r
v to vary from i to j, v has
to be tracked by the parser, resulting in an overall
O(n4) time complexity.
In the following, we show how to transform
our O(n4) CFGs into O(n3) grammars by ap-
2CFGs resulting from such transformations can further be
normalised by removing the -productions from P .
3Indeed, if 1lv or 0v derives wi,j , then v = i; if 1
r
v derives
wi,j , then v = j; if wi,j is derived from Lpv , then v = j + 1;
and if wi,j is derived from Rqv , then v = i? 1.
214
plying transformations, closely related to those in
(McAllester, 1999) and (Johnson, 2007), that elimi-
nate the 2lv and 2
r
v symbols.
We only detail the elimination of the symbols 2rv.
The elimination of the 2lv symbols can be derived
symmetrically. By construction, a 2rv symbol is the
right successor of a non-terminalRpu. Consequently,
2rv can only occur in a derivation such as
? Rpu ? ` ? R
q
u 2
r
v ? ` ? R
q
u 1
r
v R
1
v ?.
To substitute for the problematic 2rv non-terminal in
the above derivation, we derive the form Rqu 1rv R
1
v
from Rpu/R1v R
1
v where R
p
u/R1v is a new non-
terminal whose right-hand side is Rqu 1rv. We thus
transform the above derivation into the derivation
? Rpu ? ` ? R
p
u/R1v R
1
v? ` ? R
q
u 1rv R
1
v ?.
4
Because u = i ? 1 and v = j if Rpu/R1v derives
wi,j , and u = j + 1 and v = i if L
p
u\L1v derives
wi,j , the parsing algorithm does not have to track
any head indices and can consequently parse strings
in O(n3) time.
The grammars described above can be further
transformed to capture linear second-order depen-
dencies involving three distinct head indices. A
second-order dependency structure is illustrated in
Figure 1 that involves two adjacent dependents,
Miles and with, of a single head, hit.
To see how linear second-order dependencies can
be captured, consider the following derivation of a
sequence of right dependents of a head u:
? Rpu/R
1
v ? ` ? R
q
u 1
r
v ? ` ? R
q
u/R
1
w R
1
w 1
r
v ?.
The form Rqu/R1w R
1
w 1v mentions three heads: u
is the the head that governs both v and w, and w
precedes v. To encode the linear relationship be-
tween w and v, we redefine the right-hand side of
Rpu/R1v as R
q
u/R1w ?R
1
w, 1
r
v? and include the pro-
duction ?R1w, 1
r
v? ? R
1
w 1
r
v in the productions.
The relationship between the dependents w and v of
the head u is captured, because Rpu/R1v jointly gen-
erates R1w and 1
r
v.
5
Any second-order grammar resulting from trans-
forming the derivations of right and left dependents
4Symmetrically, the derivation ? Lpu ? ` ? 2
l
v L
q
u ? `
? L1v 1
l
v L
q
u ? involving the 2
l
v symbol is transformed into
? Lpu ? ` ? L
1
v L
p
u\L
1
v ? ` ? L
1
v 1
l
v L
q
u ?.
5Symmetrically, to transform the derivation of a sequence of
left dependents of u, we redefine the right-hand side of Lpu\L
1
v
as ?1lv,L
1
w? L
q
u\L
1
w and include the production ?1
l
v,L
1
w? ?
1lv L
1
w in the set of rules.
in the way described above can be parsed in O(n3),
because the head indices decorating its symbols can
be computed in O(1).
In the following section, we show how to enrich
both our first-order and second-order grammars with
hidden variables.
3 Hidden Variable Models
Because they do not stipulate the existence of
phrasal nodes, commonly used unlabelled depen-
dency models are not sufficiently expressive to dis-
criminate between distinct projections of a given
head. Both our first-order and second-order gram-
mars conflate distributionally distinct projections if
they are projected from the same head. 6
To capture various distinct projections of a head,
we annotate each of the symbols that refers to it with
a unique hidden variable. We thus constrain the dis-
tribution of the possible values of the hidden vari-
ables in a linguistically meaningful way. Figure 1 il-
lustrates such constraints: the same hidden variable
B decorates each occurrence of the PoS tag VBD of
the head hit.
Enforcing such agreement constraints between
hidden variables provides a principled way to cap-
ture not only phrasal information but also lexical in-
formation. Lexical pieces of information conveyed
by a minimal projection such as 0V BDB in Figure 1
will consistently be propagated through the deriva-
tion tree and will condition the generation of the
right and left dependents of hit.
In addition, states such as p and q that decorate
non-terminal symbols such as Rpu or L
q
u can also
capture structural information, because they can en-
code the most recent steps in the derivation history.
In the models reported in the next section, these
states are assumed to be hidden and a distribution
over their possible values is automatically induced.
4 Empirical Work and Discussion
The models reported below were trained, validated,
and tested on the commonly used sections from the
Penn Treebank. Projective dependency trees, ob-
6As observed in (Collins, 1999), an unambiguous verbal
head such as prove bearing the VB tag may project a clause with
an overt subject as well as a clause without an overt subject, but
only the latter is a possible dependent of subject control verbs
such as try.
215
Development Data ? section 24 per word per sentence
FOM: q = 1, h = 1 75.7 9.9
SOM: q = 1, h = 1 80.5 16.2
FOM: q = 2, h = 2 81.9 17.4
FOM: q = 2, h = 4 84.7 22.0
SOM: q = 2, h = 2 84.3 21.5
SOM: q = 1, h = 4 87.0 25.8
Test Data ? section 23 per word per sentence
(Eisner and Smith, 2005) 75.6 NA
SOM: q = 1, h = 4 88.0 30.6
(McDonald, 2006) 91.5 36.7
Table 1: Accuracy results on the development and test
data set, where q denotes the number of hidden states and
h the number of hidden values annotating a PoS tag in-
volved in our first-order (FOM) and second-order (SOM)
models.
tained using the rules stated in (Yamada and Mat-
sumoto, 2003), were transformed into first-order and
second-order structures. CFGs extracted from such
structures were then annotated with hidden variables
encoding the constraints described in the previous
section and trained until convergence by means of
the Inside-Outside algorithm defined in (Pereira and
Schabes, 1992) and applied in (Matsuzaki et al,
2005). To efficiently decode our hidden variable
models, we pruned the search space as in (Petrov et
al., 2006). To evaluate the performance of our mod-
els, we report two of the standard measures: the per
word and per sentence accuracy (McDonald, 2006).
Figures reported in the upper section of Table 1
measure the effect on accuracy of the transforms
we designed. Our baseline first-order model (q =
1, h = 1) reaches a poor per word accuracy that sug-
gests that information conveyed by bare PoS tags is
not fine-grained enough to accurately predict depen-
dencies. Results reported in the second line shows
that modelling adjacency relations between depen-
dents as second-order models do is relevant to accu-
racy. The third line indicates that annotating both
the states and the PoS tags of a first-order model
with two hidden values is sufficient to reach a per-
formance comparable to the one achieved by a naive
second-order model. However, comparing the re-
sults obtained by our best first-order models to the
accuracy achieved by our best second-order model
conclusively shows that first-order models exploit
such dependencies to a much lesser extent. Overall,
such results provide a first solution to the problem
left open in (Johnson, 2007) as to whether second-
order transforms are relevant to parsing accuracy or
not.
The lower section of Table 1 reports the results
achieved by our best model on the test data set and
compare them both to those obtained by the only un-
lexicalised dependency model we know of (Eisner
and Smith, 2005) and to those achieved by the state-
of-the-art dependency parser in (McDonald, 2006).
While clearly not state-of-the-art, the performance
achieved by our best model suggests that massive
lexicalisation of dependency models might not be
necessary to achieve competitive performance. Fu-
ture work will lie in investigating the issue of lex-
icalisation in the context of dependency parsing by
weakly lexicalising our hidden variable models.
References
Eugene Charniak. 2000. A maximum-entropy-inspired parser.
In NAACL?00.
Michael John Collins. 1999. Head-Driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, University
of Pennsylvania.
Jason Eisner and Noah A. Smith. 2005. Parsing with soft and
hard constraints on dependency length. In IWPT?05.
Jason Eisner. 2000. Bilexical grammars and their cubic-time
parsing algorithms. In H.Bunt and A. Nijholt, eds., Ad-
vances in Probabilistic and Other Parsing Technologies,
pages 29?62. Kluwer Academic Publishers.
Jamie Henderson. 2003. Inducing history representations for
broad-coverage statistical parsing. In NAACL-HLT?03.
Mark Johnson. 2007. Transforming projective bilexical de-
pendency grammars into efficiently-parsable cfgs with
unfold-fold. In ACL?06.
Dan Klein and Christopher D. Manning. 2003. Accurate unlex-
icalized parsing. In ACL?03.
Takuya Matsuzaki, Yusuke Miyao, and Junichi Tsujii. 2005.
Probabilistic CFG with latent annotations. In ACL?05.
David McAllester. 1999. A reformulation of eisner and
satta?s cubit time parser for split head automata gram-
mars. http://ttic.uchicago.edu/d?mcallester.
Ryan McDonald. 2006. Discriminative Training and Spanning
Tree Algorithms for Dependency Parsing. Ph.D. thesis,
University of Pennsylvania.
Fernando Pereira and Yves Schabes. 1992. Inside-outside rees-
timation form partially bracketed corpora. In ACL?92.
Slav Petrov, Leon Barrett Romain Thibaux, and Dan Klein.
2006. Learning accurate, compact, and interpretable tree
annotation. In ACL?06.
Detlef Prescher. 2005. Head-driven PCFGs with latent-head
statistics. In IWPT?05.
H. Yamada and Y. Matsumoto. 2003. Statistical dependency
analysis with support vectore machines. In IWPT?03.
216
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 288?296,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Abstraction and Generalisation in Semantic Role Labels:
PropBank, VerbNet or both?
Paola Merlo
Linguistics Department
University of Geneva
5 Rue de Candolle, 1204 Geneva
Switzerland
Paola.Merlo@unige.ch
Lonneke Van Der Plas
Linguistics Department
University of Geneva
5 Rue de Candolle, 1204 Geneva
Switzerland
Lonneke.VanDerPlas@unige.ch
Abstract
Semantic role labels are the representa-
tion of the grammatically relevant aspects
of a sentence meaning. Capturing the
nature and the number of semantic roles
in a sentence is therefore fundamental to
correctly describing the interface between
grammar and meaning. In this paper, we
compare two annotation schemes, Prop-
Bank and VerbNet, in a task-independent,
general way, analysing how well they fare
in capturing the linguistic generalisations
that are known to hold for semantic role
labels, and consequently how well they
grammaticalise aspects of meaning. We
show that VerbNet is more verb-specific
and better able to generalise to new seman-
tic role instances, while PropBank better
captures some of the structural constraints
among roles. We conclude that these two
resources should be used together, as they
are complementary.
1 Introduction
Most current approaches to language analysis as-
sume that the structure of a sentence depends on
the lexical semantics of the verb and of other pred-
icates in the sentence. It is also assumed that only
certain aspects of a sentence meaning are gram-
maticalised. Semantic role labels are the represen-
tation of the grammatically relevant aspects of a
sentence meaning.
Capturing the nature and the number of seman-
tic roles in a sentence is therefore fundamental
to correctly describe the interface between gram-
mar and meaning, and it is of paramount impor-
tance for all natural language processing (NLP)
applications that attempt to extract meaning rep-
resentations from analysed text, such as question-
answering systems or even machine translation.
The role of theories of semantic role lists is to
obtain a set of semantic roles that can apply to
any argument of any verb, to provide an unam-
biguous identifier of the grammatical roles of the
participants in the event described by the sentence
(Dowty, 1991). Starting from the first proposals
(Gruber, 1965; Fillmore, 1968; Jackendoff, 1972),
several approaches have been put forth, ranging
from a combination of very few roles to lists of
very fine-grained specificity. (See Levin and Rap-
paport Hovav (2005) for an exhaustive review).
In NLP, several proposals have been put forth in
recent years and adopted in the annotation of large
samples of text (Baker et al, 1998; Palmer et al,
2005; Kipper, 2005; Loper et al, 2007). The an-
notated PropBank corpus, and therefore implicitly
its role labels inventory, has been largely adopted
in NLP because of its exhaustiveness and because
it is coupled with syntactic annotation, properties
that make it very attractive for the automatic learn-
ing of these roles and their further applications to
NLP tasks. However, the labelling choices made
by PropBank have recently come under scrutiny
(Zapirain et al, 2008; Loper et al, 2007; Yi et al,
2007).
The annotation of PropBank labels has been
conceived in a two-tiered fashion. A first tier
assigns abstract labels such as ARG0 or ARG1,
while a separate annotation records the second-
tier, verb-sense specific meaning of these labels.
Labels ARG0 or ARG1 are assigned to the most
prominent argument in the sentence (ARG1 for
unaccusative verbs and ARG0 for all other verbs).
The other labels are assigned in the order of promi-
nence. So, while the same high-level labels are
used across verbs, they could have different mean-
ings for different verb senses. Researchers have
usually concentrated on the high-level annotation,
but as indicated in Yi et al (2007), there is rea-
son to think that these labels do not generalise
across verbs, nor to unseen verbs or to novel verb
288
senses. Because the meaning of the role annota-
tion is verb-specific, there is also reason to think
that it fragments the data and creates data sparse-
ness, making automatic learning from examples
more difficult. These short-comings are more ap-
parent in the annotation of less prominent and less
frequent roles, marked by the ARG2 to ARG5 la-
bels.
Zapirain et al (2008), Loper et al (2007) and
Yi et al (2007) investigated the ability of the Prop-
Bank role inventory to generalise compared to the
annotation in another semantic role list, proposed
in the electronic dictionary VerbNet. VerbNet la-
bels are assigned in a verb-class specific way and
have been devised to be more similar to the inven-
tories of thematic role lists usually proposed by
linguists. The results in these papers are conflict-
ing.
While Loper et al (2007) and Yi et al (2007)
show that augmenting PropBank labels with Verb-
Net labels increases generalisation of the less fre-
quent labels, such as ARG2, to new verbs and new
domains, they also show that PropBank labels per-
form better overall, in a semantic role labelling
task. Confirming this latter result, Zapirain et al
(2008) find that PropBank role labels are more ro-
bust than VerbNet labels in predicting new verb
usages, unseen verbs, and they port better to new
domains.
The apparent contradiction of these results can
be due to several confounding factors in the exper-
iments. First, the argument labels for which the
VerbNet improvement was found are infrequent,
and might therefore not have influenced the over-
all results enough to counterbalance new errors in-
troduced by the finer-grained annotation scheme;
second, the learning methods in both these exper-
imental settings are largely based on syntactic in-
formation, thereby confounding learning and gen-
eralisation due to syntax ? which would favour
the more syntactically-driven PropBank annota-
tion ? with learning due to greater generality of
the semantic role annotation; finally, task-specific
learning-based experiments do not guarantee that
the learners be sufficiently powerful to make use
of the full generality of the semantic role labels.
In this paper, we compare the two annotation
schemes, analysing how well they fare in captur-
ing the linguistic generalisations that are known
to hold for semantic role labels, and consequently
how well they grammaticalise aspects of mean-
ing. Because the well-attested strong correlation
between syntactic structure and semantic role la-
bels (Levin and Rappaport Hovav, 2005; Merlo
and Stevenson, 2001) could intervene as a con-
founding factor in this analysis, we expressly limit
our investigation to data analyses and statistical
measures that do not exploit syntactic properties or
parsing techniques. The conclusions reached this
way are not task-specific and are therefore widely
applicable.
To preview, based on results in section 3, we
conclude that PropBank is easier to learn, but
VerbNet is more informative in general, it gener-
alises better to new role instances and its labels are
more strongly correlated to specific verbs. In sec-
tion 4, we show that VerbNet labels provide finer-
grained specificity. PropBank labels are more con-
centrated on a few VerbNet labels at higher fre-
quency. This is not true at low frequency, where
VerbNet provides disambiguations to overloaded
PropBank variables. Practically, these two sets
of results indicate that both annotation schemes
could be useful in different circumstances, and at
different frequency bands. In section 5, we report
results indicating that PropBank role sets are high-
level abstractions of VerbNet role sets and that
VerbNet role sets are more verb and class-specific.
In section 6, we show that PropBank more closely
captures the thematic hierarchy and is more corre-
lated to grammatical functions, hence potentially
more useful for semantic role labelling, for learn-
ers whose features are based on the syntactic tree.
Finally, in section 7, we summarise some previ-
ous results, and we provide new statistical evi-
dence to argue that VerbNet labels are more gen-
eral across verbs. These conclusions are reached
by task-independent statistical analyses. The data
and the measures used to reach these conclusions
are discussed in the next section.
2 Materials and Method
In data analysis and inferential statistics, careful
preparation of the data and choice of the appropri-
ate statistical measures are key. We illustrate the
data and the measures used here.
2.1 Data and Semantic Role Annotation
Proposition Bank (Palmer et al, 2005) adds
Levin?s style predicate-argument annotation and
indication of verbs? alternations to the syntactic
structures of the Penn Treebank (Marcus et al,
289
1993).
It defines a limited role typology. Roles are
specified for each verb individually. Verbal pred-
icates in the Penn Treebank (PTB) receive a label
REL and their arguments are annotated with ab-
stract semantic role labels A0-A5 or AA for those
complements of the predicative verb that are con-
sidered arguments, while those complements of
the verb labelled with a semantic functional label
in the original PTB receive the composite seman-
tic role label AM-X , where X stands for labels
such as LOC, TMP or ADV, for locative, tem-
poral and adverbial modifiers respectively. Prop-
Bank uses two levels of granularity in its annota-
tion, at least conceptually. Arguments receiving
labels A0-A5 or AA do not express consistent se-
mantic roles and are specific to a verb, while argu-
ments receiving an AM-X label are supposed to
be adjuncts and the respective roles they express
are consistent across all verbs. However, among
argument labels, A0 and A1 are assigned attempt-
ing to capture Proto-Agent and Proto-Patient prop-
erties (Dowty, 1991). They are, therefore, more
valid across verbs and verb instances than the A2-
A5 labels. Numerical results in Yi et al (2007)
show that 85% of A0 occurrences translate into
Agent roles and more than 45% instances of A1
map into Patient and Patient-like roles, using a
VerbNet labelling scheme. This is also confirmed
by our counts, as illustrated in Tables 3 and 4 and
discussed in Section 4 below.
VerbNet is a lexical resource for English verbs,
yielding argumental and thematic information
(Kipper, 2005). VerbNet resembles WordNet in
spirit, it provides a verbal lexicon tying verbal se-
mantics (theta-roles and selectional restrictions) to
verbal distributional syntax. VerbNet defines 23
thematic roles that are valid across verbs. The list
of thematic roles can be seen in the first column of
Table 4.
For some of our comparisons below to be valid,
we will need to reduce the inventory of labels of
VerbNet to the same number of labels in Prop-
Bank. Following previous work (Loper et al,
2007), we define equivalence classes of VerbNet
labels. We will refer to these classes as VerbNet
groups. The groups we define are illustrated in
Figure 1. Notice also that all our comparisons,
like previous work, will be limited to the obliga-
tory arguments in PropBank, the A0 to A5, AA
arguments, to be comparable to VerbNet. VerbNet
is a lexicon and by definition it does not list op-
tional modifiers (the arguments labelled AM-X in
PropBank).
In order to support the joint use of both these re-
sources and their comparison, SemLink has been
developed (Loper et al, 2007). SemLink1 pro-
vides mappings from PropBank to VerbNet for the
WSJ portion of the Penn Treebank. The mapping
have been annotated automatically by a two-stage
process: a lexical mapping and an instance classi-
fier (Loper et al, 2007). The results were hand-
corrected. In addition to semantic roles for both
PropBank and VerbNet, SemLink contains infor-
mation about verbs, their senses and their VerbNet
classes which are extensions of Levin?s classes.
The annotations in SemLink 1.1. are not com-
plete. In the analyses presented here, we have
only considered occurrences of semantic roles for
which both a PropBank and a VerbNet label is
available in the data (roughly 45% of the Prop-
Bank semantic roles have a VerbNet semantic
role).2 Furthermore, we perform our analyses on
training and development data only. This means
that we left section 23 of the Wall Street Journal
out. The analyses are done on the basis of 106,459
semantic role pairs.
For the analysis concerning the correlation be-
tween semantic roles and syntactic dependencies
in Section 6, we merged the SemLink data with the
non-projectivised gold data of the CoNNL 2008
shared task on syntactic and semantic dependency
parsing (Surdeanu et al, 2008). Only those depen-
dencies that bear both a syntactic and a semantic
label have been counted for test and development
set. We have discarded discontinous arguments.
Analyses are based on 68,268 dependencies in to-
tal.
2.2 Measures
In the following sections, we will use simple pro-
portions, entropy, joint entropy, conditional en-
tropy, mutual information, and a normalised form
of mutual information which measures correlation
between nominal attributes called symmetric un-
certainty (Witten and Frank, 2005, 291). These are
all widely used measures (Manning and Schuetze,
1999), excepted perhaps the last one. We briefly
describe it here.
1(http://verbs.colorado.edu/semlink/)
2In some cases SemLink allows for multiple annotations.
In those cases we selected the first annotation.
290
AGENT: Agent, Agent1
PATIENT: Patient
GOAL: Recipient, Destination, Location, Source,
Material, Beneficiary, Goal
EXTENT: Extent, Asset, Value
PREDATTR: Predicate, Attribute, Theme,
Theme1, Theme2, Topic, Stimulus, Proposition
PRODUCT: Patient2, Product, Patient1
INSTRCAUSE: Instrument, Cause, Experiencer,
Actor2, Actor, Actor1
Figure 1: VerbNet Groups
Given a random variable X, the entropy H(X)
describes our uncertainty about the value of X, and
hence it quantifies the information contained in a
message trasmitted by this variable. Given two
random variables X,Y, the joint entropy H(X,Y)
describes our uncertainty about the value of the
pair (X,Y). Symmetric uncertainty is a normalised
measure of the information redundancy between
the distributions of two random variables. It cal-
culates the ratio between the joint entropy of the
two random variables if they are not independent
and the joint entropy if the two random variables
were independent (which is the sum of their indi-
vidual entropies). This measure is calculated as
follows.
U(A,B) = 2
H(A) + H(B)?H(A,B)
H(A) + H(B)
where H(X) = ??x?X p(x)logp(x) and
H(X,Y ) = ??x?X,y?Y p(x, y)logp(x, y).
Symmetric uncertainty lies between 0 and 1. A
higher value for symmetric uncertainty indicates
that the two random variables are more highly as-
sociated (more redundant), while lower values in-
dicate that the two random variables approach in-
dependence.
We use these measures to evaluate how well two
semantic role inventories capture well-known dis-
tributional generalisations. We discuss several of
these generalisations in the following sections.
3 Amount of Information in Semantic
Roles Inventory
Most proposals of semantic role inventories agree
on the fact that the number of roles should be small
to be valid generally. 3
3With the notable exception of FrameNet, which is devel-
oping a large number of labels organised hierarchically and
Task PropBank ERR VerbNet ERR
Role generalisation 62 (82?52/48) 66 (77?33/67)
No verbal features 48 (76?52/48) 43 (58?33/67)
Unseen predicates 50 (75?52/48) 37 (62?33/67)
Table 2: Percent Error rate reduction (ERR) across
role labelling sets in three tasks in Zapirain et al
(2008). ERR= (result ? baseline / 100% ? base-
line )
PropBank and VerbNet clearly differ in the level
of granularity of the semantic roles that have been
assigned to the arguments. PropBank makes fewer
distinctions than VerbNet, with 7 core argument
labels compared to VerbNet?s 23. More important
than the size of the inventory, however, is the fact
that PropBank has a much more skewed distribu-
tion than VerbNet, illustrated in Table 1. Conse-
quently, the distribution of PropBank labels has
an entropy of 1.37 bits, and even when the Verb-
Net labels are reduced to 7 equivalence classes
the distribution has an entropy of 2.06 bits. Verb-
Net therefore conveys more information, but it is
also more difficult to learn, as it is more uncertain.
An uninformed PropBank learner that simply as-
signed the most frequent label would be correct
52% of the times by always assigning an A1 label,
while for VerbNet would be correct only 33% of
the times assigning Agent.
This simple fact might cast new light on some
of the comparative conclusions of previous work.
In some interesting experiments, Zapirain et al
(2008) test generalising abilities of VerbNet and
PropBank comparatively to new role instances in
general (their Table 1, line CoNLL setting, col-
umn F1 core), and also on unknown verbs and in
the absence of verbal features. They find that a
learner based on VerbNet has worse learning per-
formance. They interpret this result as indicating
that VerbNet labels are less general and more de-
pendent on knowledge of specific verbs. However,
a comparison that takes into consideration the dif-
ferential baseline is able to factor the difficulty of
the task out of the results for the overall perfor-
mance. A simple baseline for a classifier is based
on a majority class assignment (see our Table 1).
We use the performance results reported in Zapi-
rain et al (2008) and calculate the reduction in er-
ror rate based on this differential baseline for the
two annotation schemes. We compare only the
results for the core labels in PropBank as those
interpreted frame-specifically (Ruppenhofer et al, 2006).
291
PropBank VerbNet
A0 38.8 Agent 32.8 Cause 1.9 Source 0.9 Asset 0.3 Goal 0.00
A1 51.7 Theme 26.3 Product 1.6 Actor1 0.8 Material 0.2 Agent1 0.00
A2 9.0 Topic 11.5 Extent 1.3 Theme2 0.8 Beneficiary 0.2
A3 0.5 Patient 5.8 Destination 1.2 Theme1 0.8 Proposition 0.1
A4 0.0 Experiencer 4.2 Patient1 1.2 Attribute 0.7 Value 0.1
A5 0.0 Predicate 2.3 Location 1.0 Patient2 0.5 Instrument 0.1
AA 0.0 Recipient 2.2 Stimulus 0.9 Actor2 0.3 Actor 0.0
Table 1: Distribution of PropBank core labels and VerbNet labels.
are the ones that correspond to VerbNet.4 We
find more mixed results than previously reported.
VerbNet has better role generalising ability overall
as its reduction in error rate is greater than Prop-
Bank (first line of Table 2), but it is more degraded
by lack of verb information (second and third lines
of Table 2). The importance of verb information
for VerbNet is confirmed by information-theoretic
measures. While the entropy of VerbNet labels
is higher than that of PropBank labels (2.06 bits
vs. 1.37 bits), as seen before, the conditional en-
tropy of respective PropBank and VerbNet distri-
butions given the verb is very similar, but higher
for PropBank (1.11 vs 1.03 bits), thereby indicat-
ing that the verb provides much more information
in association with VerbNet labels. The mutual in-
formation of the PropBank labels and the verbs
is only 0.26 bits, while it is 1.03 bits for Verb-
Net. These results are expected if we recall the
two-tiered logic that inspired PropBank annota-
tion, where the abstract labels are less related to
verbs than labels in VerbNet.
These results lead us to our first conclusion:
while PropBank is easier to learn, VerbNet is more
informative in general, it generalises better to new
role instances, and its labels are more strongly cor-
related to specific verbs. It is therefore advisable
to use both annotations: VerbNet labels if the verb
is available, reverting to PropBank labels if no lex-
4We assume that our majority class can roughly corre-
spond to Zapirain et al (2008)?s data. Notice however that
both sampling methods used to collect the counts are likely
to slightly overestimate frequent labels. Zapirain et al (2008)
sample only complete propositions. It is reasonable to as-
sume that higher numbered PropBank roles (A3, A4, A5) are
more difficult to define. It would therefore more often happen
that these labels are not annotated than it happens that A0,
A1, A2, the frequent labels, are not annotated. This reason-
ing is confirmed by counts on our corpus, which indicate that
incomplete propositions include a higher proportion of low
frequency labels and a lower proportion of high frequency
labels that the overall distribution. However, our method is
also likely to overestimate frequent labels, since we count all
labels, even those in incomplete propositions. By the same
reasoning, we will find more frequent labels than the under-
lying real distribution of a complete annotation.
ical information is known.
4 Equivalence Classes of Semantic Roles
An observation that holds for all semantic role la-
belling schemes is that certain labels seem to be
more similar than others, based on their ability to
occur in the same syntactic environment and to
be expressed by the same function words. For
example, Agent and Instrumental Cause are of-
ten subjects (of verbs selecting animate and inan-
imate subjects respectively); Patients/Themes can
be direct objects of transitive verbs and subjects
of change of state verbs; Goal and Beneficiary can
be passivised and undergo the dative alternation;
Instrument and Comitative are expressed by the
same preposition in many languages (see Levin
and Rappaport Hovav (2005).) However, most an-
notation schemes in NLP and linguistics assume
that semantic role labels are atomic. It is there-
fore hard to explain why labels do not appear to be
equidistant in meaning, but rather to form equiva-
lence classes in certain contexts. 5
While both role inventories under scrutiny here
use atomic labels, their joint distribution shows
interesting relations. The proportion counts are
shown in Table 3 and 4.
If we read these tables column-wise, thereby
taking the more linguistically-inspired labels in
VerbNet to be the reference labels, we observe
that the labels in PropBank are especially con-
centrated on those labels that linguistically would
be considered similar. Specifically, in Table 3
A0 mostly groups together Agents and Instrumen-
tal Causes; A1 mostly refers to Themes and Pa-
tients; while A2 refers to Goals and Themes. If we
5Clearly, VerbNet annotators recognise the need to ex-
press these similarities since they use variants of the same
label in many cases. Because the labels are atomic however,
the distance between Agent and Patient is the same as Patient
and Patient1 and the intended greater similarity of certain la-
bels is lost to a learning device. As discussed at length in the
linguistic literature, features bundles instead of atomic labels
would be the mechanism to capture the differential distance
of labels in the inventory (Levin and Rappaport Hovav, 2005).
292
A0 A1 A2 A3 A4 A5 AA
Agent 32.6 0.2 - - - - -
Patient 0.0 5.8 - - - - -
Goal 0.0 1.5 4.0 0.2 0.0 0.0 -
Extent - 0.2 1.3 0.2 - - -
PredAttr 1.2 39.3 2.9 0.0 - - 0.0
Product 0.1 2.7 0.6 - 0.0 - -
InstrCause 4.8 2.2 0.3 0.1 - - -
Table 3: Distribution of PropBank by VerbNet
group labels according to SemLink. Counts indi-
cated as 0.0 approximate zero by rounding, while
a - sign indicates that no occurrences were found.
read these tables row-wise, thereby concentrating
on the grouping of PropBank labels provided by
VerbNet labels, we see that low frequency Prop-
Bank labels are more evenly spread across Verb-
Net labels than the frequent labels, and it is more
difficult to identify a dominant label than for high-
frequency labels. Because PropBank groups to-
gether VerbNet labels at high frequency, while
VerbNet labels make different distinctions at lower
frequencies, the distribution of PropBank is much
more skewed than VerbNet, yielding the differ-
ences in distributions and entropy discussed in the
previous section.
We can draw, then, a second conclusion: while
VerbNet is finer-grained than PropBank, the two
classifications are not in contradiction with each
other. VerbNet greater specificity can be used in
different ways depending on the frequency of the
label. Practically, PropBank labels could provide
a strong generalisation to a VerbNet annotation at
high-frequency. VerbNet labels, on the other hand,
can act as disambiguators of overloaded variables
in PropBank. This conclusion was also reached
by Loper et al (2007). Thus, both annotation
schemes could be useful in different circumstances
and at different frequency bands.
5 The Combinatorics of Semantic Roles
Semantic roles exhibit paradigmatic generalisa-
tions ? generalisations across similar semantic
roles in the inventory ? (which we saw in section
4.) They also show syntagmatic generalisations,
generalisations that concern the context. One kind
of context is provided by what other roles they can
occur with. It has often been observed that cer-
tain semantic roles sets are possible, while oth-
ers are not; among the possible sets, certain are
much more frequent than others (Levin and Rap-
paport Hovav, 2005). Some linguistically-inspired
A0 A1 A2 A3 A4 A5 AA
Actor 0.0 - - - - - -
Actor1 0.8 - - - - - -
Actor2 - 0.3 0.1 - - - -
Agent1 0.0 - - - - - -
Agent 32.6 0.2 - - - - -
Asset - 0.1 0.0 0.2 - - -
Attribute - 0.1 0.7 - - - -
Beneficiary - 0.0 0.1 0.1 0.0 - -
Cause 0.7 1.1 0.1 0.1 - - -
Destination - 0.4 0.8 0.0 - - -
Experiencer 3.3 0.9 0.1 - - - -
Extent - - 1.3 - - - -
Goal - - - - 0.0 - -
Instrument - - 0.1 0.0 - - -
Location 0.0 0.4 0.6 0.0 - 0.0 -
Material - 0.1 0.1 0.0 - - -
Patient 0.0 5.8 - - - - -
Patient1 0.1 1.1 - - - - -
Patient2 - 0.1 0.5 - - - -
Predicate - 1.2 1.1 0.0 - - -
Product 0.0 1.5 0.1 - 0.0 - -
Proposition - 0.0 0.1 - - - -
Recipient - 0.3 2.0 - 0.0 - -
Source - 0.3 0.5 0.1 - - -
Stimulus - 1.0 - - - - -
Theme 0.8 25.1 0.5 0.0 - - 0.0
Theme1 0.4 0.4 0.0 0.0 - - -
Theme2 0.1 0.4 0.3 - - - -
Topic - 11.2 0.3 - - - -
Value - 0.1 - - - - -
Table 4: Distribution of PropBank by original
VerbNet labels according to SemLink. Counts
indicated as 0.0 approximate zero by rounding,
while a - sign indicates that no occurrences were
found.
semantic role labelling techniques do attempt to
model these dependencies directly (Toutanova et
al., 2008; Merlo and Musillo, 2008).
Both annotation schemes impose tight con-
straints on co-occurrence of roles, independently
of any verb information, with 62 role sets for
PropBank and 116 role combinations for VerbNet,
fewer than possible. Among the observed role
sets, some are more frequent than expected un-
der an assumption of independence between roles.
For example, in PropBank, propositions compris-
ing A0, A1 roles are observed 85% of the time,
while they would be expected to occur only in 20%
of the cases. In VerbNet the difference is also great
between the 62% observed Agent, PredAttr propo-
sitions and the 14% expected.
Constraints on possible role sets are the expres-
sion of structural constraints among roles inherited
from syntax, which we discuss in the next section,
but also of the underlying event structure of the
verb. Because of this relation, we expect a strong
correlation between role sets and their associated
293
A0,A1 A0,A2 A1,A2
Agent, Theme 11650 109 4
Agent, Topic 8572 14 0
Agent, Patient 1873 0 0
Experiencer, Theme 1591 0 15
Agent, Product 993 1 0
Agent, Predicate 960 64 0
Experiencer, Stimulus 843 0 0
Experiencer, Cause 756 0 2
Table 5: Sample of role sets correspondences
verb, as well as role sets and verb classes for both
annotation schemes. However, PropBank roles are
associated based on the meaning of the verb, but
also based on their positional prominence in the
tree, and so we can expect their relation to the ac-
tual verb entry to be weaker.
We measure here simply the correlation as in-
dicated by the symmetric uncertainty of the joint
distribution of role sets by verbs and of role sets
by verb classes, for each of the two annotation
schemes. We find that the correlation between
PropBank role sets and verb classes is weaker
than the correlation between VerbNet role sets and
verb classes, as expected (PropBank: U=0.21 vs
VerbNet: U=0.46). We also find that correlation
between PropBank role sets and verbs is weaker
than the correlation between VerbNet role sets and
verbs (PropBank: U=0.23 vs VerbNet U=0.43).
Notice that this result holds for VerbNet role label
groups, and is therefore not a side-effect of a dif-
ferent size in role inventory. This result confirms
our findings reported in Table 2, which showed
a larger degradation of VerbNet labels in the ab-
sence of verb information.
If we analyse the data, we see that many role
sets that form one single set in PropBank are split
into several sets in VerbNet, with those roles that
are different being roles that in PropBank form a
group. So, for example, a role list (A0, A1) in
PropBank will corresponds to 14 different lists in
VerbNet (when using the groups). The three most
frequent VerbNet role sets describe 86% of the
cases: (Agent, Predattr) 71%, (InstrCause, Pre-
dAttr) 9%, and (Agent, Patient) 6% . Using the
original VerbNet labels ? a very small sample of
the most frequent ones is reported in Table 5 ?
we find 39 different sets. Conversely, we see that
VerbNet sets corresponds to few PropBank sets,
even for high frequency.
The third conclusion we can draw then is two-
fold. First, while VerbNet labels have been as-
signed to be valid across verbs, as confirmed by
their ability to enter in many combinations, these
combinations are more verb and class-specific
than combinations in PropBank. Second, the fine-
grained, coarse-grained correspondence of anno-
tations between VerbNet and PropBank that was
illustrated by the results in Section 4 is also borne
out when we look at role sets: PropBank role sets
appear to be high-level abstractions of VerbNet
role sets.
6 Semantic Roles and Grammatical
Functions: the Thematic Hierarchy
A different kind of context-dependence is pro-
vided by thematic hierarchies. It is a well-attested
fact that lexical semantic properties described by
semantic roles and grammatical functions appear
to be distributed according to prominence scales
(Levin and Rappaport Hovav, 2005). Seman-
tic roles are organized according to the thematic
hierarchy (one proposal among many is Agent
> Experiencer> Goal/Source/Location> Patient
(Grimshaw, 1990)). This hierarchy captures the
fact that the options for the structural realisation
of a particular argument do not depend only on
its role, but also on the roles of other arguments.
For example in psychological verbs, the position
of the Experiencer as a syntactic subject or ob-
ject depends on whether the other role in the sen-
tence is a Stimulus, hence lower in the hierar-
chy, as in the psychological verbs of the fear class
or an Agent/Cause as in the frighten class. Two
prominence scales can combine by matching ele-
ments harmonically, higher elements with higher
elements and lower with lower (Aissen, 2003).
Grammatical functions are also distributed accord-
ing to a prominence scale. Thus, we find that most
subjects are Agents, most objects are Patients or
Themes, and most indirect objects are Goals, for
example.
The semantic role inventory, thus, should show
a certain correlation with the inventory of gram-
matical functions. However, perfect correlation is
clearly not expected as in this case the two levels
of representation would be linguistically and com-
putationally redundant. Because PropBank was
annotated according to argument prominence, we
expect to see that PropBank reflects relationships
between syntax and semantic role labels more
strongly than VerbNet. Comparing syntactic de-
pendency labels to their corresponding PropBank
or VerbNet groups labels (groups are used to elim-
294
inate the confound of different inventory sizes), we
find that the joint entropy of PropBank and depen-
dency labels is 2.61 bits while the joint entropy of
VerbNet and dependency labels is 3.32 bits. The
symmetric uncertainty of PropBank and depen-
dency labels is 0.49, while the symmetric uncer-
tainty of VerbNet and dependency labels is 0.39.
On the basis of these correlations, we can con-
firm previous findings: PropBank more closely
captures the thematic hierarchy and is more corre-
lated to grammatical functions, hence potentially
more useful for semantic role labelling, for learn-
ers whose features are based on the syntactic tree.
VerbNet, however, provides a level of annotation
that is more independent of syntactic information,
a property that might be useful in several applica-
tions, such as machine translation, where syntactic
information might be too language-specific.
7 Generality of Semantic Roles
Semantic roles are not meant to be domain-
specific, but rather to encode aspects of our con-
ceptualisation of the world. A semantic role in-
ventory that wants to be linguistically perspicuous
and also practically useful in several tasks needs to
reflect our grammatical representation of events.
VerbNet is believed to be superior in this respect
to PropBank, as it attempts to be less verb-specific
and to be portable across classes. Previous results
(Loper et al, 2007; Zapirain et al, 2008) appear to
indicate that this is not the case because a labeller
has better performance with PropBank labels than
with VerbNet labels. But these results are task-
specific, and they were obtained in the context of
parsing. Since we know that PropBank is more
closely related to grammatical function and syn-
tactic annotation than VerbNet, as indicated above
in Section 6, then these results could simply indi-
cate that parsing predicts PropBank labels better
because they are more closely related to syntactic
labels, and not because the semantic roles inven-
tory is more general.
Several of the findings in the previous sections
shed light on the generality of the semantic roles in
the two inventories. Results in Section 3 show that
previous results can be reinterpreted as indicating
that VerbNet labels generalise better to new roles.
We attempt here to determine the generality of
the ?meaning? of a role label without recourse
to a task-specific experiment. It is often claimed
in the literature that semantic roles are better de-
scribed by feature bundles. In particular, the fea-
tures sentience and volition have been shown to be
useful in distinguishing Proto-Agents from Proto-
Patients (Dowty, 1991). These features can be as-
sumed to be correlated to animacy. Animacy has
indeed been shown to be a reliable indicator of
semantic role differences (Merlo and Stevenson,
2001). Personal pronouns in English grammati-
calise animacy. We extract all the occurrences of
the unambiguously animate pronouns (I, you, he,
she, us, we, me, us, him) and the unambiguously
inanimate pronoun it, for each semantic role label,
in PropBank and VerbNet. We find occurrences
for three semantic role labels in PropBank and six
in VerbNet. We reduce the VerbNet groups to five
by merging Patient roles with PredAttr roles to
avoid artificial variation among very similar roles.
An analysis of variance of the distributions of the
pronous yields a significant effect of animacy for
VerbNet (F(4)=5.62, p< 0.05), but no significant
effect for PropBank (F(2)=4.94, p=0.11). This re-
sult is a preliminary indication that VerbNet labels
might capture basic components of meaning more
clearly than PropBank labels, and that they might
therefore be more general.
8 Conclusions
In this paper, we have proposed a task-
independent, general method to analyse anno-
tation schemes. The method is based on
information-theoretic measures and comparison
with attested linguistic generalisations, to evalu-
ate how well semantic role inventories and anno-
tations capture grammaticalised aspects of mean-
ing. We show that VerbNet is more verb-specific
and better able to generalise to new semantic roles,
while PropBank, because of its relation to syntax,
better captures some of the structural constraints
among roles. Future work will investigate another
basic property of semantic role labelling schemes:
cross-linguistic validity.
Acknowledgements
We thank James Henderson and Ivan Titov for
useful comments. The research leading to these
results has received partial funding from the EU
FP7 programme (FP7/2007-2013) under grant
agreement number 216594 (CLASSIC project:
www.classic-project.org).
295
References
Judith Aissen. 2003. Differential object marking:
Iconicity vs. economy. Natural Language and Lin-
guistic Theory, 21:435?483.
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley FrameNet project. In Proceed-
ings of the Thirty-Sixth Annual Meeting of the As-
sociation for Computational Linguistics and Seven-
teenth International Conference on Computational
Linguistics (ACL-COLING?98), pages 86?90, Mon-
treal, Canada.
David Dowty. 1991. Thematic proto-roles and argu-
ment selection. Language, 67(3):547?619.
Charles Fillmore. 1968. The case for case. In Emmon
Bach and Harms, editors, Universals in Linguistic
Theory, pages 1?88. Holt, Rinehart, and Winston.
Jane Grimshaw. 1990. Argument Structure. MIT
Press.
Jeffrey Gruber. 1965. Studies in Lexical Relation.
MIT Press, Cambridge, MA.
Ray Jackendoff. 1972. Semantic Interpretation in
Generative Grammar. MIT Press, Cambridge, MA.
Karin Kipper. 2005. VerbNet: A broad-coverage, com-
prehensive verb lexicon. Ph.D. thesis, University of
Pennsylvania.
Beth Levin and Malka Rappaport Hovav. 2005. Ar-
gument Realization. Cambridge University Press,
Cambridge, UK.
Edward Loper, Szu ting Yi, and Martha Palmer. 2007.
Combining lexical resources: Mapping between
PropBank and VerbNet. In Proceedings of the
IWCS.
Christopher Manning and Hinrich Schuetze. 1999.
Foundations of Statistical Natural Language Pro-
cessing. MIT Press.
Mitch Marcus, Beatrice Santorini, and M.A.
Marcinkiewicz. 1993. Building a large anno-
tated corpus of English: the Penn Treebank.
Computational Linguistics, 19:313?330.
Paola Merlo and Gabriele Musillo. 2008. Semantic
parsing for high-precision semantic role labelling.
In Proceedings of the Twelfth Conference on Com-
putational Natural Language Learning (CONLL-
08), pages 1?8, Manchester, UK.
Paola Merlo and Suzanne Stevenson. 2001. Automatic
verb classification based on statistical distributions
of argument structure. Computational Linguistics,
27(3):373?408.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The Proposition Bank: An annotated cor-
pus of semantic roles. Computational Linguistics,
31:71?105.
Josef Ruppenhofer, Michael Ellsworth, Miriam
Petruck, Christopher Johnson, and Jan Scheffczyk.
2006. Framenet ii: Theory and practice. Technical
report, Berkeley,CA.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu??s Ma`rquez, and Joakim Nivre. 2008. The
CoNLL-2008 shared task on joint parsing of syn-
tactic and semantic dependencies. In Proceedings
of the 12th Conference on Computational Natural
Language Learning (CoNLL-2008), pages 159?177.
Kristina Toutanova, Aria Haghighi, and Christopher D.
Manning. 2008. A global joint model for semantic
role labeling. Computational Linguistics, 34(2).
Ian Witten and Eibe Frank. 2005. Data Mining. Else-
vier.
Szu-ting Yi, Edward Loper, and Martha Palmer. 2007.
Can semantic roles generalize across genres? In
Proceedings of the Human Language Technologies
2007 (NAACL-HLT?07), pages 548?555, Rochester,
New York, April.
Ben?at Zapirain, Eneko Agirre, and Llu??s Ma`rquez.
2008. Robustness and generalization of role sets:
PropBank vs. VerbNet. In Proceedings of ACL-08:
HLT, pages 550?558, Columbus, Ohio, June.
296
 	
Proceedings of the Ninth International Workshop on Parsing Technologies (IWPT), pages 83?92,
Vancouver, October 2005. c?2005 Association for Computational Linguistics
Lexical and Structural Biases for Function Parsing
Gabriele Musillo
Depts of Linguistics and Computer Science
University of Geneva
2 Rue de Candolle
1211 Geneva 4
Switzerland
musillo4@etu.unige.ch
Paola Merlo
Department of Linguistics
University of Geneva
2 Rue de Candolle
1211 Geneva 4
Switzerland
merlo@lettres.unige.ch
Abstract
In this paper, we explore two extensions
to an existing statistical parsing model to
produce richer parse trees, annotated with
function labels. We achieve significant
improvements in parsing by modelling di-
rectly the specific nature of function la-
bels, as both expressions of the lexical se-
mantics properties of a constituent and as
syntactic elements whose distribution is
subject to structural locality constraints.
We also reach state-of-the-art accuracy
on function labelling. Our results sug-
gest that current statistical parsing meth-
ods are sufficiently robust to produce ac-
curate shallow functional or semantic an-
notation, if appropriately biased.
1 Introduction
Natural language processing methods producing
shallow semantic output are starting to emerge as the
next step towards successful developments in natural
language understanding. Incremental, robust pars-
ing systems will be the core enabling technology for
interactive, speech-based question answering and di-
alogue systems. In recent years, corpora annotated
with semantic and function labels have seen the light
(Palmer et al, 2005; Baker et al, 1998) and semantic
role labelling has taken centre-stage as a challenging
new task. State-of-the-art statistical parsers have not
yet responded to this challenge.
State-of-the-art statistical parsers trained on the
Penn Treebank (PTB) (Marcus et al, 1993) pro-
S


HHH
HH
NP-SBJ
 PPP
the authority
VP


   @@ PPPP
PPP
VBD
dropped
PP-TMP
 H
IN
at
NP
NN
midnight
NP-TMP
NNP
Tuesday
PP-DIR
 HH
TO
to
NP
QP
 PPP
$ 2.80 trillion
Figure 1: A sample syntactic structure with function
labels.
duce trees annotated with bare phrase structure la-
bels (Collins, 1999; Charniak, 2000). The trees of
the Penn Treebank, however, are also decorated with
function labels, labels that indicate the grammatical
and semantic relationship of phrases to each other
in the sentence. Figure 1 shows the simplified tree
representation with function labels for a sample sen-
tence from the PTB corpus (section 00) The Gov-
ernment?s borrowing authority dropped at midnight
Tuesday to 2.80 trillion from 2.87 trillion. Unlike
phrase structure labels, function labels are context-
dependent and encode a shallow level of phrasal and
lexical semantics, as observed first in (Blaheta and
Charniak, 2000). For example, while the authority
in Figure 1 will always be a Noun Phrase, it could
be a subject, as in the example, or an object, as in
the sentence They questioned his authority, depend-
ing on its position in the sentence. To some extent,
function labels overlap with semantic role labels as
defined in PropBank (Palmer et al, 2005). Table 1
83
Syntactic Labels Semantic Labels
DTV dative ADV adverbial
LGS logical subject BNF benefactive
PRD predicate DIR direction
PUT compl of put EXT extent
SBJ surface subject LOC locative
VOC vocative MNR manner
Miscellaneous Labels NOM nominal
CLF it-cleft PRP purpose or rea-
son
HLN headline TMP temporal
TTL title Topic Labels
CLR closely related TPC topicalized
Table 1: Complete set of function labels in the Penn
Treebank.
illustrates the complete list of function labels in the
Penn Treebank, partitioned into four classes. 1
Current statistical parsers do not use or output
this richer information because performance of the
parser usually decreases considerably, since a more
complex task is being solved. (Klein and Manning,
2003), for instance report a reduction in parsing ac-
curacy of an unlexicalised PCFG from 77.8% to
72.9% if using function labels in training. (Blaheta,
2004) also reports a decrease in performance when
attempting to integrate his function labelling system
with a full parser. Conversely, researchers interested
in producing richer semantic outputs have concen-
trated on two-stage systems, where the semantic la-
belling task is performed on the output of a parser,
in a pipeline architecture divided in several stages
(Gildea and Jurafsky, 2002; Nielsen and Pradhan,
2004; Xue and Palmer, 2004). See also the com-
mon task of (CoNLL, 2004; CoNLL, 2005; Sense-
val, 2004), where parsing has sometimes not been
used and has been replaced by chunking.
In this paper, we present a parser that produces
richer output using information available in a corpus
incrementally. Specifically, the parser outputs addi-
tional labels indicating the function of a constituent
in the tree, such as NP-SBJ or PP-TMP in the tree
1(Blaheta and Charniak, 2000) talk of function tags.We will
instead use the term function label, to indicate function identi-
fiers, as they can decorate any node in the tree. We keep the
word tag to indicate only those labels that decorate preterminal
nodes in a tree ? part-of-speech tags ? as is standard use.
shown in Figure 1.
Following (Blaheta and Charniak, 2000), we con-
centrate on syntactic and semantic function labels.
We will ignore the other two classes, for they do
not form natural classes. Like previous work, con-
stituents that do not bear any function label will re-
ceive a NULL label. Strictly speaking, this label cor-
responds to two NULL labels: the SYN-NULL and the
SEM-NULL. A node bearing the SYN-NULL label is
a node that does not bear any other syntactic label.
Analogously, the SEM-NULL label completes the set
of semantic labels. Note that both the SYN-NULL
label and the SEM-NULL are necessary, since both a
syntactic and a semantic label can label a given con-
stituent.
We present work to test the hypothesis that a cur-
rent statistical parser (Henderson, 2003) can out-
put richer information robustly, that is without any
degradation of the parser?s accuracy on the original
parsing task, by explicitly modelling function labels
as the locus where the lexical semantics of the ele-
ments in the sentence and syntactic locality domains
interact. Briefly, our method consists in augmenting
the parser with features and biases that capture both
lexical semantics projections and structural regulari-
ties underlying the distribution of sequences of func-
tion labels in a sentence. We achieve state-of-the-art
results both in parsing and function labelling. This
result has several consequences.
On the one hand, we show that it is possible to
build a single integrated robust system successfully.
This is an interesting achievement, as a task com-
bining function labelling and parsing is more com-
plex than simple parsing. While the function of a
constituent and its structural position are often cor-
related, they sometimes diverge. For example, some
nominal temporal modifiers occupy an object posi-
tion without being objects, like Tuesday in the tree
above. Moreover, given current limited availabil-
ity of annotated tree banks, this more complex task
will have to be solved with the same overall amount
of data, aggravating the difficulty of estimating the
model?s parameters due to sparse data. Solving this
more complex problem successfully, then, indicates
that the models used are robust. Our results also pro-
vide some new insights into the discussion about the
necessity of parsing for function or semantic role la-
belling (Gildea and Palmer, 2002; Punyakanok et al,
84
2005), showing that parsing is beneficial.
On the other hand, function labelling while pars-
ing opens the way to interactive applications that are
not possible in a two-stage architecture. Because the
parser produces richer output incrementally at the
same time as parsing, it can be integrated in speech-
based applications, as well as be used for language
models. Conversely, output annotated with more in-
formative labels, such as function or semantic labels,
underlies all domain-independent question answer-
ing (Jijkoun et al, 2004) or shallow semantic inter-
pretation systems (Collins and Miller, 1998; Ge and
Mooney, 2005).
2 The Basic Architecture
To achieve the complex task of assigning function
labels while parsing, we use a family of statisti-
cal parsers, the Simple Synchrony Network (SSN)
parsers (Henderson, 2003), which do not make any
explicit independence assumptions, and are there-
fore likely to adapt without much modification to the
current problem. This architecture has shown state-
of-the-art performance.
SSN parsers comprise two components, one
which estimates the parameters of a stochastic
model for syntactic trees, and one which searches for
the most probable syntactic tree given the parame-
ter estimates. As with many other statistical parsers
(Collins, 1999; Charniak, 2000), SSN parsers use
a history-based model of parsing. Events in such
a model are derivation moves. The set of well-
formed sequences of derivation moves in this parser
is defined by a Predictive LR pushdown automaton
(Nederhof, 1994), which implements a form of left-
corner parsing strategy.
This pushdown automaton operates on config-
urations of the form (?, v), where ? represents
the stack, whose right-most element is the top,
and v the remaining input. The initial configu-
ration is (ROOT,w) where ROOT is a distin-
guished non-terminal symbol. The final configu-
ration is (ROOT, ). Assuming standard notation
for context-free grammars (Nederhof, 1994), three
derivation moves are defined:
shift
([B ? ?], av) ` ([B ? ?][A ? a], v)
where A ? a and B ? ?C? are productions
such that A is a left-corner of C .
project
([B ? ?][A ? ?], v) `
([B ? ?][D ? A], v)
where A ? ?, D ? A? and B ? ?C? are
productions such that D is a left-corner of C .
attach
([B ? ?][A ? ?], v) ` ([B ? ?A], v)
where both A ? ? and B ? ?A? are produc-
tions.
The joint probability of a phrase-structure tree and
its terminal yield can be equated to the probability
of a finite (but unbounded) sequence of derivation
moves. To bound the number of parameters, stan-
dard history-based models partition the set of well-
formed sequences of transitions into equivalence
classes. While such a partition makes the problem
of searching for the most probable parse polyno-
mial, it introduces hard independence assumptions:
a derivation move only depends on the equivalence
class to which its history belongs. SSN parsers, on
the other hand, do not state any explicit indepen-
dence assumptions: they use a neural network ar-
chitecture, called Simple Synchrony Network (Hen-
derson and Lane, 1998), to induce a finite his-
tory representation of an unbounded sequence of
moves. The history representation of a parse history
d1, . . . , di?1, which we denote h(d1, . . . , di?1), is
assigned to the constituent that is on the top of the
stack before the ith move.
The representation h(d1, . . . , di?1) is computed
from a set f of features of the derivation move di?1
and from a finite set D of recent history representa-
tions h(d1, . . . , dj), where j < i ? 1. Because the
history representation computed for the move i?1 is
included in the inputs to the computation of the rep-
resentation for the next move i, virtually any infor-
mation about the derivation history could flow from
history representation to history representation and
be used to estimate the probability of a derivation
move. However, the recency preference exhibited
by recursively defined neural networks biases learn-
ing towards information which flows through fewer
85
history representations. (Henderson, 2003) exploits
this bias by directly inputting information which is
considered relevant at a given step to the history
representation of the constituent on the top of the
stack before that step. To determine which history
representations are input to which others and pro-
vide SSNs with a linguistically appropriate induc-
tive bias, the set D includes history representations
which are assigned to constituents that are struc-
turally local to a given node on the top of the stack.
In addition to history representations, the inputs to
h(d1, . . . , di?1) include hand-crafted features of the
derivation history that are meant to be relevant to
the move to be chosen at step i. For each of the ex-
periments reported here, the set D that is input to
the computation of the history representation of the
derivation moves d1, . . . , di?1 includes the most re-
cent history representation of the following nodes:
topi, the node on top of the pushdown stack be-
fore the ith move; the left-corner ancestor of topi
(that is, the second top-most node on the parser?s
stack); the leftmost child of topi; and the most re-
cent child of topi, if any. The set of features f in-
cludes the last move in the derivation, the label or
tag of topi, the tag-word pair of the most recently
shifted word, and the leftmost tag-word pair that
topi dominates. Given the hidden history represen-
tation h(d1, ? ? ? , di?1) of a derivation, a normalized
exponential output function is computed by SSNs to
estimate a probability distribution over the possible
next derivation moves di.2
The second component of SSN parsers, which
searches for the best derivation given the parame-
ter estimates, implements a severe pruning strategy.
Such pruning handles the high computational cost
of computing probability estimates with SSNs, and
renders the search tractable. The space of possible
derivations is pruned in two different ways. The first
pruning occurs immediately after a tag-word pair
has been pushed onto the stack: only a fixed beam of
the 100 best derivations ending in that tag-word pair
are expanded. For training, the width of such beam
is set to five. A second reduction of the search space
prunes the space of possible project or attach deriva-
2The on-line version of Backpropagation is used to train
SSN parsing models. It performs the gradient descent with a
maximum likelihood objective function and weight decay regu-
larization (Bishop, 1995).
tion moves: the best-first search strategy is applied
to the five best alternative decisions only.
3 Learning Lexical Projection and
Locality Domains of Function Labels
Recent approaches to functional or semantic labels
are based on two-stage architectures. The first stage
selects the elements to be labelled, while the sec-
ond determines the labels to be assigned to the se-
lected elements. While some of these models are
based on full parse trees (Gildea and Jurafsky, 2002;
Blaheta, 2004), other methods have been proposed
that eschew the need for a full parse (CoNLL, 2004;
CoNLL, 2005). Because of the way the problem has
been formulated, ? as a pipeline of parsing feeding
into labelling ? specific investigations of the inter-
action of lexical projections with the relevant struc-
tural parsing notions during function labelling has
not been studied.
The starting point of our augmentation of SSN
models is the observation that the distribution of
function labels can be better characterised struc-
turally than sequentially. Function labels, similarly
to semantic roles, represent the interface between
lexical semantics and syntax. Because they are pro-
jections of the lexical semantics of the elements in
the sentence, they are projected bottom-up, they tend
to appear low in the tree and they are infrequently
found on the higher levels of the parse tree, where
projections of grammatical, as opposed to lexical,
elements usually reside. Because they are the inter-
face level with syntax, function and semantic labels
are also subject to distributional constraints that gov-
ern syntactic dependencies, especially those govern-
ing the distribution of sequences of long distance
elements. These relations often correspond to top-
down constraints. For example, languages like Ital-
ian allow inversion of the subject (the Agent) in
transitive sentences, giving rise to a linear sequence
where the Theme precedes the Agent (Mangia la
mela Gianni, eats the apple Gianni). Despite this
freedom in the linear order, however, it is never the
case that the structural positions can be switched. It
is a well-attested typological generalisation that one
does not find sentences where the subject is a Theme
and the object is the Agent. The hierarchical de-
scription, then, captures the underlying generalisa-
86
PSfrag replacements
? ? ? ?
 ? ? ?
?1
?2. . . . . .
. . .. . .
S
VP
C-COMMAND
Figure 2: Flow of information in an SSN parser (dashed lines), enhanced by biases specific to function labels
to capture the notion of c-command (solid lines).
tion better than a model based on a linear sequence.
In our augmented model, inputs to each history
representation are selected according to a linguis-
tically motivated notion of structural locality over
which dependencies such as argument structure or
subcategorization could be specified. We attempt to
capture the sequence and the structural position by
indirectly modelling the main definition of syntac-
tic domain, the notion of c-command. Recall that
the c-command relation defines the domain of in-
teraction between two nodes in a tree, even if they
are not close to each other, provided that the first
node dominating one node also dominates the other.
This notion of c-command captures both linear and
hierarchical constraints and defines the domain in
which semantic role labelling applies, as well as
many other linguistic operations.
In SSN parsing models, the set D of nodes that
are structurally local to a given node on the top of
the stack defines the structural distance between this
given node and other nodes in the tree. Such a no-
tion of distance determines the number of history
representations through which information passes to
flow from the representation assigned to a node i to
the representation assigned to a node j. By adding
nodes to the set D, one can shorten the structural
distance between two nodes and enlarge the locality
domain over which dependencies can be specified.
To capture a locality domain appropriate for func-
tion parsing, we include two additional nodes in the
set D: the most recent child of topi labelled with a
syntactic function label and the most recent child of
topi labelled with a semantic function label. These
additions yield a model that is sensitive to regulari-
ties in structurally defined sequences of nodes bear-
ing function labels, within and across constituents.
First, in a sequence of nodes bearing function labels
within the same constituent ? possibly interspersed
with nodes not bearing function labels ? the struc-
tural distance between a node bearing a function la-
bel and any of its right siblings is shortened and con-
stant. This effect comes about because the represen-
tation of a node bearing a function label is directly
input to the representation of its parent, until a far-
ther node with a function label is attached. Second,
the distance between a node labelled with a function
label and any node that it c-commands is kept con-
stant: since the structural distance between a node
[A ? ?] on top of the stack and its left-corner an-
cestor [B ? ?] is constant, the distance between the
most recent child node of B labelled with a func-
tion label and any child of A is kept constant. This
modification of the biases is illustrated in Figure 2.
This figure displays two constituents, S and VP
with some of their respective child nodes. The VP
node is assumed to be on the top of the parser?s
stack, and the S one is supposed to be its left-corner
ancestor. The directed arcs represent the informa-
tion that flows from one node to another. Accord-
ing to the original SSN model in (Henderson, 2003),
only the information carried over by the leftmost
child and the most recent child of a constituent di-
87
rectly flows to that constituent. In the figure above,
only the information conveyed by the nodes ? and
? is directly input to the node S. Similarly, the only
bottom-up information directly input to the VP node
is conveyed by the child nodes  and ?. In both the
no-biases and H03 models, nodes bearing a function
label such as ?1 and ?2 are not directly input to their
respective parents. In our extended model, informa-
tion conveyed by ?1 and ?2 directly flows to their re-
spective parents. So the distance between the nodes
?1 and ?2, which stand in a c-command relation, is
shortened and kept constant.
As well as being subject to locality constraints,
functional labels are projected by the lexical seman-
tics of the words in the sentence. We introduce this
bottom-up lexical information by fine-grained mod-
elling of function tags in two ways. On the one hand,
extending a technique presented in (Klein and Man-
ning, 2003), we split some part-of-speech tags into
tags marked with semantic function labels. The la-
bels attached to a non-terminal which appeared to
cause the most trouble to the parser in a separate ex-
periment (DIR, LOC, MNR, PRP or TMP) were prop-
agated down to the pre-terminal tag of its head. To
affect only labels that are projections of lexical se-
mantics properties, the propagation takes into ac-
count the distance of the projection from the lexical
head to the label, and distances greater than two are
not included. Figure 3 illustrates the result of the tag
splitting operation.
On the other hand, we also split the NULL label
into mutually exclusive labels. We hypothesize that
the label NULL (ie. SYN-NULL and SEM-NULL) is a
mixture of types, some of which of semantic nature,
such as CLR, which will be more accurately learnt
separately. The NULL label was split into the mu-
tually exclusive labels CLR, OBJ and OTHER. Con-
stituents were assigned the OBJ label according to
the conditions stated in (Collins, 1999). Roughly, an
OBJ non-terminal is an NP, SBAR or S whose parent
is an S, VP or SBAR. Any such non-terminal must
not bear either syntactic or semantic function labels,
or the CLR label. In addition, the first child following
the head of a PP is marked with the OBJ label. (For
more detail on this lexical semantics projection, see
(Merlo and Musillo, 2005).)
We report the effects of these augmentations on
parsing results in the experiments described below.
S


HHH
HH
NP-SBJ
 PPP
the authority
VP


  
 
@@
@
PPPP
PPPP
VBD
dropped
PP-TMP
 H
IN-TMP
at
NP
NN
midnight
NP-TMP
NNP-TMP
Tuesday
PP-DIR
 H
TO-DIR
to
NP
QP
 PPP
$ 2.80 trillion
Figure 3: A sample syntactic structure with function
labels lowered onto the preterminals.
4 Experiments and Discussion
To assess the relevance of our fine-grained tags and
history representations for functional labelling, we
compare two augmented models to two baseline
models without these augmentations indicated in Ta-
ble 2 as no-biases and H03. The baseline called H03
refers to our runs of the parser described in (Hen-
derson, 2003), which is not trained on input anno-
tated with function labels. Comparison to this model
gives us an external reference to whether function
labelling improves parsing. The baseline called no-
biases refers to a model without any structural or
lexical biases, but trained on input annotated with
function labels. This comparison will tell us if the
biases are useful or if the reported improvements
could have been obtained without explicit manipu-
lation of the parsing biases.
All SSN function parsers were trained on sec-
tions 2-21 from the PTB and validated on section 24.
They are trained on parse trees whose labels include
syntactic and semantic function labels. The mod-
els, as well as the parser described in (Henderson,
2003), are run only once. This explains the little dif-
ference in performance between our results for H03
in our table of results and those cited in (Henderson,
2003), where the best of three runs on the valida-
tion set is chosen. To evaluate the performance of
our function parsing experiments, we extend stan-
dard Parseval measures of labelled recall and preci-
sion to include function labels.
The augmented models have a total of 188 non-
terminals to represents labels of constituents, instead
of the 33 of the baseline H03 parser. As a result
88
FLABEL FLABEL-less
F R P F R P
H03 88.6 88.3 88.9
no-biases 84.6 84.4 84.9 88.2 88.0 88.4
split-tags 86.1 85.8 86.5 88.9 88.6 89.3
split-tags+locality 86.4 86.1 86.8 89.2 88.9 89.5
Table 2: Percentage F-measure (F), recall (R), and precision (P) of the SSN baseline and augmented parsers.
of lowering the five function labels, 83 new part-of-
speech tags were introduced to partition the original
tag set. SSN parsers do not tag their input sentences.
To provide the augmented models with tagged input
sentences, we trained an SVM tagger whose features
and parameters are described in detail in (Gimenez
and Marquez, 2004). Trained on section 2-21, the
tagger reaches a performance of 95.8% on the test
set (section 23) of the PTB using our new tag set.
Both parsing results taking function labels into
account in the evaluation (FLABEL) and results
not taking them into account in the evaluation
(FLABEL-less) are reported in Table 2, which
shows results on the test set, section 23 of the PTB.
Both the model augmented only with lexical in-
formation (through tag splitting) and the one aug-
mented both with finer-grained tags and represen-
tations of syntactic locality perform better than our
comparison baseline H03, but only the latter is sig-
nificantly better (p < .01, using (Yeh, 2000)?s ran-
domised test). This indicates that while information
projected from the lexical items is very important,
only a combination of lexical semantics information
and careful modelling of syntactic domains provides
a significant improvement.
Parsing results outputting function labels (FLA-
BEL columns) reported in Table 2 indicate that pars-
ing function labels is more difficult than parsing bare
phrase-structure labels (compare the FLABEL col-
umn to the FLABEL-less column). They also show
that our model including finer-grained tags and lo-
cality biases performs better than the one including
only finer-grained tags when outputting function la-
bels. This suggests that our model with both lex-
ical and structural biases performs better than our
no-biases comparison baseline precisely because it
is able to learn to parse function labels more accu-
rately. Comparisons to the baseline without biases
indicates clearly that the observed improvements,
both on function parsing and on parsing without
taking function labels into consideration would not
have been obtained without explicit biases.
Individual performance on syntactic and seman-
tic function labelling compare favourably to previ-
ous attempts (Blaheta, 2004; Blaheta and Charniak,
2000). Note that the maximal precision or recall
score of function labelling is strictly smaller than
one-hundred percent if the precision or the recall of
the parser is less than one-hundred percent. Follow-
ing (Blaheta and Charniak, 2000), incorrectly parsed
constituents will be ignored (roughly 11% of the to-
tal) in the evaluation of the precision and recall of
the function labels, but not in the evaluation of the
parser. Of the correctly parsed constituents, some
bear function labels, but the overwhelming major-
ity do not bear any label, or rather, in our notation,
they bear a NULL label. To avoid calculating ex-
cessively optimistic scores, constituents bearing the
NULL label are not taken into consideration for com-
puting overall recall and precision figures. NULL-
labelled constituents are only needed to calculate the
precision and recall of other function labels. For
example, consider the confusion matrix M in Ta-
ble 3 below, which reports scores for the semantic
labels recovered by the no-biases model. Precision
is computed as
?
i?{ADV???TMP} M [i,i]
?
j?{ADV???TMP} M [SUM,j]
. Recall is
computed analogously. Notice that M [n, n], that is
the [SEM-NULL,SEM-NULL] cell in the matrix, is never
taken into account.
Syntactic labels are recovered with very high ac-
curacy (F 96.5%, R 95.5% and P 97.5%) by the
model with both lexical and structural biases, and
so are semantic labels, which are considerably more
difficult (F 85.6%, R 81.5% and P 90.2%). (Bla-
heta, 2004) uses specialised models for the two types
89
ASSIGNED LABELS
ADV BNF DIR EXT LOC MNR NOM PRP TMP SEM-NULL SUM
ADV 143 0 0 0 0 0 0 1 3 11 158
BNF 0 0 0 0 0 0 0 0 0 1 1
DIR 0 0 39 0 3 4 0 0 1 51 98
EXT 0 0 0 37 0 0 0 0 0 17 54
ACTUAL LOC 0 0 1 0 345 3 0 0 15 148 512
LABELS MNR 0 0 0 0 3 35 0 0 16 40 94
NOM 2 0 0 0 0 0 88 0 0 4 94
PRP 0 0 0 0 0 0 0 54 1 33 88
TMP 18 0 1 0 24 11 0 1 479 105 639
SEM-NULL 12 0 13 5 81 28 12 24 97 20292 20564
SUM 175 0 54 42 456 81 100 80 612 20702 22302
Table 3: Confusion matrix for the no-biases baseline model, tested on the validation set (section 24 of PTB).
of function labels, reaching an F-measure of 98.7%
for syntactic labels and 83.4% for semantic labels as
best accuracy measure. Previous work that uses, like
us, a single model for both types of labels reaches an
F measure of 95.7% for syntactic labels and 79.0%
for semantic labels (Blaheta and Charniak, 2000).
Although functional information is explicitly an-
notated in the PTB, it has not yet been exploited by
any state-of-the-art statistical parser with the notable
exception of the second parsing model of (Collins,
1999). Collins?s second model uses a few func-
tion labels to discriminate between arguments and
adjuncts, and includes parameters to generate sub-
categorisation frames. Subcategorisation frames are
modelled as multisets of arguments that are sisters
of a lexicalised head child. Some major differ-
ences distinguish Collins?s subcategorisation para-
meters from our structural biases. First, lexicalised
head children are not explicitly represented in our
model. Second, we do not discriminate between ar-
guments and adjuncts: we only encode the distinc-
tions between syntactic function labels and seman-
tic ones. As shown in (Merlo, 2003; Merlo and
Esteve-Ferrer, 2004) this difference does not corre-
spond to the difference between arguments and ad-
juncts. Finally, our model does not implement any
distinction between right and left subcategorisation
frames. In Collins?s model, the left and right sub-
categorisation frames are conditionally independent
and arguments occupying a complement position (to
the right of the head) are independent of arguments
occurring in a specifier position (to the left of the
head). In our model, no such independence assump-
tions are stated, because the model is biased towards
phrases related to each other by the c-command re-
lation. Such relation could involve both elements
at the left and at the right of the head. Relations
of functional assignments between subjects and ob-
jects, for example, could be captured.
The most important observation, however, is that
modelling function labels as the interface between
syntax and semantics yields a significant improve-
ment on parsing performance, as can be verified
in the FLABEL-less column of Table 2. This is a
crucial observation in the light of the current ap-
proaches to function or semantic role labelling and
its relation to parsing. An improvement in parsing
performance by better modelling of function labels
indicates that this complex problem is better solved
as a single integrated task and that current two-step
architectures might be missing on successful ways
to improve both the parsing and the labelling task.
In particular, recent models of semantic role la-
belling separate input indicators of the correlation
between the structural position in the tree and the
semantic label, such as path, from those indicators
that encode constraints on the sequence, such as the
previously assigned role (Kwon et al, 2004). In this
way, they can never encode directly the constraining
power of a certain role in a given structural position
onto a following node in its structural position. In
our augmented model, we attempt to capture these
constraints by directly modelling syntactic domains.
Our results confirm the findings in (Palmer et al,
2005). They take a critical look at some commonly
used features in the semantic role labelling task,
such as the path feature. They suggest that the path
feature is not very effective because it is sparse. Its
90
sparseness is due to the occurrence of intermediate
nodes that are not relevant for the syntactic relations
between an argument and its predicate. Our model
of domains is less noisy, because it can focus only on
c-commanding nodes bearing function labels, thus
abstracting away from those nodes that smear the
pertinent relations.
(Yi and Palmer, 2005) share the motivation of our
work, although they apply it to a different task. Like
the current work, they observe that the distributions
of semantic labels could potentially interact with
the distributions of syntactic labels and redefine the
boundaries of constituents, thus yielding trees that
reflect generalisations over both these sources of in-
formation.
Our results also confirm the importance of lexi-
cal information, the lesson drawn from (Thompson
et al, 2004), who find that correctly modelling se-
quence information is not sufficient. Lexical infor-
mation is very important, as it reflects the lexical se-
mantics of the constituents. Both factors, syntactic
domains and lexical information, are needed to sig-
nificantly improve parsing.
5 Conclusions
In this paper, we have explored a new way to im-
prove parsing results in a current statistical parser
while at the same time enriching its output. We
achieve significant improvements in parsing and
function labelling by modelling directly the specific
nature of function labels, as both expressions of
the lexical semantics properties of a constituent and
as syntactic elements whose distribution is subject
to structural locality constraints. Differently from
other approaches, the method we adopt integrates
function labelling directly in the parsing process.
Future work will lie in exploring new ways of cap-
turing syntactic domains, different from the ones at-
tempted in the current paper, such as developing new
derivation moves for nodes bearing function labels.
A more detailed analysis of the parser will also shed
light on its behaviour on sequences of function la-
bels. Finally, we plan to extend this work to learn
Propbank-style semantic role labels, which might re-
quire explicit modelling of long distance dependen-
cies and syntactic movement.
Acknowledgements
We thank the Swiss National Science Foundation
for supporting this research under grant number
101411-105286/1. We also thank James Henderson
for allowing us to use his parser and James Hender-
son and Mirella Lapata for useful discussion of this
work. All remaining errors are our own.
References
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley FrameNet project. In Chris-
tian Boitet and Pete Whitelock, editors, Proceedings
of the Thirty-Sixth Annual Meeting of the Association
for Computational Linguistics and Seventeenth In-
ternational Conference on Computational Linguistics
(ACL-COLING?98), pages 86?90, Montreal, Canada.
Morgan Kaufmann Publishers.
Christopher M. Bishop. 1995. Neural Networks for Pat-
tern Recognition. Oxford University Press, Oxford,
UK.
Don Blaheta and Eugene Charniak. 2000. Assigning
function tags to parsed text. In Proceedings of the
1st Meeting of North American Chapter of Associa-
tion for Computational Linguistics (NAACL?00), pages
234?240, Seattle, Washington.
Don Blaheta. 2004. Function Tagging. Ph.D. thesis,
Department of Computer Science, Brown University.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of the 1st Meeting of North
American Chapter of Association for Computational
Linguistics (NAACL?00), pages 132?139, Seattle,
Washington.
Michael Collins and Scott Miller. 1998. Semantic tag-
ging using a probabilistic context-free grammar. In
Proceedings of the Sixth Workshop on Very Large Cor-
pora, pages 38?48, Montreal, CA.
Michael John Collins. 1999. Head-Driven Statistical
Models for Natural Language Parsing. Ph.D. thesis,
Department of Computer Science, University of Penn-
sylvania.
CoNLL. 2004. Eighth conference on com-
putational natural language learning (conll-2004).
http://cnts.uia.ac.be/conll2004.
CoNLL. 2005. Ninth conference on com-
putational natural language learning (conll-2005).
http://cnts.uia.ac.be/conll2005.
91
Ruifang Ge and Raymond J. Mooney. 2005. A statistical
semantic parser that integrates syntax and semantics.
In Proceedings of the Ninth Conference on Computa-
tional Natural Language Learning (CONLL-05), Ann
Arbor, Michigan.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic la-
beling of semantic roles. Computational Linguistics,
28(3):245?288.
Daniel Gildea and Martha Palmer. 2002. The necessity
of parsing for predicate argument recognition. In Pro-
ceedings of the 40th Annual Meeting of the Associa-
tion for Computational Linguistics (ACL 2002), pages
239?246, Philadelphia, PA.
Jesus Gimenez and Lluis Marquez. 2004. Svmtool: A
general POS tagger generator based on Support Vec-
tor Machines. In Proceedings of the 4th International
Conference on Language Resources and Evaluation
(LREC?04), Lisbon, Portugal.
James Henderson and Peter Lane. 1998. A connec-
tionist architecture for learning to parse. In Proceed-
ings of 17th International Conference on Computa-
tional Linguistics and the 36th Annual Meeting of the
Association for Computational Linguistics (COLING-
ACL?98), pages 531?537, University of Montreal,
Canada.
Jamie Henderson. 2003. Inducing history representa-
tions for broad-coverage statistical parsing. In Pro-
ceedings of the Joint Meeting of the North American
Chapter of the Association for Computational Lin-
guistics and the Human Language Technology Con-
ference (NAACL-HLT?03), pages 103?110, Edmonton,
Canada.
Valentin Jijkoun, Maarten de Rijke, and Jori Mur. 2004.
Information extraction for question answering: Im-
proving recall through syntactic patterns. In Proceed-
ings of COLING-2004, Geneva, Switzerland.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In Proceedings of the 41st
Annual Meeting of the ACL (ACL?03), pages 423?430,
Sapporo, Japan.
Namhee Kwon, Michael Fleischman, and Eduard Hovy.
2004. Senseval automatic labeling of semantic roles
using maximum entropy models. In Senseval-3, pages
129?132, Barcelona, Spain.
Mitch Marcus, Beatrice Santorini, and M.A.
Marcinkiewicz. 1993. Building a large annotated
corpus of English: the Penn Treebank. Computational
Linguistics, 19:313?330.
Paola Merlo and Eva Esteve-Ferrer. 2004. PP attachment
and the notion of argument. University of Geneva,
manuscript.
Paola Merlo and Gabriele Musillo. 2005. Accu-
rate function parsing. In Proceedings of the Human
Language Technology Conference and Conference on
Empirical Methods in Natural Language Processing
(HLT/EMNLP 2005), Vancouver, Canada.
Paola Merlo. 2003. Generalised PP-attachment disam-
biguation using corpus-based linguistic diagnostics. In
Proceedings of the Tenth Conference of The European
Chapter of the Association for Computational Linguis-
tics (EACL?03), pages 251?258, Budapest, Hungary.
Mark Jan Nederhof. 1994. Linguistic Parsing and Pro-
gram Transformations. Ph.D. thesis, Department of
Computer Science, University of Nijmegen.
Rodney Nielsen and Sameer Pradhan. 2004. Mixing
weak learners in semantic parsing. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing (EMNLP-2004), pages 80?87,
Barcelona, Spain, July.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The Proposition Bank: An annotated corpus
of semantic roles. Computational Linguistics, 31:71?
105.
V. Punyakanok, D. Roth, and W. Yih. 2005. The neces-
sity of syntactic parsing for semantic role labeling. In
Proc. of the International Joint Conference on Artifi-
cial Intelligence (IJCAI?05).
Senseval. 2004. Third international workshop on the
evaluation of systems for the semantic analysis of text
(acl 2004). http://www.senseval.org/senseval3.
Cynthia Thompson, Siddharth Patwardhan, and Carolin
Arnold. 2004. Generative models for semantic role
labeling. In Senseval-3, Barcelona, Spain.
Nianwen Xue and Martha Palmer. 2004. Calibrating
features for semantic role labeling. In Proceedings
of the 2004 Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP-2004), pages 88?
94, Barcelona, Spain.
Alexander Yeh. 2000. More accurate tests for the statis-
tical significance of the result differences. In Procs of
the 17th International Conf. on Computatioal Linguis-
tics, pages 947?953, Saarbrucken, Germany.
Szu-ting Yi and Martha Palmer. 2005. The integration
of semantic parsing and semantic role labelling. In
Proceedings of CoNLL?05, Ann Arbor, Michigan.
92
Robust Parsing of the Proposition Bank
Gabriele Musillo
Depts of Linguistics and Computer Science
University of Geneva
2 Rue de Candolle
1211 Geneva 4
Switzerland
musillo4@etu.unige.ch
Paola Merlo
Department of Linguistics
University of Geneva
2 Rue de Candolle
1211 Geneva 4
Switzerland
merlo@lettres.unige.ch
Abstract
In this paper, we extend an existing statis-
tical parsing model to produce richer out-
put parse trees, annotated with PropBank
semantic role labels. Our results show
that the model can be robustly extended to
produce more complex output parse trees
without any loss in performance and sug-
gest that joint inference of syntactic and
semantic representations is a viable alter-
native to approaches based on a pipeline
of local processing steps.
1 Introduction
Recent successes in statistical syntactic parsing
based on supervised learning techniques trained
on a large corpus of syntactic trees (Collins, 1999;
Charniak, 2000; Henderson, 2003) have brought
forth the hope that the same approaches could be
applied to the more ambitious goal of recover-
ing the propositional content and the frame se-
mantics of a sentence. Moving towards a shal-
low semantic level of representation is a first ini-
tial step towards the distant goal of natural lan-
guage understanding and has immediate applica-
tions in question-answering and information ex-
traction. For example, an automatic flight reserva-
tion system processing the sentence I want to book
a flight from Geneva to Trento will need to know
that from Geneva denotes the origin of the flight
and to Trento denotes its destination. Knowing
that these two phrases are prepositional phrases,
the information provided by a syntactic parser, is
only moderately useful.
The growing interest in learning deeper infor-
mation is to a large extent supported and due to
the recent development of semantically annotated
databases such as FrameNet (Baker et al, 1998)
or the Proposition Bank (Palmer et al, 2005), that
can be used as training resources for a number of
supervised learning paradigms. We focus here on
the Proposition Bank (PropBank). PropBank en-
codes propositional information by adding a layer
of argument structure annotation to the syntactic
structures of the Penn Treebank (Marcus et al,
1993). Verbal predicates in the Penn Treebank
(PTB) receive a label REL and their arguments
are annotated with abstract semantic role labels
A0-A5 or AA for those complements of the pred-
icative verb that are considered arguments while
those complements of the verb labelled with a se-
mantic functional label in the original PTB receive
the composite semantic role label AM-X , where
X stands for labels such as LOC, TMP or ADV,
for locative, temporal and adverbial modifiers re-
spectively. A tree structure with PropBank labels
for a sentence from the PTB (section 00) is shown
in Figure 1 below. PropBank uses two levels of
granularity in its annotation, at least conceptually.
Arguments receiving labels A0-A5 or AA do not
express consistent semantic roles and are specific
to a verb, while arguments receiving an AM-X la-
bel are supposed to be adjuncts and the respective
roles they express are consistent across all verbs.1
Recent approaches to learning semantic role la-
bels are based on two-stage architectures. The first
stage selects the elements to be labelled, while the
second determines the labels to be assigned to the
selected elements. While some of these models
are based on full parse trees (Gildea and Jurafsky,
2002; Gildea and Palmer, 2002), other methods
have been proposed that eschew the need for a full
1There are thirteen semantic role labels for modifiers. See
(Palmer et al, 2005) for a detailed discussion of PropBank
semantic roles labels.
11
S




HH
HH
HH
HH
HH
NP-A1




PP
PP
PP
P
the government?s borrowing authority
VP










HH
HH
HH
HH
XXX
XXX
XXX
XXX
XXX
XX
VBD-REL
dropped
PP-AM-TMP
 HH
IN
at
NP
NN
midnight
NP-AM-TMP
NNP
Tuesday
PP-A4
 HH
TO
to
NP
QP


PP
P
$ 2.80 trillion
PP-A3
 HH
IN
from
NP
QP


PP
P
$ 2.87 trillion
Figure 1: A sample syntactic structure from the PropBank with semantic role annotations.
parse (CoNNL, 2004; CoNLL, 2005). Because of
the way the problem has been formulated ? as a
pipeline of parsing (or chunking) feeding into la-
belling ? specific investigations of integrated ap-
proaches that solve both the parsing and the se-
mantic role labelling problems at the same time
have not been studied.
We present work to test the hypothesis that a
current statistical parser (Henderson, 2003) can
output richer information robustly, that is with-
out any significant degradation of the parser?s ac-
curacy on the original parsing task, by explicitly
modelling semantic role labels as the interface be-
tween syntax and semantics.
We achieve promising results both on the simple
parsing task, where the accuracy of the parser is
measured on the standard Parseval measures, and
also on the parsing task where the more complex
labels of PropBank are taken into account. We will
call the former task Penn Treebank parsing (PTB
parsing) and the latter task PropBank parsing be-
low.
These results have several consequences. On
the one hand, we show that it is possible to build a
single integrated robust system successfully. This
is a meaningful achievement, as a task combining
semantic role labelling and parsing is more com-
plex than simple syntactic parsing. While the shal-
low semantics of a constituent and its structural
position are often correlated, they sometimes di-
verge. For example, some nominal temporal mod-
ifiers occupy an object position without being ob-
jects, like Tuesday in Figure 1 below. On the other
hand, our results indicate that the proposed mod-
els are robust. To model our task accurately, ad-
ditional parameters must be estimated. However,
given the current limited availability of annotated
treebanks, this more complex task will have to be
solved with the same overall amount of data, ag-
gravating the difficulty of estimating the model?s
parameters due to sparse data. The limited avail-
ability of data is increased further by the high vari-
ability of the argumental labels A0-A5 whose se-
mantics is specific to a given verb or a given verb
sense. Solving this more complex problem suc-
cessfully, then, indicates that the models used are
robust.
Finally, we achieve robustness without simpli-
fying the parsing architecture. Specifically, ro-
bustness is achieved without resorting to the stip-
ulation of strong independence assumptions to
compensate for the limited availability and high
variability of data. Consequently, such an achieve-
ment demonstrates not only that the robustness
of the parsing model, but also its scalability and
portability.
2 The Basic Parsing Architecture
To achieve the complex task of assigning seman-
tic role labels while parsing, we use a family of
statistical parsers, the Simple Synchrony Network
(SSN) parsers (Henderson, 2003), which do not
make any explicit independence assumptions, and
are therefore likely to adapt without much modi-
fication to the current problem. This architecture
has shown state-of-the-art performance.
SSN parsers comprise two components, one
which estimates the parameters of a stochastic
model for syntactic trees, and one which searches
for the most probable syntactic tree given the
12
parameter estimates. As with many other sta-
tistical parsers (Collins, 1999; Charniak, 2000),
SSN parsers use a history-based model of parsing.
Events in such a model are derivation moves. The
set of well-formed sequences of derivation moves
in this parser is defined by a Predictive LR push-
down automaton (Nederhof, 1994), which imple-
ments a form of left-corner parsing strategy. The
derivation moves include: projecting a constituent
with a specified label, attaching one constituent
to another, and shifting a tag-word pair onto the
pushdown stack.
Unlike standard history-based models, SSN
parsers do not state any explicit independence as-
sumptions between derivation steps. They use a
neural network architecture, called Simple Syn-
chrony Network (Henderson and Lane, 1998), to
induce a finite history representation of an un-
bounded sequence of moves. The history repre-
sentation of a parse history d1, . . . , di?1, which
we denote h(d1, . . . , di?1), is assigned to the con-
stituent that is on the top of the stack before the ith
move.
The representation h(d1, . . . , di?1) is computed
from a set f of features of the derivation move
di?1 and from a finite set D of recent history rep-
resentations h(d1, . . . , dj), where j < i ? 1. Be-
cause the history representation computed for the
move i ? 1 is included in the inputs to the com-
putation of the representation for the next move
i, virtually any information about the derivation
history could flow from history representation to
history representation and be used to estimate the
probability of a derivation move. However, the re-
cency preference exhibited by recursively defined
neural networks biases learning towards informa-
tion which flows through fewer history represen-
tations. (Henderson, 2003) exploits this bias by
directly inputting information which is considered
relevant at a given step to the history representa-
tion of the constituent on the top of the stack be-
fore that step. In addition to history representa-
tions, the inputs to h(d1, . . . , di?1) include hand-
crafted features of the derivation history that are
meant to be relevant to the move to be chosen
at step i. For each of the experiments reported
here, the set D that is input to the computation of
the history representation of the derivation moves
d1, . . . , di?1 includes the most recent history rep-
resentation of the following nodes: topi, the node
on top of the pushdown stack before the ith move;
the left-corner ancestor of topi (that is, the second
top-most node on the parser?s stack); the leftmost
child of topi; and the most recent child of topi, if
any. The set of features f includes the last move in
the derivation, the label or tag of topi, the tag-word
pair of the most recently shifted word, and the left-
most tag-word pair that topi dominates. Given the
hidden history representation h(d1, ? ? ? , di?1) of a
derivation, a normalized exponential output func-
tion is computed by SSNs to estimate a probabil-
ity distribution over the possible next derivation
moves di.2
The second component of SSN parsers, which
searches for the best derivation given the pa-
rameter estimates, implements a severe pruning
strategy. Such pruning handles the high compu-
tational cost of computing probability estimates
with SSNs, and renders the search tractable. The
space of possible derivations is pruned in two dif-
ferent ways. The first pruning occurs immediately
after a tag-word pair has been pushed onto the
stack: only a fixed beam of the 100 best deriva-
tions ending in that tag-word pair are expanded.
For training, the width of such beam is set to five.
A second reduction of the search space prunes
the space of possible project or attach derivation
moves: a best-first search strategy is applied to the
five best alternative decisions only.
The next section describes our model, extended
to produce richer output parse trees annotated with
semantic role labels.
3 Learning Semantic Role Labels
Previous work on learning function labels during
parsing (Merlo and Musillo, 2005; Musillo and
Merlo, 2005) assumed that function labels repre-
sent the interface between lexical semantics and
syntax. We extend this hypothesis to the seman-
tic role labels assigned in PropBank, as they are
an exhaustive extension of function labels, which
have been reorganised in a coherent inventory of
labels and assigned exhaustively to all sentences in
the PTB. Because PropBank is built on the PTB, it
inherits in part its notion of function labels which
is directly integrated into the AM-X role labels.
A0-A5 or AA labels correspond to many of the
unlabelled elements in the PTB and also to those
elements that PTB annotators had classified as re-
2The on-line version of Backpropagation is used to train
SSN parsing models. It performs a gradient descent with
a maximum likelihood objective function and weight decay
regularization (Bishop, 1995).
13
S




HH
HH
HH
HH
HH
NP-A1




PP
PP
PP
P
the government?s borrowing authority
VP










HH
HH
HH
HH
XXX
XXX
XXX
XXX
XXX
XX
VBD-REL
dropped
PP-AM-TMP
 HH
IN(-AM-TMP)
at
NP
NN
midnight
NP-AM-TMP
NNP(-AM-TMP)
Tuesday
PP-A4
 HH
TO
to
NP
QP


PP
P
$ 2.80 trillion
PP-A3
 HH
IN
from
NP
QP


PP
P
$ 2.87 trillion
Figure 2: A sample syntactic structure with semantic role labels lowered onto the preterminals.
ceiving a syntactic functional label such as SBJ
(subject) or DTV (dative).
Because they are projections of the lexical se-
mantics of the elements in the sentence, semantic
role labels are projected bottom-up, they tend to
appear low in the tree and they are infrequently
found on the higher levels of the parse tree, where
projections of grammatical, as opposed to lexical,
elements usually reside. Because they are the in-
terface level with syntax, semantic labels are also
subject to distributional constraints that govern
syntactic dependencies, such as argument struc-
ture or subcategorization. We attempt to capture
such constraints by modelling the c-command re-
lation. Recall that the c-command relation relates
two nodes in a tree, even if they are not close to
each other, provided that the first node dominat-
ing one node also dominate the other. This notion
of c-command captures both linear and hierarchi-
cal constraints and defines the domain in which
semantic role labelling applies.
While PTB function labels appear to overlap to
a large extent with PropBank semantic rolel labels,
work by (Ye and Baldwin, 2005) on semantic la-
belling prepositional phrases, however, indicates
that the function labels in the Penn Treebank are
assigned more sporadically and heterogeneously
than in PropBank. Apparently only the ?easy?
cases have been tagged functionally, because as-
signing these function tags was not the main goal
of the annotation. PropBank instead was anno-
tated exhaustively, taking all cases into account,
annotating multiple roles, coreferences and dis-
continuous constituents. It is therefore not void
of interest to test our hypothesis that, like function
labels, semantic role labels are the interface be-
tween syntax and semantics, and they need to be
recovered by applying constraints that model both
higher level nodes and lower level ones.
We assume that semantic roles are very often
projected by the lexical semantics of the words in
the sentence. We introduce this bottom-up lexical
information by fine-grained modelling of seman-
tic role labels. Extending a technique presented in
(Klein and Manning, 2003) and adopted in (Merlo
and Musillo, 2005; Musillo and Merlo, 2005) for
function labels, we split some part-of-speech tags
into tags marked with semantic role labels. The
semantic role labels attached to a non-terminal di-
rectly projected by a preterminal and belonging to
a few selected categories (DIR, EXT, LOC, MNR,
PNC, CAUS and TMP) were propagated down to
the pre-terminal part-of-speech tag of its head. To
affect only labels that are projections of lexical se-
mantics properties, the propagation takes into ac-
count the distance of the projection from the lex-
ical head to the label, and distances greater than
two are not included. Figure 2 illustrates the result
of this operation.
In our augmented model, inputs to each history
representation are selected according to a linguis-
tically motivated notion of structural locality over
which dependencies such as argument structure or
subcategorization could be specified.
In SSN parsing models, the set D of nodes that
are structurally local to a given node on top of the
stack defines the structural distance between this
given node and other nodes in the tree. Such a no-
tion of distance determines the number of history
representations through which information passes
14
? ? ? ?
? ? ? ?
?1
?2. . . . . .
. . .. . .
S
VP
C-COMMAND
Figure 3: Flow of information in original SSN parsers (dashed lines), enhanced by biases specific to
semantic role labels to capture the notion of c-command (solid lines).
to flow from the representation of a node i to the
representation of a node j. By adding nodes to
the set D, one can shorten the structural distance
between two nodes and enlarge the locality do-
main over which dependencies can be specified.
To capture a locality domain appropriate for se-
mantic role parsing, we add the most recent child
of topi labelled with a semantic role label to the set
D. These additions yield a model that is sensitive
to regularities in structurally defined sequences
of nodes bearing semantic role labels, within and
across constituents. This modification of the bi-
ases is illustrated in Figure 3.
This figure displays two constituents, S and VP
with some of their respective child nodes. The VP
node is assumed to be on the top of the parser?s
stack, and the S one is supposed to be its left-
corner ancestor. The directed arcs represent the
information that flows from one node to another.
According to the original SSN model in (Hender-
son, 2003), only the information carried over by
the leftmost child and the most recent child of a
constituent directly flows to that constituent. In
the figure above, only the information conveyed
by the nodes ? and ? is directly input to the node
S. Similarly, the only bottom-up information di-
rectly input to the VP node is conveyed by the
child nodes ? and ?. In the original SSN models,
nodes bearing a function label such as ?1 and ?2
are not directly input to their respective parents.
In our extended model, information conveyed by
?1 and ?2 directly flows to their respective par-
ents. So the distance between the nodes ?1 and
?2, which stand in a c-command relation, is short-
ened. For more information on this technique to
capture domains induced by the c-command rela-
tion, see (Musillo and Merlo, 2005).
We report the effects of these augmentations on
parsing results in the experiments described below.
4 Experiments
Our extended semantic role SSN parser was
trained on sections 2-21 and validated on section
24 from the PropBank. Training, validating and
testing data sets consist of the PTB data anno-
tated with PropBank semantic roles labels, as pro-
vided in the CoNLL-2005 shared task (Carreras
and Marquez, 2005).
Our augmented model has a total 613 of non-
terminals to represents both the PTB and Prop-
Bank labels of constituents, instead of the 33 of
the original SSN parser. The 580 newly introduced
labels consist of a standard PTB label followed
by a set of one or more PropBank semantic role
such as PP-AM-TMP or NP-A0-A1. As a result
of lowering the six AM-X semantic role labels,
240 new part-of-speech tags were introduced to
partition the original tag set which consisted of 45
tags. SSN parsers do not tag their input sentences.
To provide the augmented model with tagged in-
put sentences, we trained an SVM tagger whose
features and parameters are described in detail in
(Gimenez andMarquez, 2004). Trained on section
2-21, the tagger reaches a performance of 95.45%
on the test set (section 23) using our new tag set.
As already mentioned, argumental labels A0-A5
are specific to a given verb or a given verb sense,
thus their distribution is highly variable. To re-
duce variability, we add some of the tag-verb pairs
licensing these argumental labels to the vocabu-
15
F R P
PropBank training and PropBank parsing task 82.3 82.1 82.4
PropBank training and PTB parsing task 88.8 88.6 88.9
PTB training and PTB parsing task (Henderson, 2003) 88.6 88.3 88.9
Table 1: Percentage F-measure (F), recall (R), and precision (P) of our SSN parser on two different tasks
and the original SSN parser.
lary of our model. We reach a total of 4970 tag-
word pairs.3 This vocabulary comprises the orig-
inal 512 pairs of the original SSN model, and our
added pairs which must occur at least 10 times in
the training data. Our vocabulary as well as the
new 240 POS tags and the new 580 non-terminal
labels are included in the set f of features input to
the history representations as described in section
2.
We perform two different evaluations on our
model trained on PropBank data. Recall that
we distinguish between two parsing tasks: the
PropBank parsing task and the PTB parsing task.
To evaluate the first parsing task, we compute
the standard Parseval measures of labelled recall
and precision of constituents, taking into account
not only the 33 original labels but also the 580
newly introduced PropBank labels. This evalua-
tion gives us an indication of how accurately and
exhaustively we can recover this richer set of non-
terminal labels. The results, computed on the test-
ing data set from the PropBank, are shown on the
first line of Table 1.
To evaluate the PTB task, we compute the la-
belled recall and precision of constituents, ignor-
ing the set of PropBank semantic role labels that
our model assigns to constituents. This evalua-
tion indicates how well we perform on the stan-
dard PTB parsing task alone, and its results on the
testing data set from the PTB are shown on the
second line of Table 1.
The third line of Table 1 gives the performance
on the simpler PTB parsing task of the original
SSN parser (Henderson, 2003), that was trained
on the PTB data sets contrary to our SSN model
trained on the PropBank data sets.
5 Discussion
These results clearly indicate that our model can
perform the PTB parsing task at levels of per-
3Such pairs consists of a tag and a word token. No attempt
at collecting word types was made.
formance comparable to state-of-the-art statistical
parsing, by extensions that take the nature of the
richer labels to be recovered into account. They
also suggest that the relationship between syntac-
tic PTB parsing and semantic PropBank parsing
is strict enough that an integrated approach to the
problem of semantic role labelling is beneficial.
In particular, recent models of semantic role la-
belling separate input indicators of the correlation
between the structural position in the tree and the
semantic label, such as path, from those indicators
that encode constraints on the sequence, such as
the previously assigned role (Kwon et al, 2004).
In this way, they can never encode directly the con-
straining power of a certain role in a given struc-
tural position onto a following node in its struc-
tural position. In our augmented model, we at-
tempt to capture these constraints by directly mod-
elling syntactic domains defined by the notion of
c-command.
Our results also confirm the findings in (Palmer
et al, 2005). They take a critical look at some
commonly used features in the semantic role la-
belling task, such as the path feature. They sug-
gest that the path feature is not very effective be-
cause it is sparse. Its sparseness is due to the oc-
currence of intermediate nodes that are not rele-
vant for the syntactic relations between an argu-
ment and its predicate. Our model of domains is
less noisy, and consequently more robust, because
it can focus only on c-commanding nodes bearing
semantic role labels, thus abstracting away from
those nodes that smear the pertinent relations.
(Yi and Palmer, 2005) share the motivation of
our work. Like the current work, they observe
that the distributions of semantic labels could po-
tentially interact with the distributions of syntactic
labels and redefine the boundaries of constituents,
thus yielding trees that reflect generalisations over
both these sources of information.
To our knowledge, no results have yet been pub-
lished on parsing the PropBank. Accordingly, it is
not possible to draw a straigthforward quantitative
16
F R P
(Haghighi et al, 2005) 83.4 83.1 83.7
(Pradhan et al, 2005) 83.3 83.0 83.5
(Punyakanok et al, 2005) 83.1 82.8 83.3
(Marquez et al, 2005) 83.1 82.8 83.3
(Surdeanu and Turmo, 2005) 82.7 82.5 83.0
PropBank SSN 81.6 81.3 81.9
Table 2: Percentage F-measure (F), recall (R), and precision (P) of our Propbank SSN parser and state-
of-the-art semantic role labelling systems on the PropBank parsing task (1267 sentences from PropBank
validating data sets; Propbank data sets are available at http://www.lsi.upc.edu/ srlconll/st05/st05.html).
comparison between our PropBank SSN parser
and other PropBank parsers. However, state-of-
the-art semantic role labelling systems (CoNLL,
2005) use parse trees output by state-of-the-art
parsers (Collins, 1999; Charniak, 2000), both for
training and testing, and return partial trees anno-
tated with semantic role labels. An indirect way
of comparing our parser with semantic role la-
bellers suggests itself. We merge the partial trees
output by a semantic role labeller with the output
of a parser it was trained on, and compute Prop-
Bank parsing performance measures on the result-
ing parse trees. The first five lines of Table 2 re-
port such measures for the five best semantic role
labelling systems (Haghighi et al, 2005; Pradhan
et al, 2005; Punyakanok et al, 2005; Marquez
et al, 2005; Surdeanu and Turmo, 2005) accord-
ing to (CoNLL, 2005). The partial trees output
by these systems were merged with the parse trees
returned by (Charniak, 2000)?s parser. These sys-
tems use (Charniak, 2000)?s parse trees both for
training and testing as well as various other infor-
mation sources including sets of n-best parse trees
(Punyakanok et al, 2005; Haghighi et al, 2005)
or chunks (Marquez et al, 2005; Pradhan et al,
2005) and named entities (Surdeanu and Turmo,
2005). While our preliminary results indicated in
the last line of Table 2 are not state-of-the-art, they
do demonstrate the viability of SSN parsers for
joint inference of syntactic and semantic represen-
tations.
6 Conclusions
In this paper, we have explored extensions to an
existing state-of-the-art parsing model. We have
achieved promising results on parsing the Propo-
sition Bank, showing that our extensions are suf-
ficiently robust to produce parse trees annotated
with shallow semantic information. Future work
will lie in extracting semantic role relations from
such richly annotated trees, for applications such
as information extraction or question answering.
In addition, further research will explore the rele-
vance of semantic role features to parse reranking.
Acknowledgements
We thank the Swiss National Science Foundation
for supporting this research under grant number
101411-105286/1. We also thank James Hender-
son and Ivan Titov for allowing us to use and mod-
ify their SSN software, Xavier Carreras for pro-
viding the CoNLL-2005 shared task data sets and
the anonymous reviewers for their valuable com-
ments.
References
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley FrameNet project. In Proceed-
ings of the Thirty-Sixth Annual Meeting of the As-
sociation for Computational Linguistics and Seven-
teenth International Conference on Computational
Linguistics (ACL-COLING?98), pages 86?90, Mon-
treal, Canada.
Christopher M. Bishop. 1995. Neural Networks for
Pattern Recognition. Oxford University Press, Ox-
ford, UK.
Xavier Carreras and Lluis Marquez. 2005. Introduc-
tion to the conll-2005 shared task: Semantic role la-
beling. In Proceedings of CoNLL-2005, Ann Arbor,
MI USA.
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In Proceedings of the 1st Meeting
of North American Chapter of Association for Com-
putational Linguistics (NAACL?00), pages 132?139,
Seattle, Washington.
Michael John Collins. 1999. Head-Driven Statistical
Models for Natural Language Parsing. Ph.D. the-
17
sis, Department of Computer Science, University of
Pennsylvania.
CoNLL. 2005. Ninth Conference on Computational
Natural Language Learning (CoNLL-2005). Ann
Arbor, MI, USA.
CoNNL. 2004. Eighth Conference on Computa-
tional Natural Language Learning (CoNLL-2004).
Boston, MA, USA.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic
labeling of semantic roles. Computational Linguis-
tics, 28(3):245?288.
Daniel Gildea and Martha Palmer. 2002. The necessity
of parsing for predicate argument recognition. In
Proceedings of the 40th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL 2002),
pages 239?246, Philadelphia, PA.
Jesus Gimenez and Lluis Marquez. 2004. Svmtool:
A general POS tagger generator based on Support
Vector Machines. In Proceedings of the 4th Interna-
tional Conference on Language Resources and Eval-
uation (LREC?04), Lisbon, Portugal.
Aria Haghighi, Kristina Toutanova, and Christopher
Manning. 2005. A joint model for semantic role
labeling. In Proceedings of CoNLL-2005, Ann Ar-
bor, MI USA.
James Henderson and Peter Lane. 1998. A connection-
ist architecture for learning to parse. In Proceedings
of 17th International Conference on Computational
Linguistics and the 36th Annual Meeting of the As-
sociation for Computational Linguistics (COLING-
ACL?98), pages 531?537, University of Montreal,
Canada.
Jamie Henderson. 2003. Inducing history representa-
tions for broad-coverage statistical parsing. In Pro-
ceedings of the Joint Meeting of the North American
Chapter of the Association for Computational Lin-
guistics and the Human Language Technology Con-
ference (NAACL-HLT?03), pages 103?110, Edmon-
ton, Canada.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting of the ACL (ACL?03), pages
423?430, Sapporo, Japan.
Namhee Kwon, Michael Fleischman, and Eduard
Hovy. 2004. Senseval automatic labeling of se-
mantic roles using maximum entropy models. In
Senseval-3, pages 129?132, Barcelona, Spain.
Mitch Marcus, Beatrice Santorini, and M.A.
Marcinkiewicz. 1993. Building a large anno-
tated corpus of English: the Penn Treebank.
Computational Linguistics, 19:313?330.
Lluis Marquez, Pere Comas, Jesus Gimenez, and Neus
Catala. 2005. Semantic role labeling as sequential
tagging. In Proceedings of CoNLL-2005, Ann Ar-
bor, MI USA.
Paola Merlo and Gabriele Musillo. 2005. Accurate
function parsing. In Proceedings of Human Lan-
guage Technology Conference and Conference on
Empirical Methods in Natural Language Process-
ing, pages 620?627, Vancouver, British Columbia,
Canada, October.
Gabriele Musillo and Paola Merlo. 2005. Lexical and
structural biases for function parsing. In Proceed-
ings of the Ninth International Workshop on Pars-
ing Technology, pages 83?92, Vancouver, British
Columbia, October.
Mark Jan Nederhof. 1994. Linguistic Parsing and Pro-
gram Transformations. Ph.D. thesis, Department of
Computer Science, University of Nijmegen.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The Proposition Bank: An annotated cor-
pus of semantic roles. Computational Linguistics,
31:71?105.
Sameer Pradhan, Kadri Hacioglu, Wayne Ward,
James H. Martin, and Daniel Jurafsky. 2005. Se-
mantic role chunking combining complementary
syntactic views. In Proceedings of CoNLL-2005,
Ann Arbor, MI USA.
Vasin Punyakanok, Peter Koomen, Dan Roth, and Wen
tau Yih. 2005. Generalized inference with multiple
semantic role labeling systems. In Proceedings of
CoNLL-2005, Ann Arbor, MI USA.
Mihai Surdeanu and Jordi Turmo. 2005. Semantic role
labeling using complete syntactic analysis. In Pro-
ceedings of CoNLL-2005, Ann Arbor, MI USA.
Patrick Ye and Timothy Baldwin. 2005. Semantic role
labelling of prepositional phrases. In Proceedings of
the Second International Joint Conference on Nat-
ural Language Processing (IJCNLP-05), pages pp.
779?791, Jeju, South Korea.
Szu-ting Yi and Martha Palmer. 2005. The integration
of semantic parsing and semantic role labelling. In
Proceedings of CoNLL?05, Ann Arbor, Michigan.
18
Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL): Shared Task, pages 37?42,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
A Latent Variable Model of Synchronous Syntactic-Semantic Parsing for
Multiple Languages
Andrea Gesmundo
Univ Geneva
Dept Computer Sci
Andrea.Gesmundo@
unige.ch
James Henderson
Univ Geneva
Dept Computer Sci
James.Henderson@
unige.ch
Paola Merlo
Univ Geneva
Dept Linguistics
Paola.Merlo@
unige.ch
Ivan Titov?
Univ Illinois at U-C
Dept Computer Sci
titov@uiuc.edu
Abstract
Motivated by the large number of languages
(seven) and the short development time (two
months) of the 2009 CoNLL shared task, we
exploited latent variables to avoid the costly
process of hand-crafted feature engineering,
allowing the latent variables to induce features
from the data. We took a pre-existing gener-
ative latent variable model of joint syntactic-
semantic dependency parsing, developed for
English, and applied it to six new languages
with minimal adjustments. The parser?s ro-
bustness across languages indicates that this
parser has a very general feature set. The
parser?s high performance indicates that its la-
tent variables succeeded in inducing effective
features. This system was ranked third overall
with a macro averaged F1 score of 82.14%,
only 0.5% worse than the best system.
1 Introduction
Recent research in syntax-based statistical machine
translation and the recent availability of syntac-
tically annotated corpora for multiple languages
(Nivre et al, 2007) has provided a new opportunity
for evaluating the cross-linguistic validity of statis-
tical models of syntactic structure. This opportu-
nity has been significantly expanded with the 2009
CoNLL shared task on syntactic and semantic pars-
ing of seven languages (Hajic? et al, 2009) belonging
to several different language families.
We participate in this task with a generative,
history-based model proposed in the CoNLL 2008
0Authors in alphabetical order.
shared task for English (Henderson et al, 2008) and
further improved to tackle non-planar dependencies
(Titov et al, 2009). This model maximises the joint
probability of the syntactic and semantic dependen-
cies and thereby enforces that the output structure be
globally coherent, but the use of synchronous pars-
ing allows it to maintain separate structures for the
syntax and semantics. The probabilistic model is
based on Incremental Sigmoid Belief Networks (IS-
BNs), a recently proposed latent variable model for
syntactic structure prediction, which has shown very
good performance for both constituency (Titov and
Henderson, 2007a) and dependency parsing (Titov
and Henderson, 2007b). The use of latent variables
enables this architecture to be extended to learning
a synchronous parse of syntax and semantics with-
out overly restrictive assumptions about the linking
between syntactic and semantic structures.
In this work, we evaluate the ability of this
method to generalise across several languages. We
take the model as it was developed for English, and
apply it directly to all seven languages. The only
fine-tuning was to evaluate whether to include one
feature type which we had previously found did not
help for English, but helped overall. No other fea-
ture engineering was done. The use of latent vari-
ables to induce features automatically from the data
gives our method the adaptability necessary to per-
form well across all seven languages, and demon-
strates the lack of language specificity in the models
of Henderson et al (2008) and Titov et al (2009).
The main properties of this model, that differen-
tiate it from other approaches, is the use of syn-
chronous syntactic and semantic derivations and the
37
use of online planarisation of crossing semantic de-
pendencies. This system was ranked third overall
with a macro averaged F1 score of 82.14%, only
0.5% worse than the best system.
2 The Synchronous Model
The use of synchronous parsing allows separate
structures for syntax and semantics, while still mod-
eling their joint probability. We use the approach
to synchronous parsing proposed in Henderson et al
(2008), where we start with two separate derivations
specifying each of the two structures, then synchro-
nise these derivations at each word. The individual
derivations are based on Nivre?s shift-reduce-style
parsing algorithm (Nivre et al, 2006), as discussed
further below. First we illustrate the high-level struc-
ture of the model, discussed in more detail in Hen-
derson et al (2008).
Let Td be a syntactic dependency tree with
derivation D1d, ..., Dmdd , and Ts be a semantic
dependency graph with derivation D1s , ..., Dmss .
To define derivations for the joint structure
Td, Ts, we divide the two derivations into the
chunks between shifting each word onto the
stack, ctd = Db
t
d
d , ..., D
etd
d and cts = Db
t
ss , ..., De
t
ss ,
where Dbtd?1d = Db
t
s?1s = Shiftt?1 and
De
t
d+1
d = De
t
s+1s = Shiftt. Then the actions of
the synchronous derivations consist of quadruples
Ct = (ctd, Switch, cts, Shiftt), where Switch means
switching from syntactic to semantic mode. This
gives us the following joint probability model,
where n is the number of words in the input.
P (Td, Ts) = ?nt=1 P (Ct|C1, . . . , Ct?1) (1)
These synchronous derivations C1, . . . , Cn only re-
quire a single input queue, since the Shift actions are
synchronised, but they require two separate stacks,
one for the syntactic derivation and one for the se-
mantic derivation.
The probability of each synchronous derivation
chunk Ct is the product of four factors, related to
the syntactic level, the semantic level and the two
synchronising steps. The probability of ctd is de-
composed into one probability for each derivation
action Di, conditioned on its history using the chain
rule, and likewise for cts. These probabilities are es-
timated using the method described in section 3.
Syn cross Sem cross Sem tree No parse
Cat 0% 0% 61.4% 0%
Chi 0% 28.0% 28.6% 9.5%
Cze 22.4% 16.3% 6.1% 1.8%
Eng 7.6% 43.9% 21.4% 3.9%
Ger 28.1% 1.3% 97.4% 0.0%
Jap 0.9% 38.3% 11.2% 14.4%
Spa 0% 0% 57.1% 0%
Table 1: For each language, percentage of training sen-
tences with crossing arcs in syntax and semantics, with
semantic arcs forming a tree, and which were not parsable
using the Swap action.
One of the main characteristics of our syn-
chronous representation, unlike other synchronous
representations of syntax and semantics (Nesson et
al., 2008), is that the synchronisation is done on
words, rather than on structural components. We
take advantage of this freedom and adopt different
methods for handling crossing arcs for syntax and
for semantics.
While both syntax and semantics are represented
as dependency graphs, these graphs differ substan-
tially in their properties. Some statistics which in-
dicate these differences are shown in table 1. For
example, English syntactic dependencies form trees,
while semantic dependency structures are only trees
21.4% of the time, since in general each struc-
ture does not form a connected graph and some
nodes may have more than one parent. The syn-
tactic dependency structures for only 7.6% of En-
glish sentences contain crossing arcs, while 43.9%
of the semantic dependency structures contain cross-
ing arcs. Due to variations both in language char-
acteristics and annotation decisions across corpora,
these differences between syntax and semantics vary
across the seven languages, but they are consis-
tent enough to motivate the development of new
techniques specifically for handling semantic depen-
dency structures. In particular, we use a different
method for parsing crossing arcs.
For parsing crossing semantic arcs (i.e. non-
planar graphs), we use the approach proposed in
Titov et al (2009), which introduces an action Swap
that swaps the top two elements on the parser?s
stack. The Swap action allows the parser to reorder
words online during the parse. This allows words
to be processed in different orders during different
38
portions of the parse, so some arcs can be specified
using one ordering, then other arcs can be specified
using another ordering. Titov et al (2009) found that
only using the Swap action as a last resort is the best
strategy for English (compared to using it preemp-
tively to address future crossing arcs) and we use
the same strategy here for all languages.
Syntactic graphs do not use a Swap action.
We adopt the HEAD method of Nivre and Nils-
son (2005) to de-projectivise syntactic dependencies
outside of parsing.1
3 Features and New Developments
The synchronous derivations described above are
modelled with a type of Bayesian Network called an
Incremental Sigmoid Belief Network (ISBN) (Titov
and Henderson, 2007a). As in Henderson et al
(2008), the ISBN model distinguishes two types of
latent states: syntactic states, when syntactic deci-
sions are considered, and semantic states, when se-
mantic decision are considered. Latent states are
vectors of binary latent variables, which are condi-
tioned on variables from previous states via a pattern
of connecting edges determined by the previous de-
cisions. These latent-to-latent connections are used
to engineer soft biases which reflect the relevant do-
mains of locality in the structure being built. For
these we used the set of connections proposed in
Titov et al (2009), which includes latent-to-latent
connections both from syntax states to semantics
states and vice versa. The latent variable vectors are
also conditioned on a set of observable features of
the derivation history. For these features, we start
with the feature set from Titov et al (2009), which
extends the semantic features proposed in Hender-
son et al (2008) to allow better handling of the non-
planar structures in semantics. Most importantly, all
the features previously included for the top of the
stack were also included for the word just under the
top of the stack. To this set we added one more type
of feature, discussed below.
We made some modifications to reflect differ-
ences in the task definition between the 2008 and
2009 shared tasks, and experimented with one
type of features which had been previously imple-
1The statistics in Table 1 suggest that, for some languages,
swapping might be beneficial for syntax as well.
mented. For the former modifications, the system
was adapted to allow the use of the PFEAT and
FILLPRED fields in the data, which both resulted
in improved accuracy for all the languages. The
PFEAT data field (automatically predicted morpho-
logical features) was introduced in the system in
two ways, as an atomic feature bundle that is pre-
dicted when predicting the word, and split into its
elementary components when conditioning on a pre-
vious word, as was done in Titov and Henderson
(2007b). Because the testing data included a spec-
ification of which words were annotated as predi-
cates (the FILLPRED data field), we constrained the
parser?s output so as to be consistent with this speci-
fication. For rare predicates, if the predicate was not
in the parser?s lexicon (extracted from the training
set), then a sense was taken from the list of senses
reported in the Lexicon and Frame Set resources
available for the closed challenge. If this informa-
tion was not available, then a default sense was con-
structed based on the automatically predicted lemma
(PLEMMA) of the predicate.
We also made use of a previously implemented
type of feature that allows the prediction of a seman-
tic link between two words to be conditioned on the
syntactic dependency already predicted between the
same two words. While this feature had previously
not helped for English, it did result in an overall im-
provement across the languages.
Also, in comparison with previous experiments,
the search beam used in the decoding phase was in-
creased from 50 to 80, producing a small improve-
ment in the overall development score.
All development effort took about two person-
months, mostly by someone who had no previous
experience with the system. Most of this time was
spent on the above differences in the task definition
between the 2008 and 2009 shared tasks.
4 Results and Discussion
We participated in the joint task of the closed chal-
lenge, as described in Hajic? et al (2009). The
datasets used in this challenge are described in Taule?
et al (2008) (Catalan and Spanish), Palmer and Xue
(2009) (Chinese), Hajic? et al (2006) (Czech), Sur-
deanu et al (2008) (English), Burchardt et al (2006)
(German), and Kawahara et al (2002) (Japanese).
39
Rank Average Catalan Chinese Czech English German Japanese Spanish
macro F1 3 82.14 82.66 76.15 83.21 86.03 79.59 84.91 82.43
syntactic acc 1 @85.77 @87.86 76.11 @80.38 88.79 87.29 92.34 @87.64
semantic F1 3 78.42 77.44 76.05 86.02 83.24 71.78 77.23 77.19
Table 2: The three main scores for our system. Rank is within task.
Rank Ave Cze-ood Eng-ood Ger-ood
macro F1 3 75.93 @80.70 75.76 71.32
syn Acc 2 78.01 @76.41 80.84 76.77
sem F1 3 73.63 84.99 70.65 65.25
Table 3: Results on out-of-domain for our system. Rank
is within task.
The official results on the testing set are shown in
tables 2, 3, and 4. The symbol ?@? indicates the
best result across systems. In table 5, we show our
rankings across the different datasets, amongst sys-
tems submitted for the same task.
The overall score used to rank systems is the un-
weighted average of the syntactic labeled accuracy
and the semantic labeled F1 measure, across all lan-
guages (?macro F1? in table 2). We were ranked
third, out of 14 systems. There was only a 0.5% dif-
ference between our score and that of the best sys-
tem, while there was a 1.29% difference between our
score and the fourth ranked system. Only consid-
ering syntactic accuracy, we had the highest aver-
age score of all systems, with the highest individual
score for Catalan, Czech, and Spanish. Only con-
sidering semantic F1, we were again ranked third.
Our results for out-of-domain data (table 3) achieved
a similar level of success, although here we were
ranked second for average syntactic accuracy. Our
precision on semantic arcs was generally much bet-
ter than our recall (shown in table 4). However,
other systems had a similar imbalance, resulting in
no change in our third place ranking for semantic
precision and for semantic recall. Only when the se-
mantic precision is averaged with syntactic accuracy
do we squeeze into second place (?macro Prec?).
To get a more detailed picture of the strengths
and weaknesses of our system, we computed its rank
within each dataset, shown in table 5. Overall, our
system is robust across languages, with little fluc-
tuation in ranking for the overall score, including
for out-of-domain data. The one noticeable excep-
tion to this consistency is the syntactic score for En-
data time (min) macro F1
Czech 25% 5007 73.84
50% 3699 77.57
75% 4201 79.10
100% 6870 80.55
English 25% 1300 79.02
50% 1899 81.61
75% 3196 82.41
100% 3191 83.27
Table 6: Training times and development set accuracies
using different percentages of the training data, for Czech
and English.
glish out-of-domain data. The other ranks for En-
glish out-of-domain and English in-domain scores
are also on the poor side. These results support our
claim that our parser has not undergone much hand-
tuning, since it was originally developed for English.
It is not currently clear whether this relative differ-
ence reflects a English-specific weakness in our sys-
tem, or that many of the other systems have been
fine-tuned for English.
On the higher end of our dataset rankings, we
do relatively well on Catalan, Czech, and Span-
ish. Catalan and Spanish are unique amongst these
datasets in that they have no crossing arcs in their
semantic structure. Czech seems to have semantic
structures which are relatively well handled by our
derivations with Swap. As indicated above in ta-
ble 1, only 2% of sentences are unparsable, despite
16% requiring the Swap action. However, this argu-
ment does not explain why our parser did relatively
poorly on German semantic dependencies. Regard-
less, these observations would suggest that our sys-
tem is still having trouble with crossing dependen-
cies, despite the introduction of the Swap operation,
and that our learning method could achieve better
performance with an improved treatment of cross-
ing semantic dependencies.
Table 6 shows how accuracies and training times
vary with the size of the training dataset, for Czech
and English. Training times vary in part because
40
Rank Ave Cat Chi Cze Eng Ger Jap Spa Cze-ood Eng-ood Ger-ood
semantic Prec 3 81.60 79.08 80.93 87.45 84.92 75.60 83.75 79.44 85.90 72.89 75.19
semantic Rec 3 75.56 75.87 71.73 @84.64 81.63 68.33 71.65 75.05 @84.09 68.55 57.63
macro Prec 2 83.68 83.47 78.52 83.91 86.86 81.44 88.05 83.54 81.16 76.86 @75.98
macro Rec 3 80.66 @81.86 73.92 @82.51 85.21 77.81 81.99 81.35 @80.25 74.70 67.20
Table 4: Semantic precision and recall and macro precision and recall for our system. Rank is within task.
Rank by Ave Cat Chi Cze Eng Ger Jap Spa Ave-ood Cze-ood Eng-ood Ger-ood
macro F1 3 2 3 2 4 4 3 2 3 1 4 3
syntactic Acc 1 1 4 1 3 2 2 1 2 1 7 2
semantic F1 3 2 4 2 4 5 4 2 3 2 4 3
Table 5: Our system?s rank within task according to the three main measures, for each dataset.
?1.4
?1.2
?1
?0.8
?0.6
?0.4
?0.2
 0
 0  10  20  30  40  50
M
ac
ro
 F
1 
D
iff
er
en
ce
Words per Second
Jap
Spa
Cat
Ger
Eng
Cze
Chi
Figure 1: Difference in development set macro F1 as the
search beam is decreased from the submitted beam (80)
to 40, 20, 10, and 5, plotted against parser speed.
random variations can result in different numbers of
training cycles before convergence. Accuracies ap-
pear to be roughly log-linear with data size.
Figure 1 shows how the accuracy of the parser de-
grades as we speed it up by decreasing the search
beam used in decoding, for each language. For some
languages, a slightly smaller search beam is actually
more accurate,2 but for smaller beams the trade-off
of accuracy versus words-per-second is roughly lin-
ear. Parsing time per word is also linear in beam
width, with a zero intercept.
5 Conclusion
In the joint task of the closed challenge of the
CoNLL 2009 shared task (Hajic? et al, 2009), we in-
vestigated how well a model of syntactic-semantic
dependency parsing developed for English would
2This fact suggests that we could have gotten improved re-
sults by tailoring the search beam to individual languages.
generalise to the other six languages. This model
provides a single generative probability of the joint
syntactic and semantic dependency structures, but
allows separate representations for these two struc-
tures by parsing the two structures synchronously.
Finding the statistical correlations both between and
within these structures is facilitated through the use
of latent variables, which induce features automat-
ically from the data, thereby greatly reducing the
need for hand-coded feature engineering.
This latent variable model proved very robust
across languages, achieving a ranking of between
second and fourth on each language, including for
out-of-domain data. The extent to which the parser
does not rely on hand-crafting is underlined by the
fact that its worst ranking is for English, the lan-
guage for which it was developed (particularly for
out-of-domain data). The parser was ranked third
overall out of 14 systems, with a macro averaged F1
score of 82.14%, only 0.5% worse than the best sys-
tem.
Both joint learning and conditioning decisions
about semantic dependencies on latent representa-
tions of syntactic parsing states were crucial to the
success of our model, as was previously demon-
strated in Henderson et al (2008). There, remov-
ing this conditioning led to a 3.5% drop in the SRL
score. This result seems to contradict the gen-
eral trend in the CoNLL-2008 shared task, where
joint learning had only limited success. The lat-
ter fact may be explained by recent theoretical re-
sults demonstrating that pipelines can be preferable
to joint learning (Roth et al, 2009) when no shared
hidden representation is learnt. Our system (Hender-
son et al, 2008) was the only one which attempted to
41
learn a common hidden representation for this mul-
titask learning problem and also was the only one
which achieved significant gain from joint parameter
estimation. We believe that learning shared hidden
representations for related NLP problems is a very
promising direction for further research.
Acknowledgements
We thank Gabriele Musillo and Dan Roth for help and
advice. This work was partly funded by Swiss NSF
grants 100015-122643 and PBGE22-119276, European
Community FP7 grant 216594 (CLASSiC, www.classic-
project.org), US NSF grant SoD-HCER-0613885 and
DARPA (Bootstrap Learning Program).
References
Aljoscha Burchardt, Katrin Erk, Anette Frank, Andrea
Kowalski, Sebastian Pado?, and Manfred Pinkal. 2006.
The SALSA corpus: a German corpus resource for
lexical semantics. In Proceedings of the 5th Interna-
tional Conference on Language Resources and Evalu-
ation (LREC-2006), Genoa, Italy.
Jan Hajic?, Jarmila Panevova?, Eva Hajic?ova?, Petr
Sgall, Petr Pajas, Jan S?te?pa?nek, Jir??? Havelka, Marie
Mikulova?, and Zdene?k Z?abokrtsky?. 2006. Prague De-
pendency Treebank 2.0.
Jan Hajic?, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Anto`nia Mart??, Llu??s
Ma`rquez, Adam Meyers, Joakim Nivre, Sebastian
Pado?, Jan S?te?pa?nek, Pavel Stran?a?k, Mihai Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The CoNLL-
2009 shared task: Syntactic and semantic depen-
dencies in multiple languages. In Proceedings of
the 13th Conference on Computational Natural Lan-
guage Learning (CoNLL-2009), June 4-5, Boulder,
Colorado, USA.
James Henderson, Paola Merlo, Gabriele Musillo, and
Ivan Titov. 2008. A latent variable model of syn-
chronous parsing for syntactic and semantic dependen-
cies. In Proceedings of CONLL 2008, pages 178?182,
Manchester, UK.
Daisuke Kawahara, Sadao Kurohashi, and Ko?iti Hasida.
2002. Construction of a Japanese relevance-tagged
corpus. In Proceedings of the 3rd International
Conference on Language Resources and Evaluation
(LREC-2002), pages 2008?2013, Las Palmas, Canary
Islands.
Rebecca Nesson, Giorgio Satta, and Stuart M. Shieber.
2008. Optimal k-arization of synchronous tree-
adjoining grammar. In Proceedings of ACL-08: HLT,
pages 604?612, Columbus, Ohio, June.
Joakim Nivre and Jens Nilsson. 2005. Pseudo-projective
dependency parsing. In Proc. 43rd Meeting of Asso-
ciation for Computational Linguistics, pages 99?106,
Ann Arbor, MI.
Joakim Nivre, Johan Hall, Jens Nilsson, Gulsen Eryigit,
and Svetoslav Marinov. 2006. Pseudo-projective de-
pendency parsing with support vector machines. In
Proc. of the Tenth Conference on Computational Nat-
ural Language Learning, pages 221?225, New York,
USA.
Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan Mc-
Donald, Jens Nilsson, Sebastian Riedel, and Deniz
Yuret. 2007. The CoNLL 2007 shared task on depen-
dency parsing. In Proceedings of the CoNLL Shared
Task Session of EMNLP-CoNLL 2007, pages 915?932,
Prague, Czech Republic, June.
Martha Palmer and Nianwen Xue. 2009. Adding seman-
tic roles to the Chinese Treebank. Natural Language
Engineering, 15(1):143?172.
Dan Roth, Kevin Small, and Ivan Titov. 2009. Sequential
learning of classifiers for structured prediction prob-
lems. In AISTATS, Clearwater, Florida, USA.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu??s Ma`rquez, and Joakim Nivre. 2008. The CoNLL-
2008 shared task on joint parsing of syntactic and se-
mantic dependencies. In Proceedings of the 12th Con-
ference on Computational Natural Language Learning
(CoNLL-2008).
Mariona Taule?, Maria Anto`nia Mart??, and Marta Re-
casens. 2008. AnCora: Multilevel Annotated Corpora
for Catalan and Spanish. In Proceedings of the 6th
International Conference on Language Resources and
Evaluation (LREC-2008), Marrakesh, Morroco.
Ivan Titov and James Henderson. 2007a. Constituent
parsing with Incremental Sigmoid Belief Networks. In
Proceedings of the 45th Annual Meeting of the Asso-
ciation of Computational Linguistics, pages 632?639,
Prague, Czech Republic.
Ivan Titov and James Henderson. 2007b. Fast and ro-
bust multilingual dependency parsing with a genera-
tive latent variable model. In Proc. Joint Conf. on Em-
pirical Methods in Natural Language Processing and
Computational Natural Language Learning (EMNLP-
CoNLL 2007), Prague, Czech Republic. (CoNLL
Shared Task).
Ivan Titov, James Henderson, Paola Merlo, and Gabriele
Musillo. 2009. Online graph planarisation for syn-
chronous parsing of semantic and syntactic dependen-
cies. In Proc. Twenty-First International Joint Confer-
ence on Artificial Intelligence (IJCAI-09), Pasadena,
California.
42
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 299?304,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Scaling up Automatic Cross-Lingual Semantic Role Annotation
Lonneke van der Plas
Department of Linguistics
University of Geneva
Geneva, Switzerland
Paola Merlo
Department of Linguistics
University of Geneva
Geneva, Switzerland
{Lonneke.vanderPlas,Paola.Merlo,James.Henderson}@unige.ch
James Henderson
Department of Computer Science
University of Geneva
Geneva, Switzerland
Abstract
Broad-coverage semantic annotations for
training statistical learners are only available
for a handful of languages. Previous ap-
proaches to cross-lingual transfer of seman-
tic annotations have addressed this problem
with encouraging results on a small scale. In
this paper, we scale up previous efforts by us-
ing an automatic approach to semantic anno-
tation that does not rely on a semantic on-
tology for the target language. Moreover,
we improve the quality of the transferred se-
mantic annotations by using a joint syntactic-
semantic parser that learns the correlations be-
tween syntax and semantics of the target lan-
guage and smooths out the errors from auto-
matic transfer. We reach a labelled F-measure
for predicates and arguments of only 4% and
9% points, respectively, lower than the upper
bound from manual annotations.
1 Introduction
As data-driven techniques tackle more and more
complex natural language processing tasks, it be-
comes increasingly unfeasible to use complete, ac-
curate, hand-annotated data on a large scale for
training models in all languages. One approach to
addressing this problem is to develop methods that
automatically generate annotated data by transfer-
ring annotations in parallel corpora from languages
for which this information is available to languages
for which these data are not available (Yarowsky et
al., 2001; Fung et al, 2007; Pado? and Lapata, 2009).
Previous work on the cross-lingual transfer of se-
mantic annotations (Pado?, 2007; Basili et al, 2009)
has produced annotations of good quality for test
sets that were carefully selected based on seman-
tic ontologies on the source and target side. It has
been suggested that these annotations could be used
to train semantic role labellers (Basili et al, 2009).
In this paper, we generate high-quality broad-
coverage semantic annotations using an automatic
approach that does not rely on a semantic ontol-
ogy for the target language. Furthermore, to our
knowledge, we report the first results on using joint
syntactic-semantic learning to improve the quality
of the semantic annotations from automatic cross-
lingual transfer. Results on correlations between
syntax and semantics found in previous work (Merlo
and van der Plas, 2009; Lang and Lapata, 2010) have
led us to make use of the available syntactic anno-
tations on the target language. We use the seman-
tic annotations resulting from cross-lingual transfer
combined with syntactic annotations to train a joint
syntactic-semantic parser for the target language,
which, in turn, re-annotates the corpus (See Fig-
ure 1). We show that the semantic annotations pro-
duced by this parser are of higher quality than the
data on which it was trained.
Given our goal of producing broad-coverage an-
notations in a setting based on an aligned corpus,
our choices of formal representation and of labelling
scheme differ from previous work (Pado?, 2007;
Basili et al, 2009). We choose a dependency repre-
sentation both for the syntax and semantics because
relations are expressed as direct arcs between words.
This representation allows cross-lingual transfer to
use word-based alignments directly, eschewing the
need for complex constituent-alignment algorithms.
299
Train?a?French?syntacticparser
?Transfer?semantic?annotationsfrom?EN?to?FR?using?wordalignments
EN?syntactic?semanticannotations
EN?FR?word?aligneddata
FR?syntacticannotations
FR?semanticannotations evaluatio
n
Train?Frenchjoint?syntactic?semantic?parser
evaluatio
n
FR?syntacticannotations
FR?semanticannotations
Figure 1: System overview
We choose the semantic annotation scheme defined
by Propbank, because it has broad coverage and in-
cludes an annotated corpus, contrary to other avail-
able resources such as FrameNet (Fillmore et al,
2003) and is the preferred annotation scheme for a
joint syntactic-semantic setting (Merlo and van der
Plas, 2009). Furthermore, Monachesi et al (2007)
showed that the PropBank annotation scheme can be
used for languages other than English directly.
2 Cross-lingual semantic transfer
Data-driven induction of semantic annotation based
on parallel corpora is a well-defined and feasible
task, and it has been argued to be particularly suit-
able to semantic role label annotation because cross-
lingual parallelism improves as one moves to more
abstract linguistic levels of representation. While
Hwa et al (2002; 2005) find that direct syntactic de-
pendency parallelism between English and Spanish
concerns 37% of dependency links, Pado? (2007) re-
ports an upper-bound mapping correspondence cal-
culated on gold data of 88% F-measure for in-
dividual semantic roles, and 69% F-measure for
whole scenario-like semantic frames. Recently, Wu
and Fung (2009a; 2009b) also show that semantic
roles help in statistical machine translation, capi-
talising on a study of the correspondence between
English and Chinese which indicates that 84% of
roles transfer directly, for PropBank-style annota-
tions. These results indicate high correspondence
across languages at a shallow semantic level.
Based on these results, our transfer of semantic
annotations from English sentences to their French
translations is based on a very strong mapping hy-
pothesis, adapted from the Direct Correspondence
Assumption for syntactic dependency trees by Hwa
et al (2005).
Direct Semantic Transfer (DST) For any
pair of sentences E and F that are transla-
tions of each other, we transfer the seman-
tic relationship R(xE , yE) to R(xF , yF ) if
and only if there exists a word-alignment
between xE and xF and between yE and
yF , and we transfer the semantic property
P (xE) to P (xF ) if and only if there exists
a word-alignment between xE and xF .
The relationships which we transfer are semantic
role dependencies and the properties are predicate
senses. We introduce one constraint to the direct se-
mantic transfer. Because the semantic annotations in
the target language are limited to verbal predicates,
we only transfer predicates to words the syntactic
parser has tagged as a verb.
As reported by Hwa et al (2005), the direct cor-
respondence assumption is a strong hypothesis that
is useful to trigger a projection process, but will not
work correctly for several cases.
We used a filter to remove obviously incomplete
annotations. We know from the annotation guide-
lines used to annotate the French gold sentences that
all verbs, except modals and realisations of the verb
e?tre, should receive a predicate label. We define a
filter that removes sentences with missing predicate
labels based on PoS-information in the French sen-
tence.
2.1 Learning joint syntactic-semantic
structures
We know from previous work that there is a strong
correlation between syntax and semantics (Merlo
and van der Plas, 2009), and that this correla-
tion has been successfully applied for the unsuper-
vised induction of semantic roles (Lang and Lap-
ata, 2010). However, previous work in machine
translation leads us to believe that transferring the
correlations between syntax and semantics across
languages would be problematic due to argument-
structure divergences (Dorr, 1994). For example,
the English verb like and the French verb plaire do
not share correlations between syntax and seman-
tics. The verb like takes an A0 subject and an A1
300
direct object, whereas the verb plaire licences an A1
subject and an A0 indirect object.
We therefore transfer semantic roles cross-
lingually based only on lexical alignments and add
syntactic information after transfer. In Figure 1, we
see that cross-lingual transfer takes place at the se-
mantic level, a level that is more abstract and known
to port relatively well across languages, while the
correlations with syntax, that are known to diverge
cross-lingually, are learnt on the target language
only. We train a joint syntactic-semantic parser
on the combination of the two linguistic levels that
learns the correlations between these structures in
the target language and is able to smooth out errors
from automatic transfer.
3 Experiments
We used two statistical parsers in our transfer of
semantic annotations from English to French, one
for syntactic parsing and one for joint syntactic-
semantic parsing. In addition, we used several cor-
pora.
3.1 The statistical parsers
For our syntactic-semantic parsing model, we use
a freely-available parser (Henderson et al, 2008;
Titov et al, 2009). The probabilistic model is a joint
generative model of syntactic and semantic depen-
dencies that maximises the joint probability of the
syntactic and semantic dependencies, while building
two separate structures.
For the French syntactic parser, we used the de-
pendency parser described in Titov and Hender-
son (2007). We train the parser on the dependency
version of the French Paris treebank (Candito et al,
2009), achieving 87.2% labelled accuracy on this
data set.
3.2 Data
To transfer semantic annotation from English to
French, we used the Europarl corpus (Koehn,
2003)1. We word-align the English sentences to the
French sentences automatically using GIZA++ (Och
1As is usual practice in preprocessing for automatic align-
ment, the datasets were tokenised and lowercased and only sen-
tence pairs corresponding to a one-to-one sentence alignment
with lengths ranging from one to 40 tokens on both French and
English sides were considered.
and Ney, 2003) and include only intersective align-
ments. Furthermore, because translation shifts are
known to pose problems for the automatic projection
of semantic roles across languages (Pado?, 2007), we
select only those parallel sentences in Europarl that
are direct translations from English to French, or
vice versa. In the end, we have a word-aligned par-
allel corpus of 276-thousand sentence pairs.
Syntactic annotation is available for French. The
French Treebank (Abeille? et al, 2003) is a treebank
of 21,564 sentences annotated with constituency an-
notation. We use the automatic dependency conver-
sion of the French Treebank into dependency format
provided to us by Candito and Crabbe? and described
in Candito et al (2009).
The Penn Treebank corpus (Marcus et al, 1993)
merged with PropBank labels (Palmer et al, 2005)
and NomBank labels (Meyers, 2007) is used to train
the syntactic-semantic parser described in Subsec-
tion 3.1 to annotate the English part of the parallel
corpus.
3.3 Test sets
For testing, we used the hand-annotated data de-
scribed in (van der Plas et al, 2010). One-thousand
French sentences are extracted randomly from our
parallel corpus without any constraints on the se-
mantic parallelism of the sentences, unlike much
previous work. We randomly split those 1000 sen-
tences into test and development set containing 500
sentences each.
4 Results
We evaluate our methods for automatic annotation
generation twice: once after the transfer step, and
once after joint syntactic-semantic learning. The
comparison of these two steps will tell us whether
the joint syntactic-semantic parser is able to improve
semantic annotations by learning from the syntactic
annotations available. We evaluate the models on
unrestricted test sets2 to determine if our methods
scale up.
Table 1 shows the results of automatically an-
notating French sentences with semantic role an-
notation. The first set of columns of results re-
2Due to filtering, the test set for the transfer (filter) model is
smaller and not directly comparable to the other three models.
301
Predicates Arguments (given predicate)
Labelled Unlabelled Labelled Unlabelled
Prec Rec F Prec Rec F Prec Rec F Prec Rec F
1 Transfer (no filter) 50 31 38 91 55 69 61 48 54 72 57 64
2 Transfer (filter) 51 46 49 92 84 88 65 51 57 76 59 67
3 Transfer+parsing (no filter) 71 29 42 97 40 57 77 57 65 87 64 74
4 Transfer+parsing (filter) 61 50 55 95 78 85 71 52 60 83 61 70
5 Inter-annotator agreement 61 57 59 97 89 93 73 75 74 88 91 89
Table 1: Percent recall, precision, and F-measure for predicates and for arguments given the predicate, for the four
automatic annotation models and the manual annotation.
ports labelling and identification of predicates and
the second set of columns reports labelling and iden-
tification of arguments, respectively, for the predi-
cates that are identified. The first two rows show
the results when applying direct semantic transfer.
Rows three and four show results when using the
joint syntactic-semantic parser to re-annotate the
sentences. For both annotation models we show re-
sults when using the filter described in Section 2 and
without the filter.
The most striking result that we can read from
Table 1 is that the joint syntactic-semantic learning
step results in large improvements, especially for
argument labelling, where the F-measure increases
from 54% to 65% for the unfiltered data. The parser
is able to outperform the quality of the semantic
data on which it was trained by using the infor-
mation contained in the syntax. This result is in
accordance with results reported in Merlo and Van
der Plas (2009) and Lang and Lapata (2010), where
the authors find a high correlation between syntactic
functions and PropBank semantic roles.
Filtering improves the quality of the transferred
annotations. However, when training a parser on the
annotations we see that filtering only results in better
recall scores for predicate labelling. This is not sur-
prising given that the filters apply to completeness in
predicate labelling specifically. The improvements
from joint syntactic-semantic learning for argument
labelling are largest for the unfiltered setting, be-
cause the parser has access to larger amounts of data.
The filter removes 61% of the data.
As an upper bound we take the inter-annotator
agreement for manual annotation on a random set
of 100 sentences (van der Plas et al, 2010), given
in the last row of Table 1. The parser reaches an
F-measure on predicate labelling of 55% when us-
ing filtered data, which is very close to the up-
per bound (59%). The upper bound for argument
inter-annotator agreement is an F-measure of 74%.
The parser trained on unfiltered data reaches an
F-measure of 65%. These results on unrestricted
test sets and their comparison to manual annotation
show that we are able to scale up cross-lingual se-
mantic role annotation.
5 Discussion and error analysis
A more detailed analysis of the distribution of im-
provements over the types of roles further strength-
ens the conclusion that the parser learns the corre-
lations between syntax and semantics. It is a well-
known fact that there exists a strong correlation be-
tween syntactic function and semantic role for the
A0 and A1 arguments: A0s are commonly mapped
onto subjects and A1s are often realised as direct ob-
jects (Lang and Lapata, 2010). It is therefore not
surprising that the F-measure on these types of ar-
guments increases by 12% and 15%, respectively,
after joint-syntactic semantic learning. Since these
arguments make up 65% of the roles, this introduces
a large improvement. In addition, we find improve-
ments of more than 10% on the following adjuncts:
AM-CAU, AM-LOC, AM-MNR, and AM-MOD that to-
gether comprise 9% of the data.
With respect to predicate labelling, comparison
of the output after transfer with the output after
parsing (on the development set) shows how the
parser smooths out transfer errors and how inter-
lingual divergences can be solved by making use
of the variations we find intra-lingually. An exam-
ple is given in Figure 2. The first line shows the
predicate-argument structure given by the English
302
EN (source) Postal [A1 services] [AM-MOD must] [CONTINUE.01 continue] [C-A1 to] be public services.
FR (transfer) Les [A1services] postaux [AM-MOD doivent] [CONTINUE.01rester] des services publics.
FR (parsed) Les [A1 services] postaux [AM-MOD doivent] [REMAIN.01rester] des [A3 services] publics.
Figure 2: Differences in predicate-argument labelling after transfer and after parsing
syntactic-semantic parser to the English sentence.
The second line shows the French translation and
the predicate-argument structure as it is transferred
cross-lingually following the method described in
Section 2. Transfer maps the English predicate la-
bel CONTINUE.01 onto the French verb rester, be-
cause these two verbs are aligned. The first oc-
currence of services is aligned to the first occur-
rence of services in the English sentence and gets
the A1 label. The second occurrence of services
gets no argument label, because there is no align-
ment between the C-A1 argument to, the head of
the infinitival clause, and the French word services.
The third line shows the analysis resulting from the
syntactic-semantic parser that has been trained on a
corpus of French sentences labelled with automat-
ically transferred annotations and syntactic annota-
tions. The parser has access to several labelled ex-
amples of the predicate-argument structure of rester,
which in many other cases is translated with remain
and has the same predicate-argument structure as
rester. Consequently, the parser re-labels the verb
with REMAIN.01 and labels the argument with A3.
Because the languages and annotation framework
adopted in previous work are not directly compara-
ble to ours, and their methods have been evaluated
on restricted test sets, results are not strictly com-
parable. But for completeness, recall that our best
result for predicate identification is an F-measure
of 55% accompanied with an F-measure of 60%
for argument labelling. Pado? (2007) reports a 56%
F-measure on transferring FrameNet roles, know-
ing the predicate, from an automatically parsed and
semantically annotated English corpus. Pado? and
Pitel (2007), transferring semantic annotation to
French, report a best result of 57% F-measure for
argument labelling given the predicate. Basili et
al. (2009), in an approach based on phrase-based
machine translation to transfer FrameNet-like anno-
tation from English to Italian, report 42% recall in
identifying predicates and an aggregated 73% recall
of identifying predicates and roles given these pred-
icates. They do not report an unaggregated number
that can be compared to our 60% argument labelling.
In a recent paper, Annesi and Basili (2010) improve
the results from Basili et al (2009) by 11% using
Hidden Markov Models to support the automatic
semantic transfer. Johansson and Nugues (2006)
trained a FrameNet-based semantic role labeller for
Swedish on annotations transferred cross-lingually
from English parallel data. They report 55% F-
measure for argument labelling given the frame on
150 translated example sentences.
6 Conclusions
In this paper, we have scaled up previous efforts of
annotation by using an automatic approach to se-
mantic annotation transfer in combination with a
joint syntactic-semantic parsing architecture. We
propose a direct transfer method that requires nei-
ther manual intervention nor a semantic ontology for
the target language. This method leads to semanti-
cally annotated data of sufficient quality to train a
syntactic-semantic parser that further improves the
quality of the semantic annotation by joint learning
of syntactic-semantic structures on the target lan-
guage. The labelled F-measure of the resulting an-
notations for predicates is only 4% point lower than
the upper bound and the resulting annotations for ar-
guments only 9%.
Acknowledgements
The research leading to these results has received
funding from the EU FP7 programme (FP7/2007-
2013) under grant agreement nr 216594 (CLAS-
SIC project: www.classic-project.org), and from the
Swiss NSF under grant 122643.
References
A. Abeille?, L. Cle?ment, and F. Toussenel. 2003. Building
a treebank for French. In Treebanks: Building and
Using Parsed Corpora. Kluwer Academic Publishers.
303
P. Annesi and R. Basili. 2010. Cross-lingual alignment
of FrameNet annotations through Hidden Markov
Models. In Proceedings of CICLing.
R. Basili, D. De Cao, D. Croce, B. Coppola, and A. Mos-
chitti, 2009. Computational Linguistics and Intelli-
gent Text Processing, chapter Cross-Language Frame
Semantics Transfer in Bilingual Corpora, pages 332?
345. Springer Berlin / Heidelberg.
M.-H. Candito, B. Crabbe?, P. Denis, and F. Gue?rin. 2009.
Analyse syntaxique du franc?ais : des constituants
aux de?pendances. In Proceedings of la Confe?rence
sur le Traitement Automatique des Langues Naturelles
(TALN?09), Senlis, France.
B. Dorr. 1994. Machine translation divergences: A for-
mal description and proposed solution. Computational
Linguistics, 20(4):597?633.
C. J. Fillmore, R. Johnson, and M.R.L. Petruck. 2003.
Background to FrameNet. International journal of
lexicography, 16.3:235?250.
P. Fung, Z. Wu, Y. Yang, and D. Wu. 2007. Learn-
ing bilingual semantic frames: Shallow semantic pars-
ing vs. semantic role projection. In 11th Conference
on Theoretical and Methodological Issues in Machine
Translation (TMI 2007).
J. Henderson, P. Merlo, G. Musillo, and I. Titov. 2008. A
latent variable model of synchronous parsing for syn-
tactic and semantic dependencies. In Proceedings of
CONLL 2008, pages 178?182.
R. Hwa, P. Resnik, A. Weinberg, and O. Kolak. 2002.
Evaluating translational correspondence using anno-
tation projection. In Proceedings of the 40th Annual
Meeting of the ACL.
R. Hwa, P. Resnik, A.Weinberg, C. Cabezas, and O. Ko-
lak. 2005. Bootstrapping parsers via syntactic projec-
tion accross parallel texts. Natural language engineer-
ing, 11:311?325.
R. Johansson and P. Nugues. 2006. A FrameNet-based
semantic role labeler for Swedish. In Proceedings of
the annual Meeting of the Association for Computa-
tional Linguistics (ACL).
P. Koehn. 2003. Europarl: A multilingual corpus for
evaluation of machine translation.
J. Lang and M. Lapata. 2010. Unsupervised induction
of semantic roles. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Linguis-
tics, pages 939?947, Los Angeles, California, June.
Association for Computational Linguistics.
M. Marcus, B. Santorini, and M.A. Marcinkiewicz.
1993. Building a large annotated corpus of English:
the Penn Treebank. Comp. Ling., 19:313?330.
P. Merlo and L. van der Plas. 2009. Abstraction and gen-
eralisation in semantic role labels: PropBank, VerbNet
or both? In Proceedings of the Joint Conference of the
47th Annual Meeting of the ACL and the 4th Interna-
tional Joint Conference on Natural Language Process-
ing of the AFNLP, pages 288?296, Suntec, Singapore.
A. Meyers. 2007. Annotation guidelines for NomBank
- noun argument structure for PropBank. Technical
report, New York University.
P. Monachesi, G. Stevens, and J. Trapman. 2007. Adding
semantic role annotation to a corpus of written Dutch.
In Proceedings of the Linguistic Annotation Workshop
(LAW), pages 77?84, Prague, Czech republic.
F. J. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29:19?51.
Sebastian Pado? and Mirella Lapata. 2009. Cross-lingual
annotation projection of semantic roles. Journal of Ar-
tificial Intelligence Research, 36:307?340.
S. Pado? and G. Pitel. 2007. Annotation pre?cise du
franc?ais en se?mantique de ro?les par projection cross-
linguistique. In Proceedings of TALN.
S. Pado?. 2007. Cross-lingual Annotation Projection
Models for Role-Semantic Information. Ph.D. thesis,
Saarland University.
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The
Proposition Bank: An annotated corpus of semantic
roles. Computational Linguistics, 31:71?105.
I. Titov and J. Henderson. 2007. A latent variable model
for generative dependency parsing. In Proceedings of
the International Conference on Parsing Technologies
(IWPT-07), pages 144?155, Prague, Czech Republic.
I. Titov, J. Henderson, P. Merlo, and G. Musillo. 2009.
Online graph planarisation for synchronous parsing of
semantic and syntactic dependencies. In Proceedings
of the twenty-first international joint conference on ar-
tificial intelligence (IJCAI-09), Pasadena, California,
July.
L. van der Plas, T. Samardz?ic?, and P. Merlo. 2010. Cross-
lingual validity of PropBank in the manual annotation
of French. In In Proceedings of the 4th Linguistic An-
notation Workshop (The LAW IV), Uppsala, Sweden.
D. Wu and P. Fung. 2009a. Can semantic role labeling
improve SMT? In Proceedings of the Annual Confer-
ence of European Association of Machine Translation.
D. Wu and P. Fung. 2009b. Semantic roles for SMT:
A hybrid two-pass model. In Proceedings of the
Joint Conference of the North American Chapter of
ACL/Human Language Technology.
D. Yarowsky, G. Ngai, and R. Wicentowski. 2001. In-
ducing multilingual text analysis tools via robust pro-
jection across aligned corpora. In Proceedings of the
International Conference on Human Language Tech-
nology (HLT).
304
CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 1?8
Manchester, August 2008
Semantic Parsing for High-Precision Semantic Role Labelling
Paola Merlo
Linguistics Department
University of Geneva
5 rue de Candolle
1211 Gen`eve 4 Switzerland
merlo@lettres.unige.ch
Gabriele Musillo
Depts of Linguistics and Computer Science
University of Geneva
5 Rue de Candolle
1211 Gen`eve 4 Switzerland
musillo4@etu.unige.ch
Abstract
In this paper, we report experiments that
explore learning of syntactic and seman-
tic representations. First, we extend a
state-of-the-art statistical parser to pro-
duce a richly annotated tree that identi-
fies and labels nodes with semantic role la-
bels as well as syntactic labels. Secondly,
we explore rule-based and learning tech-
niques to extract predicate-argument struc-
tures from this enriched output. The learn-
ing method is competitive with previous
single-system proposals for semantic role
labelling, yields the best reported preci-
sion, and produces a rich output. In com-
bination with other high recall systems it
yields an F-measure of 81%.
1 Introduction
In statistical natural language processing, consid-
erable ingenuity and insight have been devoted to
developing models of syntactic information, such
as statistical parsers and taggers. Successes in
these syntactic tasks have recently paved the way
to applying novel statistical learning techniques
to levels of semantic representation, such as re-
covering the logical form of a sentence for in-
formation extraction and question-answering ap-
plications (Miller et al, 2000; Ge and Mooney,
2005; Zettlemoyer and Collins, 2007; Wong and
Mooney, 2007).
In this paper, we also focus our interest on learn-
ing semantic information. Differently from other
work that has focussed on logical form, however,
we explore the problem of recovering the syn-
tactic structure of the sentence, the propositional
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
argument-structure of its main predicates, and the
substantive labels assigned to the arguments in the
propositional structure, the semantic roles. This
rich output can be useful for information extrac-
tion and question-answering, but also for anaphora
resolution and other tasks for which the structural
information provided by full syntactic parsing is
necessary.
The task of semantic role labelling (SRL), as has
been defined by previous researchers (Gildea and
Jurafsky, 2002), requires collecting all the argu-
ments that together with a verb form a predicate-
argument structure. In most previous work, the
task has been decomposed into the argument iden-
tification and argument labelling subtasks: first the
arguments of each specific verb in the sentence are
identified by classifying constituents in the sen-
tence as arguments or not arguments. The argu-
ments are then labelled in a second step.
We propose to produce the rich syntactic-
semantic output in two steps, which are different
from the argument identification and argument la-
belling subtasks. First, we generate trees that bear
both syntactic and semantic annotation, such as
those in Figure 1. The parse tree, however, does
not explicitly encode information about predicate-
argument structure, because it does not explicitly
associate each semantic role to the verb that gov-
erns it. So, our second step consists in recovering
the predicate-argument structure of each verb by
gleaning this information in an already richly dec-
orated tree.
There are linguistic and computational reasons
to think that we can solve the joint problem of
recovering the constituent structure of a sentence
and its lexical semantics. From a linguistic point
of view, the assumption that syntactic distributions
will be predictive of semantic role assignments is
based on linking theory (Levin, 1986). Linking
theory assumes the existence of a hierarchy of se-
1
mantic roles which are mapped by default on a
hierarchy of grammatical functions and syntactic
positions, and it attempts to predict the mapping
of the underlying semantic component of a predi-
cate?s meaning onto the syntactic structure. For ex-
ample, Agents are always mapped in syntactically
higher positions than Themes. From a computa-
tional point of view, if the internal semantics of a
predicate determines the syntactic expressions of
constituents bearing a semantic role, it is then rea-
sonable to expect that knowledge about semantic
roles in a sentence will be informative of its syn-
tactic structure. It follows rather naturally that se-
mantic and syntactic parsing can be integrated into
a single complex task.
Our proposal also addresses the problem of se-
mantic role labelling from a slightly different per-
spective. We identify and label argument nodes
first, while parsing, and we group them in a
predicate-argument structure in a second step. Our
experiments investigate some of the effects that re-
sult from organising the task of semantic role la-
belling in this way, and the usefulness of some
novel features defined on syntactic trees.
In the remainder of the paper, we first illustrate
the data and the graphical model that formalise the
architecture used and its extension for semantic
parsing. We then report on two kinds of exper-
iments: we first evaluate the architecture on the
joint task of syntactic and semantic parsing and
then evaluate the joint approach on the task of se-
mantic role labelling. We conclude with a discus-
sion which highlights the practical and theoretical
contribution of this work.
2 The Data
Our experiments on joint syntactic and semantic
parsing use data that is produced automatically by
merging the Penn Treebank (PTB) with PropBank
(PRBK) (Marcus et al, 1993; Palmer et al, 2005),
as shown in Figure 1. PropBank encodes proposi-
tional information by adding a layer of argument
structure annotation to the syntactic structures of
the Penn Treebank.
1
Verbal predicates in the Penn
Treebank (PTB) receive a label REL and their ar-
guments are annotated with abstract semantic role
labels, such as A0, A1, or AA for those comple-
ments of the predicative verb that are considered
arguments. Those complements of the verb la-
1
We use PRBK data as they appear in the CONLL 2005
shared task.
S
NP-A0
the authority
VP
VBD-REL
dropped
PP-TMP
IN(TMP)
at
NP
NN
midnight
PP-DIR
TO(DIR)
to
NP
QP
$ 2.80 trillion
Figure 1: A sample syntactic structure with seman-
tic role labels.
belled with a semantic functional label in the orig-
inal PTB receive the composite semantic role label
AM-X , where X stands for labels such as LOC,
TMP or ADV, for locative, temporal and adverbial
modifiers respectively. A tree structure with Prop-
Bank labels is shown in Figure 1. (The bold labels
are not relevant for the moment and they will be
explained later.)
3 The Syntactic and Semantic Parser
Architecture
To achieve the complex task of joint syntactic and
semantic parsing, we extend a current state-of-the-
art statistical parser (Titov and Henderson, 2007)
to learn semantic role annotation as well as syntac-
tic structure. The parser uses a form of left-corner
parsing strategy to map parse trees to sequences of
derivation steps.
We choose this parser because it exhibits the
best performance for a single generative parser,
and does not impose hard independence assump-
tions. It is therefore promising for extensions
to new tasks. Following (Titov and Henderson,
2007), we describe the original parsing architec-
ture and our modifications to it as a Dynamic
Bayesian network. Our description is brief and
limited to the few aspects of interest here. For
more detail, explanations and experiments see
(Titov and Henderson, 2007). A Bayesian network
is a directed acyclic graph that illustrates the statis-
tical dependencies between the random variables
describing a set of events (Jensen, 2001). Dy-
namic networks are Bayesian networks applied to
unboundedly long sequences. They are an appro-
priate model for sequences of derivation steps in
2
dtk
St?1 t
sit
t?cD Dt?1
t?cS S
Dt
Figure 2: The pattern on connectivity and the latent
vectors of variables in an Incremental Bayesian
Network.
parsing (Titov and Henderson, 2007).
Figure 2 illustrates visually the main properties
that are of relevance for us in this parsing architec-
ture. Let T be a parse tree and D
1
, . . . , D
m
be the
sequence of parsing decisions that has led to the
building of this parse tree. Let alo each parsing
decision be composed of smaller parsing decisions
d
1
1
, . . . , d
1
k
, and let al these decisions be indepen-
dent. Then,
P (T ) = P (D
1
, . . . , D
m
)
=
?
t
P (D
t
|D
1
, . . . , D
t?1
)
=
?
t
?
k
P (d
t
k
|h(t, k))
(1)
where h(t, k) denotes the parse history for sub-
decision d
t
k
.
The figure represents a small portion of the ob-
served sequence of decisions that constitute the re-
covery of a parse tree, indicated by the observed
states D
i
. Specifically, it illustrates the pattern of
connectivity for decision d
t
k
. As can be seen the re-
lationship between different probabilistic parsing
decisions are not Markovian, nor do the decisions
influence each other directly. Past decisions can in-
fluence the current decision through state vectors
of independent latent variables, referred to as S
i
.
These state vectors encode the probability distri-
butions of features of the history of parsing steps
(the features are indicated by s
t
i
in Figure 2).
As can be seen from the picture, the pattern
of inter-connectivity allows previous non-adjacent
states to influence future states. Not all states
in the history are relevant, however. The inter-
connectivity is defined dynamically based on the
topological structure and the labels of the tree that
is being developed. This inter-connectivity de-
pends on a notion of structural locality (Hender-
son, 2003; Musillo and Merlo, 2006).
2
2
Specifically, the conditioning states are based on the
In order to extend this model to learn decisions
concerning a joint syntactic-semantic representa-
tion, the semantic information needs to be high-
lighted in the model in several ways. We modify
the network connectivity, and bias the learner.
First, we take advantage of the network?s dy-
namic connectivity to highlight the portion of the
tree that bears semantic information. We augment
the nodes that can influence parsing decisions at
the current state by explicitly adding the vectors
of latent variables related to the most recent child
bearing a semantic role label of either type (REL,
A0 to A5 or AM-X) to the connectivity of the
current decision. These additions yield a model
that is sensitive to regularities in structurally de-
fined sequences of nodes bearing semantic role la-
bels, within and across constituents. These exten-
sions enlarge the locality domain over which de-
pendencies between predicates bearing the REL
label, arguments bearing an A0-A5 label, and ad-
juncts bearing an AM-X role can be specified, and
capture both linear and hierarchical constraints be-
tween predicates, arguments and adjuncts. Enlarg-
ing the locality domain this way ensures for in-
stance that the derivation of the role DIR in Figure
1 is not independent of the derivations of the roles
TMP, REL (the predicate) and A0.
Second, this version of the Bayesian network
tags its sentences internally. Following (Musillo
and Merlo, 2005), we split some part-of-speech
tags into tags marked with semantic role labels.
The semantic role labels attached to a non-terminal
directly projected by a preterminal and belonging
to a few selected categories (DIR, EXT, LOC, MNR,
PRP, CAUS or TMP) are propagated down to the
pre-terminal part-of-speech tag of its head.
3
This
third extension biases the parser to learn the rela-
tionship between lexical items, semantic roles and
the constituents in which they occur. This tech-
nique is illustrated by the bold labels in Figure 1.
We compare this augmented model to a sim-
ple baseline parser, that does not present any of
the task-specific enhancements described above,
stack configuration of the left-corner parser and the derivation
tree built so far. The nodes in the partially built tree and stack
configuration that are selected to determine the relevant states
are the following: top, the node on top of the pushdown stack
before the current derivation move; the left-corner ancestor of
top (that is, the second top-most node on the parser stack);
the leftmost child of top; and the most recent child of top, if
any.
3
Exploratory data analysis indicates that these tags are the
most useful to disambiguate parsing decisions.
3
PTB/PRBK 24
P R F
Baseline 79.6 78.6 79.1
ST 80.5 79.4 79.9
ST+ EC 81.6 80.3 81.0
Table 1: Percentage F-measure (F), recall (R), and
precision (P) of our joint syntactic and semantic
parser on merged development PTB/PRBK data
(section 24). Legend of models: ST=Split Tags;
EC=enhanced connectivity.
other than being able to use the complex syntactic-
semantic labels. Our augmented model has a to-
tal of 613 non-terminals to represent both the PTB
and PropBank labels of constituents, instead of the
33 of the original syntactic parser. The 580 newly
introduced labels consist of a standard PTB label
followed by a set of one or more PropBank seman-
tic role such as PP-AM-TMP or NP-A0-A1. As a
result of lowering the six AM-X semantic role la-
bels, 240 new part-of-speech tags were introduced
to partition the original tag set which consisted
of 45 tags. As already mentioned, argumental la-
bels A0-A5 are specific to a given verb or a given
verb sense, thus their distribution is highly vari-
able. To reduce variability, we add the tag-verb
pairs licensing these argumental labels to the vo-
cabulary of our model. We reach a total of 4970
tag-word pairs. These pairs include, among oth-
ers, all the tag-verb pairs occuring at least 10 times
in the training data. In this very limited form of
lexicalisation, all other words are considered un-
known.
4 Parsing Experiments
Our extended joint syntactic and semantic parser
was trained on sections 2-21 and validated on sec-
tion 24 from the merged PTB/PropBank. To eval-
uate the joint syntactic and semantic parsing task,
we compute the standard Parseval measures of la-
belled recall and precision of constituents, taking
into account not only the original PTB labels, but
also the newly introduced PropBank labels. This
evaluation gives us an indication of how accurately
and exhaustively we can recover this richer set of
syntactic and semantic labels. The results, com-
puted on the development data set from section 24
of the PTB with added PropBank annotation, are
shown in Table 1. As the table indicates, both the
enhancements based on semantic roles yield an im-
provement on the baseline.
This task enables us to compare, albeit indi-
rectly, our integrated method to other methods
where semantic role labels are learnt separately
from syntactic structure. (Musillo and Merlo,
2006) report results of a merging technique where
the output of the semantic role annotation pro-
duced by the best semantic role labellers in the
2005 CONLL shared task is merged with the out-
put of Charniak?s parser. Results range between
between 82.7% and 83.4% F-measure. Our inte-
grated method almost reaches this level of perfor-
mance.
The performance of the parser on the syntactic
labels only (note reported in Table 1) is slightly de-
graded in comparison to the original syntax-only
architecture (Henderson, 2003), which reported
an F-measure of 89.1% since we reach 88.4% F-
measure for the best syntactic-semantic model (last
line of Table 1). This level of performance is still
comparable to other syntactic parsers often used
for extraction of semantic role features (88.2% F-
measure) (Collins, 1999).
These results indicate that the extended parser is
able to recover both syntactic and semantic labels
in a fully connected parse tree. While it is true that
the full fine-grained interpretation of the semantic
label is verb-specific, the PropBank labels (A0,A1,
etc) do respect some general trends. A0 labels are
assigned to the most agentive of the arguments,
while A1 labels tend to be assigned to arguments
bearing a Theme role, and A2, A3, A4 and A5 la-
bels are assigned to indirect object roles, while all
the AM-X labels tend to be assigned to adjuncts.
The fact that the parser learns these labels with-
out explicit annotation of the link between the ar-
guments and the predicate to which they are as-
signed, but based on the smoothed representation
of the derivation of the parse tree and only very
limited lexicalisation, appears to confirm linking
theory, which assumes a correlation between the
syntactic configuration of a sentence and the lexi-
cal semantic labels.
We need to show now that the quality of the
output produced by the joint syntactic and seman-
tic parsing is such that it can be used to perform
other tasks where semantic role information is cru-
cial. The most directly related task is semantic role
labelling (SRL) as defined in the shared task of
CoNLL 2005.
4
5 Extraction of Predicate-Argument
Structures
Although there is reason to think that the good
performance reported in the previous section is
due to implicit learning of the relationship of the
syntactic representation and the semantic role as-
signments, the output produced by the parser does
not explicitly encode the predicate-argument struc-
tures. Collecting these associations is required to
solve the semantic role labelling task as usually de-
fined. We experimented with two methods: a sim-
ple rule-based method and a more complex learn-
ing method.
5.1 The rule-based method
The rule-based extraction method is the natural
second step to solve the complete semantic role
labelling task, after we identify and label seman-
tic roles while parsing. Since in our proposal, we
solve most of the problem in the first step, then we
should be able to collect the predicate-argument
pairs by simple, deterministic rules. The simplic-
ity of the method also provides a useful compari-
son for more complex learning methods, which can
be justified only if they perform better than simple
rule-based predicate-argument extraction.
Our rule-based method automatically compiles
finite-state automatata defining the paths that con-
nect the first node dominating a predicate to its
semantic roles from parse trees enriched with se-
mantic role labels.
4
Such paths can then be used to
traverse parse trees returned by the parsing model
and collect argument structures. More specifically,
a sample of sentences are randomly selected from
the training section of the PTB/PRBK. For each
predicate, then, all the arguments left and right of
the predicate and all the adjuncts left and right
respectively are collected and filtered by simple
global constraints, thereby guaranteeing that only
one type of obligatory argument label (A0 to A5)
is assigned in each proposition.
When evaluated on gold data, this rule-based ex-
traction method reaches 94.9% precision, 96.9%
recall, for an F-measure of 95.9%. These results
provide an upper bound as well as indicating that,
while not perfect, the simple extraction rules reach
a very good level of correctness if the input from
the first step, syntactic and semantic parsing, is
correct. The performance is much lower when
4
It uses VanNoord?s finite-state-toolkit
http://www.let.rug.nl/ vannoord/Fsa/.
parses are not entirely correct, and semantic role
labels are missing, as indicated by the results of
72.9% precision, 66.7% (F-measure 69.7%), ob-
tained when using the best automatic parse tree.
The fact that performance depends on the qual-
ity of the output of the first step, indicates that
the extraction rules are sensitive to errors in the
parse trees, as well as errors in the labelling. This
indicates that a learning method might be more
adapted to recover from these mistakes.
5.2 The SVM learning method
In a different approach to extract predicate argu-
ment structures from the parsing output, the sec-
ond step learns to associate the right verb to each
semantically annotated node (srn) in the tree pro-
duced in the first step. Each individual (verb, srn)
pair in the tree is either a positive example (the srn
is a member of the verb?s argument structure) or a
negative example (the argument either should not
have been labelled as an argument or it is not as-
sociated to the verb). The training examples are
produced by parsing section 2-21 of the merged
PTB/PRBK data with the joint syntactic-semantic
parser and producing the training examples by
comparison with the CONLL 2005 gold proposi-
tions. There are approximately 800?000 training
examples in total. These examples are used by
an SVM classifier (Joachims, 1999).
5
. Once the
predicate-argument structures are built, they are
evaluated with the CONLL 2005 shared task cri-
teria.
5.3 The learning features
The features used for the extraction of the
predicate-argument structure reflect the syntactic
properties that are useful to identify the arguments
of a given verb. We use syntactic and semantic
node label, the path between the verb and the argu-
ment, and the part-of-speech tag of the verb, which
provides useful information about the tense of the
verb. We also use novel features that encode min-
imality conditions and locality constraints (Rizzi,
1990). Minimality is a typical property of natu-
ral languages that is attested in several domains.
In recovering predicate-argument structures, mini-
mality guarantees that the arguments are related to
the closest verb in a predicate domain, which is not
always the verb to which they are connected by the
5
We use a radial basis function kernel, where parameters
c and ? were determined by a grid search on a small subset of
2000 training examples. They are set at c=8 and ? = 0.03125.
5
shortest path. For example, the subject of an em-
bedded clause can be closer to the verb of the main
clause than to the predicate to which it should be
attached. Minimality is encoded as a binary feature
that indicates whether a verbw intervenes between
the verb v and the candidate argument srn. Mini-
mality is defined both in terms of linear precedence
(indicated below as ?) and of dominance within
the same VP group. A VP group is a stack of VPs
covering the same compound verb group, such as
[
V P
should [
V P
have [
V P
[
V
come ]]]]. Formal
definitions are given below:
minimal(v, srn, w) =
df
8
<
:
false if (v ? w ? srn or srn ? w ? v) and
VPG-dominates(v, srn, w)
true otherwise
VPG-dominates(v, srn, w) =
df
8
<
:
true if VP ? path(v, srn) and
VP ? VP-group directly dominating w
false otherwise
In addition to the minimality conditions, which
resolve ambiguity when two predicates compete to
govern an argument, we use locality constraints to
capture distinct local relationships between a verb
and the syntactic position occupied by a candidate
argument. In particular, we distinguish between in-
ternal arguments occupying a position dominated
by a VP node, external arguments occupying a
position dominated by an S node, and extracted
arguments occupying a position dominated by an
SBAR node. To approximate such structural dis-
tinctions, we introduce two binary features indicat-
ing, respectively, whether there is a a node labelled
S or SBAR on the path connecting the verb and the
candidate argument.
6 Results and Discussion
Table 2 illustrates our results on semantic role la-
belling. Notice how much more precise the learn-
ing method is than the rule-based method, when
the minimality constraint is added. The second and
third line indicate that this contribution is mostly
due to the minimality feature. The fifth and sixth
line however illustrate that these features together
are more useful than the widely used feature path.
Recall however, suffers in the learnt method. Over-
all, the learnt method is better than a rule-based
method only if path and either minimality or lo-
cality constraints are added, thus suggesting that
Prec Rec F
Learning all features 87.4 63.6 73.7
Learning all ?min 75.4 66.2 70.5
Learning all ?loc 87.4 63.6 73.6
Rule-based 72.9 66.7 69.7
Learning all ?path 80.6 60.9 69.4
Learning all ?min ?loc 74.3 63.8 68.6
Baseline 57.4 53.9 55.6
Table 2: Results on the development section (24),
rule-based, and learning, (with all features, and
without path, minimality and locality constraints)
compared to a closest verb baseline.
the choice of features is crucial to reach a level
of performance that justifies the added complex-
ity of a learning method. Both methods are much
better than a baseline that attaches each role to
a verb by the shortest path.
6
Notice that both
these approaches are not lexicalised, they apply to
all verbs. Learning experiments where the actual
verbs were used showed a little degradation as well
as a very considerable increase in training times
(precision: 87.0%; recall: 61.0%; F: 71.7%).
7
Some comments are in order to compare prop-
erly our best results ? the learning method with
all features ? to other methods. Most of the best
performing SRL systems are ensemble learners or
rerankers, or they use external sources of infor-
mation such as the PropBank frames files. While
these techniques are effective to improve classifi-
cation accuracy, we might want to compare the sin-
gle systems, thus teasing apart the contribution of
the features and the model from the contribution
of the ensemble technique. Table 3 reports the sin-
gle systems? performance on the test set. These re-
sults seem to indicate that methods like ours, based
on a first step of PropBank parsing, are compara-
ble to other methods when learning regimes are
factored out, contrary to pessimistic conclusions
in previous work (Yi and Palmer, 2005). (Yi and
Palmer, 2005) share the motivation of our work.
They observe that the distributions of semantic la-
6
In case of tie, the following verb is chosen for an A0 label
and the preceding verb is chosen for all the other labels.
7
We should notice that all these models encode the feature
path as syntactic path, because in exploratory data analysis we
found that this feature performed quite a bit better than path
encoded taking into account the semantic roles assigned to the
nodes on the path. Concerning the learning model, we notice
that a simpler, and much faster to train, linear SVM classifier
performs almost as well as the more complex RBF classifier.
It is therefore preferable if speed is important.
6
Model CONLL 23 Comments
P R F
(Surdeanu and Turmo, 2005) 80.3 73.0 76.5 Propbank frames to filter output, boosting
(Liu et al, 2005) 80.5 72.8 76.4 Single system + simple post-processing
(Moschitti et al, 2005) 76.6 75.2 75.9 Specialised kernels for each kind of role
This paper 87.6 65.8 75.1 Single system and model, locality features
(Ozgencil and McCracken, 2005) 74.7 74.2 74.4 Simple system, no external knowledge
(Johansson and Nugues, 2005) 75.5 73.2 74.3 Uses only 3 sections for training
Table 3: Final Semantic Role Labelling results on test section 23 of Propbank as encoded in the CONLL
shared task for those CONLL 2005 participants not using ensemble learning or external resources.
bels could potentially interact with the distribu-
tions of syntactic labels and redefine the bound-
aries of constituents, thus yielding trees that reflect
generalisations over both these sources of infor-
mation. They also attempt to assign SRL while
parsing, by merging only the first two steps of
the standard pipeline architecture, pruning and ar-
gument identification. Their parser outputs a bi-
nary argument-nonargument distinction. The ac-
tual fine-grained labelling is performed, as in other
methods, by an ensemble classifier. Results are
not among the best and Yi and Palmer conclude
that PropBank parsing is too difficult and suffers
from differences between chunk annotation and
tree structure. We think instead that the method is
promising, as shown by the results reported here,
once the different factors that affect performance
are teased apart.
Some qualitative observations on the errors are
useful. On the one hand, as can be noticed in Table
3, our learning method yields the best precision,
but often the worse recall and it has the most ex-
treme difference between these two scores.
8
This
is very likely to be a consequence of the method.
Since the assignment of the semantic role labels
proper is performed during parsing, the number
of nodes that require a semantic role is only 20%
of the total. Therefore the parser develops a bias
against assigning these roles in general, and recall
suffers.
9
On the other hand, precision is very good,
thanks to the rich context in which the roles are as-
signed.
This property of our method suggests that com-
bining our results with those of other existing se-
8
This observation applies also in a comparison to the other
systems that participated in the CONLL shared task.
9
The SVM classifier, on the other hand, exceeds 94% in
accuracy and its F measures are situated around 87?88% de-
pending on the feature sets.
mantic role labellers might be beneficial, since the
errors it performs are quite different. We tested
this hypothesis by combining our outputs, which
are the most precise, with the outputs of the sys-
tem that reported the best recall (Haghighi et al,
2005). The combination, performed on sections
24 and 23, gives priority to our system when it
outputs a non-null label (because of its high pre-
cision) and uses the other system?s label when our
system outputs a null label. This combination pro-
duces a result of 79.0% precision, 80.4% recall,
and 79.7% F-measure for section 24, and 80.5%
precision, 81.4% recall, and 81.0% F-measure for
section 23. We conclude that the combination is in-
deed able to exploit the positive aspects of both ap-
proaches, as the F-measure of the combined result
is better than each individual result. It is also the
best compared to the other systems of the CoNLL
shared task. Comparatively, we find that applying
the same combination technique to the output of
the system by (Haghighi et al, 2005) with the out-
put of the best system in the CoNLL 2005 shared
task (Punyakanok et al, 2005) yields combined
outputs that are not as good as the better of the
two systems (P:76.3%; R:78.6%; F:77.4% for sec-
tion 24; P:78.5%; R:80.0%; F:79.3% for section
23). This result confirms our initial hypothesis,
that combination of systems with different perfor-
mance characteristics yields greater improvement.
Another direct consequence of assigning roles
in a rich context is that in collecting arguments for
a given verb we hardly need to verify global con-
straints. Differently from previous work that had
found that global coherence constraints consider-
ably improved performance (Punyakanok et al,
2005), using global filtering contraints showed no
improvement in our learning model. Thus, these
results confirm the observations that a verb does
7
not assign its semantic roles independently of each
other (Haghighi et al, 2005). Our method too can
be seen as a way of formulating the SRL problem
in a way that is not simply classification of each in-
stance independently. Because identification of ar-
guments and their labelling is done while parsing,
the parsing history, both syntactic and semantic,
is taken into account in identifying and labelling
an argument. Semantic role labelling is integrated
in structured sequence prediction. Further integra-
tion of semantic role labelling in structured prob-
abilistic models related to the one described here
has recently been shown to result in accurate syn-
chronous parsers that derive both syntactic and se-
mantic dependency representations (Henderson et
al., 2008).
7 Conclusion
Overall our experiments indicate that an inte-
grated approach to identification and labelling fol-
lowed by predicate-argument recovery can solve
the problem of semantic role labelling at a level
of performance comparable to other approaches,
as well as yielding a richly decorated syntactic-
semantic parse tree. The high precision of our
method yields very good results in combination
with other high-recall systems. Its shortcomings
indicates that future work lies in improving recall.
Acknowledgments
We thank the Swiss NSF for supporting this research under
grant number 101411-105286/1, James Henderson for shar-
ing the SSN software, and Xavier Carreras for providing the
CoNLL-2005 data. Part of this work was completed while
the second author was visiting MIT/CSAIL, hosted by Prof.
Michael Collins.
References
Collins, Michael John. 1999. Head-driven statistical models
for natural language parsing. Ph.D. thesis, University of
Pennsylvania.
Ge, Ruifang and Raymond J. Mooney. 2005. A statistical
semantic parser that integrates syntax and semantics. In
Procs of CONLL-05, Ann Arbor, Michigan.
Gildea, Daniel and Daniel Jurafsky. 2002. Automatic
labeling of semantic roles. Computational Linguistics,
28(3):245?288.
Haghighi, Aria, Kristina Toutanova, and Christopher Man-
ning. 2005. A joint model for semantic role labeling. In
Procs of CoNLL-2005, pages 173?176, Ann Arbor, Michi-
gan.
Henderson, Jamie. 2003. Inducing history representations
for broad-coverage statistical parsing. In Procs of NAACL-
HLT?03, pages 103?110, Edmonton, Canada.
Henderson, James, Paola Merlo, Gabriele Musillo and Ivan
Titov. 2008. A latent variable model of synchronous pars-
ing for syntactic and semantic dependencies. In Procs of
CoNLL?08 Shared Task, Manchester, UK.
Jensen, Finn V. 2001. Bayesian networks and decision
graphs. Springer Verlag.
Joachims, Thorsten. 1999. Making large-scale svm learning
practical. In Schlkopf, B., C. Burges, and A. Smola, edi-
tors, Advances in Kernel Methods - Support Vector Learn-
ing. MIT Press.
Johansson, Richard and Pierre Nugues. 2005. Sparse
bayesian classification of predicate arguments. In Procs
of CoNLL-2005, pages 177?180, Ann Arbor, Michigan.
Levin, Lori. 1986. Operations on lexical form: unaccusative
rules in Germanic languages. Ph.D. thesis, Massachus-
setts Institute of Technology.
Liu, Ting, Wanxiang Che, Sheng Li, Yuxuan Hu, and Huaijun
Liu. 2005. Semantic role labeling system using maximum
entropy classifier. In Procs of CoNLL-2005, pages 189?
192, Ann Arbor, Michigan.
Marcus, Mitch, Beatrice Santorini, and M.A. Marcinkiewicz.
1993. Building a large annotated corpus of English: the
Penn Treebank. Computational Linguistics, 19:313?330.
Miller, S., H. Fox, L. Ramshaw, and R. Weischedel. 2000. A
novel use of statistical parsing to extract information from
text. In Procs of NAACL 2000.
Moschitti, Alessandro, Ana-Maria Giuglea, Bonaventura
Coppola, and Roberto Basili. 2005. Hierarchical semantic
role labeling. In Procs of CoNLL-2005, pages 201?204,
Ann Arbor, Michigan.
Musillo, Gabriele and Paola Merlo. 2005. Lexical and struc-
tural biases for function parsing. In Procs of IWPT?05,
pages 83?92, Vancouver, British Columbia, October.
Musillo, Gabriele and Paola Merlo. 2006. Accurate semantic
parsing of the proposition bank. In Procs of NAACL?06,
New York, NY.
Ozgencil, Necati Ercan and Nancy McCracken. 2005. Se-
mantic role labeling using libSVM. In Procs of CoNLL-
2005, pages 205?208, Ann Arbor, Michigan, June.
Palmer, Martha, Daniel Gildea, and Paul Kingsbury. 2005.
The Proposition Bank: An annotated corpus of semantic
roles. Computational Linguistics, 31:71?105.
Punyakanok, Vasin, Peter Koomen, Dan Roth, and Wen tau
Yih. 2005. Generalized inference with multiple seman-
tic role labeling systems. In Procs of CoNLL-2005, Ann
Arbor, MI USA.
Rizzi, Luigi. 1990. Relativized minimality. MIT Press, Cam-
bridge, MA.
Surdeanu, Mihai and Jordi Turmo. 2005. Semantic role
labeling using complete syntactic analysis. In Procs of
CoNLL?05, Ann Arbor, Michigan.
Titov, Ivan and James Henderson. 2007. Constituent parsing
with Incremental Sigmoid Belief Networks. In Procs of
ACL?07, pages 632?639, Prague, Czech Republic.
Wong, Yuk Wah and Raymond Mooney. 2007. Learning syn-
chronous grammars for semantic parsing with lambda cal-
culus. In Procs of ACL?07, pages 960?967, Prague, Czech
Republic.
Yi, Szu-ting and Martha Palmer. 2005. The integration of
semantic parsing and semantic role labelling. In Procs of
CoNLL?05, Ann Arbor, Michigan.
Zettlemoyer, Luke and Michael Collins. 2007. Online learn-
ing of relaxed CCG grammars for parsing to logical form.
In Procs of EMNLP-CoNLL?07, pages 678?687.
8
CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 178?182
Manchester, August 2008
A Latent Variable Model of Synchronous Parsing
for Syntactic and Semantic Dependencies
James Henderson
Dept Computer Science
Univ Geneva
james.henderson@
cui.unige.ch
Paola Merlo
Dept Linguistics
Univ Geneva
merlo@
lettres.unige.ch
Gabriele Musillo
Depts Linguistics
and Computer Science
Univ Geneva
musillo@
lettres.unige.ch
Ivan Titov
?
Dept Computer Science
Univ Illinois at U-C
titov@uiuc.edu
Abstract
We propose a solution to the challenge
of the CoNLL 2008 shared task that uses
a generative history-based latent variable
model to predict the most likely derivation
of a synchronous dependency parser for
both syntactic and semantic dependencies.
The submitted model yields 79.1% macro-
average F1 performance, for the joint task,
86.9% syntactic dependencies LAS and
71.0% semantic dependencies F1. A larger
model trained after the deadline achieves
80.5% macro-average F1, 87.6% syntac-
tic dependencies LAS, and 73.1% seman-
tic dependencies F1.
1 Introduction
Successes in syntactic tasks, such as statistical
parsing and tagging, have recently paved the way
to statistical learning techniques for levels of se-
mantic representation, such as recovering the log-
ical form of a sentence for information extraction
and question-answering applications (e.g. (Wong
and Mooney, 2007)) or jointly learning the syntac-
tic structure of the sentence and the propositional
argument-structure of its main predicates (Musillo
and Merlo, 2006; Merlo and Musillo, 2008). In
this vein, the CoNLL 2008 shared task sets the
challenge of learning jointly both syntactic depen-
dencies (extracted from the Penn Treebank (Mar-
cus et al, 1993) ) and semantic dependencies (ex-
tracted both from PropBank (Palmer et al, 2005)
?
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
0
Authors in alphabetical order.
and NomBank (Meyers et al, 2004) under a uni-
fied representation.
We propose a solution that uses a generative
history-based model to predict the most likely
derivation of a synchronous dependency parser for
both syntactic and semantic dependencies. Our
probabilistic model is based on Incremental Sig-
moid Belief Networks (ISBNs), a recently pro-
posed latent variable model for syntactic struc-
ture prediction, which has shown very good be-
haviour for both constituency (Titov and Hender-
son, 2007a) and dependency parsing (Titov and
Henderson, 2007b). The ability of ISBNs to in-
duce their features automatically enables us to ex-
tend this architecture to learning a synchronous
parse of syntax and semantics without modifica-
tion of the main architecture. By solving the
problem with synchronous parsing, a probabilistic
model is learnt which maximises the joint proba-
bility of the syntactic and semantic dependencies
and thereby guarantees that the output structure is
globally coherent, while at the same time building
the two structures separately. This extension of the
ISBN architecture is therefore applicable to other
problems where two independent, but related, lev-
els of representation are being learnt, such as sta-
tistical machine translation.
Currently the largest model we have trained
achieves 80.5% macro-average F1 performance for
the joint task, 87.6% syntactic dependencies LAS,
and 73.1% semantic dependencies F1.
2 The Probability Model
Our probability model is a joint generative model
of syntactic and semantic dependencies. The
two dependency structures are specified as the se-
quence of actions for a synchronous parser, which
requires each dependency structure to be projec-
178
tivised separately.
2.1 Synchronous derivations
The derivations for syntactic dependency trees are
the same as specified in (Titov and Henderson,
2007b), which are based on the shift-reduce style
parser of (Nivre et al, 2006). The derivations use a
stack and an input queue. There are actions for cre-
ating a leftward or rightward arc between the top of
the stack and the front of the queue, for popping a
word from the stack, and for shifting a word from
the queue to the stack. The derivations for seman-
tic dependency graphs use virtually the same set
of actions, but impose fewer constraints on when
they can be applied, due to the fact that a word in
a semantic dependency graph can have more than
one parent. An additional action predicate
s
was
introduced to label a predicate with sense s.
Let T
d
be a syntactic dependency tree with
derivation D
1
d
, ..., D
m
d
d
, and T
s
be a semantic de-
pendency graph with derivation D
1
s
, ..., D
m
s
s
. To
define derivations for the joint structure T
d
, T
s
,
we need to specify how the two derivations are
synchronised, and in particular make the impor-
tant choice of the granularity of the synchronisa-
tion step. Linguistic intuition would perhaps sug-
gest that syntax and semantics are connected at the
clause level ? a big step size ? while a fully in-
tegrated system would synchronise at each pars-
ing decision, thereby providing the most commu-
nication between these two levels. We choose to
synchronise the construction of the two structures
at every word ? an intermediate step size. This
choice is simpler, as it is based on the natural to-
tal order of the input, and it avoids the problems
of the more linguistically motivated choice, where
chunks corresponding to different semantic propo-
sitions would be overlapping.
We divide the two derivations into the chunks
between shifting each word onto the stack,
c
t
d
= D
b
t
d
d
, ..., D
e
t
d
d
and c
t
s
= D
b
t
s
s
, ..., D
e
t
s
s
,
where D
b
t
d
?1
d
= D
b
t
s
?1
s
= shift
t?1
and
D
e
t
d
+1
d
= D
e
t
s
+1
s
= shift
t
. Then the actions of
the synchronous derivations consist of quadruples
C
t
= (c
t
d
, switch, c
t
s
, shift
t
), where switch means
switching from syntactic to semantic mode. This
gives us the following joint probability model,
where n is the number of words in the input.
P (T
d
, T
s
) = P (C
1
, . . . , C
n
)
=
?
t
P (C
t
|C
1
, . . . , C
t?1
)
(1)
The probability of each synchronous derivation
chunk C
t
is the product of four factors, related to
the syntactic level, the semantic level and the two
synchronising steps.
P (C
t
|C
1
, . . . , C
t?1
) =
P (c
t
d
|C
1
, . . . , C
t?1
)?
P (switch|c
t
d
, C
1
, . . . , C
t?1
)?
P (c
t
s
|switch, c
t
d
, C
1
, . . . , C
t?1
)?
P (shift
t
|c
t
d
, c
t
s
, C
1
, . . . , C
t?1
)
(2)
These synchronous derivations C
1
, . . . , C
n
only
require a single input queue, since the shift opera-
tions are synchronised, but they require two sepa-
rate stacks, one for the syntactic derivation and one
for the semantic derivation.
The probability of c
t
d
is decomposed into deriva-
tion action D
i
probabilities, and likewise for c
t
s
.
P (c
t
d
|C
1
, . . . , C
t?1
)
=
?
i
P (D
i
d
|D
b
t
d
d
,. . ., D
i?1
d
, C
1
,. . ., C
t?1
)
(3)
The actions are also sometimes split into a se-
quence of elementary decisions D
i
= d
i
1
, . . . , d
i
n
,
as discussed in (Titov and Henderson, 2007a).
2.2 Projectivisation of dependencies
These derivations can only specify projective
syntactic or semantic dependency graphs. Ex-
ploratory data analysis indicates that many in-
stances of non-projectivity in the complete graph
are due to crossings of the syntactic and seman-
tic graphs. The amount of non-projectivity of the
joint syntactic-semantic graph is approximately
7.5% non-projective arcs, while summing the non-
projectivity within the two separate graphs results
in only roughly 3% non-projective arcs.
Because our synchronous derivations use two
different stacks for the syntactic and semantic de-
pendencies, respectively, we only require each in-
dividual graph to be projective. As with many de-
pendency parsers (Nivre et al, 2006; Titov and
Henderson, 2007b), we handle non-projective (i.e.
crossing) arcs by transforming them into non-
crossing arcs with augmented labels.
1
Because
our syntactic derivations are equivalent to those of
(Nivre et al, 2006), we use their HEAD methods
to projectivise the syntactic dependencies.
Although our semantic derivations use the same
set of actions as the syntactic derivations, they dif-
fer in that the graph of semantic dependencies need
1
During testing, these projectivised structures are then
transformed back to the original format for evaluation.
179
not form a tree. The only constraints we place on
the set of semantic dependencies are imposed by
the use of a stack, which excludes crossing arcs.
Given two crossing arcs, we try to uncross them
by changing an endpoint of one of the arcs. The
arc (p, a), where p is a predicate and a is an argu-
ment, is changed to (p, h), where h is the syntactic
head of argument a. Its label r is then changed to
r/d where d is the syntactic dependency of a on
h. This transformation may need to be repeated
before the arcs become uncrossed. The choice of
which arc to transform is done using a greedy al-
gorithm and a number of heuristics, without doing
any global optimisation across the data.
This projectivisation method is similar to the
HEAD method of (Nivre et al, 2006), but has two
interesting new characteristics. First, syntactic de-
pendencies are used to projectivise the semantic
dependencies. Because the graph of semantic roles
is disconnected, moving across semantic arcs is of-
ten not possible. This would cause a large number
of roles to be moved to ROOT. Second, our method
changes the semantic argument of a given pred-
icate, whereas syntactic dependency projectivisa-
tion changes the head of a given dependent. This
difference is motivated by a predicate-centred view
of semantic dependencies, as it avoids changing a
predicate to a node which is not a predicate.
3 The Learning Architecture
The synchronous derivations described above are
modelled with an Incremental Sigmoid Belief Net-
work (ISBN) (Titov and Henderson, 2007a). IS-
BNs are dynamic Bayesian Networks which incre-
mentally specify their model structure based on the
partial structure being built by a derivation. They
have previously been applied to constituency and
dependency parsing. In both cases the derivations
were based on a push-down automaton, but ISBNs
can be directly applied to any automaton. We suc-
cessfully apply ISBNs to a two-stack automaton,
without changing the machine learning methods.
3.1 The Incremental Sigmoid Belief Networks
ISBNs use vectors of latent variables to represent
properties of parsing history relevant to the next
decisions. Latent variables do not need to be anno-
tated in the training data, but instead get induced
during learning. As illustrated by the vectors S
i
in figure 1, the latent feature vectors are used to
estimate the probabilities of derivation actions D
i
.
s
SS
DD
S
i?c
i?c i?1
i?1
i
ij
Di dki
Figure 1: An ISBN for estimating
P (d
i
k
|history(i, k)) ? one of the elementary
decisions. Variables whose values are given in
history(i, k) are shaded, and latent and current
decision variables are unshaded.
Latent variable vectors are connected to variables
from previous positions via a pattern of edges de-
termined by the previous decisions. Our ISBN
model distinguishes two types of latent states: syn-
tactic states, when syntactic decisions are consid-
ered, and semantic states, when semantic decision
are made. Different patterns of interconnections
are used for different types of states. We use the
neural network approximation (Titov and Hender-
son, 2007a) to perform inference in our model.
As also illustrated in figure 1, the induced latent
variables S
i
at state i are statistically dependent on
both pre-defined features of the derivation history
D
1
, . . . , D
i?1
and the latent variables for a finite
set of relevant previous states S
i
?
, i
?
< i. Choos-
ing this set of relevant previous states is one of the
main design decisions in building an ISBN model.
By connecting to a previous state, we place that
state in the local context of the current decision.
This specification of the domain of locality deter-
mines the inductive bias of learning with ISBNs.
Thus, we need to choose the set of local (i.e. con-
nected) states in accordance with our prior knowl-
edge about which previous decisions are likely to
be particularly relevant to the current decision.
3.2 Layers and features
To choose previous relevant decisions, we make
use of the partial syntactic and semantic depen-
dency structures which have been decided so far
in the parse. Specifically, the current latent state
vector is connected to the most recent previous la-
tent state vectors (if they exist) whose configura-
tion shares a node with the current configuration,
as specified in Table 1. The nodes are chosen be-
cause their properties are thought to be relevant to
the current decision. Each row of the table indi-
cates which nodes need to be identical, while each
180
Closest Current Syn-Syn Srl-Srl Syn-Srl
Input Input + + +
Top Top + + +
RDT Top + +
LDT Top + +
HT Top + +
LDN Top + +
Input Top +
Table 1: Latent-to-latent variable connections. In-
put= input queue; Top= top of stack; RDT= right-
most right dependent of top; LDT= leftmost left
dependent of top; HT= Head of top; LDN= left-
most dependent of next (front of input).
column indicates whether the latent state vectors
are for the syntactic or semantic derivations. For
example, the first row indicates edges between the
current state and a state which had the same in-
put as the current state. The three columns indi-
cate that this edge holds within syntactic states,
within semantic states, and from syntactic to se-
mantic states. The fourth cell of the third row, for
example, indicates that there is an edge between
the current semantic state on top of the stack and
the most recent semantic state where the rightmost
dependent of the current top of the semantic stack
was at the top of the semantic stack.
Each of these relations has a distinct weight ma-
trix for the resulting edges in the ISBN, but the
same weight matrix is used at each position where
the relation applies. Training and testing times
scale linearly with the number of relations.
The pre-defined features of the parse history
which also influence the current decision are spec-
ified in table 2. The model distinguishes argument
roles of nominal predicates from argument roles of
verbal predicates.
3.3 Decoding
Given a trained ISBN as our probability esti-
mator, we search for the most probable joint
syntactic-semantic dependency structure using a
beam search. Most pruning is done just after each
shift operation (when the next word is predicted).
Global constraints (such as label uniqueness) are
not enforced by decoding, but can be learnt.
For the system whose results we submitted, we
then do a second step to improve on the choice
of syntactic dependency structure. Because of the
lack of edges in the graphical model from seman-
tic to syntactic states, it is easy to marginalise out
the semantic structure, giving us the most proba-
ble syntactic dependency structure. This syntactic
structure is then combined with the semantic struc-
State Stack Syntactic step features
LEX POS DEP
Input + +
Top syn + +
Top - 1 syn +
HT syn +
RDT syn +
LDT syn +
LDN syn +
State Stack Semantic step features
LEX POS DEP SENSE
Input + + +
Top sem + + +
Top - 1 sem + +
HT sem + +
RDT sem +
LDT sem +
LDN sem +
A0-A5 of Top sem +
A0-A5 of Input sem +
Table 2: Pre-defined features. syn=syntactic stack;
sem=semantic stack. Input= input queue; Top=
top of stack; RDT= rightmost dependent of top;
LDT= leftmost dependent of Top; HT= Head of
top; LDN= leftmost dependent of next (front of
input); A0-A5 of Top/Input= arguments of top of
stack / input.
ture from the first stage, to get our submitted re-
sults. This second stage does not maximise perfor-
mance on the joint syntactic-semantic dependency
structure, but it better fits the evaluation measure
used to rank systems.
4 Experiments and Discussion
The experimental set-up common for all the teams
is described in the introduction (Surdeanu et al,
2008). The submitted model has latent variable
vectors of 60 units, and a word frequency cut-off
of 100, resulting in a small vocabulary of 1083
words. We used a beam of size 15 to prune deriva-
tions after each shift operation to obtain the joint
structure, and a beam of size 40 when perform-
ing the marginalisation. Training took approxi-
mately 2.5 days on a standard PC with 3.0 GHz
Pentium4 CPU. It took approximately 2 hours to
parse the entire testing set (2,824 sentences) and
an additional 3 hours to perform syntactic parsing
when marginalising out the semantic structures.
2
Shortly after the submission deadline, we trained a
?large? model with a latent variable vector of size
80, a word frequency cut-off of 20, and additional
latent-to-latent connections from semantics to syn-
tax of the same configuration as the last column
2
A multifold speed-up with a small decrease in accuracy
can be achieved by using a small beam.
181
Syn Semantic Overall
LAS P R F1 P R F1
Submitted
D 86.1 78.8 64.7 71.1 82.5 75.4 78.8
W 87.8 79.6 66.2 72.3 83.7 77.0 80.2
B 80.0 66.6 55.3 60.4 73.3 67.6 70.3
WB 86.9 78.2 65.0 71.0 82.5 76.0 79.1
Joint inference
D 85.5 78.8 64.7 71.1 82.2 75.1 78.5
Large, joint inference
D 86.5 79.9 67.5 73.2 83.2 77.0 80.0
W 88.5 80.4 69.2 74.4 84.4 78.8 81.5
B 81.0 68.3 57.7 62.6 74.7 69.4 71.9
WB 87.6 79.1 67.9 73.1 83.4 77.8 80.5
Table 3: Scores on the development set and the
final testing sets (percentages). D= development
set; W=WSJ; B=Brown; WB=WSJ+Brown;
of table 1. This model took about 50% longer in
training and testing.
In table 3, we report results for the marginalised
inference (?submitted?) and joint inference for the
submitted model, and the results for joint inference
with the ?large? model. The larger model improves
on the submitted results by almost 1.5%, a signifi-
cant improvement. If completed earlier, this model
would have been fifth overall, second for syntactic
LAS, and fifth for semantic F1.
To explore the relationship between the two
components of the model, we removed the edges
between the syntax and the semantics in the sub-
mitted model. This model?s performance drops by
about 3.5% for semantic role labelling, thereby in-
dicating that the latent annotation of parsing states
helps semantic role labelling. However, it also
indicates that there is much room for improve-
ment in developing useful semantic-specific fea-
tures, which was not done for these experiments
simply due to constraints on development time.
To test whether joint learning degrades the ac-
curacy of the syntactic parsing model, we trained a
syntactic parsing model with the same features and
the same pattern of interconnections as used for the
syntactic states in our joint model. The resulting
labelled attachment score was non-significantly
lower (0.2%) than the score for the marginalised
inference with the joint model. This result sug-
gests that, though the latent variables associated
with syntactic states in the joint model were trained
to be useful in semantic role labelling, this did not
have a negative effect on syntactic parsing accu-
racy, and may even have helped.
Finally, an analysis of the errors on the develop-
ment set for the submitted model paints a coherent
picture. We find attachment of adjuncts particu-
larly hard. For dependency labels, we make the
most mistakes on modification labels, while for se-
mantic labels, we find TMP, ADV, LOC, and PRN
particularly hard. NomBank arcs are not learnt as
well as PropBank arcs: we identify PropBank SRL
arguments at F1 70.8% while Nombank arguments
reach 58.1%, and predicates at accuracy 87.9% for
PropBank and 74.9% for NomBank.
5 Conclusions
While still preliminary, these results indicate that
synchronous parsing is an effective way of build-
ing joint models on separate structures. The gen-
erality of the ISBN design used so far suggests
that ISBN?s latent feature induction extends well to
estimating very complex probability models, with
little need for feature engineering. Nonetheless,
performance could be improved by task-specific
features, which we plan for future work.
Acknowledgements
This work was partly funded by European Community FP7
grant 216594 (CLASSiC, www.classic-project.org), Swiss
NSF grant 114044, and Swiss NSF fellowships PBGE2-
117146 and PBGE22-119276. Part of this work was done
when G. Musillo was visiting MIT/CSAIL, hosted by Prof.
Michael Collins.
References
Marcus, M., B. Santorini, and M.A. Marcinkiewicz. 1993.
Building a large annotated corpus of English: the Penn
Treebank. Computational Linguistics, 19:313?330.
Merlo, P. and G. Musillo. 2008. Semantic parsing for high-
precision semantic role labelling. In Procs of CoNLL
2008, Manchester, UK.
Meyers, A., R. Reeves, C. Macleod, R. Szekely, V. Zielin-
ska, B. Young, and R. Grishman. 2004. The nombank
project: An interim report. In Meyers, A., editor, HLT-
NAACL 2004 Workshop: Frontiers in Corpus Annotation,
24?31, Boston, MA.
Musillo, G. and P. Merlo. 2006. Accurate semantic parsing
of the Proposition Bank. In Procs of NAACL 2006, New
York, NY.
Nivre, J., J. Hall, J. Nilsson, G. Eryigit, and S. Marinov. 2006.
Pseudo-projective dependency parsing with support vector
machines. In Proc. of CoNNL, 221?225, New York, USA.
Palmer, M., D. Gildea, and P. Kingsbury. 2005. The Propo-
sition Bank: An annotated corpus of semantic roles. Com-
putational Linguistics, 31:71?105.
Surdeanu, M., R. Johansson, A. Meyers, L. M`arquez, and J.
Nivre. 2008. The CoNLL-2008 shared task on joint pars-
ing of syntactic and semantic dependencies. In Procs of
CoNLL-2008, Manchester,UK.
Titov, I. and J. Henderson. 2007a. Constituent parsing with
incremental sigmoid belief networks. In Procs of ACL?07,
pages 632?639, Prague, Czech Republic.
Titov, I. and J. Henderson. 2007b. A latent variable model
for generative dependency parsing. In Procs of IWPT?07,
Prague, Czech Republic.
Wong, Y.W. and R. Mooney. 2007. Learning synchronous
grammars for semantic parsing with lambda calculus. In
Procs of ACL?07, 960?967, Prague, Czech Republic.
182
Proceedings of the Fourth Linguistic Annotation Workshop, ACL 2010, pages 113?117,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Cross-lingual Validity of PropBank in the Manual Annotation of French
Lonneke van der Plas Tanja Samardz?ic?
Linguistics Department
University of Geneva
Rue de Candolle 5, 1204 Geneva
Switzerland
{Lonneke.vanderPlas,Tanja.Samardzic,Paola.Merlo}@unige.ch
Paola Merlo
Abstract
Methods that re-use existing mono-lingual
semantic annotation resources to annotate
a new language rely on the hypothesis that
the semantic annotation scheme used is
cross-lingually valid. We test this hypoth-
esis in an annotation agreement study. We
show that the annotation scheme can be
applied cross-lingually.
1 Introduction
It is hardly a controversial statement that elegant
language subtleties and powerful linguistic im-
agery found in literary writing are lost in trans-
lation. Yet, translation preserves enough meaning
across language pairs to be useful in many appli-
cations and for many text genres.
The belief that this layer of meaning which is
preserved across languages can be formally rep-
resented and automatically calculated underlies
methods that use parallel corpora for the automatic
generation of semantic annotations through cross-
lingual transfer (Pado?, 2007; Basili et al, 2009).
A methodology similar in spirit ? re-use of the
existing resources in a different language ? has
also been applied in developing manually anno-
tated resources. Monachesi et al (2007) annotate
Dutch sentences using the PropBank annotation
scheme (Palmer et al, 2005), while Burchardt et
al. (2009) use the FrameNet framework (Fillmore
et al, 2003) to annotate a German corpus. In-
stead of building special lexicons containing the
specific semantic information needed for the an-
notation for each language separately, which is a
complex and time-consuming endeavour in itself,
these approaches rely on the lexicons already de-
veloped for English.
In this paper, we hypothesize that the level
of abstraction that is necessary to develop a se-
mantic lexicon/ontology for a single language
based on observable linguistic behaviour ? that
is a mono-lingual, item-specific annotation ? is
cross-linguistically valid. We test this hypothe-
sis by manually annotating French sentences using
the PropBank frame files developed for English.
It has been claimed that semantic parallelism
across languages is smaller when using the
PropBank semantic annotations instead of the
FrameNet scheme, because FrameNet is more ab-
stract and less verb-specific (Pado?, 2007). We are
working with the PropBank annotation scheme,
contrary to other works that use the FrameNet
scheme, such as Pado? (2007) and Basili et al
(2009). We choose this annotation for two main
reasons. First, the primary use of our annotation is
to serve as a gold standard in the task of syntactic-
semantic parsing. FrameNet does not have a prop-
erly sampled hand-annotated corpus of English,
by design. So we cannot use it for this task. Sec-
ond, in Merlo and Van der Plas (2009), the seman-
tic annotations schemes of PropBank and VerbNet
(Kipper, 2005) are compared, based on annotation
of the SemLink project (Loper et al, 2007). The
authors conclude that PropBank is the preferred
annotation for a joint syntactic-semantic setting.
If the PropBank annotation scheme is cross-
lingually valid, annotators can reach a consensus
and can do so swiftly. Thus, cross-lingual valid-
ity is measured by how well-defined the manual
annotation task is (inter-annotator agreement) and
by how hard it is to reach an agreement (pre- and
post-consensus inter-annotator agreement). In ad-
dition, we measure the impact of the level of ab-
straction of the predicate labels. Conversely, how
often labels do not transfer and distributions of dis-
agreements are indicators of lack of parallelism
across languages that we study both by quantita-
tive and qualitative analysis.
To preview the results, we find that the Prop-
Bank annotation scheme developed for English
can be applied for a large portion of French sen-
113
tences without adjustments, which confirms its
cross-lingual validity. A high level of inter-
annotator agreement is reached when the verb-
specific PropBank labels are replaced by less fine-
grained verb classes after annotating. Non-parallel
cases are mostly due to idioms and collocations.
2 Materials and Methods
Our choices of formal representation and of la-
belling scheme are driven by the goal of produc-
ing useful annotations for syntactic-semantic pars-
ing in a setting based on an aligned corpus. In the
following subsections we describe the annotation
scheme and procedure, the corpus, and phases of
annotation.
2.1 The PropBank Annotation Framework
We use the PropBank scheme for the manual anno-
tations. PropBank is a linguistic resource that con-
tains information on the semantic structure of sen-
tences. It consists of a one-million-word corpus
of naturally occurring sentences annotated with
semantic structures and a lexicon (the PropBank
frame files) that lists all the predicates (verbs) that
can be found in the annotated sentences and the
sets of semantic roles they introduce.
Predicates are marked with labels that specify
the sense of the verb in the particular sentence. Ar-
guments are marked with the labels A0 to A5. The
labels A0 and A1 have approximately the same
value with all verbs. They are used to mark in-
stances of typical AGENTS (A0) and PATIENTS
(A1). The value of other numbers varies across
verbs. Modifiers are annotated in PropBank with
the label AM. This label can have different exten-
sions depending on the semantic type of the con-
stituent, for example locatives and adverbials.
2.2 Annotation Procedure
Annotators have access to PropBank frame files
and guidelines adapted for the current task. The
frame files provide verb-specific descriptions of all
possible semantic roles and illustrate these roles
with examples as shown for the verb paid in (1)
and the verb senses of pay in Table 1. Annotators
need to look up each verb in the frame files to be
able to label it with the right verb sense and to be
able to allocate the arguments consistently.
(1) [A0 The Latin American nation] has
[REL?PAY.01 paid] [A1 very little] [A3 on its
debt] [AM?TMP since early last year].
Frame Semantic roles
pay.01 A0: payer or buyer
A1: money or attention
A2: person being paid, destination of attention
A3: commodity, paid for what
pay.02 A0: payer
pay off A1: debt
A2: owed to whom, person paid
pay.03 A0: payer or buyer
pay out A1: money or attention
A2: person being paid, destination of attention
A3: commodity, paid for what
pay.04 A1: thing succeeding or working out
pay.05 A1: thing succeeding or working out
pay off
pay.06 A0: payer
pay down A1: debt
Table 1: The PropBank lexicon entry for pay.
In our cross-lingual setting, annotators used
the English PropBank frame files to annotate the
French sentences. This means that for every pred-
icate they find in the French sentence, they need
to translate it, and find an English verb sense that
is applicable to the French verb. If an appropri-
ate entry cannot be found in the frame files for a
given predicate, the annotator is instructed to use
the ?dummy? label for the predicate and fill in the
roles according to their own insights.
For the annotation of sentences we use an adap-
tation of the user-friendly, freely available Tree
Editor (TrEd, Pajas and S?te?pa?nek, 2008). The tool
shows the syntactic analysis and the plain sentence
in the same window allowing the user to add se-
mantic arcs and labels to the nodes in the syntactic
dependency tree.
The decision to show syntactic information is
merely driven by the fact that we want to guide the
annotator in selecting the heads of phrases during
the annotation process. The sentences are parsed
by a syntactic parser (Titov and Henderson, 2007)
that we trained on syntactic dependency annota-
tions for French (Candito et al, 2009). Although
the parser is state-of-the-art (87.2% Labelled At-
tachment Score), in case of parse errors, we ask
annotators to ignore the errors of the parser and
put the label on the actual head.
2.3 Corpus
We selected the French sentences for the man-
ual annotation from the parallel Europarl corpus
(Koehn, 2005). Because translation shifts are
known to pose problems for the automatic cross-
lingual transfer of semantic roles (Pado?, 2007)
and for machine translation (Ozdowska and Way,
114
2009), and these are more likely to appear in in-
direct translations, we decided to select only those
parallel sentences, for which we can infer from the
labels used in Europarl that they are direct trans-
lations from English to French, or vice versa. We
selected 1040 sentences for annotation (40 in to-
tal for the two training phases, 100 for calibration,
and 900 for the main annotation phase.)1
2.4 Annotation Phases
The training procedure described in Figure 1
is inspired by the methodology indicated in
Pado? (2007). A set of 130 sentences were anno-
tated manually by four annotators with very good
proficiency in both French and English for the
training and the calibration phase. The remaining
900 sentences are annotated by one annotator (out
of those four), a trained linguist. Inter-annotator
agreement was measured at several points in the
annotation process marked with an arrow in Fig-
ure 1. The guidelines were adjusted after the train-
ing phase.
? Training phase
-TrainingA: 10 sentences, all annotators together
-TrainingB: 30 sentences, all annotators individually?
-Reach consensus on Training B?
? Calibration phase
-100 sentences by main annotator, one third of those by
each of the other 3 annotators?
? Main annotation phase
-900 sentences by main annotator
Figure 1: The annotation phases.
3 Results
Cross-lingual validity is measured by comparing
inter-annotator agreement at several stages in the
annotation, by measuring the agreement on less
specific predicate labelling, and by a quantitative
and qualitative analysis of non-parallel cases.
3.1 Inter-annotator Agreement for Several
Annotation Phases
To assess the quality of the manual annotations we
measured the agreement between annotators as the
average F-measure of all pairs of annotators after
each phase of the annotation procedure.2 The first
1As usual practice in preprocessing for automatic align-
ment, the datasets were tokenised and lowercased and only
sentence pairs corresponding to a 1-to-1 alignment with
lengths ranging from 1 to 40 tokens on both French and En-
glish sides were considered.
2It is a known fact that measuring annotator agreement us-
ing the kappa score is problematic in categorisation tasks that
Predicates Arguments
Lab. F Unl. F Lab. F Unl. F
TrainingB 46 85 62 75
TrainingB(cons.) 95 97 91 95
Calibration 59 93 69 84
Table 2: Percent inter-annotator agreement (F-
measure) for labelled/unlabelled predicates and
for labelled/unlabelled arguments
row of Table 2 shows that the task is hard. But
the difference between the first row and the sec-
ond row shows that there were many differences
between annotators that could be resolved. After
discussions and individual corrections the scores
are between 91% and 95%. This indicates that
the task is well-defined. Row three shows that the
agreement in the calibration phase increases a lot
compared to the last training phase (row 1). This
might in part be due to the fact that the guidelines
were adjusted by the end of the training phase, but
could also be because the annotators are getting
more acquainted to the task and the software.
As expected, because annotators used the En-
glish PropBank frame files to annotate French
verbs, the task of labelling predicates proved more
difficult than labelling semantic roles. It results in
the lowest agreement scores overall. In the follow-
ing subsections we study the sources of disagree-
ment in predicate labelling in more detail.
3.2 Inter-annotator Agreement in Predicate
Labellings
Predicate labels in PropBank apply to particular
verb senses, for example walk.01 for the first sense
of the verb walk. Even though the senses are
coarser than, for example, the senses in Word-
Net (Fellbaum, 1998), the labels are rather spe-
cific. This specificity possibly poses problems
when working in a cross-lingual setting.
We compare the agreement reached using Prop-
Bank verb sense labels with the agreement reached
using the verb classifications from VerbNet (Kip-
per, 2005) and the mapping to PropBank labels
as provided in the type mappings of the SemLink
project3 (Loper et al, 2007). If two annotators
used two different predicate labels to annotate the
do not have a fixed number of items and categories (Burchardt
et al, 2006). The F-measure is a well-known measure used
for the evaluation of many task such as syntactic-semantic
parsing, the task that is the motivation for this paper. The
choice of the F-measure makes the comparison to the perfor-
mance of the future parser easier.
3(http://verbs.colorado.edu/semlink/)
115
same verb, but those verb senses belong to the
same verb class, we count those as correct4.
The average inter-annotator agreement is rela-
tively low when we compare the annotations on
the PropBank verb sense level: 59%. However, at
the level of verb classes, the inter-annotator agree-
ment increases to 81%. This raises the issue of
whether we should not label the predicates with
verb classes instead of verb senses. By using Prop-
Bank labels for the manual annotation and replac-
ing these with verb classes in post-processing, the
benefits are two-fold: We are able to reach a high
level of cross-lingual parallelism on the annota-
tions, while keeping the manual annotation task as
specific and less abstract as possible.
3.3 Analysis of Non-Parallel Cases
For a single annotator, the main measure of cross-
lingual validity is the percentage of dummy pred-
icates in the annotation. In the sentences from the
calibration and the main annotation phase from the
main annotator (1000 sentences in total), we find
130 predicates (tokens) for which the annotator
used the ?dummy? label.
Manual inspection reveals that the ?dummy? la-
bel is mainly used for French multi-word expres-
sions (82%), most of which can be translated by
a single English verb (47%), whereas others can-
not, because they are translated by a combination
that includes a form of ?be? that is not annotated
in PropBank (25%). The 47% of multi-word ex-
pressions that receive the ?dummy? label show the
annotator?s reluctance to put a single verb label on
a French multi-word expression. The annotation
guidelines could be adapted to instruct annotators
not to hesitate in such cases.
Similarly, collocations and idiomatic expres-
sions are the main sources of disagreement in
predicate labellings among annotators. We can
conclude that, as shown in studies on other lan-
guage pairs (Burchardt et al, 2009), collocations
and idiomatic expressions were identified as verb
uses where the verb?s predicate label cannot be
transferred directly from one language to another.
4 Discussion and Related Work
Burchardt et al (2009) use English FrameNet to
4The mappings from PropBank verb sense labels to Verb-
Net verb classes are one-to-many and not complete. We
counted a pair as matching if there exists a class to which
both verb senses belong. We found a verb class for both verb
senses in about 78% of the cases and discarded the rest.
annotate a corpus of German sentences manually.
They find that the vast majority of frames can be
applied to German directly. However, around one
third of the verb senses identified in the German
corpus were not covered by FrameNet. Also, a
number of German verbs were found to be under-
specified. Finally, some problems related to treat-
ing particular verb uses were identified, such as id-
ioms, metaphors, and support verb constructions.
Monachesi et al (2007) use PropBank labels for
semi-automatic annotation of a corpus of Dutch
sentences. Semantic roles were first annotated
using a rule-based semantic parser and then cor-
rected by one annotator. Although not all Dutch
verbs could be translated to an equivalent verb
sense in English, these cases were assessed as rel-
atively rare. What proved to be problematic was
identifying the correct label for modifiers.
Bittar (2009) makes use of cross-lingual lexi-
cal transfer in annotating French verbs with event
types, by adapting a small-scale English verb lex-
icon with specified event structure (TimeML).
The inter-annotator agreement in labelling pred-
icates reported in Burchardt et al (2009) reaches
85%, while our best score (when falling back to
verb classes) is 81%. However, unlike Burchardt
et al (2009) we did not introduce any new French
labels. We find, like Monachesi et al (2007), that
non-parallel cases are less frequent than what is re-
ported in Burchardt et al (2009), which could be
due to the properties of the annotations schemes.
5 Conclusions
We can conclude that the general task of anno-
tating French sentences using English PropBank
frame files is well-defined. Nevertheless, it is a
hard task that requires linguistic training. With re-
spect to the disagreements on labelling predicates,
we can conclude that a large part can be resolved
if we compare the annotations at the level of verb
classes instead of at the very fine-grained level of
verb senses. Non-parallel cases are mostly due to
idioms and collocations. Their rate is relatively
low and can be further reduced by adapting anno-
tation guidelines.
Acknowledgments
The research leading to these results has received fund-
ing from the EU FP7 programme (FP7/2007-2013) under
grant agreement nr 216594 (CLASSIC project: www.classic-
project.org). We would like to thank Goljihan Kashaeva and
James Henderson for valuable comments.
116
References
R. Basili, D. De Cao, D. Croce, B. Coppola, and A. Moschitti,
2009. Computational Linguistics and Intelligent Text Pro-
cessing, chapter Cross-Language Frame Semantics Trans-
fer in Bilingual Corpora, pages 332?345. Springer Berlin
/ Heidelberg.
A. Bittar. 2009. Annotation of events and temporal expres-
sions in French texts. In Proceedings of the third Linguis-
tic Annotation Workshop (LAW III), pages 48?51, Suntec,
Singapore.
A. Burchardt, K. Erk, A. Frank, A. Kowalski, S. Pado?, and
M. Pinkal. 2006. The SALSA corpus: a German cor-
pus resource for lexical semantics. In Proceedings of the
5th International Conference on Language Resources and
Evaluation (LREC 2006), pages 969?974, Genoa, Italy.
A. Burchardt, K. Erk, A. Frank, A. Kowalski, S. Pado, and
M. Pinkal, 2009. Multilingual FrameNets in Computa-
tional Lexicography: Methods and Applications, chapter
FrameNet for the semantic analysis of German: Annota-
tion, representation and automation, pages 209?244. De
Gruyter Mouton, Berlin.
M.-H. Candito, B. Crabbe?, P. Denis, and F. Gue?rin.
2009. Analyse syntaxique du franc?ais : des constitu-
ants aux de?pendances. In Proceedings of la Confe?rence
sur le Traitement Automatique des Langues Naturelles
(TALN?09), Senlis, France.
C. Fellbaum. 1998. WordNet, an electronic lexical database.
MIT Press.
C. J. Fillmore, R. Johnson, and M.R.L. Petruck. 2003. Back-
ground to FrameNet. International journal of lexicogra-
phy, 16.3:235?250.
K. Kipper. 2005. VerbNet: A broad-coverage, comprehen-
sive verb lexicon. Ph.D. thesis, University of Pennsylvnia.
P. Koehn. 2005. Europarl: A parallel corpus for statistical
machine translation. In Proceedings of the MT Summit,
pages 79?86, Phuket, Thailand.
E. Loper, S-T Yi, and M. Palmer. 2007. Combining lexical
resources: Mapping between PropBank and VerbNet. In
Proceedings of the 7th International Workshop on Com-
putational Semantics (IWCS-7), pages 118?129, Tilburg,
The Netherlands.
P. Merlo and L. van der Plas. 2009. Abstraction and gen-
eralisation in semantic role labels: PropBank, VerbNet
or both? In Proceedings of the Joint Conference of the
47th Annual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing of the
AFNLP, pages 288?296, Suntec, Singapore.
P. Monachesi, G. Stevens, and J. Trapman. 2007. Adding
semantic role annotation to a corpus of written Dutch.
In Proceedings of the Linguistic Annotation Workshop
(LAW), pages 77?84, Prague, Czech republic.
S. Ozdowska and A. Way. 2009. Optimal bilingual data for
French-English PB-SMT. In Proceedings of the 13th An-
nual Conference of the European Association for Machine
Translation (EAMT?09), pages 96?103, Barcelona, Spain.
S. Pado?. 2007. Cross-lingual Annotation Projection Mod-
els for Role-Semantic Information. Ph.D. thesis, Saarland
University.
P. Pajas and J. S?te?pa?nek. 2008. Recent advances in a feature-
rich framework for treebank annotation. In Proceedings of
the 22nd International Conference on Computational Lin-
guistics (Coling 2008), pages 673?680, Manchester, UK.
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The Proposi-
tion Bank: An annotated corpus of semantic roles. Com-
putational Linguistics, 31:71?105.
I. Titov and J. Henderson. 2007. A latent variable model
for generative dependency parsing. In Proceedings of the
International Conference on Parsing Technologies (IWPT-
07), pages 144?155, Prague, Czech Republic.
117
Proceedings of the 2010 Workshop on NLP and Linguistics: Finding the Common Ground, ACL 2010, pages 52?60,
Uppsala, Sweden, 16 July 2010. c?2010 Association for Computational Linguistics
Cross-lingual variation of light verb constructions: using parallel corpora
and automatic alignment for linguistic research
Tanja Samardz?ic?
Linguistics Department
University of Geneva
Tanja.Samardzic@unige.ch
Paola Merlo
Linguistics Department
University of Geneva
Paola.Merlo@unige.ch
Abstract
Cross-lingual parallelism and small-scale
language variation have recently become
subject of research in both computational
and theoretical linguistics. In this arti-
cle, we use a parallel corpus and an auto-
matic aligner to study English light verb
constructions and their German transla-
tions. We show that parallel corpus data
can provide new empirical evidence for
better understanding the properties of light
verbs. We also study the influence that the
identified properties of light verb construc-
tions have on the quality of their automatic
alignment in a parallel corpus. We show
that, even though characterised by limited
compositionality, these constructions can
be aligned better than fully compositional
phrases, due to an interaction between the
type of light verb construction and its fre-
quency.
1 Introduction
Fine-grained contrastive studies traditionally be-
long to the field of applied linguistics, notably to
translation and second language acquisition stud-
ies. Recently, however, interest for contrastive
studies has been renewed due to developments
in the general theory of language (the notion of
micro-parameters (Kayne, 2000)) on the one hand,
and due to advances in natural language process-
ing based on the exploitation of parallel corpora,
on the other hand.
Parallel corpora are collections of translations
with explicit alignment of sentences. They are im-
portant resources for the automatic acquisition of
the cross-linguistic translation equivalents that are
needed for machine translation. There is also in-
terest in using parallel corpora to automatically de-
velop new annotated linguistic resources by pro-
jecting the annotation that already exists in one
language (usually English) (Pado?, 2007; Basili et
al., 2009). Such resources can be used for train-
ing systems for automatic parsing for different lan-
guages. Recently, parallel multilingual corpora
have also been used to improve performance in
mono-lingual tasks (Snyder et al, 2009).
For most of these applications, the aligned sen-
tences in the parallel corpora need to be analysed
into smaller units (phrases and words), which, in
turn, need to be aligned. Although crucial for suc-
cessful use of parallel corpora, word (and phrase)
alignment is still a challenging task (Och and Ney,
2003; Collins et al, 2005; Pado?, 2007).
Our research concentrates on one type of con-
struction that needs a special treatment in the task
of aligning corpora and projecting linguistic an-
notation from one language to another, namely
light verb constructions. These constructions, usu-
ally identified as paraphrases of verbs (e.g. have
a laugh means laugh, give a talk means talk),
are frequent, cross-lingually productive forms,
where simple-minded parallelism often breaks
down. Their meaning is partially uncomposi-
tional, formed in a conventional way, which means
that they cannot be analysed as regular construc-
tions and that they cannot be translated to another
language directly word by word. Unlike colloca-
tions and idioms, however, these constructions are
formed according to the same ?semi-productive?
pattern in different languages. Due to their cross-
lingual analysability, they can be expected to be
aligned at the word level in a parallel corpus, even
if their components are not direct word-to-word
translations of each other. This means that word
alignment of these constructions, needed for au-
tomatic translation and transferring annotations, is
possible, but it is not straight-forward.
An in-depth study of these constructions in the
specific context of parallel corpora and alignment
can cast new light on the correlation of their lin-
guistic and statistical properties. On the one hand,
52
the statistical large-scale analysis of the behaviour
of these constructions as the output of an align-
ment process provides novel linguistic informa-
tion, which enlarges the empirical base for the
analysis of these constructions, and complements
the traditional grammaticality judgements. On the
other hand, the linguistically fine-grained analysis
of the statistical behaviour of these constructions
provides linguistically-informed performance and
error analyses that can be used to improve align-
ers.
2 Two Types of Light Verb Constructions
and their Alignment
Light verb constructions have already been iden-
tified as one of the major sources of problems
in transferring semantic annotation between lan-
guages as close as English and German (Burchardt
et al, 2009). Light verb constructions introduce
two kinds of divergences that can pose a problem
for automatic word alignment. In the case of true
light verb constructions (Kearns, 2002), English
phrases such as have a laugh, give [stg.] a wipe,
and take a look typically correspond to German
single words, lachen, wischen, and blicken respec-
tively. Such correspondences can be expected to
result in actual parallel sentences where English
verbs have, give, and take would be either aligned
with the verbs lachen, wischen, and blicken re-
spectively or would have no alignment at all. Such
alignments are not common cases and can be ex-
pected to pose a problem to an automatic aligner.
Another type of divergence concerns construc-
tions with vague action verbs (Kearns, 2002). In
this case, English phrases such as make an agree-
ment, make a decision, and give a talk correspond
to German phrases einen Vertrag schliessen, eine
Entscheidung treffen, and einen Vortrag halten,
respectively. Parallel sentences containing these
constructions should be aligned so that English
nouns agreement, decision, and talk are aligned
with German nouns Vertrag, Entscheidung, and
Vortrag. At the same time, English verb make
should be aligned with German schliessen in the
first example, with treffen in the second, and give
should be aligned with halten in the third example.
Aligning the nouns should not pose any problem,
since these alignments are direct lexical transla-
tions (c.f. (LEO, 2006 9) online dictionary, for ex-
ample) and they can be expected to be aligned in
many different sentences. However, aligning the
verbs is necessarily more complicated, since they
are not direct translations of each other and cannot
be expected to be aligned in other contexts.1
However, the difference between the two types
of light verb constructions is not clear cut. They
are better seen as two ends of a continuum of verb
usages with different degrees of verbs? lightness
and different degrees of compositionality of the
meaning of constructions. (Stevenson et al, 2004;
Butt and Geuder, 2001; Grimshaw and Mester,
1988). Even though several English verbs have
been identified as having light usages (e.g. take,
make, have, give, pay), there has been little re-
search on the influence that the properties of the
heading light verb can have on the degree of se-
mantic compositionality of the construction.
The purpose of the present research is to exam-
ine the German translation equivalents of the range
of different English light verb constructions occur-
ring in a parallel corpus and study the differential
performance of a standard aligner on this language
pair for these constructions.
3 Experiments
Our study is based on the assumption that the qual-
ity and bijectivity of the alignment are propor-
tional to the corpus frequency and linguistic com-
positionality of the construction. Therefore, we
identify two aspects of the alignment of these con-
structions as the relevant objects of study.
First, we quantify the amount and nature of cor-
rect word alignments for light verb constructions
compared to regular verbs, as determined by hu-
man inspection. Given the described divergences
between English and German, it can be expected
that light verb constructions will be aligned with a
single word more often than constructions headed
by a regular verb. Assuming that the properties
of the heading light verbs do influence semantic
compositionality of the constructions, it can also
be expected that light verb constructions headed
by different verbs will be differently aligned to the
German translations, constituting different types
of constructions.
1Direct word-to-word English translations of schliessen
listed in the LEO dictionary, for example, are: infer, com-
prise, imply, close, close down, conclude, consummate, draw
up, lock, shut, shutdown, sign off, quit, while make is only
listed within the phrase that is translation for this particular
collocation. Similarly, English word translations for treffen
are: encounter, hook up, cross, get together, meet, meet up,
hit, hurt, score, strike, while make can only be found as a part
of the phrase-to-phrase translations.
53
Second, we evaluate the quality of automatic
word alignments of light verb constructions.
Current word alignment models are based on
the assumption that the best word alignments are
composed of the best word-to-word translations
(as an effect of using Expectation-Maximisation
for training). Factors in the translations that de-
viate from one-to-one alignments are often lex-
ically specific (fertility) and require sufficient
statistics. Because of the interaction of these
properties of the alignment model and the semi-
compositionality of light verb constructions, these
constructions can be expected to pose a problem
for automatic word alignment. Specifically, we ex-
pect lower overall quality of word alignment in the
sentences containing light verb constructions than
in the sentences that contain corresponding regular
constructions.
As indicated, however, we also expect that the
quality of automatic word alignment will be influ-
enced by different distributional phenomena that
are not necessarily related to the linguistic prop-
erties of parallel texts, in particular related to fre-
quency of some of the components of the construc-
tion.
These predictions about the alignment of light
verb constructions in English and German and
their realisations in a corpus are examined in an
experiment.
3.1 Materials and Methods
A random sample of instances of each of the de-
fined types of construction was extracted from a
large word-aligned parallel corpus and manually
examined.
3.1.1 Corpus
The instances of the phrases were taken from the
English-German portion of the Europarl corpus
(Koehn, 2005) that contains the proceedings of the
sessions held in 1999, irrespective of the source
language and of the direction of translation. Be-
fore sampling, the corpus was word-aligned using
GIZA++ (Och and Ney, 2003). Alignments were
performed in both directions, with German as the
target language and with English as the target lan-
guage.
3.1.2 Word alignment using GIZA++
The program for automatic word alignment,
GIZA++, has been developed within a system for
automatic translation. It implements a series of
statistical word-based translation models. In these
models, word alignment is represented as a single-
valued function, mapping each word in the tar-
get sentence to one word in the source sentence.
To account for the fact that some target language
words cannot be aligned with any source language
word, a special empty word (?NULL?) is intro-
duced in the source sentence.
The definition of word alignment does not al-
low many-to-many mappings between the words
of two languages, needed for representing align-
ment of non-compositional multi-word expres-
sions. However, it allows aligning multiple words
in one language to a single word in the other lan-
guage, which is needed for successful alignment
of English light verb constructions.
3.1.3 Sampling phrase instances
To study light verb constructions in a parallel cor-
pus systematically, we group the instances of the
constructions into two types: light verb construc-
tions headed by the verb take, as an example of
true light verb constructions, and those headed
by the verb make, as an example of vague action
verbs. We compare both types of light verb con-
structions to regular constructions headed by the
verbs which are WordNet synonyms of the verb
make (create, produce, draw, fix, (re)construct,
(re)build, establish) with the same subcategoriza-
tion frame.
We analyse three samples of the constructions,
one for each of the types defined by the heading
verb. Each sample contains 100 instances ran-
domly selected from the word-aligned parallel cor-
pus. The constructions are represented as ordered
pairs of words, where the first word is the verb
that heads the construction and the second is the
noun that heads the verb?s complement. Only the
constructions where the complement is the direct
object were included in the analysis.2
3.1.4 Data collection
The following data were collected for each occur-
rence of the English word pairs.
The word or words in the German sentence that
are actual translation of the English words were
identified. If either the English or German verb
2This means that constructions such as take something
into consideration were not included. The only exception to
this were the instances of the construction take something into
account. This construction was included because it is used as
a variation of take account of something with the same trans-
lations to German.
54
form included auxiliary verbs or modals, these
were not considered. Only the lexical part of the
forms were regarded as word translations.
We then determine the type of mapping be-
tween the translations. If the German transla-
tion of an English word pair includes two words
too (e.g. take+decision? Beschluss+fassen), this
was marked as the ?2-2? type. If German trans-
lation is a single word, the mapping was marked
with ?2-1?. This type of alignment is further dis-
tinguished into ?2-1N? and ?2-1V?. In the first
subtype, the English construction corresponds to
a German noun (e.g. initiative+taken ? Initia-
tive). In the second subtype, the English construc-
tion corresponds to a German verb (e.g. take+look
? anschauen). In the cases where a translation
shift occurs so that no translation can be found,
the mapping is marked with ?2-0?.
We also collect the information on automatic
alignment for each element of the English word
pair for both alignment directions. These data
were collected for the elements of English word
pairs (verbs and nouns) separately. The alignment
was assessed as ?good? if the word was aligned
with its actual translation, as ?bad? if the word was
aligned with some other word, and as ?no align? if
no alignment was found. Note that the ?no align?
label could only occur in the setting were English
was the source language, since all the words in the
sentence had to be aligned in the case where it was
the target language.
For example, a record of an occurrence of the
English construction ?make+proposal? extracted
from the bi-sentence in (1) 3 would contain the in-
formation given in (2).
(1) Target language German
EN: He made a proposal.
DE: Er(1) hat(1) einen(3) Vorschlag(4)
gemacht(3).
Target language English
DE: Er hat einen Vorschlag gemacht.
EN: He(1) made(5) a(3) proposal(4).
(2) English instance: made + proposal
German alignment: Vorschlag + gemacht
Type of mapping: 2-2
3Glosses:
Er hat einen Vorschlag gemacht.
he has a proposal made
The numbers in the brackets in the target sentences indicate
the position of the automatically aligned source word.
English
LVC
take
LVC
make
Regular
G
er
m
an
tra
ns
la
tio
n 2? 2 57 50 94
2? 1N 8 18 2
2? 1V 30 28 2
2? 0 5 4 2
Total 100 100 100
Table 1: Types of mapping between English con-
structions and their translation equivalents in Ger-
man.
Automatic alignment, target German, noun:
good, verb: no align
Automatic alignment, target English, noun:
good, verb: good
4 Results
In this section, we present the results of the analy-
ses of both correct (manual) and automatic align-
ment of the three types of constructions, pointing
out the relevant asymmetries.
4.1 Results of Manual Alignment
Table 1 shows how many times each of the four
types of mapping (2-2; 2-1N; 2-1V; 2-0) between
English constructions and their German transla-
tion equivalents occurs in the sample.
We can see that the three types of construc-
tions tend to be mapped to their German equiva-
lents in different ways. First, both types of light
verb constructions are mapped to a single Ger-
man word much more often than the regular con-
structions (38 instances of light verb constructions
with take and 46 instances of light verb construc-
tions with make vs. only 4 instances of regular
constructions.). Confirming our initial hypothe-
sis, this result suggests that the difference between
fully compositional phrases and light verb con-
structions in English can be described in terms of
the degree of the ?2-1? mapping to German trans-
lation equivalents.
An asymmetry can be observed concerning the
two subtypes of the ?2-1? mapping too. The Ger-
man equivalent of an English construction is more
often a verb if the construction is headed by the
verb take (in 30 occurrences, that is 79% of the 2-
1 cases) than if the construction is headed by the
verb make (28 occurrences, 61% cases).
55
DE EN
LVCs with
take
Both EN words 5 57
EN noun 63 79
EN verb 6 57
LVCs with
make
Both EN words 5 40
EN noun 58 58
EN verb 6 52
Regular
construction
Both EN words 26 42
EN noun 68 81
EN verb 32 47
Table 2: Well-aligned instances of LVCs with take,
with make, and with regular constructions (out of
100), produced by an automatic alignment, in both
alignment directions (target is indicated).
In the case where the German translation equiv-
alent for an English construction is a verb, both
components of the English construction are in-
cluded in the corresponding German verb, the ver-
bal category of the light verb and the lexical con-
tent of the nominal complement. These instances
are less compositional, more specific and id-
iomatic (e.g. take+care? ku?mmern, take+notice
? beru?cksichtigen).
On the other hand, English constructions that
correspond to a German noun are more compo-
sitional, less idiomatic and closer to the regular
verb usages (e.g. make+proposal ? Vorschlag,
make+changes ? Korrekturen). The noun that
is regarded as their German translation equivalent
is, in fact, the equivalent of the nominal part of
the construction, while the verbal part is simply
omitted. This result suggests that English light
verb constructions with take are less composi-
tional than the light verb constructions with make.
4.2 Results on Automatic Alignment
We evaluate the quality of automatic alignment of
light verb constructions in comparison with reg-
ular phrases taking into account two factors, the
alignment direction and the frequency of the ele-
ments of the constructions. The results are pre-
sented in the next two sections.
4.2.1 Direction of Alignment
Table 2 shows how the quality of automatic align-
ment varies depending on the direction of align-
ment, as well as on the type of construction. Re-
call that more than one target word can be aligned
to the same source word and all words of the target
have to be aligned.
It can be noted that all the three types of con-
structions are better aligned if the target language
is English. However, the difference in the quality
is bigger in light verb constructions than in regular
constructions, clearly because in this direction the
multi-word property of the English light verb con-
structions can be represented. Both words are well
aligned in light verb constructions with take in 57
cases and with make in 40 cases if the target lan-
guages is English, which is comparable with regu-
lar constructions (42 cases). However, if the target
language is German, both types of light verb con-
structions are aligned well (both words) in only 5
cases, while regular constructions are well aligned
in 26 cases.
Looking into the alignment of the elements of
the constructions (verbs and nouns) separately, we
can notice that nouns are generally better aligned
than verbs for all the three types of constructions,
and in both directions. However, this difference
is not the same in all cases. The difference in
the quality of alignment of nouns and verbs is
the same in both alignment directions for regular
constructions, but it is more pronounced in light
verb constructions if German is the target. On the
other hand, if English is the target, the difference
is smaller in light verb construction than in regular
phrases. These results suggest that the direction of
alignment influences more the alignment of verbs
than the alignment of nouns in general. This influ-
ence is much stronger in light verb constructions
than in regular constructions.
Finally, our initial hypothesis that the quality of
alignment of light verb constructions is lower than
the quality of alignment of regular constructions
has only been confirmed in the case where German
is the target language (both words well aligned in
26 cases, compared to only 5 cases in both types
of light verb constructions). Regular verbs are es-
pecially better aligned than light verbs in this case
(32 : 6). However, if the target is English, the qual-
ity of alignment of regular constructions is simi-
lar to that of light verb constructions with make
(42 and 40 good alignments respectively), while
the constructions with take are aligned even bet-
ter than the other two types (57 good alignments).
These results suggest that the type of construction
which is the least compositional and the most id-
iomatic of the three is best aligned if the direction
of alignment suits its properties.
56
Frequency take LVC make LVC Regular
Low 12 25 62
High 76 35 8
Table 3: The three types of constructions parti-
tioned by the frequency of the complements in the
sample.
Well aligned
Freq take LVC make LVC Regular
Low Both 4 33 8 32 21 34
Freq N 8 66 8 32 47 75
V 4 33 12 48 53 85
High Both 47 62 18 51 4 50
Freq N 64 84 27 77 8 100
V 58 76 18 51 4 50
Table 4: Counts and percentages of well-aligned
instances of the three types of constructions in re-
lation with the frequency of the complements in
the sample. The percentages represent the number
of well-aligned instances out of the overall number
of instances within one frequency range. English
is the target language.
4.2.2 Frequency
Since the quality of alignment of the three types
of constructions proved different from what was
expected in the case where English was the target
language, we examine further the automatic align-
ment in this direction. In particular, we study its
interaction with frequency.
The frequency of the nouns is defined as the
number of occurrences in the sample. It ranges
from 1 to 20 occurrences in the sample of 100 in-
stances. The instances of the constructions were
divided into three frequency ranges: instances
containing nouns with 1 occurrence were con-
sidered as low frequency items; those contain-
ing nouns that occurred 5 and more times in the
sample were considered as high frequency items;
nouns occurring 2, 3, and 4 times were considered
as medium frequency items. Only low and high
frequency items were considered in this analysis.
Table 3 reports the number of instances belong-
ing to different frequency ranges. It can be noted
that light verb constructions with take exhibit a
small number of low frequency nouns. The num-
ber of low frequency nouns increases in the con-
structions with make (25/100), and it is much big-
ger in regular constructions (62/100). The op-
posite is true for high frequency nouns (LVCs
with take: 76/100, with make: 35/100, regular:
8/100). Such distribution of low/high frequency
items reflects different collocational properties of
the constructions. In the most idiomatic construc-
tions (with take), lexical selection is rather limited
which results in little variation. Verbs in regular
constructions select for a wide range of different
complements with little reoccurrence. Construc-
tions with make can be placed between these two
types.
Different trends in the quality of automatic
alignment can be identified for the three types of
constructions depending on the frequency range of
the complement in the constructions, as shown in
Table 4. The quality of alignment of both com-
ponents of the constructions is comparable for all
the three types of constructions in low frequency
items (in 33% of instances of light verb construc-
tions with take, 32% of light verb constructions
with make, and 34% of regular constructions both
the verb and the noun were well aligned). It is
also improved in high frequency items in all the
three types, compared to low frequency. However,
the improvement is bigger in light verb construc-
tions with take (62% well aligned cases) than in
LVCs with make (51%) and in regular construc-
tions (50%).4
Looking into the components of the construc-
tions separately, we can notice interesting differ-
ences in the quality of automatic alignment of
verbs. The proportion of well-aligned verbs in-
creases with the frequency of their complements in
light verb constructions with take (33% of low fre-
quency items compared to 76% of high frequency
items.) It stays almost the same in light verb con-
structions with make (48% of low frequency items
and 51% of high frequency items), and it even de-
creases in regular items (85% of low frequency
items compared to only 50% of high frequency
items).
5 Discussion
The results reported in the previous section con-
firm both components of our first hypothesis (on
the expected differences in cross-lingual mapping)
and refine the conditions under which the sec-
ond hypothesis (on the expected differences in the
quality of automatic alignment) is true. We discuss
4Note that the high frequency regular items are repre-
sented with only 8 instances, which is why the trends might
not be clear enough for this subtype.
57
these conclusions in detail here.
5.1 Manual Alignment
Recall that the first component of our first hypoth-
esis indicated that it is expected that light verb
constructions will be aligned with a single word
more often than constructions headed by a regular
verb.
The analysis of corpus data has shown that
there is a clear difference between English regu-
lar phrases and light verb constructions in the way
they are mapped to their translation equivalents in
German. Regular constructions are mapped word-
by-word, with the English verb being mapped to
the German verb, and the English noun to the Ger-
man noun. A closer look into the only 4 exam-
ples where regular constructions were mapped as
?2-1? shows that this mapping is not due to the
?lightness? of the verb. In two of these cases, it is
the content of the verb that is translated, not that
of the noun (produce+goods? Produktion; estab-
lishes+rights? legt). This never happens in light
verb constructions.
On the other hand, light verb constructions are
much more often translated with a single Ger-
man word. In both subtypes of the ?2-1? map-
ping of light verb constructions, it is the con-
tent of the nominal complement that is translated,
not that of the verb. The noun is either trans-
formed into a verb (take+look ? anschauen) or
it is translated directly with the verb being omitted
(take+initiative? Initiative).
This difference provides empirical grounds for
distinguishing between semantically full and se-
mantically impoverished verbs, a task that is often
difficult on the basis of syntactic tests, since they
often exhibit the same syntactic properties.
The second component of the first hypothesis
indicated that it was expected that the two types of
light verb constructions be differently aligned.
The finding that English light verb construc-
tions with take tend to be aligned more often with a
single German verb and less often to a single Ger-
man noun than the constructions with make justi-
fies classifying the instances into the types based
on the heading verb, which is not a common prac-
tice in the linguistic literature. It suggests that
some semantic or lexical properties of these verbs
can determine the type of the construction. More
precisely, the meaning of the constructions with
take can be regarded as less compositional than the
meaning of the constructions with make. This dif-
ference is also supported by the findings of a pre-
liminary study of Serbian translation equivalents
of these constructions (Samardz?ic?, 2008). English
constructions with take tend to be translated with
a single verb in Serbian, while the constructions
with make are usually translated word-by-word. 5
5.2 Automatic alignment
The second hypothesis conjectured that we would
find lower overall quality of word alignment in
the sentences containing light verb constructions
than in the sentences that contain corresponding
regular constructions. The findings of this re-
search show that the interactions between align-
ment and types of constructions is actually more
complicated than this simple hypothesis, in some
expected and some unexpected ways. To sum-
marise, we found, first, better alignment of regu-
lar constructions compared to light verb construc-
tions only if the target language is German; sec-
ond, overall, alignment if English is target is bet-
ter than if German is target; and thirdly, we found
a clear frequency by construction interaction in the
quality of alignment.
The quality of automatic alignment of both reg-
ular constructions and light verb constructions in-
teracts with the direction of alignment. First,
the alignment is considerably better if the target
language is English than if it is German, which
confirms the findings of (Och and Ney, 2003).
Second, the expected difference in the quality of
alignment between regular constructions and light
verb constructions has only been found in the di-
rection of alignment with German as the target
language, that is where the ?2-1? mapping is ex-
cluded. However, the overall quality of alignment
in this direction is lower than in the other.
This result could be expected, given the general
morphological properties of the two languages, as
well as the formalisation of the notion of word
alignment used in the system for automatic align-
ment. According to this definition, multiple words
in the target language sentence can be aligned with
a single word in the source language sentence,
but not the other way around. Since English is
5The difference in the level of semantic compositionality
of the constructions with take and make could follow from
some semantic properties of these verbs, such as different
aspectual properties or argument structures. However, es-
tablishing such a relation would require a more systematic
semantic study of light, as well as full lexical uses of these
verbs.
58
a morphologically more analytical language than
German, multiple English words often need to be
aligned with a single German word (a situation al-
lowed if English is the target but not if German is
the target).
The phrases in (3) illustrate the two most com-
mon cases of such alignments. First, English
tends to use functional words (the preposition of
in (3a)), where German applies inflection (geni-
tive suffixes on the article des and on the noun Ba-
nanensektors in (3b). Second, compounds are re-
garded as multiple words in English (banana sec-
tor), while they are single words in German (Ba-
nanensektors). This asymmetry explains both the
fact that automatic alignment of all the three types
of constructions is better when the target language
is English and that the alignment of light verb con-
structions is worse than the alignment of regular
phrases when it is forced to be expressed as one-
to-one mapping, which occurs when German is the
alignment target.
(3) a. the infrastructure of the banana sector
b. die Infrastruktur des Bananensektors
Practically, all these factors need to be taken
into consideration in deciding which version of
alignment should be taken, be it for evaluation or
for application in other tasks such as automatic
translation or annotation projection. The inter-
section of the two directions has been proved to
provide most reliable automatic alignment (Pado?,
2007; Och and Ney, 2003). However, it excludes,
by definition, all the cases of potentially useful
good alignments that are only possible in one di-
rection of alignment.
Linguistically, the fact that the expected differ-
ence in the quality of alignment between regular
constructions and light verb constructions has only
been found in the direction where English con-
structions could not be aligned with single German
words can be seen as another empirical indication
of semantic impoverishment of light verbs in com-
parison with full lexical verbs.
Finally, we found an unexpected frequency by
construction interaction (Table 4), which explains
the finding that regular phrases are not better
aligned than light verb constructions if English is
the target language (opposite to our second hy-
pothesis). This interaction, well known in lan-
guage processing and acquisition, occurs in those
cases where marked constructions are very fre-
quent. In our case, the marked construction is the
semi-compositional light verb construction with
take, which has frequent noun complements. In
this case, despite the non-regularity of the con-
struction, alignment is performed well if the di-
rection of alignment allows its mapping to a sin-
gle word. Also, with respect to this phenomenon,
the constructions with take behave more markedly
than those with make.
What is especially interesting about these data
is the fact that the alignment is different not just
between light verb constructions and regular con-
structions, but also between the two types of
light verb constructions. The constructions with
take exhibit more consistent properties of irregu-
lar items, while the constructions withmake can be
positioned somewhere between irregular and regu-
lar items. This additionally confirms the claim that
these two types of constructions differ in the level
of semantic compositionality, providing a basis for
an improvement in their linguistic account.
6 Conclusions and Future Work
In this paper we have proposed a contrastive study
of light verb constructions based on data collected
through alignments of parallel corpora. We have
shown how a linguistically refined analysis can
shed light on particularly difficult cases for an
alignment program, a useful result for improving
current statistical machine translation systems. We
have also shown how properties and behaviours of
these constructions that can be found only in large
parallel corpora and through sophisticated compu-
tational tools can shed light on the linguistic nature
of the constructions under study.
Much remains to be done, both in this general
methodology and for this particular kind of con-
struction. As an example, we note that the fact that
nouns are aligned better than verbs in all the three
types of constructions deserves more investiga-
tion. What we do not yet know is whether this fact
can be related to some known distributional differ-
ences between these two classes or not. It might
also mean that nominal lexical items are more sta-
ble across languages than verbal ones. This can
have implications for machine translations, as well
as for annotation projection, since the stable words
can be used as pivots for alignment and transfer al-
gorithms.
59
References
Roberto Basili, Diego De Cao, Danilo Croce, Bonaven-
tura Coppola, and Alessandro Moschitti. 2009.
Cross-language frame semantics transfer in bilin-
gual corpora. In Alexander F. Gelbukh, editor,
Proceedengs of the 10th International Conference
on Intelligent Text Processing and Computational
Linguistics, pages 332?345, Mexico City, Mexico.
Springer.
Aljoscha Burchardt, Katrin Erk, Anette Frank, An-
drea Kowalski, Sebastian Pado?, and Manfred Pinkal.
2009. FrameNet for the semantic analysis of Ger-
man: Annotation, representation and automation.
In Hans Boas, editor, Multilingual FrameNets in
Computational Lexicography: Methods and appli-
cations, pages 209?244. Mouton de Gruyter.
Miriam Butt and Wilhelm Geuder. 2001. On the
(semi)lexical status of light verbs. In Norbert Corver
and Henk van Riemsdijk, editors, Semilexical Cate-
gories: On the content of function words and the
function of content words, pages 323?370, Berlin.
Mouton de Gruyter.
Michael Collins, Philipp Koehn, and Ivona Kuc?erova?.
2005. Clause restructuring for statistical machine
translation. In Proceedings of the Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 531?540, Ann Arbor. Association for
Computational Linguistics.
Jane Grimshaw and Armin Mester. 1988. Light verbs
and theta-marking. Linguistic Inquiry, 19:205?232.
Richard Kayne. 2000. Parameters and Universals.
Oxford University Press, New York.
Kate Kearns. 2002. Light verbs in English.
Manuscript.
Philipp Koehn. 2005. Europarl: A parallel corpus
for statistical machine translation. In Proceedings
of MT Summit 2005, Phuket, Thailand.
LEO. 2006-9. LEO Online Dictionary. LEO Dictio-
naryTeam, http://dict.leo.org.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?52.
Sebastian Pado?. 2007. Cross-Lingual Annotation
Projection Models for Role-Semantic Information.
Ph.D. thesis, Saarland University.
Tanja Samardz?ic?. 2008. Light verb constructions in
English and Serbian. In English Language and Lit-
erature Studies ? Structures across Cultures, pages
59?73, Belgrade. Faculty of Philology.
Benjamin Snyder, Tahira Naseem, Jacob Eisenstein,
and Regina Barzilay. 2009. Adding more languages
improves unsupervised multilingual part-of-speech
tagging: a Bayesian non-parametric approach. In
Proceedings of Human Language Technologies: The
2009 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 83?91, Boulder, Colorado, June. As-
sociation for Computational Linguistics.
Suzanne Stevenson, Afsaneh Fazly, and Ryan North.
2004. Statistical measures of the semiproductiv-
ity of light verb constructions. In Proceedings of
the ACL04 Workshop on Multiword Expressions:
Integrating Processing, pages 1?8. Association for
Computational Linguistics.
60
Proceedings of the EACL 2014 Workshop on Computational Approaches to Causality in Language, pages 40?47,
Gothenburg, Sweden, April 26, 2014.
c?2014 Association for Computational Linguistics
Likelihood of external causation in the structure of events
Tanja Samard?zi
?
c
CorpusLab, URPP Language and Space
University of Zurich
tanja.samardzic@uzh.ch
Paola Merlo
Linguistics Department
University of Geneva
Paola.Merlo@unige.ch
Abstract
This article addresses the causal structure
of events described by verbs: whether
an event happens spontaneously or it is
caused by an external causer. We automat-
ically estimate the likelihood of external
causation of events based on the distribu-
tion of causative and anticausative uses of
verbs in the causative alternation. We train
a Bayesian model and test it on a monolin-
gual and on a bilingual input. The perfor-
mance is evaluated against an independent
scale of likelihood of external causation
based on typological data. The accuracy
of a two-way classification is 85% in both
monolingual and bilingual setting. On the
task of a three-way classification, the score
is 61% in the monolingual setting and 69%
in the bilingual setting.
1 Introduction
Ubiquitously present in human thinking, causal-
ity is encoded in language in various ways. Com-
putational approaches to causality are mostly
concerned with automatic extraction of causal
schemata (Michotte, 1963; Tversky and Kahne-
man, 1982; Gilovich et al., 1985) from sponta-
neously produced texts based on linguistic encod-
ing. A key to success in this endeavour is under-
standing how human language encodes causality.
Linguistic expressions of causality, such as
causative conjunctions, verbs, morphemes, and
constructions, are highly ambiguous, encoding not
only the real-world causality, but also the struc-
ture of discourse, as well as speakers? attitudes
(Moeschler, 2011; Zufferey, 2012). Causality
judgements are hard to elicit in an annotation
project. This results in a low inter-annotator agree-
ment and makes the evaluation of automatic sys-
tems difficult (Bethard, 2007; Grivaz, 2012).
Our study addresses the relationship between
world-knowledge about causality and the gram-
mar of language, focusing on the causal structure
of events expressed by verbs. In current analyses,
the meaning of verbs is decomposed into multiple
predicates which can be in a temporal and causal
relation (Pustejovsky, 1995; Talmy, 2000; Levin
and Rappaport Hovav, 2005; Ramchand, 2008).
(1) a. Causative: Adam broke the laptop.
b. Anticausative: The laptop broke.
We propose a computational approach to the
causative alternation, illustrated in (1), in which
an event (breaking the laptop in (1)) can be disso-
ciated from its immediate causer (Adam in (1a)).
The causative alternation has been attested in al-
most all languages (Schafer, 2009), but it is re-
alised with considerable cross-linguistic variation
in the sets of alternating verbs and in the grammat-
ical encoding (Alexiadou et al., 2006; Alexiadou,
2010).
Since the causative alternation involves most
verbs, identifying the properties of verbs which al-
low them to alternate is important for developing
representations of the meaning of verbs in gen-
eral. Analysing the structural components of the
meaning of verbs proves important for tasks such
as word sense disambiguation (Lapata and Brew,
2004), semantic role labelling (M`arquez et al.,
2008), cross-linguistic transfer of semantic anno-
tation (Pad?o and Lapata, 2009; Fung et al., 2007;
van der Plas et al., 2011). The knowledge about
the likelihood of external causation might be help-
ful in the task of detecting implicit arguments of
verbs and, especially deverbal nouns (Gerber and
Chai, 2012; Roth and Frank, 2012). Knowing,
for example, that a verb expresses an externally
caused event increases the probability of an im-
plicit causer if an explicit causer is not detected in
a particular instance of the verb. Our study should
40
contribute to the development of formal and exten-
sive representations of grammatically relevant se-
mantic properties of verbs, such as Verb Net (Kip-
per Schuler, 2005) and PropBank (Palmer et al.,
2005).
2 External Causation and the Grammar
of Language
The distinction between external and internal cau-
sation in events described by verbs is introduced
by Levin and Rappaport Hovav (1994) to account
for the fact that the alternation is blocked in some
verbs such as bloom in (2). In Levin and Rappa-
port Hovav?s account, verbs which describe ex-
ternally caused events alternate (1), while verbs
which describe internally caused events do not (2).
(2) a. The flowers suddenly bloomed.
b. * The summer bloomed the flowers.
The main objection to this proposed generali-
sation is that it does not account for the cross-
linguistic variation. Since the distinction concerns
the meaning of verbs, one could expect that the
verbs which are translations of each other alter-
nate in all languages. This is, however, often not
true. There are many verbs that do alternate in
some languages, while their counterparts in other
languages do not (Alexiadou et al., 2006; Schafer,
2009; Alexiadou, 2010). For example, appear and
arrive do not alternate in English, but their equiv-
alents in Japanese or in the Salish languages do.
To account for the variation in cross-linguistic
data Alexiadou (2010) introduces the notion of
cause-unspecified events, a category between ex-
ternally caused and internally caused events. In-
troducing gradience into the classification allows
Alexiadou to propose generalisations which apply
across languages: cause-unspecified verbs alter-
nate in all languages, while only some languages
allow the alternation if the event is either exter-
nally or internally caused. To allow the alterna-
tion in the latter cases, languages need a special
grammatical mechanism. In English, for example,
this mechanism is not available, which is why only
cause-unspecified verbs alternate. The alternation
is thus blocked in both verbs describing externally
caused and internally caused events.
Alexiadou?s account is based not only on the
observations about the availability of the alterna-
tion, but also about morphological encoding of
the alternation across languages. Unlike English,
which does not mark the alternation morphologi-
cally (note that the two versions of English verbs
in (1-3) are morphologically identical), other lan-
guages encode the alternation in different ways, as
shown in (3).
(3)
Causative Anticausative
Mongolian xajl-uul-ax xajl-ax
?melt? ?melt?
Russian rasplavit rasplavit-sja
?melt? ?melt?
Japanese atum-eru atum-aru
?gather? ?gather?
An analysis of the distribution of morpholog-
ical marking across languages leads Haspelmath
(1993) to introduce the notion of likelihood into
his account of the meaning of the alternating
verbs. In a study of 31 verbs in 21 languages
from all over the world, Haspelmath notices that
certain verbs tend to get the same kind of mark-
ing across languages. For each verb, he calcu-
lates the ratio between the number of languages
which mark the anticausative version and the num-
ber of languages which mark the causative version
of the verb. He interprets this ratio as a quantita-
tive measure of how spontaneous events described
by the verbs are. As each verb is assigned a dif-
ferent score, ranking the verbs according to the
score results in a ?scale of increasing likelihood of
spontaneous occurrence?. Events with a low anti-
causative/causative ratio (e.g. boil, dry, melt) are
likely to occur spontaneously, while events with a
high ratio (e.g. break, close, split) are likely to be
caused by an external causer.
3 The Model
Our study pursues the quantitative assessment of
the likelihood of external causation in the events
described the alternating verbs. We estimate the
likelihood by means of a Bayesian model which
divides events into classes based on the distribu-
tion of causative and anticausative uses of verbs in
a corpus. By varying the settings of the model, we
address two questions discussed in the linguistic
literature: 1) Is the distinction between externally
caused and internally caused events binary,, as ar-
gued by Levin and Rappaport Hovav (1994), or are
there are intermediate classes, as argued by Alex-
iadou (2010)? and 2) Do we obtain better esti-
mation of the likelihood from cross-linguistic than
from monolingual data?
41
We design a probabilistic model which esti-
mates the likelihood of external causation and gen-
erates a probability distribution over a given num-
ber of event classes for each verb in a given set
of verbs. The model formalises the intuition that
an externally caused event tends to be expressed
by a verb in its causative realisation. In other
words, if the likelihood of external causation of the
event is encoded in the use of the verb which de-
scribes the event, then the causer is expected to ap-
pear frequently in the realisations of the verb. The
opposite is expected for internally caused events.
Cause-unspecified events are expected to appear
with and without the causer equally.
To take into account the two questions discussed
in the theoretical approaches, namely the number
of classes and the role of cross-linguistic data in
the classification of events, we design four ver-
sions of the model, varying the input data and the
number of classes in the output: a) monolingual
input and two classes; b) cross-linguistic input
and two classes; c) monolingual input and three
classes; d) cross-linguistic input and three classes.
The current cross-linguistic versions of the
model include only two languages, English and
German, because we test the models in a minimal
cross-linguistic setting. In principle, the approach
can be easily extended to include any number of
languages.
As it can be seen in its graphical representation
in Figure 1, the model consists of three variables
in the monolingual version and of four variables in
the cross-linguistic version.
V
Caus
En
V
Caus
En Ge
Figure 1: Two version of the Bayesian net model
for estimating external causation.
The first variable in both versions is the set of
verbs V . This can be any given set of verbs.
The second variable is the event class of the
verb, for which we use the symbol Caus. The val-
ues of this variable depend on the assumed classi-
fication. In the two-class version, the values are
causative, representing externally caused events,
and anticausative, representing internally caused
events. In the three-class version, the variable
can take one more value, unspecified, representing
cause-unspecified events.
The third (En) and the fourth (Ge) (in the cross-
linguistic version) variables are the surface realisa-
tions of the verbs in parallel instances. These vari-
ables take three values: causative for active tran-
sitive use, anticausative for intransitive use, and
passive for passive use in the languages in ques-
tion.
We represent the relations between the variables
as a Bayesian network. The variable that rep-
resents the event class of verbs (Caus) is unob-
served. The values for the other three variables
are observed in the data source. Note that the in-
put to the model does not contain the information
about the event class at any point.
The dependence between En and Ge in the
bilingual version of the model represents the fact
that the two instances of a verb are translations
of each other, but does not represent the direction
of translation in the actual data. The form of the
instance in one language depends on the form of
the parallel instance because they express the same
meaning in the same context, regardless of the di-
rection of translation.
Assuming that the variables are related as in
Figure 1, En and Ge are conditionally indepen-
dent of V given Caus, so we can calculate the
probability of the model as in (4) for the monolin-
gual version and as in (6) for the cross-linguistic
version.
(4)
P (v, caus, en) = P (v) ? P (caus|v) ? P (en|caus)
(5)
P (caus|v, en) =
P (v)?P (caus|v)?P (en|caus)?
caus
P (v)?P (caus|v)?P (en|caus)
We estimate the conditional probability of the
event class given the verb (P (caus|v)) by query-
ing the model, as shown in (5) for the monolingual
version and in (7) for the bilingual version..
42
(6)
P (v, caus, en, ge) =
P (v) ? P (caus|v) ? P (en|caus) ? P (ge|caus, en)
(7)
P (caus|v, en, ge) =
P (v)?P (caus|v)?P (en|caus)?P (ge|caus,en)?
caus
P (v)?P (caus|v)?P (en|caus)?P (ge|caus,en)
We assign to each verb the event class that is
most probable given the verb, as in (8).
(8)
caus class(verb) = argmax
caus
P (caus|v)
All the variables in the model are defined so that
the parameters can be estimated on the basis of
frequencies of instances of verbs automatically ex-
tracted from parsed corpora.
4 Experiments
The accuracy of the predictions of the model is
evaluated in experiments.
4.1 Materials and Methods
The verbs for which we estimate the likelihood are
the 354 verbs that participate in the causative alter-
nation in English, as listed by Levin (1993), and
the 26 verbs listed as alternating in a typological
study (Haspelmath, 1993).
We estimate the parameters of the model by
implementing the expectation-maximisation algo-
rithm. The algorithm is initialised by assigning
different arbitrary values to the parameters of the
model. The classification reported in the paper is
obtained after 100 iterations.
We train the classifier using the data automat-
ically extracted from an English-German parallel
corpus (Europarl (Koehn, 2005)). Both monolin-
gual and bilingual input data are extracted from
the parallel corpus. All German verbs which are
word-aligned with the alternating English verbs
listed in the literature are regarded as German
equivalents. By extracting cross-linguistic equiv-
alents automatically from a parallel corpus, we
avoid manual translation into German of the lists
of English verbs discussed in the literature. In this
way, we eliminate the judgements which would be
involved in the process of translation.
The corpus is syntactically parsed (using the
MaltParser (Nivre et al., 2007)) and word-aligned
(using GIZA++ (Och and Ney, 2003)). For both
the syntactic parses and word alignments, we
reuse the data provided by Bouma et al. (2010).
We extract only the instances of verbs where
both the object (if there is one) and the sub-
ject are realised in the same clause, excluding
the instances involving syntactic movements and
coreference. Transitive instances are considered
causative realisations, intransitive anticausative.
We count passive instances separately because
they are formally transitive, but they usually do not
express the causer.
German equivalents of English alternating verbs
are extracted in two steps. First, all verbs occur-
ring as transitive, intransitive, and passive were
extracted from the German sentences that are
sentence-aligned with the English sentences con-
taining the instances of alternating verbs. These
instances were considered candidate translations.
The instances that are the translations of the En-
glish instances were then selected on the basis of
word alignments. Instances where at least one el-
ement (the verb, the head of its object, or the head
of its subject) is aligned with at least one element
in the English instance were considered aligned.
Only the instances of English verbs that are
translated with a corresponding finite verbal form
in German are extracted, excluding the cases
where English verbs are translated into a corre-
sponding non-finite form such as infinitive, nomi-
nalization, or participle in German.
4.2 Evaluation
We evaluate the performance of the models against
the scale of spontaneous occurrence proposed by
Haspelmath (1993), shown in (9). We expect the
verbs classified as internally caused by our models
to correspond to the verbs with a low morpholog-
ical anticausative/causative ratio (those on the left
side of the scale). The opposite is expected for
externally caused verbs. Cause-unspecified verbs
are expected to be in the middle of Haspelmath?s
scale.
(9) boil, dry, wake up, sink, learn-teach, melt,
stop, turn, dissolve, burn, fill, finish, begin,
spread, roll, develop, rise-raise, improve,
rock, connect, change, gather, open, break,
close, split
To evaluate the output of our models against the
scale, we discretise the scale so that the agreement
43
is maximised for each version of the model. For
example, the threshold which divides the verbs
into anticausative and causative in the two-way
classification is set after the verb turn.
By evaluating the performance of our models
against a typology-based measure, we avoid elic-
iting human judgements, which is a known prob-
lem in computational approaches to causality. The
downside of this approach is that such evaluation
is currently possible for a relatively small number
of verbs.
5 Results and Discussion
Table 1 shows all the confusion matrices of the
classifications performed automatically in com-
parison with the classifications based on the typol-
ogy rankings.
1
In the two-way classification, the two versions
of the model, with monolingual and with bilingual
input, result in identical classifications. The agree-
ment of the models with the typological ranking
can be considered very good (85%). The optimal
threshold divides the verbs into two asymmetric
classes: eight verbs in the internally caused class
and eighteen in the externally caused class. The
agreement is better for the internally caused class.
In the three way-classification, the performance
of both versions of the model drops. In this set-
ting, the output of the two versions differs: there
are two verbs which are classified as externally
caused by the monolingual version and as cause-
unspecified by the bilingual version, which results
in a slightly better performance of the bilingual
version. Given the small number of evaluated
verbs, however, this tendency cannot be consid-
ered significant.
The three-way classification is more difficult
than the two-way classification, but the difficulty
is not only due to the number of classes, but also
to the fact that two of the classes are not well-
distinguished in the data. While the class of in-
ternally caused events is relatively easily distin-
guished (small number of errors in all classifica-
tions), the classes of externally caused and cause-
unspecified verbs are hard to distinguish. This
finding supports the two-way classification argued
for in the literature.
The classification performed by the bilingual
1
The table contains 26 instead of 31 verbs because corpus
data could not be reliably extracted for some phrasal verbs
listed by Haspelmath.
model indicates that the distinction between ex-
ternally caused and cause-unspecified verbs might
still exist. Compared to the monolingual clas-
sification, more verbs are classified as cause-
unspecified, and they are grouped in the middle of
the typological scale. Since the model takes into
account cross-linguistic variation in the realisa-
tions of the alternating verbs, the observed differ-
ence in the performance could be interpreted as a
sign that the distinction between cause-unspecified
and externally caused events does emerge in cross-
linguistic contexts.
While supporting the two-way classification of
events, our experiments do not provide a defi-
nite answer to the question of whether there are
more than two classes of events. To obtain sig-
nificant results, more verbs need to be evaluated.
However, the typological data used in our exper-
iments (Haspelmath, 1993) are not easily avail-
able. This kind of data are currently not included
in the typological resources (such as the WALS
database (Dryer and Haspelmath, 2013)), but they
can, in principle, be collected from other elec-
tronic sources of language documentation, which
are increasingly available for many different lan-
guages.
6 Related Work
The proposed distinction between externally and
internally caused events is addressed by McKoon
and Macfarland (2000). They study twenty-one
verbs defined in the linguistic literature as describ-
ing internally caused events and fourteen verbs de-
scribing externally caused events. Their corpus
study shows that the appearance of these verbs as
causative (transitive) and anticausative (intransi-
tive) cannot be used as a diagnostic for the kind
of meaning that has been attributed to them.
Since internally caused verbs do not enter the
alternation, they were expected to be found in in-
transitive clauses only. This, however, was not the
case. The probability for some of these verbs to
occur in a transitive clause is actually quite high
(0.63 for the verb corrode, for example). More
importantly, no difference was found in the prob-
ability of the verbs denoting internally caused and
externally caused events to occur as transitive or
as intransitive. This means that the acceptability
judgements used in the qualitative analysis do not
apply to all the verbs in question, and, also, not to
all the instances of these verbs.
44
Model 2-class 3-class
Monolingual Bilingual Monolingual Bilingual
Typology acaus caus acaus caus acaus caus unspec. acaus caus unspec.
acaus 8 0 8 0 6 0 1 6 0 1
caus 4 14 4 14 0 3 0 0 3 0
unspec. ? ? ? ? 4 5 7 4 3 9
Agreement 85% 85% 61% 69%
Table 1: Per class and overall agreement between the corpus-based and the typology-based classification
of verbs; acaus = internally caused, caus = externally caused, unspec. = cause-unspecified.
Even though the most obvious prediction con-
cerning the corpus instances of the two groups of
verbs was not confirmed, the corpus data were
still found to support the distinction between the
two groups. Examining 50 randomly selected in-
stances of transitive uses of each of the studied
verbs, McKoon and Macfarland (2000) find that,
when used in a transitive clause, internally caused
change-of-state verbs tend to occur with a limited
set of subjects, while externally caused verbs can
occur with a wider range of subjects. This differ-
ence is statistically significant.
The relation between frequencies of certain uses
and the lexical semantics of English verbs has
been explored by Merlo and Stevenson (2001) in
the context of automatic verb classification. Merlo
and Stevenson (2001) show that information col-
lected from instances of verbs in a corpus can be
used to distinguish between three different classes
which all include verbs that alternate between
transitive and intransitive use. The classes in ques-
tion are manner of motion verbs (10), which alter-
nate only in a limited number of languages, exter-
nally caused change of state verbs (11), alternating
across languages, and performance/creation verbs,
which are not lexical causatives (12).
(10) a. The horse raced past the barn.
b. The jockey raced the horse past the barn.
(11) a. The butter melted in the pan.
b. The cook melted the butter in the pan.
(12) a. The boy played.
b. The boy played soccer.
One of the most useful features for the classi-
fication proved to be the causativity feature. It
represents the fact that, in the causative alterna-
tion, the same lexical items can occur both as sub-
jects and as objects of the same verb. This feature
sets apart the two causative classes from the per-
formance class.
In the context of psycholinguistic empirical ap-
proaches to encoding causality in verbs, it has
been established that assigning a causal relation
to a sequence of events can be influenced by the
native languages (Wolff et al., 2009a; Wolff and
Ventura, 2009b). English speakers, for instance,
tend to assign causal relations more than Russian
speakers.
In our study, we draw on the fact that the se-
mantic properties of verbs are reflected in the way
they are used in a corpus, established by the pre-
vious studies. We explore this relationship further,
relating it to a deeper semantic analysis and to the
typological distribution of grammatical features.
7 Conclusion and Future Work
The experiments presented in this article provide
empirical evidence that contribute to a better un-
derstanding of the relationship between the causal
semantics of verbs, their formal morphological
and syntactic properties, and the variation in their
use. We have shown that the likelihood of external
causation of events is encoded in the distribution
of the causative and anticausative uses of verbs.
Two classes of events, externally caused and inter-
nally caused events, can be distinguished automat-
ically based on corpus data.
In future work, we will further investigate the
question of whether there are more than two
classes of events and how they are distinguished.
We will explore potential tendencies indicated by
our findings. We will apply the approach proposed
in this article to an extended data set. On one hand,
we will collect typological data for more verbs, ex-
ploring possibilities of automatic data extraction.
On the other hand, we will include more languages
in the model to ensure a better representation of
cross-linguistic variation.
45
References
Artemis Alexiadou, Elena Anagnostopoulou, and Flo-
rian Schfer. 2006. The properties of anticausatives
crosslinguistically. In Mara Frascarelli, editor,
Phases of Interpretation, pages 187?212, Berlin,
New York. Mouton de Gruyter.
Artemis Alexiadou. 2010. On the morpho-syntax of
(anti-)causative verbs. In Malka Rappaport Hovav,
Edit Doron, and Ivy Sichel, editors, Syntax, Lexical
Semantics and Event Structure, pages 177?203, Ox-
ford. Oxford University Press.
Steven Bethard. 2007. Finding Event, Temporal and
Causal Structure in Text: A Machine Learning Ap-
proach. Ph.D. thesis, University of Colorado at
Boulder.
Gerlof Bouma, Lilja ?vrelid, and Jonas Kuhn. 2010.
Towards a large parallel corpus of cleft construc-
tions. In Proceedings of the Seventh conference on
International Language Resources and Evaluation
(LREC?10), Valletta, Malta. European Language Re-
sources Association.
Matthew S. Dryer and Martin Haspelmath, editors.
2013. WALS Online. Max Planck Institute for Evo-
lutionary Anthropology, Leipzig.
Pascale Fung, Zhaojun Wu, Yongsheng Yang, and
Dekai Wu. 2007. Learning bilingual semantic
frames: Shallow semantic parsing vs. semantic role
projection. In 11th Conference on Theoretical and
Methodological Issues in Machine Translation (TMI
2007), pages 75?84, Skovde, Sweden.
Matthew Gerber and Joyce Y. Chai. 2012. Seman-
tic role labeling of implicit arguments for nominal
predicates. Computational Linguistics, 38(4):755?
798.
Thomas Gilovich, Robert Vallone, and Amos Tversky.
1985. The hot hand in basketball: On the misper-
ception of random sequences. Cognitive Psychol-
ogy, 17(3):295?314.
C?ecile Grivaz. 2012. Automatic extraction of causal
knowledge from natural language texts. Ph.D. the-
sis, University of Geneva.
Martin Haspelmath. 1993. More on typology of
inchoative/causative verb alternations. In Bernard
Comrie and Maria Polinsky, editors, Causatives
and transitivity, volume 23, pages 87?121, Amster-
dam/Philadelphia. John Benjamins Publishing Co.
Karin Kipper Schuler. 2005. VerbNet: A broad-
coverage, comprehensive verb lexicon. Ph.D. thesis,
University of Pennsylvania.
Philipp Koehn. 2005. Europarl: A parallel corpus
for statistical machine translation. In Proceedings
of MT Summit 2005, Phuket, Thailand.
Mirella Lapata and Chris Brew. 2004. Verb class
disambiguation using informative priors. Computa-
tional Linguistics, 30(1):45?73.
Beth Levin and Malka Rappaport Hovav. 1994. A pre-
liminary analysis of causative verbs in English. Lin-
gua, 92:35?77.
Beth Levin and Malka Rappaport Hovav. 2005. Ar-
gument realization. Cambridge University Press,
Cambridge.
Beth Levin. 1993. English verb classes and alterna-
tions : a preliminary investigation. The University
of Chicago Press, Chicago.
Llu??s M`arquez, Xavier Carreras, Kenneth C.
Litkowski, and Suzanne Stevenson. 2008. Se-
mantic role labeling: An introduction to the special
issue. Computational Linguistics, 34(2):145?159.
Gail McKoon and Talke Macfarland. 2000. Externally
and internally caused change of state verbs. Lan-
guage, 76(4):833?858.
Paola Merlo and Susanne Stevenson. 2001. Automatic
verb classification based on statistical distribution
of argument structure. Computational Linguistics,
27(3):373?408.
Albert Michotte. 1963. The perception of causality.
Basic Books, Oxford, England.
Jacques Moeschler. 2011. Causal, inferential and
temporal connectives: Why ?parce que? is the only
causal connective in French. In S. Hancil, editor,
Marqueurs discursifs et subjectivit?e, pages 97?114,
Rouen. Presses Universitaires de Rouen et du Havre.
Joakim Nivre, Johan Hall, Jens Nilsson, Chanev
Atanas, Gles?en Eryi?git, Sandra Kbler, Svetoslav
Marinov, and Erwin Marsi. 2007. MaltParser:
A language-independent system for data-driven de-
pendency parsing. Natural Language Engineering,
13(2):95?135.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?52.
Sebastian Pad?o and Mirella Lapata. 2009. Cross-
lingual annotation projection of semantic roles.
Journal of Artificial Intelligence Research, 36:307?
340.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The Proposition Bank: An annotated cor-
pus of semantic roles. Computational Linguistics,
31(1):71?105.
James Pustejovsky. 1995. The generative lexicon.
MIT Press, Cambridge, MA.
Gillian Ramchand. 2008. Verb Meaning and the Lex-
icon: A First Phase Syntax. Cambridge Studies
in Linguistics. Cambridge University Press, Cam-
bridge.
46
Michael Roth and Anette Frank. 2012. Aligning predi-
cate argument structures in monolingual comparable
texts: A new corpus for a new task. In *SEM 2012:
The First Joint Conference on Lexical and Compu-
tational Semantics ? Volume 1: Proceedings of the
main conference and the shared task, and Volume 2:
Proceedings of the Sixth International Workshop on
Semantic Evaluation (SemEval 2012), pages 218?
227, Montr?eal, Canada, 7-8 June. Association for
Computational Linguistics.
Florian Schafer. 2009. The causative alternation.
In Language and Linguistics Compass, volume 3,
pages 641?681. Blackwell Publishing.
Leonard Talmy. 2000. Towards a cognitive semantics.
The MIT Press, Cambridge Mass.
Amos Tversky and Daniel Kahneman. 1982. Causal
schemata in judgments under uncertainty. In Daniel
Kahneman, Paul Slovic, and Amos Tversky, editors,
Judgement Under Uncertainty: Heuristics and Bi-
ases.
Lonneke van der Plas, Paola Merlo, and James Hen-
derson. 2011. Scaling up automatic cross-lingual
semantic role annotation. In Proceedings of the 49th
Annual Meeting of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 299?304, Portland, Oregon, USA, June. As-
sociation for Computational Linguistics.
Phillip Wolff and Tatyana Ventura. 2009b. When Rus-
sians learn English: How the semantics of causation
may change. Bilingualism: Language and Cogni-
tion, 12(2):153?176.
Phillip Wolff, Ga-Hyun Jeon, and Yu Li. 2009a.
Causal agents in English, Korean and Chinese: The
role of internal and external causation. Language
and Cognition, 1(2):165?194.
Sandrine Zufferey. 2012. ?Car, parce que, puisque?
revisited: Three empirical studies on French causal
connectives. Journal of Pragmatics, 44:138?153.
47
