Complex structuring of term variants for Question Answering
James Dowdall, Fabio Rinaldi
Institute of Computational Linguistics
University of Zurich
fdowdall,rinaldig@cl.unizh.ch
Fidelia Ibekwe-SanJuan
ERSICO
University of Lyon3
ibekwe@univ-lyon3.fr
Eric SanJuan
LITA EA3097
University of Metz
eric.sanjuan@iut.univ-metz.fr
Abstract
Question Answering provides a method of
locating precise answers to specic ques-
tions but in technical domains the amount
of Multi-Word Terms complicates this task.
This paper outlines the Question Answer-
ing task in such a domain and explores two
ways of detecting relations between Multi-
Word Terms. The rst targets specic se-
mantic relations, the second uses a cluster-
ing algorithm, but they are both based on
the idea of syntactic variation. The pa-
per demonstrates how the combination of
these two methodologies provide sophisti-
cated access to technical domains.
1 Introduction
Nominal compounds are inherently ambiguous on
both the syntactic and semantic fronts. Whilst the
number of syntactic possibilities increase exponen-
tially with word length (Isabelle, 1984), semantic in-
terpretation is at best contextually dependent and in
the worst cases determined by extra-linguistic (prag-
matic) factors.
1
Technical documentation is an at-
tractive domain in which to explore nominal com-
pounds for two reasons. First, they present an abun-
dance of compounds, secondly they restrict semantic
interpretation by excluding compounds with opaque
(extra-linguistic) interpretation. The result is multi-
word terms (MWT) which are both compositional,
their formation is a function of their constituent ele-
ments (Kageura, 2002) and endocentric, the com-
pound is a hyponym of its head (Barker and Sz-
pakowicz, 1998).
1
For example, \apple juice place" (Levi, 1979)
This paper addresses the issue of structuring the
Multi-Word Terms (MWTs) for Question Answer-
ing (QA) in technical domains. The central prob-
lem is that unfamiliarity with MWTs that character-
ize such domains creates an eective barrier against
users nding answers.
Section 2 outlines the domain of focus, the
MWT extraction method and examples character-
istic MWTs. Section 3 explores the QA task in tech-
nical domains by describing the ExtrAns system,
and how it structures the MWTs for the task. Sec-
tion 4 presents TermWatch which identies syntac-
tic variants and uses a hierarchical clustering algo-
rithm to build classes of term variants. The common
ground between these two approaches is in the use
of syntactic variants to structure the terminology as
a whole. Section 5 explores how the resulting struc-
tures can be used in the QA task. After surveying
some related work in Section 6 the paper ends by
drawing conclusions on the approaches presented.
2 MWT Extraction
Before the MWTs can be structured, the terms
need to be extracted from a corpus of texts. This
stage was performed using the INTEX linguistic
parser (Silberztein, 1993). INTEX is a nite state
transducer parser. The corpus used in the present
study concerns scientic publications on the bread-
making process. It was made available by the French
Institute of Scientic Information (INIST). Without
going into much detail regarding the candidate term
extraction rules, the approach adopted can be sum-
marized as selective NLP followed by shallow pars-
ing, much in the same way as (Evans et al, 1992).
We dened morpho-syntactic properties of complex
nominal syntagms written as nite state automata,
implemented in the INTEX linguistic toolbox. IN-
TEX is equipped with linguistic resources to perform
 Knowledge 
Base
Document
Linguistic
Analysis
MWT
processing
(a)
Oine
 Knowledge 
Base
ANSWERS
Query
Document
Linguistic
Analysis
MWT
Recognition
(b)
Online
Figure 1: Schematic: ExtrAns Processing Stages
an overall morpho-syntactic analysis on the texts.
The NP automata are applied in an iterative way
on the corpus until we reach a satisfactory medium-
grained noun phrase splitting. Our concern was to
extract more or less complex terms as they appeared
in the text corpus and not atomic NP extraction.
The rationale was to conserve the associations be-
tween terms as the scientists (authors) made them
during their write-up. Examples of candidate terms
extracted at this stage are: \hydrophilic powdered
lecithin, traditional sour dough starter cultures, de-
velopment of traditional bread avour". More details
on the NP splitting rules can be found in (Ibekwe-
SanJuan, 2001). Manual validation by a domain ex-
pert produced 3651 MWTs.
3 ExtrAns
Question Answering systems attempt to extract
small snippets of text in response to a natural lan-
guage query. Briey, ExtrAns achieves this in two
distinct stages:
O-line the entire document collection is sub-
jected to linguistic analysis, which produces a full
syntactic parse for each sentence. After some inter-
mediate steps, such as anaphora resolution and dis-
ambiguation, the syntactic parse is translated into a
semantic representation designed to capture the core
meaning of the sentences. These representations are
stored in a Knowledge Base.
On-line user queries are subjected to the same
linguistic analysis. The resulting semantic rep-
resentation of the query is `matched' against the
knowledge base. These `matches' can be identied
in their original document location, so users can
contextualize these potential answers. Interest in
the specics of this process should be directed
toward (Rinaldi et al, 2002) (Dowdall et al, 2002).
In dealing with technical domains we have iden-
tied two major obstacles for a QA system which
can be summarize as the Parsing Problem and the
Paraphrase Problem.
The Parsing Problem consists in the increased
di?culty of parsing text in a technical domain due to
domain-specic sublanguage. Various types of MWT
characterize these domains, in particular referring
to specic concepts like tools, parts or procedures.
These multi word expressions might include lexical
items which are either unknown to a generic lexicon
(e.g. \acetic acid") or have a specic meaning unique
to this domain. Abbreviations and acronyms are an-
other common source of incosistencies. In such cases
the parser might either fail to identify the compound
as a phrase and consequently fail to parse the sen-
tence including such items. Alternatively the parser
might attempt to `guess' their lexical category (in
the set of open class categories), leading to an ex-
ponential growth of the number of possible syntactic
parses. Not only the internal structure of the com-
pound can be multi-way ambiguous, even the bound-
aries of the compounds might be di?cult to detect
and the parsers might try odd combinations of the
tokens belonging to the compounds with neighbour-
ing tokens.
The Paraphrase Problem resides in the imper-
fect knowledge of users of the systems, who cannot be
expected to be completely familiar with the domain
terminology. Even experienced users, who know the
domain very well, might not remember the exact
wording of a MWT and use a paraphrase to refer to
the underlying domain concept. Besides even in the
documents themselves, unless the editors have been
forced to use some strict terminology control system,
various paraphrases of the same compound will ap-
pear, and they need to be identied as co-referent.
However, it is not enough to identify all paraphrases
within the manual, novel paraphases might be cre-
ated by the users each time they query the system.
The task of QA in technical domains is to identify:
`what' needs to be known about `which' multi-word
term. Then to extract sentences that provide the
answer. How to nd the `what' is dependant on the
approach. ExtrAns uses linguistic processing which
results in a semantic representation. However, in
the TREC domain of newswire, considerable success
has been achieved by statistical measures and even
pattern matching. Here, these distinctions are unim-
portant.
What is of concern is in how to meet the two com-
peting search needs of answering specic questions
and navigating through a domain of specialized, un-
familiar MWTs.
Designed specically for technical domains, Ex-
trAns involves strategies for exploiting the abundant
MWTs that these domains hold. The approach uti-
lizes WordNet to gather the MWTs into synonymy
sets based on variation rules. The terminology is also
related through an hyponymy hierarchy.
Synonymy between MWTs is either strict, or
detected through WordNet. Strictly synonymous
MWTs coreference a single object/concept. This
link is a result of morpho-syntactic variation taking
\chemical improver action" and producing the anit-
symmetrical term \action of chemical improver".
The process simply involves inverting the Head and
introducing modiers with a preposition.
WordNet synonymy, on the other hand, comes in
three types of symmetrical variation depending on
which tokens from two MWTs can be found in the
same synset:
 WordNet Head substitution, (\bread ingestion"
and \bread consumption")
 WordNet modier substitution (\quantity of
yeast" and \amount of yeast")
 WordNet Modier and head substitution (\key
ingredient" and \functional component").
However, synonymy identied through WordNet is
dened by WordNet. As a general lexical database
not designed for specilized domains it represents
common synonymy between words. The resulting
links created between multi-word terms translates
into concepts non-specialists cannot easily distin-
guish. These links produced 1277 synsets the vast
majority of which contain two MWTs.
Hyponymy The MWTs are organized into a lex-
ical hyponymy (is a) hierarchy that exploits their
endocentricity (Barker and Szpakowicz, 1998). The
hyponymy relation is identied through two types of
rules, Left Expansions which further modies \dough
stickiness" to be \intense" producing \intense dough
stickiness". Here the original head-modier rela-
tions of the hypernym are unaltered in the hyponym.
However, with Insertion rules these relations are
stickiness
dough
stickiness
surface
stickiness
dough
increase
stickiness
wheat
dough
surface
stickiness
measure
surface
stickiness
intense
dough
stickiness
diminished
dough
stickiness
wheat
dough
stickiness
Figure 2: Hyponymy Hierarchy
changed in the potential hyponym. For example,
whatever is going on in \wheat dough stickiness", in-
serting the word \surface" to produce \wheat dough
surface stickiness" has altered the original head-
modier relations. So a generic/specic relation is
less certain. For the moment such links are permit-
ted.
This process allows multiple parents for a given
term. So \wheat dough surface stickiness" is also
a hyponym of \surface stickiness" through a left-
expansion rule. An example of this kind of hierarchy
can be seen in gure 2.
These two structures are exploited in the search
process during `matching' of queries against answers.
The strengths they bring and the limitations imposed
are explored in Section 5 after description of an al-
ternative approach to term variant structuring.
4 The TermWatch system
TermWatch (Ibekwe-SanJuan and SanJuan, 2003)
clusters term variants into classes, thus producing
a three-level structuring of terms: term, connected
component and class levels. It integrates a visual
interface developed with the Aisee graphic visualiza-
tion to enable the user explore the classes and browse
through the links between terms. Earlier stages of
this work were presented in (Ibekwe-SanJuan, 1998).
The system comprises of two major modules: a
syntactic variant identier and a clustering module
whose results are loaded onto the Aisee visualization
tool.
2
4.1 Variants identier module
Automatic term variant identication has been ex-
tensively explored in (Jacquemin, 2001). In the sec-
tions below, we will recall briey the denitions of
the variation types we identify and give examples
each type.
2
http://www.aisee.com/
Expansions are subdivided along the grammati-
cal axis: those that aect the modier words in a
term and those that aect the head word. Modier
expansions (L-Exp) describes two elementary op-
erations: left-expansion (L-Exp) and Insertion (Ins).
They both denote the addition at the leftmost po-
sition (L-Exp) or inside a term (Insertion or Ins) of
new modier elements. For instance, \gas holding
property of dough" is a left-expansion of \gas holding
property" because by transformation to a nominal
compound structure, we obtain \dough gas holding
property". Likewise, \bread dough quality character-
istics" is an insertion variant (Ins) of \bread char-
acteristics". Head expansions (R-Exp) describes
the addition of one or more nominals in the head po-
sition of a term, thus shifting the former headword
to a modier position. Thus \frozen sweet dough
baking" is a R-Exp of \frozen sweet dough". A com-
bination of the two expansion types yield left-right
expansion (LR-Exp) in that it describes addition of
words both in the modier and head positions. For
example, the relation between \nonstarch polysac-
charide" and \functional property of rye nonstarch
polysaccharide" (\rye nonstarch polysaccharide func-
tional property"). These relations are constrained in
that the added or inserted words have to be con-
tiguous, otherwise, we may not have the expected
semantic relations. Only nominal elements are con-
sidered (nouns, adjectives).
Substitutions are also dened along the gram-
matical axis to yield two sub-types : modier and
head substitution. Modier substitution (M-Sub)
describes the replacing of one modier word in term
t
1
by another word in term t
2
. Thus \bread dough
leavening" is a modier substitution (M-Sub) of
\composite dough leavening". Head substitution
(H-Sub) relates terms which share the same modi-
ers but dierent heads : \eect of xanthan gum"
and \addition of xanthan gum". These relations
are equally constrained in that they can only link
terms of equal length where one and only one item
is dierent, thus guaranteeing the interpretability of
the relations. Substitutions, since they denote non-
directional relations between terms of equal length,
engender symmetrical relations between terms on the
formal level: t
1
t
2
. Their transitive closure cre-
ates classes of terms. For instance, a set of terms
related by modier substitution (M-Sub) seem to
point to a class of \properties/attributes" shared by
a same concept (the head word) as in \bread texture,
endosperm texture, good texture" for binary terms
and \sour corn bread, sour dough bread, sour maize
bread" for ternary terms. In this last case, the chang-
ing properties seem to point to the possible special-
izations (\sour-") of the concept (\bread"). Head
substitution on the other hand gathers together sets
of terms that share the same \properties" (the mod-
ier words), thus creating a class of \concepts". For
instance, the set of term variants \frozen dough bak-
ing, frozen dough characteristics, frozen dough prod-
ucts". The common attribute is \frozen dough",
shared by this class of concepts \products, char-
acteristics, baking". (Ibekwe-SanJuan, 1998) al-
ready put forward the idea of these semantic rela-
tions and (Jacquemin, 1995) reported similar con-
ceptual relations for his insertion and coordination
variants.
4.2 Variant Clustering Module
The second module of TermWatch is a hierarchical
clustering algorithm, CPCL (Classication by Pref-
erential Clustered Link), which clusters terms based
on the variations described above. The six elemen-
tary variation relations are represented as a di-graph.
Clustering is a two-stage process. First the algorithm
builds connected components using a subset of the
variation relations, usually the modier relations (L-
Exp, Ins, M-Sub), these are the COMP relations.
The transitive closure COMP* of COMP partitions
the whole set of terms into components. These con-
nected components are sub-graphs of term variants
that share the same headword. At the second stage,
the connected components are clustered into classes
using the head relations (R-Exp, LR-Exp, H-sub),
this subset of relations is called CLAS. At this stage,
components whose terms are in one of the CLAS re-
lations are grouped basing on an edge dierentiation
coe?cient computed thus:
d
ij
=
X
R2CLAS
n
R
(i; j)
jRj
where CLAS is the set of binary head relations
(Exp D, Exp GD, Sub C), and n
R
(i; j) is the num-
ber of variants of type R between components i and
j. This coe?cient is higher when terms of two com-
ponents share many CLAS relations of a rare type
in the corpus. Components with the highest d
ij
are
clustered rst. The CPCL algorithm can be iterated
several times to suit the user's requirement or un-
til it converges. This means that the user is free to
either set the number of iterations or leave the algo-
rithm to do all the iterations until convergence. The
user only has to specify which set of variations s/he
wants to play the COMP and the CLAS role. In
theory, this distinction is already made in the sys-
tem but the user can change it. On the linguistic
Component 1 component 2
bromate measurement dough stickiness
dough stickiness measurement diminished dough stickiness
dough surface stickiness measurement dough increase stickiness
stickiness measurement intense dough stickiness
measure surface stickiness
soft red winter wheat lines dough stickiness
surface stickiness
wheat dough stickiness
wheat dough surface stickiness
Table 1: Example of a class built by TermWatch.
level, a class contains at least two connected com-
ponents, each comprising of sets of term variants
around the same head word. Class here should be
understood in a formal way: it corresponds to group-
ings of connected components resulting from a hier-
archical clustering algorithm. They are not strictly
dened semantically. Although, we nd semanti-
cally related terms within these classes, the exact
semantic relations involved between pairs of terms
are not explicitly tagged. So on the semantic level,
a class here comprises subsets of term variants re-
ecting, \class of" relations (engendered by substitu-
tions) and \hypernym/hyponym" relations (engen-
dered by modier expansions). For instance, Table
1 displays the term variants found in one class.
This class was built around two components, one
structured around the concept of \stickiness mea-
surement" (most frequent repeated segment) and the
other around the concept of \dough stickiness". We
can observe the COMP relations between term vari-
ants inside each component. The variants that ini-
tiated this class formation are in italics (the ones
sharing CLAS relations).
The TermWatch programs have been implemented
in the AWK language and can run on a Unix or
Windows system. The system is computationally
tractable and processing time is quite acceptable for
real-life applications. For instance, it took 40 sec-
onds on a normal PC running Windows to process a
graph of 3651 term variants and to load the results
onto the Aisee graphic interface. 33 classes of vari-
able sizes were produced at the 3rd iteration of the
clustering algorithm. The smallest class had 4 terms
and the biggest 218 terms! So class size depends very
much on the number and types of variation relations
present in the initial graph.
3
3
TermWatch was initially designed as a scientic and
technology watch system, hence the choices made in
syntactic term variant denitions, the clustering algo-
rithm and visualization mechanisms are tightly related
to this application. A WWW interface is currently under
construction to facilitate the return to the source texts
5 Combining the two systems
The two outlined methodologies use the existence
of syntactic variation between multi-word terms to
structure the terminology as a whole. However, each
approach reects a dierent aspect of this structure.
The ExtrAns approach is designed to identify
explicit relations between terms. The results are
(relatively) small synsets and a hierarchy of types.
For TermWatch, the organizing principle results in
larger classes of terms built around dierent head
words related by syntactic substitution or expansion.
Whilst, not specically targeting semantic relations
the classes do exhibit related terms. Some of these
relations are denable within the classes. For exam-
ple, the class presented in Table 1 contains all of the
hyponyms of \stickiness" identied in ExtrAns (g-
ure 2), but the relations are not rendered explicit in
the class. Also the class contains other terms not
involved in a specic hyponymy relation.
The utility of the classes is in capturing more
\fuzzy" relations between terms whilst avoiding the
problems of trying to dene the relation. For exam-
ple, how can the relation between t
1
: \frozen sweet
dough" and t
2
: \frozen sweet dough baking" be de-
ned ? The most obvious candidate is a part whole
relation but this is defendable only on a formal level:
i.e. t
1
is a subset of t
2
, but does that make t
1
really
a part of t
2
in any semantic sense? In other words, is
\frozen sweet dough" really a part of \frozen sweet
dough baking"?
The TermWatch system does not grapple with this
issue. The interest of these classes for the QA task is
that they exhibit these fuzzy relations. These repre-
sent wider categories of terms to be used for specic
search types. For example, when looking for gen-
eral information on \frozen sweet dough" a user may
well be interested in \baking" it, but when extract-
ing specic information on the same term the rela-
tion is inappropriate. TermWatch was designed orig-
inally for scientic and technological watch (STW).
through hyperlinks.
Term
Extraction
Term
Structure
ExtrAns
Document
synonymy
hyponymy
TermWatch
WordNet
Figure 3: Using the structures in ExtrAns
In this type of application, the expert is less inter-
ested in strict semantic relations between terms in
a taxonomy but more in capturing the association
of research topics in his/her eld. So such \fuzzy"
relations become all important.
Currently ExtrAns uses the synsets and hyponymy
hierarchy during the `matching' of queries against
documents. However, when this fails to locate any-
thing the process is nished without providing users
with any information or any further access into the
domain. What is required is to \relax" the denition
of semantic relation, or facilitate domain investiga-
tion through visualization of the terminology.
The combination of the two methodologies (de-
picted in gure 3) results in a terminology structured
along four levels of granularity. This structure repre-
sents MWTs that are: Strictly synonymous, Word-
Net related, Hierarchy of types and Clustered by
Class.
These levels can be eectively exploited in lo-
cating answers. First, extract potential answers
that involve strictly synonymous MWTs. Second,
look for potential answers with WordNet related
MWTs. Third, try hypernyms/hyponyms of the
search MWT. Finally, allow the user to browse the
classes of MWTs to identify which are of interest in
answer to the question.
TermWatch allows a user-friendly navigation of
the clustering results. Classes are mapped out as
nodes connected by edges whose length denote the
distance between them. The longer the length, the
farther the classes are from one another and thus the
lower their edge coe?cient (d
ij
). The Aisee inter-
face oers standard navigation functions which allow
users to unfold a class into its components and then
into the terms they contain. It thus reects the three-
level structuring eected by the TermWatch mod-
ules.
Figure 4 gives the graphic representation of results
obtained on the corpus. Note that only classes linked
to others are shown in this gure. Classes are la-
beled automatically by the most active term. The
layout points out central or core classes, here classes
(32, 22) which can represent the dominant terminol-
ogy, and by extension, core research topics in the
eld. This layout also brings out interesting con-
gurations like complete graphs and linear graphs.
Complete graphs. The four classes labeled by the
terms \dough behaviour" (32), \wheat our bread"
(29), \wheat bran" (6) and \dough improver" (20)
form a complete graph. They are all linked by sym-
metrical head substitution relations. We found in
these classes term variants like \wheat our dough"
(class 32); \wheat our bread" (class 29), \wheat
our supplementation, wheat our blend, wheat our
fractionation" (class 6), and nally \wheat our com-
position" (class 20). This complete graph is thus
structured around the two modier elements \wheat
our" which can reect a property shared by the
concepts of these four classes. Linear graphs. The
anti-symmetrical relations engendered by insertions
and expansions generate linear graphs, i.e., chains of
relatively long vertices starting from a central class to
the border of the graph. The visualization tool natu-
rally aligns the elements of these linear graphs, thus
highlighting them. For instance, the linear graph
formed by the three classes \dough behaviour" (32),
\frozen dough baking" (10), \dough procedure" (21)
is structured around the set of variants: \frozen
sweet dough (32) ! \frozen sweet dough baking (10)
 \frozen dough baking" (10). The last term \frozen
dough baking" establishes a strong variation relation
with terms in the third class (21) in which we found
the modiers \frozen dough" associated to three dif-
ferent head words: \characteristic, method, prod-
uct".
Given that the syntactic variations which helped
group terms give o semantic links, and given our
restricted denitions of variation relation (see 4.1), a
user seeking information can be oered these class's
contents at this stage in order to see loosely related
terms semantically which a terminological resource
(thesaurus) or WordNet may not have identied.
For instance, in the class shown in Table 1, many
of the terms may not have been related by any se-
mantic relation in WordNet (bromate measurement
and dough stickiness) because none of the head or
the modier words are in any synsets. The clus-
tering algorithm, brings these terms in one class
because \bromate measurement" is a modier sub-
stitution of \stickiness measurement" which is why
they are in the same component. Both tell us some-
thing about \measurement (or rather about measur-
able objects). On the other hand, \dough surface
stickiness measurement", in the same component, is
a left expansion of \stickiness measurement". The
Figure 4: Navigating the clusters of MWTs
two could point to a `hypernym/hyponym' relation.
Thus, from link to link, these terms are connected
to terms of the second component owing to the one
anti-symmetrical link between \dough surface stick-
iness measurement" and \surface stickiness".
From this kind of investigation, a user can choose
the MWTs of interest. This set then becomes the
basis of a second round of answering specic ques-
tions. In this way the system can provide high preci-
sion access to answers, whilst facilitating navigation
through a domain of unfamiliar MWTs.
6 Related Work
The importance of multi-word expressions (MWE)
in various natural language tasks such as auto-
matic indexing, machine translation, information
retrieval/extraction and technology watch need no
longer be proved.
The Multi-word Expression Project aims at study-
ing the properties of a wide range of expressions
including collocations, metaphors and terminology.
The motivation is in explicitly dening the character-
istics of such phrases. The results of the project will
suggest e?cient strategies for overcoming the prob-
lems MWEs cause for NLP applications (Sag et al,
2002)
Much work has been dedicated to the process of
nominal compounding (Levi, 1979) and the seman-
tic interpretation of nominal compounds (Downing,
1977) (Finin, 1980). Other works have addressed
the specic problem of extracting nominal multi-
word expressions for IR applications (Evans et al,
1992) (Smeaton and Sheridan, 1992) (Smadja, 1993)
or of representing them semantically in order to en-
hance IR systems (Popowich et al, 1992) (Gay and
Croft, 1990).
Many systems are dedicated towards structur-
ing terminology for ontology building or terminol-
ogy knowledge base construction (Aussenac-Gilles
et al, 2003). These approaches use the corpus
to identify linguistic markers which in turn point
to certain semantic relations between terms (hy-
pernym/hyponym, synonyms, meronyms). The ap-
proaches we describe are dierent in that relations
are gained through syntactic variations between the
terms.
Active research by the computational terminol-
ogy community (Jacquemin, 2001) (Bourigault et
al., 2001) (Pearson, 1998) has highlighted the im-
portance of discourse as a means of capturing the
essence of terms, hence as a good basis for struc-
turing them. Jacquemin's extensive study has also
highlighted the fact that terms are given to varia-
tions in discourse, so any endeavor to capture the re-
lations between terminological units should integrate
the variation paradigm.
7 Conclusions
Dening and identifying semantic relations between
terms is problematic but can be utilized as part of the
QA process. However, clustering MWTs based on
syntactic variation uncovers classes of terms which
reect more \fuzzy" semantic relations. These are
ideally suited to enabling navigation through the do-
main identifying terms to be used in the Question
Answering process, oering sophisticated access to a
domain. The resulting term structure can be utilized
as a computational thesaurus or incorporated as part
of a larger domain ontology.
References
N. Aussenac-Gilles, B. Biebow, and S. Szulman.
2003. D'une methode a un guide pratique de
modelisation de connaissances a partir de textes.
In Proc. of the 5th Conference on Terminologie
et Intelligence Articielle, Strasbourg, March 31 -
April 1.
K. Barker and S. Szpakowicz. 1998. Semi-Automatic
Recognition of Noun Modier Relationships. In
Proc. of COLING-ACL98, Montreal, Quebec,
Canada, August 10-14.
D. Bourigault, C. Jacquemin, and M-C. L'Homme,
editors. 2001. Recent Advances in Computational
Terminology, volume 2. John Benjamins.
J. Dowdall, M. Hess, N. Kahusk, K. Kaljurand,
M. Koit, F. Rinaldi, and K. Vider. 2002. Tech-
nical Terminology as a Critical Resource. In Proc.
of LREC-02, Las Palmas, 29 { 31 May.
P. Downing. 1977. On the creation and use of english
compound nouns. Language, (53):810 { 842.
D.A Evans, R.G. Leerts, G. Grefenstette, S.K. Han-
derson, W.R. Hersh, and A.A.Archbold. 1992.
CLARIT TREC design, experiments and results.
Technical report, Carnegie Mellon University.
T. Finin. 1980. The semantic interpretation of nom-
inal compounds. In Proceedings "Articial Intelli-
gence, pages 310 { 312. Stanford.
L.S. Gay and W.B. Croft. 1990. Interpreting nomi-
nal compounds for information retrieval. Informa-
tion Processing and Management, 26(1):21 { 38.
F. Ibekwe-SanJuan and E. SanJuan. 2003. From
term variants to research topics. Journal of
Knowledge Organization (ISKO), special issue on
Human Language Technology, 29(3/4).
F. Ibekwe-SanJuan. 1998. Terminological variation,
a means of identifying research topics from texts.
In Proc. of Joint ACL-COLING'98, pages 564 {
570, Quebec, 10-14 August.
F. Ibekwe-SanJuan. 2001. Extraction termi-
nologique avec intex. In Proc.of the 4th Annual
INTEX Workshop, Bordeaux, 10-11 June.
P. Isabelle. 1984. Another look at nominal com-
pounds. In Proc. of the 10th International Con-
ference on Computational Linguistics (COLING
'84), pages 509{516, Stanford, USA.
C. Jacquemin. 1995. A symbolic and surgical ac-
quisition of terms through variation. In Proc. of
IJCAI95, Montreal.
C. Jacquemin. 2001. Spotting and discovering terms
through Natural Language Processing. MIT Press.
K. Kageura. 2002. The dynamics of Terminology: A
descriptive theory of term formation and termino-
logical growth. John Benjamins, Amsterdam.
J. N. Levi. 1979. The syntax and semantics of com-
plex nominals. Academic press, New York.
J. Pearson. 1998. Terms in Context. John Ben-
jamins, Amsterdam.
F. Popowich, P. Mcfetridge, D. Fass, and G. Hall.
1992. Processing complex noun phrases in a natu-
ral language interface to a statistical database. In
Proceedings COLING'92, pages 46 { 51, Nantes,
August 23 { 28.
F. Rinaldi, M. Hess, D. Molla, R. Schwitter, J. Dow-
dall, G. Schneider, and R. Fournier. 2002. Answer
Extraction in Technical Domains. In Proc. of CI-
CLing 2002, Mexico City, February.
I. A. Sag, T. Baldwin, F. Bond, A. Copestake, and
D. Flickinger. 2002. Multiword Expressions: a
Pain in the Neck for NLP. In Proc. of CICLing
2002, Mexico City, February.
M. Silberztein. 1993. Dictionnaires Electroniques
et Analyse Lexicale du Francais - Le Systeme IN-
TEX. Masson, Paris.
F. Smadja. 1993. Retrieving collocations from text:
Xtract. Computational Linguistics, (19):143 { 177.
A. F. Smeaton and P. Sheridan. 1992. The appli-
cation of morpho-syntactic language processing to
eective phrase-matching. Information Processing
and Management, 28(3):349 { 369.
Coling 2010: Poster Volume, pages 1059?1067,
Beijing, August 2010
Multilingual Summarization Evaluation without Human Models
Horacio Saggion
TALN - DTIC
Universitat Pompeu Fabra
horacio.saggion@upf.edu
Juan-Manuel Torres-Moreno
LIA/Universite? d?Avignon
E?cole Polytechnique de Montre?al
juan-manuel.torres@univ-avignon.fr
Iria da Cunha
IULA/Universitat Pompeu Fabra
LIA/Universite? d?Avignon
iria.dacunha@upf.edu
Eric SanJuan
LIA/Universite? d?Avignon
eric.sanjuan@univ-avignon.fr
Patricia Vela?zquez-Morales
VM Labs
patricia vazquez@yahoo.com
Abstract
We study correlation of rankings of text
summarization systems using evaluation
methods with and without human mod-
els. We apply our comparison frame-
work to various well-established content-
based evaluation measures in text sum-
marization such as coverage, Responsive-
ness, Pyramids and ROUGE studying their
associations in various text summarization
tasks including generic and focus-based
multi-document summarization in English
and generic single-document summariza-
tion in French and Spanish. The research
is carried out using a new content-based
evaluation framework called FRESA to
compute a variety of divergences among
probability distributions.
1 Introduction
Text summarization evaluation has always been a
complex and controversial issue in computational
linguistics. In the last decade, significant ad-
vances have been made in the summarization eval-
uation field. Various evaluation frameworks have
been established and evaluation measures devel-
oped. SUMMAC (Mani et al, 2002), in 1998,
provided the first system independent framework
for summary evaluation; the Document Under-
standing Conference (DUC) (Over et al, 2007)
was the main evaluation forum from 2000 until
2007; nowadays, the Text Analysis Conference
(TAC)1 provides a forum for assessment of dif-
ferent information access technologies including
text summarization.
Evaluation in text summarization can be extrin-
sic or intrinsic (Spa?rck-Jones and Galliers, 1996).
In an extrinsic evaluation, the summaries are as-
sessed in the context of an specific task a human
or machine has to carry out; in an intrinsic eval-
uation, the summaries are evaluated in reference
to some ideal model. SUMMAC was mainly ex-
trinsic while DUC and TAC followed an intrinsic
evaluation paradigm. In order to intrinsically eval-
uate summaries, the automatic summary (peer)
has to be compared to a model summary or sum-
maries. DUC used an interface called SEE to al-
low human judges compare a peer summary to a
model summary. Using SEE, human judges give a
coverage score to the peer summary representing
the degree of overlap with the model summary.
Summarization systems obtain a final coverage
score which is the average of the coverage?s scores
associated to their summaries. The system?s cov-
erage score can then be used to rank summariza-
tion systems. In the case of query-focused sum-
marization (e.g. when the summary has to re-
spond to a question or set of questions) a Respon-
siveness score is also assigned to each summary
which indicates how responsive the summary is to
the question(s).
Because manual comparison of peer summaries
with model summaries is an arduous and costly
1http://www.nist.gov/tac
1059
process, a body of research has been produced in
the last decade on automatic content-based eval-
uation procedures. Early studies used text simi-
larity measures such as cosine similarity (with or
without weighting schema) to compare peer and
model summaries (Donaway et al, 2000), vari-
ous vocabulary overlap measures such as set of
n-grams overlap or longest common subsequence
between peer and model have also been pro-
posed (Saggion et al, 2002; Radev et al, 2003).
The Bleu machine translation evaluation measure
(Papineni et al, 2002) has also been tested in
summarization (Pastra and Saggion, 2003). The
DUC conferences adopted the ROUGE package
for content-based evaluation (Lin, 2004). It im-
plements a series of recall measures based on n-
gram co-occurrence statistics between a peer sum-
mary and a set of model summaries. ROUGE mea-
sures can be used to produce systems ranks. It
has been shown that system rankings produced
by some ROUGE measures (e.g., ROUGE-2 which
uses bi-grams) correlate with rankings produced
using coverage. In recent years the Pyramids eval-
uation method (Nenkova and Passonneau, 2004)
was introduced. It is based on the distribution
of ?content? in a set of model summaries. Sum-
mary Content Units (SCUs) are first identified in
the model summaries, then each SCU receives
a weight which is the number of models con-
taining or expressing the same unit. Peer SCUs
are identified in the peer, matched against model
SCUs, and weighted accordingly. The Pyramids
score given to the peer is the ratio of the sum
of the weights of its units and the sum of the
weights of the best possible ideal summary with
the same number of SCUs as the peer. The Pyra-
mids scores can be used for ranking summariza-
tion systems. Nenkova and Passonneau (2004)
showed that Pyramids scores produced reliable
system rankings when multiple (4 or more) mod-
els were used and that Pyramids rankings cor-
relate with rankings produced by ROUGE-2 and
ROUGE-SU2 (i.e. ROUGE with skip bi-grams).
Still this method requires the creation of models
and the identification, matching, and weighting of
SCUs in both models and peers.
Donaway et al (2000) put forward the idea of
using directly the full document for comparison
purposes, and argued that content-based measures
which compare the document to the summary may
be acceptable substitutes for those using model
summaries. A method for evaluation of sum-
marization systems without models has been re-
cently proposed (Louis and Nenkova, 2009). It is
based on the direct content-based comparison be-
tween summaries and their corresponding source
documents. Louis and Nenkova (2009) evalu-
ated the effectiveness of the Jensen-Shannon (Lin,
1991b) theoretic measure in predicting systems
ranks in two summarization tasks query-focused
and update summarization. They have shown that
ranks produced by Pyramids and ranks produced
by the Jensen-Shannon measure correlate. How-
ever, they did not investigate the effect of the mea-
sure in past summarization tasks such as generic
multi-document summarization (DUC 2004 Task
2), biographical summarization (DUC 2004 Task
5), opinion summarization (TAC 2008 OS), and
summarization in languages other than English.
We think that, in order to have a better under-
standing of document-summary evaluation mea-
sures, more research is needed. In this paper we
present a series of experiments aimed at a better
understanding of the value of the Jensen-Shannon
divergence for ranking summarization systems.
We have carried out experimentation with the
proposed measure and have verified that in cer-
tain tasks (such as those studied by (Louis and
Nenkova, 2009)) there is a strong correlation
among Pyramids and Responsiveness and the
Jensen-Shannon divergence, but as we will show
in this paper, there are datasets in which the cor-
relation is not so strong. We also present exper-
iments in Spanish and French showing positive
correlation between the Jensen-Shannon measure
and ROUGE.
The rest of the paper is organized in the follow-
ing way: First in Section 2 we introduce related
work in the area of content-based evaluation iden-
tifying the departing point for our inquiry; then in
Section 3 we explain the methodology adopted in
our work and the tools and resources used for ex-
perimentation. In Section 4 we present the experi-
ments carried out together with the results. Sec-
tion 5 discusses the results and Section 6 con-
cludes the paper.
1060
2 Related Work
One of the first works to use content-based mea-
sures in text summarization evaluation is due to
(Donaway et al, 2000) who presented an evalu-
ation framework to compare rankings of summa-
rization systems produced by recall and cosine-
based measures. They showed that there was
weak correlation between rankings produced by
recall, but that content-based measures produce
rankings which were strongly correlated, thus
paving the way for content-based measures in text
summarization evaluation.
Radev et al (2003) also compared various eval-
uation measures based on vocabulary overlap. Al-
though these measures were able to separate ran-
dom from non-random systems, no clear conclu-
sion was reached on the value of each of the mea-
sures studied.
Nowadays, a widespread summarization evalu-
ation framework is ROUGE (Lin and Hovy, 2003)
which, as we have mentioned before, offers a set
of statistics that compare peer summaries with
models. Various statistics exist depending on the
used n-gram and on the type of text processing ap-
plied to the input texts (e.g., lemmatization, stop-
word removal).
Lin et al (2006) proposed a method of evalua-
tion based on the use of ?distances? or divergences
between two probability distributions (the distri-
bution of units in the automatic summary and the
distribution of units in the model summary). They
studied two different Information Theoretic mea-
sures of divergence: the Kullback-Leibler (KL)
(Kullback and Leibler, 1951) and Jensen-Shannon
(JS) (Lin, 1991a) divergences. In this work we
use the Jensen-Shannon (JS) divergence that is
defined as follows:
DJS(P ||Q) = 12
?
w
Pw log2
2Pw
Pw +Qw
+ Qw log2
2Qw
Pw +Qw
(1)
This measure can be applied to the distribu-
tion of units in system summaries P and refer-
ence summaries Q and the value obtained used
as a score for the system summary. The method
has been tested by (Lin et al, 2006) over the
DUC 2002 corpus for single and multi docu-
ment summarization tasks showing good correla-
tion among divergence measures and both cover-
age and ROUGE rankings.
Louis and Nenkova (2009) went even further
and, as in (Donaway et al, 2000), proposed to
directly compare the distribution of words in full
documents with the distribution of words in auto-
matic summaries to derive a content-based eval-
uation measure. They found high correlation
among rankings produced using models and rank-
ings produced without models. This work is the
departing point for our inquiry into the value of
measures that do not rely on human models.
3 Methodology
The methodology of this paper mirrors the one
adopted in past work (Donaway et al, 2000;
Louis and Nenkova, 2009). Given a particular
summarization task T , p data points to be sum-
marized with input material {Ii}p?1i=0 (e.g. doc-
ument(s), questions, topics), s peer summaries
{SUMi,k}s?1k=0 for input i, and m model sum-
maries {MODELi,j}m?1j=0 for input i, we will com-
pare rankings of the s peer summaries produced
by various evaluation measures. Some measures
we use compare summaries with n out of the m
models:
MEASUREM (SUMi,k, {MODELi,j}nj=0) (2)
while other measures compare peers with all or
some of the input material:
MEASUREM (SUMi,k, I ?i) (3)
where I ?i is some subset of input Ii. The val-
ues produced by the measures for each sum-
mary SUMi,k are averaged for each system k =
0, . . . , s ? 1 and these averages are used to pro-
duce a ranking. Rankings are compared using
Spearman Rank correlation (Spiegel and Castel-
lan, 1998) used to measure the degree of associa-
tion between two variables whose values are used
to rank objects. We use this correlation to directly
compare results to those presented in (Louis and
Nenkova, 2009). Computation of correlations is
1061
done using the CPAN Statistics-RankCorrelation-
0.12 package2, which computes the rank correla-
tion between two vectors.
3.1 Tools
We carry out experimentation using a new sum-
marization evaluation framework: FRESA
?FRamework for Evaluating Summaries
Automatically? which includes document-
based summary evaluation measures based on
probabilities distribution. As in the ROUGE
package, FRESA supports different n-grams
and skip n-grams probability distributions.
The FRESA environment can be used in the
evaluation of summaries in English, French,
Spanish and Catalan, and it integrates filtering
and lemmatization in the treatment of summaries
and documents. It is developed in Perl and will be
made publicly available. We also use the ROUGE
package to compute various ROUGE statistics in
new datasets.
3.2 Summarization Tasks and Data Sets
We have conducted our experimentation with the
following summarization tasks and data sets:
Generic multi-document-summarization in En-
glish (i.e. production a short summary of a cluster
of related documents) using data fromDUC 20043
corpus task 2: 50 clusters (10 documents each) ?
294,636 words.
Focused-based summarization in English (i.e.
production a short focused multi-document sum-
mary focused on the question ?who is X??, where
X is a person?s name) using data from the DUC
2004 task 5: 50 clusters ( 10 documents each plus
a target person name) ? 284,440 words.
Update-summarization task that consists of cre-
ating a summary out of a cluster of documents and
a topic. Two sub-tasks are considered here: A)
an initial summary has to be produced based on
an initial set of documents and topic; B) an up-
date summary has to be produced from a differ-
ent (but related) cluster assuming documents used
in A) are known. The English TAC 2008 Update
2http://search.cpan.org/?gene/
Statistics-RankCorrelation-0.12/
3http://www-nlpir.nist.gov/projects/
duc/guidelines/2004.html
Summarization dataset is used which consists of
48 topics with 20 documents each ? 36,911 words.
Opinion summarization where systems have to
analyze a set of blog articles and summarize the
opinions about a target in the articles. The TAC
2008 Opinion Summarization in English4 data set
(taken from the Blogs06 Text Collection) is used:
25 clusters and targets (i.e., target entity and ques-
tions) were used ? 1,167,735 words.
Generic single-document summarization in
Spanish using the ?Spanish Medicina Cl??nica?5
corpus which is composed of 50 biomedical ar-
ticles in Spanish, each one with its corresponding
author abstract ? 124,929 words.
Generic single document summarization in
French using the ?Canadien French Sociologi-
cal Articles? corpus from the journal Perspec-
tives interdisciplinaires sur le travail et la sante?
(PISTES)6. It contains 50 sociological articles in
French with their corresponding author abstracts
? 381,039 words.
3.3 Summarization Systems
For experimentation in the TAC and the DUC
datasets we directly use the peer summaries
produced by systems participating in the eval-
uations. For experimentation in Spanish and
French (single-document summarization) we
have created summaries at the compression rates
of the model summaries using the following
summarization systems:
? CORTEX (Torres-Moreno et al, 2002), a
single-document sentence extraction system
for Spanish and French that combines vari-
ous statistical measures of relevance (angle
between sentence and topic, various Ham-
ming weights for sentences, etc.) and applies
an optimal decision algorithm for sentence
selection;
? ENERTEX (Fernandez et al, 2007), a sum-
marizer based on a theory of textual energy;
4http://www.nist.gov/tac/data/index.
html
5http://www.elsevier.es/revistas/
ctl servlet? f=7032&revistaid=2
6http://www.pistes.uqam.ca/
1062
? SUMMTERM (Vivaldi et al, 2010), a
terminology-based summarizer that is used
for summarization of medical articles and
uses specialized terminology for scoring and
ranking sentences;
? JS summarizer, a summarization system that
scores and ranks sentences according to their
Jensen-Shannon divergence to the source
document;
? a lead-based summarization system that se-
lects the lead sentences of the document;
? a random-based summarization system that
selects sentences at random;
? the multilingual word-frequency Open Text
Summarizer (Yatsko and Vishnyakov, 2007);
? the AutoSummarize program of Microsoft
Word;
? the commercial SSSummarizer7;
? the Pertinence summarizer8;
? the Copernic summarizer9.
3.4 Evaluation Measures
The following measures derived from human
assessment of the content of the summaries are
used in our experiments:
? Coverage is understood as the degree to
which one peer summary conveys the same
information as a model summary (Over et al,
2007). Coverage was used in DUC evalua-
tions.
? Responsiveness ranks summaries in a 5-point
scale indicating how well the summary sat-
isfied a given information need (Over et al,
2007). It is used in focused-based summa-
rization tasks. Responsiveness was used in
DUC-TAC evaluations.
7http://www.kryltech.com/summarizer.
htm
8http://www.pertinence.net
9http://www.copernic.com/en/products/
summarizer
? Pyramids (briefly introduced in Section 1)
(Nenkova and Passonneau, 2004) is a content
assessment measure which compares content
units in a peer summary to weighted content
units in a set of model summaries. Pyramids
is the adopted metric for content-based eval-
uation in the TAC evaluations.
For DUC and TAC datasets the values of these
measures are available and we used them directly.
We used the following automatic evaluation
measures in our experiments:
? We use the Rouge package (Lin, 2004) to
compute various statistics. For the experi-
ments presented here we used uni-grams, bi-
grams, and the skip bi-grams with maximum
skip distance of 4 (ROUGE-1, ROUGE-2 and
ROUGE-SU4). ROUGE is used to compare a
peer summary to a set of model summaries
in our framework.
? Jensen-Shannon divergence formula given in
Equation 1 is implemented in our FRESA
package with the following specification for
the probability distribution of words w.
Pw =
CTw
N (4)
Qw =
{
CSw
NS if w ? S
CTw+?
N+??B elsewhere
(5)
Where P is the probability distribution of
words w in text T and Q is the probabil-
ity distribution of words w in summary S;
N is the number of words in text and sum-
mary N = NT + NS , B = 1.5|V |, CTw is
the number of words in the text and CSw is
the number of words in the summary. For
smoothing the summary?s probabilities we
have used ? = 0.005.
4 Experiments and Results
We first replicated the experiments presented in
(Louis and Nenkova, 2009) to verify that our im-
plementation of JS produced correlation results
compatible with that work. We used the TAC
2008 Update Summarization data set and com-
puted JS and ROUGE measures for each peer
1063
summary. We produced two system rankings (one
for each measure), which were compared to rank-
ings produced using the manual Pyramids and Re-
sponsiveness scores. Spearman correlations were
computed among the different rankings. The re-
sults are presented in Table 1. These results con-
firm a high correlation among Pyramids, Respon-
siveness, and JS. We also verified high corre-
lation between JS and ROUGE-2 (0.83 Spearman
correlation, not shown in the table) in this task and
dataset.
Measure Pyr. p-value Resp. p-value
ROUGE-2 0.96 p < 0.005 0.92 p < 0.005
JS 0.85 p < 0.005 0.74 p < 0.005
Table 1: Spearman system rank correlation of
content-based measures in TAC 2008 Update
Summarization task
Then, we experimented with data from DUC
2004, TAC 2008 Opinion Summarization pilot
and with single document summarization in Span-
ish and French. In spite of the fact that the exper-
iments for French and Spanish corpora use less
data points (i.e., less summarizers per task) than
for English, results are still quite significant.
For DUC 2004, we computed the JS measure
for each peer summary in tasks 2 and 5 and we
used JS and the official ROUGE, coverage, and
Responsiveness scores to produce systems? rank-
ings. The various Spearman?s rank correlation
values for DUC 2004 are presented in Tables 2
(for task 2) and 3 (for task 5). For task 2, we have
verified a strong correlation between JS and cov-
erage. For task 5, the correlation between JS and
coverage is weak, and the correlation between JS
and Responsiveness weak and negative.
Measure Cov. p-value
ROUGE-2 0.79 p < 0.0050
JS 0.68 p < 0.0025
Table 2: Spearman system rank correlation of
content-based measures with coverage in DUC
2004 Task 2
Although the Opinion Summarization task is a
new type of summarization task and its evaluation
is a complicated issue, we have decided to com-
pare JS rankings with those obtained using Pyra-
Measure Cov. p-value Resp. p-value
ROUGE-2 0.78 p < 0.001 0.44 p < 0.05
JS 0.40 p < 0.050 -0.18 p < 0.25
Table 3: Spearman system rank correlation of
content-based measures in DUC 2004 Task 5
mids and Responsiveness in TAC 2008. Spear-
man?s correlation values are listed in Table 4. As
can be seen, there is weak and negative correla-
tion of JS with both Pyramids and Responsive-
ness. Correlation between Pyramids and Respon-
siveness rankings is high for this task (0.71 Spear-
man?s correlation value).
Measure Pyr. p-value Resp. p-value
JS -0.13 p < 0.25 -0.14 p < 0.25
Table 4: Spearman system rank correlation of
content-based measures in TAC 2008 Opinion
Summarization task
For experimentation in Spanish and French, we
have run 11 multi-lingual summarization systems
over each of the documents in the two corpora,
producing summaries at a compression rate close
to the compression rate of the provided authors?
abstracts. We have computed JS and ROUGE
measures for each summary and we have aver-
aged the measure?s values for each system. These
averages were used to produce rankings per each
measure. We computed Spearman?s correlations
for all pairs of rankings. Results are presented in
Tables 5-6. All results show medium to strong
correlation between JS and ROUGE measures.
However the JS measure based on uni-grams has
lower correlation than JSs which use n-grams of
higher order.
5 Discussion
The departing point for our inquiry into text sum-
marization evaluation has been recent work on the
use of content-based evaluation metrics that do
not rely on human models but that compare sum-
mary content to input content directly (Louis and
Nenkova, 2009). We have some positive and some
negative results regarding the direct use of the full
document in content-based evaluation. We have
verified that in both generic muti-document sum-
1064
Measure ROUGE-1 p-value ROUGE-2 p-value ROUGE-SU4 p-value
JS 0.56 p < 0.100 0.46 p < 0.100 0.45 p < 0.200
JS2 0.88 p < 0.001 0.80 p < 0.002 0.81 p < 0.005
JS4 0.88 p < 0.001 0.80 p < 0.002 0.81 p < 0.005
JSM 0.82 p < 0.005 0.71 p < 0.020 0.71 p < 0.010
Table 5: Spearman system rank correlation of content-based measures with ROUGE in the Medicina
Clinica Corpus (Spanish)
Measure ROUGE-1 p-value ROUGE-2 p-value ROUGE-2 p-value
JS 0.70 p < 0.050 0.73 p < 0.05 0.73 p < 0.500
JS2 0.93 p < 0.002 0.86 p < 0.01 0.86 p < 0.005
JS4 0.83 p < 0.020 0.76 p < 0.05 0.76 p < 0.050
JSM 0.88 p < 0.010 0.83 p < 0.02 0.83 p < 0.010
Table 6: Spearman system rank correlation of content-based measures with ROUGE in the PISTES
Sociological Articles Corpus (French)
marization and in topic-based multi-document
summarization in English correlation among mea-
sures that use human models (Pyramids, Respon-
siveness, and ROUGE) and a measure that does
not use models (the Jensen Shannon divergence)
is strong. We have found that correlation among
the same measures is weak for summarization of
biographical information and summarization of
opinions in blogs. We believe that in these cases
content-based measures should consider in addi-
tion to the input document, the summarization
task (i.e. its text-based representation) to better
assess the content of the peers, the task being a
determinant factor in the selection of content for
the summary. Our multi-lingual experiments in
generic single-document summarization confirm a
strong correlation among the Jensen-Shannon di-
vergence and ROUGE measures. It is worth not-
ing that ROUGE is in general the chosen frame-
work for presenting content-based evaluation re-
sults in non-English summarization. For the ex-
periments in Spanish, we are conscious that we
only have one model summary to compare with
the peers. Nevertheless, these models are the cor-
responding abstracts written by the authors of the
articles and this is in fact the reason for choosing
this corpus. As the experiments in (da Cunha et
al., 2007) show, the professionals of a specialized
domain (as, for example, the medical domain)
adopt similar strategies to summarize their texts
and they tend to choose roughly the same content
chunks for their summaries. Because of this, the
summary of the author of a medical article can be
taken as reference for summaries evaluation. It is
worth noting that there is still debate on the num-
ber of models to be used in summarization evalu-
ation (Owkzarzak and Dang, 2009). In the French
corpus PISTES, we suspect the situation is similar
to the Spanish case.
6 Conclusions and Future Work
This paper has presented a series of experiments
in content evaluation in text summarization to as-
sess the value of content-based measures that do
not rely on the use of model summaries for com-
parison purposes. We have carried out exten-
sive experimentation with different summariza-
tion tasks drawing a clearer picture of tasks where
the measures could be applied. This paper makes
the following contributions:
? We have shown that if we are only interested
in ranking summarization systems according
to the content of their automatic summaries,
there are tasks where models could be sub-
stituted by the full document in the computa-
tion of the Jensen-Shannon divergence mea-
sure obtaining reliable rankings. However,
we have also found that the substitution of
models by full-documents is not always ad-
visable. We have found weak correlation
among different rankings in complex sum-
marization tasks such as the summarization
of biographical information and the summa-
1065
Measure ROUGE-1 p-value ROUGE-2 p-value ROUGE-2 p-value
JS 0.83 p < 0.002 0.66 p < 0.05 0.741 p < 0.01
JS2 0.80 p < 0.005 0.59 p < 0.05 0.68 p < 0.02
JS4 0.75 p < 0.010 0.52 p < 0.10 0.62 p < 0.05
JSM 0.85 p < 0.002 0.64 p < 0.05 0.74 p < 0.01
Table 7: Spearman system rank correlation of content-based measures with ROUGE in the RPM2 Cor-
pus (French)
rization of opinions about an ?entity?.
? We have also carried out large-scale exper-
iments in Spanish and French which show
positive medium to strong correlation among
system?s ranks produced by ROUGE and di-
vergence measures that do not use the model
summaries.
? We have also presented a new framework,
FRESA, for the computation of measures
based on Jensen-Shannon divergence. Fol-
lowing the ROUGE approach, FRESA imple-
ments word uni-grams, bi-grams and skip n-
grams for the computation of divergences.
The framework is being made available to the
community for research purposes.
Although we have made a number of contribu-
tions, this paper leaves many questions open that
need to be addressed. In order to verify correlation
between ROUGE and JS, in the short term we in-
tend to extend our investigation to other languages
and datasets such as Portuguese and Chinese for
which we have access to data and summarization
technology. We also plan to apply our evaluation
framework to the rest of the DUC and TAC sum-
marization tasks to have a full picture of the corre-
lations among measures with and without human
models. In the long term we plan to incorporate a
representation of the task/topic in the computation
of the measures.
Acknowledgements
We thank three anonymous reviewers for their
valuable and enthusiastic comments. Horacio
Saggion is grateful to the Programa Ramo?n y Ca-
jal from the Ministerio de Ciencia e Innovacio?n,
Spain and to a Comenc?a grant from Universitat
Pompeu Fabra (COMENC?A10.004). This work
is partially supported by a postdoctoral grant (Na-
tional Program for Mobility of Research Human
Resources; National Plan of Scientific Research,
Development and Innovation 2008-2011) given to
Iria da Cunha by the Ministerio de Ciencia e In-
novacio?n, Spain.
References
da Cunha, Iria, Leo Wanner, and M. Teresa Cabre?.
2007. Summarization of specialized discourse: The
case of medical articles in spanish. Terminology,
13(2):249?286.
Donaway, Robert L., Kevin W. Drummey, and
Laura A. Mather. 2000. A comparison of rank-
ings produced by summarization evaluation mea-
sures. In NAACL-ANLP 2000 Workshop on Au-
tomatic Summarization, pages 69?78, Morristown,
NJ, USA. ACL.
Fernandez, Silvia, Eric SanJuan, and Juan-Manuel
Torres-Moreno. 2007. Textual Energy of Associa-
tive Memories: performants applications of Enertex
algorithm in text summarization and topic segmen-
tation. In MICAI?07, pages 861?871.
Kullback, S. and R.A. Leibler. 1951. On information
and sufficiency. Annals of Mathematical Statistics,
22(1):79?86.
Lin, C.-Y. and E. Hovy. 2003. Automatic evaluation
of summaries using n-gram co-occurrence statistics.
In Proceedings of HLT-NAACL 2003, pages 71?78,
Morristown, NJ, USA. ACL.
Lin, Chin-Yew, Guihong Cao, Jianfeng Gao, and Jian-
Yun Nie. 2006. An information-theoretic approach
to automatic evaluation of summaries. In Confer-
ence on Human Language Technology Conference
of the North American Chapter of the Association
of Computational Linguistics, pages 463?470, Mor-
ristown, NJ, USA. ACL.
Lin, J. 1991a. Divergence measures based on the
shannon entropy. IEEE Transactions on Informa-
tion Theory, 37(145-151).
1066
Lin, Jianhua. 1991b. Divergence measures based on
the shannon entropy. IEEE Transactions on Infor-
mation theory, 37:145?151.
Lin, Chin-Yew. 2004. ROUGE: A Package for
Automatic Evaluation of Summaries. In Marie-
Francine Moens, Stan Szpakowicz, editor, Text
Summarization Branches Out: ACL-04 Workshop,
pages 74?81, Barcelona, Spain, July.
Louis, Annie and Ani Nenkova. 2009. Automati-
cally Evaluating Content Selection in Summariza-
tion without Human Models. In Conference on Em-
pirical Methods in Natural Language Processing,
pages 306?314, Singapore, August. ACL.
Mani, I., G. Klein, D. House, L. Hirschman, T. Firmin,
and B. Sundheim. 2002. Summac: a text summa-
rization evaluation. Natural Language Engineering,
8(1):43?68.
Nenkova, Ani and Rebecca Passonneau. 2004. Eval-
uating Content Selection in Summarization: The
Pyramid Method. In Proceedings of NAACL-HLT
2004.
Over, Paul, Hoa Dang, and Donna Harman. 2007. Duc
in context. Information Processing & Management,
43(6):1506?1520.
Owkzarzak, Karolina and Hoa Trang Dang. 2009.
Evaluation of automatic summaries: Metrics under
varying data conditions. In Proceedings of the 2009
Workshop on Language Generation and Summari-
sation (UCNLG+Sum 2009), pages 23?30, Suntec,
Singapore, August. ACL.
Papineni, K., S. Roukos, T. Ward, , and W. J. Zhu.
2002. BLEU: a method for automatic evaluation
of machine translation. In ACL?02: 40th Annual
meeting of the Association for Computational Lin-
guistics, pages 311?318.
Pastra, K. and H. Saggion. 2003. Colouring sum-
maries Bleu. In Proceedings of Evaluation Initia-
tives in Natural Language Processing, Budapest,
Hungary, 14 April. EACL.
Radev, Dragomir R., Simone Teufel, Horacio Sag-
gion, Wai Lam, John Blitzer, Hong Qi, Arda C?elebi,
Danyu Liu, and Elliott Dra?bek. 2003. Evaluation
challenges in large-scale document summarization.
In ACL, pages 375?382.
Saggion, H., D. Radev, S. Teufel, and W. Lam. 2002.
Meta-evaluation of Summaries in a Cross-lingual
Environment using Content-based Metrics. In Pro-
ceedings of COLING 2002, pages 849?855, Taipei,
Taiwan, August 24-September 1.
Spa?rck-Jones, Karen and Julia Rose Galliers, editors.
1996. Evaluating Natural Language Processing
Systems, An Analysis and Review, volume 1083 of
Lecture Notes in Computer Science. Springer.
Spiegel, S. and N.J. Castellan, Jr. 1998. Nonparamet-
ric Statistics for the Behavioral Sciences. McGraw-
Hill International.
Torres-Moreno, Juan-Manuel, Patricia Velz?quez-
Morales, and Jean-Guy Meunier. 2002. Condenss?
de textes par des me?thodes numr?iques. In JADT?02,
volume 2, pages 723?734, St Malo, France.
Vivaldi, Jorge, Iria da Cunha, Juan-Manuel Torres-
Moreno, and Patricia Vela?zquez-Morales. 2010.
Automatic summarization using terminological and
semantic resources. In LREC?10, volume 2,
page 10, Malta.
Yatsko, V.A. and T.N. Vishnyakov. 2007. A method
for evaluating modern systems of automatic text
summarization. Automatic Documentation and
Mathematical Linguistics, 41(3):93?103.
1067
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 148?152,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Are Semantically Coherent Topic Models Useful for Ad Hoc Information
Retrieval?
Romain Deveaud Eric SanJuan
University of Avignon - LIA
Avignon, France
romain.deveaud@univ-avignon.fr
eric.sanjuan@univ-avignon.fr
Patrice Bellot
Aix-Marseille University - LSIS
Marseille, France
patrice.bellot@lsis.org
Abstract
The current topic modeling approaches for
Information Retrieval do not allow to ex-
plicitly model query-oriented latent top-
ics. More, the semantic coherence of the
topics has never been considered in this
field. We propose a model-based feedback
approach that learns Latent Dirichlet Al-
location topic models on the top-ranked
pseudo-relevant feedback, and we mea-
sure the semantic coherence of those top-
ics. We perform a first experimental eval-
uation using two major TREC test collec-
tions. Results show that retrieval perfor-
mances tend to be better when using topics
with higher semantic coherence.
1 Introduction
Representing documents as mixtures of ?topics?
has always been a challenge and an objective for
researchers working in text-related fields. Based
on the words used within a document, topic mod-
els learn topic level relations by assuming that the
document covers a small set of concepts. Learn-
ing the topics from a document collection can help
to extract high level semantic information, and
help humans to understand the meaning of doc-
uments. Latent Semantic Indexing (Deerwester
et al, 1990) (LSI), probabilistic Latent Seman-
tic Analysis (Hofmann, 2001) (pLSA) and Latent
Dirichlet Allocation (Blei et al, 2003) (LDA) are
the most famous approaches that tried to tackle
this problem throughout the years. Topics pro-
duced by these methods are generally fancy and
appealing, and often correlate well with human
concepts. This is one of the reasons of the inten-
sive use of topic models (and especially LDA) in
current research in Natural Language Processing
(NLP) related areas.
One main problem in ad hoc Information Re-
trieval (IR) is the difficulty for users to translate a
complex information need into a keyword query.
The most popular and effective approach to over-
come this problem is to improve the representa-
tion of the query by adding query-related ?con-
cepts?. This approach mostly relies on pseudo-
relevance feedback, where these so-called ?con-
cepts? are the most frequent words occurring in the
top-ranked documents retrieved by the retrieval
system (Lavrenko and Croft, 2001). From that
perspective, topic models seem attractive in the
sense that they can provide a descriptive and intu-
itive representation of concepts. But how can we
quantify the usefulness of these topics with respect
to an IR system? Recently, researchers developed
measures which evaluate the semantic coherence
of topic models (Newman et al, 2010; Mimno et
al., 2011; Stevens et al, 2012). We adopt their
view of semantic coherence and apply one of these
measures to query-oriented topics.
Several studies concentrated on improving the
quality of document ranking using topic models,
especially probabilistic ones. The approach by
Wei and Croft (2006) was the first to leverage
LDA topics to improve the estimate of document
language models and achieved good empirical re-
sults. Following this pioneering work, several
studies explored the use of pLSA and LDA un-
der different experimental settings (Park and Ra-
mamohanarao, 2009; Yi and Allan, 2009; Andrze-
jewski and Buttler, 2011; Lu et al, 2011). The re-
ported results suggest that the words and the prob-
ability distributions learned by probabilistic topic
models are effective for query expansion. The
main drawback of these approaches is that topics
are learned on the whole target document collec-
tion prior to retrieval, thus leading to a static top-
ical representation of the collection. Depending
on the query and on its specificity, topics may ei-
ther be too coarse or too fine to accurately rep-
resent the latent concepts of the query. Recently,
Ye et al (2011) proposed a method which uses
148
LDA and learns topics directly on a limited set
of documents. While this approach is a first step
towards modeling query-oriented topics, it lacks
some theoretic principles and only aims to heuris-
tically construct a ?best? topic (from all learned
topics) before expanding the query with its most
probable words. More, none of the aforemen-
tioned works studied the semantic coherence of
those generated topics. We tackle these issues by
making the following contributions:
? we introduce Topic-Driven Relevance Mod-
els, a model-based feedback approach (Zhai
and Lafferty, 2001) for integrating topic mod-
els into relevance models by learning topics
on pseudo-relevant feedback documents (as
opposed to the entire document collection),
? we explore the coherence of those generated
topics using the queries of two major and
well-established TREC test collections,
? we evaluate the effects coherent topics have
on ad hoc IR using the same test collections.
2 Topic-Driven Relevance Models
2.1 Relevance Models
The goal of relevance models is to improve
the representation of a query Q by selecting
terms from a set of initially retrieved docu-
ments (Lavrenko and Croft, 2001). As the concen-
tration of relevant documents is usually higher in
the top ranks of the ranking list, this is constituted
by a number N of top-ranked documents. Rele-
vance models usually perform better when com-
bined with the original query model (or maxi-
mum likelihood estimate). Let ??Q be this maxi-
mum likelihood query estimate and ??Q a relevance
model, the updated new query model is given by:
P (w|?Q) = ? P (w|??Q) + (1? ?)P (w|??Q) (1)
where ? ? [0, 1] is a parameter that controls the
tradeoff between the original query model and the
relevance model. One of the most robust variants
of the relevance models is computed as follows:
P (w|??Q) ?
?
?D??
P (?D)P (w|?D)
?
t?Q
P (t|?D)
(2)
where ? is a set of pseudo-relevant feedback doc-
uments and ?D is the language model of document
D. This notion of estimating a query model is
often referred to as model-based feedback (Zhai
and Lafferty, 2001). We assume P (?D) to be uni-
form, resulting in an estimated relevance model
based on a sum of document models weighted
by the query likelihood score. The final, inter-
polated, estimate expressed in equation (1) is of-
ten referred in the literature as RM3. We tackle
the null probabilities problem by smoothing the
document language model using the well-known
Dirichlet smoothing (Zhai and Lafferty, 2004).
2.2 LDA-based Feedback Model
The estimation of the feedback model ??Q consti-
tutes the first contribution of this work. We pro-
pose to explicitly model the latent topics (or con-
cepts) that exist behind an information need, and
to use them to improve the query representation.
We consider ? as the set of pseudo-relevant feed-
back documents from which the latent concepts
would be extracted. The retrieval algorithm used
to obtain these documents can be of any kind, the
important point is that ? is a reduced collection
that contains the top documents ranked by an au-
tomatic and state-of-the-art retrieval process.
Instead of viewing ? as a set of document lan-
guage models that are likely to contain topical in-
formation about the query, we take a probabilistic
topic modeling approach. We specifically focus
on Latent Dirichlet Allocation (LDA), since it is
currently one of the most representative. In LDA,
each topic multinomial distribution ?k is gener-
ated by a conjugate Dirichlet prior with parame-
ter ?, while each document multinomial distribu-
tion ?d is generated by a conjugate Dirichlet prior
with parameter ?. In other words, ?d,k is the prob-
ability of topic k occurring in document D (i.e.
P (k|D)). Respectively, ?k,w is the probability of
wordw belonging to topic k (i.e. P (w|k)). We use
variational inference implemented in the LDA-C
software1 to overcome intractability issues (Blei et
al., 2003; Griffiths and Steyvers, 2004). Under this
setting, we compute the topic-driven estimation of
the query model using the following equation:
P (w|??Q) ?
?
?D??
(
P (?D)P (w|?D)
PTM (w|D)
?
t?Q
P (t|?D)
)
(3)
where PTM (w|D) is the probability of word w
occurring in document D using the previously
1www.cs.princeton.edu/?blei/lda-c
149
5 10 20 30 40 509.
4
9.6
9.8
10.0
10.2
Cohe
rence
Number of feedback documents
Number of topics3 5 10 15 20
WT10g
5 10 20 30 40 509.
4
9.6
9.8
10.0
10.2
Cohe
rence
Number of feedback documents
Number of topics3 5 10 15 20
Robust04
Figure 1: Semantic coherence of the topic models for different values of K, in function of the number
N of feedback documents.
learned multinomial distributions. Let T? be a
topic model learned on the ? set of feedback doc-
uments, this probability is given by:
PTM (w|D) =
?
k?T?
?k,w ? ?D,k (4)
High probabilities are thus given to words that are
important in topic k, when k is an important topic
in document D. In the remainder of this paper, we
refer to this general approach as TDRM for Topic-
Driven Relevance Models.
2.3 Measuring the coherence of
query-oriented topics
TDRM relies on two important parameters: the
number of topics K that we want to learn, and
the number of feedback documents N from which
LDA learns the topics. Varying these two param-
eters can help to capture more information and to
model finer topics, but how about their global se-
mantic coherence?
Term similarities measured in restricted do-
mains was the first step for evaluating seman-
tic coherence (Gliozzo et al, 2007), and was a
first basis for the development of several topic
coherence evaluation measures (Newman et al,
2010). Computing the Pointwise Mutual Informa-
tion (PMI) of all word pairs over Wikipedia was
found to be an effective metric using news and
books corpora. Recently, Stevens et al (2012)
used (among others) an aggregate version of this
metric to evaluate large amounts of topic models.
We use this method to evaluate the coherence of
query-oriented topics. Specifically, the coherence
of a topic model T K? composed of K topics is:
c(T K? ) =
1
K
K?
i=1
?
(w,w?)?ki
log P (w,w
?) + 
P (w)P (w?) (5)
where probabilities of word occurrences and co-
occurrences are estimated using an external refer-
ence corpus. Following Newman et al (2010), we
use Wikipedia to compute PMI and set  = 1 as
in (Stevens et al, 2012).
3 Evaluation
3.1 Experimental setup
We performed our evaluation using two main
TREC2 collections: Robust04 and WT10g. Ro-
bust04 is composed 528,155 of news articles com-
ing from three newspapers and the FBIS. It sup-
ported the TREC 2004 Robust track, from which
we used the 250 query topics (numbers: 301-450,
601-700). The WT10g collection is composed of
1,692,096 web pages, and supported the TREC
Web track for four years (2001-2004). We focus
on the 2000 and 2001 ad-hoc query topics (num-
bers: 451-550). We used the open-source index-
ing and retrieval system Indri3 to run our exper-
iments. We indexed the two collections with the
exact same parameters: tokens were stemmed with
the well-known light Krovetz stemmer and stop-
words were removed using the standard English
stoplist embedded with Indri (417 words).
3.2 Semantic coherence evaluation
Most coherent topics are composed of rare words
that do not often occur in the reference corpus, but
2trec.nist.gov
3lemurproject.org/indri.php
150
0.20
0
0.20
5
0.21
0
0.21
5
0.22
0
5 10 20 30 40 50
MAP
Number of feedback documents
l
l
l
l
l
l
l
l
l l
l
l
l
l
Number of topics35101520RM3
WT10g
0.26
00
.265
0.27
00
.275
0.28
00
.285
0.29
0
5 10 20 30 40 50
MAP
Number of feedback documents
l
l
l
l
l l
l l
l
l
l
l
l
l
Number of topics35101520RM3
Robust04
Figure 2: Retrieval performance in terms of Mean Average Precision (MAP) of the TDRM approach.
Each line represent a different number of topics K, and the performance are reported in function the
number N of feedback documents. The black, plain line represents the RM3 baseline.
co-occur at lot together. We see on Figure 1 that
very coherent topics are identified in the top 5 and
10 feedback documents for the WT10g collection,
suggesting that closely related documents are re-
trieved in the top ranks. Results are quite different
on the Robust04 collection, where topic models
with 20 topics on 5 documents are the least co-
herent. However, when looking at the Robust04
documents, we see that they are on average almost
twice smaller than the WT10g web pages. We hy-
pothesize that the heterogeneous nature of the web
allows to model very different topics covering sev-
eral aspects of the query, while news articles are
contributions focused on a single subject.
Overall, the more coherent topic models contain
a reasonable amount of topics (10-15), thus allow-
ing to fit with variable amounts of documents. The
attentive reader will notice that the topic coher-
ence scores are very high compared to those pre-
viously reported in the literature (Stevens et al,
2012). The TDRM approach captures topics that
are centered around a specific information need,
often with a limited vocabulary, which favors word
co-occurrence. On the other hand, topics learned
on entire collections are coarser than ours, which
leads to lower coherence scores.
3.3 Document retrieval results
Since TDRM is based on Relevance Mod-
els (Lavrenko and Croft, 2001), we take the RM3
approach presented in Section 2.1 as baseline. The
? parameter is common between RM3 and TDRM
and is determined for each query using leave-
one-query-out cross-validation (that is: learn the
best parameter setting for all queries but one, and
evaluate the held-out query using the previously
learned parameter).
We report ad hoc document retrieval perfor-
mances in Figure 2. We noticed in the previous
section that the most coherent topic models were
modeled using 5 feedback documents and 20 top-
ics for the WT10g collection, and this parame-
ter combination also achieves the best retrieval re-
sults. Overall, using 10, 15 or 20 topics allow it
to achieve high and similar performance from 5 to
20 documents. We observe than using 20 topics
for the Robust04 collection consistently achieves
the highest results, with the topic model coherence
growing as the number of feedback documents in-
creases. Although topics coming from news ar-
ticles may be limited, they benefit from the rich
vocabulary of professional writers who are trained
to avoid repetition. Their use of synonyms allows
TDRM to model deep topics, with a comprehen-
sive description of query aspects. Since synonyms
are less likely to co-occur in encyclopedic articles
like Wikipedia, we think that, in our case, the se-
mantic coherence measure could be more accurate
using other textual resources. This measure seems
however to be effective when dealing with hetero-
geneously structured documents.
4 Conclusions & Future Work
Overall, modeling query-oriented topic models
and estimating the feedback query model using
these topics greatly improves ad hoc Information
Retrieval, compared to state-of-the-art relevance
models. While semantically coherent topic mod-
151
els do not seem to be effective in the context of a
news articles search task, they are a good indica-
tor of effectiveness in the context of web search.
Measuring the semantic coherence of query top-
ics could help predict query effectiveness or even
choose the best query-representative topic model.
Acknowledgments
This work was supported by the French Agency
for Scientific Research (Agence Nationale de
la Recherche) under CAAS project (ANR 2010
CORD 001 02).
References
David Andrzejewski and David Buttler. 2011. Latent
Topic Feedback for Information Retrieval. In Pro-
ceedings of the 17th ACM SIGKDD international
conference on Knowledge discovery and data min-
ing, KDD ?11, pages 600?608.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet Allocation. Journal of Ma-
chine Learning Research, 3:993?1022.
Scott Deerwester, Susan T. Dumais, George W. Fur-
nas, Thomas K. Landauer, and Richard Harshman.
1990. Indexing by Latent Semantic Analysis. Jour-
nal of the American Society for Information Science,
41(6):391?407.
Alfio Massimiliano Gliozzo, Marco Pennacchiotti, and
Patrick Pantel. 2007. The Domain Restriction Hy-
pothesis: Relating Term Similarity and Semantic
Consistency. In Human Language Technologies:
The 2007 Annual Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics, pages 131?138.
Thomas L Griffiths and Mark Steyvers. 2004. Find-
ing scientific topics. Proceedings of the National
Academy of Sciences of the United States of Amer-
ica, 101 Suppl.
Thomas Hofmann. 2001. Unsupervised Learning by
Probabilistic Latent Semantic Analysis. Machine
Learning, 42:177?196.
Victor Lavrenko and W. Bruce Croft. 2001.
Relevance-Based Language Models. In Proceedings
of the 24th annual international ACM SIGIR confer-
ence on Research and development in information
retrieval, SIGIR ?01, pages 120?127.
Yue Lu, Qiaozhu Mei, and ChengXiang Zhai. 2011.
Investigating task performance of probabilistic topic
models: an empirical study of PLSA and LDA. In-
formation Retrieval, 14:178?203.
David Mimno, Hanna M. Wallach, Edmund Talley,
Miriam Leenders, and Andrew McCallum. 2011.
Optimizing Semantic Coherence in Topic Models.
In Proceedings of the Conference on Empirical
Methods in Natural Language Processing, EMNLP
?11, pages 262?272.
David Newman, Jey Han Lau, Karl Grieser, and Tim-
othy Baldwin. 2010. Automatic Evaluation of
Topic Coherence. In Human Language Technolo-
gies: The 2010 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 100?108.
Laurence A. Park and Kotagiri Ramamohanarao. 2009.
The Sensitivity of Latent Dirichlet Allocation for In-
formation Retrieval. In Proceedings of the Euro-
pean Conference on Machine Learning and Knowl-
edge Discovery in Databases, ECML PKDD ?09,
pages 176?188.
Keith Stevens, Philip Kegelmeyer, David Andrzejew-
ski, and David Buttler. 2012. Exploring Topic
Coherence over Many Models and Many Topics.
In Proceedings of the 2012 Joint Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning,
EMNLP-CoNLL ?12, pages 952?961.
Xing Wei and W. Bruce Croft. 2006. LDA-based Doc-
ument Models for Ad-hoc Retrieval. In Proceedings
of the 29th annual international ACM SIGIR confer-
ence on Research and development in information
retrieval, SIGIR ?06, pages 178?185.
Zheng Ye, Jimmy Xiangji Huang, and Hongfei Lin.
2011. Finding a Good Query-Related Topic for
Boosting Pseudo-Relevance Feedback. JASIST,
62(4):748?760.
Xing Yi and James Allan. 2009. A Comparative Study
of Utilizing Topic Models for Information Retrieval.
In Proceedings of the 31th European Conference on
IR Research on Advances in Information Retrieval,
ECIR ?09, pages 29?41. Springer-Verlag.
Chengxiang Zhai and John Lafferty. 2001. Model-
based Feedback in the Language Modeling Ap-
proach to Information Retrieval. In Proceedings
of the Tenth International Conference on Informa-
tion and Knowledge Management, CIKM ?01, pages
403?410.
Chengxiang Zhai and John Lafferty. 2004. A Study of
Smoothing Methods for Language Models Applied
to Information Retrieval. ACM Transactions on In-
formation Systems, 22(2):179?214.
152
