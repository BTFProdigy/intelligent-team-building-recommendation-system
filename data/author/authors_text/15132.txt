Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1950?1961, Dublin, Ireland, August 23-29 2014.
Why Gender and Age Prediction from Tweets is Hard:
Lessons from a Crowdsourcing Experiment
Dong Nguyen
14?
Dolf Trieschnigg
14
A. Seza Do
?
gru
?
oz
23
Rilana Gravel
4
Mari
?
et Theune
1
Theo Meder
4
Franciska de Jong
1
(1) Human Media Interaction, University of Twente, Enschede, The Netherlands
(2) Netherlands Institute for Advanced Studies, Wassenaar, NL
(3) Tilburg School of Humanities, Tilburg University, Tilburg, NL
(4) Meertens Institute, Amsterdam, The Netherlands
?
Corresponding author: d.nguyen@utwente.nl
Abstract
There is a growing interest in automatically predicting the gender and age of authors from texts.
However, most research so far ignores that language use is related to the social identity of speak-
ers, which may be different from their biological identity. In this paper, we combine insights
from sociolinguistics with data collected through an online game, to underline the importance
of approaching age and gender as social variables rather than static biological variables. In our
game, thousands of players guessed the gender and age of Twitter users based on tweets alone.
We show that more than 10% of the Twitter users do not employ language that the crowd as-
sociates with their biological sex. It is also shown that older Twitter users are often perceived
to be younger. Our findings highlight the limitations of current approaches to gender and age
prediction from texts.
1 Introduction
A major thrust of research in sociolinguistics aims to uncover the relationship between social variables
such as age and gender, and language use (Holmes and Meyerhoff, 2003; Eckert and McConnell-Ginet,
2013; Eckert, 1997; Wagner, 2012). In line with scholars from a variety of disciplines, including the so-
cial sciences and philosophy, sociolinguists consider age and gender as social and fluid variables (Eckert,
2012). Gender and age are shaped depending on the societal context, the culture of the speakers involved
in a conversation, the individual experiences and the multitude of social roles: a female teenager might
also be a high school student, a piano player, a swimmer, etc. (Eckert, 2008).
Speakers use language as a resource to construct their identity (Bucholtz and Hall, 2005). For example,
a person?s gender identity is constructed through language by using linguistic features associated with
male or female speech. These features gain social meaning in a cultural and societal context. On Twitter,
users construct their identity through interacting with other users (Marwick and boyd, 2011). Depending
on the context, they may emphasize specific aspects of their identity, which leads to linguistic variation
both within and between speakers. We illustrate this with the following three tweets:
Tweet 1: I?m walking on sunshine <3 #and don?t you feel good
Tweet 2: lalaloveya <3
Tweet 3: @USER loveyou ;D
In these tweets, we find linguistic markers usually associated with females (e.g. a heart represented
as <3). Indeed, 77% of the 181 players guessed that a female wrote these tweets in our online game.
However, this is a 16-year old biological male, whose Twitter account reveals that he mostly engages
with female friends. Therefore, he may have accommodated his style to them (Danescu-Niculescu-Mizil
et al., 2011) and as a result he employs linguistic markers associated with the opposite biological sex.
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1950
Most of the NLP research focusing on predicting gender and age has approached these variables as bi-
ological and static, rather than social and fluid. For example, current approaches use supervised machine
learning models trained on tweets from males and females. However, the resulting stereotypical models
are ineffective for Twitter users who tweet differently from what is to be expected from their biological
sex.
As explained above, language use is based on social gender and age identity, and not on biological sex
and chronological age. In other words, treating gender and age as fixed biological variables in analyzing
language use is too simplistic. By comparing the biological sex and chronological age of Twitter users
with how they are perceived by the crowd (as an indication of socially constructed identities), we shed
light on the difficulty of predicting gender and age from language use and draw attention to the inherent
limitations of current approaches.
As has been demonstrated in several studies, the crowd can be used for experimentation (e.g., Munro
et al. (2010)). Our study illustrates the value of the crowd for the study of human behavior, in particular
for the experimental study of the social dimension of language use. To collect data, we created an online
game (an example of gamification (Deterding et al., 2011)) in which thousands of players (the crowd)
guessed the biological sex and age of Twitter users based on only the users? tweets. While variance
between annotators has traditionally been treated as noise, more recently variation is being treated as a
signal rather than noise (Aroyo and Welty, 2013). For example, Makatchev and Simmons (2011) analyze
how English utterances are perceived differently across language communities.
This paper follows this trend, treating variation as meaningful information. We assume that the crowd?s
perception (based on the distribution of the players? guesses) is an indication of to what extent Twitter
users emphasize their gender and age identity in their tweets. For example, when a large proportion of
the players guess the same gender for a particular user, the user is assumed to employ linguistic markers
that the crowd associates with gender-specific speech (e.g. iconic hearts used by females).
Our contributions are as follows:
? We demonstrate the use of gamification to study sociolinguistic research problems (Section 3).
? We study the difficulty of predicting an author?s gender (Section 4) and age (Section 5) from text
alone by analyzing prediction performance by the crowd. We relate our results to sociolinguistic
theories and show that approaching gender and age as fixed biological variables is too simplistic.
? Based on our findings, we reflect on current approaches to predicting age and gender from text, and
draw attention to the limitations of these approaches (Section 6).
2 Related Work
Gender Within sociolinguistics, studies on gender and language have a long history (Eckert and
McConnell-Ginet, 2013). More recently, the NLP community has become increasingly interested in
this topic. Most of the work aims at predicting the gender of authors based on their text, thereby focusing
more on prediction performance than sociolinguistic insights.
A variety of datasets have been used, including Twitter (Rao et al., 2010; Bamman et al., 2014; Fink et
al., 2012; Bergsma and Van Durme, 2013; Burger et al., 2011), blogs (Mukherjee and Liu, 2010; Schler et
al., 2005), telephone conversations (Garera and Yarowsky, 2009), YouTube (Filippova, 2012) and chats
in social networks (Peersman et al., 2011). Females tend to use more pronouns, emoticons, emotion
words, and blog words (lol, omg, etc.), while males tend to use more numbers, technology words, and
links (Rao et al., 2010; Bamman et al., 2014; Nguyen et al., 2013). These differences have also been
exploited to improve sentiment classification (Volkova et al., 2013) and cyberbullying detection (Dadvar
et al., 2012).
To the best of our knowledge, the study by Bamman et al. (2014) is the only computational study that
approaches gender as a social variable. By clustering Twitter users based on their tweets, they show that
multiple gendered styles exist. Unlike their study, we use the crowd and focus on implications for gender
and age prediction.
1951
Figure 1: Screenshot of the game. Text is translated into English (originally in Dutch). Left shows the
interface when the user needs to make a guess. Right shows the feedback interface.
Age Eckert (1997) makes a distinction between chronological (number of years since birth), biological
(physical maturity), and social age (based on life events). Most of the studies on language and age
focus on chronological age. However, speakers with the same chronological age can have very different
positions in society, resulting in variation in language use. Computational studies on language use and
age usually focus on automatic (chronological) age prediction. This has typically been modeled as
a classification problem, although this approach often suffers from ad hoc and dataset dependent age
boundaries (Rosenthal and McKeown, 2011). In contrast, recent works also explored predicting age as a
continuous variable and predicting lifestages (Nguyen et al., 2013; Nguyen et al., 2011) .
Similar to studies on gender prediction, a variety of resources have been used for age prediction, in-
cluding Twitter (Rao et al., 2010; Nguyen et al., 2013), blogs (Rosenthal and McKeown, 2011; Goswami
et al., 2009), chats in social networks (Peersman et al., 2011) and telephone conversations (Garera and
Yarowsky, 2009). Younger people use more alphabetical lengthening, more capitalization of words,
shorter words and sentences, more self-references, more slang words, and more Internet acronyms
(Rosenthal and McKeown, 2011; Nguyen et al., 2013; Rao et al., 2010; Goswami et al., 2009; Pen-
nebaker and Stone, 2003; Barbieri, 2008).
3 Data
To study how people perceive the gender and age identity of Twitter users based on their tweets, we
created an online game. Players were asked to guess the gender and age of Twitter users from tweets.
The game was part of a website (TweetGenie, www.tweetgenie.nl) that also hosted an automatic system
that predicts the gender and age of Twitter users based on their tweets (Nguyen et al., 2014). To attract
players, a link to the game was displayed on the page with the results of the automatic prediction, and
visitors were challenged to test if they were better than the automatic system (TweetGenie).
3.1 Twitter Data
We sampled Dutch Twitter users in the fall of 2012. We employed external annotators to annotate the
biological sex and chronological age (in years) using all information available through tweets, the Twitter
profile and external social media profiles such as Facebook and Linkedin. In total over 3000 Twitter users
were annotated. For more details regarding the collection of the dataset we refer to Nguyen et al. (2013).
We divided the data into train and test sets. 200 Twitter users were randomly selected from the test
set to be included in the online game (statistics are shown in Table 1). Named entities were manually
anonymized to conceal the user?s identity. Names in tweets were replaced by ?similar? names (e.g. a
first name common in a certain region in the Netherlands was replaced with another common name in
that region). This was done without knowing the actual gender and age of the Twitter users. Links were
replaced with a general [LINK] token and user mentions with @USER.
1952
Gender and age F, <20 M, <20 F, [20-40) M, [20-40) F, ?40 M, ?40
Frequency 61 60 24 23 17 15
Table 1: Statistics Twitter users in our game
3.2 Online Game
Game Setup The interface of the game is shown in Figure 1. Players guessed the biological sex (male
or female) and age (years) of a Twitter user based on only the tweets. For each user, {20, 25, 30, 35,
40} tweets were randomly selected. For a particular Twitter user, the same tweets were displayed to all
players. Twitter users were randomly selected to be displayed to the players.
To include an entertainment element, players received feedback after each guess. They were shown
the correct age and gender, the age and gender guessed by the computer, and the average guessed age
and gender distribution by the other players. In addition, a score was shown of the player versus the
computer.
Collection In May 2013, the game was launched. Media attention resulted in a large number of visitors
(Nguyen et al., 2014). We use the data collected from May 13, 2013 to August 21, 2013, resulting in
a total of 46,903 manual guesses. Players tweeted positively about the game, such as ?@USER Do you
know what is really addictive? ?Are you better than Tweetgenie? ...? and ?@USER Their game is quite
fun!? (tweets translated to English).
We filter sessions that do not seem to contain genuine guesses: when the entered age is 80 years
or above, or 8 or below. These thresholds were based on manual inspection, and chosen because it is
unlikely that the shown tweets are from users of such ages. For each guess, we registered a session ID
and an IP address. A new session started after 2 hours of inactivity. To study player performance more
robustly, we excluded multiple sessions of the same player. After three or more guesses had been made
in a session, all next sessions from the same IP address were discarded.
Statistics Statistics of the data are shown in Table 2. Figure 2 shows the distribution of the number of
guesses per session. The longest sessions consisted of 18 guesses. Some of our analyses require multiple
guesses per player. In that case, we only include players having made at least 7 guesses.
1 2 3 4 5 6 7 8 9 10 >10
Number of guesses
Freq
uenc
y
01
000
3000
5000
Figure 2: Number of guesses per session
# guesses 41,989
# sessions 15,724
Avg. time (sec) per guess 46
Avg. # guesses / session 2.67
Table 2: Statistics online game (after
cleaning)
We calculate the time taken for a guess by taking the time difference between two guesses (therefore,
no time for the first guess in each session could be measured). For each Twitter user, we calculate the
average time that was taken to guess the gender and age of the user. (Figure 3a). There is a significant
correlation (Pearson?s r = 0.291, p < 0.001) between the average time the players took to evaluate the
tweets of a Twitter user and the number of displayed tweets.
There is also a significant correlation between the average time taken for a user and the entropy over
gender guesses (Pearson?s r = 0.410, p < 0.001), and the average time taken for a user and the standard
deviation of the age guesses (Pearson?s r = 0.408, p < 0.001). Thus, on average, players spent more time
on Twitter users for whom it was more difficult to estimate gender and age.
1953
Avg time taken for Twitter user (sec)
Freq
uen
cy
0
20
40
60
80
30 35 40 45 50 55 60 65
(a) Average time taken for Twitter users
Turn
Av
era
ge 
tim
e ta
ken
30
32
34
36
2 3 4 5 6 7
(b) Average time taken per turn
Figure 3: Time taken in game
We observe that as the game progresses, players tend to take less time to make a guess. This is shown
in Figure 3b, which shows the average time taken for a turn (restricted to players with at least 7 guesses).
There was no significant correlation between time spent on a guess and the performance of players and
we did not find trends of performance increase or decrease as players progressed in the game.
3.3 Automatic Prediction
Besides studying human performance, we also compare the predictions of humans with those of an
automatic system. We split the data into train and test sets using the same splits as used by Nguyen et al.
(2013). We train a logistic regression model to predict gender (male or female), and a linear regression
model to predict the age (in years) of a person.
More specifically, given an input vector x ? R
m
, x
1
, . . . , x
m
represent features. In the case of gender
classification (e.g. y ? {?1, 1}), the model estimates a conditional distribution P (y|x, ?) = 1/(1 +
exp(?y(?
0
+ x
>
?))), where ?
0
and ? are the parameters to estimate. Age is treated as a regression
problem, and we find a prediction y? ? R for the exact age of a person y ? R using a linear regression
model: y? = ?
0
+ x
>
?. We use Ridge (also called L
2
) regularization to prevent overfitting.
We make use of the liblinear (Fan et al., 2008) and scikit-learn (Pedregosa et al., 2011) libraries.
We only use unigram features, since they have proven to be very effective for gender (Bamman et al.,
2014; Peersman et al., 2011) and age (Nguyen et al., 2013) prediction. Parameters were tuned using
cross-validation on the training set.
4 Gender
Most of the computational work on language and gender focuses on gender classification, treating gender
as fixed and classifying speakers into females and males. However, this assumes that gender is fixed and
is something people have, instead of something people do (Butler, 1990).
In this section, we first analyze the task difficulty by studying crowd performance on inferring gender
from tweets. We observe a relatively large group of Twitter users who employ language that the crowd
associates with the opposite biological sex. This, then, raises questions about the upper bound that a
prediction system based on only text can achieve.
Next, we place Twitter users on a gender continuum based on the guesses of the players and show that
treating gender as a binary variable is too simplistic. While historically gender has been treated as binary,
researchers in fields such as sociology (Lorber, 1996) and sociolinguistics (Holmes and Meyerhoff, 2003;
Bergvall et al., 1996) find this view too limited. Instead, we assume the simplest extension beyond a
binary variable: a one-dimensional gender continuum (or scale) (Bergvall et al., 1996). For example,
Bergvall (1999) talks about a ?continuum of humans? gendered practices?. While these previous studies
were based on qualitative analyses, we take a quantitative approach using the crowd.
1954
4.1 Task Difficulty
Majority vote We study crowd performance using a system based on the majority of the players?
guesses. Majority voting has proven to be a strong baseline to aggregate votes (e.g. in crowdsourcing
systems (Snow et al., 2008; Le et al., 2010)). On average, we have 210 guesses per Twitter user, providing
substantial evidence per Twitter user. A system based on majority votes achieves an accuracy of 84%
(Table 3a shows a confusion matrix). Table 3b shows a confusion matrix of the majority predictions
versus the automatic system. We find that the biological sex was predicted incorrectly by both the
majority vote system and the automatic system for 21 out of the 200 Twitter users (10.5%, not in Table).
Automatic classification systems on English tweets achieve similar performances as our majority vote
system (e.g. Bergsma and Van Durme (2013) report an accuracy of 87%, Bamman et al. (2014) 88%).
More significantly, the results suggest that 10.5% (automatic + majority) to 16% (majority) of the Dutch
Twitter users do not employ language that the crowd associates with their biological sex. As said, this
raises the question of whether we can expect much higher performances by computational systems based
on only language use.
Biological sex
Male Female
Crowd
Male 82 16
Female 16 86
(a) Crowd (majority)
Crowd
Male Female
Automatic
Male 68 22
Female 30 80
(b) Automatic vs crowd
Table 3: Confusion matrices crowd prediction
Individual players versus an automatic system When considering players with 7 or more guesses,
the average accuracy for a player is 0.71. Our automatic system achieves an accuracy of 0.69. The small
number of tweets per Twitter user in our data (20-40) makes it more difficult to automatically predict
gender.
Entropy We characterize the difficulty of inferring a user?s gender by calculating the entropy for each
Twitter user based on the gender guesses (Figure 4a). We find that the difficulty varies widely across
users, and that there are no distinct groups of ?easy? and ?difficult? users. However, we do observe an
interaction effect between the entropy of the gender guesses and the ages of the Twitter users. At an
aggregate level, we find no significant trend. Analyzing females and males separately, we observe a
significant trend with females (Pearson?s r = 0.270, p < 0.01), suggesting that older female Twitter users
tend to emphasize other aspects than their gender in tweets (as perceived by the crowd).
Persons
Ent
rop
y
0.2
0.6
1.0
0 50 100 150 200
(a) Entropy over gender guesses
0
5
10
15
20
25
0.0 0.5 1.0
Proportion of people that guessed male
Fre
que
ncy Biological 
sex
Male
Female
(b) A histogram of all Twitter users and the proportion of
players who guessed the users were male. For example,
there are 25 female users for which 10 - 20% of the players
guessed they were male.
Figure 4: Gender prediction
1955
4.2 Binarizing Gender, a Good Approach?
Using data collected through the online game we quantitatively put speakers on a gender continuum
based on how their tweets are perceived by the crowd. For each Twitter user, we calculate the proportion
of players who guessed the users were male and female. A plot is displayed in Figure 4b. We can make
the following observations:
First, the guesses by the players are based on their expectations about what kind of behaviour and
language is used by males and females. The plot shows that for some users, almost all players guessed
the same gender, indicating that these expectations are quite strong and that there are stylistic markers
and topics that the crowd strongly associates with males or females.
Second, if treating gender as a binary variable is reasonable, we would expect to see two distinct
groups. However, we observe quite an overlap between the biological males and females. There are 1)
users who conform to what is expected based on their biological sex, 2) users who deviate from what is
expected, 3) users whose tweets do not emphasize a gender identity or whose tweets have large variation
using language associated with both genders. We investigated whether this is related to their use of
Twitter (professional, personal, or both), but the number of Twitter users in our dataset who used Twitter
professionally was small and not sufficient to draw conclusions.
We now illustrate our findings using examples. The first example is a 15-year old biological female
for who the crowd guessed most strongly that she is female (96% of n=220). Three tweets from her are
shown below. She uses language typically associated with females, talking about spending time with
her girlfriends and the use of stylistic markers such as hearts and alphabetical lengthening. Thus, she
conforms strongly to what the crowd expects from her biological sex.
Tweet 4: Gezellig bij Emily en Charlotte.
Translation: Having fun with Emily and Charlotte.
Tweet 5: Hiiiiii schatjesss!
Translation: Hiiiiii cutiesss!
Tweet 6: ? @USER
Below are two tweets from a 40 year old biological female who does not employ linguistic markers
strongly associated with males or females. Therefore, only 46% of the crowd (n=200) was able to guess
that she is female.
Tweet 7: Ik viel op mijn bek. En het kabinet ook. Geinig toch? #Catshuis
Translation: I went flat on my face. And the cabinet as well. Funny right? #Catshuis
Tweet 8: Jeemig. Ik kan het bijna niet volgen allemaal.
Translation: Jeez. I almost can?t follow it all.
Twitter users vary in how much they emphasize their gender in their tweets. As a result, the difficulty
of inferring gender from tweets varies across persons, and treating gender as a binary variable ignores
much of the interesting variation within and between persons.
Automatic system We now analyze whether an automatic system is capable of capturing the position
of Twitter users on the gender continuum (as perceived by the crowd). We calculate the correlation
between the proportion of male guesses (i.e. the position on the gender continuum) and the scores of
the logistic regression classifier: ?
0
+ x
>
?. While the training data was binary (users were labeled as
male or female), a reasonable Spearman correlation of ? = 0.584 (p < 0.001) was obtained between the
classifier score and the score based on the crowd?s perception. We did not observe a significant relation
between the score of the classifier (corresponding to the confidence of the gender prediction) and age.
1956
5 Age
We start with an analysis of task difficulty, by studying crowd performance on inferring age from tweets.
Next, we show that it is particularly hard to accurately infer the chronological age of older Twitter users
from tweets.
5.1 Task Difficulty
The crowd?s average guesses As with a system based on majority vote for gender prediction, we test
the performance of a system that predicts the ages of Twitter users based on the average of all guesses.
We find that such a system achieves a Mean Absolute Error (MAE) of 4.844 years and a Pearson?s
correlation of 0.866. Although the correlation is high, the absolute errors are quite large. We find that the
crowd has difficulty predicting the ages of older Twitter users. There is a positive correlation (Pearson?s
? = 0.789) between the absolute errors and the actual age of Twitter users. There is a negative correlation
between the errors (predicted - actual age) and the actual age of Twitter users (Pearson?s ? = -0.872).
We calculate the standard deviation over all the age guesses for a user (Figure 5a) to measure the
difficulty of inferring a user?s age. There is a positive correlation between age and standard deviation of
the guesses (? = 0.691), which indicates that players have more difficulty in guessing the ages of older
Twitter users.
Individual players versus an Automatic System To estimate the performance of individual players,
we restrict our attention to players with at least 7 guesses. We find that individual players are, on average,
5.754 years off. A linear regression system achieves a MAE of 6.149 years and a Pearson correlation of
0.812. The small number of tweets in our data (20-40) increases the difficulty of the task for automatic
systems.
Age
Sta
nda
rd d
evia
tion
2
4
6
8
10
10 20 30 40 50 60
(a) Standard deviation and actual age
Actual age
Pre
dict
ed a
ge
10
20
30
40
50
60
10 20 30 40 50 60
Correct line
Human prediction
(b) Average age prediction by humans.
Figure 5: Age prediction
5.2 Inferring the Age of Older Twitter Users
Figure 5b shows the average player predictions with the actual age of the Twitter users. The red line is
the ?perfect? line, i.e. the line when the predictions would match the exact age. Black represents a fitted
LOESS curve (Cleveland et al., 1992) based on the human predictions. We find that the players tend
to overpredict the age of younger Twitter users, but even more strikingly, on average they consistently
underpredict the age of older Twitter users. The prediction errors already start at the end of the 20s, and
the gap between actual and predicted age increases with age.
This could be explained by sociolinguistic studies that have found that people between 30 and 55 years
use standard forms the most, because they experience the maximum societal pressure in the workplace to
conform (Holmes, 2013). On Twitter, this has been observed as well: Nguyen et al. (2013) found fewer
linguistic differences between older age groups than between younger age groups. This makes it difficult
for the crowd to accurately estimate the ages of older Twitter users. Younger people and retired people
use more non-standard forms (Holmes, 2013). Unfortunately, our dataset does not contain enough retired
users to analyze whether this trend is also present on Twitter.
1957
6 Discussion
We now discuss the implications of our findings for research on automatically predicting the gender and
age of authors from their texts.
Age and gender as social variables Most computational research has treated gender and age as fixed,
biological variables. The dominant approach is to use supervised machine learning methods to generalize
across a large number of examples (e.g. texts written by females and males). While the learned models
so far are effective at predicting age and gender of most people, they learn stereotypical behaviour and
therefore provide a simplistic view.
First, by using the crowd we have shown that Twitter users emphasize their gender and age in varying
degrees and in different ways, so that for example, treating gender as a binary variable is too simplistic
(Butler, 1990; Eckert and McConnell-Ginet, 2013). Many users do not employ the stereotypical language
associated with their biological sex, making models that take a static view of gender ineffective for such
users. More detailed error analyses of the prediction systems will increase understanding of the reasons
for incorrect predictions, and shed light on the relation between language use and social variables.
Second, models that assume static variables will not be able to model the interesting variation (Eisen-
stein, 2013). Models that build on recent developments in sociolinguistics will be more meaningful and
will also have the potential to contribute to new sociolinguistic insights. For example, modeling what
influences speakers to show more or less of their identity through language, or jointly modeling varia-
tion between and within speakers, are in our opinion interesting research directions. The ever increasing
amounts of social media data offer opportunities to explore these research directions.
Sampling We have shown that the difficulty of tasks such as gender and age prediction varies across
persons. Therefore, creating datasets for such tasks requires maximum attention. For example, when
a dataset is biased towards people who show a strong gender identity (e.g. by sampling followers of
accounts highly associated with males or females, such as sororities (Rao et al., 2010)), the results
obtained on such a set may not be representative of a more random set (as observed when classifying
political affiliation (Cohen and Ruths, 2013)).
Task difficulty Our study also raises the question of what level of performance can be obtained for
tasks such as predicting gender and age from only language use. Since we often form an impression
based on someone?s writing, crowd performance is a good indicator of the task difficulty. While the
crowd performance does not need to be the upper bound, it does indicate that it is difficult to predict
gender and age of a large number of Twitter users.
When taking the majority label, only 84% of the users were correctly classified according to their
biological sex. This suggests that about 16% of the Dutch Twitter users do not use language that the
crowd associates with their biological sex.
We also found that it is hard to accurately estimate the ages of older Twitter users, and we related
this to sociolinguistics studies who found less linguistic differences in older age groups due to societal
pressure in the workplace.
Limitations A limitation of our work is that we focused on language variation between persons, and not
on variation within persons. However, speakers vary their language depending on the context and their
conversation partners (e.g. accommodation effects were found in social media (Danescu-Niculescu-Mizil
et al., 2011)). For example, we assigned Twitter users an overall ?score? by placing them on a gender
continuum, ignoring the variation we find within users.
Crowdsourcing as a tool to understand NLP tasks Most research on crowdsourcing within the NLP
community has focused on how the crowd can be used to obtain fast and large amounts of annotations.
This study is an example of how the crowd can be used to obtain a deeper understanding of an NLP task.
We expect that other tasks where disagreement between annotators is meaningful (i.e. it is not only due
to noise), could potentially benefit from crowdsourcing experiments as well.
1958
7 Conclusion
In this paper, we demonstrated the successful use of the crowd to study the relation between language
use and social variables. In particular, we took a closer look at inferring gender and age from language
using data collected through an online game. We showed that treating gender and age as fixed variables
ignores the variety of ways people construct their identity through language.
Approaching age and gender as social variables will allow for richer analyses and more robust systems.
It has implications ranging from how datasets are created to how results are interpreted. We expect that
our findings also apply to other social variables, such as ethnicity and status. Instead of only focusing on
performance improvement, we encourage NLP researchers to also focus on what we can learn about the
relation between language use and social variables using computational methods.
Acknowledgements
This research was supported by the Royal Netherlands Academy of Arts and Sciences (KNAW)
and the Netherlands Organization for Scientific Research (NWO), grants IB/MP/2955 (TINPOT) and
640.005.002 (FACT). The third author is supported through the Digital Humanities research grant by
Tilburg University and a NIAS research fellowship. The authors would like to thank the players of the
TweetGenie game.
References
Lora Aroyo and Chris Welty. 2013. Crowd truth: Harnessing disagreement in crowdsourcing a relation extraction
gold standard. In Proceedings of WebSci?13.
David Bamman, Jacob Eisenstein, and Tyler Schnoebelen. 2014. Gender identity and lexical variation in social
media. Journal of Sociolinguistics, 18(2):135?160.
Federica Barbieri. 2008. Patterns of age-based linguistic variation in American English. Journal of Sociolinguis-
tics, 12(1):58?88.
Shane Bergsma and Benjamin Van Durme. 2013. Using conceptual class attributes to characterize social media
users. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages
710?720.
Victoria L. Bergvall, Janet M. Bing, and Alice F. Freed. 1996. Rethinking Language and Gender Research: Theory
and Practice. Routledge.
Victoria L. Bergvall. 1999. Toward a comprehensive theory of language and gender. Language in society,
28(02):273?293.
Mary Bucholtz and Kira Hall. 2005. Identity and interaction: A sociocultural linguistic approach. Discourse
studies, 7(4-5):585?614.
John D. Burger, John Henderson, George Kim, and Guido Zarrella. 2011. Discriminating gender on Twitter. In
Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 1301?1309.
Judith Butler. 1990. Gender Trouble: Feminism and the Subversion of Identity. Routledge.
William S. Cleveland, Eric Grosse, and William M. Shyu. 1992. Local regression models. Statistical models in S,
pages 309?376.
Raviv Cohen and Derek Ruths. 2013. Classifying political orientation on Twitter: It?s not easy! In Proceedings of
the Seventh International AAAI Conference on Weblogs and Social Media, pages 91?99.
Maral Dadvar, Franciska de Jong, Roeland Ordelman, and Dolf Trieschnigg. 2012. Improved cyberbullying de-
tection using gender information. In Proceedings of the Twelfth Dutch-Belgian Information Retrieval Workshop
(DIR 2012), pages 23?25.
Cristian Danescu-Niculescu-Mizil, Michael Gamon, and Susan Dumais. 2011. Mark my words!: linguistic style
accommodation in social media. In Proceedings of the 20th international conference on World Wide Web, pages
745?754.
1959
Sebastian Deterding, Dan Dixon, Rilla Khaled, and Lennart Nacke. 2011. From game design elements to game-
fulness: Defining ?gamification?. In Proceedings of the 15th International Academic MindTrek Conference:
Envisioning Future Media Environments, pages 9?15.
Penelope Eckert and Sally McConnell-Ginet. 2013. Language and gender. Cambridge University Press.
Penelope Eckert. 1997. Age as a sociolinguistic variable. The handbook of sociolinguistics, pages 151?167.
Penelope Eckert. 2008. Variation and the indexical field. Journal of Sociolinguistics, 12(4):453?476.
Penelope Eckert. 2012. Three waves of variation study: the emergence of meaning in the study of sociolinguistic
variation. Annual Review of Anthropology, 41:87?100.
Jacob Eisenstein. 2013. What to do about bad language on the internet. In Proceedings of the Annual Conference
of the North American Chapter of the Association for Computational Linguistics, pages 359?369.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A library
for large linear classification. Journal of Machine Learning Research, 9:1871?1874.
Katja Filippova. 2012. User demographics and language in an implicit social network. In Proceedings of the 2012
Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language
Learning, pages 1478?1488.
Clayton Fink, Jonathon Kopecky, and Maksym Morawski. 2012. Inferring gender from the content of tweets:
A region specific example. In Proceedings of the Sixth International AAAI Conference on Weblogs and Social
Media.
Nikesh Garera and David Yarowsky. 2009. Modeling latent biographic attributes in conversational genres. In
Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint
Conference on Natural Language Processing of the AFNLP, pages 710?718.
Sumit Goswami, Sudeshna Sarkar, and Mayur Rustagi. 2009. Stylometric analysis of bloggers? age and gender.
In Proceedings of the Third International ICWSM Conference, pages 214?217.
Janet Holmes and Miriam Meyerhoff. 2003. The handbook of language and gender. Wiley-Blackwell.
Janet Holmes. 2013. An introduction to sociolinguistics. Routledge.
John Le, Andy Edmonds, Vaughn Hester, and Lukas Biewald. 2010. Ensuring quality in crowdsourced search
relevance evaluation: The effects of training question distribution. In Proceedings of the SIGIR 2010 Workshop
on Crowdsourcing for Search Evaluation (CSE 2010), pages 21?26.
Judith Lorber. 1996. Beyond the binaries: Depolarizing the categories of sex, sexuality, and gender*. Sociological
Inquiry, 66(2):143?160.
Maxim Makatchev and Reid Simmons. 2011. Perception of personality and naturalness through dialogues by
native speakers of American English and Arabic. In Proceedings of the SIGDIAL 2011: the 12th Annual
Meeting of the Special Interest Group on Discourse and Dialogue, pages 286?293.
Alice E. Marwick and danah boyd. 2011. I tweet honestly, I tweet passionately: Twitter users, context collapse,
and the imagined audience. New Media & Society, 13(1):114?133.
Arjun Mukherjee and Bing Liu. 2010. Improving gender classification of blog authors. In Proceedings of the
2010 Conference on Empirical Methods in Natural Language Processing, pages 207?217.
Robert Munro, Steven Bethard, Victor Kuperman, Vicky Tzuyin Lai, Robin Melnick, Christopher Potts, Tyler
Schnoebelen, and Harry Tily. 2010. Crowdsourcing and language studies: the new generation of linguistic
data. In Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon?s
Mechanical Turk, pages 122?130.
Dong Nguyen, Noah A Smith, and Carolyn P. Ros?e. 2011. Author age prediction from text using linear regression.
In Proceedings of the 5th ACL-HLT Workshop on Language Technology for Cultural Heritage, Social Sciences,
and Humanities, pages 115?123.
Dong Nguyen, Rilana Gravel, Dolf Trieschnigg, and Theo Meder. 2013. ?How old do you think I am??: A study
of language and age in Twitter. In Proceedings of the Seventh International AAAI Conference on Weblogs and
Social Media, pages 439?448.
1960
Dong Nguyen, Dolf Trieschnigg, and Theo Meder. 2014. Tweetgenie: Development, evaluation, and lessons
learned. In Proceedings of COLING 2014.
Fabian Pedregosa, Ga?el Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Math-
ieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, Jake Vanderplas, Alexandre Passos, David Cour-
napeau, Matthieu Brucher, Matthieu Perrot, and
?
Edouard Duchesnay. 2011. Scikit-learn: Machine learning in
Python. Journal of Machine Learning Research, 12:2825?2830.
Claudia Peersman, Walter Daelemans, and Leona Van Vaerenbergh. 2011. Predicting age and gender in online so-
cial networks. In Proceedings of the 3rd international workshop on Search and mining user-generated contents,
pages 37?44.
James W. Pennebaker and Lori D. Stone. 2003. Words of wisdom: Language use over the life span. Journal of
personality and social psychology, 85(2):291?301.
Delip Rao, David Yarowsky, Abhishek Shreevats, and Manaswi Gupta. 2010. Classifying latent user attributes
in Twitter. In Proceedings of the 2nd international workshop on Search and mining user-generated contents,
pages 37?44.
Sara Rosenthal and Kathleen McKeown. 2011. Age prediction in blogs: a study of style, content, and online be-
havior in pre- and post-social media generations. In Proceedings of the 49th Annual Meeting of the Association
for Computational Linguistics, pages 763?772.
Jonathan Schler, Moshe Koppel, Shlomo Argamon, and James W. Pennebaker. 2005. Effects of age and gender
on blogging. In Proceedings of AAAI Spring Symposium on Computational Approaches for Analyzing Weblogs,
pages 199?205.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and Andrew Y. Ng. 2008. Cheap and fast?but is it good?:
Evaluating non-expert annotations for natural language tasks. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing, pages 254?263.
Svitlana Volkova, Theresa Wilson, and David Yarowsky. 2013. Exploring demographic language variations to
improve multilingual sentiment analysis in social media. In Proceedings of the 2013 Conference on Empirical
Methods on Natural Language Processing, pages 1815?1827.
Suzanne E. Wagner. 2012. Age grading in sociolinguistic theory. Language and Linguistics Compass, 6(6):371?
382.
1961
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: System Demonstrations,
pages 62?66, Dublin, Ireland, August 23-29 2014.
TweetGenie: Development, Evaluation, and Lessons Learned
Dong Nguyen
1
Dolf Trieschnigg
1
Theo Meder
2
(1) Human Media Interaction, University of Twente, Enschede, The Netherlands
(2) Meertens Institute, Amsterdam, The Netherlands
{d.nguyen,d.trieschnigg}@utwente.nl, theo.meder@meertens.knaw.nl
Abstract
TweetGenie is an online demo that infers the gender and age of Twitter users based on their
tweets. TweetGenie was able to attract thousands of visitors. We collected data by asking feed-
back from visitors and launching an online game. In this paper, we describe the development of
TweetGenie and evaluate the demo based on the received feedback and manual annotation. We
also reflect on practical lessons learned from launching a demo for the general public.
1 Introduction
The language use of speakers is related to variables such as the speaker?s gender and age (Eckert, 1997;
Eckert and McConnell-Ginet, 2013). Systems that can automatically predict such variables have been
receiving increasing attention. They enable more fine-grained analyses of trends by profiling the involved
users. They also support sociolinguistics research by shedding light on the link between variables such
as gender and age, and the language use of speakers.
In this paper, we describe TweetGenie (www.tweetgenie.nl), a website that allows visitors to enter
public Dutch Twitter accounts. The system predicts gender and age of the users behind the entered
accounts based on the 200 most recent tweets. Due to press attention from various media outlets, we
were able to attract a large number of visitors. In comparison to previous gender and age prediction
systems that have been evaluated with carefully constructed datasets, we are the first to evaluate the
performance of such a system ?in the wild?.
We first discuss the development of TweetGenie (Section 2). Next, we study the launch and TweetGe-
nie?s spread through social media, based on log data of the first week after the launch (Section 3). We
then evaluate TweetGenie based on collected feedback (Section 4) and reflect on practical issues we en-
countered while launching an online demo for the general public (Section 5). We end with a conclusion
(Section 6).
2 TweetGenie
In this section we describe the development and setup of TweetGenie.
Goals The original research (Nguyen et al., 2013) was carried out to support analyses of trends and to
study sociolinguistic aspects of language use. By launching a public demo of this research, we aimed
to 1) test the system on a large-scale ?in the wild? 2) collect data, and 3) demo the project to interested
people. Unlike most demos of NLP research, the target audience of this demo was the ?general public?.
For example, we aimed for a simple and attractive interface, and released a press announcement to reach
a large audience.
Model TweetGenie was developed based on the research and dataset described in (Nguyen et al., 2013)
and predicts the gender and age of Dutch Twitter users based on the 200 most recent tweets. First,
unigrams and bigrams are extracted from the tweets using the tokenization tool by O?Connor et al.
(2010). This feature representation was chosen, because it is fast and unigrams have shown to perform
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
62
already very well (Nguyen et al., 2013). We then trained logistic (for gender prediction) and linear (for
age prediction) regression models (Pedregosa et al., 2011) with L2 regularization.
Setup TweetGenie is available at www.tweetgenie.nl. After a visitor enters a public Twitter account,
a results page is shown (see Figure 1 for a screenshot). The results page shows the predicted age (in
years), the gender, and a gender ?score? indicating how strong the prediction was (based on x
>
? with
x being the features and ? the estimated parameters). In addition, an option is available to share their
results page on Twitter.
An overview of the components is shown in Figure 3. The first webserver hosts the frontend. A
MySQL database is used to keep track of the progress of each prediction, and to store logs and feed-
back received by users. A second webserver is used to retrieve the data from Twitter and perform the
predictions.
Feedback To collect data and improve the system, users are encouraged to provide feedback on the
predictions. On the page with the automatic prediction (Figure 1), users have the option to enter the
correct age and confirm whether the gender prediction was correct.
Online Game We also developed an online game to study how humans perform on the task. Figure 2
shows a screenshot of the interface. Players are shown 20-40 tweets per Twitter user and have to guess
the gender and age of the Twitter users behind the tweets. After each guess, players receive feedback
in various ways (the correct gender and age, the predictions by the automatic system, and the average
guesses by other players). The data collected proved to be valuable: using the data, we reflected on the
task of inferring gender and age from tweets and the limitations of current systems (Nguyen et al., 2014).
Figure 1: Screenshot prediction interface
Figure 2: Screenshot online game. Based on the shown tweets, players are asked to guess the gender and
age of the user behind the tweets.
63
3 Launching TweetGenie
TweetGenie was launched on May 13, 2013 at around 11.30 AM. To reach a large audience, a press
statement was released and messages were posted on social media networks. In this section, we analyze
the data in the first week after the launch.
Figure 3 shows the number of entered Twitter users and the number of tweets mentioning TweetGenie
in the first week after the launch. The number of tweets and the number of users entered follow similar
trends. We observe a high peak in the beginning, but it also rapidly decreases over time. The system
was asked to make a prediction 87,818 times and 9,291 tweets were posted with the word ?TweetGenie?.
1,931 of these tweets were created using the tweet sharing function of TweetGenie. The observed senti-
ment was mostly positive. If TweetGenie made an incorrect prediction, most people joked about it (e.g.
?*grin* I just became 13 years younger without plastic chirurgy #tweetgenie?). The game was played
often as well, a guess was made 31,414 times.
TimeNu
m. T
witte
r us
ers 
/ hou
r
0
400
0
800
0
0
200
400
13 May 20 May Nu
m. t
wee
ts / h
our
# Users# Tweets
(a) Entered users and tweets per hour
Webserver 1
Frontend(PHP, jQuery)
Webserver 2
Backend(Python)
MySQL database
Twitter
(b) Architecture
Figure 3: Overview of the system
4 Evaluation
We evaluate the system in two ways, 1) using the feedback from users, and 2) using manual annotation.
4.1 Evaluation Based on User Feedback
Visitors were encouraged to give feedback on the predictions of TweetGenie. In the first week, we
received feedback on the gender of 16,563 users and on the age of 17,034 users.
Reliability We randomly sampled 150 Twitter users for which we received feedback on both the gender
and age. We checked the feedback of these users by visiting their Twitter profiles. If the feedback seemed
plausible based on the profile, we assumed the feedback was correct (i.e. we did not visit any other social
media profiles to find the exact age). The results are shown in Table 1. We find that 90% of the feedback
appears to be correct. Only a small fraction (4%) of the feedback was incorrect, this could be deliberate
or due to sloppiness. The remaining feedback was on Twitter accounts of non-Dutch users (e.g. English,
German, French), or accounts that did not represent a person (e.g. a sports team, animal, multiple
persons).
Accuracy We calculate the performance based on the 135 users for who we received correct feedback.
We find that the users who gave feedback are not representative of the general Dutch Twitter population
(Nguyen et al., 2013). The users are older than average (the age distribution is shown in Figure 4). There
are more older males, and more younger females using Twitter in the Netherlands (Nguyen et al., 2013),
and as a consequence the number of males (60.7%) is higher than the number of females (39.3%).
Based on this dataset, we find that the accuracy of the gender predictions was 94%. The Mean Absolute
Error (MAE) for the age predictions is 6.1 years, which is higher than reported in (Nguyen et al., 2013).
64
Feedback Frequency Percentage
Correct 135 90%
Incorrect 6 4%
Not a Dutch account 5 3.33%
Not a person 4 2.67%
Table 1: Statistics feedback reliability
Age
Frequ
ency
10 20 30 40 50 60 700
51
01
52
02
5
Figure 4: Age distribution feedback
However, this can be explained by the observation that relatively many older Twitter users give feedback,
and as discussed in (Nguyen et al., 2013), automatic age predictions for older Twitter users are less
accurate.
4.2 Evaluation Based on Manual Annotation
We also evaluated the system by manually annotating 50 users that were randomly sampled from the
entered users in the logs. We did not include accounts that were not Dutch or did not represent individual
persons. If feedback was available for a Twitter user, we used the provided feedback (after a manual
check). Otherwise, we manually annotated the gender and age using all available information (e.g. social
media profiles, websites). The gender was correctly predicted for 82% of the users, which is lower than
measured in the evaluation based on the user feedback (Section 4.1). The Mean Absolute Error (MAE)
is 6.18 years, which is in line with the observed MAE based on the user feedback.
Our analyses confirm that users for who feedback was available are not representative of all users who
were entered in the system. Of the sampled 50 entered users, the fraction of males and females is almost
equal (52% and 48%) compared to 60.7% and 30.9% in Section 4.1. The number of users who were less
than 20 years old (15) is similar to the number of users in the range of > 20 and? 30 years (17), while in
Section 4.1 the fraction of users below 20 years is smaller. Thus, less feedback was received for younger
Twitter users.
In line with the analysis in Section 4.1, we find that relatively many older Twitter users were entered
into TweetGenie compared to a more representative set of Dutch Twitter users (Nguyen et al., 2013).
5 Lessons Learned
We learned many lessons from launching a demo for the general public.
1) Test all components of the demo. While developing the system, we focused mostly on ensuring
that the backend would be able to handle the number of visitors. However, after the demo went online,
problems arose at the frontend due to the visitor load. This was solved by only allowing a fixed number
of visitors at the same time. We also did not test the interface for non-Dutch visitors. Only later we found
out that the automatically translated version contained serious errors: international visitors were misled
that the model worked on English tweets.
2) The distribution of users trying out the demo might not correspond to the distribution in the devel-
opment dataset. While we extensively evaluated the system on a carefully constructed, representative
dataset (Nguyen et al., 2013), the numbers in this paper?s evaluation are lower. Users who were entered
into the system were not representative of the Dutch Twitter population: relatively more older Twitter
users were entered in the system, leading to more errors in the automic age prediction.
3) A demo is a good opportunity to collect data. Many visitors were willing to provide feedback or
participated in the online game. Data collected through the online game has been used to study the task
of inferring gender and age in more depth (Nguyen et al., 2014). Manual analysis of the feedback in this
paper revealed that almost all of the feedback appears to be genuine. Further research is needed to study
how the feedback on the automatic predictions can be used to improve the prediction models.
65
6 Conclusion
In this paper we discussed TweetGenie, an online system that infers the gender and age of Twitter users
based on tweets alone. We collected much feedback from the users, but also found that users who
provided feedback are not representative of all the entered users. We demonstrated that besides being a
valuable tool for user profiling, TweetGenie also appeals to the general public.
Acknowledgements
This research was supported by the Royal Netherlands Academy of Arts and Sciences (KNAW)
and the Netherlands Organization for Scientific Research (NWO), grants IB/MP/2955 (TINPOT) and
640.005.002 (FACT).
References
P. Eckert and S. McConnell-Ginet. 2013. Language and gender. Cambridge University Press.
P. Eckert. 1997. Age as a sociolinguistic variable. The handbook of sociolinguistics. Blackwell Publishers.
D. Nguyen, R. Gravel, D. Trieschnigg, and T. Meder. 2013. ?How old do you think I am??: A study of language
and age in Twitter. In Proceedings of ICWSM 2013.
D. Nguyen, D. Trieschnigg, A. S. Do?gru?oz, R. Gravel, M. Theune, T. Meder, and F.M.G. de Jong. 2014. Why
gender and age prediction from tweets is hard: Lessons from a crowdsourcing experiment. In Proceedings of
COLING 2014.
B. O?Connor, M. Krieger, and D. Ahn. 2010. TweetMotif: exploratory search and topic summarization for Twitter.
In Proceedings of ICWSM 2010.
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss,
V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. 2011. Scikit-
learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825?2830.
66
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 857?862,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Word Level Language Identification in Online Multilingual Communication
Dong Nguyen1 A. Seza Dog?ruo?z23
(1) Human Media Interaction, University of Twente, Enschede, The Netherlands
(2) Tilburg School of Humanities, Tilburg University, Tilburg, The Netherlands
(3) Language Technologies Institute, Carnegie Mellon University, Pittsburgh, USA
dong.p.ng@gmail.com, a.s.dogruoz@gmail.com
Abstract
Multilingual speakers switch between lan-
guages in online and spoken communication.
Analyses of large scale multilingual data re-
quire automatic language identification at the
word level. For our experiments with mul-
tilingual online discussions, we first tag the
language of individual words using language
models and dictionaries. Secondly, we incor-
porate context to improve the performance.
We achieve an accuracy of 98%. Besides word
level accuracy, we use two new metrics to
evaluate this task.
1 Introduction
There are more multilingual speakers in the world
than monolingual speakers (Auer and Wei, 2007).
Multilingual speakers switch across languages in
daily communication (Auer, 1999). With the in-
creasing use of social media, multilingual speakers
also communicate with each other in online environ-
ments (Paolillo, 2011). Data from such resources
can be used to study code switching patterns and lan-
guage preferences in online multilingual conversa-
tions. Although most studies on multilingual online
communication rely on manual identification of lan-
guages in relatively small datasets (Danet and Her-
ring, 2007; Androutsopoulos, 2007), there is a grow-
ing demand for automatic language identification in
larger datasets. Such a system would also be useful
for selecting the right parsers to process multilingual
documents and to build language resources for mi-
nority languages (King and Abney, 2013).
In this paper, we identify Dutch (NL) en Turkish
(TR) at the word level in a large online forum for
Turkish-Dutch speakers living in the Netherlands.
The users in the forum frequently switch languages
within posts, for example:
<TR> Sariyi ver </TR>
<NL> Wel mooi doelpunt </NL>
So far, language identification has mostly been mod-
eled as a document classification problem. Most ap-
proaches rely on character or byte n-grams, by com-
paring n-gram profiles (Cavnar and Trenkle, 1994),
or using various machine learning classifiers. While
McNamee (2005) argues that language identification
is a solved problem, classification on a more fine-
grained level (instead of document level) remains a
challenge (Hughes et al, 2006). Furthermore, lan-
guage identification is more difficult for short texts
(Baldwin and Lui, 2010; Vatanen et al, 2010), such
as queries and tweets (Bergsma et al, 2012; Carter
et al, 2012; Ceylan and Kim, 2009). Tagging in-
dividual words (without context) has been done us-
ing dictionaries, affix statistics and classifiers us-
ing character n-grams (Hammarstro?m, 2007; Got-
tron and Lipka, 2010). Although Yamaguchi and
Tanaka-Ishii (2012) segmented text by language,
their data was artificially created by randomly sam-
pling and concatenating text segments (40-160 char-
acters) from monolingual texts. Therefore, the lan-
guage switches do not reflect realistic switches as
they occur in natural texts. Most related to ours is
the work by King and Abney (2013) who labeled
languages of words in multilingual web pages, but
evaluated the task only using word level accuracy.
857
Our paper makes the following contributions: 1)
We explore two new ways to evaluate the task for an-
alyzing multilingual communication and show that
only word accuracy gives a limited view 2) We are
the first to apply this task on a conversational and
larger dataset 3) We show that features using the
context improve the performance 4) We present a
new public dataset to support research on language
identification.
In the rest of the paper, we first discuss the related
work and describe our dataset. Secondly, we present
our experiments. We finally conclude with a sum-
mary and suggestions for future work.
2 Corpus
Our data1 comes from one of the largest online
communities in The Netherlands for Turkish-Dutch
speakers. All posts from May 2006 until October
2012 were crawled. Although Dutch and Turkish
dominate the forum, English fixed phrases (e.g. no
comment, come on) are also occasionally observed.
Users switch between languages within and across
posts. Examples 1 and 2 illustrate switches between
Dutch and Turkish within the same post. Example 1
is a switch at sentence level, example 2 is a switch
at word level.
Example 1:
<NL>Mijn dag kan niet stuk :) </NL>
<TR> Cok guzel bir haber aldim </TR>
Translation: <NL> This made my day:)
</NL><TR> I received good news
</TR>
Example 2:
<TR>kahvalti</TR><NL>met
vriendinnen by my thuis </NL>
Translation: <TR>breakfast </TR>
<NL> with my girlfriends at my home
</NL>
The data is highly informal with misspellings,
lengthening of characters (e.g. hotttt), replacement
of Turkish characters (kahvalti instead of kahvalt?)
and spelling variations (tankyu instead of thank
you). Dutch and Turkish sometimes share common
spellings (e.g. ben is am in Dutch and I in Turkish),
making this a challenging task.
1Available at http://www.dongnguyen.nl/data-langid-
emnlp2013.html
Annotation
For this research, we classify words as either Turkish
or Dutch. Since Dutch and English are typologically
more similar to each other than Turkish, the English
phrases (less than 1%) are classified as Dutch. Posts
were randomly sampled and annotated by a native
Turkish speaker who is also fluent in Dutch. A na-
tive Dutch speaker annotated a random set of 100
posts (Cohen?s kappa = 0.98). The following tokens
were ignored for language identification:
? Smileys (as part of the forum markup, as well
as textual smileys such as ?:)? ).
? Numeric tokens and punctuation.
? Forum tags (e.g. [u] to underline text).
? Links, images, embedded videos etc.
? Turkish and Dutch first names and place
names2.
? Usernames when indicated with special forum
markup.
? Chat words, such as hahaha, ooooh and lol rec-
ognized using regular expressions.
Posts for which all tokens are ignored, are not
included in the corpus.
Statistics
The dataset was randomly divided into a training,
development and test set. The statistics are listed
in Table 1. The statistics show that Dutch is the
majority language, although the difference between
Turkish and Dutch is not large. We also find that the
documents (i.e. posts) are short, with on average 18
tokens per document. The data represents realistic
texts found in online multilingual communication.
Compared to previously used datasets (Yamaguchi
and Tanaka-Ishii, 2012; King and Abney, 2013), the
data is noisier and the documents are much shorter.
#NL tokens #TR tokens #Posts/(BL%)
Train 14900 (54%) 12737 (46%) 1603 (15%)
Dev 8590 (51%) 8140 (49%) 728 (19%)
Test 5895 (53%) 5293 (47%) 735 (17%)
Table 1: Number of tokens and posts for Dutch (NL) and
Turkish (TR), including % of bilingual (BL) posts
2Based on online name lists and Wikipedia pages
858
3 Experimental Setup
3.1 Training Corpora
We used the following corpora to extract dictionaries
and language models.
? GenCor: Turkish web pages (Sak et al, 2008).
? NLCOW2012: Dutch web pages (Scha?fer and
Bildhauer, 2012).
? Blog authorship corpus: English blogs (Schler
et al, 2006).
Each corpus was chunked into large segments
which were then selected randomly until 5M tokens
were obtained for each language. We tokenized the
text and kept the punctuation.
3.2 Baselines
As baselines, we use langid.py3 (Lui and Bald-
win, 2012) and van Noord?s TextCat implementa-
tion4 of the algorithm by Cavnar and Trenkle (1994).
TextCat is based on the comparison of n-gram pro-
files and langid.py on Naive Bayes with n-gram fea-
tures. For both baselines, words were entered indi-
vidually to each program. Words for which no lan-
guage could be determined were assigned to Dutch.
These models were developed to identify the lan-
guages of the documents instead of words and we
did not retrain them. Therefore, these models are
not expected to perform well on this task.
3.3 Models
We start with models that assign languages based on
only the current word. Next, we explore models and
features that can exploit the context (the other words
in the post). Words with the highest probability for
English were assigned to Dutch for evaluation.
Dictionary lookup (DICT)
We extract dictionaries with word frequencies from
the training corpora. This approach looks up the
words in the dictionaries and chooses the language
for which the word has the highest probability. If
the word does not occur in the dictionaries, Dutch is
chosen as the language.
3https://github.com/saffsd/langid.py
4http://www.let.rug.nl/?vannoord/TextCat/
Language model (LM)
We build a character n-gram language model for
each language (max. n-gram length is 5). We use
Witten-Bell smoothing and include word boundaries
for calculating the probabilities.
Dictionary + Language model (DICT+LM)
We first use the dictionary lookup approach (DICT).
If the word does not occur in dictionaries, a decision
is made using the language models (LM).
Logistic Regression (LR)
We use a logistic regression model that incorporates
context with the following features:
? (Individual word) Label assigned by the
DICT+LM model.
? (Context) The results of the LM model based on
previous + current token, and current token +
next token (e.g. the sequence ?ben thuis? (am
home) as a whole if ben is the current token).
This gives the language model more context for
estimation. We compare the use of the assigned
labels (LAB) with the use of the log probability
values (PROB) as feature values.
Conditional Random Fields (CRF)
We treat the task as a sequence labeling problem and
experiment with linear-chain Conditional Random
Fields (Lafferty et al, 2001) in three settings:
? (Individual word) A CRF with only the tags as-
signed by the DICT+LM to the individual to-
kens as a feature (BASE).
? (Context). CRFs using the LAB or PROB as ad-
ditional features (same features as in the logis-
tic regression model) to capture additional con-
text.
3.4 Implementation
Language identification was not performed for texts
within quotes. To handle the alphabetical length-
ening (e.g. lolllll), words are normalized by trim-
ming same character sequences of three characters
or more. We use the Lingpipe5 and Scikit-learn (Pe-
dregosa et al, 2011) toolkits for our experiments.
5http://alias-i.com/lingpipe/
859
Word classification Fraction Post classification
TR NL MAE
Run P R P R Acc. ? All Mono. BL F1 Acc.
Textcat 0.872 0.647 0.743 0.915 0.788 0.739 0.251 0.264 0.188 0.386 0.396
LangIDPy 0.954 0.387 0.641 0.983 0.701 0.615 0.364 0.371 0.333 0.413 0.475
DICT 0.955 0.733 0.802 0.969 0.858 0.827 0.196 0.200 0.175 0.511 0.531
LM 0.950 0.930 0.938 0.956 0.944 0.926 0.074 0.076 0.065 0.699 0.703
DICT + LM 0.951 0.934 0.942 0.957 0.946 0.943 0.067 0.067 0.063 0.711 0.717
LR + LAB 0.965 0.952 0.958 0.969 0.961 0.917 0.066 0.066 0.068 0.791 0.808
LR + PROB 0.956 0.976 0.978 0.959 0.967 0.945 0.048 0.044 0.064 0.826 0.849
CRF + BASE 0.973 0.974 0.977 0.976 0.975 0.940 0.043 0.027 0.119 0.858 0.898
CRF + LAB 0.964 0.977 0.979 0.967 0.972 0.933 0.046 0.033 0.111 0.855 0.891
CRF + PROB 0.970 0.980 0.982 0.973 0.976 0.946 0.039 0.025 0.103 0.853 0.895
Table 2: Results of language identification experiments.
3.5 Evaluation
The assigned labels can be used for computational
analysis of multilingual data in different ways. For
example, these labels can be used to analyze lan-
guage preferences in multilingual communication or
the direction of the switches (from Turkish to Dutch
or the other way around). Therefore, we evaluate the
methods from different perspectives.
The evaluation at word and post levels is done
with the following metrics:
? Word classification precision (P), recall (R) and
accuracy. Although this is the most straightfor-
ward approach to evaluate the task, it ignores
the document boundaries.
? Fraction of language in a post: Pearson?s cor-
relation (?) and Mean Absolute Error (MAE) of
proportion of Turkish in a post. This evaluates
the measured proportion of languages in a post
when the actual tags for individual words are
not needed. For example, such information is
useful for analyzing the language preferences
of users in the online forum. Besides report-
ing the MAE over all posts, we also separate
the performance over monolingual and bilin-
gual posts (BL).
? Post classification: Durham (2003) analyzed
the switch between languages in terms of the
amount of monolingual and bilingual posts.
Our posts are classified as NL, TR or bilingual
(BL) if all words are tagged in the particular
language or both. We report F1 and accuracy.
4 Results
The results are presented in Table 2. Significance
tests were done by comparing the results of the word
and post classification measures using McNemar?s
test, and comparing the MAEs using paired t-tests.
All runs were significantly different from each other
based on these tests (p < 0.05), except the MAEs of
the DICT+LM and LR+LAB runs and the MAEs and
post classification metrics between the CRFs runs.
The difficulty of the task is illustrated by exam-
ining the coverage of the tokens by the dictionaries.
24.6% of the tokens (dev + test set) appear in both
dictionaries, 31.1% only in the Turkish dictionary,
30.5% only in the Dutch dictionary and 13.9% in
none of the dictionaries.
The baselines do not perform well. This confirms
that language identification at the word level needs
different approaches than identification at the docu-
ment level. Using language models result in a bet-
ter performance than dictionaries. They can han-
dle unseen words and are more robust against the
noisy spellings. The combination of language mod-
els and dictionaries is more effective than the indi-
vidual models. The results improve when context
was added using a logistic regression model, espe-
cially with the probability values as feature values.
CRFs improve the results but the improvement
on the correlation and MAE is less. More specifi-
cally, CRFs improve the performance on monolin-
gual posts, especially when a single word is tagged
in the wrong language. However, when the influence
of the context is too high, CRFs reduce the perfor-
mance in bilingual posts.
860
This is also illustrated with the results of the post
classification. The LR+PROB run has a high recall
(0.905), but a low precision (0.559) for bilingual
posts, while the CRF+PROB approach has a low re-
call (0.611) and a high precision (0.828).
The fraction of Dutch and Turkish in posts varies
widely, providing additional challenges to the use of
CRFs for this task. Classifying posts first as mono-
lingual/bilingual and tagging individual words after-
wards for bilingual posts might improve the perfor-
mance.
The evaluation metrics highlight different aspects
of the task whereas word level accuracy gives a
limited view. We suggest using multiple metrics to
evaluate this task for future research.
Dictionaries versus Language Models
The results reported in Table 2 were obtained by
sampling 5M tokens of each language. To study the
effect of the number of tokens on the performance
of the DICT and LM runs, we vary the amount of
data. The performance of both methods increases
consistently with more data (Figure 1). We also
find that language models achieve good performance
with only a limited amount of data, and consistently
outperform the approach using dictionaries. This is
probably due to the highly informal and noisy nature
of our data.
Num. sampled tokens
Accu
racy
0.6
0.7
0.8
0.9
1.0
0 2 ? 10
6
4 ? 10
6
LM
DICT
Figure 1: Effect of sampling size
Post classification
We experimented with classifying posts into TR, NL
and bilingual using the results of the word level lan-
guage identification (Table 2: post classification).
Posts were classified as a particular language if all
words were tagged as belonging to that language,
and bilingual otherwise. Runs using CRFs achieved
the best performance.
We now experiment with allowing a margin (e.g.
a margin of 0.10 classifies posts as TR if at least
90% of the words are classified as TR). Allowing
a small margin already increases the results of sim-
pler approaches (such as the LR-PROB run, Table 3)
by making it more robust against errors. However,
allowing a margin reduces the performance of the
CRF runs.
Margin 0.0 0.05 0.10 0.15 0.20
Accuracy 0.849 0.873 0.876 0.878 0.865
Table 3: Effect of margin on post classification
(LR-PROB run)
Error analysis
The manual analysis of the results revealed three
main challenges: 1) Our data is highly informal
with many spelling variations (e.g. moimoimoi,
goooooooooooolllll) and noise (e.g. asdfghjfgsha-
haha) 2) Words sharing spelling in Dutch and Turk-
ish are difficult to identify especially when there
is no context available (e.g. a post with only one
word). These words are annotated based on their
context. For example, the word super in ?Seyma,
super? is annotated as Turkish since Seyma is also a
Turkish word. 3) Named entity recognition is neces-
sary to improve the performance of the system and
decrease the noise in evaluation. Based on precom-
piled lists, our system ignores named entities. How-
ever, some names still remain undetected (e.g. user-
names).
5 Conclusion
We presented experiments on identifying the lan-
guage of individual words in multilingual conversa-
tional data. Our results reveal that language models
are more robust than dictionaries and adding context
improves the performance. We evaluate our methods
from different perspectives based on how language
identification at word level can be used to analyze
multilingual data. The highly informal spelling in
online environments and the occurrences of named
entities pose challenges.
Future work could focus on cases with more than
two languages, and languages that are typologically
less distinct from each other or dialects (Trieschnigg
et al, 2012).
861
6 Acknowledgements
The first author was supported by the Netherlands
Organization for Scientific Research (NWO) grant
640.005.002 (FACT) and the second author through
a postdoctoral research grant in E-Humanities (Digi-
tal Humanities) by Tilburg University (NL). The au-
thors would like to thank Marie?t Theune and Dolf
Trieschnigg for feedback.
References
J. Androutsopoulos, 2007. The multilingual internet.
Language, Culture and communication online, chapter
Language choice and code-switching in German-based
diasporic web-forums., pages 340?361. Oxford: Ox-
ford University Press.
P. Auer and L. Wei. 2007. Introduction: Multilingual-
ism as a problem? Monolingualism as a problem? In
Handbook of Multilingualism and Multilingual Com-
munication, volume 5 of Handbooks of Applied Lin-
guistics, pages 1?14. Mouton de Gruyter.
P. Auer. 1999. From codeswitching via language mix-
ing to fused lects toward a dynamic typology of bilin-
gual speech. International Journal of Bilingualism,
3(4):309?332.
T. Baldwin and M. Lui. 2010. Language identification:
the long and the short of the matter. In Proceedings of
NAACL 2010.
S. Bergsma, P. McNamee, M. Bagdouri, C. Fink, and
T. Wilson. 2012. Language identification for creat-
ing language-specific twitter collections. In Proceed-
ings of the Second Workshop on Language in Social
Media.
S. Carter, W. Weerkamp, and M. Tsagkias. 2012. Mi-
croblog language identification: Overcoming the limi-
tations of short, unedited and idiomatic text. Language
Resources and Evaluation, pages 1?21.
W.B. Cavnar and J. M. Trenkle. 1994. N-gram-based
text categorization. In Proceedings of Third Annual
Symposium on Document Analysis and Information
Retrieval.
H. Ceylan and Y. Kim. 2009. Language identification of
search engine queries. In Proceedings of ACL 2009.
B. Danet and S. C. Herring. 2007. The multilingual In-
ternet: Language, culture, and communication online.
Oxford University Press Oxford.
M. Durham. 2003. Language choice on a Swiss mailing
list. Journal of Computer-Mediated Communication,
9(1).
T. Gottron and N. Lipka. 2010. A comparison of lan-
guage identification approaches on short, query-style
texts. In Proceedings of ECIR 2010.
H. Hammarstro?m. 2007. A fine-grained model for lan-
guage identification. In Proceedings of iNEWS-07
Workshop at SIGIR 2007.
B. Hughes, T. Baldwin, S. Bird, J. Nicholson, and
A. Mackinlay. 2006. Reconsidering language identifi-
cation for written language resources. In Proceedings
of LREC 2006.
B. King and S. Abney. 2013. Labeling the languages
of words in mixed-language documents using weakly
supervised methods. In Proceedings of NAACL 2013.
J. Lafferty, A. McCallum, and F. C. N. Pereira. 2001.
Conditional random fields: Probabilistic models for
segmenting and labeling sequence data. In Proceed-
ings of ICML 2001.
M. Lui and T. Baldwin. 2012. langid.py: an off-the-shelf
language identification tool. In Proceedings of ACL
2012.
P. McNamee. 2005. Language identification: a solved
problem suitable for undergraduate instruction. Jour-
nal of Computing Sciences in Colleges, 20(3):94?101.
J.C. Paolillo. 2011. ?Conversational? codeswitching on
Usenet and Internet Relay Chat. Language@Internet,
8(3).
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer,
R. Weiss, V. Dubourg, J. Vanderplas, A. Passos,
D. Cournapeau, M. Brucher, M. Perrot, and E. Duches-
nay. 2011. Scikit-learn: machine learning in Python.
Journal of Machine Learning Research, 12:2825?
2830.
H. Sak, T. Gu?ngo?r, and M. Sarac?lar. 2008. Turkish lan-
guage resources: Morphological parser, morphologi-
cal disambiguator and web corpus. In GoTAL 2008,
volume 5221 of LNCS, pages 417?427. Springer.
R. Scha?fer and F. Bildhauer. 2012. Building large cor-
pora from the web using a new efficient tool chain. In
Proceedings of LREC 2012.
J. Schler, M. Koppel, S. Argamon, and J. Pennebaker.
2006. Effects of age and gender on blogging. In Pro-
ceedings of 2006 AAAI Spring Symposium on Compu-
tational Approaches for Analyzing Weblogs.
D. Trieschnigg, D. Hiemstra, M. Theune, F. Jong, and
T. Meder. 2012. An exploration of language identifi-
cation techniques for the Dutch folktale database. In
Adaptation of Language Resources and Tools for Pro-
cessing Cultural Heritage workshop (LREC 2012).
T. Vatanen, J. J. Va?yrynen, and S. Virpioja. 2010. Lan-
guage identification of short text segments with n-
gram models. In Proceedings of LREC 2010.
H. Yamaguchi and K. Tanaka-Ishii. 2012. Text segmen-
tation by language using minimum description length.
In Proceedings of ACL 2012.
862
Proceedings of the Workshop on Language in Social Media (LSM 2011), pages 76?85,
Portland, Oregon, 23 June 2011. c?2011 Association for Computational Linguistics
Language use as a reflection of socialization in online communities
Dong Nguyen
Carnegie Mellon University
Language Technologies Institute
Pittsburgh, PA 15213
dongn@cs.cmu.edu
Carolyn P. Rose?
Carnegie Mellon University
Language Technologies Institute
Pittsburgh, PA 15213
cprose@cs.cmu.edu
Abstract
In this paper we investigate the connection be-
tween language and community membership
of long time community participants through
computational modeling techniques. We re-
port on findings from an analysis of language
usage within a popular online discussion fo-
rum with participation of thousands of users
spanning multiple years. We find community
norms of long time participants that are char-
acterized by forum specific jargon and a style
that is highly informal and shows familiarity
with specific other participants and high emo-
tional involvement in the discussion. We also
find quantitative evidence of persistent shifts
in language usage towards these norms across
users over the course of the first year of com-
munity participation. Our observed patterns
suggests language stabilization after 8 or 9
months of participation.
1 Introduction
In this paper we use text mining and machine
learning methodologies as lenses through which to
understand the connection between language use
and community membership in online communi-
ties. Specifically we examine an online medical sup-
port community called breastcancer.org. We present
analyses of data from an active online community
with the goal of uncovering the connection between
language and online community membership. In
particular, we will look at language changes that oc-
cur over time as people continue to participate in an
online community. Consistent with the Communi-
ties of Practice theory of participation within a com-
munity (Lave and Wenger, 1991), we find increas-
ing conformity to community norms within the first
year of participation that then stabilizes as partici-
pants continue their involvement in the community.
Within the Communities of Practice view, social-
ization into a community begins with peripheral par-
ticipation, during which individuals have the op-
portunity to observe community norms. Lave and
Wenger?s theory has been applied to both online
and face-to-face communities. In an online commu-
nity, observing community norms begins with lurk-
ing and reading messages before an initial post. This
is termed legitimate peripheral participation, and it
is during this stage that potential new members ob-
serve community norms in action. With an initial
post, a user embarks upon the path of centripetal
participation, as they are taking steps towards core
participation.
Becoming a core member of a community means
adopting community norms. Persistent language
changes occur as an accumulation of local accom-
modation effects (Labov, 2010a; Labov, 2010b).
The extent of the adoption reflects the commitment
to community membership. Thus, as an individual
progressively moves from the periphery of a com-
munity towards the core, their behavior will progres-
sively grow towards conformity with these norms,
although total conformity very rarely occurs. The
quantitative analysis we present in the form of a
regression model is consistent with this theoretical
perspective and allows us to see what centripetal par-
ticipation and core participation look like within the
breastcancer.org community. We are able to test the
robustness of these observations by using the extent
76
of conformity to community norms as a predictor
of how long a member has been actively participat-
ing in an online community. We will present results
from this predictive analysis as part of the quantita-
tive evidence we provide in support of this model of
community participation.
Patterns of local accommodation and of long time
language change within communities have been ex-
tensively studied in the field of variationist sociolin-
guistics. However, with respect to online commu-
nities in particular, recent research has looked at
accommodation (Danescu-Niculescu-Mizil et al,
2011; Nguyen et al, 2010) and some shorter term
language changes (i.e., over a period of a few
months). However, longitudinal analyses of lan-
guage change spanning long time periods (i.e., more
than a few months) in online communities as we
present in this paper have been largely absent from
the literature. Typically, long term language change
in sociolinguistics requires reconstructing the past
from the present using age grading techniques, since
a comprehensive historical record is typically absent
(Labov, 2010a; Labov, 2010b). Online communi-
ties present a unique opportunity to study long term
language change from a much more comprehensive
historical record of a community?s development.
In the remainder of the paper, we first review prior
work on computational models of accommodation
and language change. We then present a qualitative
view of communication within the breastcancer.org
community. We then present two quantitative analy-
ses, one that explores language change in the aggre-
gate, and another that tests the robustness of findings
from the first analysis with a regression model that
allows us to predict how long a member has been
active within the community. We conclude with dis-
cussion and future work.
2 Related work
For decades, research under the heading of Social
Accommodation Theory (Giles et al, 1973) has at-
tempted to layer a social interpretation on patterns of
linguistic variation. This extensive line of research
has provided ample quantitative evidence that peo-
ple adjust their language within interactions, some-
times to build solidarity or liking, and other times
to differentiate themselves from others (Eckert and
Rickford, 2001).
In this line of work, people have often looked
at accommodation in small discussion groups and
dyadic conversation pairs. For example, Gonza-
les et al (2010) analyzed style matching in small
group discussions, and used it to predict cohesive-
ness and task performance in the groups. Scis-
sors et al (2009) analyzed conversational pairs play-
ing a social dilemma game and interacting through
an instant messenger. They found that certain pat-
terns of high linguistic similarity characterize high
trusting pairs. Niederhoffer and Pennebaker (2002)
found linguistic style matching both at the conver-
sation level and locally at a turn-by-turn level in
dyadic conversations. Paolillo (2001) looked at the
connection between linguistic variation and strong
and weak ties in an Internet Relay Chat channel.
Nguyen et al (2010) found accommodation effects
in an online political forum that contains discus-
sions between people with different political view-
points. Recently, Danescu-Niculescu-Mizil et al
(2011) showed that accommodation was also present
in Twitter conversations.
Lam (2008) gives an overview of work on lan-
guage socialization in online communities. We
know that persistent language changes over long
time periods are the accumulated result of local ac-
commodations that occur within short-term contexts
for social reasons (Labov, 2010a; Labov, 2010b).
However, the process through which individuals
adopt the language practices of online communi-
ties has been barely explored so far. One exam-
ple of investigation within this scope is the work
of Postmes et al (2000), in which we find analy-
sis of the formation of group norms in a computer-
mediated communication setting. Specifically, they
found that small groups were formed during the pro-
cess and communication norms including language
usage patterns were present within those groups.
Over time, conformity to these norms increased.
Similarly, Cassell and Tversky (2005) looked at evo-
lution of language patterns in an online community.
In this work, the participants were students from
around the world participating in the Junior Summit
forum ?98. Cassell and Tversky found that partic-
ipants converged on style, topics, goals and strate-
gies. Analyses were computed using word frequen-
cies of common classes (such as self references) and
77
Table 1: Statistics dataset.
Posts 1,562,590
Threads 68,226
Users (at least one post) 31,307
Time-span Oct 2002 - Jan 2011
manual coding. Huffaker et al (2006) examined a
subset of the same data. When comparing consec-
utive weeks over a 6 week time period, they found
that the language diverged. They hypothesized that
this was caused by external events leading to the in-
troduction of new words.
Our research differs from the research by Cas-
sell and Tversky (2005), Huffaker et al (2006) and
Postmes et al (2000) in several respects. For ex-
ample, in all of this work, participants joined the
community simultaneously at the inception of the
community. In contrast, our community of inquiry
has evolved over time, with members joining inter-
mittently throughout the history of the community.
Additionally, our analysis spans much more time,
specifically 2 years of data rather than 3 or 4 months.
Thus, this research addresses a different question
from the way community norms are first established
at the inception of a community. In contrast, what
we investigate is how new users are socialized into
an existing community in which norms have already
been established prior to their arrival.
We are not the first researchers to study our com-
munity of inquiry (Jha and Elhadad, 2010). How-
ever, prior work on data from this forum was focused
on predicting the cancer stage of a patient rather than
issues related to language change that we investi-
gate.
3 Data description
We analyze one of the largest breast cancer forums
on the web (http://community.breastcancer.org/).
All posts and user profiles of the forum were crawled
in January 2011.
The forum serves as a platform for many differ-
ent kinds of interactions, and serving the needs of a
variety of types of users. For example, a large pro-
portion of users only join to ask some medical ques-
tions, and therefore do not stay active long. In fact,
we find that a lot of users (12,349) only post in the
first week after their registration. The distribution of
number of weeks between a user?s last post and reg-
istration date follows a power law. However, besides
these short-term users, we also find a large number
of users who appear to be looking for more social in-
volvement and continue to participate for years, even
after their disease is in remission.
This distinction in types of users is reflected in the
forum structure. The forum is well organized, con-
taining over 60 subforums targeting different topics.
Besides specific subforums targeting medical topics
(such as ?Stage I and II Breast Cancer? and ?Radi-
ation Therapy - Before, During and After?), there
are subforums for certain population groups (such
as ?Canadian Breast Cancer Survivors? and ?Sin-
gles with breast cancer?), for social purposes (such
as ?Growing our Friendships After Treatment?, ?Get
Togethers?, and ?CyberSisters Photo Album?) and
non cancer related purposes (such as ?Humor and
Games?). In many of the subforums there are spe-
cific threads that foster the formation of small sub
communities, for example threads for people who
started chemotherapy in a certain month.
In the data we find community norms of long time
participants that are characterized by forum specific
jargon and a style that is highly informal and shows
familiarity with specific other participants and high
emotional involvement in the discussion. We infer
that the forum specific jargon is distinct from what
we would find in those users outside of it, in that that
there are places in the forum explaining commonly
used abbreviations to new users. We also observe
posts within threads where users ask about certain
abbreviations used in previous posts. Some of these
abbreviations are cancer related and also used in
places other than the forum, such as dx (diagnosis),
and rads (radiation, radiotherapy). Thus, they may
be reflective of identification with a broader commu-
nity of cancer patients who are internet users. Other
often used abbreviations are dh (dear husband), dd
(dear daughter), etc. We also observed that users fre-
quently refer to members of the community by name
and even as sister(s).
Now let us look at some examples illustrating
these patterns of language change. We take as an ex-
ample a specific long-time user. We start with a post
from early in her participation, specifically from a
couple of days after her registration:
78
I am also new to the forum, but not new
to bc, diagnosed last yr, [..] My follow-
up with surgeon for reports is not until
8/9 over a week later. My husband too is
so wonderful, only married a yr in May,
1 month before bc diagnosed, I could not
get through this if it weren?t for him, never
misses an appointment, [...] I wish every-
one well. We will all survive.
The next two posts1 are from the same user, 2 to
4 years after her registration date. Both posts are di-
rected to other forum members, very informal, and
contain a lot of abbreviations (e.g. ?DH? (Dear Hus-
band), ?DD? (Dear Daughter), ?SIL? (Son in Law)).
Gee Ann I think we may have shared the
same ?moment in time? boy I am getting
paid back big time for my fun in the sun.
Well Rose enjoy your last day of freedom
- LOL. Have lots of fun with DH ?The
Harley?. Ride long and hard ( either one
you choose - OOPS ).
Oh Kim- sorry you have so much going
on - and an idiot DH on top of it all.
[..] Steph- vent away - that sucks - [..]
XOXOXOXOXOXOXOX [..], quiet week-
end kids went to DD?s & SIL on Fri-
day evening, they take them to school [..],
made an AM pop in as I am supposed to,
SIL is an idiot but then you all know that.
This anecdotal evidence illustrates the linguistic
shift we will now provide quantitative evidence for.
4 Patterns of language change
4.1 Approach
In this section we aggregate data across long time
participants and look at global patterns of language
change. Specifically, we will analyze patterns of
change in the first year after registration of these
members, and show how language patterns consis-
tently become more different from the first week of
participation and more similar to the stable pattern
found within the second year of data. Furthermore,
when comparing consecutive weeks we find that the
1Names are replaced in example
difference increases and then stabilizes by the end
of the first year. The unit of analysis is one week
of data. Because there are multiple ways to mea-
sure the similarity or difference between two distri-
butions, we explore the use of two different meth-
ods. The first metric we use is the Kullback-Leibler
(KL) divergence. Larger values indicate bigger dif-
ferences in distribution. P represents the true distri-
bution. Note that this metric is asymmetric.
KL(P,Q) =
?
i
P (i) log
P (i)
Q(i)
We also explore using the Spearman?s Rank Corre-
lation Coefficient (SRCC), which measures the sim-
ilarity of two rankings:
SRCC = 1?
6
?
i d
2
i
n(n2 ? 1)
Where di is the difference between the ranks of word
i in the two rankings and n is the total number of
words.
4.2 Sampling
In this analysis, we begin by aligning the data of ev-
ery member by registration date. We then aggregate
posts of all users by week. Thus, in week 1, we have
the posts from all users during the first week after
their registration. Note that the actual week in time
would not be the same for each of these users since
they did not all register at the same time. In this way,
a week worth of data represents the way users talk
after the corresponding number of weeks after regis-
tering with the community rather than representing
a specific period of time. Because our dataset spans
a large time period of time (i.e. more than 8 years),
it is very unlikely that patterns we find in the data
reflect external events from any specific time period.
As discussed before, a large proportion of mem-
bers only post in their first week after registration.
These short time members might already initially
differ from members who tend to participate longer
in the forum. Therefore, it might confuse the model
if we take these short time members into account.
We may observe apparent changes in language that
are artifacts of the difference in distribution of users
across weeks. Thus, because we are interested in
language change specifically, we only consider posts
of long-term participants.
79
In addition, we have limited our focus to the ini-
tial two-year period of participation, because it is
for this length of participation that we have enough
users and enough posts to make a computational
model feasible. We have also limited ourselves to
examining high frequency words, because we have
a large vocabulary but only a limited amount of data
per week. Two weeks can look artificially similar
if they both have a lot of non-occurring words. In
summary, taking above considerations into account,
we applied the following procedure:
? We only look at the first 2 years, for which we
still have a large amount of data for every week.
? We only look at members who are long-term
participants (2 years or longer), this leaves us
with 3,012 users.
? For every week, we randomly sample an equal
number of posts (i.e., 600 from each week). All
posts are taken into account (i.e. both responses
as well as thread openings).
? We only look at the distribution change of
high frequency words (words occurring at least
1,000 times), this leaves us with 1,540 unique
words. No stemming or stop word removal was
done.
4.3 Comparison with early and late
distributions
Using the dataset described in the previous section,
we compare the language of each week during the
first year after registration with language in the very
first week and with language in the second year.
First we analyze whether language in the first year
becomes more similar to language used by members
in their second year as time progresses. We there-
fore compare the word distributions of the weeks of
the first year with the overall word distribution of
the second year. We apply KL divergence where
we consider the distribution of the second year as
the ?true distribution?. The result is shown in Fig-
ure 1. We see that the KL divergence decreases,
which means that as time progresses, the word dis-
tributions look more like the distribution of the sec-
ond year. Fitting a Least Squares (LS) model, we
get an intercept of 0.121033 and slope of -0.001080
Figure 1: KL divergence between weeks in first year and
overall second year.
Figure 2: KL divergence between weeks in first year and
first week.
(r2 = 0.5528). Using the Spearman Rank Correla-
tion (SRCC) and fitting a LS model, we observe the
same pattern (r2 = 0.6435).
Our second analysis involves comparing the dis-
tributions of the first year (excluding the first week),
with the distribution of the first week. The result is
shown in Figure 2. We see that the KL divergence
increases, meaning that as time progresses, the word
distributions become less similar with the first week.
(KL: r2 = 0.6643, SRCC: r2 = 0.7962).
4.4 Comparing consecutive distributions
We now compare the distributions of consecutive
weeks to see how much language change occurs in
different time periods. For KL divergence we use the
symmetric version. Results are presented in Figure
3 and show a divergence pattern throughout the first
year that stabilizes towards the end of that first year
of participation. (KL: r2 = 0.4726, SRCC: r2 =
80
Figure 3: KL divergence between consecutive weeks.
0.8178). The divergence pattern was also observed
by Huffaker et al (2006) (related, but not equiva-
lent setting, as mentioned in the literature review).
We hypothesize that the divergence occurs because
users tend to talk about a progressively broader set
of topics as they become more involved in the com-
munity. To confirm this hypothesis, we compare the
distributions of each week with the uniform distri-
bution. We indeed find that as time progresses, the
distributions for each week become more uniform.
(KL: r2 = 0.3283, SRCC: r2 = 0.6435).
5 Predicting membership duration
In the previous section we found strong patterns of
language change in our data. We are interested in the
extent to which we can automatically predict how
many weeks the user has been a member, using only
text or meta-features from that specific week. Iden-
tifying which features predict how long a member
has been active can give more detailed insight into
the social language that characterizes the commu-
nity. In addition, it tells us how prominent the pat-
tern is among other sources of language variation.
5.1 Dataset
For this analysis, we set up the data slightly differ-
ently. Now, rather than combine data across users,
we keep the data from each user for each week sep-
arate so we can make a separate prediction for each
user during each week of their participation. Thus,
for each person, we aggregate all posts per week.
We only consider weeks in the first two years after
the registration in which there were at least 10 posts
with at least 10 tokens from that user.
Table 2: Statistics dataset.
#Docs #Persons #Posts
Training 13,273 1,591 380,143
Development 4,617 548 122,489
Test 4,571 548 134,141
5.2 Approach
Given an input vector x ? Rm containing the fea-
tures, we aim to find a prediction y? ? R for the num-
ber of weeks the person has been a member of the
community y ? R using a linear regression model:
y? = ?0 + x>? where ?0 and ? are the parameters
to estimate. Usually, the parameters are learned by
minimizing the sum of squared errors.
In order to strive for a model with high explana-
tory value, we use Linear Regression, with L1 reg-
ularization (Tibshirani, 1996). This minimizes the
sum of squared errors, but in addition adds a penalty
term ?
?m
j=1 |?j |, the sum of absolute values of the
coefficients. ? is a constant and can be found by
optimizing over the development data. As a re-
sult, this method delivers sparse models. We use
Orthant-Wise Limited-memory Quasi-Newton Op-
timizer (Andrew and Gao, 2007) as our optimiza-
tion method. This method has proven to establish
competitive performances with other optimization
methods, while producing sparse models (Gao et al,
2007).
Because our observations suggest that language
change decreases as members have been active
longer, we also experimented with applying a log
transformation on the number of weeks.
5.3 Features
For all features, we only use information that has
been available for that particular week. We explore
different types of features related to the qualitative
differences in language we discussed in Section 3:
textual, behavioral, subforum and meta-features.
5.3.1 Textual features
We explore the following textual features:
? Unigrams and bigrams.
? Part of Speech (POS) bigrams. Text was tagged
using the Stanford POS tagger (Toutanova et
al., 2003).
81
? LIWC (Pennebaker et al, 2001), a word count-
ing program that captures word classes and
stylistic features.
? Usernames. Because some of the usernames
are common words, we only consider user-
names of users active in the same thread.
? Proper names. We obtained a list containing
common female names. We ranked them ac-
cording to frequency in our dataset, and manu-
ally deleted common words in our dataset, such
as happy, hope, tuesday and may, from our list.
? Slang words. We manually compile a list of
common abbreviations and their whole words
counterpart. We then count the number of ab-
breviations and the number of whole words
used in the post. The feature value then
is (#abbrev?#wholewords)/#totalwords.
Because in some contexts no abbreviations
can be used, this feature takes into account if
the user actually chose to use the abbrevia-
tion/whole word, or if there was no need for
it.
No stemming or stopword removal is used. Fre-
quencies are normalized by length.
5.3.2 Behavioral features
We also explore additional features that indicate
the behavior of the user:
? Ratio (posts starting threads) / (total number of
posts).
? Number of posts.
5.3.3 Subforum features
We include as features the distribution of subfo-
rums the member has posted in. This captures two
intuitions. First, it is an approximation of the current
phase in the cancer process for that member. For ex-
ample, we noticed that most of the new users have
just been diagnosed, while long term users have al-
ready finished treatment. Because the subforums are
very specific (such as ?Not Diagnosed with a Re-
currence or Metastases but Concerned?), we expect
these features to give a good approximation of the
phase the user is currently in. In addition, these sub-
forums also give an indication of the user?s interest.
Table 3: Results reported with Pearsons correlation (r).
Run # Features Raw (r) Log (r)
Unigrams + Bigrams 43,126 0.547 0.621
POS 1,258 0.409 0.437
LIWC 88 0.494 0.492
Proper names 1 0.185 0.186
Usernames 1 0.150 0.102
Slang 1 0.092 0.176
Behavior 2 0.139 0.243
Subforum 65 0.404 0.419
All above 44,542 0.581 0.649
All above + Person 46,133 0.586 0.656
For example, whether the user posts mostly in med-
ical forums, or mostly in the social orientated subfo-
rums.
5.3.4 Other features
Most of the persons appear multiple times in our
dataset (e.g. multiple weeks). To help the model
control for idiosyncratic features of individual users,
we include for every person a dummy variable asso-
ciated with that user?s unique identity. This helps
the model at training time to separate variance in
language usage across users from general effects re-
lated to length of participation. Note that we do not
use these features as test time.
5.4 Results
We experimented with individual types of features
as well as all of them aggregated. The results (corre-
lations) can be found in Table 3. The features having
the most weight for long time participants in our best
model (All incl. Person, Log) are presented in Table
4. We see that for most features the performance
was higher when applying the log transformation.
This was especially the case with the unigrams and
bigrams features. For some features the difference
was less, such as for proper names and the subforum
features. This could indicate that these features have
a more linear pattern as time progresses, while word
patterns such as unigrams tend to stabilize earlier.
We find that both stylistic patterns (such as POS) as
well as patterns indicating conformity (social behav-
ior, slang words) are individually already very pre-
dictive.
In our best performing model, we find that both
82
Table 5: Qualitative grouping of textual features.
Type Short time members Long time members
Abbreviations Husband My DD (Dear Daughter), Your PS (Plastic Surgeon)
Social networks Facebook, fb
Greetings Hi all Hi girls, Hi gals
I versus other LIWC-I, My, Me LIWC-other, We, Sisters
Social support Hugs, Condolences, So sorry
Thanking Thanks, Thanx, Thx
Forum Bc org, On bco
Introducing Newbie, New here, Am new
Asking information Info, LIWC-qmarks
Table 4: Top 10 features of long term users.
Feature Weight
META - slang 0.058362195
META -propername 0.052984915
year 0.050872918
META - [person1] 0.050708718
META - [person2] 0.040548104
months 0.040400583
META - [person3] 0.039806096
LIWC - Othref 0.036080545
META - [person4] 0.035605996
POS - nnp prp 0.035033650
the slang and proper name features get a high weight
for long time participants. Furthermore, we observe
that a lot of the person meta features are included
in the model when it is trained, although as men-
tioned we do not use these features at testing time.
The fact that the model assigns them weight indi-
cates that idiosyncratic features of users explain a lot
of variance in the data. Our best performing model
has 3,518 non zero features. In Table 5 we qual-
itatively grouped and contrasted features that were
more associated with short-term or long-term mem-
bers. We see that long-term members show much
more social behavior and familiarity with each other.
This is shown to references to each other, more so-
cial support, references to social networks and ways
of greeting. They furthermore talk about the forum
itself more often by using the abbreviation ?bco?.
Short term members are characterized by words that
are used when they introduce themselves to others.
Thus we find that long time participants are char-
acterized by informal language, containing many fo-
rum specific jargon, as well as showing emotional
involvement with other forum members. Our best
run obtained a correlation of r = 0.656, giving an
r2 value of 0.430. This means that 0.43 of the vari-
ation can be explained by our model. Since there
are many other factors that influence the writing of
users, it is understandable that our model does not
explain all the variance.
6 Discussion
As discussed widely in previous literature, peo-
ple become socialized into communities over time
through their interactions with community mem-
bers. The extent of conformity to group norms re-
flects commitment to the group. Our first study
showed evidence of increasing conformity to com-
munity norms through changes in simple word dis-
tributions. The second study then tested the robust-
ness of these findings through a prediction task and
extended the language features of the first study.
Since community members tend to conform in-
creasingly to community norms over time, although
the target class for our predictive model is time, it
is reasonable to assume that what the model really
learns to predict is how long average community
members have been around by the time they sound
like that. In other words, one can think about its time
prediction as a measure of how long it sounds like
that person has been in the community. The model
would therefore overpredict for members who move
from the periphery to the core of a community faster
than average while underpredicting for those who do
so more gradually. This would be consistent with the
83
idea that rate of commitment making and conformity
is person specific.
There are two limitations that need to be ad-
dressed regarding the present studies. First, there
are certain factors that influence the rate of adop-
tion to the forum that we are not able to take into
account. For example, some people might have al-
ready been reading the forum for a while, before
they actually decide to join the community. These
people are already exposed to the community prac-
tices, and therefore might already show more con-
formity in the beginning than others.
Second, our experiments involved one online
community targeting a very specific topic. Due to
the nature of the topic, most of the active users come
from a small subpopulation (mostly women between
40-60 years). Therefore, it is a question how well
these results can be applied to other online commu-
nities.
As a future application, a model that can capture
these changes could be used in research related to
commitment in online communities.
7 Conclusion
It is widely accepted that persistent language change
in individuals occurs over time as a result of the
accumulation of local processes of accommodation.
Although previous research has looked at accommo-
dation within short periods of time, including recent
research on social media data, persistent language
change as a result of longer term involvement in an
online community is still an understudied area.
In this paper we have presented research aiming to
close this gap. We have analyzed data from a large
online breast cancer forum. Analyzing data of long
time members, we found strong patterns indicating
language changes as these members participated in
the community, especially over the course of their
first year of participation.
We then presented a regression approach to pre-
dict how long a person has been a member of the
community. Long time participants were character-
ized by showing more social behavior. Furthermore,
they used more forum specific language, such as cer-
tain abbreviations and ways of greeting. Due to the
nature of our dataset, language was also influenced
by external factors such as changes in the cancer pro-
cess of individuals.
Although our observations are intuitive and agree
with observations in previous, related literature re-
garding socialization in communities, it is still a
question whether our observations generalize to
other online communities.
In our current work we have looked at changes
across users and across contexts. However, it is well
known that individuals adapt their language depend-
ing on local interactions. Thus, a next step would
be to model the process by which local accommoda-
tion accumulates and results in long term language
change.
Acknowledgments
The authors would like to thank Michael Heilman
for the regression code and Noah Smith for ideas for
the regression experiments. This work was funded
by NSF grant IIS-0968485.
References
Galen Andrew and Jianfeng Gao. 2007. Scalable train-
ing of l1-regularized log-linear models. In Proceed-
ings of the 24th international conference on Machine
learning, ICML ?07, pages 33?40, New York, NY,
USA. ACM.
Justine Cassell and Dona Tversky. 2005. The language
of online intercultural community formation. Journal
of Computer-Mediated Communication, 10:16?33.
Cristian Danescu-Niculescu-Mizil, Michael Gamon, and
Susan Dumais. 2011. Mark my words! linguistic
style accommodation in social media. In Proceedings
of WWW.
Penelope Eckert and John R. Rickford. 2001. Style and
Sociolinguistic Variation. Cambridge: University of
Cambridge Press.
Jianfeng Gao, Galen Andrew, Mark Johnson, and
Kristina Toutanova. 2007. A comparative study of pa-
rameter estimation methods for statistical natural lan-
guage processing. In Proceedings of the 45th Annual
Meeting of the Association of Computational Linguis-
tics, pages 824?831, Prague, Czech Republic, June.
Association for Computational Linguistics.
Howard Giles, Donald M. Taylor, and Richard Bourhis.
1973. Towards a theory of interpersonal accommoda-
tion through language: some canadian data. Language
in Society, 2(02):177?192.
Amy L. Gonzales, Jeffrey T. Hancock, and James W. Pen-
nebaker. 2010. Language style matching as a predic-
tor of social dynamics in small groups. Communica-
tion Research, 37(1):3?19, February.
84
David Huffaker, Joseph Jorgensen, Francisco Iacobelli,
Paul Tepper, and Justine Cassell. 2006. Computa-
tional measures for language similarity across time
in online communities. In Proceedings of the HLT-
NAACL 2006 Workshop on Analyzing Conversations
in Text and Speech, ACTS, pages 15?22, Stroudsburg,
PA, USA. Association for Computational Linguistics.
Mukund Jha and Noe?mie Elhadad. 2010. Cancer stage
prediction based on patient online discourse. In Pro-
ceedings of the 2010 Workshop on Biomedical Natu-
ral Language Processing, BioNLP ?10, pages 64?71,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
William Labov. 2010a. Principles of Linguistic Change,
Volume I, Internal Factors. Wiley-Blackwell.
William Labov. 2010b. Principles of Linguistic Change,
Volume I, Social Factors. Wiley-Blackwell.
Wan S. E. Lam. 2008. Language socialization in on-
line communities. In Nancy H. Hornberger, editor, En-
cyclopedia of Language and Education, pages 2859?
2869. Springer US.
Jean Lave and Etienne Wenger. 1991. Situated Learn-
ing. Legitimate peripheral participation. Cambridge:
University of Cambridge Press.
Dong Nguyen, Elijah Mayfield, and Carolyn P. Rose.
2010. An analysis of perspectives in interactive set-
tings. In Proceedings of the 2010 KDD Workshop on
Social Media Analytics.
Kate G. Niederhoffer and James W. Pennebaker. 2002.
Linguistic style matching in social interaction.
John C. Paolillo. 2001. Language variation on internet
relay chat: A social network approach. Journal of So-
ciolinguistics, 5:180?213.
James W. Pennebaker, Roger J. Booth, and Martha E.
Francis, 2001. Linguistic Inquiry and Word Count
(LIWC): A Computerized Text Analysis Program.
Tom Postmes, Russell Spears, and Martin Lea. 2000.
The formation of group norms in computer-mediated
communication. Human Communication Research,
26(3):341?371.
Lauren E. Scissors, Alastair J. Gill, Kathleen Geraghty,
and Darren Gergle. 2009. In cmc we trust: the role
of similarity. In Proceedings of the 27th international
conference on Human factors in computing systems,
CHI ?09, pages 527?536, New York, NY, USA. ACM.
Robert Tibshirani. 1996. Regression shrinkage and se-
lection via the lasso. Journal of the Royal Statistical
Society. Series B (Methodological), 58(1):267?288.
Kristina Toutanova, Dan Klein, Christopher D. Manning,
and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In Pro-
ceedings of the 2003 Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics on Human Language Technology - Volume 1,
NAACL ?03, pages 173?180, Stroudsburg, PA, USA.
Association for Computational Linguistics.
85
Proceedings of the 5th ACL-HLT Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities, pages 115?123,
Portland, OR, USA, 24 June 2011. c?2011 Association for Computational Linguistics
Author Age Prediction from Text using Linear Regression
Dong Nguyen Noah A. Smith Carolyn P. Rose?
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{dongn,nasmith,cprose}@cs.cmu.edu
Abstract
While the study of the connection between
discourse patterns and personal identification
is decades old, the study of these patterns us-
ing language technologies is relatively recent.
In that more recent tradition we frame author
age prediction from text as a regression prob-
lem. We explore the same task using three
very different genres of data simultaneously:
blogs, telephone conversations, and online fo-
rum posts. We employ a technique from do-
main adaptation that allows us to train a joint
model involving all three corpora together as
well as separately and analyze differences in
predictive features across joint and corpus-
specific aspects of the model. Effective fea-
tures include both stylistic ones (such as POS
patterns) as well as content oriented ones. Us-
ing a linear regression model based on shallow
text features, we obtain correlations up to 0.74
and mean absolute errors between 4.1 and 6.8
years.
1 Introduction
A major thrust of research in sociolinguistics is to
understand the connection between the way peo-
ple use language and their community membership,
where community membership can be construed
along a variety of dimensions, including age, gen-
der, socioeconomic status and political affiliation. A
person is a member of a multiplicity of communi-
ties, and thus the person?s identity and language are
influenced by many factors.
In this paper we focus on the relationship between
age and language use. Recently, machine learning
methods have been applied to determine the age of
persons based on the language that they utter. Stud-
ies of the stylistic and content-based features that
predict age or other personal characteristics yield
new insights into the connection between discourse
and identity. However, that connection is known to
be highly contextual, such as whether the data were
collected synchronously or asynchronously, through
typed or spoken interaction, or whether participants
can see one another or not. Recent work in the area
of domain adaptation raises awareness about the ef-
fect of contextual factors on the generality of text
prediction models.
Our first contribution to this literature is an in-
vestigation of age prediction using a multi-corpus
approach. We present results and analysis across
three very different corpora: a blog corpus (Schler
et al, 2006), a transcribed telephone speech corpus
(Cieri et al, 2004) and posts from an online forum
on breast cancer. By using the domain adaptation
approach of Daume? III (2007), we train a model on
all these corpora together and separate the global
features from corpus-specific features that are asso-
ciated with age.
A second contribution is the investigation of age
prediction with age modeled as a continuous vari-
able rather than as a categorical variable. Most
prior research on age prediction has framed this as a
two-class or three-class classification problem (e.g.,
Schler et al, 2006 and Garera and Yarowsky, 2009).
In our work, modeling age as a continuous variable
is interesting not only as a more realistic representa-
tion of age, but also for practical benefits of joint
modeling of age across corpora since the bound-
115
aries for discretizing age into a categorical variable
in prior work have been chosen heuristically and in
a corpus-dependent way, making it hard to compare
performance across different kinds of data.
In the remainder of the paper, we first discuss re-
lated work and present and compare the different
datasets. We then outline our approach and results.
We conclude with discussion and future work.
2 Related work
Time is an important factor in sociolinguistic analy-
sis of language variation. While a thorough review
of this work is beyond the scope of this paper, Eckert
(1996) gives an overview of the literature on age as
a sociolinguistic variable. Linguistic variation can
occur as an individual moves through life, or as a re-
sult of changes in the community itself as it moves
through time. As an added complexity, Argamon et
al. (2007) found connections between language vari-
ation and age and gender. Features that were used
with increasing age were also used more by males
for any age. Features that were used with decreas-
ing age were used more by females. In other work,
the same features that distinguish male and female
writing also distinguish non-fiction and fiction (Arg-
amon et al, 2003). Thus, the separate effects of age,
time period, gender, topic, and genre may be diffi-
cult to tease apart in naturalistic data where many of
these variables are unknown.
Recently, machine learning approaches have been
explored to estimate the age of an author or speaker
using text uttered or written by the person. This
has been modeled as a classification problem, in a
similar spirit to sociolinguistic work where age has
been investigated in terms of differences in distri-
butions of characteristics between cohorts. In the
sociolinguistic literature, cohorts such as these are
determined either etically (arbitrary, but equal age
spans such as decades) or emically (related to life
stage, such as adolescence etc.). In machine learn-
ing research, these cohorts have typically been deter-
mined for practical reasons relating to distribution of
age groups within a corpus, although the boundaries
sometimes have also made sense from a life stage
perspective. For example, researchers have mod-
eled age as a two-class classification problem with
boundaries at age 40 (Garera and Yarowsky, 2009)
or 30 (Rao et al, 2010). Another line of work has
looked at modeling age estimation as a three-class
classification problem (Schler et al, 2006; Goswami
et al, 2009), with age groups of 13-17, 23-27 and
33-42. In addition to machine learning experiments,
other researchers have published statistical analyses
of differences in distribution related to age and lan-
guage and have found similar patterns.
As an example of one of these studies, Pen-
nebaker and Stone (2003) analyzed the relationship
between language use and aging by collecting data
from a large number of previous studies. They
used LIWC (Pennebaker et al, 2001) for analysis.
They found that with increasing age, people tend to
use more positive and fewer negative affect words,
more future-tense and less past-tense, and fewer
self-references. Furthermore, a general pattern of
increasing cognitive complexity was seen. Barbieri
(2008) uses key word analysis to analyze language
and age. Two groups (15?25 and 35?60) were com-
pared. Analysis showed that younger speakers? talk
is characterized by slang and swear words, indica-
tors of speaker stance and emotional involvement,
while older people tend to use more modals.
Age classification experiments have been con-
ducted on a wide range of types of data, in-
cluding blogs (Schler et al, 2006; Goswami et
al., 2009), telephone conversations (Garera and
Yarowsky, 2009), and recently Twitter (Rao et al,
2010). Effective features were both content fea-
tures (such as unigrams, bigrams and word classes)
as well as stylistic features (such as part-of-speech,
slang words and average sentence length). These
separate published studies present some common-
alities of findings. However, based on these re-
sults from experiments conducted on very different
datasets, it is not possible to determine how gener-
alizable the models are. Thus, there is a need for an
investigation of generalizability specifically in the
modeling of linguistic variation related to age, which
we present in this paper.
Age classification from speech data has been of
interest for many years. Recently, age regression us-
ing speech features has been explored (Spiegl et al,
2009). Spiegel?s system obtained a mean absolute
error of approximately 10 years using support vec-
tor regression. Van Heerden et al (2010) explore
combining regression estimates to improve age clas-
116
sification. As far as we are aware, we are the first to
publish results from a regression model that directly
predicts age using textual features.
3 Data description
We explore three datasets with different characteris-
tics. The data was divided into a training, develop-
ment and test set. Statistics are listed in Table 1.
3.1 Blog corpus
In August 2004 Schler et al (2006) crawled blogs
from blogger.com. Information such as gen-
der and age were provided by the users in their re-
spective profiles. Users were divided into three age
groups, and each group had an equal number of fe-
male and male bloggers. In our experiments, ev-
ery document consists of all posts from a particular
blogger.
3.2 Fisher telephone corpus
The Fisher corpus (Cieri et al, 2004) contains tran-
scripts of telephone conversations. People were ran-
domly assigned to pairs, and for (almost) every per-
son, characteristics such as gender and age were
recorded. Furthermore, for each conversation a topic
was assigned. The data was collected beginning De-
cember 2002 and continued for nearly one year. In
our experiments, we aggregate the data for each per-
son.
3.3 Breast cancer forum
We drew data from one of the most active online fo-
rums for persons with breast cancer.1 All posts and
user profiles of the forum were crawled in January
2011. Only a small proportion of users had indicated
their age in their profile. We manually annotated the
age of approximately 200 additional users with less
common ages by looking manually at their posts. An
author?s age can often be annotated because users
tend to make references to their age when they intro-
duce themselves or when telling their treatment his-
tory (e.g., I was diagnosed 2 years ago when I was
just 38). Combining this with the date of the specific
post, a birth year can be estimated. Because a per-
son?s data can span multiple years, we aggregate all
the data per year for each person. Each person was
1http://community.breastcancer.org
Figure 1: Comparison of age frequency in datasets.
0
500
1000
1500
2000
2500
10 20 30 40 50 60 70 80 90
F
re
qu
en
cy
Age
Blogs
Fisher
Cancer
assigned randomly to one of the data splits, to make
sure all documents representing the same person ap-
peared in only one split. The dataset contains posts
from October 2002 until January 2011.
3.4 Dataset comparison and statistics
The datasets differ in several respects: specificity
(general topics versus breast cancer), modality of in-
teraction (telephone conversations versus online fo-
rum versus blog post), age distribution, and amount
of data per person. The blog and Fisher dataset con-
tain approximately equal amounts of males and fe-
males, while the breast cancer dataset is heavily bi-
ased towards women.
A comparison of the age distributions of the three
corpora is given in Figure 1. The Fisher dataset
has the most uniform distribution across the ages,
while the blog data has a lot of young persons and
the breast cancer forum has a lot of older people.
The youngest person in our dataset is 13 years old
and the oldest is 88. Note that our blog corpus con-
tains gaps between different age categories, which
is an artifact of the experimental approach used by
the people who released this dataset (Schler et al,
2006).
Because all datasets were created between 2002
and 2011, we are less likely to observe results due to
cohort effects (changes that occur because of collec-
tive changes in culture, such as use of the Internet).
117
Table 1: Datasets statistics.
Blogs Fisher Cancer
Data #docs avg #tokens #docs avg #tokens #docs avg #tokens #persons
Training 9,660 13,042 5,957 3,409 2,330 22,719 1,269
Development 4,830 13,672 2,977 3,385 747 32,239 360
Test 4,830 13,206 2,980 3,376 797 26,952 368
4 Experimental setup
4.1 Linear regression
Given an input vector x ? Rm, where x1, . . . , xm
represent features (also called independent variables
or predictors), we find a prediction y? ? R for the age
of a person y ? R using a linear regression model:
y? = ?0 + x>? where ?0 and ? are the parame-
ters to estimate. Usually, the parameters are learned
by minimizing the sum of squared errors. In order
to strive for a model with high explanatory value,
we use a linear regression model with Lasso (also
called L1) regularization (Tibshirani, 1996). This
minimizes the sum of squared errors, but in addition
adds a penalty term ?
?m
j=1 |?j |. ? is a constant and
can be found by optimizing over the development
data. As a result, this method delivers sparse mod-
els. We use OWLQN to optimize the regularized
empirical risk (Andrew and Gao, 2007; Gao et al,
2007). We evaluate the models by reporting the cor-
relation and mean absolute error (MAE).
4.2 Joint model
To discover which features are important across
datasets and which are corpus-specific, we train a
model on the data of all corpora using the feature
representation proposed by Daume? III (2007). Using
this model, the original feature space is augmented
by representing each individual feature as 4 new fea-
tures: a global feature and three corpus-specific fea-
tures, specifically one for each dataset. Thus for ev-
ery feature f , we now have fglobal , fblogs , ffisher and
fcancer . For every instance, only the global and the
one specific corpus feature are set. For example for
a particular feature value xj for the blog dataset we
would have ?xj , xj , 0, 0?. If it would appear in the
cancer dataset we would have ?xj , 0, 0, xj?. Because
the resulting model using L1 regression only selects
a small subset of the features, some features may
only appear either as global features or as corpus-
specific features in the final model.
4.3 Overview different models
Besides experimenting with the joint model, we are
also interested in the performance using only the dis-
covered global features. This can be achieved by ap-
plying the weights for the global features directly as
learned by the joint model, or retraining the model
on the individual datasets using only the global fea-
tures. In summary, we have the following models:
? INDIV: Models trained on the three corpora in-
dividually.
? JOINT: Model trained on all three corpora with
features represented as in Daume? III (2007).
? JOINT-Global: Using the learned JOINT
model but only keeping the global features.
? JOINT-Global-Retrained: Using the discov-
ered global features by the JOINT model, but
retrained on each specific dataset.
4.4 Features
4.4.1 Textual features
We explore the following textual features; all fea-
tures are frequency counts normalized by the length
(number of tokens) of the document.
? Unigrams.
? POS unigrams and bigrams. Text is tagged us-
ing the Stanford POS tagger (Toutanova et al,
2003).
? LIWC (Pennebaker et al, 2001). This is a word
counting program that captures word classes
such as inclusion words (LIWC-incl: ?with,?
?and,? ?include,? etc.), causation words (LIWC-
cause: ?because,? ?hence,? etc.), and stylis-
tic characteristics such as percentage of words
longer than 6 letters (LIWC-Sixltr).
118
Figure 2: Scatterplot of true and predicted age.
-20
-10
0
10
20
30
40
50
60
70
80
90
10 20 30 40 50 60 70 80 90
P
re
di
ct
ed
ag
e
True age
4.4.2 Gender
Because the gender of a person also influences
how age is reflected in a person?s text or speech (e.g.
Argamon et al (2007) ), we add a binary feature for
the gender of the person (Male = 1, Female = 0).
This feature is only known for the blog and Fisher
dataset. For the breast cancer dataset the gender is
not known, but we assume they are all women.
5 Results and discussion
As discussed, we experiment with four different
models. We explore three different feature sets: only
unigrams, only POS, and the full feature set. The re-
sults are presented in Table 2. The most important
features using the JOINT model with the full feature
set (condition 10) are presented in Table 3.
5.1 Quantitative analysis
Overall, similar performance is obtained on the
Fisher and blog datasets. The highest correlations
were achieved on the Fisher dataset, with a best cor-
relation of r = 0.742. This gives an r2 value of
0.551, indicating that 55% of the variance can be
explained by the model. However, a higher mean
absolute error (MAE) was observed compared to
the blog dataset. This may be caused by the larger
spread in distribution of ages in the Fisher dataset.
The lowest correlations were observed on the cancer
dataset. This is probably caused by the small amount
of training instances, the noisy text, and the fact that
the ages lie very close to each other.
Overall, the joint model using all features per-
formed best (condition 10). In Figure 2 a plot is
presented that relates the true and predicted ages for
this condition. We find that for the high ages there
are more instances with high errors, probably caused
by the small amount of training data for the extreme
ages.
We find the correlation metric to be very sensitive
to the amount of data. For example, when comput-
ing the correlation over the aggregated results of all
corpora, we get a much higher correlation (0.830),
but the MAE (5.345) is closer to that computed over
the individual datasets. However, the MAE is de-
pendent on the age distributions in the corpus, which
can be observed by contrasting the MAE on the runs
of the Fisher and cancer dataset. This thus suggests
that these two measures are complementary and both
are useful as evaluation metrics for this task.
For most experiments the joint models show im-
provement over the individual models. Returning
to our question of generality, we can make several
observations. First, performance decreases signif-
icantly when only using the global features (com-
paring JOINT and JOINT-Global-retrained), con-
firming that corpus-specific features are important.
Second, learned weights of global features are rea-
sonably generalizable. When using the full feature
set, retraining the global features on the corpora di-
rectly only gives a slight improvement (e.g. com-
pare conditions 11 and 12). Third, the bias term
(?0) is very corpus-specific and has a big influence
on the MAE. For example, when comparing condi-
tions 11 and 12, the correlations are very similar but
the MAEs are much lower when the model is re-
trained. This is a result of adjusting the bias term
to the specific dataset. For example the bias term of
the model trained on only the blog dataset is 22.45,
compared to the bias of 46.11 when trained on the
cancer dataset.
In addition, we observe better performance in the
cancer dataset when retraining the model using only
the global features compared to the initial feature
set. This suggests that using the global features
might have been an effective method for feature se-
lection to prevent overfitting on this small dataset.
119
Table 2: Results on the test set, reported with Pearson?s correlation (r) and mean absolute error (MAE).
Blogs Fisher Cancer
ID Model #Features r MAE r MAE r MAE
Unigrams
1 INDIV 56,440 0.644 4.236 0.715 7.145 0.426 7.085
2 JOINT 56,440 0.694 4.232 0.723 7.066 0.530 6.537
3 JOINT-Global 656 0.605 5.800 0.628 10.370 0.461 16.632
4 JOINT-Global-retrained 656 0.658 4.409 0.675 7.529 0.498 6.797
POS
5 INDIV 4,656 0.519 5.095 0.553 8.635 0.150 7.699
6 JOINT 4,656 0.563 4.899 0.549 8.657 0.035 8.449
7 JOINT-Global 110 0.495 6.332 0.390 12.232 0.151 19.454
8 JOINT-Global-retrained 110 0.519 5.095 0.475 9.187 0.150 7.699
All features
9 INDIV 61,416 0.699 4.144 0.731 6.926 0.462 6.943
10 JOINT 61,416 0.696 4.227 0.742 6.835 0.535 6.545
11 JOINT-Global 510 0.625 5.295 0.650 11.982 0.459 17.472
12 JOINT-Global-retrained 510 0.629 4.633 0.651 7.862 0.490 6.876
5.2 Feature analysis
The most important features using the JOINT model
with the full feature set (condition 10) are presented
in Table 3. Features associated with a young age
have a negative weight, while features associated
with old age have a positive weight. For almost all
runs and evaluation metrics the full feature set gives
the best performance. However, looking at the per-
formance increase, we observe that the unigram only
baseline gives strong results. Overall, both stylistic
as well as content features are important. For con-
tent features, we see that references to family (e.g.,
?granddaughter? versus ?son?) as well as to daily
life (e.g., ?school? versus ?job?) are very predictive.
Although the performance using only POS tags
is lower, reasonable correlations are obtained using
only POS tags. In Table 3 we see many POS features
associated with old age. This is confirmed when an-
alyzing the whole feature set selected by the JOINT
model (condition 10). In this model 510 features are
nonzero, 161 of which are POS patterns. Of these,
43 have a negative weight, and 118 have a positive
weight. This thus again suggests that old age is char-
acterized more by syntactic effects than young age.
Most important features are consistent with obser-
vations from previous research. For example, in the
Fisher dataset, similar to findings from classification
experiments by Garera and Yarowsky (2009), the
word ?well? is most predictive of older age. ?Like?
has the highest association with younger age. This
agrees with observations by Barbieri (2008). As
was also observed by others, ?just? is highly associ-
ated with young persons. Consistent with literature
that males generally ?sound older? than they truly
are (Argamon et al, 2007, and others), our male
speaker feature has a high negative weight. And, in
agreement with previous observations, younger peo-
ple use more swear words and negative emotions.
The differences between the corpora are reflected
in the features that have the most weight. The effec-
tive features in the Fisher dataset are more typical
of conversational settings and effective features in
the cancer dataset are about being pregnant and hav-
ing kids. Features associated with the blog dataset
are typical of the story telling nature of many blog
posts.
Comparing the extracted corpus-specific features
with the features selected when training on the indi-
vidual corpora, we do see evidence that the JOINT
model separates general versus specific features.
For example, the most important features associ-
ated with young people in the cancer dataset when
only training on the cancer dataset (condition 9)
are: LIWC - Emoticons, LIWC - Pronoun, definitely,
120
Table 3: Most important features in the JOINT model with all features (condition 10).
(a) Features for younger people.
Global Blogs Fisher Cancer
like -1.295 you -0.387 actually -0.457 LIWC-Emotic. -0.188
gender-male -0.539 went -0.310 mean -0.343 young -0.116
LIWC-School -0.442 fun -0.216 everyone -0.273 history -0.092
just -0.354 school -0.192 definitely -0.273 mom -0.087
LIWC-Anger -0.303 but -0.189 mom -0.230 ultrasound -0.083
LIWC-Cause -0.290 LIWC-Comma -0.152 student -0.182 kids -0.071
mom -0.290 go -0.142 pretty -0.137 age -0.069
so -0.271 POS-vbp nn -0.116 POS-lrb cd -0.135 mum -0.069
definitely -0.263 thats -0.115 LIWC-Swear -0.134 POS-sym rrb -0.069
LIWC-Negemo -0.256 well -0.112 huge -0.126 discharge -0.063
(b) Features for older people.
Global Blogs Fisher Cancer
years 0.601 LIWC - Job 0.514 well 1.644 POS - dt 0.713
POS - dt 0.485 son 0.267 LIWC - WC 0.855 POS - md vb 0.450
LIWC - Incl 0.483 kids 0.228 POS - uh prp 0.504 POS - nn 0.369
POS - prp vbp 0.337 years 0.178 retired 0.492 LIWC - Negate 0.327
granddaughter 0.332 work 0.147 POS - prp vbp 0.430 POS - nn vbd 0.321
grandchildren 0.293 wife 0.142 said 0.404 POS - nnp 0.304
had 0.277 husband 0.137 POS - cc fw 0.358 us 0.287
daughter 0.272 meds 0.112 son 0.353 all 0.266
grandson 0.245 dealing 0.096 subject 0.319 good 0.248
ah 0.243 weekend 0.094 POS - cc cc 0.316 POS - cc nn 0.222
mom, mum, really, LIWC - Family, LIWC - Humans,
thank, and she. The difference in age distribution is
reflected in the feature weights. In the JOINT model,
the bias term is 24.866. Because most of the persons
in the cancer dataset are older, the features associ-
ated with young age in the cancer dataset have much
lower weights compared to the other datasets.
Because our goal is to compare features across
the corpora, we have not exploited corpus-specific
features. For example, thread or subforum features
could be used for the breast cancer corpus, and for
the Fisher dataset, one could add features that ex-
ploit the conversational setting of the data.
5.3 Examples
We present examples of text of younger and older
persons and connect them to the learned model.
The examples are manually selected to illustrate
strengths and weaknesses of the model.
5.3.1 Younger people
We first present some examples of text by young
persons. The following is an example of a 17-year
old in the blog dataset, the system predicted this to
be from a 16.48-year-old:
I can?t sleep, but this time I have school
tommorow, so I have to try I guess. My
parents got all pissed at me today because
I forgot how to do the homework [...]. Re-
ally mad, I ended it pissing off my mom
and [...] NOTHING! Damn, when I?m at
my cousin?s I have no urge to use the com-
puter like I do here, [...].
This example matches with important features de-
termined by the system, containing references to
school and parents, and usage of swearing and anger
words.
121
The following are selected turns (T) by a 19-year
old (system prediction: 17.37 years) in a conversa-
tion in the Fisher dataset.
T: yeah it?s too i just just freaked out [...]
T: that kinda sucks for them
T: they were they were like going crazy
[...]
T: it?s like against some law to like
The text has many informal words such as ?kinda?
and well as many occurrences of the word ?like.?
This example is from a 19-year old from the can-
cer dataset. The system?s prediction was far off, es-
timating an age of 35.48.
Im very young and an athlete and I really
do not want to look disfigured, especially
when I work so hard to be fit. I know it
sounds shallow, but Im young and hope
to [...] my husband one day :) [...] My
grandmother died of breast cancer at 51,
and my mother is currently dealing with a
cancerous tumor on her ovaries.
Besides explicit references to being ?very young,?
the text is much more formal than typical texts, mak-
ing it a hard example.
5.3.2 Older people
The following is a snippet from a 47-year-old
(system prediction: 34.42 years) in the blog dataset.
[...]In the weeks leading up to this meet-
ing certain of the managers repeatedly as-
serted strong positions. [...] their previous
(irresponsible yet non-negotiable) opin-
ions[...] Well, today?s my first Father?s
day [...]. Bringing a child into this world
is quite a responsibility especially with all
the fears and challenges we face. [...]
This matches some important features such as ref-
erences to jobs, as well as having kids. The many
references to the word ?father? in the whole text
might have confused the model. The following are
selected turns (T) by a 73-year old (system predic-
tion: 73.26 years) in a conversation in the Fisher
dataset.
T: ah thoughts i?m retired right now
T: i i really can?t ah think of anyth- think
of i would ah ah change considerably ah
i?m i?m very i?ve been very happily mar-
ried and i have ah three children and six
grandchildren
T: yeah that?s right well i i think i would do
things more differently fair- fairly recently
than a long time ago
This example contains references to being retired
and having grandchildren, as well as many usages
of ?ah?. The following is an example of a 70-year
old (system prediction: 71.53 years) in the cancer
dataset.
[...] I was a little bit fearful of having
surgery on both sides at once (reduction
and lift on the right, tissue expander on
the left) [...] On the good side, my son
and family live near the plastic surgeon?s
office and the hospital, [...], at least from
my son and my granddaughter [...]
6 Conclusion
We presented linear regression experiments to pre-
dict the age of a text?s author. As evaluation metrics,
we found correlation as well as mean absolute er-
ror to be complementary and useful measures. We
obtained correlations up to 0.74 and mean absolute
errors between 4.1 and 6.8 years. In three different
corpora, we found both content features and stylis-
tic features to be strong indicators of a person?s age.
Even a unigram only baseline already gives strong
performance and many POS patterns are strong in-
dicators of old age. By learning jointly from all of
the corpora, we were able to separate generally ef-
fective features from corpus-dependent ones.
Acknowledgments
The authors would like to thank the anonymous review-
ers for feedback, Michael Heilman for the regression
code, and other members of the ARK group for help run-
ning the experiments. This work was funded by NSF
grants CAREER IIS-1054319 to N.A.S. and IIS-0968485
to C.P.R.
122
References
Galen Andrew and Jianfeng Gao. 2007. Scalable train-
ing of l1-regularized log-linear models. In Proc. of
ICML.
Shlomo Argamon, Moshe Koppel, Jonathan Fine, and
Anat R. Shimoni. 2003. Gender, genre, and writing
style in formal written texts. Text, 23(3):321?346.
Shlomo Argamon, Moshe Koppel, James Pennebaker,
and Jonathan Schler. 2007. Mining the blogosphere:
age, gender, and the varieties of self-expression.
Federica Barbieri. 2008. Patterns of age-based linguistic
variation in American English. Journal of Sociolin-
guistics, 12(1):58?88.
Christopher Cieri, David Miller, and Kevin Walker.
2004. The Fisher corpus: a resource for the next gen-
erations of speech-to-text. In Proc. of LREC, pages
69?71.
Hal Daume? III. 2007. Frustratingly easy domain adapta-
tion. In Proc. of ACL.
Penelope Eckert. 1996. Age as a sociolinguistic variable.
In The Handbook of Sociolinguistics. Oxford: Black-
well.
Jianfeng Gao, Galen Andrew, Mark Johnson, and
Kristina Toutanova. 2007. A comparative study of pa-
rameter estimation methods for statistical natural lan-
guage processing. In Proc. of ACL.
Nikesh Garera and David Yarowsky. 2009. Modeling la-
tent biographic attributes in conversational genres. In
Proc. of ACL-IJCNLP.
Sumit Goswami, Sudeshna Sarkar, and Mayur Rustagi.
2009. Stylometric analysis of bloggers? age and gen-
der. In Proc. of ICWSM.
James W. Pennebaker and Lori D. Stone. 2003. Words
of wisdom: Language use over the lifespan. Journal
of Personality and Social Psychology, 85:291?301.
James W. Pennebaker, Roger J. Booth, and Martha E.
Francis, 2001. Linguistic Inquiry and Word Count
(LIWC): A Computerized Text Analysis Program.
Delip Rao, David Yarowsky, Abhishek Shreevats, and
Manaswi Gupta. 2010. Classifying latent user at-
tributes in Twitter. In Proc. of SMUC.
Jonathan Schler, Moshe Koppel, Shlomo Argamon, and
James Pennebaker. 2006. Effects of age and gender
on blogging. In Proceedings of the AAAI Spring Sym-
posia on Computational Approaches to Analyzing We-
blogs.
Werner Spiegl, Georg Stemmer, Eva Lasarcyk, Varada
Kolhatkar, Andrew Cassidy, Blaise Potard, Stephen
Shum, Young Chol Song, Puyang Xu, Peter Beyer-
lein, James Harnsberger, and Elmar No?th. 2009. Ana-
lyzing features for automatic age estimation on cross-
sectional data. In Proc. of INTERSPEECH.
Robert Tibshirani. 1996. Regression shrinkage and se-
lection via the lasso. Journal of the Royal Statistical
Society Series B (Methodological), 58(1):267?288.
Kristina Toutanova, Dan Klein, Christopher D. Manning,
and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In Proc. of
NAACL-HLT.
Charl van Heerden, Etienne Barnard, Marelie Davel,
Christiaan van der Walt, Ewald van Dyk, Michael
Feld, and Christian Muller. 2010. Combining re-
gression and classification methods for improving au-
tomatic speaker age recognition. In Proc. of ICASSP.
123
Proceedings of the 7th Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities, pages 65?73,
Sofia, Bulgaria, August 8 2013. c?2013 Association for Computational Linguistics
Learning to Extract Folktale Keywords
Dolf Trieschnigg, Dong Nguyen and Marie?t Theune
University of Twente
Enschede, The Netherlands
{d.trieschnigg,d.nguyen,m.theune}@utwente.nl
Abstract
Manually assigned keywords provide a
valuable means for accessing large docu-
ment collections. They can serve as a shal-
low document summary and enable more
efficient retrieval and aggregation of infor-
mation. In this paper we investigate key-
words in the context of the Dutch Folk-
tale Database, a large collection of stories
including fairy tales, jokes and urban leg-
ends. We carry out a quantitative and qual-
itative analysis of the keywords in the col-
lection. Up to 80% of the assigned key-
words (or a minor variation) appear in the
text itself. Human annotators show moder-
ate to substantial agreement in their judg-
ment of keywords. Finally, we evaluate a
learning to rank approach to extract and
rank keyword candidates. We conclude
that this is a promising approach to auto-
mate this time intensive task.
1 Introduction
Keywords are frequently used as a simple way
to provide descriptive metadata about collections
of documents. A set of keywords can concisely
present the most important aspects of a document
and enable quick summaries of multiple docu-
ments. The word cloud in Figure 1, for instance,
gives a quick impression of the most important
topics in a collection of over 40,000 documents (a
collection of Dutch folktales).
Keyword assignment or generation is the task
of finding the most important, topical keywords or
keyphrases to describe a document (Turney, 2000;
Frank et al, 1999). Based on keywords, small
groups of documents (Hammouda et al, 2005) or
large collections of documents (Park et al, 2002)
can be summarized. Keyword extraction is a re-
stricted case of keyword assignment: the assigned
keywords are a selection of the words or phrases
appearing in the document itself (Turney, 2000;
Frank et al, 1999).
In this paper we look into keyword extraction
in the domain of cultural heritage, in particular
for extracting keywords from folktale narratives
found in the Dutch Folktale Database (more on
this collection in section 3). These narratives
might require a different approach for extraction
than in other domains, such as news stories and
scholarly articles (Jiang et al, 2009). Stories in the
Dutch Folktale Database are annotated with un-
controlled, free-text, keywords. Because suggest-
ing keywords which do not appear in the text is a
considerably harder task to automate and to eval-
uate, we restrict ourselves to keywords extracted
from the text itself.
In the first part of this paper we study the cur-
rent practice of keyword assignment for this col-
lection. We analyze the assigned keywords in the
collection as a whole and present a more fine-
grained analysis of a sample of documents. More-
over, we investigate to what extent human anno-
tators agree on suitable keywords extracted from
the text. Manually assigning keywords is an ex-
pensive and time-consuming process. Automatic
assignment would bring down the cost and time to
archive material. In the second part of this paper
we evaluate a number of automatic keyword ex-
traction methods. We show that a learning to rank
approach gives promising results.
The overview of this paper is as follows. We
first describe related work in automatic keyword
assignment. In section 3 we introduce the Dutch
Folktale Database. In section 4 we present an anal-
ysis of the keywords currently used in the folktale
database. In section 5 we investigate the agree-
ment of human annotators on keyword extraction.
In section 6 we present and evaluate an automatic
method for extracting and ranking keywords. We
end with a discussion and conclusion in section 7.
65
Keyword (translation) Frequency
dood (death) 5,861
man (man) 4,547
vrouw (woman) 4,154
sterven (to die) 3,168
huis (house) 2,894
spokerij (haunting) 2,491
duivel (devil) 2,487
nacht (night) 2,449
voorteken (omen) 2,380
voorloop (forerunnings) 2,372
geld (money) 2,370
toverij (sorcery) 2,322
zien (to see) 2,270
heks (witch) 2,233
boer (farmer) 2,189
water (water) 2,177
angst (fear) 2,091
hekserij (witchcraft) 1,911
kind (child) 1,853
spoken (ghosts) 1,747
spook (ghost) 1,742
seks (sex) 1,659
Figure 1: Frequent keywords in the Dutch Folktale Database
2 Related Work
Because of space limitations, we limit our dis-
cussion of related work to keyword extraction in
the context of free-text indexing. Automated con-
trolled vocabulary indexing is a fundamentally
different task (see for instance Medelyan and Wit-
ten (2006) and Plaunt and Norgard (1998)).
Typically, keyword extraction consists of two
steps. In the first step candidate keywords are de-
termined and features, such as the frequency or
position in the document, are calculated to char-
acterize these keywords. In the second step the
candidates are filtered and ranked based on these
features. Both unsupervised and supervised algo-
rithms have been used to do this.
2.1 Candidate Extraction
Candidate keywords can be extracted in a number
of ways. The simplest approach is to treat each
single word as a candidate keyword, optionally
filtering out stop words or only selecting words
with a particular Part-of-Speech (Liu et al, 2009a;
Jiang et al, 2009). More sophisticated approaches
allow for multi-word keywords, by extracting con-
secutive words from the text, optionally limited to
keywords adhering to specific lexical patterns (Os-
inski and Weiss, 2005; Hulth, 2003; Rose et al,
2010; Frank et al, 1999; Turney, 2000).
2.2 Features to Characterize Keywords
Many features for characterizing candidate key-
words have been investigated previously, with
varying computational complexities and resource
requirements. The simplest features are based
on document and collection statistics, for instance
the frequency of a potential keyword in the doc-
ument and the inverse document frequency in the
collection (Turney, 2000; Hulth, 2003; Frank et
al., 1999). Examples of more complex features
are: features based on characteristics of lexical
chains, requiring a lexical database with word
meanings (Ercan and Cicekli, 2007); features re-
lated to frequencies in external document collec-
tions and query logs (Bendersky and Croft, 2008;
Yih et al, 2006; Liu et al, 2009b; Xu et al, 2010);
and a feature to determine the cohesiveness of re-
trieved documents with that keyword (Bendersky
and Croft, 2008).
2.3 Unsupervised Methods for Keyword
Extraction
Unsupervised methods for keyword extraction
typically rely on heuristics to filter and rank the
keywords in order of importance. For instance,
by ranking the candidates by their importance in
the collection ? estimated by the inverse docu-
ment frequency. Another approach is to apply the
PageRank algorithm to determine the most impor-
tant keywords based on their co-occurrence link-
structure (Mihalcea and Tarau, 2004). Liu et al
(2009b) employed clustering to extract keywords
that cover all important topics from the original
text. From each topic cluster an exemplar is deter-
mined and for each exemplar the best correspond-
ing keyword is determined.
2.4 Supervised Methods for Keyword
Extraction
Early supervised methods used training data to set
the optimal parameters for (unsupervised) systems
66
based on heuristics (Turney, 2000). Other methods
approached keyword extraction as a binary classi-
fication problem: given a candidate keyword it has
to be classified as either a keyword or not. Meth-
ods include decision trees (Bendersky and Croft,
2008), Naive Bayes (Frank et al, 1999) and Sup-
port Vector Machines (Zhang et al, 2006). Zhang
et al (2008) approached keyword extraction as a
labeling problem for which they employed condi-
tional random fields. Recently, keyword extrac-
tion has been cast as a ranking problem and learn-
ing to rank techniques have been applied to solve
it (Jiang et al, 2009). Jiang et al (2009) concluded
that learning to rank approaches performed better
than binary classifiers in the context of extracting
keywords from scholarly texts and websites. Dif-
ferent variations of learning to rank exist, see (Li,
2011) for an overview.
3 The Dutch Folktale Database
The Dutch Folktale Database is a repository of
over 40,000 folktales in Dutch, old Dutch, Frisian
and a large number of Dutch dialects. The mate-
rial has been collected in the 19th, 20th and 21th
centuries, and consists of stories from various pe-
riods, including the Middle Ages and the Renais-
sance. The collection has both an archival and a
research function. It preserves an important part
of the oral cultural heritage of the Netherlands and
can be used for comparative folk narrative studies.
Since 2004 the database is available online1.
The real value of the database does not only lie
the stories themselves, but also in their manually
added set of descriptive metadata fields. These
fields include, for example, a summary in Dutch,
a list of proper names present in the folktales, and
a list of keywords. Adding these metadata is a
time-consuming and demanding task. In fact, the
amount of work involved hampers the growth of
the folktale database. A large backlog of digitized
folktales is awaiting metadata assignment before
they can be archived in the collection. Being able
to automatically assign keywords to these docu-
ments would be a first step to speed up the archiv-
ing process.
4 Analysis of Assigned Keywords
In this section we analyze the keywords that have
been manually assigned to the stories in the Dutch
Folktale Database. First we look at the keywords
1http://www.verhalenbank.nl, in Dutch only
0 10 20 30 40 50Number of assigned keywords0
500
1000
1500
2000
Doc
ume
nt fr
equ
ency
Figure 2: Number of assigned keywords per doc-
ument
assigned to the collection as a whole. After that we
make a more fine-grained analysis of the keywords
assigned to a selection of the documents.
4.1 Quantitative Analysis
We analyzed a snapshot from the Dutch Folktale
Database (from early 2012) that consists of 41,336
folktales. On average, 15 keywords have been as-
signed to each of these documents (see Figure 2).
The median number of assigned keywords is 10,
however. The keywords vocabulary has 43,195
unique keywords, most of which consist of a sin-
gle word (90%). Figure 1 shows a word cloud
of keywords used in the collection; more frequent
keyword types appear larger. On the right, it lists
the most frequent keyword types (and their trans-
lations). The assignment of keywords to docu-
ments has a Zipfian distribution: a few keyword
types are assigned to many documents, whereas
many keyword types are assigned to few docu-
ments.
When we limit our collection to stories in Dutch
(15,147 documents), we can determine how many
of the manually assigned keywords can be found
literally in the story text2. We define the keyword
coverage of a document as the fraction of its as-
signed keywords which is found in the full text
or its summary. The average keyword coverage
of the Dutch stories is 65%. Figure 3 shows a
histogram of the coverage. It shows that most of
the documents have a keyword coverage of 0.5 or
more.
2Stories in other languages or dialects have been assigned
Dutch keywords.
67
0.0 0.2 0.4 0.6 0.8 1.0Keyword coverage by full-text and summary0
500
1000
1500
2000
2500
3000
Doc
ume
nt fr
equ
ency
Figure 3: Keyword coverage of folktales in Dutch
4.2 Qualitative Analysis
The quantitative analysis does not provide insight
into what kind of keywords have been assigned.
Therefore, we analyzed a selection of documents
more thoroughly. For each of the five largest gen-
res in the collection (fairy tale, traditional legend,
joke, urban legend and riddle) we sampled 10 tales
and manually classified the keywords assigned to
these folktales. A total of almost 1000 keywords
was analyzed. Table 1 summarizes the statistics of
this analysis. Almost 80% of the keywords appear
literally or almost literally in the text. The almost
literal appearances include keywords which differ
in quantity (plural versus singular form) and verb
forms. Verb forms vary in tense (present rather
than past tense) and infinitive keywords of sepa-
rable verbs. An example of the latter is the as-
signment of the keyword ?terugkeren?, to return,
where ?keren? (? turn) and ?terug? (? back) are
used in a sentence. Of the analyzed keywords
5% are synonyms of words appearing the text and
2.3% are hypernyms of words appearing the text
(e.g. ?wapen?, weapon, is used as a keyword with
?mes?, knife, mentioned in the text). The remain-
ing 13% of the keywords represent abstract topic,
event and activity descriptions. For example, the
keyword ?wegsturen?, to send away, when one of
the characters explicitly asks someone to leave.
Other examples are the keywords ?baan?, job, and
?arbeid?, labor, when the story is about an unem-
ployed person.
Based on these numbers we can conclude that
based on extraction techniques alone we should
be able to reproduce a large portion of the manual
keyword assignment. When thesauri are employed
to find synonyms and hypernyms, up to 87% of the
manually assigned keywords could be found. A
much harder task is to obtain the remaining 13%
Classification Count Perc.
Literal 669 67.6%
Almost literal 120 12.1%
Synonym 49 5.0%
Hypernym 23 2.3%
Typing error 2 0.2%
Other 126 12.7%
Total 989 100.0%
Table 1: Keyword types in a set of 1000 folktales
of more abstract keywords, which we will study in
future research.
5 Evaluating Agreement in Keyword
Assignment
The previous analyses raise the question whether
the keywords have been consistently assigned: do
annotators choose the same keywords when pre-
sented with the same text? Moreover, knowing
the difficulty of the task for human annotators will
give us an indication of the level of performance
we may expect from automatic keyword assign-
ment. To determine the agreement between an-
notators we asked ten annotators to classify the
vocabulary of five folktales from different genres.
Frog3 (van den Bosch et al, 2007) was used to
extract the vocabulary of lemmas. After carefully
reading a folktale, the annotator classified the al-
phabetically sorted list of lemmas extracted from
the text. Each lemma was classified as either: 1)
not a relevant keyword ? should not be assigned
to this document (non); 2) a relevant keyword ?
should be assigned (rel); 3) a highly relevant key-
word ? should definitely be assigned (hrel). The
three levels of relevance were used to see whether
annotators have a preference for certain keywords.
The pairwise agreement between annotators was
measured using Cohen?s kappa. Each document
was judged twice, totaling a set of 25 documents.
Most of the annotators were familiar with the folk-
tale database and its keywords; two were active
contributors to the database and thus had previous
experience in assigning keywords to folktales.
On average, the annotators judged 79% of the
vocabulary as non-relevant as keywords. 9% and
12% of the vocabulary was judged as relevant and
highly relevant respectively, but there was a large
variation in these percentages: some annotators
assigned more highly relevant keywords, others
assigned more relevant keywords.
3http://ilk.uvt.nl/frog/
68
Cohen?s Kappa
Classes Average ? Min Max
non, rel, hrel 0.48 0.14 0.16 0.77
non, rel + hrel 0.62 0.16 0.25 0.92
non + rel, hrel 0.47 0.20 0.0 0.84
Table 2: Classification agreement between annota-
tors. Non: non-relevant, rel: relevant, hrel: highly
relevant.
The two experienced annotators showed a con-
sistently higher average agreement in comparison
to the other annotators (0.56 and 0.50 for non, rel,
hrel; 0.7 and 0.64 for non, rel + hrel; 0.56 and 0.50
for non + rel, hrel). Moreover, they assigned more
(relevant and highly relevant) keywords to the doc-
uments on average.
Table 2 summarizes the agreement measured
between annotators. The first row indicates the
agreement when considering agreement over all
three classes; the second row indicates the agree-
ment when treating relevant and highly relevant
keywords as the same class; the last row shows
the agreement in indicating the same highly rel-
evant keywords. The numbers indicate moder-
ate agreement between annotators over all three
classes and when considering the choice of highly
relevant keywords. Annotators show substantial
agreement on deciding between non-relevant and
relevant keywords. Table 3 shows the agreement
between annotators on keywords with different
parts of speech (CGN4 tagset). Most disagree-
ments are on nouns, adjectives and verbs. Verbs
and adjectives show few agreements on relevant
and highly relevant keywords. In contrast, on
20% of the nouns annotators agree on their rele-
vance. It appears that the annotators do not agree
whether adjectives and verbs should be used as
keywords at all. We can give three other reasons
why annotators did not agree. First, for longer
stories annotators were presented with long lists
of candidate keywords. Sometimes relevant key-
words might have been simply overlooked. Sec-
ond, it turned out that some annotators selected
some keywords in favor to other keywords (for in-
stance a hyponym rather than a hypernym), where
others simply annotated both as relevant. Third,
the disagreement can be explained by lack of de-
tailed instructions. The annotators were not told
how many (highly) relevant keywords to select or
4Corpus Gesproken Nederlands (Spoken Dutch Corpus),
http://lands.let.kun.nl/cgn/ehome.htm
what criteria should be met by the keywords. Such
instructions are not available to current annotators
of the collection either.
We conclude that annotators typically agree on
the keywords from a text, but have a varying
notion of highly relevant keywords. The aver-
age keywords-based representation strongly con-
denses the documents vocabulary: a document can
be represented by a fifth (21%) of its vocabulary5.
This value can be used as a cut-off point for meth-
ods ranking extracted keywords, discussed here-
after.
6 Automatically Extracting Keywords
In the last part of this paper we look into automati-
cally extracting keywords. We compare a learning
to rank classifier to baselines based on frequency
and reuse in their ability to reproduce keywords
found in manually classified folktales.
In all cases we use the same method for extract-
ing keyword candidates. Since most of the manual
keywords are single words (90% of the used key-
word types in the collection), we simply extract
single words as keyword candidates. We use Frog
for tokenization and part of speech tagging. Stop
words are not removed.
6.1 Baseline Systems
We use a basic unsupervised baseline for keyword
extraction: the words are ranked according to de-
scending TF-IDF. We refer to this system as TF-
IDF. TF, term frequency, and IDF, inverse docu-
ment frequency, are indicators of the term?s local
and global importance and are frequently used in
information retrieval to indicate the relative impor-
tance of a word (Baeza-Yates and Ribeiro-Neto,
2011).
Note that a word appearing once in the collec-
tion has the highest IDF score. This would imply
that the most uncommon words are also the most
important resulting in a bias towards spelling er-
rors, proper names, and other uncommon words.
Hence, our second baseline takes into account
whether a keyword has been used before in a train-
ing set. Again, the candidates are ranked by de-
scending TF-IDF, but now keywords appearing in
the training collection are ranked above the key-
words not appearing in the collection. We refer to
this baseline as TF-IDF-T.
5Based on the figures that on average 9% of the vocabu-
lary is judged as relevant and 12% as highly relevant
69
Part of speech Adjective Adverb Noun Special Numeral Prep. Verb
Number of words 272 257 646 131 53 268 664
Agreement non 70% 96% 40% 95% 81% 99% 73%
rel 4% 0% 6% 0% 0% 0% 3%
hrel 1% 0% 14% 2% 2% 0% 4%
Disagreement non? rel 15% 2% 17% 2% 11% 0% 12%
non? hrel 5% 1% 8% 2% 4% 1% 5%
rel? hrel 5% 0% 15% 0% 2% 0% 4%
Table 3: Agreement and disagreement of annotators on keywords with different parts of speech. Values
are column-wise percentages. Tags with full agreement are not shown.
6.2 Learning to Rank Keywords
Following Jiang et al (2009) we apply a learn-
ing to rank technique to rank the list of extracted
keywords. We train an SVM to classify the rel-
ative ordering of pairs of keywords. Words cor-
responding to manual keywords should be ranked
higher than other words appearing in the docu-
ment. We use SVM-rank to train a linear ranking
SVM (Joachims, 2006). We use the following fea-
tures.
6.2.1 Word Context
We use the following word context features:
starts uppercase: indicates whether the token
starts with an uppercase letter (1) or not (0). Since
proper names are not used as keywords in the folk-
tale database, this feature is expected to be a neg-
ative indicator of a word being a keyword.
contains space: indicates whether the token con-
tains a space (Frog extracts some Dutch multi-
word phrases as a single token). Tokens with
spaces are not very common.
is number: indicates whether the token consists
of only digits. Numbers are expected not to be a
keyword.
contains letters: indicates whether the token con-
tains at least a single letter. Keywords are expected
to contain letters.
all capital letters: indicates whether the token
consists of only capital letters. Words with only
capital letters are not expected to be keywords.
single letter: indicates whether the token consists
of only one letter. One letter keywords are very
uncommon.
contains punctuation: indicates whether the to-
ken contains punctuation such as apostrophes.
Keywords are expected not to contain punctuation.
part of speech: indicates the part of speech of
the token (each tag is a binary feature). Nouns
are expected to be a positive indicator of key-
words (Jiang et al, 2009).
6.2.2 Document Context
We use the following document context features:
tf: the term frequency indicates the number of ap-
pearances of the word divided by the total number
of tokens in the document.
first offset: indicates the offset of the word?s
first appearance in the document, normalized by
the number of tokens in the document (follow-
ing Zhang et al (2008)). Important (key)words are
expected to be mentioned early.
first sentence offset: indicates the offset of the
first sentence in which the token appears, normal-
ized by the number of sentences in the document.
sentence importance: indicates the maxi-
mum importance of a sentence in which the
word appears, as measured by the SumBasic
score (Nenkova and Vanderwende, 2005). Sum-
Basic determines the relative importance of sen-
tences solely on word probability distributions in
the text.
dispersion: indicates the dispersion or scattering
of the word in the document. Words which are
highly dispersed are expected to be more impor-
tant. The DPnorm is used as a dispersion measure,
proposed in Gries (2008).
6.2.3 Collection Context
We use the following features from the collec-
tion/training context:
idf: the inverse document frequency indicates the
collection importance of the word based on fre-
quency: frequent terms in the collection are less
important than rare terms in the collection.
tf.idf: combines the tf and idf features by multi-
plying them. It indicates a trade-off between local
and global word importance.
is training keyword: indicates whether the word
is used in the training collection as a keyword.
assignment ratio: indicates the percentage of
documents in which the term is present in the text
and in which it is also assigned as a keyword.
70
6.3 Evaluation Method
We evaluate the ranking methods on their ability
to reproduce the manual assignment of keywords.
Ideally the ranking methods rank these manual
keywords highest. We measure the effectiveness
of ranking in terms of (mean) average precision
(MAP), precision at rank 5 (P@5) and precision at
rank R (P@R), similar to Jiang et al (2009). Note
that we use all the manually assigned keywords
as a ground truth, including words which do not
occur in the text itself. This lowers the highest
achievable performance, but it will give a better
idea of the performance for the real task.
We perform a 10-fold stratified cross-validation
with a set of 10,900 documents from the Dutch
Folktale Database, all written in modern Dutch.
6.4 Results
Table 4 lists the performance of the three tested
systems. The TF-IDF system performs worst,
and is significantly outperformed by the TF-IDF-
T system, which in turn is significantly outper-
formed by the rank-SVM system. On average,
rank-SVM returns 3 relevant keywords in its top
5. The reported mean average precision values
are affected by manual keywords which are not
present in the text itself. To put these numbers
in perspective: if we would put the manual key-
words which are in the text in an optimal ranking,
i.e. return these keywords first, we would achieve
an upper bound mean average precision of 0.5675.
Taking into account the likelihood that some of
the highly ranked false positives are relevant af-
ter all (the annotator might have missed a relevant
keyword) and considering the difficulty of the task
(given the variation in agreement between manual
annotators), we argue that the rank-SVM performs
quite well.
Jiang et al (2009) reported MAPs of 0.288 and
0.503 on the ranking of extracted keyphrases from
scholarly articles and tags from websites respec-
tively. Based on these numbers, we could argue
that assigning keywords to folktales is harder than
reproducing the tags of websites, and slightly eas-
ier than reproducing keyphrases from scientific ar-
ticles. Because of differences in the experimental
setup (e.g. size of the training set, features and
system used), it is difficult to make strong claims
on the difficulty of the task.
System MAP P@5 P@R
TF-IDF 0.260 0.394 0.317
TF-IDF-T 0.336 0.541 0.384
rank-SVM 0.399 0.631 0.453
Table 4: Keyword extraction effectiveness. The
differences between systems are statistically sig-
nificant (paired t-test, p< 0.001)
Change in
Feature MAP P@5 P@R
assignment ratio -0.036 -0.056 -0.038
is training keyword 0.006 0.002 0.005
tf.idf -0.004 -0.010 -0.002
part of speech -0.003 -0.007 0.000
dispersion -0.001 -0.001 0.000
idf 0.001 0.002 0.000
starts uppercase 0.000 0.000 -0.001
first offset 0.000 0.000 0.000
tf 0.000 0.000 0.000
contains space 0.000 0.000 0.000
is number 0.000 0.000 0.000
all capital letters 0.000 0.000 0.000
contains punctuation 0.000 0.000 0.000
contains letters 0.000 0.000 0.000
sentence importance 0.000 0.000 0.000
first sentence offset 0.000 0.000 0.000
single letter 0.000 0.000 0.000
Table 5: Differences in performance when leaving
out features. The features are ordered by descend-
ing difference in MAP.
6.5 Feature Ablation
To determine the added value of the individual fea-
tures we carried out an ablation study. Table 5
lists the changes in performance when leaving out
a particular feature (or group of features in case
of part of speech). It turns out that many features
can be left out without hurting the performance.
All the features testing simple word characteristics
(such as single letter) do not, or only marginally
influence the results. Also taking into account the
importance of sentences (sentence importance), or
the first appearance of a word (first offset and first
sentence offset) does not contribute to the results.
System MAP P@5 P@R
rank-SVM 0.399 0.631 0.453
minimum set 0.405 0.631 0.459
Table 6: Results using the full set of features and
the minimum set of features (assignment ratio,
tf.idf, part of speech and dispersion). Differences
between systems are statistically significant (t-test,
p < 0.001).
71
Genre (# stories) MAP P@5 P@R
Trad. legend (3783) 0.439 0.662 0.494
Joke (2793) 0.353 0.599 0.405
Urban legend (1729) 0.398 0.653 0.459
Riddle (1067) 0.391 0.573 0.415
Fairy tale (558) 0.404 0.670 0.477
Pers. narrative (514) 0.376 0.593 0.437
Legend (221) 0.409 0.622 0.478
None (122) 0.366 0.602 0.421
Other (113) 0.405 0.648 0.472
All (10900) 0.399 0.631 0.453
Table 7: SVM performance split according to
story genre. Values in bold are significantly dif-
ferent from the results on the other genres (inde-
pendent t-test, p-value < 0.01)
These observations suggest that almost identi-
cal results can be obtained using only the features
assignment ratio, tf.idf, part of speech and disper-
sion. The results reported in Table 6 confirm this
(we do note that these results were obtained by op-
timizing on the test set).
6.6 Performance on Folktale Genres
The folktale database contains stories from differ-
ent folktale genres, varying from legends to fairy
tales and jokes. Table 7 lists the performance mea-
sures per story genre. Values in bold indicate sig-
nificant differences with the stories from the other
genres combined. The performance on traditional
legends turns out to be significantly better than
other genres: this could be explained by the fact
that on average these stories are longer and there-
fore contain more keywords. Similarly, the de-
crease can be explained for jokes, which are much
shorter on average. Another explanation could be
that more abstract keywords are used to indicate
the type of joke. Interestingly, the riddles, which
are even shorter than jokes, do not perform sig-
nificantly worse than the other genres. Personal
narratives also underperformed in comparison to
the other genres. We cannot readily explain this,
but we suspect it may have something to do with
the fact that personal narratives are more varied in
content and contain more proper names.
7 Discussion and Conclusion
In this work we analyzed keywords in the context
of the Dutch Folktale Database. In this database,
on average 15 keywords have been assigned to a
story, many of which are single keywords which
appear literally or almost literally in the text itself.
Keyword annotators show moderate to substantial
agreement in extracting the same keywords for a
story. We showed that a learning to rank method
using features based on assignment ratio, tf.idf,
part of speech and dispersion can be effectively
used to extract and rank keyword candidates. We
believe that this system can be used to suggest
highly relevant keyword candidates to human an-
notators to speed up the archiving process.
In our evaluation we aimed to reproduce the
manual annotations, but it is unclear whether bet-
ter performing systems are actually more helpful
to the user. In an ad hoc retrieval scenario, in
which the user issues a single query and reviews
a list of retrieved documents, extracted keywords
might be used to boost the early precision of the
results. However, a user might not even notice
a difference when a different keyword extraction
system is used. Moreover, the more abstract key-
words which do not appear in the text might be
more important for the user experience. In fu-
ture work we want to get insight in how keywords
contribute to the end user experience. Ideally, the
evaluation should directly measure how useful the
various keywords are for accessing the collection.
In this work we considered only extracting key-
words from the text we want to annotate. Given
the multilingual content of the database this is a
limited approach: if the goal of assigning key-
words is to obtain a normalized representation of
the stories, this approach will require translation
of either the source text (before extraction) or the
extracted keywords. Even in the monolingual sce-
nario, the extraction of keywords is limited in deal-
ing with differences in style and word use. Writers
may use different words or use words in a differ-
ent way; ideally the representation based on key-
words is a normalized representation which closes
this semantic gap. In future work we will look into
annotation with keywords from multi-lingual the-
sauri combined with free-text keywords extracted
from the text itself. Finally, we want to look into
classification of abstract themes and topics.
Acknowledgments
This research was supported by the Folktales as
Classifiable Texts (FACT) project, part of the
CATCH programme funded by the Netherlands
Organisation for Scientific Research (NWO).
72
References
R Baeza-Yates and B. Ribeiro-Neto. 2011. Modern In-
formation Retrieval. The Concepts and Technology
Behind Search. Addison-Wesley.
M. Bendersky and W.B. Croft. 2008. Discovering key
concepts in verbose queries. In Proceedings of SI-
GIR 2008, pages 491?498.
G. Ercan and I. Cicekli. 2007. Using lexical chains
for keyword extraction. Information Processing &
Management, 43(6):1705?1714.
E. Frank, G.W. Paynter, I.H. Witten, C. Gutwin, and
C.G. Nevill-Manning. 1999. Domain-specific
keyphrase extraction. In Proceedings of IJCAI-99,
pages 668?673. Morgan Kaufmann Publishers Inc.,
San Francisco, CA, USA.
Stefan Th. Gries. 2008. Dispersions and adjusted fre-
quencies in corpora. International Journal of Cor-
pus Linguistics, 13(4):403?437.
K. Hammouda, D. Matute, and M. Kamel. 2005.
Corephrase: Keyphrase extraction for document
clustering. Machine Learning and Data Mining in
Pattern Recognition, pages 265?274.
A. Hulth. 2003. Improved automatic keyword extrac-
tion given more linguistic knowledge. In Proceed-
ings of EMNLP, volume 10, pages 216?223, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
X. Jiang, Y. Hu, and H. Li. 2009. A ranking ap-
proach to keyphrase extraction. In Proceedings of
the 32nd international ACM SIGIR conference on
Research and development in information retrieval,
pages 756?757. ACM.
T. Joachims. 2006. Training Linear SVMs in Lin-
ear Time. In the 12th ACM SIGKDD international
conference, pages 217?226, New York, NY, USA.
ACM.
H. Li. 2011. Learning to Rank for Information Re-
trieval and Natural Language Processing. Synthesis
Lectures on Human Language Technology. Morgan
& Claypool Publishers.
F. Liu, D. Pennell, F. Liu, and Y. Liu. 2009a. Unsu-
pervised approaches for automatic keyword extrac-
tion using meeting transcripts. In Proceedings of
NAACL 2009, pages 620?628. Association for Com-
putational Linguistics.
Z. Liu, P. Li, Y. Zheng, and M. Sun. 2009b. Cluster-
ing to find exemplar terms for keyphrase extraction.
In Proceedings of EMNLP, pages 257?266. Associ-
ation for Computational Linguistics.
O Medelyan and Ian H Witten. 2006. Thesaurus based
automatic keyphrase indexing. In JCDL 2006, pages
296?297. ACM.
R. Mihalcea and P. Tarau. 2004. Textrank: Bringing
order into texts. In Proceedings of EMNLP, vol-
ume 4, pages 404?411. Barcelona, Spain.
A. Nenkova and L. Vanderwende. 2005. The impact of
frequency on summarization. Microsoft Research,
Redmond, Washington, Tech. Rep. MSR-TR-2005-
101.
S. Osinski and D. Weiss. 2005. A concept-driven al-
gorithm for clustering search results. Intelligent Sys-
tems, IEEE, 20(3):48?54.
Y. Park, R.J. Byrd, and B.K. Boguraev. 2002. Auto-
matic glossary extraction: beyond terminology iden-
tification. In Proceedings of COLING 2002, pages
1?7. Association for Computational Linguistics.
Christian Plaunt and Barbara A Norgard. 1998. An
Association Based Method for Automatic Indexing
with a Controlled Vocabulary. Journal of the Ameri-
can Society for Information Science and Technology,
49(10):888?902.
S. Rose, D. Engel, N. Cramer, and W. Cowley. 2010.
Automatic keyword extraction from individual doc-
uments. In Michael W. Berry and Jacob Kogan, ed-
itors, Text Mining: Applications and Theory, pages
3?20. John Wiley & Sons.
P.D. Turney. 2000. Learning algorithms for keyphrase
extraction. Information Retrieval, 2(4):303?336.
A. van den Bosch, G.J. Busser, W. Daelemans, and
S Canisius. 2007. An efficient memory-based mor-
phosyntactic tagger and parser for Dutch. In F. van
Eynde, P. Dirix, I. Schuurman, and V. Vandeghinste,
editors, Selected Papers of the 17th Computational
Linguistics in the Netherlands Meeting, pages 99?
114, Leuven, Belgium.
S. Xu, S. Yang, and F.C.M. Lau. 2010. Keyword ex-
traction and headline generation using novel word
features. Proceedings of the Twenty-Fourth AAAI
Conference on Artificial Intelligence (AAAI-10).
W. Yih, J. Goodman, and V.R. Carvalho. 2006. Find-
ing advertising keywords on web pages. In Proceed-
ings of the 15th international conference on World
Wide Web, pages 213?222. ACM.
K. Zhang, H. Xu, J. Tang, and J. Li. 2006. Keyword
extraction using support vector machine. Advances
in Web-Age Information Management, pages 85?96.
C. Zhang, H. Wang, Y. Liu, D. Wu, Y. Liao, and
B. Wang. 2008. Automatic keyword extrac-
tion from documents using conditional random
fields. Journal of Computational Information Sys-
tems, 4(3):1169?1180.
73
Proceedings of The First Workshop on Computational Approaches to Code Switching, pages 42?50,
October 25, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Predicting Code-Switching in Multilingual Communication for
Immigrant Communities
Evangelos E. Papalexakis
Carnegie Mellon University
Pittsburgh, USA
epapalex@cs.cmu.edu
Dong Nguyen
University of Twente
Enschede, The Netherlands
d.nguyen@utwente.nl
A. Seza Do
?
gru
?
oz
Netherlands Institute
for Advanced Study
Wassenaar, The Netherlands
a.s.dogruoz@gmail.com
Abstract
Immigrant communities host multilingual
speakers who switch across languages
and cultures in their daily communication
practices. Although there are in-depth
linguistic descriptions of code-switching
across different multilingual communica-
tion settings, there is a need for au-
tomatic prediction of code-switching in
large datasets. We use emoticons and
multi-word expressions as novel features
to predict code-switching in a large online
discussion forum for the Turkish-Dutch
immigrant community in the Netherlands.
Our results indicate that multi-word ex-
pressions are powerful features to predict
code-switching.
1 Introduction
Multilingualism is the norm rather than an ex-
ception in face-to-face and online communica-
tion for millions of speakers around the world
(Auer and Wei, 2007). 50% of the EU popula-
tion is bilingual or multilingual (European Comis-
sion, 2012). Multilingual speakers in immigrant
communities switch across different languages
and cultures depending on the social and contex-
tual factors present in the communication envi-
ronment (Auer, 1988; Myers-Scotton, 2002; Ro-
maine, 1995; Toribio, 2002; Bullock and Toribio,
2009). Example (1) illustrates Turkish-Dutch
code-switching in a post about video games in an
online discussion forum for the Turkish immigrant
community in the Netherlands.
Example (1)
user1: <dutch>vette spellllllllll </dutch>..
<turkish>bir girdimmi cikamiyomm ..
yendikce yenesi geliyo insanin</turkish>
Translation: <dutch> awesome gameeeee
</dutch>.. <turkish>once you are in it, it is
hard to leave .. the more you win, the more
you want to win</turkish>
Mixing two or more languages is not a random
process. There are in-depth linguistic descriptions
of code-switching across different multilingual
contexts (Poplack, 1980; Silva-Corval?an, 1994;
Owens and Hassan, 2013). Although these studies
provide invaluable insights about code-switching
from a variety of aspects, there is a growing need
for computational analysis of code-switching in
large datasets (e.g. social media) where man-
ual analysis is not feasible. In immigrant set-
tings, multilingual/bilingual speakers switch be-
tween minority (e.g. Turkish) and majority (e.g.
Dutch) languages. Code-switching marks multi-
lingual, multi-cultural (Luna et al., 2008; Gros-
jean, 2014) and ethnic identities (De Fina, 2007)
of the speakers. By predicting code-switching
patterns in Turkish-Dutch social media data, we
aim to raise consciousness about mixed language
communication patterns in immigrant communi-
ties. Our study is innovative in the following ways:
? We performed experiments on the longest
and largest bilingual dataset analyzed so far.
? We are the first to predict code-switching in
social media data which allow us to investi-
gate features such as emoticons.
? We are the first to exploit multi-word expres-
sions to predict code-switching.
? We use automatic language identification at
the word level to create our dataset and fea-
tures that capture previous language choices.
The rest of this paper is structured as follows:
we discuss related work on code-switching and
multilingualism in Section 2, our dataset in Sec-
tion 3, a qualitative analysis in Section 4, our ex-
perimental setup and features in Section 5, our re-
sults in Section 6 and our conclusion in Section
7.
42
2 Related Work
Code-switching in sociolinguistics There is
rarely any consensus on the terminology about
mixed language use. Wei (1998) considers al-
ternations between languages at or above clause
levels as code-mixing. Romaine (1995) refers to
both inter-sentential and intra-sentential switches
as code-switching. Bilingual speakers may shift
from one language to another entirely (Poplack et
al., 1988) or they mix languages partially within
the single speech (Gumperz, 1982). In this study,
we focus on code-switching within the same post
in an online discussion forum used by Turkish-
Dutch bilinguals.
There are different theoretical models which
support (Myers-Scotton, 2002; Poplack, 1980) or
reject (MacSwan, 2005; Thomason and Kaufman,
2001) linguistic constraints on code-switching.
According to (Thomason and Kaufman, 2001;
Gardner-Chloros and Edwards, 2004) linguistic
factors are mostly unpredictable since social fac-
tors govern the multilingual environments in most
cases. Bhatt and Bolonyai (2011) have an exten-
sive study on socio-cognitive factors that lead to
code-switching across different multilingual com-
munities.
Although multilingual communication has been
widely studied through spoken data analyses, re-
search on online communication is relatively re-
cent. In terms of linguistic factors C?ardenas-
Claros and Isharyanti (2009) report differences
between Indonesian-English and Spanish-English
speakers in their amount of code-switching on
MSN (an instant messaging client). Durham
(2003) finds a tendency to switch to English over
time in an online multilingual (German, French,
Italian) discussion forum in Switzerland.
The media (e.g. IRC, Usenet, email, online
discussions) used for multilingual conversations
influence the amount of code-switching as well
(Paolillo, 2001; Hinrichs, 2006). Androutsopou-
los and Hinnenkamp (2001), Tsaliki (2003) and
Hinnenkamp (2008) have done qualitative anal-
yses of switch patterns across German-Greek-
Turkish, Greek-English and Turkish-German in
online environments respectively.
In terms of social factors, a number of studies
have investigated the link between topic and lan-
guage choices qualitatively (Ho, 2007; Androut-
sopoulos, 2007; Tang et al., 2011). These stud-
ies share the similar conclusion that multilingual
speakers use minority languages to discuss topics
related to their ethnic identity and reinforcing inti-
macy and self-disclosure (e.g. homeland, cultural
traditions, joke telling) whereas they use the ma-
jority language for sports, education, world poli-
tics, science and technology.
Computational approaches to code-switching
Recently, an increasing number of research within
NLP has focused on dealing with multilingual
documents. For example, corpora with multilin-
gual documents have been created to support stud-
ies on code-switching (e.g. Cotterell et al. (2014))
To enable the automatic processing and analysis
of documents with mixed languages, there is a
shift in focus toward language identification at the
word level (King and Abney, 2013; Nguyen and
Do?gru?oz, 2013; Lui et al., 2014). Most closely re-
lated to our work is the study by Solorio and Liu
(2008) who predict code-switching in recorded
English-Spanish conversations. Compared to their
work, we use a large-scale social media dataset
that enables us to explore novel features.
The task most closely related to automatic pre-
diction of code-switching is automatic language
identification (King and Abney, 2013; Nguyen and
Do?gru?oz, 2013; Lui et al., 2014). While automatic
language detection uses the words to identify the
language, automatic prediction of code-switching
involves predicting whether the language of the
next word is the same without having access to the
next word itself.
Language practices of the Turkish community
in the Netherlands Turkish has been in con-
tact with Dutch due to labor immigration since
the 1960s and the Turkish community is the
largest minority group (2% of the whole popula-
tion) in the Netherlands (Centraal Bureau voor de
Statistiek, 2013). In addition to their Dutch flu-
ency, second and third generations are also fluent
in Turkish through speaking it within the family
and community, regular family visits to Turkey
and watching Turkish TV through satellite dishes.
These speakers grow up speaking both languages
simultaneously rather than learning one language
after the other (De Houwer, 2009). In addition
to constant switches between Turkish and Dutch,
there are also literally translated Dutch multi-word
expressions (Do?gru?oz and Backus, 2007; Do?gru?oz
and Backus, 2009). Due to the religious back-
grounds of the Turkish-Dutch community, Arabic
43
words and phrases (e.g. greetings) are part of daily
communication. In addition, English words and
phrases are used both in Dutch and Turkish due to
the exposure to American and British media.
Although the necessity of studying immigrant
languages in Dutch online environments has been
voiced earlier (Dorleijn and Nortier, 2012), the
current study is the first to investigate mixed lan-
guage communication patterns of Turkish-Dutch
bilinguals in online environments.
3 Dataset
Our data comes from a large online forum
(Hababam) used by Turkish-Dutch speakers. The
forum is active since 2000 and contains 28 sub-
forums on a variety of topics (e.g. sports, poli-
tics, education). Each subforum consists of mul-
tiple threads which start with a thread title (e.g. a
statement or question) posted by a moderator or
user. The users are Turkish-Dutch bilinguals who
reside in the Netherlands. Although Dutch and
Turkish are used dominantly in the forum, English
(e.g. fixed expressions) and Arabic (e.g. prayers)
are occasionally used (less than 1%) as well. We
collected the data between June 2005 and October
2012 by crawling the forum. Statistics of our data
are shown in Table 1.
Frequency
Number of posts 4,519,869
Number of users 14,923
Number of threads 113,517
Number of subforums 29
Table 1: Dataset Statistics
The subforums Chit-Chat (1,671,436), Turkish
youth & love (447,436), and Turkish news & up-
dates (418,135) have the highest post frequency
whereas Columns (4727), Science & Philosophy
(5083) and Other Beliefs (6914) have the lowest
post frequency.
An automatic language identification tagger is
used to label the language of the words in posts
and titles of the threads. The tagger distinguishes
between Turkish and Dutch using logistic regres-
sion (Nguyen and Do?gru?oz, 2013) and achieves
a word accuracy of approximately 97%. We use
the language labels to train our classifier (since
given the labels we can determine whether there
is a switch or not), and to evaluate our model.
4 Types of Code-Switching
In this section, we provide a qualitative analysis of
code-switching in the online forum. We differen-
tiate between two types of code-switching: code-
switching across posts and code-switching within
the same post.
4.1 Code-switching across posts
Within the same discussion thread, users react
to posts of other users in different languages.
In example (2), user 1 posts in Dutch to tease
User 2. User 2 reacts to this message with a
humorous idiomatic expression in Turkish (i.e.
[adim cikmis] ?I made a name?) to indirectly
emphasize that there is no reason for her to defend
herself since she has already become famous as
the perfect person in the online community. This
type of humorous switch has also been observed
for Greek-English code-switching in face-to-face
communication (Gardner-Chloros and Finnis,
2003). The text is written with Dutch orthography
instead of conventional Turkish orthography (i.e.
[ad?m c??km?s?]). It is probably the case that the user
has a Dutch keyboard without Turkish characters.
However, writing with non-Turkish characters in
online environments is also becoming popular
among monolingual Turkish users from Turkey.
Example (2)
User1: <dutch> je hoefde niet gelijk in de
verdediging te schieten hoor </dutch> :P
Tra: ?you do not need to be immediately
defensive dear?
User2: <turkish> zaten adim cikmis
mukemmel sahane kusursuz insana, bi de
yine cikmasin </turkish> :(
Tra: ?I already have established a name as a
great amazing perfect person, I do not need
it to spread around once more?
Example (3) is taken from a thread about break-
fast traditions. The users have posted what they
had for breakfast that day. The first user talks
about his breakfast in Turkish and describes the
culture specific food items (e.g. borek ?Turkish
pastry?) prepared by his mother. The second user
describes a typical Dutch breakfast and therefore
switches to Dutch.
Example (3)
User1: <turkish>annemin peynirli borekleri
ve cay</turkish>
Tra: ?the cheese pastries of my mom and
tea?
44
User2: <dutch>Twee sneetjes geroost-
erd bruin brood met kipfilet en een glas
thee.</dutch>
Tra: ?Two pieces of roasted brown bread with
chicken filet and a cup of tea?
4.2 Code-switching within the same post
In addition to code-switching across posts, we en-
countered code-switching within the same post of
a user as well. Manual annotation of a subset of
the posts in Nguyen and Do?gru?oz (2013), suggests
that less than 20% of the posts contain a switch.
Example (4) is taken from a thread about Mother?s
Day and illustrates an intra-sentential switch. The
user starts the post in Dutch (vakantie boeken ?to
book a vacation?) and switches to Turkish since
booking a vacation through internet sites or a
travel agency is a typical activity associated with
the Dutch culture.
Example (4)
<dutch>vakantie boeken</dutch>
<turkish> yaptim annecigimee </turkish>
Tra
1
:?(I) <dutch>booked a holiday</dutch>
<turkish>for my mother.</turkish>?
Example (5) is taken from a thread about Turk-
ish marriages and illustrates an inter-sentential
switch. The user is advising the other users
in Turkish to be very careful about choosing
their partners. Since most Turkish community
members prefer Turkish partners and follow
Turkish traditions for marriage, she talks about
these topics in Turkish. However, she switches
to Dutch when she talks about getting a diploma
in the Dutch school system. Similar examples of
code-switching for emphasizing different identi-
ties based on topic have been observed for other
online and face-to-face communication as well
(Androutsopoulos, 2007; Gardner-Chloros, 2009).
Example (5)
<turkish>Allah korusun yani. Kocani iyi
sec diyim=) evlilik evcilik degildir.</turkish>
<dutch>Al zou ik wanneer ik getrouwd ben
een HBO diploma op zak hebben, zou ik
hem dan denk ik niet verlaten.</dutch>
Tra:?<turkish> May God protect you.
Choose your husband carefully. Marriage is
not a game </turkish> <dutch> Even if I
am married and have a university diploma, I
don?t think I will leave him </dutch>?
Code-switching through greetings, wishes and
formulaic expressions are commonly observed
1
It is possible to drop the subject pronoun in Turkish. As
typical in bilingual speech, an additional Turkish verb yap-
mak follows the Dutch verb boeken ?to book?.
in bilingual face-to-face communication and on-
line immigrant forums as well (Androutsopoulos,
2007; Gardner-Chloros, 2009).
5 Experimental Setup
The focus of this paper is on code-switching
within the same post. We discuss the setup and
features of our experiment in this section.
5.1 Goal
We cast the prediction of the code-switch point
within the post as a binary classification problem.
We define the i-th token of the post as an instance.
If the i + 1th token is in a different language, the
label is 1. Otherwise, the label is 0.
Obtaining language labels In order to label
each token of a post, we rely on the labels ob-
tained using automatic language identification at
the word level (see Section 3). This process may
not be the most accurate way of labeling each to-
ken of a post at a large scale. One particular arti-
fact of this procedure is that an automatic tagger
may falsely tag the language of a token in longer
posts. As a result, some lengthy posts might ap-
pear to have one or more code-switches by ac-
cident. However, since the accuracy of our tag-
ger is high (approx. 97% accuracy), we expect
the amount of such spurious code-switches to be
low. For future work, we plan to experiment on a
dataset based on automatic language identification
as well as a smaller dataset using manual annota-
tion.
5.2 Creating train and test sets
Before we attempt to train a classifier on our data,
we eliminate the biases and imbalances. The ma-
jority of posts do not contain any switches. As a
consequence, the number of instances that belong
to the ?0? class (i.e. no code-switching occurring
after the current word) grossly outnumber the in-
stances of class ?1?, where code-switching takes
place. In order to alleviate this class imbalance, for
all our experiments, we sample an equal amount
of instances from ?0? and ?1? classes randomly
2
,
both for our training and testing data. This way
the result will not favor the ?0? class even if we
randomly decide on the class label for each in-
stance. The average number of training and testing
2
We do 100 iterations and average the results of all these
independent samples.
45
instances per iteration was 4000 and 80000 respec-
tively. By drawing 100 independent samples from
the entire dataset, we cover a reasonable portion of
the full data and do not sacrifice the balance of the
two classes, which is crucially important for the
validity of our results.
5.3 Feature selection
We use the following features (see Table 2) to in-
vestigate code-switching within a post.
5.3.1 Non-linguistic features
Emoticons Emoticons are iconic symbols that
convey emotional information along with lan-
guage use in online environments (Dresner and
Herring, 2014). Emoticons have mostly been used
in the context of sentiment analysis (e.g. Volkova
et al. (2013), Chmiel et al. (2011)). Park et al.
(2014) studied how the use of emoticons differ
across cultures in Twitter data. Panayiotou (2004)
studied how bilinguals express emotions in face-
to-face environments in different languages. We
are the first to investigate the role of emoticons as a
non-linguistic factor in predicting code-switching
on social media.
Emoticons in our data are either signified by
a special tag [smiley:smiley type] or can
appear in any of the common ASCII emoticon
forms (e.g. :), :-) etc.). In order to detect the
emoticons, we used a hand picked list of ASCII
emoticons as our dictionary, as well as a filter that
searched for the special emoticon tag. Since we
rely on an automatic language tagger, the language
label of a particular emoticon depends on its sur-
rounding tokens. If an emoticon is within a block
of text that is tagged as Turkish, then the emoticon
will automatically obtain a Turkish label (and ac-
cordingly for Dutch). For future work, we will ex-
periment with labeling emoticons differently (e.g.
introducing a third, neutral label).
To assess the strength of emoticons as predic-
tors of code-switching, we generate 4 different
features (see Table 2). These features capture
whether or not there is an emoticon at or before
the token that we want to classify as the switch
boundary between Dutch and Turkish. We record
whether there was an emoticon at token i (i.e. the
token we want to classify), token i ? 1 and token
i ? 2.
The last emoticon feature records whether there
is any emoticon after the current token. We note
that this feature looks ahead (after the i-th token),
and therefore cannot be implemented in a real time
system which predicts code-switching on-the-fly.
However, we included the feature for exploratory
purposes.
5.3.2 Linguistic features
Language around the switch point We also in-
vestigate whether the knowledge of the language
of a couple of tokens before the token of inter-
est, as well as the language at the token of inter-
est, hold some predictive strength. These features
correspond to #1-3 in Table 2. Generally, the lan-
guage label is binary. However, if there are no to-
kens in positions i ? 2 or i ? 1 for features #1
and #2, we assign a third value to represent this
non-existence. Additionally, we explore whether
a previous code-switching in a post triggers a sec-
ond code-switching later in the same post. We test
this hypothesis by recording feature #4 which rep-
resents the existence of code-switching before to-
ken i.
Single word versus multi-word switch There
is an on-going discussion in multilingualism about
the classification of switched tokens (Poplack,
2004; Poplack, 2013) and whether there are
linguistic constraints on the switches (Myers-
Scotton, 2002). In addition to switches across in-
dividual lexical tokens, multilingual speakers also
switch across multi-word expressions.
Automatic identification of multi-word expres-
sions in monolingual language use have been
widely discussed (Baldwin et al., 2003; Baldwin
and Kim, 2010) but we know little about how to
predict switch points that include multi-word ex-
pressions. We are the first to include multi-word
expressions as a feature to predict code-switching.
We are mostly inspired by (Schwartz et al., 2013)
in identifying MWEs.
More specifically, we built a corpus of 3-gram
MWEs (2,241,484 in total) and selected the most
frequent 100 MWEs. We differentiate between
two types of MWEs: Let the i-th token of a post
be the switch point. For type 1, we take 3 tokens
(all in the same language) right before the switch
token (i.e. terms i? 3, i? 2, i? 1). [Allah razi ol-
sun] ?May the Lord be with you? and [met je eens]
?agree with you? are the two of the most frequent
MWEs (in Turkish and Dutch respectively).
For type 2, we take the tokens i ? 2, i ? 1, i
and the last token is in a different language (e.g.
[Turkse premier Recep] ?Turkish prime-minister
46
Table 2: Features
Feature # Feature Description
1 Language of token in position i ? 2
2 Language of token in position i ? 1
3 Language of token in position i (current token)
4 Was there code-switching before the current token?
5 Is there an emoticon in position i ? 2?
6 Is there an emoticon in position i ? 1?
7 Is there an emoticon in position i?
8 Are there any emoticons in positions after i?
9 Is the i-th token the first word of a 3-word multi-word expression?
10 Is the i-th token the second word of a 3-word multi-word expression?
11 Is the i-th token the third word of a 3-word multi-word expression?
Recep?).
The first type of MWEs captures whether an
MWE (all three words in the same language), sig-
nifies code-switching for token i or not.
The second type investigates whether there are
MWEs that ?spill over? the code-switching point
(i.e. the first two tokens of an MWE are in the
same language, but the third token is in another
language). In order to get a good estimate of the
MWEs in our corpus, we count the occurrences of
all these 3-grams and keep the top scoring ones in
terms of frequency, which end up as our dictionary
of MWEs.
6 Results
To evaluate the predictive strength of our features,
we conduct experiments using a Naive Bayes clas-
sifier.
In order to measure the performance, we train
the classifiers for various combinations of the fea-
tures shown in Table 2. As we described in the pre-
vious section, we train on randomly chosen, class-
balanced parts of the data and we test on randomly
selected balanced samples (disjoint from the train-
ing set), averaging over 100 runs. For each com-
bination of features, we measure and report aver-
age precision, recall, and F1-score, with respect to
positively predicting code-switching.
Table 3 illustrates the performance of individ-
ual features used in our classifier. Features that
concern the language of the previous tokens (i.e.
features #1 & #2) seem to perform better than
chance in predicting code-switching. On the other
hand, features #3 (language of the token in posi-
tion i) and #4 (previous code-switching) have the
worst performance. In fact, the obtained classi-
Table 3: Performance of individual features
Feature # Precision Recall F1 score
1 0.6305 1 0.7733
2 0.6362 1 0.7776
3 0 0 -
4 0 0 -
5 0.704 0.2116 0.3254
6 0.7637 0.2324 0.3564
7 0.8025 0.1339 0.0954
8 0.4879 0.3214 0.3875
9 0.5324 0.7819 0.6335
10 0.5257 0.8102 0.6376
11 0.5218 0.8396 0.6436
fier always predicts no code-switching regardless
of the value of the feature. Therefore, both pre-
cision and recall are 0. Features #1 & #2 behave
differently from features #3 & #4 because #1 & #2
have ternary values (the token language, or non-
existing). This probably forces the classifiers to
produce a non-constant decision. For instance, the
model for feature #1 decides positively for code-
switching if the language label is either Turkish or
Dutch and decides negatively if the label is non-
existing.
The rest of the individual features perform sim-
ilarly but worse than #1 and #2. Therefore, it is
necessary to use a combination of features instead
of single ones.
After examining how features perform individu-
ally, we further investigate how features behave in
groups. We first group the features into homoge-
nous categories (e.g. #1-#3 focus on the language
of tokens, #5-#8 record the presence of emoticons
and #9-#11 refer to MWEs). Subsequently, we
test the performance of these categories in differ-
ent combinations, and finally measure the effect of
47
Table 4: Performance of groups of features
Features Precision Recall F1 score
1-3 Language of tokens 0.6362 1 0.7777
1-4 Language + previous code-switching 0.6663 0.1312 0.6663
5-8 Emoticons 0.6638 0.397 0.2766
9-11 MWEs 0.5384 0.7476 0.626
5-11 Emoticons + MWEs 0.52 0.8718 0.6466
1-8 Language + previous code-switching + emoticons 0.6932 0.5114 0.4634
1-4, 9-11 Language + previous code-switching + MWEs 0.712 0.7297 0.7113
1-11 All 0.6847 0.8034 0.7106
using all our features for the task. Table 4 shows
the combinations of the features we used, as well
as the average precision, recall, and F1-score.
According to Table 4, the combination of the
language of the tokens (features #1-#3) and the
previous code-switching earlier in the post (fea-
tures #1-#4), and MWEs (features #9-#11) per-
form the highest in terms of precision/recall. Fea-
tures #3 and #4 have rather low performances on
their own but they yield a strong classifier in com-
bination with other features.
When we use features that record emoticons
(#5-#8) or MWEs (#9-#11) alone, the performance
of our classifier decreases. In general, MWEs out-
perform emoticons. We observe this performance
boost when we combine emoticon features with
other features (e.g. #1-#8) and with MWEs to-
gether in the same subset (#1-#4, #9-#11).
7 Conclusion
We focused on predicting code-switching points
for a mixed language online forum used by
the Turkish-Dutch immigrant community in the
Netherlands. For the first time, a long term data
set was used to investigate code-switching in so-
cial media. We are also the first to test new fea-
tures (e.g. emoticons and MWEs) to predict code-
switching and to identify the features with sig-
nificant predictive strength. For future work, we
will continue our investigation with exploring the
predictive value of these new features within the
Turkish-Dutch immigrant community as well as
others.
8 Acknowledgements
The first author was supported by the National Sci-
ence Foundation (NSF), Grant No. IIS-1247489.
The second author was supported by the Nether-
lands Organization for Scientific Research (NWO)
grant 640.005.002 (FACT). The third author was
supported by a Digital Humanities Research Grant
from Tilburg University and a research fellowship
from Netherlands Institute for Advanced Study.
References
Jannis Androutsopoulos and Volker Hinnenkamp.
2001. Code-switching in der bilingualen chat-
kommunikation: ein explorativer blick auf# hellas
und# turks. Beisswenger, Michael (ed.), pages 367?
401.
Jannis Androutsopoulos, 2007. The Multilingual In-
ternet, chapter Language choice and code-switching
in German-based diasporic web forums, pages 340?
361. Oxford University Press.
Peter Auer and Li Wei, 2007. Handbook of multilin-
gualism and multilingual communication., chapter
Introduction: Multilingualism as a problem? Mono-
lingualism as a problem, pages 1?14. Berlin: Mou-
ton de Gruyter.
Peter Auer. 1988. A conversation analytic ap-
proach to code-switching and transfer. Codeswitch-
ing: Anthropological and sociolinguistic perspec-
tives, 48:187?213.
Timothy Baldwin and Su Nam Kim. 2010. Multiword
expressions. Handbook of Natural Language Pro-
cessing, second edition. Morgan and Claypool.
Timothy Baldwin, Colin Bannard, Takaaki Tanaka, and
Dominic Widdows. 2003. An empirical model
of multiword expression decomposability. In Pro-
ceedings of the ACL 2003 workshop on Multiword
expressions: analysis, acquisition and treatment-
Volume 18, pages 89?96. Association for Computa-
tional Linguistics.
Rakesh M Bhatt and Agnes Bolonyai. 2011. Code-
switching and the optimal grammar of bilingual lan-
guage use. Bilingualism: Language and Cognition,
14(04):522?546.
Barbara E Bullock and Almeida Jacqueline Toribio.
2009. The Cambridge handbook of linguistic code-
switching, volume 1. Cambridge University Press
Cambridge.
48
Monica S. C?ardenas-Claros and Neny Isharyanti.
2009. Code-switching and code-mixing in inter-
net chatting: Between?yes,?ya,?and?si?-a case study.
The Jalt Call Journal, 5(3):67?78.
Centraal Bureau voor de Statistiek. 2013. Bevolking,
generatie, geslacht, leeftijd en herkomstgroepering.
2013.
Anna Chmiel, Julian Sienkiewicz, Mike Thelwall,
Georgios Paltoglou, Kevan Buckley, Arvid Kappas,
and Janusz A Ho?yst. 2011. Collective emotions
online and their influence on community life. PloS
one, 6(7):e22207.
Ryan Cotterell, Adithya Renduchintala, Naomi Saphra,
and Chris Callison-Burch. 2014. An algerian
arabic-french code-switched corpus. In LREC.
Anna De Fina. 2007. Code-switching and the con-
struction of ethnic identity in a community of prac-
tice. Language in Society, 36(03):371?392.
Annick De Houwer. 2009. Bilingual first language
acquisition. Multilingual Matters.
A Seza Do?gru?oz and Ad Backus. 2007. Postverbal el-
ements in immigrant Turkish: Evidence of change?
International Journal of Bilingualism, 11(2):185?
220.
A. Seza Do?gru?oz and Ad Backus. 2009. Innova-
tive constructions in Dutch Turkish: An assessment
of ongoing contact-induced change. Bilingualism:
Language and Cognition, 12(01):41?63.
Margreet Dorleijn and Jacomine Nortier, 2012. The
Cambridge Handbook of Linguistic Code-switching,
chapter Code-switching and the internet, pages 114?
127. Cambridge University Press.
Eli Dresner and Susan C Herring. 2014. Emoticons
and illocutionary force. In Perspectives on Theory
of Controversies and the Ethics of Communication,
pages 81?90. Springer.
Mercedes Durham. 2003. Language choice on a Swiss
mailing list. Journal of Computer-Mediated Com-
munication, 9(1):0?0.
European Comission. 2012. Europeans and their lan-
guages: Special barometer 386. Technical report,
European Comission.
Penelope Gardner-Chloros and Malcolm Edwards.
2004. Assumptions behind grammatical approaches
to code-switching: When the blueprint is a red
herring. Transactions of the Philological Society,
102(1):103?129.
Penelope Gardner-Chloros and Katerina Finnis. 2003.
How code-switching mediates politeness: Gender-
related speech among London Greek-Cypriots. So-
ciolinguistic Studies, 4(2):505?532.
Penelope Gardner-Chloros, 2009. Handbook of Code-
switching, chapter Sociolinguistic Factors in Code-
Switching, pages 97?114. Cambridge University
Press.
Francois Grosjean. 2014. Bicultural bilinguals. Inter-
national Journal of Bilingualism, xx(xx):1?15.
John J Gumperz. 1982. Discourse strategies, vol-
ume 1. Cambridge University Press.
Volker Hinnenkamp. 2008. Deutsch, Doyc or Doitsch?
Chatters as languagers?The case of a German?
Turkish chat room. International Journal of Mul-
tilingualism, 5(3):253?275.
Lars Hinrichs. 2006. Codeswitching on the Web: En-
glish and Jamaican Creole in E-mail Communica-
tion (Pragmatics & Beyond, Issn 0922-842x). John
Benjamins.
Judy Woon Yee Ho. 2007. Code-mixing: Linguistic
form and socio-cultural meaning. The International
Journal of Language Society and Culture, 21.
Ben King and Steven P Abney. 2013. Labeling the
languages of words in mixed-language documents
using weakly supervised methods. In HLT-NAACL,
pages 1110?1119.
Marco Lui, Jey Han Lau, and Timothy Baldwin. 2014.
Automatic detection and language identification of
multilingual documents. Transactions of the Asso-
ciation for Computational Linguistics, 2:27?40.
David Luna, Torsten Ringberg, and Laura A Peracchio.
2008. One individual, two identities: Frame switch-
ing among biculturals. Journal of Consumer Re-
search, 35(2):279?293.
Jeff MacSwan. 2005. Codeswitching and generative
grammar: A critique of the mlf model and some
remarks on ?modified minimalism?. Bilingualism:
language and cognition, 8(01):1?22.
Carol Myers-Scotton. 2002. Contact linguistics:
Bilingual encounters and grammatical outcomes.
Oxford University Press Oxford.
Dong Nguyen and A. Seza Do?gru?oz. 2013. Word level
language identification in online multilingual com-
munication. In Proceedings of EMNLP 2013.
Jonathan Owens and Jidda Hassan, 2013. Informa-
tion Structure in Spoken Arabic, chapter Conversa-
tion markers in Arabic-Hausa code-switching, pages
207?243. Routledge Arabic Linguistics. Routledge.
Alexia Panayiotou. 2004. Switching codes, switch-
ing code: Bilinguals? emotional responses in english
and greek. Journal of multilingual and multicultural
development, 25(2-3):124?139.
John C Paolillo. 2001. Language variation on internet
relay chat: A social network approach. Journal of
sociolinguistics, 5(2):180?213.
49
Jaram Park, Young Min Baek, and Meeyoung Cha.
2014. Cross-cultural comparison of nonverbal cues
in emoticons on twitter: Evidence from big data
analysis. Journal of Communication, 64(2):333?
354.
Shana Poplack, David Sankoff, and Christopher Miller.
1988. The social correlates and linguistic processes
of lexical borrowing and assimilation. Linguistics,
26(1):47?104.
Shana Poplack. 1980. Sometimes i?ll start a sentence
in spanish y termino en espanol: toward a typology
of code-switching1. Linguistics, 18(7-8):581?618.
Shana Poplack, 2004. Soziolinguistik. An interna-
tional handbook of the science of language, chapter
Codeswitching, pages 589?597. Walter de Gruyter,
2nd edition.
Shana Poplack. 2013. ?sometimes i?ll start a sentence
in spanish y termino en espa?nol?: Toward a typology
of code-switching. Linguistics, 51(Jubilee):11?14.
Suzanne Romaine. 1995. Bilingualism (2nd edn).
Malden, MA: Blackwell Publishers.
H Andrew Schwartz, Johannes C Eichstaedt, Mar-
garet L Kern, Lukasz Dziurzynski, Stephanie M Ra-
mones, Megha Agrawal, Achal Shah, Michal Kosin-
ski, David Stillwell, Martin EP Seligman, et al.
2013. Personality, gender, and age in the language
of social media: The open-vocabulary approach.
PloS one, 8(9):e73791.
Carmen Silva-Corval?an. 1994. Language Contact and
Change: Spanish in Los Angeles. ERIC.
Thamar Solorio and Yang Liu. 2008. Learning to pre-
dict code-switching points. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 973?981. Association for
Computational Linguistics.
Dai Tang, Tina Chou, Naomi Drucker, Adi Robertson,
William C Smith, and Jeffery T Hancock. 2011. A
tale of two languages: strategic self-disclosure via
language selection on facebook. In Proceedings of
the ACM 2011 conference on Computer supported
cooperative work, pages 387?390. ACM.
Sarah Grey Thomason and Terrence Kaufman. 2001.
Language contact. Edinburgh University Press Ed-
inburgh.
Almeida Jacqueline Toribio. 2002. Spanish-english
code-switching among us latinos. International
journal of the sociology of language, pages 89?120.
Liza Tsaliki. 2003. Globalization and hybridity: the
construction of greekness on the internet. The Media
of Diaspora, Routledge, London.
Svitlana Volkova, Theresa Wilson, and David
Yarowsky. 2013. Exploring demographic language
variations to improve multilingual sentiment analy-
sis in social media. In EMNLP, pages 1815?1827.
Li Wei, 1998. Codeswitching in conversation: Lan-
guage, interaction and identity, chapter The ?why?
and ?how? questions in the analysis of conversational
codeswitching, pages 156?176. Routledge.
50
