Characterising Measures of Lexical Distributional Similarity
Julie Weeds, David Weir and Diana McCarthy
Department of Informatics
University of Sussex
Brighton, BN1 9QH, UK
{juliewe, davidw,dianam}@sussex.ac.uk
Abstract
This work investigates the variation in a word?s dis-
tributionally nearest neighbours with respect to the
similarity measure used. We identify one type of
variation as being the relative frequency of the neigh-
bour words with respect to the frequency of the tar-
get word. We then demonstrate a three-way connec-
tion between relative frequency of similar words, a
concept of distributional gnerality and the seman-
tic relation of hyponymy. Finally, we consider the
impact that this has on one application of distribu-
tional similarity methods (judging the composition-
ality of collocations).
1 Introduction
Over recent years, many Natural Language Pro-
cessing (NLP) techniques have been developed
that might benefit from knowledge of distribu-
tionally similar words, i.e., words that occur in
similar contexts. For example, the sparse data
problem can make it difficult to construct lan-
guage models which predict combinations of lex-
ical events. Similarity-based smoothing (Brown
et al, 1992; Dagan et al, 1999) is an intuitively
appealing approach to this problem where prob-
abilities of unseen co-occurrences are estimated
from probabilities of seen co-occurrences of dis-
tributionally similar events.
Other potential applications apply the hy-
pothesised relationship (Harris, 1968) between
distributional similarity and semantic similar-
ity; i.e., similarity in the meaning of words can
be predicted from their distributional similarity.
One advantage of automatically generated the-
sauruses (Grefenstette, 1994; Lin, 1998; Curran
and Moens, 2002) over large-scale manually cre-
ated thesauruses such as WordNet (Fellbaum,
1998) is that they might be tailored to a partic-
ular genre or domain.
However, due to the lack of a tight defini-
tion for the concept of distributional similarity
and the broad range of potential applications, a
large number of measures of distributional sim-
ilarity have been proposed or adopted (see Sec-
tion 2). Previous work on the evaluation of dis-
tributional similarity methods tends to either
compare sets of distributionally similar words
to a manually created semantic resource (Lin,
1998; Curran and Moens, 2002) or be oriented
towards a particular task such as language mod-
elling (Dagan et al, 1999; Lee, 1999). The first
approach is not ideal since it assumes that the
goal of distributional similarity methods is to
predict semantic similarity and that the seman-
tic resource used is a valid gold standard. Fur-
ther, the second approach is clearly advanta-
geous when one wishes to apply distributional
similarity methods in a particular application
area. However, it is not at all obvious that one
universally best measure exists for all applica-
tions (Weeds and Weir, 2003). Thus, applying a
distributional similarity technique to a new ap-
plication necessitates evaluating a large number
of distributional similarity measures in addition
to evaluating the new model or algorithm.
We propose a shift in focus from attempting
to discover the overall best distributional sim-
ilarity measure to analysing the statistical and
linguistic properties of sets of distributionally
similar words returned by different measures.
This will make it possible to predict in advance
of any experimental evaluation which distribu-
tional similarity measures might be most appro-
priate for a particular application.
Further, we explore a problem faced by
the automatic thesaurus generation community,
which is that distributional similarity methods
do not seem to offer any obvious way to dis-
tinguish between the semantic relations of syn-
onymy, antonymy and hyponymy. Previous
work on this problem (Caraballo, 1999; Lin et
al., 2003) involves identifying specific phrasal
patterns within text e.g., ?Xs and other Ys? is
used as evidence that X is a hyponym of Y. Our
work explores the connection between relative
frequency, distributional generality and seman-
tic generality with promising results.
The rest of this paper is organised as follows.
In Section 2, we present ten distributional simi-
larity measures that have been proposed for use
in NLP. In Section 3, we analyse the variation in
neighbour sets returned by these measures. In
Section 4, we take one fundamental statistical
property (word frequency) and analyse correla-
tion between this and the nearest neighbour sets
generated. In Section 5, we relate relative fre-
quency to a concept of distributional generality
and the semantic relation of hyponymy. In Sec-
tion 6, we consider the effects that this has on a
potential application of distributional similarity
techniques, which is judging compositionality of
collocations.
2 Distributional similarity measures
In this section, we introduce some basic con-
cepts and then discuss the ten distributional
similarity measures used in this study.
The co-occurrence types of a target word are
the contexts, c, in which it occurs and these
have associated frequencies which may be used
to form probability estimates. In our work, the
co-occurrence types are always grammatical de-
pendency relations. For example, in Sections 3
to 5, similarity between nouns is derived from
their co-occurrences with verbs in the direct-
object position. In Section 6, similarity between
verbs is derived from their subjects and objects.
The k nearest neighbours of a target word w
are the k words for which similarity with w is
greatest. Our use of the term similarity measure
encompasses measures which should strictly be
referred to as distance, divergence or dissimilar-
ity measures. An increase in distance correlates
with a decrease in similarity. However, either
type of measure can be used to find the k near-
est neighbours of a target word.
Table 1 lists ten distributional similarity mea-
sures. The cosine measure (Salton and McGill,
1983) returns the cosine of the angle between
two vectors.
The Jensen-Shannon (JS) divergence measure
(Rao, 1983) and the ?-skew divergence measure
(Lee, 1999) are based on the Kullback-Leibler
(KL) divergence measure. The KL divergence,
or relative entropy, D(p||q), between two prob-
ability distribution functions p and q is defined
(Cover and Thomas, 1991) as the ?inefficiency
of assuming that the distribution is q when the
true distribution is p?: D(p||q) =
?
c p log
p
q .
However, D(p||q) = ? if there are any con-
texts c for which p(c) > 0 and q(c) = 0. Thus,
this measure cannot be used directly on maxi-
mum likelihood estimate (MLE) probabilities.
One possible solution is to use the JS diver-
gence measure, which measures the cost of using
the average distribution in place of each individ-
ual distribution. Another is the ?-skew diver-
gence measure, which uses the p distribution to
smooth the q distribution. The value of the pa-
rameter ? controls the extent to which the KL
divergence is approximated. We use ? = 0.99
since this provides a close approximation to the
KL divergence and has been shown to provide
good results in previous research (Lee, 2001).
The confusion probability (Sugawara et al,
1985) is an estimate of the probability that one
word can be substituted for another. Words
w1 and w2 are completely confusable if we are
equally as likely to see w2 in a given context as
we are to see w1 in that context.
Jaccard?s coefficient (Salton and McGill,
1983) calculates the proportion of features be-
longing to either word that are shared by both
words. In the simplest case, the features of a
word are defined as the contexts in which it has
been seen to occur. simja+mi is a variant (Lin,
1998) in which the features of a word are those
contexts for which the pointwise mutual infor-
mation (MI) between the word and the context
is positive, where MI can be calculated using
I(c, w) = log P (c|w)P (c) . The related Dice Coeffi-
cient (Frakes and Baeza-Yates, 1992) is omitted
here since it has been shown (van Rijsbergen,
1979) that Dice and Jaccard?s Coefficients are
monotonic in each other.
Lin?s Measure (Lin, 1998) is based on his
information-theoretic similarity theorem, which
states, ?the similarity between A and B is mea-
sured by the ratio between the amount of in-
formation needed to state the commonality of
A and B and the information needed to fully
describe what A and B are.?
The final three measures are settings in
the additive MI-based Co-occurrence Retrieval
Model (AMCRM) (Weeds and Weir, 2003;
Weeds, 2003). We can measure the precision
and the recall of a potential neighbour?s re-
trieval of the co-occurrences of the target word,
where the sets of required and retrieved co-
occurrences (F (w1) and F (w2) respectively) are
those co-occurrences for which MI is positive.
Neighbours with both high precision and high
recall retrieval can be obtained by computing
Measure Function
cosine simcm(w2, w1) =
?
c
P (c|w1).P (c|w2)
??
c
P (c|w1)2
?
c
P (c|w2)2
Jens.-Shan. distjs(w2, w1) = 12
(
D
(
p||p+q2
)
+D
(
q||p+q2
))
where p = P (c|w1) and q = P (c|w2)
?-skew dist?(w2, w1) = D (p||(?.q + (1? ?).p)) where p = P (c|w1) and q = P (c|w2)
conf. prob. simcp(w2|w1) =
?
c
P (w1|c).P (w2|c).P (c)
P (w1)
Jaccard?s simja(w2, w1) =
|F (w1)?F (w2)|
|F (w1)?F (w2)|
where F (w) = {c : P (c|v) > 0}
Jacc.+MI simja+mi(w2,W1) =
|F (w1)?F (w2)|
|F (w1)?F (w2)|
where F (w) = {c : I(c, w) > 0}
Lin?s simlin(w2, w1) =
?
F (w1)?F (w2)
(I(c,w1)+I(c,w2))
?
F (w1)
I(c,w1)+
?
F (w2)
I(c,w2)
where F (w) = {c : I(c, w) > 0}
precision simP(w2, w1) =
?
F (w1)?F (w2)
I(c,w2)
?
F (w2)
I(c,w2)
where F (w) = {c : I(c, w) > 0}
recall simR(w2, w1) =
?
F (w1)?F (w2)
I(c,w1)
?
F (w1)
I(c,w1)
where F (w) = {c : I(c, w) > 0}
harm. mean simhm(w2, w1) =
2.simP (w2,w1).simR(w2,w1)
simP (w2,w1)+simR(w2,w1)
where F (w) = {c : I(c, w) > 0}
Table 1: Ten distributional similarity measures
their harmonic mean (or F-score).
3 Overlap of neighbour sets
We have described a number of ways of calcu-
lating distributional similarity. We now con-
sider whether there is substantial variation in
a word?s distributionally nearest neighbours ac-
cording to the chosen measure. We do this by
calculating the overlap between neighbour sets
for 2000 nouns generated using different mea-
sures from direct-object data extracted from the
British National Corpus (BNC).
3.1 Experimental set-up
The data from which sets of nearest neighbours
are derived is direct-object data for 2000 nouns
extracted from the BNC using a robust accurate
statistical parser (RASP) (Briscoe and Carroll,
2002). For reasons of computational efficiency,
we limit ourselves to 2000 nouns and direct-
object relation data. Given the goal of compar-
ing neighbour sets generated by different mea-
sures, we would not expect these restrictions to
affect our findings. The complete set of 2000
nouns (WScomp) is the union of two sets WShigh
and WSlow for which nouns were selected on the
basis of frequency: WShigh contains the 1000
most frequently occurring nouns (frequency >
500), and WSlow contains the nouns ranked
3001-4000 (frequency ? 100). By excluding
mid-frequency nouns, we obtain a clear sepa-
ration between high and low frequency nouns.
The complete data-set consists of 1,596,798 co-
occurrence tokens distributed over 331,079 co-
occurrence types. From this data, we computed
the similarity between every pair of nouns ac-
cording to each distributional similarity mea-
sure. We then generated ranked sets of nearest
neighbours (of size k = 200 and where a word
is excluded from being a neighbour of itself) for
each word and each measure.
For a given word, we compute the overlap be-
tween neighbour sets using a comparison tech-
nique adapted from Lin (1998). Given a word
w, each word w? in WScomp is assigned a rank
score of k ? rank if it is one of the k near-
est neighbours of w using measure m and zero
otherwise. If NS(w,m) is the vector of such
scores for word w and measure m, then the
overlap, C(NS(w,m1),NS(w,m2)), of two neigh-
bour sets is the cosine between the two vectors:
C(NS(w,m1),NS(w,m2)) =
?
w? rm1(w
?, w)? rm2(w
?, w)
?k
i=1 i2
The overlap score indicates the extent to which
sets share members and the extent to which
they are in the same order. To achieve an over-
lap score of 1, the sets must contain exactly
the same items in exactly the same order. An
overlap score of 0 is obtained if the sets do not
contain any common items. If two sets share
roughly half their items and these shared items
are dispersed throughout the sets in a roughly
similar order, we would expect the overlap be-
tween sets to be around 0.5.
cm js ? cp ja ja+mi lin
cm 1.0(0.0) 0.69(0.12) 0.53(0.15) 0.33(0.09) 0.26(0.12) 0.28(0.15) 0.32(0.15)
js 0.69(0.12) 1.0(0.0) 0.81(0.10) 0.46(0.31) 0.48(0.18) 0.49(0.20) 0.55(0.16)
? 0.53(0.15) 0.81(0.10) 1.0(0.0) 0.61(0.08) 0.4(0.27) 0.39(0.25) 0.48(0.19)
cp 0.33(0.09) 0.46(0.31) 0.61(0.08) 1.0(0.0) 0.24(0.24) 0.20(0.18) 0.29(0.15)
ja 0.26(0.12) 0.48(0.18) 0.4(0.27) 0.24(0.24) 1.0(0.0) 0.81(0.08) 0.69(0.09)
ja+mi 0.28(0.15) 0.49(0.20) 0.39(0.25) 0.20(0.18) 0.81(0.08) 1.0(0.0) 0.81(0.10)
lin 0.32(0.15) 0.55(0.16) 0.48(0.19) 0.29(0.15) 0.69(0.09) 0.81(0.10) 1.0(0.0)
Table 2: Cross-comparison of first seven similarity measures in terms of mean overlap of neighbour
sets and corresponding standard deviations.
P R hm
cm 0.18(0.10) 0.31(0.13) 0.30(0.14)
js 0.19(0.12) 0.55(0.18) 0.51(0.18)
? 0.08(0.08) 0.74(0.14) 0.41(0.23)
cp 0.03(0.04) 0.57(0.10) 0.25(0.18)
ja 0.36(0.30) 0.38(0.30) 0.74(0.14)
ja+mi 0.42(0.30) 0.40(0.31) 0.86(0.07)
lin 0.46(0.25) 0.52(0.22) 0.95(0.039)
Table 3: Mean overlap scores for seven simi-
larity measures with precision, recall and the
harmonic mean in the AMCRM.
3.2 Results
Table 2 shows the mean overlap score between
every pair of the first seven measures in Table 1
calculated over WScomp. Table 3 shows the mean
overlap score between each of these measures
and precision, recall and the harmonic mean in
the AMCRM. In both tables, standard devia-
tions are given in brackets and boldface denotes
the highest levels of overlap for each measure.
For compactness, each measure is denoted by
its subscript from Table 1.
Although overlap between most pairs of
measures is greater than expected if sets of
200 neighbours were generated randomly from
WScomp (in this case, average overlap would be
0.08 and only the overlap between the pairs
(?,P) and (cp,P) is not significantly greater
than this at the 1% level), there are substan-
tial differences between the neighbour sets gen-
erated by different measures. For example, for
many pairs, neighbour sets do not appear to
have even half their members in common.
4 Frequency analysis
We have seen that there is a large variation in
neighbours selected by different similarity mea-
sures. In this section, we analyse how neighbour
sets vary with respect to one fundamental statis-
tical property ? word frequency. To do this, we
measure the bias in neighbour sets towards high
frequency nouns and consider how this varies
depending on whether the target noun is itself
a high frequency noun or low frequency noun.
4.1 Measuring bias
If a measure is biased towards selecting high fre-
quency words as neighbours, then we would ex-
pect that neighbour sets for this measure would
be made up mainly of words from WShigh. Fur-
ther, the more biased the measure is, the more
highly ranked these high frequency words will
tend to be. In other words, there will be high
overlap between neighbour sets generated con-
sidering all 2000 nouns as potential neighbours
and neighbour sets generated considering just
the nouns in WShigh as potential neighbours. In
the extreme case, where all of a noun?s k nearest
neighbours are high frequency nouns, the over-
lap with the high frequency noun neighbour set
will be 1 and the overlap with the low frequency
noun neighbour set will be 0. The inverse is, of
course, true if a measure is biased towards se-
lecting low frequency words as neighbours.
If NSwordset is the vector of neighbours (and
associated rank scores) for a given word, w, and
similarity measure, m, and generated consider-
ing just the words in wordset as potential neigh-
bours, then the overlap between two neighbour
sets can be computed using a cosine (as be-
fore). If Chigh = C(NScomp,NShigh) and Clow =
C(NScomp,NSlow), then we compute the bias to-
wards high frequency neighbours for word w us-
ing measure m as: biashighm(w) =
Chigh
Chigh+Clow
The value of this normalised score lies in the
range [0,1] where 1 indicates a neighbour set
completely made up of high frequency words, 0
indicates a neighbour set completely made up of
low frequency words and 0.5 indicates a neigh-
bour set with no biases towards high or low fre-
quency words. This score is more informative
than simply calculating the proportion of high
high freq. low freq.
target nouns target nouns
cm 0.90 0.87
js 0.94 0.70
? 0.98 0.90
cp 1.00 0.99
ja 0.99 0.21
ja+mi 0.95 0.14
lin 0.85 0.38
P 0.12 0.04
R 0.99 0.98
hm 0.92 0.28
Table 4: Mean value of biashigh according to
measure and frequency of target noun.
and low frequency words in each neighbour set
because it weights the importance of neighbours
by their rank in the set. Thus, a large number
of high frequency words in the positions clos-
est to the target word is considered more biased
than a large number of high frequency words
distributed throughout the neighbour set.
4.2 Results
Table 4 shows the mean value of the biashigh
score for every measure calculated over the set
of high frequency nouns and over the set of low
frequency nouns. The standard deviations (not
shown) all lie in the range [0,0.2]. Any deviation
from 0.5 of greater than 0.0234 is significant at
the 1% level.
For all measures and both sets of target
nouns, there appear to be strong tendencies to
select neighbours of particular frequencies. Fur-
ther, there appears to be three classes of mea-
sures: those that select high frequency nouns
as neighbours regardless of the frequency of the
target noun (cm, js, ?, cp andR); those that se-
lect low frequency nouns as neighbours regard-
less of the frequency of the target noun (P); and
those that select nouns of a similar frequency to
the target noun (ja, ja+mi, lin and hm).
This can also be considered in terms of distri-
butional generality. By definition, recall prefers
words that have occurred in more of the con-
texts that the target noun has, regardless of
whether it occurs in other contexts as well i.e.,
it prefers distributionally more general words.
The probability of this being the case increases
as the frequency of the potential neighbour in-
creases and so, recall tends to select high fre-
quency words. In contrast, precision prefers
words that have occurred in very few contexts
that the target word has not i.e., it prefers dis-
tributionally more specific words. The prob-
ability of this being the case increases as the
frequency of the potential neighbour decreases
and so, precision tends to select low frequency
words. The harmonic mean of precision and re-
call prefers words that have both high precision
and high recall. The probability of this being
the case is highest when the words are of sim-
ilar frequency and so, the harmonic mean will
tend to select words of a similar frequency.
5 Relative frequency and hyponymy
In this section, we consider the observed fre-
quency effects from a semantic perspective.
The concept of distributional generality in-
troduced in the previous section has parallels
with the linguistic relation of hyponymy, where
a hypernym is a semantically more general term
and a hyponym is a semantically more specific
term. For example, animal is an (indirect1) hy-
pernym of dog and conversely dog is an (indi-
rect) hyponym of animal. Although one can
obviously think of counter-examples, we would
generally expect that the more specific term dog
can only be used in contexts where animal can
be used and that the more general term animal
might be used in all of the contexts where dog
is used and possibly others. Thus, we might ex-
pect that distributional generality is correlated
with semantic generality ? a word has high
recall/low precision retrieval of its hyponyms?
co-occurrences and high precision/low recall re-
trieval of its hypernyms? co-occurrences.
Thus, if n1 and n2 are related and P(n2, n1) >
R(n2, n1), we might expect that n2 is a hy-
ponym of n1 and vice versa. However, having
discussed a connection between frequency and
distributional generality, we might also expect
to find that the frequency of the hypernymic
term is greater than that of the hyponymic
term. In order to test these hypotheses, we ex-
tracted all of the possible hyponym-hypernym
pairs (20, 415 pairs in total) from our list of 2000
nouns (using WordNet 1.6). We then calculated
the proportion for which the direction of the hy-
ponymy relation could be accurately predicted
by the relative values of precision and recall and
the proportion for which the direction of the hy-
ponymy relation could be accurately predicted
by relative frequency. We found that the direc-
tion of the hyponymy relation is correlated in
the predicted direction with the precision-recall
1There may be other concepts in the hypernym chain
between dog and animal e.g. carnivore and mammal.
values in 71% of cases and correlated in the pre-
dicted direction with relative frequency in 70%
of cases. This supports the idea of a three-way
linking between distributional generality, rela-
tive frequency and semantic generality. We now
consider the impact that this has on a potential
application of distributional similarity methods.
6 Compositionality of collocations
In its most general sense, a collocation is a ha-
bitual or lexicalised word combination. How-
ever, some collocations such as strong tea are
compositional, i.e., their meaning can be de-
termined from their constituents, whereas oth-
ers such as hot dog are not. Both types are
important in language generation since a sys-
tem must choose between alternatives but only
non-compositional ones are of interest in lan-
guage understanding since only these colloca-
tions need to be listed in the dictionary.
Baldwin et al (2003) explore empirical
models of compositionality for noun-noun com-
pounds and verb-particle constructions. Based
on the observation (Haspelmath, 2002) that
compositional collocations tend to be hyponyms
of their head constituent, they propose a model
which considers the semantic similarity between
a collocation and its constituent words.
McCarthy et al (2003) also investigate sev-
eral tests for compositionality including one
(simplexscore) based on the observation that
compositional collocations tend to be similar in
meaning to their constituent parts. They ex-
tract co-occurrence data for 111 phrasal verbs
(e.g. rip off ) and their simplex constituents
(e.g. rip) from the BNC using RASP and cal-
culate the value of simlin between each phrasal
verb and its simplex constituent. The test
simplexscore is used to rank the phrasal verbs
according to their similarity with their simplex
constituent. This ranking is correlated with hu-
man judgements of the compositionality of the
phrasal verbs using Spearman?s rank correlation
coefficient. The value obtained (0.0525) is dis-
appointing since it is not statistically significant
(the probability of this value under the null hy-
pothesis of ?no correlation? is 0.3).2
However, Haspelmath (2002) notes that a
compositional collocation is not just similar to
one of its constituents ? it can be considered to
be a hyponym of its head constituent. For ex-
ample, ?strong tea? is a type of ?tea? and ?to
2Other tests for compositionality investigated by Mc-
Carthy et al (2003) do much better.
Measure rs P (rs) under H0
simlin 0.0525 0.2946
precision -0.160 0.0475
recall 0.219 0.0110
harmonic mean 0.011 0.4562
Table 5: Correlation with compositionality for
different similarity measures
rip up? is a way of ?ripping?.
Thus, we hypothesised that a distributional
measure which tends to select more general
terms as neighbours of the phrasal verb (e.g. re-
call) would do better than measures that tend
to select more specific terms (e.g. precision) or
measures that tend to select terms of a similar
specificity (e.g simlin or the harmonic mean of
precision and recall).
Table 5 shows the results of using different
similarity measures with the simplexscore test
and data of McCarthy et al (2003). We now see
significant correlation between compositionality
judgements and distributional similarity of the
phrasal verb and its head constituent. The cor-
relation using the recall measure is significant
at the 5% level; thus we can conclude that if
the simplex verb has high recall retrieval of the
phrasal verb?s co-occurrences, then the phrasal
is likely to be compositional. The correlation
score using the precision measure is negative
since we would not expect the simplex verb to
be a hyponym of the phrasal verb and thus, if
the simplex verb does have high precision re-
trieval of the phrasal verb?s co-occurrences, it is
less likely to be compositional.
Finally, we obtained a very similar result
(0.217) by ranking phrasals according to their
inverse relative frequency with their simplex
constituent (i.e., freq(simplex)freq(phrasal) ). Thus, it would
seem that the three-way connection between
distributional generality, hyponymy and rela-
tive frequency exists for verbs as well as nouns.
7 Conclusions and further work
We have presented an analysis of a set of dis-
tributional similarity measures. We have seen
that there is a large amount of variation in the
neighbours selected by different measures and
therefore the choice of measure in a given appli-
cation is likely to be important.
We also identified one of the major axes of
variation in neighbour sets as being the fre-
quency of the neighbours selected relative to the
frequency of the target word. There are three
major classes of distributional similarity mea-
sures which can be characterised as 1) higher
frequency selecting or high recall measures; 2)
lower frequency selecting or high precision mea-
sures; and 3) similar frequency selecting or high
precision and recall measures.
A word tends to have high recall similarity
with its hyponyms and high precision similarity
with its hypernyms. Further, in the majority of
cases, it tends to be more frequent than its hy-
ponyms and less frequent than its hypernyms.
Thus, there would seem to a three way corre-
lation between word frequency, distributional
generality and semantic generality.
We have considered the impact of these ob-
servations on a technique which uses a distribu-
tional similarity measure to determine composi-
tionality of collocations. We saw that in this ap-
plication we achieve significantly better results
using a measure that tends to select higher fre-
quency words as neighbours rather than a mea-
sure that tends to select neighbours of a similar
frequency to the target word.
There are a variety of ways in which this work
might be extended. First, we could use the ob-
servations about distributional generality and
relative frequency to aid the process of organ-
ising distributionally similar words into hierar-
chies. Second, we could consider the impact of
frequency characteristics in other applications.
Third, for the general application of distribu-
tional similarity measures, it would be useful
to find other characteristics by which distribu-
tional similarity measures might be classified.
Acknowledgements
This work was funded by a UK EPSRC stu-
dentship to the first author, UK EPSRC project
GR/S26408/01 (NatHab) and UK EPSRC
project GR/N36494/01 (RASP). We would like
to thank Adam Kilgarriff and Bill Keller for use-
ful discussions.
References
Timothy Baldwin, Colin Bannard, Takaaki Tanaka,
and Dominic Widdows. 2003. An empirical model
of multiword expression decomposability. In Pro-
ceedings of the ACL-2003 Workshop on Multiword
Expressions, pages 89?96, Sapporo, Japan.
Edward Briscoe and John Carroll. 2002. Robust ac-
curate statistical annotation of general text. In
Proceedings of LREC-2002, pages 1499?1504.
P.F. Brown, V.J. DellaPietra, P.V deSouza, J.C. Lai,
and R.L. Mercer. 1992. Class-based n-gram mod-
els of natural language. Computational Linguis-
tics, 18(4):467?479.
Sharon Caraballo. 1999. Automatic construction of
a hypernym-labelled noun hierarchy from text. In
Proceedings of ACL-99, pages 120?126.
T.M. Cover and J.A. Thomas. 1991. Elements of
Information Theory. Wiley, New York.
James R. Curran and Marc Moens. 2002. Im-
provements in automatic thesaurus extraction. In
ACL-SIGLEX Workshop on Unsupervised Lexical
Acquisition, Philadelphia.
Ido Dagan, Lillian Lee, and Fernando Pereira. 1999.
Similarity-based models of word cooccurrence
probabilities. Machine Learning Journal, 34.
Christiane Fellbaum, editor. 1998. WordNet: An
Electronic Lexical Database. MIT Press.
W.B. Frakes and R. Baeza-Yates, editors. 1992. In-
formation Retrieval, Data Structures and Algo-
rithms. Prentice Hall.
Gregory Grefenstette. 1994. Corpus-derived first-,
second- and third-order word affinities. In Pro-
ceedings of Euralex, pages 279?290, Amsterdam.
Zelig S. Harris. 1968. Mathematical Structures of
Language. Wiley, New York.
Martin Haspelmath. 2002. Understanding Morphol-
ogy. Arnold Publishers.
Lillian Lee. 1999. Measures of distributional simi-
larity. In Proceedings of ACL-1999, pages 23?32.
Lillian Lee. 2001. On the effectiveness of the skew
divergence for statistical language analysis. Arti-
ficial Intelligence and Statistics, pages 65?72.
Dekang Lin, Shaojun Zhao, Lijuan Qin, and Ming
Zhou. 2003. Identifying synonyms among dis-
tributionally similar words. In Proceedings of
IJCAI-03, pages 1492?1493.
Dekang Lin. 1998. Automatic retrieval and cluster-
ing of similar words. In Proceedings of COLING-
ACL ?98, pages 768?774, Montreal.
Diana McCarthy, Bill Keller, and John Carroll.
2003. Detecting a continuum of compositionality
in phrasal verbs. In Proceedings of the ACL-2003
Workshop on Multiword Expressions, pages 73?
80, Sapporo, Japan.
C. Radhakrishna Rao. 1983. Diversity: Its measure-
ment, decomposition, apportionment and analy-
sis. Sankyha: The Indian Journal of Statistics,
44(A):1?22.
G. Salton and M.J. McGill. 1983. Introduction to
Modern Information Retrieval. McGraw-Hill.
K.M. Sugawara, K. Nishimura, K. Toshioka,
M. Okachi, and T. Kaneko. 1985. Isolated word
recognition using hidden markov models. In Pro-
ceedings of the ICASSP-1985, pages 1?4.
C.J. van Rijsbergen. 1979. Information Retrieval.
Butterworths, second edition.
Julie Weeds and David Weir. 2003. A general frame-
work for distributional similarity. In Proceedings
of EMNLP-2003, pages 81?88, Sapporo, Japan.
Julie Weeds. 2003. Measures and Applications of
Lexical Distributional Similarity. Ph.D. thesis,
Department of Informatics, University of Sussex.
Automatic Identification of Infrequent Word Senses
Diana McCarthy & Rob Koeling & Julie Weeds & John Carroll
Department of Informatics,
University of Sussex
Brighton BN1 9QH, UK
 
dianam,robk,juliewe,johnca  @sussex.ac.uk
Abstract
In this paper we show that an unsupervised method for
ranking word senses automatically can be used to iden-
tify infrequently occurring senses. We demonstrate this
using a ranking of noun senses derived from the BNC
and evaluating on the sense-tagged text available in both
SemCor and the SENSEVAL-2 English all-words task.
We show that the method does well at identifying senses
that do not occur in a corpus, and that those that are erro-
neously filtered but do occur typically have a lower fre-
quency than the other senses. This method should be
useful for word sense disambiguation systems, allowing
effort to be concentrated on more frequent senses; it may
also be useful for other tasks such as lexical acquisition.
Whilst the results on balanced corpora are promising, our
chief motivation for the method is for application to do-
main specific text. For text within a particular domain
many senses from a generic inventory will be rare, and
possibly redundant. Since a large domain specific cor-
pus of sense annotated data is not available, we evaluate
our method on domain-specific corpora and demonstrate
that sense types identified for removal are predominantly
senses from outside the domain.
1 Introduction
Much about the behaviour of words is most appro-
priately expressed in terms of word senses rather
than word forms. However, an NLP application
computing over word senses is faced with consid-
erable extra ambiguity. There are systems which
can perform word sense disambiguation (WSD) on
the words in input text, however there is room for
improvement since the best systems on the English
SENSEVAL-2 all-words task obtained at most 69%
for precision and recall. Whilst there are systems
that obtain higher precision (Magnini et al, 2001),
these typically suffer from a low recall. WSD per-
formance is affected by the degree of polysemy, but
even more so by the entropy of the frequency distri-
butions of the words? senses (Kilgarriff and Rosen-
zweig, 2000) since the distribution for many words
is highly skewed. Many of the senses in such an
inventory are rare and WSD and lexical acquisition
systems do best when they take this into account.
There are many ways that the skewed distribution
can be taken into account. One successful approach
is to back-off to the first (predominant) sense (Wilks
and Stevenson, 1998; Hoste et al, 2001). Another
possibility would be concentrate the selection pro-
cess to senses with higher frequency, and filter out
rare senses. This is implicitly done by systems
which rely on hand-tagged training corpora, since
rare senses often do not occur in the available data.
In this paper we use an unsupervised method to rank
word senses from an inventory according to preva-
lence (McCarthy et al, 2004a), and utilise the rank-
ing scores to identify senses which are rare. We use
WordNet for our inventory, since it is widely used
and freely available, but our method could in prin-
ciple be used with another MRD (we comment on
this in the conclusions). We report work with nouns
here, and leave evaluation on other PoS for the fu-
ture.
Our approach exploits automatically acquired
thesauruses which provide ?nearest neighbours? for
a given word entry. The neighbours are ordered
in terms of the distributional similarity that they
share with the target word. The neighbours relate
to different senses of the target word, so for exam-
ple the word competition in such a thesaurus pro-
vided by Lin 1 has neighbours tournament, event,
championship and then further down the ordered list
we see neighbours pertaining to a different sense
competitor,...market...price war. Pantel and Lin
(2002) demonstrate that it is possible to cluster the
neighbours into senses and relate these to WordNet
senses. In contrast, we use the distributional sim-
ilarity scores of the neighbours to rank the various
senses of the target word since we expect that the
quantity and similarity of the neighbours pertain-
ing to different senses will reflect the relative dom-
inance of the senses. This is because there will
1Available from
http://www.cs.ualberta.ca/?lindek/demos/depsim.htm
be more data for the more prevalent senses com-
pared to the less frequent senses. We use a measure
of semantic similarity from the WordNet Similarity
package to relate the senses of the target word to the
neighbours in the thesaurus.
The paper is structured as follows. The ranking
method is described elsewhere (McCarthy et al,
2004a), but we summarise in the following section
and describe how ranking scores can be used for fil-
tering word senses. Section 3 describes two exper-
iments using the BNC for acquisition of the sense
rankings with evaluation using the hand-tagged data
in i) SemCor and ii) the English SENSEVAL-2 all-
words task. We demonstrate that the majority of
senses identified by the method do not occur in these
gold-standards, and that for those that do, only a
small percentage of the sense tokens would be re-
moved in error by filtering these senses. In section 4
we use domain labels produced by (Magnini and
Cavaglia`, 2000) to demonstrate differences in the
senses filtered for a sample of words in two domain
specific corpora. We describe some related work in
section 5 and conclude in section 6.
2 Method
McCarthy et al (2004a) describe a method to pro-
duce a ranking over senses and find the predominant
sense of a word just using raw text. We summarise
the method below, and describe how we use it for
identifying candidate senses for filtering.
2.1 Ranking the Senses
In order to rank the senses of a target word (e.g.
plant) we use a thesaurus acquired from automati-
cally parsed text (section 2.2 below). This provides
the  nearest neighbours to each target word (e.g.
factory, refinery, tree etc...) along with the distribu-
tional similarity score between the target word and
its neighbour. We then use the WordNet similar-
ity package (Patwardhan and Pedersen, 2003) (see
section 2.3) to give us a semantic similarity mea-
sure (hereafter referred to as the WordNet similarity
measure) to weight the contribution that each neigh-
bour (e.g. factory) makes to the various senses of
the target word (e.g. flora, industrial, actor etc...).
We take each sense of the target word (  ) in turn
and obtain a score reflecting the prevalence which is
used for ranking. Let 
	Co-occurrence Retrieval:
A Flexible Framework for Lexical
Distributional Similarity
Julie Weeds and David Weir?
University of Sussex
Techniques that exploit knowledge of distributional similarity between words have been proposed
in many areas of Natural Language Processing. For example, in language modeling, the sparse
data problem can be alleviated by estimating the probabilities of unseen co-occurrences of events
from the probabilities of seen co-occurrences of similar events. In other applications, distribu-
tional similarity is taken to be an approximation to semantic similarity. However, due to the wide
range of potential applications and the lack of a strict definition of the concept of distributional
similarity, many methods of calculating distributional similarity have been proposed or adopted.
In this work, a flexible, parameterized framework for calculating distributional similarity is
proposed. Within this framework, the problem of finding distributionally similar words is cast
as one of co-occurrence retrieval (CR) for which precision and recall can be measured by analogy
with the way they are measured in document retrieval. As will be shown, a number of popular
existing measures of distributional similarity are simulated with parameter settings within the
CR framework. In this article, the CR framework is then used to systematically investigate three
fundamental questions concerning distributional similarity. First, is the relationship of lexical
similarity necessarily symmetric, or are there advantages to be gained from considering it as an
asymmetric relationship? Second, are some co-occurrences inherently more salient than others
in the calculation of distributional similarity? Third, is it necessary to consider the difference in
the extent to which each word occurs in each co-occurrence type?
Two application-based tasks are used for evaluation: automatic thesaurus generation and
pseudo-disambiguation. It is possible to achieve significantly better results on both these tasks by
varying the parameters within the CR framework rather than using other existing distributional
similarity measures; it will also be shown that any single unparameterized measure is unlikely to
be able to do better on both tasks. This is due to an inherent asymmetry in lexical substitutability
and therefore also in lexical distributional similarity.
1. Introduction
Over recent years, approaches to a broad range of natural language processing (NLP)
applications have been proposed that require knowledge about the similarity of words.
The application areas in which these approaches have been proposed range from speech
recognition and parse selection to information retrieval (IR) and natural language
? Department of Informatics, University of Sussex, Falmer, Brighton, BN1 9QH, UK.
Submission received: 4 May 2004; revised submission received: 16 November 2004; accepted for
publication: 16 April 2005.
? 2006 Association for Computational Linguistics
Computational Linguistics Volume 31, Number 4
generation. For example, language models that incorporate substantial lexical knowl-
edge play a key role in many statistical NLP techniques (e.g., in speech recognition
and probabilistic parse selection). However, they are difficult to acquire, since many
plausible combinations of events are not seen in corpus data. Brown et al (1992) report
that one can expect 14.7% of the word triples in any new English text to be unseen in a
training corpus of 366 million English words. In our own experiments with grammatical
relation data extracted by a Robust Accurate Statistical Parser (RASP) (Briscoe and
Carroll 1995; Carroll and Briscoe 1996) from the British National Corpus (BNC), we
found that 14% of noun-verb direct-object co-occurrence tokens and 49% of noun-verb
direct-object co-occurrence types in one half of the data set were not seen in the other
half. A statistical technique using a language model that assigns a zero probability
to these previously unseen events will rule the correct parse or interpretation of the
utterance impossible.
Similarity-based smoothing (Hindle 1990; Brown et al 1992; Dagan, Marcus, and
Markovitch 1993; Pereira, Tishby, and Lee 1993; Dagan, Lee, and Pereira 1999) provides
an intuitively appealing approach to language modeling. In order to estimate the prob-
ability of an unseen co-occurrence of events, estimates based on seen occurrences of
similar events can be combined. For example, in a speech recognition task, we might
predict that cat is a more likely subject of growl than the word cap, even though neither
co-occurrence has been seen before, based on the fact that cat is ?similar? to words that
do occur as the subject of growl (e.g., dog and tiger), whereas cap is not.
However, what is meant when we say that cat is ?similar? to dog? Are we referring to
their semantic similarity, e.g., the components of meaning they share by virtue of both
being carnivorous four-legged mammals? Or are we referring to their distributional
similarity, e.g., in keeping with the Firthian tradition,1 the fact that these words tend to
occur as the arguments of the same verbs (e.g., eat, feed, sleep) and tend to be modified
by the same adjectives (e.g., hungry and playful).
In some applications, the knowledge required is clearly semantic. In IR, documents
might be usefully retrieved that use synonymous terms or terms subsuming those speci-
fied in a user?s query (Xu and Croft 1996). In natural language generation (including text
simplification), possible words for a concept should be similar in meaning rather than
just in syntactic or distributional behavior. In these application areas, distributional sim-
ilarity can be taken to be an approximation to semantic similarity. The underlying idea is
based largely on the central claim of the distributional hypothesis (Harris 1968), that is:
The meaning of entities, and the meaning of grammatical relations among them, is
related to the restriction of combinations of these entities relative to other entities.
This hypothesized relationship between distributional similarity and semantic sim-
ilarity has given rise to a large body of work on automatic thesaurus generation (Hindle
1990; Grefenstette 1994; Lin 1998a; Curran and Moens 2002; Kilgarriff 2003). There
are inherent problems in evaluating automatic thesaurus extraction techniques, and
much research assumes a gold standard that does not exist (see Kilgarriff [2003] and
Weeds [2003] for more discussion of this). A further problem for distributional similarity
methods for automatic thesaurus generation is that they do not offer any obvious way to
distinguish between linguistic relations such as synonymy, antonymy, and hyponymy
(see Caraballo [1999] and Lin et al [2003] for work on this). Thus, one may question
1 ?You shall know a word by the company it keeps.?(Firth 1957)
440
Weeds and Weir Co-occurrence Retrieval
the benefit of automatically generating a thesaurus if one has access to large-scale
manually constructed thesauri (e.g., WordNet [Fellbaum 1998], Roget?s [Roget 1911], the
Macquarie [Bernard 1990] and Moby2). Automatic techniques give us the opportunity
to model language change over time or across domains and genres. McCarthy et al
(2004) investigate using distributional similarity methods to find predominant word
senses within a corpus, making it possible to tailor an existing resource (WordNet) to
specific domains. For example, in the computing domain, the word worm is more likely
to be used in its ?malicious computer program? sense than in its ?earthworm? sense.
This domain knowledge will be reflected in a thesaurus automatically generated from
a computing-specific corpus, which will show increased similarity between worm and
virus and reduced similarity between worm and caterpillar.
In other application areas, however, the requirement for ?similar? words to be
semantically related as well as distributionally related is less clear. For example, in
prepositional phrase attachment ambiguity resolution, it is necessary to decide whether
the prepositional phrase attaches to the verb or the noun as in the examples (1) and (2).
1. Mary ((visited their cottage) with her brother).
2. Mary (visited (their cottage with a thatched roof)).
Hindle and Rooth (1993) note that the correct decision depends on all four lexical
events (the verb, the object, the preposition, and the prepositional object). However, a
statistical model built on the basis of four lexical events must cope with extremely sparse
data. One approach (Resnik 1993; Li and Abe 1998; Clark and Weir 2000) is to induce
probability distributions over semantic classes rather than lexical items. For example, a
cottage is a type of building and a brother is a type of person, and so the co-occurrence of
any type of building and any type of person might increase the probability that the PP
in example (1) attaches to the verb.
However, it is unclear whether the classes over which probability distributions
are induced need to be semantic or whether they could be purely distributional. If
we know that two words tend to behave the same way with respect to prepositional
phrase attachment, does it matter whether they mean similar things? Other arguments
for using semantic classes over distributional classes can similarly be disputed (Weeds
2003). For example, it is not necessary for a class of objects to have a name or symbolic
label for us to know that the objects are similar and to exploit that information. Distri-
butional classes do conflate word senses, but in a task such as PP-attachment ambiguity
resolution, we are unlikely to be working with sense-tagged examples and therefore it
is for word forms that we will wish to estimate probabilities of different attachments.
Finally, distributional classes may be over-fitted to a specific corpus, but this may be
beneficial to the extent that the over-fitting reflects a specific domain or dialect.
Further, recent empirical evidence suggests that techniques based on distributional
similarity may perform as well on this task as those based on semantic similarity.
Li (2002) shows that using a fairly small corpus (126,084 sentences from the Wall
Street Journal) and a distributional similarity technique, it is possible to outperform a
state-of-the-art, WordNet-based technique in terms of accuracy, although not in terms
of coverage. Pantel and Lin (2000) report performance of 84.3% using an unsuper-
vised approach to prepositional phrase attachment based on distributional similarity
2 The Moby Thesaurus is a product of the Moby Project, which was released into the public domain by
Grady Ward in 1996.
441
Computational Linguistics Volume 31, Number 4
techniques. This significantly outperforms previous unsupervised techniques and is
drawing close to the state-of-the-art supervised techniques (88.2%).
Having discussed why distributional similarity is important, we now turn to how
to formulate it. As we have said, two words are distributionally similar if they appear in
similar contexts. We therefore need to consider what is meant by context. For example,
two words could be considered to appear in the same context if they appear in the
same sentence, the same document, or the same grammatical dependency relation. The
effect of the type of context used is discussed by Kilgarriff and Yallop (2000). They show
that the use of sentence-level and document-level context leads to ?looser? thesauri
more akin to Roget?s, whereas the use of grammatical dependency relation level context
leads to ?tighter? thesauri more akin to WordNet. The use of grammatical dependency
relations as context gives us a tighter thesaurus because it restricts distributionally
similar words to those that are plausibly inter-substitutable (Church et al 1994), giving
us the following definition of distributional similarity:
The distributional similarity of two words is the extent to which they can be
inter-substituted without changing the plausibility3 of the sentence.
This concept of lexical substitutability highlights the relationship between distri-
butional similarity and semantic similarity, since semantic similarity can be thought of
as the degree of synonymy that exists between two words, where synonymy is defined
(Church et al 1994) as follows:
Two words are absolute synonyms if they can be inter-substituted in all possible
contexts without changing the meaning.
In our empirical work, we focus on finding semantic relationships between words
such as synonymy, antonymy and hyponymy that might be found in a tighter thesaurus
such as WordNet. Hence, the proposed framework is based on the concept of substi-
tutability, and we use grammatical dependency relations as context. However, since the
framework is based on features, there is no reason why someone wishing to find topical
relationships between words, as might be found in Roget?s, could not use the framework.
We simply do not repeat the earlier work of Kilgarriff and Yallop (2000).
However, a number of questions still remain, which this work does investigate:
1. Is lexical substitutability and therefore distributional similarity
symmetric? The concept of substitution is inherently asymmetric. It is
possible to measure the appropriateness of substituting word A for word B
without measuring the appropriateness of substituting word B for word A.
Similarity has been defined in terms of inter-substitutability; but we ask
whether there is something in the inherent asymmetry of substitution that
can be exploited by an asymmetric measure of distributional similarity.
2. Are all contexts equally important? For example, some verbs, e.g., have and
get, are selectionally weak in the constraints they place on their arguments
(Resnik 1993). Should such contexts be considered on equal terms with
selectionally strong contexts in the calculation of distributional similarity?
3 We use ?plausible sentence? to refer to a sentence that might be observed in naturally occurring
language data.
442
Weeds and Weir Co-occurrence Retrieval
3. Is it necessary to consider the difference in extent to which each word
appears in each context? Is it enough to know that both words can occur in
each context, or do similar words occur in similar contexts with similar
probabilities?
In order to answer these questions, we take a pragmatic, application-oriented ap-
proach to evaluation that is based on the assumption that we want to know which
words are distributionally similar because particular applications can make use of this
information.
However, high performance in one application area is not necessarily correlated
with high performance in another application area (Weeds and Weir 2003a). Thus, it
is not clear that the same characteristics that make a distributional similarity measure
useful in one application will make it useful in another. For example, with regard to the
question about symmetry, in some applications we may prefer a word A that can be
substituted for word B in all of the contexts in which B occurs. In other applications, we
may prefer a word A that can be substituted for word B in all of the contexts in which A
occurs. For example, asked for a semantically related word to dog, we might say animal,
since animal can generally be used in place of dog, whereas we might be less likely to
say dog for animal, since dog cannot generally be used in place of animal. This preference
in the direction of the relationship between the two words is not necessarily maintained
when one considers language modeling in the face of sparse data. If we want to learn
what other contexts animal can occur in, we might look at the co-occurrences of words
such as dog, since we know that dog can generally be replaced by animal. If we want to
learn what other contexts dog can occur in, we are less likely to look at the co-occurrences
of animal, since we know that animal can occur in contexts in which dog cannot.
Rather than attempt to find a single universally optimal distributional similarity
measure, or propose using a radically different distributional similarity measure in each
possible application, we propose a flexible, parameterized framework for calculating
distributional similarity (Section 2). Within this framework, we cast the problem of
finding distributionally similar words as one of co-occurrence retrieval (CR), for which
we can measure precision and recall by analogy with the way that they are measured
in document retrieval. Different models within this framework allow us to investigate
how frequency information is incorporated into the distributional similarity measure.
Different parameter settings within each model allow us to investigate asymmetry in
similarity. In Section 3 we discuss the data and the neighbor set comparison tech-
nique used throughout our empirical work. In Section 4 we discuss a number of
existing distributional similarity measures and discuss the extent to which these can
be simulated by settings within the CR framework. In Section 5 we evaluate the CR
framework on a semantic task (WordNet prediction) and on a language modeling task
(pseudo-disambiguation).
2. Co-occurrence Retrieval
In this section, we present a flexible framework for distributional similarity. This frame-
work directly defines a similarity function, does not require smoothing of the base
language model, and allows us to systematically explore the questions about similarity
raised in Section 1. In our approach, similarity between words is viewed as a measure
of how appropriate it is to use one word (or its distribution) in place of the other. Like
relative entropy (Cover and Thomas 1991), it is inherently asymmetric, since we can
443
Computational Linguistics Volume 31, Number 4
measure how appropriate it is to use word A instead of word B separately from how
appropriate it is to use word B instead of word A.
The framework presented here is general to the extent that it can be used to compute
similarities for any set of objects where each object has an associated set of features
or co-occurrence types and these co-occurrence types have associated frequencies
that may be used to form probability estimates. Throughout our discussion, the word
for which we are finding neighbors will be referred to as the target word. If we are
computing the similarity between the target word and another word, then the second
word is a potential neighbor of the target word. A target word?s nearest neighbors are
the potential neighbors that have the highest similarity with the target word.
2.1 Basic Concepts
Let us imagine that we have formed descriptions of each word in terms of the other
words with which they co-occur in various specified grammatical relations in some
corpus. For example, the noun cat might have the co-occurrence types ?dobj-of, feed? and
?ncmod-by, hungry?. Now let us imagine that we have lost (or accidentally deleted) the
description for word w2, but before this happened we had noticed that the description
of word w2 was very similar to that of word w1. For example, the noun dog might also
have the co-occurrence types ?dobj-of, feed? and ?ncmod-by, hungry?. Hence, we decide
that we can use the description of word w1 instead of the description of word w2 and
are hopeful that nobody will notice. How well we do will depend on the validity of
substituting w1 for w2, or, in other words, the similarity between w1 and w2.
The task we have set ourselves can be seen as co-occurrence retrieval (CR). By
analogy with information retrieval, where there is a set of documents that we would
like to retrieve and a set of documents that we do retrieve, we have a scenario where
there is a set of co-occurrences that we would like to retrieve, the co-occurrences of w2,
and a set of co-occurrences that we have retrieved, the co-occurrences of w1. Continuing
the analogy, we can measure how well we have done in terms of precision and recall,
where precision tells us how much of what was retrieved was correct and recall tells us
how much of what we wanted to retrieve was retrieved.
Our flexible framework for distributional similarity is based on this notion of co-
occurrence retrieval. As the distribution of word B moves away from being identical
to that of word A, its ?similarity? with A can decrease along one or both of two
dimensions. When B occurs in contexts that word A does not, the result is a loss of
precision, but B may remain a high-recall neighbor. For example, we might expect the
noun animal to be a high-recall neighbor of the noun dog. When B does not occur in
contexts that A does occur in, the result is a loss of recall but B may remain a high-
precision neighbor. For example, we might expect the noun dog to be a high-precision
neighbor of the noun animal. We can explore the merits of symmetry and asymmetry in
a similarity measure by varying the relative importance attached to precision and recall.
This was the first question posed about distributional similarity in Section 1.
The remainder of this section is devoted to defining two types of co-occurrence
retrieval model (CRM). Additive models are based on the Boolean concept of two
objects either sharing or not sharing a particular feature (where objects are words and
features are co-occurrence types). Difference-weighted models incorporate the differ-
ence in extent to which each word has each feature. Exploring the two types of models,
both defined on the same concepts of precision and recall, allows us to investigate the
third question posed in Section 1: Is a shared context worth the same, regardless of the
difference in the extent to which each word appears in that context?
444
Weeds and Weir Co-occurrence Retrieval
We also use the CR framework to investigate the second question posed about
distributional similarity, ?Should all contexts be treated equally?,? by using differ-
ent weight functions within each type of model. Weight functions decide which co-
occurrence types are features of a word and determine the relative importance of
features. In previous work (Weeds and Weir 2003b), we experimented with weight
functions based on combinatorial, probabilistic, and mutual information (MI). These
allow us to define type-based, token-based, and MI-based CRMs, respectively. This
work extends the previous work by also considering weighted mutual information
(WMI) (Fung and McKeown 1997), the t-test (Manning and Schu?tze 1999), the z-test
(Fontenelle et al 1994), and an approximation to the log-likelihood ratio (Manning and
Schu?tze 1999) as weight functions.
2.2 Additive Models
Having considered the intuition behind calculating precision and recall for co-
occurrence retrieval, we now formulate this formally in terms of an additive model.
We first need to consider for each word w which co-occurrence types will be re-
trieved, or predicted, by it and, conversely, required in a description of it. We will refer
to these co-occurrence types as the features of w, F(w):
F(w) = {c : D(w, c) > 0} (1)
where D(w, c) is the weight associated with word w and co-occurrence type c. Possible
weight functions will be described in Section 2.3.
The shared features of word w1 and word w2 are referred to as the set of True
Positives, TP(w1, w2), which will be abbreviated to TP in the rest of this article:
TP(w1, w2) = F(w1) ? F(w2) (2)
The precision of w1?s retrieval of w2?s features is the proportion of w1?s features that
are shared by both words, where each feature is weighted by its relative importance
according to w1:
Padd(w1, w2) =
?
TP D(w1, c)
?
F(w1 ) D(w1, c)
(3)
The recall of w1?s retrieval of w2?s features is the proportion of w2?s features that
are shared by both words, where each feature is weighted by its relative importance
according to w2:
Radd(w1, w2) =
?
TP D(w2, c)
?
F(w2 ) D(w2, c)
(4)
445
Computational Linguistics Volume 31, Number 4
Table 1
Weight functions.
Dtype(w, c) =
{
1 if P(c|w) > 0
0 otherwise
Dtok(w, c) = P(c|w)
Dmi(w, c) = I(w, c) = log
(
P(c, w)
P(c)P(w)
)
Dwmi(w, c) = P(c, w) ? log
(
P(c, w)
P(c)P(w)
)
Dt(w, c) =
P(c, w)?P(c)?P(w)
?
P(c, w)
N
Dz(w, c) =
P(c, w)?P(c)?P(w)
?
P(c).P(w)
N
Dallr(w, c) = ?2 ?
(
log L
(
F(w, c), F(w), F(c)N
)
? log L
(
F(w, c), F(w), F(w, c)F(w)
))
Precision and recall both lie in the range [0,1] and are both equal to one when
each word has exactly the same features. It should also be noted that the recall of
w1?s retrieval of w2 is equal to the precision of w2?s retrieval of w1, i.e., Radd(w1, w2) =
Padd(w2, w1).
2.3 Weight Functions
The weight function plays two important roles. First, it determines which co-
occurrences of w1 and w2 are important enough to be considered part of their descrip-
tion, or by analogy with document retrieval, which co-occurrences we want to retrieve
for w2 and which co-occurrences we have retrieved using the description of w1. It is
then used to weight contexts by their importance. In the latter case, D(w1, c) tells us the
retrieval process?s perceived relevance of co-occurrence type c, and D(w2, c) tells us the
actual relevance of co-occurrence type c. The weight functions we have considered so
far are summarized in Table 1. Each weight function can be used to define its own CRM,
which we will now discuss in more detail.
Additive type-based CRM (Dtype). In this CRM, the precision of w1?s retrieval of w2 is the
proportion of co-occurrence types occurring with w1 that also occur with w2, and the
recall of w1?s retrieval of w2 is the proportion of verb co-occurrence types (or distinct
verbs) occurring with w2 that also occur with w1. In this case, the summed values of D
are always 1, and hence the expressions for precision and recall can be simplified:
Paddtype(w1, w2) =
?
TP Dtype(w1, c)
?
F(w1 ) Dtype(w1, c)
=
|TP|
|F(w1)|
(5)
446
Weeds and Weir Co-occurrence Retrieval
Raddtype(w1, w2) =
?
TP Dtype(w2, c)
?
F(w2 ) Dtype(w2, c)
=
|TP|
|F(w2)|
(6)
Additive token-based CRM (Dtok). In this CRM, the precision of w1?s retrieval of w2 is the
proportion of co-occurrence tokens occurring with w1 that also occur with w2, and the
recall of w1?s retrieval of w2 is the proportion of co-occurrence tokens occurring with w2
that also occur with w1. Hence, words have the same features as in the type-based CRM,
but each feature is given a weight based on its probability of occurrence. Since F(w) =
{c : D(w, c) > 0} = {c : P(c|w) > 0}, it follows that
?
F(w) Dtok(w, c) = 1, and therefore
the expressions for precision and recall can be simplified:
Paddtok (w1, w2) =
?
TP Dtok(w1, c)
?
F(w1 ) Dtok(w1, c)
=
?
TP
P(c, w1) (7)
Raddtok (w1, w2) =
?
TP Dtok(w2, c)
?
F(w2) Dtok(w2, c)
=
?
TP
P(c, w2) (8)
Additive MI-based CRM (Dmi). Using pointwise mutual information (MI) (Church and
Hanks 1989) as the weight function means that a co-occurrence c is considered a feature
of word n if the probability of their co-occurrence is greater than would be expected if
words occurred independently. In addition, more informative co-occurrences contribute
more to the sums in the calculation of precision and recall and hence have more weight.
Additive WMI-based CRM (Dwmi). Weighted mutual information (WMI) (Fung and
McKeown 1997) has been proposed as an alternative to MI, particularly when MI might
lead to the over-association of low-frequency events. In this function, the pointwise
MI is multiplied by the probability of the co-occurrence; hence, reducing the weight
assigned to low-probability events.
Additive t-test based CRM (Dt). The t-test (Manning and Schu?tze 1999) is a standard
statistical test that has been proposed for collocation analysis. It measures the (signed)
difference between the observed probability of co-occurrence and the expected prob-
ability of co-occurrence, as would be observed if words occurred independently. The
difference is divided by the standard deviation in the observed distribution. Similarly
to MI, this score obviously gives more weight to co-occurrences that occur more than
would be expected, and its use as the weight function results in any co-occurrences that
occur less than would be expected being ignored.
Additive z-test based CRM (Dz). The z-test (Fontenelle et al 1994) is almost identical
to the t-test. However, using the z-test, the (signed) difference between the observed
probability of co-occurrence and the expected probability of co-occurrence is divided
by the standard deviation in the expected distribution.
Additive log-likelihood ratio based CRM (Dallr). The log-likelihood ratio (Manning and
Schu?tze 1999) considers the difference (as a log ratio) in probability of the observed
frequencies of co-occurrences and individual words occurring under the null hypoth-
447
Computational Linguistics Volume 31, Number 4
esis, that words occur independently, and under the alternative hypothesis, that they
do not.
H0 : P(c|w) = p = P(c|?w) (9)
H1 : P(c|w) = p1 = p2 = P(c|?n) (10)
If f (w, c) is the frequency of w and c occurring together, f (w) is the total frequency
of w occurring in any context, f (c) is the total frequency of c occurring with any
word, and N is the grand total of co-occurrences, then the log-likelihood ratio can
be written:
Log?(w, c) = ?2. log L(H0)
L(H1)
(11)
= ?2.
?
?
?
?
?
?
?
?
?
log L
(
f (w, c), f (w), f (c)N
)
+ log L
(
f (c) ? f (w, c), N ? f (w), f (c)N
)
? log L
(
f (w, c), f (w), f (w, c)f (w)
)
? log L
(
f (c) ? f (w, c), N ? f (w), f (c)? f (w, c)N? f (w)
)
?
?
?
?
?
?
?
?
?
(12)
where L(k, n, x) = xk(1 ? x)n?k (13)
In our implementation (see Table 1), an approximation to this formula is used,
which we term the ALLR weight function. We use an approximation because the terms
that represent the probabilities of the other contexts (i.e., seeing f (c) ? f (w, c) under each
hypothesis) tend towards ?? as N increases (since the probabilities tend towards zero).
Since N is very large in our experiments (approximately 2,000,000), we found that using
the full formula led to many weights being undefined. Further, since in this case the
probability of seeing other contexts will be approximately equal under each hypothesis,
it is a reasonable approximation to make.
Another potential problem with using the log-likelihood ratio as the weight func-
tion is that it is always positive, since the observed distribution is always more
probable than the hypothesized distribution. All of the other weight functions assign
a zero or negative weight to co-occurrence types that do not occur with a given
word and thus these zero frequency co-occurrence types are never selected as features.
This is advantageous in the computation of similarity, since computing the sums
over all co-occurrence types rather than just those co-occurring with at least one of
the words is (1) very computationally expensive and (2) due to their vast number, the
effect of these zero frequency co-occurrence types tends to outweigh the effect of
those co-occurrence types that have actually occurred. Giving such weight to these
shared non-occurrences seems unintuitive and has been shown by Lee (1999) to be
undesirable in the calculation of distributional similarity. Hence, when using the
448
Weeds and Weir Co-occurrence Retrieval
ALLR as the weight function, we use the additional restriction that P(c, w) > 0 when
selecting features.
2.4 Difference-Weighted Models
In additive models, no distinction is made between features that have occurred to the
same extent with each word and features that have occurred to different extents with
each word. For example, if two words have the same features, they are considered
identical, regardless of whether the feature occurs with the same probability with each
word or not. Here, we define a type of model that allows us to capture the difference in
the extent to which each word has each feature.
We do this by defining the similarity of two words with respect to an individual
feature, using the same principles that we use to define the similarity of two words
with respect to all their features. First, we define an extent function, E(w, c), which is the
extent to which w1 goes with c and which may be, but is not necessarily, the same as
the weight function D(n, w). Possible extent functions will be discussed in Section 2.5.
Having defined this function, we can measure the precision and recall of individual
features. The precision of an individual feature c retrieved by w1 is the extent to which
both words go with c divided by the extent to which w1 goes with c. The recall of the
retrieval of c by w1 is the extent to which both words go with c divided by the extent to
which w2 goes with c.
P (w1, w2, c) =
min(E(w1, c), E(w2, c))
E(w1, c)
(14)
R(w2, w1, c) =
min(E(w1, c), E(w2, c))
E(w2, c)
(15)
Precision and recall of an individual feature, like precision and recall of a distrib-
ution, lie in the range [0,1]. We can now redefine precision and recall of a distribution
as follows:
Pdw(w1, w2) =
?
TP D(w1, c) ? P (w1, w2, c)
?
F(w1) D(w1, c)
(16)
Rdw(w1, w2) =
?
TP D(w2, c) ? R(w1, w2, c)
?
F(w2 ) D(w2, c)
(17)
Using precision and recall of individual features as weights in the definitions
of precision and recall of a distribution captures the intuition that retrieval of a
co-occurrence type is not a black-and-white matter. Features that are shared to a
similar extent are considered more important in the calculation of distributional
similarity.
449
Computational Linguistics Volume 31, Number 4
Table 2
Extent functions.
Etype(w, c) = P(c|w)
Etok(w, c) = P(c|w)
Emi(w, c) = I(w, c) = log
(
P(c, w)
P(c)P(w)
)
Ewmi(w, c) = P(c, w). log
(
P(c, w)
P(c)P(w)
)
Et(w, c) =
P(c, w)?P(c).P(w)
?
P(c, w)
N
Ez(w, c) =
P(c, w)?P(c).P(w)
?
P(c).P(w)
N
Eallr(w, c) = ?2.
(
log L
(
f (w, c), f (w), f (c)N
)
? log L
(
f (w, c), f (w), f (w, c)f (w)
))
2.5 Extent Functions
The extent functions we have considered so far are summarized in Table 2. Note that
in general, the extent function is the same as the weight function, which leads to a
standard simplification of the expressions for precision and recall in the difference-
weighted CRMs. For example, in the difference-weighted MI-based model we get the
expressions:
Pdwmi (w1, w2) =
?
TP I(w1, c) ?
min(I(w1, c),I(w2, c))
I(w1, c)
?
F(w1 ) I(w1, c)
=
?
TP min(I(w1, c), I(w2, c))
?
F(w1 ) I(w1, c)
(18)
Rdwmi (w1, w2) =
?
TP I(w2, c) ?
min(I(w2, c),I(w1, c))
I(w2, c)
?
F(w2 ) I(w2, c)
=
?
TP min(I(w2, c), I(w1, c))
?
F(w2 ) I(w2, c)
(19)
Similar expressions can be derived for the WMI-based CRM, the t-test based CRM, the
z-test based CRM, and the ALLR-based CRM. An interesting special case is the difference-
weighted token-based CRM. In this case, since
?
F(w) P(c|w) = 1, we derive the following
expressions for precision and recall:
Pdwtok (w1, w2) =
?
TP P(c|w1) ?
min(P(c|w1), P(c|w2))
P(c|w1)
?
F(w1 ) P(c|w1)
=
?
TP
min(P(c|w1), P(c|w2)) (20)
450
Weeds and Weir Co-occurrence Retrieval
Rdwtok(w1, w2) =
?
TP P(c|w2) ?
min(P(c|w2), P(c|w1))
P(c|w2)
?
F(w2 ) P(c|w2)
=
?
TP
min(P(c|w2), P(c|w1))
= Pdwtok (w1, w2) (21)
Note that although we have defined separate precision and recall functions, we
have arrived at the same expression for both in this model. As a result, this model is
symmetric.
The only CRM in which we use a different extent and weight function is the
difference-weighted type-based CRM. This is because there is no difference between types
and tokens for an individual feature; i.e., their retrieval is equivalent. In this case, the
following expressions for precision and recall are derived:
Pdwtype(w1, w2) =
?
TP Dtype(w1, c) ? Ptype(w1, w2, c)
?
F(w1 ) Dtype(w1, c)
=
?
TP
min(P(c|w1), P(c|w2))
P(c|w1)
|F(w1)|
(22)
Rdwtype(w1, w2) =
?
TP Dtype(w2, c) ? Rtype(w1, w2, c)
?
F(w2 ) Dtype(w2, c)
=
?
TP
min(P(c|w2), P(c|w1))
P(c|w2)
|F(w2)|
(23)
Note that this is different from the additive token-based model because, although
every token is effectively considered in this model, tokens are not weighted equally.
In this model, tokens are treated differently according to which type they belong.
The importance of the retrieval (or non-retrieval) of a single token depends on the
proportion of the tokens for its particular type that it constitutes.
2.6 Combining Precision and Recall
We have, so far, been concerned with defining a pair of numbers that represents the
similarity between two words. However, in applications, it is normally necessary to
compute a single number in order to determine neighborhood or cluster membership.
The classic way to combine precision and recall in IR is to compute the F-score; that is,
the harmonic mean of precision and recall:
F = mh(P ,R) = 2 ? P ? RP +R (24)
However, we do not wish to assume that a good substitute requires both high
precision and high recall of the target distribution. It may be that, in some situations,
the best word to use in place of another word is one that only retrieves correct co-
occurrences (i.e., it is a high-precision neighbor) or it may be one that retrieves all of
the required co-occurrences (i.e., it is a high-recall neighbor). The other factor in each
case may play only a secondary role or no role at all.
We can retain generality and investigate whether high precision or high recall or
high precision and high recall are required for high similarity by computing a weighted
451
Computational Linguistics Volume 31, Number 4
Table 3
Table of special values of ? and ?.
? ? Special Case
- 1 harmonic mean of precision and recall (F-score)
? 0 weighted arithmetic mean of precision and recall
1 0 precision
0 0 recall
0.5 0 unweighted arithmetic mean
arithmetic mean of the harmonic mean and the weighted arithmetic mean of precision
and recall:4
mh(P (w1, w2),R(w1, w2)) =
2.P (w1, w2).R(w1, w2)
P (w1, w2) +R(w1, w2)
(25)
ma(P (w1, w2),R(w1, w2)) = ?.P (w1, w2) + (1 ? ?).R(w1, w2) (26)
sim(w1, w2) = ?.mh(P (w1, w2),R(w1, w2))
+ (1 ? ?).ma(P (w1, w2),R(w1, w2)) (27)
where both ? and ? lie in the range [0,1]. The resulting similarity, sim(w1, w2), will also
lie in the range [0,1] where 0 is low and 1 is high. This formula can be used in combi-
nation with any of the models for precision and recall outlined earlier. Precision and
recall can be computed once for every pair of words (and every model) whereas sim-
ilarity depends on the values of ? and ?. The flexibility allows us to investigate em-
pirically the relative significance of the different terms and thus whether one (or more)
might be omitted in future work. Table 3 summarizes some special parameter settings.
2.7 Discussion
We have developed a framework based on the concept of co-occurrence retrieval
(CR). Within this framework we have defined a number of models (CRMs) that
allow us to systematically explore three questions about similarity. First, is similarity
between words necessarily a symmetric relationship, or can we gain an advantage by
considering it as an asymmetric relationship? Second, are some features inherently
more salient than others? Third, does the difference in extent to which each word takes
each feature matter?
The CRMs and the parameter settings therein correspond to alternative possibil-
ities. First, a high-precision neighbor is not necessarily a high-recall neighbor (and,
conversely, a high-recall neighbor is not necessarily a high-precision neighbor) and
therefore we are not constrained to a symmetric relationship of similarity between
4 This is as opposed to using a standard weighted F-score (Manning and Schu?tze 1999), which uses just one
parameter ?: F? = PR?R+(1??)P . We did not use this weighting because we wished to investigate the
differences between using an arithmetic mean and a harmonic mean.
452
Weeds and Weir Co-occurrence Retrieval
words. Second, the use of different weight functions varies the relative importance
attached to features. Finally, difference-weighted models contrast with additive models
in considering the difference in extent to which each word takes each feature.
3. Data and Experimental Techniques
The rest of this paper is concerned with evaluation of the proposed framework; first, by
comparing it to existing distributional similarity measures, and second, by evaluating
performance on two tasks. Throughout our empirical work, we use one data-set and one
neighbor set comparison technique, which we now discuss in advance of presenting any
of our actual experiments.
3.1 Data
The data used for all our experimental work was noun-verb direct-object data extracted
from the BNC by a Robust Accurate Statistical Parser (RASP) (Briscoe and Carroll 1995;
Carroll and Briscoe 1996). We constructed a list of nouns that occur in both our data set
and WordNet ordered by their frequency in our corpus data. Since we are interested
in the effects of word frequency on word similarity, we selected 1,000 high-frequency
nouns and 1,000 low-frequency nouns. The 1,000 high-frequency nouns were selected
as the nouns with frequency ranks of 1?1,000; this corresponds to a frequency range of
[586,20871]. The low-frequency nouns were selected as the nouns with frequency ranks
of 3,001?4,000; this corresponds to a frequency range of [72,121].
For each target noun, 80% of the available data was randomly selected as training
data and the other 20% was set aside as test data.5 The training data was used to
compute similarity scores between all possible pairwise combinations of the 2,000 nouns
and to provide (MLE) estimates of noun-verb co-occurrence probabilities in the pseudo-
disambiguation task. The test data provides unseen co-occurrences for the pseudo-
disambiguation task.
Although we only consider similarity between nouns based on co-occurrences
with verbs in the direct-object position, the generality of the techniques proposed
is not so restricted. Any of the techniques can be applied to other parts of speech,
other grammatical relations, and other types of context. We restricted the scope of our
experimental work solely for computational and evaluation reasons. However, we
could have chosen to look at the similarity between verbs or between adjectives.6 We
chose nouns as a starting point since nouns tend to allow less sense extensions than
verbs and adjectives (Pustejovsky 1995). Further, the noun hyponymy hierarchy in
WordNet, which will be used as a pseudo-gold standard for comparison, is widely
recognized in this area of research.
Some previous work on distributional similarity between nouns has used only a
single grammatical relation (e.g., Lee 1999), whereas other work has considered multiple
grammatical relations (e.g., Lin 1998a). We consider only a single grammatical relation
because we believe that it is important to evaluate the usefulness of each grammatical
relation in calculating similarity before deciding how to combine information from
5 This results in a single 80:20 split of the complete data set, in which we are guaranteed that the original
relative frequencies of the target nouns are maintained.
6 The use of grammatical relations to model context precludes finding similarities between words of
different parts of speech. Since we are looking at similarity in terms of substitutability, we would not
expect to find a word of one part of speech substitutable for a word of another part of speech.
453
Computational Linguistics Volume 31, Number 4
different relations. In previous work (Weeds 2003), we found that considering the
subject relation as well as the direct-object relation did not improve performance on
a pseudo-disambiguation task.
Our last restriction was to only consider 2,000 of the approximately 35,000 nouns
occurring in the corpus. This restriction was for computational efficiency and to avoid
computing similarities based on the potentially unreliable descriptions of very low-
frequency words. However, since our evaluation is comparative, we do not expect our
results to be affected by this or any of the other restrictions.
3.2 Neighbor Set Comparison Technique
In several of our experiments, we measure the overlap between two different similarity
measures. We use a neighbor set comparison technique adapted from Lin (1997).
In order to compare two neighbor sets of size k, we transform each neighbor set
so that each neighbor is given a rank score of k ? rank. Potential neighbors not within
a given rank distance k of the noun score zero. This transformation is required since
scores computed on different scales are to be compared and because we wish to only
consider neighbors up to a certain rank distance. The similarity between two neighbor
sets S and S? is computed as the cosine of the rank score vectors:
C(S, S?) =
?
w?S?S? s(w) ? s?(w)
?k
i=1 i
2
(28)
where s(w) and s?(w) are the rank scores of the words within each neighbor set S and S?
respectively.
In previous work (Weeds and Weir 2003b), having computed the similarity between
neighbor sets for each noun according to each pair of measures under consideration, we
computed the mean similarity across all high-frequency nouns and all low-frequency
nouns. However, since the use of the CR framework requires parameter optimization,
here, we randomly select 60% of the nouns to form a development set and use the
remaining 40% as a test set. Thus, any parameters are optimized over the development
set nouns and performance measured at these settings over the test set.
4. Alternative Distributional Similarity Measures
In this section, we consider related work on distributional similarity measures and the
extent to which some of these measures can be simulated within the CR framework.
However, there is a large body of work on distributional similarity measures; for a
more extensive review, see Weeds (2003). Here, we concentrate on a number of more
popular measures: the Dice Coefficient, Jaccard?s Coefficient, the L1 Norm, the ?-skew
divergence measure, Hindle?s measure, and Lin?s MI-based measure.
4.1 The Dice Coefficient
The Dice Coefficient (Frakes and Baeza-Yates 1992) is a popular combinatorial similarity
measure adopted from the field of Information Retrieval for use as a measure of lexical
454
Weeds and Weir Co-occurrence Retrieval
distributional similarity. It is computed as twice the ratio between the size of the inter-
section of the two feature sets and the sum of the sizes of the individual feature sets:
simdice(w1, w2) =
2 ? |F(w1) ? F(w2)|
|F(w1)|+ |F(w2)|
where F(w) = {c : P(c|w) > 0} (29)
According to this measure, the similarity between words with no shared features
is zero and the similarity between words with identical feature sets is 1. However, as
shown below, this formula is equivalent to a special case in the CR framework: the
harmonic mean of precision and recall (or F-score) using the additive type-based CRM.
mh(Paddtype(w1, w2),Raddtype(w1, w2))=
2 ? Paddtype(w1, w2) ? Raddtype(w1, w2)
Paddtype(w1, w2) +Raddtype(w1, w2)
=
2 ? |TP||F(w1 )| ?
|TP|
|F(w2 )|
|TP|
|F(w1)| +
|TP|
|F(w2 )|
=
2 ? |TP| ? |TP|
|TP| ? |F(w1)|+ |TP| ? |F(w2)|
=
2 ? |TP|
|F(w1)|+ |F(w2)|
=
2 ? |F(w1) ? F(w2)|
|F(w1)|+ |F(w2)|
= simdice(w1, w2) (30)
Thus, when ? is set to 1 in the additive type-based CRM, the Dice Coefficient is
exactly replicated.
4.2 Jaccard?s Coefficient
Jaccard?s Coefficient (Salton and McGill 1983), also known as the Tanimoto Coefficient
(Resnik 1993), is another popular combinatorial similarity measure. It can be defined as
the proportion of features belonging to either word that are shared by both words; that
is, the ratio between the size of the intersection of the feature sets and the size of the
union of feature sets:
simjacc(w1, w2) =
|F(w1) ? F(w2)|
|F(w1) ? F(w2)|
(31)
As with the Dice Coefficient, the similarity between words with no shared co-
occurrences is zero and the similarity between words with identical features is 1. Fur-
ther, as shown by van Rijsbergen (1979), the Dice Coefficient and Jaccard?s Coefficient
are monotonic in one another. Thus, although in general the scores computed by each
will be different, the orderings or rankings of objects will be the same. In other words,
for all k and w, the k nearest neighbors of word w according to Jaccard?s Coefficient will
be identical to the k nearest neighbors of word w according to the Dice Coefficient and
the harmonic mean of precision and recall in the additive type-based CRM.
455
Computational Linguistics Volume 31, Number 4
4.3 The L1 Norm
The L1 Norm (Kaufman and Rousseeuw 1990) is a member of a family of measures
known as the Minkowski Distance, for measuring the distance7 between two points
in space. The L1 Norm is also known as the Manhattan Distance, the taxi-cab distance,
the city-block distance, and the absolute value distance, since it represents the distance
traveled between the two points if you can only travel in orthogonal directions. When
used to calculate lexical distributional similarity, the dimensions of the vector space are
co-occurrence types and the values of the vector components are the probabilities of
the co-occurrence types given the word. Thus the L1 distance between two words, w1
and w2, can be written as:
distL1 (w1, w2) =
?
c
|P(c|w1) ? P(c|w2)| (32)
However, noting the algebraic equivalence A + B ? |A ? B| ? 2 ? min(A, B) and us-
ing basic probability theory, we can rewrite the L1 Norm as follows:
distL1 (w1, w2) =
?
c
|P(c|w1) ? P(c|w2)|
=
?
c
P(c|w1) + P(c|w2) ? 2 ? min(P(c|w1), P(c|w2))
=
?
c
P(c|w1) +
?
c
P(c|w2) ? 2 ?
?
c
min(P(c|w1), P(c|w2))
= 2 ? 2 ?
?
c
min(P(c|w1), P(c|w2)) (33)
However, min(P(c|w1), P(c|w2)) > 0 if and only if c ? TP. Hence:
distL1 (w1, w2) = 2 ? 2 ?
?
TP
min(P(c|w1), P(c|w2) = 2 ? 2 ? simdwtok(w1, w2) (34)
In other words, the L1 Norm is directly related to the difference-weighted token-
based CRM. The constant and multiplying factors are required, since the CRM defines
a similarity in the range [0,1], whereas the L1 Norm defines a distance in the range [0,2]
(where 0 distance is equivalent to 1 on the similarity scale).
4.4 The ?-skew Divergence Measure
The ?-skew divergence measure (Lee 1999, 2001) is a popular approximation to the
Kullback-Leibler divergence measure8 (Kullback and Leibler 1951; Cover and Thomas
1991). It is an approximation developed to be used when unreliable MLE probabilities
7 Distance measures, also referred to as divergence and dissimilarity measures, can be viewed as the
inverse of similarity measures; that is, an increase in distance correlates with a decrease in similarity.
8 The Kullback-Leibler divergence measure is also often referred to as ?relative entropy.?
456
Weeds and Weir Co-occurrence Retrieval
would result in the actual Kullback-Leibler divergence measure being equal to ?. It is
defined (Lee 1999) as:
dist?(q, r) = D(r||?.q + (1 ? ?).r) (35)
for 0 ? ? ? 1, and where:
D(p||q) =
?
x
p(x) log
p(x)
q(x)
(36)
In effect, the q distribution is smoothed with the r distribution, which results in it
always being non-zero when the r distribution is non-zero. The parameter ? controls the
extent to which the measure approximates the Kullback-Leibler divergence measure.
When ? is close to 1, the approximation is close while avoiding the problem with
zero probabilities associated with using the Kullback-Leibler divergence measure. This
theoretical justification for using a very high value of ? (e.g., 0.99) is also borne out by
empirical evidence (Lee 2001).
The ?-skew divergence measure retains the asymmetry of the Kullback-Leibler
divergence, and Weeds (2003) discusses the significance in the direction in which it
is calculated. For the purposes of this paper, we will find the neighbors of w2 by
optimizing:9
dist?(P(c|w1), P(c|w2)) (37)
Due to the form of the ?-skew divergence measure, we do not expect any of the
CRMs to exactly simulate it. However, this measure does take into account the differ-
ences between the probabilities of co-occurrences in each distribution (as a log ratio)
and therefore we might expect that it will be fairly closely simulated by the difference-
weighted token-based CRM. Further, the ?-skew divergence measure is asymmetric.
dist?(w1, w2) measures the cost of using the distribution of w1 instead of w2 and is
calculated over the verbs that occur with w2. As such, we might expect that dist? will be
a high-recall measure, since recall is calculated over the co-occurrences of w2.
In order to determine how close any approximation is in practice, we compared
the 200 nearest neighbors according to dist? and different parameter settings within
the CR framework for 1,000 high-frequency nouns and for 1,000 low-frequency nouns,
using the data and the neighbor set comparison technique described in Section 3. Table 4
shows the optimal parameters in each CRM for simulating dist?, computed over the
development set, and the mean similarity at these settings over both the development
set and the test set. From these results, we can make the following observations.
First, the differences in mean similarities over the development set and the test
set are minimal. Thus, performance of the models with respect to different parameter
settings appears stable across different words.
Second, the differences between the models are fairly small. The difference-
weighted token-based CRM achieves a fairly close approximation to dist?, but the
9 This is what Weeds (2003) refers to as dist?1.
457
Computational Linguistics Volume 31, Number 4
Table 4
Optimized similarities between CRMs and dist? and corresponding parameter settings.
Target Noun Frequency
high low
Optimal Devel. Test Optimal Devel. Test
Parameters Sim. Sim. Parameters Sim. Sim.
CRM ? ? ? ?
simaddtype 0.25 0.4 0.74 0.75 0.5 0.3 0.66 0.66
simaddtok 0.5 0.0 0.77 0.78 0.5 0.0 0.67 0.66
simaddmi 0.0 0.0 0.76 0.77 0.25 0.1 0.72 0.73
simaddwmi 0.25 0.0 0.71 0.72 0.25 0.1 0.79 0.79
simaddt 0.5 0.0 0.84 0.85 0.5 0.2 0.71 0.71
simaddz 0.5 0.0 0.79 0.80 0.5 0.1 0.63 0.63
simaddallr 0.25 0.0 0.70 0.71 0.5 0.0 0.64 0.63
simdwtype 0.0 0.25 0.70 0.71 0.0 0.0 0.52 0.53
simdwtok ? ? 0.79 0.80 ? ? 0.58 0.58
simdwmi 0.0 0.0 0.66 0.68 0.0 0.0 0.60 0.60
simdwwmi 0.0 0.1 0.66 0.67 0.0 0.1 0.58 0.58
simdwt 0.5 0.1 0.82 0.83 0.5 0.3 0.69 0.69
simdwz 0.5 0.0 0.78 0.80 0.5 0.1 0.64 0.64
simdwallr 0.75 0.0 0.53 0.54 0.75 0.0 0.48 0.48
overall best approximation is achieved by the additive t-test based CRM. Although
none of the CRMs are able to simulate dist? exactly, the closeness of approximation
achieved in the best cases (greater than 0.7) is substantially higher than the degree
of overlap observed between other measures of distributional similarity. Weeds, Weir,
and McCarthy (2004) report an average overlap of 0.4 between neighbor sets produced
using dist? and Jaccard?s Measure and an average overlap of 0.48 between neighbor sets
produced using dist? and Lin?s similarity measure.
A third observation is that all of the asymmetric models get closest at high
levels of recall for both high- and low-frequency nouns. For example, Figure 1 illustrates
the variation in mean similarity between neighbor sets with the parameters ? and
? for the additive t-test based model. As can be seen, similarity between neighbor
sets is significantly higher at high recall settings (low ?) within the model than at high-
precision settings (high ?), which suggests that dist? has high-recall CR characteristics.
4.5 Hindle?s Measure
Hindle (1990) proposed an MI-based measure, which he used to show that nouns could
be reliably clustered based on their verb co-occurrences. We consider the variant of
458
Weeds and Weir Co-occurrence Retrieval
Figure 1
Variation (with parameters ? and ?) in development set mean similarity between neighbor sets
of the additive t-test based CRM and of dist?.
Hindle?s Measure proposed by (Lin 1998a), which overcomes the problem associated
with calculating MI for word-feature combinations that do not occur:
simhind(w1, w2) =
?
T(w1)?T(w2)
min(I(c, w1), I(c, w2)) (38)
where T(w1) = {c : I(c, n) > 0}. This expression is the same as the numerator in the
expressions for precision and recall in the difference-weighted MI-based CRM:
Pdwmi (w1, w2) =
?
TP I(w1, c) ?
min(I(w1, c),I(w2, c))
I(w1, c)
?
F(w1) I(w1, c)
=
?
TP min(I(w1, c), I(w2, c))
?
F(w1 ) I(w1, c)
(39)
Rdwmi (w1, w2) =
?
TP I(w2, c) ?
min(I(w2, c),I(w1, c))
I(w2, c)
?
F(w2 ) I(w2, c)
=
?
TP min(I(w2, c), I(w1, c))
?
F(w2 ) I(w2, c)
(40)
since TP = T(w1) ? T(w2). However, we also note that the denominator in the expres-
sion for recall depends only on w2, and therefore, for a given w2, is a constant. Since w2 is
the target word, it will remain the same as we calculate each neighbor set. Accordingly,
the value of recall for each potential neighbor w1 of w2 will be the value of simhind
divided by a constant. Hence, neighbor sets derived using simhind are identical to those
obtained using recall (? = 0,? = 0) in the difference-weighted MI-based CRM.
4.6 Lin?s Measure
Lin (1998a) proposed a measure of lexical distributional similarity based on his
information-theoretic similarity theorem (Lin 1997, 1998b):
The similarity between A and B is measured by the ratio between the amount of
information needed to state the commonality of A and B and the information needed to
fully describe what A and B are.
459
Computational Linguistics Volume 31, Number 4
If the features of a word are grammatical relation contexts, the similarity between
two words w1 and w2 can be written according to Lin?s measure as:
simlin(w1, w2) =
?
T(w1)?T(w2)(I(w1, c) + I(w2, c))
?
T(w1) I(w1, c) +
?
T(w2 ) I(w2, c)
(41)
where T(w) = {c : I(w, c) > 0}. There are parallels between simlin and simdice in that both
measures compute a ratio between what is shared by the descriptions of both nouns and
the sum of the descriptions of each noun. The major difference appears to be the use of
MI, and hence we predicted that there would be a close relationship between simlin and
the harmonic mean in the additive MI-based CRM. This relationship is shown below:
mh(Paddmi (w1, w2),Raddmi (w1, w2)) =
2 ? Paddmi (w1, w2) ? Raddmi (w1, w2)
Paddmi (w1, w2) +Raddmi (w1, w2)
(42)
=
2 ?
?
TP I(w1, c)
?
F(w1)
I(w1, c)
?
?
TP I(w2, c)
?
F(w2)
I(w2, c)
?
TP I(w1, c)
?
F(w1)
I(w1, c)
+
?
TP I(w2, c)
?
F(w2)
I(w2, c)
(43)
=
2 ?
?
TP I(w1, c) ?
?
TP I(w2, c)
?
TP I(w2, c) ?
?
F(w1 ) I(w1, c) +
?
TP I(w1, c) ?
?
F(w2 ) I(w2, c)
(44)
Now if
?
TP I(w1, c) =
?
TP I(w2, c), it follows:
mh(Paddmi (w1, w2),Raddmi (w1, w2)) =
2 ?
?
TP I(w1, c)
?
F(w1 ) I(w1, c) +
?
F(w2) I(w2, c)
(45)
=
?
TP I(w1, c) + I(w2, c)
?
F(w1 ) I(w1, c) +
?
F(w2 ) I(w2, c)
(46)
=
?
T(w1)?T(w2 )(I(w1, c) + I(w2, c))
?
T(w1 ) I(w1, c) +
?
T(w2) I(w2, c)
since T(w) = F(w) (47)
= simlin(w1, w2) (48)
Thus, when the additive MI-based model is used, ? = 1 and the condition
?
TP I(w1, c) =
?
TP I(w2, c) holds, the CR framework reduces to simlin. However, this
last necessary condition for equivalence is not one we can expect to hold for many (if
any) pairs of words. In order to investigate how good an approximation the harmonic
mean is to simlin in practice, we compared neighbor sets according to each measure
using the neighbor set comparison technique outlined earlier.
Figure 2 illustrates the variation in mean similarity between neighbor sets with the
parameters ? and ?. At ? = 1, the average similarity between neighbor rankings was
0.967 for high-frequency nouns and 0.923 for low-frequency nouns. This is significantly
higher than similarities between other standard similarity measures. However, the
optimal approximation of simlin was found using ? = 0.75 and ? = 0.5 in the additive
MI-based CRM. With these settings, the development set similarity was 0.987 for high-
460
Weeds and Weir Co-occurrence Retrieval
frequency nouns and 0.977 for low-frequency nouns. This suggests that simlin allows
more compensation for lack of recall by precision and vice versa than the harmonic
mean.
4.7 Discussion
We have seen that five of the existing lexical distributional similarity measures are (ap-
proximately) equivalent to settings within the CR framework and for one other, a weak
approximation can be made. The CR framework, however, more than simulates existing
measures of distributional similarity. It defines a space of distributional similarity mea-
sures that is already populated with a few named measures. By exploring the space, we
can discover the desirable characteristics of distributional similarity measures. It may be
that the most useful measure within this space has already been discovered, or it may
be that a new optimal combination of characteristics is discovered. The primary goal,
however, is to understand how different characteristics relate to high performance in
different applications and thus explain why one measure performs better than another.
With this goal in mind, we now turn to the applications of distributional similarity.
In the next section, we consider what characteristics of distributional similarity mea-
sures are desirable in two different application areas: (1) automatic thesaurus generation
and (2) language modeling.
5. Application-Based Evaluation
As discussed by Weeds (2003), evaluation is a major problem in this area of research.
In some areas of natural language research, evaluation can be performed against a
gold standard or against human plausibility judgments. The first of these approaches
is taken by Curran and Moens (2002), who evaluate a number of different distributional
similarity measures and weight functions against a gold standard thesaurus compiled
from Roget?s, the Macquarie thesaurus, and the Moby thesaurus. However, we argue that
this approach can only be considered when distributional similarity is required as an
approximation to semantic similarity and that, in any case, it is not ideal since it is not
Figure 2
Variation (with parameters ? and ?) in development set mean similarity between neighbor sets
of the additive MI-based CRM and of simlin.
461
Computational Linguistics Volume 31, Number 4
clear that there is a single ?right answer? as to which words are most distributionally
similar. The best measure of distributional similarity will be the one that returns the
most useful neighbors in the context of a particular application and thus leads to the
best performance in that application. This section investigates whether the desirable
characteristics of a lexical distributional similarity measure in an automatic thesaurus
generation task (WordNet prediction) are the same as those in a language modeling task
(pseudo-disambiguation).
5.1 WordNet Prediction Task
In this section, we evaluate the ability of distributional similarity measures to predict
semantic similarity by making comparisons with WordNet. An underlying assumption
of this approach is that WordNet is a gold standard for semantic similarity, which, as
is discussed by Weeds (2003), is unrealistic. However, it seems reasonable to suppose
that a distributional similarity measure that more closely predicts a semantic measure
based on WordNet is more likely to be a good predictor of semantic similarity. We chose
WordNet as our gold standard for semantic similarity since, as discussed by Kilgarriff
and Yallop (2000), distributional similarity scores calculated over grammatical relation
level context tend to be more similar to tighter thesauri, such as WordNet, than looser
thesauri such as Roget?s.
5.1.1 Experimental Set-Up. There are a number of ways to measure the distance be-
tween two nouns in the WordNet noun hierarchy (see Budanitsky [1999] for a review).
In previous work (Weeds and Weir 2003b), we used the WordNet-based similarity
measure first proposed in Lin (1997) and used in Lin (1998a):
wn simlin(w1, w2) =
max
c1?S(w1 )?c2?S(w2 )
(
max
c?sup(c1 )?sup(c2 )
2 log P(c)
log (P(c1)) + log (P(c2))
)
(49)
where S(w) is the set of senses of the word w in WordNet, sup(c) is the set of possibly in-
direct super-classes of concept c in WordNet, and P(c) is the probability that a randomly
selected word refers to an instance of concept c (estimated over some corpus such as
SemCor [Miller et al 1994]).
However, in other research (Budanitsky and Hirst 2001; Patwardhan, Banerjee, and
Pedersen 2003; McCarthy, Koeling, and Weeds 2004), it has been shown that the distance
measure of Jiang and Conrath (1997) (referred to herein as the ?JC measure?) is a
superior WordNet-based semantic similarity measure:
wn distJC(w1, w2) =
max
c1?S(w1 )?c2?S(w2 )
(
max
c?sup(c1 )?sup(c2 )
2 log(c) ? log P(c1) ? log P(c2)
)
(50)
In our work, we make an empirical comparison of neighbors derived using a
WordNet-based measure and each of the distributional similarity measures using the
technique discussed in Section 3. We have carried out the same experiments using both
the Lin measure and the JC measure. Correlation between distributional similarity mea-
sures and the WordNet measure tends to be slightly higher when using the JC measure
462
Weeds and Weir Co-occurrence Retrieval
Table 5
Optimized similarities between distributional neighbor sets and WordNet derived neighbor sets.
Noun Frequency
high low
Optimal Devel Test Optimal Devel Test
Parameters Corr. Corr. Parameters Corr. Corr.
Measure ? ? (C) (C) ? ? (C) (C)
simaddtype 0.25 0.5 0.323 0.327 0.5 0.25 0.281 0.275
simaddtok 0.25 0.3 0.302 0.310 0.25 0.0 0.266 0.263
simaddmi 0.25 0.2 0.334 0.342 0.25 0.2 0.290 0.283
simaddwmi 0.25 0.2 0.282 0.293 0.25 0.0 0.274 0.266
simaddt 0.5 0.2 0.330 0.338 0.5 0.2 0.292 0.286
simaddz 0.5 0.1 0.324 0.332 0.5 0.1 0.280 0.276
simaddallr 0.25 0.2 0.298 0.304 0.25 0.1 0.272 0.267
simdwtype 0.0 0.4 0.306 0.310 0.0 0.0 0.221 0.219
simdwtok ? ? 0.285 0.294 ? ? 0.212 0.211
simdwmi 0.0 0.2 0.324 0.333 0.0 0.0 0.266 0.261
simdwwmi 0.0 0.1 0.273 0.281 0.0 0.1 0.223 0.220
simdwt 0.5 0.2 0.328 0.333 0.5 0.3 0.289 0.282
simdwz 0.5 0.1 0.324 0.329 0.5 0.2 0.280 0.276
simdwallr 0.75 0.2 0.263 0.265 0.75 0.0 0.226 0.225
simdice 0.295 0.299 0.123 0.123
simjacc 0.295 0.299 0.123 0.123
distL1 0.285 0.294 0.212 0.211
dist? 0.310 0.317 0.289 0.281
simhind 0.320 0.326 0.267 0.261
simlin 0.313 0.323 0.192 0.186
wnsimlin 0.907 0.907 0.884 0.883
(percentage increase in similarity of approximately 10%), but the relative differences
between distributional similarity measures remain approximately the same. Here, for
brevity, we present results just using the JC measure.
5.1.2 Results. As before, we present the results separately for the 1,000 high-frequency
target nouns and for the 1,000 low-frequency target nouns. Table 5 shows the optimal
parameter settings for each CRM (computed over the development set) and the mean
similarities with the JC measure at these settings in both the development set and the
test set. It also shows the mean similarities over the development set and the test set
for each of the existing similarity measures discussed in Section 4. For reference, we
also present the mean similarity for the WordNet-based measure wn simlin. For ease
463
Computational Linguistics Volume 31, Number 4
Figure 3
Bar chart illustrating test set similarity with WordNet for each distributional similarity measure.
of comparison, the test set correlation values for each distributional measure are also
illustrated in Figure 3.
We would expect a mean overlap score of 0.08 by chance. Standard deviations in
the observed test set mean similarities were all less than 0.1, and thus any difference
between mean scores of greater than 0.016 is significant at the 99% level, and differences
greater than 0.007 are significant at the 90% level. Thus, from the results in Table 5 we
can make the following observations.
First, the best-performing distributional similarity measures, in terms of WordNet
prediction, for both high- and low-frequency nouns, are the MI-based and the t-test
based CRMs. The additive MI-based CRM performs the best for high-frequency nouns
and the additive t-test based CRM performs the best for low-frequency nouns. How-
ever, the differences between these models are not statistically significant. These CRMs
perform substantially better than all of the unparameterized distributional similarity
measures, of which the best performing are simhind and simlin for high-frequency nouns
and dist?1 for low-frequency nouns. Second, the difference-weighted versions of each
model generally perform slightly worse than their additive counterparts. Thus, the
difference in extent to which each word occurs in each context does not appear to be
a factor in determining semantic similarity. Third, all of the measures perform signifi-
cantly better for high-frequency nouns than for low-frequency nouns. However, some of
the measures (simlin, simjacc and simdice) perform considerably worse for low-frequency
nouns.
We now consider the effects of ? and ? in the CRMs on performance. The pattern
of variation across the CRMs was very similar. This pattern is illustrated using one of
the best-performing CRMs (simaddmi ) in Figure 4. With reference to this figure and to the
results for the other models (not shown), we make the following observations.
464
Weeds and Weir Co-occurrence Retrieval
Figure 4
Variation in similarity with WordNet with respect to ? and ? for the additive MI-based CRM.
First, for high- and low-frequency nouns, similarity with WordNet is higher for
low values of ? than for high values of ?. In other words, neighbors according to
the WordNet based measure tend to have high-recall retrieval of the target noun?s
co-occurrences. Second, a high value of ? leads to high performance for high-frequency
nouns but poor performance for low-frequency nouns. This suggests that WordNet-
derived neighbors of high-frequency target nouns also have high-precision retrieval
of the target noun?s co-occurrences, whereas the WordNet-derived neighbors of low-
frequency target nouns do not. This also explains why particular existing measures
(Jaccard?s / the Dice Coefficient and Lin?s Measure), which are very similar to a ? = 1
setting in the CR framework, perform well for high-frequency nouns but poorly for
low-frequency nouns.
5.1.3 Discussion. Our results in this section are comparable to those of Curran and
Moens (2002), who showed that combining the t-test with Jaccard?s coefficient outper-
formed combining MI with Jaccard?s coefficient by approximately 10% in a comparison
against a gold-standard thesaurus. However, we do not find a significant difference
between using the t-test and MI in similarity calculation. Further, we found that using
a combination of precision and recall weighted towards recall performs substantially
better than using the harmonic mean (which is equivalent to Jaccard?s measure). In our
experiments, the development-set similarity using the harmonic mean in the additive
MI-based CRM was 0.312 for high-frequency nouns and 0.153 for low-frequency nouns,
and the development-set similarity using the harmonic mean in the additive t-test based
CRM was 0.294 for high-frequency nouns and 0.129 for low-frequency nouns.
5.2 Pseudo-Disambiguation Task
Pseudo-disambiguation tasks have become a standard evaluation technique (Gale,
Church, and Yarowsky 1992; Schu?tze 1992; Pereira, Tishby, and Lee 1993; Schu?tze 1998;
Lee 1999; Dagan, Lee, and Pereira 1999; Golding and Roth 1999; Rooth et al 1999; Even-
Zohar and Roth 2000; Lee 2001; Clark and Weir 2002) and, in the current setting, we
may use a noun?s neighbors to decide which of two co-occurrences is the most likely.
Although pseudo-disambiguation is an artificial task, it has relevance in at least two
application areas. First, by replacing occurrences of a particular word in a test suite with
465
Computational Linguistics Volume 31, Number 4
a pair of words from which a technique must choose, we recreate a simplified version
of the word sense disambiguation task; that is, we choose between a fixed number
of homonyms based on local context. The second is in language modeling where we
wish to estimate the probabilities of co-occurrences of events but, due to the sparse
data problem, it is often the case that a possible co-occurrence has not been seen in the
training data.
5.2.1 Experimental Set-up. A typical approach to performing pseudo-disambiguation
is as follows. A large set of noun-verb direct-object pairs is extracted from a corpus,
of which a portion is used as test data and another portion is used as training data.
The training data can be used to construct a language model and/or determine the
distributionally nearest neighbors of each noun. Noun-verb pairs (n, v1) in the test data
are replaced with noun-verb-verb triples (n, v1, v2) and the task is to decide which of the
two verbs is the most likely to take the noun as its direct object. Performance is usually
measured as error rate. We will now discuss the details of our own experimental set-up.
As already discussed (Section 3), 80% of the noun-verb direct-object data extracted
from the BNC for each of 2,000 nouns was used to compute the similarity between
nouns and is also used as the language model in the pseudo-disambiguation task, and
20% of the data was set aside as test data, providing unseen co-occurrences for this
pseudo-disambiguation task.
In order to construct the test set from the test data, we took all10 of the test data set
aside for each target noun and modified it as follows. We converted each noun-verb pair
(n, v1) in the test data into a noun-verb-verb triple (n, v1, v2). v2 was randomly selected
from the verbs that have the same frequency, calculated over all the training data, as
v1 plus or minus 1. If there are no other verbs within this frequency range, then the
test instance is discarded. This method ensures that there is no systematic bias towards
v2 being of a higher or lower frequency than v1. We also ensured that (n, v2) has not
been seen in the test or training data. Ten test instances11 were then selected for each
target noun in a two-step process of (1) while more than ten triples remained, discarding
duplicate triples and (2) randomly selecting ten triples from those remaining after
step 1. At this point, we have 10,000 test instances pertaining to high-frequency nouns
and 10,000 test instances pertaining to low-frequency nouns, and there are no biases
towards the higher-frequency or lower-frequency nouns within these sets. Each of these
sets was split into five disjoint subsets, each containing two instances for each target
noun. We use these five subsets in two ways. First, we perform five-fold cross validation.
In five-fold cross validation, we compute the optimal parameter settings in four of the
subsets and the error rate at this optimal parameter setting in the remaining subset. This
is repeated five times with a different subset held out each time. We then compute an
average optimal error rate. We cannot, however, compute an average optimal parameter
setting, since this would assume a convex relationship between parameter settings and
error rate. In order to study the relationship between parameter settings and error
rate, we combine three of the sets to form a development set and two of the sets to
form a test set. The development set is used to optimize parameters and the test set
10 Unlike Lee (1999), we do not delete instances from the test data that occur in the training data. This
is discussed in detail in (Weeds 2003), but our main justification for this approach is that a single
co-occurrence of (n, v1) compared to zero co-occurrences of (n, v2) is not necessarily sufficient evidence
to conclude that the population probability of (n, v1) is greater than that of (n, v2).
11 Ten being less than the minimum number (14) of (possibly) indistinct co-occurrences for any target noun
in the original test data.
466
Weeds and Weir Co-occurrence Retrieval
to determine error rates at the optimal settings. In graphs showing the relationship
between error rate and parameter settings, it is the error rate in this development set
that is shown. In the case of the CRMs, the parameters that are optimized are ?, ?, and
k (the number of nearest neighbors).12 For the existing measures, the only parameter
to be optimized is k.
Having constructed the test sets, the task is to take each test instance (n, v1, v2) and
use the nearest neighbors of noun n (as computed from the training data) to decide
which of (n, v1) and (n, v2) was the original co-occurrence. Each of n?s neighbors, m, is
given a vote that is equal to the difference in frequencies of the co-occurrences (m, v1)
and (m, v2) and that it casts to the verb with which it occurs most frequently. Thus,
we distinguish between cases where a neighbor occurs with each verb approximately
the same number of times and where a neighbor occurs with one verb significantly
more often than the other. The votes for each verb are summed over all of the k nearest
neighbors of n, and the verb with the most votes wins. Performance is measured as
error rate.
Error rate = 1T
(
# of incorrect choices + # of ties2
)
(51)
where T is the number of test instances and a tie results when the neighbors cannot
decide between the two alternatives.
5.2.2 Results. In this section, we present results on the pseudo-disambiguation task for
all of the CRMs described in Section 2. We also compare the results with the six existing
distributional similarity measures (Section 4) and the two WordNet-based measures
(Section 5.1).
A baseline for these experiments is the performance obtained by a technique that
backs-off to the unigram probabilities of the verbs being disambiguated. By construction
of the test set, this should be approximately 0.5. The actual empirical figures are 0.553
for the high-frequency noun test set and 0.586 for the low-frequency noun test set. The
deviation from 0.5 is due to the unigram probabilities of the verbs not being exactly
equal and to their being calculated over a larger data set than just the training data
for the 2,000 target nouns. These baseline error-rates are also different from what is
observed when all 1,999 potential neighbors are considered. In this case, we obtain
an error rate of 0.6885 for the high-frequency noun test set and 0.6178 for the low-
frequency noun test set. These differences are due to the fact that the correct choice
verb, but not the incorrect choice verb, has occurred, possibly many times, with the
target noun in the training data, but a noun is not considered as a potential neighbor of
itself.
The results are summarized in Table 6. The table gives the average optimal error
rates for each measure, and for high- and low-frequency nouns, calculated using five-
fold cross validation. For ease of comparison, the cross-validated average optimal error
rates are illustrated in Figure 5. Standard deviation in the mean optimal error rate across
the five folds was always less than 0.15 and thus differences greater than 0.028 are
significant at the 99% level and differences greater than 0.012 are significant at the 90%
level. From the results, we make the following observations.
12 We also experimented with optimizing a similarity threshold t, but found that optimizing k gave better
results (Weeds 2003).
467
Computational Linguistics Volume 31, Number 4
Table 6
Mean optimal error rates using five-fold cross-validation (when optimizing k, ? and ?).
Noun Frequency Measure Noun Frequency
Measure high low high low
simaddtype 0.196 0.197 sim
dw
type 0.214 0.185
simaddtok 0.219 0.241 sim
dw
tok 0.234 0.202
simaddmi 0.178 0.169 sim
dw
mi 0.187 0.176
simaddwmi 0.173 0.192 sim
dw
wmi 0.171 0.192
simaddt 0.154 0.172 sim
dw
t 0.163 0.186
simaddz 0.164 0.183 sim
dw
z 0.167 0.193
simaddallr 0.170 0.211 sim
dw
allr 0.171 0.215
simdice 0.215 0.204 simjacc 0.215 0.204
distL1 0.234 0.202 dist?1 0.230 0.192
simhind 0.201 0.18 simlin 0.193 0.181
wn simlin 0.295 0.294 wn distJC 0.302 0.295
baseline 0.553 0.586
First, the best measure appears to be the additive t-test based CRM. This signifi-
cantly outperforms all but one (the z-test based CRM) of the other measures for high-
frequency nouns. For low-frequency nouns, slightly higher performance is obtained
using the additive MI-based CRM. This difference, however, is not statistically signifi-
cant. Second, all of the distributional similarity measures perform considerably better
than the WordNet-based measures13 at this task for high- and low-frequency nouns.
Third, for many measures, performance over high-frequency nouns is not significantly
higher (and is in some cases lower) than over low-frequency nouns. This suggests that
distributional similarity can be used in language modeling even when there is relatively
little corpus data over which to calculate distributional similarity.
We now consider the effects of the different parameters on performance. Since we
use the development set to determine the optimal parameters, we consider performance
on the development set as each parameter is varied. Table 7 shows the optimized pa-
rameter settings in the development set, error rate at these settings in the development
set, and error rate at these settings in the test set. For the CRMs, we considered how the
performance varies with each parameter when the other parameters are held constant
at their optimum values. Figure 6 shows how performance varies with ?, and Figure 7
shows how performance varies with ? for the additive and difference-weighted t-test
based and MI-based CRMs. For reference, the optimal error rates for the best performing
existing distributional similarity measure (simlin) is also shown as a straight line on each
graph.
We do not show the variation with respect to k for any of the measures, but this was
fairly similar for all measures and is as would be expected. To begin with, considering
13 However, for this task, in contrast to earlier work, wn simlin gives slightly, although insignificantly, better
performance than wn distJC.
468
Weeds and Weir Co-occurrence Retrieval
Figure 5
Bar chart illustrating cross-validated optimal error rates for each measure when k is optimised.
more neighbors increases performance, since more neighbors allow decisions to be
made in a greater number of cases. However, when k increases beyond an optimal
value, a greater number of these decisions will be in the wrong direction, since these
words are not very similar to the target word, leading to a decrease in performance. In a
small number of cases (when using the ALLR-based CRMs or the WMI-based CRMs for
high frequency nouns), performance peaks at k = 1. This suggests that these measures
may be very good at finding a few very close neighbors.
The majority of models, including the additive t-test based and additive MI-based
CRMs, perform significantly better at low values of ? (0.25-0.5) and high values of ?
(around 0.8). This indicates that a potential neighbor with high-precision retrieval of
informative features is more useful than one with high-recall retrieval. In other words,
it seems that it is better to sacrifice being able to make decisions on every test instance
with a small number of neighbors in favor of not having neighbors that predict incorrect
verb co-occurrences. This also suggests why we saw fairly low performance by the ?-
skew divergence measure on this task, since it is closest to a high-recall setting in the
additive t-test based model. The low values of ? indicate that a combination of precision
and recall that is closer to a weighted arithmetic mean is generally better than one that
is closer to an unweighted harmonic mean. However, this does not hold for the t-test
based CRMs for low-frequency nouns. Here a higher value of ? is optimal, indicating
that, in this case, requiring both recall and precision results in high performance.
6. Conclusions and Future Directions
Our main contribution is the development of a framework, first presented in a prelim-
inary form in Weeds and Weir (2003b), that is based on the concept of lexical substi-
469
Computational Linguistics Volume 31, Number 4
Table 7
Summary of results on pseudo-disambiguation task when optimizing ?, ? and k.
Noun Frequency
high low
Optimal Devel. Test Optimal Devel. Test
Parameters Error Error Parameters Error Error
Measure ? ? k ? ? k
simaddtype 0.25 0.8 150 0.193 0.193 0.25 0.75 100 0.192 0.200
simaddtok 0 0.8 250 0.211 0.224 0.5 0.1 130 0.234 0.233
simaddmi 0.25 0.8 170 0.175 0.186 0.5 0.8 120 0.169 0.178
simaddwmi 0.0 1.0 1 0.175 0.169 0.75 0.0 100 0.183 0.182
simaddt 0.25 0.8 190 0.153 0.155 0.5 0.7 110 0.165 0.176
simaddz 0.25 0.7 40 0.165 0.163 0.5 1.0 250 0.174 0.188
simaddallr 0.0 0.9 1 0.170 0.169 0.25 0.6 90 0.204 0.210
simdwtype 0 0.6 50 0.208 0.215 0.25 0.3 190 0.177 0.188
simdwtok n/a n/a 60 0.227 0.234 n/a n/a 50 0.194 0.206
simdwmi 0.25 0.8 100 0.181 0.193 0.5 0.7 160 0.172 0.173
simdwwmi 0.0 0.0 1 0.172 0.170 0.25 0.1 450 0.183 0.190
simdwt 0.5 0.8 120 0.156 0.165 0.75 0.6 250 0.179 0.187
simdwz 0.5 0.7 50 0.166 0.171 0.75 0.9 400 0.187 0.199
simdwallr 0.0 0.9 1 0.171 0.169 0.5 1.0 180 0.208 0.212
simlin n/a n/a 50 0.190 0.199 n/a n/a 80 0.179 0.186
tutability. Here, we cast the problem of measuring distributional similarity as one of
co-occurrence retrieval (CR), for which we can measure precision and recall by analogy
with the way they are measured in document retrieval. This CR framework has then
allowed us to systematically explore various characteristics of distributional similarity
measures.
First, we asked whether lexical substitutability is necessarily symmetric. To this
end, we have explored the merits of symmetry and asymmetry in a similarity measure
by varying the relative importance attached to precision and recall. We have seen that
as the distribution of word B moves away from being identical to that of word A, its
similarity with A can decrease along one or both of two dimensions. When B occurs in
contexts that word A does not, precision is lost but B may remain a high-recall neighbor
of word A. When B does not occur in contexts that A does, recall is lost but B may remain
a high-precision neighbor of word A. Through our experimental work, which is more
thorough than that presented in Weeds and Weir (2003b), we have shown that the kind
of neighbor preferred appears to depend on the application in hand. High-precision
neighbors were more useful in the language modeling task of pseudo-disambiguation
and high-recall neighbors were more highly correlated with WordNet-derived neighbor
sets. Thus, similarity appears to be inherently asymmetric. Further, it would seem
470
Weeds and Weir Co-occurrence Retrieval
Figure 6
Performance of CRMs with respect to ? (at optimal values of k and ?).
unlikely that any single, unparameterized measure of distributional similarity would
be able to do better on both tasks.
Second, we asked whether all contexts are equally important in the calculation of
distributional similarity. To this end, we have explored the way in which frequency
information is utilized using different co-occurrence retrieval models (CRMs). Using
different weight functions, we have investigated the relative importance of different
co-occurrence types. In earlier work (Weeds and Weir 2003b), we saw that using MI
to weight features gave improved performance on the two evaluation tasks over type-
based or token-based CRMs. Here, we have seen that further gains can be made by using
the t-test as a weight function. This leads to significant improvements on the pseudo-
disambiguation task for all nouns and marginal improvements on the WordNet predic-
tion task for low-frequency nouns. To some extent, this supports the findings of Curran
and Moens (2002), who investigated a number of weight functions for distributional
similarity and showed that the t-test performed better than a number of other weight
functions including MI.
Third, we asked whether it is necessary to consider the difference in extent to which
each word appears in each context. To this end, we have herein proposed difference-
Figure 7
Performance of CRMs with respect to ? (at optimal values of k and ?).
471
Computational Linguistics Volume 31, Number 4
weighted versions of each model in which the similarity of two words in respect
of an individual feature is defined using the same principles that we use to define
the similarity of two words in respect of all their features. We have compared these
difference-weighted CRMs to their additive counterparts and shown that difference-
weighting does not seem to be a major factor and does not improve results when using
the best-performing CRMs.
Another important contribution of this work on co-occurrence retrieval is a better
understanding of existing distributional similarity measures. By comparing existing
measures with the CR framework, we can analyze their CR characteristics. As discussed
in Weeds and Weir (2003b), the Dice Coefficient and Jaccard?s Coefficient are exactly
simulated by ? = 1 in the additive type-based model and Lin?s Measure is almost
equivalent to the harmonic mean of precision and recall in the additive MI-based
model. Here, we also show that the L1 Norm is exactly simulated by the (unparame-
terized) difference-weighted token-based model, Hindle?s Measure is exactly simulated
by ? = 0,? = 0 in the additive MI-based model, and the ?-skew divergence measure
is most similar to high-recall settings in the additive t-test based CRM. Knowing that
Lin?s Measure is almost equivalent to the harmonic mean of precision and recall in
the additive MI-based model explains why this measure does badly on the WordNet
prediction task for low-frequency nouns. We have seen that recall is more important
than precision in the WordNet prediction task, whereas the nearest neighbors of a target
noun according to Lin?s Measure have both high precision and high recall. Conversely,
knowing that the ?-skew divergence measure is most closely approximated by high-
recall settings in the additive t-test based model explains why this measure performs
poorly on the pseudo-disambiguation task, since we have seen that high precision is
required for optimal performance on this task.
Finally, our evaluation of measures has been performed over a set of 2,000 nouns,
and we have shown that the performance of distributional similarity techniques for
low-frequency nouns is not significantly lower than for high-frequency nouns. This
suggests that distributional techniques might be used even when there is relatively little
data available. In the distributional domain, this means that we can use probability
estimation techniques for rare words with greater confidence. In the semantic domain,
we might be able to use distributional techniques to extend existing semantic resources
to cover rare or new words or automatically generate domain-, genre-, or dialect-specific
resources.
There are a number of major directions in which this work can be extended. First,
although the set of CRMs defined here is more extensive than that defined in Weeds and
Weir (2003b), it is still not exhaustive, and other models might be proposed. Further,
it would be interesting to combine CRMs with the feature reweighting scheme of
Geffet and Dagan (2004). These authors compare distributional similarity scores with
human judgments of semantic entailment and show that substantial (approximately
10%) improvements over using Lin?s Measure can be achieved by first calculating
similarity using Lin?s Measure and then recalculating similarity using a relative feature
focus score, which indicates how many of a word?s nearest neighbors shared that
feature.
Second, there are other potential application-based tasks that could be used to
evaluate CRMs and distributional similarity methods in general. In particular, we see
potential for the use of distributional similarity methods in prepositional phrase attach-
ment ambiguity resolution. This task has been previously tackled using semantic classes
to predict what is ultimately distributional information. Accordingly, we believe that it
should be possible to do better using the CR framework.
472
Weeds and Weir Co-occurrence Retrieval
Finally, in order to be able to truly rival manually generated thesauri, distri-
butional techniques need to be able to distinguish between different semantic relations
such as synonymy, antonymy, and hyponymy. These are important linguistic distinc-
tions, particularly in the semantic domain, since we are unlikely, say, to want to replace
a word with its antonym. Weeds, Weir, and McCarthy (2004) give preliminary results on
the the use of precision and recall to distinguish between hypernyms and hyponyms in
sets of distributionally related words.
Acknowledgments
This research was supported by an
Engineering and Physical Sciences Research
Council (EPSRC) studentship to the first
author. The authors would like to thank John
Carroll, Mirella Lapata, Adam Kilgarriff, Bill
Keller, Steve Clark, James Curran, Darren
Pearce, Diana McCarthy, and Mark
McLauchlan for helpful discussions and
insightful comments throughout the course
of the research. We would also like to thank
the anonymous reviewers of this paper for
their comments and suggestions.
References
Bernard, John R. L., editor. 1990. The
Macquarie Encyclopedic Thesaurus. The
Macquarie Library, Sydney, Australia.
Briscoe, Edward and John Carroll. 1995.
Developing and evaluating a probabilistic
LR parser of part-of-speech and
punctuation labels. In Proceedings
of the 4th ACL/SIGDAT International
Workshop on Parsing Technologies,
pages 48?58, Cambridge, MA.
Brown, Peter F., Vincent J. Della Pietra, Peter
V. deSouza, Jenifer C. Lai, and Robert L.
Mercer. 1992. Class-based n-gram models
of natural language. Computational
Linguistics, 18(4):467?479.
Budanitsky, Alexander. 1999. Lexical Semantic
Relatedness and its Application in Natural
Language Processing. Ph.D. thesis,
University of Toronto, Ontario.
Budanitsky, Alexander and Graeme Hirst.
2001. Semantic distance in WordNet: An
experimental, application-oriented
evaluation of five measures. In Proceedings
of the NAACL-01 Workshop on WordNet
and Other Lexical Resources, Pittsburgh, PA.
Caraballo, Sharon. 1999. Automatic
construction of a hypernym-labelled noun
hierarchy from text. In Proceedings of the
37th Annual Meeting of the Association for
Computational Linguistics (ACL-99),
pages 120?126, College Park, MA.
Carroll, John and Edward Briscoe. 1996.
Apportioning development effort in a
probabilistic LR parsing system through
evaluation. In Proceedings of the ACL/
SIGDAT Conference on Empirical Methods in
Natural Language Processing (EMNLP96),
pages 92?100, Santa Cruz, CA.
Church, Kenneth W., William Gale, Patrick
Hanks, Donald Hindle, and Rosamund
Moon. 1994. Lexical substitutability.
In B. T. S. Atkins and A. Zampolli, editors.
Computational Approaches to the Lexicon.
Oxford University Press, Oxford,
pages 153?177.
Church, Kenneth W. and Patrick Hanks.
1989. Word association norms, mutual
information and lexicography. In
Proceedings of the 27th Annual Conference
of the Association for Computational
Linguistics (ACL-89), pages 76?82,
Vancouver, British Columbia.
Clark, Stephen and David Weir. 2000.
A class-based probabilistic approach to
structural disambiguation. In
Proceedings of the 18th International
Conference on Computational Linguistics
(COLING-00), pages 194?200, Saarbrucken,
Germany.
Clark, Stephen and David Weir. 2002.
Class-based probability estimation using
a semantic hierarchy. Computational
Linguistics, 28(2):187?206.
Cover, T. M. and J. A. Thomas. 1991.
Elements of Information Theory. Wiley,
New York.
Curran, James R. and Marc Moens. 2002.
Improvements in automatic thesaurus
extraction. In Proceedings of the
ACL-SIGLEX Workshop on Unsupervised
Lexical Acquisition, pages 59?67,
Philadelphia, PA.
Dagan, Ido, Lillian Lee, and Fernando
Pereira. 1999. Similarity-based models of
word co-occurrence probabilities. Machine
Learning Journal, 34(1?3):43?69.
Dagan, Ido, S. Marcus, and S. Markovitch.
1993. Contextual word similarity and
estimation from sparse data. In Proceedings
of the 35th Annual Meeting of the Association
for Computational Linguistics (ACL-93),
pages 56?63, Columbus, OH.
Even-Zohar, Yair and Dan Roth. 2000. A
classification approach to word prediction.
473
Computational Linguistics Volume 31, Number 4
In Proceedings of the 1st Conference of the
North American Chapter of the Association for
Computational Linguistics (NAACL-00),
pages 124?131, Pittsburg, PA.
Fellbaum, Christiane, editor. 1998. WordNet:
An Electronic Lexical Database. MIT Press,
Cambridge, MA.
Firth, John Rupert. 1957. A synopsis of
linguistic theory 1930-1955. In Studies in
Linguistic Analysis, pages 1?32, Philogical
Society, Oxford. Reprinted in Palmer, F.
(ed.), 1968 Selected Papers of J. R. Firth,
Longman, Harlow.
Fontenelle, Thierry, Walter Bruls, Luc
Thomas, Tom Vanallemeersch, and Jacques
Jansen. 1994. DECIDE, MLAP-Project
93-19, deliverable D-1a: a survey of
collocation extraction tools. Technical
report, University of Liege, Belgium.
Frakes, W. B. and R. Baeza-Yates, editors.
1992. Information Retrieval, Data Structures
and Algorithms. Prentice Hall, New York.
Fung, Pascale and Kathleen McKeown. 1997.
A technical word- and term-translation aid
using noisy parallel corpora across
language groups. Machine Translation,
12(1?2):53?87.
Gale, William, Kenneth W. Church, and
David Yarowsky. 1992. Work on statistical
methods for word sense disambiguation.
In Working notes of the AAAI symposium on
probabilistic approaches to natural language,
pages 54?60, Menlo Park, CA.
Geffet, Maayan and Ido Dagan. 2004. Feature
vector quality and distributional similarity.
In Proceedings of the 20th International
Conference on Computational Linguistics
(COLING-04), pages 247?253, Geneva,
Switzerland.
Golding, Andrew R. and Dan Roth. 1999. A
winnow-based approach to context-
sensitive spelling correction. Machine
Learning, 34(1?3):182?190.
Grefenstette, Gregory. 1994. Corpus-derived
first-, second- and third-order word
affinities. In Proceedings of Euralex,
pages 279?290, Amsterdam, Holland.
Harris, Zelig S. 1968. Mathematical Structures
of Language. John Wiley, New York.
Hindle, Donald. 1990. Noun classification
from predicate-argument structures. In
Proceedings of the 28th Annual Meeting of the
Association for Computational Linguistics
(ACL-1990), pages 268?275, Pittsburgh, PA.
Hindle, Donald and Mats Rooth. 1993.
Structural ambiguity and lexical relations.
Computational Linguistics, 19(1):103?120.
Jiang, Jay J. and David W. Conrath. 1997.
Semantic similarity based on corpus
statistics and lexical taxonomy. In
Proceedings of the International Conference on
Research in Computational Linguistics
(ROCLING X), Taiwan.
Kaufman, Leonard and Peter J. Rousseeuw.
1990. Finding Groups in Data: An
Introduction to Cluster Analysis. John Wiley,
New York.
Kilgarriff, Adam. 2003. Thesauruses for
natural language processing. In Proceedings
of the Joint Conference on Natural Language
Processing and Knowledge Engineering,
pages 5?13, Beijing, China.
Kilgarriff, Adam and Colin Yallop. 2000.
What?s in a thesaurus. In Second Conference
on Language Resources and Evaluation
(LREC-00), pages 1371?1379, Athens.
Kullback, S. and R.A. Leibler. 1951. On
information and sufficiency. Annals of
Mathematical Statistics, 22:79?86.
Lee, Lillian. 1999. Measures of distributional
similarity. In Proceedings of the 37th Annual
Meeting of the Association for Computational
Linguistics (ACL-1999), pages 23?32,
College Park, MA.
Lee, Lillian. 2001. On the effectiveness of the
skew divergence for statistical language
analysis. Artificial Intelligence and Statistics,
pages 65?72.
Li, Hang. 2002. Word clustering and
disambiguation based on co-occurrence
data. Natural Language Engineering,
8(1):25?42.
Li, Hang and Naoki Abe. 1998. Generalizing
case frames using a thesaurus and the
MDL principle. Computational Linguistics,
24(2):217?244.
Lin, Dekang. 1997. Using syntactic
dependency as local context to resolve
word sense ambiguity. In Proceedings of the
35th Annual Meeting of the Association for
Computational Linguistics and 8th Conference
of the European Chapter of the Association for
Computational Linguistics (ACL-97),
pages 64?71, Madrid, Spain.
Lin, Dekang. 1998a. Automatic retrieval
and clustering of similar words. In
Proceedings of the 36th Annual Meeting
of the Association for Computational
Linguistics and the 17th International
Conference on Computational Linguistics
(COLING-ACL ?98), pages 768?774,
Montreal, Quebec.
Lin, Dekang. 1998b. An information-theoretic
definition of similarity. In Proceedings of
International Conference on Machine
Learning, Madison, WI.
Lin, Dekang, Shaojun Zhao, Lijuan Qin, and
Ming Zhou. 2003. Identifying synonyms
474
Weeds and Weir Co-occurrence Retrieval
among distributionally similar words. In
In Proceedings of the 18th International Joint
Conference on Artificial Intelligence
(IJCAI-03), pages 1492?1493.
Manning, Christopher D. and Hinrich
Schu?tze. 1999. Foundations of Statistical
Natural Language Processing. MIT Press,
Cambridge, MA.
McCarthy, Diana, Rob Koeling, and Julie
Weeds. 2004. Ranking WordNet senses.
Technical Report 569, Department of
Informatics, University of Sussex,
Brighton.
McCarthy, Diana, Rob Koeling, Julie Weeds,
and John Carroll. 2004. Finding
predominant word senses in untagged
text. In Proceedings of the 42nd Annual
Meeting of the Association for Computational
Linguistics (ACL-04), pages 280?287,
Barcelona, Spain.
Miller, G., M. Chodorow, S. Landes, C. Leacock,
and R. Thomas. 1994. Using a semantic
concordance for sense identification. In
Proceedings of the ARPA Human Language
Technology Workshop, Plainsboro, NJ.
Pantel, Patrick and Dekang Lin. 2000.
Word-for-word glossing of contextually
similar words. In Proceedings of the
Conference on Applied Natural Language
Processing / 1st Meeting of the North
American Chapter of the Association
for Computational Linguistics (ANLP-
NAACL-00), pages 78?85, Seattle, WA.
Patwardhan, Siddharth, Satanjeev Banerjee,
and Ted Pedersen. 2003. Using measures of
semantic relatedness for word sense
disambiguation. In Proceedings of the 4th
International Conference on Intelligent Text
Processing and Computational Linguistics,
pages 241?257, Mexico City.
Pereira, Fernando, Naftali Tishby, and Lillian
Lee. 1993. Distributional clustering of
similar words. In Proceedings of the 30th
Annual Meeting of the Association for
Computational Linguistics (ACL-93),
pages 183?190, Columbus, OH.
Pustejovsky, James. 1995. The generative
lexicon. MIT Press, Cambridge, MA.
Resnik, Philip. 1993. Selection and
Information: A Class-Based Approach
to Lexical Relationships. Ph.D. thesis,
University of Pennsylvania,
Philadelphia, PA.
Roget, Peter. 1911. Thesaurus of English
Words and Phrases. Longmans, Green and
Co., London.
Rooth, Mats, Stefan Riezler, Detlef Prescher,
Glenn Carroll, and Franz Beil. 1999.
Inducing a semantically annotated
lexicon via EM-based clustering. In
Proceedings of the 37th Annual Meeting of the
Association for Computational Linguistics
(ACL-99), pages 104?111, College Park,
MA.
Salton, Gerald and M. J. McGill. 1983.
Introduction to Modern Information Retrieval.
McGraw-Hill, New York.
Schu?tze, Hinrich. 1992. Dimensions of
meaning. In Proceedings of Conference on
Supercomputing, pages 787?796,
Minneapolis, MN.
Schu?tze, Hinrich. 1998. Automatic word
sense discrimination. Computational
Linguistics, 24(1):97?124.
van Rijsbergen, C. J. 1979. Information
Retrieval, second edition. Butterworths,
London.
Weeds, Julie. 2003. Measures and Applications
of Lexical Distributional Similarity. Ph.D.
thesis, University of Sussex, Brighton.
Weeds, Julie and David Weir. 2003a.
Finding and evaluating sets of nearest
neighbours. In Proceedings of the 2nd
International Conference on Corpus
Linguistics, pages 879?888,
Lancaster, UK.
Weeds, Julie and David Weir. 2003b. A
general framework for distributional
similarity. In Proceedings of the Conference on
Empirical Methods in Natural Language
Processing (EMNLP-03), pages 81?88,
Sapporo, Japan.
Weeds, Julie, David Weir, and Diana
McCarthy. 2004. Characterising measures
of lexical distributional similarity. In
Proceedings of the 20th International
Conference on Computational Linguistics
(COLING-04), pages 1015?1021, Geneva,
Switzerland.
Xu, Jinxi and Bruce Croft. 1996. Query
expansion using local and global
disambiguation. In Proceedings of the 19th
Annual International ACM SIGIR Conference
on Research and Development in Information
Retrieval (SIGIR-96), pages 4?11, Zurich,
Switzerland.
475

Finding Predominant Word Senses in Untagged Text
Diana McCarthy & Rob Koeling & Julie Weeds & John Carroll
Department of Informatics,
University of Sussex
Brighton BN1 9QH, UK
 
dianam,robk,juliewe,johnca  @sussex.ac.uk
Abstract
In word sense disambiguation (WSD), the heuristic
of choosing the most common sense is extremely
powerful because the distribution of the senses of a
word is often skewed. The problem with using the
predominant, or first sense heuristic, aside from the
fact that it does not take surrounding context into
account, is that it assumes some quantity of hand-
tagged data. Whilst there are a few hand-tagged
corpora available for some languages, one would
expect the frequency distribution of the senses of
words, particularly topical words, to depend on the
genre and domain of the text under consideration.
We present work on the use of a thesaurus acquired
from raw textual corpora and the WordNet similar-
ity package to find predominant noun senses auto-
matically. The acquired predominant senses give a
precision of 64% on the nouns of the SENSEVAL-
2 English all-words task. This is a very promising
result given that our method does not require any
hand-tagged text, such as SemCor. Furthermore,
we demonstrate that our method discovers appropri-
ate predominant senses for words from two domain-
specific corpora.
1 Introduction
The first sense heuristic which is often used as a
baseline for supervised WSD systems outperforms
many of these systems which take surrounding con-
text into account. This is shown by the results of
the English all-words task in SENSEVAL-2 (Cot-
ton et al, 1998) in figure 1 below, where the first
sense is that listed in WordNet for the PoS given
by the Penn TreeBank (Palmer et al, 2001). The
senses in WordNet are ordered according to the fre-
quency data in the manually tagged resource Sem-
Cor (Miller et al, 1993). Senses that have not oc-
curred in SemCor are ordered arbitrarily and af-
ter those senses of the word that have occurred.
The figure distinguishes systems which make use
of hand-tagged data (using HTD) such as SemCor,
from those that do not (without HTD). The high per-
formance of the first sense baseline is due to the
skewed frequency distribution of word senses. Even
systems which show superior performance to this
heuristic often make use of the heuristic where ev-
idence from the context is not sufficient (Hoste et
al., 2001). Whilst a first sense heuristic based on a
sense-tagged corpus such as SemCor is clearly use-
ful, there is a strong case for obtaining a first, or pre-
dominant, sense from untagged corpus data so that
a WSD system can be tuned to the genre or domain
at hand.
SemCor comprises a relatively small sample of
250,000 words. There are words where the first
sense in WordNet is counter-intuitive, because of
the size of the corpus, and because where the fre-
quency data does not indicate a first sense, the or-
dering is arbitrary. For example the first sense of
tiger in WordNet is audacious person whereas one
might expect that carnivorous animal is a more
common usage. There are only a couple of instances
of tiger within SemCor. Another example is em-
bryo, which does not occur at all in SemCor and
the first sense is listed as rudimentary plant rather
than the anticipated fertilised egg meaning. We be-
lieve that an automatic means of finding a predomi-
nant sense would be useful for systems that use it as
a means of backing-off (Wilks and Stevenson, 1998;
Hoste et al, 2001) and for systems that use it in lex-
ical acquisition (McCarthy, 1997; Merlo and Ley-
bold, 2001; Korhonen, 2002) because of the limited
size of hand-tagged resources. More importantly,
when working within a specific domain one would
wish to tune the first sense heuristic to the domain at
hand. The first sense of star in SemCor is celestial
body, however, if one were disambiguating popular
news celebrity would be preferred.
Assuming that one had an accurate WSD system
then one could obtain frequency counts for senses
and rank them with these counts. However, the most
accurate WSD systems are those which require man-
ually sense tagged data in the first place, and their
accuracy depends on the quantity of training exam-
ples (Yarowsky and Florian, 2002) available. We
020
40
60
80
100
0 20 40 60 80 100
re
ca
ll

precision
First Sense
"using HTD" "without HTD" "First Sense"
Figure 1: The first sense heuristic compared with
the SENSEVAL-2 English all-words task results
are therefore investigating a method of automati-
cally ranking WordNet senses from raw text.
Many researchers are developing thesauruses
from automatically parsed data. In these each tar-
get word is entered with an ordered list of ?near-
est neighbours?. The neighbours are words ordered
in terms of the ?distributional similarity? that they
have with the target. Distributional similarity is
a measure indicating the degree that two words, a
word and its neighbour, occur in similar contexts.
From inspection, one can see that the ordered neigh-
bours of such a thesaurus relate to the different
senses of the target word. For example, the neigh-
bours of star in a dependency-based thesaurus pro-
vided by Lin 1 has the ordered list of neighbours:
superstar, player, teammate, actor early in the list,
but one can also see words that are related to another
sense of star e.g. galaxy, sun, world and planet fur-
ther down the list. We expect that the quantity and
similarity of the neighbours pertaining to different
senses will reflect the dominance of the sense to
which they pertain. This is because there will be
more relational data for the more prevalent senses
compared to the less frequent senses. In this pa-
per we describe and evaluate a method for ranking
senses of nouns to obtain the predominant sense of
a word using the neighbours from automatically ac-
quired thesauruses. The neighbours for a word in a
thesaurus are words themselves, rather than senses.
In order to associate the neighbours with senses we
make use of another notion of similarity, ?semantic
similarity?, which exists between senses, rather than
words. We experiment with several WordNet Sim-
ilarity measures (Patwardhan and Pedersen, 2003)
which aim to capture semantic relatedness within
1Available at
http://www.cs.ualberta.ca/?lindek/demos/depsim.htm
the WordNet hierarchy. We use WordNet as our
sense inventory for this work.
The paper is structured as follows. We discuss
our method in the following section. Sections 3 and
4 concern experiments using predominant senses
from the BNC evaluated against the data in SemCor
and the SENSEVAL-2 English all-words task respec-
tively. In section 5 we present results of the method
on two domain specific sections of the Reuters cor-
pus for a sample of words. We describe some re-
lated work in section 6 and conclude in section 7.
2 Method
In order to find the predominant sense of a target
word we use a thesaurus acquired from automati-
cally parsed text based on the method of Lin (1998).
This provides the  nearest neighbours to each tar-
get word, along with the distributional similarity
score between the target word and its neighbour. We
then use the WordNet similarity package (Patward-
han and Pedersen, 2003) to give us a semantic simi-
larity measure (hereafter referred to as the WordNet
similarity measure) to weight the contribution that
each neighbour makes to the various senses of the
target word.
To find the first sense of a word (  ) we
take each sense in turn and obtain a score re-
flecting the prevalence which is used for rank-
ing. Let   	


 be the ordered
set of the top scoring  neighbours of  from
the thesaurus with associated distributional similar-
ity scores 	ff  
	
	
Using Automatically Acquired Predominant Senses for Word Sense
Disambiguation
Diana McCarthy & Rob Koeling & Julie Weeds & John Carroll
Department of Informatics,
University of Sussex
Brighton BN1 9QH, UK
 
dianam,robk,juliewe,johnca  @sussex.ac.uk
Abstract
In word sense disambiguation (WSD), the heuristic
of choosing the most common sense is extremely
powerful because the distribution of the senses of a
word is often skewed. The first (or predominant)
sense heuristic assumes the availability of hand-
tagged data. Whilst there are hand-tagged corpora
available for some languages, these are relatively
small in size and many word forms either do not
occur, or occur infrequently. In this paper we in-
vestigate the performance of an unsupervised first
sense heuristic where predominant senses are ac-
quired automatically from raw text. We evaluate on
both the SENSEVAL-2 and SENSEVAL-3 English all-
words data. For accurate WSD the first sense heuris-
tic should be used only as a back-off, where the evi-
dence from the context is not strong enough. In this
paper however, we examine the performance of the
automatically acquired first sense in isolation since
it turned out that the first sense taken from SemCor
outperformed many systems in SENSEVAL-2.
1 Introduction
The first sense heuristic which is often used as a
baseline for supervised WSD systems outperforms
many of these systems which take surrounding con-
text into account (McCarthy et al, 2004). The high
performance of the first sense baseline is due to the
skewed frequency distribution of word senses. Even
systems which show superior performance to this
heuristic often make use of the heuristic where evi-
dence from the context is not sufficient (Hoste et al,
2001).
The first sense heuristic is a powerful one. Us-
ing the first sense listed in SemCor on the SENSE-
VAL-2 English all-words data we obtained the re-
sults given in table 1, (where the PoS was given by
the gold-standard data in the SENSEVAL-2 data it-
self). 1 Recall is lower than precision because there
are many words which do not occur in SemCor. Use
1We did not include items which were tagged ?U?
(unassignable) by the human annotators.
PoS precision recall baseline
Noun 70 60 45
Verb 48 44 22
Adjective 71 59 44
Adverb 83 79 59
All PoS 67 59 41
Table 1: The SemCor first sense on the SENSEVAL-
2 English all-words data
of the first sense listed in WordNet gives 65% pre-
cision and recall for all PoS on these items. The
fourth column on table 1 gives the random base-
line which reflects the polysemy of the data. Ta-
ble 2 shows results obtained when we use the most
common sense for an item and PoS using the fre-
quency in the SENSEVAL-2 English all-words data
itself. Recall is lower than precision since we only
use the heuristic on lemmas which have occurred
more than once and where there is one sense which
has a greater frequency than the others, apart from
trivial monosemous cases. 2 Precision is higher in
table 2 than in table 1 reflecting the difference be-
tween an a priori first sense determined by Sem-
Cor, and an upper bound on the performance of this
heuristic for this data. This upper bound is quite
high because of the very skewed sense distributions
in the test data itself. The upper bound for a docu-
ment, or document collection, will depend on how
homogenous the content of that document collec-
tion is, and the skew of the word sense distributions
therein. Indeed, the bias towards one sense for a
given word in a given document or discourse was
observed by Gale et al (1992).
Whilst a first sense heuristic based on a sense-
tagged corpus such as SemCor is clearly useful,
there is a case for obtaining a first, or predomi-
nant, sense from untagged corpus data so that a WSD
2If we include polysemous items that have only occurred
once in the data we obtain a precision of 92% and a recall of
85% over all PoS.
                                             Association for Computational Linguistics
                        for the Semantic Analysis of Text, Barcelona, Spain, July 2004
                 SENSEVAL-3: Third International Workshop on the Evaluation of Systems
PoS precision recall baseline
Noun 95 73 45
Verb 79 43 22
Adjective 88 59 44
Adverb 91 72 59
All PoS 90 63 41
Table 2: The SENSEVAL-2 first sense on the SEN-
SEVAL-2 English all-words data
system can be tuned to a given genre or domain
(McCarthy et al, 2004) and also because there will
be words that occur with insufficient frequency in
the hand-tagged resources available. SemCor com-
prises a relatively small sample of 250,000 words.
There are words where the first sense in WordNet is
counter-intuitive, because this is a small sample, and
because where the frequency data does not indicate
a first sense, the ordering is arbitrary. For exam-
ple the first sense of tiger in WordNet is audacious
person whereas one might expect that carnivorous
animal is a more common usage.
Assuming that one had an accurate WSD system
then one could obtain frequency counts for senses
and rank them with these counts. However, the most
accurate WSD systems are those which require man-
ually sense tagged data in the first place, and their
accuracy depends on the quantity of training exam-
ples (Yarowsky and Florian, 2002) available. We
are investigating a method of automatically ranking
WordNet senses from raw text, with no reliance on
manually sense-tagged data such as that in SemCor.
The paper is structured as follows. We discuss
our method in the following section. Section 3 de-
scribes an experiment using predominant senses ac-
quired from the BNC evaluated on the SENSEVAL-2
English all-words task. In section 4 we present our
results on the SENSEVAL-3 English all-words task.
We discuss related work in section 5 and conclude
in section 6.
2 Method
The method is described in (McCarthy et al, 2004),
which we summarise here. We acquire thesauruses
for nouns, verbs, adjectives and adverbs based on
the method proposed by Lin (1998) using grammat-
ical relations output from the RASP parser (Briscoe
and Carroll, 2002). The grammatical contexts used
are listed in table 3, but there is scope for extending
or restricting the contexts for a given PoS.
We use the thesauruses for ranking the senses of
the target words. Each target word (  ) e.g. plant
in the thesaurus is associated with a list of nearest
PoS grammatical contexts
Noun verb in direct object or subject relation
adjective or noun modifier
Verb noun as direct object or subject
Adjective modified noun, modifing adverb
Adverb modified adjective or verb
Table 3: Grammatical contexts used for acquiring
the thesauruses
neighbours ( 	
 ) with distributional similarity
scores (  ) e.g. factory 0.28, refinery 0.17,
tree 0.14 etc... 3 Distributional similarity is a mea-
sure indicating the degree that two words, a word
and its neighbour, occur in similar contexts. The
neighbours reflect the various senses of the word
( Proceedings of the ACL Workshop on Empirical Modeling of Semantic Equivalence and Entailment, pages 7?12,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
The Distributional Similarity of Sub-Parses
Julie Weeds, David Weir and Bill Keller
Department of Informatics
University of Sussex
Brighton, BN1 9QH, UK
{juliewe, davidw, billk}@sussex.ac.uk
Abstract
This work explores computing distribu-
tional similarity between sub-parses, i.e.,
fragments of a parse tree, as an extension
to general lexical distributional similarity
techniques. In the same way that lexical
distributional similarity is used to estimate
lexical semantic similarity, we propose us-
ing distributional similarity between sub-
parses to estimate the semantic similarity of
phrases. Such a technique will allow us to
identify paraphrases where the component
words are not semantically similar. We
demonstrate the potential of the method by
applying it to a small number of examples
and showing that the paraphrases are more
similar than the non-paraphrases.
1 Introduction
An expression is said to textually entail another ex-
pression if the meaning of the second expression can
be inferred from the meaning of the first. For exam-
ple, the sentence ?London is an English city,? tex-
tually entails the sentence ?London is in England.?
As discussed by Dagan et al (2005) in their intro-
duction to the first Recognising Textual Entailment
Challenge, identifying textual entailment can be seen
as a subtask of a variety of other natural language
processing (NLP) tasks. For example, Question An-
swering (QA) can be cast as finding an answer which
is entailed by the proposition in the question. Other
identified tasks include summarization, paraphras-
ing, Information Extraction (IE), Information Re-
trieval (IR) and Machine Translation (MT).
The Natural Habitats (NatHab) project1 (Weeds
et al, 2004; Owen et al, 2005) provides an inter-
esting setting in which to study paraphrase and tex-
1http://www.informatics.susx.ac.uk/projects/nathab/
tual entailment recognition as a tool for natural lan-
guage understanding. The aim of the project is to
enable non-technical users to configure their perva-
sive computing environments. They do this by stat-
ing policies in natural language which describe how
they wish their environment to behave. For exam-
ple, a user, who wishes to restrict the use of their
colour printer to the printing of colour documents,
might have as a policy, ?Never print black-and-white
documents on my colour printer.? Similarly, a user,
who wishes to be alerted by email when their mobile
phone battery is low, might have as a policy, ?If my
mobile phone battery is low then send me an email.?
The natural language understanding task is to in-
terpret the user?s utterance with reference to a set
of policy templates and an ontology of services (e.g.
print) and concepts (e.g. document). The use of pol-
icy templates and an ontology restricts the number of
possible meanings that a user can express. However,
there is still considerable variability in the way these
policies can be expressed. Simple variations on the
theme of the second policy above include, ?Send me
an email whenever my mobile phone battery is low,?
and ?If the charge on my mobile phone is low then
email me.? Our approach is to tackle the interpreta-
tion problem by identifying parts of expressions that
are paraphrases of those expressions whose interpre-
tation with respect to the ontology is more directly
encoded. Here, we investigate extending distribu-
tional similarity methods from words to sub-parses.
The rest of this paper is organised as follows. In
Section 2 we discuss the background to our work.
We consider the limitations of an approach based on
lexical similarity and syntactic templates, which mo-
tivates us to look directly at the similarity of larger
units. In Section 3, we introduce our proposed ap-
proach, which is to measure the distributional simi-
larity of sub-parses. In Section 4, we consider exam-
ples from the Pascal Textual Entailment Challenge
7
Datasets2 (Dagan et al, 2005) and demonstrate em-
pirically how similarity can be found between corre-
sponding phrases when parts of the phrases cannot
be said to be similar. In Section 5, we present our
conclusions and directions for further work.
2 Background
One well-studied approach to the identification of
paraphrases is to employ a lexical similarity func-
tion. As noted by Barzilay and Elhadad (2003), even
a lexical function that simply computes word over-
lap can accurately select paraphrases. The prob-
lem with such a function is not in the accuracy of
the paraphrases selected, but in its low recall. One
popular way of improving recall is to relax the re-
quirement for words in each sentence to be identi-
cal in form, to being identical or similar in mean-
ing. Methods to find the semantic similarity of two
words can be broadly split into those which use lex-
ical resources, e.g., WordNet (Fellbaum, 1998), and
those which use a distributional similarity measure
(see Weeds (2003) for a review of distributional sim-
ilarity measures). Both Jijkoun and deRijke (2005)
and Herrara et al (2005) show how such a measure
of lexical semantic similarity might be incorporated
into a system for recognising textual entailment be-
tween sentences.
Previous work on the NatHab project (Weeds et
al., 2004) used such an approach to extend lexi-
cal coverage. Each of the user?s uttered words was
mapped to a set of candidate words in a core lexicon3,
identified using a measure of distributional similar-
ity. For example, the word send is used when talk-
ing about printing or about emailing, and a good
measure of lexical similarity would identify both of
these conceptual services as candidates. The best
choice of candidate was then chosen by optimising
the match between grammatical dependency rela-
tions and paths in the ontology over the entire sen-
tence. For example, an indirect-object relation be-
tween the verb send and a printer can be mapped to
the path in the ontology relating a print request to
its target printer.
As well as lexical variation, our previous work
(Weeds et al, 2004) allowed a certain amount of
syntactic variation via its use of grammatical depen-
dencies and policy templates. For example, the pas-
sive ?paraphrase? of a sentence can be identified by
comparing the sets of grammatical dependency rela-
tions produced by a shallow parser such as the RASP
2http://www.pascal-network.org/Challenges/RTE/
3The core lexicon lists a canonical word form for each
concept in the ontology.
parser (Briscoe and Carroll, 1995). In other words,
by looking at grammatical dependency relations, we
can identify that ?John is liked by Mary,? is a para-
phrase of ?Mary likes John,? and not of ?John likes
Mary.? Further, where there is a limited number of
styles of sentence, we can manually identify and list
other templates for matches over the trees or sets of
dependency relations. For example, ?If C1 then C2?
is the same as ?C2 if C1?.
However, the limitations of this approach, which
combines lexical variation, grammatical dependency
relations and template matching, become increas-
ingly obvious as one tries to scale up. As noted by
Herrera (2005), similarity at the word level is not
required for similarity at the phrasal level. For ex-
ample, in the context of our project, the phrases ?if
my mobile phone needs charging? and ?if my mobile
phone battery is low? have the same intended mean-
ing but it is not possible to obtain the second by
making substitutions for similar words in the first. It
appears that ?X needs charging? and ?battery (of X)
is low? have roughly similar meanings without their
component words having similar meanings. Further,
this does not appear to be due to either phrase being
non-compositional. As noted by Pearce (2001), it is
not possible to substitute similar words within non-
compositional collocations. In this case, however,
both phrases appear to be compositional. Words
cannot be substituted between the two phrases be-
cause they are composed in different ways.
3 Proposal
Recently, there has been much interest in find-
ing words which are distributionally similar e.g.,
Lin (1998), Lee (1999), Curran and Moens (2002),
Weeds (2003) and Geffet and Dagan (2004). Two
words are said to be distributionally similar if they
appear in similar contexts. For example, the two
words apple and pear are likely to be seen as the
objects of the verbs eat and peel, and this adds to
their distributional similarity. The Distributional
Hypothesis (Harris, 1968) proposes a connection be-
tween distributional similarity and semantic simi-
larity, which is the basis for a large body of work
on automatic thesaurus construction using distribu-
tional similarity methods (Curran and Moens, 2002;
Weeds, 2003; Geffet and Dagan, 2004).
Our proposal is that just as words have distribu-
tional similarity which can be used, with at least
some success, to estimate semantic similarity, so do
larger units of expression. We propose that the unit
of interest is a sub-parse, i.e., a fragment (connected
subgraph) of a parse tree, which can range in size
from a single word to the parse for the entire sen-
8
my
mobile
needs
phone charging
my
mobile
phone
low
is
battery
Figure 1: Parse trees for ?my mobile phone needs
charging? and ?my mobile phone battery is low?
tence. Figure 1 shows the parses for the clauses,
?my mobile phone needs charging,? and ?my mobile
phone battery is low? and highlights the fragments
(?needs charging? and ?battery is low?) for which we
might be interested in finding similarity.
In our model, we define the features or contexts of
a sub-parse to be the grammatical relations between
any component of the sub-parse and any word out-
side of the sub-parse. In the example above, both
sub-parses would have features based on their gram-
matical relation with the word phone. The level of
granularity at which to consider grammatical rela-
tions remains a matter for investigation. For exam-
ple, it might turn out to be better to distinguish
between all types of dependent or, alternatively, it
might be better to have a single class which covers
all dependents. We also consider the parents of the
sub-parse as features. In the example, ?Send me an
email if my mobile phone battery is low,? this would
be that the sub-parse modifies the verb send i.e., it
has the feature, <mod-of, send>.
Having defined these models for the unit of inter-
est, the sub-parse, and for the context of a sub-parse,
we can build up co-occurrence vectors for sub-parses
in the same way as for words. A co-occurrence vec-
tor is a conglomeration (with frequency counts) of
all of the co-occurrences of the target unit found in
a corpus. The similarity between two such vectors
or descriptions can then be found using a standard
distributional similarity measure (see Weeds (2003)).
The use of distributional evidence for larger units
than words is not new. Szpektor et al (2004) auto-
matically identify anchors in web corpus data. An-
chors are lexical elements that describe the context
of a sentence and if words are found to occur with
the same set of anchors, they are assumed to be
paraphrases. For example, the anchor set {Mozart,
1756} is a known anchor set for verbs with the mean-
ing ?born in?. However, this use of distributional
evidence requires both anchors, or contexts, to oc-
cur simultaneously with the target word. This dif-
fers from the standard notion of distributional sim-
ilarity which involves finding similarity between co-
occurrence vectors, where there is no requirement for
two features or contexts to occur simulultaneously.
Our work with distributional similarity is a gen-
eralisation of the approach taken by Lin and Pantel
(2001). These authors apply the distributional sim-
ilarity principle to paths in a parse tree. A path
exists between two words if there are grammatical
relations connecting them in a sentence. For exam-
ple, in the sentence ?John found a solution to the
problem,? there is a path between ?found? and ?so-
lution? because solution is the direct object of found.
Contexts of this path, in this sentence, are then the
grammatical relations <ncsubj, John> and <iobj,
problem> because these are grammatical relations
associated with either end of the path. In their work
on QA, Lin and Pantel restrict the grammatical re-
lations considered to two ?slots? at either end of the
path where the word occupying the slot is a noun.
Co-occurrence vectors for paths are then built up us-
ing evidence from multiple occurrences of the paths
in corpus data, for which similarity can then be cal-
culated using a standard metric (e.g., Lin (1998)).
In our work, we extend the notion of distributional
similarity from linear paths to trees. This allows us
to compute distributional similarity for any part of
an expression, of arbitrary length and complexity
(although, in practice, we are still limited by data
sparseness). Further, we do not make any restric-
tions as to the number or types of the grammatical
relation contexts associated with a tree.
4 Empirical Evidence
Practically demonstrating our proposal requires a
source of paraphrases. We first looked at the MSR
paraphrase corpus (Dolan et al, 2004) since it con-
tains a large number of sentences close enough in
meaning to be considered paraphrases. However, in-
spection of the data revealed that the lexical overlap
between the pairs of paraphrasing sentences in this
corpus is very high. The average word overlap (i.e.,
the proportion of exactly identical word forms) cal-
culated over the sentences paired by humans in the
training set is 0.70, and the lowest overlap4 for such
sentences is 0.3. This high word overlap makes this
a poor source of examples for us, since we wish to
study similarity between phrases which do not share
semantically similar words.
4A possible reason for this is that candidate sentences
were first identified automatically.
9
Consequently, for our purposes, the Pascal Textual
Entailment Recognition Challenge dataset is a more
suitable source of paraphrase data. Here the average
word overlap between textually entailing sentences is
0.39 and the lowest overlap is 0. This allows us to
easily find pairs of sub-parses which do not share sim-
ilar words. For example, in paraphrase pair id.19, we
can see that ?reduce the risk of diseases? entails ?has
health benefits?. Similarly in pair id.20, ?may keep
your blood glucose from rising too fast? entails ?im-
proves blood sugar control,? and in id.570, ?charged
in the death of? entails ?accused of having killed.?
In this last example there is semantic similarity
between the words used. The word charged is seman-
tically similar to accused. However, it is not possible
to swap the two words in these contexts since we do
not say ?charged of having killed.? Further, there is
an obvious semantic connection between the words
death and killed, but being different parts of speech
this would be easily missed by traditional distribu-
tional methods.
Consequently, in order to demonstrate the poten-
tial of our method, we have taken the phrases ?reduce
the risk of diseases?, ?has health benefits?, ?charged
in the death of? and ?accused of having killed?, con-
structed corpora for the phrases and their compo-
nents and then computed distributional similarity
between pairs of phrases and their respective com-
ponents. Under our hypotheses, paraphrases will be
more similar than non-paraphrases and there will be
no clear relation between the similarity of phrases as
a whole and the similarity of their components.
We now discuss corpus construction and distribu-
tional similarity calculation in more detail.
4.1 Corpus Construction
In order to compute distributional similarity between
sub-parses, we need to have seen a large number of
occurrences of each sub-parse. Since data sparse-
ness rules out using traditional corpora, such as the
British National Corpus (BNC), we constructed a
corpus for each phrase by mining the web. We also
constructed a similar corpus for each component of
each phrase. For example, for phrase 1, we con-
structed corpora for ?reduce the risk of diseases?,
?reduce? and ?the risk of diseases?. We do this in or-
der to avoid only have occurrences of the components
in the context of the larger phrase. Each corpus was
constructed by sending the phrase as a quoted string
to Altavista. We took the returned list of URLs (up
to the top 1000 where more than 1000 could be re-
turned), removed duplicates and then downloaded
the associated files. We then searched the files for
the lines containing the relevant string and added
Phrase Types Tokens
reduce the risk of diseases 156 389
reduce 3652 14082
the risk of diseases 135 947
has health benefits 340 884
has 3709 10221
health benefits 143 301
charged in the death of 624 1739
charged in 434 1011
the death of 348 1440
accused of having killed 88 173
accused of 679 1760
having killed 569 1707
Table 1: Number of feature types and tokens ex-
tracted for each Phrase
each of these to the corpus file for that phrase. Each
corpus file was then parsed using the RASP parser
(version 3.?) ready for feature extraction.
4.2 Computing Distributional Similarity
First, a feature extractor is run over each parsed cor-
pus file to extract occurrences of the sub-parse and
their features. The feature extractor reads in a tem-
plate for each phrase in the form of dependency re-
lations over lemmas. It checks each sentence parse
against the template (taking care that the same word
form is indeed the same occurrence of the word in the
sentence). When a match is found, the other gram-
matical relations5 for each word in the sub-parse are
output as features. When the sub-parse is only a
word, the process is simplified to finding grammati-
cal relations containing that word.
The raw feature file is then converted into a co-
occurrence vector by counting the occurrences of
each feature type. Table 1 shows the number of fea-
ture types and tokens extracted for each phrase. This
shows that we have extracted a reasonable number
of features for each phrase, since distributional sim-
ilarity techniques have been shown to work well for
words which occur more than 100 times in a given
corpus (Lin, 1998; Weeds and Weir, 2003).
We then computed the distributional similarity be-
tween each co-occurrence vector using the ?-skew
divergence measure (Lee, 1999). The ?-skew diver-
gence measure is an approximation to the Kullback-
Leibler (KL) divergence meassure between two dis-
tributions p and q:
D(p||q) =
?
x
p(x)log
p(x)
q(x)
5We currently retain all of the distinctions between
grammatical relations output by RASP.
10
The ?-skew divergence measure is designed to be
used when unreliable maximum likelihood estimates
(MLE) of probabilities would result in the KL diver-
gence being equal to ?. It is defined as:
dist?(q, r) = D(r||?.q + (1? ?).r)
where 0 ? ? ? 1. We use ? = 0.99, since this
provides a close approximation to the KL divergence
measure. The result is a number greater than or
equal to 0, where 0 indicates that the two distribu-
tions are identical. In other words, a smaller distance
indicates greater similarity.
The reason for choosing this measure is that it
can be used to compute the distance between any
two co-occurrence vectors independent of any infor-
mation about other words. This is in contrast to
many other measures, e.g., Lin (1998), which use the
co-occurrences of features with other words to com-
pute a weighting function such as mutual information
(MI) (Church and Hanks, 1989). Since we only have
corpus data for the target phrases, it is not possible
for us to use such a measure. However, the ?-skew
divergence measure has been shown (Weeds, 2003)
to perform comparably with measures which use MI,
particularly for lower frequency target words.
4.3 Results
The results, in terms of ?-skew divergence scores be-
tween pairs of phrases, are shown in Table 2. Each
set of three lines shows the similarity score between
a pair of phrases and then between respective pairs
of components. In the first two sets, the phrases
are paraphrases whereas in the second two sets, the
phrases are not.
From the table, there does appear to be some po-
tential in the use of distributional similarity between
sub-parses to identify potential paraphrases. In the
final two examples, the paired phrases are not se-
mantically similar, and as we would expect, their re-
spective distributional similarities are less (i.e., they
are further apart) than in the first two examples.
Further, we can see that there is no clear relation
between the similarity of two phrases and the simi-
larity of respective components. However in 3 out of
4 cases, the similarity between the phrases lies be-
tween that of their components. In every case, the
similarity of the phrases is less than the similarity
of the verbal components. This might be what one
would expect for the second example since the com-
ponents ?charged in? and ?accused of? are seman-
tically similar. However, in the first example, we
would have expected to see that the similarity be-
tween ?reduce the risk of diseases? and ?has health
Phrase 1 Phrase 2 Dist.
reduce the risk of diseases has health benefits 5.28
reduce has 4.95
the risk of diseases health benefits 5.58
charged in the death of accused of having killed 5.07
charged in accused of 4.86
the death of having killed 6.16
charged in the death of has health benefits 6.04
charged in has 5.54
the death of health benefits 4.70
reduce the risk of diseases accused of having killed 6.09
reduce accused of 5.77
the risk of diseases having killed 6.31
Table 2: ?-skew divergence scores between pairs of
phrases
benefits? to be greater than either pair of compo-
nents, which it is not. The reason for this is not clear
from just these examples. However, possibilities in-
clude the distributional similarity measure used, the
features selected from the corpus data and a combi-
nation of both. It may be that single words tend to
exhibit greater similarity than phrases due to their
greater relative frequencies. As a result, it may be
necessary to factor in the length or frequency of a
sub-parse into distributional similarity calculations
or comparisons thereof.
5 Conclusions and Further Work
In conclusion, it is clear that components of phrases
do not need to be semantically similar for the encom-
passing phrases to be semantically similar. Thus,
it is necessary to develop techniques which estimate
the semantic similarity of two phrases directly rather
than combining similarity scores calculated for pairs
of words.
Our approach is to find the distributional similar-
ity of the sub-parses associated with phrases by ex-
tending general techniques for finding lexical distri-
butional similarity. We have illustrated this method
for examples, showing how data sparseness can be
overcome using the web.
We have shown that finding the distributional sim-
ilarity between phrases, as outlined here, may have
potential in identifying paraphrases. In our exam-
ples, the distributional similarities of paraphrases
was higher than non-paraphrases. However, obvi-
ously, more extensive evaluation of the technique is
required before drawing more definite conclusions.
In this respect, we are currently in the pro-
cess of developing a gold standard set of similar
phrases from the Pascal Textual Entailment Chal-
11
lenge dataset. This task is not trivial since, even
though pairs of sentences are already identified as
potential paraphrases, it is still necessary to ex-
tract pairs of phrases which convey roughly the same
meaning. This is because 1) some pairs of sentences
are almost identical in word content and 2) some
pairs of sentences are quite distant in meaning sim-
ilarity. Further, it is also desirable to classify ex-
tracted pairs of paraphrases as to whether they are
lexical, syntactic, semantic or inferential in nature.
Whilst lexical (e.g. ?to gather? is similar to ?to col-
lect?) and syntactic (e.g. ?Cambodian sweatshop?
is equivalent to ?sweatshop in Cambodia?) are of in-
terest, our aim is to extend lexical techniques to the
semantic level (e.g. ?X won presidential election? is
similar to ?X became president?). Once our analysis
is complete, the data will be used to evaluate vari-
ations on the technique proposed herein and also to
compare it empirically to other techniques such as
that of Lin and Pantel (2001).
References
Regina Barzilay and Noemie Elhadad. 2003. Sentence
alignment for monolingual comparable corpora. In
Proceedings of the Conference on Empirical Methods
in Natural Language Processing (EMNLP2003), pages
25?33, Sapporo, Japan.
Edward Briscoe and John Carroll. 1995. Developing and
evaluating a probabilistic lr parser of part-of-speech
and punctuation labels. In 4th ACL/SIGDAT Inter-
national Workshop on Parsing Technologies, pages 48?
58.
Kenneth W. Church and Patrick Hanks. 1989. Word
association norms, mutual information and lexicogra-
phy. In Proceedings of the 27th Annual Conference of
the Association for Computational Linguistics (ACL-
1989), pages 76?82.
James R. Curran and Marc Moens. 2002. Improve-
ments in automatic thesaurus extraction. In ACL-
SIGLEX Workshop on Unsupervised Lexical Acquisi-
tion, Philadelphia.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2005. The pascal recognising textual entailment chal-
lenge. In Proceedings of the Recognising Textual En-
tailment Challenge 2005.
Bill Dolan, Chris Brockett, and Chris Quirk. 2004. Un-
supervised construction of large paraphrase corpora:
Exploiting massively parallel news sources. In Pro-
ceedings of the 20th International Conference on Com-
putational Linguistics, Geneva.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press.
Maayan Geffet and Ido Dagan. 2004. Feature vector
quality and distributional similarity. In Proceedings of
the 20th International Conference on Computational
Linguistics (COLING-2004), pages 247?253, Geneva.
Zelig S. Harris. 1968. Mathematical Structures of Lan-
guage. Wiley, New York.
Jesus Herrera, Anselmo Penas, and Felisa Verdejo. 2005.
Textual entailment recognition based on dependency
analysis and wordnet. In Proceedings of the Recognis-
ing Textual Entailment Challenge 2005, April.
Valentin Jijkoun and Maarten de Rijke. 2005. Recognis-
ing textual entailment using lexical similarity. In Pro-
ceedings of the Recognising Textual Entailment Chal-
lenge 2005, April.
Lillian Lee. 1999. Measures of distributional similarity.
In Proceedings of the 37th Annual Meeting of the As-
sociation for Computational Linguistics (ACL-1999),
pages 23?32.
Dekang Lin and Patrick Pantel. 2001. Discovery of in-
ference rules for question answering. Natural Language
Engineering, 7(4):343?360.
Dekang Lin. 1998. Automatic retrieval and clustering of
similar words. In Proceedings of the 36th Annual Meet-
ing of the Association for Computational Linguistics
and the 17th International Conference on Computa-
tional Linguistics (COLING-ACL ?98), pages 768?774,
Montreal.
Tim Owen, Ian Wakeman, Bill Keller, Julie Weeds, and
David Weir. 2005. Managing the policies of non-
technical users in a dynamic world. In IEEE Workshop
on Policy in Distributed Systems, Stockholm, Sweden,
May.
Darren Pearce. 2001. Synonymy in collocation extrac-
tion. In Proceedings of the NAACL Workshop on
WordNet and Other Lexical Resources: Applications,
Extensions and Customizations, Carnegie Mellon Uni-
versity, Pittsburgh.
Idan Szpektor, Hristo Tanev, Ido Dagan, and Bonaven-
tura Coppola. 2004. Scaling web-based acquisition
of entailment relations. In Proceedings of Empirical
Methods in Natural Language Processing (EMNLP)
2004, Barcelona.
Julie Weeds and David Weir. 2003. A general frame-
work for distributional similarity. In Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP-2003), Sapporo, Japan.
Julie Weeds, Bill Keller, David Weir, Tim Owen, and
Ian Wakemna. 2004. Natural language expression of
user policies in pervasive computing environments. In
Proceedings of OntoLex2004, LREC Workshop on On-
tologies and Lexical Resources in Distributed Environ-
ments, Lisbon, Portugal, May.
Julie Weeds. 2003. Measures and Applications of Lexical
Distributional Similarity. Ph.D. thesis, Department of
Informatics, University of Sussex.
12
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 2249?2259, Dublin, Ireland, August 23-29 2014.
Learning to Distinguish Hypernyms and Co-Hyponyms
Julie Weeds, Daoud Clarke, Jeremy Reffin, David Weir and Bill Keller
Department of Informatics,
University of Sussex,
Brighton, UK
juliewe,D.Clarke,J.P.Reffin,davidw,billk@sussex.ac.uk
Abstract
This work is concerned with distinguishing different semantic relations which exist between
distributionally similar words. We compare a novel approach based on training a linear Support
Vector Machine on pairs of feature vectors with state-of-the-art methods based on distributional
similarity. We show that the new supervised approach does better even when there is minimal
information about the target words in the training data, giving a 15% reduction in error rate over
unsupervised approaches.
1 Introduction
Over recent years there has been much interest in the field of distributional semantics, drawing on the
distributional hypothesis: words that occur in similar contexts tend to have similar meanings (Harris,
1954). There is a large body of work on the use of different similarity measures (Lee, 1999; Weeds and
Weir, 2003; Curran, 2004) and many researchers have built thesauri (i.e. lists of ?nearest neighbours?)
automatically and applied them in a variety of applications, generally with a good deal of success.
In early research there was much interest in how these automatically generated thesauri compare with
human-constructed gold standards such as WordNet and Roget (Lin, 1998; Kilgarriff and Yallop, 2000).
More recently, the focus has tended to shift to building thesauri to alleviate the sparse-data problem. Dis-
tributional thesauri have been used in a wide variety of areas including sentiment classification (Bollegala
et al., 2011), WSD (Miller et al., 2012; Khapra et al., 2010), textual entailment (Berant et al., 2010), pre-
dicting semantic compositionality (Bergsma et al., 2010), acquisition of semantic lexicons (McIntosh,
2010), conversation entailment (Zhang and Chai, 2010), lexical substitution (Szarvas et al., 2013), tax-
onomy induction (Fountain and Lapata, 2012), and parser lexicalisation (Rei and Briscoe, 2013).
A primary focus of distributional semantics has been on identifying words which are similar to each
other. However, semantic similarity encompasses a variety of different lexico-semantic and topical re-
lations. Even if we just consider nouns, an automatically generated thesaurus will tend to return a mix
of synonyms, antonyms, hyponyms, hypernyms, co-hyponyms, meronyms and other topically related
words. A central problem here is that whilst most measures of distributional similarity are symmetric,
some of the important semantic relations are not. The hyponymy relation (and converse hypernymy)
which forms the ISA backbone of taxonomies and ontologies such as WordNet (Fellbaum, 1989), and
determines lexical entailment (Geffet and Dagan, 2005), is asymmetric. On the other hand, the co-
hyponymy relation which relates two words unrelated by hyponymy but sharing a (close) hypernym, is
symmetric, as are synonymy and antonymy. Table 1 shows the distributionally nearest neighbours of the
words cat, animal and dog. In the list for cat we can see 2 hypernyms and 13 co-hyponyms
1
.
1
We read cat in the sense domestic cat rather than big cat, hence tiger is a co-hyponym rather than hyponym
of cat.
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings
footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
2249
cat dog 0.32, animal 0.29, rabbit 0.27, bird 0.26, bear 0.26, monkey 0.26, mouse 0.25, pig 0.25,
snake 0.24, horse 0.24, rat 0.24, elephant 0.23, tiger 0.23, deer 0.23, creature 0.23
animal bird 0.36, fish 0.34, creature 0.33, dog 0.31, horse 0.30, insect 0.30, species 0.29, cat 0.29,
human 0.28, mammal, 0.28, cattle 0.27, snake 0.27, pig 0.26, rabbit 0.26, elephant 0.25
dog cat 0.32, animal 0.31, horse 0.29, bird 0.26, rabbit 0.26, pig 0.25, bear 0.26, man 0.25, fish
0.24, boy 0.24, creature 0.24, monkey 0.24, snake 0.24, mouse 0.24, rat 0.23
Table 1: Top 15 neighbours of cat, animal and dog generated using Lin?s similarity measure (Lin,
1998) considering all words and dependency features occurring 100 or more times in Wikipedia.
Distributional similarity is being deployed (e.g., Dinu and Thater (2012)) in situations where it can
be useful to be able to distinguish between these different relationships. Consider the following two
sentences.
The cat ran across the road. (1)
The animal ran across the road. (2)
Sentence 1 textually entails sentence 2, but sentence 2 does not textually entail sentence 1. The ability
to determine whether entailment holds between the sentences, and in which direction, depends on the
ability to identify hyponymy. Given a similarity score of 0.29 between cat and animal, how do we
know which is the hyponym and which is the hypernym?
In applying distributional semantics to the problem of textual entailment, there is a need to generalise
lexical entailment to phrases and sentences. Thus, the ability to distinguish different semantic relations
is crucial if approaches to the composition of distributional representations of meaning that are currently
receiving considerable interest (Widdows, 2008; Mitchell and Lapata, 2008; Baroni and Zamparelli,
2010; Grefenstette et al., 2011; Socher et al., 2012; Weeds et al., 2014) are to be applied to the textual
entailment problem.
We formulate the challenge as follows: Consider a set of pairs of similar words ?A,B? where one of
three relationships hold between A and B: A lexically entails B, B lexically entails A or A and B are
related by co-hyponymy. Given such a set, how can we determine which relationship holds? In Section
2, we discuss existing attempts to address this problem through the use of various directional measures
of distributional similarity.
This paper considers the effectiveness of various supervised approaches, and makes the following
contributions. First, we show that a SVM can distinguish the entailment and co-hyponymy relations,
achieving a significant reduction in error rate in comparison to existing state-of-the-art methods based
on the notion of distributional generality. Second, by comparing two different data sets, one built from
BLESS (Baroni and Lenci, 2011) and the other from WordNet (Fellbaum, 1989), we derive important
insights into the requirements of a valid evaluation of supervised approaches, and provide a data set
for further research in this area. Third, we show that when learning how to determine an ontological
relationship between a pair of similar words by means of the word?s distributional vectors, quite different
vector operations are useful when identifying different ontological relationships. In particular, using the
difference between the vectors for pairs of words is appropriate for the entailment task, whereas adding
the vectors works well for the co-hyponym task.
2 Related Work
Lee (1999) noted that the substitutability of one word for another was asymmetric and proposed the
alpha-skew divergence measure, an asymmetric version of the Kullback-Leibler divergence measure. She
found that this measure improved results in language modelling, when a word?s distribution is smoothed
using the distributions of its nearest neighbours.
Weeds et al. (2004) proposed a notion of distributional generality, observing that more general words
tend to occur in a larger variety of contexts than more specific words. For example, we would expect to be
able to replace any occurrence of cat with animal and so all of the contexts of cat must be plausible
2250
contexts for animal. However, not all of the contexts of animal would be plausible for cat, e.g.,
?the monstrous animal barked at the intruder?. Weeds et al. (2004) attempt to capture this asymmetry
by framing word similarity in terms of co-occurrence retrieval (Weeds and Weir, 2003), where precision
and recall are defined as:
P
ww
(u, v) =
?
f?F (u)?F (v)
I(u, f)
?
f?F (u)
I(u, f)
and R
ww
(u, v) =
?
f?F (u)?F (v)
I(v, f)
?
f?F (v)
I(v, f)
where I(n, f) is the pointwise mutual information (PMI) between noun n and feature f and F(n) is the
set of all features f for which I(n, f) > 0.
By comparing the precision and recall of one word?s retrieval of another word?s contexts, they were
able to successfully identify the direction of an entailment relation in 71% of pairs drawn from WordNet.
However, this was not significantly better than a baseline which proposed that the most frequent word
was the most general.
Clarke (2009) formalised the idea of distributional generality using a partially ordered vector space.
He also argued for using a variation of co-occurrence retrieval where precision and recall are defined as:
P
cl
(u, v) =
?
f?F (u)?F (v)
min(I(u, f), I(v, f))
?
f?F (u)
I(u, f)
and R
cl
(u, v) =
?
f?F (u)?F (v)
min(I(u, f), I(v, f))
?
f?F (v)
I(v, f)
Lenci and Benotto (2012) took the notion further and hypothesised that more general terms should
have high recall and low precision, which would thus make it possible to distinguish them from other
related terms such as synonyms and co-hyponyms. They proposed a variant of the Clarke (2009) measure
to identify hypernyms:
invCL(u, v) =
2
?
P
cl
(u, v) ? (1?R
cl
(u, v))
Evaluation on the BLESS data set (Baroni and Lenci, 2011), showed that this measure is better at distin-
guishing hypernyms from other relations than the measures of Weeds et al. (2004) and Clarke (2009).
Geffet and Dagan (2005) proposed an approach based on feature inclusion, which extends the rationale
of Weeds et al. (2004) to lexical entailment. Using data from the web they demonstrated a strong cor-
relation between complete inclusion of prominent features and lexical entailment. However, they were
unable to assess this using an off-line corpus due to data sparseness.
Szpektor and Dagan (2008) found that the P
ww
measure tends to promote relationships between infre-
quent words with narrow vectors (i.e. those with relatively few distinct context features). They proposed
using the geometric average of P
ww
and the symmetric similarity measure of Lin (1998) in order to
penalise low frequency words.
Kotlerman et al. (2010) apply the IR evaluation method of Average Precision to the problem of identi-
fying lexical inference and use the balancing approach of Szpektor and Dagan (2008) to demote similar-
ities for narrow feature vectors; their measure is called balAPinc. They show that all of the asymmetric
similarity measures previously proposed perform much better than symmetric similarity measures on
a directionality detection experiment, and that their method and that of Clarke (2009) outperform the
others with statistical significance. They also show that their measure is superior when used for term
expansion in an event detection task.
Baroni et al. (2012) investigate the relation between phrasal and lexical entailment, and demonstrate
that support vector machines can generalise entailment relations between quantifier phrases to entailment
involving unseen quantifiers. They compare the performance of their system with the balAPinc measure.
The Stanford WordNet project (Snow et al., 2004) expands the WordNet taxonomy by analysing large
corpora to find patterns that are indicative of hyponymy. For example, the pattern ?NP
X
and otherNP
Y
?
is an indication that NP
X
is a NP
Y
, i.e. that NP
X
is a hyponym of NP
Y
. They use machine learning
to identify other such patterns from known hyponym-hypernym pairs, and then use these patterns to find
new relations in the corpus. The transitivity relation of the taxonomy is enforced by searching only over
valid taxonomies and evaluating the likelihood of each taxonomy given the available evidence (Snow
2251
et al., 2006). The approach is similar to ours in providing a supervised method of learning semantic
relations, but relies on having features for occurrences of pairs of terms rather than just vectors for terms
themselves. Our approach is therefore more generally applicable to systems which compose distribu-
tional representations of meaning.
Most recently, Rei and Briscoe (2013) note that hyponyms are well suited for lexical substitution.
In their experiments with smoothing edge scores for parser lexicalisation, they find that a directional
similarity measure, WeightedCosine
2
, performs best. Also of note, Mikolov et al. (2013) propose a vector
offset method to capture syntactic and semantic regularities between word representations learnt by a
recurrent neural network language model. Yih et al. (2012) present a method for distinguishing synonyms
and antonyms by inducing polarity in a document-term matrix before applying Latent Semantic Analysis.
Santus et al. (2014) propose identifying hypernyms using a new measure based on entropy, SLQS, which
is based on the hypothesis that the most typical linguistic contexts of a hypernym are less informative
than the most typical linguistic contexts of its hyponyms. Evaluated on pairs extracted from the BLESS
dataset (Baroni and Lenci, 2011), this measure outperforms P
ww
at both discriminating hypernym test
pairs from other types of relation and at determining the direction of the entailment relation.
3 Methodology
The code used to perform our experiments has been open sourced, and is available online.
3
3.1 Vector Representations
Distributional information was collected for all of the nouns from Wikipedia provided they had oc-
curred 100 or more times. We used a Wikimedia dump of Wikipedia from June 2011 and extracted
text using wp2txt
4
. This was part-of-speech tagged, lemmatised and dependency parsed using the Malt
Parser (Nivre, 2004). All major grammatical dependency relations involving open class parts of speech
(nsubj, dobj, iobj, conj, amod, nnmod) and also occurring 100 or more times were ex-
tracted as features of the POS-tagged and lemmatised nouns. The value of each feature is the positive
point wise mutual information (PPMI) (Church and Hanks, 1989) between the noun and the feature. The
total number of noun vectors which can be harvested from Wikipedia with these parameters is 124, 345.
Our goal is to build classifiers that establish whether or not a given semantic relation, rel, holds be-
tween two similar words A and B. Support vector machines (SVMs), which are effective across a variety
of classification scenarios, learn a boundary between two classes from a set of positive and negative ex-
ample vectors. The two classes correspond to the relation rel holding or not holding. Here, however, we
do not start with a single vector, but with two distributional vectors v
A
and v
B
for the words A and B,
respectively. These vectors must be combined in some way to produce the SVM?s input, and a number
of ways were considered, defined in Table 2. Of these operations, the vector difference (used by svm-
DIFF and knnDIFF) and direct sum (used by svmCAT) are asymmetric, whereas the sum and pointwise
multiplication (used by svmADD and svmMULT) are symmetric.
We now motivate the use of each of these operations. First, we note that pointwise multiplication
(svmMULT) is intersective. Similar vectors will have a large intersection and it might be possible to
learn the features that nouns occurring in different semantic relations should share. However, it does
not retain any information about non-shared features and it is symmetric so it is difficult to see how it
would be possible to use it to distinguish hypernyms from hyponyms. Pointwise addition (svmADD)
effectively performs the union of the features, giving emphasis to the shared features. Whilst it does
retain information about the non-shared features, it is also symmetric, making it difficult again to see
how it would be useful in determining the direction of an entailment relation
Vector difference (as used in svmDIFF and knnDIFF), on the other hand, is asymmetric. Further,
we might expect a small difference vector (containing many zeroes) to be indicative of similar nouns.
Further, considering the majority sign of features in this difference vector might indicate the direction of
2
The details of this measure are unpublished.
3
https://github.com/SussexCompSem/learninghypernyms
4
https://github.com/yohasebe/wp2txt
2252
entailment. Using an SVM, we might expect to be able to effectively learn which of these features should
be ignored and which should be combined, to decide the correct direction of entailment in the majority
number of cases in our training data. However, note that if one uses vector difference it is impossible to
distinguish between the case where a feature occurred with both nouns (to the same extent) and the case
where a feature occurs with neither noun. Accordingly, a small difference vector may indicate that both
nouns do not occur in many distinct contexts. A possible solution to this problem is to use the direct
sum of the vectors (i.e., the concatenation of the two vectors) which retains all of the information from
the original vectors. Finally, we consider the use of the single vector corresponding to the second word
(svmSING) as a baseline. High performance by this operation would indicate that we can learn features
of words which tend to be hypernyms (or co-hyponyms) without any regard to the other word in the
putative relationship.
We also note that the behaviour of these methods may differ depending on the weighting used for vec-
tors. For example, PMI is the log of a ratio of probabilities and therefore one might expect vector addition
where vectors are weighted using PMI to correspond to multiplication where vectors are weighted using
frequency or probability. However, the use of positive PMI (where negative PMI scores are regarded
equal to zero), which is consistent with other work in this area, means that this correspondence is lost.
Because of the nature of our datasets, we were concerned that systems could learn information about
the taxonomy from the relations in the training data, without making use of information in the vectors
themselves. To investigate this, we constructed random vectors to be used in place of the vectors derived
from Wikipedia. The dimensionality of the random vectors was chosen to be 1000 since this substantially
exceeds the average number (398) of non-zero features in the Wikipedia vectors.
3.2 Classifiers
We constructed linear SVMs for each of the vector operations outlined in Section 3.1. We used linear
SVMs for speed and simplicity, since the point is to compare the different vector representations of
the pairings. For comparison, we also constructed a number of supervised, unsupervised, and weakly
supervised classifiers. These are listed in Table 2. For the linear SVMs and kNN classifier, we used the
scikit-learn implementations with default settings. For k nearest neighbours, we performed a parameter
search, using nested cross-validation, varying k between 1 and 50.
For weakly supervised approaches, we evaluated the measure on the training set, then found the best
threshold p on the training set that best divides the two classes using that measure. When classifying, we
determine that the relation holds if the value of the measure exceeds p.
svmDIFF A linear SVM trained on the vector difference v
B
? v
A
svmMULT A linear SVM trained on the pointwise product vector v
B
? v
A
svmADD A linear SVM trained on the vector sum v
B
+ v
A
svmCAT A linear SVM trained on the vector concatenation v
B
? v
A
svmSING A linear SVM trained on the vector v
B
knnDIFF k nearest neighbours (knn) trained on the vector difference v
B
? v
A
.1 < k < 50
widthdiff width(B) > width(A)? rel(A,B) where width(A) is number of non-zero features in A
singlewidth width(B) > p? rel(A,B)
cosineP sim
cos
(A,B) > p? rel(A,B) where sim
cos
(A,B) is cosine similarity using PPMI
linP sim
lin
(A,B) > p? rel(A,B) (Lin, 1998)
CRdiff P
ww
(A,B) > R
ww
(A,B)? rel(A,B) (Weeds et al., 2004)
clarkediff P
cl
(A,B) > R
cl
(A,B)? rel(A,B) (Clarke, 2009)
invCLP invCL(A,B) > p? rel(A,B) (Lenci and Benotto, 2012)
balAPincP balAPinc(A,B) > p? rel(A,B) (Kotlerman et al., 2010)
most freq The most frequent label in the training data is assigned to every test point.
Table 2: Implemented classifiers
2253
3.3 Data Sets
One of key the challenges of this work has been to construct a data set which accurately and validly tests
our hypotheses. All four of our datasets detailed below are available online
5
.
In order to test our hypotheses, a data set needs to be balanced in many respects in order to prevent the
supervised classifiers making use of artefacts of the data. This would not only make it unfair to compare
the supervised approaches with the unsupervised approaches, but also make it unlikely that our results
would be generalisable to other data. Here, we outline the requirements for the data sets, the importance
of which is demonstrated by our initial results for a data set which does not satisfy all of them.
There should be an equal number of positive and negative examples of a semantic relation. Thus,
random guessing or labelling with the most frequently seen label in the training data will yield 50%
accuracy and precision. An advantage of incorporating this requirement means that evaluation can be in
terms of simple accuracy (or error rate).
It should not be possible to do well simply by considering the distributional similarity of the terms.
Hence, the negative examples need to be pairs of equally similar words, but where the relationship under
consideration does not hold.
It should not be possible to do well by pre-supposing an entailment relation and guessing the direction.
For example, it has been shown (Weeds et al., 2004) that given a pair of entailing words selected from
WordNet, over 70% of the time the more frequent word is also the entailed word.
It should not be possible to do well using ontological information learnt about one or both of the
words from the training data that is not generalisable to their distributional representations. For example,
it should not be possible for the classifier simply to learn directly from the training pairs ?cat ISA
mammal? and ?mammal ISA animal? that ?cat ISA animal?. Furthermore, we must ensure that
a classifier cannot learn that a particular word is near the top of the ontological hierarchy, and, as a
result, do well by guessing that a particular pairing probably has an entailment relation. For example,
given many pairs such as ?cat ISA animal?, ?dog ISA animal?, a system which guessed ?rabbit
ISA animal? but not ?animal ISA rabbit? would do better than random guessing. Whilst both
of these types of information could be useful in a hybrid system, they do not require any distributional
information and therefore we would not be learning anything about the distributional features of animal
which make it likely to be a hypernym.
3.3.1 BLESS
We have constructed two data sets from BLESS (Baroni and Lenci, 2011) which is a collection of ex-
amples of hypernyms, co-hyponyms, meronyms and random unrelated words for each of 200 concrete,
largely monosemous nouns. We will refer to these 200 nouns as the BLESS concepts.
hyponym
BLESS
is a set of 1976 labelled pairs of nouns. For each BLESS concept, 80% of the hypernyms
were randomly selected to provide positive examples of entailment. The remaining hypernyms for the
given concept were reversed and taken with the same number of co-hyponyms, meronyms and random
words to form negative examples of entailment. A filter was applied to ensure that duplicate pairs were
not included (e.g., if ?cat,animal? is a positive pair then ?animal,cat? cannot be a negative pair).
cohyponym
BLESS
is a set of 5835 labelled pairs of nouns. For each BLESS concept, the co-hyponyms
were taken as positive examples of this relation. The same total number of (and split evenly between)
hypernyms, meronyms and random words was taken to form the negative examples. The order of 50%
of the pairs was reversed and again duplicate pairs were disallowed.
In both cases the pairs are labelled as positive or negative for the specified semantic relation and in
both cases there are equal (?1) numbers of positive and negative examples. For 99% of the generated
BLESS pairs, both nouns had associated vectors harvested from Wikipedia. If a noun does not have an
associated vector, the classifiers use a zero vector.
5
https://github.com/SussexCompSem/learninghypernyms
2254
3.3.2 WordNet
We constructed two data sets using WordNet. Whilst these data sets are similar in size to the BLESS
data sets they more adequately satisfy the requirements laid out above
6
. We constructed a list of all non-
rare, largely monosemous, single word terms in WordNet. To be considered non-rare, a word needed to
have occurred in SemCor at least once (i.e. frequency information is provided about it in the WordNet
package) and to have occurred in Wikipedia at least 100 times. To be considered largely monosemous,
the predominant sense of the word needed to account for over 50% of the occurrences in the SemCor
frequency information provided with WordNet. This led to a list of 7613 nouns.
hyponym
WN
is a set of 2564 labelled pairs of nouns constructed in the following way. Pairs ?A,B? were
found in the list of nouns where B is an ancestor of A (i.e., A lexically entails B). Each found pair is
added either as a positive or a negative in the ratio 2:1 provided that the reverse pairing has not already
been added and provided that each word has not previously been used in that position. Co-hyponym
pairs (i.e., words which share a direct hypernym) were also found within the list of nouns. Each found
pair is added to the data set (as a negative) provided the reverse pairing has not already been added, and
provided that neither word has already been seen in that position in a pairing (either in the entailment
pairs or the co-hyponym pairs). The same number of co-hyponym pairs as hypernym-hyponym negatives
is selected. This provides a balanced data set where half of the pairs are positive examples of entailment
and the other half are semantically similar but not entailing.
cohyponym
WN
is a set of 3771 labelled pairs of nouns. It was constructed in the same way as hyponym
WN
except the same number of co-hyponym pairs were selected as the total number of entailment pairs (in
either direction). These co-hyponym pairs were labelled as positive and the entailment pairs were labelled
as negative. Thus, this provides a balanced data set where half of the pairs are positive examples of co-
hyponyms and the other half, the negative examples, are entailment pairs (with direction unspecified)
In both these sets, the average path distance between entailment pairs is 1.64, whereas path distance
between co-hyponym pairs is 2.
3.4 Experimental Setup
Most of our experiments were carried out using an implementation of five-fold cross-validation using
each combination of data set, vector set and classifier. In this setup, the pairs are randomly partitioned
into five subsets, one subset is held out for testing whilst the classifiers are trained on the remaining four,
and this process is repeated using each subset as the test set.
In initial experiments with the BLESS datasets, the SVM classifiers were able to achieve classification
accuracy of over 95% for hyponym
BLESS
and over 90% for cohyponym
BLESS
. However, the results us-
ing random vectors were not significantly different from using the distributional vectors harvested from
Wikipedia. This indicated that the classifiers were learning ontological information implicit in the train-
ing data. In order to address this, when using the BLESS datasets, we removed any pair from the training
data if either word was present in the test data. In order to preserve a reasonable amount of training data,
we implemented this approach with ten-fold cross-validation. In all subsequent experiments, across all
datasets and classifiers, we found performance by the random vectors was no higher than 52%. This
indicates that the performance seen in Table 3 is due to learning from distributional features rather than
any ontological information implicit in the training set.
4 Results
In Table 3, we compare average accuracy for a number of different classifiers on each of two tasks,
distinguishing hyponyms and distinguishing co-hyponyms, on each of the two datasets.
Looking at the results for the hyponym
BLESS
data set, we can see that the SVM methods do generally
outperform the unsupervised methods. However, the best performing model is svmSING, suggesting
that, for this data set, it is best to try to learn the distributional features of more general terms, rather than
comparing the vector representations of the two terms under consideration.
6
Note that imposing these requirements on the BLESS data sets would lead to very small data sets, since information is only
provided for 200 nouns.
2255
dataset svmDIFF svmMULT svmADD svmCAT svmSING knnDIFF
hyponym
BLESS
0.74 0.56 0.66 0.68 0.75 0.54
cohyponym
BLESS
0.62 0.39 0.41 0.40 0.40 0.58
hyponym
WN
0.75 0.45 0.37 0.74 0.69 0.50
cohyponym
WN
0.37 0.60 0.68 0.64 0.58 0.50
dataset most freq cosineP linP widthdiff singlewidth CRdiff invCLP balAPincP
hyponym
BLESS
0.54 0.53 0.54 0.56 0.58 0.52 0.54 0.54
cohyponym
BLESS
0.61 0.79 0.78 - - - - -
hyponym
WN
0.50 0.53 0.52 0.70 0.65 0.70 0.66 0.53
cohyponym
WN
0.50 0.50 0.55 - - - - -
Table 3: Accuracy Figures for the data sets generated from BLESS and WordNet (standard errors <
0.02). For cohyponyms, results for measures designed to detect hyponymy have been omitted. We also
omit results of clarkediff as these were consistently the same or less than CRdiff.
On the corresponding co-hyponym task, using the cohyponym
BLESS
data set, we see the best performing
classifier is the cosine measure. The cosine measure is able to perform relatively well here because a
substantial proportion of the negative examples (25%) are random unrelated words which will have low
cosine scores. It is also consistent with earlier work (e.g., (Lenci and Benotto, 2012)) which suggests
that measures such as the cosine measure ?prefer? words in symmetric semantic relationships such as co-
hyponymy. The poor performance of the SVM methods here can perhaps be explained by the paucity of
the training data in this experimental set up with this data set. If, for example, our test concept is robin,
our approach requires that we will not have any training pairs containing robin, or any training pairs
containing any of the words to which robin is related in the test set. In a dataset as small as BLESS,
this requirement effectively removes all knowledge of the distributional features of words in the target
domain. Hence, the need for a larger dataset as we have extracted from WordNet.
Looking at the results for the hyponym
WN
data set, the directional SVM methods (svmDIFF and svm-
CAT) substantially outperform the symmetric SVM methods, and their performance is significantly better
(at the 0.01% level) than the unsupervised methods. Also of note is the substantial difference between
svmDIFF and knnDIFF. Both of these methods are trained on the differences of vectors. However, the
linear SVM outperforms kNN by 19?25%. This may suggest that the shape of the vector space inhabited
by the positive entailment pairs is particularly conducive for learning a linear SVM. Positive and negative
pairs are close together (as evidenced by the poor performance of kNN), but generally linearly separable.
Looking at the results for the cohyponym
WN
data set, it is clear that the unsupervised methods cannot
distinguish the co-hyponym pairs from the entailing pairs. The supervised SVM methods do substantially
better, with the best performance achieved by svmADD and svmCAT. Both of these methods essentially
retain information about all of the features of both words. svmMULT does much better than svmDIFF,
which suggests that the shared features are more indicative than the non-shared features for this task.
The reasonably high performance of svmSING on both data sets suggests that words which have co-
hyponyms in the data set tend to inhabit a somewhat different part of the feature space to words which
are included as entailed words in the data set. We hypothesise that there are specific features which more
general words tend to share (regardless of their topic) which makes it possible to identify more general
words from more specific words. This is completely consistent with very recent results using SLQS, a
new entropy-based measure (Santus et al., 2014). Here, the authors hypothesise that the most typical
contexts of a hypernym are less informative than the most typical linguistic contexts of its hyponyms,
with some promising results. It would be plausible to hypothesise that svmSING is learning which nouns
typically have less informative contexts and are therefore likely to by hypernyms.
Given prior work, the performance of the balAPincP measure is lower than expected on the
hyponym
WN
dataset. Our task is slightly different to that of (Kotlerman et al., 2010), since we are deter-
mining the existence (or not) of hyponymy, rather than the direction of entailment for pairs where it is
known that a relationship exists. It could be that the measure is particularly suited to the latter task.
2256
5 Conclusions and Further Work
We have shown that it is possible to predict to a large extent whether or not there is a specific semantic
relation between two words given their distributional vectors, using a supervised approach based on
linear SVMs. The increase in accuracy over unsupervised methods is significant at the 0.01% level and
corresponds to a substantial absolute reduction in error rate (over 15%).
We have also shown that the choice of vector operation is significant. Whilst concatenating the vectors,
and therefore retaining all of the information from both vectors including direction, generally performs
well, we have also shown that different vector operations are useful in establishing different relationships.
In particular, the vector difference operation, which loses information about the original vectors, achieved
performance indistinguishable from concatenation on the entailment task, where the classifier is required
to distinguish hyponyms from other semantically related words including hypernyms. On the other
hand, the addition operation, which also loses information, outperformed concatenation by 4% (which
is statistically significant at the 0.01% level) on the coordinate task, where the classifier is required to
distinguish co-hyponyms from hyponyms and hypernyms. Hence the nature of the relationship one is
trying to establish between words determines the nature of the operation one should perform on their
associated vectors.
We have also shown that it is possible to outperform state-of-the-art unsupervised methods even when
a data set has been constructed without ontological information, and when target words have not previ-
ously been seen in that position of a relationship in the training data. Hence, we believe the supervised
methods are learning characteristics of the underlying feature space which are generalisable to new words
(inhabiting the same feature space).
In future work, we intend to apply this approach to the problem of labelling the distributional neigh-
bours found for a given word with specific semantic relations. We also plan to investigate the use of
bag-of-words (windowed) vectors instead of grammatical relations for this task.
Finally, we believe that the data sets constructed from WordNet, which we publish alongside this
paper, can be used as a useful benchmark in evaluating future advances in this area, both for supervised
and unsupervised methods.
Acknowledgements
This work was funded by UK EPSRC project EP/IO37458/1 ?A Unified Model of Compositional and
Distributional Compositional Semantics: Theory and Applications?.
References
Marco Baroni and Alessandro Lenci. 2011. How we BLESSed distributional semantic evaluation. In Proceedings
of the GEMS 2011 workshop on Geometric Models of Natural Language Semantics, EMNLP 2011.
Marco Baroni and Roberto Zamparelli. 2010. Nouns are vectors, adjectives are matrices: Representing adjective-
noun constructions in semantic space. In Proceedings of the 2010 Conference on Empirical Methods in Natural
Language Processing.
Marco Baroni, Raffaella Bernardi, Ngoc-Quynh Do, and Chung-chieh Shan. 2012. Entailment above the word
level in distributional semantics. In Proceedings of the 13th Conference of the European Chapter of the Associ-
ation for Computational Linguistics, pages 23?32. Association for Computational Linguistics.
Jonathan Berant, Ido Dagan, and Jacob Goldberger. 2010. Global learning of focused entailment graphs. In
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1220?1229,
Uppsala, Sweden, July. Association for Computational Linguistics.
Shane Bergsma, Aditya Bhargava, Hua He, and Grzegorz Kondrak. 2010. Predicting the semantic composi-
tionality of prefix verbs. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language
Processing, pages 293?303, Cambridge, MA, October. Association for Computational Linguistics.
Danushka Bollegala, David Weir, and John Carroll. 2011. Using multiple sources to construct a sentiment sen-
sitive thesaurus for cross-domain sentiment classification. In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human Language Technologies (ACL-HLT 2011).
2257
Kenneth Ward Church and Patrick Hanks. 1989. Word association norms, mutual information, and lexicography.
In Proceedings of the 27th Annual Meeting on Association for Computational Linguistics, ACL ?89, pages
76?83, Stroudsburg, PA, USA. Association for Computational Linguistics.
Daoud Clarke. 2009. Context-theoretic semantics for natural language: an overview. In Proceedings of the
Workshop of Geometric Models for Natural Language Semantics.
James Curran. 2004. From Distributional to Semantic Similarity. Ph.D. thesis, University of Edinburgh.
Georgiana Dinu and Stefan Thater. 2012. Saarland: Vector-based models of semantic textual similarity. In
Proceedings of the First Joint Conference on Lexical and Computational Semantics.
Christaine Fellbaum, editor. 1989. WordNet: An Electronic Lexical Database. The MIT Press, Cambridge,
Massachusetts.
Trevor Fountain and Mirella Lapata. 2012. Taxonomy induction using hierarchical random graphs. In Proceed-
ings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics:
Human Language Technologies, pages 466?476, Montr?eal, Canada, June.
Maayan Geffet and Ido Dagan. 2005. Lexical entailment and the distributional inclusion hypothesis. In Proceed-
ings of the 43rd meeting of the Association for Computational Liuguistics (ACL), pages 107?114.
Edward Grefenstette, Mehrnoosh Sadrzadeh, Stephen Clark, Bob Coecke, and Stephen Pulman. 2011. Concrete
sentence spaces for compositional distributional models of meaning. Proceedings of the 9th International Con-
ference on Computational Semantics (IWCS 2011), pages 125?134.
Zelig Harris. 1954. Distributional structure. Word, 10:146?162.
Mitesh Khapra, Anup Kulkarni, Saurabh Sohoney, and Pushpak Bhattacharyya. 2010. All words domain adapted
WSD: Finding a middle ground between supervision and unsupervision. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguistics, pages 1532?1541, Uppsala, Sweden, July.
Adam Kilgarriff and Colin Yallop. 2000. What?s in a thesaurus? In Proceedings of the 2nd International
Conference on Language Resources and Evaluation (LREC2000).
Lili Kotlerman, Ido Dagan, Idan Szpektor, and Maayan Zhitomirsky-Geffet. 2010. Directional distributional simi-
larity for lexical inference. Special Issue of Natural Language Engineering on Distributional Lexical Semantics,
4(16):359?389.
Lillian Lee. 1999. Measures of distributional similarity. In Proceedings of the 37th Annual Meeting of the
Association for Computational Linguistics, pages 25?32, College Park, Maryland, USA, June.
Alessandro Lenci and Giulia Benotto. 2012. Identifying hypernyms in distributional semantic spaces. In Proceed-
ings of the First Joint Conference on Lexical and Computational Semantics (*Sem).
Dekang Lin. 1998. Automatic retrieval and clustering of similar words. In Proceedings of the 17th International
Conference on Computational Linguistics (COLING 1998).
Tara McIntosh. 2010. Unsupervised discovery of negative categories in lexicon bootstrapping. In Proceedings of
the 2010 Conference on Empirical Methods in Natural Language Processing, pages 356?365, Cambridge, MA,
October. Association for Computational Linguistics.
Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig. 2013. Linguistic regularities in continuous space word rep-
resentations. In Proceedings of the 2013 Conference of the North American Chapter of the Association for
Computational Linguistics: Human Language Technologies, pages 746?751, Atlanta, Georgia, June.
Tristan Miller, Chris Biemann, Torsten Zesch, and Iryna Gurevych. 2012. Using distributional similarity for
lexical expansion in knowledge-based word sense disambiguation. In Proceedings of COLING 2012, pages
1781?1796, Mumbai, India, December.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based models of semantic composition. In Proceedings of ACL-08:
HLT, pages 236?244, Columbus, Ohio, June. Association for Computational Linguistics.
Joakim Nivre. 2004. Incrementality in deterministic dependency parsing. In Proceedings of the ACL workshop on
Incremental Parsing, pages 50?57.
2258
Marek Rei and Ted Briscoe. 2013. Parser lexicalisation through self-learning. In Proceedings of the 2013 Con-
ference of the North American Chapter of the Association for Computational Linguistics: Human Language
Technologies, pages 391?400, Atlanta, Georgia, June. Association for Computational Linguistics.
Enrico Santus, Alessandro Lenci, Qin Lu, and Sabine Schulte Im Walde. 2014. Chasing hypernyms in vector
spaces with entropy. In Proceedings of the 14th Conference of the European Chapter of the Association for
Computational Linguistics, pages 38?42, Gothenburg, Sweden, April.
Rion Snow, Daniel Jurafsky, and Andrew Y Ng. 2004. Learning syntactic patterns for automatic hypernym
discovery. Advances in Neural Information Processing Systems 17.
Rion Snow, Daniel Jurafsky, and Andrew Y Ng. 2006. Semantic taxonomy induction from heterogenous evidence.
In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting
of the Association for Computational Linguistics, pages 801?808. Association for Computational Linguistics.
Richard Socher, Brody Huval, Christopher D Manning, and Andrew Y Ng. 2012. Semantic compositionality
through recursive matrix-vector spaces. In Proceedings of the 2012 Joint Conference on Empirical Methods in
Natural Language Processing and Computational Natural Language Learning, pages 1201?1211.
Gy?orgy Szarvas, Chris Biemann, and Iryna Gurevych. 2013. Supervised all-words lexical substitution using
delexicalized features. In Proceedings of the 2013 Conference of the North American Chapter of the Association
for Computational Linguistics: Human Language Technologies, pages 1131?1141, Atlanta, Georgia, June.
Idan Szpektor and Ido Dagan. 2008. Learning entailment rules for unary templates. In Proceedings of the
22nd International Conference on Computational Linguistics (Coling 2008), pages 849?856, Manchester, UK,
August.
Julie Weeds and David Weir. 2003. A general framework for distributional similarity. In Michael Collins and Mark
Steedman, editors, Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing,
pages 81?88.
Julie Weeds, David Weir, and Diana McCarthy. 2004. Characterising measures of lexical distributional similarity.
In Proceedings of Coling 2004, pages 1015?1021, Geneva, Switzerland, Aug 23?Aug 27. COLING.
Julie Weeds, David Weir, and Jeremy Reffin. 2014. Distributional composition using higher-order dependency
vectors. In Proceedings of the 2nd Workshop on Continuous Vector Space Models and their Compositionality,
EACL 2014, Gothenburg, Sweden, April.
Dominic Widdows. 2008. Semantic vector products: Some initial investigations. In Proceedings of the Second
Symposium on Quantum Interaction, Oxford, UK, pages 1?8.
Wen-tau Yih, Geoffrey Zweig, and John Platt. 2012. Polarity inducing latent semantic analysis. In Proceedings of
the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1212?1222, Jeju Island, Korea, July. Association for Computational Linguistics.
Chen Zhang and Joyce Chai. 2010. Towards conversation entailment: An empirical investigation. In Proceedings
of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 756?766, Cambridge,
MA, October. Association for Computational Linguistics.
2259
Unsupervised Acquisition of Predominant
Word Senses
Diana McCarthy
University of Sussex
Rob Koeling
University of Sussex
Julie Weeds
University of Sussex
John Carroll?
University of Sussex
There has been a great deal of recent research into word sense disambiguation, particularly
since the inception of the Senseval evaluation exercises. Because a word often has more than
one meaning, resolving word sense ambiguity could benefit applications that need some level
of semantic interpretation of language input. A major problem is that the accuracy of word
sense disambiguation systems is strongly dependent on the quantity of manually sense-tagged
data available, and even the best systems, when tagging every word token in a document,
perform little better than a simple heuristic that guesses the first, or predominant, sense of a
word in all contexts. The success of this heuristic is due to the skewed nature of word sense
distributions. Data for the heuristic can come from either dictionaries or a sample of sense-
tagged data. However, there is a limited supply of the latter, and the sense distributions and
predominant sense of a word can depend on the domain or source of a document. (The first
sense of ?star? for example would be different in the popular press and scientific journals).
In this article, we expand on a previously proposed method for determining the predominant
sense of a word automatically from raw text. We look at a number of different data sources and
parameterizations of the method, using evaluation results and error analyses to identify where
the method performs well and also where it does not. In particular, we find that the method
does not work as well for verbs and adverbs as nouns and adjectives, but produces more accurate
predominant sense information than the widely used SemCor corpus for nouns with low coverage
in that corpus. We further show that the method is able to adapt successfully to domains when
using domain specific corpora as input and where the input can either be hand-labeled for domain
or automatically classified.
? Department of Informatics, Brighton BN1 9QH, UK. E-mail: {dianam,robk,juliewe,johnca}@sussex.ac.uk.
Submission received: 16 November 2005; revised submission received: 12 July 2006; accepted for publication
16 February 2007.
? 2007 Association for Computational Linguistics
Computational Linguistics Volume 33, Number 4
1. Introduction
In word sense disambiguation, the ?first sense? heuristic (choosing the first, or predom-
inant sense of a word) is used by most state-of-the-art systems as a back-off method
when information from the context is not sufficient to make a more informed choice.
In this article, we present an in-depth study of a method for automatically acquiring
predominant senses for words from raw text (McCarthy et al 2004a).
The method uses distributionally similar words listed as ?nearest neighbors?
in automatically acquired thesauruses (e.g., Lin 1998a), and takes advantage of the
observation that the more prevalent a sense of a word, the more neighbors will relate
to that sense, and the higher their distributional similarity scores will be. The senses
of a word are defined in a sense inventory. We use WordNet (Fellbaum 1998) because
this is widely used, is publicly available, and has plenty of gold-standard evaluation
data available (Miller et al 1993; Cotton et al 2001; Preiss and Yarowsky 2001; Mihalcea
and Edmonds 2004). The distributional strength of the neighbors is associated with the
senses of a word using a measure of semantic similarity which relies on the relationships
between word senses, such as hyponyms (available in an inventory such as WordNet)
or overlap in the definitions of word senses (available in most dictionaries), or both.
In this article we provide a detailed discussion and quantitative analysis of the
motivation behind the first sense heuristic, and a full description of our method. We
extend previously reported work in a number of different directions:
 We evaluate the method on all parts of speech (PoS) on SemCor (Miller
et al 1993). Previous experiments (McCarthy et al 2004c) evaluated only
nouns on SemCor, or all PoS but only on the Senseval-2 (Cotton et al 2001)
and Senseval-3 (Mihalcea and Edmonds 2004) data. The evaluation on all
PoS is much more extensive because the SemCor corpus is composed of
220,000 words in contrast to the 6 documents in the Senseval-2 and -3
English all words data (10,000 words).
 We compare two WordNet similarity measures in our evaluation on
nouns, and also contrast performance using two publicly available
thesauruses, both produced from the same NEWSWIRE corpus, but one
derived using a proximity-based approach and the other using
dependency relations from a parser. It turns out that the results from the
proximity-based thesaurus are comparable to those from the dependency-
based thesaurus; this is encouraging for applying the method to languages
without sophisticated analysis tools.
 We manually analyze a sample of errors from the SemCor evaluation. A
small number of errors can be traced back to inherent shortcomings of our
method, but the main source of error is due to noise from related senses.
This is a common problem for all WSD systems (Ide and Wilks 2006) but
one which is only recently starting to be addressed by the WSD
community (Navigli, Litkowski, and Hargraves 2007).
 One motivation for an automatic method for acquiring predominant
senses is that there will always be words for which there are insufficient
data available in manually sense-tagged resources. We compare the
performance of our automatic method with the first sense heuristic
derived from SemCor on nouns in the Senseval-2 data. We find that the
554
McCarthy, Koeling, Weeds, and Carroll Acquisition of Predominant Word Senses
automatic method outperforms the one obtained from manual annotations
in SemCor for nouns with fewer than five occurrences in SemCor.
 Aside from the lack of coverage of manually annotated data, there is a
need for first sense heuristics to be specific to domain. We explore the
potential for applying the method with domain-specific text for all PoS in
an experiment using a gold-standard domain-specific resource (Magnini
and Cavaglia` 2000) which we have used previously only with nouns. We
show that although there is a little mileage to be had from domain-specific
first sense heuristics for verbs, nouns benefit greatly from domain-specific
training.
 In previous work (Koeling, McCarthy, and Carroll 2005) we produced
manually sense-annotated domain-specific test corpora for a lexical
sample, and demonstrated that predominant senses acquired (from
hand-classified corpora) in the same domain as the test data outperformed
the SemCor first sense. We further this exploration by contrasting with
results from training on automatically categorized text from the English
Gigaword Corpus and show that the results are comparable to those using
hand-classified domain data.
The article is organized as follows. In the next section we motivate the use of pre-
dominant sense information in WSD systems and the need for acquiring this information
automatically. In Section 3 we give an overview of related work in WSD, focusing on the
acquisition of prior sense distributions and domain-specific sense information. Section 4
describes our acquisition method. Section 5 describes the experimental setup for the
work reported in this article. Section 6 describes four experiments. The first evaluates
the first sense heuristic using predominant sense information acquired for all PoS on
SemCor; for nouns we compare two semantic similarity methods and three different
types of distributional thesaurus. We also report an error analysis for all PoS of our
method. The second experiment compares the performance of the automatic method
to the manually produced data in SemCor, on nouns in the Senseval-2 data, looking
particularly at nouns which have a low frequency in SemCor. The third uses corpora in
restricted domains and the subject field code gold standard of Magnini and Cavaglia`
(2000) to investigate the potential for domain-specific rankings for different PoS. The
fourth compares results when we train and test on domain-specific corpora, where
the training data is (1) manually categorized for domain and from the same corpus
as the test data, and (2) where the training data is harvested automatically from another
corpus which is categorized automatically. Finally, we conclude (Section 7) and discuss
directions for future work (Section 8).
2. Motivation
The problem of disambiguating the meanings of words in text has received much
attention recently, particularly since the inception of the Senseval evaluation exercises
(Kilgarriff and Palmer 2000; Preiss and Yarowsky 2001; Mihalcea and Edmonds 2004).
One of the standard Senseval tasks (the ?all words? task) is to tag each open class word
with one of its senses, as listed in a dictionary or thesaurus such as WordNet (Fellbaum
1998). The most accurate word sense disambiguation (WSD) systems use supervised
machine learning approaches (Stevenson and Wilks 2001), trained on text which has
been sense tagged by hand. However, the performance of these systems is strongly
555
Computational Linguistics Volume 33, Number 4
dependent on the quantity of training data available (Yarowsky and Florian 2002),
and manually sense-annotated text is extremely costly to produce (Kilgarriff 1998). The
largest all words sense tagged corpus is SemCor, which is 220,000 words taken from
103 passages, each of about 2,000 words, from the Brown corpus (Francis and Kuc?era
1979) and the complete text of a 19th-century American novel, The Red Badge of Courage,
which totals 45,600 words (Landes, Leacock, and Tengi 1998). Approximately half of the
words in this corpus are open-class words (nouns, verbs, adjectives, and adverbs) and
these have been linked to WordNet senses by human taggers using a software interface.
The shortage of training data due to the high costs of tagging texts has motivated
research into unsupervised methods for WSD. But in the English all-words tasks in
Senseval-2 and Senseval-3 (Snyder and Palmer 2004), systems that did not make
use of hand-tagged data (in some form or other) performed substantially worse than
those that did. Table 1 summarizes the situation. It gives the precision and recall of
the best1 two supervised (S) and unsupervised (U)2 systems for the English all words
and English lexical sample for Senseval-23 and -3, along with the first sense baseline
(FS) reported by the task organizers.4 This is a simple application of the ?first sense?
heuristic?that is, using the most common sense of a word for every instance of it in the
test corpus, regardless of context. Although contextual WSD is of course preferable, the
baseline is a very powerful one and unsupervised systems find it surprisingly hard to
beat (indeed, some of the systems that report themselves as unsupervised actually make
some use of a manually obtained first-sense heuristic). Considering both precision and
recall, only 5 of 26 systems in the Senseval-3 English all-words task beat the first sense
heuristic as derived from SemCor (61.5%5), and then by only a few percentage points
(the top system scoring 65% precision and recall) despite using hand-tagged training
data available from SemCor and previous Senseval data sets, large sets of contextual
features, and sophisticated machine learning algorithms.
The performance of WSD systems, at least for all-words tasks, seems to have
plateaued at a level just above the first sense heuristic (Snyder and Palmer 2004). This is
due to the shortage of training data and the often fine granularity of sense distinctions.
Ide and Wilks (2006) argue that it is best to concentrate effort on distinctions which
are useful for applications and where systems can be confident of high precision. In
cases where systems are less confident, but word senses, rather than words, are needed,
the first sense heuristic is a powerful back-off strategy. This strategy is dependent on
information provided in dictionaries. Two dictionaries that have been used by English
WSD systems are the Longman Dictionary of Contemporary English (LDOCE) (Procter
1 We rank the systems by the recall scores, because this is the accuracy over the entire test set regardless of
how many items were attempted.
2 Note that the classification of systems as unsupervised is not straightforward. Systems reported as
unsupervised in the Senseval proceedings sometimes make use of some manual annotations. For
example, the top scoring system that reported itself unsupervised in the Senseval-3 lexical sample task
used manually sense-tagged training data for constructing glosses.
3 The verb lexical sample was done as a separate exercise for Senseval-2, and for brevity we have not
included the results from this task.
4 The all-words task organizers used the first sense as listed in WordNet. This is based on the SemCor first
sense because WordNet senses are ordered according to the frequency data in SemCor. However, where
senses are not found in WordNet, the ordering is arbitrarily determined as a function of the ?grind?
program (see http://wordnet.princeton.edu/man/grind.1WN.htm). The lexical sample task organizers
state that they use the ?most frequent sense? but do not stipulate if this is taken from WordNet, or
directly from SemCor.
5 This figure is the arithmetic mean of two published estimates (Snyder and Palmer 2004), the difference
being due to the treatment of multiwords.
556
McCarthy, Koeling, Weeds, and Carroll Acquisition of Predominant Word Senses
Table 1
The best two performing systems of each type (according to fine-grained recall) in Senseval-2
and -3.
All words Lexical sample
Precision (%) Recall (%) Precision (%) Recall (%)
Senseval-2 S 69.0 69.0 64.2 64.2
Senseval-2 S 63.6 63.6 63.8 63.8
Senseval-2 U 45.1 45.1 40.2 40.1
Senseval-2 U 36.0 36.0 58.1 31.9
FS baseline 57.0 57.0 47.6 47.6
Senseval-3 S 65.1 65.1 72.9 72.9
Senseval-3 S 65.1 64.2 72.6 72.6
Senseval-3 U 58.3 58.2 66.1 65.7
Senseval-3 U 55.7 54.6 56.3 56.3
FS baseline 61.5 61.5 55.2 55.2
1978) and WordNet (Fellbaum 1998). These both provide a ranking of senses accord-
ing to their predominance. The sense ordering in LDOCE is based on lexicographer
intuition, whereas in WordNet the senses are ordered according to their frequency in
SemCor (Miller et al 1993).
There are two major problems with deriving a first sense heuristic from these types
of resources. The first is that the predominant sense of a word varies according to
the source of the document (McCarthy and Carroll 2003) and with the domain. For
example, the first sense of star as derived from SemCor is celestial body, but if one were
disambiguating popular news stories then celebrity would be more likely. Domain,
topic, and genre are important in WSD (Martinez and Agirre 2000; Magnini et al 2002)
and the sense-frequency distributions of words depend on all of these factors. Any
dictionary will provide only a single sense ranking, whether this is derived from sense-
tagged data as in WordNet, lexicographer intuition as in LDOCE, or inspection of corpus
data as in the Oxford Advanced Learner?s Dictionary (Hornby 1989). A fixed order of
senses may not reflect the data that an NLP system is dealing with.
The second problem with obtaining predominant sense information applies to the
use of hand-tagged resources, such as SemCor. Such resources are relatively small due
to the cost of manual tagging (Kilgarriff 1998). Many words will simply not be covered,
or occur only a few times. For many words in WordNet the ordering of word senses is
based on a very small number of occurrences in SemCor. For example, the first sense
of tiger is an audacious person whereas most people would assume the carnivorous
animal sense is more prevalent. This is because the two senses each occur exactly once
in SemCor, and when there is no frequency information to break the tie the WordNet
sense ordering is assigned arbitrarily. There are many fairly common words (such as
the noun crane) which do not occur at all in SemCor. Table 2 gives the number and
percentage of words6 in WordNet and the BNC which do not occur in SemCor. As one
would expect from Zipf?s law, a substantial number of words do not occur in SemCor,
even when we do not consider multiwords. Many of these words are extremely rare, but
6 Here and elsewhere in this article we give figures only for words without embedded spaces, that is, not
multiwords.
557
Computational Linguistics Volume 33, Number 4
Table 2
Words (excluding multiwords) in WordNet 1.7.1 and the BNC without any data in SemCor.
WordNet types BNC types
PoS No. % No. %
noun 43,781 81.9 360,535 97.5
verb 4,741 56.4 25,292 87.6
adjective 14,991 72.3 95,908 95.4
adverb 2,405 64.4 10,223 89.2
Table 3
Polysemous word types in the Senseval-2 and -3 English all-words tasks test documents with no
data in SemCor (0 columns), or with very little data (? 1 and ? 5 occurrences). Note that there
are no annotations for adverbs in the Senseval-3 documents.
Senseval-2 Senseval-3
0 ? 1 ? 5 0 ? 1 ? 5
PoS No. % No. % No. % No. % No. % No. %
noun 12 3.2 28 7.4 49 12.9 13 3.1 26 6.3 69 16.7
verb 7 2.1 11 3.4 28 8.6 3 0.9 10 2.9 36 10.4
adjective 9 4.2 16 7.4 50 23.1 8 4.7 15 8.9 33 19.5
adverb 1 0.9 1 0.9 2 1.8 ? ? ? ? ? ?
in any given document it is likely that there will be at least some words without SemCor
data. Table 3 quantifies this, for the Senseval-2 and -3 all-words tasks test data, showing
the percentage of polysemous word types with no frequency information in SemCor, the
percentage with zero or one occurrences, and the percentage with up to five occurrences.
(For example, the table indicates that 12.9% of nouns in the Senseval-2 data, and 16.7%
in Senseval-3, have five or fewer occurrences in SemCor.) Thus, although SemCor may
cover many frequently occurring word types in a given document, there are likely to be
a substantial proportion for which there is very little or no information available.
Tables 4 and 5 present an analysis of the actual ambiguity of polysemous words
within the six documents making up the Senseval-2 and -3 all-words test data. They
show the extent to which these words are used in a predominant sense, within a
document, and the extent to which this is the same as that given by SemCor. The two
tables share a common format: columns 2?5 give percentages over all ?document/word
type? combinations. The second column shows the percentage of the ?document/word
type? combinations where the word is used in the document in only one of its senses.
The fourth column shows the same percentage but for ?document/word type? combi-
nations where the word is used in more than one sense in the document. The third and
fifth columns give the percentage of the words in the preceding columns (second and
fourth, respectively) where the first sense for the word in the document is the same as in
SemCor (FS = SC FS). For the third column, this is the only sense that this word appears
in within the document. (Note that for any row, columns 2 and 4 account for all possibil-
ities so will always add up to 100.) The sixth column gives the mean degree of polysemy,
according to WordNet, for the set of words that these figures are calculated for.
558
McCarthy, Koeling, Weeds, and Carroll Acquisition of Predominant Word Senses
Table 4
Most frequent sense analysis for Senseval-2 and -3 polysemous lemmas occurring more than
once in a document (adverb data is only from Senseval-2).
1 sense > 1 sense
PoS % FS = SC FS % % FS = SC FS % Mean polysemy
noun 72.2 52.2 27.8 7.3 5.9
verb 45.6 25.1 54.4 16.9 12.7
adjective 62.9 40.5 37.1 10.3 4.8
adverb 64.7 50.0 35.3 17.6 4.7
The figures in Table 4 are for words occurring more than once in a given Senseval
test document. The tendency for words to be used in only one sense in any given
document7 is strongest for nouns, although adverbs and adjectives also tend towards
one sense. Verbs are on average much more polysemous than the other parts of speech
yet still 45.6% of polysemous verbs which occur more than once are used in only a single
sense. However, because verbs are in general more polysemous, it makes it less likely
that if a verb occurs in only one sense in a document then it will be the one indicated by
SemCor.
The figures in Table 5 are for all words in the Senseval documents (not just those oc-
curring more than once), showing the accuracy of a SemCor-derived first-sense heuristic
for words with a frequency below a specified threshold (column 1) in SemCor. The table
shows that although having a first sense from SemCor is certainly useful, when looking
at figures for all the words in the Senseval documents a good proportion have first
senses other than the one indicated by SemCor. Furthermore, the lower the frequency
in SemCor the more likely that the first sense indicated by SemCor is wrong. (However,
the situation is slightly different for adverbs because there are not many with low
frequency in SemCor and they are on average not very polysemous, so for them a first
sense derived from a resource like SemCor?where one exists?is possibly sufficient.)
These results show that although SemCor is a useful resource, there will always be
words for which its coverage is inadequate. In addition, few languages have extensive
hand-tagged resources or sense orderings produced by lexicographers. Moreover, gen-
eral resources containing word sense information are not likely to be appropriate when
processing language for a wider variety of domains, topics, and genres. What is needed
is a means to find predominant senses automatically.
3. Related Work
Most research in WSD to date has concentrated on using contextual features, typically
neighboring words, to help infer the correct sense of a target word. In contrast, our
work is aimed at discovering the predominant sense of a word from raw text because
7 The tendency for words to be used in only one sense in a given discourse is weaker for fine-grained
distinctions (Krovetz 1998) compared to coarse-grained distinctions (Gale, Church, and Yarowsky 1992).
Nevertheless, even with a fine-grained inventory the first sense heuristic is certainly powerful, as shown
in Table 1.
559
Computational Linguistics Volume 33, Number 4
Table 5
Most frequent sense analysis for all polysemous lemmas in the Senseval-2 and -3 test data,
broken down by their frequencies of occurrence in SemCor (adverb data is only from
Senseval-2).
1 sense > 1 sense
Frequency % FS = SC FS % % FS = SC FS % Mean polysemy
noun
? 1 (54) 96.3 24.1 3.7 0.0 2.8
? 5 (118) 96.6 43.2 3.4 0.0 3.2
? 10 (191) 96.9 48.7 3.1 0.0 3.3
all (792) 88.8 51.6 11.2 2.5 5.5
verb
? 1 (21) 100.0 33.3 0.0 0.0 2.4
? 5 (64) 98.4 35.9 1.6 1.6 3.2
? 10 (110) 98.2 38.2 1.8 1.8 3.5
all (671) 82.6 39.3 17.4 5.1 9.0
adjective
? 1 (31) 93.5 19.4 6.5 0.0 2.5
? 5 (83) 95.2 34.9 4.8 1.2 2.7
? 10 (120) 90.8 40.8 9.2 1.7 2.8
all (385) 82.6 46.2 17.4 3.6 5.1
adverb
? 1 (1) 0.0 0.0 100.0 0.0 2.0
? 5 (2) 50.0 50.0 50.0 0.0 2.0
? 10 (8) 87.5 62.5 12.5 0.0 2.3
all (111) 82.9 62.2 17.1 5.4 4.0
the first sense heuristic is so powerful, and because manually sense-tagged data is not
always available.
Lapata and Brew (2004) highlighted the importance of a good prior in WSD. They
used syntactic evidence to find a prior distribution for Levin (1993) verb classes, and
incorporated this in a WSD system. Lapata and Brew obtained their priors for verb
classes directly from subcategorization evidence in a parsed corpus, whereas we use
parsed data to find distributionally similar words (nearest neighbors) to the target
word which reflect the different senses of the word and have associated distributional
similarity scores which can be used for ranking the senses according to prevalence.
We would, however, agree that subcategorization evidence should be very useful for
disambiguating verbs, and would hope to combine such evidence with our ranking
models for context-based WSD.
A major benefit of our work is that this method permits us to produce predominant
senses for any desired domain and text type. Buitelaar and Sacaleanu (2001) explored
ranking and selection of synsets in GermaNet for specific domains using the words
in a given synset, and those related by hyponymy, and a term relevance measure
taken from information retrieval. Buitelaar and Sacaleanu evaluated their method on
identifying domain-specific concepts using human judgments on 100 items. We evaluate
560
McCarthy, Koeling, Weeds, and Carroll Acquisition of Predominant Word Senses
our method using publicly available resources for balanced text, and, for domain-
specific investigations, resources we have developed ourselves (Koeling, McCarthy,
and Carroll 2005). Magnini and Cavaglia` (2000) associated WordNet word senses with
particular domains, and this has proved useful for high precision WSD (Magnini et
al. 2001); indeed, we have used their domain labels (or subject field codes, SFCs) for
evaluation (Section 6.3). Identification of these SFCs for word senses was semi-automatic
and required a considerable amount of hand-labeling. Our approach requires only raw
text from the given domain and because of this it can easily be applied to a new domain
or sense inventory, as long as there is enough appropriate text.
There are other approaches aimed at gleaning domain-specific information from
raw data. Gliozzo, Giuliano, and Strapparava (2005) induced domain models from raw
data using unsupervised latent semantic models and then fed this into a supervised
WSD model and evaluated on Senseval-3 lexical sample data in four languages. Chan
and Ng (2005) obtained probability distributions to feed into their supervised WSD mod-
els. They used multilingual parallel corpus data to provide probability estimates for a
subset of 22 nouns from the lexical sample task. They then fed this into a supervised WSD
model and verified that the estimates for prior distributions improved performance for
supervised WSD. We intend eventually to use our prevalence scores to feed into un-
supervised WSD models. Although unsupervised models seem to be beaten whenever
there is training data to be had, we anticipate that unsupervised models with improved
priors from the ranking might outperform supervised systems in situations where there
is little training data available. Whereas this article is about finding predominant senses
for back-off in a WSD system, the method could be applied to finding a prior distribution
over all word senses of each target word. It is our intention that the back-off models pro-
duced by our prevalence ranking, either as predominant senses or prior distributions
over word senses, could be combined with contextual information for WSD.
Mohammad and Hirst (2006) describe an approach to acquiring predominant senses
from corpora which makes use of the category information in the Macquarie Thesaurus.
Evaluation is performed on an artificially constructed test set from unambiguous words
in the same category as the 27 test words (nouns, verbs, and adjectives). The senses of
the words are the categories of the thesaurus and the experiment uses only two senses
of each word, the two most predominant ones. The predominance of the two senses is
altered systematically. The results are encouraging because a much smaller amount of
corpus data is needed compared to our approach. However, their method has only been
applied to an artificially constructed test set, rather than a publicly available corpus, and
has yet to be applied in a domain-specific setting, which is the chief motivation of our
work.
The work of Pantel and Lin (2002) is probably the most closely related study
that predates ours, although their ultimate goal is different. Pantel and Lin devised
a method called CBC (clustering by committee) where the 10 nearest neighbors of
a word in a distributional thesaurus are clustered to identify the various senses of
the word. Pantel and Lin use a measure of semantic similarity (Lin 1997) to evaluate
the discovered classes with respect to WordNet as a gold standard. The CBC method
obtained a precision of 61% (the percentage of senses discovered that did exist in
WordNet) and a recall of 51% (the percentage of senses discovered from the union of
those discovered with different clustering algorithms that they tried).8
8 The calculation of recall was over the union of senses discovered automatically, rather than over the
senses in WordNet, because senses in WordNet may be unattested in the data.
561
Computational Linguistics Volume 33, Number 4
Pantel and Lin?s approach is related to ours in that, in their sense discovery pro-
cedure, predominant senses have more of a chance of being found than other senses,
although their algorithm is specifically tailored to look for senses regardless of fre-
quency. To do this the algorithm removes neighbors of the target word once they
are assigned to a cluster so that less frequent senses can be discovered. Our method,
described in detail in Section 4, associates the nearest neighbors to the senses of the
target in a predefined inventory (we use WordNet). We rank the senses using a measure
which sums over the distributional similarity of neighbors weighted by the strength of
the association between the neighbors and the sense. This is done on the assumption
that more prevalent senses will have strong associations with more nearest neighbors
because they have occurred in more contexts in the corpus used for producing the
thesaurus. Both the number and the distributional similarity of the neighbors are used
in our prevalence ranking measure. Pantel and Lin process the possible clusters in order
of their average distributional similarity and number of neighbors but do not take the
number of neighbors into account in the scores given for the clusters. The measures
that Pantel and Lin associate with their clusters are determined by the cohesiveness
of the cluster with the target word because their aim is one of sense discovery. Their
measure is the similarity between the cluster and the target word and does not retain
the distributional similarity of the neighbors within the cluster. It is quite possible that
there is a low frequency sense of a target word with synonyms that form a nice cohesive
group.
Although the number of neighbors assigned to a cluster may correlate with our
ranking score, intuition suggests that a combination of the quantity and distributional
similarity of neighbors to the target word sense is best for determining the relative
predominance of senses. In Section 6 we test this hypothesis using a simplified version
of our method which only uses the number of neighbors, and assigns each to one
sense. Comparisons with the CBC algorithm as it stands would be difficult because
in order to evaluate acquisition of predominance information we have used publicly
available gold-standard sense-tagged corpora, and these have WordNet senses. CBC
will not always find WordNet senses. For example, using the on-line demonstration of
CBC,9 several common senses from nouns from the Senseval-2 lexical sample are not
discovered, including the upright object sense of post, the block of something sense
of bar, the daytime sense of day and the meaning of the word sense of the word sense.
Automatic acquisition of sense inventories is an important endeavor, and we hope to
look at ways of combining our method for detecting predominance with automatically
induced inventories such as those produced by CBC. Evaluation of induced inventories
should be done in the context of an application, because the senses will be keyed to the
acquisition corpus and not to WordNet.
Induction of senses allows coverage of senses appearing in the data that are not
present in a predefined inventory. Although we could adapt our method for use with
an automatically induced inventory, our method which uses WordNet might also be
combined with one that can automatically find new senses from text and then relate
these to WordNet synsets, as Ciaramita and Johnson (2003) and Curran (2005) do with
unknown nouns.
9 We used the demonstration at http://www.isi.edu/~pantel/Content/Demos/LexSem/cbc.htm with the
option to include all corpora (TREC-2002, TREC-9, and COSMOS).
562
McCarthy, Koeling, Weeds, and Carroll Acquisition of Predominant Word Senses
4. Method
In our method, the predominant sense for a target word is determined from a preva-
lence ranking of the possible senses for that word. The senses come from a predefined
inventory (which might be a dictionary or WordNet-like resource). The ranking is
derived using a distributional thesaurus automatically produced from a large corpus,
and a semantic similarity measure defined over the sense inventory. The distributional
thesaurus contains a set of words that are ?nearest neighbors? to the target word with
respect to similarity of the way in which they are distributed. (Distributional similarity
is based on the hypothesis of Harris, 1968, that words which occur in similar contexts
have related meanings.) The thesaurus assigns a distributional similarity score to each
neighbor word, indicating its closeness to the target word. For example, the nearest10
neighbors of sandwich might be:
salad, pizza, bread, soup...
and the nearest neighbors of the polysemous noun star11 might be:
actor, footballer, planet, circle...
These neighbors reflect the various senses of the word, which for star might be:
 a celebrity
 a celestial body
 a shape
 a sign of the zodiac12
We assume that the number and distributional similarity scores of neighbors pertaining
to a given sense of a target word will reflect the prevalence of that sense in the corpus
from which the thesaurus was derived. This is because the more prevalent senses of the
word will appear more frequently and in more contexts than other, less prevalent senses.
The neighbors of the target word relate to its senses, but are themselves word forms
rather than senses. The senses of the target word are predefined in a sense inventory
and we use a semantic similarity score defined over the sense inventory to relate the
neighbors to the various senses of the target word. The two semantic similarity scores
that we use in this article are implemented in the WordNet similarity package. One uses
the overlap in definitions of word senses, based on Lesk (1986), and the other uses a
combination of corpus statistics and the WordNet hyponym hierarchy, based on Jiang
and Conrath (1997). We describe these fully in Section 4.2. We now describe intuitively
10 In this and other examples we restrict ourselves to four neighbors for brevity.
11 In this example we assume that the sense inventory assigns four senses to star, but the inventory could
assign fewer or more depending on its level of granularity and level of detail.
12 Note that this zodiac or horoscope sense of star usually occurs as part of the multiword star sign (e.g.,
your star sign secrets revealed) or in plural form (your stars today?free online).
563
Computational Linguistics Volume 33, Number 4
the measure for ranking the senses according to predominance, and then give a more
formal definition.
The measure uses the sum total of the distributional similarity scores of the k nearest
neighbors. This total is divided between the senses of the target word by apportioning
the distributional similarity of each neighbor to the senses. The contribution of each
neighbor is measured in terms of its distributional similarity score so that ?nearer?
neighbors count for more. The distributional similarity score of each neighbor is divided
between the various senses rather than attributing the neighbor to only one sense. This
is done because neighbors can relate to more than one sense due to relationships such
as systematic polysemy. For example, in the thesaurus we describe subsequently in
Section 4.1 acquired from the BNC, chicken has neighbors duck and goose which relate to
both the meat and animal senses. We apportion the contribution of a neighbor to each
of the word senses according to a weight which is the normalized semantic similarity
score between the sense and the neighbor. We normalize the semantic similarity scores
because some of the semantic similarity scores that we use, described in Section 4.2,
can get disproportionately large. Because we normalize the semantic similarity scores,
the sum of the ranking scores for a word equals the sum of the distributional similarity
scores. To summarize, we rank the senses of the target word, such as star, by apportion-
ing the distributional similarity scores of the top k neighbors between the senses. Each
distributional similarity score (dss) is weighted by a normalized semantic similarity
score (sss) between the sense and the neighbor. This process is illustrated in Figure 1.
More formally, to find the predominant sense of a word (w) we take each sense
in turn and obtain a prevalence score. Let Nw = {n1, n2...nk} be the ordered set of the
top scoring k neighbors of w from the distributional thesaurus with associated scores
{dss(w, n1), dss(w, n2), ...dss(w, nk)}. Let senses(w) be the set of senses of w in the sense
inventory. For each sense of w (si ? senses(w)) we obtain a prevalence score by summing
over the dss(w, nj) of each neighbor (nj ? Nw) multiplied by a weight. This weight is the
sss between the target sense (si) and nj divided by the sum of all sss scores for senses(w)
Figure 1
The prevalence ranking process for the noun star.
564
McCarthy, Koeling, Weeds, and Carroll Acquisition of Predominant Word Senses
and nj. sss is the maximum WordNet similarity score (sss?) between si and the senses of
nj (sx ? senses(nj)).13 Each sense si ? senses(w) is therefore assigned a score as follows:
Prevalence Score(w, si) =
?
nj?Nw
dss(w, nj) ?
sss(si, nj)
?
si??senses(w) sss(si? , nj)
(1)
where
sss(si, nj) = max
sx?senses(nj )
sss?(si, sx) (2)
We describe dss and sss? in Sections 4.1 and 4.2. Note that the dss for a given neighbor
is shared between the different senses of w depending on the weight given by the
normalized sss.
4.1 The Distributional Similarity Score
Measures of distributional similarity take into account the shared contexts of the two
words. Several measures of distributional similarity have been described in the litera-
ture. In our experiments, dss is computed using Lin?s similarity measure (Lin 1998a).
We set the number of nearest neighbors to equal 50.14 We use three different sources of
data for our first two experiments, resulting in three distributional thesauruses. These
are described in the next section. We use domain-specific data for our third and fourth
experiments. The data sources for these are described in Sections 6.3 and 6.4.
A word, w, is described by a set of features, f , each with an associated frequency,
where each feature is a pair ?r, x? consisting of a grammatical relation name and the
other word in the relation. We computed distributional similarity scores for every pair of
words of the same PoS where each word?s total feature frequency was at least 10. A the-
saurus entry of size k for a target word w is then defined as the k most similar words to w.
A large number of distributional similarity measures have been proposed in the
literature (see Weeds 2003 for a review) and comparing them is outside the scope of this
work. However, the study of Weeds and Weir (2005) provides interesting insights into
what makes a ?good? distributional similarity measure in the contexts of semantic simi-
larity prediction and language modeling. In particular, weighting features by pointwise
mutual information (Church and Hanks 1989) appears to be beneficial. The pointwise
mutual information (I(w, f )) between a word and a feature is calculated as
I(w, f ) = log
P( f |w)
P( f )
(3)
Intuitively, this means that the occurrence of a less-common feature is more important
in describing a word than a more-common feature. For example, the verb eat is more
selective and tells us more about the meaning of its arguments than the verb be.
13 We use sss for the semantic similarity between a WordNet sense and another word, the neighbor. We use
sss? for the semantic similarity between two WordNet senses, si and a sense of the neighbor (sx).
14 From previous work (McCarthy et al 2004b), the value of k has a minimal effect on finding the
predominant sense; however, we will continue experimentation with this in the future for using our
ranking score for estimating probability distributions of senses, because a sufficiently large value of k will
be needed to include neighbors for rarer senses.
565
Computational Linguistics Volume 33, Number 4
We chose to use the distributional similarity score described by Lin (1998a) because
it is an unparameterized measure which uses pointwise mutual information to weight
features and which has been shown (Weeds 2003) to be highly competitive in making
predictions of semantic similarity. This measure is based on Lin?s information-theoretic
similarity theorem (Lin 1997):
The similarity between A and B is measured by the ratio between the amount of
information needed to state the commonality of A and B and the information needed to
fully describe what A and B are.
In our application, if T(w) is the set of features f such that I(w, f ) is positive, then the
similarity between two words, w and n, is
dss(w, n) =
?
f?T(w)?T(n)
(
I(w, f ) + I(n, f )
)
?
f?T(w) I(w, f ) +
?
f?T(n) I(n, f )
(4)
However, due to this choice of dss and the openness of the domain, we restrict ourselves
to only considering words with a total feature frequency of at least 10. Weeds et al (2005)
do show that distributional similarity can be computed for lower frequency words but
this is using a highly specialized corpus of 400,000 words from the biomedical domain.
Further, it has been shown (Weeds et al 2005; Weeds and Weir 2005) that performance
of Lin?s distributional similarity score decreases more significantly than other measures
for low frequency nouns. We leave the investigation of other distributional similarity
scores and the application to smaller corpora as areas for further study.
4.2 The Semantic Similarity Scores
WordNet is widely used for research in WSD because it is publicly available and there
are a number of associated sense-tagged corpora (Miller et al 1993; Cotton et al 2001;
Preiss and Yarowsky 2001; Mihalcea and Edmonds 2004) available for testing purposes.
Several semantic similarity scores have been proposed that leverage the structure of
WordNet; for sss? we experiment with two of these, as implemented in the WordNet
Similarity Package (Patwardhan and Pedersen 2003).
The WordNet Similarity Package implements a range of similarity scores. McCarthy
et al (2004b) experimented with six of these for the sss? used in the prevalence score,
Equation (2). In the experiments reported here we use the two scores that performed
best in that previous work. We briefly summarize them here; Patwardhan, Banerjee,
and Pedersen (2003) give a more detailed discussion. The scores measure the similarity
between two WordNet senses (s1 and s2).
lesk This measure (Banerjee and Pedersen 2002) maximizes the number of overlap-
ping words in the gloss, or definition, of the senses. It uses the glosses of semanti-
cally related (according to WordNet) senses too. We use the default version of the
measure in the package with no normalizing for gloss length, and the default set
of relations:
lesk(s1, s2) = |{W1 ? definition(s1)}| ? |{W2 ? definition(s2)}| (5)
where definitions(s) is the gloss definition of sense s concatenated with the gloss
definitions of the senses related to s where the relationships are defined by the de-
566
McCarthy, Koeling, Weeds, and Carroll Acquisition of Predominant Word Senses
fault set of relations in the relations.dat file supplied with the WordNet Similarity
package. W ? definition(s) is the set of words from the concatenated definitions.
jcn This measure (Jiang and Conrath 1997) uses corpus data to populate classes
(synsets) in the WordNet hierarchy with frequency counts. Each synset is incre-
mented with the frequency counts (from the corpus) of all words belonging to
that synset, directly or via the hyponymy relation. The frequency data is used to
calculate the ?information content? (IC; Resnik 1995) of a class as follows:
IC(s) = ?log(p(s)) (6)
Jiang and Conrath specify a distance measure:
Djcn(s1, s2) = IC(s1) + IC(s2) ? 2 ? IC(s3) (7)
where the third class (s3) is the most informative, or most specific, superordinate
synset of the two senses s1 and s2. This is converted to a similarity measure
in the WordNet Similarity package by taking the reciprocal as in Equation (8)
(which follows). For this reason, the jcn values can get very large indeed when
the distances are negligible, for example where the neighbor has a sense which is
a synonym. This is a motivation for our normalizing the sss in Equation (1).
jcn(s1, s2) = 1/Djcn(s1, s2) (8)
The IC data required for the jcn measure can be acquired automatically from raw
text. We used raw data from the BNC to create the IC files. There are various parameters
that can be set in the WordNet Similarity Package when creating these files; we used
the RESNIK method of counting frequencies in WordNet (Resnik 1995), the stop words
provided with the package, and no smoothing.
The lesk score is applicable to all parts of speech, whereas the jcn is applicable
only to nouns and verbs because it relies on IC counts which are obtained using the
hyponym links and these only exist for nouns and verbs.15 However, we did not use
jcn for verbs because in previous experiments (McCarthy et al 2004c) the lesk measure
outperformed jcn because the structure of the hyponym hierarchy is very shallow for
verbs and the measure is therefore considerably less informative for verbs than it is for
nouns.
4.3 An Example
We illustrate the application of our measure with an example. For star, if we set16 k = 4
and have the dss for the previously given neighbors as in the first row of Table 6, and
15 For verbs these pointers actually encode troponymy, which is a particular kind of entailment relation,
rather than hyponymy.
16 In this example, as before, we set k to 4 for the sake of brevity.
567
Computational Linguistics Volume 33, Number 4
Table 6
Example dss and sss scores for star and its neighbors.
Neighbors of star (dss)
Senses actor (0.22) footballer (0.12) planet (0.08) circle (0.03)
celebrity 0.42 0.53 0.02 0.01
celestial body 0.01 0.01 0.68 0.10
shape 0.0 0.0 0.02 0.78
zodiac 0.03 0.03 0.21 0.01
Total 0.46 0.57 0.93 0.90
the sss between the senses and the neighbors as in the remaining rows, the prevalence
score for celebrity would be:
= 0.22 ? 0.420.46 + 0.12 ? 0.530.57 + 0.08 ? 0.020.93 + 0.03 ? 0.010.90
= 0.2009 + 0.1116 + 0.0017 + 0.0003
= 0.3145
The prevalence score for each of the senses would be:
prevalence score(celebrity) = 0.3145
prevalence score(celestial body) = 0.0687
prevalence score(shape) = 0.0277
prevalence score(zodiac) = 0.0390
so the method would select celebrity as the predominant sense.
5. Experimental Setup
5.1 The Distributional Thesauruses
The three thesauruses used in our first two experiments were all created automatically
from raw corpus data, based either on grammatical relations between words computed
by syntactic parsers or alternatively on word proximity relations.
We created the first thesaurus, which we call BNC, from grammatical relation output
produced by the RASP system (Briscoe and Carroll 2002) applied to the 90M words of
the ?written? portion of the British National Corpus (Leech 1992), for all polysemous
nouns, verbs, adjectives, and adverbs in WordNet. For each word we considered co-
occurring words in the grammatical contexts listed in Table 7.
In the first two experiments, we also use two further automatically computed
distributional thesauruses, produced by Dekang Lin from 125M words of text from the
Wall Street Journal, San Jose Mercury News, and AP Newswire, using the same similarity
measure. The thesauruses are publicly available.17 One was constructed based on word
17 The thesauruses are available for download from http://www.cs.ualberta.ca/~lindek/
downloads.htm.
568
McCarthy, Koeling, Weeds, and Carroll Acquisition of Predominant Word Senses
Table 7
Grammatical contexts used for acquiring the BNC thesaurus.
PoS Grammatical contexts
noun verb in direct object or subject relation, adjective or noun modifier
verb noun as direct object or subject
adjective modified noun, modifying adverb
adverb modified adjective or verb
Table 8
Thesaurus coverage of polysemous words (excluding multiwords) in WordNet 1.6.
PoS Thesaurus types NISC NITH
noun BNC 7,090 2,436 115
noun DEP 6,583 2,176 217
noun PROX 6,582 2,176 217
verb BNC 2,958 553 45
adjective BNC 3,659 1,208 123
adverb BNC 505 132 38
similarities computed from syntactic dependencies produced by MINIPAR (Lin 1998b),
and the other was constructed based on textual proximity relationships between words.
We refer below to the original corpus as NEWSWIRE, and these two thesauruses as DEP
and PROX, respectively. We restricted our experiments to the nouns in these thesauruses.
Table 8 contains details of the numbers of polysemous (according to WordNet 1.6)
words contained in these thesauruses, the number of words in SemCor that were not
found in these thesauruses (NITH) and the number of words in the thesauruses that
were not in SemCor (NISC).
For the experiments described in Sections 6.3 and 6.4 we use exactly the same
method as that proposed for the BNC thesaurus, however the data source is different
and is described in those sections.
5.2 The Sense Inventory
We use WordNet version 1.6 as the sense inventory for our first three experiments, and
1.7.1 for our last experiment.18
For sss? we use the WordNet Similarity Package version 0.05 (Patwardhan and
Pedersen 2003).
18 We use 1.6 which is a rather old version of WordNet so that we can directly evaluate on the SemCor data
released with this version; we also use it to enable comparison with the results of McCarthy et al (2004a).
We use WordNet 1.7.1 for the fourth experiment, because this is the version that was used for annotating
the test data in that experiment. We plan to move to more recent versions of WordNet and experiment
with other sense inventories in the future.
569
Computational Linguistics Volume 33, Number 4
6. Experiments
In this section we describe four experiments using our method for acquiring predomi-
nant sense information.
The first experiment evaluates automatically acquired predominant senses for all
parts of speech, using SemCor as the test corpus. This extends previous work which
had only evaluated all PoS on Senseval-2 (Cotton et al 2001) and Senseval-3 (Mihalcea
and Edmonds 2004) data. The SemCor corpus is composed of 220,000 words, in contrast
to the 6 documents in the Senseval-2 and -3 English all-words data (10,000 words). We
examine the effects of using the two different semantic similarity scores that performed
well in previous work: jcn is quick to compute but lesk has the advantage that it is
applicable to all PoS and can be implemented for any dictionary with sense defini-
tions. We compare three thesauruses: one is derived from the BNC and two from the
NEWSWIRE corpus. The two from the NEWSWIRE corpus examine the requirement for
a parser by contrasting results obtained when the thesaurus is built using parsed data
compared to a proximity approach. We contrast the results of the BNC thesaurus with
a simplified version of the prevalence score which uses the number of the k neighbors
closest to a sense for ranking without using the dss and without sharing the credit for
a neighbor between senses. We also perform an error analysis on a random sample
of words for which a predominant sense was found that differed from that given by
SemCor, identifying and giving an indication of the frequencies of the main sources of
error.
The second experiment is on nouns in the Senseval-2 all-words data, again using
predominant senses acquired using each of the three distributional thesauruses, but
in this experiment we explore the benefits of an automatic first sense heuristic when
there is inadequate data in available resources. Although McCarthy et al (2004c) show
that on Senseval-2 and Senseval-3 test data a first sense heuristic derived from SemCor
outperforms the automatic method, we look at whether the method?s performance is
relatively stronger on words for which there is little data in SemCor. This is important
because, as we have shown in Table 5, low frequency words are used often in senses
other than the sense that is ranked first according to SemCor.
In addition to the issue of lack of coverage of manually annotated resources, sense
frequency will depend on the domain of the data. In the third experiment, we revisit
some previous work on noun senses and domain (McCarthy et al 2004a) using corpora
of news text about sports and finance. Using distributional thesauruses computed from
these corpora and a gold standard domain labeling of word senses we look at the
potential for computing domain-specific predominant senses for parts of speech other
than nouns.
Continuing the line of research on automatic acquisition of domain-specific pre-
dominant senses, the fourth experiment compares results when we train and test on
domain-specific corpora, where the training data is (1) manually categorized for domain
and from the same corpus as the gold-standard test data, and (2) where the training data
is harvested automatically from another corpus which is categorized automatically.
6.1 Experiment 1: All Parts of Speech
In this experiment, we evaluate the accuracy of automatically acquired predominant
senses for all open class parts of speech, taking SemCor as the gold standard. For nouns
we use the semantic similarity measures lesk and jcn, and for other parts of speech, lesk.
We use the three distributional thesauruses BNC, DEP, and PROX.
570
McCarthy, Koeling, Weeds, and Carroll Acquisition of Predominant Word Senses
The gold standard is derived from the Brown Corpus files publicly released as part
of SemCor, rather than the processed data provided in the cntlist file in the WordNet
distribution. The released SemCor files contain only the tagged data from the Brown
Corpus and do not include data from The Red Badge of Courage. We use the released data
rather than that in cntlist because this includes the actual tagged examples which are
marked for genre by the Brown files. We envisage the possibility of further experiments
with these genre markers. We only evaluate on instances where a single, unique sense
is supplied by the annotators. So, for example, we ignore instances like the following
with multiple wnsn values:
<wf cmd=done pos=NN lemma=tooth wnsn=3;1 lexsn=1:05:02::;1:08:00::>tooth</wf>
We also only evaluate on polysemous words (according to WordNet) having one sense
in SemCor which is more frequent than any other, and for which both SemCor and our
thesauruses have at least a minimal amount of data. Specifically, a word must occur
three or more times in SemCor; it must also occur in ten or more grammatical relations
in the parsed version of the BNC and have neighbors in the distributional thesaurus, or
be present in Dekang Lin?s thesaurus.19
We evaluate on nouns, verbs, adjectives, and adverbs separately, computing a num-
ber of accuracy measures, both type-based and token-based. PSacc is calculated over
word types in SemCor which have one sense which occurs more than any other. It is the
accuracy of identifying the predominant sense in SemCor. If the automatic ranking has
a tie for the top ranked sense then we score that word as incorrect.20 So we have
PSacc =
|correcttyp|
|typesmf |
? 100 (9)
where typesmf are the types in SemCor such that one sense is more frequent than
any other, the word has occurred at least three times in SemCor and has an entry
in the thesaurus. |correcttyp| is the number of these where the automatically acquired
predominant sense matches the first sense in SemCor.
PSaccBL is the predominant sense random baseline, obtained as follows:
PSaccBL =
?
w?typesmf
1
|senses(w)|
|typesmf |
? 100 (10)
WSDsc is a token-based measure. It is the WSD accuracy that would be obtained
by using the first sense heuristic with the automatically acquired predominant sense
information, in cases where there was a unique automatic top ranked sense:
WSDsc =
|correcttok|
|SCtokensafs|
? 100 (11)
19 Although we do not evaluate words for which there were no neighbors in the thesaurus, we could extend
the thesaurus to include some of these by widening the range of grammatical relations covered and
compensating for some systematic PoS tagging errors.
20 If we exclude these words with joint top ranking from the automatic method (precision rather than recall)
then we obtain marginally higher accuracy for the jcn measure but no difference for lesk.
571
Computational Linguistics Volume 33, Number 4
where |correcttok| is the number of tokens disambiguated correctly out of the tokens in
SemCor having an automatically acquired first sense (SCtokensafs).
SC FS is the WSD accuracy of the SemCor first sense heuristic on the same set of
tokens (SCtokensafs), which is the upper bound because the information it uses is derived
from the test data itself. RBL is the random baseline for the WSD task, calculated by
splitting the credit for each token to be tagged in the test data evenly between all of the
word?s senses.
RBL =
?
w?SCtokensafs
1
|senses(w)|
|SCtokensafs|
? 100 (12)
The results are shown in Table 9. We examined differences between the semantic
similarity measures (lesk and jcn), the BNC and DEP thesauruses, and the DEP and
PROX thesauruses using the ?2 test of significance with one degree of freedom (Siegel
and Castellan 1988). None of the differences between the different combinations of
similarity measures and thesauruses for the type-based measure PSacc are significant.
The differences between lesk and jcn are significant for the token-based measure WSDsc
for both the BNC and PROX thesauruses (both p < .001), however not when comparing
lesk and jcn for the DEP thesaurus. Although lesk is more accurate than jcn, at least on
the WSD task, jcn is much faster because of the precompilation of IC in the WordNet
similarity package; however, lesk has the additional benefit of being applicable to other
parts of speech. The method gives particularly good results for adjectives, given that
they have a similar random baseline to nouns. It does not do so well for adverbs and
verbs, but still performs well above the random baseline which is low for verbs due
to their high degree of polysemy. Given that the first sense heuristic from SemCor is
particularly strong for adverbs, it is disappointing that the automatic method does not
perform as well as it does on adjectives. One possible reason for this might be that
adverbs are often less strongly associated to the verbs that they modify than adjectives
are to the nouns that they modify, so the distributional thesaurus information is less
reliable. Another reason may be that less data are available for adverbs, both in the
thesaurus and also in WordNet.
Table 9
Evaluation on SemCor, polysemous words only.
Type Token
PoS Settings No. PSacc PSaccBL No. WSDsc SC FS RBL
noun lesk BNC 2,555 54.5 32.3 53,468 48.7 68.6 24.7
noun lesk DEP 2,437 56.3 32.1 52,158 49.2 68.4 24.6
noun lesk PROX 2,437 55.9 32.1 52,158 49.0 68.4 24.6
noun jcn BNC 2,555 54.0 32.3 53,429 46.1 68.6 24.7
noun jcn DEP 2,436 56.4 32.1 52,122 48.8 68.4 24.6
noun jcn PROX 2,436 55.9 32.1 52,117 47.7 68.4 24.6
verb lesk BNC 1,149 45.6 27.1 31,182 36.1 57.1 17.1
adjective lesk BNC 1,154 60.4 32.8 18,216 56.8 73.8 24.9
adverb lesk BNC 230 52.2 39.9 8,810 43.2 76.1 33.0
572
McCarthy, Koeling, Weeds, and Carroll Acquisition of Predominant Word Senses
Comparing the results for the DEP and the PROX thesauruses, we see that although
there is no significant difference in PSacc (with either lesk or jcn), there is for WSDsc
when using jcn (p < .001), but not when comparing the lesk values for these thesauruses.
Even though the differences between jcn DEP and jcn PROX are significant, the absolute
differences are nevertheless relatively small; this bodes well for applying the automatic
predominant sense method to languages less well resourced than English, because
the PROX thesaurus was produced without using a parser. The differences in results
between jcn BNC and jcn DEP for nouns are statistically significant (p < .001).21 The
better accuracy with DEP may be because the NEWSWIRE corpus is larger than the
BNC. We intend to investigate the effects of corpus size in the future. The differences in
results between lesk BNC and lesk DEP for nouns are not significant.
6.1.1 Results Using Simplified Prevalence Score. A simple variation of our method is just to
associate each neighbor with just one sense and use the number of neighbors associated
with a sense for the prevalence score. This gives a modified version of Equation (1)
where each sense si ? senses(w) is assigned a score as follows:
Simplified Prevalence Score(w, si) = |{nj ? Nw} : arg max
sk?senses(w)
(sss(sk, nj)) = si| (13)
where
sss(sk, nj) = max
sx?senses(nj )
sss?(sk, sx) (14)
For the example in Table 6, celebrity would get the top score of 2 (due to it having
the highest sss for actor and footballer), celestial body would get a score of 1 (due to its
sss with planet), shape would get 1 (due to circle), and zodiac would obtain a Simplified
Prevalence Score of 0 because it does not have the highest sss for any of the neighbors.
As the results from Table 10 show, we do not get such good results with this score.
This supports our intuition that a combination of both the number of neighbors and
their distributional similarity scores is important for determining predominance. The
rest of the article gives results and analysis for our original prevalence score as given in
Equation (1).
6.1.2 Error Analysis. We took a random sample of 80 words that occurred more than five
times in SemCor, 20 words for each PoS, from those where the automatically identified
predominant sense was different from the SemCor first sense when using the lesk sss
and BNC thesaurus and our ranking score as defined in Equation (1) (i.e., the data
represented by the first result line and the last three result lines of Table 9). Herein, we
call the automatically identified sense AUTO FS, and the SemCor sense SemCor FS. We
21 The coverage of the SemCor data by the DEP and PROX thesauruses is slightly lower than that of the
BNC-derived thesaurus due to mismatches in spelling and capitalization and also probably because the
NEWSWIRE corpus is narrower in genre and domain than the BNC.
573
Computational Linguistics Volume 33, Number 4
Table 10
Simplified prevalence score, evaluation on SemCor, polysemous words only.
Type Token
PoS Settings No. PSacc PSaccBL No. WSDsc SC FS RBL
noun lesk BNC 2,555 52.9 32.3 53,175 47.2 68.6 24.7
noun jcn BNC 2,555 50.1 32.3 52,033 46.7 69.2 24.8
verb lesk BNC 1,149 45.1 27.1 30,364 36.7 58.0 17.4
adjective lesk BNC 1,154 58.3 32.8 18,136 56.0 73.7 24.8
adverb lesk BNC 230 50.0 39.9 8,802 42.2 76.1 33.0
manually inspected the data for each of the words to find the source of the problem.
We did not have the (substantial) resources that would be required to sense tag all
occurrences of these words in the BNC to see what their actual first senses were. Instead,
we examined the parses, grammatical relations, and sense definitions for the words to
see why the AUTO FS was ranked above the SemCor FS. We found the following main
types of error:22
corpora The difference appears to be due to genuine divergence between the BNC
and SemCor. For this error type we looked at the BNC parses to see if the acquired
predominant sense was clearly due to differences in the corpus data. There may
be other errors that should have been assigned this category, but without access
to sense tagged BNC data we could not be sure of this, so we used this category
conservatively. An example of this error is the adjective solid which has the good
quality first sense in the Brown files in SemCor, but the firm sense according to
our BNC automatic ranking.
related The automatic predominant sense is closely related to the SemCor first sense.
Although many word senses are related to some extent, the category was picked
where a close relationship seemed to be the main cause of the error. An example
is the noun straw which has two senses in WordNet 1.6, fibre used for hats and
fodder and plant material. The SemCor FS was the former whereas our AUTO FS
was the latter.
competing Two or more related senses are ranked highly but they are overtaken by
an unrelated sense. For example, the ranking and scores for the noun transmission
are:
WordNet sense Description Prevalence score
5 gears 1.79
2 communication 1.20
1 act of sending a message 1.19
3 fraction of radiant energy 0.48
4 infection 0.15
22 There were a few other, less numerous types of error, for example systematic PoS mis-tagging of particles
(such as down) as adverbs.
574
McCarthy, Koeling, Weeds, and Carroll Acquisition of Predominant Word Senses
The act of sending a message sense is overtaken by the gears sense because the
credit from shared distributional neighbors is split between it and the communi-
cation sense.
neighbors There are not many neighbors related to the sense. There can be various
reasons for this, such as the sense having restricted contexts of occurrence or only
a small number of near synonyms existing for the sense. An example of this is
the adjective live where the SemCor FS unrecorded sense seems to occur in the
BNC corpus more than the alive sense; there are plenty of grammatical relations
pertaining to this sense, but there are few distributional neighbors near in meaning
to unrecorded.23
spurious similarity The WordNet similarity scores were misled by spurious relation-
ships to neighbors; this can occur in dense areas such as the ?physical object?
region of the noun hyponym hierarchy. An example of this is the verb tap which
has neighbors push and press which are related to the AUTO FS (solicit) as well as
the SemCor FS (strike lightly).
The results of the error analysis are shown in Table 11. The analysis shows that
differences between the training (BNC) and testing (SemCor) corpora are not a major
source of error. Although SemCor itself (the released files from the Brown corpus
comprising only 200,000 words) is not large enough to build a thesaurus with entries
for a reasonable portion of the words, we did build a thesaurus from the entire Brown
corpus (1 million words) to see the effect of corpus data. The results are compared
to those from the BNC in Table 12 on the set of words which had thesaurus entries
in the Brown data (to make the results more comparable, because the corpora are of
such different sizes). We also show the average results for 10 random selections of a
1 million word random sample of the BNC. To do this we randomly selected 190 th of the
tuples.24 The differences in the WSDsc for the BNC 190 sample and the Brown corpus are
significant (p < .01 on the ?2), but the differences in PSacc are not significant. Although
the entire BNC produced better results than the Brown data, this is undoubtedly due to
the difference in size of the corpus. Taking a comparably sized sample, the results are
slightly better from Brown which is the corpus from which SemCor is taken.
For nouns, it was apparent that in two cases less-prevalent senses were receiving a
higher ranking simply because the credit for some neighbors associated with another
meaning was split between related senses (error type competing). This was not ob-
served for other parts of speech, possibly because the AUTO FS was rarely unrelated to
the SemCor FS.
There were some problems arising from spurious similarity. One possible source
of such problems is due to the ambiguity of the neighbor; in the future we will look
at reducing this source of error by removing neighbors which have a value for sx in
Equation (2) which is not the same as that preferred by the other senses of the target
word (w). For adverbs, all the cases that were categorized as spurious similarity were
also noted to be related to the SemCor FS, though they were not categorized as related
as this was not considered the primary cause of the error.
The analysis was hardest for verbs. Verbs are on average highly polysemous, and
often have senses that are related. Furthermore, the structure of the WordNet verb tro-
ponym hierarchy is very shallow compared to the noun hyponymy hierarchy, so there
23 The closest neighbors to the adjective live are adult, forthcoming, lively, solo, excellent, stuffed, living, dead,
and australian weekly.
24 The variance for the 190 sample for PSacc was 0.46 and for WSDsc it was 0.49.
575
Computational Linguistics Volume 33, Number 4
Table 11
Results of the error analysis for the sample of 80 words.
PoS
noun verb adjective adverb All PoS
corpora 1 2 1 1 5
related 8 12 13 8 41
competing 2 0 0 0 2
neighbors 4 3 2 2 11
spurious similarity 5 3 4 9 21
Table 12
SemCor results for Nouns using jcn.
Thesaurus PSacc% WSDsc %
full BNC 53.8 44.9
1
90 BNC 46.6 40.8
Brown 47.2 41.7
are more possibilities for spurious similarities from overlap of glosses. So, although we
tried to identify the main problem source, for verbs the problems usually arose from a
combination of factors and the relatedness of the senses was usually one of these.
Relatedness of senses and fine-grained distinctions are major sources of error. There
have been various attempts to group WordNet senses both manually and automati-
cally (Agirre and Lopez de Lacalle 2003; McCarthy 2006; Palmer, Dang, and Fellbaum
2007). Indeed, McCarthy demonstrated that distributional and semantic similarity can
be used for relating word senses and that such methods increase accuracy of first
sense heuristics, including the automatic method proposed here. WSD is improved with
coarser-grained inventories but ultimately, performance depends on the application.
For example, the noun bar has 11 senses in WordNet 1.6. These include the pub sense
as well as the counter sense and these are related to a certain extent. One might want
to group them when acquiring predominant senses, but there may be situations where
they should be distinguished. For example, if one were to ask a robot to ?go to the bar?
one would hope it could use the context to go get the drinks rather than replying that it
is already there! Even in cases where fine-grained distinctions are ultimately required, it
may be helpful to have a coarse-grained prior and then use contextual features to tease
apart subtle sense distinctions.
From our error analysis, the problem of semantically isolated senses (identified as
neighbors) was not a major source of error, but still causes some problems. One possible
remedy might be to identify these cases by looking for neighbors which relate strongly
to a sense which none of the other neighbors relate to and weighting the contribution
from these neighbors more. This may however give rise to further errors because of the
noise introduced by focusing on individual neighbors. We will explore such directions
in future work.
576
McCarthy, Koeling, Weeds, and Carroll Acquisition of Predominant Word Senses
In this experiment we did not assign any credit for near misses. In many cases
of error the SemCor FS nonetheless received a high prevalence score. In the future
we hope to use the score for probability estimation, and combine this with contextual
information for WSD as in related work by Lapata and Brew (2004) and Chan and Ng
(2005).
6.2 Experiment 2: Frequency and the SemCor First Sense Heuristic
In the previous section we described an evaluation of the accuracy of automatically
acquired predominant sense information. We carried out the evaluation with respect to
SemCor in order to have as much test data as possible. To obtain reasonably reliable
gold-standard first-sense data and first-sense heuristic upper bounds, we limited the
evaluation to words occurring at least three times in SemCor. Clearly this scenario is
unrealistic. For many words, and particularly for nouns, there is very little or no data in
SemCor; Table 2 shows that 81.9% of nouns (excluding multiwords) listed in WordNet
do not occur at all in SemCor. Thus, even for English, which has substantial manually
sense-tagged resources, coverage is severely limited for many words.
For a more realistic comparison of automatic and manual heuristics, we therefore
now change to a different test corpus, the Senseval-2 English all-words task data set. We
focus on nouns and evaluate using all words regardless of their frequencies in SemCor.
We examine the effect of frequency in SemCor on performance of a SemCor-derived
heuristic in comparison to results from our automatic method on the same words. Our
hypothesis is that although automatically acquired predominant sense information may
not outperform first-sense data obtained from a hand-tagged resource over all words in
a text, the information may well be more accurate for low frequency items.
We use a mapping between different WordNet versions25 (Daude?, Padro?, and Rigau
2000) to obtain the Senseval-2 all words noun data (originally distributed with 1.7 sense
numbers) with 1.6 sense numbers. As well as examining the performance of our method
in contrast to the SemCor heuristic, we calculate an upper bound for this using the
first sense heuristic from the Senseval-2 all-words data itself. This is obtained for nouns
with two or more occurrences in the Senseval-2 data and where one sense occurs more
than any of the others. We calculate type, precision, and recall, using this Senseval-2
first-sense as the gold standard. The recall measure is the same as PSacc described
previously, except that we include items which do not have entries in the thesaurus,
scoring them incorrect. Precision only includes items where there is a sense ranked
higher than any other for that word with the prevalence score, that is, it does not include
items with a joint automatic ranking. We also calculate token precision and recall (WSD).
These measures relate to WSDsc, but again, recall includes words not in the thesaurus
which are scored incorrect, and precision does not include items with a joint automatic
ranking. We also separately compute WSD precision for words not in SemCor (NISC).
The results are shown in Table 13.26
The automatically acquired predominant sense results (the first six lines of results
in the table) are approaching the SemCor-derived results (third line from the bottom of
the table). The NISC results are particularly encouraging, but with the caveat that there
are only 17 such words in the data. The precision for these items is higher than the
25 This mapping is available at http://www.lsi.upc.es/~nlp/tools/mapping.html.
26 Note that these figures are lower than those of McCarthy et al (2004a) in a similar experiment because
the evaluation here is only on polysemous words.
577
Computational Linguistics Volume 33, Number 4
Table 13
Evaluating predominant sense information for polysemous nouns on the Senseval-2 all-words
task data.
Type WSD/token
Settings Precision (%) Recall (%) Precision (%) Recall (%) Precision NISC (%)
lesk BNC 56.3 53.7 54.6 53.4 58.3
lesk DEP 52.0 47.2 52.6 48.7 58.3
lesk PROX 52.0 47.2 52.3 48.5 58.3
jcn BNC 52.4 50.0 51.8 50.6 66.7
jcn DEP 52.0 47.2 58.0 53.7 83.3
jcn PROX 53.1 48.1 57.3 53.1 83.3
SemCor 64.8 63.0 58.5 57.3 0.0
Senseval-2 ? ? 90.8 60.1 100.0
RBL 26.5 26.5 26.0 26.0 50.0
overall figure. This is because the nouns involved are less frequent so tend to be less
polysemous and consequently have a higher random baseline. There are a few nouns
that are not in the automatic ranking, but this is due to the fact that neighbors were
not collected for these nouns in the thesaurus because of tagging or parser errors or
the particular set of grammatical relations used. It should be possible to extend the
range of grammatical relations, or use proximity-based relations, so that neighbors can
be obtained in these cases. It would also be possible to assign some credit in the case of
joint top ranked senses to increase coverage.
Looking at Table 13 in more detail, it seems to be the case that although the BNC
thesaurus does well in identifying the first sense of a word (the type results), the PROX
and DEP thesauruses from the NEWSWIRE corpus return better WSD results when
used with the jcn measure. This is possibly because jcn works well for more frequent
items due to its incorporation of frequency information, and the NEWSWIRE corpus
has more data for frequent words, although coverage is not as good as the BNC as
seen by the bigger differences in precision and recall and the figures in Table 8. The
lower coverage may be due to the narrower domain and genre of the NEWSWIRE
corpus, though spelling and capitalization differences probably also account for some
differences.
Table 14 shows results on the Senseval-2 nouns for the best similarity measure
and thesaurus combinations in Table 13 for nouns at or below various frequencies
in SemCor. (The differences between the DEP and PROX thesauruses are negligible at
frequencies of 10 or below, so for those we report only the results for DEP.) As we
anticipated, for low frequency words the automatic methods do give more accurate
predominant sense information than SemCor. The low number of test items at frequency
five or less means that results for jcn with the BNC thesaurus are not significantly better
when compared with SemCor (p = .05); however the lesk WSD results are significantly
better (p < .01 for the ? 1 threshold and p < .05 for the ? 5 threshold). On the whole, we
see that the automatic method, using either jcn or lesk and any of the three thesauruses,
tend to give better results than SemCor on nouns which have low coverage in SemCor.
Figures 2 and 3 show the precision for type and token (WSD) evaluation where the
items have a frequency at or below given thresholds in SemCor. Although the manually
578
McCarthy, Koeling, Weeds, and Carroll Acquisition of Predominant Word Senses
Table 14
Senseval-2 results, polysemous nouns only, broken down by their frequencies of occurrence in
SemCor.
Type WSD/token
No. of occurrences in Precision Recall Precision Recall
SemCor (no. of words) Settings (%) (%) (%) (%)
jcn BNC 100.0 33.3 66.7 47.1
lesk BNC 100.0 33.3 58.3 41.2
0 (17) jcn DEP 100.0 33.3 83.3 58.8
lesk DEP 100.0 33.3 58.3 41.2
SemCor 0.0 0.0 0.0 0.0
Senseval-2 ? ? 100.0 52.9
RBL 38.9 38.9 46.1 46.1
jcn BNC 66.7 44.4 54.1 45.5
lesk BNC 83.3 55.6 67.6 56.8
jcn DEP 50.0 22.2 51.7 34.1
? 1 (44) lesk DEP 75.0 33.3 69.0 45.5
SemCor 50.0 33.3 33.3 20.5
Senseval-2 ? ? 93.3 63.6
RBL 40.7 40.7 42.8 42.8
jcn BNC 80.0 61.5 63.0 57.5
lesk BNC 90.0 69.2 71.2 65.0
? 5 (80) jcn DEP 71.4 38.5 56.7 42.5
lesk DEP 85.7 46.2 70.0 52.5
SemCor 60.0 46.2 54.0 42.5
Senseval-2 ? ? 95.9 58.8
RBL 38.1 38.1 39.1 39.1
jcn BNC 75.0 63.2 59.3 55.8
lesk BNC 68.8 57.9 62.8 59.2
? 10 (120) jcn DEP 66.7 42.1 56.8 45.0
lesk DEP 58.3 36.8 58.9 46.7
SemCor 68.8 57.9 57.3 49.2
Senseval-2 ? ? 96.8 50.8
RBL 37.5 37.5 38.0 38.0
jcn BNC 76.0 67.9 66.7 64.8
lesk BNC 64.0 57.1 71.6 69.6
? 15 (250) jcn DEP 60.0 42.9 68.8 55.6
lesk DEP 55.0 39.3 67.3 54.4
jcn PROX 70.0 50.0 72.3 58.4
lesk PROX 55.0 39.3 66.8 54.0
SemCor 64.0 57.1 66.5 62.0
Senseval-2 ? ? 98.8 68.4
RBL 32.9 32.9 30.4 30.4
jcn BNC 52.4 50.0 51.8 50.6
lesk BNC 56.3 53.7 54.6 53.4
all (786) jcn DEP 52.0 47.2 58.0 53.7
lesk DEP 52.0 47.2 52.6 48.7
jcn PROX 53.1 48.1 57.3 53.1
lesk PROX 52.0 47.2 52.3 48.5
SemCor 64.8 63.0 58.5 57.3
Senseval-2 ? ? 90.8 60.1
RBL 26.5 26.5 26.0 26.0
579
Computational Linguistics Volume 33, Number 4
Figure 2
?TYPE? precision on finding the predominant sense for the Senseval-2 English all-words test
data for nouns having a frequency less than or equal to various thresholds.
produced SemCor first-sense heuristic outperforms the automatic methods over all the
test items (see the ?all? results in Table 14), when items are below a frequency threshold
of five the automatic methods give better results. Indeed, as the threshold is moved
up to 20 and even 30, more nouns are covered, and the automatic methods are still
comparable and in some cases competitive with the SemCor heuristic.
6.3 Experiment 3: The Influence of Domain
In this experiment, we investigate the potential of the automatic ranking method for
computing predominant senses with respect to particular domains. We have previ-
ously demonstrated that the method produces intuitive domain-specific models for
nouns (McCarthy et al 2004a), and that these can be more accurate than first senses de-
rived from SemCor for words salient to a domain (Koeling, McCarthy, and Carroll 2005).
Here we investigate the behavior for other parts of speech, using a similar experimental
setup to that of McCarthy et al That work used the subject field codes (SFC) (Magnini
and Cavaglia` 2000)27 as a gold standard. In SFC the Princeton English WordNet is
augmented with some domain labels. Every synset in WordNet?s sense inventory is
annotated with at least one domain label, selected from a set of about 200 labels. These
labels are organized in a tree structure. Each synset of WordNet 1.6 is labeled with one
or more labels. The label factotum is assigned if any other is inadequate. The first level
consists of five main categories (e.g., doctrines and social science) and factotum.doctrines
27 More recently referred to as WordNet Domains (WN-DOMAINS).
580
McCarthy, Koeling, Weeds, and Carroll Acquisition of Predominant Word Senses
Figure 3
WSD precision on the Senseval-2 English all-words test data for nouns having a frequency less
than or equal to various thresholds.
has subcategories such as art, religion, and psychology. Some subcategories are further
divided in subcategories (e.g., dance, music, and theatre are subcategories of art).
McCarthy et al (2004a) used two domain-specific corpora for input to the method
for finding predominant senses. The corpora were obtained from the Reuters Corpus,
Volume 1 (RCV1; Rose, Stevenson, and Whitehead 2002) using the Reuters topic codes.
The two domain-specific corpora were:
SPORTS (Reuters topic code GSPO), 9.1 million words
FINANCE (Reuters topic codes ECAT and MCAT), 32.5 million words
In that work we produced sense rankings for a set of 38 nouns which have at
least one synset with an economy SFC label and one with a sport SFC label. We then
demonstrated that there were more sport labels assigned to the predominant senses
acquired from the SPORTS corpus and more economy labels assigned to those from the
FINANCE corpus. The predominant senses from both domains had a similarly high
percentage of factotum (domain-independent) labels. We reproduce the results here (in
Figure 4) for ease of reference, and for comparison with other results presented in this
section. The y-axis in this figure shows the percentage of the predominant sense labels
for these 38 nouns that have the SFC label indicated by the x-axis.
We envisaged running the same experiment with verbs, adjectives, and adverbs,
although we suspected that these would show less domain-specific tendencies and
there would be fewer candidate words to work with. The SFC labels for all senses of
polysemous words (excluding multiwords) in the various parts of speech are shown in
Table 15. We see from the distribution of factotum labels across the parts of speech that
nouns are certainly the PoS most likely to be influenced by domain.
To produce results like Figure 4 for each PoS, we needed words having at least one
synset with a sport label and one with an economy label. There were 20 such verbs but
581
Computational Linguistics Volume 33, Number 4
Figure 4
Distribution of domain labels of predominant senses for 38 polysemous nouns ranked using the
SPORTS and FINANCE corpora.
only two adjectives and no adverbs meeting this condition. We therefore performed
the experiment only with verbs. To do this we used the SPORTS and FINANCE corpora
as before, computing thesauruses for verbs using the grammatical relations specified
in Table 7. The results for the distribution of domain labels of the predominant senses
Table 15
Most frequent SFC labels for all senses of polysemous words in WordNet, by part of speech.
Domain % Domain %
noun biology 29.3 verb factotum 67.0
factotum 20.7 psychology 3.5
art 6.2 sport 2.9
sport 3.1 art 2.5
medicine 3.1 biology 2.5
other 37.6 other 21.6
adjective factotum 67.8 adverb factotum 81.4
biology 6.5 psychology 7.5
art 3.2 art 1.8
psychology 2.7 physics 1.1
physics 1.9 economy 1.1
other 17.9 other 7.1
582
McCarthy, Koeling, Weeds, and Carroll Acquisition of Predominant Word Senses
acquired from the SPORTS and FINANCE corpora are shown in Figure 5. We see the same
tendency for sport labels for predominant senses from the SPORTS corpus and economy
labels for the predominant senses from the FINANCE corpus, but the relationship is
less marked compared with nouns because of the high proportions of factotum senses
in both corpora for verbs. We believe that acquisition of domain-specific predominant
senses should be focused on those words which show domain-specific tendencies. We
hope to put more work into automatic detection of these tendencies using indicators
such as domain salience and words that have different sense rankings in a given domain
compared to the BNC (as discussed by Koeling, McCarthy, and Carroll 2005).
6.4 Experiment 4: Domain-Specific Predominant Sense Acquisition
In the final set of experiments we evaluate the acquired predominant senses for domain-
specific corpora. The first of the two experiments was reported by Koeling, McCarthy,
and Carroll (2005), but we extend it by the second experiment reported subsequently.
Because there are no publicly available domain-specific manually sense-tagged corpora,
we created our own gold standard. The two chosen domains (SPORTS and FINANCE) and
the domain-neutral corpus (BNC) are the same as we used in the previous experiment.
We selected 40 words and we sampled (randomly) sentences containing these words
from the three corpora and asked annotators to choose the correct sense for the target
words. The set consists of 17 words which have at least one sense assigned an economy
domain label and at least one sense assigned a sports label: club, manager, record, right,
bill, check, competition, conversion, crew, delivery, division, fishing, reserve, return, score,
receiver, running; eight words that are particular salient in the SPORTS domain: fan,
star, transfer, striker, goal, title, tie, coach; eight words that are particular salient in the
Figure 5
Distribution of domain labels of predominant senses for 20 polysemous verbs ranked using the
SPORTS and FINANCE corpora.
583
Computational Linguistics Volume 33, Number 4
Table 16
WSD using predominant senses, training, and testing on all domain combinations
(hand-classified corpora).
Testing
Training BNC FINANCE SPORTS
BNC 40.7 43.3 33.2
FINANCE 39.1 49.9 24.0
SPORTS 25.7 19.7 43.7
Random BL 19.8 19.6 19.4
SemCor FS 32.0 (32.9) 33.9 (35.0) 16.3 (16.8)
FINANCE domain: package, chip, bond, market, strike, bank, share, target; and seven words
that are equally salient in both domains: will, phase, half, top, performance, level, country.
Koeling, McCarthy, and Carroll (2005) give further details of the construction of the gold
standard.
In the first experiment, we train on a corpus of documents with manually assigned
domain labels (i.e., sub-corpora of the Reuters corpus, see Section 6.3), and we test on
data from the same source. In a second experiment we build a text classifier, use the
text classifier to obtain SPORTS and FINANCE corpora (using general newswire text from
the English Gigaword Corpus; Graff 2003) and test on the gold-standard data from the
Reuters corpus. The second experiment eliminates issues about dependencies between
training and test data and will shed light on the question of how robust the acquired
predominant sense method is with respect to noise in the input data. At the same time,
the second experiment paves the way towards creating predominant sense inventories
for any conceivable domain.
6.4.1 Experiment Using Hand-Labeled Data. In this section we focus on the predominant
sense evaluation of the experiments described by Koeling, McCarthy, and Carroll (2005).
After running the predominant sense finding algorithms on the raw text of the two do-
main corpora (SPORTS and FINANCE) and the domain-neutral corpus (BNC), we evaluate
the accuracy of performing WSD on the sample of 40 words purely with the first sense
heuristic using all nine combinations of training and test corpora. The results (as given
in Table 16) are compared with a random baseline (?Random BL?)28 and the accuracy
using the first sense heuristic from SemCor (?SemCor FS?).29
The results in Table 16 show that the best results are obtained when the predominant
senses are acquired using the appropriate domain (i.e., test and training data from the
same domain). Moreover, when trained on the domain-relevant corpora, the random
baseline as well as the baseline provided by SemCor are comfortably beaten. It can be
observed from these results that apparently the BNC is more similar to the FINANCE
corpus than it is to the SPORTS corpus. The results for the SPORTS domain lag behind the
results for the FINANCE domain by almost 6 percentage points. This could be because
28 The random baseline is
?
i?tokens
1
#senses(i) .
29 The precision is given alongside in brackets because a predominant sense for the word striker is not
supplied by SemCor. The automatic method proposes a predominant sense in every case.
584
McCarthy, Koeling, Weeds, and Carroll Acquisition of Predominant Word Senses
Table 17
WSD using predominant senses, training, and testing on all domain combinations (automatically
classified corpora).
Testing
Training BNC FINANCE SPORTS
BNC 40.7 43.3 33.2
FINANCE 38.2 44.0 29.0
SPORTS 27.0 23.4 45.0
Random BL 19.8 19.6 19.4
SemCor FS 32.0 (32.9) 33.9 (35.0) 16.3 (16.8)
of the smaller amount of training data available (32M words versus 9M words), but it
could also be an artifact of this particular selection of words.
6.4.2 Experiment Using Automatically Classified Data. Although the previous experiment
shows that it is possible to acquire domain-specific predominant senses successfully, the
usefulness of doing this will be far greater if there is no need to classify corpora with
respect to domain by hand. There is no such thing as a standard domain specification
because the definition of a domain depends on user and application. It would be
advantageous if we could automatically obtain a user-/application-specific corpus from
which to acquire predominant senses.
In this section we describe an experiment where we build a text classifier using
WordNet as a sense inventory and the SFC domain extension (see Section 6.3). We
extracted bags of domain-specific words from WordNet for all the defined domains by
collecting all the word senses (synsets) and corresponding glosses associated with each
domain label. These bags of words are the fingerprints for the domains and we used
them to train a Support Vector Machine (SVM) text classifier using TwentyOne.30
The classifier distinguishes between 48 classes (the first and second levels of the
SFC hierarchy). When a document is evaluated by the classifier, it returns a list of
all the classes (domains) it recognizes and an associated confidence score reflecting the
certainty that the document belongs to that particular domain. We classified 10 months?
worth of data from the English Gigaword Corpus using this classifier and assigned each
document to the corpus belonging to the highest scoring class of the classifier?s output.
The level of confidence was ignored at this stage.
This resulted in a SPORTS corpus comprising about 11M words and a FINANCE
corpus of about 27M words. The predominant sense finding algorithm was run on the
raw text of these two corpora and we followed exactly the same evaluation strategy as
in the previous section. The results are summarized in Table 17 and are very similar
to those based on hand-labeled corpora. Again, the best results are obtained when test
and training data are derived from the same domain. The FINANCE?FINANCE result
is slightly worse, but is still well above both Random and the SemCor baseline. The
SPORTS?SPORTS result has slightly improved over the result reported in the previous
30 TwentyOne Classifier is an Irion Technologies product: www.irion.ml/products/english/
products classify.html.
585
Computational Linguistics Volume 33, Number 4
section. The reason for these differences may well be because the FINANCE corpus used
for this experiment is smaller and the SPORTS corpus is slightly larger than those used in
the hand-labeled experiment.
Automatically classifying documents inherently introduces noise in the training
corpora. This experiment to test the robustness of our method for finding predominant
senses suggests that it deals well with the noise. Further experiments that take the
confidence levels of the classifier into account will allow us to create corpora with less
noise and will allow us to find the right balance between corpus size and corpus quality.
7. Conclusions
In this article we have argued that information on the predominant sense of words is
important, and that it is desirable to be able to infer this automatically from unlabeled
text. We presented a number of evaluations investigating various facets of a previously
proposed method for automatically acquiring this information (McCarthy et al 2004a).
The evaluations extend ones in previous publications in a number of ways: they use
larger, balanced test data sets, and they compare alternative semantic similarity scores
and distributional thesauruses derived from different corpora and based on different
kinds of relations. We also looked in detail at areas where the method performs well
and also where it does not, and carried out a manual error analysis to identify the types
of mistakes it makes.
Our main results are:
 The predominant sense acquisition method produces promising results
overall for all open class parts of speech, when evaluated on SemCor, a
large balanced corpus.
 The highest accuracies are for nouns and adjectives; overall accuracy for
verbs is lower, but they have the lowest random baseline; adverbs have the
lowest average polysemy but gains over the random baseline are lower
than for other PoS.
 Using a thesaurus computed from proximity-based relations produces
almost as good results as using an otherwise identical one computed from
syntactic dependency-based relations.
 Lesk?s semantic similarity score (Banerjee and Pedersen 2002, lesk)
produces particularly good results for nouns which have low corpus
frequencies; Jiang and Conrath?s (1997, jcn) score does well on higher
frequency words.31
 For low frequency nouns in SemCor, the method, using any combination
of automatically acquired thesaurus and semantic similarity score that we
tried, produces more accurate predominant sense information than
SemCor. In particular, for nouns with a frequency of five or less (12.9% of
the polysemous nouns in the Senseval-2 data) it outperforms the SemCor
first sense heuristic. As the threshold is increased, the SemCor first sense
31 The lesk score has wider applicability than jcn since it can be applied to all parts of speech. It can also be
used with any sense inventory which has textual definitions for its senses even if the inventory does not
contain WordNet-like semantic relations.
586
McCarthy, Koeling, Weeds, and Carroll Acquisition of Predominant Word Senses
heuristic becomes more competitive, but some of the automatic methods
are still outperforming it for nouns occurring 20 or fewer times in SemCor.
 Nouns show a stronger tendency for domain-specific meanings than other
parts of speech, but predominant senses for verbs acquired automatically
with respect to domain-specific corpora also correlate with the appropriate
domain labeling for those senses.
 Predominant senses acquired using domain-specific corpora outperform
those from SemCor in a WSD task, for a selection of nouns, using corpora
consisting of either hand-classified or automatically-classified documents.
8. Further Work
We are continuing to work on automatic ranking of word senses for WSD. Our next step
will be to use the numeric values of sense prevalence scores to compare the skews in
the distributions of word senses across different corpora and see if this enables us to
detect automatically words for which a domain- or genre-specific ranking is warranted.
Looking at skews should also help in predicting words for which contextual WSD is
likely to be particularly powerful, for example when more than one sense is scored
as being highly prevalent. In such situations we will combine our method with an
approach to unsupervised context-based WSD which uses the collocates of the distri-
butional neighbors associated with each of the senses as contextual features.
Our error analysis shows that many errors in identifying predominant senses are
caused by the sense distinctions in WordNet being particularly fine-grained. We have
recently (Koeling and McCarthy 2007) evaluated our method on the coarse-grained
English all words task at SemEval (Navigli, Litkowski, and Hargraves 2007). We will fol-
low work on finding relationships between WordNet senses to induce coarser-grained
classes (McCarthy 2006), and on automatic induction of senses (Pantel and Lin 2002)
and adapt our method to acquire prevalence rankings for these. The granularity of the
inventory will depend on the application and we plan to apply rankings over such
inventories for WSD within the context of a task, such as lexical substitution (McCarthy
and Navigli 2007).
To date we have only applied our methods to English. We plan to apply our
approach to other languages for which sense tagged resources of the size of SemCor are
not available. Given the good results with Lin?s proximity based thesaurus we believe
our method should work even for languages which do not have high quality parsers
available.
Acknowledgments
This work was supported by the UK EPSRC
project EP/C537262 ?Ranking Word Senses
for Disambiguation: Models and
Applications,? and a UK Royal Society
Dorothy Hodgkin Fellowship to the first
author. We are grateful to Dekang Lin for
making his thesaurus data publicly available
and to Siddharth Patwardhan and Ted
Pedersen for the WordNet Similarity
Package. We thank the anonymous reviewers
for the many helpful comments and
suggestions they made.
References
Agirre, Eneko and Oier Lopez de Lacalle.
2003. Clustering WordNet word senses. In
Recent Advances in Natural Language
Processing, pages 121?130, Borovets,
Bulgaria.
Banerjee, Satanjeev and Ted Pedersen. 2002.
An adapted Lesk algorithm for word sense
disambiguation using WordNet. In
Proceedings of the Third International
Conference on Intelligent Text Processing and
Computational Linguistics (CICLing-02),
pages 136?145, Mexico City.
587
Computational Linguistics Volume 33, Number 4
Briscoe, Edward and John Carroll. 2002.
Robust accurate statistical annotation of
general text. In Proceedings of the Third
International Conference on Language
Resources and Evaluation (LREC),
pages 1499?1504, Las Palmas, Canary
Islands, Spain.
Buitelaar, Paul and Bogdan Sacaleanu.
2001. Ranking and selecting synsets by
domain relevance. In Proceedings of
WordNet and Other Lexical Resources:
Applications, Extensions and Customizations,
NAACL 2001 Workshop, pages 119?124,
Pittsburgh, PA.
Chan, Yee Seng and Hwee Tou Ng. 2005.
Word sense disambiguation with
distribution estimation. In Proceedings of
the 19th International Joint Conference on
Artificial Intelligence (IJCAI 2005),
pages 1010?1015, Edinburgh, UK.
Church, Kenneth W. and Patrick Hanks.
1989. Word association norms, mutual
information and lexicography. In
Proceedings of the 27th Annual Conference of
the Association for Computational Linguistics
(ACL-89), pages 76?82, Vancouver, British
Columbia, Canada.
Ciaramita, Massimiliano and Mark Johnson.
2003. Supersense tagging of unknown
nouns in WordNet. In Proceedings of the
Conference on Empirical Methods in Natural
Language Processing (EMNLP 2003),
pages 168?175, Sapporo, Japan.
Cotton, Scott, Phil Edmonds, Adam
Kilgarriff, and Martha Palmer. 2001.
Senseval-2. http://www.sle.sharp.
co.uk/senseval2.
Curran, James. 2005. Supersense tagging
of unknown nouns using semantic
similarity. In Proceedings of the 43rd
Annual Meeting of the Association for
Computational Linguistics (ACL?05), pages
26?33, Ann Arbor, MI.
Daude?, Jordi, Lluis Padro?, and German
Rigau. 2000. Mapping WordNets using
structural information. In Proceedings of the
38th Annual Meeting of the Association for
Computational Linguistics, pages 504?511,
Hong Kong.
Fellbaum, Christiane, editor. 1998. WordNet,
An Electronic Lexical Database. The MIT
Press, Cambridge, MA.
Francis, W. Nelson and Henry Kuc?era, 1979.
Manual of Information to Accompany a
Standard Corpus of Present-Day Edited
American English, for Use with Digital
Computers. Department of Linguistics,
Brown University, Providence, RI. Revised
and amplified ed.
Gale, William, Kenneth Church, and David
Yarowsky. 1992. One sense per discourse.
In Proceedings of the 4th DARPA Speech and
Natural Language Workshop, pages 233?237,
Harriman, NY.
Gliozzo, Alfio, Claudio Giuliano, and
Carlo Strapparava. 2005. Domain kernels
for word sense disambiguation. In
Proceedings of the 43rd Annual Meeting of the
Association for Computational Linguistics,
pages 403?410, Ann Arbor, MI.
Graff, David. 2003. English Gigaword.
Linguistic Data Consortium, Philadelphia,
PA.
Harris, Zellig S. 1968. Mathematical Structures
of Languages. Wiley, New York, NY.
Hornby, A. S. 1989. Oxford Advanced Learner?s
Dictionary of Current English. Oxford
University Press, Oxford, UK.
Ide, Nancy and Yorick Wilks. 2006. Making
sense about sense. In Eneko Agirre and
Phil Edmonds, editors, Word Sense
Disambiguation, Algorithms and Applications.
Springer, Dordrecht, The Netherlands,
pages 47?73.
Jiang, Jay and David Conrath. 1997.
Semantic similarity based on corpus
statistics and lexical taxonomy. In 10th
International Conference on Research in
Computational Linguistics, pages 19?33,
Taiwan.
Kilgarriff, Adam. 1998. Gold standard
datasets for evaluating word sense
disambiguation programs. Computer
Speech and Language, 12(3):453?472.
Kilgarriff, Adam and Martha Palmer, editors.
2000. Senseval: Special Issue of the Journal
Computers and the Humanities, volume
34(1?2). Kluwer, Dordrecht, The
Netherlands.
Koeling, Rob and Diana McCarthy. 2007.
Sussx: WSD using automatically acquired
predominant senses. In Proceedings of
ACL/SIGLEX SemEval-2007, pages 314?317,
Prague, Czech Republic.
Koeling, Rob, Diana McCarthy, and John
Carroll. 2005. Domain-specific sense
distributions and predominant sense
acquisition. In Proceedings of the Human
Language Technology Conference and
EMNLP, pages 419?426, Vancouver, British
Columbia, Canada.
Krovetz, Robert. 1998. More than one sense
per discourse. In Proceedings of the
ACL-SIGLEX Senseval Workshop.
http://www.itri.bton.ac.uk/events/
senseval/ARCHIVE/PROCEEDINGS/.
Landes, Shari, Claudia Leacock, and
Randee I. Tengi, editors. 1998. Building
588
McCarthy, Koeling, Weeds, and Carroll Acquisition of Predominant Word Senses
Semantic Concordances. The MIT Press,
Cambridge, MA.
Lapata, Mirella and Chris Brew. 2004. Verb
class disambiguation using informative
priors. Computational Linguistics,
30(1):45?75.
Leech, Geoffrey. 1992. 100 million words of
English: The British National Corpus.
Language Research, 28(1):1?13.
Lesk, Michael. 1986. Automatic sense
disambiguation using machine readable
dictionaries: How to tell a pine cone from
an ice cream cone. In Proceedings of the
ACM SIGDOC Conference, pages 24?26,
Toronto, Canada.
Levin, Beth. 1993. English Verb Classes and
Alternations: A Preliminary Investigation.
University of Chicago Press, Chicago and
London.
Lin, Dekang. 1997. Using syntactic
dependency as local context to resolve
word sense ambiguity. In Proceedings of the
35th Annual Meeting of the Association for
Computational Linguistics and 8th Conference
of the European Chapter of the Association
for Computational Linguistics (ACL-97),
pages 64?71, Madrid, Spain.
Lin, Dekang. 1998a. Automatic retrieval and
clustering of similar words. In Proceedings
of COLING-ACL?98, pages 768?774,
Montreal, Canada.
Lin, Dekang. 1998b. Dependency-based
evaluation of MINIPAR. In Proceedings of
the Workshop on the Evaluation of Parsing
Systems, pages 48?56, Granada, Spain.
http://www.cs.ualberta.ca/~lindek/
minipar.htm.
Magnini, Bernardo and Gabriela Cavaglia`.
2000. Integrating subject field codes into
WordNet. In Proceedings of LREC-2000,
pages 1413?1418, Athens, Greece.
Magnini, Bernardo, Carlo Strapparava,
Giovanni Pezzuli, and Alfio Gliozzo. 2001.
Using domain information for word sense
disambiguation. In Proceedings of the
Senseval-2 Workshop, pages 111?114,
Toulouse, France.
Magnini, Bernardo, Carlo Strapparava,
Giovanni Pezzulo, and Alfio Gliozzo. 2002.
The role of domain information in word
sense disambiguation. Natural Language
Engineering, 8(4):359?373.
Martinez, David and Eneko Agirre. 2000.
One sense per collocation and genre/topic
variations. In Proceedings of the Joint
SIGDAT Conference on Empirical Methods in
Natural Language Processing and Very Large
Corpora, pages 207?215, Hong Kong.
McCarthy, Diana. 2006. Relating WordNet
senses for word sense disambiguation. In
Proceedings of the EACL 06 Workshop:
Making Sense of Sense: Bringing
Psycholinguistics and Computational
Linguistics Together, pages 17?24, Trento,
Italy.
McCarthy, Diana and John Carroll. 2003.
Disambiguating nouns, verbs, and
adjectives using automatically acquired
selectional preferences. Computational
Linguistics, 29(4):639?654.
McCarthy, Diana, Rob Koeling, Julie Weeds,
and John Carroll. 2004a. Finding
predominant senses in untagged text. In
Proceedings of the 42nd Annual Meeting of the
Association for Computational Linguistics,
pages 280?287, Barcelona, Spain.
McCarthy, Diana, Rob Koeling, Julie Weeds,
and John Carroll. 2004b. Ranking WordNet
senses automatically. CSRP 569,
Department of Informatics, University of
Sussex, January.
McCarthy, Diana, Rob Koeling, Julie Weeds,
and John Carroll. 2004c. Using
automatically acquired predominant
senses for word sense disambiguation. In
Proceedings of the ACL Senseval-3 Workshop,
pages 151?154, Barcelona, Spain.
McCarthy, Diana and Roberto Navigli. 2007.
SemEval-2007 task 10: English lexical
substitution task. In Proceedings of
ACL/SIGLEX SemEval-2007, pages 48?53,
Prague, Czech Republic.
Mihalcea, Rada and Phil Edmonds, editors.
2004. Proceedings Senseval-3 3rd
International Workshop on Evaluating Word
Sense Disambiguation Systems. ACL,
Barcelona, Spain.
Miller, George A., Claudia Leacock, Randee
Tengi, and Ross T. Bunker. 1993. A
semantic concordance. In Proceedings of the
ARPA Workshop on Human Language
Technology, pages 303?308, San Francisco,
CA.
Mohammad, Saif and Graeme Hirst.
2006. Determining word sense dominance
using a thesaurus. In Proceedings of
the 11th Conference of the European Chapter
of the Association for Computational
Linguistics (EACL-2006), pages 121?128,
Trento, Italy.
Navigli, Roberto, Ken Litkowski, and
Orin Hargraves. 2007. SemEval-2007
task 7: Coarse-grained English all-words
task. In Proceedings of ACL/SIGLEX
SemEval-2007, pages 30?35, Prague,
Czech Republic.
589
Computational Linguistics Volume 33, Number 4
Palmer, Martha, Hoa Trang Dang, and
Christiane Fellbaum. 2007. Making
fine-grained and coarse-grained sense
distinctions, both manually and
automatically. Natural Language
Engineering, 13(02):137?163.
Pantel, Patrick and Dekang Lin.
2002. Discovering word senses from
text. In Proceedings of ACM SIGKDD
Conference on Knowledge Discovery and
Data Mining, pages 613?619, Edmonton,
Alberta, Canada.
Patwardhan, Siddharth, Satanjeev Banerjee,
and Ted Pedersen. 2003. Using measures of
semantic relatedness for word sense
disambiguation. In Proceedings of the Fourth
International Conference on Intelligent Text
Processing and Computational Linguistics
(CICLing 2003), pages 241?257, Mexico
City, Mexico.
Patwardhan, Siddharth and Ted Pedersen.
2003. The CPAN WordNet::Similarity
Package. http://search.cpan.org/~sid/
WordNet-Similarity-0.05/.
Preiss, Judita and David Yarowsky, editors.
2001. Proceedings of Senseval-2 Second
International Workshop on Evaluating Word
Sense Disambiguation Systems. ACL,
Toulouse, France.
Procter, Paul, editor. 1978. Longman
Dictionary of Contemporary English.
Longman Group Ltd., Harlow, UK.
Resnik, Philip. 1995. Using information
content to evaluate semantic similarity
in a taxonomy. In 14th International
Joint Conference on Artificial Intelligence,
pages 448?453, Montreal, Canada.
Rose, Tony G., Mary Stevenson, and Miles
Whitehead. 2002. The Reuters Corpus
volume 1?From yesterday?s news to
tomorrow?s language resources. In
Proceedings of the 3rd International
Conference on Language Resources and
Evaluation, pages 827?833, Las Palmas,
Canary Islands, Spain.
Siegel, Sidney and N. John Castellan.
1988. Non-Parametric Statistics for the
Behavioral Sciences. McGraw-Hill,
New York, NY.
Snyder, Benjamin and Martha Palmer.
2004. The English all-words task.
In Proceedings of the ACL Senseval-3
Workshop, pages 41?43, Barcelona,
Spain.
Stevenson, Mark and Yorick Wilks. 2001.
The interaction of knowledge sources for
word sense disambiguation. Computational
Linguistics, 27(3):321?350.
Weeds, Julie. 2003. Measures and
Applications of Lexical Distributional
Similarity. Ph.D. thesis, Department of
Informatics, University of Sussex,
Brighton, UK.
Weeds, Julie, James Dowdall, Gerold
Schneider, Bill Keller, and David Weir.
2005. Using distributional similarity to
organise biomedical terminology.
Terminology, 11(1):107?141.
Weeds, Julie and David Weir. 2005.
Co-occurrence Retrieval: A flexible
framework for lexical distributional
similarity. Computational Linguistics,
31(4):439?476.
Yarowsky, David and Radu Florian. 2002.
Evaluating sense disambiguation
performance across diverse parameter
spaces. Natural Language Engineering,
8(4):293?310.
590
Proceedings of the 2nd Workshop on Continuous Vector Space Models and their Compositionality (CVSC) @ EACL 2014, pages 11?20,
Gothenburg, Sweden, April 26-30 2014.
c
?2014 Association for Computational Linguistics
Distributional Composition using Higher-Order Dependency Vectors
Julie Weeds, David Weir and Jeremy Reffin
Department of Informatics
University of Sussex
Brighton, BN1 9QH, UK
{J.E.Weeds, D.J.Weir, J.P.Reffin}@sussex.ac.uk
Abstract
This paper concerns how to apply compo-
sitional methods to vectors based on gram-
matical dependency relation vectors. We
demonstrate the potential of a novel ap-
proach which uses higher-order grammat-
ical dependency relations as features. We
apply the approach to adjective-noun com-
pounds with promising results in the pre-
diction of the vectors for (held-out) ob-
served phrases.
1 Introduction
Vector space models of semantics characterise the
meaning of a word in terms of distributional fea-
tures derived from word co-occurrences. The most
widely adopted basis for word co-occurrence is
proximity, i.e. that two words (or more generally
lexemes) are taken to co-occur when they occur
together within a certain sized window, or within
the same sentence, paragraph, or document. Lin
(1998), in contrast, took the syntactic relationship
between co-occurring words into account: the dis-
tributional features of a word are based on the
word?s grammatical dependents as found in a de-
pendency parsed corpus. For example, observing
that the word glass appears as the indirect object
of the verb fill, provides evidence that the word
glass has the distributional feature iobj:fill, where
iobj denotes the inverse indirect object grammati-
cal relation. The use of grammatical dependents as
word features has been exploited in the discovery
of tight semantic relations, such as synonymy and
hypernymy, where an evaluation against a gold
standard such as WordNet (Fellbaum, 1998) can
be made (Lin, 1998; Weeds and Weir, 2003; Cur-
ran, 2004).
Pado and Lapata (2007) took this further by
considering not just direct grammatical depen-
dents, but also including indirect dependents.
Thus, observing the sentence She filled her glass
slowly would provide evidence that the word glass
has the distributional feature iobj:advmod:slowly
where iobj:advmod captures the indirect depen-
dency relationship between glass and slowly in the
sentence.
Note that Pado and Lapata (2007) included
a basis mapping function that gave their frame-
work flexibility as to how to map paths such as
iobj:advmod:slowly onto the basis of the vector
space. Indeed, the instantiation of their framework
that they adopt in their experiments uses a ba-
sis mapping function that removes the dependency
path to leave just the word, so iobj:advmod:slowly
would be mapped to slowly.
In this paper, we are concerned with the prob-
lem of distributional semantic composition. We
show that the idea that the distributional seman-
tics of a word can be captured with higher-order
dependency relationships, provides the basis for
a simple approach to compositional distributional
semantics. While our approach is quite gen-
eral, dealing with arbitrarily high-order depen-
dency relationships, and the composition of ar-
bitrary phrases, in this paper we consider only
first and second order dependency relations, and
adjective-noun composition.
In Section 2, we illustrate our proposal by
showing how second order dependency relations
can play a role in computing the semantics of
adjective-noun composition. In Section 3 we de-
scribe a number of experiments that are intended
to evaluate the approach, with the results presented
in Section 4.
The basis for our evaluation follows Baroni and
11
Zamparelli (2010) and Guevara (2010). Typically,
compositional distributional semantic models can
be used to generate an (inferred) distributional
vector for a phrase from the (observed) distribu-
tional vectors of the phrase?s constituents. One
of the motivations for doing this is that the ob-
served distributional vectors for most phrases tend
to be very sparse, a consequence of the frequency
with which typical phrases occur in even large cor-
pora. However, there are phrases that occur suffi-
ciently frequently that a reasonable characterisa-
tion of their meaning can be captured with their
observed distributional vector. Such phrases can
be exploited in order to assess the quality of a
model of composition. This is achieved by mea-
suring the distributional similarity of the observed
and inferred distributional vectors for these high
frequency phrases.
The contributions of this paper are as follows.
We propose a novel approach to phrasal composi-
tion which uses higher order grammatical depen-
dency relations as features. We demonstrate its
potential in the context of adjective-noun compo-
sition by comparing (held-out) observed and in-
ferred phrasal vectors. Further, we compare dif-
ferent vector operations, different feature associa-
tion scores and investigate the effect of weighting
features before or after composition.
2 Composition with Higher-order
Dependencies
Consider the problem of adjective-noun compo-
sition. For example, what is the meaning of the
phrase small child? How does it relate to the
meanings of the lexemes small and child? Figure 1
shows a dependency analysis for the sentence The
very small wet child cried loudly. Tables 1 and
2 show the grammatical dependencies (with other
open-class words) for the lexemes small and child
which would be extracted from it.
the/D very/R small/J wet/J child/N cry/V loudly/R
amod
amod
advmod
det
nsubj advmod
Figure 1: Example Dependency Tree
From Table 1 we see what kinds of (higher-
order) dependency paths appear in the distribu-
tional features of adjectives such as small. Simi-
larly, Table 2 indicates this for nouns such as child.
1st-order advmod:very/R
amod:child
2nd-order amod:amod:wet/J
amod:nsubj:cry/V
3rd-order amod:nsubj:advmod:loudly/R
Table 1: Grammatical Dependencies of small
1st-order amod:wet/J
amod:small/J
nsubj:cry/V
2nd-order amod:advmod:very/R
nsubj:advmod:loudly/R
Table 2: Grammatical Dependencies of child
It is clear that with a conventional grammatical
dependency-based approach where only first or-
der dependencies for small and child would be
considered, there will be very little overlap be-
tween the features of nouns and adjectives because
quite different grammatical relations are used in
the two types of vectors, and correspondingly lex-
emes with different parts of speech appear at the
end of these paths.
However, as our example illustrates, it is possi-
ble to align the 2nd-order feature space of adjec-
tives with the 1st-order feature space of nouns. In
this example, we have evidence that children cry
and that small things cry. Consequently, in order
to compose an adjective with a noun, we would
want to align 2nd-order features of the adjective
with 1st-order features of the noun; this gives us a
prediction of the first order features of the noun in
the context of the adjective
1
.
This idea extends in a straightforward way be-
yond adjective-noun composition. For example, it
is possible to align the 3rd order features of ad-
jectives with 2nd order features of nouns, which is
something that would be useful if one wanted to
compose verbs with their arguments. These argu-
ments will include adjective-noun compounds and
therefore adjective-noun compounds require 2nd-
order features which can be aligned with the first
order features of the verbs. This is, however, not
1
Note that it would also be possible to align 2nd-order
features of the noun with 1st-order features of the adjective,
resulting in a prediction of the first order features of the ad-
jective in the context of the noun.
12
something that we will pursue further in this paper.
We now clarify how features vectors are aligned
and then composed. Suppose that the lexemes w
1
and w
2
which we wish to compose are connected
by relation r. Let w
1
be the head of the relation
and w
2
be the dependent. In our example, w
1
is
child, w
2
is small and r is amod. We first pro-
duce a reduced vector for w
2
which is designed
to lie in a comparable feature space as the vector
for w
1
. To do this we take the set of 2nd order
features of w
2
which start with the relation r? and
reduce them to first order features (by removing
the r? at the start of the path). So in our example,
we create a reduced vector for small where fea-
tures amod:nsubj:x for some token x are reduced
to nsubj:x, features amod:amod:x for some token
x are reduced to the feature amod:x, and features
amod:nsubj:advmod:x for some token x are re-
duced to nsubj:advmod:x. Once the vector for w
2
has been reduced, it can be composed with the vec-
tor for w
1
using standard vector operations.
In Section 3 we describe experiments that ex-
plore the effectiveness of this approach to distri-
butional composition by measuring the similarity
of composed vectors with observed vectors for a
set of frequently occurring adjective-noun pairs
(details given below). We evaluate a number of
instantiations of our approach, and in particular,
there are three aspects of the model where alter-
native solutions are available: the choice of which
vector composition operation to use; the choice of
how to weight dependency features; and the ques-
tion as to whether feature weighting should take
place before or after composition.
Vector composition operation. We consider
each of the following seven alternatives: pointwise
addition (add), pointwise multiplication (mult),
pointwise geometric mean
2
(gm), pointwise max-
imum (max), pointwise minimum (min), first ar-
gument (hd), second argument (dp). The latter
two operations simply return the first (respectively
second) of the input vectors.
Feature weighting. We consider three options.
Much work in this area has used positive pointwise
mutual information (PPMI) (Church and Hanks,
1989) to weight the features. However, PPMI is
known to over-emphasise low frequency events,
and as a result there has been a recent shift to-
wards using positive localised mutual information
2
The geometric mean of x and y is
?
(x ? y).
PPMI(x, y) =
{
I(x, y) if I(x, y) > 0
0 otherwise
where I(x, y) = log
P (x,y)
P (x).P (y)
PLMI(x, y) =
{
L(x, y) if L(x, y) > 0
0 otherwise
where L(x, y) = P (x, y).log(
P (x,y)
P (x).P (y)
PNPMI(x, y) =
{
N(x, y) if N(x, y) > 0
0 otherwise
where N(x, y) =
1
?log(P (y)
.log
P (x,y)
P (x).P (y)
Table 3: Feature Association Scores
(PLMI) (Scheible et al., 2013) and positive nor-
malised point wise mutual information (PNPMI)
(Bouma, 2009). For definitions, see Table 3.
Timing of feature weighting. We consider two
alternatives: we can weight features before com-
position so that the composition operation is ap-
plied to weighted vectors, or we can compose vec-
tors prior to feature weighting, in which case the
composition operation is applied to unweighted
vectors, and feature weighting is applied in the
context of making a similarity calculation. In other
work, the former order is often implied. For exam-
ple, Boleda et al. (2013) state that they use ?PMI
to weight the co-occurrence matrix?. However, if
we allow the second order, features which might
have a zero association score in the context of the
the individual lexemes, could be considered sig-
nificant in the context of the phrase.
3 Evaluation
Our experimental evaluation of the approach is
based on the assumption, which is commonly
made elsewhere, that where there is a reasonable
amount of corpus data available for a phrase, this
will generate a good estimate of the vector of the
phrase. It has been shown (Turney, 2012; Baroni
and Zamparelli, 2010) that such ?observed? vec-
tors are indeed reasonable for adjective-noun and
noun-noun compounds. Hence, in order to evalu-
ate the compositional models under consideration
here, we compare observed phrasal vectors with
inferred phrasal vectors, where the comparison is
made using the cosine measure. We note that it is
13
not possible to draw conclusions from the absolute
value of the cosine score since this would favour
models which always assign higher cosine scores.
Hence, we draw conclusions from the change in
cosine score with respect to a baseline within the
same model.
Methodology
For each noun and adjective which occur more
than a threshold number of times in a corpus, we
first extract conventional first order dependency
vectors. The features of these lexemes define the
semantic space, and feature probabilities (for use
in association scores) are calculated from this data.
Given a list of adjective-noun phrases, we ex-
tract first order vectors for the nouns and second
order vectors for the adjectives, which we refer to
as observed constituent vectors. We also extract
first order vectors for the nouns in the context of
the adjective, which we refer to as the observed
phrasal vector.
For each adjective-noun pair, we build bespoke
constituent vectors for the adjective and noun, in
which we remove all counts which arise from co-
occurrences with that specific adjective-noun pair.
It is these constituent vectors that are used as the
basis for inferring the vector for that particular
adjective-noun phrase.
Our rationale for this is as follows. Without this
modification, the observed constituent vectors will
contain co-occurrences which are due to the ob-
served adjective-noun vector co-occurrences. To
see why this is undesirable, suppose that one of the
adjective-noun phrases was small child. We take
the observed vector for small child to be what we
are calling the observed phrasal vector for child (in
the context of small). Suppose that when building
the observed phrasal vector, we observe the phrase
the small child cried. This will lead to a count for
the feature nsubj:cry in the observed phrasal vec-
tor for child.
But if we are not careful, this same phrase will
contribute to counts in the constituent vectors for
small and child, producing counts for the features
amod:nsubj:cry and nsubj:cry, in their respective
vectors. To see why these counts should not be in-
cluded when building the constituent vectors that
we compose to produce inferred vectors for the
adjective-noun phrase small child, consider the
case where all of the evidence for small things be-
ing things that can cry and children being things
that can crying comes from having observed the
phrase small children crying. Despite not having
learnt anything about the composition of small and
child in general, we would be able to infer the cry
feature for the phrase. An adequate model of com-
position should be able to infer this on the basis
that other small things have been seen to cry, and
that non-small children have been seen to cry.
Here, we compare the proposed approach,
based on higher order dependencies, with the
standard method of composing conventional first-
order dependency vectors. The vector operation,
hd provides a baseline for comparison which is
the same in both approaches. This baseline corre-
sponds to a composition model where the first or-
der dependencies of the phrase (i.e. the noun in the
context of the adjective) are taken to be the same
as the first order dependencies of the uncontextu-
alized noun. For example, if we have never seen
the phrase small child before, we would assume
that it means the same as the head word child.
We hypothesise that it is not possible to im-
prove on this baseline using traditional first-order
dependency relation vectors, since the vector for
the modifier does not contain features of the right
type, but that with the proposed approach, the in-
ferred vector for a phrase such as small child will
be closer than observed vector for child to the ob-
served vector for small child. We also ask the re-
lated question of whether our inferred vector for
small child is closer than the constituent vector for
small to the observed vector for small child. This
comparison is achieved through use of the vector
operation dp that ignores the vector for the head,
simply returning a first-order vector derived from
the dependent.
Experimental Settings
Our corpus is a mid-2011 dump of WikiPedia.
This has been part-of-speech tagged, lemmatised
and dependency parsed using the Malt Parser
(Nivre, 2004). All major grammatical dependency
relations involving open class parts of speech
(nsubj, dobj, iobj, conj, amod, advmod, nnmod)
have been extracted for all POS-tagged and lem-
matised nouns and adjectives occurring 100 or
more times. In past work with conventional de-
pendency relation vectors we found that using a
feature threshold of 100, weighting features with
PPMI and a cosine similarity score work well.
For experimental purposes, we have taken
14
spanish british african japanese
modern classical female natural
digital military medical musical
scientific free black white
heavy common small large
strong short long good
similar previous future original
former subsequent next possible
Table 4: Adjectives considered
32 of the most frequently occurring adjectives
(see Table 4). These adjectives include ones
which would generally be considered intersective
(e.g., female), subsective (e.g,, long) and non-
subsective/intensional (e.g., former) (Pustejovsky,
2013) . For all of these adjectives there are at least
100 adjective-noun phrases which occur at least
100 times in the corpus. We randomly selected
50 of the phrases for each adjective. Note that
our proposed method does not require any hyper
parameters to be set during training, nor does it
require a certain number of phrases per adjective.
For the purpose of these experiments we have a list
of 1600 adjective-noun phrases, all of which occur
at least 100 times in WikiPedia.
4 Results and Discussion
Tables 5 and 6 summarise the average cosines for
the proposed higher-order dependency approach
and the conventional first-order dependency ap-
proach, respectively. In each case, we consider
each combination of vector operation, feature as-
sociation score, and composition timing (i.e. be-
fore, or after, vector weighting).
Table 7 shows the average improvement over
the baseline (hd), for each combination of exper-
imental variables, when considering the proposed
higher-order dependency approach. Note that this
is an average of paired differences (and not the dif-
ference of the averages in Table 6). For brevity, we
omit the results for PNPMI here, since there do not
appear to be substantial differences between using
PPMI and PNPMI. To indicate statistical signifi-
cance, we show estimated standard errors in the
means. All differences are statistically significant
(under a paired t-test) except those marked ?.
From Table 5, we see that none of the com-
positional operations on conventional dependency
vectors are able to beat the baseline of selecting
the head vector (hd). This is independent of the
choice of association measure and the order in
which weighting and composition are carried out.
For the higher order dependency vectors (Tables
6 and 7), we note, in contrast, that some com-
positional operations produce large increases in
cosine score compared to the head vector alone
(hd). Table 7 examines the statistical significance
of these differences. We find that for the inter-
sective composition operations (mult, min, and
gm), performance is statistically superior to using
the head alone in all experimental conditions stud-
ied. By contrast, additive measures (add, max)
typically have no impact, or decrease performance
marginally relative to the head alone. An explana-
tion for these significant differences is that inter-
sective vector operations are able to encapsulate
the way that an adjective disambiguates and spe-
cialises the sense of the noun that it is modifying.
We also note that the alternative baseline, dp,
which estimates the features of a phrase to be the
aggregation of all things which are modified by
the adjective, performs significantly worse than
the standard baseline, hd, which estimates the fea-
tures of a phrase to be the features of the head
noun. This is consistent with the intuition that the
distributional vector for small child should more
similar to the vector for child than it is to the vec-
tor for the things that can be small.
Considering the different intersective opera-
tions, mult appears to be the best choice when
the feature association score is PPMI or PNPMI
and gm appears to be the best choice when the fea-
ture association score is PLMI.
Further, PLMI consistently gives all of the vec-
tor pairings higher cosine scores than PPMI. Since
PLMI assigns less weight to low frequency event
and more weight to high frequency events, this
suggests that all of the composition methods, in-
cluding the baseline (hd), do better at predicting
the high frequency co-occurrences. This is not sur-
prising as these will more likely have been seen
with the phrasal constituents in other contexts.
Our final observation, based on Table 6, is that
the best order in which to carry out weighting and
composition appears to depend on the choice of
feature association score. In general, it appears
better to weight the features and then compose
vectors. This is always true when using PNPMI
or PLMI. However, using PPMI, the highest per-
formance is achieved by composing the raw vec-
tors using multiplication and then weighing the
15
weight:compose compose:weight
PPMI PNPMI PLMI PPMI PNPMI PLMI
x? s x? s x? s x? s x? s x? s
add 0.12 (0.06) 0.13 (0.05) 0.15 (0.16) 0.11 (0.05) 0.12 (0.06) 0.22 (0.20)
max 0.12 (0.06) 0.13 (0.05) 0.15 (0.16) 0.11 (0.05) 0.12 (0.06) 0.22 (0.20)
mult 0.06 (0.05) 0.06 (0.06) 0.06 (0.11) 0.07 (0.05) 0.07 (0.12) 0.07 (0.05)
min 0.05 (0.05) 0.06 (0.05) 0.04 (0.09) 0.05 (0.04) 0.05 (0.04) 0.04 (0.08)
gm 0.06 (0.05) 0.06 (0.05) 0.07 (0.11) 0.05 (0.04) 0.06 (0.04) 0.08 (0.11)
hd 0.13 (0.07) 0.15 (0.07) 0.28 (0.22) 0.13 (0.07) 0.15 (0.07) 0.28 (0.22)
Table 5: Means and Standard Deviations for Cosines Between Observed and Predicted Vectors for Con-
ventional First-Order Dependency Based Approach.
weight:compose compose:weight
PPMI PNPMI PLMI PPMI PNPMI PLMI
x? s x? s x? s x? s x? s x? s
add 0.14 (0.06) 0.16 (0.06) 0.29 (0.21) 0.10 (0.04) 0.12 (0.05) 0.29 (0.22)
max 0.10 (0.04) 0.11 (0.04) 0.27 (0.21) 0.10 (0.04) 0.11 (0.04) 0.26 (0.21)
mult 0.30 (0.12) 0.33 (0.12) 0.40 (0.29) 0.34 (0.10) 0.32 (0.10) 0.32 (0.27)
min 0.26 (0.11) 0.27 (0.11) 0.40 (0.24) 0.24 (0.10) 0.25 (0.10) 0.37 (0.23)
gm 0.27 (0.11) 0.29 (0.11) 0.46 (0.20) 0.26 (0.10) 0.27 (0.10) 0.44 (0.22)
dp 0.10 (0.05) 0.10 (0.05) 0.20 (0.20) 0.10 (0.05) 0.10 (0.05) 0.20 (0.20)
hd 0.13 (0.07) 0.15 (0.07) 0.28 (0.22) 0.13 (0.07) 0.15 (0.07) 0.28 (0.22)
Table 6: Means and Standard Deviations for Cosines Between Observed and Predicted Vectors for Pro-
posed Higher-Order Dependency Based Approach
remaining features. This can be explained by
considering the recall and precision of the com-
posed vector?s prediction of the observed vec-
tor. If we compose using gm before weighting
vectors, we increase the recall of the prediction,
but decrease precision. Whether we use PPMI,
PNPMI or PLMI, recall of features increases from
88.8% to 99.5% and precision drops from 5.5% to
4.8%. If we compose using mult before weight-
ing vectors, contrary to expectation, recall de-
creases and precision increases. Whether we use
PPMI, PNPMI or PLMI, recall of features de-
creases from 88.8% to 59.4% but precision in-
creases from 5.5% to 18.9%. Hence, multiplica-
tion of the raw vectors is causing a lot of potential
shared features to be ?lost? when the weighting
is subsequently carried out (since multiplication
stretches out the value space). This leads to an
increase in cosines when PPMI is used for weight-
ing, and a decrease in cosines when PLMI is used.
Hence, it appears that the features being removed
by multiplying the raw vectors before weighting
must be low frequency co-occurrences, which are
not observed with the phrase.
5 Related Work
In this work, we bring together ideas from sev-
eral different strands of distributional semantics:
incorporating syntactic information into the distri-
butional representation of a lexeme; representing
phrasal meaning by creating distributional repre-
sentations through composition; and representing
word meaning in context by modifying the distri-
butional representation of a word.
The use of syntactic structure in distributional
representations is not new. Two of the earliest
proponents of distributional semantics, Lin (1998)
and Lee (1999) used features based on first order
dependency relations between words in their dis-
tributional representations. More recently, Pado
and Lapata (2007) propose a semantic space based
on dependency paths. This model outperformed
traditional word-based models which do not take
syntax into account in a synonymy relation detec-
tion task and a prevalent sense acquisition task.
The problem of representing phrasal meaning
has traditionally been tackled by taking vector rep-
resentations for words (Turney and Pantel, 2010)
and combining them using some function to pro-
16
weight:compose compose:weight
PPMI PLMI PPMI PLMI
x? s
x?
x? s
x?
x? s
x?
x? s
x?
add 0.01 (0.001) ?0.004 (0.003) -0.03 (0.001) ?0.006 (0.004)
max -0.03 (0.001) -0.01 (0.003) -0.04 (0.001) -0.02 (0.003)
mult 0.16 (0.002) 0.11 (0.006) 0.21 (0.002) 0.03 (0.006)
min 0.13 (0.001) 0.11 (0.007) 0.10 (0.001) 0.09 (0.007)
gm 0.14 (0.001) 0.18 (0.005) 0.12 (0.001) 0.16 (0.005)
dp -0.03 (0.002) -0.09 (0.007) -0.04 (0.002) -0.09 (0.007)
Table 7: Means and Standard Errors for Increases in Cosine with respect to the hd Baseline for Proposed
Higher-Order Dependency Based Approach. All differences statistically significant (under a paired t-
test) except those marked ?.
duce a data structure that represents the phrase
or sentence. Mitchell and Lapata (2008, 2010)
found that simple additive and multiplicative func-
tions applied to proximity-based vector represen-
tations were no less effective than more com-
plex functions when performance was assessed
against human similarity judgements of simple
paired phrases.
The simple functions evaluated by Mitchell and
Lapata (2008) are generally acknowledged to have
serious theoretical limitations in their treatment
of composition. How can a commutative func-
tion such as multiplication or addition provide dif-
ferent interpretations for different word orderings
such as window glass and glass window? The
majority of attempts to rectify this have offered
a more complex, non-commutative function ?
such as weighted addition ? or taken the view
that some or all words are no longer simple vec-
tors. For example, in the work of Baroni and
Zamparelli (2010) and Guevara (2010), an adjec-
tive is viewed as a modifying function and rep-
resented by a matrix. Coecke et al. (2011) and
Grefenstette et al. (2013) also incorporate the no-
tion of function application from formal seman-
tics. They derived function application from syn-
tactic structure, representing functions as tensors
and arguments as vectors. The MV-RNN model
of Socher et al. (2012) broadened the Baroni and
Zamparelli (2010) approach; all words, regardless
of part-of-speech, were modelled with both a vec-
tor and a matrix. This approach also shared fea-
tures with Coecke et al. (2011) in using syntax
to guide the order of phrasal composition. These
higher order structures are typically learnt or in-
duced using a supervised machine learning tech-
nique. For example, Baroni and Zamparelli (2010)
learnt their adjectival matrixes by performing re-
gression analysis over pairs of observed nouns and
adjective-noun phrases. As a consequence of the
computational expense of the machine learning
techniques involved, implementations of these ap-
proaches typically require a considerable amount
of dimensionality reduction.
A long-standing topic in distributional seman-
tics has been the modification of a canonical repre-
sentation of a lexeme?s meaning to reflect the con-
text in which it is found. Typically, a canonical
vector for a lexeme is estimated from all corpus
occurrences and the vector then modified to reflect
the instance context (Lund and Burgess, 1996;
Erk and Pad?o, 2008; Mitchell and Lapata, 2008;
Thater et al., 2009; Thater et al., 2010; Thater et
al., 2011; Van de Cruys et al., 2011; Erk, 2012).
As described in Mitchell and Lapata (2008, 2010),
lexeme vectors have typically been modified using
simple additive and multiplicative compositional
functions. Other approaches, however, share with
our proposal the use of syntax to drive modifica-
tion of the distributional representation (Erk and
Pad?o, 2008; Thater et al., 2009; Thater et al., 2010;
Thater et al., 2011). For example, in the SVS rep-
resentation of Erk and Pad?o (2008), a word was
represented by a set of vectors: one which en-
codes its lexical meaning in terms of distribution-
ally similar words
3
, and one which encodes the
selectional preferences of each grammatical rela-
tion it supports. A word?s meaning vector was up-
dated in the context of another word by combining
it with the appropriate selectional preferences vec-
3
These are referred to as second-order vectors using
the terminology of Grefenstette (1994) and Sch?utze (1998).
However, this refers to a second-order affinity between the
words and is not related to the use of grammatical depen-
dency relations.
17
tor of the contextualising word.
Turney (2012) offered a model of phrasal level
similarity which combines assessments of word-
level semantic relations. This work used two
different word-level distributional representations
to encapsulate two types of similarity. Distribu-
tional similarity calculated from proximity-based
features was used to estimate domain similarity
and distributional similarity calculated from syn-
tactic pattern based features is used to estimate
functional similarity. The similarity of a pair of
compound noun phrases was computed as a func-
tion of the similarities of the components. Cru-
cially different from other models of phrasal level
similarity, it does not attempt to derive modified
vectors for phrases or words in context.
6 Conclusions and Further Work
Vectors based on grammatical dependency rela-
tions are known to be useful in the discovery of
tight semantic relations, such as synonymy and
hypernymy, between lexemes (Lin, 1998; Weeds
and Weir, 2003; Curran, 2004). It would be use-
ful to be able to extend these methods to deter-
mine similarity between phrases (of potentially
different lengths). However, conventional ap-
proaches to composition, which have been ap-
plied to proximity-based vectors, cannot sensibly
be used on vectors that are based on grammatical
dependency relations.
In our approach, we consider the vector for a
phrase to be the vector for the head lexeme in
the context of the other phrasal constituents. Like
Pado and Lapata (2007), we extend the concept
of a grammatical dependency relation feature to
include dependency relation paths which incor-
porate higher-order dependencies between words.
We have shown how it is possible to align the de-
pendency path features for words of different syn-
tactic types, and thus produce composed vectors
which predict the features of one constituent in the
context of the other constituent.
In our experiments with AN compounds, we
have shown that these predicted vectors are closer
than the head constituent?s vector to the observed
phrasal vector. We have shown this is true even
when the observed phrase is in fact unobserved,
i.e. when its co-occurrences do not contribute to
the constituents? vectors. Consistent with work us-
ing proximity-based vectors, we have found that
intersective operations perform substantially bet-
ter than additive operations. This can be under-
stood by viewing the intersective operations as en-
capsulating the way that adjectives can specialise
the meaning of the nouns that they modify.
We have investigated the interaction between
the vector operation used for composition, the fea-
ture association score and the timing of applying
feature weights. We have found that multiplication
works best if using PPMI to weight features, but
that geometric mean is better if using the increas-
ingly popular PLMI weighting measure. Whilst
applying an intersective composition operation be-
fore applying feature weighting does allow more
features to be retained in the predicted vector (it
is possible to achieve 99.5% recall), in general,
this does not correspond with an increase in co-
sine scores. In general, the corresponding drop in
precision (i.e., the over-prediction of unobserved
features) causes the cosine to decrease. The one
exception to this is using multiplication with the
PPMI feature weighting score. Here we actually
see a drop in recall, and an increase in precision
due to the nature of multiplication and PPMI.
One assumption that has been made throughout
the work, is that the observed phrasal vector pro-
vides a good estimate of the distributional repre-
sentation of the phrase and, consequently, the best
composition method is the one which returns the
most similar prediction. However, in general, we
notice that while the recall of the compositional
methods is good, the precision is very low. Lack of
precision may be due to the prevalence of plausi-
ble, but unobserved, co-occurrences of the phrase.
Consequently, this introduces uncertainty into the
conclusions which can be drawn from a study such
as this. Further work is required to develop effec-
tive intrinsic and extrinsic evaluations of models
of composition.
A further interesting area of study is whether
distributional models that include higher-order
grammatical dependencies can tell us more about
the lexical semantics of a word than the conven-
tional first-order models, for example by distin-
guishing semantic relations such as synonymy,
antonymy, hypernymy and co-hyponymy.
Acknowledgements
This work was funded by UK EPSRC project
EP/IO37458/1 ?A Unified Model of Composi-
tional and Distributional Compositional Seman-
tics: Theory and Applications?.
18
References
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing.
Gemma Boleda, Marco Baroni, The Nghia Pham, and
Louise McNally. 2013. Intensionality was only al-
leged: On adjective-noun composition in distribu-
tional semantics. In Proceedings of the 10th Inter-
national Conference on Computational Semantics
(IWCS 2013) ? Long Papers, pages 35?46, Pots-
dam, Germany, March. Association for Computa-
tional Linguistics.
Gerlof Bouma. 2009. Normalised (point wise) mu-
tual information in collocation extraction, from form
to meaning: Processing texts automatically. In Pro-
ceedings of the Biennial International Conference of
the German Society for Computational Linguistics
and Language Technology.
Kenneth Ward Church and Patrick Hanks. 1989. Word
association norms, mutual information, and lexicog-
raphy. In Proceedings of the 27th Annual Meeting
on Association for Computational Linguistics, ACL
?89, pages 76?83, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
Bob Coecke, Mehrnoosh Sadrzadeh, and Stephen
Clark. 2011. Mathematical foundations for a com-
positional distributed model of meaning. Linguistic
Analysis, 36(1-4):345?384.
James Curran. 2004. From Distributional to Semantic
Similarity. Ph.D. thesis, University of Edinburgh.
Katrin Erk and Sebastian Pad?o. 2008. A structured
vector space model for word meaning in context.
In Proceedings of the 2008 Conference on Empiri-
cal Methods in Natural Language Processing, pages
897?906, Honolulu, Hawaii, October. Association
for Computational Linguistics.
Katrin Erk. 2012. Vector space models of word mean-
ing and phrase meaning: a survey. Language and
Linguistics Compass, 6(10):635?653.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. Bradford Books.
Edward Grefenstette, Georgiana Dinu, Yao-Zhong
Zhang, Mehrnoosh Sadrzadeh, and Marco Baroni.
2013. Multi-step regression learning for compo-
sitional distributional semantics. Proceedings of
the 10th International Conference on Computational
Semantics (IWCS 2013).
Gregory Grefenstette. 1994. Corpus-derived first, sec-
ond and third-order word affinities. In Proceedings
of Euralex 1994.
Emiliano Guevara. 2010. A Regression Model of
Adjective-Noun Compositionality in Distributional
Semantics. In Proceedings of the ACL GEMS Work-
shop, pages 33?37.
Lillian Lee. 1999. Measures of distributional simi-
larity. In Proceedings of the 37th Annual Meeting
of the Association for Computational Linguistics,
pages 25?32, College Park, Maryland, USA, June.
Association for Computational Linguistics.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of the 17th Inter-
national Conference on Computational Linguistings
(COLING 1998).
K. Lund and C. Burgess. 1996. Producing
high-dimensional semantic spaces from lexical co-
occurrence. Behavior Research Methods, Instru-
mentation, and Computers, 28:203?208.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings
of ACL-08: HLT, pages 236?244, Columbus, Ohio,
June. Association for Computational Linguistics.
Jeff Mitchell and Mirella Lapata. 2010. Composition
in distributional models of semantics. Cognitive Sci-
ence, 34(8):1388?1429.
Joakim Nivre. 2004. Incrementality in determinis-
tic dependency parsing. In Proceedings of the ACL
Workshop on Incremental Parsing, pages 50?57.
Sebastian Pado and Mirella Lapata. 2007.
Dependency-based construction of semantic space
models. Computational Linguistics, 33(2):161?199.
James Pustejovsky. 2013. Inference patterns with in-
tensional adjectives. In Proceedings of the IWCS
Workshop on Interoperable Semantic Annotation,
Potsdam,Germany, March. Association for Compu-
tational Linguistics.
Silke Scheible, Sabine Schulte im Walde, and Sylvia
Springorum. 2013. Uncovering distributional dif-
ferences between synonyms and antonyms in a word
space model. In Proceedings of the International
Joint Conference on Natural Language Processing,
pages 489?497, Nagoya, Japan.
Heinrich Sch?utze. 1998. Automatic word sense dis-
crimination. Computational Linguistics, 24(1):97?
123.
Richard Socher, Brody Huval, Christopher D Manning,
and Andrew Y Ng. 2012. Semantic compositional-
ity through recursive matrix-vector spaces. In Pro-
ceedings of the 2012 Joint Conference on Empiri-
cal Methods in Natural Language Processing and
Computational Natural Language Learning, pages
1201?1211. Association for Computational Linguis-
tics.
Stefan Thater, Georgiana Dinu, and Manfred Pinkal.
2009. Ranking paraphrases in context. In Proceed-
ings of the 2009 Workshop on Applied Textual Infer-
ence, pages 44?47, Suntec, Singapore, August. As-
sociation for Computational Linguistics.
19
Stefan Thater, Hagen F?urstenau, and Manfred Pinkal.
2010. Contextualizing semantic representations us-
ing syntactically enriched vector models. In Pro-
ceedings of the 48th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 948?957,
Uppsala, Sweden, July. Association for Computa-
tional Linguistics.
Stefan Thater, Hagen Frstenau, and Manfred Pinkal.
2011. Word meaning in context: A simple and ef-
fective vector model. In Proceedings of 5th Interna-
tional Joint Conference on Natural Language Pro-
cessing (IJCNLP 2011).
P. D. Turney and P. Pantel. 2010. From frequency to
meaning: Vector space models of semantics. Jour-
nal of Artificial Intelligence Research, 37:141?188.
Peter D. Turney. 2012. Domain and function: A dual-
space model of semantic relations and compositions.
Journal of Artificial Intelligence Research, 44.
Tim Van de Cruys, Thierry Poibeau, and Anna Korho-
nen. 2011. Latent vector weighting for word mean-
ing in context. In Proceedings of the 2011 Con-
ference on Empirical Methods in Natural Language
Processing, pages 1012?1022, Edinburgh, Scotland,
UK., July. Association for Computational Linguis-
tics.
Julie Weeds and David Weir. 2003. A general frame-
work for distributional similarity. In Proceedings of
the 2003 Conference on Empirical Methods in Nat-
ural Language Processing, pages 81?88, Sapporo,
Japan.
20
