Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 819?826, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Automatic Question Generation for Vocabulary Assessment 
 
Jonathan C. Brown Gwen A. Frishkoff Maxine Eskenazi 
Language Technologies Institute 
Carnegie Mellon University 
Pittsburgh, PA 15213  USA 
Learning Research &  
Development Center 
University of Pittsburgh 
Pittsburgh, PA 15260  USA 
Language Technologies Institute 
Carnegie Mellon University 
Pittsburgh, PA 15213  USA 
jonbrown@cs.cmu.edu gwenf@pitt.edu max@cs.cmu.edu 
 
 
 
 
Abstract 
In the REAP system, users are automati-
cally provided with texts to read targeted to 
their individual reading levels. To find ap-
propriate texts, the user?s vocabulary 
knowledge must be assessed. We describe 
an approach to automatically generating 
questions for vocabulary assessment. Tra-
ditionally, these assessments have been 
hand-written. Using data from WordNet, 
we generate 6 types of vocabulary ques-
tions. They can have several forms, includ-
ing wordbank and multiple-choice. We 
present experimental results that suggest 
that these automatically-generated ques-
tions give a measure of vocabulary skill 
that correlates well with subject perform-
ance on independently developed human-
written questions. In addition, strong corre-
lations with standardized vocabulary tests 
point to the validity of our approach to 
automatic assessment of word knowledge.  
1 Introduction 
The REAP system automatically provides users 
with individualized authentic texts to read. These 
texts, usually retrieved from the Web, are chosen 
to satisfy several criteria. First, they are selected to 
match the reading level of the student (Collins-
Thompson and Callan, 2004). They must also have 
vocabulary terms known to the student. To meet 
this goal, it is necessary to construct an accurate 
model of the student?s vocabulary knowledge 
(Brown and Eskenazi, 2004). Using this model, the 
system can locate documents that include a given 
percentage (e.g., 95%) of words that are known to 
the student. The remaining percentage (e.g. 5%) 
consists of new words that the student needs to 
learn. This percentage is controlled so that there is 
not so much stretch in the document that the stu-
dent cannot focus their attention on understanding 
the new words and the meaning of the text. After 
reading the text, the student?s understanding of 
new words is assessed. The student?s responses are 
used to update the student model, to support re-
trieval of furture documents that take into account 
the changes in student word knowledge.  
In this paper, we describe our work on automatic 
generation of vocabulary assessment questions. We 
also report results from a study that was designed 
to assess the validity of the generated questions. In 
addition to the importance of these assessments in 
the REAP system, tests of word knowledge are 
central to research on reading and language and are 
of practical importance for student placement and 
in enabling teachers to track improvements in word 
knowledge throughout the school year. Because 
tests such as these are traditionally hand-written, 
development is time-consuming and often relies on 
methods that are informal and subjective. The re-
search described here addresses these issues 
through development of automated, explicit meth-
ods for generation of vocabulary tests. In addition, 
these tools are designed to capture the graded and 
complex nature of word knowledge, allowing for 
more fine-grained assessment of word learning.  
2 Measuring Vocabulary Knowledge 
Word knowledge is not all-or-none. Rather, there 
are different aspects, such as knowledge of the 
spoken form, the written form, grammatical behav-
819
ior, collocation behavior, word frequency, stylistic 
register constraints, conceptual meaning, and the 
associations a word has with other related words 
(Nation, 1990). In this paper, we focus on knowl-
edge of conceptual word meaning. Because word 
meaning itself is complex, our focus is not simply 
on all-or-none estimates of vocabulary knowledge, 
but also on graded and incomplete knowledge of 
meanings that readers possess for different words 
and at different stages of acquisition.  
Several models have been proposed to account 
for these multiple levels of word knowledge. For 
example, Dale posited four stages of knowledge of 
word meaning (Dale and O?Rourke, 1965). In 
stage 1, the subject has never seen the word. In 
stage 2, she has seen the word but is unable to ver-
balize its meaning. In stage 3, the subject recog-
nizes the word in a given context and has partial 
word knowledge. In stage 4, the subject has full 
word knowledge, and can explain the word mean-
ing so that its usage is clear in multiple contexts.  
Stahl (1986) proposed a similar model of word 
knowledge, the levels of which overlap with Dale?s 
last two stages. According to this model, the first 
level is characterized by association processing, or 
the passive association of the new word meaning 
with other, familiar concepts. The second level, 
comprehension processing, involves active com-
prehension of the word in a particular context. The 
third level, generation processing, requires usage 
of a word in a novel context reflecting a deep (and 
multidimensional) understanding of its meaning.  
Taking Stahl?s framework as a working model, 
we constructed multiple types of vocabulary ques-
tions designed to assess different ?stages? or ?lev-
els? of word knowledge. 
3 Question Generation 
In this section, we describe the process used to 
generate vocabulary questions. After introducing 
the WordNet resource we discuss the six question 
types and the forms in which they appear. The use 
of distractors is covered in section 3.3. 
3.1 WordNet 
WordNet is a lexical resource in which English 
nouns, verbs, adjectives, and adverbs are grouped 
into synonym sets. A word may appear in a num-
ber of these synonym sets, or synsets, each corre-
sponding to a single lexical concept and a single 
sense of the word (Fellbaum ed., 1998). The word 
?bat? has ten distinct senses and thus appears in ten 
synsets in WordNet. Five of these senses corre-
spond to noun senses, and the other five corre-
spond to verb senses. The synset for the verb sense 
of the word which refers to batting one?s eyelashes 
contains the words ?bat? and ?flutter?, while the 
synset for the noun sense of the word which refers 
to the flying mammal contains the words ?bat? and 
?chiropteran?. Each sense or synset is accompa-
nied by a definition and, often, example sentences 
or phrases. A synset can also be linked to other 
synsets with various relations, including synonym, 
antonym, hypernym, hyponym, and other syntactic 
and semantic relations (Fellbaum ed., 1998). For a 
particular word sense, we programmatically access 
WordNet to find definitions, example phrases, etc. 
3.2 Question Types 
Given Stahl?s three levels of word mastery and the 
information available in WordNet, we generated 6 
types of questions: definition, synonym, antonym, 
hypernym, hyponym, and cloze questions.  
In order to retrieve data from WordNet, we must 
choose the correct sense of the word. The system 
can work with input of varying specificity. The 
most specific case is when we have all the data: the 
word itself and a number indicating the sense of 
the word with respect to WordNet?s synsets. When 
the target words are known beforehand and the 
word list is short enough, the intended sense can be 
hand-annotated. More often, however, the input is 
comprised of just the target word and its part of 
speech (POS). It is much easier to annotate POS 
than it is to annotate the sense. In addition, POS 
tagging can be done automatically in many cases. 
In the REAP system, where the user has just read a 
specific text, the words of the document were al-
ready automatically POS annotated. When there is 
only one sense of the word per part of speech, we 
can simply select the correct sense of the word in 
WordNet. Otherwise, we select the most frequently 
used sense of the word with the correct POS, using 
WordNet?s frequency data. If we have only the 
word, we select the most frequent sense, ignoring 
part of speech. Future work will use word sense 
disambiguation techniques to automatically deter-
mine the correct word sense given a document that 
includes the target word, as in REAP (Brown and 
Eskenazi, 2004). 
820
Once the system has determined the word sense, 
it can retrieve data from WordNet for each of the 6 
question types. The definition question requires a 
definition of the word, available in WordNet?s 
gloss for the chosen sense. The system chooses the 
first definition which does not include the target 
word. This question should provide evidence for 
the first of Stahl?s three levels, association process-
ing, although this was not explicitly evaluated. 
The synonym question has the testee match the 
target word to a synonym. The system can extract 
this synonym from WordNet using two methods. 
One method is to select words that belong to the 
same synset as the target word and are thus syno-
nyms. In addition, the synonym relation in Word-
Net may connect this synset to another synset, and 
all the words in the latter are acceptable synonyms. 
The system prefers words in the synset to those in 
synonym synsets. It also restricts synonyms to sin-
gle words and to words which are not morphologi-
cal variants of the target word. When more than 
one word satisfies all criteria, the most frequently 
used synonym is chosen, since this should make 
the question easier. This question could be consid-
ered either association processing or comprehen-
sion processing. If the testee has seen this synonym 
(e.g. as a hint), this question type would require 
association processing as a word is simply being 
associated with another already-presented word. 
Otherwise, this may require comprehension proc-
essing ? understanding beyond memorization. 
The antonym question requires matching a word 
with an antonymous word. WordNet provides two 
kinds of relations that can be used to procure anto-
nyms: direct and indirect antonyms. Direct anto-
nyms are antonyms of the target word, whereas 
indirect antonyms are direct antonyms of a syno-
nym of the target. The words ?fast? and ?slow? are 
direct antonyms of one another. The word ?quick? 
does not have a direct antonym, but it does have an 
indirect antonym, ?slow?, via ?fast?, its synonym. 
When more than one antonym is available, the 
most frequently used is chosen. Unless the testee 
has already seen the antonym, this type of question 
is normally considered to provide evidence for 
Stahl?s second level, comprehension processing. 
The hypernym and hyponym questions are simi-
lar in structure. Hypernym is the generic term used 
to describe a whole class of specific instances. The 
word ?organism? is a hypernym of ?person?. Hy-
ponyms are members of a class. The words 
?adult?, ?expert? and ?worker? are hyponyms of 
?person?. For the questions the testee matches the 
target word to either a hypernym or hyponym. For 
more than one possibility, the most frequently used 
term is chosen. Unless the testee has previously 
seen the hypernym or hyponym, these questions 
are normally regarded as providing evidence for 
Stahl?s second level. 
Cloze is the final question type. It requires the 
use of the target word in a specific context, either a 
complete sentence or a phrase. The example sen-
tence or phrase is retrieved from the gloss for a 
specific word sense in WordNet. There is often 
more than one example phrase. The system prefers 
longer phrases, a feature designed to increase the 
probability of retrieving complete sentences. Pas-
sages using the target word are preferred, although 
examples for any of the words in the synset are 
appropriate. The present word is replaced by a 
blank in the cloze question phrase. Some consider 
a cloze question to be more difficult than any of 
the other question types, but it is still expected to 
provide evidence for Stahl?s second level. 
Although our question types provide evidence 
for the highest level of schemes such as Dale?s 
four stages, they do not provide evidence for 
Stahl?s highest level, generation processing, where 
the testee must, for instance, write a sentence using 
the word in a personalized context. We expect 
questions that provide evidence of this level to re-
quire free-form or near-free-form responses, which 
we do not yet alow. We expect the six question 
types to be of increasing difficulty, with definition 
or synonym being the easiest and cloze the hardest. 
3.3 Question Forms 
Each of the 6 types of questions can be generated 
in several forms, the primary ones being wordbank 
and multiple-choice. In wordbank, the testee sees a 
list of answer choices, followed by a set of ques-
tions or statements (see Figure 1). For the defini-
tion version, each of the items below the wordbank 
is a definition. The testee must select the word 
which best corresponds to the definition. For the 
synonym and antonym questions, the testee selects 
the word which is the most similar or the most op-
posite in meaning to the synonym or antonym. For 
the hypernym and hyponym question types, the 
testee is asked to complete phrases such as ?___ is 
a kind of person? (with target ?adult?) or ?person 
821
is a kind of ___? (with target ?organism?). In the 
cloze question, the testee fills in the blank with the 
appropriate word. There is traditionally one ques-
tion for each target word in the wordbank. These 
questions require no information beyond the target 
words and their definitions, synonyms, hypernyms, 
etc. 
 
Wordbank: 
verbose   infallible   obdurate   opaque 
 
Choose the word from the wordbank that best completes each 
phrase below: 
 
1. ___ windows of the jail 
2. the Catholic Church considers the Pope ___ 
3. ___ and ineffective instructional methods 
4. the child's misery would move even the most ___ heart 
 
Fig. 1.  Example Wordbank Question 
 
The second generated form is multiple-choice, 
with one question per target word. The testee sees 
the main question, the stem, followed by several 
answer choices, of which only one is correct (see 
Figure 2). Depending on the question type, the tar-
get word may appear in either the stem or the an-
swer choices. For the definition question type, the 
stem holds the definition of the target word and 
one of the answer choices is the target word. For 
the word ?verbose?, the stem would be ?using or 
containing too many words? and the choices ?an-
cillary?, ?churlish?, ?verbose?, and ?convivial?. 
The cloze question is of a similar form, with the 
stem containing the example sentence or phrase 
with a blank where the target word should be used. 
For ?verbose?, we have the stem ?___ and ineffec-
tive instructional methods? and choices ?verbose?, 
?incipient?, ?invidious?, and ?titular?. For the 
synonym, antonym, hypernym, and hyponym ques-
tions, the target word appears in the stem instead of 
the answer choices. The synonym question for the 
word ?verbose? would have the stem ?Select the 
word that is most similar in meaning to the word 
verbose? with choices ?inflammable?, ?piping?, 
matrilineal?, and ?long-winded?. The antonym 
question would have the stem ?Select the word that 
is most opposite in meaning to the word verbose? 
and the choices ?discernable?, ?concise?, ?unbro-
ken?, and ?soused?. Figure 2 shows a formatted 
example of an automatically generated multiple-
choice cloze question for the word ?obdurate?. 
 
Choose the word that best completes the phrase below: 
 
the child's misery would move even the most ___ heart 
 
A) torpid 
B) invidious 
C) stolid 
D) obdurate 
 
Fig. 2.  Example Multiple-Choice Cloze Question 
 
Two issues to consider when creating multiple-
choice format questions are the wording or appear-
ance of the questions and the criteria for selection 
of distractors. We followed the guidelines for good 
multiple-choice questions described by researchers 
such as Graesser and Wisher (2001). In accord 
with these guidelines, our questions had 4 choices, 
although the number of choices is a variable sup-
plied to the question generation software. We also 
considered the most appropriate wording for these 
questions, leading us to choose stems such as ?Se-
lect the word that is most similar in meaning to the 
word plausible? for the synonym question rather 
than ?Choose the word that means the same as the 
word plausible.? The latter would be problematic 
when the correct answer is a near-synonym rather 
than a word with precisely the same meaning.  
Concerning distractor choice, the question gen-
eration system chooses distractors of the same part 
of speech and similar frequency to the correct an-
swer, as recommended by Coniam (1997). For the 
synonym, antonym, hypernym, and hyponym ques-
tions, the correct answer is the highest frequency 
word of all the words chosen from WordNet that 
satisfy all the criteria. Thus, the distractors are of 
the same POS and similar frequency to the syno-
nym, antonym, or whatever word is the correct 
answer, as opposed to the target word. The system 
chooses distractors from Kilgarriff?s (1995) word 
frequency database, based on the British National 
Corpus (BNC) (Burnage, 1991). The system 
chooses 20 words from this database that are of the 
same POS and are equal or similar in frequency to 
the correct answer, and randomly chooses the dis-
tractors from these words. Since the distractors 
may be different for each run of the question gen-
eration software, slightly different versions of the 
same basic question may appear. The words of the 
BNC and the word frequency database have been 
POS tagged using the CLAWS tagger (Leech, 
1994). This tagger uses detailed POS tags, ena-
bling us to choose distractors that are, for instance, 
822
verbs in the past tense, when the correct answer is 
such as verb, instead of selecting verbs of un-
known tense. In the definition and cloze questions, 
the correct answer is the target word itself, so dis-
tractors are chosen based on this word. The system 
also restricts distractors to be in the list of target 
words so that the testee cannot simply choose the 
word that appears in the stems of other questions. 
An alternate multiple-choice question format is 
used when the testee has just read a document us-
ing the target word, as in the REAP system (Brown 
and Eskenazi, 2004). In this case, the system also 
attempts to finds words which may be semantically 
related to the correct answer, as in (Nagy, 1985). 
This is done by choosing distractors that satisfy the 
standard criteria and were present in the document. 
This should increase the chance that the distractors 
are semantically related and eliminate the chance 
that a testee will simply select as the correct an-
swer the word that appeared in the document they 
just read, without understanding the word meaning.  
4 Question Assessment 
The validity of the automatically generated vo-
cabulary questions was examined in reference to 
human-generated questions for 75 low-frequency 
English words. We compared student performance 
(accuracy and response time) on the computer and 
human-generated questions. We focused on the 
automatically generated multiple-choice questions, 
with distractors based on frequency and POS. We 
did not examine using more complicated strategies 
for picking distractors or assume there was an as-
sociated text. Four of the six computer-generated 
question types were assessed: the definition, syno-
nym, antonym, and cloze questions. Hypernym and 
hyponym questions were excluded, since we were 
unable to generate a large number of these ques-
tions for adjectives, which constitute a large por-
tion of the word list. Subject scores on the 
computer and human-generated assessments were  
compared with scores on standardized measures of 
reading and vocabulary skill, as described below. 
4.1 Question Coverage 
Potential experimental stimuli comprised 156 low-
frequency and rare English words that have been 
used in previous studies of vocabulary skill in na-
tive English-speaking adults. We first examined 
the percentage of words for which we could gener-
ate various question types. We were unable to gen-
erate any questions for 16 of these words, or ~9% 
of the list, since they were not in WordNet. Table 1 
shows the percentage of words for which each of 
the four question types was generated. All four 
questions were able to be generated for only 75 
(about half) of the words. Therefore, the experi-
mental word list included only these 75 items. 
Given the rarity of the words, we predicted that the 
percentage of words for which we could generate 
questions would be lower than average. However, 
we expected that the percentage of words for 
which we could generate synonym and antonym 
questions to be higher than average, due to the 
heavy focus on adjectives in this list. 
 
Question type Percentage of Questions 
Generated  
Definition Question 91% 
Synonym Question 80% 
Antonym Question 60% 
Cloze Question 60% 
Table 1. Question Coverage for the 156-Word List 
4.2 Experiment Design 
Behavioral measures of vocabulary knowledge 
were acquired for the 75 target words using the 
four computer-generated question types described 
above, as well as five human-generated question 
types. The human-generated questions were devel-
oped by a group of three learning researchers, 
without knowledge of the computer-generated 
question types. Researchers were asked merely to 
develop a set of question types that could be used 
to assess different levels, or different aspects, of 
word knowledge. Examples of each question type 
(including distractors) were hand-written for each 
of the 75 words. 
Two of the five human-generated assessments, 
the synonym and cloze questions, were similar in 
form to the corresponding computer-generated 
question types in that they had the same type of 
stem and answer. The other three human-generated 
questions included an inference task, a sentence 
completion task, and a question based on the Os-
good semantic differential (Osgood, 1970). In the 
inference task, participants were asked to select a 
context where the target word could be meaning-
fully applied. For example, the correct response to 
823
the question ?Which of the following is most likely 
to be lenitive?? was ?a glass of iced tea,? and dis-
tractors were ?a shot of tequila,? ?a bowl of rice,? 
and ?a cup of chowder.? In the sentence comple-
tion task, the participant was presented with a sen-
tence fragment containing the target word and was 
asked to choose the most probable completion. For 
example, the stem could be ?The music was so 
lenitive?,? with the correct answer ??it was 
tempting to lie back and go to sleep,? and with dis-
tractors such as ??it took some concentration to 
appreciate the complexity.? The fifth question type 
was based on the Osgood semantic differential, a 
factor-analytic model of word-level semantic di-
mensions (Osgood, 1970). Numerous studies using 
the Osgood paradigm have shown that variability 
in the semantic ?structure? of word meanings can 
largely be accounted for in terms of three dimen-
sions, valence (good?bad), potency (strong?weak), 
and activity (active?passive). In our version of the 
Osgood task, subjects were asked to classify a 
word such as ?lenitive? along one of these dimen-
sions (e.g., more good or more bad).  
In addition to the human-generated questions, 
we administered a battery of standardized tests, 
including the Nelson-Denny Reading Test, the Ra-
ven?s Matrices Test, and the Lexical Knowledge 
Battery. The Nelson-Denny Reading Test is a stan-
dardized test of vocabulary and reading compre-
hension (Brown, 1981). The Raven?s Matrices Test 
is a test of non-verbal reasoning (Raven, 1960). 
The Lexical Knowledge Battery has multiple sub-
sections that test orthographic and phonological 
skills (Perfetti and Hart, 2001).  
Twenty-one native-English speaking adults par-
ticipated in two experiment sessions. Session 1 
lasted for about one hour and included the battery 
of vocabulary and reading-related assessments de-
scribed above. Session 2 lasted between two and 
three hours and comprised 10 tasks, including the 
five human and four computer-generated ques-
tions. The experiment began with a confidence-
rating task, in which participants indicated with a 
key press how well they knew the meaning of each 
target word (on a 1?5 scale). This task was not 
speeded. For the remaining tasks, subjects were 
asked to respond ?as quickly as possible without 
making errors.? Test items for a given question 
type were answered together. The order of the 
tasks (question types) and the order of the 75 items 
within each task were randomized across subjects.  
4.3 Experiment Results 
We report on four aspects of this study: participant 
performance on questions, correlations between 
question types, correlations with confidence rat-
ings, and correlations with external assessments. 
Mean accuracy scores for each question type 
varied from .5286 to .6452. Performance on indi-
vidual words and across subjects (averaging across 
words) varied widely. The easiest question types 
(those with the highest average accuracy), were the 
computer-generated definition task and the human-
generated semantic differential task, both having 
mean accuracy scores of .6452. The hardest was 
the computer-generated cloze task, with a mean 
score of .5286. The accuracy on computer-
generated synonym and antonym questions fall 
between these two limits, with slightly greater ac-
curacy on the synonym type. This implies a gen-
eral ordering of difficulty from definition to cloze, 
as expected. The accuracies on the other human-
generated questions also fall into this range.  
We also computed correlations between the dif-
ferent question types. Mean accuracies were highly 
and statistically significantly correlated across the 
nine question types (r>.7, p<.01 for all correla-
tions). The correlation between participant accu-
racy on the computer-generated synonym and the 
human-generated synonym questions was particu-
larly high (r=.906), as was the correlation between 
the human and computer cloze questions (r= .860). 
The pattern of correlations for the response-time 
(RT) data was more complicated and is discussed 
elsewhere (Frishkoff et al In Prep). Importantly, 
RTs for the human versus computer versions of 
both the synonym and cloze questions were 
strongly correlated (r>.7, p<.01), just as for the 
accuracy results. The accuracy correlations imply 
that the computer-generated questions are giving a 
measure of vocabulary skill for specific words that 
correlates well with that of the human-generated 
questions.  
An item analysis (test item discrimination) was 
also performed. For each word, scores on a particu-
lar question type were compared with the compos-
ite test score for that word. This analysis revealed 
relatively low correlations (.12 < r < .25) between 
the individual question types and the test as a 
whole (without that question type). Since the ques-
tion types were designed to test different aspects of 
vocabulary knowledge, this result is encouraging.  
824
In addition, the average total-score correlations 
for the four computer-generated questions (r=.18) 
and for the five human-generated questions (r=.19) 
were not significantly different. This is positive, 
since it suggests that the human and computer-
generated vocabulary test are accounting for simi-
lar patterns of variance across the different ques-
tion types. 
The average correlation between accuracy on 
the question types and confidence ratings for a par-
ticular word was .265. This correlation was unex-
pectedly low. This may be because participants 
thought they knew these words, but were confused 
by their rarity, or because confidence simply does 
not correlate well with accuracy. Further work is 
needed to determine whether confidence ratings 
can be accurate predictors of vocabulary knowl-
edge. 
Finally, we examined correlations between par-
ticipant performance on the nine question types 
and the external assessments. The correlations be-
tween the accuracy on each of the nine question 
types and the Nelson-Denny vocabulary subtest 
were fairly high (.61 < r < .85, p=.01 for all com-
parisons). Thus, both the computer and human-
generated questions show good correspondence 
with an external assessment of vocabulary skill. 
Correlations between the accuracy on the question 
types and the Nelson-Denny reading comprehen-
sion test were mixed, showing a higher correlation 
with vocabulary than reading comprehension. Cor-
relations between the accuracy on the nine ques-
tion types and the Raven?s Matrices test of 
nonverbal reasoning were positive, but low and not 
statistically significant. This provides strong evi-
dence that the computer-generated vocabulary 
questions tap vocabulary knowledge specifically, 
rather than intelligence in general. 
5 Related Work 
Cloze tests are one area of related work. They were 
originally intended to measure text readability 
(Taylor, 1953) since native speakers should be able 
to reproduce certain removed words in a readable 
text. Other researchers have used it to assess read-
ing comprehension (Ruddell, 1964), with students 
filling in the blanks, given a high quality text. The 
main issue in automating the creation of cloze tests 
is determining which words to remove from the 
text. Coniam (1997) examined a several options for 
determining the words to remove and produced 
relatively good-quality cloze tests by removing 
words with the same POS or similar frequency.  
Wolfe (1976) automatically generated reading 
comprehension questions. This involved various 
techniques for rewriting sentences into questions, 
testing syntactic understanding of individual sen-
tences. Of the 50 questions Wolfe was able to gen-
erate for a single text, 34 were found to be 
satisfactory. More recently, Kunichika (2003) car-
ried out work in automatically generating reading 
comprehension questions that included both syn-
tactic and semantic questions, and was able to gen-
erate several different types of questions, including 
asking about the content of a sentence, using dic-
tionaries of synonyms and antonyms to generate 
questions such as ?Is Jane busy?? from sentences 
like ?Jane is free.?, and testing semantic under-
standing across sentence boundaries. Approx. 93% 
of the generated questions were found to be satis-
factory. 
Aist (2001) automatically generated factoids to 
assist students reading. The factoids gave a syno-
nym, an antonym, or a hypernym for the word, 
which were automatically extracted from Word-
Net. He also automated the creation of a single 
type of vocabulary question, with the target word 
in the stem and the correct answer a synonym, hy-
pernym, or sibling from WordNet. It is unclear 
what type of vocabulary knowledge this question 
would tap, given the different possible answers. 
6 Conclusions 
Extending our experiments to the question types 
that we have not yet assessed is an important next 
step. In addition, we want to assess questions indi-
vidually, evaluating their use of distractors. Fi-
nally, we need to assess questions generated on 
word lists with different characteristics. 
There are also a number of ongoing extensions 
to this project. One is the creation of new question 
types to test other aspects of word knowledge. An-
other is using other resources such as text collec-
tions to enable us to generate more questions per 
word, especially for the cloze questions. In addi-
tion, we are looking at ways to predict word 
knowledge using confidence ratings and morpho-
logical and semantic cohorts in situations where 
we cannot perform a standard assessment or cannot 
test all the vocabulary words we would like to. 
825
In this paper, we have described our work in 
automatically generating questions for vocabulary 
assessment. We have described the six types of 
computer-generated questions and the forms in 
which they appear. Finally, we have presented evi-
dence that the computer-generated questions give a 
measure of vocabulary skill for individual words 
that correlates well with human-written questions 
and standardized assessments of vocabulary skill. 
Acknowledgements 
The authors would like to thank Jamie Callan and 
Kevyn Collins-Thompson for their help in this re-
search. The authors would also like to thank Eve 
Landen, Erika Taylor, and Charles Perfetti for their 
assistance with experimental stimuli and data col-
lection. This project is supported U.S. Department 
of Education, Institute of Education Sciences, 
Award #R305G030123, and the APA/IES Postdoc-
toral Education Research Training fellowship 
awarded by the Department of Education, Institute 
of Education Sciences, Grant #R305U030004. Any 
opinions, findings and conclusions or recommen-
dations expressed in this material are those of the 
authors and do not necessarily reflect the views of 
the U.S. Department of Education.  
References  
Gregory Aist. 2001. Towards automatic glossarization: 
automatically constructing and administering vo-
cabulary assistance factoids and multiple-choice as-
sessment, International Journal of AI in Ed., 2001.  
James Brown, J. M. Bennett, and Gerald Hanna. 1981. 
The Nelson-Denny Reading Test. Chicago: The Riv-
erside Publishing Company. 
Jonathan Brown and Maxine Eskenazi. 2004. Retrieval 
of Authentic Documents for Reader-Specific Lexical 
Practice. In Proceedings of InSTIL/ICALL Sympo-
sium 2004. Venice, Italy, 2004. 
Gavin Burnage. 1991. Text Corpora and the British Na-
tional Corpus. Computers & Texts 2, Nov, 1991.  
Kevyn Collins-Thompson and Jamie Callan. 2004. A 
language modeling approach to predicting reading 
difficulty. In Proceedings of the HLT/NAACL 2004 
Conference. Boston, 2004.  
David Coniam. 1997. A preliminary inquiry into using 
corpus word frequency data in the automatic genera-
tion of English language cloze tests. CALICO Jour-
nal, Volume 14, No. 2. 
Edgar Dale and Joseph O?Rourke. 1986. Vocabulary 
building. Columbus, Ohio: Zaner-Bloser. 
Christiane Fellbaum, Ed. 1998. WordNet. An electronic 
lexical database. Ed. by Christiane Fellbaum, preface 
by George Miller. Cambridge, MA: MIT Press; 1998. 
Arthur C. Graesser, R. A. Wisher. 2001. Question Gen-
eration as a Learning Multiplier in Distributed 
Learning Environments. Army research inst for the 
behavioral and social sciences Alexandria VA. Re-
port number A654993, 2001. 
Adam Kilgarriff. 1995. 
http://www.itri.brighton.ac.uk/~Adam.Kilgarriff/bnc-
readme.html 
H. Kunichika, T. Katayama, T. Hirashima, and A. Ta-
keuchi. 2003. Automated question generation meth-
ods for intelligent English learning systems and its 
evaluation. Proceedings of ICCE2004, Hong Kong. 
G. Leech, R. Garside, and M. Bryant. 1994. CLAWS4: 
The tagging of the British National Corpus. In Proc. 
of 15th International Conference on Computational 
Linguistics, Kyoto, Japan, 622-628, 1994. 
W.E. Nagy, P.A. Herman, and R.C. Anderson. 1985. 
Learning words from context. Reading Research 
Quarterly, 20, 233-253. 
Paul Nation. 1990. Teaching and learning vocabulary. 
Rowley, MA: Newbury House. 
Charles E. Osgood, P. H. Tannenbaum, and G. J. Suci. 
1957. The Measurement of Meaning. Urbana: Uni-
versity of Illinois Press. 
Charles A. Perfetti, and Lesley Hart. 2001. The lexical 
quality hypothesis. In L. Verhoeven, C. Elbro & P. 
Reitsma (Eds.), Precursors of Functional Literacy 
(Vol. 11, pp. 67?86). Amsterdam: John Benjamins. 
J.C. Raven. 1960. Progressive matrices, standard. San 
Antonio, TX: Psychological Corporation.  
R. B. Ruddell. 1964. A study of the cloze comprehen-
sion technique in relation to structurally controlled 
reading material. Improvement of Reading Through 
Classroom Practice, 9, 298-303. 
Steven A. Stahl. 1986. Three principals of effective vo-
cabulary instruction. Journal of Reading, 29. 
W.L. Taylor. 1953. Cloze procedure: a new tool for 
measuring readability. Journalism Quarterly, 30. 
John H. Wolfe. 1976. Automatic question generation 
from text - an aid to independent study. ACM 
SIGCUE Bulletin, 2(1), 104-112. 
826
Non-Native Users in the Let?s Go!! Spoken Dialogue System:
Dealing with Linguistic Mismatch
Antoine Raux and Maxine Eskenazi
Language Technologies Institute
Carnegie Mellon University
5000 Forbes Avenue, Pittsburgh, PA 15232, USA
{antoine+,max+}@cs.cmu.edu
Abstract
This paper describes the CMU Let?s Go!! bus
information system, an experimental system
designed to study the use of spoken dialogue
interfaces by non-native speakers. The differ-
ences in performance of the speech recogni-
tion and language understanding modules of
the system when confronted with native and
non-native spontaneous speech are analyzed.
Focus is placed on the linguistic mismatch be-
tween the user input and the system?s expecta-
tions, and on its implications in terms of lan-
guage modeling and parsing performance. The
effect of including non-native data when build-
ing the speech recognition and language under-
standing modules is discussed. In order to close
the gap between non-native and native input, a
method is proposed to automatically generate
confirmation prompts that are both close to the
user?s input and covered by the system?s lan-
guage model and grammar, in order to help the
user acquire idiomatic expressions appropriate
to the task.
1 Introduction
1.1 Spoken Dialogue Systems and Non-Native
Speakers
Spoken dialogue systems rely on models of human lan-
guage to understand users? spoken input. Such models
cover the acoustic and linguistic space of the common
language used by the system and the user. In current
systems, these models are learned from large corpora of
recorded and transcribed conversations matching the do-
main of the system. In most of the cases, these cor-
pora are gathered from native speakers of the language
because they are the main target of the system and be-
cause developers and researchers are often native speak-
ers themselves. However, when the common language
is not the users? native language, their utterances might
fall out of this ?standard? native model, seriously degrad-
ing the recognition accuracy and overall system perfor-
mance. As telephone-based information access systems
become more common and available to the general pub-
lic, this inability to deal with non-native speakers (or with
any ?non-standard? subgroup such as the elderly) is a
serious limitation since, at least for some applications,
(e.g. tourist information, legal/social advice) non-native
speakers represent a significant portion of the everyday
user population.
1.2 Previous Work on Non-Native Speech
Recognition
Over the past ten years, extensive work has been done
on non-native speech recognition. Early research aimed
at endowing Computer Assisted Language Learning soft-
ware with speech recognition capabilities (e.g. (Eske-
nazi and Hansma, 1998), (Witt and Young, 1997)). Usu-
ally such systems are targeted at one specific popula-
tion, that is, people who share the same native language
(L1). Thus, most research in non-native speech recog-
nition uses knowledge of the L1, as well as databases
of accented speech specially recorded from speakers of
the target population. Ideally, by training acoustic mod-
els on target non-native speech, one would capture its
specific characteristics just as training on native speech
does. However collecting amounts of non-native speech
that are large enough to fully train speaker-independent
models is a hard and often impractical task. Therefore, re-
searchers have resorted to using smaller amounts of non-
native speech to retrain or adapt models that were orig-
inally trained on large corpora of native speech. As for
native speech, such methods were mostly applied to read
speech, with some success (e.g. (Mayfield Tomokiyo and
Waibel, 2001)).
Unfortunately, we know from past research on na-
tive speech recognition that read speech models perform
poorly on conversational speech (Furui, 2001), which is
the style used when talking to spoken dialogue systems.
A few studies have built and used databases of non-native
conversational speech for evaluation (Byrne et al, 1998),
and training (Wang and Schultz, 2003).
In all those cases, the native language of the speaker is
known in advance. One exception is (Fischer et al, 2001)
who apply multilingual speech recognition methods to
non-native speech recognition. The authors train acoustic
models on a database comprising native speech from five
European languages (English, Spanish, French, German
and Italian) and use them to recognize non-native English
from speakers of 10 European countries. However, their
task is the recognition of read digit strings, quite different
from conversational speech.
Also, because of the difficulty researchers have to
record large amounts of spontaneous non-native speech,
no thorough study of the impact of the linguistic differ-
ences between native and non-native spontaneous speech
has been conducted to our knowledge. The two spon-
taneous non-native speech studies cited above, report
perplexity and out-of-vocabulary (OOV) word rate (for
(Wang and Schultz, 2003)) but do not provide any analy-
sis.
In this paper, while acknowledging the importance of
acoustic mismatch between native models and non-native
input, we focus on linguistic mismatch in the context of
a task-based spoken dialogue system. This includes dif-
ferences in word choices which influences the number of
OOV words, and syntax which affects the performance of
the speech recognizer?s language model and of the natu-
ral language understanding (NLU) grammar.
1.3 Non-Native Speakers as Language Learners
All the research on non-native speech recognition de-
scribed in the previous section sees non-native speakers
as a population whose acoustic characteristics need to
be modeled specifically but in a static way, just like one
would model the acoustics of male and female voices dif-
ferently. A different approach to the problem is to see
non-native speakers as engaged in the process of acquir-
ing the target language?s acoustic, phonetic and linguistic
properties. In this paradigm, adapting dialogue systems
to non-native speakers does not only mean being able to
recognize and understand their speech as it is, but also
to help them acquire the vocabulary, grammar, and pho-
netic knowledge necessary to fulfill the task the system
was designed for.
This idea follows decades of language teaching re-
search that, since the mid sixties, has emphasized the
value of learning language in realistic situations, in order
to perform specific tasks. Immersion is widely consid-
ered as the best way to learn to speak a language and mod-
ern approaches to foreign language teaching try to mimic
its characteristics. If the student cannot be present in the
country the language is spoken in, then the student should
be put into a series of situations imitating the linguistic
experience that he/she would have in the target country.
Thus, most current language teaching methods, following
the Communicative Approach (Littlewood, 1981) have
focused on creating exercises where the student is forced
to use language quickly in realistic situations and thus to
learn from the situation itself as well as from reactions to
the student?s actions.
From a different viewpoint, (Bortfeld and Brennan,
1997) showed in a psycholinguistic study that non-native
speakers engaged in conversation-based tasks with native
speakers do not only achieve the primary goal of the task
through collaborative effort but also acquire idiomatic ex-
pressions about the task from the interaction.
The research described in this paper, has the dual goal
of improving the accessibility of spoken dialogue systems
to non-native speakers and of studying the usability of a
computer for task-based language learning that simulates
immersion.
The next section gives an overview of the CMU Let?s
Go!! bus information system that we built and use in
our experiments. Section 3 describes and analyzes the re-
sults of experiments aimed at comparing the accuracy of
speech recognition and the quality of language modeling
on both native and non-native data. Section 4 describes
the use of automatically generated confirmation prompts
to help the user speak the language expected by the sys-
tem. Finally, section 5 draws conclusions and presents
future directions of research.
2 Overview of the System
2.1 The CMU Let?s Go!! Bus Information System
In order to study the use of spoken dialogue systems by
non-native speakers in a realistic setting, we built Let?s
Go!!, a spoken dialogue system that provides bus sched-
ule information for the Pittsburgh area(Raux et al, 2003).
As shown in Figure 1, the system is composed of five ba-
sic modules: the speech recognizer, the parser, the dia-
log manager, the language generator, and the speech syn-
thesizer. Speech recognition is performed by the Sphinx
II speech recognizer (Huang et al, 1992). The Phoenix
parser (Ward and Issar, 1994) is in charge of natural lan-
guage understanding. The dialogue manager is based
on the RavenClaw framework (Bohus and Rudnicky,
2003). Natural language generation is done by a simple
template-based generation module, and speech synthe-
sis by the Festival speech synthesis system (Black et al,
1998). The original system uses a high quality limited-
domain voice recorded especially for the project but for
some experiments, lower quality, more flexible voices
Figure 1: General architecture of the Let?s Go!! bus in-
formation system.
have been used. All modules communicate through the
Galaxy-II (Seneff et al, 1998) framework.
2.2 Definition of the Domain
The Port Authority of Allegheny County, which man-
ages the buses in Pittsburgh provided the full database of
bus routes and schedules. Overall, this database contains
more than 10,000 bus stops but we restricted our system
to 5 routes and 559 bus stops in areas where international
students are likely to travel since they are our main target
population at present.
In order to improve speech recognition accuracy, we
concatenated the words in the name of each bus stop
(e.g. ?Fifth and Grant?) and made them into a single en-
try in the recognizer?s lexicon. Because there are usu-
ally several variant names for each bus stop and since we
included other places such as landmarks and neighbor-
hoods, the total size of the lexicon is 9914 words.
2.3 Data Collection Experiments
To gather enough data to train and test acoustic and lan-
guage models, we had the system running, advertising
it to international students at our university, as well as
conducting several studies. In those studies, we gave sce-
narios to the participants in the form of a web page with
maps indicating the places of departure and destination,
as well as additional time and/or route preferences. There
was as little written English as possible in the descrip-
tion of the scenarios to prevent influencing the language
habits of the participants. Participants then called the sys-
tem over the phone to get the required information. One
experiment conducted in June 2003 netted 119 calls from
11 different non-native speakers (5 of them were from
India and 6 from Japan), as well as 25 calls from 4 na-
tive speakers of American English. Another experiment
in August 2003 allowed the collection of 47 calls from
6 non-native speakers of various linguistic backgrounds.
The rest of the non-native data comes from unsollicited
Native Non-Native
Word Error Rate 20.4 % 52.0 %
Table 1: Word Error Rate of the speech recognizer with a
native language model on native and non-native data.
individual callers labelled as non-native by a human an-
notator who transcribed their speech. The total size of the
spontaneous non-native corpus is 1757 utterances.
3 Recognition and Understanding of
Non-Native Speech
3.1 Recognition Accuracy
We used acoustic models trained on data consisting of
phone calls to the CMU Communicator system(Rudnicky
et al, 2000). The data was split into gender specific
sets and corresponding models were built. At recognition
time, the system runs the two sets of models in parallel
and for each utterance selects the result that has the high-
est recognition score, as computed by Sphinx. The lan-
guage model is a class-based trigram model built on 3074
utterances from past calls to the Let?s Go!! system, in
which place names, time expressions and bus route names
are each replaced by a generic class name to compensate
for the lack of training data.
In order to evaluate the performance of these models on
native and non-native speakers, we used 449 utterances
from non-native users (from the August experiment and
the unsollicited calls) and 452 from native users of the
system. The results of recognition on the two data sets
are given in Table 1. Even for native speakers, perfor-
mance was not very high with a word error rate of 20.4%.
Yet, this is acceptable given the small amount of training
data for the language model and the conversational na-
ture of the speech. However, performance degrades sig-
nificantly for non-native speakers, with a word error rate
of 52.0%. The two main potential reasons for this loss
are acoustic mismatch and linguistic mismatch. Acoustic
mismatch arises from the variations between the native
speech on which the acoustic models were trained and
non-native speech, which often include different accents
and pronunciations. On the other hand, linguistic mis-
match stems from variations or errors in syntax and word
choice, between the native corpus on which the language
model was trained and non-native speech.
3.2 Impact of Linguistic Mismatch on the
Performance of the Language Model
To analyze the effect of linguistic mismatch, we com-
pared the number of out-of-vocabulary words (OOV) and
the perplexity of the model on the transcription of the test
utterances. Table 2 shows the results. The percentage of
Native Non-Native Difference Significance
% OOV words 1.2 % 3.09 % 157.5 % p < 10?4
% utt. w/ OOV words 5.9 % 14.0 % 174.5 % p < 10?5
Perplexity 22.89 36.55 59.7 % ?
% words parsed 63.3 % 56.0 % 56.0 % p < 10?9
% utt. fully parsed 56.4 % 49.7 % 49.7 % p < 0.05
Table 2: The native language model and parsing grammar applied to native and non-native speech transcriptions. The
statistical significance of the difference between the native and non-native sets is computed using the chi-square test
for equality of distributions.
OOVs is 3.09% for non-native speakers, more than 2.5
times higher than it is for native speakers, which shows
the difference in word choices made by each population.
Such differences include words that are correctly used but
are not frequent in native speech. For example, when
referring to bus stops by street intersections, all native
speakers in our training set simply used ?A and B?, hence
the word ?intersection? was not in the language model.
On the other hand, many non-native speakers used the full
expression ?the intersection of A and B?. Note that the
differences inside the place name itself (e.g. ?A and B? vs
?A at B?) are abstracted away by the class-based model,
since all variants are replaced by the same class name
(words like ?intersection? and ?corner? were kept out of
the class to reduce the number of elements in the ?place?
class). In other cases non-native speakers used inappro-
priate words, such as ?bus timing? for ?bus schedule?,
which were not in the language model. Ultimately, OOVs
affect 14.0% of the utterances as opposed to 5.9% for na-
tive utterances, which is significant, since an utterance
containing an OOV is more likely to contain recognition
errors even on its in-vocabulary words, since the OOV
prevents the language model from accurately matching
the utterance. Differences between the native and non-
native set in both OOV rate and the ratio of utterances
containing OOVs were statistically significant.
We computed the perplexity of the model on the utter-
ances that did not contain any OOV. The perplexity of the
model on this subset of the non-native test set is 36.55,
59.7% higher than that on the native set. This reflects
differences in syntax and selected constructions. For ex-
ample, although native speakers almost always used the
same expression to request a bus departure time (?When
does the bus leave ...??), non-natives used a wider variety
of sentences (e.g. ?Which time I have to leave??, ?What
the next bus I have to take??). Both the difference be-
tween native and non-native and the larger variability of
non-native language account for the larger perplexity of
the model over the non-native set. This results seems to
disagree with what (Wang and Schultz, 2003) found in
their study, where the perplexity was larger on the native
set. Unfortunately, they do not describe the data used to
train the language model so it is hard to draw any conclu-
sions. But one main difference is that their experiment
focused only on German speakers of English, whereas
we collected data from a much more diverse population.
3.3 Impact of the Linguistic Mismatch on Language
Understanding
The Phoenix parser used in the natural language under-
standing module of the system is a robust, context-free
grammar-based parser. Grammar rules, including op-
tional words, are compiled into a grammar network that
is used to parse user input. When no complete parse
is found, which is often the case with spoken language,
Phoenix looks for partial parses and returns the parse for-
est that it is most confident in. Confidence is based on
internal measures such as the number of words covered
by the parses and the number of parse trees in the parse
forest (for an equal number of covered words, a smaller
number of parse trees is preferred).
The grammar rules were hand written by the devel-
opers of the system. Initially, since no data was avail-
able, choices were made based on their intuition and on
a small scale Wizard-of-Oz experiment. Then, after the
first version of the system was made available, the gram-
mar was extended according to actual calls to the system.
The grammar has thus undergone continuous change, as
is often the case in spoken dialogue systems.
The grammar used in this experiment (the ?native?
grammar) was designed based for native speech without
adaptation to non-native data. It provides full parses of
sentences like ?When is the next bus going to the air-
port??, but also, due to the robustness of the parser, partial
parses to ungrammatical sentences like ?What time bus
leave airport??. Once compiled, the grammar network
consisted of 1537 states and 3076 arcs. The two bot-
tom rows of Table 2 show the performance of the parser
on human-transcribed native and non-native utterances.
Both the number of words that could be parsed and the
number of sentences for which a full parse was obtained
are larger for native speakers (resp. 63.3% and 56.4%)
than non-native (56% and 49.7%), although the relative
differences are not as large as those observed for the lan-
Figure 2: Comparison of the relative gain obtained by
using a language model and grammar that includes some
non-native data over the original purely native model, on
transcribed native and non-native speech.
guage model. This can be attributed to the original dif-
ficulty of the task since even native speech contains a
lot of disfluencies that make it difficult to parse. As a
consequence, robust parsers such as Phoenix, which are
designed to be flexible enough to handle native disfluen-
cies, can deal with some of the specificities of non-native
speech. Yet, the chi-square test shows that the difference
between the native and non-native set is very significant
for the ratio of words parsed and mildly so for the ratio
of fully parsed sentences. The weak significance of the
latter can be partly explained by the small number of ut-
terances in the corpora.
3.4 Effect of Additional Non-Native Data on
Language Modeling and Parsing
In order to study the improvement of performance pro-
vided by mixing native and non-native data in the lan-
guage model, we built a second language model (the
?mixed? model), using the 3074 sentences of the native
model to which were added 1308 sentences collected
from non-native calls to the system not included in the
test set. Using this model, we were able to reduce the
OOV rate by 56.6% and perplexity by 23.6% for our non-
native test set. While the additional data also improved
the performance of the model on native utterances, the
improvement was relatively smaller than for non-native
speakers (12.1%). As can be seen by comparing Tables
2 and 3, this observation is also true of OOV rate (56.6%
improvement for non-native vs 50.0% for native) and the
proportion of sentences with OOVs (43.1% vs 55.7%).
Figure 2 shows the relative improvement due to the mixed
LM over the native LM on the native and non-native set.
We also evaluated the impact of additional non-native
data on natural language understanding. In this case,
since we wrote the grammar manually and incrementally
over time, it is not possible to directly ?add the non-
native data? to the grammar. Instead, we compared the
June 2003 version of the grammar, which is mostly based
on native speech, to its September 2003 version, which
contains modifications based on the non-native data col-
lected during the summer. This part is therefore an eval-
uation of the impact of the human grammar design done
by the authors based on additional non-native data. At
that point, the compiled grammar had grown to contain
1719 states and 3424 arcs which represents an increase
of respectively 11.8% and 11.3% over the ?native? gram-
mar. Modifications include the addition of new words
(e.g. ?reach? as a synonym of ?arrive?), new constructs
(e.g. ?What is the next bus??) and the relaxation of some
syntactic constraints to accept ungrammatical sentences
(e.g. ?I want to arrive the airport at five? instead of ?I
want to arrive at the airport at five?). Using this new
grammar, the proportion of words parsed and sentences
fully parsed improved by respectively 10.4% and 11.3%
for the native set and by 17.3% and 11.7% for the non-
native set. We believe that, as for the language model, the
reduction in the number of OOVs is the main explana-
tion behind the better improvement in word coverage ob-
served for the non-native set compared to the native set.
The reduction of the difference between the native and
non-native sets is also reflected in the weaker significance
levels for all ratios except that of fully parsed utterances,
in 3, larger p-values meaning that there is a larger proba-
bility that the differences between the ratios were due to
spurious differences between the corpora rather than to
their (non-)nativeness.
This confirms that even for populations with a wide
variety of linguistic backgrounds, adding non-native data
does reduce the linguistic mismatch between the model
and new, unseen, non-native speech. Another explana-
tion is that, on a narrow domain such as bus schedule
information, the linguistic variance of non-native speech
is much larger than that of native speech. Therefore,
less data is required to accurately model native speech
than non-native speech. It also appears from these results
that, in the context of task-based spoken dialogue sys-
tems, higher-level modules, such as the natural language
understanding module, are less sensitive to explicit mod-
eling of non-nativeness. This can be explained by the fact
that such modules were designed to be flexible in order to
compensate for speech recognition errors. This flexibility
benefits non-native speakers as well, regardless of addi-
tional recognition errors.
3.5 Effect of Additional Non-Native Data on Speech
Recognition
Unfortunately, the reduction of linguistic mismatch was
not observed on recognition results. While using the new
language model improved word error rate on both native
Native Non-Native Difference Significance
% OOV words 0.6 % 1.34 % 123.3 % p < 0.05
% utt. w/ OOV words 2.9 % 6.2 % 113.8 % p < 0.01
Perplexity 20.12 27.92 38.8 % ?
% words parsed 69.9 % 65.7 % 65.7 % p < 10?3
% utt. fully parsed 62.8 % 55.5 % 55.5 % p < 0.05
Table 3: The mixed language model and parsing grammar applied to native and non-native speech transcriptions.
Significance is computed using the chi-square test, except for perplexity where the relative difference is reported.
Figure 3: Word Error Rate on Native and Non-Native
Data using a Native and a Mixed Language Model
and non-native utterances (resp. to 17.8% and 47.8%,
see Figure 3 ), the impact was relatively larger for native
speech. This is an indication that acoustics play a promi-
nent role in the loss of accuracy of speech recognition on
non-native speech. Acoustic differences between native
and non-native speakers are likely to be larger than the
linguistic ones, since, particularly on such a limited and
common domain, it is easier for non-native speakers to
master syntax and word choice than to improve their ac-
cent and pronunciation habits. Differences among non-
native speakers of different origins are also very large
in the acoustic domain, making it hard to create a single
acoustic model matching all non-native speakers. Finally,
the fact that additional non-native data improves perfor-
mance on native speech is a sign that, generally speak-
ing, the lack of training data for the language model is a
limiting factor for recognition accuracy. Indeed, if there
was enough data to model native speech, additional non-
native data should increase the variance and therefore the
perplexity on native speech.
4 Adaptive Lexical Entrainment as a
Solution to Linguistic Mismatch
4.1 Gearing the User To the System?s Language
The previous section described the issue of recogniz-
ing and understanding non-native speech and solutions to
adapt traditional systems to non-native speakers. Another
approach is to help non-native users adapt to the system
by learning appropriate words and expressions. Lexical
entrainment is the phenomenon by which, in a conversa-
tion, speakers negotiate a common ground of expressions
to refer to objects or topics. Developers of spoken di-
alogue systems frequently take advantage of lexical en-
trainment to help users speak utterances that are within
the language model of the system. This is done by care-
fully designing the system prompts to contain only words
that are recognized by the recognition and understanding
modules (Gustafson et al, 1997). However, in the case
of non-native users, there is no guarantee that users actu-
ally know the words the system wants them to use. Also,
even if they do, some non-native speakers might prefer
to use other words, which they pronounce better or that
they better know how to use. For those reasons, we be-
lieve that to be optimal, the system must try to match the
user?s choice of words in its own prompts. This idea is
motivated by the observations of (Bortfeld and Brennan,
1997), who showed that this type of adaptation occurs
in human-human conversations between native and non-
native speakers.
The role of the system?s ?native? prompts is to take
the users through the shortest path from their current lin-
guistic state to the system?s expectations. In fact, this is
not only true for non-native speakers and lexical entrain-
ment is often described as a negotiation process between
the speakers (Clark and Wilkes-Gibbs, 1986). However,
while it is possible for limited-domain system design-
ers to establish a set of words and constructions that are
widely used among native speakers, the variable nature
of the expressions mastered by non-native speakers make
adaptation a desirable feature of the system.
4.2 Automatic Generation of Corrective Prompts
In this study, not all prompts were modified to match the
user?s choice of words. Instead, the focus was placed on
confirmation prompts that both ensure proper understand-
ing between the user and the system and lexically entrain
the user towards the system?s expected input. Two ques-
tions arise: how to generate the prompts and when to
trigger them. Our approach has been to design a list of
target prompts that fit the system?s language model and
grammar and find the closest target prompt to each user
input. The distance between a user utterance as recog-
nized by Sphinx and each of the target utterances is com-
puted by the same dynamic programming algorithm that
is traditionally used to compute word error rate in speech
recognition evaluation. It determines the number of word
insertions, deletions and substitutions that lead from the
target prompt to the user?s utterance. The target prompt
that is closest, i.e. that requires the fewest operations to
match the input, is selected. In addition, words that rep-
resent important concepts such as places, times or bus
route numbers, are given additional weight. This follows
the assumption that a target sentence is not appropriate
if it has a missing or an extra concept compared to the
utterance. We also used this heuristic to answer the sec-
ond question: when to trigger the confirmation prompts.
The system asks for a confirmation whenever a target sen-
tence is found that contains the same concepts as the user
input and differs from it by at least one word. In this case
a prompt like ?Did you mean ...? followed by the tar-
get sentence is generated. Finally, the dynamic program-
ming algorithm used to align the utterances also locates
the words that actually differ between the input and the
target. This information is sent to the speech synthesizer,
which puts particular emphasis on the words that differ.
To provide natural emphasis, the intonation of all sen-
tences is generated by the method described in (Raux and
Black, 2003) that concatenates portions of natural into-
national contours from recorded utterances into a contour
appropriate for each prompt. Since the domain-limited
voice recorded for the project does not allow us to either
generate non-recorded prompts or to modify the contour
of the utterances, we used a different, generic voice for
this version of the system.
4.3 Application and Example
The method described in the previous paragraph was im-
plemented in the system and tested in a small pilot study.
We manually wrote 35 different target prompts describing
departure and destination places, times and route num-
bers, based on our knowledge of the system?s language
model and grammar. An example of a confirmation di-
alogue obtained from one of these prompts is given in
Figure 4. In the first user utterance, the preposition ?to? is
missing, either because it was not pronounced by the user
or because it was not recognized by the speech recog-
nition module. As a consequence, the utterance cannot
be fully parsed by the language understanding module.
In parallel, the confirmation module computes the dis-
tance between the user?s input and each of the 35 target
prompts, and identifies the closest one as ?I want to go to
the airport?. At the same time it finds that the user?s utter-
ance is obtained from the target by deleting the word ?to?
and therefore stresses it in the confirmation prompt. Once
S: What can I do for you?
U: I want to go the airport.
S: Sorry, I didn?t get that.
Did you mean:
I want to go TO the airport?
U: Yes
S: To the airport.
Where are you leaving from?
U: ...
Figure 4: Example of an adaptive confirmation dialogue.
The capital ?TO? indicate that the word was emphasized
by the system.
the user answers ?yes? to the confirmation prompt, the
target prompt is sent to the parser as if it had been uttered
by the user and the state of the dialogue is updated ac-
cordingly. If the user answers ?no?, the prompt is simply
discarded. We found that this method works well when
speech recognition is only slightly degraded and/or when
the recognition errors mostly concern grammar and func-
tion words. In such cases, this approach is often able to
repair utterances that would not be parsed correctly other-
wise. However, when too many recognition errors occur,
or when they affect the values of the concepts (i.e. the
system recognizes one place name instead of another),
the users receive too many confirmation prompts to which
they must respond negatively. Combined with the diffi-
culty that non-native speakers have in understanding un-
expected synthesized utterances, this results in cognitive
overload on the user. Yet, this method provides an easy
way (since the designer only has to provide the list of tar-
get prompts) to generate adaptive confirmation prompts
that are likely to help lexical entrainment.
5 Conclusion and Future Directions
In this paper, we described the Let?s Go!! bus information
system, a dialogue system targetted at non-native speak-
ers of English. In order to investigate ways to improve the
communication between non-native users and the system,
we recorded calls from both native and non-native speak-
ers and analyzed their linguistic properties. We found
that besides the problem of acoustic mismatch that results
from the differences in accent and pronunciation habits,
linguistic mismatch is also significant and degrades the
performance of the language model and the natural lan-
guage understanding module. We are exploring two solu-
tions to reduce the linguistic gap between native and non-
native users. First we studied the impact of taking into
account non-native data to model the user?s language and
second we designed a mechanism to generate confirma-
tion prompts that both match the user?s input and a set of
predefined target utterances, so as to help the user acquire
idiomatic expressions related to the task.
Real-world systems like Let?s Go!! are in constant evo-
lution because the data that is collected from users call-
ing the system is used to refine the acoustic and linguis-
tic models of the system. In the near future, our priority
is to collect more data to improve the acoustic models
of the system and address the specific issues related to
a general non-native population, which does not share a
common native language. We will also work on integrat-
ing the confirmation prompt generation method proposed
in this work with state-of-the-art confidence annotation
methods.
6 Acknowledgments
The authors would like to thank Alan W Black, Dan Bo-
hus and Brian Langner for their help with this research.
This material is based upon work supported by the U.S.
National Science Foundation under Grant No. 0208835,
?LET?S GO: improved speech interfaces for the general
public?. Any opinions, findings, and conclusions or rec-
ommendations expressed in this material are those of the
authors and do not necessarily reflect the views of the Na-
tional Science Foundation.
References
A. Black, P. Taylor, and R. Caley. 1998. The Festival
speech synthesis system. http://festvox.org/festival.
D. Bohus and A. Rudnicky. 2003. Ravenclaw: Dia-
log management using hierarchical task decomposition
and an expectation agenda. In Proc. Eurospeech 2003,
pages 597?600, Geneva, Switzerland.
H. Bortfeld and S. Brennan. 1997. Use and acquisition
of idiomatic expressions in referring by native and non-
native speakers. Discourse Processes, 23:119?147.
W. Byrne, E. Knodt, S. Khudanpur, and J. Bernstein.
1998. Is automatic speech recognition ready for non-
native speech? A data collection effort and initial ex-
periments in modeling conversational hispanic english.
In Proc. ESCA Workshop on Speech Technology in
Language Learning, pages 37?40, Marholmen, Swe-
den.
H. Clark and D. Wilkes-Gibbs. 1986. Referring as a
collaborative process. Cognition, 22:1?39.
M. Eskenazi and S. Hansma. 1998. The Fluency pronun-
ciation trainer. In Proc. ESCA Workshop on Speech
Technology in Language Learning, pages 77?80.
V. Fischer, E. Janke, S. Kunzmann, and T. Ross. 2001.
Multilingual acoustic models for the recognition of
non-native speech. In Proc. ASRU ?01, Madonna di
Campiglio, Italy.
S. Furui. 2001. From read speech recognition to spon-
taneous speech understanding. In Proc. 6th Natural
Language Processing Pacific Rim Symposium, pages
19?25, Tokyo, Japan.
J Gustafson, A. Larsson, R. Carlson, and K. Hellman.
1997. How do system questions influence lexical
choices in user answers? In Proc. Eurospeech ?97,
pages 2275?2278, Rhodes, Greece.
X. Huang, F. Alleva, H.-W. Hon, K.-F. Hwang, M.-Y.
Lee, and R. Rosenfeld. 1992. The SPHINX-II speech
recognition system: an overview. Computer Speech
and Language, 7(2):137?148.
W. Littlewood. 1981. Communicative Language Teach-
ing. Cambridge University Press.
L. Mayfield Tomokiyo and A. Waibel. 2001. Adapta-
tion methods for non-native speech. In Proc. Multilin-
guality in Spoken Language Processing, Aalborg, Den-
mark.
A. Raux and A. Black. 2003. A unit selection ap-
proach to f0 modeling and its application to empha-
sis. In Proc. IEEE Automatic Speech Recognition and
Understanding Workshop 2003, pages 700?705, Saint
Thomas, US Virgin Islands.
A. Raux, B. Langner, A. Black, and M. Eskenazi. 2003.
Lets go: Improving spoken dialog systems for the el-
derly and non-natives. In Proc. Eurospeech 2003,
pages 753?756, Geneva, Switzerland.
A. Rudnicky, C. Bennett, A. Black, A. Chotimongkol,
K. Lenzo, A. Oh, and R. Singh. 2000. Task and do-
main specific modelling in the carnegie mellon com-
municator system. In Proc. ICSLP 2000, Beijing,
China.
S. Seneff, E. Hurley, R. Lau, C. Pao, P. Schmid, and
V. Zue. 1998. Galaxy-II: A reference architecture for
conversational system development. In Proc. ICSLP
?98, Sydney, Australia.
Z. Wang and T. Schultz. 2003. Non-native spontaneous
speech recognition through polyphone decision tree
specialization. In Proc. Eurospeech ?03, pages 1449?
1452, Geneva, Switzerland.
W. Ward and S. Issar. 1994. Recent improvements in the
CMU spoken language understanding system. In Proc.
ARPA Human Language Technology Workshop, pages
213?216, Plainsboro, NJ.
S. Witt and S. Young. 1997. Language learning based on
non-native speech recognition. In Proc. Eurospeech
?97, pages 633?636, Rhodes, Greece.
Proceedings of NAACL HLT 2007, pages 460?467,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Combining Lexical and Grammatical Features to Improve Readability 
Measures for First and Second Language Texts 
Michael J. Heilman 
 
Kevyn Collins-
Thompson 
Jamie Callan 
 
Maxine Eskenazi 
 
Language Technologies Institute 
School of Computer Science 
Carnegie Mellon University 
4502 Newell Simon Hall 
Pittsburgh, PA 15213-8213 
 {mheilman,kct,callan,max}@cs.cmu.edu 
 
 
Abstract 
This work evaluates a system that uses in-
terpolated predictions of reading difficulty 
that are based on both vocabulary and 
grammatical features.  The combined ap-
proach is compared to individual gram-
mar- and language modeling-based 
approaches.  While the vocabulary-based 
language modeling approach outper-
formed the grammar-based approach, 
grammar-based predictions can be com-
bined using confidence scores with the 
vocabulary-based predictions to produce 
more accurate predictions of reading dif-
ficulty for both first and second language 
texts.  The results also indicate that gram-
matical features may play a more impor-
tant role in second language readability 
than in first language readability. 
1 Introduction 
The REAP tutoring system (Heilman, et al 2006), 
aims to provide authentic reading materials of the 
appropriate difficulty level, in terms of both vo-
cabulary and grammar, for English as a Second 
Language students.  An automatic measure of read-
ability that incorporated both lexical and gram-
matical features was thus needed. 
For first language (L1) learners (i.e., children 
learning their native tongue), reading level has 
been predicted using a variety of techniques, based 
on models of a student?s lexicon, grammatical sur-
face features such as sentence length (Flesch, 
1948), or combinations of such features (Schwarm 
and Ostendorf, 2005).  It was shown by Collins-
Thompson and Callan (2004) that a vocabulary-
based language modeling approach was effective at 
predicting the readability of grades 1 to 12 of Web 
documents of varying length, even with high levels 
of noise.   
Prior work on first language readability by 
Schwarm and Ostendorf (2005) incorporated 
grammatical surface features such as parse tree 
depth and average number of verb phrases.  This 
work combining grammatical and lexical features 
was promising, but it was not clear to what extent 
the grammatical features improved predictions.   
Also, discussions with L2 instructors suggest 
that a more detailed grammatical analysis of texts 
that examines features such as passive voice and 
various verb tenses can provide better features with 
which to predict reading difficulty.  One goal of 
this work is to show that the use of pedagogically 
motivated grammatical features (e.g., passive 
voice, rather than the number of words per sen-
tence) can improve readability measures based on 
lexical features alone. 
One of the differences between L1 and L2 read-
ability is the timeline and processes by which first 
and second languages are acquired.  First language 
acquisition begins at infancy, and the primary 
grammatical structures of the target language are 
acquired by age four in typically developing chil-
460
dren (Bates, 2003).  That is, most grammar is ac-
quired prior to the beginning of a child?s formal 
education.  Therefore, most grammatical features 
seen at high reading levels such as high school are 
present with similar frequencies at low reading 
levels such as grades 1-3 that correspond to ele-
mentary school-age children.  It should be noted 
that sentence length is one grammar-related differ-
ence that can be observed as L1 reading level in-
creases.  Sentences are kept short in texts for low 
L1 reading levels in order to reduce the cognitive 
load on child readers.  The average sentence length 
of texts increases with the age and reading level of 
the intended audience.  This phenomenon has been 
utilized in early readability measures (Flesch, 
1948).  Vocabulary change, however, continues 
even into adulthood, and has been shown to be a 
more effective predictor of L1 readability than 
simpler measures such as sentence length (Collins-
Thompson and Callan, 2005). 
Second language learners, unlike their L1 coun-
terparts, are still very much in the process of ac-
quiring the grammar of their target language.  In 
fact, even intermediate and advanced students of 
second languages, who correspond to higher L2 
reading levels, often struggle with the grammatical 
structures of their target language.  This phenome-
non suggests that grammatical features may play a 
more important role in predicting and measuring 
L2 readability.  That is not to say, however, that 
vocabulary cannot be used to predict L2 reading 
levels.  Second language learners are learning both 
vocabulary and grammar concurrently, and reading 
materials for this population are chosen or au-
thored according to both lexical and grammatical 
complexity.  Therefore, the authors predict that a 
readability measure for texts intended for second 
language learners that incorporates both grammati-
cal and lexical features could clearly outperform a 
measure based on only one of these two types of 
features. 
This paper begins with descriptions of the lan-
guage modeling and grammar-based prediction 
systems.  A description of the experiments follows 
that covers both the evaluation metrics and corpora 
used.  Experimental results are presented, followed 
by a discussion of these results, and a summary of 
the conclusions of this work.  
2 Language Model Readability Prediction 
for First Language Texts 
Statistical language modeling exploits patterns of 
use in language.  To build a statistical model of 
text, training examples are used to collect statistics 
such as word frequency and order.  Each training 
example has a label that tells the model the ?true? 
category of the example.  In this approach, one 
statistical model is built for each grade level to be 
predicted. 
The statistical language modeling approach has 
several advantages over traditional readability 
formulas, which are usually based on linear regres-
sion with two or three variables.  First, a language 
modeling approach generally gives much better 
accuracy for Web documents and short passages 
(Collins-Thompson and Callan, 2004).  Second, 
language modeling provides a probability distribu-
tion across all grade models, not just a single pre-
diction.  Third, language modeling provides more 
data on the relative difficulty of each word in the 
document.  This might allow an application, for 
example, to provide more accurate vocabulary as-
sistance. 
The statistical model used for this study is 
based on a variation of the multinomial Na?ve 
Bayes classifier.  For a given text passage T, the 
semantic difficulty of T relative to a specific grade 
level Gi is predicted by calculating the likelihood 
that the words of T were generated from a repre-
sentative language model of Gi.  This likelihood is 
calculated for each of a number of language mod-
els, corresponding to reading difficulty levels.  The 
reading difficulty of the passage is then estimated 
as the grade level of the language model most 
likely to have generated the passage T. 
The language models employed in this work are 
simple: they are based on unigrams and assume 
that the probability of a token is independent of the 
surrounding tokens.  A unigram language model is 
simply defined by a list of types (words) and their 
individual probabilities.  Although this is a weak 
model, it can be effectively trained from less la-
beled data than more complex models, such as bi-
gram or trigram models.  Additionally, higher 
order n-gram models might capture grammatical as 
well as lexical differences.  The relative contribu-
tions of grammatical and lexical features were thus 
better distinguished by using unigram language 
461
models that more exclusively focus on lexical dif-
ferences. 
In this language modeling approach, a genera-
tive model is assumed for a passage T, in which a 
hypothetical author generates the tokens of T by: 
1. Choosing a grade language model, Gi, 
from the set G = {Gi} of 12 unigram language 
models, according to a prior probability distri-
bution P(Gi). 
2. Choosing a passage length |T| in tokens ac-
cording to a probability distribution P(|T|). 
3. Sampling |T| tokens from Gi?s multinomial 
word distribution according to the ?na?ve? as-
sumption that each token is independent of all 
other tokens in the passage, given the language 
model Gi. 
These assumptions lead to the following expres-
sion for the probability of T being generated by 
language model Gi according to a multinomial dis-
tribution: 
 
?
?
=
Vw
wC
i
i
wC
GwPTTPGTP )!(
)|(|!||)(|)|(
)(
 
 
Next, according to Bayes? Theorem: 
  
)(
)|()()|(
TP
GTPGPTGP iii = . 
 
Substituting (1) into (2), taking logarithms, and 
simplifying produces: 
 
SRwC
GwPwCTGP
Vw
i
Vw
i
loglog)!(log
)|(log)()|(log
++?
=


?
?
, 
 
where V is the list of all types in the passage T, w is 
a type in V, and C(w) is the number of tokens with 
type w in T.  For simplicity, the factor R represents 
the contribution of the prior P(Gi), and S represents 
the contribution of the passage length |T|, given the 
grade level.   
Two further assumptions are made to simplify 
the illustration: 
1. That all grades are equally likely a priori.   
That is, 
G
i N
GP 1)( =  where NG is the number 
of grade levels.  For example, if there are 12 
grade levels, then NG = 12.  This allows log R to 
be ignored. 
2. That all passage lengths (up to a maximum 
length M) are equally likely.  This allows log S 
to be ignored. 
These may be poor assumptions in a real appli-
cation, but they can be easily included or excluded 
in the model as desired.  The log C(w)! term can 
also be ignored because it is constant across levels.  
Under these conditions, an extremely simple form 
for the grade likelihood remains.  In order to find 
which model Gi maximizes Equation (3), the 
model which Gi that maximizes the following 
equation must be found: 
 
)|(log)()|( i
Vw
i GwPwCGTL 
?
=  
 
This is straightforward to compute: for each token 
in the passage T, the log probability of the token 
according to the language model of Gi is calcu-
lated.  Summing the log probabilities of all tokens 
produces the overall likelihood of the passage, 
given the grade.  The grade level with the maxi-
mum likelihood is then chosen as the final read-
ability level prediction. 
This study employs a slightly more sophisti-
cated extension of this model, in which a sliding 
window is moved across the text, with a grade pre-
diction being made for each window.  This results 
in a distribution of grade predictions.  The grade 
level corresponding to a given percentile of this 
distribution is chosen as the prediction for the en-
tire document.  The values used in these experi-
ments for the percentile thresholds for L1 and L2 
were chosen by accuracy on held-out data. 
3 Grammatical Construction Readability 
Prediction for Second Language Texts 
The following sections describe the approach to 
predicting readability based on grammatical fea-
tures.  As with any classifier, two components are 
required to classify texts by their reading level: 
first, a definition for and method of identifying 
features; second, an algorithm for using these fea-
tures to classify a given text.  A third component, 
training data, is also necessary in this classification 
462
task.  The corpus of materials used for training and 
testing is discussed in a subsequent section. 
3.1 Features for Grammar-based Prediction 
L2 learners usually learn grammatical patterns ex-
plicitly from grammar explanations in L2 text-
books, unlike their L1 counterparts who learn them 
implicitly through natural interactions.  Grammati-
cal features would therefore seem to be an essential 
component of an automatic readability measure for 
L2 learners, who must actively acquire both the 
lexicon and grammar of their target language. 
The grammar-based readability measure relies 
on being able to automatically identify grammati-
cal constructions in text.  Doing so is a multi-step 
process that begins by syntactically parsing the 
document.  The Stanford Parser (Klein and Man-
ning, 2002) was used to produce constituent struc-
ture trees.  The choice of parser is not essential to 
the approach, although the accuracy of parsing 
does play a role in successful identification of cer-
tain grammatical patterns. PCFG scores from the 
parser were also used to filter out some of the ill-
formed text present in the test corpora.  The default 
training set of Penn Treebank (Marcus et al 1993) 
was used for the parser because the domain and 
style of those texts actually matches fairly well 
with the domain and style of the texts on which a 
reading level predictor for second language learn-
ers might be used. 
Once a document is parsed, the predictor uses 
Tgrep2 (Rohde, 2005), a tree structure searching 
tool, to identify instances of the target patterns.  A 
Tgrep2 pattern defines dominance, sisterhood, 
precedence, and other relationships between nodes 
in the parse tree for a sentence.  A pattern can also 
place constraints on the terminal symbols (e.g., 
words and punctuation), such that a pattern might 
require a form of the copula ?be? to exist in a cer-
tain position in the construction.  An example of a 
TGrep2 search pattern for the progressive verb 
tense is the following: 
 
?VP < /^VB/ < (VP < VBG)? 
 
Searching for this pattern returns sentences in 
which a verb phrase (VP) dominates an auxiliary 
verb (whose symbol begins with VB) as well as 
another verb phrase, which in turn dominates a 
verb in gerund form (VBG).  An example of a 
matching sentence is, ?The student was reading a 
book,? shown in Figure 2. 
 
Figure 2: The parse tree for an example sentence 
that matches a pattern for progressive verb tense. 
 
A set of 22 relevant grammatical constructions 
were identified from grammar textbooks for three 
different ESL levels (Fuchs et al, 2005).  These 
grammar textbooks had different authors and pub-
lishers than the ones used in the evaluation corpora 
in order to minimize the chance of experimental 
results not generalizing beyond the specific materi-
als employed in this study.  The ESL levels corre-
spond to the low-intermediate (hereafter, level 3), 
high-intermediate (level 4), and advanced (level 5) 
courses at the University of Pittsburgh?s English 
Language Institute.  The constructions identified in 
these grammar textbooks were then implemented 
in the form of Tgrep2 patterns.   
 
Feature  Lowest Level Highest Level 
Passive Voice 0.11 0.71 
Past Participle 0.28 1.63 
Perfect Tense 0.01 0.33 
Relative Clause 0.54 0.60 
Continuous 
Tense 
0.19 0.27 
Modal 0.80 1.44 
Table 1: The rates of occurrence per 100 words of 
a few of the features used by the grammar-based 
predictor.  Rates are shown for the lowest (2) and 
highest (5) levels in the L2 corpus. 
 
The rate of occurrence of constructions was 
calculated on a per word basis.  A per-word rather 
a book 
The student 
S 
VP 
VBD VP 
VBG 
NP 
was 
reading 
NP 
463
than a per-sentence measure was chosen because a 
per-sentence measure would depend too greatly on 
sentence length, which also varies by level.  It was 
also desirable to avoid having sentence length con-
founded with other features.  Table 1 shows that 
the rates of occurrence of certain constructions be-
come more frequent as level increases.  This sys-
tematic variation across levels is the basis for the 
grammar-based readability predictions. 
A second feature set was defined that consisted 
of 12 grammatical features that could easily be 
identified without computationally intensive syn-
tactic parsing.  These features included sentence 
length, the various verb forms in English, includ-
ing the present, progressive, past, perfect, continu-
ous tenses, as well as part of speech labels for 
words.  The goal of using a second feature set was 
to examine how dependent prediction quality was 
on a specific set of features, as well as to test the 
extent to which the output of syntactic parsing 
might improve prediction accuracy. 
3.2 Algorithm for Grammatical Feature-
based Classification 
A k-Nearest Neighbor (kNN) algorithm is used for 
classification based on the grammatical features 
described above.  The kNN algorithm is an in-
stance-based learning technique originally devel-
oped by Cover and Hart (1967) by which a test 
instance is classified according to the classifica-
tions of a given number (k) of training instances 
closest to it.  Distance is defined in this work as the 
Euclidean distance of feature vectors.  Mitchell 
(1997) provides more details on the kNN algo-
rithm.  This algorithm was chosen because it has 
been shown to be effective in text classification 
tasks when compared to other popular methods 
(Yang 1999).  A k value of 12 was chosen because 
it provided the best performance on held-out data. 
Additionally, it is straightforward to calculate 
a confidence measure with which kNN predictions 
can be combined with predictions from other clas-
sifiers?in this case with predictions from the uni-
gram language modeling-based approach described 
above.  A confidence measure was important in 
this task because it provided a means with which to 
combine the grammar-based predictions with the 
predictions from the language modeling-based 
predictor while maintaining separate models for 
each type of feature.  These separate models were 
maintained to better determine the relative contri-
butions of grammatical and lexical features. 
A static linear interpolation of predictions us-
ing the two approaches led to only minimal reduc-
tions of prediction error, likely because predictions 
from the poorer performing grammar-based classi-
fier were always given the same weight.  However, 
with the confidence measures, predictions from the 
grammar-based classifier could be given more 
weight when the confidence measure was high, and 
less weight when the measure was low and the 
predictions were likely to be inaccurate.  The case-
dependent interpolation of prediction values al-
lowed for the effective combination of language 
modeling- and grammar-based predictions.  
The confidence measure employed is the pro-
portion of the k most similar training examples, or 
nearest neighbors, that agree with the final label 
chosen for a given test document.  For example, if 
seven of ten neighbors have the same label, then 
the confidence score will be 0.6.  The interpolated 
readability prediction value is calculated as fol-
lows: 
 
LI = LLM + CkNN * LGR, 
 
where LLM is the language model-based prediction, 
LGR is the grammar-based prediction from the kNN 
algorithm, and CkNN is the confidence value for the 
kNN prediction.  The language modeling approach 
is treated as a black box, but it would likely be 
beneficial to have confidence measures for it as 
well. 
4 Descriptions of Experiments 
This section describes the experiments used to test 
the hypothesis that grammar-based features can 
improve readability measures for English, espe-
cially for second language texts.  The measures 
and cross-validation setup are described.  A de-
scription of the evaluation corpora of labeled first 
and second language texts follows. 
4.1 Experimental Setup 
Two measurements were used in evaluating the 
effectiveness of the reading level predictions.  
First, the correlation coefficient evaluated whether 
the trends of prediction values matched the trends 
for human-labeled texts.  Second, the mean 
squared error of prediction values provided a 
464
measure of how correct each of the predictors was 
on average,  penalizing more severe errors more 
heavily.  Mean square error was used rather than 
simple accuracy (i.e., number correct divided by 
sample size) because the task of readability predic-
tion is more akin to regression than classification.  
Evaluation measures such as accuracy, precision, 
and recall are thus less meaningful for readability 
prediction tasks because they do not capture the 
fact that an error of 4 levels is more costly than an 
error of a single level. 
A nine-fold cross-validation was employed.  
The data was first split into ten sets.  One set was 
used as held-out data for selecting the parameter k 
for the kNN algorithm and the percentile value for 
the language modeling predictor, and then the re-
maining nine were used to evaluate the quality of 
predictions.  Each of these nine was in turn se-
lected as the test set, and the other eight were used 
as training data. 
4.2 Corpora of Labeled Texts 
Two corpora of labeled texts were used in the 
evaluation.  The first corpus was from a set of texts 
gathered from the Web for a prior evaluation of the 
language modeling approach.  The 362 texts had 
been assigned L1 levels (1-12) by grade school 
teachers, and consisted of approximately 250,000 
words.  For more details on the L1 corpus, see 
(Collins-Thompson and Callan, 2005). 
The second corpora consisted of textbook mate-
rials (Adelson-Goldstein and Howard, 2004, for 
level 2; Ediger and Pavlik, 2000, for levels 3 and 4; 
Silberstein, 2002, for level 5) from a series of Eng-
lish as a Second Language reading courses at the 
English Language Institute at the University of 
Pittsburgh.  The four reading practice textbooks 
that constitute this corpus were from separate au-
thors and publishers than the grammar textbooks 
used to select and define grammatical features.  
The reading textbooks in the corpus are used in 
courses intended for beginning (level 2) through 
advanced (level 5) students.  The textbooks were 
scanned into electronic format, and divided into 
fifty roughly equally sized files.  This second lan-
guage corpus consisted of approximately 200,000 
words. 
Although the sources and formats of the two 
corpora were different, they share a number of 
characteristics.  Their size was roughly equal. The 
documents in both were also fairly but not per-
fectly evenly distributed across the levels.  Both 
corpora also contained a significant amount of 
noise which made accurate prediction of reading 
level more challenging.  The L1 corpus was from 
the Web, and therefore contained navigation 
menus, links, and the like.  The texts in the L2 cor-
pus also contained significant levels of noise due to 
the inclusion of directions preceding readings, ex-
ercises and questions following readings, as well as 
labels on figures and charts.  The scanned files 
were not hand-corrected in this study, in part to test 
that the measures are robust to noise, which is pre-
sent in the Web documents for which the readabil-
ity measures are employed in the REAP tutoring 
system.  
The grammar-based prediction seems to be 
more significantly negatively affected by the noise 
in the two corpora because the features rely more 
on dependencies between different words in the 
text.  For example, if a word happened to be part of 
an image caption rather than a well-formed sen-
tence, the unigram language modeling approach 
would only be affected for that word, but the 
grammar-based approach might be affected for 
features spanning an entire clause or sentence. 
5 Results of Experiments 
The results show that for both the first and sec-
ond language corpora, the language modeling 
(LM) approach alone produced more accurate pre-
dictions than the grammar-based approach alone.  
The mean squared error values (Table 2) were 
lower, and the correlation coefficients (Table 3) 
were higher for the LM predictor than the gram-
mar-based predictor.   
The results also indicate that while grammar-
based predictions are not as accurate as the vo-
cabulary-based scores, they can be combined with 
vocabulary-based scores to produce more accurate 
interpolated scores.  The interpolated predictions 
combined by using the kNN confidence measure 
were slightly and in most tests significantly more 
accurate in terms of mean squared error than the 
predictions from either single measure.   Interpola-
tion using the first set of grammatical features led 
to 7% and 22% reductions in mean squared error 
on the L1 and L2 corpora, respectively.  These re-
sults were verified using a one-tailed paired t-test 
465
of the squared error values of the predictions, and 
significance levels are indicated in Table 2. 
 
Mean Squared Error Values 
Test Set (Num. Levels) L1(12) L2(4) 
Language Modeling 5.02 0.51 
Grammar 10.27 1.08 
Interpolation 4.65* 0.40** 
Grammar2 (feature set #2) 12.77 1.26 
Interp2. (feature set #2) 4.73 0.43* 
Table 2.  Comparison of Mean Squared Error of 
predictions compared to human labels for different 
methods.  Interpolated values are significantly bet-
ter compared to language modeling predictions 
where indicated (* = p<0.05, ** = p<0.01). 
 
Correlation Coefficients 
Test Set (Num. Levels) L1(12) L2(4) 
Language Modeling 0.71 0.80 
Grammar 0.46 0.55 
Interpolation 0.72 0.83 
Grammar2 (feature set #2) 0.34 0.48 
Interp2. (feature set #2) 0.72 0.81 
Table 3.  Comparison of Correlation Coefficients 
of prediction values to human labels for different 
prediction methods. 
 
The trends were similar for both sets of gram-
matical features.  However, the first set of features 
that included complex syntactic constructs led to 
better performance than the second set, which in-
cluded only verb tenses, part of speech labels, and 
sentence length.  Therefore, when syntactic parsing 
is not feasible because of corpora size, it seems 
that grammatical features requiring only part-of-
speech tagging and word counts may still improve 
readability predictions.  This is practically impor-
tant because parsing can be too computationally 
intensive for large corpora. 
All prediction methods performed better, in 
terms of correlations, on the L2 corpus than on the 
L1 corpus.  The L2 corpus is somewhat smaller in 
size and should, if only on the basis of training ma-
terial available to the prediction algorithms, actu-
ally be more difficult to predict than the L1 corpus.  
To ensure that the range of levels was not causing 
the four-level L2 corpus to have higher predictions 
than the twelve-level L1 corpus, the L1 corpus was 
also divided into four bins (grades 1-3, 4-6, 7-9, 
10-12).  The accuracy of predictions for the binned 
version of the L1 corpus was not substantially dif-
ferent than for the 12-level version. 
6 Discussion 
In the experimental tests, the LM approach was 
more effective for measuring both L1 and L2 read-
ability.  There are several potential causes of this 
effect.  First, the language modeling approach can 
utilize all the words as they appear in the text as 
features, while the grammatical features were cho-
sen and defined manually.  As a result, the LM 
approach can make measurements on a text for as 
many features as there are words in its lexicon.  
Additionally, the noise present in the corpora likely 
affected the grammar-based approach dispropor-
tionately more because that method relies on accu-
rate parsing of relationships between words. 
Additionally, English is a morphologically im-
poverished language compared to most languages.  
Text classification, information retrieval, and many 
other human language technology tasks can be ac-
complished for English without accounting for 
grammatical features such as morphological inflec-
tions.  For example, an information retrieval sys-
tem can perform reasonably well in English 
without performing stemming, which does not 
greatly increase performance except when queries 
and documents are short (Krovetz, 1993). 
However, most languages have a rich morphol-
ogy by which a single root form may have thou-
sands or perhaps millions of inflected or derived 
forms.  Language technologies must account for 
morphological features in such languages or the 
vocabulary grows so large that it becomes unman-
ageable.  Lee (2004), for example, showed that 
morphological analysis can improve the quality of 
statistical machine translation for Arabic.  Thus it 
seems that grammatical features could contribute 
even more to measures of readability for texts in 
other languages. 
That said, the use of grammatical features ap-
pears to play a more important role in readability 
measures for L2 than for L1.  When interpolated 
with grammar-based scores, the reduction of mean 
squared error over the language modeling approach 
for L1 was only 7%, while for L2 the reduction or 
squared error was 22%.  An evaluation on corpora 
with less noise would likely bring out these differ-
466
ences further and show grammar to be an even 
more important factor in second language readabil-
ity.  This result is consistent with the fact that sec-
ond language learners are still in the process of 
acquiring the basic grammatical constructs of their 
target language. 
7 Conclusion 
The results of this work suggest that grammatical 
features can play a role in predicting reading diffi-
culty levels for both first and second language texts 
in English.  Although a vocabulary-based language 
modeling approach outperformed the grammar-
based predictor, an interpolated measure using 
confidence scores for the grammar-based predic-
tions showed improvement over both individual 
measures.  Also, grammar appears to play a more 
important role in second language readability than 
in first language readability.  Ongoing work aims 
to improve grammar-based readability by reducing 
noise in training data, automatically creating larger 
grammar feature sets, and applying more sophisti-
cated modeling techniques. 
8 Acknowledgements 
We would like to acknowledge Lori Levin for use-
ful advice regarding grammatical constructions, as 
well as the anonymous reviewers for their sugges-
tions.  
This material is based on work supported by 
NSF grant IIS-0096139 and Dept. of Education 
grant R305G03123. Any opinions, findings, con-
clusions or recommendations expressed in this ma-
terial are the authors', and do not necessarily reflect 
those of the sponsors. 
References 
J. Adelson-Goldstein and L. Howard. 2004.  Read and 
Reflect 1.  Oxford University Press, USA. 
E. Bates. 2003. On the nature and nurture of language. 
In R. Levi-Montalcini, D. Baltimore, R. Dulbecco, F. 
Jacob, E. Bizzi, P. Calissano, & V. Volterra (Eds.), 
Frontiers of biology: The brain of Homo sapiens (pp. 
241?265). Rome: Istituto della Enciclopedia Italiana 
fondata da Giovanni Trecanni. 
M. Fuchs, M. Bonner, M. Westheimer. 2005.  Focus on 
Grammar, 3rd Edition.  Pearson ESL. 
K. Collins-Thompson and J. Callan. 2004.  A language 
modeling approach to predicting reading difficulty.  
Proceedings of the HLT/NAACL Annual Conference. 
T. Cover and P. Hart. 1967.  Nearest neighbor pattern 
classification.  IEEE Transactions on Information 
Theory, 13, 21-27. 
A. Ediger and C. Pavlik.  2000.  Reading Connections 
Intermediate.  Oxford University Press, USA. 
A. Ediger and C. Pavlik.  2000.  Reading Connections 
High Intermediate.  Oxford University Press, USA. 
M. Heilman, K. Collins-Thompson, J. Callan & M. Es-
kenazi. 2006. Classroom success of an Intelligent Tu-
toring System for lexical practice and reading 
comprehension. Proceedings of the Ninth Interna-
tional Conference on Spoken Language Processing. 
D. Klein and C. D. Manning. 2002. Fast Exact Inference 
with a Factored Model for Natural Language Parsing. 
Advances in Neural Information Processing Systems 
15 (NIPS 2002), December 2002. 
R. Krovetz. 1993. Viewing morphology as an inference 
process. SIGIR-93, 191?202. 
Y. Lee. 2004.  Morphological Analysis for Statistical 
Machine Translation.  Proceedings of the 
HLT/NAACL Annual Conference. 
M. Marcus, B. Santorini and M. Marcinkiewicz. 1993. 
"Building a large annotated corpus of English: the 
Penn Treebank." Computational Linguistics, 19(2). 
T. Mitchell. 1997. Machine Learning.  The McGraw-
Hill Companies, Inc.  pp. 231-236. 
D. Rohde. 2005. Tgrep2 User Manual. 
http://tedlab.mit.edu/~dr/Tgrep2/tgrep2.pdf. 
S. Schwarm, and M. Ostendorf. 2005.  Reading Level 
Assessment Using Support Vector Machines and Sta-
tistical Language Models.  Proceedings of the Annual 
Meeting of the Association for Computational Lin-
guistics. 
S. Silberstein, B. K. Dobson, and M. A. Clarke. 2002. 
Reader's Choice, 4th edition.  University of Michigan 
Press/ESL. 
Y. Yang. 1999.  A re-examination of text categorization 
methods.  Proceedings of ACM SIGIR Conference on 
Research and Development in Information Retrieval 
(SIGIR'99, pp 42--49). 
467
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 629?637,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
A Finite-State Turn-Taking Model for Spoken Dialog Systems
Antoine Raux?
Honda Research Institute
800 California Street
Mountain View, CA 94041, USA
araux@hra.com
Maxine Eskenazi
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
max@cs.cmu.edu
Abstract
This paper introduces the Finite-State Turn-
Taking Machine (FSTTM), a new model to
control the turn-taking behavior of conversa-
tional agents. Based on a non-deterministic
finite-state machine, the FSTTM uses a cost
matrix and decision theoretic principles to se-
lect a turn-taking action at any time. We show
how the model can be applied to the problem
of end-of-turn detection. Evaluation results on
a deployed spoken dialog system show that the
FSTTM provides significantly higher respon-
siveness than previous approaches.
1 Introduction
Turn-taking, the process by which participants in a
conversation alternate speech and silence, is an es-
sential component of spoken interaction. In order to
lead productive conversations, people need not only
know what to say but also when to say it. Decades
of research on Conversation Analysis and psycholin-
guistics (Duncan, 1972; Sacks et al, 1974; Ore-
stro?m, 1983; Schegloff, 2000; Wesseling and van
Son, 2005) have shown that human turn-taking be-
havior relies on a wide range of rules and signals
at many different levels of language, from prosody
to syntax, semantics, and discourse structure. In
contrast, turn-taking in spoken dialog systems is of-
ten reduced to ad hoc rules only based on very low
level features. This simplistic approach leads to in-
efficient, unnatural, and possibly confusing behavior
(Porzel and Baudis, 2004; Ward et al, 2005).
? This research was conducted when the first author was a
student at the Language Technologies Institute.
Recently, more complex models of turn-taking
have been proposed (Cassell et al, 2001; Thorisson,
2002; Kronild, 2006). Yet, these models still rely
extensively on hand-coded expert knowledge and
do not lend themselves to data-driven optimization.
Furthermore, to our knowledge, no such model has
been deployed in a widely used system outside of the
laboratory. In this paper, we propose a flexible, prac-
tical model of turn-taking behavior that builds upon
previous work on finite-state models of the conver-
sational floor. Because of its simplicity and gener-
ality, this model can be applied to many turn-taking
phenomena. At the same time, being grounded in
decision theory, it lends itself well to data-driven op-
timization. We illustrate our approach by applying
the model to a specific turn-taking task: end-of-turn
detection.
2 Conversational Floor as a Finite-State
Machine
2.1 6-state finite state models of turn-taking
In the 1960?s and early 1970?s, several researchers
proposed models to explain the rhythmic turn-taking
patterns in human conversation. In particular, Jaffe
and Feldstein (1970) studied the mean duration of
pauses, switching pauses (when a different speaker
takes the floor), simultaneous speech, and (single-
speaker) vocalizations in recorded dyadic conversa-
tions. Based on their observation that these dura-
tions follow exponential distributions, they proposed
first-order Markov models to capture the alterna-
tion of speech and silence in dialog. Their initial
model had four states: only participant A is speak-
629
Figure 1: Our six-state model of turn-taking, inspired by
Jaffe and Feldstein (1970) and Brady (1969). See section
3.1 for a description of the states.
ing; only participant B is speaking; both participants
are speaking; and neither participant is speaking.
However, such a model fails to distinguish switch-
ing pauses from A to B from switching pauses from
B to A. Based on this observation, they extend their
model to a six-state model which they found to bet-
ter fit their data than the four-state model. Around
the same time, Brady (1969) developed a very sim-
ilar six-state model. He trained the parameters on a
recorded conversation and compared the generated
conversations to the original real one along several
dimensions (pause and speech segment durations,
overlaps, etc), finding that his model generally pro-
duced a good fit of the data.
2.2 Finite-State Models for Control
While Jaffe, Feldstein and Brady were primarily
concerned with the analysis of human-human con-
versations, more recently, several researchers have
proposed finite-state machines to control conversa-
tional agents. For instance, Cassell et al (2001)
model the conversational state of an embodied real
estate agent as a 5-state machine. Two states indi-
cate whether a user is present or not, whereas the
other three indicate who holds the floor between the
user and the agent, or whether the floor is open.
Floor conflicts are not captured by this machine and
are presumably resolved through simple rules (e.g.
when the user speaks, the agent yields the floor).
Kronild (2006) proposes a much more complex
model, based on Harel statecharts, which are an ex-
tension of finite-state machines for modeling and vi-
sualizing abstract control (Harel, 1987).
Thorisson?s Ymir architecture (Thorisson, 2002)
is an attempt to model the cognitive processes in-
volved in conversation. It features dialog states, cap-
turing, for example, who has the floor, and rules that
govern the transition from one state to another based
on ?boolean conditions of perceptual features?.
All these models are deterministic. At any point
in time, the agent knows who owns the floor and uses
fixed rules to take appropriate actions. These ap-
proaches assume 1) that the system can obtain per-
fectly reliable information on the state of the world,
and 2) that the state itself is unambiguous.
3 The Finite-State Turn-Taking Machine
3.1 Extending the 6-state model for control
Our model, the Finite-State Turn-Taking Machine
(FSTTM), uses the same six states as Jaffe and
Feldstein: USER and SY STEM represent states
where one and only one of the participants claims
the floor, FREES and FREEU states where no
participant claims the floor (following, resp., a
SY STEM and USER state), and BOTHS and
BOTHU states where both participants claim the
floor (following, resp. a SY STEM and USER
state). However, we apply this model to the control
of a conversational agent, with a goal similar to that
of Cassel, Thorisson, and Kronild. One important
distinction is that we define the states in terms of the
participants? intentions and obligations (in the sense
of Traum and Allen (1994)) rather than the surface
level observation of speech vs silence. For example,
the state is USER when the user has the obligation
to speak (to respond to a system question) or the in-
tention to speak, while at the same time, the system
does not hold the floor. This does not necessarily
mean that the user is speaking, for example at pauses
during a user utterance.
As can be seen in Figure 1, not all transitions are
valid. First, there is no direct transition between any
of the intermediate states (the two FREE states and
two BOTH states). The assumption is that to go
from any of these state to another, the model will
first go to either SY STEM or USER. This is an
630
approximation as there might be cases where, for
example, both the system and user start speaking
at the exact same time, going from a FREE state
to a BOTH state. However these cases are rare
enough that they can be approximated using a tran-
sition through either SY STEM or USER. Sec-
ond, because intermediate states are conditioned on
who had the floor previously, not all valid transitions
are bidirectional. For example, there is no transi-
tion from SY STEM to BOTHU . We associate
pairs of user/system actions to each transition. The
four possible actions are Grab the floor, Release the
floor, Wait while not claiming the floor, and Keep
the floor. For example, transition from SY STEM
to FREES corresponds to the user waiting silently
and the system releasing the floor at the end of a
prompt, noted (R,W ) (we always note the system
action first and user action second).
This representation allows us to formalize a wide
variety of turn-taking phenomena in a unified frame-
work. Specifically, there are 4 types of 2-step transi-
tions from a single-floor-holder state (SY STEM or
USER) to another (or the same) single-floor-holder
state, which represent typical turn-taking phenom-
ena:
Turn transitions with gap are the most common
way the floor goes from one participant to the
other. For example, at the end of a user utter-
ance, once the user finishes speaking, the floor
becomes free, after which the system starts re-
sponding, thus grabbing the floor. The resulting
state sequence is:
SY STEM (R,W )? FREES
(W,G)? USER
Conversely, the transition with gap following a
system prompt corresponds to:
USER (R,W )? FREES
(W,G? USER
Turn transitions with overlap happen when a par-
ticipant grabs the floor while it still belongs to
the other. For example, when a user barges in
on a system prompt, both participants hold the
floor. Then, the system recognizes the barge-
in attempt and relinquishes the floor, which be-
comes user?s.
SY STEM (K,G)? BOTHS
(R,K? USER
And conversely, when the system interrupts the
user mid-utterance (which in dialog systems is
more often the result of an intentional cut-in,
rather than intentional interruption), the state
sequence is:
USER (G,K)? BOTHU
(K,R)? SY STEM
Failed interruptions happen when a participant
barges in on the other and then withdraws be-
fore the original floor holder releases the floor.
For example, when the system interrupts the
user (often by mistake) but detects it and in-
terrupts itself:
USER (G,K)? BOTHU
(R,K? USER
The converse is usually the result of the system
failing to react fast enough to a user barge-in:
SY STEM (K,G)? BOTHS
(K,R)? SY STEM
Note that backchannels seem to fit in this cat-
egory too. However, since backchannels, by
definition, do not represent an attempt to grab
the floor, they are not captured by the model
as it is (for example, the floor should remain
SY STEM when a user backchannels a sys-
tem utterance).
Time outs start like transitions with gap but the in-
tended next speaker (e.g. the user after a system
prompt) does not take the floor and the original
floor holder grabs it back. For instance, after a
system prompt, if the floor remains free for a
certain amount of time, the system attempts to
re-establish the communication with the user,
as follows:
SY STEM (R,W )? FREES
(G,W? SY STEM
The opposite also happens when the system is
to slow to respond to the user:
USER (W,R)? FREEU
(W,G? USER
While all the transitions above were described
as deterministic, the actual state of the model is
not fully observable. Specifically, while the system
631
knows whether its claiming the floor or not, it can
only believe with some degree of uncertainty that
the user does so. The system?s knowledge of its own
claim to the floor splits the state space into two dis-
joint subsets. When the system claims the floor, the
state can be SY STEM , BOTHS , or BOTHU ).
When the system does not claim the floor, the state
can be USER, FREEU , or FREES). In either
case, the system needs to recognize the user?s in-
tention (i.e. whether the user claims to the floor or
not) to maintain a probability distribution over the
three states. Since the distinction between the two
BOTH states (resp. the two FREE states) is based
on past history that can be known with a high level
of certainty, the uncertainty in state distribution is
fully characterized by the probability that the user is
claiming the floor, which will have to be estimated
from observations, as we will see below.
3.2 Cost of Turn-Taking Actions
The problem we are facing is that of choosing the
best system action given the system?s belief about
the current state of the model. That is achieved by
applying the probabilistic decision theory principle
of selecting the action with lowest expected cost.
The actions available to the system are the four de-
scribed above (G,R,K,W ), although not all actions
are available in all states. In fact, as can be seen in
Table 1, there are always only two actions available
in each state, depending on whether the system is
claiming the floor or not.
Each action in each state has a particular cost.
While there are many possible ways of defining
these costs, we propose a simple cost structure that
derives from the principles laid out in Sacks et al
(1974):
Participants in a conversation attempt to
minimize gaps and overlaps.
From this general principle, we derive three rules to
drive the design of a cost matrix:
1. The cost of an action that resolves either a gap
or an overlap is zero
2. The cost of an action that creates unwanted gap
or overlap is equal to a constant parameter (po-
tentially different for each action/state pair)
3. The cost of an action that maintains a gap or
overlap is either a constant or an increasing
function of the total time spent in that state
The resulting cost matrix is shown in Table 1, where
? CS is the cost of interrupting a system prompt
before its end when the user is not claiming the
floor (false interruption)
? CO(? ) is the cost of remaining in an overlap
that is already ? ms long
? CU is the cost of grabbing the floor when the
user is holding it (cut-in)
? CG(? ) is the cost of remaining in a gap that is
already ? ms long
This cost structure makes a number of simplifying
assumptions and there are many other possible cost
matrices. For example, the cost of interrupting the
user might vary depending on what has already been
said in the utterance, so does the cost of interrupt-
ing a system prompt. A more principled approach
to setting the costs would be to estimate from per-
ceptual experiments or user studies what the impact
of remaining in gap or overlap is compared to that
of a cut-in or false interruption. However, as a first
approximation, the proposed cost structure offers a
simple way to take into account some of the con-
straints of interaction.
3.3 Decision Theoretic Action Selection
Given the state space and the cost matrix given
above, the optimal decision at any point in time is
the one that yields the lowest expected cost, where
the expected cost of action A is:
C(A) = ?
S??
P (s = S|O) ? C(A,S)
where ? is the set of states, O are the observable
features of the world, and C(A,S) is the cost of ac-
tion A in state S, from the cost matrix in Table 1.
In addition to the cost matrix? four constants, which
we will consider as parameters of the model, it is
thus necessary to estimate P (s = S|O), which as
seen above amounts to estimate the probability that
the user is claiming the floor. Key to applying the
FSTTM to a practical turn-taking problem is thus
the construction of accurate estimates of the proba-
bilities P (s = S|O).
632
PPPPPPPPState
Action K R W G
SY STEM 0 CS - -
BOTHS CO(?) 0 - -
BOTHU CO(?) 0 - -
USER - - 0 CU
FREEU - - CG(?) 0
FREES - - CG(?) 0
Table 1: Cost of system actions in each state (K: keep the floor, R: release the floor, W : wait without the floor, G:
grab the floor, ? : time spent in current state, -: action unavailable).
4 Endpointing with the FSTTM
4.1 Problem formalization
In our FSTTM formalism, endpointing is the prob-
lem of selecting between the Wait and the Grab ac-
tions during a user utterance. We make the simplify-
ing assumption that, once a user utterance has been
detected, the only states with non-zero probability
are USER and FREEU . While this does not cap-
ture cases where the system erroneously detects user
speech (because there is, for example, background
noise), it represents a working first approximation
of the problem.
The main issue is to estimate the probability
P (s = FREEU |Ot) (hereafter abbreviated as
P (F |Ot), P (s = USER|Ot) being abbreviated as
P (U |Ot)) where Ot represents all observable fea-
tures at time t. Given that probability, the expected
cost of grabbing the floor is:
C(G|Ot) = P (U |Ot) ? CU + P (F |Ot) ? 0
= (1? P (F |Ot)) ? CU
Similarly, the expected cost of waiting is:
C(W |Ot) = P (F |Ot) ? CG(?)
The system endpoints whenever the expected cost
of grabbing the floor becomes higher than that of
waiting.
We consider two separate cases for computing
both P (F |Ot) and CG(?): when a pause has been
detected by the voice activity detector (VAD), and
when no pause has been detected (yet). In the fol-
lowing sections, we provide details on the approxi-
mations and estimation methods for these two cases.
4.2 At pauses
If a pause has been detected by the VAD, we set
the cost of waiting in the FREEU state to be pro-
portional to the duration of the pause so far. If the
user has released the floor, the duration of the current
pause corresponds to the time spent in the FREEU
state, i.e. ? in the cost matrix of Table 1. In this case,
we set CG(?) = CpG ? ? as a simple application of
rule 3 from section 3.2.
We decompose the observations at time t,Ot, into
observations available at the start of the pause (O),
and observations made during the pause. With only
audio information available, the only information
available during the pause is its duration so far, i.e.
? . Specifically, we know that d ? ? , where d is the
total duration of the pause (with d = ? at the end
of a turn1). Consequently, P (F |Ot) can be rewritten
using Bayes rule as
P (F |Ot) = P (d ? ? |O,F ) ? P (F |O)P (d ? ? |O)
= P (F |O)P (d ? ? |O)
where P (F |O) is the probability that the user re-
leased the floor without any knowledge of the dura-
tion of the pause, and P (d ? ? |O) is the probability
that the pause will last at least ? ms. We further de-
compose P (d ? ? |O) into
P (d ? ? |O) = P (d ? ?, U |O) + P (d ? ?, F |O)
1Note that this is an approximation since the user could start
speaking again after releasing the floor to reestablish the chan-
nel (e.g. by saying ?Hello??). However, in the vast majority of
cases, the time after which the user resumes speaking is signifi-
cantly longer than the time the system takes to endpoint.
633
= P (d ? ? |O,U) ? P (U |O) +
P (d ? ? |O,F ) ? P (F |O)
= P (d ? ? |O,U) ? (1? P (F |O))
+P (F |O)
Consequently, P (F |Ot) is a function of P (F |O)
and P (d ? ? |O,U). We estimate P (F |O) by step-
wise logistic regression on a training set of pauses
labeled for finality (whether the pause is turn-final or
turn-internal), using a wide range of features avail-
able from various components of the dialog system.
Based on the well established observation that pause
durations follow an exponential distribution (Jaffe
and Feldstein, 1970; Lennes and Anttila, 2002; Raux
et al, 2008), P (d ? ? |O,U) is a function of mean
pause duration, computed on the training set.
4.3 In speech
In some cases, it is not necessary to wait for the VAD
to detect a pause to know with high confidence that
the user has released the floor. For example, after a
simple yes/no question, if the user says ?YES?, they
are very likely to have released the floor, regardless
of how long they remain silent afterwards. In order
to exploit this fact and improve the responsiveness
of the system in these highly predictable cases, we
use a separate model to compute the expected costs
of waiting and grabbing the floor before any pause is
detected by the VAD (specifically, whenever the du-
ration of the current pause is between 0 and 200 ms).
In this case, we set the cost of waiting to a constant
CsG. We train a logistic regression model to estimate
P (F |Ot) each time a new partial hypothesis is pro-
duced by the ASR during a user utterance. We use
the same set of features as above.
5 Evaluation
5.1 Corpus and Features
We evaluated the effectiveness of the FSTTM on
an actual deployed spoken dialog system. The sys-
tem provides bus schedule information for a mid-
size North American city. It is actually used by the
general public and therefore constantly operates and
collects data. In order to train the various proba-
bility estimation models and evaluate the approach
in batch, we first collected a corpus of 586 dialogs
between May 4, and May 14, 2008 (the ?2008 cor-
pus?).
All of the features we used can be automatically
extracted at runtime, and most of them were readily
available in the system. They include dialog state in-
formation, turn-taking features, such as whether the
current user utterance is a barge-in, and semantic
information derived from the dialog state and par-
tial recognition hypotheses provided by the speech
recognizer. Dialog state is abstracted to three high-
level states, which correspond to the type of system
prompt directly preceding the user utterance: Open
question (?What can I do for you??); Closed ques-
tion (e.g. ?Where do you want to go??); and Confir-
mation (e.g. ?Going to the airport. Is this correct??).
To capture lexical cues correlated with the end of
turns, we created a new feature called the boundary
LM score. To compute it, we used previously col-
lected data to train dialog-state-dependent statistical
language models to estimate the probability that the
hypothesis is complete. Boundary LM score is de-
fined as the ratio of the log likelihood of the hypoth-
esis being complete by that of the hypothesis being
incomplete.
5.2 Estimating P (F |Ot)
We trained two logistic regression models using
stepwise regression and 10-fold cross-validation for
evaluation. The first model, whose performance
is given in Table 2, estimates P (F |O) at pauses.
The model is unable to improve classification accu-
racy over the majority baseline for each state, how-
ever, the statistically significant improvement in av-
erage log likelihood indicates that the probability
estimates are improved by using the features. The
most informative feature in all three states was the
boundary LM score introduced in section 5.1. Other
selected features included the average number of
words per user utterance so far and whether the cur-
rent utterance is a barge-in (for the Open and Closed
question states), as well as whether the partial hy-
pothesis contained a confirmation marker such as
?YES? or ?SURE? (for the Confirmation state).
The second model performs the same regression,
this time on all partial hypotheses received during
speech segments. As seen in the ?S? columns in Ta-
ble 2, classification error was significantly reduced
and the gain in average log likelihood were larger
634
Open question Closed question Confirmation
P S P S P S
Majority Baseline 38% 20% 25% 32% 12% 36%
Classification Error 35% 17% 26% 22% 12% 17%
Baseline log likelihood -0.66 -0.50 -0.56 -0.63 -0.36 -0.65
Log likelihood -0.61 -0.40 -0.50 -0.49 -0.30 -0.40
Table 2: Performance of state-specific logistic regression for estimating P (F |O) at pauses (P) and in speech (S).
(a) In-pause evaluation on the 2007 corpus. (a) Anytime evaluation on the 2008 corpus.
Figure 2: Batch evaluation of FSTTM endpointing.
than at pauses, particularly for the ?Closed ques-
tion? and ?Confirmation? states. Again, boundary
LM score was the most informative feature. The
duration of the pause at the end of the partial hy-
pothesis (between 0 and 200 ms) also proved well
correlated with finality.
5.3 Batch Evaluation of the FSTTM
We performed two batch evaluations of the FSTTM.
The first one aims at comparing in-pause-FSTTM
with a fixed-threshold baseline as well as previous
data-driven endpointing methods proposed in Ferrer
et al (2003) (reimplemented by us) and Raux et al
(2008). This evaluation was done on the corpus used
in Raux et al (2008) (the ?2007 corpus?). As seen
in Figure 2 (a), the FSTTM outperforms all other ap-
proaches (albeit only slightly compared to Ferrer et
al.), improving over the fixed threshold baseline by
up to 29.5%.
Second, we compared the anytime-FSTTM with
in-pause-FSTTM and a fixed-threshold baseline (for
reference) on the more recent 2008 corpus (since the
2007 corpus did not contain all necessary features
for anytime-FSTTM). We set CpG = 1 and set CsG
to either 0, leading to an endpointer that never end-
points during speech (in-pause-FSTTM), or 1000
(anytime-FSTTM). In both cases, we vary CU to
compute the latency / cut-in rate trade-off curve.
The results are shown in Figure 2 (b). Anytime-
FSTTM endpointing is consistently better than in-
pause-FSTTM. For example, at a cut-in rate of 5%,
anytime-FSTTM yields latencies that are on average
17% shorter than in-pause-FSTTM, and 40% shorter
than the baseline. Additionally, we found that, in
anytime-FSTTM, 30 to 40% of the turns are end-
pointed before the pause is detected by the VAD.
5.4 Live Evaluation
To confirm the results of the batch evaluation, we
implemented our FSTTM model in the deployed
system a let it run for ten days using either FSTTM
or a fixed threshold for endpointing, resulting in
a corpus of 171 FSTTM and 148 control dialogs.
For FSTTM, we set CpG = 1, CsG = 500, and
CU = 5000. In the batch evaluation, these values
correspond to a cut-in rate of 6.3% and an average
latency of 320 ms. For the control condition, we
set the fixed endpointing threshold to 555 ms, which
also corresponded to about 6.3% cut-ins.
Figure 3 shows the average latency and cut-in rate
635
(a) Latency (b) Cut-in rates
Figure 3: Live evaluation results. All confidence intervals for latency (not shown on the figure) fall within +/? 4ms.
for both conditions. The FSTTM improves over the
baseline on all metrics, reducing average latency by
193 ms (p < 0.05), cut-in rate by 1.5% (although
this result is not statistically significant).
6 Discussion
Both batch and live evaluation results confirm the
effectiveness of the FSTTM approach in improv-
ing system responsiveness. This approach signif-
icantly reduced endpointing latency over previous
approaches. Boundary LM score got the highest
weight in the regression, indicating that in a domain
such as telephone-based information access, lexical
cues are very informative for endpointing. The fact
that boundary LMs can be computed without any hu-
man transcription effort (since they are trained on
ASR output) makes them all the more appealing.
Essentially, the FSTTM provides a simple, unified
model of turn-taking that lends itself to data-driven
optimization. While we discussed specific cost
structures and probability estimation techniques, the
framework?s flexibility opens it to other choices at
many levels. By formalizing the overall turn-taking
process in a probabilistic, decision-theoretic frame-
work, the FSTTM extends and generalizes previous
classification-based approaches to endpointing such
as those proposed by Sato et al (2002), Ferrer et
al. (2003), Takeuchi et al (2004), and our previous
work (Raux et al, 2008).
Possible extensions of the approach include data-
driven cost matrices to relax some of the assump-
tions introduced in section 3.2, as well as more com-
plex state structures to handle, for example, multi-
party conversations.
Finally, we plan to investigate more principled ap-
proaches, such as Partially Observable Markov De-
cision Processes or Dynamic Bayesian Networks, to
model the different sources of uncertainty (detection
errors and inherent ambiguity) and track the state
distribution over time. Raux (2009) provides more
details on all aspects of the approach and its possi-
ble extensions.
7 Conclusion
In this paper, motivated by existing finite-state mod-
els of turn-taking in dyadic conversations, we pro-
pose the Finite-State Turn-Taking Machine, an ap-
proach to turn-taking that relies on three core ele-
ments: a non-deterministic finite-state machine that
captures the conversational floor; a cost matrix that
models the impact of different system actions in dif-
ferent states; and a decision-theoretic action selec-
tion mechanism. We describe the application of the
FSTTM to the key turn-taking phenomenon of end-
of-turn detection. Evaluation both offline and by
applying the FSTTM to a deployed spoken dialog
system system showed that it performs significantly
better than a fixed-threshold baseline.
Acknowledgments
This work is supported by the US National Science
Foundation under grant number 0208835. Any opin-
ions, findings, and conclusions or recommendations
expressed in this material are those of the authors
and do not necessarily reflect the views of the NSF.
We would like to thank Alan Black for his many
comments and advice.
636
References
P. T. Brady. 1969. A model for generating on-off speech
patterns in two-way conversation. The Bell System
Technical Journal, 48:2445?2472.
J. Cassell, T. Bickmore, L. Campbell, H. Vilhjalmsson,
and H. Yan. 2001. More than just a pretty face: con-
versational protocols and the affordances of embodi-
ment. Knowledge-Based Systems, 14:55?64.
S. Duncan. 1972. Some signals and rules for taking
speaking turns in conversations. Journal of Person-
ality and Social Psychology, 23(2):283?292.
L. Ferrer, E. Shriberg, and A. Stolcke. 2003. A prosody-
based approach to end-of-utterance detection that does
not require speech recognition. In ICASSP, Hong
Kong.
D. Harel. 1987. Statecharts: A visual formalism for
complex systems. Science of Computer Programming,
8:231?274.
J. Jaffe and S. Feldstein. 1970. Rhythms of Dialogue.
Academic Press.
F. Kronild. 2006. Turn taking for artificial conversational
agents. In Cooperative Information Agents X, Edin-
burgh, UK.
Mietta Lennes and Hanna Anttila. 2002. Prosodic fea-
tures associated with the distribution of turns in finnish
informal dialogues. In Petri Korhonen, editor, The
Phonetics Symposium 2002, volume Report 67, pages
149?158. Laboratory of Acoustics and Audio Signal
Processing, Helsinki University of Technology.
B. Orestro?m. 1983. Turn-Taking in English Conversa-
tion. CWK Gleerup, Lund.
R. Porzel and M. Baudis. 2004. The tao of chi:
Towards effective human-computer interaction. In
HLT/NAACL 2004, Boston, MA.
A. Raux, , and M. Eskenazi. 2008. Optimizing endpoint-
ing thresholds using dialogue features in a spoken dia-
logue system. In Proc. SIGdial 2008, Columbus, OH,
USA.
A. Raux. 2009. Flexible Turn-Taking for Spoken Dialog
Systems. Ph.D. thesis, Carnegie Mellon University.
H. Sacks, E. A. Schegloff, and G. Jefferson. 1974.
A simplest systematics for the organization of turn-
taking for conversation. Language, 50(4):696?735.
R. Sato, R. Higashinaka, M. Tamoto, M. Nakano, and
K. Aikawa. 2002. Learning decision trees to deter-
mine turn-taking by spoken dialogue systems. In IC-
SLP 2002, Denver, CO.
E.A. Schegloff. 2000. Overlapping talk and the orga-
nization of turn-taking for conversation. Language in
Society, 29:1?63.
M. Takeuchi, N. Kitaoka, and S. Nakagawa. 2004.
Timing detection for realtime dialog systems using
prosodic and linguistic information. In Proc. Speech
Prosody 04, Nara, Japan.
K. R. Thorisson, 2002. Multimodality in Language and
Speech Systems, chapter Natural Turn-Taking Needs
No Manual: Computational Theory and Model, From
Perception to Action, pages 173?207. Kluwer Aca-
demic Publishers.
D. R. Traum and J. F. Allen. 1994. Discourse obligations
in dialogue. In Proc. ACL-94, pages 1?8.
N. Ward, A. Rivera, K. Ward, and D. Novick. 2005. Root
causes of lost time and user stress in a simple dialog
system. In Interspeech 2005, Lisbon, Portugal.
W. Wesseling and R.J.J.H. van Son. 2005. Timing of
experimentally elicited minimal responses as quanti-
tative evidence for the use of intonation in projecting
TRPs. In Interspeech 2005, pages 3389?3392, Lisbon,
Portugal.
637
Tutorial Abstracts of ACL-08: HLT, page 2,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Building Practical Spoken Dialog Systems 
Antoine Raux1, Brian Langner2, Alan W Black3, Maxine Eskenazi4 Language Technologies Institute Carnegie Mellon University Pittsburgh, PA, USA {antoine,blangner,awb,max}@cs.cmu.edu 
                                                           1 http://www.cs.cmu.edu/~antoine 2 http://www.cs.cmu.edu/~blangner 3 http://www.cs.cmu.edu/~awb 4 http://www.cs.cmu.edu/~max 
 
 
1 Abstract This tutorial will give a practical description of the free software Carnegie Mellon Olympus 2 Spoken Dialog Architecture. Building real working dialog systems that are robust enough for the general pub-lic to use is difficult. Most frequently, the func-tionality of the conversations is severely limited - down to simple question-answer pairs. While off-the-shelf toolkits help the development of such simple systems, they do not support more ad-vanced, natural dialogs nor do they offer the trans-parency and flexibility required by computational linguistic researchers.  However, Olympus 2 offers a complete dialog system with automatic speech recognition (Sphinx) and synthesis (SAPI, Festi-val) and has been used, along with previous ver-sions of Olympus, for teaching and research at Carnegie Mellon and elsewhere for some 5 years. Overall, a dozen dialog systems have been built using various versions of Olympus, handling tasks ranging from providing bus schedule information to guidance through maintenance procedures for complex machinery, to personal calendar manage-ment. In addition to simplifying the development of dialog systems, Olympus provides a transparent platform for teaching and conducting research on all aspects of dialog systems, including speech rec-ognition and synthesis, natural language under-standing and generation, and dialog and interaction management. The tutorial will give a brief introduction to spoken dialog systems before going into detail 
about how to create your own dialog system within Olympus 2, using the Let's Go bus information system as an example. Further, we will provide guidelines on how to use an actual deployed spo-ken dialog system such as Let's Go to validate re-search results in the real world. As a possible testbed for such research, we will describe Let's Go Lab, which provides access to both the Let's Go system and its genuine user population for research experiments. 2 Outline Part 1 1.1 Introduction 1.2 Overview of current spoken dialog  system architectures 1.3 Description of the Olympus2 dialog  architecture 1.4 How to build an Olympus2 spoken  dialog system Part 2 2.1 Advanced Topics a. Improving ASR b. Improving TTS c. Dealing with ASR Errors d. Logs and Tools 2.2 Using Olympus2 for research and  applications 2.3 Final summary 
2
Bridging the Gap: Academic and Industrial Research in Dialog Technologies Workshop Proceedings, pages 32?39,
NAACL-HLT, Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Olympus: an open-source framework 
for conversational spoken language interface research 
 
 
Dan Bohus, Antoine Raux, Thomas K. Harris, 
Maxine Eskenazi, Alexander I. Rudnicky 
School of Computer Science 
Carnegie Mellon University 
{dbohus, antoine, tkharris, max, air}@cs.cmu.edu 
 
 
 
 
Abstract 
We introduce Olympus, a freely available 
framework for research in conversational 
interfaces. Olympus? open, transparent, 
flexible, modular and scalable nature fa-
cilitates the development of large-scale, 
real-world systems, and enables research 
leading to technological and scientific ad-
vances in conversational spoken language 
interfaces. In this paper, we describe the 
overall architecture, several systems 
spanning different domains, and a number 
of current research efforts supported by 
Olympus.  
1 Introduction  
Spoken language interfaces developed in industrial 
and academic settings differ in terms of goals, the 
types of tasks and research questions addressed, 
and the kinds of resources available.  
In order to be economically viable, most indus-
try groups need to develop real-world applications 
that serve large and varied customer populations. 
As a result, they gain insight into the research 
questions that are truly significant for current-
generation technologies. When needed, they are 
able to focus large resources (typically unavailable 
in academia) on addressing these questions. To 
protect their investments, companies do not gener-
ally disseminate new technologies and results. 
In contrast, academia pursues long-term scien-
tific research goals, which are not tied to immedi-
ate economic returns or customer populations. As a 
result, academic groups are free to explore a larger 
variety of research questions, even with a high risk 
of failure or a lack of immediate payoff. Academic 
groups also engage in a more open exchange of 
ideas and results. However, building spoken lan-
guage interfaces requires significant investments 
that are sometimes beyond the reach of academic 
researchers. As a consequence, research in acade-
mia is oftentimes conducted with toy systems and 
skewed user populations. In turn, this raises ques-
tions about the validity of the results and hinders 
the research impact.  
In an effort to address this problem and facilitate 
research on relevant, real-world questions, we have 
developed Olympus, a freely available framework 
for building and studying conversational spoken 
language interfaces. The Olympus architecture, 
described in Section 3, has its roots in the CMU 
Communicator project (Rudnicky et al, 1999). 
Based on that experience and subsequent projects, 
we have engineered Olympus into an open, trans-
parent, flexible, modular, and scalable architecture. 
To date, Olympus has been used to develop and 
deploy a number of spoken language interfaces 
spanning different domains and interaction types; 
these systems are presented in Section 4. They are 
currently supporting research on diverse aspects of 
spoken language interaction. Section 5 discusses 
three such efforts: error handling, multi-participant 
conversation, and turn-taking. 
We believe that Olympus and other similar tool-
kits, discussed in Section 6, are essential in order 
to bridge the gap between industry and academia. 
Such frameworks lower the cost of entry for re-
32
search on practical conversational interfaces. They 
also promote technology transfer through the reuse 
of components, and support direct comparisons 
between systems and technologies.  
2 Desired characteristics 
While developing Olympus, we identified a num-
ber of characteristics that in our opinion are neces-
sary to effectively support and foster research. The 
framework should be open, transparent, flexible, 
modular, and scalable.  
Open. Complete source code should be avail-
able for all the components so that researchers and 
engineers can inspect and modify it towards their 
ends. Ideally, source code should be free for both 
research and commercial purposes and grow 
through contributions from the user community. 
Transparent / Analytic. Open source code 
promotes transparency, but beyond that researchers 
must be able to analyze the system?s behavior. To 
this end, every component should provide detailed 
accounts of their internal state. Furthermore, tools 
for data visualization and analysis should be an 
integral part of the framework. 
Flexible. The framework should be able to ac-
commodate a wide range of applications and re-
search interests, and allow easy integration of new 
technologies. 
Modular / Reusable. Specific functions (e.g. 
speech recognition, parsing) should be encapsu-
lated in components with rich and well-defined 
interfaces, and an application-independent design. 
This will promote reusability, and will lessen the 
system development effort.  
Scalable. While frameworks that rely on sim-
ple, well established approaches (e.g. finite-state 
dialogs in VoiceXML) allow the development of 
large-scale systems, this is usually not the case for 
frameworks that provide the flexibility and trans-
parency needed for research. However, some re-
search questions are not apparent until one moves 
from toy systems into large-scale applications. The 
framework should strive to not compromise scal-
ability for the sake of flexibility or transparency. 
3 Architecture  
At the high level, a typical Olympus application 
consists of a series of components connected in a 
classical, pipeline architecture, as illustrated by the 
bold components in Figure 1. The audio signal for 
the user utterance is captured and passed through a 
speech recognition module that produces a recog-
nition hypothesis (e.g., two p.m.). The recognition 
hypothesis is then forwarded to a language under-
standing component that extracts the relevant con-
cepts (e.g., [time=2p.m.]), and then through a 
confidence annotation module that assigns a confi-
dence score. Next, a dialog manager integrates this 
semantic input into the current context, and pro-
duces the next action to be taken by the system in 
the form of the semantic output (e.g., {request 
end_time}). A language generation module pro-
duces the corresponding surface form, which is 
subsequently passed to a speech synthesis module 
and rendered as audio.  
Galaxy communication infrastructure. While 
the pipeline shown in bold in Figure 1 captures the 
logical flow of information in the system, in prac-
tice the system components communicate via a 
centralized message-passing infrastructure ? Gal-
axy (Seneff et al, 1998). Each component is im-
plemented as a separate process that connects to a 
traffic router ? the Galaxy hub. The messages are 
sent through the hub, which forwards them to the 
appropriate destination. The routing logic is de-
scribed via a configuration script. 
Speech recognition. Olympus uses the Sphinx 
decoding engine (Huang et al, 1992). A recogni-
tion server captures the audio stream, forwards it to 
a set of parallel recognition engines, and collects 
the corresponding recognition results. The set of 
best hypotheses (one from each engine) is then 
forwarded to the language understanding compo-
nent. The recognition engines can also generate n-
best lists, but that process significantly slows down 
the systems and has not been used live. Interfaces 
to connect Sphinx-II and Sphinx-III engines, as 
well as a DTMF (touch-tone) decoder to the recog-
nition server are currently available. The individual 
recognition engines can use either n-gram- or 
grammar-based language models. Dialog-state 
specific as well as class-based language models are 
supported, and tools for constructing language and 
acoustic models from data are readily available. 
Most of the Olympus systems described in the next 
section use two gender-specific Sphinx-II recog-
nizers in parallel. Other parallel decoder configura-
tions can also be created and used.  
Language understanding is performed by 
Phoenix, a robust semantic parser (Ward and Issar, 
33
1994). Phoenix uses a semantic grammar to parse 
the incoming set of recognition hypotheses. This 
grammar is assembled by concatenating a set of 
reusable grammar rules that capture domain-
independent constructs like [Yes], [No], [Help], 
[Repeat], and [Number], with a set of domain-
specific grammar rules authored by the system de-
veloper. For each recognition hypothesis the output 
of the parser consists of a sequence of slots con-
taining the concepts extracted from the utterance.  
Confidence annotation. From Phoenix, the set 
of parsed hypotheses is passed to Helios, the con-
fidence annotation component. Helios uses features 
from different knowledge sources in the system 
(e.g., recognition, understanding, dialog) to com-
pute a confidence score for each hypothesis. This 
score reflects the probability of correct understand-
ing, i.e. how much the system trusts that the cur-
rent semantic interpretation corresponds to the 
user?s intention. The hypothesis with the highest 
score is forwarded to the dialog manager.  
Dialog management. Olympus uses the Raven-
Claw dialog management framework (Bohus and 
Rudnicky, 2003). In a RavenClaw-based dialog 
manager, the domain-specific dialog task is repre-
sented as a tree whose internal nodes capture the 
hierarchical structure of the dialog, and whose 
leaves encapsulate atomic dialog actions (e.g., ask-
ing a question, providing an answer, accessing a 
database). A domain-independent dialog engine 
executes this dialog task, interprets the input in the 
current dialog context and decides which action to 
engage next. In the process, the dialog manager 
may exchange information with other domain-
specific agents (e.g., application back-end, data-
base access, temporal reference resolution agent). 
Language generation. The semantic output of 
the dialog manager is sent to the Rosetta template-
based language generation component, which pro-
duces the corresponding surface form. Like the 
Phoenix grammar, the language generation tem-
plates are assembled by concatenating a set of pre-
defined, domain-independent templates, with 
manually authored task-specific templates.  
Speech synthesis. The prompts are synthesized 
by the Kalliope speech synthesis module. Kalliope 
can be currently configured to use Festival (Black 
and Lenzo, 2000), which is an open-source speech 
synthesis system, or Cepstral Swift (Cepstral 
2005), a commercial engine. Finally, Kalliope also 
supports the SSML markup language.  
Other components. The various components 
briefly described above form the core of the Olym-
pus dialog system framework. Additional compo-
nents have been created throughout the 
development of various systems, and, given the 
modularity of the architecture, can be easily re-
used. These include a telephony component, a text 
Parsing 
PHOENIX 
Recognition 
Server 
Lang. Gen 
ROSETTA 
Synthesis 
KALLIOPE 
? 
SPHINX SPHINX 
SPHINX 
Confidence 
HELIOS 
HUB 
Text I/O 
TTYSERVER 
Application 
Back-end 
Dialog. Mgr. 
RAVENCLAW 
Date-Time 
resolution 
Process 
Monitor 
Until what time 
would you like  
the room? 
{request end_time} 
Figure 1. The Olympus dialog system reference architecture (a typical system) 
two p.m. [time=2pm] [time=2pm]/0.65 
34
input-and-output interface, and a temporal refer-
ence resolution agent that translates complex date-
time expressions (including relative references, 
holidays, etc.) into a canonical form. Recently, a 
Jabber interface was implemented to support inter-
actions via the popular GoogleTalk internet mes-
saging system. A Skype speech client component 
is also available.  
Data Analysis. Last but not least, a variety of 
tools for logging, data processing and data ana-
lytics are also available as part of the framework. 
These tools have been used for a wide variety of 
tasks ranging from system monitoring, to trends 
analysis, to training of internal models. 
A key characteristic shared by all the Olympus 
components is the clear separation between do-
main-independent programs and domain-specific 
resources. This decoupling promotes reuse and 
lessens the system development effort. To build a 
new system, one can focus simply on developing 
resources (e.g., language model, grammar, dialog 
task specification, generation templates) without 
having to do any programming. On the other hand, 
since all components are open-source, any part of 
the system can be modified, for example to test 
new algorithms or compare approaches. 
4 Systems 
To date, the Olympus framework has been used to 
successfully build and deploy several spoken dia-
log systems spanning different domains and inter-
action types (see Table 1). Given the limited space, 
we discuss only three of these systems in a bit 
more detail: Let?s Go!, LARRI, and TeamTalk. 
More information about the other systems can be 
found in (RavenClaw-Olympus, 2007). 
4.1 Let?s Go! 
The Let?s Go! Bus Information System (Raux et al
2005; 2006) is a telephone-based spoken dialog 
system that provides access to bus schedules. In-
teraction with the system starts with an open 
prompt, followed by a system-directed phase 
where the user is asked the missing information. 
Each of the three or four pieces of information 
provided (origin, destination, time of travel, and 
optional bus route) is explicitly confirmed. The 
system knows 12 bus routes, and about 1800 place 
names. 
Originally developed as an in-lab research sys-
tem, Let?s Go! has been open to the general public 
since March, 2005. Outside of business hours, calls 
to the bus company are transferred to Let?s Go!, 
providing a constant flow of genuine dialogs 
(about 40 calls per weeknight and 70 per weekend 
night). As of March, 2007, a corpus of about 
30,000 calls to the system has been collected and 
partially transcribed and annotated. In itself, this 
publicly available corpus constitutes a unique re-
source for the community. In addition, the system 
itself has been modified for research experiments 
(e.g., Raux et al, 2005, Bohus et al, 2006). Be-
tween-system studies have been conducted by run-
ning several versions of the system in parallel and 
picking one at random for every call. We have re-
cently opened this system to researchers from other 
groups who wish to conduct their own experi-
ments. 
4.2 LARRI 
LARRI (Bohus and Rudnicky, 2002a) is a multi-
modal system for support of maintenance and re-
pair activities for F/A-18 aircraft mechanics. The 
system implements an Interactive Electronic Tech-
nical Manual.  
LARRI integrates a graphical user interface for 
easy visualization of dense technical information 
(e.g., instructions, schematics, video-streams) with 
a spoken dialog system that facilitates information 
access and offers assistance throughout the execu-
tion of procedural tasks. The GUI is accessible via 
a translucent head-worn display connected to a 
wearable client computer. A rotary mouse (dial) 
provides direct access to the GUI elements.  
After an initial log-in phase, LARRI guides the 
user through the selected task, which consists of a 
sequence of steps containing instructions, option-
ally followed by verification questions. Basic steps 
can include animations or short video sequences 
that can be accessed by the user through the GUI 
or through spoken commands. The user can also 
take the initiative and access the documentation, 
either via the GUI or by simple commands such as 
?go to step 15? or ?show me the figure?. 
The Olympus architecture was easily adapted 
for this mobile and multi-modal setting. The wear-
able computer hosts audio input and output clients, 
as well as the graphical user interface. The Galaxy 
hub architecture allows us to easily connect these 
35
components to the rest of the system, which runs 
on a separate server computer. The rotary-mouse 
events from the GUI are rendered as semantic in-
puts and are sent to Helios which in turn forwards 
the corresponding messages to the dialog manager.  
4.3 TeamTalk 
TeamTalk (Harris et al, 2005) is a multi-modal 
interface that facilitates communication between a 
human operator and a team of heterogeneous ro-
bots, and is designed for a multi-robot-assisted 
treasure-hunt domain. The human operator uses 
spoken language in concert with pen-gestures on a 
shared live map to elicit support from teams of ro-
bots. This support comes in the forms of mapping 
unexplored areas, searching explored areas for ob-
jects of interest, and leading the human to said ob-
jects. TeamTalk has been built as a fully functional 
interface to real robots, including the Pioneer 
P2DX and the Segway RMP. In addition, it can 
interface with virtual robots within the high-
fidelity USARSim (Balakirsky et al, 2006) simula-
tion environment. TeamTalk constitutes an excel-
lent platform for multi-agent dialog research. 
To build TeamTalk, we had to address two chal-
lenges to current architecture. The multi-
participant nature of the interaction required multi-
ple dialog managers; the live map with pen-
gestured references required a multi-modal integra-
tion. Again, the flexibility and transparency of the 
Olympus framework allowed for relatively simple 
solutions to both of these challenges. To accom-
modate multi-participant dialog, each robot in the 
domain is associated with its own RavenClaw-
based dialog manager, but all robots share the 
other Olympus components: speech recognition, 
language understanding, language generation and 
speech synthesis. To accommodate the live map 
GUI, a Galaxy server was built in Java that could 
send the user?s inputs to Helios and receive outputs 
from RavenClaw. 
5 Research 
The Olympus framework, along with the systems 
developed using it, provides a robust basis for re-
search in spoken language interfaces. In this sec-
tion, we briefly outline three current research 
efforts supported by this architecture. Information 
about other supported research can be found in 
(RavenClaw-Olympus, 2007). 
5.1 Error handling  
A persistent and important problem in today?s spo-
ken language interfaces is their lack of robustness 
when faced with understanding errors. This prob-
lem stems from current limitations in speech rec-
ognition, and appears across most domains and 
interaction types. In the last three years, we con-
ducted research aimed at improving robustness in 
spoken language interfaces by: (1) endowing them 
with the ability to accurately detect errors, (2) de-
System name Domain / Description Genre 
RoomLine 
(Bohus and Rudnicky 2005) 
telephone-based system that provides support for conference 
room reservation and scheduling within the School of Com-
puter Science at CMU. 
information access (mixed 
initiative) 
Let?s Go! Public 
(Raux et al2005) 
telephone-based system that provides access to bus schedule 
information in the greater Pittsburgh area. 
information access 
(system initiative) 
LARRI 
(Bohus and Rudnicky 2002) 
multi-modal system that provides assistance to F/A-18 aircraft 
personnel during maintenance tasks. 
multi-modal task guidance 
and procedure browsing 
Intelligent Procedure  
Assistant 
(Aist et al2002) 
early prototype for a multi-modal system aimed at providing 
guidance and support to the astronauts on the International 
Space Station during the execution of procedural tasks and 
checklists. 
multi-modal task guidance 
and procedure browsing 
TeamTalk 
(Harris et al2005) 
multi-participant spoken language command-and-control inter-
face for a team of robots in the treasure-hunt domain. 
multi-participant command-
and-control 
VERA telephone-based taskable agent that can be instructed to de-liver messages to a third party and make wake-up calls. 
voice mail / message deliv-
ery 
Madeleine text-based dialog system for medical diagnosis. diagnosis 
ConQuest 
(Bohus et al2007) 
telephone-based spoken dialog system that provides confer-
ence schedule information. 
information access 
(mixed-initiative) 
RavenCalendar 
(Stenchikova et al2007). 
multimodal dialog system for managing personal calendar 
information, such as meetings, classes, appointments and 
reminders (uses Google Calendar as a back-end)  
information access and 
scheduling 
 Table 1. Olympus-based spoken dialog systems (shaded cells indicate deployed systems) 
36
veloping a rich repertoire of error recovery strate-
gies and (3) developing scalable, data-driven ap-
proaches for building error recovery policies1. Two 
of the dialog systems from Table 1 (Let?s Go! and 
RoomLine) have provided a realistic experimental 
platform for investigating these issues and evaluat-
ing the proposed solutions.   
With respect to error detection, we have devel-
oped tools for learning confidence annotation 
models by integrating information from multiple 
knowledge sources in the system (Bohus and Rud-
nicky, 2002b). Additionally, Bohus and Rudnicky 
(2006) proposed a data-driven approach for con-
structing more accurate beliefs in spoken language 
interfaces by integrating information across multi-
ple turns in the conversation. Experiments with the 
RoomLine system showed that the proposed belief 
updating models led to significant improvements 
(equivalent with a 13.5% absolute reduction in 
WER) in both the effectiveness and the efficiency 
of the interaction.  
With respect to error recovery strategies, we 
have developed and evaluated a large set of strate-
gies for handling misunderstandings and non-
understandings (Bohus and Rudnicky, 2005). The 
strategies are implemented in a task-decoupled 
manner in the RavenClaw dialog management 
framework. 
Finally, in (Bohus et al, 2006) we have pro-
posed a novel online-learning based approach for 
building error recovery policies over a large set 
of non-understanding recovery strategies. An em-
pirical evaluation conducted in the context of the 
Let?s Go! system showed that the proposed ap-
proach led to a 12.5% increase in the non-
understanding recovery rate; this improvement was 
attained in a relatively short (10-day) time period.  
The models, tools and strategies developed 
throughout this research can and have been easily 
reused in other Olympus-based systems. 
5.2 Multi-participant conversation  
Conversational interfaces are generally built for 
one-on-one conversation. This has been a workable 
assumption for telephone-based systems, and a 
useful one for many single-purpose applications. 
However this assumption will soon become 
strained as a growing collection of always-
                                                          
1
 A policy specifies how the system should choose between 
different recovery strategies at runtime.  
available agents (e.g., personal trainers, pedestrian 
guides, or calendar systems) and embodied agents 
(e.g., appliances and robots) feature spoken lan-
guage interfaces. When there are multiple active 
agents that wish to engage in spoken dialog, new 
issues arise. On the input side, the agents need to 
be able to identify the addressee of any given user 
utterance. On the output side, the agents need to 
address the problem of channel contention, i.e., 
multiple participants speaking over each other. 
Two architectural solutions can be envisioned: 
(1) the agents share a single interface that under-
stands multi-agent requirements, or (2) each agent 
uses its own interface and handles multi-participant 
behavior. Agents that provide different services 
have different dialog requirements, and we believe 
this makes a centralized interface problematic. Fur-
thermore, the second solution better fits human 
communication behavior and therefore is likely to 
be more natural and habitable.  
TeamTalk is a conversational system that fol-
lows the second approach: each robot has its own 
dialog manager. Processed user inputs are sent to 
all dialog managers in the system; each dialog 
manager decides based on a simple algorithm 
(Harris et al, 2004) whether or not the current in-
put is addressed to it. If so, an action is taken. Oth-
erwise the input is ignored; it will be processed and 
responded to by another robot. On the output side, 
to address the channel contention problem, each 
RavenClaw output message is augmented with in-
formation about the identity of the robot that gen-
erated it. The shared synthesis component queues 
the messages and plays them back sequentially 
with the corresponding voice. 
We are currently looking into two additional 
challenges related to multi-participant dialog. We 
are interested in how to address groups and sub-
groups in addition to individuals of a group, and 
we are also interested in how to cope with multiple 
humans in addition to multiple agents. Some ex-
periments investigating solutions to both of these 
issues have been conducted. Analysis of the results 
and refinements of these methods are ongoing. 
5.3 Timing and turn-taking  
While a lot of research has focused on higher lev-
els of conversation such as natural language under-
standing and dialog planning, low-level inter-
actional phenomena such as turn-taking have not 
37
received as much attention. As a result, current 
systems either constrain the interaction to a rigid 
one-speaker-at-a-time style or expose themselves 
to interactional problems such as inappropriate 
delays, spurious interruptions, or turn over-taking 
(Raux et al, 2006). To a large extent, these issues 
stem from the fact that in common dialog architec-
tures, including Olympus, the dialog manager 
works asynchronously from the real world (i.e., 
utterances and actions that are planned are as-
sumed to be executed instantaneously). This means 
that user barge-ins and backchannels are often in-
terpreted in an incorrect context, which leads to 
confusion, unexpected user behavior and potential 
dialog breakdowns. Additionally, dialog systems? 
low-level interactional behavior is generally the 
result of ad-hoc rules encoded in different compo-
nents that are not precisely coordinated. 
In order to investigate and resolve these is-
sues, we are currently developing version 2 of the 
Olympus framework. In addition to all the compo-
nents described in this paper, Olympus 2 features 
an Interaction Manager which handles the precise 
timing of events perceived from the real world 
(e.g., user utterances) and of system actions (e.g., 
prompts). By providing an interface between the 
actual conversation and the asynchronous dialog 
manager, Olympus 2 allows a more reactive behav-
ior without sacrificing the powerful dialog man-
agement features offered by RavenClaw. Olympus 
2 is designed so that current Olympus-based sys-
tems can be upgraded with minimal effort.  
This novel architecture, initially deployed in 
the Let?s Go system, will enable research on turn-
taking and other low-level conversational phenom-
ena. Investigations within the context of other ex-
isting systems, such as LARRI and TeamTalk, will 
uncover novel challenges and research directions.  
6 Discussion and conclusion 
The primary goal of the Olympus framework is to 
enable research that leads to technological and sci-
entific advances in spoken language interfaces.  
Olympus is however by no means a singular ef-
fort. Several other toolkits for research and devel-
opment are available to the community. They 
differ on a number of dimensions, such as objec-
tives, scientific underpinnings, as well as techno-
logical and implementation aspects. Several 
toolkits, both commercial, e.g., TellMe, BeVocal, 
and academic, e.g., Ariadne (2007), SpeechBuilder 
(Glass et al, 2004), and the CSLU toolkit (Cole, 
1999), are used for rapid development. Some, e.g., 
CSLU and SpeechBuilder, have also been used for 
educational purposes. And yet others, such as 
Olympus, GALATEEA (Kawamoto et al, 2002) 
and DIPPER (Bos et al, 2003) are primarily used 
for research. Different toolkits rely on different 
theories and dialog representations: finite-state, 
slot-filling, plan-based, information state-update. 
Each toolkit balances tradeoffs between complex-
ity, ease-of-use, control, robustness, flexibility, etc. 
We believe the strengths of the Olympus 
framework lie not only in its current components, 
but also in its open, transparent, and flexible na-
ture. As we have seen in the previous sections, 
these characteristics have allowed us to develop 
and deploy practical, real-world systems operating 
in a broad spectrum of domains. Through these 
systems, Olympus provides an excellent basis for 
research on a wide variety of spoken dialog issues. 
The modular construction promotes the transfer 
and reuse of research contributions across systems.  
While desirable, an in-depth understanding of 
the differences between all these toolkits remains 
an open question. We believe that an open ex-
change of experiences and resources across toolkits 
will create a better understanding of the current 
state-of-the-art, generate new ideas, and lead to 
better systems for everyone. Towards this end, we 
are making the Olympus framework, as well as a 
number of systems and dialog corpora, freely 
available to the community. 
Acknowledgements 
We would like to thank all those who have brought 
contributions to the components underlying the 
Olympus dialog system framework. Neither Olym-
pus nor the dialog systems discussed in this paper 
would have been possible without their help. We 
particularly wish to thank Alan W Black for his 
continued support and advice. Work on Olympus 
components and systems was supported in part by 
DARPA, under contract NBCH-D-03-0010, Boe-
ing, under contract CMU-BA-GTA-1, and the US 
National Science Foundation under grant number 
0208835. Any opinions, findings, and conclusions 
or recommendations expressed in this material are 
those of the authors and do not necessarily reflect 
the views of the funding agencies.  
38
References  
Aist, G., Dowding, J., Hockey, B.A., Rayner, M., 
Hieronymus, J., Bohus, D., Boven, B., Blaylock, N., 
Campana, E., Early, S., Gorrell, G., and Phan, S., 
2003. Talking through procedures: An intelligent 
Space Station procedure assistant, in Proc. of EACL-
2003, Budapest, Hungary 
Ariadne, 2007, The Ariadne web-site, as of January 
2007, http://www.opendialog.org/. 
Balakirsky, S., Scrapper, C., Carpin, S., and Lewis, M. 
2006. UsarSim: providing a framework for multi-
robot performance evaluation, in Proc. of PerMIS. 
Black, A. and Lenzo, K., 2000. Building Voices in the 
Festival Speech System, http://festvox.org/bsv/, 2000. 
Bohus, D., Grau Puerto, S., Huggins-Daines, D., Keri, 
V., Krishna, G., Kumar, K., Raux, A., Tomko, S., 
2007. Conquest ? an Open-Source Dialog System for 
Conferences, in Proc. of HLT 2007, Rochester, USA. 
Bohus, D., Langner, B., Raux, A., Black, A., Eskenazi, 
M., Rudnicky, A.  2006.  Online Supervised Learning 
of Non-understanding Recovery Policies, in Proc. of 
SLT-2006, Aruba.  
Bohus, D., and Rudnicky, A.  2006.  A K-hypotheses + 
Other Belief Updating Model, in Proc. of the AAAI 
Workshop on Statistical and Empirical Methods in 
Spoken Dialogue Systems, 2006. 
Bohus, D., and Rudnicky, A.,  2005.  Sorry I didn?t 
Catch That: An Investigation of Non-understanding 
Errors and Recovery Strategies, in Proc. of SIGdial-
2005, Lisbon, Portugal. 
Bohus, D., and Rudnicky, A., 2003. RavenClaw: Dialog 
Management Using Hierarchical Task Decomposi-
tion and an Expectation Agenda, in Proc. of Eu-
rospeech 2003, Geneva, Switzerland. 
Bohus, D., and Rudnicky, A., 2002a. LARRI: A Lan-
guage-based Maintenance and Repair Assistant, in 
Proc. of IDS-2002, Kloster Irsee, Germany. 
Bohus, D., and Rudnicky, A., 2002b. Integrating Multi-
ple Knowledge Sources in the CMU Communicator 
Dialog System, Technical Report CMU-CS-02-190. 
Bos, J., Klein, E., Lemon, O., and Oka, T., 2003. 
DIPPER: Description and Formalisation of an In-
formation-State Update Dialogue System Architec-
ture, in Proc. of SIGdial-2003, Sapporo, Japan 
Cepstral, LLC, 2005. SwiftTM: Small Footprint Text-to-
Speech Synthesizer, http://www.cepstral.com. 
Cole, R., 1999. Tools for Research and Education in 
Speech Science, in Proc. of the International Confer-
ence of Phonetic Sciences, San Francisco, USA. 
Glass, J., Weinstein, E., Cyphers, S., Polifroni, J., 2004. 
A Framework for Developing Conversational Inter-
faces, in Proc. of CADUI, Funchal, Portugal. 
Harris, T. K., Banerjee, S., Rudnicky, A., Sison, J. 
Bodine, K., and Black, A. 2004. A Research Platform 
for Multi-Agent Dialogue Dynamics, in Proc. of The 
IEEE International Workshop on Robotics and Hu-
man Interactive Communications, Kurashiki, Japan. 
Harris, T. K., Banerjee, S., Rudnicky, A. 2005. Hetero-
genous Multi-Robot Dialogues for Search Tasks, in 
AAAI Spring Symposium: Dialogical Robots, Palo 
Alto, California. 
Huang, X., Alleva, F., Hon, H.-W., Hwang, M.-Y., Lee, 
K.-F. and Rosenfeld, R., 1992. The SPHINX-II 
Speech Recognition System: an overview, in Com-
puter Speech and Language, 7(2), pp 137-148, 1992. 
Kawamoto, S.,  Shimodaira, H., Nitta, T., Nishimoto, 
T., Nakamura, S., Itou, K., Morishima, S., Yotsukura, 
T., Kai, A., Lee, A., Yamashita, Y., Kobayashi, T., 
Tokuda, K., Hirose, K., Minematsu, N., Yamada, A., 
Den, Y., Utsuro, T., and Sagayama, S., 2002. Open-
source software for developing anthropomorphic 
spoken dialog agent, in Proc. of PRICAI-02, Interna-
tional Workshop on Lifelike Animated Agents. 
Raux, A., Langner, B., Bohus, D., Black, A., and Eske-
nazi, M.  2005, Let's Go Public! Taking a Spoken 
Dialog System to the Real World, in Proc. of Inter-
speech 2005, Lisbon, Portugal. 
Raux, A., Bohus, D., Langner, B., Black, A., and Eske-
nazi, M. 2006 Doing Research on a Deployed Spoken 
Dialogue System: One Year of Let's Go! Experience, 
in Proc. of Interspeech 2006, Pittsburgh, USA. 
RavenClaw-Olympus web page, as of January 2007: 
http://www.ravenclaw-olympus.org/. 
Rudnicky, A., Thayer, E., Constantinides, P., Tchou, C., 
Shern, R., Lenzo, K., Xu W., and Oh, A., 1999. Cre-
ating natural dialogs in the Carnegie Mellon Com-
municator system, in Proc. of Eurospeech 1999. 
Seneff, S., Hurley, E., Lau, R., Pao, C., Schmid, P., and 
Zue V. 1998 Galaxy-II: A reference architecture for 
conversational system development, in Proc. of 
ICSLP98, Sydney, Australia. 
Stenchikova, S., Mucha, B., Hoffman, S., Stent, A., 
2007. RavenCalendar: A Multimodal Dialog System 
for Managing A Personal Calendar, in Proc. of HLT 
2007, Rochester, USA.  
Ward, W., and Issar, S., 1994. Recent improvements in 
the CMU spoken language understanding system, in 
Proc. of the ARPA Human Language Technology 
Workshop, pages 213?216, Plainsboro, NJ. 
39
Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, pages 1?10,
Columbus, June 2008. c?2008 Association for Computational Linguistics
Optimizing Endpointing Thresholds using Dialogue
Features in a Spoken Dialogue System
Antoine Raux and Maxine Eskenazi
{antoine,max}@cs.cmu.edu
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
Abstract
This paper describes a novel algorithm to dy-
namically set endpointing thresholds based on
a rich set of dialogue features to detect the end
of user utterances in a dialogue system. By
analyzing the relationship between silences in
user?s speech to a spoken dialogue system and
a wide range of automatically extracted fea-
tures from discourse, semantics, prosody, tim-
ing and speaker characteristics, we found that
all features correlate with pause duration and
with whether a silence indicates the end of the
turn, with semantics and timing being the most
informative. Based on these features, the pro-
posed method reduces latency by up to 24%
over a fixed threshold baseline. Offline evalu-
ation results were confirmed by implementing
the proposed algorithm in the Let?s Go system.
1 Introduction
1.1 Responsiveness in Dialogue
Although the quality of speech technologies has im-
proved drastically and spoken interaction with ma-
chines is becoming a part of the everyday life of
many people, dialogues with artificial agents still
fall far short of their human counterpart in terms of
both comfort and efficiency. Besides lingering prob-
lems in speech recognition and understanding, Ward
et al(Ward et al, 2005) identified turn-taking is-
sues, specifically responsiveness, as important short-
comings. Dialogues with artificial agents are typi-
cally rigid, following a strict one-speaker-at-a-time
structure with significant latencies between turns.
In a previous paper, we concurred with these find-
ings when analyzing issues with the Let?s Go system
(Raux et al, 2006). In contrast, empirical studies
of conversation have shown that human-human dia-
logues commonly feature swift exchanges with lit-
tle or no gap between turns, or even non-disruptive
overlap (Jaffe and Feldstein, 1970; Sacks et al,
1974). According to Conversation Analysis and
psycholinguistic studies, responsiveness in human
conversations is possible because participants in the
conversation exchange cues indicating when a turn
might end, and are able to anticipate points at which
they can take over the floor smoothly. Much re-
search has been devoted to finding these cues, lead-
ing to the identification of many aspects of language
and dialogue that relate to turn-taking behavior, in-
cluding syntax (Sacks et al, 1974; Ford and Thomp-
son, 1996; Furo, 2001), prosody (Duncan, 1972;
Orestro?m, 1983; Chafe, 1992; Ford and Thompson,
1996; Koiso et al, 1998; Furo, 2001), and seman-
tics (Orestro?m, 1983; Furo, 2001). However, re-
garding this last aspect, Orestrom notes about his
corpus that ?there is no simple way to formaliz-
ing a semantic analysis of this conversational mate-
rial?. This difficulty in formalizing higher levels of
conversation might explain the relatively low inter-
est that conversational analysts have had in seman-
tics and discourse. Yet, as conversational analysts
focused on micro-levels of dialogue such as turn-
taking, computational linguists uncovered and for-
malized macro-level dialogue structure and devised
well-defined representations of semantics for at least
some forms of dialogues (Allen and Perrault, 1980;
Grosz and Sidner, 1986; Clark, 1996), which have in
turn been implemented in spoken dialogue systems
(Rich and Sidner, 1998; Allen et al, 2005).
1
1.2 Current Approaches to Turn-Taking in
Spoken Dialogue Systems
Unfortunately, while socio- and psycho-linguists re-
vealed the complexity of conversational turn-taking
behavior, designers of practical spoken dialogue sys-
tems have stuck to a simplistic approach to end-of-
turn detection (hereafter endpointing). Typically, si-
lences in user speech are detected using a low-level
Voice Activity Detector (VAD) and a turn is consid-
ered finished once a silence lasts longer than a fixed
threshold. This approach has the advantage of being
simple, only relying on easily computable low-level
features. However, it leads to suboptimal behavior
in many instances. First, False Alarms (FA) hap-
pen when a pause lasts longer than the threshold and
gets wrongly classified as a gap1. Second, latency
occurs at every gap, because the system must wait
for the duration of the threshold before classifying a
silence as gap. When setting the threshold, system
designers must consider the trade-off between these
two issues: setting a low threshold reduces latency
but increases FA rate, while setting a high threshold
reduces FA rate but increases latency.
To help overcome the shortcomings of the single-
threshold approach, several researchers have pro-
posed to exploit various features. Sato et al(Sato
et al, 2002) used decision trees to classify pauses
longer than 750 ms as gap or pause. By using fea-
tures from semantics, syntax, dialogue state, and
prosody, they were able to improve the classification
accuracy from a baseline of 76.2% to 83.9%. While
this important study shows encouraging results on
the value of using various sources of information in
a dialogue system, the proposed approach (classify-
ing long silences) is not completely realistic (what
happens when a gap is misclassified as a pause?) and
does not attempt to optimize latency. An extension
to this approach was proposed in (Takeuchi et al,
2004), in which a turn-taking decision is made every
100 ms during pauses. However, in this latter work
the features are limited to timing, prosody, and syn-
tax (part-of-speech). Also the reported classification
results, with F-measures around 50% or below do
not seem to be sufficient for practical use.
1We use the terminology from (Sacks et al, 1974) where a
pause is a silence within a turn while a gap is a silence between
turns. We use the term silence to encompass both types.
Similarly, Ferrer and her colleagues (Ferrer et al,
2003) proposed the use of multiple decision trees,
each triggered at a specific time in the pause, to de-
cide to either endpoint or defer the decision to the
next tree, unless the user resumes speaking. Using
features like vowel duration or pitch for the region
immediately preceding the silence, combined with a
language model that predicts gaps based on the pre-
ceding words, Ferrer et alare able shorten latency
while keeping the FA rate constant. On a corpus
of recorded spoken dialogue-like utterances (ATIS),
they report reductions of up to 81% for some FA
rates. While very promising, this approach has sev-
eral disadvantages. First it relies on a small set of
possible decision points for each pause, preventing
fine optimization between them. Second, the trees
are trained on increasingly smaller datasets requir-
ing smoothing of the tree scores to compensate for
poor training of the later trees (which are trained
on increasingly small subsets of pauses from the
training set). Finally, and perhaps most importantly,
these authors have investigated prosodic and lexical
features, but not other aspects of dialogue, such as
discourse structure, timing, and semantics.
In this paper, we propose a new approach to end-
pointing that directly optimizes thresholds using au-
tomatically extracted dialogue features ranging from
discourse to timing and prosody. Section 2 out-
lines the proposed algorithm. Section 3 describes
the analysis of the relationship between silences and
a wide range of features available to a standard spo-
ken dialogue system (hereafter dialogue features).
Evaluation results, both offline and in the deployed
Let?s Go system are given in Section 4.
2 Dynamic Endpointing Threshold
Decision Trees
2.1 Overview
One issue with current approaches to endpointing
is that they rely on binary gap/pause classifiers and
the relationship between optimizing for classifica-
tion accuracy vs optmizing to minimize latency is
unclear. Also, the performance we obtained when
applying classification-based approaches to the Let?s
Go data was disappointing. The accuracy of the clas-
sifiers was not sufficient for practical purposes, even
with the improvements proposed by (Ferrer et al,
2
2003). We hypothesize that the discrepancy between
these results and the good performances reported by
others is due to the noisiness of the Let?s Go data
(see Section 3.1.1). To overcome these issues, we
propose a method that directly optimizes endpoint-
ing thresholds using a two-stage process. First, si-
lences are clustered based on dialogue features so
as to create groups of silences with similar proper-
ties. Second, a single threshold is set for each clus-
ter, so as to minimize the overall latency at a given
false alarm rate. The result of the training process
is thus a decision tree on dialogue features that con-
tains thresholds at its leaves. At runtime, every time
a silence is detected, the dialogue system runs the
decision tree and sets its endpointing threshold ac-
cordingly. The following sections describe the two
training stages.
2.2 Feature-based Silence Clustering
The goal of the first stage of training is to clus-
ter silences with a similar FA rate/latency trade-
off. The intuition is that we would like to generate
low-threshold clusters, which contain mostly gaps
and short pauses, and clusters where long pauses
would be concentrated with no or very few gaps,
allowing to set high thresholds that reduce cut-in
rate without hurting overall latency. We used a
standard top-down clustering algorithm that exhaus-
tively searches binary splits of the data based on fea-
ture values. The split that yields the minimal overall
cost is kept, where the cost Cn of cluster Kn is de-
fined by the following function:
Cn = Gn ?
?
1
|K|
?
p?K
Duration(p)2 (1)
where Gn the number of gaps in Kn and
Duration(p) the duration of a pause p, set to zero
for gaps. While other cost functions are possible, the
intuition behind this formula is that it captures both
the cluster?s gap ratio (first factor) and its pause du-
ration distribution (second factor: root mean square
of pause duration). The splitting process is repeated
recursively until the reduction in cost between the
original cost and the sum of the costs of the two split
clusters falls below a certain threshold. By minimiz-
ing C(K), the clustering algorithm will find ques-
tions that yield clusters with either a small Gn, i.e.
mostly pauses, or a small root mean square pause
duration. Ultimately, at the leaves of the tree are sets
of silences that will share the same threshold.
2.3 Cluster Threshold Optimization
Given the clusters generated by the first phase, the
goal of the second phase is to find a threshold for
each cluster so that the overall latency is minimized
at a given FA rate. Under the assumption that pause
durations follow an exponential distribution, which
is supported by previous work and our own data (see
Section 3.2), we show in Figure 3 in appendix that
there is a unique set of thresholds that minimizes la-
tency and that the threshold for any cluster n is given
by:
?n =
?n ? log(?n ?
E??n
P
?n
)
Gn
(2)
where ?n and ?n can be estimated from the data.
3 Silences and Dialogue Features
3.1 Overview of the Data
3.1.1 The Let?s Go Corpus
Let?s Go is a telephone-based spoken dialogue
system that provides bus schedule information for
the Pittsburgh metropolitan area. It is built on the
Olympus architecture (Bohus et al, 2007), using the
RavenClaw dialogue management framework, and
the Apollo interaction manager (Raux et al, 2007)
as core components. Outside of business hours
callers to the bus company?s customer service are
offered the option to use Let?s Go. All calls are
recorded and extensively logged for further analy-
sis. The corpus used for this study was collected
between December 26, 2007 and January 25, 2008,
with a total of 1326 dialogues, and 18013 user turns.
Of the calls that had at least 4 user turns, 73% were
complete, meaning that the system provided some
schedule information to the user.
While working on real user data has its advan-
tages (large amounts of data, increased validity of
the results), it also has its challenges. In the case of
Let?s Go, users call from phones of varying quality
(cell phones and landlines), often with background
noises such as cars, infant cries, loud television sets,
etc. The wide variability of the acoustic conditions
makes any sound processing more prone to error
3
than on carefully recorded corpora. For example, as
reported in (Raux et al, 2005), the original speech
recognizer had been found to yield a 17% word error
rate on a corpus of dialogues collected by recruit-
ing subjects to call the system from an office. On
the live Let?s Go data, that same recognizer had a
68% WER. After acoustic and language model re-
training/adaptation, that number was brought down
to about 30% but it is still a testimony to the diffi-
culty of obtaining robust features, particularly from
acoustics.
3.1.2 Correcting Runtime Endpointing Errors
Let?s Go uses a GMM-based VAD trained on pre-
viously transcribed dialogues. Endpointing deci-
sions are based on a fixed 700 ms threshold on the
duration of the detected silences. One issue when
analyzing pause distributions from the corpus is that
observed user behavior was affected by system?s be-
havior at runtime. Most notably, because of the fixed
threshold, no recorded pause lasts more than 700 ms.
To compensate for that, we used a simple heuristic
to rule some online endpointing decisions as erro-
neous. If a user turn is followed within 1200 ms by
another user turn, we consider these two turns to be
in fact a single turn, unless the first turn was a user
barge-in. This heuristic was established by hand-
labeling 200 dialogues from a previous corpus with
endpointing errors (i.e. each turn was annotated as
correctly or incorrectly endpointed). On this dataset,
the heuristic has a precision of 70.6% and a recall of
75.5% for endpointing errors. Unless specified, all
subsequent results are based on this modified cor-
pus.
3.2 Turn-Internal Pause Duration Distribution
Overall there were 9563 pauses in the corpus, which
amounts to 0.53 pauses per turn. The latency / FA
rate trade-off for the corpus is plotted in Figure 1.
This curve follows an exponential function (the R2
on the linear regression of latency on Log(FA) is
0.99). This stems from the fact that pause duration
approximately follows an exponential distribution,
which has been observed by others in the past (Jaffe
and Feldstein, 1970; Lennes and Anttila, 2002).
One consequence of the exponential-like distribu-
tion is that short pauses strongly dominate the distri-
bution. We decided to exclude silences shorter than
Figure 1: Overall False Alarm / Latency trade-off in the
Let?s Go corpus. The dashed line represents a fitted curve
of the form FA = e?+??Latency .
200 ms from most of the following analysis for two
reasons: 1) they are more prone to voice activity
detection errors or short non-pause silences within
speech (e.g. unvoiced stop closure), and 2) in order
to apply the results found here to online endpointing
by the system, some amount of time is required to
detect the silence and compute necessary features,
making endpointing decisions on such very short si-
lences impractical. Once short silences have been
excluded, there are 3083 pauses in the corpus, 0.17
per turn.
3.3 Relationship Between Dialogue Features
and Silence Distributions
3.3.1 Statistical Analysis
In order to get some insight into the interaction
of the various aspects of dialogue and silence char-
acteristics, we investigated a number of features au-
tomatically extracted from the dialogue recordings
and system logs. Each feature is used to split the
set of silences into two subsets. For nominal fea-
tures, all possible splits of one value vs all the others
are tested, while for continuous and ordinal features,
we tried a number of thresholds and report the one
that yielded the strongest results. In order to avoid
extreme cases that split the data into one very large
and one very small set, we excluded all splits where
either of the two sets had fewer than 1000 silences.
All the investigated splits are reported in Appendix,
in Table 1 and 2. We compare the two subsets gen-
erated by each possible split in terms of two metrics:
? Gap Ratio (GR), defined as the proportion of
4
gaps among all silences of a given set. We re-
port the absolute difference in GR between the
two sets, and use chi-square in a 2x2 design
(pause vs gap and one subset vs the other) to
test for statistical significance at the 0.01 level,
using Bonferroni correction to compensate for
multiple testings.
? Mean pause duration. The strength of the in-
teraction is shown by the difference in mean
pause duration, and we use Mann Whitney?s
Rank Sum test for statistical significance, again
at the 0.01 level, using Bonferroni correction.
We group features into five categories: discourse,
semantics, prosody, turn-taking, and speaker charac-
teristics, described in the following sections.
3.3.2 Discourse Structure
Discourse structure is captured by the system?s di-
alogue act immediately preceding the current user
turn. In the Let?s Go dialogues, 97.9% of sys-
tem dialogue acts directly preceding user turns are
questions2. Of these, 13% are open questions (e.g.
?What can I do for you??), 39% are closed ques-
tions (e.g. ?Where are you leaving from??) and 46%
are confirmation requests (e.g. ?Leaving from the
airport. Is this correct??)3. There are many more
pauses in user responses to open questions than to
the other types (cf Table 1). One explanation is that
user answers to open questions tend to be longer
(2046 ms on average, to be contrasted with 1268 ms
for turns following closed questions and 819 ms for
responses to confirmation questions). Conversely,
confirmation questions lead to responses with sig-
nificantly fewer pauses. 78% of such turns con-
tained only one word, single YES and NO answers
accounting for 81% of these one-word responses,
which obviously do not lend themselves to pauses.
Discourse context also has an effect on pause dura-
tions, albeit a weak one, with open questions leading
to turns with shorter pauses. One possible explana-
tion for this is that pauses after closed and confirma-
tion questions tend to reflect more hesitations and/or
2The remaining 2.1% belong to other cases such as the user
barging in right after the system utters a statement.
3The high number of confirmations comes from the fact that
Let?s Go is designed to ask the user to explicitly confirm every
concept.
confusion on the user?s side, whereas responses to
open questions also have pauses in the normal flow
of speech.
3.3.3 Semantics
Semantic features are based on partial speech
recognition results and on their interpretation in the
current dialogue context. We use the most recent
recognition hypothesis available at the time when
the silence starts, parse it using the system?s standard
parser and grammar, and match the parse against the
?expectation agenda? that RavenClaw (Bohus and
Rudnicky, 2003) maintains. The expectation level
of a partial utterance indicates how well it fits in the
current dialogue context. A level of 0 means that
the utterance can be interpreted as a direct answer
to the last system prompt (e.g. a ?PLACE? con-
cept as an answer to ?Where are you leaving from??,
a ?YES? or a ?NO? after a confirmation question).
Higher levels correspond to utterances that fit in a
broader dialogue context (e.g. a place name after
the system asks ?Leaving from the airport. Is this
correct??, or ?HELP? in any context). Finally, non-
understandings, which do not match any expecta-
tion, are given a matching level of +?.
Expectation level is strongly related to both fi-
nality and pause duration. Pauses following par-
tial utterances of expectation level 0 are signifi-
cantly more likely to be gaps than those matching
any higher level. Also, very unexpected partial ut-
terances (and non-understandings) contain shorter
pauses than more expected ones. Another indica-
tive feature for finality is the presence of a posi-
tive marker (i.e. a word like ?YES? or ?SURE?) in
the partial utterance. Utterances that contain such a
marker are more likely to be finished than others. In
contrast, the effect of negative markers is not signif-
icant. This can be explained by the fact that nega-
tive responses to confirmation often lead to longer
corrective utterances more prone to pauses. Indeed,
91% of complete utterances that contain a positive
marker are single-word, against 67% for negative
markers.
3.3.4 Prosody
We extracted three types of prosodic features:
acoustic energy of the last vowel, pitch of the last
voiced region, and duration of the last vowel. Vowel
5
location and duration were estimated by performing
phoneme alignment with the speech recognizer. Du-
ration was normalized to account for both vowel and
speaker identity. Energy was computed as the log-
transformed signal intensity on 10ms frames. Pitch
was extracted using the Snack toolkit (Sjolander,
2004), also at 10ms intervals. For both energy and
pitch, the slope of the contour was computed by lin-
ear regression, and the mean value was normalized
by Z-transformation using statistics of the dialogue-
so-far. As a consequence, all threshold values for
means are expressed in terms of standard deviations
from the current speaker?s mean value.
Vowel energy, both slope and mean, yielded the
highest correlation with silence finality, although it
did not rank as high as features from other cate-
gories. As expected, vowels immediately preced-
ing gaps tend to have lower and falling intensity,
whereas rising intensity makes it more likely that the
turn is not finished. On the other hand, extremely
high pitch is a strong cue to longer pauses, but only
happen in 5.6% of the pauses.
3.3.5 Timing
Timing features, available from the Interaction
Manager, provide the strongest cue to finality. The
longer the on-going turn has been, the less likely it is
that the current silence is a gap. This is true both in
terms of time elapsed since the beginning of the ut-
terance and number of pauses observed so far. This
latter feature also correlates well with mean pause
duration, earlier pauses of a turn tending to be longer
than later ones.
3.3.6 Speaker Characteristics
These features correspond to the observed pausal
behavior so far in the dialogue. The idea is that dif-
ferent speakers follow different patterns in the way
they speak (and pause), and that the system should
be able to learn these patterns to anticipate future
behavior. Specifically, we look at the mean num-
ber of pauses per utterance observed so far, and the
mean pause duration observed so far for the current
dialogue. Both features correlate reasonably well
with silence finality: a higher mean duration indi-
cates that upcoming silences are also less likely to
be final, so does a higher mean number of pauses
per turn.
3.4 Discussion
What emerges from the analysis above is that fea-
tures from all aspects of dialogue provide informa-
tion on silence characteristics. While most previous
research has focused on prosody as a cue to detect
the end of utterances, timing, discourse, semantic
and previously observed silences appear to corre-
late more strongly with silence finality in our corpus.
This can be partly explained by the fact that prosodic
features are harder to reliably estimate on noisy data
and that prosodic features are in fact correlated to
higher levels of dialogue such as discourse and se-
mantics. However, we believe our results make a
strong case in favor of a broader approach to turn-
taking for conversational agents, making the most
of all the features that are readily available to such
systems. Indeed, particularly in constrained systems
like Let?s Go, higher level features like discourse
and semantics might be more robust to poor acoustic
conditions than prosodic features. Still, our findings
on mean pause durations suggest that prosodic fea-
tures might be best put to use when trying to pre-
dict pause duration, or whether a pause will occur
or not. The key to more natural and responsive di-
alogue systems lies in their ability to combine all
these features in order to make prompt and robust
turn-taking decisions.
4 Evaluation of Threshold Decision Trees
4.1 Offline Evaluation Set-Up
We evaluated the approach introduced in Section 2
on the Let?s Go corpus. The set of features was ex-
tended to contain a total of 4 discourse features, 6
semantic features, 5 timing/turn-taking features, 43
prosodic features, and 6 speaker characteristic fea-
tures. All evaluations were performed by 10-fold
cross-validation on the corpus. Based on the pro-
posed algorithm, we built a decision tree and com-
puted optimal cluster thresholds for different overall
FA rates. We report average latency as a function
of the proportion of turns for which any pause was
erroneously endpointed, which is closer to real per-
formance than silence FA rate since, once a turn has
been endpointed, all subsequent silences are irrele-
vant.
6
Figure 2: Performance of the proposed approach using
different feature sets.
4.2 Performance of Different Feature Sets
First we evaluated each feature set individually. The
results are shown in Figure 2. We concentrate on the
2-6% range of turn cut-in rate where any reasonable
operational value is likely to lie (the 700 ms thresh-
old of the baseline Let?s Go system yields about 4%
cut-in rate). All feature sets improve over the base-
line. Statistical significance of the result was tested
by performing a paired sign test on latencies for the
whole dataset, comparing, for each FA rate the pro-
portion of gaps for which the proposed approach
gives a shorter threshold than the single-threshold
baseline. Latencies produced by the decision tree
for all feature sets were all found to be significantly
shorter (p < 0.0001) than the corresponding base-
line threshold.
The best performing feature set is semantics, fol-
lowed by timing, prosody, speaker, and discourse.
The maximum relative latency reductions for each
feature set range from 12% to 22%. When using all
features, the performance improves by a small but
significant amount compared to any single set, up to
a maximum latency reduction of 24%. This confirms
that the algorithm is able to combine features effec-
tively, and that the features themselves are not com-
pletely redundant. However, while removing seman-
tic or timing features from the complete set degrades
the performance, this is not the case for discourse,
speaker, nor prosodic features. This result, similar
to what (Sato et al, 2002) reported in their own ex-
periment, indicates that prosodic features might be
redundant with semantic and timing features.
4.3 Live Evaluation
We confirmed the offline evaluation?s findings by
implementing the proposed approach in Let?s Go?s
Interaction Manager. Since prosodic features were
not found to be helpful and since their online ex-
traction is costly and error-prone, we did not include
them. At the beginning of each dialogue, the sys-
tem was randomly set as a baseline version, using a
700 ms fixed threshold, or as an experimental ver-
sion using the tree learned from the offline corpus.
Results show that median latency (which includes
both the endpointing threshold and the time to pro-
duce the system?s response) is significantly shorter
in the experimental version (561 ms) than in the
baseline (957 ms). Overall, the proposed approach
reduced latency by 50% or more in about 48% of the
turns. However, global results like these might not
reflect the actual improvement in user experience.
Indeed, we know from human-human dialogues that
relatively long latencies are normal in some circum-
stances while very short or no latency is expected
in others. The proposed algorithm reproduces some
of these aspects. For example, after open questions,
where more uncertainty and variability is expected,
the experimental version is in fact slightly slower
(1047 ms vs 993 ms). On the other hand, it is faster
after closed question (800 ms vs 965 ms) and par-
ticularly after confirmation requests (324 ms vs 965
ms), which are more predictable parts of the dia-
logue where high responsiveness is both achievable
and natural. This latter result indicates that our ap-
proach has the potential to improve explicit confir-
mations, which are often thought to be tedious and
irritating to the user.
5 Conclusion
In this paper, we described an algorithm to dynami-
cally set endpointing threshold for each silence. We
analyzed the relationship between silence distribu-
tion and a wide range of automatically extracted fea-
tures from discourse, semantics, prosody, timing and
speaker characteristics. When all features are used,
the proposed method reduced latency by up to 24%
for reasonable false alarm rates. Prosodic features
did not help threshold optimization once other fea-
ture were included. The practicality of the approach
and the offline evaluation results were confirmed by
7
implementing the proposed algorithm in the Let?s
Go system.
Acknowledgments
This work is supported by the US National Science
Foundation under grant number 0208835. Any opin-
ions, findings, and conclusions or recommendations
expressed in this material are those of the authors
and do not necessarily reflect the views of the NSF.
We would like to thank Alan Black for his many
comments and advice.
References
J. F. Allen and C. R. Perrault. 1980. Analyzing intention
in utterances. Artificial Intelligence, 15:143?178.
J. F. Allen, G. Ferguson, A. Stent, S. Stoness, M. Swift,
L. Galescu, N. Chambers, E. Campana, and G. S. Aist.
2005. Two diverse systems built using generic compo-
nents for spoken dialogue (recent progress on trips). In
Interactive Demonstration Track, Association of Com-
putational Linguistics Annual Meeting, Ann Arbor,
MI.
D. Bohus and A. Rudnicky. 2003. RavenClaw: Dia-
log management using hierarchical task decomposi-
tion and an expectation agenda. In Eurospeech03,
Geneva, Switzerland.
D. Bohus, A. Raux, T. Harris, M. Eskenazi, and A. Rud-
nicky. 2007. Olympus: an open-source framework
for conversational spoken language interface research.
In HLT-NAACL 2007 workshop on Bridging the Gap:
Academic and Industrial Research in Dialog Technol-
ogy, Rochester, NY, USA.
W. L. Chafe, 1992. Talking Data: Transcription
and Coding Methods for Language Research, chapter
Prosodic and Functional Units of Language, pages 33?
43. Lawrence Erlbaum.
H.H. Clark. 1996. Using language. Cambridge Univer-
sity Press.
S. Duncan. 1972. Some signals and rules for taking
speaking turns in conversations. Journal of Person-
ality and Social Psychology, 23(2):283?292.
L. Ferrer, E. Shriberg, and A. Stolcke. 2003. A prosody-
based approach to end-of-utterance detection that does
not require speech recognition. In ICASSP, Hong
Kong.
C. E. Ford and S. A. Thompson, 1996. Interaction and
Grammar, chapter Interactional Units in Conversation:
Syntactic, Intonational, and Pragmatic Resources for
the Management of Turns, pages 134?184. Cambridge
University Press.
H. Furo. 2001. Turn-Taking in English and Japanese.
Projectability in Grammar, Intonation, and Semantics.
Routeledge.
B. J. Grosz and C. Sidner. 1986. Attention, intentions,
and the structure of discourse. Computational Lin-
guistics, 12(3):175?204.
J. Jaffe and S. Feldstein. 1970. Rhythms of Dialogue.
Academic Press.
H. Koiso, Y. Horiuchi, S. Tutiya, A. Ichikawa, and
Y. Den. 1998. An analysis of turn-taking and
backchannels based on prosodic and syntactic features
in japanese map task dialogs. Language and Speech,
41(3-4):295?321.
Mietta Lennes and Hanna Anttila. 2002. Prosodic fea-
tures associated with the distribution of turns in finnish
informal dialogues. In Petri Korhonen, editor, The
Phonetics Symposium 2002, volume Report 67, pages
149?158. Laboratory of Acoustics and Audio Signal
Processing, Helsinki University of Technology.
B. Orestro?m. 1983. Turn-Taking in English Conversa-
tion. CWK Gleerup, Lund.
A. Raux, B. Langner, D. Bohus, A. W. Black, and M. Es-
kenazi. 2005. Let?s Go Public! taking a spoken dialog
system to the real world. In Proc. Interspeech 2005,
Lisbon, Portugal.
A. Raux, D. Bohus, B. Langner, A. W. Black, and M. Es-
kenazi. 2006. Doing research on a deployed spoken
dialogue system: One year of Let?s Go! experience.
In Proc. Interspeech 2006, Pittsburgh, PA, USA.
A. Raux, , and M. Eskenazi. 2007. A multi-layer ar-
chitecture for semi-synchronous event-driven dialogue
management. In Proc. ASRU 2007, Kyoto, Japan.
C. Rich and C.L. Sidner. 1998. Collagen: A collabora-
tion manager for software interface agents. An Inter-
national Journal: User Modeling and User-Adapted
Interaction, 8(3-4):315?350.
H. Sacks, E. A. Schegloff, and G. Jefferson. 1974.
A simplest systematics for the organization of turn-
taking for conversation. Language, 50(4):696?735.
R. Sato, R. Higashinaka, M. Tamoto, M. Nakano, and
K. Aikawa. 2002. Learning decision trees to deter-
mine turn-taking by spoken dialogue systems. In IC-
SLP 2002, Denver, CO.
Kare Sjolander. 2004. The snack sound toolkit.
http://www.speech.kth.se/snack/.
M. Takeuchi, N. Kitaoka, and S. Nakagawa. 2004.
Timing detection for realtime dialog systems using
prosodic and linguistic information. In Proc. Speech
Prosody 04, Nara, Japan.
N. Ward, A. Rivera, K. Ward, and D. Novick. 2005. Root
causes of lost time and user stress in a simple dialog
system. In Interspeech 2005, Lisbon, Portugal.
8
Category Feature test Number of Gap Ratio DifferenceSilences
Timing Pause start time ? 3000 ms 1836 / 19260 65% / 87% -23%
Timing Pause number ? 2 3379 / 17717 69% / 88% -19%
Discourse Previous question is open 3376 / 17720 70% / 88% -18%
Semantics Utterance expectation level ? 1 10025 / 11071 78% / 92% -14%
Individual Mean pause duration ? 500 ms 1336 / 19760 72% / 86% -14%
Semantics Utterance contains a positive marker 4690 / 16406 96% / 82% 13%
Prosody Mean energy of last vowel ? 5 1528 / 19568 74% / 86% -12%
Prosody Slope of energy on last vowel ? 0 6922 / 14174 78% / 89% -10%
Individual Mean number of pauses per utterance ? 3 1929 / 19267 76% / 86% -10%
Semantic Utterance is a non-understanding 6023/15073 79% / 88% -9%
Discourse Previous question is a confirmation 8893 / 12203 90% / 82% 8%
Prosody Duration of last vowel ? 1 1319 / 19777 78% / 86% -8%
Prosody Mean pitch on last voiced region ? 5 1136 / 19960 92% / 85% 7%
Prosody Slope of pitch on last voiced region ? 0 6617 / 14479 82% / 87% -4%
Semantics Utterance contains a negative marker 2667 / 18429 87% / 85% 2%*
Discourse Previous question is closed 8451 / 12645 86% / 85% 1%*
Table 1: Effect of Dialogue Features on Pause Finality. In columns 3 and 4, the first number is for silences for which
the condition in column 2 is true, while the second number is for those silences where the condition is false. * indicates
that the results are not statistically significant at the 0.01 level.
Category Feature test Number of Mean pause DifferencePauses Duration (ms) (ms)
Prosody Mean pitch on last voiced region ? 4 172 / 2911 608 / 482 126
Semantics Utterance Expectation Level ? 4 2202 / 881 475 / 526 -51
Prosody Slope of energy on last vowel ? 1 382 / 2701 446 / 495 -39
Timing Pause number ? 2 1031 / 2052 459 / 504 -45
Discourse Previous question is open 1015 / 2068 460 / 504 -43
Individual Mean pause duration ? 500 ms 370 / 2713 455 / 494 -39*
Prosody Mean energy of last vowel ? 4.5 404 / 2679 456 / 494 -38*
Semantics Utterance contains a positive marker 211 / 2872 522 / 487 35*
Discourse Previous question is closed 1178 / 1905 510 / 477 33*
Timing Pause start time ? 3000 ms 650 / 2433 465 / 496 -31*
Semantic Utterance is a non-understanding 1247 / 1836 472 / 502 -30*
Prosody Duration of last vowel ? 0.4 1194 / 1889 507 / 478 29*
Individual Mean number of pauses per utterance ? 2 461 / 2622 474 / 492 -19*
Semantics Utterance contains a negative marker 344 / 2739 504 / 488 16*
Prosody Slope of pitch on last voiced segment ? 0 1158 / 1925 482 / 494 -12*
Discourse Previous question is a confirmation 867 / 2216 496 / 487 9*
Table 2: Effect of Dialogue Features on Pause Duration. In columns 3 and 4, the first number is for silences for which
the condition in column 2 is true, while the second number is for those silences where the condition is false. * indicates
that the results are not statistically significant at the 0.01 level.
9
Let (Kn) be a set of n silence clusters, the goal is to set the thresholds (?n) that minimize overall mean
latency, while yielding a fixed, given number of false alarms E. let us define Gn the number of gaps among
the silences of Kn. For each cluster, let us define En(?n) the number of false alarms yielded by threshold
?n in cluster n, and the total latency Ln by:
Ln(?n) = Gn ? ?n (3)
Assuming pause durations follow an exponential distribution, as shown in Section 3, the following relation
holds between Ln and En:
e
Ln(?n)
?n = ?n ? En(?n) (4)
where ?K and ?K are cluster-specific coefficients estimated by linear regression in the log domain. If we
take the log of both sides, we obtain:
Ln(?n) = ?n ? log(?n ? En(?n)) (5)
Theorem 1. If (?n) is a set of thresholds that minimizes
?
n Ln such that
?
n En(?n) = E, then
?As.t.?n, dLndEn (?n) = A
Informal proof. The proof can be done by contradiction. Let us assume (?n) is a set of thresholds that
minimizes
?
n Ln, and ?(p, q)s.t.
dLp
dEp
(?p) >
dLq
dEq
(?q). Then, there exists small neighborhoods of ?p and ?q
where Lp(Ep) and Lq(Eq) can be approximated by their tangents. Since their slopes differ, it is possible to
find a small  such that the decrease in FA yielded by ?p +  is exactly compensated by the increase yielded
by ?q ? , but the reduction in latency in Kq is bigger than the increase in Kp, which contradicts the fact
that (?n) minimizes L.
From Theorem 1, we get ?As.t.?n dLndEn = A. Thus, by deriving Equation 5,
?n
En
= A which gives En = ?nA .
Given that
?
En = E,
P
?n
A = E. Hence, A =
P
?n
E . From 5, we can infer the values of Ln(?n) and,
using 3, the optimal threshold ?n for each cluster:
?n =
?n ? log(?n ?
E??n
P
?n
)
Gn
(6)
where the values of ?n and ?n can be estimated by linear regression from the data based on 5.
Figure 3: Derivation of the formula for optimal thresholds
10
Proceedings of the Third ACL Workshop on Innovative Use of NLP for Building Educational Applications, pages 71?79,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
An Analysis of Statistical Models and Features for Reading Difficulty
Prediction
Michael Heilman, Kevyn Collins-Thompson and Maxine Eskenazi
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{mheilman,kct,max}@cs.cmu.edu
Abstract
A reading difficulty measure can be described
as a function or model that maps a text to a
numerical value corresponding to a difficulty
or grade level. We describe a measure of read-
ability that uses a combination of lexical fea-
tures and grammatical features that are derived
from subtrees of syntactic parses. We also
tested statistical models for nominal, ordinal,
and interval scales of measurement. The re-
sults indicate that a model for ordinal regres-
sion, such as the proportional odds model, us-
ing a combination of grammatical and lexical
features is most effective at predicting reading
difficulty.
1 Introduction
A reading difficulty, or readability, measure can be
described as a function or model that maps a text
to a numerical value corresponding to a difficulty or
grade level. Inputs to this function are usually statis-
tics for various lexical and grammatical features of
the text. The output is one of a set of ordered dif-
ficulty levels, usually corresponding to grade levels
for elementary school through high school. As such,
reading difficulty prediction can be viewed as a re-
gression of grade level on a set of textual features.
Early work on readability measures employed
simple proxies for grammatical and lexical complex-
ity, including sentence length and the number of syl-
lables in a word. Fairly simple features were often
employed because of a lack of computational power.
Such features exhibit high bias because they rely on
strong assumptions about what makes a text difficult
to read. For example, the use of sentence length as a
measure of grammatical complexity assumes that a
longer sentence is more grammatically complex than
a shorter one, which is often but not always the case.
In one early model, the Dale-Chall model (Dale and
Chall, 1948; Chall and Dale, 1995), reading diffi-
culty is a linear function of the mean sentence length
and the percentage of rare words, as defined by a list
of 3,000 words commonly known by 4th grade. In
this paper, sentence length is defined as the mean
number of words in the sentences of a text.
Many early measures did not employ direct esti-
mates of word frequency due to computational lim-
itations (e.g., (Gunning, 1952; McLaughlin, 1969;
Kincaid et al, 1975)). Instead, these measures relied
on the strong relationship between the frequency of
and the number of syllables in a word. More fre-
quent words are more likely to have fewer syllables
(e.g., ?the?) than less frequent words (e.g., ?vocab-
ulary?), an association that is related to Zipf?s Law
(Zipf, 1935). The Flesch-Kincaid measure (Kincaid
et al, 1975) is probably the most common reading
difficulty measure in use. It is implemented in com-
mon word processing programs. This measure is a
linear function of the mean number of syllables per
word and the mean number of words per sentence.
Klare (1974) provides a summary of other early
work on readability.
More recent approaches to reading difficulty em-
ploy more sophisticated models that make use of the
growth in computational power. The Lexile Frame-
work (e.g., (Stenner, 1996)) uses individual word
frequency estimates as a measure of lexical diffi-
culty. The word frequency estimates are derived
71
from a large, varied corpus of text. Lexile uses a
Rasch model (Rasch, 1980) with the mean log word
frequency as a lexical feature and the log of the mean
sentence length as a grammatical feature. The Rasch
model, related to logistic regression, is used to esti-
mate the level of a student that would comprehend
75% of a given text. The converted log odds ratio
called a ?Lexile? that is used as part of this measure
can be easily mapped to grade school levels.
A reading difficulty measure developed by
Collins-Thompson and Callan (2005) uses
smoothed unigram language modeling to capture
the predictive ability of individual words based
on their frequency at each reading difficulty level.
Collins-Thompson and Callan found that certain
words were very predictive of certain levels. For
example, ?grownup? was very predictive of grade
1, and ?essay? was very predictive of grade 12. For
a given text, this measure estimates the likelihood
that the text was generated by each level?s language
model. The prediction is the level of the model with
the highest likelihood of generating the text. There
are no grammatical features.
Natural language processing techniques enable
more sophisticated grammatical analysis for read-
ing difficulty measures. Rather than using sentence
length as a proxy, measures can employ tools for au-
tomatic analysis of the syntactic structure of texts
(e.g., (Charniak, 2000)). A measure by Schwarm
and Ostendorf (2005) incorporates syntactic analy-
ses, among a variety of other types of features. It in-
cludes four grammatical features derived from syn-
tactic parses of text: the mean parse tree height, the
mean number of noun phrases, mean number of verb
phrases, and mean number of ?SBARs.? ?SBARs?
are non-terminal nodes that are associated with sub-
ordinate clauses. Their system led to better pre-
dictions than the Flesch-Kincaid and Lexile mea-
sures, but the predictive value of the grammatical
features is not entirely clear. In initial experiments
using such course-grain grammatical features alone,
rather than in conjunction with language modeling
and other features as in Schwarm and Ostendorf?s
system, we found relatively poor prediction perfor-
mance. Our final approach using subtrees of syn-
tactic parses allows for a finer level of discrimina-
tion that may support the detection of differences in
grade levels between texts that exhibit the same high
level features.
A reading difficulty measure developed by Heil-
man, Collins-Thompson, Callan, and Eskenazi
(2007) uses the frequency of grammatical construc-
tions as a measure of grammatical difficulty. A set
of approximately twenty constructions were selected
from English as a Second Language grammar text-
books. This set includes grammatical constructions
such as the passive voice, relative clauses, and vari-
ous verb tenses. The frequencies are used as features
for a nearest neighbor classification algorithm. The
unigram language modeling approach of Collins-
Thompson and Callan (2005) is used to estimate
lexical difficulty in this measure. The final predic-
tion is a linear function of the lexical and grammat-
ical components. That model assumes that gram-
matical difficulty is adequately captured by a small
number of constructions chosen according to de-
tailed knowledge of English grammar. In that work,
the constructions were selected from an English as
a Second Language grammar textbook, a labor- and
knowledge-intensive task that may be less practical
for other languages.
We aim to identify the appropriate scale of mea-
surement for reading difficulty?nominal, ordinal, or
interval?by comparing the effectiveness of statistical
models for each type of data. We also extend pre-
vious work combining lexical and grammatical fea-
tures (Heilman et al, 2007) by making it possible
to include a large number of grammatical features
derived from syntactic structures without requiring
significant linguistic or pedagogical content knowl-
edge, such as a reference guide for the grammar of
the language of interest.
2 Types of Features
2.1 Lexical Features
This section and the following section describe the
lexical and grammatical features used in our read-
ing difficulty models. The lexical features are the
relative frequencies of word unigrams. The use of
word unigrams is a standard approach in text clas-
sification (Yang and Pedersen, 1997), and has also
been successfully used to predict reading difficulty
(Collins-Thompson and Callan, 2005). Higher order
n-grams such as bigrams and trigrams were not used
as features because they did not improve predictions
72
in preliminary tests. The specific set of lexical fea-
tures was chosen based on the frequencies of words
in the training corpus. The system performs mor-
phological stemming and stopword removal. The
remaining 5000 most common words comprised the
lexical feature set.
2.2 Grammatical Features
Grammatical features are extracted from automatic
context-free grammar parses of sentences. The sys-
tem computes relative frequencies of partial syn-
tactic derivations, which will be called ?subtrees?
hereafter. The approach extends (Heilman et al,
2007), where frequencies of manually defined syn-
tactic patterns were extracted from syntactic struc-
tures. In that approach, the features are defined man-
ually using linguistic knowledge of the target lan-
guage to implement tree search patterns, a labor- and
knowledge-intensive process. The approach advo-
cated in this paper, however, extracts frequencies for
an automatically defined set of subtree patterns. The
system considers all subtrees up to a given depth that
occur in the training corpus. Examples of grammati-
cal features at levels 0 through 2 are shown in Figure
1. The sentence for the parse tree shown was taken
from a third grade text.
For depth 0, the system includes all subtrees con-
sisting of just nonterminal nodes. This includes all
parts of speech, as well as non-terminal nodes for
noun phrases, adjective phrases, clauses, etc. For
depth 1, the system includes subtrees corresponding
to the application of a single context free grammar
rule in the derivation of the tree. An example of a
feature at this level would be a sentence node that
dominates nodes for noun phrases and verb phrases.
For deeper levels, the system includes subtrees cor-
responding to the successive application of rules on
non-terminals symbols until either a terminal sym-
bol is reached or the given depth is reached. An
example feature for level 2 is a subtree in which
a prepositional phrase node dominates a preposi-
tion node and noun phrase node, and the preposition
node in turn dominates a preposition, and the noun
phrase dominates determiner, adjective, and noun
nodes.
We used a maximum depth of 3 in our exper-
iments. Features of deeper levels occur less fre-
quently in general, and deeper levels were avoided
due to data sparseness. A depth first search algo-
rithm extracts candidate grammatical features from
the training corpus. First, a context-free grammar
parser (Klein and Manning, 2003) derives parse
trees for all texts in the training corpus. The algo-
rithm traverses these parses, at each node counting
all subtree features up to the given depth that are
rooted at that node. The subtree features are sorted
by their overall counts in the corpus. In our ex-
periments, frequencies of the most common 1000
subtrees were chosen as the final features. These
included 64 level 0 features corresponding to non-
terminal symbols, 334 level 1 features, 461 level 2
features, and 141 level 3 features. Deeper levels
have more possible features, but sparsity at level 3
resulted in fewer level 3 features being selected.
In our experiments, the subtrees included terminal
symbols for stopwords. However, the system effec-
tively removed content word terminals from parses
before extracting features. The system could be
modified to include terminal symbols for content
words, or even to ignore all nodes for terminal sym-
bols. Subtree features including terminal symbols
for content words would, of course, occur with low
frequency and not likely be included in the final fea-
ture set. Terminal symbols for content words were
omitted so that lexical information was not included
in the set of grammatical features. Similar to leaving
higher order n-grams out of the lexical feature set,
omitting terminal symbols for content words avoids
confounding grammatical and lexical information in
the grammatical feature set. Subtree counts are nor-
malized by the number of words in a text to compute
the relative frequencies. Normalization by the num-
ber of sentences in a text is also possible, but did not
perform as well in preliminary tests. The Stanford
Parser (Klein andManning, 2003) version 1.5.1 was
used to derive tree structures for sentences. We used
the unlexicalized model included in the distribution
which was trained on Wall Street Journal texts.
3 Statistical Models
3.1 Scales of Measurement for Reading
Difficulty
Several statistical models were tested for effective-
ness at predicting reading difficulty. The appropri-
ateness of these models depends on the nature of
73
Figure 1: Parse Tree for Sentence from Third Grade Text with Example Subtree Features.
reading difficulty data, particularly the scale of mea-
surement. The standard unit for reading difficulty is
the grade level. First through twelfth grade levels in
American schools have been used in previous work
(e.g., (Heilman et al, 2007; Collins-Thompson and
Callan, 2005)). English as a Second Language lev-
els have also been used (Heilman et al, 2007),
as well as grade levels for other languages such
as French (Collins-Thompson and Callan, 2005).
While these grades are assigned evenly spaced inte-
gers, the ranges of reading difficulty corresponding
to these grades are not necessarily evenly spaced. It
is possible, of course, that assuming even spacing
between levels might produce more parsimonious
and accurate statistical models. A more reasonable
assumption is that the grade numbers assigned to
each difficulty level denote an ordering: for exam-
ple, that grade 1 is in some sense less than grade 2,
which is less than grade 3, etc. Different statistical
models handle this assumption more or less well.
Statistics generally distinguish four scales of mea-
surement, which are, ordered by increasing assump-
tions about the relationships between values: nomi-
nal, ordinal, interval, and ratio (Stevens, 1946; Co-
hen et al, 2003). Nominal data involve no relation-
ships between the labels or classes of the data. An
example would be types of fruits, where a model
might be used to make decisions between apples and
oranges. This type of prediction is generally called
classification in machine learning and related fields.
Ordinal data have a natural ordering, but the val-
ues are not necessarily evenly spaced. For exam-
ple, data about the severity of illnesses might have
labels such as mild, moderate, severe, deceased, in
which the transitions between consecutive classes
all have the same direction but not the same mag-
nitude. Making predictions about such data is gen-
erally called ordinal regression (McCullagh, 1980).
Interval data, however, are both ordered and evenly
spaced. An example would be temperature as mea-
sured in Fahrenheit degrees. Such data have an ar-
bitrary zero point, and negative values may occur.
Ratio data, of which annual income is an example,
do have a meaningful zero point. We will not dis-
cuss ratio data further since its distinction from in-
terval data is not relevant to this paper. It is not clear
to which scale reading difficulty corresponds. The
assumption of an interval scale allows for simpler
models with fewer parameters. However, models for
ordinal or even nominal data might be more appro-
priate if the strong assumption of an interval scale
does not hold.
We experimented with three linear and log-linear
models corresponding to interval, ordinal, and nom-
inal data. Parameters were estimated using L2 reg-
ularization, which corresponds to a Gaussian prior
distribution with zero mean and a user-specified
variance over the parameters. We chose these mod-
els because they are commonly used in the statis-
tics, machine learning, and behavioral science com-
munities, and aimed to set up meaningful compar-
isons among the scales of measurement. Other ma-
chine learning algorithms might also be employed.
In fact, we briefly tested the maximummargin (Vap-
nik, 1995) approach, which led to comparable re-
sults and might be worth exploring in future work.
74
3.2 Linear Regression
Linear Regression (LIN) produces a linear model in
which the dependent, or outcome, variable is a lin-
ear function of the values for predictor variables,
or features. A prediction for a given text is the
inner product of a vector of feature values for the
text and a vector of regression coefficients estimated
from training data. For the case of reading difficulty,
the grade level is a linear combination of the lexi-
cal and/or grammatical feature values. LIN provides
continuous estimates of reading difficulty, such that
a prediction might fall between grade levels. The
estimates were not rounded to whole numbers in the
experiments. For rare cases of an LIN prediction
falling outside the appropriate range of grade levels,
the value was set to the maximum or minimum grade
level. LIN implicitly assumes that the data fall on
an interval scale, meaning that the levels are evenly
spaced. The LIN model has relatively few parame-
ters but makes strong assumptions about the scale of
measurement. For details, see (Hastie et al, 2001).
3.3 Proportional Odds Model
The Proportional Odds (PO) model, also called the
parallel regression model and the cumulative logit
model, is a form of log-linear, or exponential, model
for ordinal data (McCullagh, 1980). Given a new
unlabeled instance as input, the model provides es-
timates of the probability that the instance belongs
to a class at or above a particular level. In Equation
(1), P (y ? j) is this estimated probability, ?j is an
intercept parameter for the given level j, ? is vector
of regression coefficients, Xi is the vector of feature
values for instance i, and yi is the predicted reading
difficulty level.
P (yi ? j) =
exp(?j + ?TXi)
1 + exp(?j + ?TXi)
(1)
ln
P (yi ? j)
1? P (yi ? j)
= ?j + ?TXi (2)
The PO model has a parameter ?j for the thresh-
old, or intercept, at each level j, but only a single set
? of parameters for the features. These two types of
parameters correspond to an implicit assumption of
ordinality. Having a single set of parameters for fea-
tures across the levels means that changes in feature
values proportionally affect the odds of transitioning
from any one class to another.
The estimated probability of an instance belong-
ing to a particular class is the difference between es-
timates for that class and the next highest class. For
example, the estimated probability of a text being
at the eighth grade level would be the estimate for
being at or above eighth grade minus the estimate
for being at or above ninth grade. As in binary lo-
gistic regression, the PO model estimates log odds
ratios based on the values of features or predictor
variables. The numerator of the odds ratio is the
probability of being at or above a level, and the de-
nominator is the probability of being below a level.
Equation (2) shows the form of the model that is
linear in the parameters.
3.4 Multi-class Logistic Regression
Multi-class Logistic Regression (LOG), or multino-
mial logit regression, is a log-linear model for nom-
inal data. In contrast to the simpler PO model, the
model maintains parameters for all of the features
for every class except one category, which is used
for comparison. Thus, for reading difficulty, there
are about 11 times as many parameters to estimate
compared to LIN and PO. The increased difficulty
of parameter estimation for this model is offset for
domains in which assumptions of ordinality or lin-
earity do not hold. For more details, see (Hastie et
al., 2001).
4 Evaluation
4.1 Web Corpus
The corpus of materials used for training and test-
ing the models consists of the content text extracted
from Web pages with reading difficulty level labels.
Web pages were used because the system for pre-
dicting reading difficulty is being used as part of the
REAP tutoring system, which finds authentic and
appropriate Web pages for English vocabulary prac-
tice (Brown and Eskenazi, 2004; Heilman et al,
2006). Approximately half of these texts were au-
thored by students at the particular grade level, and
half were authored by teachers or writers and aimed
at readers at a particular grade level. Texts were
found for grade levels 1 through 12. The twelfth
grade level also included some post-secondary level
75
texts. Various genres and subjects were represented.
In all cases, either the text itself or a link to it iden-
tified it as having a certain level. The content text
was manually extracted from these Web pages so
that noisy information such as navigation menus and
advertisements were not included. Automatic con-
tent extraction may, however, be able to remove such
noisy information without human intervention (e.g.,
(Gupta et al, 2003)). This Web corpus is adapted
from the corpora used in prior work on reading dif-
ficulty predication (Collins-Thompson and Callan,
2005; Heilman et al, 2007). We modified that cor-
pus because it contained a number of documents
pertaining to mathematics and vocabulary practice.
The majority of tokens in these texts were not part
of well-formed, grammatical sentences suitable for
reading practice. Since our goal is to measure the
difficulty of reading passages, we removed these
documents and added additional texts consisting of
more suitable reading material. The corpus con-
sisted of approximately 150,000 words, distributed
among 289 texts. The number of texts for each grade
level was approximately the same, with at least 28
texts at each level. The mean length in words of
the texts was approximately 500 words, which corre-
sponds to about a page. Texts for lower grades were
necessarily shorter. We extracted excerpts for higher
level texts so that texts were otherwise roughly equal
in length across levels. For these excerpts, the first
500 or so words of text were extracted, while re-
specting sentence and paragraph boundaries.
4.2 Evaluation Metrics
Root mean square error (RMSE), Pearson?s correla-
tion coefficient, and accuracy within 1 grade level
served as metrics for evaluating the performance
of reading difficulty predictions. Multiple statistics
were used because it is not entirely clear what the
best measure of prediction quality is for reading dif-
ficulty. RMSE is the square root of the empirical
mean of the squared error of predictions. It more
strongly penalizes those errors that are further away
from the true value. It can be interpreted as the aver-
age number of grade levels that predictions measure
deviate from human-assigned labels.
Pearson?s correlation coefficient measures the
strength of the linear relationship, or similarity of
trends, between two random variables. A high corre-
lation would indicate that difficult texts would more
likely receive high predicted difficulty values, and
easier texts would be more likely to receive low pre-
dicted difficulty values. Correlations do not, how-
ever, measure the degree to which values match in
absolute terms.
Adjacent accuracy is the proportion of predic-
tions that were within one grade level of the human-
assigned label for the given text. Exact accuracy is
too stringent a measure because the human-assigned
reading levels are not always perfect and consis-
tent. For example, one school might read ?Romeo
and Juliet? in 9th grade while another school might
read it in 10th grade. The drawback of this accuracy
metric is that predictions that are two levels off are
treated the same as predictions that are ten levels off.
4.3 Baselines
The performance of other algorithms for estimat-
ing reading difficulty was estimated using the same
data. These comparison include Collins-Thompson
and Callan?s implementation of their language mod-
eling approach (2005), an implementation of the
Flesch-Kincaid reading level measure (Kincaid et
al., 1975), and a measure using word frequency and
sentence length similar to Lexile (Stenner et al,
1983). We did not directly test the approach de-
scribed by (Heilman et al, 2007). We observe
that its reported results for first language texts were
not significantly different in terms of correlation and
only slightly better in terms of mean squared er-
ror than the language modeling approach. Finally,
a simple uniform baseline, which always chose the
middle value of 6.5, was tested.
The Lexile-like measure (LX) used the same two
features as the Lexile measure: mean log frequency
or words and log mean sentence length. Instead of
using a Rasch model and converting scores to ?Lex-
iles,? however, the PO model was used to directly
predict grade levels. The log frequency values for
words were estimated from the second release of the
American National Corpus (Reppen et al, 2005),
a 20 million word corpus with texts in American
English from different genres on a variety of sub-
jects. Using the proportional odds models is effec-
tively equivalent to using Lexile?s Rasch model and
mapping its output to grade levels. The major differ-
ence between the Lexile measure and the implemen-
76
tation used in these experiments is the training data
sets used to estimated word frequencies and model
parameters.
4.4 Procedure
The Web Corpus was randomly split into training
and test sets. The test set consisted of 25% of the
individual texts at each level, a total of 84 texts.
Ten-fold stratified cross-validation on the training
set was employed to estimate the prediction per-
formance according to the evaluation metrics. In
cross-validation, data are partitioned randomly into
a given number of folds, and each fold is used for
testing while all others are used for training. For
more details and a discussion of validation meth-
ods, see (Hastie et al, 2001). The regularization
hyper-parameters were tuned on the training set dur-
ing cross-validation by a simple grid search. After
cross-validation, models were trained on the entire
training set, and then evaluated using the held-out
test data.
We tested whether each feature-set, algorithm pair
or baseline performed significantly differently than
our hypothesized best model, the PO model with
the combined feature set. We employed the bias-
corrected and accelerated (BCa) Bootstrap (Efron
and Tibshirani, 1993) with 50,000 replications of the
held-out test data to generate confidence intervals
for differences in evaluation results. If the (1??)%
confidence intervals for the difference do not con-
tain zero, which is the value corresponding to the
null hypothesis, then that difference is significant at
the ? level. For example, the 99% confidence inter-
val for the difference in adjacent accuracy between
the language modeling baseline and the PO model
with the combined feature set was (-1.86, -0.336),
indicating that this difference is significant at the .01
level since it does not contain zero.
5 Results
Table 1 presents correlation coefficients, RMSE val-
ues, and accuracy values for cross-validation and
held-out test data. Statistical significance was tested
only for the held-out test data since the hyper-
parameters were tuned during cross-validation. Our
discussion of the results pertains mostly to the eval-
uation on the test-set.
Of the various statistical models, the PO model
for ordinal data appears to provide superior perfor-
mance over the LIN and LOG models. Compared
to the LOG model, the PO model performs sig-
nificantly better in terms of correlation and RMSE
and comparably well in terms of adjacent accuracy.
Compared to the LINmodel, the POmodel performs
almost as well in terms of correlation, comparably
well in terms of RMSE, and far better in terms of
accuracy.
The performance of the methods when using dif-
ferent feature sets does not clearly indicate a best set
of features to use for predicting reading difficulty.
For the PO model, none of the feature sets lead to
significant gains over the others in terms of any of
the metrics. However, the combined feature set led
to the best performance in terms of correlation and
adjacent accuracy during cross-validation as well as
RMSE on the test set, suggesting at the very least
that including the extra features does not degrade
performance.
The PO model with the combined feature set out-
performed most of the baseline measures. LX had
the same accuracy value on the test set. The LX
method appears to perform the best in general of
the baselines models. Interestingly, LX uses pro-
portional odds logistic regression like PO, and thus
assumes an ordinal but not interval scale of measure-
ment. RMSE values were significantly lower for the
PO model than for LX and the language modeling
approach.
No statistically significant advantages are seen
for PO model when compared to Flesch-Kincaid.
We observe however, that for the sample of web
pages which constitutes the evaluation corpus the
PO model produced superior results across evalua-
tion metrics. That is, PO performed better in terms
of adjacent accuracy, RMSE, and correlation coeffi-
cients, both in cross-validation and testing with held-
out data.
6 Discussion
In our tests, the PO model, which assumes ordinal
data, lead to the most effective predictions of read-
ing difficulty in general. This result indicates that the
reading difficulty of texts, according to grade level,
lies on an ordinal scale of measurement. That is,
77
Method Features Cross-Validation Held-Out Test Set
Correl. RMSE Adj. Acc. Correl. RMSE Adj. Acc.
LIN Lexical .629 2.73 .242 .779 2.42 .167**
Grammatical .767 2.26 .294 .753 2.33 .274*
Combined .679 2.57 .284 .819** 2.21 .226**
PO Lexical .713 2.57 .498 .780 2.29 .464
Grammatical .762 2.22 .505 .734 2.42 .560
Combined .773 2.24 .519 .767 2.23 .440
LOG Lexical .517 3.24 .443 .619* 2.83* .548
Grammatical .632 2.87 .443 .506** 3.38** .464
Combined .582 2.94 .446 .652* 2.71* .556
LX - .659 2.77 .467 .731 2.67* .464
Lang. Modeling - .590 2.74 .370 .630 2.70** .381
Flesch-Kincaid - .697 2.66 .388 .718 2.54 .369
Uniform - .000 3.39 .170 .000** 3.45** .167**
Table 1: Results from Cross-Validation and Test Set Evaluations, as measured by Correlation Coefficients (Correl.),
Root Mean Square Error (RMSE), and Adjacent Accuracy. The best result for each metric for each evaluation is
given in bold. Asterisks indicate significant differences compared to the PO model with a Combined Feature Set. * =
p < .05, ** = p < .01.
reading difficulty appears to increase steadily but not
linearly with grade level. As such, the LIN approach
that produces linear models was less effective, par-
ticularly in terms of adjacent accuracy. The LOG
model, for nominal data, also led to inferior perfor-
mance compared to the PO model, which can be at-
tributed to the difficulty of accurately estimating a
more complex model with many parameters for each
level.
Our tests found that grammatical features alone
can be effective predictors of readability. This find-
ing disagrees with a previous result that found that a
model using a combination of lexical and manually
defined grammatical features (Heilman et al, 2007)
outperformed a model using grammatical features
alone. The superior predictive ability of the mod-
els we describe that use grammatical features can be
attributed to the automatic derivation of a grammat-
ical feature set that is more than an order of magni-
tude larger than in the previous approach. Our ap-
proach enables the use of much larger grammatical
feature sets because it does not require the extensive
linguistic knowledge and effort to manually define
the grammatical features. The automatic approach
also enables an easier transition to other languages,
assuming a parser is available. Using the combined
feature set did not hurt performance, however, and
since regularized statistical models can avoid over-
fitting large numbers of parameters, a combined fea-
ture set still seems appropriate.
Acknowledgments
We thank Jamie Callan for his comments and sug-
gestions. This research was supported in part by
the Institute of Education Sciences, U.S. Depart-
ment of Education, through Grant R305B040063 to
Carnegie Mellon University; Dept. of Education
grant R305G03123; the Pittsburgh Science of Learn-
ing Center which is funded by the National Sci-
ence Foundation, award number SBE-0354420; and
a National Science Foundation Graduate Research
Fellowship awarded to the first author. Any opin-
ions, findings, conclusions, or recommendations ex-
pressed in this material are the authors, and do not
necessarily reflect those of the sponsors.
References
Jon Brown and Maxine Eskenazi. 2004. Retrieval of au-
thentic documents for reader-specific lexical practice.
Proceedings of InSTIL/ICALL Symposium 2004.
78
J. S. Chall and E. Dale. 1995. Readability Revisited:
The New Dale-Chall Readability Formula. Brookline
Books. Cambridge, MA.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. Proceedings of the NAACL.
J. Cohen, P. Cohen, S. G. West, and L. S. Aiken. 2003.
Applied Multiple Regression/Correlation Analysis for
the Behavioral Sciences, 3rd Edition. Lawrence Erl-
baum Associates, Inc.
Michael Collins and Nigel Duffy. 2002. Convolution
Kernels for Natural Language. Advances in Neural In-
formation Processing Systems..
Kevyn Collins-Thompson and Jamie Callan. 2005.
Predicting reading difficulty with statistical language
models. Journal of the American Society for Informa-
tion Science and Technology, 56(13). pp. 1448-1462..
E. Dale and J. S. Chall. 1948. A Formula for Predicting
Readability. Educational Research Bulletin Vol. 27,
No. 1.
Bradley Efron and Robert J. Tibshirani. 1993. An In-
troduction to the Bootstrap. Chapman and Hall, New
York.
R. Gunning. 1952. The technique of clear writing..
McGraw-Hill, New York.
S. Gupta, G. Kaiser, D. Neistadt, and P. Grimm. 2003.
DOM-based content extraction of HTML documents.
ACM Press, New York.
Trevor Hastie, Robert Tibshirani, Jerome Friedman.
2003. The Elements of Statistical Learning:Data Min-
ing, Inference, and Prediction. Springer.
Michael Heilman, Kevyn Collins-Thompson, Jamie
Callan, and Maxine Eskenazi. 2007. Combining Lex-
ical and Grammatical Features to Improve Readability
Measures for First and Second Language Texts. Pro-
ceedings of the Human Language Technology Confer-
ence. Rochester, NY.
Michael Heilman, Kevyn Collins-Thompson, Jamie
Callan, and Maxine Eskenazi. 2006. Classroom suc-
cess of an Intelligent Tutoring System for lexical prac-
tice and reading comprehension. Proceedings of the
Ninth International Conference on Spoken Language
Processing. Pittsburgh, PA.
J. Kincaid, R. Fishburne, R. Rodgers, and B. Chissom.
1975. Derivation of new readability formulas for navy
enlisted personnel. Branch Report 8-75. Chief of
Naval Training, Millington, TN.
G. R. Klare. 1974. Assessing Readability. Reading Re-
search Quarterly, Vol. 10, No. 1. pp. 62-102..
Dan Klein and Christopher D. Manning. 2003. Accurate
Unlexicalized Parsing. Proceedings of the 41st Meet-
ing of the Association for Computational Linguistics,
pp. 423-430.
G. H. McLaughlin. 1969. SMOG grading: A new read-
ability formula. Journal of Reading.
P. McCullagh. 1980. Regression Models for Ordinal
Data. Journal of the Royal Statistical Society. Series
B (Methodological), Vol. 42, No. 2. pp. 109-142.
G. Rasch. 1980. Probabilistic Models for Some Intelli-
gence and Attainment Tests. MESA Press, Chicago,
IL.
G. Rasch. 2005. American National Corpus (ANC) Sec-
ond Release.. Linguistic Data Consortium. Philadel-
phia, PA.
Sarah Schwarm and Mari Ostendorf. 2005. Read-
ing level assessment using support vector machines
and statistical language models. Proceedings of the
43rd Annual Meeting on Association for Computa-
tional Linguistics.
A. J. Stenner, M. Smith, and D. S. Burdick. 1983. To-
ward a Theory of Construct Definition. Journal of Ed-
ucational Measurement, Vol. 20, No. 4. pp. 305-316.
A. J. Stenner. 1996. Measuring reading comprehension
with the Lexile framework. Fourth North American
Conference on Adolescent/Adult Literacy.
S. S. Stevens. 1946. On the theory of scales of measure-
ment. Science, 103, pp. 677-680.
V. N. Vapnik. 1995. The Nature of Statistical Learning
Theory. Springer.
Y. Yang and J. P. Pedersen. 1997. A Comparative Study
on Feature Selection in Text Categorization. Proceed-
ings of the Fourteenth International Conference on
Machine Learning (ICML?97), pp. 412-420.
G. K. Zipf. 1935. The Psychobiology off Language.
Houghton Mifflin, Boston, MA.
79
Proceedings of the Third ACL Workshop on Innovative Use of NLP for Building Educational Applications, pages 80?88,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Retrieval of Reading Materials for Vocabulary and Reading Practice
Michael Heilman, Le Zhao, Juan Pino and Maxine Eskenazi
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{mheilman,lezhao,jmpino,max}@cs.cmu.edu
Abstract
Finding appropriate, authentic reading mate-
rials is a challenge for language instructors.
The Web is a vast resource of texts, but most
pages are not suitable for reading practice, and
commercial search engines are not well suited
to finding texts that satisfy pedagogical con-
straints such as reading level, length, text qual-
ity, and presence of target vocabulary. We
present a system that uses various language
technologies to facilitate the retrieval and pre-
sentation of authentic reading materials gath-
ered from the Web. It is currently deployed in
two English as a Second Language courses at
the University of Pittsburgh.
1 Introduction
Reading practice is an important component of first
and second language learning, especially with re-
gards to vocabulary learning (Hafiz and Tudor,
1989). Appropriating suitable reading material for
the needs of a particular curriculum or particular stu-
dent, however, is a challenging process. Manually
authoring or editing readings is time-consuming and
raises issues of authenticity, which are particularly
significant in second language learning (Peacock,
1997). On the other hand, the Web is a vast resource
of authentic reading material, but commercial search
engines which are designed for a wide variety of in-
formation needs may not effectively facilitate the re-
trieval of appropriate readings for language learners.
In order to demonstrate the problem of finding ap-
propriate reading materials, here is a typical exam-
ple of an information need from a teacher of an En-
glish as a Second Language (ESL) course focused
on reading skills. This example was encountered
during the development of the system. It should
be noted that while we describe the system in the
context of ESL, we claim that the approach is gen-
eral enough to be applied to first language reading
practice and to languages other than English. To
fit within his existing curriculum, the ESL teacher
wanted to find texts on the specific topic of ?interna-
tional travel.? He sought texts that contained at least
a few words from the list of target vocabulary that
his student were learning that week. In addition, he
needed the texts to be within a particular range of
reading difficulty, fifth to eighth grade in an Ameri-
can school, and shorter than a thousand words.
Sending the query ?international travel? to a pop-
ular search engine did not produce a useful list of re-
sults1. The first result was a travel warning from the
Department of State2, which was at a high reading
level (grade 10 according to the approach described
by (Heilman et al, 2008)) and not likely to be of
interest to ESL students because of legal and techni-
cal details. Most of the subsequent results were for
commercial web sites and travel agencies. A query
for a subset of the target vocabulary words for the
course also produced poor results. Since the search
engine used strict boolean retrieval methods, the top
results for the query ?deduce deviate hierarchy im-
plicit undertake? were all long lists of ESL vocabu-
lary words3.
We describe a search system, called REAP
Search, that is tailored to the needs of language
1www.google.com, March 5, 2008
2http://travel.state.gov/travel/cis pa tw/cis pa tw 1168.html
3e.g., www.espindle.org/university word list uwl.html
80
teachers and learners. The system facilitates the re-
trieval of texts satisfying particular pedagogical con-
straints such as reading level and text length, and al-
lows the user to constrain results so that they con-
tain at least some, but not necessarily all, of the
words from a user-specified target vocabulary list.
It also filters out inappropriate material as well as
pages that do not contain significant amounts of text
in well-formed sentences. The system provides sup-
port for learners including an interface for reading
texts, easy access to dictionary definitions, and vo-
cabulary exercises for practice and review.
The educational application employs multiple
language technologies to achieve its various goals.
Information retrieval and web search technologies
provide the core components. Automated text clas-
sifiers organize potential readings by general topic
area and reading difficulty. We are also developing
an approach to measuring reading difficulty that uses
a parser to extract grammatical structures. Part of
Speech (POS) tagging is used to filter web pages to
maintain text quality.
2 Path of a Reading
In the REAP Search system, reading materials take a
path from the Web to students through various inter-
mediate steps as depicted in Figure 1. First, a crawl-
ing program issues queries to large-scale commer-
cial search engines to retrieve candidate documents.
These documents are annotated, filtered, and stored
in a digital library, or corpus. This digital library cre-
ation process is done offline. A customized search
interface facilitates the retrieval of useful reading
materials by teachers, who have particular curricu-
lar goals and constraints as part of their information
needs. The teachers organize their selected readings
through a curriculum manager. The reading inter-
face for students accesses the curriculum manager?s
database and provides the texts along with support
in the form of dictionary definitions and practice ex-
ercises.
3 Creating a Digital Library of Readings
The foundation of the system is a digital library of
potential reading material. The customized search
component does not search the Web directly, but
rather accesses this filtered and annotated database
of Web pages. The current library consists of ap-
proximately five million documents. Construction
of the digital library begins with a set of target vo-
cabulary words that might be covered by a course or
set of courses (typically 100-1,500 words), and a set
of constraints on text characteristics. The constraints
can be divided into three sets: those that can be ex-
pressed in a search engine query (e.g., target words,
number of target words per text, date, Web domain),
those that can be applied using just information in
the Web search result list (e.g., document size), and
those that require local annotation and filtering (e.g.,
reading level, text quality, profanity).
The system obtains candidate documents by
query-based crawling, as opposed to following
chains of links. The query-based document crawl-
ing approach is designed to download documents
for particular target words. Queries are submitted
to a commercial Web search engine4, result links are
downloaded, and then the corresponding documents
are downloaded. A commercial web search engine
is used to avoid the cost of maintaining a massive,
overly general web corpus.
Queries consist of combinations of multiple tar-
get words. The system generates 30 queries for each
target word (30 is a manageable and sufficient num-
ber in practice). These are spread across 2-, 3-,
and 4-word combinations with other target words.
Queries to search engines can often specify a date
range. We employ ranges to find more recent mate-
rial, which students prefer. The tasks of submitting
queries, downloading the result pages, and extract-
ing document links are distributed among a dozen
or so clients running on desktop machines, to run as
background tasks. The clients periodically upload
their results to a server, and request a new batch of
queries.
Once the server has a list of candidate pages, it
downloads them and applies various filters. The fi-
nal yield of texts is typically approximately one per-
cent of the originally downloaded results. Many web
pages are too long, contain too little well-formed
text, or are far above the appropriate reading level
for language learners. After downloading docu-
ments, the system annotates them as described in
the next section. It then stores the pages in a full-
4www.altavista.com
81
Figure 1: Path of Reading Materials from the Web to a Student.
text search engine called Indri, which is part of
the Lemur Toolkit5. This index provides a consis-
tent and efficient interface to the documents. Using
Lemur and the Indri Query Language allows for the
retrieval of annotated documents according to user-
specified constraints.
4 Annotations and Filters
Annotators automatically tag the documents in the
corpus to enable the filtering and retrieval of read-
ing material that matches user-specified pedagogical
constraints. Annotations include reading difficulty,
general topic area, text quality, and text length. Text
length is simply the number of word tokens appear-
ing in the document.
4.1 Reading Level
The system employs a language modeling ap-
proach developed by Collins-Thompson and Callan
(Collins-Thompson and Callan, 2005) that creates a
model of the lexicon for each grade level and pre-
dicts reading level, or readability, of given docu-
ments according to those models. The readabil-
ity predictor is a specialized Naive Bayes classi-
fier with lexical unigram features. For web docu-
ments in particular, Collins-Thompson and Callan
report that this language modeling-based prediction
has a stronger correlation with human-assigned lev-
els than other commonly used readability measures.
This automatic readability measure allows the sys-
tem to satisfy user-specified constraints on reading
difficulty.
We are also experimenting with using syntac-
tic features to predict reading difficulty. Heilman,
Collins-Thompson, and Eskenazi (Heilman et al,
2008) describe an approach that combines predic-
tions based on lexical and grammatical features. The
5www.lemurproject.org
grammatical features are frequencies of occurrence
of grammatical constructions, which are computed
from automatic parses of input texts. Using multiple
measures of reading difficulty that focus on different
aspects of language may allow users more freedom
to find texts that match their needs. For example,
a teacher may want to find grammatically simpler
texts for use in a lesson focused on introducing dif-
ficult vocabulary.
4.2 General Topic Area
A set of binary topic classifiers automatically clas-
sifies each potential reading by its general topic, as
described by Heilman, Juffs, and Eskenazi (2007).
This component allows users to search for readings
on their general interests without specifying a par-
ticular query (e.g., ?international travel?) that might
unnecessarily constrain the results to a very narrow
topic.
A Linear Support Vector Machine text classifier
(Joachims, 1999) was trained on Web pages from
the Open Directory Project (ODP)6. These pages ef-
fectively have human-assigned topic labels because
they are organized into a multi-level hierarchy of
topics. The following general topics were manually
selected from categories in the ODP: Movies and
Theater; Music; Visual Arts; Computers and Tech-
nology; Business; Math, Physics and Chemistry; Bi-
ology and Environment; Social Sciences; Health and
Medicine; Fitness and Nutrition; Religion; Politics;
Law and Crime; History; American Sports; and Out-
door Recreation.
Web pages from the ODP were used as gold-
standard labels in the training data for the classi-
fiers. SVM-Light (Joachims, 1999) was used as an
implementation of the Support Vector Machines. In
preliminary tests, the linear kernel produced slightly
6dmoz.org
82
better performance than a radial basis function ker-
nel. The values of the decision functions of the clas-
sifiers for each topic are used to annotate readings
with their likely topics.
The binary classifiers for each topic category were
evaluated according to the F1 measure, the harmonic
mean of precision and recall, using leave-one-out
cross-validation. Values for the F1 statistic range
from .68 to .86, with a mean value of .76 across
topics. For comparison, random guessing would be
expected to correctly choose the gold-standard label
only ten percent of the time. During an error analy-
sis, we observed that many of the erroneous classifi-
cations were, in fact, plausible for a human to make
as well. Many readings span multiple topics. For
example, a document on a hospital merger might be
classified as ?Health and Medicine? when the cor-
rect label is ?Business.? In the evaluation, the gold
standard included only the single topic specified by
the ODP. The final system, however, assigns multi-
ple topic labels when appropriate.
4.3 Text Quality
A major challenge of using Web documents for ed-
ucational applications is that many web pages con-
tain little or no text in well-formed sentences and
paragraphs. We refer to this problem as ?Text Qual-
ity.? Many pages consist of lists of links, navigation
menus, multimedia, tables of numerical data, etc. A
special annotation tool filters out such pages so that
they do not clutter up search results and make it dif-
ficult for users to find suitable reading materials.
The text quality filter estimates the proportion of
the word tokens in a page that are contained in well-
formed sentences. To do this it parses the Document
Object Model structure of the web page, and orga-
nizes it into text units delineated by the markup tags
in the document. Each new paragraph, table ele-
ment, span, or divider markup tag corresponds to the
beginning of a new text unit. The system then runs
a POS tagger7 over each text unit. We have found
that a simple check for whether the text unit con-
tains both a noun and a verb can effectively distin-
guish between content text units and those text units
that are just part of links, menus, etc. The proportion
7The OpenNLP toolkit?s tagger was used
(opennlp.sourceforge.net).
of the total tokens that are part of content text units
serves as a useful measure of text quality. We have
found that a threshold of about 85% content text is
appropriate, since most web pages contain at least
some non-content text in links, menus, etc. This ap-
proach to content extraction is related to previous
work on increasing the accessibility of web pages
(Gupta et al, 2003).
5 Constructing Queries
Users search for readings in the annotated corpus
through a simple interface that appears similar to,
but extends the functionality of, the interfaces for
commercial web search engines. Figure 2 shows
a screenshot of the interface. Users have the op-
tion to specify ad hoc queries in a text field. They
can also use drop down menus to specify optional
minimum and/or maximum reading levels and text
lengths. Another optional drop-down menu allows
users to constrain the general topic area of results. A
separate screen allows users to specify a list of tar-
get vocabulary words, some but not all of which are
required to appear in the search results. For ease of
use, the target word list is stored for an entire session
(i.e., until the web browser application is closed)
rather than specified with each query. After the user
submits a query, the system displays multiple results
per screen with titles and snippets.
5.1 Ranked versus Boolean Retrieval
In a standard boolean retrieval model, with AND as
the default operator, the results list consists of doc-
uments that contain all query terms. In conjunc-
tion with relevance ranking techniques, commercial
search engines typically use this model, a great ad-
vantage of which is speed. Boolean retrieval can en-
counter problems when queries have many terms be-
cause every one of the terms must appear in a doc-
ument for it to be selected. In such cases, few or
no satisfactory results may be retrieved. This issue
is relevant because a teacher might want to search
for texts that contain some, but not necessarily all,
of a list of target vocabulary words. For example,
a teacher might have a list of ten words, and any
text with five of those words would be useful to give
as vocabulary and reading practice. In such cases,
ranked retrieval models are more appropriate be-
83
Figure 2: Screenshot of Search Interface for Finding Appropriate Readings.
cause they do not require that all of the query terms
appear. Instead, these models prefer multiple occur-
rences of different word types as opposed to multiple
occurrences of the same word tokens, allowing them
to rank documents with more distinct query terms
higher than those with distinct query terms. Docu-
ments that contain only some of the query terms are
thus assigned nonzero weights, allowing the user to
find useful texts that contain only some of the target
vocabulary. The REAP search system uses the Indri
Query Language?s ?combine? and ?weight? opera-
tors to implement a ranked retrieval model for target
vocabulary. For more information on text retrieval
models, see (Manning et al, 2008).
5.2 Example Query
Figure 3 shows an example of a structured query
produced by the system from a teacher?s original
query and constraints. This example was slightly
altered from its original form for clarity of presen-
tation. The first line with the filrej operator filters
and rejects any documents that contain any of a long
list of words considered to be profanity, which are
omitted in the illustration for brevity and posterity.
The filreq operator in line 2 requires that all of the
constraints on reading level, text length and quality
in lines 2-4 are met. The weight operator at the start
of line 5 balances between the ad hoc query terms in
line 5 and the user-specific target vocabulary terms
in lines 6-8. The uw10 operator on line 5 tells the
system to prefer texts where the query terms appear
together in an unordered window of size 10. Such
proximity operators cause search engines to prefer
documents in which query terms appear near each
other. The implicit assumption is that the terms in
queries such as ?coal miners safety? are more likely
to appear in the same sentence or paragraph in rele-
vant documents than irrelevant ones, even if they do
not appear consecutively. Importantly, query terms
are separated from target words because there are
usually a much greater number of target words, and
thus combining the two sets would often result in
the query terms being ignored. The higher weight
assigned to the set of target words ensures they are
not ignored.
6 Learner and Teacher Support
In addition to search facilities, the system provides
extensive support for students to read and learn from
texts as well as support for teachers to track stu-
dents? progress. All interfaces are web-based for
easy access and portability. Teachers use the search
system to find readings, which are stored in a cur-
riculum manager that allows them to organize their
selected texts. The manager interface allows teach-
ers to perform tasks such as specifying the order
of presentation of their selected readings, choosing
target words to be highlighted in the texts to focus
learner attention, and specifying time limits for each
text.
The list of available readings are shown to stu-
dents when they log in during class time or for
homework. Students select a text to read and move
on to the reading interface, which is illustrated in
Figure 4. The chosen web page is displayed in its
original format except that the original hyperlinks
and pop-ups are disabled. Target words that were
84
Figure 3: Example Structured Query. The line numbers on the left are for reference only.
chosen by the teacher are highlighted and linked to
definitions. Students may also click on any other
unknown words to access definitions. The dictio-
nary definitions are provided from the Cambridge
Advanced Learner?s Dictionary8, which is authored
specifically for ESL learners. All dictionary access
is logged, and teachers can easily see which words
students look up.
The system also provides vocabulary exercises af-
ter each reading for additional practice and review
of target words. Currently, students complete cloze,
or fill-in-the-blank, exercises for each target word in
the readings. Other types of exercises are certainly
possible. For extra review, students also complete
exercises for target words from previous readings.
Students receive immediate feedback on the prac-
tice and review exercises. Currently, sets of the ex-
ercises are manually authored for each target word
and stored in a database, but we are exploring auto-
mated question generation techniques (Brown et al,
2005; Liu et al, 2005). At runtime, the system se-
lects practice and review exercises from this reposi-
tory.
7 Related Work
A number of recent projects have taken similar ap-
proaches to providing authentic texts for language
learners. WERTi (Amaral et al, 2006) is an in-
telligent automatic workbook that uses texts from
the Web to increase knowledge of English gram-
matical forms and functions. READ-X (Miltsakaki
and Troutt, 2007) is a tool for finding texts at spec-
ified reading levels. SourceFinder (Sheehan et al,
2007) is an authoring tool for finding suitable texts
for standardized test items on verbal reasoning and
8dictionary.cambridge.org
reading comprehension.
The REAP Tutor (Brown and Eskenazi, 2004;
Heilman et al, 2006) for ESL vocabulary takes a
slightly different approach. Rather than teachers
choosing texts as in the REAP Search system, the
REAP Tutor itself selects individualized practice
readings from a digital library. The readings contain
target vocabulary words that a given student needs
to learn based on a student model. While the in-
dividualized REAP Tutor has the potential to better
match the needs of each student since each student
can work with different texts, a drawback of its ap-
proach is that instructors may have difficulty coor-
dinating group discussion about readings and inte-
grating the Tutor into their curriculum. In the REAP
Search system, however, teachers can find texts that
match the needs and interests of the class as a whole.
While some degree of individualization is lost, the
advantages of better coordinated support from teach-
ers and classroom integration are gained.
8 Pilot Study
8.1 Description
Two teachers and over fifty students in two ESL
courses at the University of Pittsburgh used the sys-
tem as part of a pilot study in the Spring of 2008.
The courses focus on developing the reading skills
of high-intermediate ESL learners. The target vo-
cabulary words covered in the courses come from
the Academic Word List (Coxhead, 2000), a list
of broad-coverage, general purpose English words
that frequently appear in academic writing. Students
used the system once per week in a fifty-minute class
for eight weeks. For approximately half of a ses-
sion, students read the teacher-selected readings and
worked through individualized practice exercises.
85
Figure 4: Screenshot of Student Interface Displaying a Reading and Dictionary Definition.
For the other half of each session, the teacher pro-
vided direct instruction on and facilitated discussion
about the texts and target words, making connec-
tions to the rest of the curriculum when possible.
For each session, the teachers found three to five
readings. Students read through at least two of the
readings, which were discussed in class. The extra
readings allowed faster readers to progress at their
own pace if they complete the first two. Teachers
learned to use the system in a training session that
lasted about 30 minutes.
8.2 Usage Analysis
To better understand the two teachers? interactions
with the search system, we analyzed query log data
from a four week period. In total, the teachers used
the system to select 23 readings for their students.
In the process, they issued 47 unique queries to the
system. Thus, on average they issued 2.04 queries
per chosen text. Ideally, a user would only have to
issue a single query to find useful texts, but from
the teachers? comments it appears that the system?s
usability is sufficiently good in general. Most of
the time, they specified 20 target words, only some
of which appeared in their selected readings. The
teachers included ad hoc queries only some of the
time. These were informational in nature and ad-
dressed a variety of topics. Example queries in-
clude the following: ?surviving winter?, ?coal min-
ers safety?, ?gender roles?, and ?unidentified flying
objects?. The teachers chose these topics because
they matched up with topics discussed in other parts
of their courses? curricula. In other cases, it was
more important for them to search for texts with tar-
get vocabulary rather than those on specific topics,
so they only specified target words and pedagogical
constraints.
8.3 Post-test and Survey Results
At the end of the semester, students took an exit sur-
vey followed by a post-test consisting of cloze vo-
cabulary questions for the target words they prac-
ticed with the system. In previous semesters, the
REAP Tutor has been used in one of the two courses
that were part of the pilot study. For comparison
with those results, we focus our analysis on the sub-
set of data for the 20 students in that course. The
exit survey results, shown in 5, indicate that stu-
dents felt it was easy-to-use and should be used in
future classes. These survey results are actually very
similar to previous results from a Spring 2006 study
with the REAP Tutor (Heilman et al, 2006). How-
ever, responses to the prompt ?My teacher helped
me to learn by discussing the readings after I read
86
Figure 5: The results from the pilot study exit survey, which used a Likert response format from 1-5 with 1=Strongly
Disagree, 3=Neither Agree nor Disagree, and 5=Strongly Agree. Error bars indicate standard deviations.
them? suggest that the tight integration of an edu-
cational system with other classroom activities, in-
cluding teacher-led discussions, can be beneficial.
Learning of target words was directly measured
by the post-test. On average, students answered
89% of cloze exercises correctly, compared to less
than 50% in previous studies with the REAP Tutor.
A direct comparison to those studies is challenging
since the system in this study provided instruction
on words that students were also studying as part of
their regular coursework, whereas systems in previ-
ous studies did not.
9 Discussion and Future Work
We have described a system that enables teachers
to find appropriate, authentic texts from the Web
for vocabulary and reading practice. A variety of
language technologies ranging from text retrieval to
POS tagging perform essential functions in the sys-
tem. The system has been used in two courses by
over fifty ESL students.
A number of questions remain. Can language
learners effectively and efficiently use such a system
to search for reading materials directly, rather than
reading what a teacher selects? Students could use
the system, but a more polished user interface and
further progress on filtering out readings of low text
quality is necessary. Is such an approach adaptable
to other languages, especially less commonly taught
languages for which there are fewer available Web
pages? Certainly there are sufficient resources avail-
able on the Web in commonly taught languages such
as French or Japanese, but extending to other lan-
guages with fewer resources might be significantly
more challenging. How effective would such a tool
be in a first language classroom? Such an approach
should be suitable for use in first language class-
rooms, especially by teachers who need to find sup-
plemental materials for struggling readers. Are there
enough high-quality, low-reading level texts for very
young readers? From observations made while de-
veloping REAP, the proportion of Web pages below
fourth grade reading level is small. Finding appro-
priate materials for beginning readers is a challenge
that the REAP developers are actively addressing.
Issues of speed and scale are also important to
consider. Complex queries such as the one shown
in Figure 3 are not as efficient as boolean queries.
The current system takes a few seconds to return re-
sults from its database of several million readings.
Scaling up to a much larger digital library may re-
quire sophisticated distributed processing of queries
across multiple disks or multiple servers. However,
we maintain that this is an effective approach for
providing texts within a particular grade level range
or known target word list.
Acknowledgments
This research was supported in part by the Insti-
tute of Education Sciences, U.S. Department of Ed-
ucation, through Grant R305B040063 to Carnegie
Mellon University; Dept. of Education grant
R305G03123; the Pittsburgh Science of Learning
Center which is funded by the National Science
Foundation, award number SBE-0354420; and a Na-
tional Science Foundation Graduate Research Fel-
lowship awarded to the first author. Any opin-
ions, findings, conclusions, or recommendations ex-
pressed in this material are the authors, and do not
necessarily reflect those of the sponsors.
References
Luiz Amaral, Vanessa Metcalf and Detmar Meurers.
87
2006. Language Awareness through Re-use of NLP
Technology. Pre-conference Workshop on NLP in
CALL ? Computational and Linguistic Challenges.
CALICO 2006.
Jon Brown and Maxine Eskenazi. 2004. Retrieval of
authentic documents for reader-specific lexical prac-
tice. Proceedings of InSTIL/ICALL Symposium 2004.
Venice, Italy.
Jon Brown, Gwen Frishkoff, and Maxine Eskenazi.
2005. Automatic question generation for vocabulary
assessment. Proceedings of HLT/EMNLP 2005. Van-
couver, B.C.
Kevyn Collins-Thompson and Jamie Callan. 2005.
Predicting reading difficulty with statistical language
models. Journal of the American Society for Informa-
tion Science and Technology, 56(13). pp. 1448-1462.
Averil Coxhead. 2000. A New Academic Word List.
TESOL Quarterly, 34(2). pp. 213-238.
S. Gupta, G. Kaiser, D. Neistadt, and P. Grimm. 2003.
DOM-based content extraction of HTML documents.
ACM Press, New York.
F. M. Hafiz and Ian Tudor. 1989. Extensive reading
and the development of language skills. ELT Journal
43(1):4-13. Oxford University Press.
Michael Heilman, Kevyn Collins-Thompson, Maxine Es-
kenazi. 2008. An Analysis of Statistical Models and
Features for Reading Difficulty Prediction. The 3rd
Workshop on Innovative Use of NLP for Building Edu-
cational Applications. Association for Computational
Linguistics.
Michael Heilman, Alan Juffs, Maxine Eskenazi. 2007.
Choosing Reading Passages for Vocabulary Learning
by Topic to Increase Intrinsic Motivation. Proceedings
of the 13th International Conferenced on Artificial In-
telligence in Education. Marina del Rey, CA.
Michael Heilman, Kevyn Collins-Thompson, Jamie
Callan, and Maxine Eskenazi. 2006. Classroom suc-
cess of an Intelligent Tutoring System for lexical prac-
tice and reading comprehension. Proceedings of the
Ninth International Conference on Spoken Language
Processing. Pittsburgh, PA.
Thorsten Joachims. 1999. Making large-Scale SVM
Learning Practical. Advances in Kernel Methods -
Support Vector Learning, B. Schlkopf and C. Burges
and A. Smola (ed.) MIT-Press.
Chao-Lin Liu, Chun-Hung Wang, Zhao-Ming Gao, and
Shang-Ming Huang. 2005. Applications of Lexical
Information for Algorithmically Composing Multiple-
Choice Cloze Items Proceedings of the Second
Workshop on Building Educational Applications Us-
ing NLP. Association for Computational Linguistics.
Christopher D. Manning, Prabhakar Raghavan and Hin-
rich Schtze. 2008. Introduction to Information Re-
trieval. Cambridge University Press. Draft available
at http://www-csli.stanford.edu/?hinrich/information-
retrieval-book.html.
Eleni Miltsakaki and Audrey Troutt. 2007. Read-X: Au-
tomatic Evaluation of Reading Difficulty of Web Text.
Proceedings of E-Learn 2007, sponsored by the Asso-
ciation for the Advancement of Computing in Educa-
tion. Quebec, Canada.
Matthew Peacock. 1997. The effect of authentic mate-
rials on the motivation of EFL learners. ELT Journal
51(2):144-156. Oxford University Press.
Kathleen M. Sheehan, Irene Kostin, Yoko Futagi. 2007.
SourceFinder: A Construct-Driven Approach for Lo-
cating Appropriately Targeted Reading Comprehen-
sion Source Texts. Proceedings of the SLaTE Work-
shop on Speech and Language Technology in Educa-
tion. Carnegie Mellon University and International
Speech Communication Association (ISCA).
88
Proceedings of the NAACL HLT Workshop on Innovative Use of NLP for Building Educational Applications, pages 43?46,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
An Application of Latent Semantic Analysis to Word Sense Discrimination
for Words with Related and Unrelated Meanings
Juan Pino and Maxine Eskenazi
(jmpino, max)@cs.cmu.edu
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
Abstract
We present an application of Latent Semantic
Analysis to word sense discrimination within
a tutor for English vocabulary learning. We
attempt to match the meaning of a word in a
document with the meaning of the same word
in a fill-in-the-blank question. We compare
the performance of the Lesk algorithm to La-
tent Semantic Analysis. We also compare the
performance of Latent Semantic Analysis on a
set of words with several unrelated meanings
and on a set of words having both related and
unrelated meanings.
1 Introduction
In this paper, we present an application of Latent
Semantic Analysis (LSA) to word sense discrimi-
nation (WSD) within a tutor for English vocabu-
lary learning for non-native speakers. This tutor re-
trieves documents from the Web that contain tar-
get words a student needs to learn and that are at
an appropriate reading level (Collins-Thompson and
Callan, 2005). It presents a document to the student
and then follows the document reading with practice
questions that measure how the student?s knowledge
has evolved. It is important that the fill-in-the-blank
questions (also known as cloze questions) that we
ask to the students allow us to determine their vocab-
ulary knowledge accurately. An example of cloze
question is shown in Figure 1.
Some words have more than one meaning and so
the cloze question we give could be about a different
meaning than the one that the student learned in the
document. This is something that can lead to confu-
sion and must be avoided. To do this, we need to use
some automatic measure of semantic similarity.
Figure 1: Example of cloze question.
To define the problem formally, given a target
word w, a string r (the reading) containing w and
n strings q1, ..., qn (the sentences used for the ques-
tions) each containing w, find the strings qi where
the meaning of w is closest to its meaning in r.
We make the problem simpler by selecting only one
question.
This problem is challenging because the context
defined by cloze questions is short. Furthermore,
a word can have only slight variations in meaning
that even humans find sometimes difficult to distin-
guish. LSA was originally applied to Information
Retrieval (Dumais et al, 1988). It was shown to be
able to match short queries to relevant documents
even when there were no exact matches between the
words. Therefore LSA would seem to be an appro-
priate technique for matching a short context, such
as a question, with a whole document.
So we are looking to first discriminate between
the meanings of words, such as ?compound?, that
have several very different meanings (a chemical
compound or a set of buildings) and then to dis-
ambiguate words that have senses that are closely
related such as ?comprise? (?be composed of? or
?compose?). In the following sections, we present
43
LSA and some of its applications, then we present
some experimental results that compare a baseline to
the use of LSA for both tasks we have just described.
We expect the task to be easier on words with unre-
lated meanings. In addition, we expect that LSA will
perform better when we use context selection on the
documents.
2 Related Work
LSA was originally applied to Information Retrieval
(Dumais et al, 1988) and called Latent Semantic In-
dexing (LSI). It is based on the singular value de-
composition (SVD) theorem. A m ? n matrix X
with m ? n can be written as X = U ?S ?V T where
U is a m ? n matrix such that UT ? U = Im; S is
a n?n diagonal matrix whose diagonal coefficients
are in decreasing order; and V is a n?n matrix such
that V T ? V = In.
X is typically a term-document matrix that repre-
sents the occurrences of vocabulary words in a set of
documents. LSI uses truncated SVD, that is it con-
siders the first r columns of U (written Ur), the r
highest coefficients in S (Sr) and the first r columns
of V (Vr). Similarity between a query and a docu-
ment represented by vectors d and q is performed by
computing the cosine similarity between S?1r ?UTr ?d
and S?1r ?UTr ?q. The motivation for computing sim-
ilarity in a different space is to cope with the sparsity
of the vectors in the original space. The motivation
for truncating SVD is that only the most meaning-
ful semantic components of the document and the
query are represented after this transformation and
that noise is discarded.
LSA was subsequently applied to number of prob-
lems, such as synonym detection (Landauer et al,
1998), document clustering (Song and Park, 2007),
vocabulary acquisition simulation (Landauer and
Dumais, 1997), etc.
Levin and colleagues (2006) applied LSA to word
sense discrimination. They clustered documents
containing ambiguous words and for a test instance
of a document, they assigned the document to its
closest cluster. Our approach is to assign to a doc-
ument the question that is closest. In addition, we
examine the cases where a word has several unre-
lated meanings and where a word has several closely
related meanings.
3 Experimental Setup
We used a database of 62 manually generated cloze
questions covering 16 target words1. We manually
annotated the senses of the target words in these
questions using WordNet senses (Fellbaum, 1998).
For each word and for each sense, we manually gath-
ered documents from the Web containing the target
word with the corresponding sense. There were 84
documents in total. We added 97 documents ex-
tracted from the tutor database of documents that
contained at least one target word but we did not an-
notate their meaning.
We wanted to evaluate the performances of LSA
for WSD for words with unrelated meanings and for
words with both related and unrelated meanings. For
the first type of evaluation, we retained four target
words. For the second type of evaluation, all 16
words were included. We also wanted to evaluate
the influence of the size of the context of the tar-
get words. We therefore considered two matrices:
a term-document matrix and a term-context matrix
where context designates five sentences around the
target word in the document. In both cases each
cell of the matrix had a tf-idf weight. Finally, we
wanted to investigate the influence of the dimension
reduction on performance. In our experiments, we
explored these three directions.
4 Results
4.1 Baseline
We first used a variant of the Lesk algorithm (Lesk,
1986), which is based on word exact match. This al-
gorithm seems well suited for the unsupervised ap-
proach we took here since we were dealing with
discrimination rather than disambiguation. Given
a document d and a question q, we computed the
number of word tokens that were shared between d
and q, excluding the target word. The words were
lower cased and stemmed using the Porter stem-
mer. Stop words and punctuation were discarded;
we used the standard English stopword list. Finally,
we selected a window of nw words around the tar-
get word in the question q and a window of ns
sentences around the target word in the document
d. In order to detect sentence boundaries, we used
1available at: www.cs.cmu.edu/ jmpino/questions.xls
44
the OpenNLP toolkit (Baldridge et al, 2002). With
nw = 10 and ns = 2, we obtained an accuracy of
61% for the Lesk algorithm. This can be compared
to a random baseline of 44% accuracy.
4.2 LSA
We indexed the document database using the Lemur
toolkit (Allan et al, 2003). The database contained
both the manually annotated documents and the doc-
uments used by the tutor and containing the target
words. The Colt package (Binko et al, ) was used
to perform singular value decomposition and matrix
operations because it supports sparse matrix oper-
ations. We explored three directions in our analy-
sis. We investigated how LSA performs for words
with related meanings and for words with unrelated
meanings. We also explored the influence of the
truncation parameter r. Finally, we examined if re-
ducing the document to a selected context of the tar-
get word improved performance.
Figures 2 and 3 plot accuracy versus dimension
reduction in different cases. In all cases, LSA out-
performs the baseline for certain values of the trun-
cation parameter and when context selection was
used. This shows that LSA is well suited for measur-
ing semantic similarity between two contexts when
at least one of them is short. In general, using the full
dimension in SVD hurts the performances. Dimen-
sion reduction indeed helps discarding noise and
noise is certainly present in our experiments since
we do not perform stemming and do not use a stop-
word list. One could argue that filling the matrix
cells with tf-idf weights already gives less impor-
tance to noisy words.
Figure 2 shows that selecting context in docu-
ments does not give much improvement in accuracy.
It might be that the amount of context selected de-
pends on each document. Here we had a fixed size
context of five sentences around the target word.
In Figure 3, selecting context gives some im-
provement, although not statistically significant,
over the case with the whole document as context.
The best performance obtained for words with un-
related meanings and context selection is also better
than the performance for words with related and un-
related meanings.
 0
 0.2
 0.4
 0.6
 0.8
 1
 60  80  100  120  140  160  180
Ac
cu
ra
cy
Truncation Parameter
Accuracy vs. Dimension Reduction for Related Meanings with Different Contexts
whole document
selected context
Lesk baseline
Figure 2: Accuracy vs. r, the truncation parameter,
for words with related and unrelated meanings and with
whole document or selected context (95% confidence for
whole document: [0.59; 0.65], 95% confidence for se-
lected context: [0.52; 0.67])
 0
 0.2
 0.4
 0.6
 0.8
 1
 5  10  15  20  25  30  35
Ac
cu
ra
cy
Truncation Parameter
Accuracy vs. Dimension Reduction for Unrelated Meanings with Different Contexts
whole document
selected context
Lesk baseline
Figure 3: Accuracy vs. r, the truncation parameter, for
words with unrelated meanings only and with whole doc-
uments or selected context ((95% confidence for whole
document: [0.50; 0.59], 95% confidence for selected con-
text: [0.52; 0.71]))
45
5 Discussion
LSA helps overcome sparsity of short contexts such
as questions and gives an improvement over the ex-
act match baseline. However, reducing the context
of the documents to five sentences around the tar-
get word does not seem to give significant improve-
ment. This might be due to the fact that capturing
the right context for a meaning is a difficult task
and that a fixed size context does not always rep-
resent a relevant context. It is yet unclear how to set
the truncation parameter. Although dimension re-
duction seems to help, better results are sometimes
obtained when the truncation parameter is close to
full dimension or when the truncation parameter is
farther from the full dimension.
6 Conclusion and Future Work
We have shown that LSA, which can be considered
as a second-order representation of the documents
and question vectors, is better suited than the Lesk
algorithm, which is a first-order representation of
vectors, for measuring semantic similarity between
a short context such as a question and a longer con-
text such as a document. Dimension reduction was
shown to play an important role in the performances.
However, LSA is relatively difficult to apply to large
amounts of data because SVD is computationally in-
tensive when the vocabulary size is not limited. In
the context of tutoring systems, LSA could not be
applied on the fly, the documents would need to be
preprocessed and annotated beforehand.
We would like to further apply this promising
technique for WSD. Our tutor is able to provide def-
initions when a student is reading a document. We
currently provide all available definitions. It would
be more beneficial to present only the definitions
that are relevant to the meaning of the word in the
document or at least to order them according to their
semantic similarity with the context. We would also
like to investigate how the size of the selected con-
text in a document can affect performance. Finally,
we would like to compare LSA performance to other
second-order vector representations such as vectors
induced from co-occurrence statistics.
Acknowledgments
Thanks Mehrbod Sharifi, David Klahr and Ken
Koedinger for fruitful discussions. This research is
supported by NSF grant SBE-0354420. Any conclu-
sions expressed here are those of the authors.
References
James Allan, Jamie Callan, Kevin Collins-Thompson,
Bruce Croft, Fangfang Feng, David Fisher, John Laf-
ferty, Leah Larkey, Thi N. Truong, Paul Ogilvie, et al
2003. The lemur toolkit for language modeling and
information retrieval.
Jason Baldridge, Thomas Morton, and Gann Bierner.
2002. The opennlp maximum entropy package. Tech-
nical report, Technical report, SourceForge.
Pavel Binko, Dino Ferrero Merlino, Wolfgang Hoschek,
Tony Johnson, Andreas Pfeiffer, et al Open source
libraries for high performance scientific and technical
computing in java.
Kevyn Collins-Thompson and Jamie Callan. 2005.
Predicting reading difficulty with statistical language
models. Journal of the American Society for Informa-
tion Science and Technology, 56(13):1448?1462.
Susane T. Dumais, George W. Furnas, Thomas K.
Landauer, Scott Deerwester, and Richard Harshman.
1988. Using latent semantic analysis to improve ac-
cess to textual information. In Proceedings of the
SIGCHI conference on Human factors in computing
systems, pages 281?285.
Christiane Fellbaum. 1998. WordNet: An electronic lex-
ical database. MIT press.
Thomas K. Landauer and Susan T. Dumais. 1997. A so-
lution to plato?s problem: The latent semantic analysis
theory of acquisition, induction and representation of
knowledge. Psychological review, 104:211?240.
Thomas K. Landauer, Peter W. Foltz, and Darrell La-
ham. 1998. An introduction to latent semantic analy-
sis. Discourse processes, 25:259?284.
Michael Lesk. 1986. Automatic sense disambiguation
using machine readable dictionaries: How to tell a pine
cone from an ice cream cone. In Proceedings of the
5th annual international conference on Systems Docu-
mentation, pages 24?26.
Esther Levin, Mehrbod Sharifi, and Jerry Ball. 2006.
Evaluation of utility of lsa for word sense discrimina-
tion. In Proceedings of HLT/NAACL, pages 77?80.
Wei Song and Soon Cheol Park. 2007. A novel docu-
ment clustering model based on latent semantic analy-
sis. In Proceedings of the Third International Confer-
ence on Semantics, Knowledge and Grid, pages 539?
542.
46
Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 337?340,
Queen Mary University of London, September 2009. c?2009 Association for Computational Linguistics
?Vm?????m??%<S?j?m??V<SSm?jm??
? ??
? ?
?
???"<9?
P??Vm?g%mS~??g???mm9V?<?~? S<?j?<jm??"?9?m??%?j? Vm? %?"?~?9%??? ?g? ??V<SSm?jm???V<??VmS?m~?g?9???Vm?g?9???<? g%mS~??<SS???%?j?~m<%Sm~? 9??<"%??????g? ??m?? <?~?m9V?%6?m??? ?"%?j%?j? ?m?? m?m"?? %???Vm? g%mS~?? <?~? g<9%S%<%?j? <~M<?9mm???%?? 9?"m?"m?m<"9Vd? ??SV??jV? Vm? %~m<??g? <?????m?? ~%<S?j?m? 9V<SSm?jm? V<?? ?mm?? ~%??9???m~? g?"???m?%m?? V%?? %?? Vm? g%"??<?m?? ???"%?j? Vm?m?~%?9???%???? ?jmVm"?<?~?<?m?9??9"mm?<9%??d???
{? ??%M<%??? g?"? <? 9m?"<S%?m~? ?V<S?Sm?jm?
?Vm? %~m<? ?g? <? 9m?"<S%?m~? 9V<SSm?jm? %?? Vm? g%mS~???g? ??mm9V? <?~? S<?j?<jm??Vm"m? ~%ggm"m?? ??m??<?~? m9V?%6?m?? <"m? <??S%m~? ?? Vm? ?<m? ~<<?V<???mm???%V? ??? g?"? ??m? %md? ??"??<?S? Vm????Sm?jV?<?~???99m??g?S?9V<SSm?jm?%??Vm??????g??~m~? ??mm9V? "m9?j?%%??? ?m? ?g? 9V<SSm?jm?? +B??V%9V??<"m~?%??Vm?{Yp???<?~?9<m???<??m<??%??Vm?{YY??d? ??Vm??%SS?9??%??m? V"??jV?~%ggm"m???"?j"<???<?~?%????%m??g??m%?j?9"%%9%?m~?g?"? g??9??%?j? ???<%9? ??mm9V? ?"?9m??%?j? ?????? ????%?Sm? m<??"m?? ?g? ??99m???? %? %?? 9Sm<"? V<? Vm?V<Mm?VmS?m~?<?m?????g??~<m?<SS??mm"d????Vm? %??"<?? ??%??? %?? <? ??99m??g?S? 9V<S?Sm?jm?<"m??<??mSS?~mg%?m~?<???V<?Vm?9???%?9???%~m"?? ?? ?m? 9V<SSm?j%?j? g?"? Vm? 9?""m?? ?<m??g? Vm?<"????gg%9%m?? ???m"??g??<"%9%?<????%V?M<"%m~???m?????V<?Vm??m"g?"<?9m??g?~%ggm"?m?? m9V?%6?m?? 9<?? ?m? mM<S?<m~?? jm?m"<SS? <9?9m?m~?mM<S?<%?????99m???9"%m"%<??<?~?<??m??m??g?9?SS<??"<%????m?mm???<"%9%?<??????V<?~m<%S???g? Vm%"? m?"%m?? <? ?m? "m<???<?S? ?V<"m~d? ? P? %??%??"<??V<?Vm?9V<SSm?jm?~?m??????m9?m?<????"%jV? 9??m%%???? ??? ?"%M%?j? ???"?~?9m? Vm???m?????m?m?9??"<jm??m?S?"%?j??m??%~m<?d??
(? P?"?~?9%???
 %V%?? Vm? g%mS~? ?g? ????m?? ~%<S?j?m? ??m???m"V<??? Vm? ??? ?%%S<"? 9??"~%?<m~? mM<S?<%????g? ?S%?Sm? ??m?? ?<?? ?<"? ?g? Vm? ?????g??~m~? ????%9<?"? ?"?j"<d? ? ?V%?? ?"?j"<?j<Mm? ????m?? ~%<S?j?m? ??m? <99m??? ?? <? gS%jV?%?g?"<%???<?~?????%?j???md??P??(????(??{?<????m"? ?g? ??m?? ~mMmS??m~? <?? ?<"? ?g? Vm??"?j"<? ?m"m? 9m?"<SS? mM<S?<m~? ??P??d? ? ?????m"??g? mm"?<S?9<SSm"??<99m??m~? m<9V???m?<?~? Vm? "m??S?? ?m"m? ???S%?Vm~? %?? YBd? ? ?%?9m?????m?? ~%<S?j?m? ??m?? ~?? ??? V<Mm? <?? 9Sm<"? <?m<??"m? ?g? ??99m??? <?? ?????? <? ???m"? ?g?~%ggm"m??m<??"m???m"m?~m?%j?m~????V???m9%Mm?<?~? ???m9%Mmd? ? ??~? ?Vm"? <?<S?m?? ?g? Vm?mM<S?<%??? ?m"m? <~m? ?? <? ???m"? ?g? j"?????%?9S?~%?j?{Bd??
Vm"? 9??m%%???? V<Mm? ?mm?? ~mMmS??m~?%?9S?~%?j?Vm??m??m"??"%?m?B???V%9V?<~~"m??m??Vm? %???m?? ?g? ?<? Sm<?? ?"%j%?<SS?? m??<?m~?9??Mm"?<%???? V<? <m?? ?? ?<??? Vm? ??"%?j??m?d? ? ?V%?? m<"?? V??mMm"?? Vm??m??m"? ?"%?m? %???m%?j?VmS~? %?? 9????9%????%V? P?m"??mm9V?(??Y?<?~??%SS?V<Mm?<???mm9V?9????m?d?
+? ?"????m~?????m??%<S?j?m??V<SSm?jm?
?SV??jV? %? %?? 9Sm<"? Vm"m? <"m? <?? ????%?Sm?9V?%9m??g?"?V%??9V<SSm?jm??m?gmmS?Vm?g%"??"???~??g? Vm? 9V<SSm?jm? ?V??S~? ?m? ?%?Sm? <?~? 9Sm<"?? <S?S??%?j? g?"? Vm? ?m?? ?g? <???? ?? j"??? %?? M<"%m??Mm"?Vm?m<"?d???"?%?%%<S??"????<S???%S~???????"?????m?m"%m?9m?%????%S~%?j?~m?S?m~?????m??~%<?S?j?m???m?d?? ?"???m?V<??<S"m<~?9?SSm9m~?<?S<"jm????m"??g?m<?Sm?~%<S?j?m??V<?9<???m???m~? g?"? "<%?%?j? <?~?<~m? <M<%S<?Sm? %?? Vm? g%"??m<"????<"%9%?<??d????Vm? <"jm? ~?<%?? %?? ???? %?g?"<%??d? ? ?V%???<??9V??m???m9<??m?%?%??<?????S<"?~?<%?????m~????mMm"<S?~%ggm"m?? ~%<S?j?m???m?d? ? P? %????m?g?S???Vm????S%9?<?~?~?m???"m6?%"m?~m<S%?j??%V??m??%%Mm? %?g?"<%??d? ? ?Vm? ??m? %?? ?%?Sm?m???jV? V<? %? 9<?? ?m? "m?%?Smm?m~? ?%V???
?S<?? ? S<9??<?~??<%?m? ??m?<?%?<?j?<jm??m9V??S?j%m??P??%?m??<"?mj%m??mSS??? ?%Mm"?%???%???"jV?????? ???<???< 9?d9?dm~???<mB?
?
337
mm??%Mm?<~~%%??<S???"?d??m?V<Mm?<S"m<~?9?S?Sm9m~? ?Mm"?	
????? ~m%~m?%g%m~? ~%<S?j?m?? %?? Vm?~?<%?? V<? ?m? 9<?? ~%?"%??m? g?"? "<%?%?j? ??"????m?d??Vm? ?m9??~? %??"<??~m<%S? ??~mg%?m? %???V<??m?m?m9??g?Vm??<"%9%?<????Vm?<???d???Vm"m?%??<??%~m?M<"%m??g?"m?m<"9V?%?m"m???%??Vm?9?""m??????m??~%<S?j?m? g%mS~d? P? %?? %????%?Sm? g?"?<??%??jSm?<??????"?M%~m???%<?Sm?9V<SSm?jm??g?"?mMm"???m?%??%??g%"??%m"<%???????%?%??%??"<????g%?~?<?<???%???V%9V?<?????<?%<S?????m?9<??9??mmd?%Mm?? Vm? ???? %?g?"<%??? <??? <?? Vm? ????%~m?"m<9V%?j?g?"?Vm?g%"??m<"???m??mm?V"mm?SmM?mS???g??<"%9%?<%????{d??%S~? <? ???? %?g?"<%??? ??m? ?%V???"?????m??~%<S?j?m?<"9V%m9?"m?(d? ?%S~? <? ???? %?g?"<%??? ??m? ?%V?g"mm? ??g?<"m? ??S?? ??9V? <??S????PPd?+d? ?<?m? Vm? m%?%?j?m?? ?????P?g?"?<%??? ??m? <?~? <~<?? %??%V? ??"?9????m??d??Vm?m?V"mm?SmMmS???ggm"?<?9m"<%??<?????g?g"mm?~?? ?? Vm??<"%9%?<??d? ? Pg? ?<"%9%?<??? <"m???S?%?m"m?m~? %?? ??m? ?<SS? ?<"? ?g? Vm? ~%<S?j?m? <???Vm?9<??<~~m~??m??9????m?????Vm?m?? ??
??? P?g?"<%??? ??m? 	B?? ??9V? <?? <? ?m?? "m9??j?%?m"?? <? ?m?? ??Vm?%?m"?? ?"? <~~"m??? 9?"m? ~%<?S?j?9????m???S%?m?m""?"?"m9?Mm"?m9d???Vm?%?%?%<S? S<?j?<jm? ???S~? ?m? ?????jS%?V? ??? ?m????S~?S%?m???mm?~?V%??%??S<m"?m<"?d?
+d{? M<S?<%???
M<S?<%??? ?g? ????m?? ~%<S?j?m? ??m?? %?? ?%SS?Mm"? ?9V? <? "m?m<"9V? %???m? <?~? ?m? ?mm? Vm?????m?? %<S?j?m? ?V<SSm?jm? <?? <? m9V<?%?? ??<%~?V<?<"m<??g?"m?m<"9Vd???SS??%?j??"?9?"m??%??Vm???mm9V???Vm?%??S%??<"~??V<SSm?jm?(B???m??"????m? ??V<Mm?<????m"??g? ~%ggm"m??j"??????g???m"??9<SS?Vm????%m~???m?d?"????{???%<S?j??m?m<"9Vm"???m<9V??<"%9%?<??j"?????%SS??"?M%~m~?<????m"??g?9<SSm"???V???%SS??m?j%Mm???9m?<"%???<?~?<??m~? ??9<SS???m??g?Vm??<"%9%?<%?j???m?d? ?%<S?j?m??m?m<"9Vm"??<"m??"??<?S? Vm??m????m"???g? <???m?<?~?<"m? <S???Vm???? ?m"???<SS? %?m"m?m~? %?? 9??Sm%?j? Vm?<??d?"???? (? ? <%Mm? ??m<?m"? ?~m"j"<~?<m???V%?? j"???? ?%SS? ?m? ?<%~? <?~? %?m?~m~? ?? ?m? Vm????V??jm?m????j"?????g?9<SSm"?d???"????+??? ?S??mm"?????"m6?m?%?j?g?"?M?S???mm"?? V"??jV?<%S%?j? S%??? <?~? Vm? ?m?? ?m? ?%SS?9?SSm9?Vm?V%"~??m??g?9<SSm"?d??
<M%?j? V"mm? ?m?? ?g? 9<SSm"???%SS? m?<?Sm? ??? ???m"g?"? 9?""mS<%??? ?m?mm?? Vm? j"????? <?~?Vm"mg?"m?"???g%?~?"mS%<?Sm??<%?%9?d??
m?<S????"????m? ??"??? Vm??V<SSm?jm???? ???SmMmS?d???SS??<"%9%?<%?j???m???%SS?<?m??<"?%??Vm?m?%?%%<S?mM<S?<%???d???Vm??m????"?????<?Sm????m?? 9<?? Vm?? ?m? ~m?S?m~? ??? Vm?m????
%Mm? ??m? ?V%9V? ?"?M%~m?? ???? %?g?"<%??? ??Vm? ?m??Sm? ?g? ?%???"jVd? ? ?V??? "m<S? ??m"??? ?V??<"m? %?m"m?m~? %?? Vm? %m? ?g? Vm? ?m? ???? "<Vm"?V<?? Vm? ??99m??? ?g? Vm? ~%<S?j?m? ??m?? ?%SS??"?M%~m?<??<~~%%??<S??m??g?9<SSm"??<%?%9?d??Vm? g%"?? j"????? ?g? ??m"?? ?%SS? ?m? j%Mm???9m?<"%??? ?Vm? <"m? ??S%?mS? ?? ?m? g<%S%<"? ?%V??%???"jV? ????m???? <?~? ?%SS? g%SS? %?? <? ?m??6?m?%???<%"m? <gm"? m<9V? 9<SSd? ??"? Vm? S%Mm? m???%V?"m<S???m"?????6?m?%???<%"m??%SS??m?????%?Smd??m<S? ??m"?? <"m? ??%?m"m?m~? %?? <???m"%?j? <??g?"Vm"? 6?m?%????? <?~? <? ~m?%j?? V<? jm?? ??m?????m? ?? g%SS? %?? 6?m?%???<%"m?? ???S~? ?"??<?S?%mS~?<?~%ggm"m??"m??S?V<???<?~<"~?"m<S???m"?d?
M<S?<%????%SS??m? V"??jV???Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 1?6,
Baltimore, Maryland USA, June 23-24, 2014. c?2014 Association for Computational Linguistics
Cross-Lingual Information to the Rescue in Keyword Extraction 
 
1Chung-Chi Huang 2Maxine Eskenazi 3Jaime Carbonell 4Lun-Wei Ku 5Ping-Che Yang 
1,2,3Language Technologies Institute, CMU, United States 
4Institute of Information Science, Academia Sinica, Taiwan 
5Institute for Information Industry, Taipei, Taiwan 
{1u901571,4lunwei.jennifer.ku}@gmail.com 
{2max+,3jgc}@cs.cmu.edu 5maciaclark@iii.org.tw 
 
  
  
Abstract 
We introduce a method that extracts keywords 
in a language with the help of the other. In our 
approach, we bridge and fuse conventionally 
irrelevant word statistics in languages. The 
method involves estimating preferences for 
keywords w.r.t. domain topics and generating 
cross-lingual bridges for word statistics 
integration. At run-time, we transform parallel 
articles into word graphs, build cross-lingual 
edges, and exploit PageRank with word 
keyness information for keyword extraction. 
We present the system, BiKEA, that applies 
the method to keyword analysis. Experiments 
show that keyword extraction benefits from 
PageRank, globally learned keyword 
preferences, and cross-lingual word statistics 
interaction which respects language diversity. 
1 Introduction 
Recently, an increasing number of Web services 
target extracting keywords in articles for content 
understanding, event tracking, or opinion mining. 
Existing keyword extraction algorithm (KEA) 
typically looks at articles monolingually and 
calculate word significance in certain language. 
However, the calculation in another language 
may tell the story differently since languages 
differ in grammar, phrase structure, and word 
usage, thus word statistics on keyword analysis. 
Consider the English article in Figure 1. Based 
on the English content alone, monolingual KEA 
may not derive the best keyword set. A better set 
might be obtained by referring to the article and 
its counterpart in another language (e.g., 
Chinese). Different word statistics in articles of 
different languages may help, due to language 
divergence such as phrasal structure (i.e., word 
order) and word usage and repetition (resulting 
from word translation or word sense) and so on. 
For example, bilingual phrases ?social 
reintegration? and ?????? in Figure 1 have 
inverse word orders (?social? translates into ??
? ? and ?reintegration? into ? ? ? ?), both 
?prosthesis? and ?artificial limbs? translate into 
????, and ?physical? can be associated with ??
? ? and ??? ? in ?physical therapist? and 
?physical rehabilitation? respectively. Intuitively, 
using cross-lingual statistics (implicitly 
leveraging language divergence) can help look at 
articles from different perspectives and extract 
keywords more accurately. 
We present a system, BiKEA, that learns to 
identify keywords in a language with the help of 
the other. The cross-language information is 
expected to reinforce language similarities and 
value language dissimilarities, and better 
understand articles in terms of keywords. An 
example keyword analysis of an English article 
is shown in Figure 1. BiKEA has aligned the 
parallel articles at word level and determined the 
scores of topical keyword preferences for words. 
BiKEA learns these topic-related scores during 
training by analyzing a collection of articles. We 
will describe the BiKEA training process in more 
detail in Section 3. 
At run-time, BiKEA transforms an article in a 
language (e.g., English) into PageRank word 
graph where vertices are words in the article and 
edges between vertices indicate the words? co-
occurrences. To hear another side of the story, 
BiKEA also constructs graph from its counterpart 
in another language (e.g., Chinese). These two 
independent graphs are then bridged over nodes 
1
  
 
 
 
 
 
 
 
 
 
Figure 1. An example BiKEA keyword analysis for an article.
that are bilingually equivalent or aligned. The 
bridging is to take language divergence into 
account and to allow for language-wise 
interaction over word statistics. BiKEA, then in 
bilingual context, iterates with learned word 
keyness scores to find keywords. In our 
prototype, BiKEA returns keyword candidates of 
the article for keyword evaluation (see Figure 1); 
alternatively, the keywords returned by BiKEA 
can be used as candidates for social tagging the 
article or used as input to an article 
recommendation system. 
2 Related Work 
Keyword extraction has been an area of active 
research and applied to NLP tasks such as 
document categorization (Manning and Schutze, 
2000), indexing (Li et al., 2004), and text mining 
on social networking services ((Li et al., 2010); 
(Zhao et al., 2011); (Wu et al., 2010)). 
The body of KEA focuses on learning word 
statistics in document collection. Approaches 
such as tfidf and entropy, using local document 
and/or across-document information, pose strong 
baselines. On the other hand, Mihalcea and 
Tarau (2004) apply PageRank, connecting words 
locally, to extract essential words. In our work, 
we leverage globally learned keyword 
preferences in PageRank to identify keywords. 
Recent work has been done on incorporating 
semantics into PageRank. For example, Liu et al. 
(2010) construct PageRank synonym graph to 
accommodate words with similar meaning. And 
Huang and Ku (2013) weigh PageRank edges 
based on nodes? degrees of reference. In contrast, 
we bridge PageRank graphs of parallel articles to 
facilitate statistics re-distribution or interaction 
between the involved languages. 
In studies more closely related to our work, 
Liu et al. (2010) and Zhao et al. (2011) present 
PageRank algorithms leveraging article topic 
information for keyword identification. The main 
differences from our current work are that the 
article topics we exploit are specified by humans 
not by automated systems, and that our 
PageRank graphs are built and connected 
bilingually. 
In contrast to the previous research in keyword 
extraction, we present a system that 
automatically learns topical keyword preferences 
and constructs and inter-connects PageRank 
graphs in bilingual context, expected to yield 
better and more accurate keyword lists for 
articles. To the best of our knowledge, we are the 
first to exploit cross-lingual information and take 
advantage of language divergence in keyword 
extraction. 
3 The BiKEA System 
Submitting natural language articles to keyword 
extraction systems may not work very well. 
Keyword extractors typically look at articles 
from monolingual points of view. Unfortunately, 
word statistics derived based on a language may 
The English Article: 
I've been in Afghanistan for 21 years. I work for the Red Cross and I'm a physical therapist. My job is to 
make arms and legs -- well it's not completely true. We do more than that. We provide the patients, the 
Afghan disabled, first with the physical rehabilitation then with the social reintegration. It's a very logical 
plan, but it was not always like this. For many years, we were just providing them with artificial limbs. It 
took quite many years for the program to become what it is now. ? 
 
Its Chinese Counterpart: ??????? 21 ?? ????????? ?????????? ??????????? -- ?????????? ?????????? ???????? ???????? ???????, ??????? ???????????? ?????????? ??????????? ????? ???????? ?????????????? 
 
Word Alignment Information: 
physical (??), therapist (???), social (??), reintegration (??), physical (??), rehabilitation  (??), prosthesis (??), ? 
 
Scores of Topical Keyword Preferences for Words: 
(English)    prosthesis: 0.32; artificial leg: 0.21; physical therapist: 0.15; rehabilitation: 0.08; ? 
(Chinese)   ??: 0.41; ?????: 0.15; ??:0.10; ???: 0.08, ? 
 
English Keywords from Bilingual Perspectives: 
prosthesis, artificial, leg, rehabilitation, orthopedic, ? 
2
be biased due to the language?s grammar, phrase 
structure, word usage and repetition and so on. 
To identify keyword lists from natural language 
articles, a promising approach is to automatically 
bridge the original monolingual framework with 
bilingual parallel information expected to respect 
language similarities and diversities at the same 
time.  
3.1 Problem Statement 
We focus on the first step of the article 
recommendation process: identifying a set of 
words likely to be essential to a given article. 
These keyword candidates are then returned as 
the output of the system. The returned keyword 
list can be examined by human users directly, or 
passed on to article recommendation systems for 
article retrieval (in terms of the extracted 
keywords). Thus, it is crucial that keywords be 
present in the candidate list and that the list not 
be too large to overwhelm users or the 
subsequent (typically computationally expensive) 
article recommendation systems. Therefore, our 
goal is to return reasonable-sized set of keyword 
candidates that, at the same time, must contain 
essential terms in the article. We now formally 
state the problem that we are addressing. 
Problem Statement: We are given a bilingual 
parallel article collection of various topics from 
social media (e.g., TED), an article ARTe in 
language e, and its counterpart ARTc in language 
c. Our goal is to determine a set of words that are 
likely to contain important words of ARTe. For 
this, we bridge language-specific statistics of 
ARTe and ARTc via bilingual information (e.g., 
word alignments) and consider word keyness 
w.r.t. ARTe?s topic such that cross-lingual 
diversities are valued in extracting keywords in e. 
In the rest of this section, we describe our 
solution to this problem. First, we define 
strategies for estimating keyword preferences for 
words under different article topics (Section 3.2). 
These strategies rely on a set of article-topic 
pairs collected from the Web (Section 4.1), and 
are monolingual, language-dependent 
estimations. Finally, we show how BiKEA 
generates keyword lists for articles leveraging 
PageRank algorithm with word keyness and 
cross-lingual information (Section 3.3). 
3.2 Topical Keyword Preferences 
We attempt to estimate keyword preferences 
with respect to a wide range of article topics. 
Basically, the estimation is to calculate word 
significance in a domain topic. Our learning 
process is shown in Figure 2. 
 
 
 
 
 
 
Figure 2. Outline of the process used 
to train BiKEA. 
In the first two stages of the learning process, we 
generate two sets of article and word information. 
The input to these stages is a set of articles and 
their domain topics. The output is a set of pairs 
of article ID and word in the article, e.g., 
(ARTe=1, we=?prosthesis?) in language e or 
(ARTc=1, wc=????) in language c, and a set of 
pairs of article topic and word in the article, e.g., 
(tpe=?disability?, we=?prosthesis?) in e and 
(tpe=?disability?, wc=????) in c. Note that the 
topic information is shared between the involved 
languages, and that we confine the calculation of 
such word statistics in their specific language to 
respect language diversities and the language-
specific word statistics will later interact in 
PageRank at run-time (See Section 3.3). 
The third stage estimates keyword preferences 
for words across articles and domain topics using 
aforementioned (ART,w) and (tp,w) sets. In our 
paper, two popular estimation strategies in 
Information Retrieval are explored. They are as 
follows. 
tfidf. tfidf(w)=freq(ART,w)/appr(ART?,w) where 
term frequency in an article is divided by its 
appearance in the article collection to distinguish 
important words from common words. 
ent. entropy(w)= -?
tp?
Pr(tp?|w)?log(Pr(tp?|w)) 
where  a word?s uncertainty in topics is used to 
estimate its associations with domain topics. 
These strategies take global information (i.e., 
article collection) into account, and will be used 
as keyword preference models, bilingually 
intertwined, in PageRank at run-time which 
locally connects words (i.e., within articles). 
3.3 Run-Time Keyword Extraction 
Once language-specific keyword preference 
scores for words are automatically learned, they 
are stored for run-time reference. BiKEA then 
uses the procedure in Figure 3 to fuse the 
originally language-independent word statistics 
(1) Generate article-word pairs in training data 
(2) Generate topic-word pairs in training data 
(3) Estimate keyword preferences for words w.r.t.  
      article topic based on various strategies 
(4) Output word-and-keyword-preference-score  
      pairs for various strategies 
3
to determine keyword list for a given article. In 
this procedure a machine translation technique 
(i.e., IBM word aligner) is exploited to glue 
statistics in the involved languages and make 
bilingually motivated random-walk algorithm 
(i.e., PageRank) possible. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 3. Extracting keywords at run-time. 
 
Once language-specific keyword preference 
scores for words are automatically learned, they 
are stored for run-time reference. BiKEA then 
uses the procedure in Figure 3 to fuse the 
originally language-independent word statistics 
to determine keyword list for a given article. In 
this procedure a machine translation technique 
(i.e., IBM word aligner) is exploited to glue 
statistics in the involved languages and make 
bilingually motivated random-walk algorithm 
(i.e., PageRank) possible.  
In Steps (1) and (2) we construct PageRank 
word graphs for the article ARTe in language e 
and its counterpart ARTc in language c. They are 
built individually to respect language properties 
(such as subject-verb-object or subject-object-
verb structure). Figure 4 shows the algorithm. In 
this algorithm, EW stores normalized edge 
weights for word wi and wj (Step (2)). And EW 
is a v by v matrix where v is the vocabulary size 
of ARTe and ARTc. Note that the graph is directed 
(from words to words that follow) and edge 
weights are words? co-occurrences within 
window size WS. Additionally we incorporate 
edge weight multiplier m>1 to propagate more 
PageRank scores to content words, with the 
intuition that content words are more likely to be 
keywords (Step (2)). 
 
 
 
 
 
 
 
Figure 4. Constructing PageRank word graph. 
Step (3) in Figure 3 linearly combines word 
graphs EWe and EWc using ?. We use ? to 
balance language properties or statistics, and 
BiKEA backs off to monolingual KEA if ? is one. 
In Step (4) of Figure 3 for each word 
alignment (wic, wje), we construct a link between 
the word nodes with the weight BiWeight. The 
inter-language link is to reinforce language 
similarities and respect language divergence 
while the weight aims to elevate the cross-
language statistics interaction. Word alignments 
are derived using IBM models 1-5 (Och and Ney, 
2003). The inter-language link is directed from 
wi
c
 to wj
e
, basically from language c to e based on 
the directional word-aligning entry (wic, wje). The 
bridging is expected to help keyword extraction 
in language e with the statistics in language c. 
Although alternative approach can be used for 
bridging, our approach is intuitive, and most 
importantly in compliance with the directional 
spirit of PageRank. 
Step (6) sets KP of keyword preference model 
using topical preference scores learned from 
Section 3.2, while Step (7) initializes KN of 
PageRank scores or, in our case, word keyness 
scores. Then we distribute keyness scores until 
the number of iteration or the average score 
differences of two consecutive iterations reach 
their respective limits. In each iteration, a word?s 
keyness score is the linear combination of its 
keyword preference score and the sum of the 
propagation of its inbound words? previous 
PageRank scores. For the word wje in ARTe, any 
edge (wie,wje) in ARTe, and any edge (wkc,wje) in 
WA, its new PageRank score is computed as 
below. 
procedure PredictKW(ARTe,ARTc,KeyPrefs,WA,?,N) 
//Construct language-specific word graph for PageRank 
(1)  EWe=constructPRwordGraph(ARTe) 
(2)  EWc=constructPRwordGraph(ARTc) 
//Construct inter-language bridges 
(3)  EW=?? EWe+(1-?) ? EWc 
       for each word alignment (wic, wje) in WA 
         if IsContWord(wic) and IsContWord(wje) 
(4a)      EW[i,j]+=1? BiWeightcont 
         else 
(4b)      EW[i,j]+=1? BiWeightnoncont 
(5)  normalize each row of EW to sum to 1 
//Iterate for PageRank 
(6)  set KP1 ?v to 
             [KeyPrefs(w1), KeyPrefs(w2), ?,KeyPrefs(wv)] 
(7)  initialize KN1 ?v to [1/v,1/ v, ?,1/v] 
       repeat 
(8a)  KN?=?? KN? EW+(1-?) ? KP 
(8b)  normalize KN? to sum to 1 
(8c)  update KN with KN? after the check of KN and KN? 
       until maxIter or avgDifference(KN,KN?) ? smallDiff 
(9)  rankedKeywords=Sort words in decreasing order of KN 
       return the N rankedKeywords in e with highest 
scores 
procedure constructPRwordGraph(ART) 
(1) EWv ?v=0v ?v 
      for each sentence st in ART 
         for each word wi in st 
            for each word wj in st where i<j and j-i ? WS 
         if not IsContWord(wi) and IsContWord(wj) 
(2a)            EW[i,j]+=1? m 
               elif not IsContWord(wi) and not IsContWord(wj) 
(2b)            EW[i,j]+=1 ? (1/m) 
               elif IsContWord(wi) and not IsContWord(wj) 
(2c)            EW[i,j]+=1? (1/m) 
               elif IsContWord(wi) and IsContWord(wj) 
(2d)            EW[i,j]+=1 ? m 
       return EW 
4
???[1, ?] =? ?
??
? ? ????[1, ?] ? ???[?, ?] +???(1 ? ?) ????[1, ?] ? ??[?, ?]??? ??
?
+ (1 ??) ? ??[1, ?] 
 
Once the iterative process stops, we rank 
words according to their final keyness scores and 
return top N ranked words in language e as 
keyword candidates of the given article ARTe. An 
example keyword analysis for an English article 
on our working prototype is shown in Figure 1. 
Note that language similarities and dissimilarities 
lead to different word statistics in articles of 
difference languages, and combining such word 
statistics helps to generate more promising 
keyword lists. 
4 Experiments 
BiKEA was designed to identify words of 
importance in an article that are likely to cover 
the keywords of the article. As such, BiKEA will 
be trained and evaluated over articles. 
Furthermore, since the goal of BiKEA is to 
determine a good (representative) set of 
keywords with the help of cross-lingual 
information, we evaluate BiKEA on bilingual 
parallel articles. In this section, we first present 
the data sets for training BiKEA (Section 4.1). 
Then, Section 4.2 reports the experimental 
results under different system settings. 
4.1 Data Sets 
We collected approximately 1,500 English 
transcripts (3.8M word tokens and 63K word 
types) along with their Chinese counterparts 
(3.4M and 73K) from TED (www.ted.com) for 
our experiments. The GENIA tagger (Tsuruoka 
and Tsujii, 2005) was used to lemmatize and 
part-of-speech tag the English transcripts while 
the CKIP segmenter (Ma and Chen, 2003) 
segment the Chinese. 
30 parallel articles were randomly chosen and 
manually annotated for keywords on the English 
side to examine the effectiveness of BiKEA in 
English keyword extraction with the help of 
Chinese. 
4.2 Experimental Results 
Table 1 summarizes the performance of the 
baseline tfidf and our best systems on the test set. 
The evaluation metrics are nDCG (Jarvelin and 
Kekalainen, 2002), precision, and mean 
reciprocal rank. 
(a) @N=5 nDCG P MRR 
tfidf .509 .213 .469 
PR+tfidf .676 .400 .621 
BiKEA+tfidf .703 .406 .655 
 
(b) @N=7 nDCG P MRR 
tfidf .517 .180 .475 
PR+tfidf .688 .323 .626 
BiKEA+tfidf .720 .338 .660 
 
(c) @N=10 nDCG P MRR 
tfidf .527 .133 .479 
PR+tfidf .686 .273 .626 
BiKEA+tfidf .717 .304 .663 
Table 1. System performance at 
(a) N=5 (b) N=7 (c) N=10. 
As we can see, monolingual PageRank (i.e., 
PR) and bilingual PageRank (BiKEA), using 
global information tfidf, outperform tfidf. They 
relatively boost nDCG by 32% and P by 87%. 
The MRR scores also indicate their superiority: 
their top-two candidates are often keywords vs. 
the 2nd place candidates from tfidf. 
Encouragingly, BiKEA+tfidf achieves better 
performance than the strong monolingual 
PR+tfidf across N?s. Specifically, it further 
improves nDCG relatively by 4.6% and MRR 
relatively by 5.4%. 
Overall, the topical keyword preferences, and 
the inter-language bridging and the bilingual 
score propagation in PageRank are simple yet 
effective. And respecting language statistics and 
properties helps keyword extraction. 
5 Summary 
We have introduced a method for extracting 
keywords in bilingual context. The method 
involves estimating keyword preferences, word-
aligning parallel articles, and bridging language-
specific word statistics using PageRank. 
Evaluation has shown that the method can 
identify more keywords and rank them higher in 
the candidate list than monolingual KEAs. As for 
future work, we would like to explore the 
possibility of incorporating the articles? reader 
feedback into keyword extraction. We would 
also like to examine the proposed methodology 
in a multi-lingual setting.  
5
Acknowledgement 
This study is conducted under the ?Online and 
Offline integrated Smart Commerce Platform 
(1/4)? of the Institute for Information Industry 
which is subsidized by the Ministry of Economy 
Affairs of the Republic of China. 
References  
Scott A. Golder and Bernardo A. Huberman. 
2006. Usage patterns of collaborative tagging 
systems. Information Science, 32(2): 198-208. 
Harry Halpin, Valentin Robu, and Hana 
Shepherd. 2007. The complex dynamics of 
collaborative tagging. In Proceedings of the 
WWW, pages 211-220. 
Chung-chi Huang and Lun-wei Ku. 2013. 
Interest analysis using semantic PageRank and 
social interaction content. In Proceedings of 
the ICDM Workshop on Sentiment Elicitation 
from Natural Text for Information Retrieval 
and Extraction, pages 929-936. 
Kalervo Jarvelin and Jaana Kekalainen. 2002. 
Cumulated gain-based evaluation of IR 
technologies. ACM Transactions on 
Information Systems, 20(4): 422-446. 
Philipp Koehn, Franz Josef Och, and Daniel 
Marcu. 2003. Statistical phrase-based 
translation. In Proceedings of the North 
American Chapter of the Association for 
Computational Linguistics, pages 48-54. 
Quanzhi Li, Yi-Fang Wu, Razvan Bot, and Xin 
Chen. 2004. Incorporating document 
keyphrases in search results. In Proceedings of 
the Americas Conference on Information 
Systems. 
Zhenhui Li, Ging Zhou, Yun-Fang Juan, and 
Jiawei Han. 2010. Keyword extraction for 
social snippets. In Proceedings of the WWW, 
pages 1143-1144. 
Marina Litvak and Mark Last. 2008. Graph-
based keyword extraction for single-document 
summarization. In Proceedings of the ACL 
Workshop on Multi-Source Multilingual 
Information Extraction and Summarization, 
pages 17-24. 
Zhengyang Liu, Jianyi Liu, Wenbin Yao, Cong 
Wang. 2010. Keyword extraction using 
PageRank on synonym networks. In 
Proceedings of the ICEEE, pages 1-4. 
Zhiyuan Liu, Wenyi Huang, Yabin Zheng, and 
Maosong Sun. 2010. Automatic keyphrase 
extraction via topic decomposition. In 
Proceedings of the EMNLP, pages 366-376. 
Wei-Yun Ma and Keh-Jiann Chen. 2003. 
Introduction to CKIP Chinese word 
segmentation system for the first international 
Chinese word segmentation bakeoff. In 
Proceedings of the ACL Workshop on Chinese 
Language Processing. 
Chris D. Manning and Hinrich Schutze. 2000. 
Foundations of statistical natural language 
processing. MIT Press. 
Rada Mihalcea and Paul Tarau. 2004. TextRank: 
Bringing orders into texts. In Proceedings of 
the EMNLP, pages 404-411. 
Franz Josef Och and Hermann Ney. 2003. A 
systematic comparison of various statistical 
alignment models. Computational Linguistics, 
29(1): 19-51. 
Yoshimasa Tsuruoka and Jun?ichi Tsujii. 2005. 
Bidirectional inference with the easiest-first 
strategy for tagging sequence data. In 
Proceedings of the EMNLP, pages 467-474. 
Peter D. Turney. 2000. Learning algorithms for 
keyphrase extraction. Information Retrieval, 
2(4): 303-336. 
Wei Wu, Bin Zhang, and Mari Ostendorf. 2010. 
Automatic generation of personalized 
annotation tags for Twitter users. In 
Proceedings of the NAACL, pages 689-692. 
Wayne Xin Zhao, Jing Jiang, Jing He, Yang 
Song, Palakorn Achananuparp, Ee-Peng Lim, 
and Xiaoming Li. 2011. Topical keyword 
extraction from Twitter. In Proceedings of the 
ACL, pages 379-388. 
 
6
Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon?s Mechanical Turk, pages 21?29,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Clustering dictionary definitions using Amazon Mechanical Turk  
Gabriel Parent Maxine Eskenazi 
Language Technologies Institute 
Carnegie Mellon University 
5000 Forbes Avenue 
15213 Pittsburgh, USA 
 
{gparent,max}@cs.cmu.edu 
 
 
 
Abstract 
Vocabulary tutors need word sense disambig-
uation (WSD) in order to provide exercises 
and assessments that match the sense of words 
being taught. Using expert annotators to build 
a WSD training set for all the words supported 
would be too expensive. Crowdsourcing that 
task seems to be a good solution.  However, a 
first required step is to define what the possi-
ble sense labels to assign to word occurrence 
are.  This can be viewed as a clustering task 
on dictionary definitions. This paper evaluates 
the possibility of using Amazon Mechanical 
Turk (MTurk) to carry out that prerequisite 
step to WSD.  We propose two different ap-
proaches to using a crowd to accomplish clus-
tering: one where the worker has a global 
view of the task, and one where only a local 
view is available.  We discuss how we can 
aggregate multiple workers? clusters together, 
as well as pros and cons of our two approach-
es.  We show that either approach has an inte-
rannotator agreement with experts that 
corresponds to the agreement between ex-
perts, and so using MTurk to cluster dictio-
nary definitions appears to be a reliable 
approach. 
1 Introduction 
For some applications it is useful to disambiguate 
the meanings of a polysemous word. For example, 
if we show a student a text containing a word like 
?bank? and then automatically generate questions 
about the meaning of that word as it appeared in 
the text (say as the bank of a river), we would like 
to have the meaning of the word in the questions 
match the text meaning. Teachers do this each time 
they assess a student on vocabulary knowledge.  
For intelligent tutoring systems, two options are 
available. The first one is to ask a teacher to go 
through all the material and label each appearance 
of a polysemous word with its sense.  This option 
is used only if there is a relatively small quantity of 
material. Beyond that, automatic processing, 
known as Word Sense Disambiguation (WSD) is 
essential. Most approaches are supervised and need 
large amounts of data to train the classifier for each 
and every word that is to be taught and assessed.  
Amazon Mechanical Turk (MTurk) has been 
used for the purpose of word sense disambiguation 
(Snow et al 2008). The results show that non-
experts do very well (100% accuracy) when asked 
to identify the correct sense of a word out of a fi-
nite set of labels created by an expert. It is there-
fore possible to use MTurk to build a training 
corpus for WSD. In order to extend the Snow et al
crowdsourced disambiguation to a large number of 
words, we need an efficient way to create the set of 
senses of a word. Asking an expert to do this is 
costly in time and money. Thus it is necessary to 
have an efficient Word Sense Induction (WSI) sys-
tem. A WSI system induces the different senses of 
a word and provides the corresponding sense la-
bels.  This is the first step to crowdsourcing WSD 
on a large scale. 
While many studies have shown that MTurk 
can be used for labeling tasks (Snow et al 2008), 
to rate automatically constructed artifacts 
(Callison-Burch, 2009, Alonso et al 2008) and to 
transcribe speech (Ledlie et al 2009, Gruenstein et 
al, 2009), to our knowledge, there has not been 
much work on evaluating the use of MTurk for 
21
clustering tasks. The goal of this paper is to inves-
tigate different options available to crowdsource a 
clustering task and evaluate their efficiency in the 
concrete application of word sense induction. 
2 Background 
2.1 WSD for vocabulary tutoring  
Our interest in the use of MTurk for disambigua-
tion comes from work on a vocabulary tutor; 
REAP (Heilman et al 2006). The tutor searches for 
documents from the Web that are appropriate for a 
student to use to learn vocabulary from context 
(appropriate reading level, for example). Since the 
system finds a large number of documents, making 
a rich repository of learning material, it is impossi-
ble to process all the documents manually. When a 
document for vocabulary learning is presented to a 
student, the system should show the definition of 
the words to be learned (focus words). In some 
cases a word has several meanings for the same 
part of speech and thus it has several definitions. 
Hence the need for WSD to be included in vocabu-
lary tutors. 
2.2 WSI and WSD 
The identification of a list of senses for a given 
word in a corpus of documents is called word 
sense induction (WSI). SemEval 2007 and 2010 
(SigLex, 2008) both evaluate WSI systems. The 
I2R system achieved the best results in 2007 with 
an F-score of 81.6% (I2R by Niu (2007)).  Snow et 
al (2007) have a good description of the inherent 
problem of WSI where the appropriate granularity 
of the clusters varies for each application. They try 
to solve this problem by building hierarchical-like 
word sense structures. In our case, each dictionary 
definition for a word could be considered as a 
unique sense for that word. Then, when using 
MTurk as a platform for WSD, we could simply 
ask the workers to select which of the dictionary 
definitions best expresses the meaning of the 
words in a document.  The problem here is that 
most dictionaries give quite several definitions for 
a word.    Defining one sense label per dictionary 
definition would result in too many labels, which 
would, in turn, make the MTurk WSD less effi-
cient and our dataset sparser, thus decreasing the 
quality of the classifier.  Another option, investi-
gated by Chklovski and Mihalcea (2003), is to use 
WordNet sense definitions as the possible labels.  
They obtained more than 100,000 labeled instances 
from a crowd of volunteers.  They conclude that 
WordNet senses are not coarse enough to provide 
high interannotator agreement, and exploit workers 
disagreement on the WSD task to derive coarser 
senses. 
The granularity of the senses for each word is a 
parameter that is dependent on the application. In 
our case, we want to be able to assess a student on 
the sense of a word that the student has just been 
taught. Learners have the ability to generalize the 
context in which a word is learned.  For example, 
if a student learns the meaning of the word ?bark? 
as the sound of a dog, they can generalize that this 
can also apply to human shouting. Hence, there is 
no need for two separate senses here. However, a 
student could not generalize the meaning ?hard 
cover of a tree? from that first meaning of ?bark?.  
This implies that students should be able to distin-
guish coarse word senses. (Kulkarni et al, 2007) 
have looked at automatic clustering of dictionary 
definitions. They compared K-Means clustering 
with Spectral Clustering. Various features were 
investigated: raw, normalized word overlap with 
and without stop words. The best combination re-
sults in 74% of the clusters having no misclassified 
definitions. If those misclassified definitions end 
up being used to represent possible sense labels in 
WSD, wrong labels might decrease the quality of 
the disambiguation stage. If a student is shown a 
definition that does not match the sense of a word 
in a particular context, they are likely to build the 
wrong conceptual link. Our application requires 
higher accuracy than that achieved by automatic 
approaches, since students? learning can be directly 
affected by the error rate.  
2.3 Clustering with MTurk 
The possible interaction between users and cluster-
ing algorithms has been explored in the past.  
Huang and Mitchell (2006) present an example of 
how user feedback can be used to improve cluster-
ing results.  In this study, the users were not asked 
to provide clustering solutions. Instead, they fine 
tuned the automatically generated solution. 
With the advent of MTurk, we can use human 
judgment to build clustering solutions. There are 
multiple approaches for combining workforce: pa-
rallel with aggregation (Snow et al 2008), iterative 
22
(Little et al 2009) and collaboration between 
workers (Horton, Turker Talk, 2009). These strate-
gies have been investigated for many applications, 
most of which are for labeling, a few for cluster-
ing. The Deneme blog presents an experiment 
where website clustering is carried out using 
MTurk (Little, Website Clustering, 2009). The 
workers? judgments on the similarity between two 
websites are used to build a distance matrix for the 
distance between websites. Jagadeesan and others 
(2009) asked workers to identify similar objects in 
a pool of 3D CAD models. They then used fre-
quently co-occurring objects to build a distance 
matrix, upon which they then applied hierarchical 
clustering. Those two approaches are different: the 
first gives the worker only two items of the set (a 
local view of the task), while the latter offers the 
worker a global view of the task. In the next sec-
tions we will measure the accuracy of these ap-
proaches and their advantages and disadvantages. 
3 Obtaining clusters from a crowd 
REAP is used to teach English vocabulary and to 
conduct learning studies in a real setting, in a local 
ESL school. The vocabulary tutor provides instruc-
tions for the 270 words on the school?s core voca-
bulary list, which has been built using the 
Academic Word List (Coxhead, 2000). In order to 
investigate how WSI could be accomplished using 
Amazon Mechanical Turk, 50 words were random-
ly sampled from the 270, and their definitions were 
extracted from the Longman Dictionary of Con-
temporary English (LDOCE) and the Cambridge 
Advanced Learner's Dictionary (CALD).  There 
was an average of 6.3 definitions per word. 
The problem of clustering dictionary definitions 
involves solving two sub-problems: how many 
clusters there are, and which definitions belong to 
which clusters.  We could have asked workers to 
solve both problems at the same time by having 
them dynamically change the number of clusters in 
our interface.  We decided not to do this due to the 
fact that some words have more than 12 defini-
tions. Since the worker already needs to keep track 
of the semantics of each cluster, we felt that having 
them modify the number of sense boxes would 
increase their cognitive load to the point that we 
would see a decrease in the accuracy of the results. 
Thus the first task involved determining the 
number of general meanings (which in our case 
determines the number of clusters) that there are in 
a list of definitions. The workers were shown the 
word and a list of its definitions, for example, for 
the word ?clarify?:  
 
 to make something clearer and easier to    
understand 
 to make something clear or easier to under-
stand by giving more details or a simpler explana-
tion 
 to remove water and unwanted substances 
from fat, such as butter, by heating it 
 
They were then asked: ?How many general 
meanings of the word clarify are there in the fol-
lowing definitions??  We gave a definition of what 
we meant by general versus specific meanings, 
along with several examples.  The worker was 
asked to enter a number in a text box (in the above 
example the majority answered 2).  This 2-cent 
HIT was completed 13 times for every 50 words, 
for a total of 650 assignments and $13.00. A ma-
jority vote was used to aggregate the workers? re-
sults, giving us the number of clusters in which the 
definitions were grouped.  In case of a tie, the low-
est number of clusters was retained, since our ap-
plication requires coarse-grained senses. 
The number of ?general meanings? we obtained 
in this first HIT1 was then used in two different 
HITs.  We use these two HITs to determine which 
definitions should be clustered together. In the first 
setup, which we called ?global-view? the workers 
had a view of the entire task. They were shown the 
word and all of its definitions. They were then 
prompted to drag-and-drop the definitions into dif-
ferent sense boxes, making sure to group the defi-
nitions that belong to the same general meaning 
together (Figure 3, Appendix). Once again, an ex-
plicit definition of what was expected for ?general 
meaning? along with examples was given. Also, a 
flash demo of how to use the interface was pro-
vided. The worker got 3 cents for this HIT. It was 
completed 5 times for each of the 50 words, for a 
total cost of $7.50. We created another HIT where 
the workers were not given all of the definitions; 
we called this setup ?local-view?.  The worker was 
asked to indicate if two definitions of a word were 
related to the same meaning or different meanings 
                                                          
1 The code and data used for the different HITs are available at 
http://www.cs.cmu.edu/~gparent/amt/wsi/ 
23
(Figure 4, Appendix).  For each word, we created 
all possible pairs of definitions. This accounts for 
an average of 21 pairs for all of the 50 words. For 
each pair, 5 different workers voted on whether it 
contained the same or different meanings, earning 
1 cent for each answer. The total cost here was 
$52.50. The agreement between workers was used 
to build a distance matrix: if the 5 workers agreed 
that the two definitions concerned the same sense, 
the distance was set to 0. Otherwise, it was set to 
the number of workers who thought they con-
cerned different senses, up to a distance of 5. Hie-
rarchical clustering was then used to build 
clustering solutions from the distance matrices. We 
used complete linkage clustering, with Ward?s cri-
terion. 
4 Evaluation of global-view vs. local-view 
approaches 
In order to evaluate our two approaches, we 
created a gold-standard (GS). Since the task of 
WSI is strongly influenced by an annotator?s grain 
size preference for the senses, four expert annota-
tors were asked to create the GS. The literature 
offers many metrics to compare two annotators? 
clustering solutions (Purity and Entropy (Zhao and 
Karypis, 2001), clustering F-Measure (Fung et al, 
2003) and many others).  SemEval-2 includes a 
WSI task where V-Measure (Rosenberg and Hir-
schberg, 2007) is used to evaluate the clustering 
solutions. V-Measure involves two metrics, homo-
geneity and completeness, that can be thought of as 
precision and recall.  Perfect homogeneity is ob-
tained if the solutions have clusters whose data 
points belong to a single cluster in the GS. Perfect 
completeness is obtained if the clusters in the GS 
contain data points that belong to a single cluster in 
the evaluated solution. The V-Measure is a 
(weighted) harmonic mean of the homogeneity and 
of the completeness metrics. Table 1 shows inter-
annotator agreement (ITA) among four experts on 
the test dataset, using the average V-Measure over 
all the 50 sense clusters. 
 
 
 
 
 
  GS #1 GS #2 GS #3 GS #4 
GS #1 1,000 0,850 0,766 0,770 
GS #2 0,850 1,000 0,763 0,796 
GS #3 0,766 0,763 1,000 0,689 
GS #4 0,770 0,796 0,689 1,000 
Table 1 - ITA on WSI task for four annotators 
 
We can obtain the agreement between one ex-
pert and the three others by averaging the three V-
Measures. We finally obtain an ?Experts vs. Ex-
perts? ITA of 0.772 by averaging this value for all 
of our experts. The standard deviation for this ITA 
is 0.031.To be considered reliable, non-expert clus-
tering would have to agree with the 4 experts with 
a similar result. 
5 Aggregating clustering solutions from 
multiple workers 
Using a majority vote with the local-view HIT is 
an easy way of taking advantage of the ?wisdom of 
crowd? principle. In order to address clustering 
from a local-view perspective, we need to build all 
possible pairs of elements. The number of those 
pairs is O(n2) on the number of elements to cluster. 
Thus the cost grows quickly for large clustering 
problems. For 100 elements to cluster there are 
4950 pairs of elements to show to workers. For 
large problems, a better approach would be to give 
the problem to multiple workers through global-
view, and then find a way to merge all of the clus-
tering solutions to benefit from the wisdom of 
crowd. Consensus clustering (Topchy et al 2005) 
has emerged as a way of combining multiple weak 
clusterings into a better one. The cluster-based si-
milarity partitioning algorithm (CSPA) (Strehl and 
Ghosh, 2002) uses the idea that elements that are 
frequently clustered together have high similarity. 
With MTurk, this involves asking multiple workers 
to provide full clusterings, and then, for each pair 
of elements, counting the number of times they co-
occur in the same clusters. This count is used as a 
similarity measure between elements, which then 
is used to build a distance matrix. We can then use 
it to recluster elements. The results from this tech-
nique on our word sense induction problem are 
shown in the next section. 
24
Another possibility is to determine which clus-
tering solution is the centroid of the set of cluster-
ings obtained from the worker. Finding centroid 
clustering (Hu and Sung, 2006) requires a be-
tween-cluster distance metric. We decided to use 
the entropy-based V-Measure for this purpose. For 
every pair of workers? solutions, we obtain their 
relative distance by calculating 
 1-VMeasure(cluster #1,cluster #2). 
Then, for each candidate?s clusters, we average the 
distance with every other candidate?s.  The candi-
date with the lowest average distance, the centroid, 
is picked as the ?crowd solution?. Results from this 
technique are also shown in the next section. 
6 Results 
For the first HIT the goal was to determine the 
number of distinct senses in a list of definitions. 
The Pearson correlation between the four annota-
tors on the number of clusters they used for the 50 
words was computed. These correlations can be 
viewed as how much the different annotators had 
the same idea of the grain size to be used to define 
senses. While experts 1, 2 and 4 seem to agree on 
grain size (correlation between 0.71 and 0.75), ex-
pert 3 had a different opinion. Correlations be-
tween that expert and the three others are between 
0.53 and 0.58. The average correlation between 
experts is 0.63. On the other hand, the crowd solu-
tion does not agree as well with experts #1,#2 and 
#4 (Pearson correlation of 0.64, 0.68, 0.66), while 
it better approaches expert 3, with a correlation of 
0.68. The average correlation between the non-
expert solution and the experts? solutions is 0.67.  
Another way to analyze the agreement on grain 
size of the word sense between annotators is to 
sum the absolute difference of number of clusters 
for the 50 words (Table 3).  In this way, we can 
specifically examine the results for the four anno-
tators and for the non-expert crowd (N-E) solution, 
averaging that difference for each annotator versus 
all of the others (including the N-E solution). 
To determine how a clustering solution com-
pared to our GS, we computed the V-Measure for 
all 50 words between the solution and each GS.  
By averaging the score on the four GSs, we get an 
averaged ITA score between the clustering solution 
and the experts. For the sake of comparison, we 
first computed the score of a random solution, 
where definitions are randomly assigned to any 
one cluster. We also implemented K-means clus-
tering using normalized word-overlap (Kulkarni et 
al., 2007), which has the best score on their test set.   
The resulting averaged ITA of our local-view 
approaches that of all 4 experts. We did the same 
with the global-view after applying CSPA and our 
centroid identification algorithm to the 5 clustering 
solutions the workers submitted. Table 2 shows the 
agreement between each expert and those ap-
proaches, as well as the averaged ITA. 
For the local-view and global-view ?centroid?, 
we looked at how the crowd size would affect the 
accuracy.  We first computed the averaged ITA by 
considering the answers from the first worker.  
Then, step by step, we added the answers from the 
second, third, fourth and fifth workers, each time 
computing the averaged ITA. Figure 1 shows the 
ITA as a function of the workers.   
  
Random K-Means local 
global  
CSPA 
global  
centroid 
GS #1 0,387 0,586 0,737 0,741 0,741 
GS #2 0,415 0,613 0,765 0,777 0,777 
GS #3 0,385 0,609 0,794 0,805 0,809 
GS #4 0,399 0,606 0,768 0,776 0,776 
Avg. ITA 0.396 ? 0.014 0.603 ? 0.012 0.766 ? 0.023 0.775 ? 0.026 0.776 ? 0.028 
 
Table 2 - Interannotator agreement for our different approaches (bold numbers are within one standard 
deviation of the Expert vs. Expert ITA of 0.772 ? 0.031 described in section 4) 
 
 GS #1 GS #2 GS #3 GS #4 N-E 
GS #1 0 24 26 29 26 
GS #2 24 0 30 27 26 
GS #3 26 30 0 37 20 
GS #4 29 27 37 0 27 
N-E 26 26 20 27 0 
Average 26.25 26.75 28.25 30 24.75 
Table 3 - Absolute difference of number of clusters 
between annotators 
 
25
7 Discussion 
Since our two approaches are based on the result of 
the first HIT, which determines the number of 
clusters, the accuracy of that first task is extremely 
important. It turns out that the correlation between 
the crowd solution and the experts (0.67) is actual-
ly higher than the average correlation between ex-
perts (0.63). One way to explain this is that of the 4 
experts, 3 had a similar opinion on what the grain 
size should be, while the other one had a different 
opinion. The crowd picked a grain size that was 
actually between those two opinions, thus resulting 
in a higher correlation. This hypothesis is also sup-
ported by Table 3. The average difference in the 
number of clusters is lower for the N-E solution 
than for any expert solution. The crowd of 13 was 
able to come up with a grain size that could be 
seen as a good consensus of the four annotators? 
grain size. This allows us to believe that using the 
crowd to determine the number of clusters for our 
two approaches is a reliable technique.  
As expected, Table 3 indicates that our two set-
ups behave better than randomly assigning defini-
tions to clusters.  This is a good indication that the 
workers did not complete our tasks randomly. The 
automatic approach (K-Means) clearly behaves 
better than the random baseline. However, the 
clusters obtained with this approach agree less with 
the experts than any of our crowdsourced ap-
proaches. This confirms the intuition that humans 
are better at distinguishing word senses than an 
automatic approach like K-Means.  
Our first hypothesis was that global-view would 
give us the best results: since the worker complet-
ing a global-view HIT has an overall view of the 
task, they should be able to provide a better solu-
tion. The results indicate that the local-view and 
global-view approaches give similar results in 
terms of ITA. Both of those approaches have clos-
er agreement with the experts, than the experts 
have with each other (all ITAs are around 77%).   
Here is an example of a solution that the crowd 
provided through local-view for the verb ?tape? 
with the definitions; 
 
A. To record something on tape 
B. To use strips of sticky material, especially to fix 
two things together or to fasten a parcel 
C. To record sound or picture onto a tape 
D. To tie a bandage firmly around an injured part of 
someone?s body, strap 
E. To fasten a package, box etc with tape 
 
The crowd created two clusters: one by group-
ing A and C to create a ?record audio/video? sense, 
and another one by grouping B,D and E to create a 
?fasten? sense. This solution was also chosen by 
two of the four experts. One of the other experts 
grouped definitions E with A and C, which is 
clearly an error since there is no shared meaning.  
The last expert created three clusters, by assigning 
D to a different cluster than B and E. This decision 
can be considered valid since there is a small se-
mantic distinction between D and B/E from the 
fact that D is ?fasten? for the specific case of in-
jured body parts. However, a student could gene-
ralize D from B and E. So that expert?s grain size 
does not correspond to our specifications. 
We investigated two different aggregation tech-
niques for clustering solutions, CSPA and centroid 
identification. In this application, both techniques 
give very similar results with only 2 clusters out of 
50 words differing between the two techniques. 
Centroid identification is easier to implement, and 
doesn?t require reclustering the elements. Figure 1 
shows the impact of adding more workers to the 
crowd. While it seems advantageous to use 3 
workers? opinions rather than only 1, (gain of 
0.04), adding a fourth and fifth worker does not 
improve the average ITA.   
Local-view is more tolerant to errors than glob-
al-view.  If a chaotic worker randomly answers one 
pair of elements, the entire final clustering will not 
be affected. If a chaotic (or cheating) worker an-
swers randomly in global-view, the entire cluster-
ing solution will be random. Thus, while a policy 
of using only one worker?s answer for a local-view 
 
Figure 1 - Impact of the crowd size on the ITA of the 
local and global approaches 
26
HIT could be adopted, the same policy might result 
in poor clustering if used for the global-view HIT.   
However, global-view has the advantage over 
local-view of being cheaper. Figure 2 shows the 
distribution of the number of definitions extracted 
from both LDOCE and CALD per word (starting at 
word with more than 6 definitions). Since the lo-
cal-view cost increases in a quadratic manner as 
the number of elements to cluster increases it 
would cost more than $275,000 to group the defi-
nitions of 30,000 words coming from the two dic-
tionaries (using the parameters described in 3).  It 
would be possible to modify it to only ask workers 
for the similarity of a subset of pairs of elements 
and then reconstruct the incomplete distance ma-
trix (Hathaway and Bezdek, 2002). A better option 
for clustering a very large amount of elements is to 
use global-view. For the same 30,000 words above, 
the cost of grouping definitions using this tech-
nique would be around $4,500.  This would imply 
that worker would have to create clusters from set 
of over 22 definitions.  Keeping the cost constant 
while increasing the number of elements to cluster 
might decrease the workers? motivation. Thus scal-
ing up a global-view HIT requires increasing the 
reward. It also requires vigilance on how much 
cognitive load the workers have to handle. Cogni-
tive load can be seen as a function of the number 
of elements to cluster and of the number of clusters 
that a new element can be assigned to. If a worker 
only has to decide if an element should be in A or 
B, the cognitive load is low. But if the worker has 
to decide among many more classes, the cognitive 
load may increase to a point where the worker is 
hampered from providing a correct answer.  
8 Conclusion 
We evaluated two different approaches for crowd-
sourcing dictionary definition clustering as a 
means of achieving WSI. Global-view provides an 
interface to the worker where all the elements to 
be clustered are displayed, while local-view dis-
plays only two elements at a time and prompts the 
worker for their similarity. Both approaches show 
as much agreement with experts as the experts do 
with one another. Applying either CSPA or centro-
id identification allows the solution to benefit from 
the wisdom of crowd effect, and shows similar 
results. While global-view is cheaper than local-
view, it is also strongly affected by worker error, 
and sensitive to the effect of increased cognitive 
load.  
It appears that the task of clustering definitions 
to form word senses is a subjective one, due to dif-
ferent ideas of what the grain size of the senses 
should be. Thus, even though it seems that our two 
approaches provide results that are as good as 
those of an expert, it would be interesting to try 
crowdsourced clustering on a clustering problem 
where an objective ground truth exists. For exam-
ple, we could take several audio recordings from 
each of several different persons. After mixing up 
the recordings from the different speakers, we 
could ask workers to clusters all the recordings 
from the same person. This would provide an even 
stronger evaluation of local-view against global-
view since we could compare them to the true so-
lution, the real identity of the speaker.  
There are several interesting modifications that 
could also be attempted. The local-view task could 
ask for similarity on a scale of 1 to 5, instead of a 
binary choice of same/different meaning. Also, 
since using global-view with one large problem 
causes high cognitive load, we could partition a 
bigger problem, e.g., with 30 definitions, into 3 
problems including 10 definitions. Using the same 
interface as global-view, the workers could cluster 
the sub-problems. We could then use CSPA to 
merge local clusters into a final cluster with the 30 
definitions.   
In this paper we have examined clustering word 
sense definitions. Two approaches were studied, 
and their advantages and disadvantages were de-
scribed. We have shown that the use of human 
computation for WSI, with an appropriate crowd 
 
Figure 2- Distribution of the number of definitions 
 
0
250
500
750
1000
N
u
m
b
e
r 
o
f 
w
o
rd
s
Number of definitions
27
size and mean of aggregation, is as reliable as us-
ing expert judgments.   
Acknowledgements 
Funding for this research is provided by the Na-
tional Science Foundation, Grant Number SBE-
0836012 to the Pittsburgh Science of Learning 
Center (PSLC, http://www.learnlab.org). 
References 
Alonso, O., Rose, D. E., & Stewart, B. (2008). Crowd-
sourcing for relevance evaluation. ACM SIGIR Fo-
rum , 42 (2), pp. 9-15. 
Callison-Burch, C. (2009). Fast, Cheap, and Creative: 
Evaluating Translation Quality Using Amazon?s Me-
chanical Turk. Proceedings of EMNLP 2009.  
Coxhead, A. (2000). A new academic word list. TESOL 
quarterly , 34 (2), 213-238. 
Chklovski, T. & Mihalcea, R. (2003). Exploiting 
agreement and disagreement of human annotators for 
word sense disambiguation.  Proceedings of RANLP 
2003. 
Fung, B. C., Wang, K., & Ester, M. (2003). Hierarchical 
document clustering using frequent itemsets. Proc. of 
the SIAM International Conference on Data Mining.  
Gruenstein, A., McGraw, I., & Sutherland, A. (2009). 
"A self-transcribing speech corpus: collecting conti-
nuous speech with an online educational game". 
SLaTE Workshop.  
Hathaway, R. J., & Bezdek, J. C. (2001). Fuzzy c-means 
clustering of incomplete data. IEEE Transactions on 
Systems, Man, and Cybernetics, Part B , 31 (5), 735-
744. 
Heilman, M. Collins-Thompson, K., Callan, J. & Eske-
nazi M. (2006).  Classroom success of an Intelligent 
Tutoring System for lexical practice and reading 
comprehension. Proceedings of the Ninth Interna-
tional Conference on Spoken Language. 
Horton, J. (2009, 12 11). Turker Talk. Retrieved 01 
2010, from Deneme: 
http://groups.csail.mit.edu/uid/deneme/?p=436 
Hu, T., & Sung, S. Y. (2006). Finding centroid cluster-
ings with entropy-based criteria. Knowledge and In-
formation Systems , 10 (4), 505-514. 
Huang, Y., & Mitchell, T. M. (2006). Text clustering 
with extended user feedback. Proceedings of the 29th 
annual international ACM SIGIR conference on Re-
search and development in information retrieval (p. 
420). ACM. 
Jagadeesan, A., Lynn, A., Corney, J., Yan, X., Wenzel, 
J., Sherlock, A., et al (2009). Geometric reasoning 
via internet CrowdSourcing. 2009 SIAM/ACM Joint 
Conference on Geometric and Physical Modeling 
(pp. 313-318). ACM. 
Kulkarni, A., Callan, J., & Eskenazi, M. (2007). Dictio-
nary Definitions: The Likes and the Unlikes. Pro-
ceedings of the SLaTE Workshop on Speech and 
Language Technology in Education. Farmington, PA, 
USA. 
Ledlie, J., Odero, B., Minkow, E., Kiss, I., & Polifroni, 
J. (2009). Crowd Translator: On Building Localized 
Speech Recognizers through Micropayments. Nokia 
Research Center. 
Little, G. (2009, 08 22). Website Clustering. Retrieved 
01 2010, from Deneme: 
http://groups.csail.mit.edu/uid/deneme/?p=244 
Little, G., Chilton, L. B., Goldman, M., & Miller, R. C. 
(2009). TurKit: tools for iterative tasks on mechani-
cal Turk. Proceedings of the ACM SIGKDD Work-
shop on Human Computation (pp. 29-30). ACM. 
Niu, Z.-Y., Dong-Hong, J., & Chew-Lim, T. (2007). I2r: 
Three systems for word sense discrimination, chinese 
word sense disambiguation, and english word sense 
disambiguation. Proceedings of the Fourth Interna-
tional Workshop on Semantic Evaluations (SemEval-
2007) (pp. 177-182). Prague, Czech Republic: Asso-
ciation for Computational Linguistics. 
Rosenberg, A., & Hirschberg, J. (2007). V-measure: A 
conditional entropy-based external cluster evaluation 
measure. Proceedings of the 2007 Joint Conference 
EMNLP-CoNLL, (pp. 410-420). 
SigLex, A. (2008). Retrieved 01 2010, from SemEval-2, 
Evaluation Exercises on Semantic Evaluation: 
http://semeval2.fbk.eu/semeval2.php 
Snow, R., O'Connor, B., Jurafsky, D., & Ng, A. Y. 
(2008). Cheap and fast---but is it good? Evaluating 
non-expert annotations for natural language tasks. 
Proceedings of the Conference on Empirical Methods 
in Natural Language Processing (pp. 254-263). Asso-
ciation for Computational Linguistics. 
Snow, R., Prakash, S., Jurafsky, D., & Ng, A. Y. (2007). 
Learning to Merge Word Senses. Proceedings of the 
2007 Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational Natural 
Language Learning (EMNLP-CoNLL) (pp. 1005-
1014). Prague, Czech Republic: Association for 
Computational Linguistics. 
Strehl, A., & Ghosh, J. (2003). Cluster ensembles---a 
knowledge reuse framework for combining multiple 
partitions. The Journal of Machine Learning Re-
search , 3, 583-617. 
Topchy, A., Jain, A. K., & Punch, W. (2005). Clustering 
ensembles: Models of consensus and weak partitions. 
IEEE Transactions on Pattern Analysis and Machine 
Intelligence , 27 (12), 1866-1881. 
Zhao, Y., & Karypis, G. (2001). Criterion functions for 
document clustering: Experiments and analysis. Re-
port TR 01?40, Department of Computer Science, 
University of Minnesota. 
28
Figure 3: Example of a global-view HIT for the word ?code? (not all of the instructions are shown) 
Appendix 
 
Figure 4: Example of a local-view HIT for the word ?aid? (not all of the instructions are shown) 
29
Proceedings of the NAACL HLT 2010 Fifth Workshop on Innovative Use of NLP for Building Educational Applications, pages 49?56,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Predicting Cloze Task Quality for Vocabulary Training
Adam Skory Maxine Eskenazi
Language Technologies Institute
Carnegie Mellon University
Pittsburgh PA 15213, USA
{askory,max}@cs.cmu.edu
Abstract
Computer  generation  of  cloze  tasks  still  falls 
short of full automation; most current systems 
are  used  by  teachers  as  authoring  aids. 
Improved methods to estimate cloze quality are 
needed  for  full  automation.  We  investigated 
lexical reading difficulty as a novel automatic 
estimator  of  cloze  quality,  to  which  co-
occurrence  frequency of  words was compared 
as an alternate estimator. Rather than relying on 
expert evaluation of cloze quality, we submitted 
open  cloze  tasks  to  workers  on  Amazon 
Mechanical  Turk (AMT) and discuss ways  to 
measure  of  the  results  of  these  tasks.  Results 
show  one  statistically  significant  correlation 
between  the  above  measures  and  estimators, 
which  was  lexical  co-occurrence  and  Cloze 
Easiness.  Reading difficulty was not found to 
correlate  significantly.  We  gave  subsets  of 
cloze sentences to an English teacher as a gold 
standard.  Sentences  selected  by co-occurrence 
and Cloze Easiness  were  ranked most  highly, 
corroborating the evidence from AMT.
1 Cloze Tasks
Cloze  tasks,  described  in  Taylor  (1953),  are 
activities  in  which  one  or  several  words  are 
removed from a sentence and a student is asked to 
fill  in the missing content.  That  sentence can be 
referred  to  as  the  'stem',  and  the  removed  term 
itself as the 'key'.  (Higgins, 2006)  The portion of 
the sentence from which the key has been removed 
is the 'blank'. 'Open cloze' tasks are those in which 
the student can propose any answer. 'Closed cloze' 
describes multiple choice tasks in which the key is 
presented along with a set of several 'distractors'.
1.1 Cloze Tasks in Assessment
Assessment is the best known application of cloze 
tasks. As described in (Alderson, 1979), the ?cloze 
procedure?  is  that  in  which  multiple  words  are 
removed at  intervals  from a text.  This  is  mostly 
used  in  first  language  (L1)  education.  Alderson 
describes  three  deletion  strategies:  random 
deletion, deletion of every nth word, and targeted 
deletion,  in  which  certain  words  are  manually 
chosen and deleted by an instructor.  Theories  of 
lexical  quality  (Perfetti  & Hart,  2001)  and word 
knowledge levels (Dale, 1965) illustrate why cloze 
tasks can effectively assess multiple dimensions of 
vocabulary knowledge.
Perfetti & Hart explain that lexical knowledge 
can  be  decomposed  into  orthographic,  phonetic, 
syntactic,  and  semantic  constituents.  The  lexical 
quality of a given word can then be defined as a 
measure based on both the depth of knowledge of 
each  constituent  and  the  degree  to  which  those 
constituents are bonded together. Cloze tasks allow 
a test author to select for specific combinations of 
constituents to assess (Bachman, 1982). 
1.2 Instructional Cloze Tasks
Cloze tasks can be employed for instruction as well 
as  assessment.  Jongsma  (1980)  showed  that 
targeted deletion is an effective use of instructional 
passage-based  cloze  tasks.  Repeated  exposure  to 
frequent words leads first to familiarity with those 
words, and increasingly to suppositions about their 
semantic  and  syntactic  constituents.  Producing 
cloze tasks through targeted deletion takes implicit, 
receptive word knowledge, and forces the student 
49
to consider explicitly how to match features of the 
stem with  what  is  known about  features  of  any 
keys she may consider.
2 Automatic Generation of Cloze Tasks
Most  cloze  task  ?generation?  systems  are  really 
cloze task  identification systems. That is, given a 
set  of  requirements,  such  as  a  specific  key  and 
syntactic structure (Higgins 2006) for the stem, a 
system looks into a database of pre-processed text 
and attempts to identify sentences matching those 
criteria.  Thus,  the content  generated for a closed 
cloze is the stem (by deletion of the key), and a set 
of  distractors.  In  the  case  of  some  systems,  a 
human  content  author  may  manually  tailor  the 
resulting stems to meet further needs.
Identifying  suitable  sentences  from  natural 
language  corpora  is  desirable  because  the 
sentences  that  are  found  will  be  authentic. 
Depending  on  the  choice  of  corpora,  sentences 
should also be well-formed and suitable in terms of 
reading level and content.  Newspaper text is one 
popular source (Hoshino & Nakagawa, 2005; Liu 
et  al.,  2005;  Lee  &  Seneff,  2007).  Pino  et  al. 
(2008)  use  documents  from  a  corpus  of  texts 
retrieved  from  the  internet  and  subsequently 
filtered  according  to  readability  level,  category, 
and  appropriateness  of  content.  Using  a  broader 
corpus  increases  the  number  and  variability  of 
potential  matching sentences, but also lowers the 
confidence that sentences will be well-formed and 
contain appropriate language (Brown & Eskenazi, 
2004).
2.1 Tag-based Sentence Search
Several cloze item authoring tools (Liu et al 2005; 
Higgins,  2006)  implement  specialized  tag-based 
sentence  search.  This  goes  back  to  the  original 
distribution  of  the  Penn  Treebank  and  the 
corresponding  tgrep program.  Developed by Pito 
in  1992  (Pito,  1994)  this  program  allows 
researchers to search for corpus text according to 
sequences  of part  of  speech (POS) tags  and tree 
structure.
The linguists' Search Engine (Resnik & Elkiss, 
2005)  takes  the  capabilities  of  tgrep yet  further, 
providing  a  simplified  interface  for  linguists  to 
search within tagged corpora along both syntactic 
and lexical features.
Both  tgrep  and  the  Linguists'  Search  Engine 
were not designed as cloze sentence search tools, 
but they paved the way for similar tools specialized 
for this task. For example, Higgins' (2006) system 
uses  a  regular  expression  engine  that  can  work 
either on the tag level, the text level or both. This 
allows  test  content  creators  to  quickly  find 
sentences  within  very  narrow  criteria.  They  can 
then alter these sentences as necessary.
Liu et al (2005) use sentences from a corpus of 
newspaper  text  tagged  for  POS  and  lemma. 
Candidate sentences are found by searching on the 
key and its POS as well as the POS sequence of 
surrounding  terms.  In  their  system  results  are 
filtered for proper word sense by comparing other 
words  in  the  stem with data  from WordNet  and 
HowNet,  databases  of  inter-word  semantic 
relations.
2.2 Statistical Sentence Search
Pino et al(2009) use co-occurrence frequencies to 
identify  candidate  sentences.  They  used  the 
Stanford Parser (Klein & Manning, 2003) to detect 
sentences within a desired range of complexity and 
likely well-formedness. Co-occurrence frequencies 
of words in the corpus were calculated and keys 
were  compared  to  other  words  in  the  stem  to 
determine  cloze quality,  producing suitable cloze 
questions  66.53%  of  the  time.  This  method 
operates  on  the  theory  that  the  quality  of  the 
context  of  a  stem is  based on  the  co-occurrence 
scores of other words in the sentence. Along with 
this  result,  Pino  et  al.  incorporated  syntactic 
complexity in terms of the number of parses found. 
Hoshino  &  Nakagawa  (2005)  use  machine 
learning  techniques  to  train  a  cloze  task  search 
system. Their system, rather than finding sentences 
suitable  for  cloze  tasks,  attempts  to  automate 
deletion for passage-based cloze. The features used 
include  sentence  length  and  POS  of  keys  and 
surrounding words. Both a Na?ve Bayes and a K-
Nearest Neighbor classifier were trained to find the 
most likely words for deletion within news articles. 
To train the system they labeled cloze sentences 
from a TOEIC training test as true, then shifted the 
position  of  the  blanks  from those  sentences  and 
50
labeled  the  resulting  sentences  as  false.  Manual 
evaluation  of  the  results  showed  that,  for  both 
classifiers, experts saw over 90% of the deletions 
as either easy to solve or merely possible to solve.
3 Reading Level and Information Theory
An  information-theoretical  basis  for  an  entirely 
novel approach to automated cloze sentence search 
is  found  in  Finn  (1978).  Finn  defines  Cloze 
Easiness as ?the percent of subjects filling in the 
correct word in a cloze task.? Another metric of the 
quality of  a  cloze task is  context  restriction;  the 
number of solutions perceived as acceptable keys 
for a given stem.  Finn's theory of lexical  feature 
transfer  provides  one  mechanism  to  explain 
context  restriction.  The  theory  involves  the 
information content of a blank.
According to Shannon's  (1948) seminal  work 
on information theory,  the  information contained 
in a given term is inverse to its predictability.  In 
other words, if a term appears despite following a 
history after which is it considered very unlikely to 
occur, that word has high information content. For 
example, consider the partial sentence ?She drives 
a  nice...?.  A  reader  forms  hypotheses  about  the 
next  word  before  seeing  it,  and  thus  expects  an 
overall  meaning  of  the  sentence.  A  word  that 
conforms to this hypothesis, such as the word 'car', 
does little to change a reader's knowledge and thus 
has little  information.  If instead the next word is 
'taxi', 'tank', or 'ambulance', unforeseen knowledge 
is gained and relative information is higher.
According to Finn (1978) the applicability of 
this  theory  to  Cloze  Easiness  can  be  explained 
though lexical transfer features. These features can 
be both syntactic and semantic, and they serve to 
interrelate  words  within  a  sentence.  If  a  large 
number  of  lexical  transfer  features  are  within  a 
given proximity of a blank, then the set of words 
matching those features will  be highly restricted. 
Given that each choice of answer will  be from a 
smaller  pool  of  options,  the  probability  of  that 
answer  will  be  much  higher.  Thus,  a  highly 
probable key has correspondingly low information 
content. 
Predicting  context  restriction  is  of  benefit  to 
automatic  generation  of  cloze  tasks.  Cloze 
Easiness  improves  if  a  student  chooses  from  a 
smaller set of possibilities. The instructional value 
of  a  highly  context-restricted  cloze  task  is  also 
higher by providing a richer set of lexical transfer 
features with which to associate vocabulary.
Finn's  application  of  information  theory  to 
Cloze Easiness and context restriction provides one 
possible  new  avenue  to  improve  the  quality  of 
generated cloze tasks. We hypothesize that words 
of higher reading levels contain higher numbers of 
transfer  features  and  thus  their  presence  in  a 
sentence  can  be  correlated  with  its  degree  of 
context  restriction.  To  the  authors'  knowledge 
reading  level  has  not  been previously applied  to 
this problem.
We can use a unigram reading level model to 
investigate  this  hypothesis.  Returning  to  the 
example words for the partial sentence ?She drives 
a  nice...?,  we  can  see  that  our  current  model 
classifies the highly expected word, 'car', at reading 
level  1,  while 'taxi','tank',  and 'ambulance',  are at 
reading levels 5, 6, and 11 respectively.
3.1 Reading Level Estimators
The estimation of reading level is a complex topic 
unto  itself.  Early  work  used  heuristics  based  on 
average  sentence  length  and  the  percentage  of 
words deemed unknown to a baseline reader. (Dale 
& Chall, 1948; Dale, 1965) Another early measure, 
the Flesch-Kincaid measure, (Kincaid et al, 1975) 
uses a function of the syllable length of words in a 
document and the average sentence length.
More recent work on the topic also focuses on 
readability  classification  at  the  document  level. 
Collins-Thompson  & Callan  (2005)  use  unigram 
language  models  without  syntactic  features. 
Heilman et al (2008) use a probabilistic parser and 
unigram language models to combine grammatical 
and lexical features. (Petersen & Ostendorf, 2006) 
add higher-order n-gram features to the above to 
train  support  vector  machine  classifiers  for  each 
grade level.
These  recent  methods  perform  well  to 
characterize the level  of  an entire  document,  but 
they are untested for single sentences. We wish to 
investigate if  a  robust  unigram model  of reading 
level can be employed to improve the estimation of 
cloze quality at the sentence level. By extension of 
Finn's  (1978)  hypothesis,  it  is  in  fact  not  the 
51
overall  level  of  the sentence that has a predicted 
effect  on cloze context  restriction,  but  rather  the 
reading  level  of  the  words  in  proximity  to  the 
blank. Thus we propose that it should be possible 
to find a correlation between cloze quality and the 
reading levels of words in near context to the blank 
of a cloze task. 
4 The Approach
We investigate a multi-staged filtering approach to 
cloze sentence generation. Several variations of the 
final filtering step of this approach were employed 
and correlations sought between the resulting sets 
of  each  filter  variation.  The  subset  predicted  to 
contain the best sentences by each filter was finally 
submitted to expert review as a gold standard test 
of cloze quality.
This study compares two features of sentences, 
finding  the  levels  of  context  restriction 
experimentally. The first feature in question is the 
maximum reading level  found in near-context  to 
the  blank.  The  second  feature  is  the  mean  skip 
bigram co-occurrence score  of words within that 
context.
Amazon Mechanical Turk (AMT) is used as a 
novel  cloze  quality  evaluation  method.  This 
method  is  validated  by  both  positive  correlation 
with   the  known-valid  (Pino  et  al.,  2008)  co-
occurrence  score  predictor,  and  an  expert  gold 
standard. Experimental results from AMT are then 
used to evaluate the hypothesis that reading level 
can be used as a new, alternative predictor of cloze 
quality.
4.1 Cloze Sentence Filtering
The first step in preparing material for this study 
was to obtain a set of keys. We expect that in most 
applications of sentence-based cloze tasks the set 
of  keys  is  pre-determined  by instructional  goals. 
Due  to  this  constraint,  we  choose  a  set  of  keys 
distributed across several reading levels and hold it 
as fixed. Four words were picked from the set of 
words common in texts labeled as grades four, six, 
eight, ten, and twelve respectively.
201,025  sentences  containing  these  keys  were 
automatically  extracted  from  a  corpus  of  web 
documents  as  the  initial  filtering  step.  This 
collection  of  sentences  was  then  limited  to 
sentences of length 25 words or less. Filtering by 
sentence  length  reduced  the  set  to  136,837 
sentences.
A probabilistic  parser  was  used  to  score  each 
sentence. This parser gives log-probability values 
corresponding to confidence of the best parse. A 
threshold  for  this  confidence  score  was  chosen 
manually  and  sentences  with  scores  below  the 
threshold were removed,  reducing the number  of 
sentences to 29,439.
4.2 Grade Level
Grade  level  in  this  study  is  determined  by  a 
smoothed  unigram  model  based  on  normalized 
concentrations  within  labeled  documents.  A 
sentence is assigned the grade level of the highest 
level word in context of the key.
4.3 Co-occurrence Scores
Skip bigram co-occurrence counts were calculated 
from  the  Brown  (Francis  &  Kucera,  1979)  and 
OANC (OANC, 2009) corpora. A given sentence's 
score is calculated as the mean of the probabilities 
of finding that sentence's context for the key.
These  probabilities  are  defined  on  the  triplet 
(key, word, window size), in which key is the target 
word to be removed, word any term in the corpus, 
and  window size is a positive integer less than or 
equal to the length of the sentence.
This probability is estimated as the number  of 
times  word is found within the same sentence as 
key and  within  an  absolute  window  size  of 2 
positions from key, divided by the total number of 
times  all  terms  are  found in that  window.  These 
scores are thus maximum likelihood estimators of 
the probability of word given key and window size:
4th: 'little', 'thought', 'voice',  'animals'
6th: ?president', 'sportsmanship', 'national',  experience'
8th: 'college', 'wildlife', 'beautiful', 'competition'
10th: 'medical', 'elevations','qualities', 'independent'
12th: 'scientists',  'citizens', 'discovered', 'university'
Figure 1: common words per grade level.
52
(1) For some key k , word w, and window-size m :
Cj(w, k) := count of times w found j words from the 
position of k, within the same sentence.
(2) For a vocabulary V and for some positive integer 
window-size m, let n = (m-1) / 2, then:
i.e. if our corpus consisted of the single sentence
?This is a good example sentence.?:
C?1 (w = good, k = example) = 1
C1 (w = sentence, k = example) = 1
P (w = good | k = example, m = 3) =  1 / (1+1)= .5
Finally, the overall score of the sentence is taken 
to be the mean of the skip bigram probabilities of 
all words in context of the key.
4.4 Variable Filtering by Grade and Score
Skip bigram scores were calculated for all words 
co-occurrent  in  a  sentence  with  each  of  our  20 
keys. To maximize the observable effect of the two 
dimensions of grade level and co-occurrence score, 
the  goal  was  to  find  sentences  representing 
combinations  of  ranges  within those  dimensions. 
To  achieve  this  it  was  necessary  to  pick  the 
window size that best  balances variance of these 
dimensions  with a  reasonably flat  distribution of 
sentences.
In terms  of  grade level,  smaller  window sizes 
resulted  in  very few sentences  with  at  least  one 
high-level  word,  while  larger  window  sizes 
resulted in few sentences with no high-level words. 
Variance  in  co-occurrence  score,  on  the  other 
hand, was maximal at a window size of 3 words, 
and  dropped  off  until  nearly  flattening  out  at  a 
window size  of  20 words.  A window size  of  15 
words was found to offer a reasonable distribution 
of grade level while preserving sufficient variance 
of co-occurrence score.
Using the above window-size, we created filters 
according to maximum grade level:  one each for 
the grade ranges 5-6, 7-8, 9-10,  and 11-12.  Four 
more  filters  were  created  according  to  co-
occurrence score: one selecting the highest-scoring 
quartile  of  sentences,  one  the  second  highest-
scoring quartile, and so on. Each grade level filter 
was combined with each co-occurrence score filter 
creating 4x4=16 composite filters.  By combining 
these filters we can create a final set of sentences 
for  analysis  with  high  confidence  of  having  a 
significant  number  of  sentences  representing  all 
possible values  of grade level  and co-occurrence 
score. At most two sentences were chosen for each 
of the 20 keys using these composite filters. The 
final number of sentences was 540.
4.5 Experimental Cloze Quality
Previous  evaluation  of  automatically  generated 
cloze tasks has relied on expert judgments. (Pino et 
al., 2008; Liu et al, 2005) We present the use of 
crowdsourcing techniques as a new approach for 
this  evaluation.  We believe  the approach can be 
validated  by  statistically  significant  correlations 
with predicted cloze quality and comparison with 
expert judgments.
The  set  of  540  sentences  were  presented  to 
workers  from Amazon  Mechanical  Turk  (AMT), 
an  online  marketplace  for  ?human  intelligence 
tasks.? Each worker was shown up to twenty of the 
stems of these sentences as open cloze tasks. No 
worker was allowed to see more than one stem for 
the  same  key.  Workers  were  instructed  to  enter 
only those words that  ?absolutely make  sense in 
this context?, but were not encouraged to submit 
any particular  number  of answers.  Workers were 
paid US$.04 per sentence, and the task was limited 
to workers with approval ratings on past tasks at or 
above 90%.
For  each  sentence  under  review  each  worker 
contributes one subset of answers. Cloze Easiness, 
as  defined  by  Finn  (1978)  is  calculated  as  the 
percentage of these subsets containing the original 
key.  We  define  context  restriction on  n as  the 
percentage of answer subsets containing n or fewer 
words.
Using the example sentence: ?Take this cloze 
sentence, for    (example)  .? We can find the set of 
answer subsets A:
A  =  {  A1={example, free, fun, me}
A2={example,instance}
A3={instance}     }
Then, Cloze Easiness is |{A1,A2}| / |A| ? .67 and 
Context restriction (on one or two words) is |
{A2,A3}| / |A| ? .67
53
5 Results
Each sentence in the final set was seen, on average, 
by  27  Mechanical  Turk  workers.  We  wish  to 
correlate measures of Cloze Easiness and context 
restriction  with  cloze  quality  predictors  of 
maximum  grade  level  and  score.  We  use  the 
Pearson correlation  coefficient  (PCC)  to  test  the 
linear relationship between each measure of cloze 
quality and each predictor.
Table (1)  shows these PCC values. All of  the 
values are positive, meaning there is a correlation 
showing that one value will tend to increase as the 
other increases. The strongest correlation is that of 
co-occurrence and Cloze Easiness. This is also the 
only statistically significant correlation. The value 
of  P(H0)  represents  the  likelihood  of  the  null 
hypothesis:  that  two  random  distributions 
generated  the  same  correlation.  Values  of  P(H0) 
under  0.05  can  be  considered  statistically 
significant.
Figure  (3)  shows  scatter  plots  of  these  four 
correlations  in  which  each  dot  represents  one 
sentence. 
The top-leftmost  plot  shows the correlation of 
co-occurrence  score  (on  the  x-axis),  and  Cloze 
Easiness (on the y-axis). Co-occurrence scores are 
shown on a log-scale. The line through these points 
represents a linear regression, which is in this case 
statistically significant.
The  bottom-left  plot  shows  correlation  of  co-
occurrence score (x-axis) with context restriction. 
In this case context  restriction was calculated on 
n=2,  i.e.  the  percent  of  answers  containing  only 
Cloze Easiness PCC = 0.2043
P(H0)=1.6965e-06
PCC = 0.0671
P(H0)=0.1193
Context 
Restriction (2)
PCC = 0.0649
P(H0)=0.1317
PCC = 0.07
P(H0)=0.1038
Co-occurrence Maximum Grade
Table (1): Pearson Correlation Coefficient and 
probability of null hypothesis for estimators and 
measures of cloze quality.
Figure (3): Scatter plots of all sentences with cloze quality measure as y-axis, and cloze quality estimator as x-axis. 
The linear regression of each distribution is shown.
54
one  or  two  words.  The  linear  regression  shows 
there  is  a  small  (statistically  insignificant) 
correlation.
The  top-right  plot  shows  Cloze  Easiness  (y-
axis)  per  grade  level  (x-axis).  The  bottom  left 
shows context restriction (y-axis) as a function of 
grade level.  In both cases linear regressions here 
also show small, statistically insignificant positive 
correlations.
The lack of significant correlations for three out 
of four combinations of measures and estimators is 
not grounds to dismiss these measures. Across all 
sentences,  the  measure  of  context  restriction  is 
highly variant, at 47.9%. This is possibly the result 
of the methodology; in an attempt to avoid biasing 
the AMT workers, we did not specify the desirable 
number  of  answers.  This  led  to  many  workers 
interpreting the task differently.
In terms of maximum grade level, the lack of a 
significant correlation with context restriction does 
not  absolutely  refute  Finn  (1978)'s  hypothesis. 
Finn  specifies  that  semantic  transfer  features 
should be in  ?lexical  scope? of a  blank.  A clear 
definition of ?lexical scope? was not presented. We 
generalized scope to mean proximity within a fixed 
contextual window size. It is possible that a more 
precise definition of ?lexical scope? will provide a 
stronger  correlation  of  reading  level  and  context 
restriction.
5.1 Expert Validation
Finally,  while  we  have  shown  a  statistically 
significant  positive  correlation  between  co-
occurrence  scores  and  Cloze  Easiness,  we  still 
need to demonstrate that Cloze Easiness is a valid 
measure of cloze quality. To do so, we selected the 
set  of  20  sentences  that  ranked  highest  by  co-
occurrence score and by Cloze Easiness to submit 
to expert evaluation. Due to overlap between these 
two  sets,  choosing  distinct  sentences  for  both 
would  require  choosing  some  sentences  ranked 
below the top 20 for each category.  Accordingly, 
we  chose  to  submit  just  one  set  based  on  both 
criteria in combination.
Along with these 20 sentences, as controls, we 
also  selected  two  more  distinct  sets  of  20 
sentences:  one  set  of  sentences  measuring  most 
highly  in  context  restriction,  and  one  set  most 
highly estimated by maximum grade level.
We asked a former English teacher to read each 
open cloze,  without  the  key,  and rate,  on  a  five 
point  Likert  scale,  her  agreement  with  the 
statement  ?This  is  a  very  good  fill-in-the-blank  
sentence.? where 1 means strong agreement, and 5 
means strong disagreement.
Expert evaluation on 
5-point Scale
Mean Standard
Deviation
20
best 
sentences
as 
determined 
by:
Cloze Easiness and co-
occurrence score 2.25 1.37
Context restriction 3.05 1.36
Maximum grade level 3.15 1.2
Table (2): Mean ratings for each sentence category.
The results in Table (2) show that, on average, 
the correlated results of selecting sentences based 
on Cloze Easiness and co-occurrence score are in 
fact rated more highly by our expert as compared 
to sentences selected based on context restriction, 
which is, in turn, rated more highly than sentences 
selected  by maximum grade  level.  Using  a  one-
sample t-test and a population mean of 2.5, we find 
a p-value of .0815 for our expert's ratings.
6 Conclusion
We  present  a  multi-step  filter-based  paradigm 
under which diverse estimators of cloze quality can 
be applied towards the goal of full automation of 
cloze  task  generation.  In  our  implementation  of 
this  approach sentences  were  found  for  a  set  of 
keys,  and  then  filtered  by  maximum  length  and 
likelihood  of  well-formedness.  We  then  tested 
combinations  of  two  estimators  and  two 
experimental measures of cloze quality for the next 
filtering step.
We presented an information-theoretical  basis 
for the use of reading level as a novel estimator for 
cloze quality. The hypothesis that maximum grade 
level should be correlated with context restriction 
was  not,  however,  shown  with  statistical 
significance.  A  stronger  correlation  might  be 
shown with a different experimental methodology 
and a more refined definition of lexical scope.
55
As an alternative to expert evaluation of cloze 
quality,  we  investigated  the  use  of  non-expert 
workers  on  AMT.  A  statistically  significant 
correlation was found between the co-occurrence 
score of a sentence and its experimental measure of 
Cloze  Easiness.  This  is  evidence  that 
crowdsourcing  techniques  agree  with  expert 
evaluation of co-occurrence scores in past studies.
To gain further evidence of the validity of these 
experimental  results,  sentences  selected  by  a 
composite filter of co-occurrence score and Cloze 
Easiness were compared to sentences selected by 
context  restriction  and  reading  level.  An  expert 
evaluation  showed  a  preference  for  sentences 
selected by the composite filter.
We  believe  that  this  method  of  cloze  task 
selection is promising. It will now be tested in a 
real  learning  situation.  This  work  contributes 
insight  into  methods  for  improving  technologies 
such as intelligent tutoring systems and language 
games.
References
Alderson,  J.  C.  (1979).  The  Cloze  Procedure  and 
Proficiency  in  English  as  a  Foreign  Language.  TESOL 
Quarterly, 13(2), 219-227. doi: 10.2307/3586211.
Bachman, L. F. (1982). The Trait Structure of Cloze Test 
Scores. TESOL Quarterly, 16(1), 61.
Brown, J., & Eskenazi, M. (2004). Retrieval of Authentic 
Documents  for  Reader-Specific  Lexical  Practice.  In 
InSTIL/ICALL Symposium (Vol. 2). Venice, Italy.
Collins-Thompson,  K.,  &  Callan,  J.  (2005).  Predicting 
reading difficulty with statistical language models. Journal  
of  the  American  Society  for  Information  Science  and  
Technology, 56(13), 1448-1462.
Dale, E. (1965). Vocabulary measurement: Techniques and 
major findings. Elementary English, 42, 395-401.
Dale, E., & Chall, J. S. (1948). A Formula for Predicting 
Readability:  Instructions.  Educational  Research  Bulletin, 
Vol. 27(2), 37-54.
Finn,  P.  J.  (1978).  Word  frequency,  information  theory, 
and  cloze  performance:  A  transfer  feature  theory  of 
processing in reading. Reading Research Quarterly, 13(4), 
508-537.
Francis,  W.  N.  &  Kucera,  H.  (1979).  Brown   Corpus 
Manual,  Brown  University  Department  of  Linguistics.  
Providence, RI
Heilman,  M.,  Collins-Thompson,  K.,  &  Eskenazi,  M. 
(2008). An Analysis of Statistical Models and Features for 
Reading  Difficulty  Prediction. 3rd  Workshop  on 
Innovative  Use  of  NLP  for  Building  Educational  
Applications. Assoc. for Computational Linguistics.
Higgins,  D.  (2006).  Item  Distiller:  Text  retrieval  for 
computer-assisted test item creation. ETS, Princeton, NJ.
Hoshino,  A.,  &  Nakagawa,  H.  (2005).  A  real-time 
multiple-choice question generation for language testing ? 
a  preliminary  study?.  In  2nd  Workshop  on  Building  
Educational  Applications  Using  NLP (pp.  17-20).  Ann 
Arbor, MI: Association for Computational Linguistics. 
Jongsma, E. (1980). Cloze instructional research: A second 
look.  Newark,  DE:  International  Reading  Association. 
Urbana, IL.
Kincaid,  J.,  Fishburne,  R.,  Rodgers,  R.,  &  Chissom,  B. 
(1975).  Derivation  of  new readability  formulas  for  navy 
enlisted  personnel.  Research  Branch  Report.  Millington, 
TN.
Klein, D. & Manning, C. (2003). Accurate Unlexicalized 
Parsing. (pp. 423-430) In Proceedings of the 41st Meeting  
of the Assoc. for Computational Linguistics. 
Lee,  J.,  &  Seneff,  S.  (2007).  Automatic  Generation  of 
Cloze Items for Prepositions. Proceedings of In.
Liu,  C.,  Wang,  C.,  Gao,  Z.,  &  Huang,  S.  (2005). 
Applications  of  Lexical  Information  for  Algorithmically 
Composing Multiple-Choice Cloze Items. In  Proceedings 
of the 2nd Workshop on Building Educational Applications  
Using  NLP (p.  1?8).  Ann  Arbor,  MI:  Association  for 
Computational Linguistics.
Open  American  National  Corpus  (2009) 
americannationalcorpus.org/OANC/
Perfetti,  C.,  &  Hart,  L.  (2001).  Lexical  bases  of  
comprehension skill. (D. Gorfein) (pp. 67-86). Washington 
D.C.: American Psychological Association. 
Petersen,  S.  E.,  &  Ostendorf,  M.  (2006).  Assessing  the 
reading level of web pages. In ICSLP (Vol. pages, pp. 833-
836).
Pino, J., Heilman, M., & Eskenazi, M. (2008). A Selection 
Strategy  to  Improve  Cloze  Question  Quality.  In 
Proceedings  of  the  Workshop  on  Intelligent  Tutoring  
Systems for Ill-Defined Domains.
Pito, R. (1994). tgrep README 
www.ldc.upenn.edu/ldc/online/treebank/README.long
Resnik,  P.,  &  Elkiss,  A.  (2005).  The  Linguist?s  Search 
Engine:  An  Overview.  Association  for  Computational  
Linguistics, (June), 33-36.
Shannon,  C.  (1948).  A  Mathematical  Theory  of 
Communication.  Bell System Technical Journal,  27, 379?
423, 623?656.
Taylor,  W.  L.  (1953).  Cloze  procedure:  a  new  tool  for 
measuring readability. Journalism Quarterly, 30, 415-453.
56
Proceedings of the Sixth Workshop on Innovative Use of NLP for Building Educational Applications, pages 76?80,
Portland, Oregon, 24 June 2011. c?2011 Association for Computational Linguistics
Effect of Word Complexity on L2 Vocabulary Learning 
 
 
Kevin Dela Rosa 
Language Technologies Institute 
Carnegie Mellon University 
5000 Forbes Ave. Pittsburgh, PA 
kdelaros@cs.cmu.edu 
Maxine Eskenazi 
Language Technologies Institute 
Carnegie Mellon University 
5000 Forbes Ave. Pittsburgh, PA 
max@cs.cmu.edu 
 
 
Abstract 
Research has shown that a number of 
factors, such as maturational constraints, 
previous language background, and 
attention, can have an effect on L2 
acquisition. One related issue that remains 
to be explored is what factors make an 
individual word more easily learned. In 
this study we propose that word 
complexity, on both the phonetic and 
semantic levels, affect L2 vocabulary 
learning. Two studies showed that words 
with simple grapheme-to-phoneme ratios 
were easier to learn than more 
phonetically complex words, and that 
words with two or fewer word senses 
were easier to learn that those with three 
or more.  
1 Introduction 
There is much computer-assisted language learning 
(CALL) literature that explores effective methods 
of teaching vocabulary. In recent studies conducted 
using the REAP system, which finds documents 
from the internet to teach vocabulary, we have 
shown that speech synthesis reinforces written text 
for learning in reading activities (Dela Rosa et al, 
2010), and we have also shown that context-
sensitive dictionary definitions afford better 
vocabulary learning for L2 language students (Dela 
Rosa and Eskenazi, 2011).  
One issue that remains to be explored in this 
context: determining what factors make an 
individual word easier to learn. We propose that 
word complexity, on both the phonetic and 
semantic levels, can affect how easily an L2 
vocabulary word can be learned.  
In this paper we first discuss past work on 
factors that impact vocabulary acquisition in 
intelligent tutoring environments, and then explore 
work on defining the complexity of a word with 
respect to vocabulary learning. Next we describe 
two classroom studies we conducted with ESL 
college students to test the effect of word 
complexity on L2 vocabulary learning. Finally we 
examine our results and suggest future research 
directions. 
2 Background 
Many studies have been conducted to investigate 
the relationship between different variables and 
second language learning. For example, the age of 
the foreign language learner is often pointed to as a 
major factor in determining whether an individual 
will be successful in learning a new language 
(Marinova-Todd, 2000).  
In the domain of L2 vocabulary instruction, 
researchers have shown that factors such as 
maturational constraints, attention, previous 
language background, and order of acquisition, can 
all affect L2 vocabulary acquisition (Oxford and 
Scarcella, 1994). Additionally, another factor that 
affects L2 vocabulary learning is the number of 
exposures of a practice item that a student receives 
during learning activities. In a study on the effects 
of spacing and retention of vocabulary pairs, 
Pavlik and Anderson (2005) showed that each time 
an item is practiced, it receives an increment of 
76
strength, but these increments decay as a power 
function of time. Furthermore, it is generally 
accepted that reading is beneficial to vocabulary 
acquisition (Perfetti, 2010).  
One group of factors in foreign language 
vocabulary instruction that has often been 
overlooked is at the level of the individual word, 
such as word complexity. In sections 3.2 and 3.3, 
we describe two simple measures of phonetic and 
semantic word complexity that were examined 
during our classroom studies. There have been 
work on defining the complexity of a word, such as 
Jakielski?s (1998) Index of Phonetic Complexity, 
but we do not know of work that measures the 
effect of word complexity on L2 vocabulary 
learning. 
3 Classroom Study Setup 
In order to determine the effect that word 
complexity, in both the phonetic and semantic 
levels, have on L2 language learners, we 
conducted two in-vivo studies with ESL students at 
the English Language Institute of the University of 
Pittsburgh. The first study focused on the effect of 
phonetic word complexity on vocabulary learning. 
The second study explored the effect of semantic 
word complexity, in the form of the number of 
senses a word has, on vocabulary learning. Both 
studies and the tutoring system that was used are 
described in the next sections. 
3.1 Overview of the Tutoring System 
The tutoring system, REAP, is a web-based 
language tutor developed at Carnegie Mellon that 
harvests documents from the internet for L2 
vocabulary learning and reading comprehension 
(Heilman et al, 2006). It has been used as a testing 
platform for cognitive science studies. This system 
has the ability to provide reader-specific passages 
by consulting profiles that model a reader?s 
reading level, topic interests, and vocabulary goals. 
The system?s interface has several features that 
enhance a student?s learning experience. One key 
feature is that it provides users with the ability to 
listen to the spoken version of any word that 
appears in a document, making use of the Cepstral 
Text-to-Speech system (2001) to synthesize words 
on the fly when clicked on. Additionally, students 
can look up the definition of any of the words they 
encounter while reading the documents using an 
embedded electronic dictionary. The system also 
automatically highlights focus words, i.e. the 
words targeted for vocabulary learning in a 
particular reading. 
3.2 Study 1: Phonetic Complexity 
In Study 1, we looked at the effect that phonetic 
complexity, one measure of a word?s complexity, 
has on learning a word, and whether this 
complexity causes a word to be learned more 
easily when multimodal input is provided in the 
form of written text accompanied by spoken text 
generated through speech synthesis. To measure a 
word?s phonetic complexity, we used the ratio of a 
word?s graphemes to phonemes, where words with 
a ratio closer to 1 were simpler than those with a 
ratio much greater or less than 1. Note that for this 
study, simple letters have been used as the 
grapheme units. 
For example, the word cat has a simple one-to-
one mapping between its graphemes and phonemes 
(C A T vs. K AE T), while other words like 
borough and index have a more complex 
relationship (B O R O U G H vs. B ER OW, and I 
N D E X vs. IH N D EH K S), with grapheme-to-
phoneme ratios greater than 1 and less than 1 
respectively. 
For this study, there were 21 intermediate-level 
ESL college students at the University of Pittsburgh?s 
English Language Institute whose native languages 
included Arabic, Chinese and Spanish. Weekly 
group readings were given as class activities, 
centered on a total of 18 focus words, followed by 
practice closed cloze questions (multiple-choice 
fill-in-the-blank with 5 answer choices provided, 
and distractors coming from the Academic Word 
List or words that are similar but do not fit the 
blank properly) on the focus words that appeared 
in the particular reading. The focus words used in 
this study were taken evenly from the following 
word groups: 
? Words with grapheme-to?phoneme ratio 
equal to 1 [6 words] 
? Words with grapheme-to?phoneme ratio 
greater than 1 [6 words] 
? Words with grapheme-to?phoneme ratio 
less than 1 [6 words] 
A pre-test was administered at the beginning of 
the study, consisting of closed cloze questions 
about the focus words. A similar set of questions 
77
was presented to the students during the post-test, 
which occurred one week after the last reading 
activity. Between the pre-test and post-test, 6 
reading activities were administered, one per week, 
each focused on a single document. This activity 
typically took students 20-30 minutes to complete.  
3.3 Study 2: Semantic Complexity 
In Study 2, we investigated the effect that multiple 
word-senses, another measure of word complexity, 
have on learning a word. There were 21 
intermediate-level ESL college students at the 
University of Pittsburgh?s English Language Institute, 
whose native languages included Arabic, Chinese, 
Korean, and Spanish. As in Study 1 there was a 
pre-test, a post-test, and a series of weekly 
documents to be read featuring the focus words. In 
total there were 26 focus words, all of which were 
taken from the Academic Word List and 7 weekly 
reading activities.  
 
 
 
With respect to word complexity, the focus 
words were divided into the following groups: 
? Words with 1 sense [8 words] 
? Words with 2 senses [10 words] 
? Words with 3 or more senses [8 words] 
4 Results 
The results of both of our studies showed that the 
use of the tutoring system significantly helped 
students improve their performance on the 
vocabulary tests, as made evident by the average 
overall gains between the pre-test and post-test (p 
< 0.001). Note that the error bars shown in this 
section show the standard error. Also note that 
normalized gain, the measurement being used to 
describe improvement in both studies, is given the 
by the following:  
 
If the post-test score is greater than the pre-test 
score, then 
 
Normalized gain = (post-test score ? pre-test 
score) / (maximum-possible-score ? pre-test score) 
 
Otherwise, 
 
Normalized gain = (post-test score ? pre-test 
score) / (pre-test score) 
 
In Study 1, the average normalized gain 
between the pre-test and post-test was 0.2563 (? 
0.0466). Figure 1 illustrates the differences in 
vocabulary gain when the gains are separated by 
word condition type. The average gains per 
condition are 0.2222, 0.1270, and 0.1191 for the 
conditions of grapheme-to-phoneme ratio = 1, 
grapheme-to-phoneme ratio > 1, and grapheme-to-
phoneme ratio < 1 respectively. 
In Study 2, the average normalized gain 
between the pre-test and post-test was 0.5323 (? 
0.0833). Figure 2 illustrates the impact of word 
sense complexity on vocabulary gains. With 
respect to word sense complexity, the average 
gains per condition are 0.2495, 0.4163, and 0.1699 
for the 1-sense, 2-senses, and 3-or-more senses 
conditions respectively. 
5 Discussion  
The results of both studies tend to confirm our 
initial hypotheses and suggest that word 
complexity, in the forms of phonetic complexity 
and the number of word senses a word has, does 
make a significant difference in how easily an L2 
vocabulary word is learned.  
 
Figure 2: Impact of word sense complexity on the 
improvement between pre-test & post-test in Study 2 
 
Figure 1: Impact of phonetic complexity on the 
improvement between pre-test & post-test in Study 1 
78
In Study 1, we see that the ?simple? words 
(those with grapheme-to-phoneme ratios equal to 
1), afford more learning than the more ?complex? 
words, as made evident by the difference in gains 
between the pre-test and post-test (p < 0.04) shown 
in Figure 1. This result suggests that the phonetic 
complexity of a word may play a role in learning 
that word in an intelligent tutoring environment. 
In Study 2, the words with many senses (3 or 
more) have significantly lower gains than words 
with 1 or 2 senses (p < 0.05). There was no 
significant difference in gains between words with 
1 word sense and words with 2 word senses, as 
shown in Figure 2. This result seems to suggest 
that words with 2 or fewer word senses are 
generally easier for L2 students to learn than those 
with 3 or more word senses. This could be because 
a student has a harder time choosing the correct 
meaning of a word amongst many choices. Fewer 
choices seem to afford more learning than showing 
just the right one, which may indicate that by 
comparing two meanings with the meaning in the 
document, the student is actively constructing her 
knowledge of the word. Dela Rosa and Eskenazi 
(2011) found that giving students only the correct 
meaning of a polysemous word afforded less 
learning than giving them several meanings in a 
ranked order. 
6 Conclusion and Future Directions 
This paper demonstrates that word complexity can 
affect how easily an L2 vocabulary word can be 
learned. We proposed two dimensions of word 
complexity, one based on the complexity of a 
word?s grapheme to phoneme ratio, and another 
based on the number of meanings a word has. Two 
in-vivo studies were conducted with ESL college 
students to test our hypothesis. Our results suggest 
that word complexity on both the phonetic and 
semantic level does have an effect on L2 
vocabulary learning.  
A future research direction that this work 
suggests is the search for other measures of word 
complexity, such as a more complex measure of 
grapheme to phoneme ratio, for example taking 
into account the ambiguity of a particular 
grapheme, or more complex measures of semantic 
complexity, like one that may take the average 
number of synonyms a word sense has, to 
determine their effect on learning using an 
intelligent tutoring system. This information could 
help define different ways to teach different words, 
providing more scaffolding for harder words, for 
example. 
We would also like to investigate whether the 
average aggregate vocabulary learning trends of 
different native language groups correlates with 
different measures of word complexity, and thus 
might reveal a relation between the structure of L1 
and difficulties in L2 vocabulary learning. 
Finally we would like to investigate whether 
providing examples of focus word usage prior to or 
following a reading activity is beneficial to 
vocabulary learning.  
Acknowledgments 
This project is supported through the Pittsburgh 
Science of Learning Center which is funded by the 
US National Science Foundation under grant 
number SBE-0836012. Any opinions, findings, and 
conclusions or recommendations expressed in this 
material are those of the authors and do not 
necessarily reflect the views of the NSF.  
The authors would like to thank Betsy Davis, 
Gregory Mizera, Dawn McCormick, Chris Ortiz, 
Tamar Bernfeld, and Kimberly Rehak at the 
University of Pittsburgh?s English Language 
Institute for their participation and input in the 
classroom studies.  
References 
Cepstral Text-to-Speech. 2001. http://www.cepstral.com 
 
Kevin Dela Rosa, Gabriel Parent, and Maxine Eskenazi. 
2010. Multimodal learning of words: A study on the 
use of speech synthesis to reinforce written text in L2 
language learning. Proceedings of the 2010 ISCA 
Workshop on Speech and Language Technology in 
Education. 
 
Kevin Dela Rosa and Maxine Eskenazi. 2011. Impact of 
Word Sense Disambiguation on Ordering Dictionary 
Definitions in Vocabulary Learning Tutors. 
Proceedings of the 24th Florida Artificial Intelligence 
Research Society Conference. 
 
Michael Heilman, Kevyn Collins-Thompson, Jamie 
Callan, and Maxine Eskenazi. 2006. Classroom 
success of an Intelligent Tutoring System for lexical 
practice and reading comprehension. Proceedings of 
the 9th International Conference on Spoken 
Language. 
 
79
Kathy Jakiellski. 1998. Motor Organization in the 
Acquisition of Consonant Clusters. PhD Thesis, 
University of Texas at Austin. 
 
Stefka Marinova-Todd, D. Bradford Marshall, And 
Catherine Snow. 2000. Three Misconceptions About 
Age and L2 Learning. TESOL Quarterly, 34(1): 9-
34. 
 
Rebecca Oxford and Robin Scarcella. 1994. Second 
Language Vocabulary Learning Among Adults: State 
Of The Art In Vocabulary Instruction. System, 22(2): 
231-243. 
 
Philip Pavlik and John Anderson. 2005. Practice and 
forgetting effects on vocabulary memory: An 
activation-based model of the spacing effect. 
Cognitive Science, 29 (4): 559-586. 
 
Charles Perfetti. 2010. Decoding, Vocabulary, and 
Comprehension: The Golden Triangle of Reading 
Skill. In Bringing Reading Researchers to Life, 
Margret McKeown & Linda Kucan (editors). 
Guilford Press, New York, US.  
Appendix A. Words Used in Studies 
Words from Study 1: condominium, exotic, 
boost, escapism, yearning, asylum, blatant, 
denizens, partisan, expats, influx, levy, taxes, 
lucrative, sector, ostracism, taunts, withdrawal 
 
Words from Study 2: established, incorporated, 
intervention, coherent, facilitate, induce, relax, 
designed, flexible, inspected, registered, category, 
enforce, illustrations, accumulate, hypothesis, 
period, qualitative, simulations, conducted, debate, 
domestic, found, concentrate, depression, register 
80
Proceedings of the Sixth Workshop on Innovative Use of NLP for Building Educational Applications, pages 136?141,
Portland, Oregon, 24 June 2011. c?2011 Association for Computational Linguistics
Predicting Change in Student Motivation by Measuring Cohesion between
Tutor and Student
Arthur Ward
Department of Biomedical
Informatics
University of Pittsburgh
Pittsburgh, Pa., 15232
akw13@pitt.edu
Diane Litman
Department of Computer
Science, LRDC
University of Pittsburgh
Pittsburgh, Pa., 15260
litman@cs.pitt.edu
Maxine Eskenazi
Language Technologies
Institute
Carnegie Mellon University
Pittsburgh, Pa., 15213
max@cmu.edu
Abstract
We apply a previously reported measure of di-
alog cohesion to a corpus of spoken tutoring
dialogs in which motivation was measured.
We find that cohesion significantly predicts
changes in student motivation, as measured
with a modified MSLQ instrument. This sug-
gests that non-intrusive dialog measures can
be used to measure motivation during tutoring.
1 Introduction
Motivation is widely believed to be an important fac-
tor in learning, and many studies have found rela-
tionships between motivation and educational out-
comes. For example Pintrich and DeGroot (1990)
found that students? motivational state was a signif-
icant predictor of classroom performance. In ad-
dition, pedagogically significant behaviors such as
dictionary lookup in the REAP (Brown and Eske-
nazi, 2004) vocabulary tutor have been shown to
be positively correlated with motivation assessments
(DelaRosa and Eskenazi, 2011). Also, in a separate
study with the REAP tutor, attempts to manipulate
reading motivation by presenting more interesting
stories were shown to improve vocabulary learning
(Heilman et al, 2010).
In addition to influencing learning outcomes, mo-
tivational state may also affect which interventions
will be effective during tutoring. For example, Ward
and Litman (2011) have shown that motivation can
significantly affect which students benefit from a re-
flective reading following interactive tutoring with a
the Itspoke (Litman and Silliman, 2004) tutor.
An accurate way to measure student motivation
during tutoring could therefore be valuable to Intel-
ligent Tutoring System (ITS) researchers. Several
self-report instruments have been developed which
measure various aspects of motivation (e.g. (Pintrich
and DeGroot, 1990; McKenna and Kear, 1990)).
However, these instruments are too intrusive to be
administered during tutoring, for fear of fatally dis-
rupting learning. We would prefer a non-intrusive
measure which would allow an ITS to detect when
student motivation is decreasing so as to launch a
motivational intervention. Similarly, the ITS should
be able to detect when motivation is increasing
again, to determine if the intervention worked. As
mentioned above, such a measure might also allow
an ITS to determine when it would be effective to
use certain instructional tactics.
In this work we investigate cohesion as a non-
intrusive measure of motivation for natural language
dialog based ITS. As defined more precisely below,
our measure of cohesion quantifies lexical and se-
mantic similarity between tutor and student dialog
utterances. We hypothesize that this measure of lexi-
cal similarity may be related to motivation in part be-
cause other measures of dialog similarity have been
shown to be related to task success. For example,
there is evidence that perceived similarity between a
student?s own speech rate and that of a recorded task
request increases the student?s feelings of immedi-
acy, which are in turn linked to greater compliance
with the request to perform a task (Buller and Aune,
1992). 1 In addition, Ward and Litman (2006; 2008)
investigated a measure of lexical similarity between
1In this experiment, the task was to watch a series of videos.
136
the tutor and student partners in a tutoring dialog
which was shown to be correlated with task success
in several corpora of tutorial dialogs.
Measures of cohesion have also been used in a va-
riety of NLP tasks such as measuring text readability
(e.g. (Pitler and Nenkova, 2008)), measuring stylis-
tic differences in text (Mccarthy et al, 2006), and
for topic segmentation in tutorial dialog (Olney and
Cai, 2005).
Given the previously mentioned results relating
motivation to educational task success, these links
between task success and cohesion lead us to hy-
pothesize a direct correlation between motivation
and cohesion when using the Itspoke tutor.
We will first briefly describe the Itspoke tutor, and
the corpus of tutoring dialogs used in this study. We
will then describe the instrument we used to mea-
sure motivation both before and immediately after
tutoring, then we will describe the algorithm used
to measure cohesion in the tutoring dialogs. Finally,
we show results of correlations between the measure
of motivation and the measure of cohesion. We will
find that the change in motivation is significantly
correlated with dialog cohesion.
2 Itspoke System and Corpus
Itspoke (Intelligent Tutoring SPOKEn dialog sys-
tem) is a spoken dialog tutoring system which
teaches qualitative physics. It provides a spoken di-
alog interface to the Why2-Atlas (VanLehn et al,
2002) tutor, and has recently been re-implemented
using the TuTalk (Jordan et al, 2007) dialog plat-
form. The Itspoke tutor presents a problem in qual-
itative physics, and asks the student an initial ques-
tion. The student answers the question, and the di-
alog continues until all points have been covered to
the tutor?s satisfaction.
The corpus used in the current work was collected
in a previous study (Ward and Litman, 2011), us-
ing novice subjects who had never taken a college
physics course. Before tutoring, students were given
a motivation survey which will be described in Sec-
tion 3. They then engaged Itspoke in five tutoring
dialogs as described above. Immediately after tu-
toring they were given the motivation questionnaire
again, with tenses changed as appropriate.
166 subjects were recruited by flyer, by advertise-
Speaker Utterance
Tutor To see which vehicle?s change in motion
is greater, we use the definition of accel-
eration. What is the definition of accel-
eration?
Student Change in velocity.
Tutor Excellent. Acceleration is defined as the
amount velocity changes per unit time.
Table 1: Dialog turns, with Token, Stem, and Semantic
Similarity Matches in bold (as discussed in Section 4).
ment during an undergraduate psychology course, or
from the University of Pittsburgh?s psychology sub-
ject pool. Of these, 40 were dismissed after pretest
as ?middle third? knowledge students, following ex-
treme groups design (Feldt, 1961). Extreme groups
design was adopted to increase the power of a ?high?
vs ?low? knowledge comparison, which is reported
elsewhere (Ward, 2010). Another 27 students were
not used for various reasons including incomplete
data. This left a corpus of 99 subjects who each par-
ticipated in 5 tutorial dialogs.
Table 1 shows an exchange from one of these di-
alogs. The tutor asks a question about the current
problem, which the student then answers. The tutor
restates the answer, and (later in the dialog) proceeds
on to the next point of discussion.
3 Motivation Measure
In this study we measure motivation using a reduced
version of the ?Motivated Strategies for Learning
Questionnaire (MSLQ)? developed by Pintrich and
DeGroot (1990). This version of the MSLQ is also
based on work by Roll (2009), who adapted it for
use in an IPL (Invention as Preparation for Learning
(Schwartz and Martin, 2004)) tutoring environment.
Our motivational survey is shown in Figure 1.
Questions one and two address ?self-regulation,?
particularly the students? tendency to manage and
control their own effort. Question one is on a re-
versed scale relative to the other questions, so re-
sponses to it were inverted. Question three addresses
?self-efficacy,? the students? expectation of success
on the task. Questions four and five address ?intrin-
sic value,? the students? beliefs about the importance
and interest of the task.
These dimensions of motivation are theoretically
137
Please read the following statements and then
click a number on the scale that best matches
how true it is of you. 1 means ?not at all true
of me? whereas 7 means ?very true of me?.
1. I think that when the tutor is talking
I will be thinking of other things and
won?t really listen to what is being said.
2. If I could take as much time as I want,
I would spend a lot of time on physics
tutoring sessions.
3. I think I am going to find the physics tu-
tor activities difficult.
4. I think I will be able to use what I learn
in the physics tutor sessions in my other
classes.
5. I think that what I will learn in the
physics tutor sessions is useful for me to
know.
Figure 1: Pre-tutoring Motivational Survey
distinct. However, except for question three (the
self-efficacy question), responses to these questions
were all very significantly correlated with each other
in our survey (p < .01).
Table 2 shows values of Cronbach?s Alpha (Cron-
bach, 1951) for various subsets of the motivation
questions. Alpha measures the internal consistency
of responses to a multi-point questionnaire, and is a
function of the number of test items and the corre-
lation between them. Higher values are thought to
indicate that the various test items are measuring the
same underlying latent construct. For this study we
omit Question 3, maximizing Alpha at .716. This
is just above the commonly accepted (Gliem and
Gliem, 2003) threshold for reliability in such an in-
strument.
Questions Alpha
1, 2, 3, 4, 5 0.531
1, 2, 4, 5 0.716
2, 4, 5 0.703
4, 5 0.683
Table 2: Alpha reliability
scores for various subsets
of questions.
As mentioned above,
this instrument was ad-
ministered both before
and (with suitable tense
changes) immediately
after tutoring. We will
report correlations using
mean scores on the
pre- and post-tutoring
measures, as well as
for the change-in-motivation score, calculated as
post-minus-pre.
4 Semantic Cohesion Measure
In this work we measure cohesion between tutor and
student using the ?semantic cohesion? measure first
reported by Ward and Litman (2008). This measure
counts the number of ?cohesive ties? (Halliday and
Hasan, 1976) between adjacent tutor and student di-
alog turns. A cohesive tie can be the repetition of an
exact word or word stem, or the use of two words
with similar meanings in adjacent turns. Stop words
are excluded, and cohesive ties are counted in both
the student-to-tutor and the tutor-to-student direc-
tions. For example, in the dialog shown in Table 1,
the final tutor turn repeats the word ?velocity? from
the previous student turn. This repetition would be
counted as an exact cohesive tie. Similarly, the tu-
tor uses the word ?changes? following the student?s
use of ?change.? This would be counted as a stem
repetition cohesive tie.
Finally, the student?s use of ?velocity? will be
counted as a cohesive tie because of its semantic
similarity to ?acceleration,? from the preceding turn.
The algorithm therefore counts four ties in Table 1.
As described more completely in (Ward and Litman,
2008), semantic similarity cohesive ties are counted
by measuring two words? proximity in the Word-
Net (Miller et al, 1990) hierarchy. We use a simple
path distance similarity measure, as implemented in
NLTK (Loper and Bird, 2002). This measure counts
the number of edges N in the shortest path between
two words in WordNet, and calculates similarity as 1
/ (1 + N). Our implementation of this semantic simi-
larity measure allows setting a threshold ?, such that
only word pairs with stronger-than-threshold simi-
larity are counted. Table 3 shows some semantic
similarity pairs counted with a threshold of 0.3.
motion-contact
man-person
decrease-acceleration
acceleration-change
travel-flying
Table 3: Example Se-
mantic ties: ? = 0.3
We obtain a normal-
ized cohesion score for
each dialog by dividing
the tie count by the num-
ber of turns in the dialog.
We then sum the line nor-
malized counts over all
the dialogs for each stu-
dent, resulting in a per-
student cohesion measure.
138
5 Results
We ran correlations between the change-in-
motivation score described in Section 3 and the
semantic similarity measure of cohesion described
in Section 4. We report results for a semantic
similarity threshold of .3 for consistency with
(Ward and Litman, 2008), however the pattern of
results is not sensitive to this threshold. Significant
results were obtained for all thresholds between .2
and .5, in .1 increments. 2 In addition, we report
results for the motivation measure with the third
question removed for consistency with (Ward and
Litman, 2011). However the pattern of results is not
sensitive to this exclusion, either. Significant results
were also obtained using the entire questionnaire.
Motivation
Measure Cor. pValue
pre-Tutoring 0.02 0.86
Change 0.21 0.03
post-Tutoring 0.19 0.055
Table 4: Cohesion - Motivation
Correlations. N = 99. ? = 0.3
In all cases,
the change in
motivation was
found to be
significantly
and positively
correlated with
the cohesive-
ness of the
tutoring dialog. More lexical similarity between
tutor and student was predictive of increased student
motivation. As shown in the middle row of Table
4, the correlation with motivational change, using a
threshold of .3 and the reduced motivation measure
was r(97)= .21, p = 0.03.
Interestingly, as shown in the top and bottom rows
of Table 4, neither motivation before tutoring r(97)
= .02, p=.86, nor after tutoring r(97) = .19, p = .055,
was significantly correlated with cohesion, although
the post-tutoring measure achieves a strong trend.
Pre- and post-tutoring mean motivation levels
were, however, significantly correlated with each
other (R(97) = .69, p < .0001). Mean motivation
levels also showed a non-significant improvement
from 4.31 before tutoring to 4.44 after tutoring.
6 Discussion and Future Work
We have brought forward evidence that cohesion in
tutorial dialog, as measured in this paper, is corre-
lated with changes in student motivation. This sug-
2Note from the path distance formula that thresholds be-
tween .5 and 1 are impossible
gests that dialog cohesion may be useful as a non-
intrusive measure of motivational fluctuations.
As discussed in Section 1, other researchers have
investigated various types of cohesion, and their re-
lationship to things such as task success and learn-
ing. In addition, work has been done investigating
the role of motivation in learning. However, we be-
lieve ours is the first work relating dialog cohesion
directly to user motivation.
The presence of a correlation between cohesion
and motivation leaves open the possibility that more
motivated students are experiencing greater task
success in the tutor, and so generating more cohe-
sive dialogs. 3 Note, however, that the very non-
significant correlation between pre-dialog motiva-
tion and dialog cohesion argues against this pos-
sibility. Instead, it seems that some process is
both creating dialog cohesion and improving student
motivation. The lack of significance in the post-
dialog/motivation correlation may be due to data
sparsity.
In future work, we hope to investigate other dia-
log features which may be even better predictors of
student motivation. As mentioned in Section 1, we
became interested in dialog similarity metrics partly
because of their association with task success. These
kinds of associations between task success and dia-
log have also been shown for dialog entrainment.
In this discussion we will use the term ?entrain-
ment? for the phenomenon in which conversational
partners? speech features become more similar to
each other at many levels, including word choice,
over the course of a dialog. 4 As mentioned above,
we use the term ?cohesion? for overall similarity of
word choice between speakers in a dialog, perhaps
resulting from entrainment.
Users appear to entrain strongly with dialog sys-
tems. For example, Brennan (1996) has found that
users are likely to adopt the terms used by a WOZ
dialog system, and that this tendency is at least as
strong as with human dialog partners. Similarly, Par-
ent and Eskenazi (2010) showed that users of the
Let?s Go (Raux et al, 2005) spoken dialog system
quickly entrain to its lexical choices.
3We thank an anonymous reviewer for prompting this dis-
cussion.
4This definition conflates studies of priming, alignment,
convergence and accommodation.
139
As with measures of dialog similarity, dialog en-
trainment has been found to be related to satisfac-
tion and success in task oriented dialogs. For ex-
ample, Reitter and Moore (2007) found that lexi-
cal and syntactic repetition predicted task success
in the MapTask corpus. Similarly, Ward and Lit-
man (2007) found that lexical and acoustic-prosodic
entrainment are correlated with task success in the
Itspoke dialog system. Interestingly, in that work
entrainment was more strongly correlated with task
success than a measure of dialog cohesion similar
to the one used in the current paper. This raises the
question of whether such a measure of dialog en-
trainment might also be a better predictor of motiva-
tion than the current measure of cohesion. We hope
in future work to further investigate this possibility.
Finally, because we are interested in predicting
motivation during tutoring, our dialog metrics may
be improved by making them sensitive to the educa-
tional domain. For example, exploratory work with
our tutor has suggested that a measure of cohesion
which only counts cohesive ties between physics
terms is better correlated with certain measures of
learning than a measure which counts non-physics
terms. This suggests that measures of cohesion
or entrainment which recognize educational domain
words may also improve correlations with motiva-
tion.
Acknowledgments
This work was supported by the ONR (N00014-07-
1-0039), by the NSF (0631930 and 0914615), and
by the LRDC and ISP at the University of Pittsburgh.
References
Susan E. Brennan. 1996. Lexical entrainment in sponta-
neous dialog. In International Symposium on Spoken
Dialog, pages 41?44.
Jonathan Brown and Maxine Eskenazi. 2004. Re-
trieval of authentic documents for reader-specific lexi-
cal practice. In In Proceedings of InSTIL/ICALL Sym-
posium.
David Buller and R.Kelly Aune. 1992. The effects of
speech rate similarity on compliance: Application of
communication accommodation theory. Western Jour-
nal of Communication, 56:37?53.
Lee Cronbach. 1951. Coefficient alpha and the internal
structure of tests. Psychometrika, 16(3):297?334.
Kevin DelaRosa and Maxine Eskenazi. 2011. Self-
assessment of motivation: Explicit and implicit indi-
cators of l2 vocabulary learning. Proceedings 15th In-
ternational Conference on Artificial Intelligence Edu-
cation (AIED).
Leonard Feldt. 1961. The use of extreme groups to
test for the presence of a relationship. Psychometrika,
26(3):307?316.
Joesph Gliem and Rosemary Gliem. 2003. Calculating,
interpreting, and reporting cronbach?s alpha reliability
coefficient for likert-type scales. Midwest Research to
Practice in Adult, Continuing and Community Educa-
tion.
M. A. K. Halliday and Ruqaiya Hasan. 1976. Cohesion
in English. English Language Series. Pearson Educa-
tion Limited.
Michael Heilman, Kevyn Collins-Thompson, Jamie
Callan, Maxine Eskenazi, Alan Juffs, and Lois Wil-
son. 2010. Personalization of reading passages im-
proves vocabulary acquisition. International Journal
of Artificial Intelligence in Education, 20:73?98, Jan-
uary.
Pamela Jordan, Brian Hall, Michael Ringenberg, Yui Cui,
and Carolyn Rose?. 2007. Tools for authoring a di-
alogue agent that participates in learning studies. In
Proc. of Artificial Intelligence in Ed., AIED, pages 43?
50.
D. Litman and S. Silliman. 2004. ITSPOKE: An intelli-
gent tutoring spoken dialogue system. In Companion
Proc. of the Human Language Technology Conf: 4th
Meeting of the North American Chap. of the Assoc. for
Computational Linguistics.
Edward Loper and Steven Bird. 2002. Nltk: The natural
language toolkit. In In Proceedings of the ACL Work-
shop on Effective Tools and Methodologies for Teach-
ing Natural Language Processing and Computational
Linguistics. Philadelphia: Association for Computa-
tional Linguistics.
Philip M. Mccarthy, Gwyneth A. Lewis, David F. Dufty,
and Danielle S. Mcnamara. 2006. Analyzing writ-
ing styles with coh-metrix. In In Proceedings of the
Florida Artificial Intelligence Research Society Inter-
national Conference (FLAIRS.
M.C. McKenna and D.J. Kear. 1990. Measuring attitude
toward reading: A new tool for teachers. The Reading
Teacher, 43(8):626?639.
George A. Miller, Richard Beckwith, Christiane Fell-
baum, Derek Gross, and Katherine J. Miller. 1990.
Introduction to WordNet: An on-line lexical database.
International Journal of Lexicography (special issue),
3 (4):235?312.
Andrew Olney and Zhiqiang Cai. 2005. An orthonormal
basis for topic segmentation in tutorial dialog. In Pro-
140
ceedings of the Human Language Technology Confer-
ence and Conference on Empirical Methods in Natural
Language Processing (HLT/EMNLP), pages 971?978.
Vancouver, October.
Gabriel Parent and Maxine Eskenazi. 2010. Lexical en-
trainment of real users in the let?s go spoken dialog
system. In Proceedings Interspeech-2010, pages 3018
? 3021, Makuhari, Chiba, Japan.
Paul Pintrich and Elisabeth DeGroot. 1990. Motivational
and self-regulated learning components of classroom
academic performance. Journal of Educational Psy-
chology, 82(1):33?40.
Emily Pitler and Ani Nenkova. 2008. Revisiting read-
ability: A unified framework for predicting text qual-
ity. In Proceedings of Empirical Methods in Natural
Language Processing (EMNLP), pages 186 ? 195.
Antoine Raux, Brian Langner, Dan Bohus, Alan W
Black, and Maxine Eskenazi. 2005. Lets go public!
taking a spoken dialog system to the real world. In
Proceedings Interspeech-2005, pages 885 ? 888, Lis-
bon, Portugal.
David Reitter and Johanna D. Moore. 2007. Predict-
ing success in dialogue. In In Proceedings of the 45th
Annual Meeting of the Association of Computational
Linguistics (ACL), pages 808 ? 815, Prague, Czech Re-
public.
Ido Roll. 2009. Structured Invention Tasks to Pre-
pare Students for Future Learning: Means, Mecha-
nisms, and Cognitive Processes. Doctor of philoso-
phy, Carnegie Mellon University, 5000 Forbes Ave.
Pittsburgh, Pa.
Daniel Schwartz and Taylor Martin. 2004. Inventing
to prepare for future learning: The hidden efficiency
of encouraging original student production in statistics
instruction. Cognition and Instruction, 22:129 ? 184.
K. VanLehn, P. Jordan, C. Rose, D. Bhembe, M. Boettner,
A. Gaydos, M. Makatchev, U. Pappuswamy, M. Rin-
genberg, A. Roque, S. Siler, and Srivastava R. 2002.
The architecture of why2-atlas: A coach for qualita-
tive physics essay writing. In Proc. 6th Int. Conf. on
Intelligent Tutoring Systems, pages 158?167.
Arthur Ward and Diane Litman. 2006. Cohesion and
learning in a tutorial spoken dialog system. In Pro-
ceedings of the 19th International FLAIRS Conference
(FLAIRS-19), pages 533?538, May.
Arthur Ward and Diane Litman. 2007. Dialog con-
vergence and learning. In Proceedings 13th Interna-
tional Conference on Artificial Intelligence Education
(AIED), Los Angeles, Ca.
Arthur Ward and Diane Litman. 2008. Semantic
cohesion and learning. In Proceedings 9th Inter-
national Conference on Intelligent Tutoring Systems
(ITS), pages 459?469, Ann Arbor, June.
Arthur Ward and Diane Litman. 2011. Adding abstrac-
tive reflection to a tutorial dialog system. Proceedings
24th International FLAIRS (Florida Artificial Intelli-
gence Research Society) Conference.
Arthur Ward. 2010. Reflection and Learning Robust-
ness in a Natural Language Conceptual Physics Tutor-
ing System. Doctor of philosophy, University of Pitts-
burgh, Pittsburgh, PA. 15260.
141
Proceedings of the SIGDIAL 2011: the 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 2?7,
Portland, Oregon, June 17-18, 2011. c?2011 Association for Computational Linguistics
Spoken Dialog Challenge 2010: 
 Comparison of Live and Control Test Results 
Alan W Black1, Susanne Burger1, Alistair Conkie4, Helen Hastie2, Simon Keizer3,  Oliver 
Lemon2, Nicolas Merigaud2, Gabriel Parent1, Gabriel Schubiner1, Blaise Thomson3, Jason 
D. Williams4, Kai Yu3, Steve Young3 and Maxine Eskenazi1 
1Language Technologies Institute, Carnegie Mellon University, Pittsburgh, USA 
2Dept of Mathematical and Computer Science, Heriot-Watt University, Edinburgh, UK 
3Engineering Department, Cambridge University, Cambridge, UK 
4AT&T Labs ? Research, Florham Park, NJ, USA 
awb@cs.cmu.edu 
 
 
Abstract 
The Spoken Dialog Challenge 2010 was an 
exercise to investigate how different spo-
ken dialog systems perform on the same 
task.  The existing Let?s Go Pittsburgh Bus 
Information System was used as a task and 
four teams provided systems that were first 
tested in controlled conditions with speech 
researchers as users. The three most stable 
systems were then deployed to real callers.  
This paper presents the results of the live 
tests, and compares them with the control 
test results. Results show considerable 
variation both between systems and be-
tween the control and live tests.  Interest-
ingly, relatively high task completion for 
controlled tests did not always predict 
relatively high task completion for live 
tests.  Moreover, even though the systems 
were quite different in their designs, we 
saw very similar correlations between word 
error rate and task completion for all the 
systems.  The dialog data collected is 
available to the research community. 
1 Background 
The goal of the Spoken Dialog Challenge (SDC) is 
to investigate how different dialog systems per-
form on a similar task.  It is designed as a regularly 
recurring challenge. The first one took place in 
2010. SDC participants were to provide one or 
more of three things: a system; a simulated user, 
and/or an evaluation metric.   The task chosen for 
the first SDC was one that already had a large 
number of real callers. This had several advan-
tages. First, there was a system that had been used 
by many callers. Second, there was a substantial 
dataset that participants could use to train their sys-
tems.  Finally, there were real callers, rather than 
only lab testers.  Past work has found systems 
which appear to perform well in lab tests do not 
always perform well when deployed to real callers, 
in part because real callers behave differently than 
lab testers, and usage conditions can be considera-
bly different [Raux et al2005, Ai et al2008].  De-
ploying systems to real users is an important trait 
of the Spoken Dialog Challenge. 
The CMU Let?s Go Bus Information system 
[Raux et al2006] provides bus schedule informa-
tion for the general population of Pittsburgh.  It is 
directly connected to the local Port Authority, 
whose evening calls for bus information are redi-
rected to the automated system.  The system has 
been running since March 2005 and has served 
over 130K calls. 
The software and the previous years of dialog 
data were released to participants of the challenge 
to allow them to construct their own systems.  A 
number of sites started the challenge, and four sites 
successfully built systems, including the original 
CMU system. 
An important aspect of the challenge is that 
the quality of service to the end users (people in 
Pittsburgh) had to be maintained and thus an initial 
robustness and quality test was carried out on con-
tributed systems.  This control test provided sce-
narios over a web interface and required 
researchers from the participating sites to call each 
of the systems.  The results of this control test were 
published in [Black et al 2010] and by the individ-
ual participants [Williams et al 2010, Thomson et 
al. 2010, Hastie et al 2010] and they are repro-
2
duced below to give the reader a comparison with 
the later live tests. 
Important distinctions between the control 
test callers and the live test callers were that the 
control test callers were primarily spoken dialog 
researchers from around the world.  Although they 
were usually calling from more controlled acoustic 
conditions, most were not knowledgeable about 
Pittsburgh geography.     
As mentioned above, four systems took part 
in the SDC.  Following the practice of other chal-
lenges, we will not explicitly identify the sites 
where these systems were developed. We simply 
refer to them as SYS1-4 in the results.  We will, 
however, state that one of the systems is the system 
that has been running for this task for several 
years. The architectures of the systems cover a 
number of different techniques for building spoken 
dialog systems, including agenda based systems, 
VoiceXML and statistical techniques. 
2 Conditions of Control and Live tests 
For this task, the caller needs to provide the depar-
ture stop, the arrival stop and the time of departure 
or arrival in order for the system to be able to per-
form a lookup in the schedule database. The route 
number can also be provided and used in the 
lookup, but it is not necessary. The present live 
system covers the East End of Pittsburgh.  Al-
though the Port Authority message states that other 
areas are not covered, callers may still ask for 
routes that are not in the East End; in this case, the 
live system must say it doesn?t have information 
available.  Some events that affect the length of the 
dialog include whether the system uses implicit or 
explicit confirmation or some combination of both, 
whether the system has an open-ended first turn or 
a directed one, and whether it deals with requests 
for the previous and/or following bus (this latter 
should have been present in all of the systems). 
Just before the SDC started, the Port Author-
ity had removed some of its bus routes. The sys-
tems were required to be capable of informing the 
caller that the route had been canceled, and then 
giving them a suitable alternative. 
SDC systems answer live calls when the Port 
Authority call center is closed in the evening and 
early morning.  There are quite different types and 
volumes of calls over the different days of the 
week.  Weekend days typically have more calls, in 
part because the call center is open fewer hours on 
weekends.  Figure 1 shows a histogram of average 
calls per hour for the evening and the early morn-
ing of each day of the week. 
 
calls per weekday / ave per hour
0
1
2
3
4
5
6
7
8
9
10
Fr-
19-
0
Sa-
0-8
Sa-
16
-
0
Su-
0-8
Su-
16
-
0
Mo
-
0-7
Mo
-
19
-
0
Tu
-
0-7
Tu
-
19-
0
We
-
0-7
We
-
19
-
0
Th
-
0-7
Th
-
19-
0
Fr-
0-7
 
Figure 1: average number of calls per hour on weekends 
(dark bars) and weekdays. Listed are names of days and 
times before and after midnight when callers called the 
system. 
 
The control tests were set up through a simple 
web interface that presented 8 different scenarios 
to callers. Callers were given a phone number to 
call; each caller spoke to each of the 4 different 
systems twice.  A typical scenario was presented 
with few words, mainly relying on graphics in or-
der to avoid influencing the caller?s choice of vo-
cabulary.  An example is shown in Figure 2. 
 
 
 
Figure 2: Typical scenario for the control tests.  This 
example requests that the user find a bus from the cor-
ner of Forbes and Morewood (near CMU) to the airport, 
using bus route 28X, arriving by 10:45 AM. 
 
3
3 Control Test Results 
The logs from the four systems were labeled for 
task success by hand.  A call is successful if any of 
the following outputs are correctly issued: 
 
? Bus schedule for the requested departure and 
arrival stops for the stated bus number (if giv-
en). 
? A statement that there is no bus available for 
that route. 
? A statement that there is no scheduled bus at 
that time. 
 
We additionally allowed the following boundary 
cases: 
 
? A departure/arrival stop within 15 minutes 
walk. 
? Departure/arrival times within one hour of re-
quested time. 
? An alternate bus number that serves the re-
quested route. 
 
In the control tests, SYS2 had system connection 
issues that caused a number of calls to fail to con-
nect, as well as a poorer task completion.  It was 
not included in the live tests.  It should be pointed 
out that SYS2 was developed by a single graduate 
student as a class project while the other systems 
were developed by teams of researchers.  The re-
sults of the Control Tests are shown in Table 1 and 
are discussed further below. 
 
Table 1. Results of hand analysis of the four systems in 
the control test 
 The three major classes of system response 
are as follows.  no_info: this occurs when the sys-
tem gives neither a specific time nor a valid excuse 
(bus not covered, or none at that time).  no_info 
calls can be treated as errors (even though there 
maybe be valid reasons such as the caller hangs up 
because the bus they are waiting for arrives).  
donthave: identifies calls that state the requested 
bus is not covered by the system or that there is no 
bus at the requested time. pos_out: identifies calls 
where a specific time schedule is given.  Both 
donthave and pos_out calls may be correct or er-
roneous (e.g the given information is not for the 
requested bus,  the departure stop is wrong, etc). 
4 Live Tests Results 
In the live tests the actual Pittsburgh callers had 
access to three systems: SYS1, SYS3, and SYS4.  
Although engineering issues may not always be 
seen to be as relevant as scientific results, it is im-
portant to acknowledge several issues that had to 
be overcome in order to run the live tests. 
Since the Pittsburgh Bus Information System 
is a real system, it is regularly updated with new 
schedules from the Port Authority. This happens 
about every three months and sometimes includes 
changes in bus routes as well as times and stops. 
The SDC participants were given these updates 
and were allowed the time to make the changes to 
their systems. Making things more difficult is the 
fact that the Port Authority often only releases the 
schedules a few days ahead of the change. Another 
concern was that the live tests be run within one 
schedule period so that the change in schedule 
would not affect the results.   
The second engineering issue concerned 
telephony connectivity. There had to be a way to 
transfer calls from the Port Authority to the par-
ticipating systems (that were run at the participat-
ing sites, not at CMU) without slowing down or 
perturbing service to the callers.  This was 
achieved by an elaborate set of call-forwarding 
mechanisms that performed very reliably.  How-
ever, since one system was in Europe, connections 
to it were sometimes not as reliable as to the US-
based systems.  
 
 SYS1 SYS3 SYS4 
Total Calls 678 451 742 
Non-empty calls 633 430 670 
no_ info 18.5% 14.0% 11.0% 
donthave 26.4% 30.0% 17.6% 
donthave_corr 47.3% 40.3% 37.3% 
donthave_incorr 52.7% 59.7% 62.7% 
pos_out 55.1% 56.0% 71.3% 
pos_out_corr 86.8% 93.8% 91.6% 
pos_out_incorr 13.2% 6.2% 8.4% 
 
Table 2. Results of hand analysis of the three systems in 
the live tests.  Row labels are the same as in Table 1. 
 SYS1 SYS2 SYS3 SYS4 
Total Calls 91 61 75 83 
no_ info 3.3% 37.7% 1.3% 9.6% 
donthave 17.6% 24.6% 14.7% 9.6% 
donthave_corr 68.8% 33.3% 100.0% 100.0% 
donthave_incorr 31.3% 66.7% 0.0% 0.0% 
pos_out 79.1% 37.7% 84.0% 80.7% 
pos_out_corr 66.7% 78.3% 88.9% 80.6% 
pos_out_incorr 33.3% 21.7% 11.1% 19.4% 
4
We ran each of the three systems for multiple two 
day periods over July and August 2010.  This de-
sign gave each system an equal distribution of 
weekdays and weekends, and also ensured that 
repeat-callers within the same day experienced the 
same system. 
One of the participating systems (SYS4) 
could support simultaneous calls, but the other two 
could not and the caller would receive a busy sig-
nal if the system was already in use.  This, how-
ever, did not happen very often. 
Results of hand analysis of real calls are 
shown in Table 4 alongside the results for the Con-
trol Test for easy comparison.  In the live tests we 
had an additional category of call types ? empty 
calls (0-turn calls) ? which are calls where there 
are no user turns, for example because the caller 
hung up or was disconnected before saying any-
thing.  Each system had 14 days of calls and exter-
nal daily factors may change the number of calls. 
We do suspect that telephony issues may have pre-
vented some calls from getting through to SYS3 on 
some occasions.   
Table 3 provides call duration information for 
each of the systems in both the control and live 
tests. 
 
 
 Length (s) Turns/call Words/turn 
SYS1 control 155 18.29 2.87 (2.84) 
SYS1 live 111 16.24 2.15 (1.03) 
SYS2 control 147 17.57 1.63 (1.62) 
SYS3 control 96 10.28 2.73 (1.94) 
SYS3 live 80 9.56 2.22 (1.14) 
SYS4 control 154 14.70 2.25 (1.78) 
SYS4 live 126 11.00 1.63 (0.77) 
 
Table 3: For live tests, average length of each call, aver-
age number of turns per call, and average number of 
words per turn (numbers in brackets are standard devia-
tions). 
 
Each of the systems used a different speech 
recognizer.  In order to understand the impact of 
word error rate on the results, all the data were 
hand transcribed to provide orthographic transcrip-
tions of each user turn.   Summary word error sta-
tistics are shown in Table 4.   However, summary 
statistics do not show the correlation between word 
error rate and dialogue success.  To achieve this, 
following Thomson et al(2010), we computed a 
logistic regression of success against word error 
rate (WER) for each of the systems. Figure 3 
shows the regressions for the Control Tests and 
Figure 4 for the Live Tests.  
 
 SYS1 SYS3 SYS4 
Control 38.4 27.9 27.5 
Live 43.8 42.5 35.7 
 
Table 4: Average dialogue word error rate (WER). 
 
0 20 40 60 80 100
0.
0
0.
2
0.
4
0.
6
0.
8
1.
0
WER
Su
cc
es
s 
Ra
te
Sys4
Sys3
Sys1
 
Figure 3: Logistic regression of control test success vs 
WER for the three fully tested systems 
0 20 40 60 80 100
0.
0
0.
2
0.
4
0.
6
0.
8
1.
0
WER
Su
cc
e
ss
Sys1
Sys3
Sys4
 
Figure 4: Logistic regression of live success vs WER for 
the three fully tested systems 
 
5
In order to compare the control and live tests, 
we can calculate task completion as the percentage 
of calls that gave a correct result.  We include only 
non-empty calls (excluding 0-turn calls), and treat 
all no_info calls as being incorrect, even though 
some may be due to extraneous reasons such as the 
bus turning up (Table 5). 
 
 SYS1 SYS3 SYS4 
Control 64.9% (5.0%) 89.4% (3.6%) 74.6% (4.8%) 
Live 60.3% (1.9%) 64.6% (2.3%) 71.9% (1.7%) 
 
Table 5: Live and control test task completion (std. err).  
 
5 Discussion 
All systems had lower WER and higher task com-
pletion in the controlled test vs. the live test.  This 
agrees with past work [Raux et al2005, Ai et al
2008], and underscores the challenges of deploying 
real-world systems. 
For all systems, dialogs with controlled sub-
jects were longer than with live callers ? both in 
terms of length and number of turns.  In addition, 
for all systems, live callers used shorter utterances 
than controlled subjects.  Controlled subjects may 
be more patient than live callers, or perhaps live 
callers were more likely to abandon calls in the 
face of higher recognition error rates.   
Some interesting differences between the sys-
tems are evident in the live tests.  Looking at dia-
log durations, SYS3 used confirmations least often, 
and yielded the fastest dialogs (80s/call).  SYS1 
made extensive use of confirmations, yielding the 
most turns of any system and slightly longer dia-
logs (111s/call).  SYS4 was the most system-
directed, always collecting information one ele-
ment at a time.  As a result it was the slowest of the 
systems (126s/call), but because it often used im-
plicit confirmation instead of explicit confirmation, 
it had fewer turns/call than SYS1.   
For task completion, SYS3 performed best in 
the controlled trials, with SYS1 worst and SYS4 in 
between.  However in the live test, SYS4 per-
formed best, with SYS3 and SYS1 similar and 
worse.  It was surprising that task completion for 
SYS3 was the highest for the controlled tests yet 
among the lowest for the live tests.  Investigating 
this, we found that much of the variability in task 
completion for the live tests appears to be due to 
WER.  In the control tests SYS3 and SYS4 had 
similar error rates but the success rate of SYS3 was 
higher.  The regression in Figure 3 shows this 
clearly.   In the live tests SYS3 had a significantly 
higher word error rate and average success rate 
was much lower than in SYS4.   
It is interesting to speculate on why the rec-
ognition rates for SYS3 and SYS4 were different 
in the live tests, but were comparable in the control 
tests.  In a spoken dialogue system the architecture 
has a considerable impact on the measured word 
error rate.  Not only will the language model and 
use of dialogue context be different, but the dia-
logue design and form of system prompts will in-
fluence the form and content of user inputs.   Thus, 
word error rates do not just depend on the quality 
of the acoustic models ? they depend on the whole 
system design.  As noted above, SYS4 was more 
system-directed than SYS3 and this probably con-
tributed to the comparatively better ASR perform-
ance with live users.   In the control tests, the 
behavior of users (research lab workers) may have 
been less dependent on the manner in which users 
were prompted for information by the system.  
Overall, of course, it is user satisfaction and task 
success which matter. 
6 Corpus Availability and Evaluation 
The SDC2010 database of all logs from all systems 
including audio plus hand transcribed utterances, 
and hand defined success values is released 
through CMU?s Dialog Research Center 
(http://dialrc.org). 
One of the core goals of the Spoken Dialog 
Challenge is to not only create an opportunity for 
researchers to test their systems on a common plat-
form with real users, but also create common data 
sets for testing evaluation metrics.  Although some 
work has been done on this for the control test data 
(e.g. [Zhu et al2010]), we expect further evalua-
tion techniques will be applied to these data. 
One particular issue which arose during this 
evaluation concerned the difficulty of defining pre-
cisely what constitutes task success.  A precise de-
finition is important to developers, especially if 
reinforcement style learning is being used to opti-
mize the success.  In an information seeking task 
of the type described here, task success is straight-
forward when the user?s requirements can be satis-
fied but more difficult if some form of constraint 
relaxation is required.   For example, if the user 
6
asks if there is a bus from the current location to 
the airport ? the answer ?No.? may be strictly cor-
rect but not necessarily helpful.  Should this dia-
logue be scored as successful or not?  The answer 
?No, but there is a stop two blocks away where 
you can take the number 28X bus direct to the air-
port.? is clearly more useful to the user.  Should 
success therefore be a numeric measure rather than 
a binary decision?  And if a measure, how can it be 
precisely defined?  A second and related issue is 
the need for evaluation algorithms which deter-
mine task success automatically.   Without these, 
system optimization will remain an art rather than 
a science. 
7 Conclusions 
This paper has described the first attempt at an ex-
ercise to investigate how different spoken dialog 
systems perform on the same task.  The existing 
Let?s Go Pittsburgh Bus Information System was 
used as a task and four teams provided systems 
that were first tested in controlled conditions with 
speech researchers as users. The three most stable 
systems were then deployed ?live? with real call-
ers. Results show considerable variation both be-
tween systems and between the control and live 
tests.  Interestingly, relatively high task completion 
for controlled tests did not always predict rela-
tively high task completion for live tests.  This 
confirms the importance of testing on live callers, 
not just usability subjects. 
 The general organization and framework 
of the evaluation worked well.  The ability to route 
audio telephone calls to anywhere in the world us-
ing voice over IP protocols was critical to the suc-
cess of the challenge since it provides a way for 
individual research labs to test their in-house sys-
tems without the need to port them to a central co-
ordinating site. 
 Finally, the critical role of precise evalua-
tion metrics was noted and the need for automatic 
tools to compute them.  Developers need these at 
an early stage in the cycle to ensure that when sys-
tems are subsequently evaluated, the results and 
system behaviors can be properly compared.  
Acknowledgments 
Thanks to AT&T Research for providing telephony 
support for transporting telephone calls during the 
live tests.  This work was in part supported by the 
US National Science foundation under the project 
?Dialogue Research Center?.   
References  
Ai, H., Raux, A., Bohus, D., Eskenzai, M., and Litman, 
D.  (2008)  ?Comparing spoken dialog corpora col-
lected with recruited subjects versus real users?, Proc 
SIGDial, Columbus, Ohio, USA.  
Black, A., Burger, S., Langner, B., Parent, G., and Es-
kenazi, M. (2010) ?Spoken Dialog Challenge 2010?, 
SLT 2010, Berkeley, CA.  
Hastie, H., Merigaud, N., Liu, X and Oliver Lemon. 
(2010) ? ?Let?s Go Dude?, Using The Spoken Dia-
logue Challenge to Teach Spoken Dialogue Devel-
opment?, SLT 2010, Berkeley, CA. 
Raux, A., Langner, B., Bohus, D., Black, A., Eskenazi, 
M.  (2005)  ?Let?s go public! Taking a spoken dialog 
system to the real world?, Interspeech 2005, Lisbon, 
Portugal. 
Raux, A., Bohus, D., Langner, B., Black, A., and Eske-
nazi, M. (2006) ?Doing Research on a Deployed 
Spoken Dialogue System: One Year of Let's Go! Ex-
perience?, Interspeech 2006 - ICSLP, Pittsburgh, PA.  
Thomson B., Yu, K. Keizer, S., Gasic, M., Jurcicek, F.,  
Mairesse, F. and Young, S. ?Bayesian Dialogue Sys-
tem for the Let?s Go Spoken Dialogue Challenge?, 
SLT 2010, Berkeley, CA. 
Williams, J., Arizmendi, I., and Conkie, A. ?Demonstra-
tion of AT&T ?Let?s Go?: A Production-Grade Statis-
tical Spoken Dialog System.? SLT 2010, Berkeley, 
CA. 
Zhu, Y., Yang, Z., Meng, H., Li, B., Levow, G., and 
King, I. (2010) ?Using Finite State Machines for 
Evaluating Spoken Dialog Systems?, SLT 2010, 
Berkeley, CA. 
7
Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 50?59,
Seoul, South Korea, 5-6 July 2012. c?2012 Association for Computational Linguistics
An Unsupervised Approach to User Simulation: toward Self-Improving
Dialog Systems
Sungjin Lee1,2 and Maxine Eskenazi1
1Language Technologies Institute, Carnegie Mellon University, Pittsburgh, Pennsylvania
2Computer Science and Engineering, Pohang University of Science and Technology, South Korea
{sungjin.lee, max}@cs.cmu.edu1, junion@postech.ac.kr2
Abstract
This paper proposes an unsupervised ap-
proach to user simulation in order to automati-
cally furnish updates and assessments of a de-
ployed spoken dialog system. The proposed
method adopts a dynamic Bayesian network
to infer the unobservable true user action from
which the parameters of other components are
naturally derived. To verify the quality of the
simulation, the proposed method was applied
to the Let?s Go domain (Raux et al, 2005)
and a set of measures was used to analyze the
simulated data at several levels. The results
showed a very close correspondence between
the real and simulated data, implying that it is
possible to create a realistic user simulator that
does not necessitate human intervention.
1 Introduction
For the past decade statistical approaches to dialog
modeling have shown positive results for optimizing
a dialog strategy with real data by applying well-
understood machine learning methods such as rein-
forcement learning (Henderson et al, 2008; Thom-
son and Young, 2010; Williams and Young, 2007b).
User simulation is becoming an essential component
in developing and evaluating such systems. In this
paper we describe an unsupervised process to au-
tomatically develop user simulators. The motiva-
tion for this comes from the fact that many systems
are presently moving from being simple lab simu-
lations to actual deployed systems with real users.
These systems furnish a constant flow of new data
that needs to be processed in some way. Our goal is
to minimize human intervention in processing this
data. Previously, data had to be hand-annotated, a
slow and costly process. Recently crowdsourcing
has made annotation faster and less expensive, but
all of the data still has to be processed and time
must be spent in creating the annotation interface
and tasks, and in quality control. Our goal is to pro-
cess the metadata (e.g. user actions, goals, error ty-
pology) in an unsupervised manner. And our method
eliminates the need for human transcription and an-
notation by inferring the user goal from grounding
information. We also consider user actions as la-
tent variables which are inferred based on observa-
tions from Automatic Speech Recognition (ASR).
We used the above inferred user actions paired with
the observed actions to build an error model. Since
the focus of this work is placed on improving and
evaluating the dialog strategy, error simulation can
be carried out at the semantic level. This eliminates
the need for transcription, which would have neces-
sitated an error simulation at the surface level. The
end result here will be a system that has as little hu-
man intervention as possible.
This paper is structured as follows. Section 2 de-
scribes previous research and the novelty of our ap-
proach. Section 3 elaborates on our proposed un-
supervised approach to user simulation. Section 4
explains the experimental setup. Section 5 presents
and discusses the results. Finally, Section 6 con-
cludes with a brief summary and suggestions for fu-
ture research.
2 Related Work
Previous user simulation studies can be roughly cat-
egorized into rule-based methods (Chung, 2005;
50
Lopez-Cozar et al, 2006; Schatzmann et al, 2007a)
and data-driven methods (Cuayahuitl et al, 2005;
Eckert et al, 1997; Jung et al, 2009; Levin et al,
2000; Georgila et al, 2006; Pietquin, 2004). Rule-
based methods generally allow for more control over
their designs for the target domain while data-driven
methods afford more portability from one domain to
another and are attractive for modeling user behav-
ior based on real data. Although development costs
for data-driven methods are typically lower than
those of rule-based methods, previous data-driven
approaches have still required a certain amount of
human effort. Most intention-level models take a
semantically annotated corpus to produce user in-
tention without introducing errors (Cuayahuitl et al,
2005; Jung et al, 2009). Surface-level approaches
need transcribed data to train their surface form and
error generating models (Jung et al, 2009; Schatz-
mann et al, 2007b). A few studies have attempted to
directly simulate the intention, surface, and error by
applying their statistical methods on the recognized
data rather than on the transcribed data (Georgila et
al., 2006; Schatzmann et al, 2005). Although such
approaches can avoid human intervention, the sole
incorporation of erroneous user action can propa-
gate those errors to the higher-level discourse fea-
tures which are computed from them, and thus could
result in less realistic user behavior. In this work, the
true user action is treated as a hidden variable and,
further, its associated dialog history is also viewed as
latent so that the uncertainty of the true user action
is properly controlled in a principled manner. Syed
and Williams (2008) adopted the Expectation Max-
imization algorithm for parameter learning for a la-
tent variable model. But their method still requires a
small amount of transcribed data to learn the obser-
vation confusability, and it suffers from overfitting
as a general property of maximum likelihood. To
address this problem, we propose a Bayesian learn-
ing method, which requires no transcribed data.
3 Unsupervised Approach to User
Simulation
Before describing each component in detail, we
present the overall process of user simulation with
an example in the Let?s Go domain in Figure 1. To
begin a dialog, the user simulator first sets the user
Figure 1: The overall process of user simulation in the
Let?s Go domain, where users call the spoken dialog sys-
tem to get bus schedule information for Pittsburgh
goal by sampling the goal model. Then the user sim-
ulator engages in a conversation with the dialog sys-
tem until the termination model ends it. At each
turn, the termination model randomly determines
whether the dialog will continue or not. If the dia-
log continues, the user model generates user actions
at the predicate level with respect to the given user
goal and system action. Having the user actions, the
error template model transforms some user actions
into other actions if necessary and determines which
action will receive an incorrect value. After that, the
error value model substantiates the values by draw-
ing a confusable value if specified to be incorrect or
by using the goal value. Finally, a confidence score
will be attached to the user action by sampling the
confidence score model which conditions on the cor-
rectness of the final user action.
3.1 Goal Model
The goal model is the first component to be de-
fined in terms of the working flow of the user sim-
ulator. In order to generate a plausible user goal
in accordance with the frequency at which it ap-
pears in a real situation, the dialog logs are parsed
to look for the grounding information1 that the users
have provided. Since the representation of a user
goal in this study is a vector of constraints required
by a user, for example [Route:61C, Source:CMU,
1Specifically, we used explicitly confirmed information by
the system for this study
51
Destination:AIRPORT, Time:6 PM], each time we
encounter grounding information that includes the
constraints used in the backend queries, this is added
to the user goal. If two actions contradict each other,
the later action overwrites the earlier one. Once all
of the user goals in the data have been gathered,
a discrete distribution over the user goal is learned
using a maximum likelihood estimation. Because
many variables later in this paper are discrete, a gen-
eral notation of a conditional discrete distribution is
expressed as follows:
p(xi|xpa(i),?) =
?
k,k?
??(pa(i),k)?(xi,k
?)
k,k? (1)
where k represents the joint configuration of all the
parents of i and ?(?, ?) denotes Kronecker delta. Note
that ?k? ?k,k? = 1. Given this notation, the goal
model ? can be written in the following form:
g ? p(g|?) =
?
k
??(g,k)k (2)
3.2 User Model
Having generated a user goal, the next task is to infer
an appropriate user action for the given goal and sys-
tem action. This is what the user model does. Since
one of key properties of our unsupervised approach
is that the true user actions are not observable, the
user model should maintain a belief over the dia-
log state by taking into consideration the observed
user actions. Inspired by (Williams et al, 2005),
to keep the complexity of the user model tractable,
a dynamic Bayesian network is adopted with sev-
eral conditional independence assumptions, giving
rise to the graphical structure which is shown in Fig-
ure 2. Unlike belief tracking in a dialog system, the
user goal in a user simulation is pre-determined be-
fore the beginning of the dialog. As with most pre-
vious studies, this property allows the user model
to deal with a predicate-level action consisting of a
speech act and a concept (e.g. [Inform(Source), In-
form(Time)]) and is only concerned about whether a
given field is specified or not in the user goal (e.g.
Bus:Unspecified, Source:Specified). This abstract-
level handling enables the user model to employ ex-
act inference algorithms such as the junction tree
algorithm (Lauritzen and Spiegelhalter, 1988) for
more efficient reasoning over the graphical structure.
Figure 2: The graphical structure of the dynamic
Bayesian network for the user model. g denotes the user
goal and st,ut,ht,ot represents the system action, the
user action, the dialog history, and the observed user ac-
tion for each time slice, respectively. The shaded items
are observable and the transparent ones are latent.
The joint distribution for this model is given by
p(g,S,H,U,O|?)
= p(h0|pi)
?
t
p(ut|g, st,ht?1,?)
? p(ht|ht?1,ut,?)p(ot|ut, ?)
(3)
where a capital letter stands for the set of
corresponding random variables, e.g., U =
{u1, . . . ,uN}, and ? = {pi,?,?, ?} denotes the
set of parameters governing the model2.
For a given user goal, the user model basically
performs an inference to obtain a marginal distribu-
tion over ut for each time step from which it can
sample the probability of a user action in a given
context:
ut ? p(ut|g, st1,ut?11 ,?) (4)
where st1 denotes the set of system actions from time
1 to time t and ut?11 is the set of previously sampled
user actions from time 1 to time t? 1.
3.2.1 Parameter Estimation
As far as parameters are concerned, ? is a determin-
istic function that yields a fraction of an observed
confidence score in accordance with the degree of
agreement between ut and ot:
p(ot|ut) = CS(ot) ?
( |ot ? ut|
|ot ? ut|
)p
+  (5)
2Here, uniform prior distributions are assigned on g and S
52
where CS(?) returns the confidence score of the as-
sociated observation and p is a control variable over
the strength of disagreement penalty3. In addition, pi
and ? are deterministically set by simple discourse
rules, for example:
p(ht = Informed|ht?1,ut) ={
1 if ht?1 = Informed or ut = Inform(?),
0 otherwise.
(6)
The only parameter that needs to be learned in the
user model, therefore, is ? and it can be estimated
by maximizing the likelihood function (Equation 7).
The likelihood function is obtained from the joint
distribution (Equation 3) by marginalizing over the
latent variables.
p(g,S,O|?) =
?
H,U
p(g,S,H,U,O|?) (7)
Since direct maximization of the likelihood func-
tion will lead to complex expressions with no
closed-form solutions due to the latent variables, the
Expectation-Maximization (EM) algorithm is an ef-
ficient framework for finding maximum likelihood
estimates.
As it is well acknowledged, however, that over-
fitting can arise as a general property of maximum
likelihood, especially when only a small amount of
data is available, a Bayesian approach needs to be
adopted. In a Bayesian model, any unknown pa-
rameter is given a prior distribution and is absorbed
into the set of latent variables, thus it is infeasible
to directly evaluate the posterior distribution of the
latent variables and the expectations with respect to
this distribution. Therefore a deterministic approx-
imation, called mean field theory (Parisi, 1988), is
applied.
In mean field theory, the family of posterior distri-
butions of the latent variables is assumed to be par-
titioned into disjoint groups:
q(Z) =
M?
i=1
qi(Zi) (8)
where Z = {z1, . . . , zN} denotes all latent variables
including parameters and Zi is a disjoint group.
3For this study, p was set to 1.0
Amongst all distributions q(Z) having the form of
Equation 8, we then seek the member of this family
for which the divergence from the true posterior dis-
tribution is minimized. To achieve this, the follow-
ing optimization with respect to each of the qi(Zi)
factors is to be performed in turn (Bishop, 2006):
ln q?j (Zj) = Ei 6=j
[
ln p(X,Z)
]
+ const (9)
where X = {x1, . . . ,xN} denotes all observed vari-
ables and Ei 6=j means an expectation with respect to
the q distributions over all groups Zi for i 6= j.
Now, we apply the mean field theory to the user
model. Before doing so, we need to introduce the
prior over the parameter ? which is a product of
Dirichlet distributions4.
p(?) =
?
k
Dir(?k|?0k)
=
?
k
C(?0k)
?
l
??
0
k?1
k,l
(10)
where k represents the joint configuration of all of
the parents and C(?0k) is the normalization constant
for the Dirichlet distribution. Note that for symme-
try we have chosen the same parameter ?0k for each
of the components.
Next we approximate the posterior distribution,
q(H,U,?) using a factorized form, q(H,U)q(?).
Then we first apply Equation 9 to find an expression
for the optimal factor q?(?):
ln q?(?) = EH,U
[
ln p(g,S,H,U,O,?)
]
+ const
= EH,U
[?
t
ln p(ut|g, st,ht?1,?)
]
+ ln p(?) + const
=
?
t
?
i,j,k,l
(
EH,U
[
?i,j,k,l
]
ln?i,j,k,l
)
+
?
i,j,k,l
(?oi,j,k,l ? 1) ln?i,j,k,l + const
=
?
i,j,k,l
((
EH,U[ni,j,k,l] + (?oi,j,k,l ? 1)
)
? ln?i,j,k,l
)
+ const
(11)
4Note that priors over parameters for deterministic distribu-
tions (e.i., pi,?,and ?) are not necessary.
53
where ?i,j,k,l denotes ?(g, i)?(st, j)?(ht?1, k)
?(ut, l) and ni,j,k,l is the number of times where
g = i, st = j,ht?1 = k, and ut = l. This leads
to a product of Dirichlet distributions by taking the
exponential of both sides of the equation:
q?(?) =
?
i,j,k
Dir(?i,j,k|?i,j,k),
?i,j,k,l = ?0i,j,k,l + EH,U[ni,j,k,l]
(12)
To evaluate the quantity EH,U[ni,j,k,l], Equation 9
needs to be applied once again to obtain an op-
timal approximation of the posterior distribution
q?(H,U).
ln q?(H,U) = E?
[
ln p(g,S,H,U,O,?)
]
+ const
= E?
[?
t
ln p(ut|g, st,ht?1,?)
+ ln p(ht|ht?1,ut)
+ ln p(ot|ut)
]
+ const
=
?
t
(
E?
[
ln p(ut|g, st,ht?1,?)
]
+ ln p(ht|ht?1,ut)
+ ln p(ot|ut)
)
+ const
(13)
where E?
[
ln p(ut|g, st,ht?1,?)
] can be obtained
using Equation 12 and properties of the Dirichlet
distribution:
E?
[
ln p(ut|g, st,ht?1,?)
]
=
?
i,j,k,l
?i,j,k,lE?
[
ln?i,j,k,l
]
=
?
i,j,k,l
?i,j,k,l(?(?i,j,k,l)? ?(??i,j,k))
(14)
where ?(?) is the digamma function with ??i,j,k =?
l ?i,j,k,l. Because computing EH,U[ni,j,k,l] is
equivalent to summing each of the marginal poste-
rior probabilities q?(ht?1,ut) with the same con-
figuration of conditioning variables, this can be
done efficiently by using the junction tree algorithm.
Note that the expression on the right-hand side for
both q?(?) and q?(H,U) depends on expectations
computed with respect to the other factors. We
will therefore seek a consistent solution by cycling
through the factors and replacing each in turn with a
revised estimate.
3.3 Error Model
The purpose of the error model is to alter the user
action to reflect the prevalent speech recognition and
understanding errors. The error generation process
consists of three steps: the error model first gen-
erates an error template then fills it with erroneous
values, and finally attaches a confidence score.
Given a user action, the error model maps it into a
distorted form according to the probability distribu-
tion of the error template model ?:
T (u) ? p(T (u)|u) =
?
k,k?
??(u,k)?(T (u),k
?)
k,k? (15)
where T (?) is a random function that maps a pred-
icate of the user action to an error template, e.g.
T (Inform(Time)) ? Inform(Route:incorrect). To
learn the parameters, the hidden variable ut is sam-
pled using Equation 4 for each observation ot in the
training data and the value part of each observation
is replaced with a binary value representing its cor-
rectness with respect to the user goal. This results in
a set of complete data on which the maximum like-
lihood estimates of ? are learned.
With the error template provided, next, the error
model fills it with incorrect values if necessary fol-
lowing the distribution of the error value model ?
which is separately defined for each concept, other-
wise it will keep the correct value:
C(v) ? p(C(v)|v) =
?
k,k?
??(v,k)?(C(v),k?) (16)
where C(?) is a random function which maps a cor-
rect value to a confusable value, e.g. C(Forbes) ?
Forward. As with the error template model, the pa-
rameters of the error value model are also easily
trained on the dataset of all pairs of a user goal value
and the associated observed value. Because no er-
ror values can be observed for a given goal value, an
unconditional probability distribution is also trained
as a backoff.
Finally, the error model assigns a confidence
score by sampling the confidence score model ?
54
which is separately defined for each concept:
s ? p(s|c) =
?
k,k?
??(c,k)?(s,k?) (17)
where s denotes the confidence score and c repre-
sents the correctness of the value of the user action
which is previously determined by the error tem-
plate model. Since two decimal places are used to
describe the confidence score, the confidence score
model is represented with a discrete distribution.
This lends itself to trivial parameter learning similar
to other models by computing maximum likelihood
estimates on the set of observed confidence scores
conditioned on the correctness of the relevant val-
ues.
In sum, for example, having a user action
[Inform(Source:Forbes), Inform(Time:6 PM)] go
through the sequence of aforementioned models
possibly leads to [Inform(Source:Forward), In-
form(Route:6C)].
3.4 Termination Model
Few studies have been conducted to estimate the
probability that a dialog will terminate at a certain
turn in the user simulation. Most existing work
attempts to treat a termination initiated by a user
as one of the dialog actions in their user models.
These models usually have a limited dialog history
that they can use to determine the next user action.
This Markov assumption is well-suited to ordinary
dialog actions, each generally showing a correspon-
dence with previous dialog actions. It is not diffi-
cult, however, to see that more global contexts (e.g.,
cumulative number of incorrect confirmations) will
help lead a user to terminate a failed dialog. In ad-
dition, the termination action occurs only once at
the end of a dialog unlike the other actions. Thus,
we do not need to put the termination action into
the user model. In order to easily incorporate many
global features involving an entire dialog (Table 1)
into the termination model, the logistic regression
model is adapted. At every turn, before getting into
the user model, we randomly determine whether a
dialog will stop according to the posterior probabil-
ity of the termination model given the current dialog
context.
Feature Description
NT Number of turns
RIC Ratio of incorrect confirmations
RICW Ratio of incorrect confirmationswithin a window
RNONU Ratio of non-understanding
RNONUW Ratio of non-understandingwithin a window
ACS Averaged confidence score
ACSW Averaged confidence scorewithin a window
RCOP Ratio of cooperative turns
RCOPW Ratio of cooperative turnswithin a window
RRT C Ratio of relevant system turnsfor each concept
RRTW C Ratio of relevant system turnsfor each concept within a window
NV C Number of values appeared foreach concept
Table 1: A description of features used for a logistic
regression model to capture the termination probability.
The window size was set to 5 for this study.
4 Experimental Setup
4.1 Data
To verify the proposed method, three months of data
from the Let?s Go domain were split into two months
of training data and one month of test data. Also,
to take the error level into consideration, we classi-
fied the data into four groups according to the aver-
aged confidence score and used each group of data
to build a different error model for each error level.
For comparison purposes, simulated data was gen-
erated for both training and test data by feeding the
same context of each piece of data to the proposed
method. Due to the characteristics of the bus sched-
ule information domain, there are a number of cases
where no bus schedule is available, such as requests
for uncovered routes and places. Such cases were
excluded for clearer interpretation of the result, giv-
ing us the data sets described in Table 2.
4.2 Measures
To date, a variety of evaluation methods have been
proposed in the literature (Cuayahuitl et al, 2005;
Jung et al, 2009; Georgila et al, 2006; Pietquin and
55
Training data Test data
Number of dialogs 1,275 669
Number of turns 9,645 5,103
Table 2: A description of experimental data sets.
Hastie, 2011; Schatzmann et al, 2005; Williams,
2007a). Nevertheless, it remains difficult to find
a suitable set of evaluation measures to assess the
quality of the user simulation. We have chosen
to adopt a set of the most commonly used mea-
sures. Firstly, expected precision (EP), expected re-
call (ER) and F-Score offer a reliable method for
comparing real and simulated data even though it
is not possible to specify the levels that need to be
satisfied to conclude that the simulation is realistic.
These are computed by comparison of the simulated
and real user action for each turn in the corpus:
EP = 100 ? Number of identical actionsNumber of simulated actions (18)
ER = 100 ? Number of identical actionsNumber of real actions (19)
F-Score = 100 ? 2 ? EP ? EREP + ER (20)
Next, several descriptive statistics are employed to
show the closeness of the real and simulated data
in a statistical sense. The distribution of different
user action types, turn length and confidence score
can show constitutional similarity. It is still possible,
however, to be greatly different in their interdepen-
dence and cause quite different behavior at the dia-
log level even though there is a constitutional sim-
ilarity. Therefore, the dialog-level statistics such as
dialog completion rate and averaged dialog length
were also computed by running the user simulator
with the Let?s Go dialog system.
5 Results
As mentioned in Section 4.2, expected precision and
recall were measured. Whereas previous studies
only reported the scores computed in the predicate
level, i.e. speech act and concept, we also measured
the scores based on the output of the error template
model which is the predicate-level action with an
indicator of the correctness of the associated value
(Figure 1). The result (Table 3) shows a moderate
Training data Test data
Error Mark w/o w/ w/o w/
EP 58.13 45.12 54.44 41.86
ER 58.40 45.33 54.61 41.99
F-Score 58.27 45.22 54.52 41.93
Table 3: Expected precision, expected recall and F-Score
balance between agreement and variation which is
a very desirable characteristic of a user simulator
since a simulated user is expected not only to resem-
ble real data but also to cover diverse unseen behav-
ior to a reasonable extent. As a natural consequence
of the increased degree of freedom, the scores con-
sidering error marking are consistently lower. In ad-
dition, the results of test data are slightly lower than
those of training data, as expected, yet a suitable bal-
ance remains.
Next, the comparative distributions of different
actions between real and simulated data are pre-
sented for both training and test data (Figure 3).
The results are also based on the output of the er-
ror template model to further show how errors are
distributed over different actions. The distributions
of simulated data either from training or test data
show a close match to the corresponding real dis-
tributions. Interestingly, even though the error ratio
of the test data is noticeably different from that of
the training data, the proposed method is still able
to generate similar results. This means the vari-
ables and their conditional probabilities of the pro-
posed method were designed and estimated properly
enough to capture the tendency of user behavior with
respect to various dialog contexts. Moreover, the
comparison of the turn length distribution (Figure 4)
indicates that the simulated data successfully repli-
cated the real data for both training and test data.
The results of confidence score simulation are pre-
sented in Figure 55. For both training and test data,
the simulated confidence score displays forms that
are very similar to the real ones.
Finally, to confirm the resemblance on the dialog
level, the comparative results of dialog completion
rate and averaged dialog length are summarized in
Table 4. As shown in the dialog completion result,
the simulated user is a little harder than the real user
5Due to the space limitation, the detailed illustrations for
each action type are put in Appendix A.
56
Figure 3: A comparison of the distribution of different
actions between real and simulated data for both training
and test data
Figure 4: A comparison of the distribution of turn length
between real and simulated data for both training and test
data
to accomplish the purpose. Also, the variation of the
simulated data as far as turn length is concerned was
greater than that of the real data, although the aver-
aged lengths were similar to each other. This might
indicate the need to improve the termination model.
The proposed method for the termination model is
confined to incorporating only semantic-level fea-
tures but a variety of different features would, of
course, cause the end of a dialog, e.g. system de-
lay, acoustic features, spatial and temporal context,
weather and user groups.
6 Conclusion
In this paper, we presented a novel unsupervised ap-
proach for user simulation which is especially de-
sirable for real deployed systems. The proposed
Figure 5: A comparison of the distribution of confidence
score between real and simulated data for both training
and test data
Real Simulated
DCR (%) 59.68 55.04
ADL mean std. mean std.
Success 10.62 4.59 11.08 5.10
Fail 7.75 6.20 7.75 8.64
Total 9.46 5.48 9.50 7.12
Table 4: A comparison of dialog completion rate (DCR)
and averaged dialog length (ADL) which is presented ac-
cording to the dialog result.
method can cover the whole pipeline of user sim-
ulation on the semantic level without human inter-
vention. Also the quality of simulated data has been
demonstrated to be similar to the real data over a
number of commonly employed metrics. Although
the proposed method does not deal with simulat-
ing N-best ASR results, the extension to support
N-best results will be one of our future efforts, as
soon as the Let?s Go system uses N-best results.
Our future work also includes evaluation on improv-
ing and evaluating dialog strategies. Furthermore, it
would be scientifically more interesting to compare
the proposed method with a supervised approach us-
ing a corpus with semantic transcriptions. On the
other hand, as an interesting application, the pro-
posed user model could be exploited as a part of be-
lief tracking in a spoken dialog system since it also
considers a user action to be hidden.
57
Acknowledgments
We would like to thank Alan Black for helpful com-
ments and discussion. This work was supported by
the second Brain Korea 21 project.
References
C. Bishop, 2006. Pattern Recognition and Machine
Learning. Springer.
G. Chung, 2004. Developing a Flexible Spoken Dialog
System Using Simulation. In Proceedings of ACL.
H. Cuayahuitl, S. Renals, O. Lemon, H. Shimodaira,
2005. Humancomputer dialogue simulation using hid-
den Markov models. In Proceedings of ASRU.
W. Eckert, E. Levin, R. Pieraccini, 1997. User modeling
for spoken dialogue system evaluation. In Proceed-
ings of ASRU.
K. Georgila, J. Henderson, O. Lemon, 2006. User simu-
lation for spoken dialogue systems: Learning and eval-
uation. In Proceedings of Interspeech.
J. Henderson, O. Lemon, K. Georgila, 2008. Hybrid Re-
inforcement / Supervised Learning of Dialogue Poli-
cies from Fixed Datasets. Computational Linguistics,
34(4):487-511
S. Jung, C. Lee, K. Kim, M. Jeong, G. Lee, 2009.
Data-driven user simulation for automated evaluation
of spoken dialog systems. Computer Speech and Lan-
guage, 23(4):479?509.
S. Lauritzen and D. J. Spiegelhalter, 1988. Local Com-
putation and Probabilities on Graphical Structures and
their Applications to Expert Systems. Journal of
Royal Statistical Society, 50(2):157?224.
E. Levin, R. Pieraccini, W. Eckert, 2000. A stochastic
model of humanmachine interaction for learning di-
alogstrategies. IEEE Transactions on Speech and Au-
dio Processing, 8(1):11-23.
R. Lopez-Cozar, Z. Callejas, and M. McTear, 2006. Test-
ing the performance of spoken dialogue systems by
means of an articially simulated user. Articial Intel-
ligence Review, 26(4):291-323.
G. Parisi, 1988. Statistical Field Theory. Addison-
Wesley.
O. Pietquin, 2004. A Framework for Unsupervised
Learning of Dialogue Strategies. Ph.D. thesis, Faculty
of Engineering.
O. Pietquin and H. Hastie, 2011. A survey on metrics
for the evaluation of user simulations. The Knowledge
Engineering Review.
A. Raux, B. Langner, D. Bohus, A. W Black, and M.
Eskenazi, 2005. Let?s Go Public! Taking a Spoken
Dialog System to the Real World. In Proceedings of
Interspeech.
J. Schatzmann, K. Georgila, S. Young, 2005. Quantita-
tive evaluation of user simulation techniques for spo-
ken dialogue systems. In Proceedings of SIGdial.
J. Schatzmann, B. Thomson, K. Weilhammer, H. Ye, S.
Young, 2007. Agenda-based user simulation for boot-
strapping a POMDP dialogue system. In Proceedings
of HLT/NAACL.
J. Schatzmann, B. Thomson, S. Young, 2007. Error
simulation for training statistical dialogue systems. In
Proceedings of ASRU.
U. Syed and J. Williams, 2008. Using automatically
transcribed dialogs to learn user models in a spoken
dialog system. In Proceedings of ACL.
B. Thomson and S. Young, 2010. Bayesian update
of dialogue state: A POMDP framework for spoken
dialogue systems. Computer Speech & Language,
24(4):562-588.
J. Williams, P. Poupart, and S. Young, 2005. Factored
Partially Observable Markov Decision Processes for
Dialogue Management. In Proceedings of Knowledge
and Reasoning in Practical Dialogue Systems.
J. Williams, 2007. A Method for Evaluating and Com-
paring User Simulations: The Cramer-von Mises Di-
vergence. In Proceedings of ASRU.
J. Williams and S. Young, 2007. Partially observable
Markov decision processes for spoken dialog systems.
Computer Speech & Language, 21(2):393-422.
58
Appendices
Appendix A. Distribution of confidence score for each concept
Figure 6: A comparison of the distribution of confidence score between real and simulated data for the training data
Figure 7: A comparison of the distribution of confidence score between real and simulated data for the test data
59
Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 189?196,
Seoul, South Korea, 5-6 July 2012. c?2012 Association for Computational Linguistics
Exploiting Machine-Transcribed Dialog Corpus to Improve Multiple Dialog
States Tracking Methods
Sungjin Lee1,2 and Maxine Eskenazi1
1Language Technologies Institute, Carnegie Mellon University, Pittsburgh, Pennsylvania
2Computer Science and Engineering, Pohang University of Science and Technology, South Korea
{sungjin.lee, max}@cs.cmu.edu1, junion@postech.ac.kr2
Abstract
This paper proposes the use of unsuper-
vised approaches to improve components of
partition-based belief tracking systems. The
proposed method adopts a dynamic Bayesian
network to learn the user action model directly
from a machine-transcribed dialog corpus. It
also addresses confidence score calibration to
improve the observation model in a unsuper-
vised manner using dialog-level grounding in-
formation. To verify the effectiveness of the
proposed method, we applied it to the Let?s Go
domain (Raux et al, 2005). Overall system
performance for several comparative models
were measured. The results show that the pro-
posed method can learn an effective user ac-
tion model without human intervention. In
addition, the calibrated confidence score was
verified by demonstrating the positive influ-
ence on the user action model learning process
and on overall system performance.
1 Introduction
With present Automatic Speech Recognition (ASR)
and Spoken Language Understanding (SLU) errors,
it is impossible to directly observe the true user goal
and action. It is crucial, therefore, to efficiently infer
this true state from erroneous observations over mul-
tiple dialog turns. The Partially Observable Markov
Decision Process (POMDP) framework has offered
a well-founded theory for this purpose (Henderson
et al, 2008; Thomson and Young, 2010a; Williams
and Young, 2007; Young et al, 2010). Several
approximate methods have also emerged to tackle
the vast complexity of representing and maintaining
belief states, e.g., partition-based approaches (Ga-
sic and Young, 2011; Williams, 2010; Young et
al., 2010) and Bayesian network (BN)-based meth-
ods (Raux and Ma, 2011; Thomson and Young,
2010a). The partition-based approaches attempt to
group user goals into a small number of partitions
and split a partition only when a distinction is re-
quired by observations. This property endows it
with the high scalability that is suitable for fairly
complex domains. However, the parameter learn-
ing procedures for the partition-based methods is
still limited to hand-crafting or the use of a sim-
ple maximum likelihood estimation (Keizer et al,
2008; Roy et al, 2000; Thomson and Young, 2010a;
Williams, 2008). In contrast, several unsupervised
methods which do not require human transcription
and annotation have been recently proposed to learn
BN-based models (Jurcicek et al, 2010; Syed and
Williams, 2008; Thomson et al, 2010b). In this pa-
per we describe an unsupervised process that can be
applied to the partition-based methods. We adopt a
dynamic Bayesian network to learn the user action
model which defines the likelihood of user actions
for a given context. In addition, we propose a simple
confidence score calibration method to improve the
observation model which represents the probability
of an observation given the true user action.
This paper is structured as follows. Section 2 de-
scribes previous research and the novelty of our ap-
proach. Section 3 and Section 4 elaborate on our
proposed unsupervised approach. Section 5 explains
the experimental setup. Section 6 presents and dis-
cusses the results. Finally, Section 7 concludes with
a brief summary and suggestions for future research.
189
2 Background and Related Work
In order to reduce the complexity of the belief states
over the POMDP states, the following factorization
of the belief state has been commonly applied to the
belief update procedure (Williams et al, 2005):
b(gt,ut,ht)
? p(ot|ut)? ?? ?
observation model
?
ht?1
p(ht|ht?1,ut, st)? ?? ?
dialog history model
p(ut|gt, st,ht?1)? ?? ?
user action model
?
gt?1
p(gt|gt?1, st?1)? ?? ?
user goal model?
ut?1
b(gt?1,ut?1,ht?1)
(1)
where gt, st,ut,ht,ot represents the user goal, the
system action, the user action, the dialog history,
and the observed user action for each time slice, re-
spectively. The user goal model describes how the
user goal evolves. In the partition-based approaches,
this model is further approximated by assuming that
the user does not change their mind during the dia-
log (Young et al, 2010):
?
gt?1
p(gt|gt?1, st?1) = p(pt|pt?1) (2)
where pt is a partition from the current turn. The di-
alog history model indicates how the dialog history
changes and can be set deterministically by simple
discourse rules, for example:
p(ht = Informed|ht?1,ut, st) ={
1 if ht?1 = Informed or ut = Inform(?),
0 otherwise.
(3)
The user action model defines how likely user ac-
tions are. By employing partitions, this can be ap-
proximated by the bigram model of system and user
action at the predicate level, and the matching func-
tion (Keizer et al, 2008):
p(ut|gt, st,ht?1)
? p(T (ut)|T (st)) ? M(ut,pt, st)
(4)
where T (?) denotes the predicate of the action
and M(?) indicates whether or not the user action
matches the partition and system action. However,
it turned out that the bigram user action model did
not provide an additional gain over the improve-
ment achieved by the matching function according
to (Keizer et al, 2008). This might indicate that
it is necessary to incorporate more historical infor-
mation. To make use of historical information in
an unsupervised manner, the Expectation Maximiza-
tion algorithm was adopted to obtain maximum like-
lihood estimates (Syed and Williams, 2008). But
these methods still require a small amount of tran-
scribed data to learn the observation confusability,
and they suffer from overfitting as a general prop-
erty of maximum likelihood. To address this prob-
lem, we propose a Bayesian learning method, which
requires no transcribed data.
The observation model represents the probability
of an observation given the true user action. The
observation model is usually approximated with the
confidence score computed from the ASR and SLU
results:
p(ot|ut) ? p(ut|ot) (5)
It is therefore of vital importance that we obtain the
most accurate confidence score as possible. We pro-
pose an efficient method that can improve the confi-
dence score by calibrating it using grounding infor-
mation.
3 User Action Model
To learn the user action model, a dynamic Bayesian
network is adopted with several conditional inde-
pendence assumptions similar to Equation 1. This
gives rise to the graphical structure shown in Fig-
ure 1. As mentioned in Section 2, the user ac-
tion model deals with actions at the predicate level1.
This abstract-level handling enables the user action
model to employ exact inference algorithms such as
the junction tree algorithm (Lauritzen and Spiegel-
halter, 1988) for more efficient reasoning over the
graphical structure.
1To keep the notation uncluttered, we will omit T (?).
190
Figure 1: The graphical structure of the dynamic
Bayesian network for the user action model. The shaded
items are observable and the transparent ones are latent.
The joint distribution for this model is given by
p(S,H,U,O|?)
= p(h0|pi)
?
t
p(ut|st,ht?1,?)
? p(ht|ht?1,ut,?)p(ot|ut, ?)
(6)
where a capital letter stands for the set of
corresponding random variables, e.g., U =
{u1, . . . ,uN}, and ? = {pi,?,?, ?} denotes the
set of parameters governing the model2.
Unlike previous research which learns ? using
maximum likelihood estimation, we use a determin-
istic function that yields a fraction of an observed
confidence score in accordance with the degree of
agreement between ut and ot:
p(ot|ut) = CS(ot) ?
( |ot ? ut|
|ot ? ut|
)
+  (7)
where CS(?) returns the confidence score of the as-
sociated observation. As mentioned above, pi and
? are deterministically set by simple discourse rules
(Equation 3). This only leaves the user action model
? to be learned. In a Bayesian model, any unknown
parameter is given a prior distribution and is ab-
sorbed into the set of latent variables, thus it is not
feasible to directly evaluate the posterior distribution
of the latent variables and the expectations with re-
spect to this distribution. Therefore a determinis-
tic approximation, called mean field theory (Parisi,
1988), is applied.
In mean field theory, the family of posterior distri-
butions of the latent variables is assumed to be par-
titioned into disjoint groups:
q(Z) =
M?
i=1
qi(Zi) (8)
2Here, a uniform prior distribution is assigned on S
where Z = {z1, . . . , zN} denotes all latent variables
including parameters and Zi is a disjoint group.
Amongst all distributions q(Z) having the form of
Equation 8, we then seek the member of this family
for which the divergence from the true posterior dis-
tribution is minimized. To achieve this, the follow-
ing optimization with respect to each of the qi(Zi)
factors is to be performed in turn (Bishop, 2006):
ln q?j (Zj) = Ei 6=j
[
ln(X,Z)
]
+ const (9)
where X = {x1, . . . ,xN} denotes all observed vari-
ables and Ei 6=j means an expectation with respect to
the q distributions over all groups Zi for i 6= j.
Now we apply the mean field theory to the user
model. Before doing so, we need to introduce the
prior over the parameter ? which is a product of
Dirichlet distributions3.
p(?) =
?
k
Dir(?k|?0k)
=
?
k
C(?0k)
?
l
??
0
k?1
k,l
(10)
where k represents the joint configuration of all of
the parents and C(?0k) is the normalization constant
for the Dirichlet distribution. Note that for symme-
try we have chosen the same parameter ?0k for each
of the components.
Next we approximate the posterior distribution,
q(H,U,?) using a factorized form, q(H,U)q(?).
Then we first apply Equation 9 to find an expression
for the optimal factor q?(?):
3Note that priors over parameters for deterministic distribu-
tions (e.i., pi,?,and ?) are not necessary.
191
ln q?(?) = EH,U
[
ln p(S,H,U,O,?)
]
+ const
= EH,U
[?
t
ln p(ut|st,ht?1,?)
]
+ ln p(?) + const
=
?
t
?
i,j,k
(
EH,U
[
?i,j,k
]
ln?i,j,k
)
+
?
i,j,k
(?oi,j,k ? 1) ln?i,j,k + const
=
?
i,j,k
((
EH,U[ni,j,k] + (?oi,j,k ? 1)
)
? ln?i,j,k
)
+ const
(11)
where ?(?, ?) denotes Kronecker delta and ?i,j,k de-
notes ?(st, i)?(ht?1, j) ?(ut, k). ni,j,k is the num-
ber of times where , st = i,ht?1 = j, and ut = k.
This leads to a product of Dirichlet distributions by
taking the exponential of both sides of the equation:
q?(?) =
?
i,j
Dir(?i,j |?i,j),
?i,j,k = ?0i,j,k + EH,U[ni,j,k]
(12)
To evaluate the quantity EH,U[ni,j,k], Equation 9
needs to be applied once again to obtain an op-
timal approximation of the posterior distribution
q?(H,U).
ln q?(H,U) = E?
[
ln p(S,H,U,O,?)
]
+ const
= E?
[?
t
ln p(ut|st,ht?1,?)
+ ln p(ht|ht?1,ut)
+ ln p(ot|ut)
]
+ const
=
?
t
(
E?
[
ln p(ut|st,ht?1,?)
]
+ ln p(ht|ht?1,ut)
+ ln p(ot|ut)
)
+ const
(13)
where E?
[
ln p(ut|st,ht?1,?)
] can be obtained us-
ing Equation 12 and properties of the Dirichlet dis-
tribution:
E?
[
ln p(ut|st,ht?1,?)
]
=
?
i,j,k
?i,j,kE?
[
ln?i,j,k
]
=
?
i,j,k
?i,j,k(?(?i,j,k)? ?(??i,j))
(14)
where ?(?) is the digamma function with ??i,j =?
k ?i,j,k. Because computing EH,U[ni,j,k] is
equivalent to summing each of the marginal poste-
rior probabilities q?(ht?1,ut) with the same con-
figuration of conditioning variables, this can be
done efficiently by using the junction tree algorithm.
Note that the expression on the right-hand side for
both q?(?) and q?(H,U) depends on expectations
computed with respect to the other factors. We
will therefore seek a consistent solution by cycling
through the factors and replacing each in turn with a
revised estimate.
4 Confidence Score Calibration
As shown in Section 2, we can obtain a better obser-
vation model by improving confidence score accu-
racy. Since the confidence score is usually computed
using the ASR and SLU results, it can be enhanced
by adding dialog-level information. Basically, the
confidence score represents how likely it is that the
recognized input is correct. This means that a well-
calibrated confidence score should satisfy that prop-
erty such that:
p(ut = a|ot = a) '
?
k ?(uk, a)?(ok, a)?
k ?(ok, a)
(15)
However, the empirical distribution on the right side
of this equation often does not well match the con-
fidence score measure on the left side. If a large
corpus with highly accurate annotation was used, a
straightforward remedy for this problem would be to
construct a mapping function from the given confi-
dence score measure to the empirical distribution.
This leads us to propose an unsupervised method
that estimates the empirical distribution and con-
structs the mapping function which is fast enough
to run in real time. Note that we will not construct
192
Figure 2: Illustrations of confidence score calibration for the representative concepts in the Let?s Go domain
a mapping function for each instance, but rather
for each concept, since the former could cause se-
vere data sparseness. In order to estimate the em-
pirical distribution in an unsupervised manner, we
exploit grounding information4 as true labels. We
first parse dialog logs to look for the grounding in-
formation that the users have provided. Each time
we encounter grounding information that includes
the constraints used in the backend queries, this is
added to the list. If two actions contradict each other,
the later action overwrites the earlier one. Then,
for each observation in the data, we determine its
correctness by comparing it with the grounding in-
formation. Next, we gather two sets of confidence
scores with respect to correctness, on which we ap-
ply a Gaussian kernel-based density estimation. Af-
4Specifically, we used explicitly confirmed information by
the system for this study
ter that, we scale the two estimated densities by their
total number of elements to see how the ratio of cor-
rect ones over the sum of correct and incorrect ones
varies according to the confidence score. The ratio
computed above will be the calibrated score:
c? = dc(c)dc(c) + dinc(c)
(16)
where c? indicates the calibrated confidence score
and c is the input confidence score. dc(?) denotes
the scaled density for the correct set and dinc(?) is
the scaled density for the incorrect set.
Note that this approach tends to yield a more
conservative confidence score since correct user ac-
tions can exist, even though they may not match
the grounding information. Finally, in order to effi-
ciently obtain the calibrated score for a given confi-
dence score, we employ the sparse Bayesian regres-
sion (Tipping, 2001) with the Gaussian kernel. By
193
virtue of the sparse representation, we only need to
consider a few so-called relevance vectors to com-
pute the score:
y(x) =
?
xn?RV
wnk(x,xn) + b (17)
where RV denotes the set of relevance vectors,
|RV |  |{xn}|. k(?, ?) represents a kernel function
and b is a bias parameter. Figure 2 shows the afore-
mentioned process for several representative con-
cepts in the Let?s Go domain.
5 Experimental Setup
To verify the proposed method, three months of data
from the Let?s Go domain were used to train the
user action model and the observation model. The
training data consists of 2,718 dialogs and 23,044
turns in total. To evaluate the user action model,
we compared overall system performance with three
different configurations: 1) the uniform distribution,
2) the user action model without historical infor-
mation5 which is comparable to the bigram model
of (Keizer et al, 2008), 3) the user action model with
historical information included. For system perfor-
mance evaluation, we used a user simulator (Lee and
Eskenazi, 2012) which provides a large number of
dialogs with statistically similar conditions. Also,
the simulated user enables us to examine how per-
formance changes over a variety of error levels. This
simulated user supports four error levels and each
model was evaluated by generating 2,000 dialogs at
each error level. System performance was measured
in terms of average dialog success rate. A dialog is
considered to be successful if the system provides
the bus schedule information that satisfies the user
goal.
To measure the effectiveness of the calibration
method, we conducted two experiments. First, we
applied the calibration method to parameter learn-
ing for the user action model by using the calibrated
confidence score in Equation 7. We compared the
log-likelihood of two models, one with calibration
and the other without calibration. Second, we com-
pared overall system performance with four differ-
ent settings: 1) the user action model with histori-
5This model was constructed by marginalizing out the his-
torical variables.
cal information and the observation model with cal-
ibration, 2) the user action model with historical in-
formation and the observation model without cali-
bration, 3) the user action model without historical
information and the observation model with calibra-
tion, 4) the user action model without historical in-
formation and the observation model without cali-
bration.
6 Results
The effect of parameter learning of the user action
model on average dialog success rate is shown in
Figure 3. While, in the previous study, the bigram
model unexpectedly did not show a significant ef-
fect, our result here indicates that our comparable
model, i.e. the model with historical information ex-
cluded, significantly outperformed the baseline uni-
form model. The difference could be attributed to
the fact that the previous study did not take tran-
scription errors into consideration, whereas our ap-
proach handles the problem by treating the true user
action as hidden. However, we cannot directly com-
pare this result with the previous study since the tar-
get domains are different. The model with historical
information included also consistently surpassed the
uniform model. Interestingly, there is a noticeable
trend: the model without historical information per-
forms better as the error level increases. This result
may indicate that the simpler model is more robust
Figure 3: The effect of parameter learning of each user
action model on overall system performance. The error
bar represents standard error.
194
Figure 4: The effect of confidence score calibration on
the log-likelihood of the user action model during the
training process.
Figure 5: The effect of confidence score calibration for
the observation model on overall system performance.
The error bar shows standard error.
to error. Although average dialog success rates be-
came almost zero at error level four, this result is a
natural consequence of the fact that the majority of
the dialogs in this corpus are failed dialogs.
Figure 4 shows the effect of confidence score
calibration on the log-likelihood of the user action
model during the training process. To take into ac-
count the fact that different confidence scores result
in different log-likelihoods regardless of the qual-
ity of the confidence score, we shifted both log-
likelihoods to zero at the beginning. This modifica-
tion more clearly shows how the quality of the confi-
dence score influences the log-likelihood maximiza-
tion process. The result shows that the calibrated
confidence score gives greater log-likelihood gains,
which implies that the user action model can better
describe the distribution of the data.
The effect of confidence score calibration for the
observation model on average dialog success rate is
presented in Figure 5. For both the user action model
with historical information included and excluded,
the application of the confidence score calibration
consistently improved overall system performance.
This result implies the possibility of automatically
improving confidence scores in a modularized man-
ner without introducing a dependence on the under-
lying methods of ASR and SLU.
7 Conclusion
In this paper, we have presented novel unsupervised
approaches for learning the user action model and
improving the observation model that constitute the
partition-based belief tracking method. Our pro-
posed method can learn a user action model directly
from a machine-transcribed spoken dialog corpus.
The enhanced system performance shows the effec-
tiveness of the learned model in spite of the lack of
human intervention. Also, we have addressed con-
fidence score calibration in a unsupervised fashion
using dialog-level grounding information. The pro-
posed method was verified by showing the positive
influence on the user action model learning process
and the overall system performance evaluation. This
method may take us a step closer to being able to
automatically update our models while the system is
live. Although the proposed method does not deal
with N-best ASR results, the extension to support
N-best results will be one of our future directions,
as soon as the Let?s Go system uses N-best ASR re-
sults.
Acknowledgments
This work was supported by the second Brain Korea
21 project.
References
C. Bishop, 2006. Pattern Recognition and Machine
Learning. Springer.
195
M. Gasic and S. Young, 2011. Effective handling
of dialogue state in the hidden information state
POMDP-based dialogue manager. ACM Transactions
on Speech and Language Processing, 7(3).
J. Henderson, O. Lemon, K. Georgila, 2008. Hybrid Re-
inforcement / Supervised Learning of Dialogue Poli-
cies from Fixed Datasets. Computational Linguistics,
34(4):487-511.
F. Jurcicek, B. Thomson and S. Young, 2011. Natu-
ral Actor and Belief Critic: Reinforcement algorithm
for learning parameters of dialogue systems modelled
as POMDPs. ACM Transactions on Speech and Lan-
guage Processing, 7(3).
S. Keizer, M. Gasic, F. Mairesse, B. Thomson, K. Yu, S.
Young, 2008. Modelling User Behaviour in the HIS-
POMDP Dialogue Manager. In Proceedings of SLT.
S. Lauritzen and D. J. Spiegelhalter, 1988. Local Com-
putation and Probabilities on Graphical Structures and
their Applications to Expert Systems. Journal of
Royal Statistical Society, 50(2):157?224.
S. Lee and M. Eskenazi, 2012. An Unsuper-
vised Approach to User Simulation: toward Self-
Improving Dialog Systems. In Proceedings of SIG-
DIAL. http://infinitive.lti.cs.cmu.edu:9090.
G. Parisi, 1988. Statistical Field Theory. Addison-
Wesley.
A. Raux, B. Langner, D. Bohus, A. W Black, and M.
Eskenazi, 2005. Let?s Go Public! Taking a Spoken
Dialog System to the Real World. In Proceedings of
Interspeech.
A. Raux and Y. Ma, 2011. Efficient Probabilistic Track-
ing of User Goal and Dialog History for Spoken Dia-
log Systems. In Proceedings of Interspeech.
N. Roy, J. Pineau, and S. Thrun, 2000. Spoken dia-
logue management using probabilistic reasoning. In
Proceedings of ACL.
U. Syed and J. Williams, 2008. Using automatically
transcribed dialogs to learn user models in a spoken
dialog system. In Proceedings of ACL.
B. Thomson and S. Young, 2010. Bayesian update
of dialogue state: A POMDP framework for spoken
dialogue systems. Computer Speech & Language,
24(4):562-588.
B. Thomson, F. Jurccek, M. Gasic, S. Keizer, F. Mairesse,
K. Yu, S. Young, 2010. Parameter learning for
POMDP spoken dialogue models. In Proceedings of
SLT.
M. Tipping, 2001. Sparse Bayesian Learning and
the Relevance Vector Machine. Journal of Machine
Learning Research, 1:211?244.
J. Williams, P. Poupart, and S. Young, 2005. Factored
Partially Observable Markov Decision Processes for
Dialogue Management. In Proceedings of Knowledge
and Reasoning in Practical Dialogue Systems.
J. Williams and S. Young, 2007. Partially observable
Markov decision processes for spoken dialog systems.
Computer Speech & Language, 21(2):393-422.
J. Williams, 2008. Exploiting the ASR N-best by track-
ing multiple dialog state hypotheses. In Proceedings
of Interspeech.
J. Williams, 2010. Incremental partition recombination
for efficient tracking of multiple dialog states. In Pro-
ceedings of ICASSP.
S. Young, M. Gasic, S. Keizer, F. Mairesse, J. Schatz-
mann, B. Thomson and K. Yu, 2010. The Hidden
Information State Model: a practical framework for
POMDP-based spoken dialogue management. Com-
puter Speech and Language, 24(2):150?174.
196
NAACL-HLT 2012 Workshop on Future directions and needs in the Spoken Dialog Community: Tools and Data, pages 19?20,
Montre?al, Canada, June 7, 2012. c?2012 Association for Computational Linguistics
!"#"$%&'($%)#(*+,&(+&-.*/%+&'(01*2&-3,#%4,5&&
6&7*44"+(#3&*8&9*,,(:(1(#(%,&
!
610+&;&<10)/&0+=&>0?(+%&@,/%+0A(&&&
"#$%&#%'!(')*$+,+%-'.!/$.0-0&0'1!!
2#3$'%-'!4',,+$!5$-6'3.-071!8-00.9&3%*1!8:1!5;:!
!"#$%&"'()*+,*&-,./-0
!
!
6:,#$0)#&
:!.<+='$!>-#,+%!.7.0'?!)+$.-.0.!+@!#!$&?A
9'3!+@!$+$A03-6-#,,7!-$0'3#)0-$%!)+?<+$'$0.B!
/$!+3>'3!0+!#,,+C!$'C!.0&>'$0.1!3'.'#3)*'3.!
#$>! >'6',+<'3.! 0+! ?'#$-$%@&,,7! #$>! 3',#A
0-6',7! 3#<->,7! '$0'3! 0*'! @-',>! -0! -.! )3-0-)#,!
0*#01!>'.<-0'!0*'-3!)+?<,'D-071!0*'!3'.+&3)'.!
9'! #))'..-9,'! #$>! '#.7! 0+! &.'B! E6'37+$'!
.*+&,>! 9'! #9,'! 0+! .0#30! 9&-,>-$%! $'C! 0')*A
$+,+%-'.! C-0*+&0! .<'$>-$%! #! .-%$-@-)#$0!
#?+&$0! +@! 0-?'! 3'A-$6'$0-$%! 0*'! C*'',B!!
(*'3'!#3'!@+&3!,'6',.!+@!.&<<+30!0*#0!C'!9'A
,-'6'!$'C!'$03#$0.!.*+&,>!*#6'B!!FG!:!@,'D-A
9,'!+<'$! .+&3)'! .7.0'?! 0*#0! 3&$.!+$!?#$7!
>-@@'3'$0! +<'3#0-$%! .7.0'?.1! -.! C',,! >+)&A
?'$0'>!#$>!.&<<+30.!9+0*!.-?<,'!#$>!)+?A
<,'D! >-#,+%! .7.0'?.B! ! HG! "+%.! #$>! .<'')*!
@-,'.! @3+?! #! ,#3%'! $&?9'3! +@! >-#,+%.! 0*#0!
'$#9,'! #$#,7.-.! #$>! 03#-$-$%! +@! $'C! .7.A
0'?.! #$>! 0')*$-I&'.B! JG! :$! #)0&#,! .'0! +@!
3'#,! &.'3.! 0*#0! .<'#=! 0+! 0*'! .7.0'?! +$! #!
3'%&,#3! 9#.-.B! KG!(*'! #9-,-07! 0+! 3&$! .0&>-'.!
+$!)+?<,'0'!3'#,!&.'3!<,#0@+3?.B!
B! <0)/2$*"+=&
(*'! %+#,!+@! 0*'!L-#,+%!M'.'#3)*!2'$0'3! NL-#,M2G!
*#.!9''$! 0+!<3+6->'! 0*'! .<+='$!>-#,+%!)+??&$-07!
C-0*!0*3''!,'6',.!+@!.&<<+30!-$!0*'!@+3?!+@!0++,.!#$>!
>#0#! @+3! .<+='$! >-#,+%! .7.0'?.O! +<'$! .+&3)'! .+@0A
C#3'P! ,+%.! #$>! .<'')*! >#0#! @3+?! 3'#,! >-#,+%.1! #!
)+??&$-07!+@!3'#,!&.'3.!0*#0!&.'!#!.7.0'?!3'%&,#3,7!
+$!3'#,1!&.'@&,!<,#0@+3?.!+$!C*-)*!3'.'#3)*'3.!)#$!
3&$! .0&>-'.B! /$! 0*-.! .*+30! <#<'3! C'! >'.)3-9'! 0*'.'!
@+&3! ','?'$0.! 0*#0! +&3! 2'$0'3! *#.! '$>'#6+3'>! 0+!
<3+6->'B!"++=-$%!0+!0*'!@&0&3'!C'! ,++=!0+!0*'!.<+A
='$! >-#,+%! )+??&$-07! 0+! )+$03-9&0'! +0*'3! <,#0A
@+3?.!0+!+&3.!0+!%-6'!$'C)+?'3.!0+!0*'!@-',>!#!3-)*!
.'0!+@!'D<'3-?'$0#,!<,#0@+3?.!+$!C*-)*!0+!,'#3$!0*'!
3+<'.B!!
!
!.%+&,*"$)%&,.*/%+&=(01*2&,*8#C0$%O!Q'!#,3'#>7!
<3+6->'1! #.! R<'$! ;+&3)'! .+@0C#3'1! 0*'! 245!
R,7?<&.! ;<+='$!L-#,+%! ;7.0'?! 0*#0! +@@'3.!:;M1!
((;1!#!L-#,+%!4#$#%'3!#$>!+0*'3!)+?<+$'$0.!0*#0!
#,,+C!>'6',+<'3.!0+!9&-,>!9+0*!.-?<,'!#$>!)+?<,'D!
>-#,+%! .7.0'?.B! !Q*-,'! 0*-.! #3)*-0')0&3'! *#.! 9''$!
&.'>! -$! ?#$7! .7.0'?.! N.+?'! +@! 0*'?! #3'O! ('#?A
(#,=! ST#33-.! '0! #,! HUUKV1! M#6'$2#,'$>#3! S;0'$A
)*-=+6#!'0!#,B!HUUWV1!2+$X&'.0!SY+*&.!'0!#,B!HUUWV!
#$>! "'0Z.! [+! SM#&D! '0! #,B! HUU\VG1! -0! $''>.! 0+! 9'!
#))+?<#$-'>!97!?+3'!.&<<+30! -$! 0*'!@+3?!+@!9+0*!
>+)&?'$0#0-+$!#$>!@,'D-9-,-07B!/0!.*+&,>!#,.+!$+0!9'!
0*'! +$,7! <,#0@+3?! 0*#0! -.! #6#-,#9,'! 0+! 0*'! )+??&A
$-07! 0+! 3&$! .0&>-'.B! (*'3'! #3'! $'C! .0&>'$0.1!
3'.'#3)*'3.!#$>!>'6',+<'3.!C*+!C#$0!0+!*+$'!0*'-3!
.=-,,.!97!#>#<0-$%!#!>-#,+%!#3)*-0')0&3'!#$>!3&$$-$%!
-0!+$!#!3'#,!&.'3!<,#0@+3?B!/$!+3>'3!0+!?#='!-0!'#.-'3!
@+3!0*'.'!$'C)+?'3.!0+!9&-,>!>-#,+%!.7.0'?.!-$!0*'!
@+3?! +@! .*+30! *+?'C+3=! #..-%$?'$0.! N<'3*#<.! -$!
FAH!C''=.G1! @+3! #! 3'%&,#3! ),#..1!R,7?<&.!?&.0! 9'!
?+3'!@,'D-9,'1!#$>!'#.-'3!0+!&$>'3.0#$>!#$>!?#.0'3B!
Q-0*! 0*'!+<'$!.+&3)'!)+3'!.7.0'?!0*#0!*#.!
#,3'#>7! 9''$! 3','#.'>1!C'! <,#$! 0+! #>>! 6-30&#,!?#A
)*-$'.! 0*#0! *#6'! #,,! +@! 0*'! )+?<+$'$0.! <3'A
-$.0#,,'>1!#.!>+$'!-$!#$+0*'3!#3'#!97!S(+=&>#!'0!#,!
HUFHVB! (*-.! C-,,! ?#='! -0! '#.-'3! @+3! $'C)+?'3.! 0+!
.0#30!C3-0-$%! #$>!?+>-@7-$%!>-#,+%! .7.0'?.! -??'A
>-#0',7! 3#0*'3! 0*#$! .<'$>-$%! 0-?'! -$.0#,,-$%! 9,#)=!
9+D! .+@0C#3'B! (*-.! -?<,-'.! 0*#0! +&3! 'D-.0-$%!Q-$A
>+C.!.&<<+30!?&.0!9'!'D0'$>'>!0+!#,.+!)+6'3!"-$&D!
#$>!4#)!R;]B!!!
!
D*2& =0#0& 8$*4& =(01*2,O! ;+?'! +@! 0*'! .-%$-@-)#$01!
'D)-0-$%! #>6#$)'.! 0*#0! *#6'! 3')'$0,7! 9''$! .''$! -$!
0*'! 3'#,?! +@! .<+='$! >-#,+%! .7.0'?.! &.'! .0#0-.0-)#,!
?+>',-$%B! (*-.! -?<,-'.! 0*'! #)&0'! $''>! @+3! >#0#1!
#9+6'!#,,! !."10/"2"1! 0+! 03#-$! 0*'!?+>',.B!(*'!<,#0A
@+3?.!0*#0!<3+6->'!0*#0!>#0#!0+!#!)+??&$-07!.*+&,>!
@+,,+C!#!.0#$>#3>-^'>!@+3?#0! -$! 0*'!.#?'!C#7!0*#0!
.<'')*!@-,'.!*#6'!9')+?'!.0#$>#3>-^'>B! !"+%!L#0#!
)#$! 9'! &.'>! @+3! +@@,-$'! #$#,7.-.! 0*#01! -$! 0&3$1! )#$!
19
#@@+3>! >''<'3! @-3.0! *#$>! -$.-%*0! -$0+! *+C! .<+='$!
>-#,+%!.7.0'?.B!
!
6&)*44"+(#3&*8&$%01&",%$,&0+=&$%01&.10#8*$4,&#*&
$"+&,#"=(%,O!!C'!*#6'!.''$!S_+&$%1!HUFUV!0*#0!-0!-.!
$+! ,+$%'3! 3'#.+$#9,'! 0+! 0'.0! #! *7<+0*'.-.! #9+&0!
.<+='$!>-#,+%!C-0*!#!.?#,,!$&?9'3!+@!<#->!<#30-)-A
<#$0.B!!E$>!&.'3.!?&.0!9'!3'#,O!0*'7!*#6'!.+?'!-$A
0'3'.0! -$! 0*'!+&0)+?'! +@! 0*'! 0#.=! #0! *#$>!#$>! 0*'7!
#3'!$+0!&.-$%! 0*'! .7.0'?! `&.0! 9')#&.'! 0*'7a3'!<#->!
#$>b+3!)+,,')0-$%!'6#,&#0-+$!>#0#B!!(*-.!%+#,!-.!>-@A
@-)&,0B! T+C'6'31! C'! #0! L-#,M2!C#$0! 0+! <3+6->'! #!
)'$03#,-^'>!?')*#$-.?!0+!%-6'!0*'!$'C)+?'3.!N#$>!
#,3'#>7!'.0#9,-.*'>!3'.'#3)*'3.!#.!C',,G!#))'..!0+!#!
%3+&<!+@!<,#0@+3?.!C-0*! 3'#,!&.'3.B!(*'3'!?&.0!9'!
0*'!<+..-9-,-07!+@!+90#-$-$%!#!0#$%-9,'!9'$'@-0!@3+?!
0*'.'! 3'#,! <,#0@+3?.! #$>! 0*'! 3'.'#3)*! )+??&$-07!
.*+&,>!9'!C-,,-$%!0+!+<'$!0*'-3!.7.0'?.!0+!+0*'3.!.+!
0*#0! 0*'7!)#$! 0'.0! 0*'-3! ->'#.! -$! #! 3'#,-.0-)! )+$0'D01!
C-0*!#!.-%$-@-)#$0!$&?9'3!+@!3'#,!)#,,'3.B!!!
!
R&3! )&33'$0! '@@+30.! *#6'! +@0'$! )'$0'3'>! +$! ),#..-)!
0','<*+$'A9#.'>! -$@+3?#0-+$! %-6-$%! .7.0'?.B! [+A
-$%!@+3C#3>!-0!-.!-?<+30#$0!0+!0#='!#!C->'3!6-'C!+@!
0*'! 07<'.! +@! .<+='$! >-#,+%! 3'.'#3)*! C'! )#$! #>A
>3'..B! !(*&.!C'! #3'! #,.+! -$0'3'.0'>! -$! .&<<+30-$%O!
?&,0-?+>#,! -$0'3#)0-+$1! *&?#$A3+9+0! -$0'3#)0-+$1!
?&,0-A<#307!)+??&$-)#0-+$!#$>!'6'$!0#.=.!C-0*!$+!
),'#3!>'@-$-0-+$!+@! 0#.=! )+?<,'0-+$! N'B%B! )+$6'3.#A
0-+$#,!9#$0'3GB!!Q*#0!-.!-?<+30#$0!-.!$+0!<3+?+0-$%!
+$'!07<'!+@!3'.'#3)*!?+3'!0*#$!#$+0*'3B!/0!-.!?#=A
-$%!?#$7!>-@@'3'$0!3'#,A&.'3!<,#0@+3?.!#6#-,#9,'!0+!
0*'!)+??&$-07!#0!,#3%'B!
245Z.!L-#,M2!<3+<+.'.! 0+!#)0!#.!#!),'#3A
-$%! *+&.'! @+3! .+@0C#3'! N+&3! +C$! #$>! 0*#0! +@! #$7!
+0*'3.G1! >#0#! N9+0*! .<'')*! #$>! ,+%@-,'.G1! #$>! 3&$A
0-?'!3'#,!#<<,-)#0-+$b3'#,!&.'3!<,#0@+3?.! 0*#0!%-6'.!
0*'! )+??&$-07! #! )'$03#,! <,#)'! 0+! @-$>! #! <,#0@+3?!
0*#0!)+33'.<+$>.!0+!0*'-3!$''>.1!0+!)+$$')0!0*'?!0+!
0*'!>'6',+<'3.!+@!0*#0!<,#0@+3?1!#$>!0+!*',<!>-.03-9A
&0'! 0*'! >#0#! N.<'')*! #$>! ,+%@-,'.G! )+?-$%! @3+?!
0*'-3!&.'!+@!-0B!!!
(*'.'!0*3''!#)0-+$.!C-,,!9&-,>!)+??&$-0-'.!
+@!$'C!3'.'#3)*'3.!#$>!>'6',+<'3.!C*+1!@3+?!0*'-3!
&.'! +@! 0*-.! <,'0*+3#! +@! <,#0@+3?.! C-,,! '$3-)*! 0*'!
,#00'3!C-0*!C*#0! 0*'7!*#6'! ,'#3$'>!#$>!C-,,! '$3-)*!
+&3!)+??&$-07!C-0*!0*'-3!<3'.'$)'B!!
Q'! '$6-.#%'! 0*'! @+,,+C-$%! .)'$#3-+B! :!
.0&>'$0! *#.! #! .*+30! #..-%$?'$0! 0+! ?#='! .+?'!
)*#$%'! 0+! +$'! +@! 0*'! 9#.-)! #3)*-0')0&3'.! 0*#0! *#.!
9''$!?#>'!#6#-,#9,'!97!L-#,M2B!Q*'$!0*'!#..-%$A
?'$0! -.! @-$-.*'>1! 0*'7! ,-$=! 0*'-3! .7.0'?! 0+! #! <,#0A
@+3?! 0*#0! 0*'7! @+&$>! 0*3+&%*! +&3! )'$03#,! ,-.0-$%B!
M'#,! &.'3.! )#,,! 0*#0! <,#0@+3?! N*'3'1! +&3! .0&>'$0Z.!
.7.0'?G!C*'$!0*'7!$''>!C*#0!-.!9'-$%!+@@'3'>!N-$A
@+3?#0-+$!+$!#!%++>!6'%'0#3-#$!3'.0#&3#$0!-$!2#?A
93->%'1!C*'$! 0*'!$'D0!9&.! 0+! 0*'!#-3<+30! -.!)+?-$%!
-$!8-00.9&3%*1!#!>-.)&..-+$!+@!$'C!0*-$%.!0+!.''!-$!#!
?&.'&?1!'0)GB!(*'!.0&>'$0!)#$!0*'$!#))'..!0*'!3'#,!
&.'3! >#0#! 0*#0! *#.! 9''$! )+,,')0'>! C*-,'! 0*'-3! 6'3A
.-+$! +@! 0*'! .7.0'?!C#.! 3&$$-$%B! 8'3*#<.! -$! #! @+,A
,+C-$%! #..-%$?'$01! -@! 0*'7! <3+6->'>! 0C+! 6'3.-+$.!
+@! 0*'! .7.0'?1! 0*'7! )#$! @-$>! +&01! @3+?! #$#,7^-$%!
0*'!>#0#1!C*-)*!)+$>-0-+$!C+3='>!9'.0B!/@!0*'7!<3+A
6->'>! +$'! )+$>-0-+$1! #$>! +0*'3! .0&>'$0.! <3+6->'>!
+0*'3! +$'.1! 0*'$! 0*'7! )#$! )+?<#3'! 0*'-3! 3'.&,0.! 0+!
0*+.'!+@!0*'!+0*'3!.0&>'$0.B!!
:! )+$.0#$0,7! #6#-,#9,'! 3'.+&3)'1! 0*'! )+?A
<'0-0-+$! 9'0C''$! 6'3.-+$.! +@! #! .7.0'?! >+'.! $+0!
*#6'!0+!9'!*',>!#0!#!0-?'!0*#0!?#7!9'!-$)+$6'$-'$0!
@+3!.+?'B!/0!)#$!9'!#$!+$%+-$%!'6'$0!0*#0!3'.'#3)*A
'3.! )#$! <#30-)-<#0'! -$! C*'$! -0! -.! )+$6'$-'$0! @+3!
0*'?!0+!>+!.+B!!
&
E%8%$%+)%,&
!
T#33-.1!(B1!Y#$'3`''1!;B1!M&>$-)=71!:B1!;-.+$1!cB1!Y+>-$'1!
dB1! #$>! Y,#)=1! :B! NHUUKG! e:!M'.'#3)*! 8,#0@+3?! @+3!
4&,0-A:%'$0! L-#,+%&'! L7$#?-).f1! /EEE! Q+3=.*+<!
+$!M+9+0-).!#$>!T&?#$!/$0'3#)0-6'!2+??&$-)#0-+$B!!
;0'$)*-=+6#1! ;B1! 4&)*#1! YB1! T+@@?#$1! ;B1! ;0'$01! :B!
NHUUWG1!!fM#6'$2#,'$>#3O!:!4&,0-?+>#,!L-#,+%!;7.A
0'?! @+3! 4#$#%-$%! #! 8'3.+$#,! 2#,'$>#3fB! T"(A
g::2"!HUUWB!
Y+*&.1!YB1![3#&1!;1!T&%%-$.AL#$'.1!LB1!d'3-1!hB1!:$&A
?#)*-<#,,-1! [B1! d&?#31! MB1! M#&D1! :B! i! (+?=+1! ;B!
NHUUWG! f2+$X&'.0O! #$! R<'$A;+&3)'! L-#,+%! ;7.0'?!
@+3!2+$@'3'$)'.f1!T"(Ag::2"!HUUWB!!
M#&D1!:B1!!"#$%$'31!YB1!Y+*&.1!LB1!Y,#)=1!:B1!#$>!ED='A
$#^-1!4B! NHUUjG! e"'0Z.![+!8&9,-)k!(#=-$%!#!;<+='$!
L-#,+%!;7.0'?!0+!0*'!M'#,!Q+3,>Bf!/$0'3.<'')*!HUUjB!!
(+=&>#1!d1!"''1!:1!_#?#%-.*-1!cB!'0!#,B!NHUFHG!e;<+='$!
L-#,+%! .7.0'?!l3#?'C+3=! 9#.'>! +$!5.'3!['$'3#0'>!
2+$0'$0f1!g#%+7#! /$.0-0&0'! +@!(')*$+,+%71! #$>!5$-A
6'3.-07!+@!E>-$9&3%*1!@&$>'>!&$>'3!c;(!2ME;(!83+A
%3#?B!!!
_+&$%1! ;B! e;0-,,! (#,=-$%! 0+! 4#)*-$'.! N2+%$-0-6',7!
;<'#=-$%G1! d'7$+0'! /$0'3.<'')*! HUFU1!4#=&*#3-! c#A
<#$B!
20
Proceedings of the 2th Workshop of Natural Language Processing for Improving Textual Accessibility (NLP4ITA), pages 20?28,
Atlanta, Georgia, 14 June 2013. c?2013 Association for Computational Linguistics
Tools for non-native readers: the case for translation and simplification 
 
 
 Maxine Eskenazi Yibin Lin  
 Language Technologies Institute  Language Technologies Institute 
 Carnegie Mellon University  Carnegie Mellon University 
 Pittsburgh PA 15213  Pittsburgh PA 15213 
max@cmu.edu   yibinl@cs.cmu.edu 
 
Oscar Saz  
 Language Technologies Institute 
 Carnegie Mellon University 
 Pittsburgh PA 15213 
 osaz@cs.cmu.edu  
 
 
 
Abstract 
One of the populations that often needs some 
form of help to read everyday documents is 
non-native speakers. This paper discusses aid 
at the word and word string levels and focuses 
on the possibility of using translation and 
simplification. Seen from the perspective of 
the non-native as an ever-learning reader, we 
show how translation may be of more harm 
than help in understanding and retaining the 
meaning of a word while simplification holds 
promise. We conclude that if reading every-
day documents can be considered as a learn-
ing activity as well as a practical necessity, 
then our study reinforces the arguments that 
defend the use of simplification to make docu-
ments that non-natives need to read more ac-
cessible. 
1 Introduction 
 There are many tools that natural language 
processing (NLP) can offer disadvantaged readers 
to aid them in understanding a document. Readers 
may be at a disadvantage due to poor sight, to cog-
nitive disabilities, or simply to reading in a lan-
guage other than their native one (L1). This paper 
addresses that last case. For non-native readers, 
there are a number of aids that could be made 
available to them. Some aids help on the word 
level, assuming that the understanding of a specific 
word is what is impeding comprehension. Others 
address a more global level, presuming that the 
understanding blockage is due lack of comprehen-
sion of the meaning of a group of words. Our work 
addresses learning English vocabulary, for which 
we have conducted studies on both word-level and 
higher-level aids. We argue that our findings can 
inform what can be done to make documents more 
understandable in general for non-natives. 
 In the past, we have studied the effect of 
aids such as ordered definitions (Dela Rosa and 
Eskenazi, 2011) and synthesized speech (Dela 
Rosa et al, 2010) on learning vocabulary from web 
documents. These aids have been aimed at the 
word level and have been shown to help learning. 
We explored the wider context around an unknown 
word in an effort to give the non-native reader an 
understanding of the several-word context around 
an unknown word in order to help understanding of 
the meaning of the text.  
 Reading documents to learn a language is a 
very different activity from reading an everyday 
document (like a rental agreement) out of neces-
sity. Yet we find that there are similarities between 
the two activities. We believe that, unlike for some 
other categories of disadvantaged readers, each 
document that a non-native reads is a learning 
moment and that they learn the target language 
more with each encounter. These incremental addi-
tions to the readers? knowledge enable them to be 
increasingly capable of tackling future unknown 
documents. It also reflects on the manner with 
20
which readers tackle a document since some un-
derstanding of the words has to take place in order 
for the document to be understood. We believe that 
these similarities warrant using learning findings to 
guide the choice of NLP tools used in document 
processing for non-native readers. The learning 
environment is used in this paper to measure 
document understanding.  
2 Background  
 Using learning as a means of estimating 
the usefulness of NLP techniques in making texts 
more accessible, we can examine the positions that 
the learning community has taken on the educa-
tional value of several of these techniques. 
 Translation (the use of L1 in second lan-
guage (L2) vocabulary acquisition) is the area in 
which we find the greatest controversy. Models of 
L2 lexical acquisition represent acquisition of new 
L2 words as an assimilation through an L1 lemma 
that is generalized and applied to concepts in L2 
(Jiang, 2000; Kroll and Sunderman, 2003). Exces-
sive use of L1 is believed to reduce L2 fluency and 
to fossilize errors. Context, dictionary definitions 
and examples of other sentences in which a word 
could be used are commonly considered to be the 
most effective tools since students can interiorize 
the concept of the new word without reliance on 
L1. This implies that the use of such techniques 
can lead to better learning and improved fluency 
than direct use of L1 translation. This claim has 
been challenged by Grace (1998), showing that 
that when translation is provided, there are higher 
scores on vocabulary tests both in the short-term 
and long-term use of the new words. Prince (1996) 
also claimed that the more proficient students 
benefit more from translation on short-term lexical 
recall tasks, since it is easier for them to get rid of 
the L1 scaffolding. These studies and others have 
been hampered by the ability to accurately measure 
the extent of the subjects? use of translation. The 
REAP software described below has afforded a 
more precise estimate of use and of retention of 
vocabulary items. 
 Simplification has had more widespread 
acceptance. Simplified texts have often been pro-
vided to language learners either along with the 
original text or alone (Burstein et al 2007, Peter-
sen and Ostendorf, 2007). These texts have been 
used as reading comprehension exercises or text-
book reading materials (Crossley, et al 2007). Ac-
cording to Oh (2008), simplification typically uses 
shorter sentences, simpler grammar and controlled 
vocabulary. The use of simplified texts has been 
shown to significantly help students? reading com-
prehension (Yano, et al 1994, Oh 2008). However, 
there has not been any research specifically about 
whether reading the simplified texts, rather than 
the original ones, will affect the students? vocabu-
lary acquisition. There are a few disadvantages 
related to simplifying texts for ESL students. Yano 
et al (1994) note that simplified texts may appear 
unnatural, giving them a lack of flow, thus making 
them difficult to read. They may also lack the 
complex grammar structures that commonly exist 
in the real world (that students should be exposed 
to). The simplified texts used in these studies were 
created by hand and are usually written with the 
express intention of featuring certain vocabulary 
and/or syntactic elements for the purpose of being 
used by a non-native learner. 
 To address the link between vocabulary 
and comprehension of a text, the literature often 
reveals mastery of vocabulary as the key. Perfetti 
(2010) emphasized the vocabulary-comprehension 
link. Increased vocabulary has been shown to in-
crease comprehension. Thus text comprehension 
for non-natives could depend on either presenting 
only words that they can understand or offering an 
aid for understanding any challenging words that 
they may encounter.  
2.1 NLP techniques 
 Assuming that we can aid a non-native in 
understanding a document by using natural lan-
guage processing techniques, numerous possibili-
ties present themselves. We can help the student 
both on the word level and on a more global (con-
textual) level. On the word level, the one aid that 
does not appear to need any processing is diction-
ary definitions. Access to an online dictionary 
would give the student definitions to any word in 
question. However, many words are polysemous, 
often having several meanings for the same part of 
speech (like ?bank?). In that case, the reader has to 
choose which one of the meanings is the right one 
for the context of the text at hand. This dilemma 
(and possible incorrect choice) can be avoided by 
using word sense disambiguation (Dela Rosa and 
Eskenazi 2011). We showed that when definitions 
21
are presented in an ordered list, according to the 
best fit in the context, students learned words bet-
ter. Another word-level aid is the use of speech 
synthesis to speak the word to the reader (Dela 
Rosa 2010). Non-natives know some words au-
rally, but have never seen them in written form. 
This aid is especially helpful when the orthography 
of an unknown word makes it difficult to deduce 
the pronunciation (as in ?thought?). Another aid 
presents a word in other contexts. Giving the stu-
dent the ability to compare several contexts with 
their contrasting meanings is helpful for learning. 
These contexts can be found by searching for sen-
tences with a target word and a set of commonly 
co-occurring context words. 
 While research in vocabulary acquisition 
over the years has shown positive results for many 
word-centric learning aids, it is interesting to ex-
pand the offerings to context-level aids. We were 
also curious to see if the use of the REAP platform 
(Brown and Eskenazi, 2005) could help add to the 
knowledge of the role of translation in L2 vocabu-
lary learning. This is what brought us to examine 
the effect of translation and simplification on 
learning. These two techniques, thanks to the use 
of NLP, could be totally automated in the future. 
Research in machine translation (MT) goes back 
several decades and many types of statistical mod-
els have been employed (Koehn, 2010). If all of 
the documents to be translated are in one given 
domain, then sufficiently good automatically trans-
lations can be obtained.  
 Automated simplification is a newer do-
main. There has been significant progress in sim-
plifying documents for use by specific 
disadvantaged populations (Alusio et al2010, 
Bach et al 2011, Chandrasekar and Srinivas, 1997, 
Inui et al 2003, Medero and Ostendorf, 2011, 
Yaskar et al2010). Like Alusio and colleagues, 
who work with low-literacy populations, and a few 
other authors, we are concerned not only about the 
quality of the simplification, but also about 
whether the simplified documents actually help 
disadvantaged readers. 
 We could have also looked at summariza-
tion, which uses some of the same techniques that 
are used for simplification. In some early unpub-
lished studies, we found that students experienced 
difficulty when asked to summarize a passage. 
They usually responded by simply cutting and 
pasting the first sentence of that passage. This 
could have meant that students just could not pro-
duce a well-structured sentence and thus avoided 
doing so. But non-natives, who are asked to iden-
tify the appropriate summary out of four possibili-
ties in a multiple choice question, also had much 
difficulty. Thus, rather than giving a very high-
level overview of a passage through summariza-
tion, we chose to look at the intermediate level aids 
that would also contribute to vocabulary under-
standing: translation and simplification of local 
contexts.  
 Translation and simplification can both be 
characterized as relating to overall context, operat-
ing effectively on a string of several words rather 
than on only one word. They both aid in under-
standing the meaning of the whole string as op-
posed to just one target word, and their help for 
unknown words is through making the context of 
the word clear enough to surmise the meaning of 
the word. Besides its controversial status, transla-
tion had also attracted our interest when we ob-
served the students? efforts to get translations for 
tasks in class. We wanted to find out if translation 
had different properties from all other aids. Trans-
lation is different from the aids that we had used in 
the past in two ways:  
? it uses L1  
? it covers several-word contexts, rather 
than just one word.  
To tease apart these two characteristics, we became 
interested in simplification, which shares the sec-
ond characteristic, but not the first. 
3 The REAP tutor 
 The studies in this paper used the CMU 
REAP intelligent tutor. That tutor provides curricu-
lum for vocabulary acquisition for non-native stu-
dents while serving as a platform for research 
studies (Brown and Eskenazi, 2005). REAP gives 
students texts retrieved from the Internet that are 
matched to their reading level and their preferences 
(Heilman et al, 2008) and helps them acquire new 
words from context (Juffs et al, 2006). REAP in-
corporates several features like pop-up word defi-
nitions, examples of the word in other contexts, 
text-to-speech synthesis of words and translation of 
words to the student?s native language. 
 REAP presents the reading in any web 
browser (see Figure 1). Upon registration, students 
enter their native language. To get a definition, 
22
clicking on a word brings up a pop-up window 
showing the definition and examples of use of that 
word and a button for hearing the pronunciation of 
the word. Focus words, the words that the teacher 
has chosen for the students to learn, are highlighted 
in the text.  
 From the beginning, REAP has shown that 
it can improve students? acquisition of new vo-
cabulary in English (Heilman et al, 2006). Fea-
tures embedded in REAP have been validated in 
several experimental studies which showed the 
learning outcomes achieved by the students. REAP 
has been used to study motivation as well as learn-
ing gains. 
 
  
Figure 1. REAP interface and features for a 
student whose native language is Mandarin. 
 
4 The translation study 
 REAP was used to study whether transla-
tion helped students to learn vocabulary (Lin, Saz 
and Eskenazi, in review). These studies explored 
whether the students both learned more and be-
came more fluent when they use translation. It is 
challenging to measure fluency. While it is impos-
sible to record everything that the student says in 
her everyday conversations and then measure the 
average rapidity of response, one can measure the 
increase in the rapidity of response from the mo-
ment an item (post-test question) appears on the 
screen to when the student clicks on the answer 
and can compare results for that student as well as 
across groups of students. The documents used in 
this study were gathered from a crawl of the inter-
net for documents containing certain focus words 
that students were to learn. The documents were 
filtered to be at the level of the students and the 
topics were varied, from sports to current events, 
for example. The translation (bilingual dictionary) 
of the words in this study was provided by 
WordReference.com and the Bing Translator 
(http://www.microsofttranslator.com/) for the 
documents (contexts) in the study. The translations 
of all of the focus words in all of the students? L1s 
were manually checked by native speakers to make 
sure that the translated word corresponded with the 
specific context in which it appeared. If necessary, 
a change in the translation was made to make it 
context-appropriate. 
 All studies described in this paper were 
included as regular curricula at the English Lan-
guage Institute of the University of Pittsburgh. The 
first study involved 27 students taking the Level 5 
Reading course (high-intermediate learners); 25 
were native speakers of Arabic, 1 spoke Spanish 
and 1 spoke Turkish. The second study involved 
26 students in Level 5: 22 of them were native 
Arabic speakers, 2 were Mandarin Chinese speak-
ers and 2 were Korean speakers. There were two 
studies to determine whether the way that the stu-
dents requested translations had an effect on the 
amount of translations they asked for. 
 For both studies, the first session consisted 
of a pre-test which measured knowledge of a set of 
focus words in multiple-choice cloze questions 
(Taylor 1953), where the target word was removed 
from a full, meaningful sentence. There were 2 
questions per focus word. Post-reading (immedi-
ately after reading a document) and post-test (after 
all the training sessions were over) questions had 
the same form as the pre-test and involved com-
pletely different sentences. 
 In each training session, students had one 
400-500 word reading. After each reading, they 
took the post-reading test where they answered 2 
previously unseen cloze questions per focus word. 
The students were shown their results along with 
the correct answers to the cloze questions at the 
end of each post reading test. In the last session, 
the students took a post-test with content similar to 
the pre-test, 2 new unseen questions per focus 
word. 
 The first study took place for 8 weeks in 
the fall of 2011. Each reading session had one 
reading prepared for the students with 4 focus 
words, for a total of 24 focus words. The second 
23
study took place for 6 weeks in the spring of 2012. 
There were also 24 focus words in this study. 
 The main difference in the setup of both 
studies was how the students accessed a transla-
tion. For the fall 2011 study students had to type or 
copy and paste one or more words into a box at the 
bottom of the screen to get the translation. In the 
spring 2012 study they used a left mouseclick to 
get the translation. In both studies, the students 
could click (left mouseclick in fall 2011 and right 
mouseclick in spring 2012) to obtain the definition 
from the Cambridge Advanced Learners? Diction-
ary (CALD, Walter, 2005) and to listen to text-to-
speech synthesis of the word (Cepstral, 2012). 
The accuracy of each student at the pre-
test, post-reading and post-test was calculated as 
the percentage of correct answers over the total 
number of questions in the test. The fluency was 
calculated as the median response time of a given 
student to answer each question. To measure flu-
ency, we used the median and not the mean of the 
response times since the mean was distorted by a 
few instances of very long response duration for a 
few questions (possibly due to distractions). We 
also used comparative measures, such as gain and 
normalized gain in accuracy between two different 
assessment tasks (for instance, from pre-test to 
post-test) (Hake, 1998). A positive value of the 
gain and the normalized gain means that the stu-
dent achieved higher scores in the post-test. 
 We note that only 14 (17%) of the transla-
tions are for focus words.  
 The results show that students used transla-
tion when it was easier (clicking instead of typing), 
in detriment to using dictionary definitions. Stu-
dents did not request definitions or translations for 
all of the focus words. This may indicate that they 
are not indiscriminately clicking on words, as has 
sometimes been seen in the past. Rather they may 
be making an effort to click on words they felt they 
did not know well. 
 Dictionary Translation 
 All 
words 
Focus 
words 
All 
words 
Focus 
words 
Fall?11 5.29 2.35 2.31 0.64 
Spring?12 1.78 0.84 8.15 2.35 
 
Table 1. Use of dictionary and translation (4 focus 
words/reading in Fall?11, 3 focus words/reading in 
Spring?12). Average is per student and per reading. 
 
 We then examined the accuracy of the stu-
dents for just the words that they chose to translate. 
Table 2 shows that accuracy increases in post-
reading tests and post-tests with respect to the pre-
test for both studies. But there is a drop in the post-
test scores with respect to the post-reading tests in 
spring 2012. Furthermore, there is an increase in 
response time in the post-test, which is more pro-
nounced for spring 2012. These are the first indica-
tions of possible differences in student 
performance related to their patterns in the use of 
translations. 
 
 Accuracy Fluency 
 Scores (mean and standard deviation) Response 
time (median 
value) 
 Pre-test Post-
reading  
Post-test Pre-
test 
Post
-test 
Fall    
?11 
0.35?0.15 0.67?0.11 0.65?0.08 20 
sec. 
22 
sec. 
Spring?
12 
0.48?0.25 0.74?0.16 0.62?0.17 18 
sec. 
23.5 
sec. 
 
Table 2. Accuracy and fluency results for translated 
words. 
 
 To find whether the amount of translation 
actually affected this result, spring 2012 students 
were separated into 2 groups: the 13 students who 
used the least number of translations overall and 
the 13 students who used the most translations. 
Figure 2 shows the normalized gains in post-
reading tests and post-tests over the pre-test for 
these 2 groups. Both groups present a similar gain 
in post-reading (approximately 0.35) and, while 
this gain was lower for groups on the post-test, the 
students who used translation the most had a larger 
loss. Although not significant (p = 0.48), this dif-
ference, which is approximately 0.07 in normalized 
gain, indicates that these students are having more 
difficulty transferring the knowledge they may 
have acquired in the longer term. The low signifi-
cance is mainly due to the relatively small number 
of participants in the study. 
5 The simplification study 
 In this study the setup, using REAP as the 
platform, was similar to the translation study. The 
students could click right for translations or left for 
simplifications and could type a word in a box at 
the bottom of the screen for definitions. Transla-
tions and simplifications could be for one or sev-
24
eral words at a time. The number of questions on 
focus words (24 words this time), over the pretest, 
post-reading test and the post-test remained the 
same. There were 20 students in this study. There 
were 11 speakers of Arabic, 3 of Japanese, 2 each 
of Korean and Chinese and one each of Spanish 
and Serbo-Croatian. 
 
 FIGURE 2. Gains in post-reading and post-test de-
pending on the amount of translation used 
 
 Again, the translations were carried out 
automatically as described above, with a human 
verification pass. The simplifications were created 
by one of the authors by replacing less frequent 
words with appropriate more frequent ones (Leroy 
and Endicott, 2011) and splitting complex sen-
tences into shorter ones. An example of a simplifi-
cation:  
for: ? They began immigrating in large numbers in the 
1960s for economic reasons and now make up a third of 
the population?but there are also Africans, West Indi-
ans, Pakistanis, Indians, Turks, Chinese, and Eastern 
Europeans.?  
the simplified form was: ?They began immigrating in 
large numbers in the 1960s for economic reasons. These 
people now make up a third of the population. There are 
also Africans, West Indians, Pakistanis, Indians, Turks, 
Chinese, and Eastern Europeans.? 
Overall, they requested 218 simplifications, 82 
translations and 79 dictionary lookups. This was 
surprising to us. Given the large number of transla-
tion requests in the past two studies, we were pre-
pared to see overwhelmingly more clicks for 
translations than for simplifications. This result is 
important in deciding what aids can be given to 
non-native readers. While we thought that a reader 
would prefer an aid that involved translation, this 
result shows an acceptance of the L2 aid. Non-
natives probably realize the educational value of 
the L2 tool and voluntarily choose to use it. 
 Only 14 (17%) of the translations con-
tained focus words while 102 (47%) of the simpli-
fications did. Given the small number of focus 
word translations, results cannot be significant. 
REAP significantly helps students to learn focus 
words in general ( p<0 .05 ). Post-reading tests 
show lower accuracy than the post-test. The t-test 
shows that the difference here is not statistically 
significant ( p= 0 . 2 6 ).  
 To control for the quality of the study, we 
compared overall learning gains from this study 
with that of the two translation studies above on 
Table 3 and found them to be similar 
 
 Normalized Gain 
 Pre-test to Post-
reading 
Pre-test to post-test 
Fall?12 0.10 ? 0.24 0.17 ? 0.28 
Fall?11 0.31 ? 0.33 0.31 ? 0.28 
Spring?12 0.35 ? 0.28 0.22 ? 0.21 
Table 3. Learning Outcome: Gains (gain + deviation)  
  
 Figure 3 shows the number of requests for 
simplification and translation for each of the six 
documents in the study compared to their readabil-
ity level (Heilman 2008). We note that the hardest 
document (#6) was not the one for which the most 
aid was requested. This could simply be due to the 
decreasing number of requests for aid over time. 
 
 
Figure 3: Readability vs number of translations and 
simplifications  
25
 To control for any outlier document, we 
also looked at whether any one of the six docu-
ments required more translation than simplifica-
tion. Figure 3 also shows that the trend to request 
more simplification held true for all of the docu-
ments. We note that this can only be called a trend 
due to the significant standard deviation which, in 
turn, is due to the low number of participants. The 
first document was where the requests for the two 
were almost equal. This could be due to the stu-
dents trying out both possibilities to see what they 
liked or to the fact that over a short time they real-
ized the greater value of the L2 aid. 
 Table 4 shows the normalized gains for 
focus words that were translated or simplified. The 
low number of translation requests lead to results 
that are not significant. We note that for simplifica-
tion there is a trend implying learning gains at both 
the post-reading test and, in long term retention, 
for the post-test. 
 
Normalized gain 
Aid pre-test to post-
reading 
pre-test to post-
test 
No. 
items 
Translation -0.07 ? 0.15 0.22 ? 0.13 14 
Simplification 0.27 ? 0.17 0.28 ? 0.18 98 
 
Table 4: Normalized Gain (average and standard 
deviation) for focus words that were translated or 
simplified and number of clicks on focus words 
 
Normalized Gain 
 Pre-test to post-
reading 
Pre-test to post-
test 
no. of 
questions 
Focus words 
not translated 
0.06?0.26 0.17?0.30 946 
Focus words 
not simplified 
0.06?0.26 0.18?0.31 862 
 
Table 5: Normalized Gain (average and standard 
deviation) for focus words that were not translated 
or simplified and number of questions 
 
In the case of non-translated and non-simplified 
focus words, although there was also some room 
for improvement (and at first, it would seem that 
the learning gains are larger), there are some vari-
ables that have not been taken into account here. 
One is that a subject could have often requested 
definitions. Some subjects may benefit more from 
the use of the definitions than from other types of 
help. We will test this hypothesis in the future, 
when we have more data, to see if the benefits 
from each type of help are greater for some sub-
jects than for others. While we are not convinced 
that this is the cause for the differences we see 
here, we do believe that hearing the words when 
working through the documents may be a factor. 
Since the students only have the written form of 
the word at pre-test time, they may know the word 
to hear it, but not by sight. In past years in our use 
of REAP in the classroom, we have noticed many 
students suddenly recognizing a word after hearing 
it (from clicking on the synthesis option). Again 
due to lack of sufficient data, we cannot explore 
this further for this dataset, but plan to look at this 
and any other possible variables in the near future. 
6 Conclusions and further directions 
 We have argued that exploring the learning 
results of non-natives when using various aids for 
learning vocabulary through context may guide our 
choices of reading aids for this population.  
 We have specifically explored the use of 
translation and of simplification. Both simplifica-
tion and translation are voluntarily used by stu-
dents and when both are available, students tend to 
prefer simplification. This should make the use of 
simplified documents in real life reading situations 
very acceptable to non-natives.  
 The overuse of translation contributes to a 
decline in long term retention of new vocabulary 
while the use of simplification appears to aid in 
retention. This could mean that reading any simpli-
fied document may benefit the ever-learning non-
native when encountering future documents.  
 In REAP, we collect documents from the 
Internet and characterize them by reading level. 
We also characterize them by topic (sports, health, 
etc). While we choose these documents to keep up 
the students? interest, they in no way represent the 
real challenges of dealing with a rental agreement, 
a bank loan document, etc. While REAP does in-
still fundamentals of vocabulary understanding, it 
does not have the student apply this knowledge to 
the situations that are encountered in the real 
world. This is an essential need that can be fulfilled 
by members of the NLP community working to-
gether to create a database of real life challenging 
documents that can be annotated and used as a ba-
sis of comparison of research results. These docu-
ments should also be annotated for readability, etc. 
Such a realistic database can then serve the com-
26
munity as a whole as it develops novel and robust 
simplification tools. 
 
ACKNOWLEDGEMENTS 
 This work is supported through the Re-
finement and Fluency Thrust of the Pittsburgh Sci-
ence of Learning Center which is funded by the US 
National Science Foundation under grant number 
SBE-0836012. Any opinions, findings, and conclu-
sions or recommendations expressed in this mate-
rial are those of the authors and do not necessarily 
reflect the views of the NSF. Oscar Saz was sup-
ported with a Fulbright/MEX fellowship. 
References  
Alusio, S., Specia, L., Gasperin, C., Scarton, C., 2010, 
Readability Assessment for Text Simplification, Proc 
NAACL HLT Fifth Workshop on Innovative Use of 
NLP for  Building Educational Applications, p. 1-9. 
Bach, N., Gao, Q.,Vogel, S., Waibel A., 2011, TriS: A 
Statistical Sentence Simplifier with Log-linear Mod-
els and Margin-based Discriminative Training  In 
Proceedings of the 5th International Joint Conference 
on Natural Language Processing (IJCNLP 2011), 
Chiang Mai, Thailand. 
Brown, J., Eskenazi, M., 2005, Student, text and cur-
riculum modeling for reader-specific document re-
trieval, In Hamza, M.-H. (Ed.) Proceedings of the 
IASTED International Conference on Human-
Computer Interaction (pp. 44-47). Anaheim, CA: 
Acta Press. 
Burstein, J., Shore, J., Sabatini, J., Lee, Y., Ventura, M., 
2007, The automated text adaptation tool, in Demo 
proceedings of NAACL-HLT, Rochester. 
Cepstral Text-to-Speech, 2000, Retrieved Sep. 8, 2012, 
from http://www.cepstral.com/. 
Chandrasekar, R. and Srinivas, B., 1997, Automatic 
induction of rules for text simplification. Knowledge-
Based Systems, 10(3):183--190. 
Coxhead, A., 2000, A New Academic Word List. 
TESOL Quarterly, 34(2), pp. 213-238. 
doi:10.2307/3587951 
Crossley, S. A., Louwerse, M. M., McCarthy, P. M., & 
McNamara, D. S., 2007, A linguistic analysis of sim-
plified and authentic texts. The Modern Language 
Journal, 91(1), 15-30.  
Dela Rosa, K., Eskenazi, M., 2011, Impact of Word 
Sense Disambiguation on Ordering Dictionary Defi-
nitions in Vocabulary Learning Tutors, Proceedings 
of the 24th International FLAIRS Conference. 
Dela Rosa, K., Parent, G.,Eskenazi, M., 2010, Multimo-
dal learning of words: A study on the use of speech 
synthesis to reinforce written text in L2 language 
learning, Proceedings of the ISCA Workshop on 
Speech and Language Technology in Education 
(SLaTE 2010). 
Geer, P., 2011, GRE Verbal Workbook. Hauppauge, 
NY: Barron?s Educational Series. 
Grace, C. A., 1998, Retention of Word Meanings In-
ferred from Context and Sentence-Level Transla-
tions: Implications for the Design of Beginning-
Level CALL Software. The Modern Language Jour-
nal, 82, 533?544. doi: 10.1111/j.1540-
4781.1998.tb05541.x 
Hake, R., 1998, Interactive-engagement versus tradi-
tional methods: a six-thousand- student survey of 
mechanics test data for introductory physics courses. 
American Journal of Physics, 66, 64 ? 74.  
Heilman, M., Collins-Thompson, K., Callan, J. and Es-
kenazi, M., 2006, Classroom success of an Intelligent 
Tutoring System for lexical practice and reading 
comprehension. Proceedings of the Ninth Interna-
tional Conference on Spoken Language Processing 
(pp. 829-832). Pittsburgh, PA. 
Heilman, M., Zhao, L., Pino, J., and Eskenazi, M., 2008, 
In Tetreault, T., Burstein, J.  and  De Felice, R. (Ed.) 
Retrieval of Reading Materials for Vocabulary and 
Reading Practice. Proceedings of the 3rd Workshop 
on Innovative Use of NLP for Building Educational 
Applications (pp.80-88), Columbus, OH: Association 
for Computational Linguistics. 
doi:10.3115/1631836.1631846 
Inui, K., A. Fujita, T. Takahashi, R. Iida and T. Iwakura, 
2003, Text simplification for reading assistance: a 
project note, Proceedings of the second international 
workshop on paraphrasing-volume 16, pages 9--16. 
Association for Computational Linguistics. 
Jiang, N., 2000, Lexical representation and development 
in a second language. Applied Linguistics, 21(1), 47-
77. doi: 10.1093/applin/21.1.47 
Juffs, A., Wilson, L., Eskenazi, M., Callan, J., Brown, 
J., Collins-Thompson, K., Heilman, M., Pelletreau, 
T. and Sanders, J., 2006, Robust learning of vocabu-
lary: investigating the relationship between learner 
behaviour and the acquisition of vocabulary. Paper 
presented at the 40th Annual TESOL Convention and 
Exhibit (TESOL 2006), Tampa Bay, FL. 
Koehn, P., 2010, Statistical machine translation. Cam-
bridge University Press. 
Kroll, J. F. and Sunderman, G., 2003, Cognitive Proc-
esses in Second Language Learners and Bilinguals: 
27
The Development of Lexical and Conceptual Repre-
sentations. In C.J. Doughty and M. H. Long (Ed.), 
The Handbook of Second Language Acquisition. Ox-
ford, UK: Blackwell Publishing Ltd,. doi: 
10.1002/9780470756492.ch5 
Leroy, G., Endicott, J.E., 2011, Term familiarity to indi-
cate perceived and actual difficulty of text in medical 
digital libraries (ICADL 2011), Beijing. 
Lin, Y., Saz, O., Eskenazi, M. (in review) Measuring 
the impact of translation on the accuracy and fluency 
of vocabulary acquisition of English  
Medero, J., Ostendorf, M.,  2011, Identifying Targets 
for Syntactic Simplification," Proc. ISCA SLaTE 
ITRW Workshop.  
Oh, S-Y, 2008, Two types of input modification and 
EFL reading comprehension: simplification versus 
elaboration, TQD 2008, vol.35-1. 
Perfetti, C.C., 2010, Decoding, vocabulary and compre-
hension: the golden triangle of reading skill, in M.G. 
McKeown and L. Kucan (Eds), Bringing reading re-
searchers to life: essays in honor of Isabel Beck, pp. 
291-303, New York: Guilford. 
Petersen, S., Ostendorf, 2007, Text simplification for 
language learners: a corpus analysis, Proc ISCA 
SLaTE2007, Farmington PA 
Prince, P., 1996, Second Language Vocabulary Learn-
ing: The Role of Context versus Translations as a 
Function of Proficiency. Modern Language Journal, 
80(4), 478-493. doi:10.2307/329727 
Taylor, W.L., 1953, Cloze procedure: a new tool for 
measuring readability, Journalism Quarterly, vol.30, 
pp. 415-433. 
Walter, E., 2005, Cambridge Advanced Learner's Dic-
tionary, 2nd Edition. Cambridge, UK: Cambridge 
University 
Yano, Y., Long, M. H., & Ross, S., 1994, The effects of 
simplified and elaborated texts on foreign language 
reading comprehension, Language Learning, 44(2), 
189-219. 
Yatskar, M., Pang, B., Danescu-Niculescu-Mizil, C., 
Lee, L., 2010, For the sake of simplicity : unsuper-
vised extraction of lexical simplifications from 
Wikipedia, Proc. NAACL 2010, p. 365-368. 
 
28
Proceedings of the SIGDIAL 2013 Conference, pages 414?422,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
Recipe For Building Robust Spoken Dialog State Trackers:                         
Dialog State Tracking Challenge System Description 
 
 
Sungjin Lee 
Language Technologies Institute,  
Carnegie Mellon University,        
Pittsburgh, Pennsylvania, USA 
sungjin.lee@cs.cmu.edu  
Maxine Eskenazi 
Language Technologies Institute,   
Carnegie Mellon University,        
Pittsburgh, Pennsylvania, USA 
max@cs.cmu.edu 
 
  
 
Abstract 
For robust spoken conversational interaction, 
many dialog state tracking algorithms have 
been developed. Few studies, however, have 
reported the strengths and weaknesses of each 
method. The Dialog State Tracking Challenge 
(DSTC) is designed to address this issue by 
comparing various methods on the same 
domain. In this paper, we present a set of 
techniques that build a robust dialog state 
tracker with high performance: wide-coverage 
and well-calibrated data selection, feature-rich 
discriminative model design, generalization 
improvement techniques and unsupervised 
prior adaptation. The DSTC results show that 
the proposed method is superior to other 
systems on average on both the development 
and test datasets.  
1 Introduction 
Even though we have recently seen an explosive 
growth of interest in speech-enabled applications, 
there are still many problems to overcome in 
order to provide users with practical and 
profitable services. One of the long-standing 
problems which may often frustrate users is 
Automatic Speech Recognition (ASR) error. Due 
to ASR error, it is barely possible to directly 
observe what the user said and finally figure out 
the true user goal. The aim of dialog state 
tracking is, therefore, to accurately estimate the 
true dialog state from erroneous observations as 
a dialog unfolds. 
In order to achieve this goal, many dialog state 
tracking algorithms have been developed. Few 
studies, however, have reported the strengths and 
weaknesses of each method. The Dialog State 
Tracking Challenge1  (DSTC) was organized to 
advance state-of-the-art technologies for dialog 
state tracking by allowing for reliable 
comparisons between different approaches using 
the same datasets. Unlike other machine 
learning-based empirical tasks, DSTC is also 
carefully designed to take into consideration 
diverse realistic mismatches. For instance, there 
are test datasets that were collected by systems 
using different speech recognizers, spoken 
language understanding (SLU) modules, and 
dialog managers. Also there are test datasets that 
were produced by similar systems but deployed 
at a different time (1 year later) with extended 
coverage. Since such mismatches between 
training and test data may often happen in real 
deployment, it is important to build a tracker 
which constantly shows high performance across 
all test datasets despite various mismatches. 
The aim of this paper is to describe a set of 
techniques used to build a robust tracker with 
high performance: wide-coverage and well-
calibrated data selection, feature-rich 
discriminative model design, generalization 
improvement techniques and unsupervised prior 
adaptation. Our challenge systems are basically 
various combinations of those techniques. The 
DSTC results demonstrate the effectiveness of 
each technique.  
This paper is structured as follows. Section 2 
describes the challenge setup. Section 3 
elaborates on our proposed approaches. Section 4 
briefly describes previous research and other 
systems that participated in DSTC. Section 5 
presents and discusses the results. Finally, 
Section 6 concludes with a brief summary and 
suggestions for future research.  
                                                 
1 http://research.microsoft.com/en-us/events/dstc/ 
414
2 Dialog State Tracking Challenge 
This section describes the task for DSTC and 
datasets provided for training and test. Most part 
of this section is borrowed from the DSTC 
manual2.  
2.1 Task Description 
DSTC data is taken from several different 
spoken dialog systems which all provided bus 
schedule information for Pittsburgh, 
Pennsylvania, USA as part of the Spoken Dialog 
Challenge (Black et al, 2011). There are 9 slots 
which are evaluated: route, from.desc, 
from.neighborhood, from.monument, to.desc, 
to.neighborhood, to.monument, date, and time. 
Since both marginal and joint representations of 
dialog states are important for deciding dialog 
actions, the challenge takes into consideration 
both. Each joint representation is an assignment 
of values to all slots.  Thus there are 9 marginal 
outputs and 1 joint output in total, which are all 
evaluated separately. 
The dialog tracker receives SLU N-best 
hypotheses for each user turn, each with a 
confidence score. In general, there are a large 
number of values for each slot, and the coverage 
of N-best hypotheses is good, thus the challenge 
confines consideration of goals to slots and 
values that have been observed in an SLU output. 
By exploiting this aspect, the task of a dialog 
state tracker is to generate a set of observed slot 
and value pairs, with a score between 0 and 1. 
The sum of all scores is restricted to sum to 1.0. 
Thus 1.0 ? total score is defined as the score of a 
special value None that indicates the user?s goal 
has not yet been appeared on any SLU output. 
2.2 Datasets 
The data is divided into 2 training sets and 4 test 
sets (Table 1). For standardized development sets, 
each training set is split in half. Participants were 
asked to report results on the second half of each 
set. The data from group A in train2, and test1 
was collected using essentially the same dialog 
system. Only a few updates were made to reflect 
changes to the bus schedule. The data in test2 
was collected using a different version of group 
A?s dialog manager. The data from group B in 
train3 and test3 were collected using essentially 
the same dialog system; the main difference is 
that test3 covers more bus routes. Test4 tests the 
condition when training and testing using totally 
                                                 
2 http://research.microsoft.com/apps/pubs/?id=169024 
different dialog systems, and when there is no 
same-system training data available. 
2.3 Metrics 
There are a variety of aspects of tracker 
performance that were measured: accuracy, mean 
reciprocal rank (MRR), ROC curves, Average 
score 3 , and Brier score 4 . There are three 
schedules for determining which turns to include 
in each evaluation. 
? Schedule 1: Include all turns. 
? Schedule 2: Include a turn for a given 
concept only if that concept either appears on 
the SLU N-Best list in that turn, or if the 
system?s action references that concept in 
that turn. 
? Schedule 3: Include only the turn before the 
system starts over from the beginning, and 
the last turn of the dialog. 
3 Recipe for Building a Robust Tracker  
In this section, we present several ingredients for 
building a robust state tracker that come into play 
at various levels of the development process: 
from data selection to model adaptation. 
3.1 Wide-Coverage and Well-Calibrated 
Data Selection 
The first step to create a robust dialog state 
tracker is the use of data which covers diverse 
system dialog actions and user inputs with well-
calibrated confidence scores. Since dialog 
policies can be varying according to how a 
dialog proceeds, it is crucial to arrange a training 
dialog corpus with well-balanced dialog actions. 
For example, group A datasets barely have 
implicit confirmation and heavily rely on explicit 
confirmation, while group B datasets have both 
types of confirmation. Thus a model trained on 
group A datasets cannot exploit implicit 
                                                 
3 the average score assigned to the correct item 
4 the L2 norm between the vector of scores output by 
dialog state tracker and a vector with 1 in the position 
of the correct item, and 0 elsewhere 
Dataset Source Calls Time period 
train2 Group A 678 Summer 2010 
train3 Group B 779 Summer 2010 
test1 Group A 765 Winter 2011-12 
test2 Group A 983 Winter 2011-12 
test3 Group B 1037 Winter 2011-12 
test4 Group C 451 Summer 2010 
 
Table 1: Dataset description. 
 
415
confirmation when applied to group B datasets, 
whereas a model trained on group B datasets can 
be applied to group A datasets without much 
loss.  
Another important aspect of the data is how 
well user inputs are calibrated. If the confidence 
score is well-calibrated, confirmation can be 
skipped in the case of a hypothesis with a high 
confidence.  On the contrary, if the quality of the 
confidence score is very poor, a successful dialog 
will only be possible via heavy use of 
confirmation. Thus a model trained on a well-
calibrated dataset is likely to perform well on the 
poorly-calibrated dataset because of backup 
confirmation. Whereas, a model trained on the 
poorly-calibrated dataset will not perform well 
on the well-calibrated dataset due to the 
mismatch of the confidence score as well as the 
scarceness of confirmation information. The 
group A datasets have been shown to be poorly 
calibrated (Lee and Eskenazi, 2012); this is also 
shown in Fig. 2. Group B datasets are relatively 
well-calibrated, however. 
The importance of wide coverage and well-
calibrated data can be observed by examining the 
results of entry1 and entry2 (Fig. 1) which are 
trained on group A and B datasets, respectively. 
3.2 Feature-Rich Discriminative Model Design 
Most previous approaches are based on 
generative temporal modeling where the current 
dialog state is estimated using a few features 
such as the current system action and N-best 
hypotheses with corresponding confidence scores 
given the estimated dialog state at the previous 
turn (Gasic and Young, 2011; Lee and Eskenazi, 
2012; Raux and Ma, 2011; Thomson and Young, 
2010; Williams, 2010; Young et al, 2010). 
However, several fundamental questions have 
been raised recently about the formulation of the 
dialog state update as a generative temporal 
model: limitation in modeling correlations 
between observations in different time slices; and 
the insensitive discrimination between true and 
false dialog states (Williams, 2012).  
 
Figure 2: Estimated empirical accuracy of confidence 
score for from slot. Ideally calibrated confidence score 
should be directly proportional to empirical accuracy. 
 
 
 
Figure 1: Diagram showing the relation between datasets and models. Each team could have up to five systems 
entered. Our challenge entries are tagged by their entry numbers. More detailed descriptions about each model 
are provided in Section 3. 
416
In fact, such limitations can be improved by 
adopting a discriminative approach, which 
enables the incorporation of a rich set of features 
without worrying about their interdependence 
(Sutton and McCallum, 2006). For example, a 
hypothesis that repeats with low confidence 
scores is likely to be a manifestation of ASR 
error correlations between observations in 
different time slices. Thus, the highest 
confidence score that a hypothesis has attained 
so far could be a useful feature in preventing 
repeated incorrect hypotheses from defeating the 
correct hypothesis (which had a higher score but 
was only seen once). Another useful feature 
could be the distribution of confidence scores 
that a hypothesis has attained thus far, since it 
may not have the same effect as having a single 
observation with the total score due to the 
potential nonlinearity of confidence scores. 
There are many other potentially useful features. 
The entire list of features used for the challenge 
system is found in Appendix A. 
In addition to the role of rich features in 
performance enhancement, the incorporation of 
rich features is also important for robust state 
tracking. If the tracker estimates the true state by 
considering various aspects of observations and 
prior knowledge, then the influence of 
differences in certain factors between datasets 
can be mitigated by many other factors that are 
retained relatively unchanged between datasets.  
For the challenge system, we employed a 
Maximum Entropy (MaxEnt) model which is one 
of most powerful undirected graphical models. 
Unlike previous work using MaxEnt (Bohus and 
Rudnicky, 2006) where the model is limited to 
maintain only the top K-best hypotheses, we 
amended MaxEnt to allow for the entire set of 
observed hypotheses to be incorporated; Several 
feature functions which differ only by output 
labels were aggregated into one common feature 
function so that they can share common 
parameters and gather their statistics together 
(Appendix A). This modification is also crucial 
for robust estimation of the model parameters 
since some slots such as from and to can have 
about 104 values but most of them are not seen in 
the training corpus. 
The effectiveness of feature-rich 
discriminative modeling can be observed by 
comparing the results of DMALL and PBM (Fig. 
1) which are discriminative and generative 
models, respectively. 
Note that interesting relational constraints, e.g. 
whether or not departure and arrival places are 
valid on a route, can be incorporated by adopting 
a structured model such as Conditional Random 
Field (CRF). But CRF was not used for the 
challenge since the bus information that was 
provided is not compatible with every dataset. 
The effectiveness of a structured model has been 
investigated in a separate publication (Lee, 2013). 
3.3 Generalization Improvement Techniques 
Even though the incorporation of a set of rich 
features helps overcome the weaknesses of 
previous approaches, it also implies a risk of 
overfitting training datasets due to its increased 
capacity of function class. Overfitting is a serious 
hazard especially for test datasets that are 
severely dissimilar to training datasets. As noted 
above, since the test datasets of the challenge are 
intentionally arranged to have various 
mismatches, it is crucial that we prevent a model 
from overfitting training datasets. In the rest of 
this section, we describe various ways of 
controlling the capacity of a model.  
 The most obvious method to control the 
capacity is to penalize larger weights 
proportional to the squared values of the weights 
or the absolute values of the weights. We employ 
the Orthant-wise Limited-memory Quasi Newton 
optimizer (Andrew and Gao, 2007) for L1 
regularization. The weights for L1 regularization 
were set to be 10 and 3 for the prior features and 
the other features, respectively. These values 
were chosen through cross-validation over 
several values rather than doing a thorough 
search. 
A second method, which is often convenient, 
is to start with small weights and then stop the 
learning before it has time to overfit provided 
that it finds the true regularities before it finds 
the spurious regularities that are related to 
specific training datasets. It could be hard, 
however, to decide when to stop. A typical 
technique is to keep learning until the 
performance on the validation set gets worse and 
then stop training and go back to the best point. 
For the challenge systems, we applied a simpler 
method that is to stop the training if the average 
objective function change over the course of 10 
previous iterations is less than 0.1, which is 
usually set to a much smaller number such as 10-4. 
In general, prediction errors can be 
decomposed into two main subcomponents, i.e., 
error due to bias and variance (Hastie et. al, 
2009). It is also known that there is a tradeoff 
between bias and variance. If a model is flexible 
enough to fit the given data, errors due to bias 
417
will decrease while errors due to variance will 
increase. The methods stated above try to 
achieve less error by decreasing errors due to 
variance. However we cannot avoid increasing 
errors due to bias in this way. Thus we need a 
method to alleviate the tradeoff between bias and 
variance.  
System combination is one powerful way to 
reduce variance without raising bias. If we 
average models that have different forms and 
make different mistakes, the average will do 
better than the individual models. This effect is 
largest when the models make very different 
predictions from one another. We could make the 
models different by simply employing different 
machine learning algorithms as well as by 
training them on different subsets of the training 
data.  
The challenge system, entry3, consists of three 
discriminative models and one generative model 
(Fig. 1). Entry1 and entry2 were trained on 
different training datasets to make them produce 
different predictions. DMCOND is a discriminative 
model trained on both train2 and train3. Also, 
DMCOND differs from other discriminative 
models in the way that it was trained: the 
parameters associated with the features which are 
computable without grounding action 
information (features (1), (5), (8), (9) and (10) in 
Appendix A) are trained first and then the other 
features are learned given the former parameters. 
The idea behind this training method is to 
encourage the model to put more weight on 
dialog policy invariant features. The final 
component PBM is the AT&T Statistical Dialog 
Toolkit 5  which is one of the state-of-the-art 
generative model-based systems. We modified it 
to process implicit confirmation and incorporate 
the prior distribution which was estimated on the 
training corpus. The prior distribution was 
smoothed by an approximate Good-Turing 
estimation on the fly when the system encounters 
an unseen value at run time. The improvement 
from system combination is verified by the 
results of entry3. 
3.4 Unsupervised Prior Adaptation 
While a prior is a highly effective type of 
information for dialog state tracking, it is also 
able to hamper the performance when incorrectly 
estimated. Thus it is worthwhile to investigate 
adapting the prior to the test datasets. Since a 
dialog state tracker is meant to estimate the 
                                                 
5 http://www2.research.att.com/sw/tools/asdt/ 
posterior probabilities over hypotheses, we can 
extract estimated labels from test datasets by 
setting an appropriate threshold, taking the 
hypotheses with a greater probability than the 
threshold as labels. By combining the predictive 
prior from test datasets and the prior from 
training datasets, we adapted entry2 and entry3 
in an unsupervised way to produce entry5 and 
entry4, respectively (Fig. 1). For each test dataset, 
we used different thresholds: 0.95 for test1, test2 
and test3, and 0.85 for test4. 
4 Related Work 
Since the Partially Observable Markov Decision 
Process (POMDP) framework has offered a 
well-founded theory for both state tracking and 
decision making, most earlier studies adopted 
generative temporal models, the typical way to 
formulate belief state updates for POMDP-based 
systems (Williams and Young, 2007). Several 
approximate methods have also emerged to 
tackle the vast complexity of representing and 
maintaining belief states, e.g., partition-based 
approaches (Gasic and Young, 2011; Lee and 
Eskenazi, 2012; Williams, 2010; Young et al, 
2010) and Bayesian network (BN)-based 
methods (Raux and Ma, 2011; Thomson and 
Young, 2010). A drawback of the previous 
generative models is that it is hard to incorporate 
a rich set of observation features, which are often 
partly dependent on one another. Moreover, the 
quality of the confidence score will be critical to 
all generative models proposed so far, since they 
do not usually try to handle potential nonlinearity 
in confidence scores.  
As far as discriminative models are concerned, 
the MaxEnt model has been applied (Bohus and 
Rudnicky, 2006). But the model is restricted to 
maintaining only the top K-best hypotheses, 
where K is a predefined parameter, resulting in 
potential degradation of performance and 
difficulties in extending it to structured models. 
Finally, there is a wide range of systems that 
participated in Dialog State Tracking Challenge 
2013: from rule-based systems to fairly complex 
statistical methods such as Deep Neural 
Networks. Since we have not only traditional 
generative models such as Dynamic Bayesian 
Network and partition-based approaches, but also 
newly-proposed discriminative approaches such 
as log-linear models, Support Vector Machines 
and Deep Neural Networks, the analysis of the 
challenge results is expected to reveal valuable 
lessons and future research directions. 
418
5 Results and Discussion  
The official results of the challenge are publicly 
available and our team is team6. As mentioned in 
Section 2.3, there are a variety of aspects of 
tracker performance that were measured on 
different schedules. Since prediction accuracy at 
the end of a dialog directly translates to the 
success of the entire task, we first show the 
average accuracy across all test datasets 
measured at schedule 3 in Fig. 3. The average 
accuracy at schedule 3 also well represents how 
robust a state tracker is since the test datasets are 
widely distributed in the dimensions of dialog 
policies, dialog length and the quality of user 
input and confidence score.  
First of all, we note that our 4 entries 
(entries2-5) took the top positions in both the All 
and Joint categories. Entry4, which showed the 
best performance, outperformed the best entry 
from other teams by 4.59% (entry2 of team9) 
and 10.1% (entry2 of team2). Specificially, the 
large improvement in Joint implies that our 
model performs evenly well for all slots and is 
more robust to the traits of each slot. 
Furthermore, from the results we can verify 
the effectiveness of each technique for achieving 
robustness. Given the large gap between the 
performance of entry1 and of entry2, it is clearly 
shown that a model trained on a wide-coverage 
and well-calibrated dialog corpus can be 
applicable to a broad range of test datasets 
without much loss. Even though entry2 was 
trained on only 344 dialogs (the first half of 
train3), it already surpasses most of competing 
models.  
The utility of a feature-rich discriminative 
model is demonstrated by the fact that DMALL 
greatly outperformed PBM. We also note that 
just using a discriminative model does not 
 
 
 
(a) All slot: a weighted average accuracy across all slots 
 
(b) Joint slot 
 
Figure 3: Accuracy measured at schedule 3 averaged over the test and development datasets. Models which do 
not appear in Fig. 1 are the best system of each team except for us. Rule denotes a rule-based system, Hybrid a 
hybrid system of discriminative and generative approaches, DiscTemp a discriminative temporal model, RForest 
a random forest model, DNN a deep neural network model, DiscJoint a discriminative model which deals with 
slots jointly, SVM a support vector machine model, and DBN a dynamic Bayesian network mode. 
419
guarantee improved performance since many 
discriminative systems that participated in the 
challenge underperformed some of the entries 
that were based on generative modeling or rules. 
This result implies that devising effective 
features is central to performance.  
In addition, this result also points to the 
necessity of controlling the capacity of a model. 
While our models constantly show good 
performance both on development sets and test 
sets, the performance of the other models 
significantly dropped off. In fact, this explains 
why Hybrid and Rule systems switch their 
positions in the Joint slot. Moreover, many other 
systems in the graph tail seem to be severely 
overfitted, resulting in poor performance on test 
datasets despite relatively good performance on 
development datasets. As expected, system 
combination gives rise to better accuracy without 
loss of robustness; entry3 clearly outperforms 
each of its components, i.e. entry1, entry2, 
DMCOND and PBM, on both development and test 
datasets. 
Finally, the improvement observed when 
using unsupervised prior adaptation is also 
shown to be positive but its effect size is not 
significant: entry5 vs. entry2 and entry4 vs. 
entry3. Given that the way in which we have 
adapted the model is fairly primitive, we believe 
that there is much room to refine the 
unsupervised adaptation method.  
MRR measures the average of 1/R, where R is 
the rank of the first correct hypothesis. MRR at 
schedule 3 measures the quality of the final 
ranking which may be most important to a multi-
modal interface that can display results to the 
user. Even though the results are not displayed 
due to space limitations, the results for MRR are 
very similar to those for accuracy. Our 4 entries 
(entries2-5) still take the top positions. 
The ROC curves assess the discrimination of 
the top hypothesis? score. The better 
discrimination at schedule 2 may be helpful for 
reducing unnecessary confirmations for values 
with sufficiently high belief. Also, the better 
discrimination at schedule 3 may enable a model 
to adapt to test data in an unsupervised manner 
by allowing us to set a proper threshold to 
produce predictive labels. The ROC curves of 
our systems again showed the highest levels of 
discrimination. 
6 Conclusion 
In this paper, we presented a set of techniques to 
build a robust dialog state tracker without losing 
performance: wide-coverage and well-calibrated 
data selection, feature-rich discriminative model 
design, generalization improvement techniques 
and unsupervised prior adaptation. The results in 
terms of various metrics show that the proposed 
method is truly useful for building a tracker 
prominently robust not only to mismatches 
between training and test datasets but also to the 
traits of different slots. Since we used relatively 
simple features for this work, there is much room 
to boost performance through feature 
engineering. Also, more thorough search for 
regularization weights can give additional 
performance gain. Moreover, one can extend the 
present discriminative model presented here to a 
structured version which can improve 
performance further by allowing  relational 
constraints to be incorporated (Lee, 2013). 
Finally, we believe that once a more detailed and 
thorough investigation of the challenge results 
has been carried out, we will be able to take the 
best of each system and combine them to 
generate a much better dialog state tracker.   
Acknowledgments 
This work was funded by NSF grant IIS0914927. 
The opinions expressed in this paper do not 
necessarily reflect those of NSF. 
References  
G. Andrew and J. Gao, 2007. Scalable training of L1-
regularized log-linear models. In Proceedings of 
ICML. 
A. Black et al, 2011. Spoken dialog challenge 2010: 
Comparison of live and control test results. In 
Proceedings of SIGDIAL. 
D. Bohus and A. Rudnicky, 2006. A K hypotheses + 
other belief updating model. In Proceedings of 
AAAI Workshop on Statistical and Empirical 
Approaches for Spoken Dialogue Systems. 
M. Gasic and S. Young, 2011. Effective handling of 
dialogue state in the hidden information state 
POMDP-based dialogue manager. ACM 
Transactions on Speech and Language Processing, 
7(3). 
T. Hastie, R. Tibshirani, and J. Friedman, 2009. The 
Elements of Statistical Learning: Data Mining, 
Inference, and Prediction (2nd edition). Springer. 
420
S. Lee and M. Eskenazi, 2012. Exploiting Machine-
Transcribed Dialog Corpus to Improve Multiple 
Dialog  States Tracking Methods. In Proceedings 
of SIGDIAL, 2012. 
S. Lee, 2013. Structured Discriminative Model For 
Dialog State Tracking. Submitted to SIGDIAL, 
2013. 
A. Raux, B. Langner, D. Bohus, A. W Black, and M. 
Eskenazi, 2005. Let?s Go Public! Taking a Spoken 
Dialog System to the Real World. In Proceedings 
of Interspeech. 
A. Raux and Y. Ma, 2011. Efficient Probabilistic 
Tracking of User Goal and Dialog History for 
Spoken Dialog Systems. In Proceedings of 
Interspeech. 
C. Sutton and A. McCallum, 2006. An Introduction to 
Conditional Random Fields for Relational 
Learning. Introduction to Statistical Relational 
Learning. Cambridge: MIT Press. 
B. Thomson and S. Young, 2010. Bayesian update of 
dialogue state: A POMDP framework for spoken 
dialogue systems. Computer Speech & Language, 
24(4):562-588. 
B. Thomson, F. Jurccek, M. Gasic, S. Keizer, F. 
Mairesse, K. Yu, S. Young, 2010a. Parameter 
learning for POMDP spoken dialogue models. In 
Proceedings of SLT.  
J. Williams and S. Young, 2007. Partially observable 
Markov decision processes for spoken dialog 
systems. Computer Speech & Language, 
21(2):393-422. 
J. Williams, 2010. Incremental partition 
recombination for efficient tracking of multiple 
dialog states. In Proceedings of ICASSP. 
J. Williams, 2011. An Empirical Evaluation of a 
Statistical Dialog System in Public Use, In 
Proceedings of SIGDIAL. 
J. Williams, 2012. A Critical Analysis of Two 
Statistical Spoken Dialog Systems in Public Use. 
In Proceedings of SLT. 
S. Young, M. Gasic, S. Keizer, F. Mairesse, J. Schatz-
mann, B. Thomson and K. Yu, 2010. The Hidden 
Information State Model: a practical framework for 
POMDP-based spoken dialogue management. 
Computer Speech and Language, 24(2):150?174. 
 
Appendix A. Feature Functions 
Feature functions are playing a central role to the 
performance of discriminative models. We 
describe the feature functions that we used for 
the challenge system in the following. To 
facilitate readers? understanding an example of 
feature extraction is illustrated in Fig. 4. 
One of the most fundamental features for 
dialog state tracking should exploit the 
confidence scores assigned to an informed 
hypothesis. The simplest form could be direct 
use of confidence scores. But often pre-trained 
confidence measures fail to match the empirical 
distribution of a given dialog domain (Lee and 
Eskenazi, 2012; Thomson et al 2010). Also the 
distribution of confidence scores that a 
hypothesis has attained so far may not have the 
same effect as the total score of the confidence 
scores (e.g., in Fig. 4, two observations for 61C 
with confidence score 0.3 vs. 0.6 which is the 
sum of the scores). Thus we create a feature 
function that divides the range of confidence 
scores into bins and returns the frequency of 
observations that fall into the corresponding bin: 
 
       (    
 )  
        {
                 (       (    
 ))
           
   
(1) 
 
where      ( )  returns the set of confidence 
scores whose action informs   in the sequence of 
observations   
 .         (   )  computes the 
frequency of observations that fall into the     
bin. 
There are two types of grounding actions 
which are popular in spoken dialog systems, i.e., 
implicit and explicit confirmation. To leverage 
affirmative or negative responses, the following 
feature functions are introduced in a similar 
fashion as the        feature function: 
 
       (    
 )  
       {
                 (       (    
 ))
           
   
(2) 
 
       (    
 )  
       {
                 (       (    
 ))
           
   
(3) 
 
where      ( )  /      ( )  returns the set of 
confidence scores whose associated action 
affirms / negates   in the sequence of 
observations   
 . 
 
           (    
 )  
                         {
                  (    
 )
           
   
(4) 
 
421
where          ( ) indicates whether or not the 
user has negated the system?s implicit 
confirmation in the sequence of observations   
 . 
One of interesting feature functions is the so-
called baseline feature which exploits the output 
of a baseline system. The following feature 
function emulates the output of the baseline 
system which always selects the top ASR 
hypothesis for the entire dialog: 
 
          (    
 )  
      {
            (           (    
 ))
           
   
(5) 
 
where          ( )  returns the maximum 
confidence score whose action informs   in the 
sequence of observations   
 .    (   )  indicates 
whether or not the maximum score falls into the 
    bin. 
Yet another feature function of this kind is the 
accumulated score which adds up all confidence 
scores associated with inform and affirm and 
subtracts the ones with negation: 
 
         (    
 )  
                  
{
 
 
 
                    (    
 )
                                (    
 )
                                (    
 )
                 
   
(6) 
 
Since we have a partition-based tracker, it is also 
possible to take advantage of its output: 
 
         (    
 )  
                          {
            (    
 ))
           
   
(7) 
 
where    ( )  returns the posterior probability 
of a hypothesis estimated by the partition-based 
tracker. Note that such feature functions as 
         ( ) ,         ( )  and    ( )  are not 
independent of the others defined previously, 
which may cause generative models to produce 
deficient probability distributions. 
It is known that prior information can boost 
the performance (Williams, 2012) if the prior is 
well-estimated. One of advantages of generative 
models is that they provide a natural mechanism 
to incorporate a prior. Discriminative models 
also can exploit a prior by introducing additional 
feature functions: 
 
      (    
 )  
            {
            (            ( ))
           
   
(8) 
 
where           ( ) returns the fraction of 
occurrences of   in the set of true labels. 
If the system cannot process a certain user 
request, it is highly likely that the user change 
his/her goal. The following feature function is 
designed to take care of such cases: 
 
        (    
 )  {
             ( )
           
   (9) 
 
where     ( ) indicates whether or not   is out-
of-coverage. 
As with other log-linear models, we also have 
feature functions for bias: 
 
    (    
 )    
        (    
 )   {
          
            
 
(10) 
 
Note that we have an additional bias term for 
None to estimate an appropriate weight for it. 
Here, None is a special value to indicate that the 
true hypothesis has not yet appeared in the ASR 
N-best lists. Since there are generally a large 
number of values for each concept, the 
probability of the true hypothesis will be very 
small unless the true hypothesis appears on the 
N-best lists. Thus we can make inferences on the 
model very quickly by focusing only on the 
observed hypotheses at the cost of little 
performance degradation. 
 
 
Figure 4: A simplified example of feature extraction for the route concept. It shows the values that each feature 
will have when three consecutive user inputs are given. 
 
422
Proceedings of the 3rd Workshop on Predicting and Improving Text Readability for Target Reader Populations (PITR) @ EACL 2014, pages 84?93,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
An Open Corpus of Everyday Documents for Simplification Tasks
David Pellow and Maxine Eskenazi
Language Technologies Institute, Carnegie Mellon University
Pittsburgh PA USA
dpellow@cs.cmu.edu, max@cs.cmu.edu
Abstract
In recent years interest in creating statisti-
cal automated text simplification systems
has increased. Many of these systems have
used parallel corpora of articles taken from
Wikipedia and Simple Wikipedia or from
Simple Wikipedia revision histories and
generate Simple Wikipedia articles. In this
work we motivate the need to construct a
large, accessible corpus of everyday docu-
ments along with their simplifications for
the development and evaluation of simpli-
fication systems that make everyday doc-
uments more accessible. We present a de-
tailed description of what this corpus will
look like and the basic corpus of every-
day documents we have already collected.
This latter contains everyday documents
from many domains including driver?s li-
censing, government aid and banking. It
contains a total of over 120,000 sentences.
We describe our preliminary work evaluat-
ing the feasibility of using crowdsourcing
to generate simplifications for these docu-
ments. This is the basis for our future ex-
tended corpus which will be available to
the community of researchers interested in
simplification of everyday documents.
1 Introduction
People constantly interact with texts in everyday
life. While many people read for enjoyment, some
texts must be read out of necessity. For example, to
file taxes, open a bank account, apply for a driver?s
license or rent a house, one must read instructions
and the contents of forms, applications, and other
documents. For people with limited reading ability
- whether because they are not native speakers of
the language, have an incomplete education, have
a disability, or for some other reason - the reading
level of these everyday documents can limit acces-
sibility and affect their well-being.
The need to present people with texts that are
at a reading level which is suitable for them has
motivated research into measuring readability of
any given text in order to assess whether automatic
simplification has rendered a more difficult text
into a more readable one. Readability can be mea-
sured using tools which assess the reading level of
a text. We define simplification as the process of
changing a text to lower its reading level without
removing necessary information or producing an
ungrammatical result. This is similar to the def-
inition of (cf. (Zhu et al., 2010)), except that we
avoid defining a specific, limited, set of simplifica-
tion operations. The Related Work section details
research into measures of readability and work on
automatic simplification systems.
We have begun to construct a large, accessi-
ble corpus of everyday documents. This corpus
will eventually contain thousands of these doc-
uments, each having statistics characterising its
contents, and multiple readability measures. Mul-
tiple different simplifications will be collected for
the original documents and their content statistics
and readability measures will be included in the
corpus. This type of large and accessible corpus is
of vital importance in driving development of au-
tomated text simplification. It will provide training
material for the systems as well as a common basis
of evaluating results from different systems.
Thus far, we have collected a basic corpus of ev-
eryday documents from a wide variety of sources.
We plan to extend this basic corpus to create the
much larger and more structured corpus that we
describe here. We have also carried out a pre-
liminary study to evaluate the feasibility of using
crowdsourcing as one source of simplifications in
the extended corpus. We have used Amazon Me-
chanical Turk (AMT) and collected 10 simplifica-
tions each for 200 sentences from the basic cor-
84
pus to determine feasibility, a good experimental
design, quality control of the simplifications, and
time and cost effectiveness.
In the next section we discuss related work rel-
evant to creating and evaluating a large corpus of
everyday documents and their simplifications. In
Section 3 we further demonstrate the need for a
corpus of everyday documents. Section 4 presents
a description of our existing basic corpus. Section
5 describes the details of the extended corpus and
presents our evaluation of the feasibility of using
crowdsourcing to generate human simplifications
for the corpus. Section 6 shows how the extended
corpus will be made accessible. Section 7 con-
cludes and outlines the future work that we will
undertake to develop the extended corpus.
2 Related Work
2.1 Readability Evaluation
Measures of readability are important because
they help us assess the reading level of any doc-
ument, provide a target for simplification systems,
and help evaluate and compare the performance
of different simplification systems. Several mea-
sures of readability have been proposed; DuBay
(2004) counted 200 such measures developed by
the 1980s and the number has grown, with more
advanced automated measures introduced since
then.
Early measures of readability such as the
Flesch-Kincaid grade level formula (Kincaid et
al., 1975) use counts of surface features of the text
such as number of words and number of sentences.
While these older measures are less sophisticated
than more modern reading level classifiers, they
are still widely used and reported and recent work
has shown that they can be a good first approxi-
mation of more complex measures (
?
Stajner et al.,
2012).
More recent approaches use more complicated
features and machine learning techniques to learn
classifiers that can predict readability. For exam-
ple, Heilman et al. (2007) combine a naive Bayes
classifier that uses a vocabulary-based language
model with a k-Nearest Neighbors classifier us-
ing grammatical features and interpolate the two to
predict reading grade level. Feng et al. (2010) and
Franc?ois and Miltsakaki (2012) examine a large
number of possible textual features at various lev-
els and compare SVM and Linear Regression clas-
sifiers to predict grade level. Vajjala and Meurers
(2012) reported significantly higher accuracy on a
similar task using Multi-level Perceptron classifi-
cation.
The above two methods of measuring readabil-
ity can be computed directly using the text of a
document itself. To evaluate the performance of
a simplification system which aims to make texts
easier to read and understand, it is also useful
to measure improvement in individuals? reading
and comprehension of the texts. Siddharthan and
Katsos (2012) recently studied sentence recall to
test comprehension; and Temnikova and Maneva
(2013) evaluated simplifications using the readers?
ability to answer multiple choice questions about
the text.
2.2 Automated Text Simplification Systems
Since the mid-90s several systems have been de-
veloped to automatically simplify texts. Early sys-
tems used hand-crafted syntactic simplification
rules; for example, Chandrasekar et al. (1996),
one of the earliest attempts at automated simpli-
fication. Rule-based systems continue to be used,
amongst others, Siddharthan (2006), Aluisio and
Gasperin (2010), and Bott et al. (2012).
Many of the more recent systems are
statistically-based adapting techniques devel-
oped for statistical machine translation. Zhu
et al. (2010) train a probabilistic model of a
variety of sentence simplification rules using
expectation maximization with a parallel corpus
of aligned sentences from Wikipedia and Simple
Wikipedia. Woodsend and Lapata (2011) present
a system that uses quasi-synchronous grammar
rules learned from Simple Wikipedia edit histo-
ries. They solve an integer linear programming
(ILP) problem to select both which sentences are
simplified (based on a model learned from aligned
Wikipedia-Simple Wikipedia articles) and what
the best simplification is. Feblowitz and Kauchak
(2013) use parallel sentences from Wikipedia
and Simple Wikipedia to learn synchronous tree
substitution grammar rules.
2.3 Corpora for Text Simplification
Presently there are limited resources for statisti-
cal simplification methods that need to train on a
parallel corpus of original and simplified texts. As
mentioned in the previous section, common data
sources are Simple Wikipedia revision histories
and aligned sentences from parallel Wikipedia and
Simple Wikipedia articles. Petersen and Ostendorf
85
(2007) present an analysis of a corpus of 104 orig-
inal and abridged news articles, and Barzilay and
Elhadad (2003) present a system for aligning sen-
tences trained on a corpus of parallel Encyclope-
dia Britannica and Britannica Elementary articles.
Other work generates parallel corpora of original
and simplified texts in languages other than En-
glish for which Simple Wikipedia is not available.
For example, Klerke and S?gaard (2012) built a
sentence-aligned corpus from 3701 original and
simplified Danish news articles, and Klaper et al.
(2013) collected 256 parallel German and simple
German articles.
2.4 Crowdsourcing for Text Simplification
and Corpus Generation
Crowdsourcing uses the aggregate of work per-
formed by many non-expert workers on small
tasks to generate high quality results for some
larger task. To the best of our knowledge crowd-
sourcing has not previously been explored in
detail to generate text simplifications. Crowd-
sourcing has, however, been used to evaluate
the quality of automatically generated simplifica-
tions. Feblowitz and Kauchak (2013) used AMT
to collect human judgements of the simplifica-
tions generated by their system and De Clercq et
al. (2014) performed an extensive evaluation of
crowdsourced readability judgements compared to
expert judgements.
Crowdsourcing has also been used to gener-
ate translations. The recent statistical machine
translation-inspired approaches to automated sim-
plification motivate the possibility of using crowd-
sourcing to collect simplifications. Ambati and
Vogel (2010) and Zaidan and Callison-Burch
(2011) both demonstrate the feasibility of collect-
ing quality translations using AMT. Post et al.
(2012) generated parallel corpora between English
and six Indian languages using AMT.
3 The Need for a Corpus of Everyday
Documents
A high quality parallel corpus is necessary to drive
research in automated text simplification and eval-
uation. As shown in the Related Work section,
most statistically driven simplification systems
have used parallel Wikipedia - Simple Wikipedia
articles and Simple Wikipedia edit histories. The
resulting systems take Wikipedia articles as in-
put and generate simplified versions of those ar-
ticles. While this demonstrates the possibility of
automated text simplification, we believe that a
primary goal for simplification systems should
be to increase accessibility for those with poor
reading skills to the texts which are most impor-
tant to them. Creating a corpus of everyday doc-
uments will allow automated simplification tech-
niques to be applied to texts from this domain. In
addition, systems trained using Simple Wikipedia
only target a single reading level - that of Simple
Wikipedia. A corpus containing multiple different
simplifications at different reading levels for any
given original will allow text simplification sys-
tems to target specific reading levels.
The research needs that this corpus aims to meet
are:
? A large and accessible set of original every-
day documents to:
? provide a training and test set for auto-
mated text simplification
? A set of multiple human-generated simpli-
fications at different reading levels for the
same set of original documents to provide:
? accessible training data for automated
text simplification systems
? the ability to model how the same doc-
ument is simplified to different reading
levels
? An accessible location to share simplifica-
tions of the same documents that have been
generated by different systems to enable:
? comparative evaluation of the perfor-
mance of several systems
? easier identification and analysis of spe-
cific challenges common to all systems
4 Description of the Basic Corpus of
Everyday Documents
We have collected a first set of everyday docu-
ments. This will be extended to generate the cor-
pus described in the following section. The present
documents are heavily biased to the domain of
driving since they include driving test preparation
materials from all fifty U.S. states. This section
presents the information collected about each doc-
ument and its organisation in the basic corpus. The
basic corpus is available at: https://dialrc.
org/simplification/data.html.
86
4.1 Document Fields
Each document has a name which includes
information about the source, contents, and
type of document. For example the name
of the Alabama Driver Manual document is
al dps driver man. The corpus entry for each
document also includes the full title, the document
source (url for documents available online), the
document type and domain, the date retrieved, and
the date added to the corpus. For each document
the number of sentences, the number of words, the
average sentence length, the Flesch-Kincaid grade
level score, and the lexical (L) and grammatical
(G) reading level scores described in Heilman et
al. (2007) are also reported. An example of an en-
try for the Alabama Driver Manual is shown in
Table 1. The documents are split so that each sen-
tence is on a separate line to enable easy align-
ments between the original and simplified versions
of the documents.
Document Name al dps driver man
Full Title Alabama Driver Manual
Document Type Manual
Domain Driver?s Licensing
# Sentences 1,626
# Words 28,404
Avg. # words/sent 17.47
F-K Grade Level 10.21
Reading Level (L) 10
Reading Level (G) 8.38
Source http://1.usa.gov/1jjd4vw
Date Added 10/01/2013
Date Accessed 10/01/2013
Table 1: Example basic corpus entry for Alabama
Driver Manual
4.2 Corpus Statistics
There is wide variation between the different doc-
uments included in the corpus, across documents
from different domains and also for documents
from the same domain. This includes variability
in both document length and reading level. For ex-
ample, the driving manuals range from a lexical
reading level of 8.2 for New Mexico to 10.4 for
Nebraska. Table 2 shows the statistics for the dif-
ferent reading levels for the documents which have
been collected, using the lexical readability mea-
sure and rounding to the nearest grade level. Ta-
ble 3 shows the different domains for which docu-
ments have been collected and the statistics for the
documents in those domains.
Reading Level (L) # Documents # Sentences
4 1 23
5 0 0
6 4 200
7 1 695
8 6 1,869
9 30 36,783
10 54 83,123
11 4 1,457
12 1 461
Table 2: Corpus statistics by lexical reading level
5 Description of an Extended Corpus of
Everyday Documents
To meet the needs described in Section 3 the ba-
sic corpus will be extended significantly. We are
starting to collect more everyday documents from
each of the domains in the basic corpus and to
extend the corpus to other everyday document
domains including prescription instructions, ad-
vertising materials, mandatory educational test-
ing, and operating manuals for common products.
We are also collecting human-generated simpli-
fications for these documents. We will open up
the corpus for outside contributions of more doc-
uments, readability statistics and simplifications
generated by various human and automated meth-
ods. This section describes what the extended cor-
pus will contain and the preliminary work to gen-
erate simplified versions of the documents we
presently have.
5.1 Document Fields
The extended corpus includes both original doc-
uments and their simplified versions. The original
documents will include all the same information as
the basic corpus, listed in Section 4.1. Novel read-
ability measures for each document can be con-
tributed. For each readability measure that is con-
tributed, the name of the measure, document score,
date added, as well as relevant references to the
system used to calculate it will be included.
Multiple simplified versions of each original
document can be contributed. The simplification
for each sentence in the original document will be
on the same line in the simplified document as the
corresponding sentence in the original document.
Each simplified version will include a brief de-
scription of how it was simplified and relevant ref-
erences to the simplification method. As with the
original documents, the date added, optional com-
ments and the same document statistics and read-
87
Domain # Doc-
uments
Avg. #
Sentences
Avg. #
Words
Avg. #
words/sent
Total #
Sentences
Total #
Words
Avg. F-K
Grade Level
Avg. Read-
ability (L)
Avg. Read-
ability (G)
Driver?s
Licensing
60 1927.6 30,352.6 16.1 115,657 1,821,155 9.54 9.6 7.9
Vehicles 3 46.7 1,118.3 22.5 140 3355 13.3 8.2 7.9
Government
Documents
11 150 2,242.8 16.4 1650 24,671 10.5 8.6 8.4
Utilities 5 412.8 8,447.2 21.5 2,064 42,236 13.4 9.8 8.9
Banking 3 158 2,900 17.6 474 8,700 11.4 10.5 8.9
Leasing 4 101 2,386.8 23.8 404 9,547 13.7 9.0 8.7
Government
Aid
10 317.4 5,197.5 17.4 3,174 51,975 10.7 9.2 8.8
Shopping 3 281 5,266.7 19.7 843 15,800 12.2 9.9 9.0
Other 2 102.5 1,634 16.0 205 3268 9.7 8.8 8.2
All 101 1,233.8 19,611.0 17.2 124,611 1,980,707 10.4 9.4 8.2
Table 3: Corpus statistics for the basic corpus documents
ability metrics will be included. Additional read-
ability metrics can also be contributed and docu-
mented.
5.2 Generating Simplifications Using
Crowdsourcing
We conducted a preliminary study to determine
the feasibility of collecting simplifications using
crowdsourcing. We used AMT as the crowdsourc-
ing platform to collect sentence-level simplifica-
tion annotations for sentences randomly selected
from the basic corpus of everyday documents.
5.2.1 AMT Task Details
We collected 10 simplification annotations for
each of the 200 sentences which we posted in
two sets of Human Intelligence Tasks (HITs) to
AMT. Each HIT included up to four sentences and
included an optional comment box that allowed
workers to submit comments or suggestions about
the HIT. Workers were paid $0.25 for each HIT,
and 11 workers were given a $0.05 bonus for sub-
mitting comments which helped improve the task
design and remove design errors in the first itera-
tion of the HIT design. The first set of HITs was
completed in 20.5 hours and the second set in only
6.25 hours. The total cost for all 2000 simplifica-
tion annotations was $163.51 for 592 HITs, each
with up to four simplifications. The breakdown of
this cost is shown in Table 4.
Item Cost
592 HITs $148.00
11 bonuses $0.55
AMT fees $14.96
Total $163.51
Table 4: Breakdown of AMT costs
5.2.2 Quality Control Measures
To ensure quality, we provided a training session
which shows workers explanations, examples, and
counter-examples of multiple simplification tech-
niques. These include lexical simplification, re-
ordering, sentence splitting, removing unneces-
sary information, adding additional explanations,
and making no change for sentences that are al-
ready simple enough. One of the training examples
is the following:
Original Sentence: ?Do not use only parking lights, day or
night, when vehicle is in motion.?
Simplification: ?When your vehicle is moving do not use
only the parking lights. This applies both at night and dur-
ing the day.?
The explanations demonstrated how lexical sim-
plification, sentence splitting, and reordering tech-
niques were used.
The training session also tested workers? abili-
ties to apply these techniques. Workers were given
four test sentences to simplify. Test 1 required lex-
ical simplification. Test 2 was a counter-example
of a sentence which did not require simplifica-
tion. Test 3 required sentence splitting. Test 4 re-
quired either moving or deleting an unclear modi-
fying clause. We chose the test sentences directly
from the corpus and modified them where neces-
sary to ensure that they contained the features be-
ing tested. Workers could take the training session
and submit answers as many times as they wanted,
but could not work on a task without first success-
fully completing the entire session. After complet-
ing the training session once, workers could com-
plete as many HITs as were available to them.
In addition to the training session, we blocked
submissions with empty or garbage answers (de-
fined as those with more than 15% of the words
88
not in a dictionary). We also blocked copy-paste
functions to discourage worker laziness. Workers
who submitted multiple answers that were either
very close to or very far from the original sentence
were flagged and their submissions were manually
reviewed to determine whether to approve them.
Similarity was measured using the ratio of Leven-
shtein distance to alignment length; Levenshtein
distance is a common, simple metric for measur-
ing the edit distance between two strings. The
Levenshtein ratio
(
1?
Levenshtein dist.
alignment length
)
provides
a normalised similarity measure which is robust
to length differences in the inputs. We also asked
workers to rate their confidence in each simplifi-
cation they submitted on a five point scale ranging
from ?Not at all? to ?Very confident?.
5.2.3 Effectiveness of Quality Control
Measures
To determine the quality of the AMT simplifica-
tions, we examine the effectiveness of the qual-
ity control measures described in the previous sec-
tion.
Training: In addition to providing training and
simplification experience to workers who worked
on the task, the training session effectively blocked
workers who were not able to complete it and
spammers. Of the 358 workers who looked at the
training session only 184 completed it (51%) and
we found that no bots or spammers had completed
the training session. Tables 5 and 6 show the per-
formance on the four tests in the training session
for workers who completed the training session
and for those who did not, respectively.
# of workers 181
Avg. # Attempts Test 1 1.1
Avg. # Attempts Test 2 1.5
Avg. # Attempts Test 3 1.6
Avg. # Attempts Test 4 1.4
Table 5: Training statistics for workers who com-
pleted training
# of workers 174
# Completed Test 1 82
# Completed Test 2 47
# Completed Test 3 1
Table 6: Training statistics for workers who did not
complete training
Blocking empty and garbage submissions:
Empty simplifications and cut-paste functions
were blocked using client-side scripts and we did
not collect statistics of how many workers at-
tempted either of these actions. One worker sub-
mitted a comment requesting that we do not block
copy-paste functions. In total only 0.6% of sub-
missions were detected as garbage and blocked.
Manual reviews: We (the first author) reviewed
workers who were automatically flagged five or
more times. We found that this was not an effective
way to detect work to be rejected since there were
many false positives and workers who did more
HITs were more likely to get flagged. None of the
workers flagged for review had submitted simpli-
fications that were rejected.
5.2.4 Evaluating Simplification Quality
To determine whether it is feasible to use crowd-
sourced simplifications to simplify documents for
the extended corpus, we examine the quality of
the simplifications submitted. The quality control
measures described in the previous sections are
designed to ensure that workers know what is
meant by simplification and how to apply some
simplification techniques, to block spammers, and
to limit worker laziness. However, workers were
free to simplify sentences creatively and encour-
aged to use their judgement in applying any tech-
niques that seem best to them.
It is difficult to verify the quality of the simplifi-
cation annotations that were submitted or to deter-
mine how to decide what simplification to chose
as the ?correct? one for the corpus. For any given
sentence there is no ?right? answer for what the
simplification should be; there are many different
possible simplifications, each of which could be
valid. For example, below is an original sentence
taken from a driving manual with two of the sim-
plifications that were submitted for it.
Original Sentence: ?Vehicles in any lane, except the right
lane used for slower traffic, should be prepared to move
to another lane to allow faster traffic to pass.?
Simplification 1: ?Vehicles that are not in the right lane
should be prepared to move to another lane in order to
allow faster traffic to pass.?
Simplification 2: ?Vehicles not in the right lane should be
ready to move to another lane so faster traffic can pass
them. The right lane is for slower traffic.?
There are a number of heuristics that could
be used to detect which simplifications are most
likely to be the best choice to use in the corpus.
The average time for workers to complete one
HIT of up to four simplifications was 3.85 min-
89
utes. This includes the time to complete the train-
ing session during a worker?s first HIT; excluding
this, we estimate the average time per HIT is ap-
proximately 2.75 minutes. Simplifications which
are completed in significantly less time, especially
when the original sentence is long, can be flagged
for review or simply thrown out if there are enough
other simplifications for the sentence.
Workers? confidence in their simplifications can
also be used to exclude simplifications which were
submitted with low confidence (using worker con-
fidence as a quality control filter was explored by
Parent and Eskenazi (2010)). Table 7 shows the
statistics for the worker-submitted confidences.
Again, simplifications with very low confidence
Confidence Level # of answers
1 (Not at all) 9
2 (Somewhat confident) 143
3 (Neutral) 251
4 (Confident) 1030
5 (Very confident) 567
Table 7: Self-assessed worker confidences in their
simplifications
can either be reviewed or thrown out if there are
enough other simplifications for the sentence.
Worker agreement can also be used to detect
simplifications that are very different from those
submitted by other workers. Using the similarity
ratio of Levenshtein distance to alignment length,
we calculated which simplifications had at most
one other simplification with which they have a
similarity ratio above a specific threshold (here re-
ferred to as ?outliers?). Table 8 reports how many
simplifications are outliers while varying the sim-
ilarity threshold. Since there are many different
Threshold 90% 85% 75% 65% 50%
# Outliers 1251 927 500 174 12
Table 8: Number of outlier simplifications with
similarity ratio above the threshold for at most one
other simplification
valid simplifications possible for any given sen-
tence this is not necessarily the best way to de-
tect poor quality submissions. For example, one
of the outliers, using the 50% threshold, was a
simplification of the sentence ?When following a
tractor-trailer, observe its turn signals before try-
ing to pass? which simplified by using a negative
- ?Don?t try to pass ... without ...?. This outlier
was the only simplification of this sentence which
used the negative but it is not necessarily a poor
one. However, the results in Table 7 do show that
there are many simplifications which are similar to
each other, indicating that multiple workers agree
on one simplification. One of these similar sim-
plifications could be used in the corpus, or multi-
ple different possible simplifications could be in-
cluded.
To further verify that usable simplifications can
be generated using AMT the first author manu-
ally reviewed the 1000 simplifications of 100 sen-
tences submitted for the first set of HITs. We
judged whether each simplification was grammat-
ical and whether it was a valid simplification. This
is a qualitative judgement, but simplifications were
judged to be invalid simplifications if they had sig-
nificant missing or added information compared
to the original sentence or added significant ex-
tra grammatical or lexical complexity for no ap-
parent reason. The remaining grammatical, valid
simplifications were judged as more simple, neu-
tral, or less simple than the original for each of the
following features: length, vocabulary, and gram-
matical structure. The results of this review are
shown in Table 9. These results show that approx-
imately 15% of the simplifications were ungram-
matical or invalid, further motivating the need to
use the other features, such as worker agreement
and confidence, to automatically remove poor sim-
plifications.
5.2.5 Extending the Corpus Using
Crowdsourcing
The preliminary work undertaken demonstrates
that it is feasible to quickly collect multiple sim-
plifications for each sentence relatively inexpen-
sively. We have also presented an evaluation of
the quality of the crowdsourced simplifications
and several methods of determining which sim-
plifications could be used in the extended corpus.
More work is still needed to determine the most
cost effective way of getting simplification results
that are of sufficient quality to use without gather-
ing overly redundant simplifications for each sen-
tence. Additionally, simplifications of more sen-
tences are needed to assess improvements in read-
ing level since the reading level measures we use
are not accurate for very short input texts.
90
Un-
grammatical
Invalid
(excludes
ungrammatical)
Simpler
vocabulary
Less
simple
vocabulary
Equivalent
vocabulary
Grammatically
simpler
Less
grammatically
simple
Equivalent
grammar
Longer Shorter Same
length
35 122 383 21 596 455 21 524 99 537 364
Table 9: Manual evaluation of 1000 AMT simplifications. Numbers of simplifications with each feature.
6 Contributing to & Accessing the
Corpus
6.1 Contributing to the Extended Corpus
The following items can be contributed to the cor-
pus: original everyday copyright-free documents,
manual or automated simplifications of the orig-
inal documents (or parts of the documents), and
readability scores for original or simplified docu-
ments.
Original documents submitted to the corpus can
be from any domain. Our working definition of an
everyday document is any document which peo-
ple may have a need to access in their everyday
life. Examples include government and licensing
forms and their instructions, banking forms, pre-
scription instructions, mandatory educational test-
ing, leasing and rental agreements, loyalty pro-
gram sign-up forms and other similar documents.
We excluded Wikipedia pages because we found
that many article pairs actually had few parallel
sentences. Documents should be in English and of
North American origin to avoid dialect-specific is-
sues.
Hand generated or automatically generated sim-
plifications of everyday documents are also wel-
come. They should be accompanied the informa-
tion detailed in Section 5.1. The document statis-
tics listed in Sections 4 and 5 will be added for
each simplified document.
Readability scores can be contributed for any of
the documents.They should also include the infor-
mation detailed in Section 5.1 and pertinent infor-
mation about the system that generated the scores.
6.2 Accessing the Extended Corpus
The extended corpus will be made publicly acces-
sible at the same location as the basic corpus. The
names and statistics of each of the documents will
be tabulated and both the original and simplified
documents, and their statistics, will be available to
download. Users will submit their name or organi-
zational affiliation along with a very brief descrip-
tion of how they plan to use the data. This will
allow us to keep track of how the corpus is be-
ing used and how it could be made more useful to
those researching simplification.
The goal of this corpus is to make its contents as
accessible as possible. However, many of the orig-
inal documents from non-governmental sources
may not be freely distributed and will instead be
included under a data license, unlike the remain-
der of the corpus and the simplifications
1
.
7 Conclusions & Future Work
In this paper we have given the motivation for cre-
ating a large and publicly accessible corpus of ev-
eryday documents and their simplifications. This
corpus will advance research into automated sim-
plification and evaluation for everyday documents.
We have already collected a basic corpus of every-
day documents and demonstrated the feasibility of
collecting large numbers of simplifications using
crowdsourcing. We have defined what information
the extended corpus will contain and how contri-
butions can be made to it.
There is significantly more work which must be
completed in the future to create an extended cor-
pus which meets the needs described in this paper.
There are three tasks that we plan to undertake in
order to complete this corpus: we will collect sig-
nificantly more everyday documents; we will man-
age a large crowdsourcing task to generate simpli-
fications for thousands of the sentences in these
documents; and we will create a website to enable
access and contribution to the extended simplifi-
cation corpus. By making this work accessible we
hope to motivate others to contribute to the corpus
and to use it to advance automated text simplifica-
tion and evaluation techniques for the domain of
everyday documents.
Acknowledgments
The authors would like to thank the anonymous
reviewers for their detailed and helpful feedback
and comments on the paper.
1
Thanks to Professor Jamie Callan for explaining some
of the issues with including these types of documents in our
dataset.
91
References
Sandra Aluisio and Caroline Gasperin. 2010. Foster-
ing digital inclusion and accessibility: The porsim-
ples project for simplification of portuguese texts.
In Proc. of the NAACL HLT 2010 Young Investi-
gators Workshop on Computational Approaches to
Languages of the Americas, pages 46?53. Associa-
tion for Computational Linguistics.
Vamshi Ambati and Stephan Vogel. 2010. Can crowds
build parallel corpora for machine translation sys-
tems? In Proc. of the NAACL HLT 2010 Workshop
on Creating Speech and Language Data with Ama-
zon?s Mechanical Turk, pages 62?65. Association
for Computational Linguistics.
Regina Barzilay and Noemie Elhadad. 2003. Sentence
alignment for monolingual comparable corpora. In
Proc. of the 2003 Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP ?03,
pages 25?32. Association for Computational Lin-
guistics.
Stefan Bott, Horacio Saggion, and David Figueroa.
2012. A hybrid system for spanish text simplifi-
cation. In Proc. of the Third Workshop on Speech
and Language Processing for Assistive Technolo-
gies, pages 75?84. Association for Computational
Linguistics.
R. Chandrasekar, Christine Doran, and B. Srinivas.
1996. Motivations and methods for text simplifica-
tion. In Proc. of the 16th Conference on Compu-
tational Linguistics - Volume 2, COLING ?96, pages
1041?1044. Association for Computational Linguis-
tics.
Orph?ee De Clercq, Veronique Hoste, Bart Desmet,
Philip van Oosten, Martine De Cock, and Lieve
Macken. 2014. Using the crowd for readabil-
ity prediction. Natural Language Engineering,
FirstView:1?33.
William H. DuBay. 2004. The Princi-
ples of Readability. Costa Mesa, CA:
Impact Information, http://www.impact-
information.com/impactinfo/readability02.pdf.
Dan Feblowitz and David Kauchak. 2013. Sentence
simplification as tree transduction. In Proc. of the
Second Workshop on Predicting and Improving Text
Readability for Target Reader Populations, pages 1?
10. Association for Computational Linguistics.
Lijun Feng, Martin Jansche, Matt Huenerfauth, and
No?emie Elhadad. 2010. A comparison of fea-
tures for automatic readability assessment. In Col-
ing 2010: Posters, pages 276?284. Coling 2010 Or-
ganizing Committee.
Thomas Franc?ois and Eleni Miltsakaki. 2012. Do nlp
and machine learning improve traditional readability
formulas? In Proc. of the First Workshop on Predict-
ing and Improving Text Readability for target reader
populations, pages 49?57. Association for Compu-
tational Linguistics.
Michael Heilman, Kevyn Collins-Thompson, Jamie
Callan, and Maxine Eskenazi. 2007. Combin-
ing lexical and grammatical features to improve
readability measures for first and second language
texts. In HLT-NAACL 2007: Main Proceedings,
pages 460?467. Association for Computational Lin-
guistics.
J. Peter Kincaid, Robert P. Fishburne Jr., Richard L.
Rogers, and Brad S. Chissom. 1975. Derivation
of new readability formulas (automated readability
index, fog count and flesch reading ease formula)
for navy enlisted personnel. Technical report, Naval
Technical Training Command, Millington Tn.
David Klaper, Sarah Ebling, and Martin Volk. 2013.
Building a german/simple german parallel corpus
for automatic text simplification. In Proc. of the
Second Workshop on Predicting and Improving Text
Readability for Target Reader Populations, pages
11?19. Association for Computational Linguistics.
Sigrid Klerke and Anders S?gaard. 2012. Dsim, a dan-
ish parallel corpus for text simplification. In Proc.
of the Eighth Language Resources and Evaluation
Conference (LREC 2012), pages 4015?4018. Euro-
pean Language Resources Association (ELRA).
Gabriel Parent and Maxine Eskenazi. 2010. Toward
better crowdsourced transcription: Transcription of
a year of the let?s go bus information system data.
In SLT, pages 312?317. IEEE.
Sarah E Petersen and Mari Ostendorf. 2007. Text sim-
plification for language learners: a corpus analysis.
In Proc. of Workshop on Speech and Language Tech-
nology for Education, pages 69?72.
Matt Post, Chris Callison-Burch, and Miles Osborne.
2012. Constructing parallel corpora for six indian
languages via crowdsourcing. In Proc. of the Sev-
enth Workshop on Statistical Machine Translation,
pages 401?409. Association for Computational Lin-
guistics.
Advaith Siddharthan and Napoleon Katsos. 2012. Of-
fline sentence processing measures for testing read-
ability with users. In Proc. of the First Workshop
on Predicting and Improving Text Readability for
target reader populations, pages 17?24. Association
for Computational Linguistics.
Advaith Siddharthan. 2006. Syntactic simplification
and text cohesion. Research on Language and Com-
putation, 4(1):77?109.
Irina Temnikova and Galina Maneva. 2013. The c-
score ? proposing a reading comprehension metrics
as a common evaluation measure for text simplifica-
tion. In Proc. of the Second Workshop on Predicting
and Improving Text Readability for Target Reader
Populations, pages 20?29. Association for Compu-
tational Linguistics.
92
Sowmya Vajjala and Detmar Meurers. 2012. On im-
proving the accuracy of readability classification us-
ing insights from second language acquisition. In
Proc. of the Seventh Workshop on Building Educa-
tional Applications Using NLP, pages 163?173. As-
sociation for Computational Linguistics.
Sanja
?
Stajner, Richard Evans, Constantin Orasan, , and
Ruslan Mitkov. 2012. What can readability mea-
sures really tell us about text complexity? In Proc.
of the Workshop on Natural Language Processing
for Improving Textual Accessibility (NLP4ITA).
Kristian Woodsend and Mirella Lapata. 2011. Wik-
isimple: Automatic simplification of wikipedia arti-
cles. In Proc. of the Twenty-Fifth AAAI Conference
on Artificial Intelligence, pages 927?932.
Omar F. Zaidan and Chris Callison-Burch. 2011.
Crowdsourcing translation: Professional quality
from non-professionals. In Proc. of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies, pages
1220?1229. Association for Computational Linguis-
tics.
Zhemin Zhu, Delphine Bernhard, and Iryna Gurevych.
2010. A monolingual tree-based translation model
for sentence simplification. In Proc. of the 23rd In-
ternational Conference on Computational Linguis-
tics (Coling 2010), pages 1353?1361.
93
