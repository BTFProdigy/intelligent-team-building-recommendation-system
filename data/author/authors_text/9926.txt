Event Detection and Summarization in Weblogs with Temporal Collocations 
Chun-Yuan Teng and Hsin-Hsi Chen 
Department of Computer Science and Information Engineering 
National Taiwan University 
Taipei, Taiwan 
{r93019, hhchen}@csie.ntu.edu.tw 
Abstract 
 
This paper deals with the relationship between weblog content and time. With the proposed temporal mutual information, we analyze 
the collocations in time dimension, and the interesting collocations related to special events. The temporal mutual information is 
employed to observe the strength of term-to-term associations over time. An event detection algorithm identifies the collocations that 
may cause an event in a specific timestamp. An event summarization algorithm retrieves a set of collocations which describe an event. 
We compare our approach with the approach without considering the time interval. The experimental results demonstrate that the 
temporal collocations capture the real world semantics and real world events over time. 
 
1. 
2. 
Introduction 
Compared with traditional media such as online news 
and enterprise websites, weblogs have several unique 
characteristics, e.g., containing abundant life experiences 
and public opinions toward different topics, highly 
sensitive to the events occurring in the real world, and 
associated with the personal information of bloggers. 
Some works have been proposed to leverage these 
characteristics, e.g., the study of the relationship between 
the content and bloggers? profiles (Adamic & Glance, 
2005; Burger & Henderson, 2006; Teng & Chen, 2006), 
and content and real events (Glance, Hurst & Tornkiyo, 
2004; Kim, 2005; Thelwall, 2006; Thompson, 2003). 
In this paper, we will use temporal collocation to 
model the term-to-term association over time.  In the past, 
some useful collocation models (Manning & Sch?tze, 
1999) have been proposed such as mean and variance, 
hypothesis test, mutual information, etc. Some works 
analyze the weblogs from the aspect of time like the 
dynamics of weblogs in time and location (Mei, et al, 
2006), the weblog posting behavior (Doran, Griffith & 
Henderson, 2006; Hurst, 2006), the topic extraction (Oka, 
Abe & Kato, 2006), etc. The impacts of events on social 
media are also discussed, e.g., the change of weblogs after 
London attack (Thelwall, 2006), the relationship between 
the warblog and weblogs (Kim, 2005; Thompson, 2003), 
etc. 
This paper is organized as follows. Section 2 defines 
temporal collocation to model the strength of term-to-term 
associations over time.  Section 3 introduces an event 
detection algorithm to detect the events in weblogs, and 
an event summarization algorithm to extract the 
description of an event in a specific time with temporal 
collocations. Section 4 shows and discusses the 
experimental results.  Section 5 concludes the remarks. 
Temporal Collocations 
We derive the temporal collocations from Shannon?s 
mutual information (Manning & Sch?tze, 1999) which is 
defined as follows (Definition 1). 
Definition 1 (Mutual Information) The mutual 
information of two terms x and y is defined as: 
)()(
),(log),(),(
yPxP
yxPyxPyxI =  
where P(x,y) is the co-occurrence probability of x and y, 
and P(x) and P(y) denote the occurrence probability of x 
and y, respectively. 
Following the definition of mutual information, we 
derive the temporal mutual information modeling the 
term-to-term association over time, and the definition is 
given as follows.  
 Definition 2 (Temporal Mutual Information) Given 
a timestamp t and a pair of terms x and y, the temporal 
mutual information of x and y in t is defined as: 
)|()|(
)|,(log)|,()|,(
tyPtxP
tyxPtyxPtyxI =
where P(x,y|t) is the probability of co-occurrence of terms 
x and y in timestamp t, P(x|t) and P(y|t) denote the 
probability of occurrences of x and y in timestamp t, 
respectively. 
To measure the change of mutual information in time 
dimension, we define the change of temporal mutual 
information as follows. 
Definition 3 (Change of Temporal Mutual 
Information) Given time interval [t1, t2], the change of 
temporal mutual information is defined as: 
12
12
21
)|,()|,(),,,(
tt
tyxItyxIttyxC ?
?=  
where C(x,y,t1,t2) is the change of temporal mutual 
information of terms x and y in time interval [t1, t2], I(x,y| 
t1) and I(x,y| t2) are the temporal mutual information in 
time t1 and t2, respectively. 
3. Event Detection 
Event detection aims to identify the collocations 
resulting in events and then retrieve the description of 
events. Figure 1 sketches an example of event detection. 
The weblog is parsed into a set of collocations. All 
collocations are processed and monitored to identify the 
plausible events.  Here, a regular event ?Mother?s day? 
and an irregular event ?Typhoon Chanchu? are detected.  
The event ?Typhoon Chanchu? is described by the words  
 
 
 
 
 
 
 
 
 
 
 
 
Figure 1: An Example of Event Detection
?Typhoon?, ?Chanchu?, ?2k?, ?Eye?, ?Path? and 
?chinaphillippine?.  
The architecture of an event detection system includes 
a preprocessing phase for parsing the weblogs and 
retrieving the collocations; an event detection phase 
detecting the unusual peak of the change of temporal 
mutual information and identifying the set of collocations 
which may result in an event in a specific time duration; 
and an event summarization phase extracting the 
collocations related to the seed collocations found in a 
specific time duration. 
The most important part in the preprocessing phase is 
collocation extraction. We retrieve the collocations from 
the sentences in blog posts. The candidates are two terms 
within a window size. Due to the size of candidates, we 
have to identify the set of tracking terms for further 
analysis. In this paper, those candidates containing 
stopwords or with low change of temporal mutual 
information are removed. 
In the event detection phase, we detect events by 
using the peak of temporal mutual information in time 
dimension.  However, the regular pattern of temporal 
mutual information may cause problems to our detection. 
Therefore, we remove the regular pattern by seasonal 
index, and then detect the plausible events by measuring 
the unusual peak of temporal mutual information. 
If a topic is suddenly discussed, the relationship 
between the related terms will become higher. Two 
alternatives including change of temporal mutual 
information and relative change of temporal mutual 
information are employed to detect unusual events. Given 
timestamps t1 and t2 with temporal mutual information 
MI1 and MI2, the change of temporal mutual information 
is calculated by (MI2-MI1). The relative change of 
temporal mutual information is calculated by (MI2-
MI1)/MI1. 
For each plausible event, there is a seed collocation, 
e.g., ?Typhoon Chanchu?. In the event description 
retrieval phase, we try to select the collocations with the 
highest mutual information with the word w in a seed 
collocation. They will form a collocation network for the 
event.  Initially, the seed collocation is placed into the 
network.  When a new collocation is added, we compute 
the mutual information of the multiword collocations by 
the following formula, where n is the number of 
collocations in the network up to now. 
?= n iMInInformatioMutualMultiwo  
If the multiword mutual information is lower than a 
threshold, the algorithm stops and returns the words in the 
collocation network as a description of the event.  Figure 
2 sketches an example.  The collocations ?Chanchu?s 
path?, ?Typhoon eye?, and ?Chanchu affects? are added 
into the network in sequence based on their MI. 
We have two alternatives to add the collocations to 
the event description. The first method adds the 
collocations which have the highest mutual information 
as discussed above. In contrast, the second method adds 
the collocations which have the highest product of mutual 
information and change of temporal mutual information. 
 
 
 
 
 
 
Figure 2: An Example of Collocation network 
4. 
4.1. 
Experiments and Discussions 
Temporal Mutual Information versus 
Mutual Information 
In the experiments, we adopt the ICWSM weblog data 
set (Teng & Chen, 2007; ICWSM, 2007). This data set 
collected from May 1, 2006 through May 20, 2006 is 
about 20 GB. Without loss of generality, we use the 
English weblog of 2,734,518 articles for analysis. 
To evaluate the effectiveness of time information, we 
made the experiments based on mutual information 
(Definition 1) and temporal mutual information 
(Definition 2). The former called the incremental 
approach measures the mutual information at each time 
point based on all available temporal information at that 
time. The latter called the interval-based approach 
considers the temporal mutual information in different 
time stamps.  Figures 3 and 4 show the comparisons 
between interval-based approach and incremental 
approach, respectively, in the event of Da Vinci Code.   
We find that ?Tom Hanks? has higher change of 
temporal mutual information compared to ?Da Vinci 
Code?. Compared to the incremental approach in Figure 4, 
the interval-based approach can reflect the exact release 
date of ?Da Vinci Code.? 
 rd
=i 1 4.2. Evaluation of Event Detection 
We consider the events of May 2006 listed in 
wikipedia1 as gold standard. On the one hand, the events 
posted in wikipedia are not always complete, so that we 
adopt recall rate as our evaluation metric.  On the other 
hand, the events specified in wikipedia are not always 
discussed in weblogs.  Thus, we search the contents of 
blog post to verify if the events were touched on in our 
blog corpus. Before evaluation, we remove the events 
listed in wikipedia, but not referenced in the weblogs. 
 
 
 
 
 
 
 
 
 
 
 
Figure 3: Interval-based Approach in Da Vinci Code  
 
 
 
 
 
 
 
 
Figure 4: Incremental Approach in Da Vinci Code 
gure 5 sketches the idea of evaluation.  The left side 
of t s figure shows the collocations detected by our event 
dete tion system, and the right side shows the events 
liste  in wikipedia.  After matching these two lists, we 
can find that the first three listed events were correctly 
identified by our system.  Only the event ?Nepal Civil 
War? was listed, but not found. Thus, the recall rate is 
75% in this case. 
 
 
 
 
 
 
 
Figure 5: Evaluation of Event Detection Phase 
As discussed in Section 3, we adopt change of 
temporal mutual information, and relative change of 
temporal mutual information to detect the peak. In Figure 
6, we compare the two methods to detect the events in 
weblogs. The relative change of temporal mutual 
information achieves better performance than the change 
of temporal mutual information. 
                                                     
1 http://en.wikipedia.org/wiki/May_2006 
Table 1 and Table 2 list the top 20 collocations based 
on these two approaches, respectively. The results of the 
first approach show that some collocations are related to 
the feelings such as ?fell left? and time such as ?Saturday 
night?. In contrast, the results of the second approach 
show more interesting collocations related to the news 
events at that time, such as terrorists ?zacarias 
moussaoui? and ?paramod mahajan.? These two persons 
were killed in May 3. Besides, ?Geena Davis? got the 
golden award in May 3. That explains why the 
collocations detected by relative change of temporal 
mutual information are better than those detected by 
change of temporal mutual information. 
-20
-15
-10
-5
0
5
10
1 3 5 7 9 11 13 15 17 19
Time (day)
M
ut
ua
l i
nf
or
m
at
io
n
Da-Vinci Tom Hanks
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 6: Performance of Event Detection Phase 
-15
-10
-5
0
5
10
1 3 5 7 9 11 13 15 17 19
Time (day)
M
ut
ua
l i
nf
or
m
at
io
n
Da-Vinci Tom Hanks
Collocations CMI Collocations CMI 
May 03 9276.08 Current music 1842.67
Illegal immigrants 5833.17 Hate studying 1722.32
Feel left 5411.57 Stephen Colbert 1709.59
Saturday night 4155.29 Thursday night 1678.78
Past weekend 2405.32 Can?t believe 1533.33
White house 2208.89 Feel asleep 1428.18
Red sox 2208.43 Ice cream 1373.23
Album tool 2120.30 Oh god 1369.52
Sunday morning 2006.78 Illegalimmigration 1368.12
16.56
f 
CMI
32.50
31.63
29.09
28.45
28.34
28.13Sunday night 1992.37 Pretty cool 13
Table 1: Top 20 collocations with highest change o
temporal mutual information 
Collocations CMI Collocations 
casinos online 618.36 Diet sodas 
zacarias moussaoui 154.68 Ving rhames 
Tsunami warning 107.93 Stock picks 
Conspirator zacarias 71.62 Happy hump 
Artist formerly 57.04 Wong kan 
Federal  
Jury 
41.78 Sixapartcom 
movabletype Wed 3 39.20 Aaron echolls 27.48
Pramod mahajan 35.41 Phnom penh 25.78
BBC  
Version 
35.21 Livejournal 
sixapartcom 
23.83  Fi
hi
c
dGeena davis 33.64 George yeo 20.34
Table 2: Top 20 collocations with highest relative change 
of mutual information 
4.3. Evaluation of Event Summarization 
As discussed in Section 3, we have two methods to 
include collocations to the event description. Method 1 
employs the highest mutual information, and Method 2 
utilizes the highest product of mutual information and 
change of temporal mutual information. Figure 7 shows 
the performance of Method 1 and Method 2. We can see 
that the performance of Method 2 is better than that of 
Method 1 in most cases. 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 7: Overall Performance of Event Summarization 
The results of event summarization by Method 2 are 
shown in Figure 8. Typhoon Chanchu appeared in the 
Pacific Ocean on May 10, 2006, passed through 
Philippine and China and resulted in disasters in these 
areas on May 13 and 18, 2006.  The appearance of the 
typhoon Chanchu cannot be found from the events listed 
in wikipedia on May 10.  However, we can identify the 
appearance of typhoon Chanchu from the description of 
the typhoon appearance such as ?typhoon named? and 
?Typhoon eye.  In addition, the typhoon Chanchu?s path 
can also be inferred from the retrieved collocations such 
as ?Philippine China? and ?near China?. The response of 
bloggers such as ?unexpected typhoon? and ?8 typhoons? 
is also extracted.   
 
 
 
 
 
 
 
 
 
 
Figure 8: Event Summarization for Typhoon Chanchu 
5. Concluding Remarks 
This paper introduces temporal mutual information to 
capture term-term association over time in weblogs. The 
extracted collocation with unusual peak which is in terms 
of relative change of temporal mutual information is 
selected to represent an event.  We collect those 
collocations with the highest product of mutual 
information and change of temporal mutual information 
to summarize the specific event.  The experiments on 
ICWSM weblog data set and evaluation with wikipedia 
event lists at the same period as weblogs demonstrate the 
feasibility of the proposed temporal collocation model 
and event detection algorithms. 
Currently, we do not consider user groups and 
locations. This methodology will be extended to model 
the collocations over time and location, and the 
relationship between the user-preferred usage of 
collocations and the profile of users. 
Acknowledgments 
Research of this paper was partially supported by 
National Science Council, Taiwan (NSC96-2628-E-002-
240-MY3) and Excellent Research Projects of National 
Taiwan University (96R0062-AE00-02). 
References 
Adamic, L.A., Glance, N. (2005). The Political 
Blogosphere and the 2004 U.S. Election: Divided 
They Blog. In: Proceedings of the 3rd International 
Workshop on Link Discovery, pp. 36--43. 
Burger, J.D., Henderson J.C. (2006). An Exploration of 
Observable Features Related to Blogger Age. In: 
Proceedings of AAAI 2006 Spring Symposium on 
Computational Approaches to Analysing Weblogs, pp. 
15--20. 
Doran, C., Griffith, J., Henderson, J. (2006). Highlights 
from 12 Months of Blogs. In: Proceedings of AAAI 
2006 Spring Symposium on Computational 
Approaches to Analysing Weblogs, pp. 30--33. 
Glance, N., Hurst, M., Tornkiyo, T. (2004). Blogpulse: 
Automated Trend Discovery for Weblogs. In: 
Proceedings of WWW 2004 Workshop on the 
Weblogging Ecosystem: Aggregation, Analysis, and 
Dynamics. 
Hurst, M. (2006). 24 Hours in the Blogosphere. In: 
Proceedings of AAAI 2006 Spring Symposium on 
Computational Approaches to Analysing Weblogs, pp. 
73--77. 
ICWSM (2007). http://www.icwsm.org/data.html 
Kim, J.H. (2005). Blog as an Oppositional Medium? A 
Semantic Network Analysis on the Iraq War Blogs. In: 
Internet Research 6.0: Internet Generations. 
 
Manning, C.D., Sch?tze, H. (1999). Foundations of 
Statistical Natural Language Processing, The MIT 
Press, London England. 
Mei, Q., Liu, C., Su, H., Zhai, C. (2006). A Probabilistic 
Approach to Spatiotemporal Theme Pattern Mining on 
Weblogs. In: Proceedings of the 15th International 
Conference on World Wide Web, Edinburgh, Scotland, 
pp. 533--542. 
Oka, M., Abe, H., Kato, K. (2006). Extracting Topics 
from Weblogs Through Frequency Segments. In: 
Proceedings of WWW 2006 Annual Workshop on the 
Weblogging Ecosystem: Aggregation, Analysis, and 
Dynamics. 
Teng, C.Y., Chen, H.H. (2006). Detection of Bloggers? 
Interest: Using Textual, Temporal, and Interactive 
Features. In: Proceeding of IEEE/WIC/ACM 
International Conference on Web Intelligence, pp. 
366--369. 
Teng, C.Y., Chen, H.H. (2007). Analyzing Temporal 
Collocations in Weblogs. In: Proceeding of 
International Conference on Weblogs and Social 
Media, 303--304. 
Thelwall, M. (2006). Blogs During the London Attacks: 
Top Information Sources and Topics. In: Proceedings 
of 3rd Annual Workshop on the Weblogging 
Ecosystem: Aggregation, Analysis and Dynamics. 
Thompson, G. (2003). Weblogs, Warblogs, the Public 
Sphere, and Bubbles. Transformations, 7(2). 
Building parallel corpora for eContent professionals 
M. Gavrilidou, P. Labropoulou, E. Desipri, V. Giouli, V. Antonopoulos, S. Piperidis 
Institute for Language and Speech Processing  
Epidavrou & Artemidos 6 
151 25 Maroussi, Greece. 
{maria, penny, elina, voula, vantonop, spip} @ilsp.gr 
 
Abstract 
This paper reports on completed work carried 
out in the framework of the INTERA project, 
and specifically, on the production of 
multilingual resources (LRs) for eContent 
purposes. The paper presents the methodology 
adopted for the development of the corpus 
(acquisition and processing of the textual 
data), discusses the divergence of the initial 
assumptions from the actual situation met 
during this procedure, and concludes with a 
summarization of the problems attested which 
undermine the viability of multilingual parallel 
corpora construction.  
1 Introduction 
INTERA (Integrated European language data 
Repository Area, Contract 22076Y2C2DMAL2) is 
an EU-funded project within the eContent 
framework, aiming at  
? building an integrated European Language 
Resources (LRs) area by connecting existing 
data centers at regional, national and 
international level, and 
? at proposing "ways and techniques for LRs 
packaging to make it a profitable and attractive 
task to eContent professionals"; as an 
application of this task, the production of 
multilingual resources, namely parallel corpora 
and multilingual terminologies extracted from 
these, is undertaken (INTERA Technical 
Annex). 
This paper focuses on the second aim of the 
project, presenting the work carried out in the area 
of parallel corpus production, identifying the steps 
followed in this process, in order to point out the 
problematic areas involved in the task and suggest 
ways of encompassing them. 
2 Methodology and specifications  
The process usually followed in the LRs 
production involves the following tasks: (a) 
identification of user needs and requirements, (b) 
specifications for the selection, construction and 
packaging of the LRs, (c) identification of potential 
sources, (d) construction of the LRs per se, (e) 
promotion and distribution of the LRs. 
Given that INTERA is an eContent project, the 
target user group defined by the Technical Annex 
of the project was eContent professionals and 
users; furthermore, it was decided that the LRs to 
be produced (which would be of interest to this 
group) would be parallel corpora and multilingual 
terminological lists. Finally, the most important 
objective of the LRs production was the definition 
of a business model which would be attractive to 
the abovementioned target group. 
The following sections discuss the actual steps 
taken for the implementation of these 
requirements.  
The target group of eContent players addressed 
by the project has been further defined as 
consisting of professionals involved with the:  
? production of digital content (authors or 
publishers)  
? Globalization, Internationalization, Localiz-
ation and Translation (GILT) processes, and  
? development of Human Language 
Technology (HLT) software, ranging from 
multilingual information retrieval and 
extraction tools, to content management and 
Computer-Assisted Translation or Machine 
Translation solutions. 
The next step concerned the identification of 
user needs and requirements on the basis of the 
professionals? working habits and processes. This 
was achieved by exploiting the results of a number 
of previous initiatives to roadmap the state-of-the-
art in multilingual LRs, in combination with new 
initiatives undertaken in the framework of the 
project and targeted to the eContent world.  
The surveys conducted in the framework of the 
ENABLER project (Maegaard et al 2003, 
Gavrilidou & Desipri 2003) provided insights as to 
the existence and availability of different types of 
LRs, language demand, domains of interest, 
standards, etc. Although ENABLER focused on 
the LRs developer?s point of view, a number of 
valuable results were elicited. Other surveys, such 
as those conducted by ELRA and its distribution 
agency ELDA aiming at determining the needs of 
users with respect to available and potentially 
available LRs (http://www.elra.info/), or surveys 
available over the Internet through the sites of 
international organizations such as LISA and IDC 
or consultancy firms (http://www.globalsight.com, 
LISA 2001, LISA/AIIM 2001, LISA/OSCAR 
2003) shed a light as to the availability of 
resources and relevant tools.  
The information elicited from these surveys was 
coupled by a study of the activities of the eContent 
professionals as regards LRs, conducted in the 
framework of INTERA (Gavrilidou et al 2004) 
through the circulation of a questionnaire 
distributed to potential users, as well as through 
personal contacts with a number of actors in the 
relevant fields. The main areas of the study 
concerned the types of LRs the eContent 
professionals are interested in, domains and 
languages of interest, and, most important, policies 
concerning the way they acquire, use and exploit 
LRs and tools. 
The study of the target group yielded the 
following specifications: 
? domains: it is obvious that eContent users are 
more interested in specialized domains than in 
general language resources; moreover, the 
survey results showed health/medicine, 
tourism, education, law, automotive industry 
and IT/telecommunications, as being the 
prevailing ones. In the framework of the 
INTERA project, however, we decided to 
focus on the prevailing domains as long as 
they promote multilingual and multicultural 
content. The selected domains are: health, 
tourism, education and law, which correspond 
to the predominant digital activities, namely, 
eTourism, eHealth, eLearning, eGovernment 
and eCommerce. 
? languages: the focus of eContent and the needs 
of the users pointed towards the less widely 
spoken languages, including Balkan and 
Central and Eastern European languages (i.e 
the languages of the new EU countries).  
The project aims at the construction of a 
multilingual parallel corpus of 12 million 
words in total. The ideal scenario for the 
intended application of term extraction would 
be that of having a corpus with a source or 
pivot language and translations of the same 
texts in a number of target languages; 
however, given that the project aims at 
proposing realistic solutions to be adopted in 
the future by prospective LRs creators, real-life 
drawbacks should be taken into account; 
therefore, the limitations in the availability of 
existing resources (see section 3.1) dictated the 
decision to collect resources for four pairs of 
languages: Greek-English, Bulgarian-English, 
Slovene-English and Serbian-English. 
The specifications for the processing of the 
corpus have been based on the requirements of its 
intended application, which is the extraction of 
terminology, and involve the following tasks: 
? alignment of the texts: for the specific 
application purposes, alignment at sentence 
level has been deemed sufficient; however, the 
quality of the output is considered crucial; 
therefore, automatic processing is followed by 
human validation by language experts; 
? external and internal structural annotation: the 
minimal requirements include segmentation at 
sentence level for the alignment task and 
metadata information that will be required for 
the distribution and re-use of the corpus; 
? linguistic processing: below-Part of Speech 
(PoS) tagging and lemmatization is the 
minimum information required for the 
automatic term extraction task. 
To ensure re-usability of the collected and 
processed material, compliance with the following 
internationally accredited standards was decided: 
? the aligned material conforms to the TMX 
standard (Translation Memory eXchange, 
http://www.lisa.org/tmx/), which is XML-
compliant. Being a vendor-neutral, open 
standard for storing and exchanging translation 
memories created by Computer Aided 
Translation (CAT) and localization tools, 
TMX standard was identified as a requirement 
for the eContent professionals. It allows easier 
exchange of translation memory data between 
tools and/or translation vendors with little or 
no loss of critical data during the process; 
? for the external annotation, the IMDI metadata 
schema (IMDI, Metadata Elements for Session 
Descriptions, Version 3.0.4, Sept. 2003, 
http://www.mpi.nl/world/ISLE/schemas/schem
as_frame.html) has been selected; the internal 
structural annotation adheres to the XCES 
standard, i.e. the XML version of the Corpus 
Encoding Standard (XCES, 
http://www.cs.vassar.edu/XCES/ and CES, 
http://www.cs.vassar.edu/CES/CES1-0.html). 
? the linguistic annotation of the texts also 
adheres to the XCES standard, which 
incorporates the EAGLES guidelines for 
morphosyntactic annotation 
(http://www.ilc.cnr.it/EAGLES96/home.html). 
3 Corpus construction 
3.1 Text collection 
In order to construct the parallel corpus, the first 
step consisted in the identification of potential 
sources, i.e. existing parallel corpora and, 
alternatively or additionally, textual material that 
could be used for the creation from scratch of the 
INTERA corpus. 
Previous surveys (see section 2) that identify 
existing LRs as well as a search over the Internet 
attested the scarcity of available resources in the 
selected languages and domains, and so, the idea of 
re-using existing corpora was abandoned in favour 
of the construction of a new corpus from scratch. 
The identification process of potential sources 
had to take into consideration the following 
requirements: 
? to obtain texts from a variety of sources of 
interest to the eContent society, 
? to ensure that the material was free of 
Intellectual Property Rights problems, either 
through the arrangement of specific 
agreements or by obtaining them from public 
sources. 
The ideal candidates, in this respect, mainly 
consist of texts available over the Internet, 
provided by organizations/institutions that wish to 
make their own material available in more than one 
language, such as international organizations (e.g. 
United Nations, European Union, World Health 
Organization, Non-Governmental Organizations, 
etc.), multinational companies, companies with 
activities outside their own country (e.g. data 
describing company profiles & activities, product 
catalogues, etc.), public administration services 
(e.g. regarding bilateral agreements, regulations for 
immigrants, etc.), news agencies (targeting 
international broadcasting or for foreign language 
audience within their own country), official 
national government sites, national tourism 
organizations, etc. In all the above cases, the 
material consists of either web content per se (i.e. 
mainly bilingual web sites, rarely trilingual or 
quadrilingual) or of texts (official documents, 
technical reports, etc.) included in the web sites. 
A more careful investigation, however, of web 
texts showed that although Internet is rapidly 
becoming multilingual, it is not yet parallel, 
especially as regards the languages involved in the 
project: most international bodies include original 
and translated texts but only in the more widely 
spoken languages. Moreover, a closer inspection of 
web texts that "seem" parallel, on the basis of 
structural similarities (e.g. similar size, paragraph 
segmentation, possible "anchors", such as list 
enumerators, etc.) showed that only sporadic parts 
of them were parallel. More problems arise from 
the fact that texts may contain large parts of 
foreign language material (e.g. EU regulations that 
include amendments to previous regulations by 
including the replacement text of specific 
paragraphs in all EU languages). 
Given the above observations, cooperation with 
other data centers, with proven expertise in the 
area of LRs production for the specific project 
languages was sought; this would ensure content 
quality of the corpus, both during the selection (i.e. 
native speakers are better qualified to recognize 
true parallel material) and the encoding and 
validation processes, especially as regards the 
alignment validation and the linguistic processing. 
ILSP remains responsible for the construction of 
the Greek-English corpus, the collection and 
harmonization of the four subcorpora, the 
linguistic processing of the English texts and the 
addition of the IMDI metadata. 
3.2 Text processing 
Depending on the source that provided the 
original material (e.g. web site content, publishing 
house, translation company, etc.), different 
processing was required in order to arrive at the 
desired format adhering to the specifications set by 
the INTERA project; such as, indicatively: 
? conversion of the original PDF/RTF/HTML 
etc. files into the format required by the 
various tools (tokenizer, aligner, tagger), 
? cleanup of the texts from unwanted material 
(e.g. tables, figures, foreign language material, 
etc.) 
? re-structuring of the original monolingual texts 
from the TMX file, when the source was the 
output of a Translation Memory, 
? manual or semi-automatic annotation of 
metadata. 
 
Each language team undertook the processing of 
the collected material (i.e. alignment and human 
validation, structural and linguistic annotation 
without human validation), using their own tools, 
thus ensuring that no time is lost over training with 
new tools and that the required language-
dependent tools (especially taggers) used in the 
project are the most appropriate ones. The material 
to be delivered, however, at the end of all 
processes must be conformant to the selected 
standards. 
The intervention of ILSP takes place only at the 
end of this process, with the purpose of validating 
the conformance of the results and of harmonizing 
any problematic issues. The most important point 
of this process is the linguistic annotation and, 
specifically, the harmonization of the different 
tagsets used. In conformance with the 
methodology adopted in the project, i.e. of re-using 
existing material, whenever possible, with the least 
possible interventions, so as to ensure time and 
cost efficiency, it was decided to re-use only 
existing tools for each language, without making 
any modifications to the tools themselves but only 
conversion(s) of their output. Therefore, the task of 
harmonizing the output with regard to the 
morphosyntactic tags employed by each tagger is 
the last stage of the procedure, where all tagsets are 
mapped to one, based on the EAGLES guidelines.  
4 Conclusions  
In this paper, we described the methodology 
followed in the construction of a multilingual 
parallel corpus; this task has been interpreted as a 
test application endeavor in the process of defining 
a business model for the LRs production. The 
effort was to identify gaps and shortcomings in the 
process usually employed by LRs producers (or 
users who might wish to create their own LRs) and 
to suggest ways of remedying them. Our findings 
include: 
? problems faced during the acquisition phase: 
although an increasing supply of raw data (e.g. 
over Internet) and tools capable of exploiting 
this data (e.g. web crawlers that can identify 
and download texts in a given language) is 
attested, there is also a need for the 
enhancement of these tools with more 
intelligent techniques (e.g. incorporation of 
alignment techniques during the acquisition 
process in order to spot potential parallel texts, 
identification and mark-up of large foreign 
language excerpts), 
? problems faced during the processing phase: 
in order to enhance the LRs production effort, 
the re-use of existing tools is considered 
crucial. It is true that an increasing number of 
tools are available for text processing; 
however, this is oriented mainly towards the 
major languages. Moreover, information 
concerning the existence, availability and 
operation of existing tools is not easy to locate 
? a gap that the other pillar of INTERA tries to 
remedy through the building of an integrated 
European Language Resources area. 
Additionally, tools must be enhanced with 
respect to two directions: improvement of the 
tools themselves (e.g. more robust alignment 
techniques) and interoperability of all relevant 
tools currently used at different phases of 
processing. The issue of interoperability is 
closely related with the issue of standards. The 
promotion and deployment of existing 
standards as well as the creation of new 
standards, when these are lacking, is important 
to ensure viability and re-use of LRs, given the 
cost of their production. 
References  
Gavrilidou, M., E. Desipri. 2003. Final Version of 
the Survey, ENABLER Deliverable 2.1.  
Gavrilidou, M. E. Desipri, P. Labropoulou, S. 
Piperidis, N. Calzolari, M. Monachini & C. 
Soria. 2004. Technical specifications for the 
selection and encoding of multilingual resources, 
INTERA (Integrated European language data 
Repository Area), Deliverable 5.1. 
IMDI, Metadata Elements for Session 
Descriptions, Version 3.0.4, Sept. 2003. 
INTERA ? eContent  2002 Integrated European 
languages data Repository Area, Technical 
Annex. 
LISA. 2001. The LISA Globalization Strategies 
Awareness Survey. 
LISA/AIIM. 2001. The Black Hole in the Internet: 
LISA/AIIM Globalization Survey. 
LISA/OSCAR. 2003. Translation Memory Survey. 
Maegaard, B., K. Choukri, V. Mapelli, M. 
Nikkhou & C. Povlsen. 2003. Language 
resources-Industrial needs, ENABLER 
Deliverable 4.2. 
 
 
Proceedings of the EACL 2009 Workshop on Language Technology and Resources for Cultural Heritage,
Social Sciences, Humanities, and Education ?LaTeCH ? SHELT&R 2009, pages 35?42,
Athens, Greece, 30 March 2009. c?2009 Association for Computational Linguistics
A web-enabled and speech-enhanced parallel corpus 
of Greek - Bulgarian cultural texts 
 
 
Voula Giouli 
Institute for Language & Speech 
Processing Athens, Greece 
voula@ilsp.gr 
Nikos Glaros 
Institute for Language & Speech 
Processing Athens, Greece 
nglaros@ilsp.gr 
 
Kiril Simov 
Institute for Parallel Processing, 
BAS, Sofia, Bulgaria 
kivs@bultreebank.or 
Petya Osenova 
Institute for Parallel Processing, 
BAS, Sofia, Bulgaria 
petya@bultreebank.org 
 
Abstract 
This paper reports on completed work carried 
out in the framework of an EU-funded project 
aimed at (a) developing a bilingual collection 
of cultural texts in Greek and Bulgarian, (b) 
creating a number of accompanying resources 
that will facilitate study of the primary texts 
across languages, and (c) integrating a system 
which aims to provide web-enabled and 
speech-enhanced access to digitized bilingual 
Cultural Heritage resources. This simple user 
interface, which incorporates advanced search 
mechanisms, also offers innovative accessibil-
ity for visually impaired Greek and Bulgarian 
users. The rationale behind the work (and the 
relative resource) was to promote the com-
parative study of the cultural heritage of the 
two countries. 
1 Introduction 
The document describes a bilingual Greek (EL) 
and Bulgarian (BG) collection of literary and 
folklore texts along with the metadata that were 
deemed necessary for the efficient management 
and retrieval of the textual data. Section 2 out-
lines the project aims that guided selection and 
annotation of the texts, whereas Section 3 pre-
sents the primary data that comprise the bilingual 
textual collection and the methodology adopted 
for collecting them. Section 4 elaborates on the 
metadata scheme that has been implemented to 
describe the primary data and the linguistic anno-
tation tailored to facilitate search and retrieval at 
the document, phrase or word level. This scheme 
is compliant to widely accepted standards so as 
to ensure reusability of the resource at hand. Sec-
tion 5 presents the Language Technologies (LT) 
deployed in the project elaborating on the Greek 
and the Bulgarian text processing tools, and dis-
cusses the LT methods that have been (a) ex-
ploited in the course of the project to facilitate 
the web-interface construction and (b) integrated 
in the search and retrieval mechanisms to im-
prove the system performance. Finally, Section 6 
describes the main components of the web inter-
face and the way various features are exploited to 
facilitate users? access to the data. In the last sec-
tion, we present conclusions and future work. 
2 Project description 
The project aims at highlighting cultural re-
sources that, as of yet, remain non-exploited to 
their greatest extent, and at creating the neces-
sary infrastructure with the support of LT with a 
view to promoting the study of cultural heritage 
of the eligible neighboring areas and  raising 
awareness about their common cultural identity. 
To serve these objectives, the project had a con-
crete target, that is, the creation of a textual col-
lection and of accompanying material that would 
be appropriate for the promotion and study of the 
cultural heritage of the neighboring areas in 
Greece and Bulgaria (Thrace and the neighboring 
Smolyan, Blagoevgrad, Kardjali, Khaskovo ar-
eas), the focus being on literature, folklore and 
language. To this end, the main activities within 
the project life-cycle were to: 
? record and roadmap the literary production 
of the afore mentioned areas spanning from 
the 19th century till the present days along 
with written records on folk culture and folk-
tales from the eligible areas. These should 
form a pool of candidate texts from which 
35
the most appropriate for the project objec-
tives could be selected; 
? record and roadmap existing translations of 
literary works in both languages to serve for 
the creation of the parallel corpus; 
? select textual material representative of the 
two cultures, and thus, suitable for their 
comparative study; 
? digitize the selected (printed) material to a 
format suitable for long-term preservation; 
? collect meta-texts relevant to the selected 
literary and folklore texts, that is, texts about 
the literary works, biographies of the se-
lected authors, criticism, etc.; these comprise 
part of the accompanying material 
? document the data with any information 
deemed necessary for its preservation and 
exploitation, catering for their interrelation 
so as to highlight their common features and 
allow unified access to the whole set alng 
text types / genres and languages; 
? extract bilingual glossaries from the primary 
collection of literary and folklore texts also 
accounted for as accompanying material; the 
project caters for the extraction of EL and 
BG terms and names of Persons and Loca-
tions and their translation equivalents in the 
other language; 
? make the primary resource along with the 
accompanying material (meta-texts and glos-
saries) publicly available over the internet to 
all interested parties, ranging from the re-
search community to laypersons, school stu-
dents and people interested in finding out 
more about the particular areas; 
? facilitate access to the material that wouldn?t 
be hampered by users? computer literacy 
and/or language barriers. To cater for the lat-
ter, the web interface would be as simple as 
possible ? yet functional ? and the data 
should be available in both languages (Greek 
and Bulgarian) plus in English. 
3 The bilingual Greek ? Bulgarian Cul-
tural Corpus 
Along with the aforementioned lines, the col-
lection comprises parallel EL ? BG literary and 
folklore texts. The main specifications for the 
Greek - Bulgarian Cultural Corpus (GBCC) crea-
tion were: 
? to build a bilingual resource that could be 
used as a means to study cultural similarities 
and/or differences between the neighboring 
areas of Greece and Bulgaria the focus being 
on literature, folklore and folktales;  
? to provide a representative sample of (a) lit-
erature written by authors from Thrace -that 
is from the entire area of Thrace- or about 
Thrace, spanning between the 19th century - 
today, (b) folklore texts about Thrace, that 
would normally reflect cultural as well as 
linguistic elements either shared by the two 
people or unique to each culture, and (c) 
folktales and legends from Thrace, the latter 
being the intermediate between literature and 
folklore. 
In order to gather the candidate texts and au-
thors for such a collection we exploited both 
printed and digitized sources, i.e., (on-line and 
printed) anthologies of Bulgarian, Greek or Bal-
kan literature, digital archives, web resources and 
library material. The outcome of this extensive 
research was a wealth of literary works including 
titles by the most prominent authors in Bulgaria 
and Greece. The selection of the authors, who 
would finally participate in GBCC, was based on 
the following criteria: (a) author's impact to 
Greek or Bulgarian literature respectively; and 
(b) author's contribution to his county's folk 
study or other major sectors such as journalism 
and education.  
Additionally, to ensure corpus ?representa-
tiveness? to some extend, we tried to include the 
full range of the literary texts (poetry, fiction, 
short stories) and in proportion to the literary 
production with respect to the parameters of 
place, time and author. To this end, we think we 
have avoided biases and the corpus models all 
language varieties spoken in the areas and at dif-
ferent periods. 
Moreover, the "inner" content characteristics 
of texts were used as the basic criteria for text 
selection. To this end, we chose texts which 
demonstrate the two people's cultural similarities 
and affinity along with each author's most impor-
tant and representative works. Beyond the above, 
the availability of a translation in the other lan-
guage and IPR issues also influenced text selec-
tion. 
The collection of the primary data currently 
comprises of (135) literary works, (70) BG (Bul-
garian) and 65 EL (Greek). Moreover, (30) BG 
folk texts and 30 EL folk texts along with (25) 
BG folktales and 31 EL folktales were added in 
order to build a corpus as balanced as possible 
and representative of each country's culture. In 
terms of tokens, the corpus amounts to 700,000 
36
in total (circa 350,000 tokens per language): the 
literature part is about 550,000 tokens, whereas, 
the folklore and legend sub-corpus is about 
150,000 tokens. 
Moreover, to cater for the project requirement 
that the corpus should be bilingual, available 
translations of the primary EL ? BG literary 
works were also selected to form the parallel lit-
erary corpus. Additionally, an extensive transla-
tion work was also carried out by specialized 
translators where applicable (folklore texts and 
folktales). 
The collection covers EL and BG literary pro-
duction dating from the 19th century till the pre-
sent day, and also texts (both literary or folklore) 
that are written in the dialect(s) used in the eligi-
ble areas. This, in effect, is reflected in the lan-
guage varieties represented in the textual collec-
tion that range from contemporary to non-
contemporary, and from normal to dialectical or 
even mixed language. 
Finally, the collection of primary data was 
also coupled with accompanying material (con-
tent metadata) for each literary work (literary 
criticism) and for each author (biographical in-
formation, list of works, etc.). Along with all the 
above, texts about the common cultural elements 
were also included. 
4 Corpus Annotation 
After text selection, digitization and extended 
manual validation (where appropriate) were per-
formed. Normalization of the primary data was 
kept to a minimum so as to cater, for example, 
for the conversion from the Greek polytonic to 
the monotonic encoding system. Furthermore, to 
ensure efficient content handling and retrieval 
and also to facilitate access to the resource at 
hand via the platform that has been developed, 
metadata descriptions and linguistic annotations 
were added across two pillars: (a) indexing and 
retrieval, and (b) further facilitating the compara-
tive study of textual data. To this end, metadata 
descriptions and linguistic annotations compliant 
with internationally accepted standards were 
added to the raw material. The metadata scheme 
deployed in this project is compliant with inter-
nationally accredited standards with certain 
modifications that cater for the peculiarities of 
the data. 
More specifically, the metadata scheme im-
plemented in this project builds on XCES, the 
XML version of the Corpus Encoding Standard 
(XCES, http://www.cs.vassar.edu/XCES/ and 
CES, http://www.cs.vassar.edu/CES/CES1-
0.html), which has been proposed by EAGLES 
(http://www.ilc.cnr.it/EAGLES96/home.html) 
and is compliant with the specifications of the 
Text Encoding Initiative (http://www.tei-c.org, 
Text Encoding Initiative (TEI Guidelines for 
Electronic Text Encoding and Interchange). 
From the total number of elements proposed by 
these guidelines, the annotation of the parallel 
corpus at hand has been restricted to the recogni-
tion of structural units at the sentence level, 
which is the minimum level required for the 
alignment and term extraction processes. That 
means that the requirements of CES Level 1 con-
formance are met; as regards CES Level 2 the 
requirements (but not the recommendations) are 
also met, and from CES Level 3 requirements, 
annotation for sentence boundaries is met. 
Additionally, metadata elements have been 
deployed which encode information necessary 
for text indexing with respect to text title, author, 
publisher, publication date, etc. (bibliographical 
information) and for the classification of each 
text according to text type/genre and topic, the 
latter being applicable to folklore texts and folk 
tales. Classification of folklore texts is based on 
the widely accepted Aarne-Thompson classifica-
tion system (Aarne, 1961). 
To this end, to assure documentation com-
pleteness, and facilitate the inter-relation among 
primary data and the accompanying material (bi-
ographies, criticism, etc) the documentation 
scheme has been extended accordingly. The 
aforementioned metadata descriptions are kept 
separately from the data in an xml header that is 
to be deployed by the web interface for search 
and retrieval purposes. 
The external structural annotation (including 
text classification) of the corpus also adheres to 
the IMDI metadata scheme (IMDI, Metadata 
Elements for Session Descriptions, Version 
3.0.4, Sept. 2003). Adaptations proposed specifi-
cally concerning Written Language Resources 
have been taken into account. IMDI metadata 
elements for catalogue descriptions (IMDI, 
Metadata Elements for Catalogue Descriptions, 
Version 2.1, June 2001) were also taken into ac-
count to render the corpus compatible with exist-
ing formalisms (ELRA, and LDC). This type of 
metadata descriptions was added manually to the 
texts. 
To further enhance the capabili-
ties/functionalities of the final application, ren-
dering, thus the collection a useful resource to 
prospective users and researchers, further annota-
37
tions at various levels of linguistic analysis were 
integrated across two pillars: (a) efficient index-
ing and retrieval; and (b) further facilitating the 
comparative study of textual data by means of 
bilingual glossaries which were constructed 
semi-automatically, and via the visualization of 
aligned parallel texts.  
Text processing at the monolingual level com-
prises the following procedures: (a) handling and 
tokenization, (b) Part-of-Speech (POS) tagging 
and lemmatization, (c) surface syntactic analysis, 
(d) indexing with terms/keywords and 
phrases/Named Entities (NEs) pertaining to the 
types Location (LOC) and Person (PER). 
Annotations at these levels were added semi-
automatically, by deploying existing generic 
Natural Language Processing (NLP) tools that 
were developed for the languages at hand, 
whereas extensive and intensive validations were 
performed via several ways. Indeed, although the 
tools deployed have reported to achieve high ac-
curacy rates in the domains/genres they were 
intended for, the specific nature of the data led to 
a significant reduction. To this end, half of the 
annotations were checked manually. After the 
identification of the errors in this part of the cor-
pus, we have performed a manual check in the 
second part of the corpus only for these cases 
which were recognized as errors during the vali-
dation of the first part. For some of the cases 
relevant constraints in the systems were written, 
which automatically find places where some 
rules were not met. Tools customization was also 
performed by adding new rules applicable for the 
language varieties to be handled, and also by ex-
tending/modifying the resources used (word and 
name lists, etc.).  
Finally, alignment of parallel texts (primary 
source documents and their translations) has also 
been performed at both sentence and phrase 
level. As expected, poems posited the major dif-
ficulties due the fuzziness in identifying sentence 
boundaries, and alignments at the phrase level 
were favored instead. 
5 Language Technologies 
In what follows the Greek and Bulgarian Text 
Processing Components will be described. 
5.1 The Greek pipe-line 
In the case of the Greek data, text processing 
was applied via an existing pipeline of shallow 
processing tools for the Greek language. These 
include: 
? Handling and tokenization; following com-
mon practice, the Greek tokenizer makes use 
of a set of regular expressions, coupled with 
precompiled lists of abbreviations, and a set 
of simple heuristics (Papageorgiou et al, 
2002) for the recognition of word and sen-
tence boundaries, abbreviations, digits, and 
simple dates.  
? POS-tagging and lemmatization; a tagger 
that is based on Brill's TBL architecture 
(Brill, 1997), modified to address peculiari-
ties of the Greek language (Papageorgiou et 
al., 2000) was used in order to assign mor-
phosyntactic information to tokenized words. 
Furthermore, the tagger uses a PAROLE-
compliant tagset of 584 different part-of-
speech tags. Following POS tagging, lemmas 
are retrieved from a Greek morphological 
lexicon. 
? Surface syntactic analysis; the Greek chun-
ker is based on a grammar of 186 rules 
(Boutsis et al, 2000) developed for the 
automatic recognition of non-recursive 
phrasal categories: adjectives, adverbs, 
prepositional phrases, nouns, verbs (chunks) 
(Papageorgiou et al, 2002). 
? Term extraction; a Greek Term Extractor 
was used for spotting terms and idiomatic 
words (Georgantopoulos, Piperidis, 2000). 
Term Extractor's method proceeds in three 
pipelined stages: (a) morphosyntactic anno-
tation of the domain corpus, (b) corpus pars-
ing based on a pattern grammar endowed 
with regular expressions and feature-
structure unification, and (c) lemmatization. 
Candidate terms are then statistically evalu-
ated with an aim to skim valid domain terms 
and lessen the overgeneration effect caused 
by pattern grammars (hybrid methodology). 
Named Entity Recognition was then per-
formed using MENER (Maximum Entropy 
Named Entity Recognizer), a system compatible 
with the ACE (Automatic Content Extraction) 
scheme, catering for the recognition and classifi-
cation of the following types of NEs: person 
(PER), organization (ORG), location (LOC) and 
geopolitical entity (GPE) (Giouli et al, 2006). 
5.2 Bulgarian Tools 
In the processing of the Bulgarian part of the 
corpus we have been using generic language 
technology tools developed for Bulgarian. Here 
is the list of tools that we have used. They are 
38
implemented within the CLaRK System (Simov 
et al 2001) via:  
Tokenization, Morphosyntactic tagging, 
Lemmatization; Tokenization is implemented as 
a hierarchy of tokenizers within the CLaRK sys-
tem. Morphosyntactic tagging is done on the ba-
sis a morphological lexicon which covers the 
grammatical information of about 100 000 lex-
emes (1 600 000 word forms); a gazetteers of 
about 25000 names and 1500 abbreviations. We 
are using the BulTreeBank tagset, which is a 
more specialized version of Multext-east tagset. 
The disambiguation is done in two steps. Ini-
tially, a rule-based module solves the sure cases 
for which manual rules can be written. Then, for 
the next step, a neural-network-based disam-
biguator is being exploited (Simov and Osenova 
2001). Lemmatization is implemented as rules 
which convert each word form in the lemma. The 
rules are assigned to the word forms in the lexi-
con. This ensures very high level of accuracy. 
Partial Grammars have also been constructed 
for Sentence splitting, Named-entity recognition, 
and Chunking. 
5.3 Alignments 
To facilitate the comparative study of parallel 
documents, source texts were automatically 
aligned with their translations. Alignments at the 
sentence level were performed semi-
automatically by means of the ILSP Aligner, 
which is a language independent tool that uses 
surface linguistic information coupled with in-
formation about possible unit delimiters depend-
ing on the level at which the alignment is sought. 
The resulting translation equivalents were stored 
in files conformant to the internationally accred-
ited TMX standard (Translation Memory eX-
change, http://www.lisa.org/tmx/), which is 
XML-compliant, vendor-neutral open standard 
for storing and exchanging translation memories 
created by Computer Aided Translation (CAT) 
and localization tools. 
Moreover, terms pertaining to the folklore do-
main as well as names of Persons and Locations 
identified in the EL - BG parallel texts were 
semi-automatically aligned. The outcome of the 
process of text alignment at below the sentence 
level was then validated manually. 
5.4 Tools Customization and metadata 
harmonization 
As it has already been stated, the tools that 
were deployed for the linguistic processing are 
generic ones that were initially developed for 
different text types/genres. Moreover, the data at 
hand posed another difficulty that is, coping with 
older/obsolete language usage. In fact, some of 
the literary works were written in the 19th cen-
tury or the beginning of 20th century, and their 
language reflects the writing standards of the 
corresponding period. 
Therefore, as it was expected, the overall per-
formance of the afore-mentioned tools was lower 
than the one reported for the texts these tools 
were initially trained for. 
To this end, performance at POS-tagging level 
dropped from 97% to 77% for the Greek data 
since no normalization of the primary data was 
performed. On the other hand, the BG morpho-
logical analyzer coverage, whose benchmark per-
formance is 96% 
dropped to 92 % on poems and folktales and to 
94% on literary texts and legends. 
The reason was that the language of processed 
literary texts and legends came normalized from 
the sources, while the poems and folktales kept 
some percentage of archaic or dialect words. 
Thus, additionally to the guesser, a post POS 
processing was performed on the unknown 
words. Moreover, the accuracy of the neural 
network disambiguator and the rule-based one 
was 97 %. i.e. the same as for other applications. 
Processing at the levels of chunks and NEs were 
even lower. Within the project we had to tune the 
tools to the specific language types, such as dia-
chronically remote texts and domain specific 
texts (folklore). Also, some words with higher 
distribution in the target regions appear in some 
of the works. In order to deal with them we had 
to extend the used lexicons, to create a guesser 
for the unknown words and add new rules to the 
chunk grammar to handle some specific word 
order within the texts. 
Additionally, the deployment of tools that are 
specific to each language and compatible with 
completely distinct annotation standards brought 
about the issue of metadata harmonization. To 
this end, although the Greek tools were devel-
oped to confront to the afore-mentioned annota-
tion standards, this was not the case for Bulgar-
ian. The first encoding scheme followed the 
BulTreeBank morphological and chunk 
annotation scheme. Afterwards, the information 
was transferred into the project 
scheme in order to be consistent with the Greek 
data and applicable for web representation. As a 
result, the morphosyntactic features of the BG 
tagset, which is a more specialized version of the 
39
Multext-East tagset were mapped onto the rela-
tive PAROLE tags. 
6 The web interface 
All the data collected (being the primary liter-
ary or folklore texts or meta-documents, etc.) 
along with their translations, the multi-layered 
annotations, and the resulting glossaries were 
integrated in a database platform that was devel-
oped to serve as a content management system. 
Being the backbone of that platform, the meta-
data material facilitates the interlinking of similar 
documents, and the access to the primary data 
via the web. To this end, a specially designed 
web site was developed to satisfy the needs of 
end-users (the general public and the special 
groups of researchers and other scientists). The 
website features a trilingual interface (Greek, 
Bulgarian, English) as well as advanced search 
and retrieval mechanisms on the entire bilingual 
content or a user-specified part of it. The users 
can perform combined searches by author name, 
title, genre, etc. Furthermore, they can search for 
single keywords/wordforms or for two word-
forms that can be a user-specified number of 
words apart from each other. Searches by lemma 
and/or by phrase have been also implemented. 
The latter rely on a matcher, which tries to link 
the query word(s) with the stored lem-
mas/wordforms. Additionally, a stemmer for 
Greek and Bulgarian has been used for the on-
line stemming of queries, which will then be 
matched with the already stemmed corpus. When 
all the above fails, fuzzy matching techniques are 
being employed, facilitating, thus, effective 
query expansion functionality. Finally, apart 
from wordforms and lemmas, the collection can 
also be queried for morphosyntactic tags or any 
combination thereof; results, then, come in the 
form of concordances and statistics (frequency 
information), hence the relative document(s) can 
also be retrieved. Moreover, users can search the 
whole corpus or define a sub-corpus based on the 
classification and annotation parameters accom-
panying each text, thus, creating sub-corpora of a 
specific author, or belonging to a specific genre, 
text type, domain, time period, etc. 
In addition, the web interface lets the users to 
simultaneously view on screen both Greek and 
Bulgarian texts, aligned and in parallel,, so that 
to become acquainted with the comparative as-
pects of the two languages or perform specific 
linguistic, lexicographic or translation tasks. Al-
ternatively, the user can consult the bilingual 
glossary of terms and the aligned list of NEs. The 
latter is often very interesting, especially with 
respect to Location entities, since transliteration 
is usually non-adequate.  
The design of the web interface effectively 
blends simplicity and advanced functionality so 
that to fully support the intended usage scenarios 
(comparative study of literary and folklore texts 
equally by specialists, laymen or students, lan-
guage and/or literary teaching and learning, lexi-
cographic projects, etc.). Finally, the web inter-
face has been enhanced by integrating last gen-
eration of synthetic speech technology for both 
Greek and Bulgarian. This speech-enhanced user 
interface (S. Raptis et al 2005), offers innovative 
web accessibility for blind and vision impaired 
Greek and Bulgarian users as well as for other 
users who use speech as their preferable modal-
ity to information access. The key-feature of this 
web-speech technology is that it lets users to in-
teract with the underlying system; so that they 
can hear only the portions of a specific web page 
they are interested in, being able at the same time 
to navigate through the entire web site and visit 
only the web pages of their choice. 
7 Conclusions and future work  
We have described work targeted at the promo-
tion and study of the cultural heritage of the 
cross-border regions of Greece ? Bulgaria, the 
focus been on literature, folklore and language of 
the two people, by means of modern and techno-
logically advanced platforms. To this end, a digi-
tal collection of literary and folklore texts has 
been compiled along with accompanying mate-
rial selected from various (online and printed 
sources), which is integrated into a platform with 
advanced search and retrieval mechanisms. 
However, the cultural value of the bilingual cul-
tural Greek-Bulgarian corpus goes beyond the 
border areas that it was intended for, because it 
shows the similarities and the differences be-
tween the two neighboring countries. More spe-
cifically, it can be used for supporting the acqui-
sition of the other language in both countries. 
Also, it can be explored for comparing the cul-
tural and social attitudes in diachronic depth and 
genre variety. Apart from the usages from a hu-
manities point of view, the corpus can become a 
good base for testing taggers, parsers and align-
ers. It would especially challenge the processing 
of the regional dialects, the language of poems, 
and the language of non-contemporary works. 
40
Future work is being envisaged in the following 
directions: extending the corpus with more texts, 
and respectively the glossaries ? with more 
terms, adding more layers of linguistic analysis 
(predicate-argument structure, etc.), and further 
enhance search and retrieval with the construc-
tion and deployment of an applicable thesaurus. 
 
Acknowledgments 
We would like to thank the anonymous review-
ers for useful suggestions and comments. Most 
of the work presented in this paper was done in 
the framework of a project that was funded under 
the Community Initiative Programme INTER-
REG III A / PHARE CBC Greece ? Bulgaria. 
The project was implemented by the Institute for 
Language and Speech Processing (ILSP, 
www.ilsp.gr) and a group of academics and re-
searchers from the Sofia University St. Kliment 
Ohridski (www.uni-sofia.bg). 
References  
Antti Aarne. 1961. The Types of the Folktale: A Clas-
sification and Bibliography. Translated and 
Enlarged by Stith Thompson. 2nd rev. ed. Helsinki: 
Suomalainen Tiedeakatemia / FF Communications. 
Sotiris Boutsis, Prokopis Prokopidis, Voula Giouli 
and Stelios Piperidis. 2000. A Robust Parser for 
Unrestricted Greek Tex. In Proceedings of the 2nd 
Language and Resources Evaluation Conference, 
467-473, Athens, Greece. 
Michel G?n?reux. 2007. Cultural Heritage Digital 
Resources: From Extraction to Querying, Lan-
guage Technology for Cultural Heritage Data 
(LaTeCH 2007), Workshop at ACL 2007, June 
23rd?30th 2007, Prague, Czech Republic. 
Byron Georgantopoulos and Stelios Piperidis, 2000. 
Term-based Identification of Sentences for Text 
Summarization. In Proceedings of LREC2000 
Voula Giouli, Alexis Konstandinidis, Elina Desypri, 
Harris Papageorgiou. 2006. Multi-domain Multi-
lingual Named Entity Recognition: Revisiting & 
Grounding the resources issue. In Proceedings of 
LREC 2006. 
IMDI, Metadata Elements for Catalogue Descriptions, 
Version 2.1, June 2001 
IMDI, Metadata Elements for Session Descriptions, 
Version 3.0.4, Sept. 2003. 
Harris Papageorgiou, L. Cranias, Stelios 
Piperidis1994. Automatic alignment in parallel 
corpora. In Proceedings of ACL 1994. 
Harris Papageorgiou, Prokopis Prokopidis, Voula 
Giouli, Iasonas Demiros, Alexis Konstantinidis, 
and Stelios Piperidis. 2002. Multi-level XML-based 
Corpus Annotation. Proceedings of the 3nd Lan-
guage and Resources Evaluation Conference. 
Harris Papageorgiou, Prokopis Prokopidis, Voula 
Giouli, and Stelios Piperidis. 2000. A Unified POS 
Tagging Architecture and its Application to Greek. 
In Proceedings of the 2nd Language and Resources 
Evaluation Conference, Athens, Greece, pp 1455-
1462. 
Stelios Piperidis. 1995. Interactive corpus based 
translation drafting tool. In ASLIB Proceedings 
47(3), March 1995. 
Spyros Raptis, I. Spais and P. Tsiakoulis. 2005.  A 
Tool for Enhancing Web Accessibility: Synthetic 
Speech and Content Restructuring?. In Proc. HCII 
2005: 11th International Conference on Human-
Computer Interaction, 22-27 July, Las Vegas, Ne-
vada, USA. 
Kiril Simov, Z. Peev, M. Kouylekov, A. Simov, M. 
Dimitrov, and A. Kiryakov. 2001. CLaRK - an 
XML-based System for Corpora Development. 
Corpus Linguistics 2001 Conference. pp 558-560. 
Kiril Simov, and Petya Osenova. A Hybrid System for 
MorphoSyntactic Disambiguation in Bulgarian. In: 
Proc. of the RANLP 2001 Conference, Tzigov 
Chark, Bulgaria, 5-7 September 2001. pages 288-
290. 
Ren? Witte, Thomas Gitzinger, Thomas Kappler, and 
Ralf Krestel. 2008. A Semantic Wiki Approach to 
Cultural Heritage Data Management. Language 
Technology for Cultural Heritage Data (LaTeCH 
2008), Workshop at LREC 2008, June 1st, 2008, 
Marrakech, Morocco. 
41
Proceedings of the 10th Workshop on Multiword Expressions (MWE 2014), pages 43?47,
Gothenburg, Sweden, 26-27 April 2014. c?2014 Association for Computational Linguistics
Encoding MWEs in a conceptual lexicon
Aggeliki Fotopoulou, Stella Markantonatou, Voula Giouli
Institute for Language and Speech Processing, R.C. ?Athena? 
{afotop;marks;voula}@ilsp.athena-innovation.gr
Abstract
The proposed  paper  reports  on  work  in  progress 
aimed at the development of a conceptual lexicon 
of  Modern  Greek  (MG)  and  the  encoding  of 
MWEs  in  it.  Morphosyntactic  and  semantic 
properties  of  these  expressions  were  specified 
formally and encoded in the lexicon. The resulting 
resource will be applicable for a number of NLP 
applications.
1 Introduction
 Substantial research  in  linguistics  has  been 
devoted  to  the  analysis  and  classification  of 
MWEs from different perspectives (Fraser, 1970; 
Chomsky,  1980;  M. Gross 1982,  1988;  Ruwet, 
1983;  der  Linden,  1992;  Nunberg et  al.,  1994; 
Howarth, 1996; Jackendoff, 1997; Moon, 1998; 
Fellbaum,  2007).  Moreover,  cognitive  and 
psycholinguistic  approaches  to  MWEs  (Lakoff, 
1993;  Gibbs,  1998;  Glucksberg,  1993; 
Diakogiorgi&Fotopoulou, 2012) have accounted 
for  their  interpretation.  Within  the  NLP 
community,  there  is  a  growing  interest  in  the 
identification  of  MWEs  and  their  robust 
treatment,  as  this  seems  to  improve  parsing 
accuracy  (Nivre  and  Nilsson,  2004;  Arun  and 
Keller, 2005). In this respect, the development of 
large-scale,  robust  language resources that  may 
be integrated in parsing systems is of paramount 
importance. Representation, however, of MWEs 
in lexica poses a number of challenges. 
2 Basic Notions
Typically,  fixed  MWEs  are  identified  and 
classified on the basis of  semantic,  lexical  and 
morphosyntactic criteria. (M. Gross, 1982, 1987; 
Lamiroy, 2003), namely:
? non-compositionality:  i.e.,  the meaning of 
the  expression  cannot  be  computed  from 
the  meanings of  its  constituents  and  the 
rules used to combine them. Nevertheless, 
according  to  (Nunberg  et  al,  1994), 
compositionality refers to the fact that the 
constituents  of  some  idioms  ?carry 
identifiable  parts  of  the  idiomatic 
meaning?.  Variability has  been  further 
emphasised in (Hamblin and Gibbs 1999) 
and  (Nunberg  et  al.  1994):  fixed 
expressions  appear  in  a  continuum  of 
compositionality,  which  ranges  from 
expressions  that  are  very  analysable  to 
others  that  are  partially  analysable  or 
ultimately non-analysable.
? non-substitutability:  at  least  one  of  the 
expression  constituents  does  not  enter  in 
alternations at the paradigmatic axis 
? non-modifiability: MWEs are syntactically 
rigid structures, in that there are constraints 
concerning  modification,  transformations, 
etc. 
These  criteria,  however,  do  not  apply  in  all 
cases in a uniform way. The  variability attested 
brings about the notion ?degree of fixedness? (G. 
Gross 1996). The kind and degree of fixedness 
result in the classification of these expressions as 
fixed,  semi-fixed,  syntactically flexible or 
collocations (Sag et al, 2002). It is crucial for a 
satisfactory  MWEs  representation  in  a 
computational lexicon to provide an accurate and 
functional  formal  modelling  of  fixedness, 
variability and compositionality. 
In this paper, we will discuss the classification 
and encoding of compounds and fixed MWEs in 
a conceptually organised lexicon of MG. 
3 The conceptual lexicon 
The conceptually organised lexicon that is under 
development  (Markantonatou  &  Fotopoulou, 
2007)  capitalises  on two basic  notions:  (a)  the 
notion  of  lexical  fields,  along  with  (b)  the 
Saussurian notion of sign and its two inseparable 
facets,  namely,  the  SIGNIFIER and  the 
SIGNIFIED as the building blocks (main classes) 
of the underlying ontology.
43
In this sense, the intended language resource is 
a  linguistic  ontology  in  which  words  are 
instances in the  SIGNIFIER class. At this level, 
morphological,  syntactic  and  functional 
information about lemmas is encoded. Similarly, 
word meanings are instances in the  SIGNIFIED 
class.  Each instance in the  SIGNIFIER class is 
mapped onto a concept, the latter represented as 
an instance in the SIGNIFIED class.
The  Instances  of  the  class  SIGNIFIER are 
specified  for  (a)  features  pertaining  to  lexical 
semantic  relations  (i.e,  synonymy,  antonymy); 
(b)  lexical  relations  such  as  word  families, 
allomorphs,  syntactic  variants  etc.;  and  (c) 
morphosyntactic  properties  (PoS,  gender, 
declension,  argument  structure,  word  specific 
information  etc.).  Values  for  these features  are 
assigned to both single- and multi-word entries 
in the lexicon. MWEs are further coupled with 
rich  linguistic  information  pertaining  to  the 
lexical, syntactic and semantic levels.
4 Encoding MWEs in the lexicon
MWEs  are  encoded  as  instances  in  the 
SIGNIFIER class  of our ontology and are also 
mapped onto the corresponding concepts or word 
meanings (instances in the SIGNIFIED class).
In the remaining, we focus on the encoding of 
MWEs as instances in the SIGNIFIER class. We 
cater  for  sub-classes  corresponding  to 
grammatical  categories  (verb,  noun,  adjective, 
adverb,  preposition,  etc)  under  the  class 
SIGNIFIER in our schema. The class MWEs (as 
opposed to the class Simple Lexical Units) has 
been  defined  further  under  the  verb,  noun, 
adjective and adverb sub-classes.
Syntactic  configurations  pertaining  to  each 
class are also represented as distinct sub-classes 
hierarchically  organised  under  the  verb,  noun, 
adjective  and  adverb  classes.  Morphosyntactic 
properties, selectional preferences, and semantic 
interpretation  patterns  are  provided  for  each 
MWE depending on the grammatical category it 
pertains  to;  encoding  is  based  on  a  set  of 
parameters represented as feature-value pairs.
More  precisely,  a  typology  of  Greek  verbal 
MWEs has been defined in (Fotopoulou,  1993, 
Mini, 2009) (NP V NP1 NP2?) and of nominal 
MWEs in (Anastasiadis,  1986) (Adj  N,  NN?) 
on  the  basis  of  the  lexical  and  syntactic 
configurations involved. This typology has been 
mapped onto a hierarchy under classes  verb and 
noun).
In our approach, the main distinction between 
collocations and  fixed  MWEs is  made  explicit. 
The  degree  and  type  of  fixedness  are  then 
encoded  as  features.  Further  morphosyntactic 
information  is  also  encoded  depending  on  the 
grammatical  category  of  the  MWE  (i.e., 
declension  of  one  or  more  constituents, 
only_singular or  only_plural for nouns, etc.). In 
this way, information that may be useful for the 
automatic identification and interpretation of the 
MWEs may be retained. Moreover, the standard 
set  of  features  inherited  from  the  class 
SIGNIFIER  is  also  retained  (PoS,  Gender, 
Number, Tense, synonyms, antonyms, etc.).
4.1.The encoding schema
We have so far implemented an encoding schema 
for  nominal  and  verbal  MWEs.  We  aimed  at 
encoding rich linguistic knowledge in a formal 
way  that  would  be  exploitable  in  computer 
applications.  The  two  types  of  fixedness 
(collocations and fixed) are encoded as features: 
(a) Lexical_variance, and (b) Is_actually.
The feature  Lexical_variance1 has as possible 
values (yes or no). Collocations (assigned a yes 
value)  are  further  specified  with  respect  to 
alternative lemmas; these lemmas are encoded in 
the appropriate feature Variants. For instance, in 
example  (1)  the  two  alternative  lemmas  are 
??????????? and ???????????:
 
(1) ????????   (???????????  /  ???????????) 
(=emergency (situations / circumstances))
The feature  Is_actually (with possible values 
yes  or  no)  encodes  information  about  the 
interpretation  pattern:  a  value  yes signifies  a 
compositional  or  partially  compositional 
meaning; on the contrary,  a value  no denotes a 
non-compositional  interpretation  (fixed 
meaning).
Collocations  are  by  default  assigned feature 
values  corresponding  to  a  compositional 
meaning.  In  these  cases,  the  feature 
maintains_meaning further  specifies  the 
constituent(s)  that  contribute  to  the  non-fixed 
interpretation of the expression. For example, the 
meaning of the compound in (2) is retained from 
the  meaning  of  the  first  noun  ?????? (=trip), 
which,  in  turn,  is  the  value  assigned  to  the 
maintains_meaning feature:
1In  our  MWE  classification  scheme,  a  lexical  unit  is 
considered ?fixed? at the lemma level. This is because MG 
is a heavily inflected language.
44
(2) ??????  ??????? (trip  -  lightning  (=very 
sudden and short trip)
<maintains_meaning =  ?????? />
Finally,  the  feature  has_meta_meaning 
signifies  further  the  constituent(s)  ?  if  any  ? 
bearing a figurative meaning.  For example,  the 
compound  ??????  ??????? in  (2)  assumes  the 
figurative meaning of the second noun  ??????? 
(=very sudden and short-term).
On  the  contrary,  verbal  and  nominal 
expressions  with  a  non-compositional  meaning 
are  assigned  a  negative  value (no) for  the 
Is_actually feature since their constituents do not 
contribute to a compositional meaning; therefore, 
the  features maintains_meaning  and 
has_meta_meaning  are  left  empty  as  non-
applicable. This is exemplified in (3) below; the 
constituents ??????? (=kids?) and ???? (=joy) of 
the  expression  ??????? ???? (=playground)  do 
not contribute to the overall interpretation:
(3) ??????? ???? (=playground)
<maintains_meaning/>
<has_meta_meaning/>
This schema that applies to both nominal and 
verbal MWES, is presented in Table 1 below.
Slot Values
mwe_type Fixed;   collocation
Lexical_variance  Boolean (yes, no)
        Variants string 
?s_actually Boolean (yes, no)
  maintains_meaning String
 has_meta_meaning String
Table 1 The encoding schema for nouns & verbs
4.2.Nominal MWEs
Furthermore,  nominal  MWEs are also assigned 
values  for  features  that  are  specific  to  the 
nominal  MWEs.  Information  on  inflected 
constituents  -  if  any  ?  is  provided  in  the 
declension feature; values for  only_singular and 
only_plural provide further morphological/usage 
information;  when  used  in  combination  with 
other  features  (i.e,  is_actually)  this  type  of 
information is  evidence  of  fixedness. Frequent 
co-occurrence patterns with verbs are provided in 
the  verb_combined feature;  finally,  alternative 
nominalised  forms  are  listed  as  values  of  the 
feature  nominalization. The schema is presented 
in the table below:
only singular Boolean (yes, no)
only plural: Boolean (yes, no)
N_declension ?1, ?2, ?1_?2, Adj_N 
verb_combined string 
Nominalization string 
Table 2 The encoding schema for nouns
4.3.Verbal MWEs 
In the typology adopted for the verbal idiomatic 
expressions,  fixedness can  be  limited  to  only 
certain  constituents  of  the  sentence; a 
combination of fixed and non-fixed constituents 
in  Subject or  Object position  is  permitted.  For 
example,  in  sentences  (4)  and  (5)  below, 
fixedness relies on the relation among the verbs 
and the nouns that function as Objects (direct and 
indirect) and as Subject respectively:
(4) ???? ????NP-acc, Obj ???? ????PP 
to  give  way to  anger  (=to swallow one?s 
pride/anger) 
(5) ??????? ?? ???????? ???NP-nom, Subj
my lights are  switched on  (=to  become 
very angry)
Moreover,  the  typology  allows  for  a  restricted 
alternation of fixed elements of the expression. 
For  example,  in  the  MWE  in  (6),  the  two 
alternative lemmas are ???? and ?????????:
(6) ???? / ?????????   ??? ?????? ?? ?? ?????
to  undertake    to    offer   /  promise   the sky with 
the stars
This information is encoded in verbal MWEs, 
namely:  (a) the syntactic properties of the verb 
that occurs in the expression (valency); and (b) 
45
fixed and non-fixed arguments either in  Subject 
or  Object position.  Moreover,  selectional 
restrictions applied to the arguments (such as +/-
human) are also added.
The  encoding  schema  that  applies  to  verbal 
MWEs  specifically  is  presented  in  Table  3.  In 
this  schema,  N signifies  a  non-fixed  noun, 
whereas C denotes a fixed one; number 0 (in N0 
and C0) is used to represent a noun (either fixed 
or non-fixed in Subject position), and 1, 2, 3, etc. 
denote  complements  in  Object position  (or 
complements  of  prepositional  phrases).  Other 
features  provide  rich  linguistic  information 
regarding facets of the expression in terms of: (a) 
selectional restrictions (i.e., the features N0_type,  
N1_type,  etc., accept  as  values  the  semantic 
category in which a noun in  Subject  or  Object  
position respectively,  belongs  to),  (b)  syntactic 
alternations  (i.e.,  Poss_Ppv  encodes  the 
alternation  among  possessive and  personal 
pronoun),  grammatical  information  (i.e., 
Ppv_case  encodes  the  case  of  the  personal 
pronoun), etc.
Slot Value
N0_type hum, -hum, npc 
C0_variants string 
Poss=Ppv Boolean (yes or no)
Ppv_case gen, acc 
N1_type hum, -hum, npc (Nom de 
partie du corps/noun of 
the part of  body) 
N2_type hum, -hum, npc
N3_type hum, -hum, npc
C1_variants string 
C2_variants string 
C3_variants string 
Table 3. The encoding schema for verbs
Alternative  nouns  (in  Subject  or  Object 
position)  that  oftern  co-occur  with  the  verbal 
expression  are  also  provided  for  (C0_variant, 
C1_variant, etc).
5. Discussion
As it has been shown above, in our lexicon we 
have  opted  for  an  approach  to  MWE 
representation  that  builds  on  rich  linguistic 
knowledge. The linguistic classifications adopted 
deal  with  morphology,  syntax,  and  semantics 
interface  aspects.  Thus,  a  lexicon  ?  grammar 
representation of MWEs has been constructed by 
encoding  key  morphosyntactic  and  semantic 
information.The  typology  of  verbal  MWEs 
shares  common  characteristics  with  similar 
efforts  for  other  languages  (i.e,  DuELME, 
Gregoire,  2010 Morphosyntactic  properties  and 
selectional  preferences  account  better  for  a 
number  of  phenomena,  inherent  in  the  Greek 
language,  as  for  example  word order  and gaps 
attested in running text.
More specifically, Greek is a language with a 
relatively  free  word  order,  and  idiomatic 
expressions  often  occur  in  texts  in  various 
configurations. The encoding of fixed and non-
fixed  constituents  provides,  therefore,  extra 
information for the identification of expressions 
in texts.  Moreover, the identification of MWEs 
as  collocations  entails  a  relatively  loose 
fixedness,  allowing,  thus,  for  gaps  and 
discontinuities as shown in (7):
(7) ?? ????? ???? ?????? ?????????-?????
The  political  party  has  a  number of 
candidates record (=many candidates)
6. Conclusions and Future work
We have  given  an  overview of  the  conceptual 
lexicon  currently  under  development  and  the 
treatment of MWEs in it. We have so far treated 
nominal  and  verbal  MWEs  (~1000  entries). 
Future  work  involves  the  population  of  the 
lexicon with new expressions also pertaining to 
the grammatical categories adjective and adverb 
and the definition of a fine-grained typology for 
the  latter.  Moreover,  a  more  granular 
representation  of  fixedness  will  be  attempted. 
Compatibility  of  the  resource  with  diverse 
syntactic  approaches  will  also  be  investigated. 
The  evaluation  of  the  final  resource  will  be 
performed  by  integrating  it  in  a  tool  that 
automatically recognizes MWEs in texts.
References
???????????-????????? ?. (1986).  ? ???????? ????  
?????  ???????????.  ???????????:  ???????????? 
????????????  ????????????  (???????????? 
????????? ??????????? ??????).
46
Arun,  A.  and  F.  Keller.  2005.  Lexicalisation  in 
crosslinguistic  probablisitic  parsing:  The  case  of 
french. In Proceedings of the 43rd Annual Meeting  
of  the Association for Computational  Linguistics, 
pp 306?313. Ann Arbor, MI
Chomsky, N. 1980.  Rules and Representations. New 
York: Columbia University Press.
Diakogiorgi,  K.  &  Fotopoulou,  A.  2012. 
Interpretation of Idiomatic Expressions by Greek 
Speaking Children: implications for the Linguistic 
and  Psycholinguistic  Research.  An 
interdisciplinary  approach.  Lingvisticae  
Investigationes, Volume  35:1.  1-27, John 
Benjamins, Paris, France
Fellbaum,  C.  2007.  Introduction.  Fellbaum, C.  (ed). 
Idioms and Collocations: Corpus-based Linguistic  
and Lexicographic Studies. London: Continuum, 1-
Fotopoulou, A. 1993. Une Classification des Phrases  
? Compl?ments Fig?s en Grec Moderne.  Doctoral 
Thesis, Universite Paris VIII.
Fraser,  B.  1970.  Idioms  within  a  Transformational 
Grammar. Foundations of language, 1, 22-42.
Fritzinger,  F.,  Weller,  M.,  and  Heid.  U.  2010.  A 
survey  of  idiomatic  preposition-noun-verb  tiples  on 
token level. In Proceedings of LREC-10.
Gr?goire,  N.  2010.  DuELME:  a  Dutch  electronic 
lexicon  of  multiword  expressions;  Lang 
Resources & Evaluation (2010) 44:23?39
Gibbs  R.W.  1998.  The  Fight  over  Metaphor  in 
Thought and Language. In A.N. Katz, C. Cacciari, 
R.W.  Gibbs  &  M.  Turner  (eds.),  Figurative 
Language and Thought. OUP, 88-118.
Glucksberg, S. 1993. Idiom meanings and allusional 
context.  In  Idioms:  Processing,  structure,  and  
intepretation.  C.  Cacciari  and  P.  Tabossi  (eds.). 
Hillsdale, NJ: Erlbaum, 201-225. 
Gross,  G.  1996.  Les  expressions fig?es  en fran?ais. 
Noms  compos?s  et  autres  locutions.  Paris/Gap: 
Ophrys.
Gross, M. 1982. Une classification des phrases fig?es 
du fran?ais. Revue Qu?b?coise de Linguistique 11 
(2), 151-185.
Gross,  M.  1988a.  Les  limites  de  la  phrase  fig?e. 
Langage 90: 7-23
 Gross,  Maurice.  1988b.  Sur  les  phrases  fig?es 
complexes du fran?ais. Langue fran?aise 77: 4770.
Hamblin, J., and Gibbs, W. R. 1999. Why You Can?t 
Kick  the  Bucket  as  You  Slowly  Die:  Verbs  in 
Idiom Comprehension. Journal of Psycholinguistic  
Research. 28 (1): 25-39.
Howarth P.A. 1996. Phraseology in English academic 
writing. Lexicographica Series 75.  T?bingen: Max 
Niemeyer. Jackendoff R. 1997. The Architecture of  
the Language Faculty. MIT Press.
Lakoff  G.  1993.  The  Contemporary  Theory  of 
Metaphor.  In  A.  Ortony  (ed.), Metaphor  and 
Thought, 2nd  edition  Cambridge  University  Press, 
202-251.
Lamiroy,  B.  2003.  Les  notions  linguistiques  de 
figement  et  de  contrainte.  Lingvisticae 
Investigationes 26:1,  53-66, 
Amsterdam/Philadelphia: John Benjamins.
van der Linden E-J. 1992. Incremental processing and 
the  hierarchical  lexicon.  Computational  
Linguistics, 18, 219-238
Markantonatou,  Stella  and  Fotopoulou,  Aggeliki. 
2007. The tool ?Ekfrasi". In Proceedings of the 8th  
International  Conference  on  Greek  Linguistics,  
The Lexicography Workshop. Ioannina, Greece.
Markantonatou,  S.,  Fotopoulou,  ?.,  Mini,  M.  & 
Alexopoulou, M. 2010. In search of the right word. 
In Proceedings of Cogalex-2: Cognitive Aspects of  
the  Lexicon,  2nd  SIGLEX  endorsed  Workshop. 
Beijing.
Mini, M. 2009. Linguistic and Psycholinguistic Study  
of Fixed Verbal Expressions with Fixed Subject in  
Modern  Greek:  A  Morphosyntactic  Analysis,  
Lexicosemantic  Gradation  and  Processing  by  
Elementary School Children. Unpublished doctoral 
dissertation. University of Patras.
Moon,  R.  1998.  Fixed  Expressions  and  Idioms  in  
English: A Corpus-Based Approach. Oxford: OUP.
Nivre,  J.  and  Nilsson,  J.  2004.  Multiword  units  in 
syntactic parsing. Workshop on Methodologies and  
Evaluation  of  Multiword  Units  in  Real-World  
Applications.
Nunberg  ,G.,  Sag  I.,  Wasow,  T.  1994.  Idioms. 
Language 70, 491-538.
Ruwet,  N.  1983.  Du  Bon  Usage  des  Expressions 
Idiomatiques  dans  l'Argumentation  en  Syntaxe 
G?n?rative.  Revue Qu?becoise de Linguistique  13 
(1): 9-145.
Sag,  I.A.,  Bond,  F.,  Copestake  A.,  Flickinger,  D. 
2001.  Multiword  Expressions.  LinGO  Working 
PaperNo.2001-01. 
Sag,  Ivan  A.,  T.Baldwin,  F.Bond,  A. Copestake and 
Dan Flickinger.2001.Multiword  Expressions:  A 
Pain in the Neck for  NLP. LinGO Working Paper 
No.  2001-03.  In  Alexander  Gelbukh,  ed.,  (2002) 
Proceedings of COLING-2002.
 
47
Proceedings of the Workshop on Lexical and Grammatical Resources for Language Processing, pages 39?45,
Coling 2014, Dublin, Ireland, August 24 2014.
 Linguistically motivated Language Resources for Sentiment Analysis 
 
 
Voula Giouli 
 
Aggeliki Fotopoulou 
Institute for Language and Speech Processing, Athena RIC 
{voula;afotop}@ilsp.athena-innovation.gr 
 
  
 
Abstract 
Computational approaches to sentiment analysis focus on the identification, extraction, summarization 
and visualization of emotion and opinion expressed in texts. These tasks require large-scale language re-
sources (LRs) developed either manually or semi-automatically. Building them from scratch, however, 
is a laborious and costly task, and re-using and repurposing already existing ones is a solution to this 
bottleneck. We hereby present work aimed at the extension and enrichment of existing general-purpose 
LRs, namely a set of computational lexica, and their integration in a new emotion lexicon that would be 
applicable for a number of Natural Language Processing applications beyond mere syntactic parsing. 
1 Introduction 
The abundance of user-generated content over the web has brought about the shift of interest to the 
opinion and emotion expressed by people or groups of people with respect to a specific target entity, 
product, subject matter, etc. The task of sentiment analysis involves determining the so-called private 
states (beliefs, feelings, and speculations) expressed in a particular text or text segment as opposed to 
factual information. More precisely, it is focused on the following: (a) identification of sentiment ex-
pressions in textual data and their classification as appropriate, and (b) recognition of participants in 
the private state, as for example, the entities identified as the Source and Target of the emotion. More 
recently, aspect-based sentiment analysis has also been in the focus of research (Wilson, 2008). 
Traditionally, classification of sentiment expressions is usually attempted in terms of the general 
notion of polarity defined as positive, negative and neutral. Traditional approaches to text classifica-
tion based on stochastic methods are quite effective when applied for sentiment analysis yielding quite 
satisfactory results. However, certain applications require for more fine-grained classifications of sen-
timent i.e. the identification of emotional states such as anger, sadness, surprise, satisfaction, etc. in 
place of mere recognition of the polarity. Such applications might be the identification of certain emo-
tions expressed by customers (i.e., satisfaction, or dissatisfaction) with respect to some product or ser-
vice, or the analysis of emotions and feelings described by users in blogs, wikis, fora and social media 
(Klenner at al., 2009). In this respect, stochastic approaches fail to recognize multiple or even conflict-
ing emotions expressed in a document or text segment. In these cases, linguistic (syntactic and seman-
tic knowledge) is necessary in order to assess the overall polarity of a clause and or the feeling ex-
pressed in it. 
The paper is organised as follows: In section 2 we present the aims and scope of the specific work; 
section 3 gives an overview of related work on affective LRs, whereas section 4 gives an account of 
the LRs developed within the framework of Lexicon ? Grammar. Our efforts towards enriching the 
existing resources with semantic information and re-purposing them are presented in sections 5 and 6 
respectively, while section 7 outlines our conclusions and prospects for future research. 
2 Aims and scope 
We present work aimed at extending, enriching and re-purposing existing LRs, the ultimate goal being 
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer 
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 
39
their integration in a tool for sentiment analysis. In specific, a suite of computational lexica developed 
within the framework of Lexicon ? Grammar (LG) and treating verbal and nominal predicates denot-
ing emotion were used. These resources were initially constructed manually as a means to describe 
general language, and they bear rich linguistic information that would be otherwise difficult to encode 
in an automatic way, namely (a) subcategorisation information, (b) semantic and distributional proper-
ties, and (c) syntactic transformations of the predicates. Within the current work, semantic information 
that is meaningful for sentiment analysis was also added to lexicon entries. The final resource was 
then used to bootstrap a grammar of emotions. This grammar is a rule-based approach to sentiment 
analysis aimed at capturing and modeling linguistic knowledge that is necessary for the task at hand.  
The work presented here was based on a previous study (Giouli et al., 2013), making further exten-
sive use of the Hellenic National Corpus (HNC), a large reference corpus for the Greek language 
(Hatzigeorgiou et al, 2000).  Additionally, a suite of specialized corpora that were developed to guide 
sentiment studies in multimodal (Mouka et al., 2012) and in textual (Giouli and Fotopoulou, 2013) 
data was used. Thus, the resulting Greek Sentiment Corpus, that amounts to c. ~250K tokens, com-
prises audiovisual material (movies dialogues), and texts selected manually from various sources over 
the web. More particularly, the online edition of two newspapers along with a news portal were 
searched on a daily basis for the identification and selection of commentaries dealing with a set of 
predefined topics; Greek blogs and fora were also used as sources for text collection. The afore-
mentioned corpus was annotated at the sentence and phrase level for opinion and emotion, and was 
subsequently used to populate the sentiment lexicon under construction. Moreover, initial steps were 
made towards creating a rule-based system for the identification of sentiment expressions in texts and 
computing the overall phrase polarity in context on the basis of corpus evidence. 
3 Related work 
A number of large-scale lexica appropriate for sentiment analysis have been developed either manual-
ly or semi-automatically. These range from mere word lists to more elaborate resources. General In-
quirer (Stone et al. 1966), the Subjectivity lexicon integrated in OpinionFinder (Wiebe et al., 2005), 
and SentiWordNet (Esuli and Sebastiani 2006) are examples of such affective lexica. On the other 
hand, WordNet-Affect (Strapparava and Valitutti 2004), an extension of WordNet Domains, is linguis-
tically oriented as it comprises a subset of synsets that are suitable to represent affective concepts in 
correlation with affective words. A set of A-labels is used to mark concepts representing emotions or 
emotional states, moods, eliciting emotions situations, and emotional responses. Finally, EmotiNet 
(Balahur et al, 2011) is a knowledge base (KB) for representing and storing affective reaction to real-
life contexts and action chains described in text. 
From a purely linguistic perspective ? yet with a view to Natural Language Processing - substantial 
work has been devoted to the semantic classification of verbal predicates denoting emotion in 
(Mathieu, 1999). In this work, verbs denoting emotional states and evaluative stances should also be 
classified according to the so-called semantic field?. Verbs were, thus, categorized into homogenous 
semantic classes which share common syntactic properties; this classification is claimed to facilitate 
semantic interpretation.  
Statistical approaches to sentiment analysis feature a ?bag-of-word? representation (Hu and Liu, 
2004). Rule-based systems, on the other hand, exploit linguistic knowledge in the form of syntac-
tic/lexical patterns for computing polarity in context. In most cases, negative particles and modality 
are reported as the most obvious shifters that affect sentiment polarity (Polanyi and Zaenen 2006, Jia 
et al. 2009, Wiegand et al. 2010, Benamara et al., 2012).  Finally, compositionality features have been 
explored for the computation of multiple or conflicted sentiments on the basis of deep linguistic analy-
sis (Moilanen and Pulman, 2007), (Neviarouskaya et al., 2009), (Klenner et al., 2009). 
4 Lexicon ? Grammar tables 
4.1 Lexicon ? Grammar framework 
The Lexical Resources hereby exploited were initially constructed in accordance with the Lexicon-
Grammar (LG) methodological framework (Gross 1975), (Gross 1981). Being a model of syntax lim-
ited to the elementary sentences of the form Subject ? Verb ? Object, the theory argues that the unit of 
40
meaning is located at the sentence rather than the word level. To this end, linguistic analysis consists 
in converting each elementary sentence to its predicate-argument structure. Additionally, main com-
plements (subject, object) are separated from other complements (adjuncts) on the basis of formal cri-
teria; adverbial complements (i.e., prepositional phrases) are considered as crucial arguments only in 
the case that they characterize certain verb frames: 
 
(1)  John removed the cups from the table. 
 
To cater for a more fine-grained classification, and the creation of homogenous word classes, this 
formal syntactic definition is further coupled with distributional properties associated with words, i.e., 
types of prepositions, features attached to nouns in subject and complement positions, etc. A set of 
transformation rules, construed as equivalence relations between sentences, further generate equivalent 
structures. It becomes evident, therefore, that the resulting resources are rich in linguistic information 
(syntactic structure, distributional properties and permitted transformational rules), which is encoded 
formally in the so-called LG tables. 
4.2 The Lexicon ? Grammar of verb and noun predicates denoting emotion 
Within the LG framework, 130 noun predicates denoting emotions (Nsent) in Modern Greek were se-
lected and classified into 3 classes, according to their syntactic and distributional properties 
(Fotopoulou & al., 2008). The 1st class comprises nouns of interpersonal relations with an obligatory 
prepositional complement and a conversed construction, as for example ????????? (= admiration). 
The 2nd class are indicative of an external cause including a non obligatory prepositional complement, 
as for example ????? (= fear). The 3rd class without complements have a static character, as for ex-
ample ??????? (= happiness). Identification of the specific light verbs (or support verbs, Vsup) they 
select for was also performed. Furthermore, their distributional properties and their co-occurrence with 
specific verbs expressing diverse modalities (aspect, intensity, control, manifestation or verbal expres-
sion) have also been encoded in a formal way. These properties reveal the restrictions nouns impose 
on the lexical choice of verbs. 
Furthermore, 339 Greek verbal predicates denoting emotion (Vsent) have been selected from vari-
ous sources (i.e. existing reference lexicographic works and corpora) and were subsequently classified 
in five LG tables. Classification was performed on the basis of the following axes: (i) syntactic infor-
mation (i.e, subcategorisation information); (ii) selectional restrictions (+Hum/ -Hum) imposed over 
their Subject and Object complements; and (iii) transformation rules. More precisely, as far as syntac-
tic structure is concerned, the predicates under consideration were identified to appear in both transi-
tive and intransitive constructions being represented as ?0 V N1 and ?0 V respectively. Certain verbs 
also allow for a prepositional phrase complement represented as ?0 V Prep N11 configurations. A 
close inspection over the data revealed the relationship between the N0 or N1 complements that de-
note the Experiencer of the emotion (i.e., the entity feeling the emotion). In two of the resulting clas-
ses the Experiencer is projected as the structural Subject of the verb, whereas the Theme or Stimulus is 
projected as their structural object. Similarly, the remaining 3 classes realize the Theme/Stimulus as the 
subject and the Experiencer as their object, their distinguishing property being their participation in 
unaccusative and middle constructions, the latter being linked to the implicit presence of an Agent 
(middle) and the absence of an Agent (unaccusative). These properties have been checked for the 
whole range of lexical data based on both linguistic introspection and corpus evidence. 
A number of Harrisian constructions and transformations (Harris, 1951; 1964; 1968) have been ex-
tensively utilized within the LG formalism to define syntactically related and semantically equivalent 
structures. Apart from passivisation and middle alternation constructions - also relevant to emotion 
predicates - the restructuring transformation has been accounted for (Guillet and Lecl?re, 1981): 
 
(2) ? ??????? ???????? ?? ????? ??? ?? ?????? ???. 
The John admires  the Maria for the courage-her. 
John admires Maria for her courage. 
                                                 
1 Adopting the LG notation, ?0 denotes a Noun in Subject position of a given verb V, whereas, N1 denotes its 
Object. 
41
 
(3) ? ??????? ???????? ?? ?????? ??? ??????. 
The John  admires the courage the Maria-of 
John admires Maria?s courage. 
 
Moreover, each verbal predicate was also coupled with morphologically-related adjectives and 
nouns, and the alignment of semantically equivalent nominal, verbal and adjectival structures was per-
formed thereof. A number of semantically equivalent paraphrases of the verbs with the morphologi-
cally related nouns and adjectives were also encoded in the tables. 
Finally, following the same methodology, a set of 2,500 verbal multi-word expressions denoting 
emotions were identified from corpora and classified in 13 categories according to their syntactic 
structure. The final resource comprises a total of ~3000 entries, organized in 21 LG tables with lem-
mas inter-connected via the tables relative to verbs. 
5 Semantic classification of emotion predicates 
Semantic classification of the verbal predicates has also been performed on the basis of their underly-
ing semantics. In this way, the syntactic and distributional properties encoded in the LG tables have 
been coupled with semantic information that defines an affective taxonomy. These properties were 
added as columns in the tables that describe the verb predicates. Our goal was to group together predi-
cates that are synonyms or near synonyms and to create an affective taxonomy hierarchical organized. 
To this end, certain abstractions and generalizations were performed where necessary for defining 
classes of emotion types.  
Initially, 59 classes of emotion-related-senses were identified. At the next stage, a number of itera-
tions followed aimed at grouping together senses that are semantically related. This procedure resulted 
in the identification of a set of senses that may be used as taxonomy of emotions. Following practices 
adopted in similar endeavours (i.e., Mathieu, 1999), each class was further assigned a tag that uniquely 
identifies the respective class. The following classes (19 classes) were identified: anger, fear, sadness, 
disgust, surprise, anticipation, acceptance, joy, love, hate, disappointment, indifference, shame, envy, 
jealousy, relaxedness, respect, resentment, and remorse. 
Next, each entry was further specified as regards the specific relation that holds between the entry 
and the emotion type it belongs to. A set of properties were then defined for which each entry was then 
examined, namely: FeelEmotion, EmotionManifestation, Behaviour, and EntailsEmotion.  
At a more abstract level, entries were further assigned a value for the semantic property polarity. 
Following previous works (Mathieu and Fellbaum, 2010), the encoding caters for the apriori polarity 
of the emotion denoted which subsumes one of the following values: (a) positive, i.e. predicates which 
express a pleasant feeling; (b) negative, i.e., predicates which express an unpleasant feeling; (c) neu-
tral, and (d) ambiguous, i.e., predicates expressing a feeling the polarity of which is context-dependent 
(e.g., surprise). 
Moreover, to better account for the semantic distinction between near synonyms that occur within a 
class such as ??????? (= I am scared), ?????????????? (=panic), etc., entries are further coupled with 
the feature intensity with possible values: low, medium, high, uncertain. Intensity was attributed to the 
lexical items on the basis of linguistic introspection and the definitions of lexical entries. 
6 Transforming Lexicon-Grammar tables to a grammar of emotions 
Being initially developed to serve as a means of linguistic description, this framework has, never-the-
less, been proved to be applicable for the construction of robust computational lexica. And although it 
has been claimed (Mathieu, 2008) that the information is not directly exploitable for NLP applications 
due to the fact that certain pieces of information are not formally encoded or are implicit, a number of 
works (Hathout and Namer 1998, Danlos and Sagot 2009) have successfully managed to reformat LG 
tables in efficient large-scale NLP lexica.  
To this end, we have tried to exploit information available in the tables and make the mappings that 
are necessary for the task of sentiment recognition. On the one hand, subcategorisation information 
with respect to selectional restrictions imposed over the Subject and Object of the verbal predicates 
was exploited. Once a verbal predicate has been identified, the constituent either in Subject or Object 
42
position that is also assigned a (+Hum) property corresponds unambiguously to the Experiencer of the 
emotion depending on the class it belongs to (i.e., SubjectExperiencer or Object Experiencer). Simi-
larly, the NP in Object position of verbs that pertain to the 2nd class ????? (=love) corresponds to the 
Target of the emotion. All other constituents correspond to the Trigger or Cause. 
On these grounds, initial steps towards building a rule-based component that identifies emotion ver-
bal and nominal predicates in texts along with the participating entities, namely the Experiencer and 
Target of the emotion expressed have been performed. To this end, a library of local grammars (Con-
stant, 2003) for emotion predicates has been constructed modeling structures in the annotated corpus. 
Local grammars (also referred to in the literature as graphs) are algebraic grammars formulated as 
combinations of sequences of grammatical symbols in the form of regular expressions that describe 
natural language. In this sense, they are a powerful tool to represent the majority of linguistic phenom-
ena in an intuitive manner. Moreover, they are compiled into finite state transducers that transform 
input text by inserting or removing special markers. Rules are sequentially applied to the text using 
longest match. We made use of the UNITEX platform (Paumier, 2013) for creating the graphs and 
then compiling them into finite state transducers. UNITEX consists of three modules, namely, corpus 
handling, lexicon development and grammar development that are integrated into a single intuitive 
graphical user interface. Based on the Lexicon-Grammar tables developed for the verbal predicates 
(c.f. section 2 above), we initially created five parameterized graphs manually; these graphs depict the 
syntactic and semantic properties of the predicates. At the next stage, a set of graphs was constructed 
automatically using UNITEX, each one representing the syntactic and semantic properties of a given 
predicate.  
It should be noted, however, that LG tables provide descriptions at an abstract level. To remedy this 
shortcoming, a number of graphs and sub-graphs describing a wide range of syntactic phenomena 
(noun phrase, coordination, modifiers, negation, and valency shifters) were constructed manually. The 
set of graphs comprises a grammar applied to the text as a cascade for the identification of the emotive 
predicate, being either verbal or nominal, its polarity and the participants of the emotion event that can 
be identified from the underlying structure ? namely the Experiencer and the Theme and the Cause. 
7 Conclusions and future work 
We have described work aimed at enriching, re-purposing and re-using already available LRs for a 
new task, namely identification of emotion expressions in texts. The existing lexica carry rich linguis-
tic information which has been mapped onto categories that are meaningful for the task. Our efforts 
have been oriented towards developing a rule-based system that efficiently will eventually recognise 
emotion expressions in texts and the participants in the emotion event. 
Future work has been planned already, consisting of the exploitation of other properties that are en-
coded in the LG tables, as for example the restructuring property as a facet of the aspect-based senti-
ment analysis and the conversion of the enriched LG tables to a standardised lexical format. Finally, 
the validation of the final resource is due against the manually annotated corpus. 
  
Acknowledgments 
The research within the project LangTERRA: Enhancing the Research Potential of ILSP/?Athena? 
R.C. in Language Technology in the European Research ERA leading to these results has received 
funding from the European Union Seventh Framework Programme (FP7/2007-2013). 
References 
Alexandra Balahur and Jes?s M. Hermida and Andr?s Montoyo and Rafael Mu?oz. 2011. EmotiNet: A 
Knowledge Base for Emotion Detection in Text Built on the Appraisal Theories. In R. Mu?oz et al. (Eds.): 
Natural Language Processing and Information Systems, Lecture Notes in Computer Science, Volume 6716, 
Springer-Verlag Berlin Heidelberg 2011,  pp 27-39. 
Farah Benamara, Baptiste Chardon, Yannick Mathieu, Vladimir Popescu, and Nicholas Asher. 2012. How do 
Negation and Modality Impact on Opinions? In Proceedings of the Workshop on Extra-Propositional Aspects 
of Meaning in Computational Linguistics, ExProM ?12, Jeju, Republic of Korea, 2012, pp 10?18. 
43
Matthieu Constant. 2003. Grammaires locales pour l?analyse automatique de textes : m?thodes de construction 
et outils de gestion. Th?se de doctorat, Universit? de Marne-la-Vall?e. 
Laurence Danlos and Beno?t Sagot. 2009. Constructions pronominales dans Dicovalence et le lexique-
grammaire: Integration dans le Lefff. Actes du 27e Colloque international sur le lexique et la grammaire. 
Andrea Esuli and Fabrizio Sebastiani. 2006. SENTIWORDNET: A Publicly Available Lexical Resource for 
Opinion Mining, in Proceedings of the 5th Conference on Language Resources and Evaluation (LREC 2006), 
Genova, Italy, pp 417-422. 
Christiane Fellbaum. 1998. WordNet: An Electronic Lexical Database. Cambridge, MA: MIT Press. 
Aggeliki Fotopoulou, Marianna Mini, Mavina Pantazara and Argiro Moustaki. 2008. La combinatoire lexicale 
des noms de sentiments en grec modern. In Iva Novacova & Agnes Tutin (eds), Le lexique des ?motions. 
ELLUG, Grenoble. 
Voula Giouli and Aggeliki Fotopoulou. 2012. Emotion verbs in Greek. From Lexicon-Grammar tables to multi-
purpose syntactic and semantic lexica. In Proceedings of the XV Euralex International Congress (EURALEX 
2012). Oslo, Norway. 
Voula Giouli and Aggeliki Fotopoulou. 2013. Developing Language Resources for Sentiment Analysis in Greek. 
In Proceedings of the Workshop ?The semantic domain of emotions: cross-domain and cross-lingual  consid-
erations. From words to phrases/text and beyond?. Workshop organized within the framework of the Interna-
tional Conference in Greek Linguistics.  ICGL, Rhodes. 
Voula Giouli, Aggeliki Fotopoulou, Effie Mouka, and Ioannis E. Saridakis. 2013. Annotating Sentiment Expres-
sions for Lexical Resourcres. In Blumenthal, Peter, Novakova, Iva, Siepmann, Dirk (eds.), Les ?motions dans 
le discours. Emotions in discourse. Frankfurt, Main et al.: Peter Lang. 
Maurice Gross. 1975. M?thodes en syntaxe. R?gime des constructions compl?tives. Hermann, Paris. 
Maurice Gross. 1981. Les bases empiriques de la notion de pr?dicat s?mantique. Langages 15, 7-52. 
Allain Guillet and Christian Lecl?re. 1981. La restructuration du sujet. Langages 65. Paris, France. 
Zelling S. Harris. 1951. Methods in Structural Linguistics. The University of Chicago Press, Chicago. 
Zelling S. Harris. 1964. The Elementary Transformations. In T.D.A.P. University of Pennsylvania 54, Pennsyl-
vania. 
Zelling S. Harris. 1968. Mathematical Structures of Language. Wiley, New York. 
Nabil Hathout and Fiammetta Namer. 1998. Automatic Construction and Validation of French Large Lexical 
Resources: Reuse of Verb Theoretical Linguistic Descriptions. In Proceedings of the Language Resources 
and Evaluation Conference, Grenada, Spain. 
Nick Hatzigeorgiu, Maria Gavrilidou, Stelios Piperidis, George Carayannis, Anna Papakostopoulou, Anna 
Spiliotopoulou, Anna Vacalopoulou, Penny Labropoulou, Elena Mantzari, Harris Papageorgiou, and Iason 
Demiros. 2000. Design and Implementation of the Online ILSP Greek Corpus. In Proceedings of the 2nd 
Language Resources and Evaluation Conference ( LREC, 2000), Athens, Greece. 
Lifeng Jia, Clement Yu and Weiyi Meng. 2009. The effect of Negation on Sentiment Analysis and Retrieval 
Effectiveness. In Proceedings of the 18th ACM conference on Information and knowledge management, Hong 
Kong, pp. 1827-1830. 
Manfred Klenner, Stefanos Petrakis and Angela Fahrni. 2009. Robust Compositional Polarity Classification. In 
Recent Advances in Natural Language Processing (RANLP), Borovets, Bulgaria 
Yvette Yannick Mathieu. 1999. Un classement s?mantique des verbes psychologiques. Cahiers du C.I.EL: 
pp.115-134 
Yvette Yannick Mathieu. 2008. Navigation dans un texte ? la recherche des sentiments. Linguisticae Investiga-
tiones. 31:2, pp. 313-322. 
Yvette Yannick Mathieu and Christiane Fellbaum, 2010. Verbs of Emotion in French and English. Emotion, vol. 
70, 2010. 
Karo Moilanen and Stephen Pulman. 2007. Sentiment Composition. In Proceedings of Recent Advances in Natu-
ral Language Processing (RANLP), Borovets, Bulgaria, 2007, pp 378?382. 
44
Effie Mouka, Voula Giouli, Aggeliki Fotopoulou, and Ioannis E. Saridakis. 2012. Opinion and emotion in mov-
ies: a modular perspective to annotation. In Proceedings of the 4th International Workshop on Corpora for 
Research on Emotion, Sentiment & Social Signals (ES? 2012). Istanbul, Turkey. 
Alena Neviarouskaya, Helmut Prendinger, and Mitsuru Ishizuka. 2009. Compositionality Principle in Recogni-
tion of Fine-Grained Emotions from Text. In Proceedings of the International Conference on Weblogs and 
Social Media, AAAI, San Jose, USA, May 2009, pp. 278?281. 
S?bastien Paumier. 2003. UNITEX User Manual. 
Livia Polanyi and Annie Zaenen. 2006. Contextual Valence Shifters. In Shanahan, J., Qu, Y., and Wiebe, J. 
Computing Attitude and Affect in Text: Theory and Applications. Berlin: Springer, pp. 1-10. 
Philip J. Stone and Earl B. Hunt. 1963. A computer approach to content analysis: studies using the General In-
quirer system. In Proceedings of the May 21-23, 1963, Spring Joint Computer Conference. Detroit, Michigan, 
pp. 241-256. 
 Carlo Strapparava and Alessandro Valitutti. 2004. WordNet-Affect: an affective extension of WordNet. In Pro-
ceedings of Language Resources and Evaluation Conference (LREC 2004), pp. 1083-1086. 
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005. Recognizing Contextual Polarity in Phrase-Level 
Sentiment Analysis. In Proceedings  of HLT-EMNLP-2005. 
Teresa Wilson. 2008. Fine-grained Subjectivity and Sentiment Analysis: Recognizing the Intensity, Polarity, and 
Attitudes of Private States. University of Pittsburgh. Available at: 
http://mpqa.cs.pitt.edu/data/TAWilsonDissertationCh7Attitudes.pdf. [Accessed November 2011] 
Michael Wiegand, Alexandra Balahur, Benjamin Roth, Dietrich Klakow and Andres Montoyo. 2010. A survey 
on the Role of Negation in Sentiment Analysis. In: Proceedings of NeSp-NLP ?10, pp. 60-68. 
45
