Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 286?296, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Source Language Adaptation for Resource-Poor Machine Translation
Pidong Wang
Department of Computer Science
National University of Singapore
13 Computing Drive
Singapore 117417
wangpd@comp.nus.edu.sg
Preslav Nakov
QCRI
Qatar Foundation
Tornado Tower, P.O. 5825
Doha, Qatar
pnakov@qf.org.qa
Hwee Tou Ng
Department of Computer Science
National University of Singapore
13 Computing Drive
Singapore 117417
nght@comp.nus.edu.sg
Abstract
We propose a novel, language-independent
approach for improving machine translation
from a resource-poor language to X by adapt-
ing a large bi-text for a related resource-rich
language and X (the same target language).
We assume a small bi-text for the resource-
poor language to X pair, which we use to
learn word-level and phrase-level paraphrases
and cross-lingual morphological variants be-
tween the resource-rich and the resource-poor
language; we then adapt the former to get
closer to the latter. Our experiments for
Indonesian/Malay?English translation show
that using the large adapted resource-rich bi-
text yields 6.7 BLEU points of improvement
over the unadapted one and 2.6 BLEU points
over the original small bi-text. Moreover,
combining the small bi-text with the adapted
bi-text outperforms the corresponding com-
binations with the unadapted bi-text by 1.5?
3 BLEU points. We also demonstrate applica-
bility to other languages and domains.
1 Introduction
Statistical machine translation (SMT) systems learn
how to translate from large sentence-aligned bilin-
gual corpora of human-generated translations, called
bi-texts. Unfortunately, collecting sufficiently large,
high-quality bi-texts is hard, and thus most of the
6,500+ world languages remain resource-poor. For-
tunately, many of these resource-poor languages
are related to some resource-rich language, with
whom they overlap in vocabulary and share cog-
nates, which offers opportunities for bi-text reuse.
Example pairs of such resource rich?poor lan-
guages include Spanish?Catalan, Finnish?Estonian,
Swedish?Norwegian, Russian?Ukrainian, Irish?
Gaelic Scottish, Standard German?Swiss Ger-
man, Modern Standard Arabic?Dialectical Arabic
(e.g., Gulf, Egyptian), Turkish?Azerbaijani, etc.
Previous work has already demonstrated the ben-
efits of using a bi-text for a related resource-rich
language to X (e.g., X=English) to improve ma-
chine translation from a resource-poor language to
X (Nakov and Ng, 2009; Nakov and Ng, 2012).
Here we take a different, orthogonal approach: we
adapt the resource-rich language to get closer to the
resource-poor one.
We assume a small bi-text for the resource-poor
language, which we use to learn word-level and
phrase-level paraphrases and cross-lingual morpho-
logical variants between the two languages. Assum-
ing translation into the same target language X , we
adapt (the source side of) a large training bi-text for
a related resource-rich language and X .
Training on the adapted large bi-text yields very
significant improvements in translation quality com-
pared to both (a) training on the unadapted version,
and (b) training on the small bi-text for the resource-
poor language. We further achieve very sizable im-
provements when combining the small bi-text with
the large adapted bi-text, compared to combining the
former with the unadapted bi-text.
While we focus on adapting Malay to look like
Indonesian in our experiments, we also demonstrate
the applicability of our approach to another language
pair, Bulgarian?Macedonian, which is also from a
different domain.
286
2 Related Work
One relevant line of research is on machine trans-
lation between closely related languages, which is
arguably simpler than general SMT, and thus can
be handled using word-for-word translation, man-
ual language-specific rules that take care of the nec-
essary morphological and syntactic transformations,
or character-level translation/transliteration. This
has been tried for a number of language pairs in-
cluding Czech?Slovak (Hajic? et al2000), Turkish?
Crimean Tatar (Altintas and Cicekli, 2002), Irish?
Scottish Gaelic (Scannell, 2006), and Bulgarian?
Macedonian (Nakov and Tiedemann, 2012). In con-
trast, we have a different objective ? we do not carry
out full translation but rather adaptation since our
ultimate goal is to translate into a third language X .
A special case of this same line of research is the
translation between dialects of the same language,
e.g., between Cantonese and Mandarin (Zhang,
1998), or between a dialect of a language and a stan-
dard version of that language, e.g., between some
Arabic dialect (e.g., Egyptian) and Modern Standard
Arabic (Bakr et al2008; Sawaf, 2010; Salloum and
Habash, 2011). Here again, manual rules and/or
language-specific tools are typically used. In the
case of Arabic dialects, a further complication arises
by the informal status of the dialects, which are not
standardized and not used in formal contexts but
rather only in informal online communities1 such as
social networks, chats, Twitter and SMS messages.
This causes further mismatch in domain and genre.
Thus, translating from Arabic dialects to Modern
Standard Arabic requires, among other things, nor-
malizing informal text to a formal form. In fact,
this is a more general problem, which arises with
informal sources like SMS messages and Tweets for
just any language (Aw et al2006; Han and Bald-
win, 2011). Here the main focus is on coping with
spelling errors, abbreviations, and slang, which are
typically addressed using string edit distance, while
also taking pronunciation into account. This is dif-
ferent from our task, where we try to adapt good,
formal text from one language into another.
A second relevant line of research is on language
adaptation and normalization, when done specifi-
cally for improving SMT into another language.
1The Egyptian Wikipedia is one notable exception.
For example, Marujo et al2011) described a
rule-based system for adapting Brazilian Portuguese
(BP) to European Portuguese (EP), which they used
to adapt BP?English bi-texts to EP?English. They
report small improvements in BLEU for EP?English
translation when training on the adapted ?EP??En
bi-text compared to using the unadapted BP?En
(38.55 vs. 38.29), or when an EP?English bi-text is
used in addition to the adapted/unadapted one (41.07
vs. 40.91 BLEU). Unlike this work, which heav-
ily relied on language-specific rules, our approach is
statistical, and largely language-independent; more-
over, our improvements are much more sizable.
A third relevant line of research is on reusing bi-
texts between related languages without or with very
little adaptation, which works well for very closely
related languages. For example, our previous work
(Nakov and Ng, 2009; Nakov and Ng, 2012) ex-
perimented with various techniques for combining
a small bi-text for a resource-poor language (In-
donesian or Spanish, pretending that Spanish is
resource-poor) with a much larger bi-text for a re-
lated resource-rich language (Malay or Portuguese);
the target language of all bi-texts was English. How-
ever, our previous work did not attempt language
adaptation, except for very simple transliteration for
Portuguese?Spanish that ignored context entirely;
since it could not substitute one word for a com-
pletely different word, it did not help much for
Malay?Indonesian, which use unified spelling. Still,
once we have language-adapted the large bi-text, it
makes sense to try to combine it further with the
small bi-text; thus, below we will directly compare
and combine these two approaches.
Another alternative, which we do not explore in
this work, is to use cascaded translation using a
pivot language (Utiyama and Isahara, 2007; Cohn
and Lapata, 2007; Wu and Wang, 2009). Unfortu-
nately, using the resource-rich language as a pivot
(poor?rich?X) would require an additional paral-
lel poor?rich bi-text, which we do not have. Pivoting
over the target X (rich?X?poor) for the purpose
of language adaptation, on the other hand, would
miss the opportunity to exploit the relationship be-
tween the resource-poor and the resource-rich lan-
guage; this would also be circular since the first step
would ask an SMT system to translate its own train-
ing data (we only have one rich?X bi-text).
287
3 Malay and Indonesian
Malay and Indonesian are closely related, mutually
intelligible Austronesian languages with 180 million
speakers combined. They have a unified spelling,
with occasional differences, e.g., kerana vs. karena
(?because?), Inggeris vs. Inggris (?English?), and
wang vs. uang (?money?).
They differ more substantially in vocabulary,
mostly because of loan words, where Malay typi-
cally follows the English pronunciation, while In-
donesian tends to follow Dutch, e.g., televisyen vs.
televisi, Julai vs. Juli, and Jordan vs. Yordania.
While there are many cognates between the two
languages, there are also a lot of false friends, e.g.,
polisi means policy in Malay but police in Indone-
sian. There are also many partial cognates, e.g.,
nanti means both will (future tense marker) and later
in Malay but only later in Indonesian.
Thus, fluent Malay and fluent Indonesian can dif-
fer substantially. Consider, for example, Article 1 of
the Universal Declaration of Human Rights:2
? Semua manusia dilahirkan bebas dan samarata dari segi kemu-
liaan dan hak-hak. Mereka mempunyai pemikiran dan perasaan
hati dan hendaklah bertindak di antara satu sama lain dengan
semangat persaudaraan. (Malay)
? Semua orang dilahirkan merdeka dan mempunyai marta-
bat dan hak-hak yang sama. Mereka dikaruniai akal dan
hati nurani dan hendaknya bergaul satu sama lain dalam
semangat persaudaraan. (Indonesian)
There is only 50% overlap at the word level, but
the actual vocabulary overlap is much higher, e.g.,
there is only one word in the Malay text that does
not exist in Indonesian: samarata (?equal?). Other
differences are due to the use of different morpho-
logical forms, e.g., hendaklah vs. hendaknya (?con-
science?), derivational variants of hendak (?want?).
Of course, word choice in translation is often a
matter of taste. Thus, we asked a native speaker of
Indonesian to adapt the Malay version to Indonesian
while preserving as many words as possible:
? Semua manusia dilahirkan bebas dan mempunyai martabat
dan hak-hak yang sama. Mereka mempunyai pemikiran dan
perasaan dan hendaklah bergaul satu sama lain dalam
semangat persaudaraan. (Indonesian)
2English: All human beings are born free and equal in dig-
nity and rights. They are endowed with reason and conscience
and should act towards one another in a spirit of brotherhood.
Obtaining this latter version from the original
Malay text requires three word-level operations:
(1) deletion of dari, segi, (2) insertion of yang, sama,
and (3) substitution of samarata with mempunyai.
Unfortunately, we do not have parallel Malay-
Indonesian text, which complicates the process of
learning when to apply these operations. Thus, be-
low we restrict our attention to the simplest and most
common operation of word substitution only, leav-
ing the other two3 operations for future work.
Note that word substitution is enough in many
cases, e.g., it is all that is needed for the following
Malay-Indonesian sentence pair:4
? KDNK Malaysia dijangka cecah 8 peratus pada tahun 2010.
? PDB Malaysia akan mencapai 8 persen pada tahun 2010.
4 Method
We improve machine translation from a resource-
poor language (Indonesian) to English by adapting a
bi-text for a related resource-rich language (Malay)
and English, using word-level and phrase-level para-
phrases and cross-lingual morphological variants.
4.1 Word-Level Paraphrasing
Given a Malay sentence, we generate a confusion
network containing multiple Indonesian word-level
paraphrase options for each Malay word. Each such
Indonesian option is associated with a correspond-
ing weight in the network, which is defined as the
probability of this option being a translation of the
original Malay word (see Eq. 1 below). We decode
this confusion network using a large Indonesian lan-
guage model, thus generating a ranked list of n cor-
responding adapted ?Indonesian? sentences.
Then, we pair each such adapted ?Indonesian?
sentence with the English counter-part for the
Malay sentence it was derived from, thus obtain-
ing a synthetic ?Indonesian??English bi-text. Fi-
nally, we combine this synthetic bi-text with the
original Indonesian?English one to train the final
Indonesian?English SMT system.
Below we first describe how we generate word-
level Indonesian options and corresponding weights
for the Malay words. Then, we explain how we
build, decode, and improve the confusion network.
3There are other potentially useful operations, e.g., a correct
translation for the Malay samarata can be obtained by splitting
it into the Indonesian sequence sama rata.
4Malaysia?s GDP is expected to reach 8 percent in 2010.
288
4.1.1 Inducing Word-Level Paraphrases
We use pivoting over English to induce potential
Indonesian translations for a given Malay word.
First, we generate separate word-level alignments
for the Indonesian?English and the Malay?English
bi-texts. Then, we induce Indonesian-Malay word
translation pairs assuming that if an Indonesian word
i and a Malay word m are aligned to the same
English word e, they could be mutual translations.
Each translation pair is associated with a conditional
probability, estimated by pivoting over English:
Pr(i|m) =
?
e
Pr(i|e)Pr(e|m) (1)
Pr(i|e) and Pr(e|m) are estimated using maxi-
mum likelihood from the word alignments. Follow-
ing (Callison-Burch et al2006), we further assume
that i is conditionally independent of m given e.
4.1.2 Confusion Network Construction
Given a Malay sentence, we construct an Indone-
sian confusion network, where each Malay word is
augmented with a set of network transitions: pos-
sible Indonesian word translations. The weight
of such a transition is the conditional Indonesian-
Malay translation probability as calculated by Eq. 1;
the original Malay word is assigned a weight of 1.
Note that we paraphrase each word in the in-
put Malay sentence as opposed to only those Malay
words that we believe not to exist in Indonesian, e.g.,
because they do not appear in our Indonesian mono-
lingual text. This is necessary because of the large
number of false friends and partial cognates between
Malay and Indonesian (see Section 3).
Finally, we decode the confusion network for a
Malay sentence using a large Indonesian language
model, and we extract an n-best list.5 Table 1
shows the 10-best adapted ?Indonesian? sentences6
we generated for the confusion network in Figure 1.
4.1.3 Further Refinements
Many of our paraphrases are bad: some have very
low probabilities, while others involve rare words
for which the probability estimates are unreliable.
5For balance, in case of less than n adaptations for a Malay
sentence, we randomly repeat some of the available ones.
6According to a native Indonesian speaker, options 1 and 3
in Table 1 are perfect adaptations, options 2 and 5 have a wrong
word order, and the rest are grammatical though not perfect.
Moreover, the options we propose for a Malay
word are inherently restricted to the small Indone-
sian vocabulary of the Indonesian?English bi-text.
Below we describe how we address these issues.
Score-based filtering. We filter out translation
pairs whose probabilities (Eq. 1) are lower than
some threshold (tuned on the dev dataset), e.g., 0.01.
Improved estimations for Pr(i|e). We concate-
nate k copies of the Indonesian?English bi-text and
one copy of the Malay?English bi-text, where the
value of k is selected so that we have roughly the
same number of Indonesian and Malay sentences.
Then, we generate word-level alignments for the
resulting bi-text. Finally, we truncate these align-
ments keeping them for one copy of the original
Indonesian?English bi-text only. Thus, we end up
with improved word alignments for the Indonesian?
English bi-text, and with better estimations for Eq. 1.
Since Malay and Indonesian share many cognates,
this improves word alignments for Indonesian words
that occur rarely in the small Indonesian?English bi-
text but are relatively frequent in the larger Malay?
English one; it also helps for some frequent words.
Cross-lingual morphological variants. We in-
crease the Indonesian options for a Malay word us-
ing morphology. Since the set of Indonesian op-
tions for a Malay word in pivoting is restricted to
the Indonesian vocabulary of the small Indonesian?
English bi-text, this is a severe limitation of pivot-
ing. Thus, assuming a large monolingual Indone-
sian text, we first build a lexicon of the words in the
text. Then, we lemmatize these words using two dif-
ferent lemmatizers: the Malay lemmatizer of Bald-
win and Awab (2006), and a similar Indonesian lem-
matizer. Since these two analyzers have different
strengths and weaknesses, we combine their outputs
to increase recall. Next, we group all Indonesian
words that share the same lemma, e.g., for minum,
we obtain {diminum, diminumkan, diminumnya, makan-minum,
makananminuman, meminum, meminumkan, meminumnya, meminum-
minuman, minum, minum-minum, minum-minuman, minuman, minu-
manku, minumannya, peminum, peminumnya, perminum, terminum}.
Since Malay and Indonesian are subject to the same
morphological processes and share many lemmata,
we use such groups to propose Indonesian transla-
tion options for a Malay word. We first lemmatize
the target Malay word, and then we find all groups
of Indonesian words the Malay lemmata belong to.
289
0 1
pdb|0.576172
sebesar|0.052080
maka|0.026044
perkiraan|0.026035
panggar|0.026035
rkp|0.026035
gdp|0.026034
2malaysia|1.0 3
akan|0.079793
untuk|0.050155
diharapkan|0.044511
diperkirakan|0.039131
ke|0.018960
dapat|0.018436
adalah|0.017422
menjadi|0.011655
ini|0.011158
4
remaja|0.047619
mencapai|0.042930
hit|0.030612
sr|0.030482
guncang|0.023810
di|0.022778
untuk|0.018425
hits|0.013605
diguncang|0.010074
58|1.0 6persen|0.473588per|0.148886 7
pada|1.0 8tahun|1.0 92010|1.0 10.|1.0
Figure 1: Indonesian confusion network for the Malay sentence ?KDNK Malaysia dijangka cecah 8 peratus pada tahun 2010.?
Arcs with scores below 0.01 are omitted, and words that exist in Indonesian are not paraphrased (for better readability).
Rank ?Indonesian? Sentence
1 pdb malaysia akan mencapai 8 persen pada tahun 2010 .
2 pdb malaysia untuk mencapai 8 persen pada tahun 2010 .
3 pdb malaysia diperkirakan mencapai 8 persen pada tahun 2010 .
4 maka malaysia akan mencapai 8 persen pada tahun 2010 .
5 maka malaysia untuk mencapai 8 persen pada tahun 2010 .
6 pdb malaysia dapat mencapai 8 persen pada tahun 2010 .
7 maka malaysia diperkirakan mencapai 8 persen pada tahun 2010 .
8 sebesar malaysia akan mencapai 8 persen pada tahun 2010 .
9 pdb malaysia diharapkan mencapai 8 persen pada tahun 2010 .
10 pdb malaysia ini mencapai 8 persen pada tahun 2010 .
Table 1: The 10-best ?Indonesian? sentences extracted from the confusion network in Figure 1.
The union of these groups is the set of morpholog-
ical variants that we will add to the confusion net-
work as additional options for the Malay word.7 For
example, given seperminuman (?drinking?) in the
Malay input, we first find its stem minum, and then
we get the above example set of Indonesian words,
which contains some reasonable substitutes such as
minuman (?drink?). In the confusion network, the
weight of the original Malay word is set to 1, while
the weight of a morphological option is one minus
the minimum edit distance ratio (Ristad and Yian-
ilos, 1998) between it and the Malay word, multi-
plied by the highest probability for all pivoting vari-
ants for the Malay word.
4.2 Phrase-Level Paraphrasing
Word-level paraphrasing ignores context when gen-
erating Indonesian variants, relying on the Indone-
sian language model to make the right contextual
choice. We also try to model context more directly
by generating adaptation options at the phrase level.
7While the different morphological forms typically have dif-
ferent meanings, e.g., minum (?drink?) vs. peminum (?drinker?),
in some cases the forms could have the same translation in En-
glish, e.g., minum (?drink?, verb) vs. minuman (?drink?, noun).
This is our motivation for trying morphological variants, even
though they are almost exclusively derivational, and thus quite
risky as translational variants; see also (Nakov and Ng, 2011).
Phrase-level paraphrase induction. We use
standard phrase-based SMT techniques to build sep-
arate phrase tables for the Indonesian?English and
the Malay?English bi-texts, where we have four
conditional probabilities: forward/reverse phrase
translation probability, and forward/reverse lexical-
ized phrase translation probability. We pivot over
English to generate Indonesian-Malay phrase pairs,
whose probabilities are derived from the corre-
sponding ones in the two phrase tables using Eq. 1.
Cross-lingual morphological variants. While
phrase-level paraphrasing models context better, it
remains limited in the size of its Indonesian vocab-
ulary by the small Indonesian?English bi-text, just
like word-level paraphrasing was. We address this
by transforming the sentences in the development
and the test Indonesian?English bi-texts into confu-
sion networks, where we add Malay morphological
variants for the Indonesian words, weighting them as
before. Note that we do not alter the training bi-text.
4.3 Combining Bi-texts
We combine the Indonesian?English and the syn-
thetic ?Indonesian??English bi-texts as follows:
Simple concatenation. Assuming the two bi-
texts are of comparable quality, we simply train an
SMT system on their concatenation.
290
Balanced concatenation with repetitions. How-
ever, the two bi-texts are not directly comparable and
are clearly not equally good as a source of training
data for an Indonesian-English SMT system. For
one thing, the ?Indonesian??English bi-text is ob-
tained from n-best lists, i.e., it has exactly n very
similar variants for each Malay sentence. Moreover,
the original Malay?English bi-text is much larger
in size than the Indonesian?English one, and now
it has been further expanded n times in order to be-
come an ?Indonesian??English bi-text, which means
that it will dominate the concatenation due to its
size. In order to counter-balance this, we repeat the
smaller Indonesian?English bi-text enough times so
that we can make the number of sentences it contains
roughly the same as for the ?Indonesian??English
bi-text; then we concatenate the two bi-texts and we
train an SMT system on the resulting bi-text.
Sophisticated phrase table combination. Fi-
nally, we experiment with a method for combining
phrase tables proposed in (Nakov and Ng, 2009;
Nakov and Ng, 2012). The first phrase table is
extracted from word alignments for the balanced
concatenation with repetitions, which are then trun-
cated so that they are kept for only one copy of the
Indonesian?English bi-text. The second table is built
from the simple concatenation. The two tables are
then merged as follows: all phrase pairs from the
first one are retained, and to them are added those
phrase pairs from the second one that are not present
in the first one. Each phrase pair retains its orig-
inal scores, which are further augmented with 1?3
additional feature scores indicating its origin: the
first/second/third feature is 1 if the pair came from
the first/second/both table(s), and 0 otherwise. We
experiment using all three, the first two, or the first
feature only; we also try setting the features to 0.5
instead of 0. This makes the following six combina-
tions (0, 00, 000, .5, .5.5, .5.5.5); on testing, we use
the one that achieves the highest BLEU score on the
development set.
Other possibilities for combining the phrase ta-
bles include using alternative decoding paths (Birch
et al2007), simple linear interpolation, and direct
phrase table merging with extra features (Callison-
Burch et al2006); they were previously found in-
ferior to the last two approaches above (Nakov and
Ng, 2009; Nakov and Ng, 2012).
5 Experiments
We run two kinds of experiments: (a) isolated,
where we train on the synthetic ?Indonesian??
English bi-text only, and (b) combined, where we
combine it with the Indonesian?English bi-text.
5.1 Datasets
In our experiments, we use the following datasets,
normally required for Indonesian?English SMT:
? Indonesian?English train bi-text (IN2EN):
28,383 sentence pairs; 915,192 English tokens;
796,787 Indonesian tokens;
? Indon.?English dev bi-text (IN2EN-dev):
2,000 sentence pairs; 36,584 English tokens;
35,708 Indonesian tokens;
? Indon.?English test bi-text (IN2EN-test):
2,018 sentence pairs; 37,101 English tokens;
35,509 Indonesian tokens;
? Monolingual English text (EN-LM): 174,443
sentences; 5,071,988 English tokens.
We also use a Malay?English set (to be turned
into ?Indonesian??English), and monolingual In-
donesian text (for decoding the confusion network):
? Malay?English train bi-text (ML2EN):
290,000 sentence pairs; 8,638,780 English
tokens; 8,061,729 Malay tokens;
? Monolingual Indonesian text (IN-LM):
1,132,082 sentences; 20,452,064 Indonesian
tokens.
5.2 Baseline Systems
We build five baseline systems ? two using a sin-
gle bi-text, ML2EN or IN2EN, and three combin-
ing ML2EN and IN2EN, using simple concatenation,
balanced concatenation, and sophisticated phrase ta-
ble combination. The last combination is a very
strong baseline and the most relevant one we need
to improve upon.
5.3 Isolated Experiments
The isolated experiments only use the adapted
?Indonesian??English bi-text, which allows for a di-
rect comparison to using ML2EN / IN2EN only.
5.3.1 Word-Level Paraphrasing
In our word-level paraphrasing experiments, we
adapt Malay to Indonesian using three kinds of con-
fusion networks (see Section 4.1.3 for details):
291
? CN:pivot ? using word-level pivoting only;
? CN:pivot? ? using word-level pivoting, with
probabilities from word alignments for IN2EN
that were improved using ML2EN;
? CN:pivot?+morph ? CN:pivot? augmented with
cross-lingual morphological variants.
There are two parameter values to be tuned
on IN2EN-dev for the above confusion networks:
(1) the minimum pivoting probability threshold for
the Malay-Indonesian word-level paraphrases, and
(2) the number of n-best Indonesian-adapted sen-
tences that are to be generated for each input Malay
sentence. We try {0.001, 0.005, 0.01, 0.05} for the
threshold and {1, 5, 10} for n.
5.3.2 Phrase-Level Paraphrasing
In our phrase-level paraphrasing experiments, we
use pivoted phrase tables (PPT) with the following
features for each phrase table entry (in addition to
the phrase penalty; see Section 4.2 for more details):
? PPT:1 ? only uses the forward conditional
translation probability;
? PPT:4 ? uses all four conditional probabilities;
? PPT:4::CN:morph ? PPT:4 but used with a
cross-lingual morphological confusion network
for the dev/test Indonesian sentences.
Here we tune one parameter only: the number of
n-best Indonesian-adapted sentences to be generated
for each input Malay sentence; we try {1, 5, 10}.
5.4 Combined Experiments
These experiments assess the impact of our adap-
tation approach when combined with the original
Indonesian?English bi-text IN2EN as opposed to
combining ML2EN with IN2EN (as was in the last
three baselines). We experiment with the same three
combinations: simple concatenation, balanced con-
catenation, and sophisticated phrase table combina-
tion. We tune the parameters as before; for the last
combination, we further tune the six extra feature
combinations (see Section 4.3 for details).
6 Results and Discussion
For all tables, statistically significant improvements
(p < 0.01), according to Collins et al2005)?s sign
test, over the baseline are in bold; in case of two
baselines, underline is used for the second baseline.
System BLEU
ML2EN 14.50
IN2EN 18.67
Simple concatenation 18.49
Balanced concatenation 19.79
Sophisticated phrase table combination 20.10(.5.5)
Table 2: The five baselines. The subscript indicates the
parameters found on IN2EN-dev and used for IN2EN-test.
The scores that are statistically significantly better than
ML2EN and IN2EN (p < 0.01, Collins? sign test) are
shown in bold and are underlined, respectively.
6.1 Baseline Experiments
The results for the baseline systems are shown in Ta-
ble 2. We can see that training on ML2EN instead of
IN2EN yields over 4 points absolute drop in BLEU
(Papineni et al2002) score, even though ML2EN is
about 10 times larger than IN2EN and both bi-texts
are from the same domain. This confirms the exis-
tence of important differences between Malay and
Indonesian. While simple concatenation does not
help, balanced concatenation with repetitions im-
proves by 1.12 BLEU points over IN2EN, which
shows the importance of giving IN2EN a proper
weight in the combined bi-text. This is further re-
confirmed by the sophisticated phrase table combi-
nation, which yields an additional absolute gain of
0.31 BLEU points.
6.2 Isolated Experiments
Table 3 shows the results for the isolated experi-
ments. We can see that word-level paraphrasing
improves by up to 5.56 and 1.39 BLEU points
over the two baselines (both statistically signifi-
cant). Compared to ML2EN, CN:pivot yields an ab-
solute improvement of 4.41 BLEU points, CN:pivot?
adds another 0.59, and CN:pivot?+morph adds 0.56
more. The scores for TER (v. 0.7.25) and METEOR
(v. 1.3) are on par with those for BLEU (NIST v. 13).
Table 3 further shows that the optimal parameters
for the word-level SMT systems (CN:*) involve a
very low probability cutoff, and a high number of
n-best sentences. This shows that they are robust to
noise, probably because bad source-side phrases are
unlikely to match the test-time input. Note also the
effect of repetitions: good word choices are shared
by many n-best sentences, and thus they would have
higher probabilities compared to bad word choices.
292
n-gram precision
System 1-gr. 2-gr. 3-gr. 4-gr. BLEU TER METEOR
ML2EN (baseline) 48.34 19.22 9.54 4.98 14.50 67.14 43.28
IN2EN (baseline) 55.04 23.90 12.87 7.18 18.67 61.99 54.34
CN:pivot 54.50 24.41 13.09 7.35 18.91(+4.41,+0.24)(0.005,10best) 61.94 51.07
CN:pivot? 55.05 25.09 13.60 7.69 19.50(+5.00,+0.83)(0.001,10best) 61.25 51.97
(i) CN:pivot?+morph 55.97 25.73 14.06 7.99 20.06(+5.56,+1.39)(0.005,10best) 60.31 55.65
PPT:1 55.11 25.04 13.66 7.80 19.58(+5.08,+0.91)(10best) 60.92 51.93
PPT:4 56.64 26.20 14.53 8.40 20.63(+6.13,+1.96)(10best) 59.33 54.23
(ii) PPT:4::CN:morph 56.91 26.53 14.76 8.55 20.89(+6.39,+2.22)(10best) 59.30 57.19
System combination: (i) + (ii) 57.73 27.00 15.03 8.71 21.24(+6.74,+2.57) 58.19 54.63
Table 3: Isolated experiments. The subscript shows the best tuning parameters, and the superscript shows the absolute
test improvement over the ML2EN and the IN2EN baselines. The last line shows system combination results.
Combining IN2EN with an adapted version of ML2EN
Combination with Simple Concatenation Balanced Concatenation Sophisticated Combination
(i) + ML2EN (unadapted; baseline) 18.49 19.79 20.10(.5.5)
+ CN:pivot 19.99(+1.50)(0.001,1best) 20.16
(+0.37)
(0.001,10best) 20.32
(+0.22)
(0.01,10best,.5.5)
+ CN:pivot? 20.03(+1.54)(0.05,1best) 20.80
(+1.01)
(0.05,10best) 20.55
(+0.45)
(0.05,10best,.5.5)
(ii) + CN:pivot?+morph 20.60(+2.11)(0.01,10best) 21.15
(+1.36)
(0.01,10best) 21.05
(+0.95)
(0.01,5best,00)
+ PPT:1 20.61(+2.12)(1best) 20.71
(+0.92)
(10best) 20.32
(+0.22)
(1best,000)
+ PPT:4 20.75(+2.26)(1best) 21.08
(+1.29)
(5best) 20.76
(+0.66)
(10best,.5.5.5)
(iii) + PPT:4::CN:morph 21.01(+2.52)(1best) 21.31
(+1.52)
(5best) 20.98
(+0.88)
(10best,.5)
System combination: (i) + (ii) + (iii) 21.55(+3.06) 21.64(+1.85) 21.62(+1.52)
Table 4: Combined experiments: BLEU. The best tuning parameter values are in subscript, and the absolute test
improvement over the corresponding baseline (on top of each column) is in superscript.
The gap between ML2EN and IN2EN for unigram
precision could be explained by vocabulary differ-
ences between Malay and Indonesian. Compared
to IN2EN, all CN:* models have higher 2/3/4-gram
precision. However, CN:pivot has lower unigram
precision, which could be due to bad word align-
ments, as the results for CN:pivot? show.
When morphological variants are further added,
the unigram precision improves by almost 1% ab-
solute over CN:pivot?. This shows the importance
of morphology for overcoming the limitations of the
small Indonesian vocabulary of the IN2EN bi-text.
The lower part of Table 3 shows that phrase-level
paraphrasing performs a bit better. This confirms the
importance of modeling context for closely-related
languages like Malay and Indonesian, which are rich
in false friends and partial cognates. We further
see that using more scores in the phrase table is
better. Extending the Indonesian vocabulary with
cross-lingual morphological variants is still helpful,
though not as much as at the word-level.
Finally, the combination of the output of
the best PPT and the best CN systems using
MEMT (Heafield and Lavie, 2010) yields even fur-
ther improvements, which shows that the two kinds
of paraphrases are complementary. The best overall
BLEU score for our isolated experiments is 21.24,
which is better than the results for all five baselines
in Table 2, including the three bi-text combination
baselines, which only achieve up to 20.10 BLEU.
6.3 Combined Experiments
Table 4 shows the performance of the three bi-
text combination strategies (see Section 4.3 for ad-
ditional details) when applied to combine IN2EN
(1) with the original ML2EN and (2) with various
adapted versions of it.
We can see that for the word-level paraphras-
ing experiments (CN:*), all combinations except
for CN:pivot perform significantly better than their
corresponding baselines, but the improvements are
most sizeable for the simple concatenation.
293
Note that while there is a difference of 0.31 BLEU
points between the balanced concatenation and the
sophisticated combination for the original ML2EN,
they differ little for the adapted versions. This is
probably due to the sophisticated combination as-
suming that the second bi-text is worse than the first
one, which is not really the case for the adapted ver-
sions: as Table 3 shows, they all outperform IN2EN.
Overall, phrase-level paraphrasing performs a bit
better than word-level paraphrasing, and system
combination with MEMT improves even further.
This is consistent with the isolated experiments.
7 Further Analysis
Paraphrasing non-Indonesian words only. In
CN:* above, we paraphrased each word in the Malay
input, because of false friends like polisi and partial
cognates like nanti. This risks proposing worse al-
ternatives, e.g., changing beliau (?he?, respectful) to
ia (?he?, casual), which confusion network weights
and LM would not always handle. Thus, we tried
paraphrasing non-Indonesian words only, i.e., those
not in IN-LM. Since IN-LM occasionally contains
some Malay-specific words, we also tried paraphras-
ing words that occur at most t times in IN-LM. Ta-
ble 5 shows that this hurts by up to 1 BLEU point
for t = 0; 10, and a bit less for t = 20; 40.
System BLEU
CN:pivot, t = 0 17.88(0.01,5best)
CN:pivot, t = 10 17.88(0.05,10best)
CN:pivot, t = 20 18.14(0.01,5best)
CN:pivot, t = 40 18.34(0.01,5best)
CN:pivot (i.e., paraphrase all) 18.91(0.005,10best)
Table 5: Paraphrasing non-Indonesian words only:
those appearing at most t times in IN-LM.
Manual evaluation. We asked a native Indone-
sian speaker who does not speak Malay to judge
whether our ?Indonesian? adaptations are more un-
derstandable to him than the original Malay in-
put for 100 random sentences. We presented him
with two extreme systems: (a) the conservative
CN:pivot,t=0 vs. (b) CN:pivot?+morph. Since the
latter is noisy, the top 3 choices were judged for
it. Table 6 shows that CN:pivot,t=0 is better/equal
to the original 53%/31% of the time. In contrast,
CN:pivot?+morph is typically worse than the orig-
inal; even compared to the best in top 3, the bet-
ter:worse ratio is 45%:43%.
Still, this latter model works better, which means
that phrase-based SMT systems are robust to noise
and prefer more variety. Note also that the judg-
ments were at the sentence level, while phrases are
sub-sentential, i.e., there can be many good phrases
in a ?bad? sentence.
System Better Equal Worse
CN:pivot, t = 0(Rank1) 53% 31% 16%
CN:pivot?+morph(Rank1) 38% 8% 54%
CN:pivot?+morph(Rank2) 41% 9% 50%
CN:pivot?+morph(Rank3) 32% 11% 57%
CN:pivot?+morph(Ranks:1?3) 45% 12% 43%
Table 6: Human judgments: Malay vs. ?Indonesian?.
The parameter values are those from Tables 3 and 5.
Reversed Adaptation. In all experiments above,
we were adapting the Malay sentences to look like
Indonesian. Here we try to reverse the direction of
adaptation, i.e., to adapt Indonesian to Malay: we
thus build a ?Malay? confusion network for each
dev/test Indonesian sentence to be used as an in-
put to a Malay?English SMT system trained on the
ML2EN dataset. We tried two variations of this idea:
? lattice: Use Indonesian-to-Malay confusion
networks directly as input to the ML2EN SMT
system, i.e., tune a log-linear model using con-
fusion networks for the source side of the
IN2EN-dev dataset, and then evaluate the tuned
system using confusion networks for the source
side of the IN2EN-test dataset.
? 1-best: Use the 1-best output from the
Indonesian-to-Malay confusion network for
each sentence of IN2EN-dev and IN2EN-test.
Then pair each 1-best output with the corre-
sponding English sentence. Finally, get an
adapted ?Malay??English development set and
an adapted ?Malay??English test set, and use
them to tune and evaluate the ML2EN SMT
system.
Table 7 shows that both variations perform worse
than CN:pivot. We believe this is because lattice en-
codes many options, but does not use a Malay LM,
while 1-best uses a Malay LM, but has to commit
to 1-best. In contrast, CN:pivot uses both n-best
outputs and an Indonesian LM; designing a similar
setup for reversed adaptation is a research direction
we would like to pursue in future work.
294
System BLEU
CN:pivot (Malay?Indonesian) 18.91(0.005,10best)
CN:pivot (Indonesian?Malay) ? lattice 17.22(0.05)
CN:pivot (Indonesian?Malay) ? 1-best 17.77(0.001)
Table 7: Reversed adaptation: Indonesian to Malay.
Adapting Macedonian to Bulgarian. We ex-
perimented with another pair of closely-related lan-
guages,8 Macedonian (MK) and Bulgarian (BG), us-
ing data from a different, non-newswire domain: the
OPUS corpus of movie subtitles (Tiedemann, 2009).
We used datasets of sizes that are comparable to
those in the previous experiments: 160K MK2EN
and 1.5M BG2EN sentence pairs (1.2M and 11.5M
EN words). Since the sentences were short, we used
10K MK2EN sentence pairs for tuning and testing
(77K and 72K English words). For the LM, we used
9.2M Macedonian and 433M English words.
Table 8 shows that both CN:* and PPT:* yield
statistically significant improvements over balanced
concatenation with unadapted BG2EN; system com-
bination with MEMT improves even further. This
indicates that our approach can work for other pairs
of related languages and even for other domains.
We should note though that the improvements
here are less sizeable than for Indonesian/Malay.
This may be due to our monolingual MK dataset be-
ing smaller (10M MK vs. 20M IN words), and too
noisy, containing many OCR errors, typos, concate-
nated words, and even some Bulgarian text. More-
over, Macedonian and Bulgarian are arguably some-
what more dissimilar than Malay and Indonesian.
System BLEU TER METEOR
BG2EN (baseline) 24.57 57.64 41.60
MK2EN (baseline) 26.46 54.55 46.15
Balanced concatenation of MK2EN with an adapted BG2EN
+ BG2EN (unadapted) 27.33 54.61 48.16
+ CN:pivot?+morph 27.97(+0.64,+1.51) 54.08 49.65
+ PPT:4::CN:morph 28.38(+1.05,+1.92) 53.35 48.21
Combining last three 29.05(+1.72,+2.59) 52.31 50.96
Table 8: Improving Macedonian?English SMT by
adapting Bulgarian to Macedonian.
8There is a heated political and linguistic debate about
whether Macedonian represents a separate language or is a re-
gional literary form of Bulgarian. Since there are no clear cri-
teria for distinguishing a dialect from a language, linguists are
divided on this issue. Politically, the Macedonian remains un-
recognized as a language by Bulgaria and Greece.
8 Conclusion and Future Work
We have presented a novel approach for improving
machine translation for a resource-poor language by
adapting a bi-text for a related resource-rich lan-
guage, using confusion networks, word/phrase-level
paraphrasing, and morphological analysis.
We have achieved very significant improvements
over several baselines (6.7 BLEU points over an un-
adapted version of ML2EN, 2.6 BLEU points over
IN2EN, and 1.5?3 BLEU points over three bi-text
combinations of ML2EN and IN2EN), thus proving
the potential of the idea. We have further demon-
strated the applicability of the general approach to
other languages and domains.
In future work, we would like to add word dele-
tion, insertion, splitting, and concatenation as al-
lowed editing operations. We further want to ex-
plore tighter integration of word-based and phrase-
based paraphrasing. Finally, we plan experiments
with other language pairs and application to other
linguistic problems.
Acknowledgments
We would like to give special thanks to Harta Wijaya
and Aldrian Obaja Muis, native speakers of Indone-
sian, for their help in the linguistic analysis of the
input and output of our system. We would also like
to thank the anonymous reviewers for their construc-
tive comments and suggestions, which have helped
us improve the quality of this paper.
This research is supported by the Singapore Na-
tional Research Foundation under its International
Research Centre @ Singapore Funding Initiative
and administered by the IDM Programme Office.
References
Kemal Altintas and Ilyas Cicekli. 2002. A machine
translation system between a pair of closely related
languages. In Proceedings of the 17th International
Symposium on Computer and Information Sciences,
ISCIS ?02, pages 192?196.
AiTi Aw, Min Zhang, Juan Xiao, and Jian Su. 2006. A
phrase-based statistical model for SMS text normal-
ization. In Proceedings of the 21st International Con-
ference on Computational Linguistics and 44th Annual
Meeting of the Association for Computational Linguis-
tics, ACL-COLING ?06.
295
Hitham Abo Bakr, Khaled Shaalan, and Ibrahim Ziedan.
2008. A hybrid approach for converting written Egyp-
tian colloquial dialect into diacritized Arabic. In Pro-
ceedings of the 6th International Conference on Infor-
matics and Systems, INFOS ?08.
Timothy Baldwin and Su?ad Awab. 2006. Open source
corpus analysis tools for Malay. In Proceedings of the
5th International Conference on Language Resources
and Evaluation, LREC ?06, pages 2212?2215.
Alexandra Birch, Miles Osborne, and Philipp Koehn.
2007. CCG supertags in factored statistical machine
translation. In Proceedings of the Second Workshop
on Statistical Machine Translation, WMT ?07, pages
9?16.
Chris Callison-Burch, Philipp Koehn, and Miles Os-
borne. 2006. Improved statistical machine transla-
tion using paraphrases. In Proceedings of the Human
Language Technology Conference of NAACL, HLT-
NAACL ?06, pages 17?24.
Trevor Cohn and Mirella Lapata. 2007. Machine trans-
lation by triangulation: Making effective use of multi-
parallel corpora. In Proceedings of the 45th Annual
Meeting of the Association for Computational Linguis-
tics, ACL ?07, pages 728?735.
Michael Collins, Philipp Koehn, and Ivona Kuc?erova?.
2005. Clause restructuring for statistical machine
translation. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguistics,
ACL ?05, pages 531?540.
Jan Hajic?, Jan Hric, and Vladislav Kubon?. 2000. Ma-
chine translation of very close languages. In Proceed-
ings of the Sixth Conference on Applied Natural Lan-
guage Processing, ANLP ?00, pages 7?12.
Bo Han and Timothy Baldwin. 2011. Lexical normal-
isation of short text messages: Makn sens a #twitter.
In Proceedings of the 49th Annual Meeting of the As-
sociation for Computational Linguistics: Human Lan-
guage Technologies, ACL-HLT ?11, pages 368?378.
Kenneth Heafield and Alon Lavie. 2010. Combin-
ing machine translation output with open source:
The Carnegie Mellon multi-engine machine transla-
tion scheme. The Prague Bulletin of Mathematical
Linguistics, 93(1):27?36.
Lu??s Marujo, Nuno Grazina, Tiago Lu??s, Wang Ling,
Lu??sa Coheur, and Isabel Trancoso. 2011. BP2EP -
adaptation of Brazilian Portuguese texts to European
Portuguese. In Proceedings of the 15th Conference
of the European Association for Machine Translation,
EAMT ?11, pages 129?136.
Preslav Nakov and Hwee Tou Ng. 2009. Improved statis-
tical machine translation for resource-poor languages
using related resource-rich languages. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing, EMNLP ?09, pages 1358?1367.
Preslav Nakov and Hwee Tou Ng. 2011. Trans-
lating from morphologically complex languages: A
paraphrase-based approach. In Proceedings of the
49th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies,
ACL-HLT ?11, pages 1298?1307.
Preslav Nakov and Hwee Tou Ng. 2012. Improving
statistical machine translation for a resource-poor lan-
guage using related resource-rich languages. Journal
of Artificial Intelligence Research, 44:179?222.
Preslav Nakov and Jo?rg Tiedemann. 2012. Combin-
ing word-level and character-level models for machine
translation between closely-related languages. In Pro-
ceedings of the 50th Annual Meeting of the Association
for Computational Linguistics, ACL-Short ?12.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting of the Association for Computa-
tional Linguistics, ACL ?02, pages 311?318.
Eric Ristad and Peter Yianilos. 1998. Learning string-
edit distance. IEEE Transactions on Pattern Analysis
and Machine Intelligence, 20(5):522?532.
Wael Salloum and Nizar Habash. 2011. Dialectal
to Standard Arabic paraphrasing to improve Arabic-
English statistical machine translation. In Proc. of the
Workshop on Algorithms and Resources for Modelling
of Dialects and Language Varieties, pages 10?21.
Hassan Sawaf. 2010. Arabic dialect handling in hybrid
machine translation. In Proceedings of the 9th Confer-
ence of the Association for Machine Translation in the
Americas, AMTA ?09.
Kevin P. Scannell. 2006. Machine translation for closely
related language pairs. In Proceedings of the LREC
2006 Workshop on Strategies for Developing Machine
Translation for Minority Languages.
Jo?rg Tiedemann. 2009. News from OPUS - a collection
of multilingual parallel corpora with tools and inter-
faces. In Recent Advances in Natural Language Pro-
cessing, volume V, pages 237?248.
Masao Utiyama and Hitoshi Isahara. 2007. A com-
parison of pivot methods for phrase-based statistical
machine translation. In Proceedings of the Human
Language Technology Conference of NAACL, HLT-
NAACL ?07, pages 484?491.
Hua Wu and Haifeng Wang. 2009. Revisiting pivot lan-
guage approach for machine translation. In Proceed-
ings of the Joint Conference of the 47th Annual Meet-
ing of the ACL, ACL ?09, pages 154?162.
Xiaoheng Zhang. 1998. Dialect MT: a case study be-
tween Cantonese and Mandarin. In Proceedings of the
17th International Conference on Computational Lin-
guistics, COLING ?98, pages 1460?1464.
296
Proceedings of NAACL-HLT 2013, pages 471?481,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
A Beam-Search Decoder for Normalization of Social Media Text
with Application to Machine Translation
Pidong Wang
Department of Computer Science
National University of Singapore
13 Computing Drive
Singapore 117417
wangpd@comp.nus.edu.sg
Hwee Tou Ng
Department of Computer Science
National University of Singapore
13 Computing Drive
Singapore 117417
nght@comp.nus.edu.sg
Abstract
Social media texts are written in an infor-
mal style, which hinders other natural lan-
guage processing (NLP) applications such as
machine translation. Text normalization is
thus important for processing of social media
text. Previous work mostly focused on nor-
malizing words by replacing an informal word
with its formal form. In this paper, to fur-
ther improve other downstream NLP applica-
tions, we argue that other normalization oper-
ations should also be performed, e.g., missing
word recovery and punctuation correction. A
novel beam-search decoder is proposed to ef-
fectively integrate various normalization oper-
ations. Empirical results show that our system
obtains statistically significant improvements
over two strong baselines in both normaliza-
tion and translation tasks, for both Chinese
and English.
1 Introduction
Social media texts include SMS (Short Message
Service) messages, Twitter messages, Facebook up-
dates, etc. They are different from formal texts due
to their significant informal characteristics, so they
always pose difficulties for applications such as ma-
chine translation (MT) (Aw et al, 2005) and named
entity recognition (Liu et al, 2011), because of a
lack of training data containing informal texts. Thus,
the applications always suffer from a substantial per-
formance drop when evaluated on social media texts.
For example, Ritter et al (2011) reported a drop
from 90% to 76% on part-of-speech tagging, and
Foster et al (2011) found a drop of 20% in depen-
dency parsing.
Creating training data of social media texts specif-
ically for a text processing task is time-consuming.
For example, to create parallel Chinese-English
training texts for translation of social media texts,
it takes three minutes on average to translate an in-
formally written social media text of eleven words
from Chinese into English. On the other hand, it
takes thirty seconds to normalize the same message,
a six-fold increase in speed. After training a text nor-
malization system to normalize social media texts,
we can use an existing statistical machine translation
(SMT) system trained on normal texts (non-social
media texts) to carry out translation. So we argue
that normalization followed by regular translation is
a more practical approach. Thus, text normalization
is important for social media text processing.
Most previous work on normalization of social
media text focused on word substitution (Beaufort
et al, 2010; Gouws et al, 2011; Han and Baldwin,
2011; Liu et al, 2012). However, we argue that
some other normalization operations besides word
substitution are also critical for subsequent natu-
ral language processing (NLP) applications, such
as missing word recovery (e.g., zero pronouns) and
punctuation correction.
In this paper, we propose a novel beam-search
decoder for normalization of social media text for
MT. Our decoder can effectively integrate differ-
ent normalization operations together. In contrast
to previous work, some of our normalization opera-
tions are specifically designed for MT, e.g., missing
word recovery based on conditional random fields
471
(CRF) (Lafferty et al, 2001) and punctuation cor-
rection based on dynamic conditional random fields
(DCRF) (Sutton et al, 2004).
To the best of our knowledge, our work is the
first to perform missing word recovery and punc-
tuation correction for normalization of social me-
dia text, and also the first to perform message-level
normalization of Chinese social media text. We in-
vestigate the effects on translating social media text
after addressing various characteristics of informal
social media text through normalization. To show
the applicability of our normalization approach for
different languages, we experiment with two lan-
guages, Chinese and English. We achieved statisti-
cally significant improvements over two strong base-
lines: an improvement of 9.98%/7.35% in BLEU
scores for normalization of Chinese/English social
media text, and an improvement of 1.38%/1.35% in
BLEU scores for translation of Chinese/English so-
cial media text. We created two corpora: a Chinese
corpus containing 1,000 Weibo1 messages with their
normalizations and English translations; and another
similar English corpus containing 2,000 SMS mes-
sages from the NUS SMS corpus (How and Kan,
2005). As far as we know, our corpora are the first
publicly available Chinese/English corpora for nor-
malization and translation of social media text2.
2 Related Work
Zhu et al (2007) performed text normalization of
informally written email messages using CRF (Laf-
ferty et al, 2001). Due to its importance, normaliza-
tion of social media text has been extensively studied
recently. Aw et al (2005) proposed a noisy chan-
nel model consisting of different operations: sub-
stitution of non-standard acronyms, deletion of fla-
vor words, and insertion of auxiliary verbs and sub-
ject pronouns. Choudhury et al (2007) used hid-
den Markov model to perform word-level normal-
ization. Kobus et al (2008) combined MT and auto-
matic speech recognition (ASR) to better normalize
French SMS message. Cook and Stevenson (2009)
used an unsupervised noisy channel model consid-
ering different word formation processes. Han and
Baldwin (2011) normalized informal words using
1A Chinese version of Twitter at www.weibo.com
2Available at www.comp.nus.edu.sg/?nlp/corpora.html
morphophonemic similarity. Pennell and Liu (2011)
only dealt with SMS abbreviations. Xue et al (2011)
normalized social media texts incorporating ortho-
graphic, phonetic, contextual, and acronym factors.
Liu et al (2012) designed a system combining dif-
ferent human perspectives to perform word-level
normalization. Oliva et al (2013) normalized Span-
ish SMS messages using a normalization and a pho-
netic dictionary. For normalization of Chinese so-
cial media text, Xia et al (2005) investigated infor-
mal phrase detection, and Li and Yarowsky (2008)
mined informal-formal phrase pairs from Web cor-
pora.
All the above work focused on normalizing
words. In contrast, our work also performs other
normalization operations such as missing word re-
covery and punctuation correction, to further im-
prove machine translation. Previously, Aw et al
(2006) adopted phrase-based MT to perform SMS
normalization, and required a relatively large num-
ber of manually normalized SMS messages. In con-
trast, our approach performs beam search at the sen-
tence level, and does not require large training data.
We evaluate the success of social media text nor-
malization in the context of machine translation, so
research on machine translation of social media text
is relevant to our work. However, there is not much
comparative evaluation of social media text transla-
tion other than the Haitian Creole to English SMS
translation task in the 2011 Workshop on Statistical
Machine Translation (WMT 2011) (Callison-Burch
et al, 2011). However, the setup of the WMT 2011
task is different from ours, in that the task provided
parallel training data of SMS texts and their transla-
tions. As such, text normalization is not necessary
in that task. For example, the best reported system
in that task (Costa-jussa` and Banchs, 2011) did not
perform SMS message normalization.
In speech to speech translation (Paul, 2009;
Nakov et al, 2009), the input texts contain wrongly
transcribed words due to errors in automatic speech
recognition, whereas social media texts contain ab-
breviations, new words, etc. Although the input
texts in both cases deviate from normal texts, the ex-
act deviations are different.
472
Category Freq. Example
Punctuation 81 ??[hi]?(???[hi .]);
Pronunciation 47 ?[watch](??[don?t]);??(???[this]);
New word 43 ?[bud](??[cute]);
Interjection 27 ??[ok]?[oh](??[ok]);
Pronoun 23 ??[want](?[i]??[want]);
Segmentation 14 ???(??[don?t]???[this]);
Pronunciation 288 4(for); oredi(already);
Abbreviation 98 slp(sleep); whr(where);
Prefix 74 lect(lecture); doin(doing);
Punctuation 69 where r u(where r u ?);
Interjection 68 ok lor .(ok .);
Quotation 24 im sure(i ?m sure); dont go(don ?t go);
Be 24 i coming; you free?;
Tokenization 19 ok.why ?(ok . why ?);
Time 2 end at 730(end at 7:30); 1130 am(11:30 am);
Table 1: Occurrence frequency of various informal char-
acteristics in 200 Chinese/English social media texts.
3 Challenges in Normalization of Social
Media Text
To better understand the informal characteristics of
social media texts, we first analyzed a small sample
of such texts in Chinese and English. We crawled
200 Chinese messages from Weibo. The informal
characteristics of these messages are shown in the
first half of Table 1. The manually normalized form
is shown in round brackets, and the English gloss is
shown in square brackets. Omitted, extraneous, and
misused punctuation symbols occur frequently. On
average, each Chinese message contains only less
than one informal word, and many informal words
are either new words or existing words with new
meaning. The messages also contain redundant in-
terjections like ??[oh]?. Pronouns are often omit-
ted in Chinese messages, especially for ??[I]?. Chi-
nese informal words can be wrongly segmented due
to lack of word segmentation training data contain-
ing informal words.
Similarly, 200 English SMS messages were ran-
domly selected from the NUS SMS corpus (How
and Kan, 2005). The informal characteristics of
these messages are shown in the second half of Ta-
ble 1. We found that our English messages contain
more informal words than Chinese messages. En-
glish words are shortened in three ways: (1) using
a shorter word form with similar pronunciation; (2)
abbreviating a formal word; and (3) using only a pre-
fix of a formal word. Other informal characteristics
include: (1) informal punctuation conventions in-
cluding omitted and misused punctuation; (2) redun-
dant interjections; (3) quotation-related problems,
e.g., omitted quotation marks; (4) ?be? omission;
(5) tokenization problems; and (6) informally writ-
ten time expressions.
4 Methods
As can be seen in Section 3, social media texts of
different languages exhibit different informal char-
acteristics. For example, English messages have
more informal words than Chinese messages, while
punctuation problems are more prevalent for Chi-
nese messages. Also, fixing different types of infor-
mal characteristics often depends on each other. For
example, to be able to correct punctuation, it helps
that the surrounding words are already correctly nor-
malized. On the other hand, with punctuation al-
ready corrected, it will be easier to normalize the
surrounding words.
In this section, we first present our punctuation
correction method based on a DCRF model, and
then present missing word recovery based on a CRF
model. Next, we present a novel beam-search de-
coder for normalization of social media text, which
can effectively integrate different normalization op-
erations, including statistical and rule-based normal-
ization. Finally, details of text normalization for
Chinese and English are presented.
4.1 Punctuation Correction
In normalization of social media text, punctuation
correction is also important besides word normal-
ization, as the subsequent NLP applications are typ-
ically trained on formal texts with correct punctua-
tion. We define punctuation correction as correcting
punctuation in sentences which may have no or un-
reliable punctuation. The task performs three punc-
tuation operations: insertion, deletion, and substitu-
tion.
To our knowledge, no previous work has been
done on punctuation correction for normalization of
social media text. In ASR, punctuation prediction
only inserts punctuation symbols into ASR output
that has no punctuation (Kim and Woodland, 2001;
Huang and Zweig, 2002), but without punctuation
deletion or substitution. Lu and Ng (2010) argued
that punctuation prediction should be jointly per-
473
formed with sentence boundary detection, so they
modeled punctuation prediction using a two-layer
DCRF model (Sutton et al, 2004).
We also believe that punctuation correction is
closely related to sentence boundary detection.
Thus, we propose a two-layer DCRF model for
punctuation correction. Layer 1 gives the actual
punctuation tags None, Comma, Period, Question-
Mark, and Exclamatory-Mark. Layer 2 gives
the sentence boundary, including tags Declarative-
Begin, Declarative-In, Question-Begin, Question-In,
Exclamatory-Begin, and Exclamatory-In, indicating
whether the current word is at the beginning of (or
inside) a declarative, question, or exclamatory sen-
tence.
We use word n-grams (n = 1, 2, 3) and punctu-
ation symbols within 5 words before and after the
current word as binary features in the DCRF model.
As an example, Table 2 shows the tags and features
for the word ?where? in the message ?where| .|? i|
can| not| see| you| !|!?, where the punctuation sym-
bols after the vertical bars are the corrected symbols.
Tags Content
Layer 1 Question-Mark
Layer 2 Question-Begin
Features Content
unigram <s>@-1 where@0 i@1 can@2 not@3
see@4 you@5
bigram <s>+where@-1 where+i@0 i+can@1
can+not@2 not+see@3 see+you@4
you+</s>@5
trigram <s>+where+i@-1 where+i+can@0
i+can+not@1 can+not+see@2
not+see+you@3 see+you+</s>@4
punctuation .@0 !@5
Table 2: An example of tags and features used in punctu-
ation correction.
Due to the lack of informal training texts with cor-
rected punctuation, we train our punctuation correc-
tion model on formal texts with synthetically cre-
ated punctuation errors. We randomly add, delete,
and substitute punctuation symbols in formal texts
with equal probabilities. Specifically, for s ? {, .?!},
P (none|s) = P (, |s) = P (.|s) = P (?|s) =
P (!|s) = 0.2 denotes the probability of replacing
a punctuation symbol s (replacing s by none de-
notes deletion); and for a real word (not a punctua-
tion symbol)w, P (none|w) = P (, |w) = P (.|w) =
P (?|w) = P (!|w) = 0.2 denotes the probability
of inserting a punctuation symbol after w (inserting
none after w denotes no insertion).
4.2 Missing Word Recovery
As shown in Section 3, some words are often omit-
ted in social media texts, e.g., the pronoun ??[I]?
in Chinese and be in English. To fix this problem,
we propose a CRF model to recover such missing
words. We explain the CRF model using be in En-
glish. The CRF model has five tags: None, BE, IS,
ARE, and AM. In an input sentence, every token (in-
cluding words, punctuation symbols, and a special
start-of-sentence placeholder) will be assigned a tag,
denoting the insertion of the form of be after the to-
ken. We use the same n-gram features as our punc-
tuation correction model, but exclude the punctua-
tion features. The model is trained on synthetically
created training texts in which be has been randomly
deleted with probability 0.5.
4.3 A Decoder for Text Normalization
When designing our text normalization system, we
aim for a general framework that can be applied to
text normalization across different languages with
minimal effort. This is a challenging task, since so-
cial media texts in different languages exhibit differ-
ent informal characteristics, as illustrated in Section
3. Motivated by the beam-search decoders for SMT
(Koehn et al, 2007), ASR (Young et al, 2002), and
grammatical error correction (Dahlmeier and Ng,
2012), we propose a novel beam-search decoder for
normalization of social media text.
Given an input message, the normalization de-
coder searches for its best normalization, i.e., the
best hypothesis, by iteratively performing two sub-
tasks: (1) producing new sentence-level hypotheses
from hypotheses in the current stack, carried out by
hypothesis producers; and (2) evaluating the new hy-
potheses to retain good ones, carried out by feature
functions. Each hypothesis is the result of apply-
ing successive normalization operations on the ini-
tial input message, where each normalization oper-
ation is carried out by one hypothesis producer that
deals with one aspect of the informal characteristics
of social media text. The hypotheses are grouped
into stacks, where stack i stores all hypotheses ob-
tained by applying i hypothesis producers on the in-
put message. The beam-search algorithm is shown
474
where you .
whr you are
whr u
whr you
Dictionary: u=>you
be
where you
Abbreviation: whr=>where
Punctuation
where are you
Be
where are you ?
Punctuation
Figure 1: An example search tree when normalizing ?whr
u?. The solid (dashed) boxes represent good (bad) hy-
potheses. The hypothesis producers are indicated on the
edges.
in Algorithm 1, and Figure 1 shows an example
search tree for an English message.
Algorithm 1 The beam-search decoder
INPUT: a raw message M whose length is N
RETURN: the best normalization for M
1: initialize hypothesisStacks[N+1] and hypothesisProducers;
2: add the initial hypothesis M to stack hypothesisStacks[0];
3: for i? 0 to N-1 do
4: for each hypo in hypothesisStacks[i] do
5: for each producer in hypothesisProducers do
6: for each newHypo produced by producer from hypo do
7: add newHypo to hypothesisStacks[i+1];
8: prune hypothesisStacks[i+1];
9: return the best hypothesis in hypothesisStacks[0...N];
We give the details of the hypothesis producers
for Chinese and English social media texts in the
next two subsections. A number of the hypothesis
producers detect and deal with informal words w
present in a hypothesis by relying on bigram counts
of w in a large corpus of formal texts. Specifically,
a word w in a hypothesis . . . w?1ww1 . . . is consid-
ered an informal word if both bigrams w?1w and
ww1 occur infrequently (? 5) in the formal corpus.
Given a hypothesis message h, the feature func-
tions include a language model score (the normal-
ized sentence probability of h), an informal word
count penalty (the number of informal words de-
tected in h), and count feature functions. Each count
feature function gives the count of the modifications
made by a hypothesis producer. The feature func-
tions are used by the decoder to distinguish good
hypotheses from bad ones. All feature functions are
combined using a linear model to obtain the score
for a hypothesis h:
score(h) =
?
i
?ifi(h), (1)
where fi is the i-th feature function with weight ?i.
The weights of the feature functions are tuned using
the pairwise ranking optimization algorithm (Hop-
kins and May, 2011) on the development set.
4.4 Text Normalization for Chinese
Taking into account the informal characteristics of
Chinese social media text in Section 3, we design the
following hypothesis producers for Chinese text
normalization:
Dictionary: We have manually assembled a dic-
tionary of 703 informal-formal word pairs from the
Internet. The word pairs are used to produce new
hypotheses. For example, given a hypothesis ??
?[magical horse] ??[time]?, if the dictionary
contains the word pair ?(??,??[what])?, the
Dictionary hypothesis producer generates a new hy-
pothesis ???[what]??[time]?.
Punctuation: A punctuation correction model
(Section 4.1) is adopted to correct punctuation in
the current hypothesis, e.g., it may normalize ??
?[what]??[time]? into ???????.
Pronunciation: We use Chinese Pinyin to model
the pronunciation similarity of words. To accom-
plish this, we pair some Pinyin initials that sound
similar into a group. The groups of paired Pinyin
initials are (c, ch), (s, sh), and (z, zh). For exam-
ple, given the hypothesis ???[Beijing]??[tube]
??[come]?, the Pinyin of the informal word ??
?? is ?t ong z i?. The Pinyin of the formal word
???[comrade]? is ?t ong zh i?. Since the sim-
ilar sounding Pinyin initials z and zh are paired
in a group, a new hypothesis ???[Beijing] ?
?[comrade]??[come]? can be produced.
In practice, this hypothesis producer can propose
many spurious candidates w? for an informal word
w. As such, after we replace w by w? in the hypoth-
esis, we require that some 4-gram containing w? and
its surrounding words in the hypothesis appears in a
formal corpus. We call this filtering process contex-
tual filtering.
475
Pronoun: With the method of Section 4.2, a CRF
model is trained to recover the missing pronoun
??[I]?.
Interjection: If a word w in a pre-defined list
of frequent redundant interjections appears at the
end of a sentence, we produce a new hypothesis by
removing w, e.g., from ???[ok] ?[oh]? to ??
?[ok]?.
Resegmentation: This hypothesis producer fixes
word segmentation problems. If an informal word is
a concatenation of two constituent informal words
w1 and w2 in our normalization dictionary, the in-
formal word will be segmented into two words w1
and w2. As a result, the Dictionary hypothesis pro-
ducer can subsequently normalize w1 and w2.
4.5 Text Normalization for English
Similar to Chinese text normalization, we also cre-
ate the Dictionary, Punctuation, and Interjection hy-
pothesis producers for English text normalization.
We also add the following English-specific hypoth-
esis producers:
Pronunciation: This hypothesis producer uses
pronunciation similarity to find formal candidates
for a given informal word. It considers a word as
a sequence of letters and convert it into a sequence
of phones using phrase-based SMT trained on the
CMU pronouncing dictionary (Weide, 1998). Simi-
lar sounding phones are paired together in a group:
(ah, ao), (ow, uw), and (s, z). To illustrate, in the hy-
pothesis ?wat is it?, the informal word ?wat? maps
to the phone sequence ?w ao t?. Since the formal
word ?what? maps to the phone sequence ?w ah t?
and the phones ah and ao are paired in a group, the
new hypothesis ?what is it? is generated.
Be: We train a CRF model to recover missing
words be, as described in Section 4.2.
Retokenization: This hypothesis producer fixes
tokenization problems. More precisely, given an in-
formal word which is not a URL or email address
and contains a period, it splits the informal word at
the period. For example, ?how r u.where r u? is nor-
malized to ?how r u . where r u?.
Prefix: This hypothesis producer generates a for-
mal word w? for an informal word w if w is a prefix
of w?. To avoid spurious candidates, we only gener-
ate w? if |w| ? 3 and |w?| ? |w| ? 4.
Quotation: If an informal word ends with a letter
in (m, s, t) and if the word produced by inserting a
quotation mark before the letter is a formal word, a
new hypothesis with the quotation mark inserted is
produced. This hypothesis producer thus generates
?i?m? from ?im?, ?she?s? from ?shes?, ?isn?t? from
?isnt?, etc.
Abbreviation: Letters denoting the vowels in a
formal word are often deleted to form an infor-
mal word. This hypothesis producer generates a
formal word w? from an informal word w if w?
can be obtained from w by adding missing vowels.
To avoid spurious candidates, we only consider w
where |w| ? 2.
Time: If a number can be a potential time expres-
sion and appears after ?at? or before ?am? or ?pm?, a
new hypothesis is produced by changing the number
into a time expression, e.g., ?1130 am? is normal-
ized to ?11 : 30 am?.
Since the Pronunciation, Prefix, and Abbreviation
hypothesis producers can propose spurious candi-
dates for an informal word, we also use contextual
filtering to further filter the candidates for these hy-
pothesis producers.
5 Experiments
5.1 Evaluation Corpora
As previous work (Choudhury et al, 2007; Han and
Baldwin, 2011; Liu et al, 2012) mostly focused
on word normalization, no data is available with
corrected punctuation and recovered missing words.
We thus create the following two corpora (Table 3):
Chinese-English corpus We crawled 1,000 mes-
sages from Weibo which were first normalized into
formal Chinese and then translated into formal En-
glish. The first half of the corpus serves as our de-
velopment set to tune our text normalization decoder
for Chinese, while the second half serves as the test
set to evaluate text normalization for Chinese and
Chinese-English MT.
English-Chinese corpus From the NUS English
SMS corpus (How and Kan, 2005), we randomly se-
lected 2,000 messages. The messages were first nor-
malized into formal English and then translated into
formal Chinese. Similar to the Chinese-English cor-
pus, the first half of the corpus serves as our devel-
opment set while the second half serves as the test
set.
476
Corpus # messages # tokens (EN/CN/NCN)
CN2EN-dev 500 6.95K/5.45K/5.70K
CN2EN-test 500 7.14K/5.64K/5.82K
Corpus # messages # tokens (EN/CN/NEN)
EN2CN-dev 1,000 16.63K/18.14K/18.21K
EN2CN-test 1,000 16.14K/17.69K/17.76K
Table 3: Statistics of the corpora. CN2EN-dev/CN2EN-
test is the development/test set in our Chinese-
English experiments. EN2CN-dev/EN2CN-test is the
development/test set in our English-Chinese experi-
ments. NEN/NCN denotes manually normalized En-
glish/Chinese texts.
The formal corpus used (as described in Section
4) is the concatenation of two Chinese-English spo-
ken parallel corpora: the IWSLT 2009 corpus (Paul,
2009) and another spoken text corpus collected at
the Harbin Institute of Technology3. The language
model used for Chinese (English) text normalization
is the Chinese (English) side of the formal corpus
and the LDC Chinese (English) Gigaword corpus.
To evaluate the effect of text normalization
on MT, we build phrase-based MT systems
using Moses (Koehn et al, 2007), with word
alignments generated by GIZA++ (Och and
Ney, 2003). The MT training data contains
the above formal corpus and some LDC4 par-
allel corpora (LDC2000T46, LDC2002E18,
LDC2003E14, LDC2004E12, LDC2005T06,
LDC2005T10, LDC2007T23, LDC2008T06,
LDC2008T08, LDC2008T18, LDC2009T02,
LDC2009T06, LDC2009T15, LDC2010T03). In
total, 214M/192M English/Chinese tokens are used
to train our MT systems. The language model
of the Chinese-English (English-Chinese) MT
system is the English (Chinese) side of the FBIS
corpus (LDC2003E14) and the English (Chinese)
Gigaword corpus. Our MT systems are tuned on the
manually normalized messages of our development
sets.
Following (Aw et al, 2006; Oliva et al, 2013),
we use BLEU scores (Papineni et al, 2002) to eval-
uate text normalization. We also use BLEU scores
to evaluate MT quality. We use the sign test to de-
termine statistical significance, for both text normal-
ization and translation.
3http://mitlab.hit.edu.cn/
4http://www.ldc.upenn.edu/Catalog/
5.2 Baselines
We compare our text normalization decoder against
three baseline methods for performing text normal-
ization. We then send the respective normalized
texts to the same MT system to evaluate the effect
of text normalization on MT.
The simplest baseline for text normalization is
one that does no text normalization. The raw text
(un-normalized) is simply passed on to the MT sys-
tem for translation. We call this baseline ORIGINAL.
The second baseline, LATTICE, is to use a lattice
to normalize text. For each input message, a lattice
is generated in which each informal word is aug-
mented with its formal candidates taken from the
same normalization dictionary (downloaded from
Internet) used in our text normalization decoder. The
lattice is then decoded by the same language model
used in our text normalization decoder to generate
the normalized text (Stolcke, 2002). Another pos-
sible way of using lattice is to directly feed the lat-
tice to the MT system (Eidelman et al, 2011), but
since in this paper, we assume that the MT system
can only translate plain text, we leave this as future
work.
The third baseline, PBMT, is a competitive base-
line that performs text normalization via phrase-
based MT, as proposed in Aw et al (2006). Moses
(Koehn et al, 2007) is used to perform text normal-
ization, by ?translating? un-normalized text to nor-
malized text. The training data used is the same
development set used in our text normalization de-
coder. The normalized text is then sent to our MT
system for translation. This method was also used in
the SMS translation task of WMT 2011 by (Stymne,
2011).
In the tables showing experimental results, nor-
malization and translation BLEU scores that are sig-
nificantly higher than (p < 0.01) the LATTICE or
PBMT baseline are in bold or underlined, respec-
tively.
5.3 Chinese-English Experimental Results
The Chinese-English normalization and translation
results are shown in Table 4. The first group of
experiments is the three baselines, and the second
group is an oracle experiment using manually nor-
malized messages as the output of text normaliza-
477
BLEU scores (%)
System Normalization MT
ORIGINAL baseline 61.01 9.06
LATTICE baseline 74.52 11.50
PBMT baseline 76.77 12.65
ORACLE 100.00 15.04
Dictionary 77.80 12.35
Punctuation 65.95 9.63
Pronunciation 61.30 9.13
Pronoun 61.11 9.01
Interjection 61.05 9.14
Resegmentation 60.98 9.03
Dictionary 77.80 12.35
+Punctuation 84.69 13.37
+Pronunciation 84.69 13.40
+Pronoun 84.96 13.50
+Interjection 85.33 13.68
+Resegmentation 86.75 14.03
Table 4: Chinese-English experimental results.
tion which indicates the theoretical upper bounds of
perfect normalization. In the normalization experi-
ments, the ORIGINAL baseline gets a BLEU score
of 61.01%, and the LATTICE baseline greatly im-
proves the ORIGINAL baseline by 13.51%, which
shows that the dictionary collected from the Inter-
net is highly effective in text normalization. The
PBMT baseline further improves the BLEU score by
2.25%. In the corresponding MT experiments, as the
normalization BLEU scores increase, the MT BLEU
scores also increase.
The third group is the isolated experiments, i.e.,
each experiment only uses one hypothesis producer.
As expected, the individual hypothesis producers
alone do not work well except the Dictionary hy-
pothesis producer. One interesting discovery is that
the Dictionary hypothesis producer outperforms the
LATTICE baseline, which shows that our normaliza-
tion decoder can utilize the dictionary more effec-
tively, probably because of the additional features
used in our normalization decoder such as the infor-
mal word penalty. The Resegmentation hypothesis
producer alone worsens the BLEU scores, since it
can only split informal words, and is designed to
work together with other hypothesis producers to
normalize words.
The last group is the combined experiments. We
add each hypothesis producer in the order of its nor-
malization effectiveness in the isolated experiments.
Adding the Punctuation hypothesis producer greatly
improves the BLEU scores of both normalization
and translation, which confirms the importance of
punctuation correction. The Pronoun and Inter-
jection hypothesis producers also contribute some
improvements. Finally, Resegmentation signifi-
cantly improves the normalization/translation BLEU
scores by 1.42%/0.35%. Compared with the isolated
experiments, the combined experiments show that
our normalization decoder can effectively integrate
different hypothesis producers to achieve better per-
formance for both text normalization and transla-
tion.
Overall, in the Chinese text normalization exper-
iments, our normalization decoder outperforms the
best baseline PBMT by 9.98% in BLEU score. In
the Chinese-English MT experiments, the normal-
ized texts output by our normalization decoder lead
to improved translation quality compared to normal-
ization by the PBMT baseline, by 1.38% in BLEU
score.
5.4 English-Chinese Experimental Results
The English-Chinese normalization and translation
results are shown in Table 5, with the same experi-
mental setup as in the Chinese-English experiments.
The text normalization BLEU score of the ORIG-
INAL baseline is much lower in English compared
to Chinese, since the English texts contain more in-
formal words. Again, the individual hypothesis pro-
ducers alone do not work well, except the Dictio-
nary hypothesis producer. The Retokenization hy-
pothesis producer greatly improves the normaliza-
tion/translation BLEU scores by 2.37%/0.86%. The
Punctuation hypothesis producer helps less for En-
glish compared to Chinese, suggesting that our Chi-
nese texts contain noisier punctuation.
Overall, we achieved similar improvements in En-
glish text normalization and English-Chinese trans-
lation, and the improvements in BLEU scores are
7.35% and 1.35% respectively.
5.5 Further Analysis
The effect of contextual filtering. To measure
the effect of contextual filtering proposed in Sec-
tion 4.4, we ran our normalization decoder with-
out contextual filtering. We obtained BLEU scores
of 65.05%/22.38% in the English-Chinese experi-
ments, which were lower than 66.54%/22.81% ob-
478
BLEU scores (%)
System Normalization MT
ORIGINAL baseline 37.38 13.63
LATTICE baseline 56.98 20.56
PBMT baseline 59.19 21.46
ORACLE 100.00 28.48
Dictionary 59.90 20.84
Retokenization 38.79 14.06
Prefix 38.68 13.90
Interjection 38.37 13.92
Quotation 38.04 13.65
Abbreviation 37.94 13.74
Time 37.65 13.66
Pronunciation 37.62 13.80
Punctuation 37.62 13.79
Be 37.47 13.59
Dictionary 59.90 20.84
+Retokenization 62.27 21.70
+Prefix 63.22 21.88
+Interjection 64.85 22.30
+Quotation 65.24 22.31
+Abbreviation 65.35 22.34
+Time 65.59 22.38
+Pronunciation 65.64 22.38
+Punctuation 66.38 22.74
+Be 66.54 22.81
Table 5: English-Chinese experimental results.
tained with contextual filtering. This shows the ben-
eficial effect of contextual filtering.
Decoding speed. The decoding speed of our text
normalization decoder was 0.2 seconds per message
on our test sets, using a 2.27 GHz Intel Xeon CPU
with 32 GB memory.
The effect of text normalization decoder on
MT. We manually analyzed the effect of our text
normalization decoder on MT. For example, given
the un-normalized English test message ?yeah must
sign up , im in lt25?, our English-Chinese MT
system translated it into ??[yeah] ??[must] ?
?[sign up] ? im ?[in] lt25? On the other hand,
our normalization decoder normalized it into ?yeah
must sign up , i ?m in lt25 .? which was then trans-
lated into ?????? ,?? lt25?? by our MT
system. This example shows that our text normal-
ization decoder uses word normalization and punc-
tuation correction to improve translation.
6 Conclusion
This paper presents a novel beam-search decoder
for normalization of social media text. Our de-
coder for text normalization effectively integrates
multiple normalization operations. In our experi-
ments, we achieved statistically significant improve-
ments over two strong baselines: an improvement of
9.98%/7.35% in BLEU scores for normalization of
Chinese/English social media text, and an improve-
ment of 1.38%/1.35% in BLEU scores for transla-
tion of Chinese/English social media text. Future
work can investigate how to more tightly integrate
our beam-search decoder for text normalization with
a standard MT decoder, e.g., by using a lattice or an
n-best list.
Acknowledgments
We thank all the anonymous reviewers for their com-
ments which have helped us improve this paper.
This research is supported by the Singapore Na-
tional Research Foundation under its International
Research Centre @ Singapore Funding Initiative
and administered by the IDM Programme Office.
References
AiTi Aw, Min Zhang, PohKhim Yeo, ZhenZhen Fan, and
Jian Su. 2005. Input normalization for an English-
to-Chinese SMS translation system. In Proceedings of
the Tenth MT Summit.
AiTi Aw, Min Zhang, Juan Xiao, and Jian Su. 2006. A
phrase-based statistical model for SMS text normaliza-
tion. In Proceedings of ACL-COLING.
Richard Beaufort, Sophie Roekhaut, Louise-Ame?lie
Cougnon, and Ce?drick Fairon. 2010. A hybrid
rule/model-based finite-state framework for normaliz-
ing SMS messages. In Proceedings of ACL.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Omar Zaidan. 2011. Findings of the 2011 work-
shop on statistical machine translation. In Proceedings
of WMT.
Monojit Choudhury, Rahul Saraf, Vijit Jain, Animesh
Mukherjee, Sudeshna Sarkar, and Anupam Basu.
2007. Investigation and modeling of the structure of
texting language. International Journal on Document
Analysis and Recognition, 10(3-4):157?174.
Paul Cook and Suzanne Stevenson. 2009. An unsu-
pervised model for text message normalization. In
Proceedings of the Workshop on Computational Ap-
proaches to Linguistic Creativity.
Marta R. Costa-jussa` and Rafael E. Banchs. 2011. The
BM-I2R Haitian-Cre?ole-to-English translation system
description for the WMT 2011 evaluation campaign.
In Proceedings of WMT.
Daniel Dahlmeier and Hwee Tou Ng. 2012. A beam-
search decoder for grammatical error correction. In
Proceedings of EMNLP-CoNLL.
479
Vladimir Eidelman, Kristy Hollingshead, and Philip
Resnik. 2011. Noisy SMS machine translation in low-
density languages. In Proceedings of WMT.
Jennifer Foster, O?zlem C?etinoglu, Joachim Wagner,
Joseph Le Roux, Stephen Hogan, Joakim Nivre,
Deirdre Hogan, and Josef Van Genabith. 2011. #
hardtoparse: POS tagging and parsing the twitterverse.
In Proceedings of the AAAI Workshop On Analyzing
Microtext.
Stephan Gouws, Dirk Hovy, and Donald Metzler. 2011.
Unsupervised mining of lexical variants from noisy
text. In Proceedings of the First Workshop on Unsu-
pervised Learning in NLP.
Bo Han and Timothy Baldwin. 2011. Lexical normalisa-
tion of short text messages: Makn sens a #twitter. In
Proceedings of ACL-HLT.
Mark Hopkins and Jonathan May. 2011. Tuning as rank-
ing. In Proceedings of EMNLP.
Yijue How and Min-Yen Kan. 2005. Optimizing pre-
dictive text entry for short message service on mobile
phones. In Proceedings of Human Computer Inter-
faces International.
Jing Huang and Geoffrey Zweig. 2002. Maximum en-
tropy model for punctuation annotation from speech.
In Proceedings of ICSLP.
Ji Hwan Kim and P. C. Woodland. 2001. The use of
prosody in a combined system for punctuation gener-
ation and speech recognition. In Proceedings of Eu-
rospeech.
Catherine Kobus, Franc?ois Yvon, and Ge?raldine
Damnati. 2008. Normalizing SMS: are two metaphors
better than one? In Proceedings of COLING.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of ACL Demo and Poster Sessions.
John D. Lafferty, Andrew McCallum, and Fernando C. N.
Pereira. 2001. Conditional random fields: Probabilis-
tic models for segmenting and labeling sequence data.
In Proceedings of the Eighteenth International Con-
ference on Machine Learning.
Zhifei Li and David Yarowsky. 2008. Mining and mod-
eling relations between formal and informal Chinese
phrases from web corpora. In Proceedings of EMNLP.
Xiaohua Liu, Shaodian Zhang, Furu Wei, and Ming
Zhou. 2011. Recognizing named entities in tweets.
In Proceedings of ACL-HLT.
Fei Liu, Fuliang Weng, and Xiao Jiang. 2012. A broad-
coverage normalization system for social media lan-
guage. In Proceedings of ACL.
Wei Lu and Hwee Tou Ng. 2010. Better punctuation
prediction with dynamic conditional random fields. In
Proceedings of EMNLP.
Preslav Nakov, Chang Liu, Wei Lu, and Hwee Tou Ng.
2009. The NUS statistical machine translation system
for IWSLT 2009. In Proceedings of the International
Workshop on Spoken Language Translation.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
J. Oliva, J. I. Serrano, M. D. Del Castillo, and A?. Igesias.
2013. A SMS normalization system integrating mul-
tiple grammatical resources. Natural Language Engi-
neering, 19(1):121?141.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proceedings of ACL.
Michael Paul. 2009. Overview of the IWSLT 2009 eval-
uation campaign. In Proceedings of the International
Workshop on Spoken Language Translation.
Deana L. Pennell and Yang Liu. 2011. A character-
level machine translation approach for normalization
of SMS abbreviations. In Proceedings of IJCNLP.
Alan Ritter, Sam Clark, Mausam, and Oren Etzioni.
2011. Named entity recognition in tweets: An experi-
mental study. In Proceedings of EMNLP.
Andreas Stolcke. 2002. SRILM ? An extensible lan-
guage modeling toolkit. In Proceedings of the Inter-
national Conference on Spoken Language Processing.
Sara Stymne. 2011. Spell checking techniques for re-
placement of unknown words and data cleaning for
Haitian Creole SMS translation. In Proceedings of
WMT.
Charles Sutton, Khashayar Rohanimanesh, and Andrew
McCallum. 2004. Dynamic conditional random
fields: Factorized probabilistic models for labeling and
segmenting sequence data. In Proceedings of the 21st
International Conference on Machine Learning.
Robert L. Weide. 1998. The CMU pronouncing
dictionary. URL: http://www.speech.cs.cmu.edu/cgi-
bin/cmudict.
Yunqing Xia, Kam-Fai Wong, and Wei Gao. 2005. NIL
is not nothing: Recognition of Chinese network infor-
mal language expressions. In 4th SIGHAN Workshop
on Chinese Language Processing.
Zhenzhen Xue, Dawei Yin, and Brian D. Davison. 2011.
Normalizing microtext. In Proceedings of the AAAI
Workshop on Analyzing Microtext.
Steve Young, Gunnar Evermann, Dan Kershaw, Gareth
Moore, Julian Odell, Dave Ollason, Valtcho Valtchev,
and Phil Woodland. 2002. The HTK book. Cam-
bridge University Engineering Department.
480
Conghui Zhu, Jie Tang, Hang Li, Hwee Tou Ng, and Tie-
Jun Zhao. 2007. A unified tagging approach to text
normalization. In Proceedings of ACL.
481
