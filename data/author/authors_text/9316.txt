Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007, pp. 1129?1133,
Prague, June 2007. c?2007 Association for Computational Linguistics
A Two-stage Parser for Multilingual Dependency Parsing
Wenliang Chen, Yujie Zhang, Hitoshi Isahara
Computational Linguistics Group
National Institute of Information and Communications Technology
3-5 Hikari-dai, Seika-cho, Soraku-gun, Kyoto, Japan, 619-0289
{chenwl, yujie, isahara}@nict.go.jp
Abstract
We present a two-stage multilingual de-
pendency parsing system submitted to the
Multilingual Track of CoNLL-2007. The
parser first identifies dependencies using a
deterministic parsing method and then labels
those dependencies as a sequence labeling
problem. We describe the features used in
each stage. For four languages with differ-
ent values of ROOT, we design some spe-
cial features for the ROOT labeler. Then we
present evaluation results and error analyses
focusing on Chinese.
1 Introduction
The CoNLL-2007 shared tasks include two tracks:
the Multilingual Track and Domain Adaptation
Track(Nivre et al, 2007). We took part the Multi-
lingual Track of all ten languages provided by the
CoNLL-2007 shared task organizers(Hajic? et al,
2004; Aduriz et al, 2003; Mart?? et al, 2007; Chen
et al, 2003; Bo?hmova? et al, 2003; Marcus et al,
1993; Johansson and Nugues, 2007; Prokopidis et
al., 2005; Csendes et al, 2005; Montemagni et al,
2003; Oflazer et al, 2003) .
In this paper, we describe a two-stage parsing
system consisting of an unlabeled parser and a se-
quence labeler, which was submitted to the Multi-
lingual Track. At the first stage, we use the pars-
ing model proposed by (Nivre, 2003) to assign the
arcs between the words. Then we obtain a depen-
dency parsing tree based on the arcs. At the sec-
ond stage, we use a SVM-based approach(Kudo and
Matsumoto, 2001) to tag the dependency label for
each arc. The labeling is treated as a sequence la-
beling problem. We design some special features
for tagging the labels of ROOT for Arabic, Basque,
Czech, and Greek, which have different labels for
ROOT. The experimental results show that our ap-
proach can provide higher scores than average.
2 Two-Stage Parsing
2.1 The Unlabeled Parser
The unlabeled parser predicts unlabeled directed de-
pendencies. This parser is primarily based on the
parsing models described by (Nivre, 2003). The al-
gorithm makes a dependency parsing tree in one left-
to-right pass over the input, and uses a stack to store
the processed tokens. The behaviors of the parser
are defined by four elementary actions (where TOP
is the token on top of the stack and NEXT is the next
token in the original input string):
? Left-Arc(LA): Add an arc from NEXT to TOP;
pop the stack.
? Right-Arc(RA): Add an arc from TOP to
NEXT; push NEXT onto the stack.
? Reduce(RE): Pop the stack.
? Shift(SH): Push NEXT onto the stack.
Although (Nivre et al, 2006) used the pseudo-
projective approach to process non-projective de-
pendencies, here we only derive projective depen-
dency tree. We use MaltParser(Nivre et al, 2006)
1129
V0.41 to implement the unlabeled parser, and use
the SVM model as the classifier. More specifically,
the MaltParser use LIBSVM(Chang and Lin, 2001)
with a quadratic kernel and the built-in one-versus-
all strategy for multi-class classification.
2.1.1 Features for Parsing
The MaltParser is a history-based parsing model,
which relies on features of the derivation history
to predict the next parser action. We represent the
features extracted from the fields of the data repre-
sentation, including FORM, LEMMA, CPOSTAG,
POSTAG, and FEATS. We use the features for all
languages that are listed as follows:
? The FORM features: the FORM of TOP and
NEXT, the FORM of the token immediately
before NEXT in original input string, and the
FORM of the head of TOP.
? The LEMMA features: the LEMMA of TOP
and NEXT, the LEMMA of the token immedi-
ately before NEXT in original input string, and
the LEMMA of the head of TOP.
? The CPOS features: the CPOSTAG of TOP and
NEXT, and the CPOSTAG of next left token of
the head of TOP.
? The POS features: the POSTAG of TOP and
NEXT, the POSTAG of next three tokens af-
ter NEXT, the POSTAG of the token immedi-
ately before NEXT in original input string, the
POSTAG of the token immediately below TOP,
and the POSTAG of the token immediately af-
ter rightmost dependent of TOP.
? The FEATS features: the FEATS of TOP and
NEXT.
But note that the fields LEMMA and FEATS are not
available for all languages.
2.2 The Sequence Labeler
2.2.1 The Sequence Problem
We denote by x = x
1
, ..., xn a sentence with n
words and by y a corresponding dependency tree. A
dependency tree is represented from ROOT to leaves
1The tool is available at
http://w3.msi.vxu.se/?nivre/research/MaltParser.html
with a set of ordered pairs (i, j) ? y in which xj is a
dependent and xi is the head. We have produced the
dependency tree y at the first stage. In this stage, we
assign a label l
(i,j) to each pair.
As described in (McDonald et al, 2006), we treat
the labeling of dependencies as a sequence labeling
problem. Suppose that we consider a head xi with
dependents xj1, ..., xjM . We then consider the la-
bels of (i, j1), ..., (i, jM) as a sequence. We use the
model to find the solution:
lmax = arg max
l
s(l, i, y, x) (1)
And we consider a first-order Markov chain of la-
bels.
We used the package YamCha (V0.33)2 to imple-
ment the SVM model for labeling. YamCha is a
powerful tool for sequence labeling(Kudo and Mat-
sumoto, 2001).
2.2.2 Features for Labeling
After the first stage, we know the unlabeled de-
pendency parsing tree for the input sentence. This
information forms the basis for part of the features
of the second stage. For the sequence labeler, we
define the individual features, the pair features, the
verb features, the neighbor features, and the position
features. All the features are listed as follows:
? The individual features: the FORM, the
LEMMA, the CPOSTAG, the POSTAG, and
the FEATS of the parent and child node.
? The pair features: the direction of depen-
dency, the combination of lemmata of the
parent and child node, the combination of
parent?s LEMMA and child?s CPOSTAG, the
combination of parent?s CPOSTAG and child?s
LEMMA, and the combination of FEATS of
parent and child.
? The verb features: whether the parent or child
is the first or last verb in the sentence.
? The neighbor features: the combination of
CPOSTAG and LEMMA of the left and right
neighbors of the parent and child, number of
children, CPOSTAG sequence of children.
2YamCha is available at
http://chasen.org/?taku/software/yamcha/
1130
? The position features: whether the child is the
first or last word in the sentence and whether
the child is the first word of left or right of par-
ent.
2.2.3 Features for the Root Labeler
Because there are four languages have different
labels for root, we define the features for the root
labeler. The features are listed as follows:
? The individual features: the FORM, the
LEMMA, the CPOSTAG, the POSTAG, and
the FEATS of the parent and child node.
? The verb features: whether the child is the first
or last verb in the sentence.
? The neighbor features: the combination of
CPOSTAG and LEMMA of the left and right
neighbors of the parent and child, number of
children, CPOSTAG sequence of children.
? The position features: whether the child is the
first or last word in the sentence and whether
the child is the first word of left or right of par-
ent.
3 Evaluation Results
We evaluated our system in the Multilingual Track
for all languages. For the unlabeled parser, we chose
the parameters for the MaltParser based on perfor-
mance from a held-out section of the training data.
We also chose the parameters for Yamcha based on
performance from training data.
Our official results are shown at Table 1. Perfor-
mance is measured by labeled accuracy and unla-
beled accuracy. These results showed that our two-
stage system can achieve good performance. For all
languages, our system provided better results than
average performance of all the systems(Nivre et al,
2007). Compared with top 3 scores, our system
provided slightly worse performance. The reasons
may be that we just used projective parsing algo-
rithms while all languages except Chinese have non-
projective structure. Another reason was that we did
not tune good parameters for the system due to lack
of time.
Data Set LA UA
Arabic 74.65 83.49
Basque 72.39 78.63
Catalan 86.66 90.87
Chinese 81.24 85.91
Czech 73.69 80.14
English 83.81 84.91
Greek 74.42 81.16
Hungarian 75.34 79.25
Italian 82.04 85.91
Turkish 76.31 81.92
average 78.06 83.22
Table 1: The results of proposed approach. LA-
BELED ATTACHMENT SCORE(LA) and UNLA-
BELED ATTACHMENT SCORE(UA)
4 General Error Analysis
4.1 Chinese
For Chinese, the system achieved 81.24% on labeled
accuracy and 85.91% on unlabeled accuracy. We
also ran the MaltParser to provide the labels. Be-
sides the same features, we added the DEPREL fea-
tures: the dependency type of TOP, the dependency
type of the token leftmost of TOP, the dependency
type of the token rightmost of TOP, and the de-
pendency type of the token leftmost of NEXT. The
labeled accuracy of MaltParser was 80.84%, 0.4%
lower than our system.
Some conjunctions, prepositions, and DE3 at-
tached to their head words with much lower ac-
curacy: 74% for DE, 76% for conjunctions, and
71% for prepositions. In the test data, these words
formed 19.7%. For Chinese parsing, coordination
and preposition phrase attachment were hard prob-
lems. (Chen et al, 2006) defined the special features
for coordinations for chunking. In the future, we
plan to define some special features for these words.
Now we focused words where most of the errors
occur as Table 2 shows. For ??/DE?, there was
32.4% error rate of 383 occurrences. And most of
them were assigned incorrect labels between ?prop-
erty? and ?predication?: 45 times for ?property? in-
stead of ?predication? and 20 times for ?predica-
tion? instead of ?property?. For examples, ??/DE?
3including ??/?/?/??.
1131
num any head dep both
?/ DE 383 124 35 116 27
a/ C 117 38 36 37 35
?/ P 67 20 6 19 5
??/ N 31 10 8 4 2
?/ V 72 8 8 8 8
Table 2: The words where most of errors occur in
Chinese data.
in ???/?/??/??(popular TV channel)? was
to be tagged as ?property? instead of ?predication?,
while ??/DE? in ????/?/??(volunteer of
museum)? was to be tagged as ?predication? instead
of ?property?. It was very hard to tell the labels be-
tween the words around ???. Humans can make
the distinction between property and predication for
???, because we have background knowledge of
the words. So if we can incorporate the additional
knowledge for the system, the system may assign
the correct label.
For ?a/C?, it was hard to assign the head, 36
wrong head of all 38 errors. It often appeared at
coordination expressions. For example, the head
of ?a? at ??/?/?/?/a/?/?/?/??/(Besides
extreme cool and too amazing)? was ????, and
the head of ?a? at ????/??/?/??/a/?/?
?/?/??(Give the visitors solid and methodical
knowledge)? was ????.
5 Conclusion
In this paper, we presented our two-stage depen-
dency parsing system submitted to the Multilingual
Track of CoNLL-2007 shared task. We used Nivre?s
method to produce the dependency arcs and the se-
quence labeler to produce the dependency labels.
The experimental results showed that our system can
provide good performance for all languages.
References
A. Abeille?, editor. 2003. Treebanks: Building and Using
Parsed Corpora. Kluwer.
I. Aduriz, M. J. Aranzabe, J. M. Arriola, A. Atutxa,
A. Diaz de Ilarraza, A. Garmendia, and M. Oronoz.
2003. Construction of a Basque dependency treebank.
In Proc. of the 2nd Workshop on Treebanks and Lin-
guistic Theories (TLT), pages 201?204.
A. Bo?hmova?, J. Hajic?, E. Hajic?ova?, and B. Hladka?. 2003.
The PDT: a 3-level annotation scenario. In Abeille?
(Abeille?, 2003), chapter 7, pages 103?127.
C.C. Chang and C.J. Lin. 2001. LIBSVM: a library
for support vector machines. Software available at
http://www. csie. ntu. edu. tw/cjlin/libsvm, 80:604?
611.
K. Chen, C. Luo, M. Chang, F. Chen, C. Chen, C. Huang,
and Z. Gao. 2003. Sinica treebank: Design criteria,
representational issues and implementation. In Abeille?
(Abeille?, 2003), chapter 13, pages 231?248.
Wenliang Chen, Yujie Zhang, and Hitoshi Isahara. 2006.
An empirical study of chinese chunking. In COL-
ING/ACL 2006(Poster Sessions), Sydney, Australia,
July.
D. Csendes, J. Csirik, T. Gyimo?thy, and A. Kocsor. 2005.
The Szeged Treebank. Springer.
J. Hajic?, O. Smrz?, P. Zema?nek, J. S?naidauf, and E. Bes?ka.
2004. Prague Arabic dependency treebank: Develop-
ment in data and tools. In Proc. of the NEMLAR In-
tern. Conf. on Arabic Language Resources and Tools,
pages 110?117.
R. Johansson and P. Nugues. 2007. Extended
constituent-to-dependency conversion for English. In
Proc. of the 16th Nordic Conference on Computational
Linguistics (NODALIDA).
Taku Kudo and Yuji Matsumoto. 2001. Chunking
with support vector machines. In In Proceedings of
NAACL01.
M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993.
Building a large annotated corpus of English: the Penn
Treebank. Computational Linguistics, 19(2):313?330.
M. A. Mart??, M. Taule?, L. Ma`rquez, and M. Bertran.
2007. CESS-ECE: A multilingual and multilevel
annotated corpus. Available for download from:
http://www.lsi.upc.edu/?mbertran/cess-ece/.
Ryan McDonald, Kevin Lerman, and Fernando Pereira.
2006. Multilingual dependency analysis with a two-
stage discriminative parser. In Proceedings of the
Tenth Conference on Computational Natural Lan-
guage Learning (CoNLL-X), pages 216?220, New
York City, June. Association for Computational Lin-
guistics.
S. Montemagni, F. Barsotti, M. Battista, N. Calzolari,
O. Corazzari, A. Lenci, A. Zampolli, F. Fanciulli,
M. Massetani, R. Raffaelli, R. Basili, M. T. Pazienza,
D. Saracino, F. Zanzotto, N. Nana, F. Pianesi, and
R. Delmonte. 2003. Building the Italian Syntactic-
Semantic Treebank. In Abeille? (Abeille?, 2003), chap-
ter 11, pages 189?210.
1132
J. Nivre, J. Hall, J. Nilsson, G. Eryigit, and S Marinov.
2006. Labeled pseudo-projective dependency parsing
with support vector machines.
J. Nivre, J. Hall, S. Ku?bler, R. McDonald, J. Nilsson,
S. Riedel, and D. Yuret. 2007. The CoNLL 2007
shared task on dependency parsing. In Proc. of the
Joint Conf. on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL).
J. Nivre. 2003. An efficient algorithm for projective
dependency parsing. Proceedings of the 8th Inter-
national Workshop on Parsing Technologies (IWPT),
pages 149?160.
K. Oflazer, B. Say, D. Zeynep Hakkani-Tu?r, and G. Tu?r.
2003. Building a Turkish treebank. In Abeille?
(Abeille?, 2003), chapter 15, pages 261?277.
P. Prokopidis, E. Desypri, M. Koutsombogera, H. Papa-
georgiou, and S. Piperidis. 2005. Theoretical and
practical issues in the construction of a Greek depen-
dency treebank. In Proc. of the 4th Workshop on Tree-
banks and Linguistic Theories (TLT), pages 149?160.
1133
Building an Annotated Japanese-Chinese Parallel Corpus  
? A Part of NICT Multilingual Corpora  
Yujie Zhang and  Kiyotaka Uchimoto and Qing Ma and Hitoshi Isahara 
 
National Institute of Information and Communications Technology 
3-5 Hikaridai, Seika-cho, Soraku-gun, Kyoto, Japan, 619-0289 
(yujie, uchimoto,qma, isahara)@nict.go.jp
 
Abstract 
We are constricting a Japanese-Chinese 
parallel corpus, which is a part of the 
NICT Multilingual Corpora. The corpus is 
general domain, of large scale of about 
40,000 sentence pairs, long sentences, 
annotated with detailed information and 
high quality. To the best of our knowledge, 
this will be the first annotated Japanese-
Chinese parallel corpus in the world. We 
created the corpus by selecting Japanese 
sentences from Mainichi Newspaper and 
then manually translating them into 
Chinese. We then annotated the corpus 
with morphological and syntactic 
structures and alignments at word and 
phrase levels. This paper describes the 
specification in human translation and the 
scheme of detailed information annotation, 
and the tools we developed in the corpus 
construction. The experience we obtained 
and points we paid special attentions are 
also introduced for share with other 
researches in corpora construction.     
1 Introduction 
A parallel corpus is a collection of articles, 
paragraphs, or sentences in two different languages. 
Since a parallel corpus contains translation 
correspondences between the source text and its 
translations at different level of constituents, it is a 
critical resource for extracting translation 
knowledge in machine translation (MT). Although 
recently some versions of machine translation 
software have become available in the market, 
translation quality is still a significant problem. 
Therefore, a detailed examination into human 
translation is still required. This will provide a basis 
for radically improving machine translation in the 
near future. In addition, in MT system development, 
the example-based method and the statistics-based 
method are widely researched and applied. So, 
parallel corpora are required by the translation 
studies and practical system development.   
The raw text of a parallel corpus contains 
implicit knowledge. If we annotate some 
information, we can get explicit knowledge from 
the corpus. The more information that is annotated 
on a parallel corpus, the more knowledge we can 
get from the corpus. The parallel corpora of 
European languages are usually raw texts without 
annotation on syntactic structure since their 
syntactic structures are similar and MT does not 
require such annotation information. However, 
when language pairs are different in syntactic 
structures, such as the pair of English and Japanese 
and the pair of Japanese and Chinese, 
transformation between syntactic structures is 
difficult. A parallel corpus annotated with syntactic 
structures would thus be helpful to MT.  Besides 
MT, an annotated parallel corpus can be applied to 
cross-lingual information retrieval, language 
teaching, machine-aided translation, bilingual 
lexicography, and word-sense disambiguation.  
Parallel corpora between European languages 
are well developed and are available through the 
Linguistic Data Consortium (LDC). However, 
parallel corpora between European languages and 
Asian languages are less developed, and parallel 
corpora between two Asian languages are even less 
developed.  
The National Institute of Information and 
Communications Technology therefore started a 
project to build multilingual parallel corpora in 
2002 (Uchimoto et al, 2004). The project focuses 
on Asian language pairs and annotation of detailed 
information, including syntactic structure and 
alignment at word and phrase levels. We call the 
corpus the NICT Multilingual Corpora. The corpus 
will be open to the public in the near future. 
2 Overview of the NICT Multilingual 
Corpora 
At present, a Japanese-English parallel corpus and a 
Japanese-Chinese parallel corpus are under 
construction following systematic specifications. 
The parallel texts in each corpus consist of the 
original text in the source language and its 
translations in the target language. The original data 
is from newspaper articles or journals, such as 
85
Mainichi Newspaper in Japanese. The original 
articles were translated by skilled translators. In 
human translation, the articles of one domain were 
all assigned to the same translators to maintain 
consistent terminology in the target language. 
Different translators then revised the translated 
articles. Each article was translated one sentence to 
one sentence, so the obtained parallel corpora are 
already sentence aligned.  
  The details of the current version of the NICT 
Multilingual Corpora are listed in Table 1. 
Corpora Total Original Translation 
Japanese 
(19,669 
sentences, 
Mainichi 
Newspaper) 
English 
Translation 
Japanese-
English 
Parallel 
Corpus 
37,987 
sentence 
pairs; 
(English 
900,000 
words) English 
(18,318 
Sentences, 
Wall Street 
Journal) 
Japanese 
Translation 
Japanese-
Chinese 
Parallel 
Corpus 
38,383 
sentence 
pairs; 
(Chinese 
1,410,892 
Characters, 
926,838 
words) 
Japanese 
(38,383 
sentences, 
Mainichi 
Newspaper) 
Chinese 
Translation 
Table 1 Details of current version of NICT Multilingual 
Corpora 
 
   The following is an example of English and 
Chinese translations of a Japanese sentence from 
Mainichi Newspaper. 
[Ex. 1] 
J: ????????????????????????
?????? 
E: They were all about nineteen years old and had 
no strength left even to answer questions. 
C: ?????????????????????
?????????????  
 
In addition to the human translation, another big 
task is annotating the information. We finish the 
task by two steps: automatic annotation and human 
revision. In automatic annotation, we applied 
existing analysis techniques and tag sets. In human 
revision, we developed assisting tools that have 
powerful functions to help annotators in revision. 
The annotation task for each language included 
morphological and syntactic structure annotation.  
The annotation task for each language pair included 
alignments at word and phrase level.  
The NICT Multilingual Corpora constructed in 
this way have the following characteristics. 
(1) Since the original data is from newspaper and 
journals, the domain of each corpus is therefore rich.  
(2) Each corpus consists of original sentences and 
their translations, so they are already sentence 
aligned. In translation of each sentence, the context 
of the article is also considered. Thus, the context of 
each original article is also well maintained in its 
translation, which can be exploited in the future. 
(3) The corpora are annotated at high quality with 
morphological and syntactic structures and 
word/phrase alignment.  
 In the following section, we will describe the 
details in the construction of the Japanese-Chinese 
parallel corpus. 
3 Human Translation from Japanese to 
Chinese   
About 40,000 Japanese sentences from issues of 
Mainichi Newspaper were translated by skilled 
translators. The translation guidelines were as 
follows. 
(1) One Japanese sentence is translated into one 
Chinese sentence. 
(2) Among several translation candidates, the one 
that is close to the original sentence in syntactic 
structure is preferred. The aim is to avoid 
translating a sentence too freely, i.e., 
paraphrasing. 
(3) To obtain intelligible Chinese translations, 
information of the proceeding sentences in the 
same article should be added. Especially, a 
subject should be supplemented because a 
subject is usually required in Chinese, while in 
Japanese subjects are often omitted . 
(4)  To obtain natural Chinese translations, 
supplement, deletion, replacement, and 
paraphrase  should be made when necessary. 
When a translation is very long, word order can 
be changed or commons can be inserted. These 
are the restrictions on (2), i.e., the naturalness 
of the Chinese translations is the priority.  
 
  One problem in translation is how to translate 
proper nouns in the newspaper articles. We pay 
special attentions to them in the following way.  
(1) Proper nouns  
When proper nouns did not exist in Japanese-
Chinese dictionaries, new translations were created 
and then confirmed using the Chinese web. For 
kanji in proper nouns, if there was a Chinese 
character having the same orthography as the kanji, 
the Chinese character was used in the Chinese 
translation; if there was a traditional Chinese 
character having the same orthography as the kanji, 
the simplified character of the traditional Chinese 
character was used in the translation; otherwise, a 
Chinese character whose orthography is similar to 
that of the kanji was used in the translation.  
(2) Special things in Japan 
86
 Explanations were added if necessary. For example, 
?????, translated from ????? (grand sumo 
tournament), is well known in China, while ????, 
translated from ???? (spring labor offensive), is 
not known in China. In this case, an explanation 
???????? was added behind the unfamiliar 
term. We attempt to introduce new words about 
Japanese culture into Chinese through the 
construction of the corpus.    
 
   Producing high-quality Chinese translations is 
crucial to this parallel corpus. We controlled the 
quality by the following treatments.  
(1) The first revision of a translated article was 
conducted by a different translator after the first 
translation. The reviewers checked whether the 
meanings of the Chinese translations corresponded 
accurately to the meanings of the original sentences 
and modified the Chinese translations if necessary. 
(2) The second revision was conducted by Chinese 
natives without referring to the original sentences. 
The reviewers checked whether the Chinese 
translations were natural and passed the unnatural 
translations back to translators for modification. 
(3) The third revision was conducted by a Chinese 
native in the annotation process of Chinese 
morphological information. The words that did not 
exist in the dictionary of contemporary Chinese 
were checked to determine whether they were new 
words. If not, the words were designated as 
informal or not written language and were replaced 
with suitable words. The word sequences that 
missed the Chinese language model?s part-of-
speech chain were also adjusted.        
 
 Until now, 38,383 Japanese sentences have 
been translated to Chinese, and of those, 22,000 
Chinese translations have been revised three times, 
and we are still working on the remaining 18,000 
Chinese translations.  
4 Morphological Information Annotation 
Annotation consists of automatic analyses and 
manual revision. 
4.1 Annotation on Japanese Sentences  
Japanese morphological and syntactic analyses 
follow the definitions of part-of-speech categories 
and syntactic labels of the Corpus of Spontaneous 
Japanese (Maekawa, 2000).  
A morphological analyzer developed in that 
project was applied for automatic annotation on the 
Japanese sentences and then the automatically 
tagged sentences were revised manually. An 
annotated senetence is illustrated in Figure 1, which 
is the Japanese sentence in Ex. 1 in Section 2.  
 
 
 
 
 
 
# S-ID:950104141-008 
* 0 2D 
???? ???? * ?? * * * 
* 1 2D 
?? ?????? * ?? ?? * * 
? ?? * ??? ???????? * * 
?? ??? * ??? ???????? * * 
? ? * ?? ???? * * 
* 2 6D 
?? ???? * ?? ???? * * 
? ? ? ??? * ??? ???????? 
? ? * ?? ?? * * 
* 3 4D 
?? ???? * ?? ???? * * 
? ? * ?? ??? * * 
* 4 5D 
??? ???? ??? ?? * ???? ??? 
* 5 6D 
?? ???? * ?? ???? * * 
? ? * ?? ??? * * 
* 6 -1D 
??? ???? ?? ?? * ???? ???? 
? ? ?? ??? ?????? ???? ??? 
?? ?? ?? ??? ???? ???? ??? 
? ? * ?? ?? * * 
EOJ 
 
Figure 1. An annotated Japanese sentence 
 
The data of one sentence begins from the line ?# S-
ID... ? and ends with the mark ?EOJ?. The line 
headed by ?*? indicates the beginning of a phrase 
and the following lines are morphemes in that 
phrase. For example, the line ?* 0 2D? indicates the 
phrase whose number is 0. The following line ???
??  ? ? ? ?  * ? ?  * * *? indicates the 
morpheme in the phrase. There are seven fields in 
each morpheme line, token form, phonetic alphabet, 
dictionary form, part-of-speech, sub-part-of-speech, 
verbal category and conjugation form. In the line ?* 
0 2D?, the numeral 2 in ?2D? indicates that the 
phrase 0 ?????? modifies the phrase 2 ???
? ? ?. The syntactic structure analysis adopts 
dependency-structure analysis in which modifier-
modified relations between phrases are determined. 
The dependency-structure of the example in Figure 
1 is demonstrated in Figure 2. 
 
???? ?????? ???? ??? ??? ??? ???????
 
       Figure 2  Example of syntactic structure 
 
87
4.2 Annotation on Chinese Sentences 
For Chinese morphological analysis, we used the 
analyser developed by Peking University, where the 
research on definition of Chinese words and the 
criteria of word segmentation has been conducted 
for over ten years. The achievements include a 
grammatical knowledge base of contemporary 
Chinese, an automatic morphological analyser, and 
an annotated People?s Daily Corpus. Since the 
definition and tagset are widely used in Chinese 
language processing, we also took the criteria as the 
basis of our guidelines.  
A morphological analyzer developed by Peking 
University (Zhou and Yu, 1994) was applied for 
automatic annotation of the Chinese sentences and 
then the automatically tagged sentences were 
revised by humans. An annotated sentence is 
illustrated in Figure 3, which is the Chinese 
sentence in Ex. 1 in Section 2. 
 
S-ID: 950104141-008 
??/r  ??/j  ??/n  ?/d  ?/v   ??/m  ?/q 
??/m  ?/u  ???/n   ?/w  ??/r  ??/d  
?/p  ??/v  ??/n  ?/u  ??/n  ?/d   
??/v  ?/w 
Figure 3  An annotated Chinese sentence  
 
4.3 Tool for Manual Revision 
We developed a tool to assist annotators in revision. 
The tool has both Japanese and Chinese versions. 
Here, we introduce the Chinese version. The input 
of the tool is the automatically segmented and part-
of-speech tagged sentences and the output is revised 
data. The basic functions include separating a 
sequence of characters into two words, combining 
two segmented words into one word, and selecting 
a part-of-speech for a segmented word from a list of 
parts-of-speech. In addition, the tool has the 
following functions. 
(1) Retrieves a word in the grammatical knowledge 
base of contemporary Chinese of Peking University 
(Yu et al, 1997).  
This is convenient when annotators want to 
confirm whether a segmented word is authorized by 
the grammatical knowledge base, and when they 
want to know the parts-of-speech of a word defined 
by the grammatical knowledge base.  
(2) Retrieves a word in other annotated corpora or 
the sentences that have been revised.   
This is convenient when annotators want to see 
how the same word has been annotated before.  
(3) Retrieves a word in the current file.  
It collects all the sentences in the current file 
that contain the same word and then sorts their 
context on the left and right of the word. By 
referring to the sorted contexts, annotators can 
select words with the same syntactic roles and 
change all of the parts-of-speech to a certain one all 
in one operation. This is convenient when 
annotators want to process the same word in 
different sentences, aiming for consistency in 
annotation.     
(4) Adds new words to the grammatical knowledge 
base dynamically.  
The updated grammatical knowledge base can 
be used by the morphological analyser in the next 
analysis. 
(5) Indexes to sentences by an index file.  
The automatically discovered erroneous 
annotations can be stored in one index file, pointing 
to the sentences that are to be revised.  
 
The interface of the tool is shown in Figure 4 
and Figure 5. 
 
Figure 4 Interface of the manual revision tool (Retrieves 
a word in the grammatical knowledge base of 
contemporary Chinese) 
 
Figure 5    Interface of the manual revision tool 
(Retrieves a word in the current file) 
  
 
In Figure 4, the small window in the lower left 
displays the retrieved result of the word ? ??? in 
the grammatical knowledge base; the lower right 
window displays the retrieved result of the same 
word in the annotated People?s Daily Corpus. 
88
In Figure 5, the small window in the lower left is used to 
define retrieval conditions in the current file. In this 
example, the orthography of ???? is defined. The 
lower right window displays the sentences containing the 
word  ???? retrieved from the current file. The left and 
right contexts of one word are shown with the retrieved 
word. The contents of any column can be sorted by 
clicking the top line of the column. 
5 Annotation of word alignment  
Since automatic word alignment techniques cannot 
reach as high a level as the morphological analyses, 
we adopt a practical method of using multiple 
aligners. One aligner is a lexical knowledge-based 
approach, which was implemented by us based on 
the work of Ker (Ker and Chang, 1997). Another 
aligner is the well-known GIZA++ toolkit, which is 
a statistics-based approach. For GIZA++, two 
directions were adopted: the Chinese sentences 
were used as source sentences and the Japanese 
sentences as target sentences, and vice versa.  
The results produced by the lexical knowledge-
based aligner, C? J of GIZA++, and J?C of 
GIZA++ were selected in a majority decision. If an 
alignment result was produced by two or three 
aligners at the same time, the result was accepted. 
Otherwise, was abandoned.  In this way, we aimed 
to utilize the results of each aligner and maintain 
high precision at the same time. Table 2 showed the 
evaluation results of the multi-aligner on 1,127 test 
sentence pairs, which were manually annotated with 
gold standards, totally 17,332 alignments.  
 
 Precision 
(%) 
Recall 
(%) 
F-measure
Multi-aligner 79.3 62.7 70
Table 2 Evaluation results of the multi-aligner 
  
The multi-aligner produced satisfactory results. 
This performance is evidence that the multi-aligner 
is feasible for use in assisting word alignment 
annotation.  
For manual revision, we also developed an 
assisting tool, which consist of a graphical interface 
and internal data management. Annotators can 
correct the output of the automatic aligner and add 
alignments that it has not identified. In addition to 
assisting with word alignment, the tool also 
supports annotation on phrase alignment. Since 
Japanese sentences have been annotated with phrase 
structures, annotators can select each phrase on the 
Japanese side and then align them with words on 
the Chinese side. For idioms in Japanese sentences, 
two or more phrases can be selected. 
The input and output file of the manual 
annotation is in XML format. The data of one 
sentence pair consists of the Chinese sentence 
annotated with morphological information, the 
Japanese sentence annotated with morphological 
and syntactic structure information, word alignment, 
and phrase alignment.  
The alignment annotation at word and phrase is 
ongoing, the former focusing on lexical translations 
and the latter focusing on pattern translations. After 
a certain amount of data is annotated, we plan to 
exploit the annotated data to improve the 
performance of automatic word alignment. We will 
also investigate a method to automatically identify 
phrase alignments from the annotated word 
alignment and a method to automatically discover 
the syntactic structures on the Chinese side from the 
annotated phrase alignments.       
6 Conclusion  
We have described the construction of a Japanese-
Chinese parallel corpus, a part of the NICT 
Multilingual Corpus. The corpus consists of about 
40,000 pairs of Japanese sentences and their 
Chinese translations. The Japanese sentences are 
annotated with morphological and syntactic 
structures and the Chinese sentences are annotated 
with morphological information. In addition, word 
and phrase alignments are annotated. A high quality 
of annotation was obtained through manual 
revisions, which were greatly assisted by the 
revision tools we developed in the project. To the 
best of our knowledge, this will be the first 
annotated Japanese-Chinese parallel corpus in the 
world.  
In the future, we will finish the annotation on the 
remaining data and add syntactic structures to the 
Chinese sentences.  
  
References  
Dice, L.R. 1945. Measures of the amount of 
ecologic association between species. Journal of 
Ecology (26), pages 297?302. 
Ker, S.J., Chang, J.S. 1997. A Class-based 
Approach to Word Alignment. Computational 
Linguistics, Vol. 23, Num. 2, pages 313?343. 
Liu Q. 2004.  Research into some aspects of 
Chinese-English machine translation. Doctoral 
Dissertation.  
Maekawa, K., Koiso, H., Furui, F., Isahara, H. 2000. 
Spontaneous Speech Corpus of Japanese. 
Proceedings of LREC2000, pages 947?952. 
LDC. 1992.  Linguistic data Consortium. 
http://www.ldc.upenn.edu/. 
Uchimoto, K. and Zhang,Y., Sudo, K., Murata, M., and 
Sekine, S.,  Isahara,  H. Multilingual Aligned Parallel 
89
Treebank Corpus Reflecting Contextual Information 
and Its Applications. Proceedings of the MLR2004: 
PostCOLING Workshop on Multilingual Linguistic 
Resources, pages 63-70. 
Yamada, K., Knight, K. 2001.A syntax-based Statistical 
Translation Model. In Proceedings of the ACL , pages 
523-530.  
Yu, Shiwen. 1997. Grammatical Knowledge Base of 
Contemporary Chinese. Tsinghua Publishing 
Company. 
Zhang, Y., Ma, Q., Isahara, H. 2005. Automatic 
Construction of Japanese-Chinese Translation 
Dictionary Using English as Intermediary. Journal of 
Natural Language Processing, Vol. 12, No. 2, pages 
63-85. 
Zhou, Q., Yu, S. 1994.  Blending Segmentation with 
Tagging in Chinese Language Corpus 
Processing.  In Proc. of COLING-94, pages 
1274?1278. 
 
 
90
Dependency Parsing with Short Dependency Relations in Unlabeled Data
Wenliang Chen, Daisuke Kawahara, Kiyotaka Uchimoto, Yujie Zhang, Hitoshi Isahara
Computational Linguistics Group
National Institute of Information and Communications Technology
3-5 Hikari-dai, Seika-cho, Soraku-gun, Kyoto, Japan, 619-0289
{chenwl, dk, uchimoto, yujie, isahara}@nict.go.jp
Abstract
This paper presents an effective dependency
parsing approach of incorporating short de-
pendency information from unlabeled data.
The unlabeled data is automatically parsed
by a deterministic dependency parser, which
can provide relatively high performance for
short dependencies between words. We then
train another parser which uses the informa-
tion on short dependency relations extracted
from the output of the first parser. Our pro-
posed approach achieves an unlabeled at-
tachment score of 86.52, an absolute 1.24%
improvement over the baseline system on
the data set of Chinese Treebank.
1 Introduction
In dependency parsing, we attempt to build the
dependency links between words from a sen-
tence. Given sufficient labeled data, there are sev-
eral supervised learning methods for training high-
performance dependency parsers(Nivre et al, 2007).
However, current statistical dependency parsers pro-
vide worse results if the dependency length be-
comes longer (McDonald and Nivre, 2007). Here
the length of a dependency from word w
i
and word
w
j
is simply equal to |i ? j|. Figure 1 shows the
F
1
score1 provided by a deterministic parser rela-
tive to dependency length on our testing data. From
1precision represents the percentage of predicted arcs of
length d that are correct and recall measures the percentage of
gold standard arcs of length d that are correctly predicted.
F
1
= 2? precision? recall/(precision + recall)
the figure, we find that F
1
score decreases when de-
pendency length increases as (McDonald and Nivre,
2007) found. We also notice that the parser pro-
vides good results for short dependencies (94.57%
for dependency length = 1 and 89.40% for depen-
dency length = 2). In this paper, short dependency
refers to the dependencies whose length is 1 or 2.
 30
 40
 50
 60
 70
 80
 90
 100
 0  5  10  15  20  25  30
F1
Dependency Length
baseline
Figure 1: F-score relative to dependency length
Labeled data is expensive, while unlabeled data
can be obtained easily. In this paper, we present an
approach of incorporating unlabeled data for depen-
dency parsing. First, all the sentences in unlabeled
data are parsed by a dependency parser, which can
provide state-of-the-art performance. We then ex-
tract information on short dependency relations from
the parsed data, because the performance for short
dependencies is relatively higher than others. Fi-
nally, we train another parser by using the informa-
tion as features.
The proposed method can be regarded as a semi-
supervised learning method. Currently, most semi-
88
supervised methods seem to do well with artificially
restricted labeled data, but they are unable to outper-
form the best supervised baseline when more labeled
data is added. In our experiments, we show that our
approach significantly outperforms a state-of-the-art
parser, which is trained on full labeled data.
2 Motivation and previous work
The goal in dependency parsing is to tag dependency
links that show the head-modifier relations between
words. A simple example is in Figure 2, where the
link between a and bird denotes that a is the depen-
dent of the head bird.
I    see    a    beautiful    bird    .
Figure 2: Example dependency graph.
We define that word distance of word w
i
and word
w
j
is equal to |i ? j|. Usually, the two words in a
head-dependent relation in one sentence can be adja-
cent words (word distance = 1) or neighboring words
(word distance = 2) in other sentences. For exam-
ple, ?a? and ?bird? has head-dependent relation in
the sentence at Figure 2. They can also be adjacent
words in the sentence ?I see a bird.?.
Suppose that our task is Chinese dependency
parsing. Here, the string ????JJ(Specialist-
level)/? ?NN(working)/? ?NN(discussion)?
should be tagged as the solution (a) in Figure
3. However, our current parser may choose the
solution (b) in Figure 3 without any additional
information. The point is how to assign the head for
????(Specialist-level)?. Is it ???(working)?
or ???(discussion)??
  
  
  
(b)
(a)
Figure 3: Two solutions for ????(Specialist-
level)/??(working)/??(discussion)?
As Figure 1 suggests, the current dependency
parser is good at tagging the relation between ad-
jacent words. Thus, we expect that dependencies
of adjacent words can provide useful information
for parsing words, whose word distances are longer.
When we search the string ????(Specialist-
level)/??(discussion)? at google.com, many rele-
vant documents can be retrieved. If we have a good
parser, we may assign the relations between the two
words in the retrieved documents as Figure 4 shows.
We can find that ???(discussion)? is the head of
????(Specialist-level)? in many cases.
1)?525	26
///,//?
2)?Proceedings of NAACL HLT 2007, pages 33?40,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Automatic Evaluation of Machine Translation Based on Rate of
Accomplishment of Sub-goals
Kiyotaka Uchimoto and Katsunori Kotani and Yujie Zhang and Hitoshi Isahara
National Institute of Information and Communications Technology
3-5, Hikari-dai, Seika-cho, Soraku-gun, Kyoto, 619-0289, Japan
{uchimoto,yujie,isahara}@nict.go.jp, kat@khn.nict.go.jp
Abstract
The quality of a sentence translated by a
machine translation (MT) system is dif-
ficult to evaluate. We propose a method
for automatically evaluating the quality
of each translation. In general, when
translating a given sentence, one or more
conditions should be satisfied to maintain
a high translation quality. In English-
Japanese translation, for example, prepo-
sitions and infinitives must be appropri-
ately translated. We show several proce-
dures that enable evaluating the quality of
a translated sentence more appropriately
than using conventional methods. The
first procedure is constructing a test set
where the conditions are assigned to each
test-set sentence in the form of yes/no
questions. The second procedure is devel-
oping a system that determines an answer
to each question. The third procedure is
combining a measure based on the ques-
tions and conventional measures. We also
present a method for automatically gener-
ating sub-goals in the form of yes/no ques-
tions and estimating the rate of accom-
plishment of the sub-goals. Promising re-
sults are shown.
1 Introduction
In machine translation (MT) research, appropriately
evaluating the quality of MT results is an important
issue. In recent years, many researchers have tried
to automatically evaluate the quality of MT and im-
prove the performance of automatic MT evaluations
(Niessen et al, 2000; Akiba et al, 2001; Papineni et
al., 2002; NIST, 2002; Leusch et al, 2003; Turian et
al., 2003; Babych and Hartley, 2004; Lin and Och,
2004; Banerjee and Lavie, 2005; Gimen?ez et al,
2005) because improving the performance of auto-
matic MT evaluation is expected to enable us to use
and improve MT systems efficiently. For example,
Och reported that the quality of MT results was im-
proved by using automatic MT evaluation measures
for the parameter tuning of an MT system (Och,
2003). This report shows that the quality of MT re-
sults improves as the performance of automatic MT
evaluation improves.
MT systems can be ranked if a set of MT re-
sults for each system and their reference translations
are given. Usually, about 300 or more sentences
are used to automatically rank MT systems (Koehn,
2004). However, the quality of a sentence translated
by an MT system is difficult to evaluate. For exam-
ple, the results of five MTs into Japanese of the sen-
tence ?The percentage of stomach cancer among the
workers appears to be the highest for any asbestos
workers.? are shown in Table 1. A conventional au-
tomatic evaluation method ranks the fifth MT result
first although its human subjective evaluation is the
lowest. This is because conventional methods are
based on the similarity between a translated sentence
and its reference translation, and they give the trans-
lated sentence a high score when the two sentences
are globally similar to each other in terms of lexical
overlap. However, in the case of the above example,
33
Table 1: Examples of conventional automatic evaluations.
Original sentence The percentage of stomach cancer among the workers appears to be the highest for any asbestos work-
ers.
Reference translation
(in Japanese)
roudousha no igan no wariai wa , asubesuto roudousha no tame ni saikou to naru youda .
System MT results BLEU NIST Fluency Adequacy
1 roudousha no aida no igan no paasenteeji wa , donoyouna ishiwata
roudousha no tame ni demo mottomo ookii youdearu .
0.2111 2.1328 2 3
2 roudousha no aida no igan no paasenteeji wa, arayuru asubesuto
roudousha no tame ni mottomo takai youni omowa re masu .
0.2572 2.1234 2 3
3 roudousha no aida no igan no paasenteeji wa donna asubesuto no tame
ni mo mottomo takai youni mie masu
0 1.8094 1 2
4 roudousha no aida no igan no paasenteeji wa nin?ino ishiwata ni wa
mottomo takaku mie masu .
0 1.5902 1 2
5 roudousha no naka no igan no wariai wa donna asubesuto ni mo mot-
tomo takai youni mieru .
0.2692 2.2640 1 2
the most important thing to maintain a high trans-
lation quality is to correctly translate ?for? into the
target language, and it would be difficult to detect
the importance just by comparing an MT result and
its reference translations even if the number of ref-
erence translations is increased.
In general, when translating a given sentence, one
or more conditions should be satisfied to maintain a
high translation quality. In this paper, we show that
constructing a test set where the conditions that are
mainly established from a linguistic point of view
are assigned to each test-set sentence in the form
of yes/no questions, developing a system that de-
termines an answer to each question, and combin-
ing a measure based on the questions and conven-
tional measures enable the evaluation of the quality
of a translated sentence more appropriately than us-
ing conventional methods. We also present a method
for automatically generating sub-goals in the form of
yes/no questions and estimating the rate of accom-
plishment of the sub-goals.
2 Test Set for Evaluating Machine
Translation Quality
2.1 Test Set
Two main types of data are used for evaluating MT
quality. One type of data is constructed by arbi-
trarily collecting sentence pairs in the source- and
target-languages, and the other is constructed by in-
tensively collecting sentence pairs that include lin-
guistic phenomena that are difficult to automatically
translate. Recently, MT evaluation campaigns such
as the International Workshop on Spoken Language
Translation 1, NISTMachine Translation Evaluation
2
, and HTRDP Evaluation 3 were organized to sup-
port the improvement of MT techniques. The data
used in the evaluation campaigns were arbitrarily
collected from newspaper articles or travel conver-
sation data for fair evaluation. They are classified
as the former type of data mentioned above. On the
other hand, the data provided by NTT (Ikehara et al,
1994) and that constructed by JEIDA (Isahara, 1995)
are classified as the latter type. Almost all the data
mentioned above consist of only parallel translations
in two languages. Data with information for evaluat-
ing MT results, such as JEIDA?s are rarely found. In
this paper, we call data that consist of parallel trans-
lations collected for MT evaluation and that the in-
formation for MT evaluation is assigned to, a test
set.
The most characteristic information assigned to
the JEIDA test set is the yes/no question for assess-
ing the translation results. For example, a yes/no
question such as ?Is ?for? translated into an expres-
sion representing a cause/reason such as ?de??? (in
Japanese) is assigned to a test-set sentence. We can
evaluate MT results objectively by answering the
question. An example of a test-set sample consist-
ing of an ID, a source-language sample sentence, its
reference translation, and a question is as follows.
1http://www.slt.atr.jp/IWSLT2006/
2http://www.nist.gov/speech/tests/mt/index.htm
3http://www.863data.org.cn/
34
ID 1.1.7.1.3-1
Sample sen-
tence
The percentage of stomach can-
cer among the workers appears
to be the highest for any asbestos
workers.
Reference
translation
(in Japanese)
roudousha no igan no wariai wa
, asubesuto roudousha no tame
ni saikou to naru youda .
Question Is ?appear to? translated into an
auxiliary verb such as ?youda??
The questions are classified mainly in terms of
grammar, and the numbers to the left of the hyphen-
ation of each ID such as 1.1.7.1.3 represent the cat-
egories of the questions. For example, the above
question is related to catenative verbs.
The JEIDA test set consists of two parts, one for
the evaluation of English-Japanese MT and the other
for that of Japanese-English MT. We focused on the
part for English-Japanese MT. This part consists of
769 sample sentences, each of which has a yes/no
question.
The 769 sentences were translated by using five
commercial MT systems to investigate the relation-
ship between subjective evaluation based on yes/no
questions and conventional subjective evaluation
based on fluency and adequacy. The instruction for
the subjective evaluation based on fluency and ad-
equacy followed that given in the TIDES specifi-
cation (TIDES, 2002). The subjective evaluation
based on yes/no questions was done by manually
answering each question for each translation. The
subjective evaluation based on the yes/no questions
was stable; namely, it was almost independent of
the human subjects in our preliminary investigation.
There were only two questions for which the an-
swers generated inconsistency in the subjective eval-
uation when 1,500 question-answer pairs were ran-
domly sampled and evaluated by two human sub-
jects.
Then, we investigated the correlation between the
two types of subjective evaluation. The correlation
coefficients mentioned in this paper are statistically
significant at the 1% or less significance level. The
Spearman rank-order correlation coefficient is used
in this paper. In the subjective evaluation based on
yes/no questions, yes and no were numerically trans-
formed into 1 and ?1. For 3,845 translations ob-
tained by using five MT systems, the correlation co-
efficients between the subjective evaluations based
on yes/no questions and based on fluency and ade-
quacy were 0.48 for fluency and 0.63 for adequacy.
These results indicate that the two subjective evalu-
ations have relatively strong correlations. The cor-
relation is especially strong between the subjective
evaluation based on yes/no questions and adequacy.
2.2 Expansion of JEIDA Test Set
Each sample sentence in the JEIDA test set has only
one question. Therefore, in the subjective evalua-
tion using the JEIDA test set, translation errors that
do not involve the pre-assigned question are ignored
even if they are serious. Therefore, translations that
have serious errors that are not related to the ques-
tion tend to be evaluated as being of high quality.
To solve this problem, we expanded the test set by
adding new questions about translations with the se-
rious errors.
Sentences whose average grades were three or
less for fluency and adequacy for the translation re-
sults of the five MT systems were selected for the
expansion. Besides them, sentences whose average
grades were more than three for fluency and ade-
quacy for the translation results of the five MT sys-
tems were selected when a majority of evaluation
results based on yes/no questions about the transla-
tions of the five MT systems were no. The number
of selected sentences was 150. The expansion was
manually performed using the following steps.
1. Serious translation errors are extracted from the
MT results.
2. For each extracted error, questions strongly re-
lated to the error are searched for in the test set.
If related questions are found, the same types
of questions are generated for the selected sen-
tence, and the same ID as that of the related
question is assigned to each generated question.
Otherwise, questions are newly generated, and
a new ID is assigned to each generated ques-
tion.
3. Each MT result is evaluated according to each
added question.
Eventually, one or more questions were assigned to
each selected sentence in the test set. Among the 150
35
Table 2: Expanded test-set samples.
ID 1.1.7.1.3-1
Original Sample sentence The percentage of stomach cancer among the workers appears to be the highest for any
asbestos workers.
Reference translation
(in Japanese)
roudousha no igan no wariai wa , asubesuto roudousha no tame ni saikou to naru youda
.
Question (Q-0) Is ?appear to? translated into an auxiliary verb such as ?youda??
ID 1.1.6.1.3-5
Expanded Translation error ?For? is not translated appropriately.
Question-1 (Q-1) Is ?for? translated into an expression representing a cause/reason such as ?. . .de??
ID Additional-1
Expanded Translation error Some expressions are not translated.
Question-2 (Q-2) Are all English words translated into Japanese?
Table 3: Examples of subjective evaluations based on yes/no questions.
Answer
System MT results Q-0 Q-1 Q-2 Fluency Adequacy
1 roudousha no aida no igan no paasenteeji wa , donoyouna ishiwata
roudousha no tame ni demo mottomo ookii youdearu .
Yes No Yes 2 3
2 roudousha no aida no igan no paasenteeji wa, arayuru asubesuto
roudousha no tame ni mottomo takai youni omowa re masu .
Yes Yes Yes 2 3
3 roudousha no aida no igan no paasenteeji wa donna asubesuto no
tame ni mo mottomo takai youni mie masu
Yes No No 1 2
4 roudousha no aida no igan no paasenteeji wa nin?ino ishiwata ni
wa mottomo takaku mie masu .
Yes No No 1 2
5 roudousha no naka no igan no wariai wa donna asubesuto ni mo
mottomo takai youni mieru .
Yes No No 1 2
selected sentences, questions were newly assigned
to 103 sentences. The number of added questions
was 148. The maximum number of questions added
to a sentence was five. After expanding the test set,
the correlation coefficients between the subjective
evaluations based on yes/no questions and based on
fluency and adequacy increased from 0.48 to 0.51
for fluency and from 0.63 to 0.66 for adequacy. The
differences between the correlation coefficients ob-
tained before and after the expansion are statistically
significant at the 5% or less significance level for
adequacy. These results indicate that the expansion
of the test set significantly improves the correlation
between the subjective evaluations based on yes/no
questions and based on adequacy. When two or
more questions were assigned to a test-set sentence,
the subjective evaluation based on the questions was
decided by the majority answer. The majority an-
swers, yes and no, were numerically transformed
into 1 and ?1. Ties between yes and no were trans-
formed into 0. Examples of added questions and
the subjective evaluations based on the questions are
shown in Tables 2 and 3.
3 Automatic Evaluation of Machine
Translation Based on Rate of
Accomplishment of Sub-goals
3.1 A New Measure for Evaluating Machine
Translation Quality
The JEIDA test set was not designed for auto-
matic evaluation but for human subjective evalua-
tion. However, a measure for automatic MT evalu-
ation that strongly correlates fluency and adequacy
is likely to be established because the subjective
evaluation based on yes/no questions has a rela-
tively strong correlation with the subjective evalua-
tion based on fluency and adequacy, as mentioned in
Section 2. In this section, we describe a method for
automatically evaluating MT quality by predicting
an answer to each yes/no question and using those
answers.
Hereafter, we assume that each yes/no question is
defined as a sub-goal that a given translation should
satisfy and that the sub-goal is accomplished if the
answer to the corresponding yes/no question to the
sub-goal is yes. We also assume that the sub-goal
is unaccomplished if the answer is no. A new eval-
uation score, A, is defined based on a multiple lin-
36
Table 4: Examples of Patterns.
Sample sentence She lived there by herself.
Question Is ?by herself? translated as ?hitori de??
Pattern The answer is yes if the pattern [hitori dake de|hitori kiri de |tandoku de|tanshin de] is included in a
translation. Otherwise, the answer is no.
Sample sentence They speak English in New Zealand.
Question The personal pronoun ?they? is omitted in a translation like ?nyuujiilando de wa eigo wo hanasu??
Pattern The answer is yes if the pattern [karera wa|sore ra wa] is not included in a translation. Otherwise, the
answer is no.
ear regression model as follows using the rate of ac-
complishment of the sub-goals and the similarities
between a given translation and its reference trans-
lation. The best-fitted line for the observed data is
calculated by the method of least-squares (Draper
and Smith, 1981).
A =
m
?
i=1
?
S
i
? S
i
(1)
+
n
?
j=1
(?
Q
j
? Q
j
+ ?
Q
?
j
? Q
?
j
) + ?

Q
j
=
{
1 : if subgoal is accomplished
0 : otherwise (2)
Q
?
j
=
{
1 : if subgoal is unaccomplished
0 : otherwise (3)
Here, the term Q
j
corresponds to the rate of accom-
plishment of the sub-goal having the i-th ID, and
?
Q
j
is a weight for the rate of accomplishment. The
term Q
?
j
corresponds to the rate of unaccomplish-
ment of the sub-goal having the i-th ID, and ?
Q
?
j
is a
weight for the rate of unaccomplishment. The value
n indicates the number of types of sub-goals. The
term ?

is constant.
The term S
i
indicates a similarity between a trans-
lated sentence and its reference translation, and ?
S
i
is a weight for the similarity. Many methods for cal-
culating the similarity have been proposed (Niessen
et al, 2000; Akiba et al, 2001; Papineni et al, 2002;
NIST, 2002; Leusch et al, 2003; Turian et al, 2003;
Babych and Hartley, 2004; Lin and Och, 2004;
Banerjee and Lavie, 2005; Gimen?ez et al, 2005).
In our research, 23 scores, namely BLEU (Papineni
et al, 2002) with maximum n-gram lengths of 1, 2,
3, and 4, NIST (NIST, 2002) with maximum n-gram
lengths of 1, 2, 3, 4, and 5, GTM (Turian et al, 2003)
with exponents of 1.0, 2.0, and 3.0, METEOR (ex-
act) (Banerjee and Lavie, 2005), WER (Niessen et
al., 2000), PER (Leusch et al, 2003), and ROUGE
(Lin, 2004) with n-gram lengths of 1, 2, 3, and 4 and
4 variants (LCS, S?, SU?, W-1.2), were used to cal-
culate each similarity S
i
. Therefore, the value of m
in Eq. (1) was 23. Japanese word segmentation was
performed by using JUMAN 4 in our experiments.
As you can see, the definition of our new measure
is based on a combination of an evaluation measure
focusing on local information and that focusing on
global information.
3.2 Automatic Estimation of Rate of
Accomplishment of Sub-goals
The rate of accomplishment of sub-goals is esti-
mated by determining the answer to each question
as yes or no. This section describes a method based
on simple patterns for determining the answers.
An answer to each question is automatically de-
termined by checking whether patterns are included
in a translation or not. The patterns are constructed
for each question. All of the patterns are expressed
in hiragana characters. Before applying the pat-
terns to a given translation, the translation is trans-
formed into hiragana characters, and all punctuation
is eliminated. The transformation to hiragana char-
acters was performed by using JUMAN in our ex-
periments.
Test-set sentences, the questions assigned to
them, and the patterns constructed for the questions
are shown in Table 4. In the patterns, the symbol ?|?
represents ?OR?.
3.3 Automatic Sub-goal Generation and
Automatic Estimation of Rate of
Accomplishment of Sub-goals
We found that expressions important for maintain-
ing a high translation quality were often commonly
4http://www.kc.t.u-tokyo.ac.jp/nl-resource/juman.html
37
included in the reference translations for each test-
set sentence. We also found that the expression was
also related to the yes/no question assigned to the
test-set sentence. Therefore, we automatically gen-
erate yes/no questions in the following steps.
1. For each test-set sentence, a set of words com-
monly appearing in the reference translations
are extracted.
2. For each combination of n words in the set
of words extracted in the first step, skip word
n-grams commonly appearing in the reference
translations in the same word order are selected
as a set of common skip word n-grams.
3. For each test-set sentence, the sub-goal is de-
fined as the yes/no question ?Are all of the com-
mon skip word n-grams included in the transla-
tion??
If no common skip word n-grams are found, the
yes/no question is not generated. The answer to the
yes/no question is determined to be yes if all of the
common skip word n-grams are included in a trans-
lation. Otherwise, the answer is determined to be
no.
This scheme assigns greater weight to important
phrases that should be included in the translation to
maintain a high translation quality. Our observation
is that those important phrases are often common
between human translations. A similar scheme was
proposed by Babych and Hartley (Babych and Hart-
ley, 2004) for BLEU. In their scheme, greater weight
is assigned to components that are salient through-
out the document. Therefore, their scheme focuses
on global context while our scheme focuses on local
context. We believe that the two schemes are com-
plementary to each other.
4 Experiments and Discussion
In our experiments, the translation results of three
MT systems and their subjective evaluation results
were used as a development set for constructing the
patterns described in Section 3.2 and for tuning the
parameters ?
S
i
, ?
Q
j
, ?
Q
?
j
, and ?

in Eq. (1). The
translations and evaluation results of the remaining
two MT systems were used as an evaluation set for
testing.
In the development set, each test-set sentence has
at least one question, at least one reference transla-
tion, three MT results, and subjective evaluation re-
sults of the three MT results. The patterns for deter-
mining yes/no answers were manually constructed
for the questions assigned to the 769 test-set sen-
tences. There were 917 questions assigned to them.
Among them, the patterns could be constructed for
898 questions assigned to 767 test-set sentences.
The remaining 19 questions were skipped because
making simple patterns as described in Section 3.2
was difficult; for example, one of the questions
was ?Is the whole sentence translated into one sen-
tence??. The yes/no answer determination accura-
cies obtained by using the patterns are shown in Ta-
ble 5.
Table 5: Results of yes/no answer determination.
Test set Accuracy
Development 97.6% (2,629/2,694)
Evaluation 82.8% (1,487/1,796)
We investigated the correlation between the eval-
uation score, A in Eq. (1) and the subjective eval-
uations, fluency and adequacy, for the 769 test-set
sentences. First, to maximize the correlation coeffi-
cients between the evaluation score, A, and the hu-
man subjective evaluations, fluency and adequacy,
the optimal values of ?
S
i
, ?
Q
j
, ?
Q
?
j
, and ?

in
Eq. (1) were investigated using the development
set within a framework of multiple linear regression
modeling (Draper and Smith, 1981). Then, the cor-
relation coefficients were investigated by using the
optimal value set. The results are shown in Table 6,
7, and 8. In these tables, ?Conventional method? in-
dicates the correlation coefficients obtained when A
was calculated by using only similarities S
i
. ?Con-
ventional method (combination)? is a combination
of existing automatic evaluation methods from the
literature. ?Our method (automatic)? indicates the
correlation coefficients obtained when the results of
the automatic determination of yes/no answers were
used to calculate Q
j
and Q?
j
in Eq. (1). For the 19
questions for which the patterns could not be con-
structed, Q
j
was set at 0. ?Our method (full au-
tomatic)? indicates the correlation coefficients ob-
tained when the results of the automatic sub-goal
generation and determination of rate of accomplish-
38
Table 6: Coefficients of correlation between evaluation score A and fluency/adequacy. (A reference transla-
tion is used to calculate S
i
.)
Method fluency adequacy
Development set Evaluation set Development set Evaluation set
Conventional method (WER) 0.43 0.48 0.42 0.48
Conventional method (combination) 0.52 0.51 0.49 0.47
Our method (automatic) 0.90? 0.59? 0.89? 0.62?
Our method (upper bound) 0.90? 0.62? 0.90? 0.68?
Table 7: Coefficients of correlation between evaluation score A and fluency/adequacy. (Three reference
translations are used to calculate S
i
.)
Method fluency adequacy
Development set Evaluation set Development set Evaluation set
Conventional method (WER) 0.47 0.51 0.45 0.51
Conventional method (combination) 0.54 0.54 0.51 0.52
Our method (automatic) 0.90? 0.60? 0.90? 0.64?
Our method (full automatic) 0.85? 0.58 0.84? 0.60?
Our method (upper bound) 0.90? 0.62? 0.90? 0.69?
Table 8: Coefficients of correlation between evaluation score A and fluency/adequacy. (Five reference
translations are used to calculate S
i
.)
Method fluency adequacy
Development set Evaluation set Development set Evaluation set
Conventional method (WER) 0.49 0.53 0.46 0.53
Conventional method (combination) 0.56 0.56 0.52 0.54
Our method (automatic) 0.90? 0.60 0.90? 0.63?
Our method (full automatic) 0.86? 0.59 0.85? 0.60?
Our method (upper bound) 0.91? 0.63? 0.90? 0.69?
In these tables, ? indicates significance at the 5% or less significance level.
ment of sub-goals were used to calculate Q
j
and Q?
j
in Eq. (1). Skip word trigrams, skip word bigrams,
and skip word unigrams were used for generating
the sub-goals according to our preliminary experi-
ments. ?Our method (upper bound)? indicates the
correlation coefficients obtained when human judg-
ments on the questions were used to calculate Q
j
and Q?
j
.
As shown in Table 6, 7, and 8, our methods signif-
icantly outperform the conventional methods from
literature. Note that WER outperformed other indi-
vidual measures like BLEU and NIST in our exper-
iments, and the combination of existing automatic
evaluation methods from the literature outperformed
individual lexical similarity measures by themselves
in almost all cases. The differences between the
correlation coefficients obtained using our method
and the conventional methods are statistically sig-
nificant at the 5% or less significance level for flu-
ency and adequacy, even if the number of reference
translations increases, except in three cases shown
in Table 7 and 8. This indicates that considering
the rate of accomplishment of sub-goals to automat-
ically evaluate the quality of each translation is use-
ful, especially when the number of reference trans-
lations is small.
The differences between the correlation coeffi-
cients obtained using two automatic methods are not
significant. These results indicate that we can reduce
the development cost for constructing sub-goals.
However, there are still significant gaps between the
correlation coefficients obtained using a fully auto-
matic method and upper bounds. These gaps indi-
cate that we need further improvement in automatic
sub-goal generation and automatic estimation of rate
of accomplishment of sub-goals, which is our future
work.
Human judgments of adequacy and fluency are
known to be noisy, with varying levels of intercoder
agreement. Recent work has tended to apply cross-
judge normalization to address this issue (Blatz et
al., 2003). We would like to evaluate against the
normalized data in the future.
39
5 Conclusion and Future Work
We demonstrated that the quality of a translated sen-
tence can be evaluated more appropriately than by
using conventional methods. That was demonstrated
by constructing a test set where the conditions that
should be satisfied to maintain a high translation
quality are assigned to each test-set sentence in the
form of a question, by developing a system that de-
termines an answer to each question, and by com-
bining a measure based on the questions and con-
ventional measures. We also presented a method for
automatically generating sub-goals in the form of
yes/no questions and estimating the rate of accom-
plishment of the sub-goals. Promising results were
obtained.
In the near future, we would like to expand the
test set to improve the upper bound obtained by
our method. We are also planning to expand the
method and improve the accuracy of the automatic
sub-goal generation and determination of the rate of
accomplishment of sub-goals. The sub-goals of a
given sentence should be generated by considering
the complexity of the sentence and the alignment in-
formation between the original source-language sen-
tence and its translation. Further advanced genera-
tion and estimation would give us information about
the erroneous parts of MT results and their quality.
We believe that future research would allow us to
develop high-quality MT systems by tuning the sys-
tem parameters based on the automatic MT evalua-
tion measures.
Acknowledgments
The guideline for expanding the test set is based on that con-
structed by the Technical Research Committee of the AAMT
(Asia-Pacific Association for Machine Translation) The authors
would like to thank the committee members, especially, Mr.
Kentaro Ogura, Ms. Miwako Shimazu, Mr. Tatsuya Sukehiro,
Mr. Masaru Fuji, and Ms. Yoshiko Matsukawa for their coop-
eration. This research is partially supported by special coordi-
nation funds for promoting science and technology.
References
Yasuhiro Akiba, Kenji Imamura, and Eiichiro Sumita. 2001.
Using Multiple Edit Distances to Automatically Rank Ma-
chine Translation Output. In Proceedings of the MT Summit
VIII, pages 15?20.
Bogdan Babych and Anthony Hartley. 2004. Extending the
BLEU MT Evaluation Method with Frequency Weightings.
In Proceedings of the 42nd ACL, pages 622?629.
Satanjeev Banerjee and Alon Lavie. 2005. Meteor: An au-
tomatic metric for mt evaluation with improved correlation
with human judgments. In Proceedings of Workshop on In-
trinsic and Extrinsic Evaluation Measures for MT and/or
Summarization, pages 65?72.
John Blatz, Erin Fitzgerald, George Foster, Simona Gandrabur,
Cyril Goutte, Alex Kulesza, Alberto Sanchis, and Nicola
Ueffing. 2003. Confidence Estimation for Machine Trans-
lation. Technical report, Center for Language and Speech
Processing, Johns Hopkins University. Summer Workshop
Final Report.
Norman R. Draper and Harry Smith. 1981. Applied Regression
Analysis. 2nd edition. Wiley.
Jesus? Gimen?ez, Enrique Amigo
?
, and Chiori Hori. 2005. Ma-
chine translation evaluation inside qarla. In Proceedings of
the IWSLT?05.
Satoru Ikehara, Satoshi Shirai, and Kentaro Ogura. 1994. Cri-
teria for Evaluating the Linguistic Quality of Japanese to
English Machine Translations. Transactions of the JSAI,
9(4):569?579. (in Japanese).
Hitoshi Isahara. 1995. JEIDA?s Test-Sets for Quality Evalua-
tion of MT Systems ? Technical Evaluation from the Devel-
oper?s Point of View.
Philipp Koehn. 2004. Statistical Significance Tests for Ma-
chine Translation Evaluation. In Proceedings of the 2004
Conference on EMNLP, pages 388?395.
Gregor Leusch, Nicola Ueffing, and Hermann Ney. 2003. A
Novel String-to-String Distance Measure with Applications
to Machine Translation Evaluation. In Proceedings of the
MT Summit IX, pages 240?247.
Chin-Yew Lin and Franz Josef Och. 2004. ORANGE: a
Method for Evaluating Automatic Evaluation Metrics for
Machine Translation. In Proceedings of the 20th COLING,
pages 501?507.
Chin-Yew Lin. 2004. ROUGE: A Package for Automatic Eval-
uation of Summaries. In Proceedings of the Workshop on
Text Summarization Branches Out, pages 74?81.
Sonja Niessen, Franz Josef Och, Gregor Leusch, and Hermann
Ney. 2000. An Evaluation Tool for Machine Translation:
Fast Evaluation for MT Research. In Proceedings of the
LREC 2000, pages 39?45.
NIST. 2002. Automatic Evaluation of Machine Translation
Quality Using N-gram Co-Occurrence Statistics. Technical
report, NIST.
Franz Josef Och. 2003. Minimum Error Training in Statistical
Machine Translation. In Proceedings of the 41st ACL, pages
160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei jing Zhu.
2002. BLEU: a Method for Automatic Evaluation of Ma-
chine Translation. In Proceedings of the 40th ACL, pages
311?318.
TIDES. 2002. Linguistic Data Annotation Specifi-
cation: Assessment of Fluency and Adequacy in
Arabic-English and Chinese-English Translations.
http://www.ldc.upenn.edu/Projects/TIDES/Translation/
TransAssess02.pdf.
Joseph P. Turian, Luke Shen, and I. Dan Melamed. 2003. Eval-
uation of Machine Translation and its Evaluation. In Pro-
ceedings of the MT Summit IX, pages 386?393.
40
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 97?104,
Sydney, July 2006. c?2006 Association for Computational Linguistics
An Empirical Study of Chinese Chunking
Wenliang Chen, Yujie Zhang, Hitoshi Isahara
Computational Linguistics Group
National Institute of Information and Communications Technology
3-5 Hikari-dai, Seika-cho, Soraku-gun, Kyoto, Japan, 619-0289
{chenwl, yujie, isahara}@nict.go.jp
Abstract
In this paper, we describe an empirical
study of Chinese chunking on a corpus,
which is extracted from UPENN Chinese
Treebank-4 (CTB4). First, we compare
the performance of the state-of-the-art ma-
chine learning models. Then we propose
two approaches in order to improve the
performance of Chinese chunking. 1) We
propose an approach to resolve the spe-
cial problems of Chinese chunking. This
approach extends the chunk tags for ev-
ery problem by a tag-extension function.
2) We propose two novel voting meth-
ods based on the characteristics of chunk-
ing task. Compared with traditional vot-
ing methods, the proposed voting methods
consider long distance information. The
experimental results show that the SVMs
model outperforms the other models and
that our proposed approaches can improve
performance significantly.
1 Introduction
Chunking identifies the non-recursive cores of
various types of phrases in text, possibly as a
precursor to full parsing or information extrac-
tion. Steven P. Abney was the first person
to introduce chunks for parsing(Abney, 1991).
Ramshaw and Marcus(Ramshaw and Marcus,
1995) first represented base noun phrase recog-
nition as a machine learning problem. In 2000,
CoNLL-2000 introduced a shared task to tag
many kinds of phrases besides noun phrases in
English(Sang and Buchholz, 2000). Addition-
ally, many machine learning approaches, such as
Support Vector Machines (SVMs)(Vapnik, 1995),
Conditional Random Fields (CRFs)(Lafferty et
al., 2001), Memory-based Learning (MBL)(Park
and Zhang, 2003), Transformation-based Learn-
ing (TBL)(Brill, 1995), and Hidden Markov Mod-
els (HMMs)(Zhou et al, 2000), have been applied
to text chunking(Sang and Buchholz, 2000; Ham-
merton et al, 2002).
Chinese chunking is a difficult task, and much
work has been done on this topic(Li et al, 2003a;
Tan et al, 2005; Wu et al, 2005; Zhao et al,
2000). However, there are many different Chinese
chunk definitions, which are derived from differ-
ent data sets(Li et al, 2004; Zhang and Zhou,
2002). Therefore, comparing the performance of
previous studies in Chinese chunking is very dif-
ficult. Furthermore, compared with the other lan-
guages, there are some special problems for Chi-
nese chunking(Li et al, 2004).
In this paper, we extracted the chunking corpus
from UPENN Chinese Treebank-4(CTB4). We
presented an empirical study of Chinese chunk-
ing on this corpus. First, we made an evaluation
on the corpus to clarify the performance of state-
of-the-art models in Chinese chunking. Then we
proposed two approaches in order to improve the
performance of Chinese chunking. 1) We pro-
posed an approach to resolve the special prob-
lems of Chinese chunking. This approach ex-
tended the chunk tags for every problem by a tag-
extension function. 2) We proposed two novel vot-
ing methods based on the characteristics of chunk-
ing task. Compared with traditional voting meth-
ods, the proposed voting methods considered long
distance information. The experimental results
showed the proposed approaches can improve the
performance of Chinese chunking significantly.
The rest of this paper is as follows: Section 2
describes the definitions of Chinese chunks. Sec-
97
tion 3 simply introduces the models and features
for Chinese chunking. Section 4 proposes a tag-
extension method. Section 5 proposes two new
voting approaches. Section 6 explains the exper-
imental results. Finally, in section 7 we draw the
conclusions.
2 Definitions of Chinese Chunks
We defined the Chinese chunks based on the CTB4
dataset1. Many researchers have extracted the
chunks from different versions of CTB(Tan et al,
2005; Li et al, 2003b). However, these studies did
not provide sufficient detail. We developed a tool2
to extract the corpus from CTB4 by modifying the
tool Chunklink3.
2.1 Chunk Types
Here we define 12 types of chunks4: ADJP, ADVP,
CLP, DNP, DP, DVP, LCP, LST, NP, PP, QP,
VP(Xue et al, 2000). Table 1 provides definitions
of these chunks.
Type Definition
ADJP Adjective Phrase
ADVP Adverbial Phrase
CLP Classifier Phrase
DNP DEG Phrase
DP Determiner Phrase
DVP DEV phrase
LCP Localizer Phrase
LST List Marker
NP Noun Phrase
PP Prepositional Phrase
QP Quantifier Phrase
VP Verb Phrase
Table 1: Definition of Chunks
2.2 Data Representation
To represent the chunks clearly, we represent the
data with an IOB-based model as the CoNLL00
shared task did, in which every word is to be
tagged with a chunk type label extended with I
(inside a chunk), O (outside a chunk), and B (in-
side a chunk, but also the first word of the chunk).
1More detailed information at
http://www.cis.upenn.edu/ chinese/.
2Tool is available at
http://www.nlplab.cn/chenwl/tools/chunklinkctb.txt.
3Tool is available at http://ilk.uvt.nl/software.html#chunklink.
4There are 15 types in the Upenn Chinese TreeBank. The
other chunk types are FRAG, PRN, and UCP.
Each chunk type could be extended with I or B
tags. For instance, NP could be represented as
two types of tags, B-NP or I-NP. Therefore, we
have 25 types of chunk tags based on the IOB-
based model. Every word in a sentence will be
tagged with one of these chunk tags. For in-
stance, the sentence (word segmented and Part-of-
Speech tagged) ??-NR(He) /??-VV(reached)
/??-NR(Beijing) /??-NN(airport) /?/? will
be tagged as follows:
Example 1:
S1: [NP?][VP??][NP??/??][O?]
S2: ?B-NP /??B-VP /??B-NP /??I-NP /?O /
Here S1 denotes that the sentence is tagged with
chunk types, and S2 denotes that the sentence is
tagged with chunk tags based on the IOB-based
model.
With data representation, the problem of Chi-
nese chunking can be regarded as a sequence tag-
ging task. That is to say, given a sequence of
tokens (words pairing with Part-of-Speech tags),
x = x1, x2, ..., xn, we need to generate a sequence
of chunk tags, y = y1, y2, ..., yn.
2.3 Data Set
CTB4 dataset consists of 838 files. In the ex-
periments, we used the first 728 files (FID from
chtb 001.fid to chtb 899.fid) as training data, and
the other 110 files (FID from chtb 900.fid to
chtb 1078.fid) as testing data. In the following
sections, we use the CTB4 Corpus to refer to the
extracted data set. Table 2 lists details on the
CTB4 Corpus data used in this study.
Training Test
Num of Files 728 110
Num of Sentences 9,878 5,290
Num of Words 238,906 165,862
Num of Phrases 141,426 101,449
Table 2: Information of the CTB4 Corpus
3 Chinese Chunking
3.1 Models for Chinese Chunking
In this paper, we applied four models, includ-
ing SVMs, CRFs, TBL, and MBL, which have
achieved good performance in other languages.
We only describe these models briefly since full
details are presented elsewhere(Kudo and Mat-
sumoto, 2001; Sha and Pereira, 2003; Ramshaw
and Marcus, 1995; Sang, 2002).
98
3.1.1 SVMs
Support Vector Machines (SVMs) is a pow-
erful supervised learning paradigm based on the
Structured Risk Minimization principle from com-
putational learning theory(Vapnik, 1995). Kudo
and Matsumoto(Kudo and Matsumoto, 2000) ap-
plied SVMs to English chunking and achieved
the best performance in the CoNLL00 shared
task(Sang and Buchholz, 2000). They created 231
SVMs classifiers to predict the unique pairs of
chunk tags.The final decision was given by their
weighted voting. Then the label sequence was
chosen using a dynamic programming algorithm.
Tan et al (Tan et al, 2004) applied SVMs to
Chinese chunking. They used sigmoid functions
to extract probabilities from SVMs outputs as the
post-processing of classification. In this paper, we
used Yamcha (V0.33)5 in our experiments.
3.1.2 CRFs
Conditional Random Fields is a powerful se-
quence labeling model(Lafferty et al, 2001) that
combine the advantages of both the generative
model and the classification model. Sha and
Pereira(Sha and Pereira, 2003) showed that state-
of-the-art results can be achieved using CRFs in
English chunking. CRFs allow us to utilize a large
number of observation features as well as differ-
ent state sequence based features and other fea-
tures we want to add. Tan et al (Tan et al, 2005)
applied CRFs to Chinese chunking and their ex-
perimental results showed that the CRFs approach
provided better performance than HMM. In this
paper, we used MALLET (V0.3.2)6(McCallum,
2002) to implement the CRF model.
3.1.3 TBL
Transformation based learning(TBL), first in-
troduced by Eric Brill(Brill, 1995), is mainly
based on the idea of successively transforming the
data in order to correct the error. The transforma-
tion rules obtained are usually few , yet power-
ful. TBL was applied to Chinese chunking by Li
et al(Li et al, 2004) and TBL provided good per-
formance on their corpus. In this paper, we used
fnTBL (V1.0)7 to implement the TBL model.
5Yamcha is available at
http://chasen.org/ taku/software/yamcha/
6MALLET is available at
http://mallet.cs.umass.edu/index.php/Main Page
7fnTBL is available at
http://nlp.cs.jhu.edu/ rflorian/fntbl/index.html
3.1.4 MBL
Memory-based Learning (also called instance
based learning) is a non-parametric inductive
learning paradigm that stores training instances in
a memory structure on which predictions of new
instances are based(Walter et al, 1999). The simi-
larity between the new instance X and example Y
in memory is computed using a distance metric.
Tjong Kim Sang(Sang, 2002) applied memory-
based learning(MBL) to English chunking. MBL
performs well for a variety of shallow parsing
tasks, often yielding good results. In this paper,
we used TiMBL8(Daelemans et al, 2004) to im-
plement the MBL model.
3.2 Features
The observations are based on features that are
able to represent the difference between the two
events. We utilize both lexical and Part-Of-
Speech(POS) information as the features.
We use the lexical and POS information within
a fixed window. We also consider different combi-
nations of them. The features are listed as follows:
? WORD: uni-gram and bi-grams of words in
an n window.
? POS: uni-gram and bi-grams of POS in an n
window.
? WORD+POS: Both the features of WORD
and POS.
where n is a predefined number to denote window
size.
For instance, the WORD features at the 3rd
position (??-NR) in Example 1 (set n as 2):
?? L2 ?? L1 ?? 0 ?? R1 ? R2?(uni-
gram) and ?? ?? LB1?? ?? B0?? ?
? RB1 ?? ? RB2?(bi-gram). Thus features
of WORD have 9 items(5 from uni-gram and
4 from bi-grams). In the similar way, fea-
tures of POS also have 9 items and features of
WORD+POS have 18 items(9+9).
4 Tag-Extension
In Chinese chunking, there are some difficult prob-
lems, which are related to Special Terms, Noun-
Noun Compounds, Named Entities Tagging and
Coordination. In this section, we propose an ap-
proach to resolve these problems by extending the
chunk tags.
8TiMBL is available at http://ilk.uvt.nl/timbl/
99
In the current data representation, the chunk
tags are too generic to construct accurate models.
Therefore, we define a tag-extension function fs
in order to extend the chunk tags as follows:
Te = fs(T,Q) = T ?Q (1)
where, T denotes the original tag set, Q denotes
the problem set, and Te denotes the extended tag
set. For instance, we have an q problem(q ? Q).
Then we extend the chunk tags with q. For NP
Recognition, we have two new tags: B-NP-q and
I-NP-q. Here we name this approach as Tag-
Extension.
In the following three cases study, we demon-
strate that how to use Tag-Extension to resolve the
difficult problems in NP Recognition.
1) Special Terms: this kind of noun phrases
is special terms such as ??/ ??(Life)/ ?
?(Forbidden Zone)/ ?/?, which are bracketed
with the punctuation ??, ?, ?, ?, ?, ??.
They are divided into two types: chunks with these
punctuation and chunks without these punctua-
tion. For instance, ??/ ??/ ??/ ?/? is an
NP chunk (?B-NP/ ??I-NP/ ??I-NP/ ?I-
NP/) while ??/??(forever)/ ??(full-blown)/
?(DE)/???(Chinese Redbud)/?/? is tagged
as (?O/ ??O /??O/ ?O/ ???B-NP/
?O/). We extend the tags with SPE for Special
Terms: B-NP-SPE and I-NP-SPE.
2) Coordination: These problems are related
to the conjunctions ??(and), ?(and), ?(or),
?(and)?. They can be divided into two types:
chunks with conjunctions and chunks without
conjunctions. For instance, ???(HongKong)/
?(and)/??(Macau)/? is an NP chunk (??B-
NP/ ?I-NP/ ??I-NP/), while in ???(least)/
??(salary)/ ?(and)/ ???(living mainte-
nance)/? it is difficult to tell whether ???? is a
shared modifier or not, even for people. We extend
the tags with COO for Coordination: B-NP-COO
and I-NP-COO.
3) Named Entities Tagging: Named Enti-
ties(NE)(Sang and Meulder, 2003) are not dis-
tinguished in CTB4, and they are all tagged as
?NR?. However, they play different roles in
chunks, especial in noun phrases. For instance,
???-NR(Macau)/ ??-NN(Airport)? and ??
?-NR(Hong Kong)/??-NN(Airport)? vs ???
?-NR(Deng Xiaoping)/ ??-NN(Mr.)? and ??
??-NR(Song Weiping) ??-NN(President)?.
Here ???? and ???? are LOCATION, while
????? and ????? are PERSON. To investi-
gate the effect of Named Entities, we use a LOCA-
TION dictionary, which is generated from the PFR
corpus9 of ICL, Peking University, to tag location
words in the CTB4 Corpus. Then we extend the
tags with LOC for this problem: B-NP-LOC and
I-NP-LOC.
From the above cases study, we know the steps
of Tag-Extension. Firstly, identifying a special
problem of chunking. Secondly, extending the
chunk tags via Equation (1). Finally, replacing the
tags of related tokens with new chunk tags. After
Tag-Extension, we use new added chunk tags to
describe some special problems.
5 Voting Methods
Kudo and Matsumoto(Kudo and Matsumoto,
2001) reported that they achieved higher accuracy
by applying voting of systems that were trained
using different data representations. Tjong Kim
Sang et al(Sang and Buchholz, 2000) reported
similar results by combining different systems.
In order to provide better results, we also ap-
ply the voting of basic systems, including SVMs,
CRFs, MBL and TBL. Depending on the charac-
teristics in the chunking task, we propose two new
voting methods. In these two voting methods, we
consider long distance information.
In the weighted voting method, we can assign
different weights to the results of the individ-
ual system(van Halteren et al, 1998). However,
it requires a larger amount of computational ca-
pacity as the training data is divided and is re-
peatedly used to obtain the voting weights. In
this paper, we give the same weight to all ba-
sic systems in our voting methods. Suppose, we
have K basic systems, the input sentence is x =
x1, x2, ..., xn, and the results of K basic systems
are tj = t1j , t2j , ..., tnj , 1 ? j ? K. Then our
goal is to gain a new result y = y1, y2, ..., yn by
voting.
5.1 Basic Voting
This is traditional voting method, which is the
same as Uniform Weight in (Kudo and Mat-
sumoto, 2001). Here we name it as Basic Voting.
For each position, we have K candidates from K
basic systems. After voting, we choose the candi-
date with the most votes as the final result for each
position.
9More information at http://www.icl.pku.edu
100
5.2 Sent-based Voting
In this paper, we treat chunking as a sequence la-
beling task. Here we apply this idea in computing
the votes of one sentence instead of one word. We
name it as Sent-based Voting. For one sentence,
we have K candidates, which are the tagged se-
quences produced by K basic systems. First, we
vote on each position, as done in Basic Voting.
Then we compute the votes of every candidate by
accumulating the votes of each position. Finally,
we choose the candidate with the most votes as
the final result for the sentence. That is to say, we
make a decision based on the votes of the whole
sentence instead of each position.
5.3 Phrase-based Voting
In chunking, one phrase includes one or more
words, and the word tags in one phrase depend on
each other. Therefore, we propose a novel vot-
ing method based on phrases, and we compute the
votes of one phrase instead of one word or one sen-
tence. Here we name it as Phrase-based Voting.
There are two steps in the Phrase-based Voting
procedure. First, we segment one sentence into
pieces. Then we calculate the votes of the pieces.
Table 3 is the algorithm of Phrase-based Voting,
where F (tij , tik) is a binary function:
F (tij , tik) =
{
1 : tij = tik
0 : tij 6= tik (2)
In the segmenting step, we seek the ?O? or ?B-
XP? (XP can be replaced by any type of phrase)
tags, in the results of basic systems. Then we get a
new piece if all K results have the ?O? or ?B-XP?
tags at the same position.
In the voting step, the goal is to choose a result
for each piece. For each piece, we have K candi-
dates. First, we vote on each position within the
piece, as done in Basic Voting. Then we accumu-
late the votes of each position for every candidate.
Finally, we pick the one, which has the most votes,
as the final result for the piece.
The difference in these three voting methods is
that we make the decisions in different ranges: Ba-
sic Voting is at one word; Phrase-based Voting is
in one piece; and Sent-based Voting is in one sen-
tence.
6 Experiments
In this section, we investigated the performance of
Chinese chunking on the CTB4 Corpus.
Input:
Sequence: x = x1, ..., xn;
K results: tj = t1j , ..., tnj , 1 ? j ? K.
Output:
Voted results: y = y1, y2, ..., yn
Segmenting: Segment the sentence into pieces.
Pieces[]=null; begin = 1
For each i in (2, n){
For each j in (1,K)
if(tij is not ?O? and ?B-XP?) break;
if(j > K){
add new piece: p = xbegin, ..., xi?1 into Pieces;
begin = i; }}
Voting: Choose the result with the most votes for each
piece: p = xbegin, ..., xend.
Votes[K] = 0;
For each k in (1,K)
V otes[k] =
?
begin?i?end,1?j?K
F (tij , tik) (3)
kmax = argmax1?k?K(V otes[k]);
Choose tbegin,kmax , ..., tend,kmax as the result for
piece p.
Table 3: Algorithm of Phrase-based Voting
6.1 Experimental Setting
To investigate the chunker sensitivity to the size
of the training set, we generated different sizes of
training sets, including 1%, 2%, 5%, 10%, 20%,
50%, and 100% of the total training data.
In our experiments, we used all the default pa-
rameter settings of the packages. Our SVMs and
CRFs chunkers have a first-order Markov depen-
dency between chunk tags.
We evaluated the results as CONLL2000 share-
task did. The performance of the algorithm was
measured with two scores: precision P and recall
R. Precision measures how many chunks found by
the algorithm are correct and the recall rate con-
tains the percentage of chunks defined in the cor-
pus that were found by the chunking program. The
two rates can be combined in one measure:
F1 = 2? P ?RR+ P (4)
In this paper, we report the results with F1 score.
6.2 Experimental Results
6.2.1 POS vs. WORD+POS
In this experiment, we compared the perfor-
mance of different feature representations, in-
101
 70
 75
 80
 85
 90
 95
 0.01  0.02  0.05  0.1  0.2  0.5  1
F
1
Size of Training data
SVM_WPSVM_PCRF_WPCRF_P
Figure 1: Results of different features
cluding POS and WORD+ POS(See section 3.2),
and set the window size as 2. We also inves-
tigated the effects of different sizes of training
data. The SVMs and CRFs approaches were used
in the experiments because they provided good
performance in chunking(Kudo and Matsumoto,
2001)(Sha and Pereira, 2003).
Figure 1 shows the experimental results, where
xtics denotes the size of the training data, ?WP?
refers to WORD+POS, ?P? refers to POS. We can
see from the figure that WORD+POS yielded bet-
ter performance than POS in the most cases. How-
ever, when the size of training data was small,
the performance was similar. With WORD+POS,
SVMs provided higher accuracy than CRFs in
all training sizes. However, with POS, CRFs
yielded better performance than SVMs in large
scale training sizes. Furthermore, we found SVMs
with WORD+POS provided 4.07% higher accu-
racy than with POS, while CRFs provided 2.73%
higher accuracy.
6.2.2 Comparison of Models
In this experiment, we compared the perfor-
mance of the models, including SVMs, CRFs,
MBL, and TBL, in Chinese chunking. In the ex-
periments, we used the feature WORD+POS and
set the window size as 2 for the first two mod-
els. For MBL, WORD features were within a one-
window size, and POS features were within a two-
window size. We used the original data for TBL
without any reformatting.
Table 4 shows the comparative results of the
models. We found that the SVMs approach was
superior to the other ones. It yielded results that
were 0.72%, 1.51%, and 3.58% higher accuracy
than respective CRFs, TBL, and MBL approaches.
SVMs CRFs TBL MBL
ADJP 84.45 84.55 85.95 80.48
ADVP 83.12 82.74 81.98 77.95
CLP 5.26 0.00 0.00 3.70
DNP 99.65 99.64 99.65 99.61
DP 99.70 99.40 99.70 99.46
DVP 96.77 92.89 99.61 99.41
LCP 99.85 99.85 99.74 99.82
LST 68.75 68.25 56.72 64.75
NP 90.54 89.79 89.82 87.90
PP 99.67 99.66 99.67 99.59
QP 96.73 96.53 96.60 96.40
VP 89.74 88.50 85.75 82.51
+ 91.46 90.74 89.95 87.88
Table 4: Comparative Results of Models
Method Precision Recall F1
CRFs 91.47 90.01 90.74
SVMs 92.03 90.91 91.46
V1 91.97 90.66 91.31
V2 92.32 90.93 91.62
V3 92.40 90.97 91.68
Table 5: Voting Results
Giving more details for each category, the SVMs
approach provided the best results in ten cate-
gories, the CRFs in one category, and the TBL in
five categories.
6.2.3 Comparison of Voting Methods
In this section, we compared the performance of
the voting methods of four basic systems, which
were used in Section 6.2.2. Table 5 shows the
results of the voting systems, where V1 refers
to Basic Voting, V2 refers to Sent-based Voting,
and V3 refers to Phrase-based Voting. We found
that Basic Voting provided slightly worse results
than SVMs. However, by applying the Sent-
based Voting method, we achieved higher accu-
racy than any single system. Furthermore, we
were able to achieve more higher accuracy by ap-
plying Phrase-based Voting. Phrase-based Voting
provided 0.22% and 0.94% higher accuracy than
respective SVMs, CRFs approaches, the best two
single systems.
The results suggested that the Phrase-based Vot-
ing method is quite suitable for chunking task. The
Phrase-based Voting method considers one chunk
as a voting unit instead of one word or one sen-
tence.
102
SVMs CRFs TBL MBL V3
NPR 90.62 89.72 89.89 87.77 90.92
COO 90.61 89.78 90.05 87.80 91.03
SPE 90.65 90.14 90.31 87.77 91.00
LOC 90.53 89.83 89.69 87.78 90.86
NPR* - - - - 91.13
Table 6: Results of Tag-Extension in NP Recogni-
tion
6.2.4 Tag-Extension
NP is the most important phrase in Chinese
chunking and about 47% phrases in the CTB4 Cor-
pus are NPs. In this experiment, we presented the
results of Tag-Extension in NP Recognition.
Table 6 shows the experimental results of Tag-
Extension, where ?NPR? refers to chunking with-
out any extension, ?SPE? refers to chunking
with Special Terms Tag-Extension, ?COO? refers
to chunking with Coordination Tag-Extension,
?LOC? refers to chunking with LOCATION Tag-
Extension, ?NPR*? refers to voting of eight sys-
tems(four of SPE and four of COO), and ?V3?
refers to Phrase-based Voting method.
For NP Recognition, SVMs also yielded the
best results. But it was surprised that TBL pro-
vided 0.17% higher accuracy than CRFs. By ap-
plying Phrase-based Voting, we achieved better re-
sults, 0.30% higher accuracy than SVMs.
From the table, we can see that the Tag-
Extension approach can provide better results. In
COO, TBL got the most improvement with 0.16%.
And in SPE, TBL and CRFs got the same improve-
ment with 0.42%. We also found that Phrase-
based Voting can improve the performance signif-
icantly. NPR* provided 0.51% higher than SVMs,
the best single system.
For LOC, the voting method helped to improve
the performance, provided at least 0.33% higher
accuracy than any single system. But we also
found that CRFs and MBL provided better results
while SVMs and TBL yielded worse results. The
reason was that our NE tagging method was very
simple. We believe NE tagging can be effective
in Chinese chunking, if we use a highly accurate
Named Entity Recognition system.
7 Conclusions
In this paper, we conducted an empirical study of
Chinese chunking. We compared the performance
of four models, SVMs, CRFs, MBL, and TBL.
We also investigated the effects of using different
sizes of training data. In order to provide higher
accuracy, we proposed two new voting methods
according to the characteristics of the chunking
task. We proposed the Tag-Extension approach to
resolve the special problems of Chinese chunking
by extending the chunk tags.
The experimental results showed that the SVMs
model was superior to the other three models.
We also found that part-of-speech tags played an
important role in Chinese chunking because the
gap of the performance between WORD+POS and
POS was very small.
We found that the proposed voting approaches
can provide higher accuracy than any single sys-
tem can. In particular, the Phrase-based Voting ap-
proach is more suitable for chunking task than the
other two voting approaches. Our experimental
results also indicated that the Tag-Extension ap-
proach can improve the performance significantly.
References
Steven P. Abney. 1991. Parsing by chunks. In
Robert C. Berwick, Steven P. Abney, and Carol
Tenny, editors, Principle-Based Parsing: Computa-
tion and Psycholinguistics, pages 257?278. Kluwer,
Dordrecht.
Eric Brill. 1995. Transformation-based error-driven
learning and natural language processing: A case
study in part of speech tagging. Computational Lin-
guistics, 21(4):543?565.
Walter Daelemans, Jakub Zavrel, Ko van der Sloot,
and Antal van den Bosch. 2004. Timbl: Tilburg
memory-based learner v5.1.
James Hammerton, Miles Osborne, Susan Armstrong,
and Walter Daelemans. 2002. Introduction to spe-
cial issue on machine learning approaches to shallow
parsing. JMLR, 2(3):551?558.
Taku Kudo and Yuji Matsumoto. 2000. Use of sup-
port vector learning for chunk identification. In In
Proceedings of CoNLL-2000 and LLL-2000, pages
142?144.
Taku Kudo and Yuji Matsumoto. 2001. Chunking
with support vector machines. In In Proceedings of
NAACL01.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In International Conference on Ma-
chine Learning (ICML01).
103
Heng Li, Jonathan J. Webster, Chunyu Kit, and Tian-
shun Yao. 2003a. Transductive hmm based chi-
nese text chunking. In Proceedings of IEEE NLP-
KE2003, pages 257?262, Beijing, China.
Sujian Li, Qun Liu, and Zhifeng Yang. 2003b. Chunk-
ing parsing with maximum entropy principle (in chi-
nese). Chinese Journal of Computers, 26(12):1722?
1727.
Hongqiao Li, Changning Huang, Jianfeng Gao, and Xi-
aozhong Fan. 2004. Chinese chunking with another
type of spec. In The Third SIGHAN Workshop on
Chinese Language Processing.
Andrew Kachites McCallum. 2002. Mal-
let: A machine learning for language toolkit.
http://mallet.cs.umass.edu.
Seong-Bae Park and Byoung-Tak Zhang. 2003.
Text chunking by combining hand-crafted rules and
memory-based learning. In ACL, pages 497?504.
Lance Ramshaw and Mitch Marcus. 1995. Text
chunking using transformation-based learning. In
David Yarovsky and Kenneth Church, editors, Pro-
ceedings of the Third Workshop on Very Large Cor-
pora, pages 82?94, Somerset, New Jersey. Associa-
tion for Computational Linguistics.
Erik F. Tjong Kim Sang and Sabine Buchholz. 2000.
Introduction to the conll-2000 shared task: Chunk-
ing. In Proceedings of CoNLL-2000 and LLL2000,
pages 127?132, Lisbin, Portugal.
Erik F. Tjong Kim Sang and Fien De Meulder.
2003. Introduction to the conll-2003 shared task:
Language-independent named entity recognition. In
Proceedings of CoNLL-2003.
Erik F. Tjong Kim Sang. 2002. Memory-based shal-
low parsing. JMLR, 2(3):559?594.
Fei Sha and Fernando Pereira. 2003. Shallow parsing
with conditional random fields. In Proceedings of
HLT-NAACL03.
Yongmei Tan, Tianshun Yao, Qing Chen, and Jingbo
Zhu. 2004. Chinese chunk identification using svms
plus sigmoid. In IJCNLP, pages 527?536.
Yongmei Tan, Tianshun Yao, Qing Chen, and Jingbo
Zhu. 2005. Applying conditional random fields
to chinese shallow parsing. In Proceedings of
CICLing-2005, pages 167?176, Mexico City, Mex-
ico. Springer.
Hans van Halteren, Jakub Zavrel, and Walter Daele-
mans. 1998. Improving data driven wordclass tag-
ging by system combination. In COLING-ACL,
pages 491?497.
V. Vapnik. 1995. The Nature of Statistical Learning
Theory. Springer-Verlag, New York.
Daelemans Walter, Sabine Buchholz, and Jorn Veen-
stra. 1999. Memory-based shallow parsing.
Shih-Hung Wu, Cheng-Wei Shih, Chia-Wei Wu,
Tzong-Han Tsai, and Wen-Lian Hsu. 2005. Ap-
plying maximum entropy to robust chinese shallow
parsing. In Proceedings of ROCLING2005.
Nianwen Xue, Fei Xia, Shizhe Huang, and Anthony
Kroch. 2000. The bracketing guidelines for the
penn chinese treebank. Technical report, University
of Pennsylvania.
Yuqi Zhang and Qiang Zhou. 2002. Chinese base-
phrases chunking. In Proceedings of The First
SIGHAN Workshop on Chinese Language Process-
ing.
Tiejun Zhao, Muyun Yang, Fang Liu, Jianmin Yao, and
Hao Yu. 2000. Statistics based hybrid approach to
chinese base phrase identification. In Proceedings
of Second Chinese Language Processing Workshop.
GuoDong Zhou, Jian Su, and TongGuan Tey. 2000.
Hybrid text chunking. In Claire Cardie, Walter
Daelemans, Claire Ne?dellec, and Erik Tjong Kim
Sang, editors, Proceedings of the CoNLL00, Lis-
bon, 2000, pages 163?165. Association for Compu-
tational Linguistics, Somerset, New Jersey.
104
Multilingual Aligned Parallel Treebank Corpus Reflecting
Contextual Information and Its Applications
Kiyotaka Uchimoto? Yujie Zhang? Kiyoshi Sudo?
Masaki Murata? Satoshi Sekine? Hitoshi Isahara?
?National Institute of Information and Communications Technology
3-5 Hikari-dai, Seika-cho, Soraku-gun,
Kyoto 619-0289, Japan
{uchimoto,yujie,murata,isahara}@nict.go.jp
?New York University
715 Broadway, 7th floor
New York, NY 10003, USA
{sudo,sekine}@cs.nyu.edu
Abstract
This paper describes Japanese-English-Chinese
aligned parallel treebank corpora of newspaper
articles. They have been constructed by trans-
lating each sentence in the Penn Treebank and
the Kyoto University text corpus into a cor-
responding natural sentence in a target lan-
guage. Each sentence is translated so as to
reflect its contextual information and is anno-
tated with morphological and syntactic struc-
tures and phrasal alignment. This paper also
describes the possible applications of the par-
allel corpus and proposes a new framework to
aid in translation. In this framework, paral-
lel translations whose source language sentence
is similar to a given sentence can be semi-
automatically generated. In this paper we show
that the framework can be achieved by using
our aligned parallel treebank corpus.
1 Introduction
Recently, accurate machine translation systems
can be constructed by using parallel corpora
(Och and Ney, 2000; Germann et al, 2001).
However, almost all existing machine transla-
tion systems do not consider the problem of
translating a given sentence into a natural sen-
tence reflecting its contextual information in the
target language. One of the main reasons for
this is that we had many problems that had to
be solved by one-sentence to one-sentence ma-
chine translation before we could solve the con-
textual problem. Another reason is that it was
difficult to simply investigate the influence of
the context on the translation because sentence
correspondences of the existing bilingual doc-
uments are rarely one-to-one, and are usually
one-to-many or many-to-many.
On the other hand, high-quality treebanks
such as the Penn Treebank (Marcus et al, 1993)
and the Kyoto University text corpus (Kuro-
hashi and Nagao, 1997) have contributed to
improving the accuracies of fundamental tech-
niques for natural language processing such as
morphological analysis and syntactic structure
analysis. However, almost all of these high-
quality treebanks are based on monolingual cor-
pora and do not have bilingual or multilin-
gual information. There are few high-quality
bilingual or multilingual treebank corpora be-
cause parallel corpora have mainly been actively
used for machine translation between related
languages such as English and French, there-
fore their syntactic structures are not required
so much for aligning words or phrases. How-
ever, syntactic structures are necessary for ma-
chine translation between languages whose syn-
tactic structures are different from each other,
such as in Japanese-English, Japanese-Chinese,
and Chinese-English machine translations, be-
cause it is more difficult to automatically align
words or phrases between two unrelated lan-
guages than between two related languages. Ac-
tually, it has been reported that syntactic struc-
tures contribute to improving the accuracy of
word alignment between Japanese and English
(Yamada and Knight, 2001). Therefore, if we
had a high-quality parallel treebank corpus, the
accuracies of machine translation between lan-
guages whose syntactic structures are differ-
ent from each other would improve. Further-
more, if the parallel treebank corpus had word
or phrase alignment, the accuracy of automatic
word or phrase alignment would increase by
using the parallel treebank corpus as training
data. However, so far, there is no aligned par-
allel treebank corpus whose domain is not re-
stricted. For example, the Japanese Electronics
Industry Development Association?s (JEIDA?s)
bilingual corpus (Isahara and Haruno, 2000)
has sentence, phrase, and proper noun align-
ment. However, it does not have morphologi-
cal and syntactic information, the alignment is
partial, and the target is restricted to a white
paper. The Advance Telecommunications Re-
search dialogue database (ATR, 1992) is a par-
allel treebank corpus between Japanese and En-
glish. However, it does not have word or phrase
alignment, and the target domain is restricted
to travel conversation.
Therefore, we have been constructing aligned
parallel treebank corpora of newspaper articles
between languages whose syntactic structures
are different from each other since 2001; they
meet the following conditions.
1. It is easy to investigate the influence of the con-
text on the translation, which means the sen-
tences that come before and after a particular
sentence, and that help us to understand the
meaning of a particular word such as a pro-
noun.
2. The annotated information in the existing
monolingual high-quality treebanks can be uti-
lized.
3. They are open to the public.
To construct parallel corpora that satisfy these
conditions, each sentence in the Penn Tree-
bank (Release 2) and the Kyoto University text
corpus (Version 3.0) has been translated into
a corresponding natural sentence reflecting its
contextual information in a target language by
skilled translators, revised by native speakers,
and each parallel translation has been anno-
tated with morphological and syntactic struc-
tures, and phrasal alignment. Henceforth, we
call the parallel corpus that is constructed by
pursuing the above policy an aligned parallel
treebank corpus reflecting contextual informa-
tion. In this paper, we describe an aligned par-
allel treebank corpus of newspaper articles be-
tween Japanese, English, and Chinese, and its
applications.
2 Construction of Aligned Parallel
Treebank Corpus Reflecting
Contextual Information
2.1 Human Translation of Existing
Monolingual Treebank
The Penn Treebank is a tagged corpus of Wall
Street Journal material, and it is divided into 24
sections. The Kyoto University text corpus is a
tagged corpus of the Mainichi newspaper, which
is divided into 16 sections according to the cat-
egories of articles such as the sports section and
the economy section. To maintain the consis-
tency of expressions in translation, a few partic-
ular translators were assigned to translate arti-
cles in a particular section, and the same trans-
lator was assigned to the same section. The
instructions to translators for Japanese-English
translation is basically as follows.
1. One-sentence to one-sentence translation as a
rule
Translate a source sentence into a target sen-
tence. In case the translated sentence becomes
unnatural by pursuing this policy, leave a com-
ment.
2. Natural translation reflecting contextual infor-
mation
Except in the case that the translated sentence
becomes unnatural by pursuing policy 1, trans-
late a source sentence into a target sentence
naturally.
By deletion, replacement, or supplementation,
let the translated sentence be natural in the
context.
In an entire article, the translated sentences
must maintain the same meaning and informa-
tion as those of the original sentences.
3. Translations of proper nouns
Find out the translations of proper nouns by
looking up the nouns in a dictionary or by using
a web search. In case a translation cannot be
found, use a temporary name and report it.
We started the construction of Japanese-
Chinese parallel corpus in 2002. The Japanese
sentences of the Kyoto University text corpus
were also translated into Chinese by human
translators. Then each translated Chinese sen-
tence was revised by a second Chinese native.
The instruction to the translators is the same
as that given in the Japanese-English human
translations.
The breakdown of the parallel corpora is
shown in Table 1. We are planning to trans-
late the remaining 18,714 sentences of the Kyoto
University text corpus and the remaining 30,890
sentences of the Penn Treebank. As for the nat-
uralness of the translated sentences, there are
207 (1%) unnatural English sentences of the
Kyoto University text corpus, and 462 (2.5%)
unnatural Japanese sentences of the Penn Tree-
bank generated by pursuing policy 1.
2.2 Morphological and Syntactic
Annotation
In the following sections, we describe the anno-
tated information of the parallel treebank cor-
pus based on the Kyoto University text corpus.
2.2.1 Morphological and Syntactic
Information of Japanese-English
corpus
Translated English sentences were analyzed by
using the Charniak Parser (Charniak, 1999).
Then, the parsed sentences were manually re-
vised. The definitions of part-of-speech (POS)
categories and syntactic labels follow those of
the Treebank I style (Marcus et al, 1993).
We have finished revising the 10,328 parsed
sentences that appeared from January 1st to
11th. An example of morphological and syn-
tactic structures is shown in Figure 1. In this
figure, ?S-ID? means the sentence ID in the
Kyoto University text corpus. EOJ means the
boundary between a Japanese parsed sentence
and an English parsed sentence. The definition
of Japanese morphological and syntactic infor-
mation follows that of the Kyoto University text
corpus (Version 3.0). The syntactic structure is
represented by dependencies between Japanese
phrasal units called bunsetsus. The phrasal
Table 1: Breakdown of the parallel corpora
Original corpus Languages # of parallel sentences
Kyoto University text corpus Japanese-English 19,669 (from Jan. 1st to 17th in 1995)
Japanese-Chinese 38,383 (all)
Penn Treebank Japanese-English 18,318 (from section 0 to 9)
Total Japanese-English 37,987 (Approximately 900,000 English words)
Japanese-Chinese 38,383 (Approximately 900,000 Chinese words)
# S-ID:950104141-008
* 0 2D
???? ???? * ?? * * *
* 1 2D
?? ?????? * ?? ?? * *
? ?? * ??? ???????? * *
?? ??? * ??? ???????? * *
? ? * ?? ???? * *
* 2 6D
?? ???? * ?? ???? * *
? ? ? ??? * ??? ????????
? ? * ?? ?? * *
* 3 4D
?? ???? * ?? ???? * *
? ? * ?? ??? * *
* 4 5D
??? ???? ??? ?? * ???? ???
* 5 6D
?? ???? * ?? ???? * *
? ? * ?? ??? * *
* 6 -1D
??? ???? ?? ?? * ?????? ??????
? ? ?? ??? ?????? ???? ???
?? ?? ?? ??? ????????? ???????? ???
? ? * ?? ?? * *
EOJ
(S1 (S (NP (PRP They))
(VP (VP (VBD were)
(NP (DT all))
(ADJP (NP (QP (RB about)
(CD nineteen))
(NNS years))
(JJ old)))
(CC and)
(VP (VBD had)
(S (NP (DT no)
(NN strength))
(VP (VBN left)
(SBAR (S (VP (ADVP (RB even))
(TO to)
(VP (VB answer)
(NP (NNS questions))))))))))
(. .)))
EOE
Figure 1: Example of morphological and syn-
tactic information.
units or bunsetsus are minimal linguistic units
obtained by segmenting a sentence naturally in
terms of semantics and phonetics, and each of
them consists of one or more morphemes.
2.2.2 Chinese Morphological
Information of Japanese-Chinese
corpus
Chinese sentences are composed of strings of
Hanzi and there are no spaces between words.
The morphological annotation, therefore, in-
cludes providing tags of word boundaries and
POSs of words. We analyzed the Chinese sen-
tences by using the morphological analyzer de-
veloped by Peking University (Zhou and Duan,
1994). There are 39 categories in this POS set.
Then the automatically tagged sentences were
revised by the third native Chinese. In this
pass the Chinese translations were revised again
while the results of word segmentation and POS
tagging were revised. Therefore the Chinese
translations are obtained with a high quality.
We have finished revising the 12,000 tagged sen-
tences. The revision of the remaining sentences
is ongoing. An example of tagged Chinese sen-
tences is shown in Figure 2. The letters shown
Figure 2: Example of morphological informa-
tion of Chinese corpus.
after ?/? indicate POSs. The Chinese sentence is
the translation of the Japanese sentence in Fig-
ure 1. The Chinese sentences are GB encoded.
The 38,383 translated Chinese sentences have
1,410,892 Hanzi and 926,838 words.
2.3 Phrasal Alignment
This section describes the annotated informa-
tion of 19,669 sentences of the Kyoto University
text corpus.
The minimum alignment unit should be as
small as possible, because bigger units can be
constructed from units of the minimum size.
However, we decided to define a bunsetsu as the
minimum alignment unit. One of the main rea-
sons for this is that the smaller the unit is, the
higher the human annotation cost is. Another
reason is that if we define a word or a morpheme
as a minimum alignment unit, expressions such
as post-positional particles in Japanese and arti-
cles in English often do not have alignments. To
effectively absorb those expressions and to align
as many parts as possible, we found that a big-
ger unit than a word or a morpheme is suitable
as the minimum alignment unit. We call the
minimum alignment based on bunsetsu align-
ment units the bunsetsu unit translation pair.
Bigger pairs than the bunsetsu unit translation
pairs can be automatically extracted based on
the bunsetsu unit translation pairs. We call all
of the pairs, including bunsetsu unit transla-
tion pairs, translation pairs. The bunsetsu unit
translation pairs for idiomatic expressions often
become unnatural. In this case, two or more
bunsetsu units are combined and handled as a
minimum alignment unit. The breakdown of
the bunsetsu unit translation pairs is shown in
Table 2.
Table 2: Breakdown of the bunsetsu unit trans-
lation pairs.
(1) total # of translation pairs 172,255
(2) # of different translation pairs 146,397
(3) # of Japanese expressions 110,284
(4) # of English expressions 111,111
(5) average # of English expressions 1.33
corresponding to a Japanese expression ((2)/(3))
(6) average # of Japanese expressions 1.32
corresponding to a English expression ((2)/(4))
(7) # of ambiguous Japanese expressions 15,699
(8) # of ambiguous English expressions 12,442
(9) # of bunsetsu unit translation pairs 17,719
consisting of two or more bunsetsus
An example of phrasal alignment is shown in
Figure 3. A Japanese sentence is shown from
the line after the S-ID to the EOJ. Each line
indicates a bunsetsu. Each rectangular line in-
dicates a dependency between bunsetsus. The
leftmost number in each line indicates the bun-
setsu ID. The corresponding English sentence is
shown in the next line after that of the EOJ
(End of Japanese) until the EOE (End of En-
glish). The English expressions corresponding
to each bunsetsu are tagged with the corre-
sponding bunsetsu ID such as <P id=?bunsetsu
ID?></P>. When there are two or more fig-
ures in the tag id such as id=?1,2?, it means two
or more bunsetsus are combined and handled as
a minimum alignment unit.
For example, we can extract the following
translation pairs from Figure 3.
 (J) ??? (yunyuu-ga) / ????? (kaikin-sa-reta);
(E)that had been under the ban
 (J) ??????? (beikoku-san-ringo-no); (E)of apples
imported from the U.S.
 (J) ???? (dai-ichi-bin-ga); (E)The first cargo
 (J)???????(uridasa-reta); (E)was brought to the
market.
 (J) ??????? (beikoku-san-ringo-no) / ????
(dai-ichi-bin-ga); (E)The first cargo / of apples im-
ported from the U.S.
# S-ID:950110003-001
1 ????????????????
2 ????????????????
3 ????????????????
4 ????????????????
5 ????????????????
6 ????????????????
7 ????????????????
8 ????????????????
9 ????????????????
10 ???????????????
11 ????????????????
EOJ
<P id="4">The first cargo</P> <P id="3">of apples
imported from the U.S.</P> <P id="1,2">that had been
under the ban</P> <P id="7">completed</P> <P id="6">
quarantine</P> <P id="7">and</P> <P id="11">was brought
to the market</P> <P id="10">for the first time</P>
<P id="5">on the 9th</P> <P id="9">at major supermarket
chain stores</P> <P id="8">in the Tokyo metropolitan
area</P> <P id="11">.</P>
EOE
Figure 3: Example of phrasal alignment.
 (J) ??????? (beikoku-san-ringo-no) / ????
(dai-ichi-bin-ga) /???????(uridasa-reta); (E)The
first cargo / of apples imported from the U.S. / was
brought to the market.
Here, Japanese and English expressions are
divided by the symbol ?;?, and ?/? means a
bunsetsu boundary.
An overview of the criteria of the alignment
is as follows. Align as many parts as possible,
except if a certain part is redundant. More de-
tailed criteria will be attached with our corpus
when it is open to the public.
1. Alignment of English grammatical elements
that are not expressed in Japanese
English articles, possessive pronouns, infinitive
to, and auxiliary verbs are joined with nouns
and verbs.
2. Alignment between a noun and its substitute
expression
A noun can be aligned with its substitute ex-
pression such as a pronoun.
3. Alignment of Japanese ellipses
An English expression is joined with its related
elements. For example, the English subject is
joined with its related verb.
4. Alignment of supplementary or explanatory ex-
pression in English
Supplementary or explanatory expressions in
English are joined with their related words.
? Ex.?
# S-ID:950104142-003
1 ???????????
2 ???????????
3 ???????????
4 ???????????
5 ???????????
6 ???????????
EOJ
<P id="1">The Chinese character used for "ka"</P>
has such meanings as "beautiful" and "splendid."
EOE
?"?? (ka)??? (niwa)" corresponds to
"The Chinese character used for "ka""
5. Alignment of date and time
When a Japanese noun representing date and
time is adverbial, the English preposition is
joined with the date and time.
6. Alignment of coordinate structures
When English expressions represented by ?X
(A + B)? correspond to Japanese expressions
represented by ?XA + XB?, the alignment of
X overlaps.
? Ex.?
# S-ID:950106149-005
1 ?????????????
2 ?????????????
3 ?????????????
4 ?????????????
5 ?????????????
6 ?????????????
7 ?????????????
8 ?????????????
EOJ
In the Kinki Region, disposal of wastes started
<P id="2"><P id="4"> at offshore sites of</P>
Amagasaki</P> and <P id="4">Izumiotsu</P> from
1989 and 1991 respectively.
EOE
?"??? (Amagasaki-oki) ? (de)" corresponds to
"at offshore sites of Amagasaki"
?"???? (Izumiotsu-oki) ? (de)" corresponds to
"at offshore sites of ? Izumiotsu"
3 Applications of Aligned Parallel
Treebank Corpus
3.1 Use for Evaluation of Conventional
Methods
The corpus as described in Section 2 can be
used for the evaluation of English-Japanese and
Japanese-English machine translation. We can
directly compare various methods of machine
translation by using this corpus. It can be sum-
marized as follows in terms of the characteristics
of the corpus.
One-sentence to one-sentence translation
can be simply used for the evaluation of
various methods of machine translation.
Morphological and syntactic information
can be used for the evaluation of methods
that actively use morphological and syntactic
information, such as methods for example-
based machine translation (Nagao, 1981;
Watanabe et al, 2003), or transfer-based
machine translation (Imamura, 2002).
Phrasal alignment is used for the evaluation of
automatically acquired translation knowledge
(Yamamoto and Matsumoto, 2003).
An actual comparison and evaluation is our
future work.
3.2 Analysis of Translation
One-sentence to one-sentence translation
reflects contextual information. Therefore, it
is suitable to investigate the influence of the
context on the translation. For example, we
can investigate the difference in the use of
demonstratives and pronouns between English
and Japanese. We can also investigate the
difference in the use of anaphora.
Morphological and syntactic information
and phrasal alignment can be used to investi-
gate the appropriate unit and size of transla-
tion rules and the relationship between syntac-
tic structures and phrasal alignment.
3.3 Use in Conventional Systems
One-sentence to one-sentence translation
can be used for training a statistical translation
model such as GIZA++ (Och and Ney, 2000),
which could be a strong baseline system for
machine translation.
Morphological and syntactic information
and phrasal alignment can be used to acquire
translation knowledge for example-based ma-
chine translation and transfer-based machine
translation.
In order to show what kind of units are help-
ful for example-based machine translation, we
investigated whether the Japanese sentences of
newspaper articles appearing on January 17,
1995, which we call test-set sentences, could be
translated into English sentences by using trans-
lation pairs appearing from January 1st to 16th
as a database. First, we found that only one out
of 1,234 test-set sentences agreed with one out
of 18,435 sentences in the database. Therefore,
a simple sentence search will not work well. On
the other hand, 6,659 bunsetsus out of 12,632
bunsetsus in the test-set sentences agreed with
those in the database. If words in bunsetsus are
expanded into their synonyms, the combination
of the expanded bunsetsus sets in the database
may cover the test-set sentences. Next, there-
fore, we investigated whether the Japanese test-
set sentences could be translated into English
sentences by simply combining translation pairs
appearing in the database. Given a Japanese
sentence, words were extracted from it and
translation pairs that include those words or
their synonyms, which were manually evalu-
ated, were extracted from the database. Then,
the English sentence was manually generated by
just combining English expressions in the ex-
tracted translation pairs. One hundred two rel-
atively short sentences (the average number of
bunsetsus is about 9.8) were selected as inputs.
The number of equivalent translations, which
mean that the translated sentence is grammat-
ical and has the same meaning as the source
sentence, was 9. The number of similar transla-
tions, which mean that the translated sentence
is ungrammatical, or different or wrong mean-
ings of words, tenses, and prepositions are used
in the translated sentence, was 83. The num-
ber of other translations, which mean that some
words are missing, or the meaning of the trans-
lated sentence is completely different from that
of the original sentence, was 10. For example,
the original parallel translation is as follows:
Japanese:????????????????????????
????????????????????????
English: New Party Sakigake proposed that towards the or-
dinary session, both parties found a council to dis-
cuss policy and Diet management.
Given the Japanese sentence, the translated
sentence was:
Translation:Sakigake Party suggested to set up an organiza-
tion between the two parties towards the regular
session of the Diet to discuss under the theme of
policies and the management of the Diet.
This result shows that only 9% of input sen-
tences can be translated into sentences equiv-
alent to the original ones. However, we found
that approximately 90% of input sentences can
be translated into English sentences that are
equivalent or similar to the original ones.
3.4 Similar Parallel Translation
Generation
The original aim of constructing an aligned par-
allel treebank corpus as described in Section 2 is
to achieve a new framework for translation aid
as described below.
It would be very convenient if multilingual
sentences could be generated by just writing
sentences in our mother language. Today, it
can be formally achieved by using commercial
machine translation systems. However, the au-
tomatically translated sentences are often in-
comprehensible. Therefore, we have to revise
the original and translated sentences by find-
ing and referring to parallel translation whose
source language sentence is similar to the orig-
inal one. In many cases, however, we cannot
find such similar parallel translations to the in-
put sentence. Therefore, it is difficult for users
who do not have enough knowledge of the target
languages to generate comprehensible sentences
in several languages by just searching similar
parallel translations in this way. Therefore, we
propose to generate similar parallel translations
whose source language sentence is similar to
the input sentence. We call this framework for
translation aid similar parallel translation gen-
eration.
We investigated whether the framework can
be achieved by using our aligned parallel tree-
bank corpus. As the first step of this study,
we investigated whether an appropriate parallel
translation can be generated by simply combin-
ing translation pairs extracted from our aligned
parallel treebank corpus in the following steps.
1. Extract each content word with its adjacent
function word in each bunsetsu in a given sen-
tence
2. The extracted content words and their adjacent
function words are expanded into their syn-
onyms and class words whose major and minor
POS categories are the same
3. Find translation pairs including the expanded
content words with their expanded adjacent
function words in the given sentence
4. For each bunsetsu, select a translation pair that
has similar dependency relationship to those in
the given sentence
5. Generate a parallel translation by combining
the selected translation pairs
The input sentences were randomly selected
from 102 sentences described in Section 3.3.
The above steps, except the third step, were
basically conducted manually. The Examples
of the input sentences and generated parallel
translations are shown in Figure 4.
The basic unit of translation pairs in our
aligned parallel treebank corpus is a bunsetsu,
and the basic unit in the selection of transla-
tion pairs is also a bunsetsu. One of the ad-
vantages of using a bunsetsu as a basic unit is
that a Japanese expression represented as one
of various expressions in English, or omitted in
English, such as Japanese post-positional par-
ticles, is paired with a content word. There-
fore, the translation of such an expression is ap-
propriately selected together with the transla-
tion of a content word when a certain trans-
lation pair is selected. If the translation of
such an expression was selected independently
of the translation of a content word, the com-
bination of each translation would be ungram-
matical or unnatural. Another advantage of the
basic unit, bunsetsu, is that we can easily refer
to dependency information between bunsetsus
when we select an appropriate translation pair
because the original treebank has the depen-
dency information between bunsetsus. These
advantages are utilized in the above generation
steps. For example, in the first step, a content
word ??? (kokkai, Diet session)? in the sec-
ond example in Figure 4 was extracted from the
bunsetsu ????? (tsuujo-kokkai, the ordinary
Diet session) ? (ni, case marker)?, and it was
expanded into its class word ?? (kai, meeting)?
in the second step. Then, a translation pair
?(J)??????????? (kokuren-kodomo-
no-kenri-iinkai)? (ni, case marker); (E)the UN
Committee on the Rights of the Child /(J)
?? (taishi); (E)towards? was extracted as a
translation pair in the third step. Since the
dependency between ????????????
(kokuren-kodomo-no-kenri-iinkai, the UN Com-
mittee on the Rights of the Child)? and ???
(taishi, towards)? is similar to that between ?
???? (tsuujo-kokkai, the ordinary Diet ses-
sion)? (ni, case marker)? and ??? (muke, to-
wards)? in the input sentence, this translation
pair was selected in the fourth step. Finally,
the bunsetsu ???????????? (kokuren-
kodomo-no-kenri-iinkai, the UN Committee on
the Rights of the Child) ? (ni, case marker)?
and its translation ?the UN Committee on the
Rights of the Child? was used for generation of
a parallel translation in the fifth step.
When we use the generated parallel transla-
tion for the exact translation of the input sen-
tence, we should replace ??????????
?? (kokuren-kodomo-no-kenri-iinkai)? and its
translation ?the UN Committee on the Rights
of the Child? with ????? (tsuujo-kokkai, the
ordinary Diet session)? and its translation ?the
ordinary Diet session? by consulting a bilingual
dictionary. In this example, ??? (sono)? and
?them? should also be replaced with ??? (ry-
oto)? and ?both parties?. It is easy to identify
words in the generated translation that should
be replaced with words in the input sentence
because each bunsetsu in translation pairs is al-
ready aligned. In such cases, templates such as
?[?? (kaigi)]? (ni)?? (muke)? and ?towards
[council]? can be automatically generated by
generalizing content words expanded in the sec-
ond step and their translation in the generated
translation. The average number of English ex-
pressions corresponding to a Japanese expres-
sion is 1.3 as shown in Table 2. Even when there
are two or more possible English expressions, an
appropriate English expression can be chosen
by selecting a Japanese expression by referring
to dependencies in extracted translation pairs.
Therefore, in many cases, English sentences can
be generated just by reordering the selected ex-
pressions. The English word order was esti-
mated manually in this experiment. However,
we can automatically estimate English word or-
der by using a language model or an English
surface sentence generator such as FERGUS
(Bangalore and Rambow, 2000). Unnatural or
ungrammatical parallel translations are some-
times generated in the above steps. However,
comprehensible translations can be generated
as shown in Figure 4. The biggest advantage
of this framework is that comprehensible target
sentences can be generated basically by refer-
ring only to source sentences. Although it is
costly to search and select appropriate transla-
tion pairs, we believe that human labor can be
reduced by developing a human interface. For
example, when we use a Japanese text gener-
ation system from keywords (Uchimoto et al,
2002), users should only select appropriate key-
words.
We are investigating whether or not we can
generate similar parallel translations to all of
the Japanese sentences appearing on January
17, 1995. So far, we found that we can gen-
erate similar parallel translations to 691 out of
840 sentences (the average number of bunsetsus
is about 10.3) including the 102 sentences de-
scribed in Section 3.3. We found that we could
not generate similar parallel translations to 149
out of 840 sentences.
In the proposed framework of similar paral-
lel translation generation, the language appear-
ing in a corpus corresponds to a controlled lan-
guage, and users are allowed to use only the
controlled language to write sentences in the
source language. We believe that high-quality
bilingual or multilingual documents can be gen-
erated by letting us adapt ourselves to the con-
trolled environment in this way.
4 Conclusion
This paper described aligned parallel treebank
corpora of newspaper articles between lan-
guages whose syntactic structures are different
from each other; they meet the following condi-
tions.
1. It is easy to investigate the influence of the con-
text on the translation.
2. The annotated information in the existing
monolingual high-quality treebanks can be uti-
lized.
3. It is open to the public.
To construct parallel corpora that satisfy
these conditions, each sentence in the existing
monolingual high-quality treebanks has been
translated into a corresponding natural sentence
reflecting its contextual information in a target
language by skilled translators, and each par-
allel translation has been annotated with mor-
phological and syntactic structures and phrasal
alignment.
This paper also described the possible ap-
plications of the parallel corpus and proposed
a similar parallel translation generation frame-
work. In this framework, a parallel translation
whose source language sentence is similar to a
given sentence can be semi-automatically gen-
erated. In this paper we demonstrated that
the framework could be achieved by using our
aligned parallel treebank corpus.
In the near future, the aligned parallel tree-
bank corpora will be open to the public, and
expanded. We are planning to use the corpora
actively for machine translation, as a transla-
tion aid, and for second language learning. We
are also planning to develop automatic or semi-
automatic alignment system and an efficient in-
terface for machine translation aid.
Input sentence
(Japanese only)
???????????????????????????????????????????(Prime Minister
Murayama and Finance Minister Takemura met in the presidential office and they exchanged their
opinions, mainly on the issue of the new faction being formed by the New Democratic Union.)
Generated paral-
lel translation
(J) ????????????????????????????????????????????
(E) Finance Minister Takemura held the meeting at the official residence to exchange views about the
formation of the new party of the New Democratic Union.
Input sentence
(Japanese only)
????????????????????????????????????????????????(New
Party Sakigake proposed that towards the ordinary session, both parties found a council to discuss policy
and Diet management.)
Generated paral-
lel translation
(J) ????????????????????????????????????????????????
???????
(E) Sakigake proposed to set up an organization between them towards the UN Committee on the Rights
of the Child to discuss under the theme of policies and the management of the Diet.
Input sentence
(Japanese only)
?????????????????????????????????????????????(The meeting
was also intended to slow the movement towards the new party by the New Democratic Union, which is
trying to deepen the relationship with the New Frontier Party.)
Generated paral-
lel translation
(J) ?????????????????????????????????????????????
(E) The meeting had meanings to restrict the movement that the new party of New Democratic Union
is progressing to strengthen the coalition with The New Frontier Party.
Input sentence
(Japanese only)
?????????????????????????????????????????????????
???????(Lower House Diet Member Tatsuo Kawabata of the New Frontier Party decided on the
16th that he would hand in notification of his secession to the party on the 17th, in order to form a new
faction with Sadao Yamahana?s group.)
Generated paral-
lel translation
(J) ????????????????????????????????????????????????
?????????
(E) On 16th Tatsuo Kawabata, a member of the House of Representatives of the New Frontier Party
decided to submit The notice to leave the party to the Shinsei Party on the 17th in order to establish a
new faction with Yuukichi Amano and others.
Input sentence
(Japanese only)
???????????????????????????(As for the faction name in the Upper House,
they will decide after they consider how to form a relationship with Democratic Reform Union.)
Generated paral-
lel translation
(J) ?????????????????????
(E) The name of the faction will be decided after discussing the relationship with the JTUC.
Figure 4: Example of generated similar parallel translations.
Acknowledgments
We thank the Mainichi Newspapers for permis-
sion to use their data.
References
ATR. 1992. Dialogue Database. http://www.red.atr.co.jp/
database page/taiwa.html.
S. Bangalore and O. Rambow. 2000. Exploiting a Probabilis-
tic Hierarchical Model for Generation. In Proceedings of
the COLING, pages 42?48.
E. Charniak. 1999. A Maximum-Entropy-Inspired Parser.
Technical Report CS-99-12.
U. Germann, M. Jahr, K. Knight, D. Marcu, and K. Yamada
2001. Fast Decoding and Optimal Decoding for Machine
Translation. In Proceedings of the ACL-EACL, pages 228?
235.
K. Imamura. 2002. Application of translation knowledge ac-
quired by hierarchical phrase alignment for pattern-based
MT. In Proceedings of the TMI, pages 74?84.
H. Isahara and M. Haruno. 2000. Japanese-English aligned
bilingual corpora. In Jean Veronis, editor, Parallel Text
Processing - Alignment and Use of Translation Corpora,
pages 313?334. Kluwer Academic Publishers.
S. Kurohashi and M. Nagao. 1997. Building a Japanese
Parsed Corpus while Improving the Parsing System. In
Proceedings of the NLPRS, pages 451?456.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz. 1993.
Building a Large Annotated Corpus of English: The Penn
Treebank. Computational Linguistics, 19(2):313?330.
M. Nagao. 1981. A Framework of a Mechanical Translation
between Japanese and English by Analogy Principle. In
Proceedings of the International NATO Symposium on Ar-
tificial and Human Intelligence.
F. J. Och and H. Ney. 2000. Improved Statistical Alignment
Models. In Proceedings of the ACL, pages 440?447.
K. Uchimoto, S. Sekine, and H. Isahara. 2002. Text Gen-
eration from Keywords. In Proceedings of the COLING,
pages 1037?1043.
H. Watanabe, S. Kurohashi, and E. Aramaki. 2003. Finding
Translation Patterns from Paired Source and Target De-
pendency Structures. In Michael Carl and Andy Way, ed-
itors, Recent Advances in Example-Based Machine Trans-
lation, pages 397?420. Kluwer Academic Publishers.
K. Yamada and K. Knight. 2001. A Syntax-based Statistical
Translation Model. In Proceedings of the ACL, pages 523?
530.
K. Yamamoto and Y. Matsumoto. 2003. Extracting Transla-
tion Knowledge from Parallel Corpora. In Michael Carl
and Andy Way, editors, Recent Advances in Example-
Based Machine Translation, pages 365?395. Kluwer Aca-
demic Publishers.
Q. Zhou and H. Duan. 1994. Segmentation and POS Tag-
ging in the Construction of Contemporary Chinese Cor-
pus. Journal of Computer Science of China, Vol.85. (in
Chinese)
Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 118?121,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Chinese Named Entity Recognition with Conditional Random Fields
Wenliang Chen and Yujie Zhang and Hitoshi Isahara
Computational Linguistics Group
National Institute of Information and Communications Technology
3-5 Hikari-dai, Seika-cho, Soraku-gun, Kyoto, Japan, 619-0289
{chenwl, yujie, isahara}@nict.go.jp
Abstract
We present a Chinese Named Entity
Recognition (NER) system submitted to
the close track of Sighan Bakeoff2006.
We define some additional features via do-
ing statistics in training corpus. Our sys-
tem incorporates basic features and addi-
tional features based on Conditional Ran-
dom Fields (CRFs). In order to correct in-
consistently results, we perform the post-
processing procedure according to n-best
results given by the CRFs model. Our fi-
nal system achieved a F-score of 85.14 at
MSRA, 89.03 at CityU, and 76.27 at LDC.
1 Introduction
Named Entity Recognition task in the 2006 Sighan
Bakeoff includes three corpora: Microsoft Re-
search (MSRA), City University of Hong Kong
(CityU), and Linguistic Data Consortium (LDC).
There are four types of Named Entities in the cor-
pora: Person Name, Organization Name, Location
Name, and Geopolitical Entity (only included in
LDC corpus).
We attend the close track of all three cor-
pora. In the close track, we can not use any
external resources. Thus except basic features,
we define some additional features by applying
statistics in training corpus to replace external re-
sources. Firstly, we perform word segmentation
using a simple left-to-right maximum matching al-
gorithm, in which we use a word dictionary gen-
erated by doing n-gram statistics. Then we de-
fine the features based on word boundaries. Sec-
ondly, we generate several lists according to the
relative position to Named Entity (NE). We de-
fine another type of features based on these lists.
Using these features, we build a Conditional Ran-
dom Fields(CRFs)-based Named Entity Recogni-
tion (NER) System. We use the system to generate
n-best results for every sentence, and then perform
a post-processing.
2 Conditional Random Fields
2.1 The model
Conditional Random Fields(CRFs), a statistical
sequence modeling framework, was first intro-
duced by Lafferty et alLafferty et al, 2001).
The model has been used for chunking(Sha and
Pereira, 2003). We only describe the model
briefly since full details are presented in the pa-
per(Lafferty et al, 2001).
In this paper, we regard Chinese NER as a se-
quence labeling problem. For our sequence label-
ing problem, we create a linear-chain CRFs based
on an undirected graph G = (V,E), where V is
the set of random variables Y = {Yi|1 ? i ? n},
for each of n tokens in an input sentence and
E = {(Yi?1, Yi)|1 ? i ? n} is the set of n ? 1
edges forming a linear chain. For each sentence x,
we define two non-negative factors:
exp(?Kk=1 ?kfk(yi?1, yi, x)) for each edge
exp(?K?k=1 ?
?
kf
?
k(yi, x)) for each node
where fk is a binary feature function, and K and
K ? are the number of features defined for edges
and nodes respectively. Following Lafferty et
al(Lafferty et al, 2001), the conditional probabil-
ity of a sequence of tags y given a sequence of
tokens x is:
P (y|x) = 1Z(x)exp(
?
i,k
?kfk(yi?1, yi, x) +
?
i,k
??kf
?
k(yi, x))
(1)
where Z(x) is the normalization constant. Given
the training data D, a set of sentences (characters
118
Tag Meaning
0 (zero) Not part of a named entity
PER A person name
ORG An organization name
LOC A location name
GPE A geopolitical entity
Table 1: Named Entities in the Data
with their corresponding tags), the parameters of
the model are trained to maximize the conditional
log-likelihood. When testing, given a sentence x
in the test data, the tagging sequence y is given by
Argmaxy?P (y?|x).
CRFs allow us to utilize a large number of ob-
servation features as well as different state se-
quence based features and other features we want
to add.
2.2 CRFs for Chinese NER
Our CRFs-based system has a first-order Markov
dependency between NER tags.
In our experiments, we do not use feature selec-
tion and all features are used in training and test-
ing. We use the following feature functions:
f(yi?1, yi, x, i) = p(x, i)q(yi?1, yi) (2)
where p(x, i) is a predicate on the input sequence
x and current position i and q(yi?1, yi) is a predi-
cate on pairs of labels. For instance, p(x, i) might
be ?the char at position i is?(and)?.
In our system, we used CRF++ (V0.42)1 to im-
plement the CRFs model.
3 Chinese Named Entity Recognition
The training data format is similar to that of the
CoNLL NER task 2002, adapted for Chinese. The
data is presented in two-column format, where the
first column consists of the character and the sec-
ond is a tag.
Table 1 shows the types of Named Entities in the
data. Every character is to be tagged with a NE
type label extended with B (Beginning character
of a NE) and I (Non-beginning character of a NE),
or 0 (Not part of a NE).
To obtain a good-quality estimation of the con-
ditional probability of the event tag, the observa-
tions should be based on features that represent the
difference of the two events. In our system, we de-
fine three types of features for the CRFs model.
1CRF++ is available at
http://chasen.org/ taku/software/CRF++/
3.1 Basic Features
The basic features of our system list as follows:
? . Cn(n = ?2,?1, 0, 1, 2)
? . CnCn+1(n = ?1, 0)
Where C refers to a Chinese character while C0
denotes the current character and Cn(C?n) de-
notes the character n positions to the right (left)
of the current character.
For example, given a character sequence ???
????, when considering the character C0 de-
notes ???, C?1 denotes ???, C?1C0 denotes ??
??, and so on.
3.2 Word Boundary Features
The sentences in training data are based on char-
acters. However, there are many features related to
the words. For instance, the word ???? can be a
important feature for Person Name. We perform
word segmentation using the left-to-right maxi-
mum matching algorithm, in which we use a word
dictionary generated by doing n-gram statistics in
training corpus. Then we use the word boundary
tags as the features for the model.
Firstly, we construct a word dictionary by ex-
tracting N-grams from training corpus as follows:
1. Extract arbitrary N-grams (2 ? n ? 10,
Frequency ? 10 ) from training corpus. We
get a list W1.
2. Use a tool to perform statistical substring
reduction in W1[ described in (Lv et al,
2004)]2. We get a list W2.
3. Construct a character list (CH)3, in which the
characters are top 20 frequency in training
corpus.
4. Remove the strings from W2, which contain
the characters in the list CH. We get final N-
grams list W3.
Secondly, we use W3 as a dictionary for left-
to-right maximum matching word segmentation.
We assign word boundary tags to sentences. Each
character can be assigned one of 4 possible bound-
ary tags: ?B? for a character that begins a word
and is followed by another character, ?M? for a
2Tools are available at
http://homepages.inf.ed.ac.uk/s0450736/Software
3To collect some characters such as punctuation, ???,
??? and so on.
119
character that occurs in the middle of a word, ?E?
for a character that ends a word, and ?S? for a char-
acter that occurs as a single-character word.
The word boundary features of our system list
as follows:
? . WTn(n = ?1, 0, 1)
Where WT refers to the word boundary tag while
WT0 denotes the tag of current character and
WTn(WT?n) denotes the tag n positions to the
right (left) of the current character.
3.3 Char Features
If we can use external resources, we often use the
lists of surname, suffix of named entity and prefix
of named entity for Chinese NER. In our system,
we generate these lists automatically from training
corpus by the procedure as follows:
? PSur: uni-gram characters, first characters of Person
Name. (surname)
? PC: uni-gram characters in Person Name.
? PPre: bi-gram characters before Person Name. (prefix
of Person Name)
? PSuf: bi-gram characters after Person Name. (suffix of
Person Name)
? LC: uni-gram characters in Location Name or Geopo-
litical entity.
? LSuf: uni-gram characters, the last characters of Loca-
tion Name or Geopolitical Entity. (suffix of Location
Name or Geopolitical Entity)
? OC: uni-gram characters in Organization Name.
? OSuf: uni-gram characters, the last characters of Orga-
nization Name. (suffix of Organization Name)
? OBSuf: bi-gram characters, the last two characters of
Organization Name. (suffix of Organization Name)
We remove the items in uni-gram lists if their fre-
quencies are less than 5 and in bi-gram lists if
their frequencies are less than 2. Based on these
lists, we assign the tags to every character. For in-
stance, if a character is included in PSur list, then
we assign a tag ?PSur 1?, otherwise assign a tag
?PSur 0?. Then we define the char features as fol-
lows:
? . PSur0PC0;
? . PSurnPCnPSurn+1PCn+1(n = ?1, 0);
? . PPre0;
? . PSuf0;
? . LC0OC0;
S is the list of sentences, S = {s1, s2, ..., sn}.
T is m-best results of S, T = {t1, t2, ..., tn}, which ti
is a set of m-best results of si.
pij is the score of tij , that is the jth result in ti.
Collect NE list:
Loop i in [1, n]
if(pi0 ? 0.5){
Exacting all NEs from ti0 to add into NEList.}
Replacing:
Loop i in [1, n]
if(pi0 ? 0.5){
FinalResult(si) = ti0.}
else{
TmpResult = ti0.
Loop j in [m, 1]
if(the NEs in tij is included in NEList){
Replace the matching string in TmpResult with new
NE tags.}
FinalResult(si) = TmpResult.
}
Table 2: The algorithm of Post-processing
? . LCnOCnLCn+1OCn+1(n = ?1, 0);
? . LSuf0OSuf0;
? . LSufnOSufnLSufn+1OSufn+1(n = ?1, 0);
4 Post-Processing
There are inconsistently results, which are tagged
by the CRFs model. Thus we perform a post-
processing step to correct these errors.
The post-processing tries to assign the correct
tags according to n-best results for every sentence.
Our system outputs top 20 labeled sequences for
each sentence with the confident scores. The post-
processing algorithm is shown at Table 2. Firstly,
we collect NE list from high confident results.
Secondly, we re-assign the tags for low confident
results using the NE list.
5 Evaluation Results
5.1 Results on Sighan bakeoff 2006
We evaluated our system in the close track, on
all three corpora, namely Microsoft Research
(MSRA), City University of Hong Kong (CityU),
and Linguistic Data Consortium (LDC). Our offi-
cial Bakeoff results are shown at Table 3, where
the columns P, R, and FB1 show precision, recall
and F measure(? = 1). We used all three types of
features in our final system.
In order to evaluate the contribution of fea-
tures, we conducted the experiments of each type
of features using the test sets with gold-standard
dataset. Table 4 shows the experimental results,
120
MSRA P R FB1
LOC 92.81 88.53 90.62
ORG 81.93 81.07 81.50
PER 85.41 74.15 79.38
Overall 88.14 82.34 85.14
CityU P R FB1
LOC 92.21 92.00 92.11
ORG 87.83 74.23 80.46
PER 92.77 89.05 90.87
Overall 91.43 86.76 89.03
LDC P R FB1
GPE 83.78 80.36 82.04
LOC 51.11 21.70 30.46
ORG 71.79 60.82 65.85
PER 82.40 75.58 78.84
Overall 80.26 72.65 76.27
Table 3: Our official Bakeoff results
MSRA CityU LDC
F1 84.73 88.26 76.18
+F2 88.67 76.30
+F3 88.74
Post 85.23 89.03 76.66
Table 4: Results of different combinations
where F1 refers to use basic features, F2 refers to
use the word boundary features, F3 refers to use
the char features, and Post refers to perform the
post-processing.
The results indicated that word boundary fea-
tures helped on LDC and CityU, char features only
helped on CityU and the post-processing always
helped to improve the performance.
6 Conclusion
This paper presented our Named Entity Recogni-
tion system for the close track of Bakeoff2006.
Our approach was based on Conditional Random
Fields model. Except basic features, we defined
the additional features by doing statistics in train-
ing corpus. In addition, we performed a post-
processing according to n-best results generated
by the CRFs model. The evaluation results showed
that our system achieved state-of-the-art perfor-
mance on all three corpora in the close track.
References
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In International Conference on Ma-
chine Learning (ICML01).
Xueqiang Lv, Le Zhang, and Junfeng Hu. 2004. Statis-
tical substring reduction in linear time. In Proceed-
ings of IJCNLP-04, HaiNan island, P.R.China.
Fei Sha and Fernando Pereira. 2003. Shallow parsing
with conditional random fields. In Proceedings of
HLT-NAACL03.
121
Paraphrasing of Chinese Utterances
Yujie Zhang?
Communications Research Laboratory
2-2-2, Hikaridai, Seika-cho, Soraku-gun, Kyoto 619-0289 Japan
yujie@crl.go.jp
Kazuhide Yamamoto
ATR Spoken Language Translation Research Laboratories
2-2-2, Hikaridai, Seika-cho, Soraku-gun, Kyoto 619-0288 Japan
yamamoto@fw.ipsj.or.jp
Abstract
One of the key issues in spoken language trans-
lation is how to deal with unrestricted expres-
sions in spontaneous utterances. This research
is centered on the development of a Chinese
paraphraser that automatically paraphrases ut-
terances prior to transfer in Chinese-Japanese
spoken language translation. In this paper, a
pattern-based approach to paraphrasing is pro-
posed for which only morphological analysis is
required. In addition, a pattern construction
method is described through which paraphras-
ing patterns can be efficiently learned from a
paraphrase corpus and human experience. Us-
ing the implemented paraphraser and the ob-
tained patterns, a paraphrasing experiment was
conducted and the results were evaluated.
1 Introduction
In spoken language translation one of the key
issues is how to deal with unrestricted expres-
sions in spontaneous utterances. To resolve this
problem, we have proposed a paraphrasing ap-
proach in which the utterances are automati-
cally paraphrased prior to transfer (Yamamoto
et al, 2001; Yamamoto, 2002). The paraphras-
ing process aims to bridge the gap between the
unrestricted expressions in the input and the
limited expressions that the transfer can trans-
late. In fact, paraphrasing actions are often seen
in daily communication. When a listener can-
not understand what a speaker said, the speaker
usually says it again using other words, i.e., he
paraphrases. In a Chinese-Japanese spoken lan-
guage translation system, the pre-processing of
Chinese utterances is involved and we attempt
to apply a paraphrasing approach. This paper
? This work was done when the author stayed at ATR
Spoken Language Translation Research Laboratories.
is focused on the paraphrasing of Chinese utter-
ances.
Some cases of paraphrasing research with cer-
tain targets have been reported. For example,
there has been work on rewriting the source
language in machine translation with a focus
on reducing syntactic ambiguities (Shirai et al,
1993), research on paraphrasing paper titles
with a focus on transforming syntactic struc-
tures to achieve readability (Sato, 1999), and
research on paraphrasing Japanese in summa-
rization with a focus on transforming a noun
modifier into a noun phrase (Kataoka et al,
1999). We have reported some research on
Chinese paraphrasing (Zhang and Yamamoto,
2001; Zhang et al, 2001; Zong et al, 2001). The
techniques of paraphrasing natural language
can be applied not only to the pre-processing
of machine translation but also to information
retrieval and summarization.
2 Goals and Approach
In the pre-processing stage of translation, Chi-
nese paraphrasing focuses on
(1) transforming the expressions of spoken lan-
guage into formal expressions,
(2) reducing syntactic and semantic ambigui-
ties,
(3) generating as many different expressions as
possible in order to include expressions that
can be translated by the transfer, and
(4) paraphrasing the main constituents of the
utterance in case the paraphrasing of the
whole utterance has no effect.
The aim of paraphrasing types (1), (2) and (4)
is to simplify the expressions of utterances, and
that of paraphrasing type (3) is to increase the
variations of utterances. At present, we focus
on paraphrasing types (1), (2) and (3).
Paraphrasing is a process that automatically
generates new expressions that have the same
meaning as the input sentence. At first glance
one would think that the problem could be re-
solved by separating it into two processes: the
parsing process that analyzes the input sentence
and obtains its meaning, and the generation
process that generates sentences from the ob-
tained meaning. However, this solution is not
practicable for the following reasons.
? At present, the techniques of parsing and
semantics analysis of the Chinese language
are far below the level needed for appli-
cation. When studying spoken language,
research on parsing and research on se-
mantics analysis are major themes them-
selves. For automatic paraphrasing, we
should first determine what kind of anal-
ysis is required and then start to develop a
parser or a semantics analyzer.
? Even if meanings can be obtained, goal (3)
cannot be achieved if only one sentence is
generated. Here, the demand that para-
phrasing should generate multiple expres-
sions is the most important. This focus is
different from that of conventional sentence
generation.
In fact, the paraphrasing can be conducted
at many different levels, for instance, words,
phrases, or larger constituents. Although the
paraphrasing of such constituents is probably
related to context, it is not true that paraphras-
ing is impossible without being able to under-
stand the whole sentence (Kataoka et al, 1999).
The paraphrasing process encounters the fol-
lowing problems. (i) How to identify objects,
i.e., which components of an input sentence will
be paraphrased, (ii) how to generate new sen-
tences, and (iii) how to ensure that the gener-
ated sentences have the same meaning as the
input sentence. In order to avoid the large cost
of syntax and semantics analysis, we propose
a pattern-based approach to paraphrasing in
which only morphological analysis is required.
The focus is placed on how to generate as many
different expressions as possible and how to get
paraphrasing patterns from a paraphrasing cor-
pus.
Table 1. Part of the part-of-speech tag set of
the Penn Chinese Treebank
Symbol Explanation
NN common noun
NR proper noun
PN pronoun
DT determiner
DEC  in a relative-clause
DEG associative 
M measure word
JJ other noun-modifier
VA predicative adjective
VC 
VE  as the main verb
VV other verb
AD adverb
P preposition excl.  and 
LC localizer
CD cardinal number
OD ordinal number
SP sentence-final particle
BA  in ba-construction
CC coordinating conjunction
3 Paraphrasing Pattern
The paraphrase corpus of the spoken Chinese
language consists of 20,000 original sentences
and 44,480 paraphrases, one original sentence
having at least two paraphrases (Zhang et al,
2001). The paraphrases were obtained by the
manual rephrasing of the original sentences:
words may be reordered, some words may be
substituted with synonyms, or the syntactic
structures may be changed. Such a paraphrase
corpus contains the knowledge of how to gener-
ate paraphrases for one sentence. We intend to
get paraphrasing patterns from the corpus. By
pairing each paraphrase with its corresponding
original sentence, 44,480 pairs were obtained.
Hereafter, we call such pairs paraphrase pairs.
Word segmentation and part-of-speech tagging
were carried out on the paraphrase pairs. The
part-of-speech tagger accepted the Penn Chi-
nese Treebank tag set, which comprises 33
parts-of-speech (Xia, 2000). A part of the Penn
Chinese Treebank tag set is shown in Table 1.
3.1 Extraction of Instances
For one paraphrase pair, the paraphrase may
differ from its original sentence in one of the
following paraphrasing phenomena: (1) word
order, (2) substitution of synonyms, and (3)
change of syntactic structure. For most para-
phrase pairs, the paraphrases contain a mixture
of the above phenomena. We need to classify
the paraphrasing phenomena and learn the rela-
tive paraphrasing patterns. In this way, we can
restrict the paraphrasing process to some lan-
guage phenomena and summarize the changes
in the information of the resultant paraphrases.
The following paraphrasing phenomena were
considered and related paraphrase pairs were
extracted.
3.1.1 Word Order
Word order in the spoken Chinese is com-
paratively free. In the paraphrase corpus,
quite a large proportion of the paraphrases
is created by word reordering. We extracted
the paraphrase pairs in which the morpheme
number of the original sentence is equal to that
of the paraphrase and each morpheme of the
original sentence appears in the paraphrase and
vice versa. One example is shown in 3-1.
[3? 1] An extracted paraphrase pair.
Original:  /VV  /AD  /VV
 /P  /PN  /VA 	 /SP
(Please call me again, could you?)
Paraphrase:  /VV  /AD  /P  /PN
 /VV  /VA 	 /SP
Guided by the extracted paraphrase pair, we
can in fact paraphrase the original sentence by
reordering its words according to the word order
of the paraphrase. The extracted paraphrase
pairs of this kind provided instances for learning
word order paraphrasing patterns.
3.1.2 Negative Expressions
In some paraphrase pairs, we observed that
paraphrasing phenomena were related to
negative expressions. For example, original
sentences include negative words ? (do not
)? or ? (did not)? , but their corresponding
paraphrases appear as affirmative forms with-
out these negative words. This fact implied
that the sentences could be simplified by delet-
ing the negative expressions. For this purpose,
the paraphrase pairs were extracted in which
the original sentences included the words ??
or ?? and the corresponding paraphrases did
not. One example is shown in 3-2.
[3? 2]
Original: ? /VV ? /AD ?? /VV  /PN 

/DEG ?? /NN
(Do you know my telephone number?)
Paraphrase: ?? /VV  /PN 
 /DEG ??
/NN 	 /SP
3.1.3 Expression of ??
The Chinese language has a few grammatical
markers. The particle ?? is one of such mark-
ers. The sentences with the form ?S(subject)
V(verb) O(object) C(complement)? may be
changed into the form ?S  O V C? by
inserting the particle ?? (Zhang and Sato,
1999). The usage of ?? emphasizes the
object by moving it before the verb. When
the particle ?? is in a sentence, it is easier
to identify the object. So the insertion of ??
will supply more information about syntactic
structure and reduce syntactic ambiguities.
Moreover, paraphrasing the sentences with
particle ?? may be more exact because the
identification of the object is more accurate.
We extracted the paraphrase pairs in which the
original sentences included the particle ??
and the corresponding paraphrases did not.
See example 3-3 below.
[3? 3]
Original:  /DT  /M  /NN  /VV 
/PN  /VV
(Could you fill out this form, please.)
Paraphrase:  /VV  /PN  /BA  /DT
 /M  /NN  /VV
(Could you make this form filled out, please.)
3.2 Automatic Generalization of
Instances
Then we attempted to generalize the extracted
instances in order to obtain paraphrasing pat-
terns. For each extracted paraphrase pair, the
original sentence is generalized to make the
matching part of the pattern, and the para-
phrase is generalized to make the generation
part of the pattern. The matching part spec-
ifies the components that will be paraphrased
as well as the context conditions. The genera-
tion part defines how to construct a paraphrase.
When the constituted pattern is applied to one
input sentence, if the input matches with the
matching part, a new sentence will be generated
according to the generation part.
In fact, the purpose of generalization is to
get a regular expression from the original sen-
tence and to get an operation expression con-
taining substitutions from the paraphrase. As
shown in 3-3, both the original sentence and the
paraphrase are series of morphemes, and each
morpheme consists of a part-of-speech and an
orthographic expression. The important thing
in paraphrasing is to maintain meaning. To
what extent the series of morphemes will be
generalized depends on each paraphrasing pair.
First, parts-of-speech keep the syntactic infor-
mation and therefore they should be kept. Sec-
ond, orthographic expressions of verbs, auxil-
iary verbs, adverbs, etc., are important in de-
ciding the main meaning of the sentence and
therefore they should also be kept. The ortho-
graphic expressions of other categories, such as
nouns, pronouns and numerals, can be general-
ized to an abstract level by replacing each or-
thographic expression with a wild card.
The pattern generalized from 3-3 is illus-
trated in 3-4. The left part is the matching part
and the right part is the generation part. The
lexical information may be an orthographic
expression or a variable represented by symbol
Xi. Xi in the matching part is in fact a
wild card, which means it can match with
any orthographic expression in the matching
operation. Xi in the generation part defines a
substitution operation.
[3? 4] A generalized pattern.
 /DT  /M X1/NN  /VV X2/PN  /VV
?  /VV X2/PN  /BA  /DT  /M X1/NN
 /VV
However, we found two problems in this kind
of automatic generalization. The first is that re-
strictions on the patterns generalized from long
sentences are too specific at the lexical level. In
fact, the clauses and noun phrases used as modi-
fiers have no effect on the considered paraphras-
ing phenomena and can be generalized further.
The second is that some orthographic expres-
sions with important meanings are generalized
to wild cards, for instance, the numeral ?	

(how many)? may imply that the sentence is
interrogative. Therefore, a method is needed
to prevent some orthographic expressions from
being automatically replaced with wild cards.
3.3 Semi-Automatic Generalization of
Instances
Specifying which morphemes should be general-
ized and which orthographic expressions should
be kept requires human experience. In order to
integrate human experience into automatic gen-
eralization, we developed a semi-automatic gen-
eralization tool. The tool consists of description
symbols and a transformation program. The
description symbols are designed for people to
define generalization information on instances,
and the transformation program automatically
transforms the defined instances into patterns.
Three description symbols are defined as fol-
lows.
[ ]: This symbol is followed by a numeral
and is used to enclose a sequence of mor-
phemes. The enclosed part is a syntac-
tic component, e.g., a noun phrase or a
clause. Except for the part-of-speech of the
last morpheme, the enclosed part will be
replaced with a variable. In the Chinese
language, the syntactic property of a se-
quence of words is most likely reflected in
the last word, so we keep the part-of-speech
of the last morpheme. The enclosed parts
in the original sentence and the paraphrase
denoted by the same numerals will be re-
placed with the same variables.
{ }: This symbol is used to enclose a mor-
pheme. The orthographic expression of the
morpheme will be kept. In this way, the
lexical information of morphemes can be
utilized to define the context. A few or-
thographic expressions can be defined in-
side one symbol so that words that can be
paraphrased in the same way can be stored
as one pattern.
? ?: This symbol is used to enclose a mor-
pheme. The orthographic expression of the
morpheme will be replaced with a variable.
In this way, the orthographic expressions of
verbs or adverbs can also be generalized.
The usage of the symbols is explained in 3-5 and
3-6. Example 3-5 is a paraphrase pair in which
description symbols are defined. Example 3-6 is
the paraphrasing pattern generalized from 3-5.
[3? 5] A defined instance.
Original:  /VV  /VV  /PN  /CD ?
/M? [ /NN 
 /DEG]1 [ /NN  /NN]2
(Could you give me two copies of the Japanese
pamphlet, please?)
Paraphrase: [ /NN 
 /DEG]1 [ 
	 ffProceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 73?83,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
SMT Helps Bitext Dependency Parsing
Wenliang Chen??, Jun?ichi Kazama?, Min Zhang?, Yoshimasa Tsuruoka??,
Yujie Zhang??, Yiou Wang?, Kentaro Torisawa? and Haizhou Li?
?Human Language Technology, Institute for Infocomm Research, Singapore
?National Institute of Information and Communications Technology (NICT), Japan
?School of Information Science, JAIST, Japan
?Beijing Jiaotong University, China
{wechen, mzhang, hli}@i2r.a-star.edu.sg
{kazama, torisawa, yujie, wangyiou}@nict.go.jp
tsuruoka@jaist.ac.jp
Abstract
We propose a method to improve the accuracy
of parsing bilingual texts (bitexts) with the
help of statistical machine translation (SMT)
systems. Previous bitext parsing methods use
human-annotated bilingual treebanks that are
hard to obtain. Instead, our approach uses an
auto-generated bilingual treebank to produce
bilingual constraints. However, because the
auto-generated bilingual treebank contains er-
rors, the bilingual constraints are noisy. To
overcome this problem, we use large-scale
unannotated data to verify the constraints and
design a set of effective bilingual features for
parsing models based on the verified results.
The experimental results show that our new
parsers significantly outperform state-of-the-
art baselines. Moreover, our approach is still
able to provide improvement when we use a
larger monolingual treebank that results in a
much stronger baseline. Especially notable
is that our approach can be used in a purely
monolingual setting with the help of SMT.
1 Introduction
Recently there have been several studies aiming to
improve the performance of parsing bilingual texts
(bitexts) (Smith and Smith, 2004; Burkett and Klein,
2008; Huang et al, 2009; Zhao et al, 2009; Chen
et al, 2010). In bitext parsing, we can use the in-
formation based on ?bilingual constraints? (Burkett
and Klein, 2008), which do not exist in monolingual
sentences. More accurate bitext parsing results can
be effectively used in the training of syntax-based
machine translation systems (Liu and Huang, 2010).
Most previous studies rely on bilingual treebanks
to provide bilingual constraints for bitext parsing.
Burkett and Klein (2008) proposed joint models on
bitexts to improve the performance on either or both
sides. Their method uses bilingual treebanks that
have human-annotated tree structures on both sides.
Huang et al (2009) presented a method to train a
source-language parser by using the reordering in-
formation on words between the sentences on two
sides. It uses another type of bilingual treebanks
that have tree structures on the source sentences and
their human-translated sentences. Chen et al (2010)
also used bilingual treebanks and made use of tree
structures on the target side. However, the bilingual
treebanks are hard to obtain, partly because of the
high cost of human translation. Thus, in their experi-
ments, they applied their methods to a small data set,
the manually translated portion of the Chinese Tree-
bank (CTB) which contains only about 3,000 sen-
tences. On the other hand, many large-scale mono-
lingual treebanks exist, such as the Penn English
Treebank (PTB) (Marcus et al, 1993) (about 40,000
sentences in Version 3) and the latest version of CTB
(over 50,000 sentences in Version 7).
In this paper, we propose a bitext parsing ap-
proach in which we produce the bilingual constraints
on existing monolingual treebanks with the help of
SMT systems. In other words, we aim to improve
source-language parsing with the help of automatic
translations.
In our approach, we first use an SMT system
to translate the sentences of a source monolingual
treebank into the target language. Then, the target
sentences are parsed by a parser trained on a tar-
get monolingual treebank. We then obtain a bilin-
gual treebank that has human annotated trees on the
source side and auto-generated trees on the target
side. Although the sentences and parse trees on the
73
target side are not perfect, we expect that we can
improve bitext parsing performance by using this
newly auto-generated bilingual treebank. We build
word alignment links automatically using a word
alignment tool. Then we can produce a set of bilin-
gual constraints between the two sides.
Because the translation, parsing, and word align-
ment are done automatically, the constraints are not
reliable. To overcome this problem, we verify the
constraints by using large-scale unannotated mono-
lingual sentences and bilingual sentence pairs. Fi-
nally, we design a set of bilingual features based on
the verified results for parsing models.
Our approach uses existing resources including
monolingual treebanks to train monolingual parsers
on both sides, bilingual unannotated data to train
SMT systems and to extract bilingual subtrees,
and target monolingual unannotated data to extract
monolingual subtrees. In summary, we make the fol-
lowing contributions:
? We propose an approach that uses an auto-
generated bilingual treebank rather than
human-annotated bilingual treebanks used in
previous studies (Burkett and Klein, 2008;
Huang et al, 2009; Chen et al, 2010). The
auto-generated bilingual treebank is built with
the help of SMT systems.
? We verify the unreliable constraints by using
the existing large-scale unannotated data and
design a set of effective bilingual features over
the verified results. Compared to Chen et al
(2010) that also used tree structures on the tar-
get side, our approach defines the features on
the auto-translated sentences and auto-parsed
trees, while theirs generates the features by
some rules on the human-translated sentences.
? Our parser significantly outperforms state-of-
the-art baseline systems on the standard test
data of CTB containing about 3,000 sentences.
Moreover, our approach continues to achieve
improvement when we build our system us-
ing the latest version of CTB (over 50,000 sen-
tences) that results in a much stronger baseline.
? We show the possibility that we can improve
the performance even if the test set has no hu-
man translation. This means that our proposed
approach can be used in a purely monolingual
setting with the help of SMT. To our knowl-
edge, this paper is the first one that demon-
strates this widened applicability, unlike the
previous studies that assumed that the parser is
applied only on the bitexts made by humans.
Throughout this paper, we use Chinese as the
source language and English as the target language.
The rest of this paper is organized as follows. Sec-
tion 2 introduces the motivation of this work. Sec-
tion 3 briefly introduces the parsing model used in
the experiments. Section 4 describes a set of bilin-
gual features based on the bilingual constraints and
Section 5 describes how to use large-scale unanno-
tated data to verify the bilingual constraints and de-
fine another set of bilingual features based on the
verified results. Section 6 explains the experimental
results. Finally, in Section 7 we draw conclusions.
2 Motivation
Here, bitext parsing is the task of parsing source sen-
tences with the help of their corresponding transla-
tions. Figure 1-(a) shows an example of the input
of bitext parsing, where ROOT is an artificial root
token inserted at the beginning and does not depend
on any other token in the sentence, the dashed undi-
rected links are word alignment links, and the di-
rected links between words indicate that they have
a dependency relation. Given such inputs, we build
dependency trees for the source sentences. Figure
1-(b) shows the output of bitext parsing for the ex-
ample in 1-(a).
ROOT!? ?? ?? ? ? ?? ?? ? ?? ??ta gaodu pingjia le yu lipeng zongli de huitan jieguo!! !!!
ROOT H hi hl d d h l f h f i h P Li!! e! g y!commen e !t e!resu ts!!o !!!t e!con erence!!!!w t !! eng
(a)
ROOT!? ?? ?? ? ? ?? ?? ? ?? ??ta gaodu pingjia le yu lipeng zongli de huitan jieguo!! !!(b)Figure 1: Input and output of our approach
In bitext parsing, some ambiguities exist on the
source side, but they may be unambiguous on the
74
target side. These differences are expected to help
improve source-side parsing.
Suppose we have a Chinese sentence shown in
Figure 2-(a). In this sentence, there is a nomi-
nalization case (Li and Thompson, 1997) in which
the particle ??(de)/nominalizer? is placed after the
verb compound ???(peiyu)??(qilai)/cultivate?
to modify ???(jiqiao)/skill?. This nominaliza-
tion is a relative clause, but does not have a clue
about its boundary. That is, it is very hard to deter-
mine which word is the head of ???(jiqiao)/skill?.
The head may be ???(fahui)/demonstrate? or ??
?(peiyu)/cultivate?, as shown in Figure 2-(b) and
-(c), where (b) is correct.
? ?? ?? ??? ?? ?? ?? ?? ?? ? ?? ? ??ta xiwang quanti yundongyuan chongfeng fahui pingshi peiyu qilai de!!liliang he!jiqiaoPN!!!!!!!VV!!!!!!!!!DT!!!!!!!!!!!!!!!NN!!!!!!!!!!!!!!!AD!!!!!!!!!!!!!!VV!!!!!!AD!!!!!!!VV!!!!VV DEC NN!!!!CC!!!NN(a)
? ?? ?? ??? ?? ?? ?? ?? ?? ? ?? ? ??ta xiwang quanti yundongyuan chongfeng fahui pingshi peiyu qilai de!!liliang he!jiqiao(b)
? ?? ?? ??? ?? ?? ?? ?? ?? ? ?? ? ??ta xiwang quanti yundongyuan chongfeng fahui pingshi peiyu qilai de!!liliang he!jiqiao(c)Figure 2: Example of an ambiguity on the Chinese side
In its English translation (Figure 3), word ?that? is
a clue indicating the relative clause which shows the
relation between ?skill? and ?cultivate?, as shown in
Figure 3. The figure shows that the translation can
provide useful bilingual constraints. From the de-
pendency tree on the target side, we find that the
word ?skill? corresponding to ???(jiqiao)/skill?
depends on the word ?demonstrate? corresponding
to ???(fahui)/demonstrate?, while the word ?cul-
tivate? corresponding to ???(peiyu)/cultivate? is a
grandchild of ?skill?. This is a positive evidence for
supporting ???(fahui)/demonstrate? as being the
head of ???(jiqiao)/skill?.
The above case uses the human translation on
the target side. However, there are few human-
annotated bilingual treebanks and the existing bilin-
gual treebanks are usually small. In contrast, there
are large-scale monolingual treebanks, e.g., the PTB
and the latest version of CTB. So we want to use
existing resources to generate a bilingual treebank
with the help of SMT systems. We hope to improve
source side parsing by using this newly built bilin-
gual treebank.
? ?? ?? ??? ?? ?? ?? ?? ?? ? ?? ? ??ta xiwang quanti yundongyuan chongfeng fahui pingshi peiyu qilai de!!liliang he!jiqiao
He!hoped!that!all!the!athletes!would!!fully!demonstrate!the!strength!and!skill!that!they!cultivate!daily
Figure 3: Example of human translation
? ?? ?? ??? ?? ?? ?? ?? ?? ? ?? ? ??ta xiwang quanti yundongyuan chongfeng fahui pingshi peiyu qilai de!!liliang he!jiqiao
he!expressed!the!hope!that!all!athletes!used!to!give!full!play!to!the!country!'s!strength!and!skills!
Figure 4: Example of Moses translation
Figure 4 shows an example of a translation us-
ing a Moses-based system, where the target sen-
tence is parsed by a monolingual target parser. The
translation contains some errors, but it does contain
some correct parts that can be used for disambigua-
tion. In the figure, the word ?skills? corresponding
to ???(jiqiao)/skill? is a grandchild of the word
?play? corresponding to ???(fahui)/demonstrate?.
This is a positive evidence for supporting ??
?(fahui)/demonstrate? as being the head of ??
?(jiqiao)/skill?.
From this example, although the sentences and
parse trees on the target side are not perfect, we
still can explore useful information to improve bitext
parsing. In this paper, we focus on how to design
a method to verify such unreliable bilingual con-
straints.
3 Parsing model
In this paper, we implement our approach based
on graph-based parsing models (McDonald and
Pereira, 2006; Carreras, 2007). Note that our ap-
proach can also be applied to transition-based pars-
ing models (Nivre, 2003; Yamada and Matsumoto,
2003).
The graph-based parsing model is to search for
the maximum spanning tree (MST) in a graph (Mc-
Donald and Pereira, 2006). The formulation defines
the score of a dependency tree to be the sum of edge
scores,
75
s(x, y) =
?
g?y
score(w, x, g) =
?
g?y
w ?f(x, g) (1)
where x is an input sentence, y is a dependency
tree for x, and g is a spanning subgraph of y. f(x, g)
can be based on arbitrary features of the subgraph
and the input sequence x and the feature weight
vector w are the parameters to be learned by using
MIRA (Crammer and Singer, 2003) during training.
In our approach, we use two types of features
for the parsing model. One is monolingual fea-
tures based on the source sentences. The mono-
lingual features include the first- and second- order
features presented in McDonald and Pereira (2006)
and the parent-child-grandchild features used in Car-
reras (2007). The other one is bilingual features (de-
scribed in Sections 4 and 5) that consider the bilin-
gual constraints.
We call the parser with the monolingual features
on the source side Parsers, and the parser with the
monolingual features on the target side Parsert.
4 Original bilingual features
In this paper, we generate two types of bilingual fea-
tures, original and verified bilingual features. The
original bilingual features (described in this section)
are based on the bilingual constraints without being
verified by large-scale unannotated data. And the
verified bilingual features (described in Section 5)
are based on the bilingual constraints verified by us-
ing large-scale unannotated data.
4.1 Auto-generated bilingual treebank
Assuming that we have monolingual treebanks on
the source side, an SMT system that can translate
the source sentences into the target language, and a
Parsert trained on the target monolingual treebank.
We first translate the sentences of the source
monolingual treebank into the target language using
the SMT system. Usually, SMT systems can output
the word alignment links directly. If they can not, we
perform word alignment using some publicly avail-
able tools, such as Giza++ (Och and Ney, 2003) or
Berkeley Aligner (Liang et al, 2006; DeNero and
Klein, 2007). The translated sentences are parsed by
the Parsert. Then, we have a newly auto-generated
bilingual treebank.?
4.2 Bilingual constraint functions
In this paper, we focus on the first- and second-
order graph models (McDonald and Pereira, 2006;
Carreras, 2007). Thus we produce the constraints
for bigram (a single edge) and trigram (adjacent
edges) dependencies in the graph model. For the tri-
gram dependencies, we consider the parent-sibling
and parent-child-grandchild structures described in
McDonald and Pereira (2006) and Carreras (2007).
We leave the third-order models (Koo and Collins,
2010) for a future study.
Suppose that we have a (candidate) dependency
relation rs that can be a bigram or trigram de-
pendency. We examine whether the corresponding
words of the source words of rs have a dependency
relation rt in the target trees. We also consider the
direction of the dependency relation. The corre-
sponding word of the head should also be the head
in rt. We define a binary function for this bilingual
constraint: Fbn(rsn : rtk), where n and k refers to
the types of the dependencies (2 for bigram and 3 for
trigram). For example, in rs2 : rt3, rs2 is a bigram
dependency on the source side and rt3 is a trigram
dependency on the target side.
4.2.1 Bigram constraint function: Fb2
For rs2, we consider two types of bilingual con-
straints. The first constraint function, denoted as
Fb2(rs2 : rt2), checks if the corresponding words
also have a direct dependency relation rt2. Figure
5 shows an example, where the source word ??
?(quanti)? depends on ????(yundongyuan)?
and word ?all? corresponding to ???(quanti)? de-
pends on word ?athletes? corresponding to ???
?(yundongyuan)?. In this case, Fb2(rs2 : rt2) =
+. However, when the source words are ??(ta)?
and ???(xiwang)?, this time their corresponding
words ?He? and ?hope? do not have a direct depen-
dency relation. In this case, Fb2(rs2 : rt2)=?.
The second constraint function, denoted as
Fb2(rs2 : rt3), checks if the corresponding words
form a parent-child-grandchild relation that often
occurs in translation (Koehn et al, 2003). Figure 6
shows an example. The source word ???(jiqiao)?
depends on ???(fahui)? while its corresponding
word ?skills? indirectly depends on ?play? which
corresponds to ???(fahui)? via ?to?. In this case,
Fb2(rs2 : rt3)=+.
76
? ?? ?? ??? ?? ?? ?? ?? ?? ? ?? ? ??ta xiwang quanti yundongyuan chongfeng fahui pingshi peiyu qilai de!!liliang he!jiqiao
he!expressed!the!hope!that!all!athletes!used!to!give!full!play!to!the!country!'s!strength!and!skills!
Figure 5: Example of bilingual constraints (2to2)
? ?? ?? ??? ?? ?? ?? ?? ?? ? ?? ? ??ta xiwang quanti yundongyuan chongfeng fahui pingshi peiyu qilai de!!liliang he!jiqiao
he!expressed!the!hope!that!all!athletes!used!to!give!full!play!to!the!country!'s!strength!and!skills!
Figure 6: Example of bilingual constraints (2to3)
4.2.2 Trigram constraint function: Fb3
For a second-order relation on the source side,
we consider one type of constraint. We have three
source words that form a second-order relation and
all of them have the corresponding words. We
define function Fb3(rs3 : rt3) for this constraint.
The function checks if the corresponding words
form a trigram dependencies structure. An exam-
ple is shown in Figure 7. The source words ??
?(liliang)?, ??(he)?, and ???(jiqiao)? form a
parent-sibling structure, while their corresponding
words ?strength?, ?and?, and ?skills? also form a
parent-sibling structure on the target side. In this
case, function Fb3(rs3 : rt3)=+.
? ?? ?? ??? ?? ?? ?? ?? ?? ? ?? ? ??ta xiwang quanti yundongyuan chongfeng fahui pingshi peiyu qilai de!!liliang he!jiqiao
he!expressed!the!hope!that!all!athletes!used!to!give!full!play!to!the!country!'s!strength!and!skills!
Figure 7: Example of bilingual constraints (3to3)
4.3 Bilingual reordering function: Fro
Huang et al (2009) proposed features based on
reordering between languages for a shift-reduce
parser. They define the features based on word-
alignment information to verify whether the corre-
sponding words form a contiguous span to resolve
shift-reduce conflicts. We also implement similar
features in our system. For example, in Figure 1-
(a) the source span is [??(huitan), ??(jieguo)],
which maps onto [results, conference]. Because no
word within this target span is aligned to a source
word outside of the source span, this span is a con-
tiguous span. In this case, function Fro =+, other-
wise Fro=?.
4.4 Original bilingual features
We define original bilingual features based on the
bilingual constraint functions and the bilingual re-
ordering function.
Table 1 lists the original features, where Dir
refers to the directions1 of the source-side dependen-
cies, Fb2 can be Fb2(rs2 : rt2) and Fb2(rs2 : rt3),
and Fb3 is Fb3(rs3 : rt3). Each line of the table
defines a feature template that is a combination of
functions.
First-order features Second-order features
?Fro?
?Fb2, Dir? ?Fb3, Dir?
?Fb2, Dir, Fro? ?Fb3, Dir, Fro?
Table 1: Original bilingual features
We use an example to show how to generate the
original bilingual features in practice. In Figure 4,
we want to define the bilingual features for the bi-
gram dependency (rs2) between ???(fahui)? and
???(jiqiao)?. The corresponding words form a tri-
gram relation rt3 in the target dependency tree. The
direction of the bigram dependency is right. Then
we have feature ??Fb2(rs2 : rt3)=+, RIGHT ?? for
the second first-order feature template in Table 1.
5 Verified bilingual features
However, because the bilingual treebank is gener-
ated automatically, using the bilingual constraints
alone is not reliable. Therefore, in this section we
verify the constraints by using large-scale unanno-
tated data to overcome this problem. More specifi-
cally, rtk of the constraint is verified by checking a
list of target monolingual subtrees and rsn : rtk is
verified by checking a list of bilingual subtrees. The
subtrees are extracted from the large-scale unanno-
tated data. The basic idea is as follows: if the de-
pendency structures of a bilingual constraint can be
found in the list of the target monolingual subtrees
1For the second order features, Dir is the combination of
the directions of two dependencies.
77
or bilingual subtrees, this constraint will probably be
reliable.
We first parse the large-scale unannotated mono-
lingual and bilingual data. Subsequently, we ex-
tract the monolingual and bilingual subtrees from
the parsed data. We then verify the bilingual con-
straints using the extracted subtrees. Finally, we
generate the bilingual features based on the verified
results for the parsing models.
5.1 Verified constraint functions
5.1.1 Monolingual target subtrees
Chen et al (2009) proposed a simple method to
extract subtrees from large-scale monolingual data
and used them as features to improve monolingual
parsing. Following their method, we parse large
unannotated data with the Parsert and obtain the sub-
tree list (STt) on the target side. We extract two
types of subtrees: bigram (two words) subtree and
trigram (three words) subtree.
H b ht b h b k
ROOT!!He!!!!!bought!!!!!a!!!!book
e!!!!! oug oug t!! oo !
a book b ht b k!!!!!
(a) (b)
oug !!!a!!!!! oo !
Figure 8: Example of monolingual subtree extraction
From the dependency tree in Figure 8-(a), we ob-
tain the subtrees shown in Figure 8-(b) where the
first three are bigram subtrees and the last one is
a trigram subtree. After extraction, we obtain the
subtree list STt that includes two sets, one for bi-
gram subtrees, and the other one for trigram sub-
trees. We remove the subtrees occurring only once
in the data. For each set, we assign labels to the
extracted subtrees according to their frequencies by
using the same method as that of Chen et al (2009).
If the frequency of a subtree is in the top 10% in the
corresponding set, it is labeled HF. If the frequency
is between the top 20% and 30%, it is labeled MF.
We assign the label LF to the remaining subtrees.
We use Type(stt) to refer to the label of a subtree,
stt.
5.1.2 Verified target constraint function:
Fvt(rtk)
We use the extracted target subtrees to verify the
rtk of the bilingual constraints. In fact, rtk is a can-
didate subtree. If the rtk is included in STt, func-
tion Fvt(rtk) = Type(rtk), otherwise Fvt(rtk) =
ZERO. For example, in Figure 5 the bigram struc-
ture of ?all? and ?athletes? can form a bigram sub-
tree that is included STt and its label is HF. In this
case, Fvt(rt2)= HF .
5.1.3 Bilingual subtrees
We extract bilingual subtrees from a bilingual
corpus, which is parsed by the Parsers and Parsert
on both sides. We extract three types of bilingual
subtrees: bigram-bigram (stbi22), bigram-trigram
(stbi23), and trigram-trigram (stbi33) subtrees. For
example, stbi22 consists of a bigram subtree on the
source side and a bigram subtree on the target side.
? ? ? ?? ? ? ? ??ROOT! ?ta shi yi ming xuesheng
ROOT!!He!!!!!is!!!!!a!!!!!student He!!!!!is is!!!!!student
(a) (b)
Figure 9: Example of bilingual subtree extraction
From the dependency tree in Figure 9-(a), we
obtain the bilingual subtrees shown in Figure 9-
(b). Figure 9-(b) shows the extracted bigram-bigram
bilingual subtrees. After extraction, we obtain the
bilingual subtrees STbi. We remove the subtrees oc-
curring only once in the data.
5.1.4 Verified bilingual constraint function:
Fvb(rbink)
We use the extracted bilingual subtrees to verify
the rsn : rtk (rbink in short) of the bilingual con-
straints. rsn and rtk form a candidate bilingual sub-
tree stbink. If the stbink is included in STbi, function
Fvb(rbink)=+, otherwise Fvb(rbink)=?.
5.2 Verified bilingual features
Then, we define another set of bilingual features by
combining the verified constraint functions. We call
these bilingual features ?verified bilingual features?.
78
Table 2 lists the verified bilingual features used in
our experiments, where each line defines a feature
template that is a combination of functions.
We use an example to show how to generate the
verified bilingual features in practice. In Figure 4,
we want to define the verified features for the bi-
gram dependency (rs2) between ???(fahui)? and
???(jiqiao)?. The corresponding words form a
trigram relation rt3. The direction of the bigram
dependency is right. Suppose we can find rt3 in
STt with label MF and can not find the candidate
bilingual subtree in STbi. Then we have feature
??Fb2(rs2 : rt3) = +, Fvt(rt3) = MF,RIGHT ??
for the third first-order feature template and feature
??Fb2(rs2 : rt3)=+, Fvb(rbi23)=?, RIGHT ?? for
the fifth in Table 2.
First-order features Second-order features
?Fro?
?Fb2, Fvt(rtk)? ?Fb3, Fvt(rtk)?
?Fb2, Fvt(rtk), Dir? ?Fb3, Fvt(rtk), Dir?
?Fb2, Fvb(rbink)? ?Fb3, Fvb(rbink)?
?Fb2, Fvb(rbink), Dir? ?Fb3, Fvb(rbink), Dir?
?Fb2, Fro, Fvb(rbink)?
Table 2: Verified bilingual features
6 Experiments
We evaluated the proposed method on the translated
portion of the Chinese Treebank V2 (referred to as
CTB2tp) (Bies et al, 2007), articles 1-325 of CTB,
which have English translations with gold-standard
parse trees. The tool ?Penn2Malt?2 was used to con-
vert the data into dependency structures. Following
the studies of Burkett and Klein (2008), Huang et
al. (2009) and Chen et al (2010), we used the ex-
act same data split: 1-270 for training, 301-325 for
development, and 271-300 for testing. Note that we
did not use human translation on the English side
of this bilingual treebank to train our new parsers.
For testing, we used two settings: a test with hu-
man translation and another with auto-translation.
To process unannotated data, we trained a first-order
Parsers on the training data.
To prove that the proposed method can work on
larger monolingual treebanks, we also tested our
2http://w3.msi.vxu.se/?nivre/research/Penn2Malt.html
methods on the CTB7 (LDC2010T07) that includes
much more sentences than CTB2tp. We used arti-
cles 301-325 for development, 271-300 for testing,
and the other articles for training. That is, we eval-
uated the systems on the same test data as CTB2tp.
Table 3 shows the statistical information on the data
sets.
Train Dev Test
CTB2tp 2,745 273 290
CTB7 50,747 273 290
Table 3: Number of sentences of data sets used
We built Chinese-to-English SMT systems based
on Moses3. Minimum error rate training (MERT)
with respect to BLEU score was used to tune the de-
coder?s parameters. The translation model was cre-
ated from the FBIS corpus (LDC2003E14). We used
SRILM4 to train a 5-gram language model. The lan-
guage model was trained on the target side of the
FBIS corpus and the Xinhua news in English Gi-
gaword corpus (LDC2009T13). The development
and test sets were from NIST MT08 evaluation cam-
paign5. We then used the SMT systems to translate
the training data of CTB2tp and CTB7.
To directly compare with the results of Huang
et al (2009) and Chen et al (2010), we also used
the same word alignment tool, Berkeley Aligner
(Liang et al, 2006; DeNero and Klein, 2007), to
perform word alignment for CTB2tp and CTB7.
We trained a Berkeley Aligner on the FBIS corpus
(LDC2003E14). We removed notoriously bad links
in {a, an, the}?{?(de),?(le)} following the work
of Huang et al (2009).
To train an English parser, we used the PTB
(Marcus et al, 1993) in our experiments and the
tool ?Penn2Malt? to convert the data. We split the
data into a training set (sections 2-21), a develop-
ment set (section 22), and a test set (section 23).
We trained first-order and second-order Parsert on
the training data. The unlabeled attachment score
(UAS) of the second-order Parsert was 91.92, in-
dicating state-of-the-art accuracy on the test data.
We used the second-order Parsert to parse the auto-
translated/human-made target sentences in the CTB
3http://www.statmt.org/moses/
4http://www.speech.sri.com/projects/srilm/download.html
5http://www.itl.nist.gov/iad/mig//tests/mt/2008/
79
data.
To extract English subtrees, we used the BLLIP
corpus (Charniak et al, 2000) that contains about
43 million words of WSJ texts. We used the MX-
POST tagger (Ratnaparkhi, 1996) trained on train-
ing data to assign POS tags and used the first-order
Parsert to process the sentences of the BLLIP cor-
pus. To extract bilingual subtrees, we used the FBIS
corpus and an additional bilingual corpus contain-
ing 800,000 sentence pairs from the training data of
NIST MT08 evaluation campaign. On the Chinese
side, we used the morphological analyzer described
in (Kruengkrai et al, 2009) trained on the training
data of CTBtp to perform word segmentation and
POS tagging and used the first-order Parsers to parse
all the sentences in the data. On the English side, we
used the same procedure as we did for the BLLIP
corpus. Word alignment was performed using the
Berkeley Aligner.
We reported the parser quality by the UAS, i.e.,
the percentage of tokens (excluding all punctuation
tokens) with correct HEADs.
6.1 Experimental settings
For baseline systems, we used the monolingual fea-
tures mentioned in Section 3. We called these fea-
tures basic features. To compare the results of (Bur-
kett and Klein, 2008; Huang et al, 2009; Chen et
al., 2010), we used the test data with human trans-
lation in the following three experiments. The tar-
get sentences were parsed by using the second-order
Parsert. We used PAG to refer to our parsers trained
on the auto-generated bilingual treebank.
6.2 Training with CTB2tp
Order-1 Order-2
Baseline 84.35 87.20
PAGo 84.71(+0.36) 87.85(+0.65)
PAG 85.37(+1.02) 88.49(+1.29)
ORACLE 85.79(+1.44) 88.87(+1.67)
Table 4: Results of training with CTB2tp
First, we conducted the experiments on the stan-
dard data set of CTB2tp, which was also used in
other studies (Burkett and Klein, 2008; Huang et al,
2009; Chen et al, 2010). The results are given in
Table 4, where Baseline refers to the system with
the basic features, PAGo refers to that after adding
the original bilingual features of Table 1 to Baseline,
PAG refers to that after adding the verified bilingual
features of Table 2 to Baseline, and ORACLE6 refers
to using human-translation for training data with
adding the features of Table 1. We obtained an ab-
solute improvement of 1.02 points for the first-order
model and 1.29 points for the second-order model by
adding the verified bilingual features. The improve-
ments of the final systems (PAG) over the Baselines
were significant in McNemar?s Test (p < 0.001 for
the first-order model and p < 0.0001 for the second-
order model). If we used the original bilingual fea-
tures (PAGo), the system dropped 0.66 points for the
first-order and 0.64 points for the second-order com-
pared with system PAG. This indicated that the ver-
ified bilingual constraints did provide useful infor-
mation for the parsing models.
We also found that PAG was about 0.3 points
lower than ORACLE. The reason is mainly due
to the imperfect translations, although we used
the large-scale subtree lists to help verify the con-
straints. We tried adding the features of Table 2 to
the ORACLE system, but the results were worse.
These facts indicated that our approach obtained the
benefits from the verified constraints, while using
the bilingual constraints alone was enough for OR-
ACLE.
6.3 Training with CTB7
 0.83
 0.84
 0.85
 0.86
 0.87
 0.88
 0.89
 0.9
 0.91
 0.92
 5  10  20  30  40  50
U
A
S
Amount of training data (K)
Baseline1PAG1Baseline2PAG2
Figure 10: Results of using different sizes of training data
Here, we demonstrate that our approach is still
able to provide improvement, even if we use larger
6Note that we also used the tool to perform the word align-
ment automatically.
80
Baseline D10 D20 D50 D100 GTran
BLEU n/a 14.71 15.84 16.92 17.95 n/a
UAS 87.20 87.63 87.67 88.20 88.49 88.58
Table 5: Results of using different translations
training data that result in strong baseline systems.
We incrementally increased the training sentences
from the CTB7. Figure 10 shows the results of us-
ing different sizes of CTB7 training data, where the
numbers of the x-axis refer to the sentence numbers
of training data used, Baseline1 and Baseline2 re-
fer to the first- and second-order baseline systems,
and PAG1 and PAG2 refer to our first- and second-
order systems. The figure indicated that our sys-
tem always outperformed the baseline systems. For
small data sizes, our system performed much better
than the baselines. For example, when using 5,000
sentences, our second-order system provided a 1.26
points improvement over the second-order baseline.
Finally, when we used all of the CTB7 training
data, our system achieved 91.66 for the second-order
model, while the baseline achieved 91.10.
6.4 With different settings of SMT systems
We investigated the effects of different settings of
SMT systems. We randomly selected 10%, 20%,
and 50% of FBIS to train the Moses systems and
used them to translate CTB2tp. The results are in
Table 5, where D10, D20, D50, and D100 refer to
the system with 10%, 20%, 50%, and 100% data re-
spectively. For reference, we also used the Google-
translate online system7, indicated as GTran in the
table, to translate the CTB2tp.
From the table, we found that our system outper-
formed the Baseline even if we used only 10% of the
FBIS corpus. The BLEU and UAS scores became
higher, when we used more data of the FBIS corpus.
And the gaps among the results of D50, D100, and
GTran were small. This indicated that our approach
was very robust to the noise produced by the SMT
systems.
6.5 Testing with auto-translation
We also translated the test data into English using
the Moses system and tested the parsers on the new
7http://translate.google.com/
test data. Table 6 shows the results. The results
showed that PAG outperformed the baseline systems
for both the first- and second-order models. This
indicated that our approach can provide improve-
ment in a purely monolingual setting with the help
of SMT.
Order-1 Order-2
Baseline 84.35 87.20
PAG 84.88(+0.53) 87.89(+0.69)
Table 6: Results of testing with auto-translation (training
with CTB2tp)
6.6 Comparison results
With CTB2tp With CTB7
Type System UAS System UAS
M Baseline 87.20 Baseline 91.10
HA
Huang2009 86.3 n/a
Chen2010BI 88.56
Chen2010ALL 90.13
AG PAG 88.49 PAG 91.66PAG+STs 89.75
Table 7: Comparison of our results with other pre-
vious reported systems. Type M denotes training on
monolingual treebank. Types HA and AG denote training
on human-annotated and auto-generated bilingual tree-
banks respectively.
We compared our results with the results reported
previously for the same data. Table 7 lists the re-
sults, where Huang2009 refers to the result of Huang
et al (2009), Chen2010BI refers to the result of
using bilingual features in Chen et al (2010), and
Chen2010ALL refers to the result of using all of
the features in Chen et al (2010). The results
showed that our new parser achieved better accuracy
than Huang2009 and comparable to Chen2010BI .
To achieve higher performance, we also added the
source subtree features (Chen et al, 2009) to our
system: PAG+STs. The new result is close to
Chen2010ALL. Compared with the approaches of
81
Huang et al (2009) and Chen et al (2010), our
approach used an auto-generated bilingual treebank
while theirs used a human-annotated bilingual tree-
bank. By using all of the training data of CTB7, we
obtained a more powerful baseline that performed
much better than the previous reported results. Our
parser achieved 91.66, much higher accuracy than
the others.
7 Conclusion
We have presented a simple yet effective approach
to improve bitext parsing with the help of SMT sys-
tems. Although we trained our parser on an auto-
generated bilingual treebank, we achieved an accu-
racy comparable to the systems trained on human-
annotated bilingual treebanks on the standard test
data. Moreover, our approach continued to pro-
vide improvement over the baseline systems when
we used a much larger monolingual treebank (over
50,000 sentences) where target human translations
are not available and very hard to construct. We also
demonstrated that the proposed approach can be ef-
fective in a purely monolingual setting with the help
of SMT.
Acknowledgments
This study was started when Wenliang Chen, Yu-
jie Zhang, and Yoshimasa Tsuruoka were members
of Language Infrastructure Group, National Insti-
tute of Information and Communications Technol-
ogy (NICT), Japan. We would also thank the anony-
mous reviewers for their detailed comments, which
have helped us to improve the quality of this work.
References
Ann Bies, Martha Palmer, Justin Mott, and Colin Warner.
2007. English Chinese Translation Treebank V 1.0,
LDC2007T02. Linguistic Data Consortium.
David Burkett and Dan Klein. 2008. Two languages are
better than one (for syntactic parsing). In Proceedings
of EMNLP 2008, pages 877?886, Honolulu, Hawaii,
October. Association for Computational Linguistics.
Xavier Carreras. 2007. Experiments with a higher-order
projective dependency parser. In Proceedings of the
CoNLL Shared Task Session of EMNLP-CoNLL 2007,
pages 957?961, Prague, Czech Republic, June. Asso-
ciation for Computational Linguistics.
Eugene Charniak, Don Blaheta, Niyu Ge, Keith Hall,
John Hale, and Mark Johnson. 2000. BLLIP 1987-
89 WSJ Corpus Release 1, LDC2000T43. Linguistic
Data Consortium.
Wenliang Chen, Jun?ichi Kazama, Kiyotaka Uchimoto,
and Kentaro Torisawa. 2009. Improving dependency
parsing with subtrees from auto-parsed data. In Pro-
ceedings of EMNLP 2009, pages 570?579, Singapore,
August.
Wenliang Chen, Jun?ichi Kazama, and Kentaro Torisawa.
2010. Bitext dependency parsing with bilingual sub-
tree constraints. In Proceedings of ACL 2010, pages
21?29, Uppsala, Sweden, July. Association for Com-
putational Linguistics.
Koby Crammer and Yoram Singer. 2003. Ultraconser-
vative online algorithms for multiclass problems. J.
Mach. Learn. Res., 3:951?991.
John DeNero and Dan Klein. 2007. Tailoring word
alignments to syntactic machine translation. In Pro-
ceedings of ACL 2007, pages 17?24, Prague, Czech
Republic, June. Association for Computational Lin-
guistics.
Liang Huang, Wenbin Jiang, and Qun Liu. 2009.
Bilingually-constrained (monolingual) shift-reduce
parsing. In Proceedings of EMNLP 2009, pages 1222?
1231, Singapore, August. Association for Computa-
tional Linguistics.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proceedings of
NAACL 2003, pages 48?54. Association for Computa-
tional Linguistics.
Terry Koo and Michael Collins. 2010. Efficient third-
order dependency parsers. In Proceedings of ACL
2010, pages 1?11, Uppsala, Sweden, July. Association
for Computational Linguistics.
Canasai Kruengkrai, Kiyotaka Uchimoto, Jun?ichi
Kazama, Yiou Wang, Kentaro Torisawa, and Hitoshi
Isahara. 2009. An error-driven word-character hybrid
model for joint Chinese word segmentation and POS
tagging. In Proceedings of ACL-IJCNLP2009, pages
513?521, Suntec, Singapore, August. Association for
Computational Linguistics.
Charles N. Li and Sandra A. Thompson. 1997. Man-
darin Chinese - A Functional Reference Grammar.
University of California Press.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proceedings of NAACL 2006,
pages 104?111, New York City, USA, June. Associa-
tion for Computational Linguistics.
Yang Liu and Liang Huang. 2010. Tree-based and forest-
based translation. In Tutorial Abstracts of ACL 2010,
page 2, Uppsala, Sweden, July. Association for Com-
putational Linguistics.
82
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: the Penn Treebank. Computational
Linguisticss, 19(2):313?330.
Ryan McDonald and Fernando Pereira. 2006. On-
line learning of approximate dependency parsing algo-
rithms. In Proceedings of EACL 2006, pages 81?88.
Joakim Nivre. 2003. An efficient algorithm for
projective dependency parsing. In Proceedings of
IWPT2003, pages 149?160.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Adwait Ratnaparkhi. 1996. A maximum entropy model
for part-of-speech tagging. In Proceedings of EMNLP
1996, pages 133?142.
David A. Smith and Noah A. Smith. 2004. Bilingual
parsing with factored estimation: Using English to
parse Korean. In Proceedings of EMNLP 2004, pages
49?56.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statistical
dependency analysis with support vector machines. In
Proceedings of IWPT 2003, pages 195?206.
Hai Zhao, Yan Song, Chunyu Kit, and Guodong Zhou.
2009. Cross language dependency parsing us-
ing a bilingual lexicon. In Proceedings of ACL-
IJCNLP2009, pages 55?63, Suntec, Singapore, Au-
gust. Association for Computational Linguistics.
83
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 155?160,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Dependency-based Pre-ordering for Chinese-English Machine Translation
Jingsheng Cai
??
Masao Utiyama
?
Eiichiro Sumita
?
Yujie Zhang
?
?
School of Computer and Information Technology, Beijing Jiaotong University
?
National Institute of Information and Communications Technology
joycetsai99@gmail.com
{mutiyama, eiichiro.sumita}@nict.go.jp
yjzhang@bjtu.edu.cn
Abstract
In statistical machine translation (SMT),
syntax-based pre-ordering of the source
language is an effective method for deal-
ing with language pairs where there are
great differences in their respective word
orders. This paper introduces a novel
pre-ordering approach based on depen-
dency parsing for Chinese-English SMT.
We present a set of dependency-based pre-
ordering rules which improved the BLEU
score by 1.61 on the NIST 2006 evalua-
tion data. We also investigate the accuracy
of the rule set by conducting human eval-
uations.
1 Introduction
SMT systems have difficulties translating between
distant language pairs such as Chinese and En-
glish. The reason for this is that there are great
differences in their word orders. Reordering there-
fore becomes a key issue in SMT systems between
distant language pairs.
Previous work has shown that the approaches
tackling the problem by introducing a pre-ordering
procedure into phrase-based SMT (PBSMT) were
effective. These pre-ordering approaches first
parse the source language sentences to create parse
trees. Then, syntactic reordering rules are ap-
plied to these parse trees with the goal of re-
ordering the source language sentences into the
word order of the target language. Syntax-based
pre-ordering by employing constituent parsing
have demonstrated effectiveness in many language
pairs, such as English-French (Xia and McCord,
2004), German-English (Collins et al, 2005),
Chinese-English (Wang et al, 2007; Zhang et al,
2008), and English-Japanese (Lee et al, 2010).
?
This work was done when the first author was on an
internship in NICT.
As a kind of constituent structure, HPSG (Pol-
lard and Sag, 1994) parsing-based pre-ordering
showed improvements in SVO-SOV translations,
such as English-Japanese (Isozaki et al, 2010; Wu
et al, 2011) and Chinese-Japanese (Han et al,
2012). Since dependency parsing is more concise
than constituent parsing in describing sentences,
some research has used dependency parsing in
pre-ordering approaches for language pairs such
as Arabic-English (Habash, 2007), and English-
SOV languages (Xu et al, 2009; Katz-Brown et
al., 2011). The pre-ordering rules can be made
manually (Collins et al, 2005; Wang et al, 2007;
Han et al, 2012) or extracted automatically from
a parallel corpus (Xia and McCord, 2004; Habash,
2007; Zhang et al, 2007; Wu et al, 2011).
The purpose of this paper is to introduce a novel
dependency-based pre-ordering approach through
creating a pre-ordering rule set and applying it to
the Chinese-English PBSMT system. Experiment
results showed that our pre-ordering rule set im-
proved the BLEU score on the NIST 2006 evalua-
tion data by 1.61. Moreover, this rule set substan-
tially decreased the total times of rule application
about 60%, compared with a constituent-based ap-
proach (Wang et al, 2007). We also conducted hu-
man evaluations in order to assess its accuracy. To
our knowledge, our manually created pre-ordering
rule set is the first Chinese-English dependency-
based pre-ordering rule set.
The most similar work to this paper is that of
Wang et al (2007). They created a set of pre-
ordering rules for constituent parsers for Chinese-
English PBSMT. In contrast, we propose a set of
pre-ordering rules for dependency parsers. We
argue that even though the rules by Wang et al
(2007) exist, it is almost impossible to automati-
cally convert their rules into rules that are appli-
cable to dependency parsers. In fact, we aban-
doned our initial attempts to automatically convert
their rules into rules for dependency parsers, and
155
(a) A constituent parse tree
(b) Stanford typed dependency parse tree
Figure 1: A constituent parse tree and its cor-
responding Stanford typed dependency parse tree
for the same Chinese sentence.
spent more than two months discovering the rules
introduced in this paper. By applying our rules
and Wang et al?s rules, one can use both depen-
dency and constituency parsers for pre-ordering in
Chinese-English PBSMT.
This is especially important on the point of the
system combination of PBSMT systems, because
the diversity of outputs from machine translation
systems is important for system combination (Cer
et al, 2013). By using both our rules and Wang et
al.?s rules, one can obtain diverse machine trans-
lation results because the pre-ordering results of
these two rule sets are generally different.
Another similar work is that of (Xu et al, 2009).
They created a pre-ordering rule set for depen-
dency parsers from English to several SOV lan-
guages. In contrast, our rule set is for Chinese-
English PBSMT. That is, the direction of transla-
tion is opposite. Because there are a lot of lan-
guage specific decisions that reflect specific as-
pects of the source language and the language pair
combination, our rule set provides a valuable re-
source for pre-ordering in Chinese-English PB-
SMT.
2 Dependency-based Pre-ordering Rule
Set
Figure 1 shows a constituent parse tree and its
Stanford typed dependency parse tree for the same
Figure 2: An example of a preposition phrase with
a plmod structure. The phrase translates into ?in
front of the US embassy?.
Chinese sentence. As shown in the figure, the
number of nodes in the dependency parse tree
(i.e. 9) is much fewer than that in its correspond-
ing constituent parse tree (i.e. 17). Because de-
pendency parse trees are generally more concise
than the constituent ones, they can conduct long-
distance reorderings in a finer way. Thus, we at-
tempted to conduct pre-ordering based on depen-
dency parsing. There are two widely-used de-
pendency systems ? Stanford typed dependencies
and CoNLL typed dependencies. For Chinese,
there are 45 types of grammatical relations for
Stanford typed dependencies (Chang et al, 2009)
and 25 for CoNLL typed dependencies. As we
thought that Stanford typed dependencies could
describe language phenomena more meticulously
owing to more types of grammatical relations, we
preferred to use it for searching candidate pre-
ordering rules.
We designed two types of formats in our
dependency-based pre-ordering rules. They are:
Type-1: x : y
Type-2: x - y
Here, both x and y are dependency relations
(e.g., plmod or lobj in Figure 2). We define the
dependency structure of a dependency relation as
the structure containing the dependent word (e.g.,
the word directly indicated by plmod, or ??? in
Figure 2) and the whole subtree under the depen-
dency relation (all of the words that directly or
indirectly depend on the dependent word, or the
words under ??? in Figure 2). Further, we define
X and Y as the corresponding dependency struc-
tures of the dependency relations x and y, respec-
tively. We define X\Y as structure X except Y. For
example, in Figure 2, let x and y denote plmod and
lobj dependency relations, then X represents ???
and all words under ???, Y represents ?????
and all words under ?????, and X\Y represents
156
Figure 3: An example of rcmod structure within
an nsubj structure. The phrase translates into ?a
senior official close to Sharon said?.
???. For Type-1, Y is a sub-structure of X. The
rule repositions X\Y to the position before Y. For
Type-2, X and Y are ordered sibling structures un-
der a same parent node. The rule repositions X to
the position after Y.
We obtained rules as the following steps:
1 Search the Chinese dependency parse trees
in the corpus and rank all of the structures
matching the two types of rules respectively
according to their frequencies. Note that
while calculating the frequencies of Type-
1 structures, we dismissed the structures in
which X occurred before Y originally.
2 Filtration. 1) Filter out the structures which
occurred less than 5,000 times. 2) Filter
out the structures from which it was almost
impossible to derive candidate pre-ordering
rules because x or y was an ?irrespective? de-
pendency relation, for example, root, conj, cc
and so on.
3 Investigate the remaining structures. For each
kind of structure, we selected some of the
sample dependency parse trees that contained
it, tried to restructure the parse trees accord-
ing to the matched rule and judged the re-
ordered Chinese phrases. If the reordering
produced a Chinese phrase that had a closer
word order to that of the English one, this
structure would be a candidate pre-ordering
rule.
4 Conduct primary experiments which used the
same training set and development set as the
experiments described in Section 3. In the
primary experiments, we tested the effective-
ness of the candidate rules and filtered the
ones that did not work based on the BLEU
scores on the development set.
Figure 4: An example of rcmod structure with a
preposition modifier. The phrase translates into ?a
press conference held in Kabul?.
As a result, we obtained eight pre-ordering rules
in total, which can be divided into three depen-
dency relation categories. They are: plmod (lo-
calizer modifier of a preposition), rcmod (relative
clause modifier) and prep (preposition modifer).
Each of these categories are discussed in detail be-
low.
plmod Figure 2 shows an example of a preposi-
tional phrase with a plmod structure, which trans-
lates literally into ?in the US embassy front?. In
Chinese, the dependent word of a plmod relation
(e.g., ??? in Figure 2) occurs in the last position
of the prepositional phrase. However, in English,
this kind of word (e.g., ?front? in the caption of
Figure 2) always occur directly after prepositions,
which is to say, in the second position in a preposi-
tional phrase. Therefore, we applied a rule plmod
: lobj (localizer object) to reposition the depen-
dent word of the plmod relation (e.g., ??? in Fig-
ure 2) to the position before the lobj structure (e.g.,
??? ???? in Figure 2). In this case, it also
comes directly after the preposition. Similarly, we
created a rule plmod : lccomp (clausal comple-
ment of a localizer).
rcmod Figure 3 shows an example of an rcmod
structure under an nsubj (nominal subject) struc-
ture. Here ?mw? means ?measure word?. As
shown in the figure, relative clause modifiers in
Chinese (e.g., ??? ?? ?? in Figure 3) oc-
curs before the noun being modified, which is in
contrast to English (e.g., ?close to Sharon? in the
caption of Figure 3), where they come after. Thus,
we introduced a series of rules NOUN : rcmod
to restructure rcmod structures so that the noun
is moved to the head. In this example, with the
application of an nsubj : rcmod rule, the phrase
can be translated into ?a senior official close to
Sharon say?, which has a word order very close
to English. Since a noun can be nsubj, dobj (di-
rect object), pobj (prepositional object) and lobj
157
Type System Parser BLEU Counts #Sent.
- No pre-ordering - 29.96 - -
Constituent WR07 Berkeley 31.45 2,561,937 852,052
Dependency OUR DEP 1 Berkeley Const. 31.54 978,013 556,752
OUR DEP 2 Mate 31.57 947,441 547,084
Table 1: The comparison of four systems, including the performance (BLEU) on the test set, the total
count of each rule set and the number of sentences they were applied to on the training set.
Figure 5: An example of verb phrase with a
preposition modifier. The phrase translates into
?Musharraf told reporters here?.
in Stanford typed dependencies, we created four
rules from the NOUN pattern. Note that for some
preposition modifiers, we needed a rule rcmod :
prep to conduct the same work. For instance, the
Chinese phrase in Figure 4 can be translated into
?hold in Kabul press conference? with the appli-
cation of this rule.
prep Within verb phrases, the positions of prep
structures are quite different between Chinese and
English. Figure 5 shows an example of a verb
phrase with a preposition modifier (prep), which
literally translates into ?Musharraf at this place tell
reporter?. Recognizing that prep structures occur
before the verb in Chinese (e.g., ????? in Fig-
ure 5) but after the verb in English (usually in the
last position of a verb phrase, e.g., ?here? in the
caption of Figure 5), we applied a rule prep - dobj
to reposition prep structures after their sibling dobj
structures.
In summary, the dependency-based pre-
ordering rule set has eight rules: plmod : lobj,
plmod : lccomp, nsubj : rcmod, dobj : rcmod,
pobj : rcmod, lobj : rcmod, rcmod : prep, and
prep - dobj.
3 Experiments
We used the MOSES PBSMT system (Koehn et
al., 2007) in our experiments. The training data,
which included those data used in Wang et al
(2007), contained 1 million pairs of sentences ex-
tracted from the Linguistic Data Consortium?s par-
allel news corpora. Our development set was
the official NIST MT evaluation data from 2002
to 2005, consisting of 4476 Chinese-English sen-
tences pairs. Our test set was the NIST 2006 MT
evaluation data, consisting of 1664 sentence pairs.
We employed the Stanford Segmenter
1
to segment
all of the data sets. For evaluation, we used BLEU
scores (Papineni et al, 2002).
We implemented the constituent-based pre-
ordering rule set in Wang et al (2007) for compar-
ison, which is called WR07 below. The Berkeley
Parser (Petrov et al, 2006) was employed for pars-
ing the Chinese sentences. For training the Berke-
ley Parser, we used Chinese Treebank (CTB) 7.0.
We conducted our dependency-based pre-
ordering experiments on the Berkeley Parser and
the Mate Parser (Bohnet, 2010), which were
shown to be the two best parsers for Stanford
typed dependencies (Che et al, 2012). First, we
converted the constituent parse trees in the re-
sults of the Berkeley Parser into dependency parse
trees by employing a tool in the Stanford Parser
(Klein and Manning, 2003). For the Mate Parser,
POS tagged inputs are required both in training
and in inference. Thus, we then extracted the
POS information from the results of the Berke-
ley Parser and used these as the pre-specified POS
tags for the Mate Parser. Finally, we applied our
dependency-based pre-ordering rule set to the de-
pendency parse trees created from the converted
Berkeley Parser and the Mate Parser, respectively.
Table 1 presents a comparison of the system
without pre-ordering, the constituent system us-
ing WR07 and two dependency systems employ-
ing the converted Berkeley Parser and the Mate
Parser, respectively. It shows the BLEU scores on
the test set and the statistics of pre-ordering on the
training set, which includes the total count of each
rule set and the number of sentences they were ap-
1
http://nlp.stanford.edu/software/segmenter.shtml
158
Category Count Correct Incorrect Accuracy
plmod 42 26 16 61.9%
rcmod 89 49 40 55.1%
prep 54 36 18 66.7%
All 185 111 74 60.0%
Table 2: Accuracy of the dependency-based pre-ordering rules on a set of 200 sentences randomly se-
lected from the development set.
plied to. Both of our dependency systems outper-
formed WR07 slightly but were not significant at
p = 0.05. However, both of them substantially de-
creased the total times about 60% (or 1,600,000)
for pre-ordering rule applications on the training
set, compared with WR07. In our opinion, the rea-
son for the great decrease was that the dependency
parse trees were more concise than the constituent
parse trees in describing sentences and they could
also describe the reordering at the sentence level in
a finer way. In contrast, the constituent parse trees
were more redundant and they needed more nodes
to conduct long-distance reordering. In this case,
the affect of the performance of the constituent
parsers on pre-ordering is larger than that of the
dependency ones so that the constituent parsers are
likely to bring about more incorrect pre-orderings.
Similar to Wang et al (2007), we carried out
human evaluations to assess the accuracy of our
dependency-based pre-ordering rules by employ-
ing the system ?OUR DEP 2? in Table 1. The
evaluation set contained 200 sentences randomly
selected from the development set. Among them,
107 sentences contained at least one rule and the
rules were applied 185 times totally. Since the
accuracy check for dependency parse trees took
great deal of time, we did not try to select er-
ror free (100% accurately parsed) sentences. A
bilingual speaker of Chinese and English looked
at an original Chinese phrase and the pre-ordered
one with their corresponding English phrase and
judged whether the pre-ordering obtained a Chi-
nese phrase that had a closer word order to the En-
glish one. Table 2 shows the accuracies of three
categories of our dependency-based pre-ordering
rules. The overall accuracy of this rule set is
60.0%, which is almost at the same level as the
WR07 rule set (62.1%), according to the similar
evaluation (200 sentences and one annotator) con-
ducted in Wang et al (2007). Notice that some
of the incorrect pre-orderings may be caused by
erroneous parsing as also suggested by Wang et
al. (2007). Through human evaluations, we found
that 19 out of the total 74 incorrect pre-orderings
resulted from errors in parsing. Among them, 13
incorrect pre-orderings applied the rules of the rc-
mod category. The analysis suggests that we need
to introduce constraints on the rule application of
this category in the future.
4 Conclusion
In this paper, we introduced a novel pre-ordering
approach based on dependency parsing for a
Chinese-English PBSMT system. The results
showed that our approach achieved a BLEU score
gain of 1.61. Moreover, our dependency-based
pre-ordering rule set substantially decreased the
time for applying pre-ordering rules about 60%
compared with WR07, on the training set of 1M
sentences pairs. The overall accuracy of our rule
set is 60.0%, which is almost at the same level as
the WR07 rule set. These results indicated that
dependency parsing is more effective for conduct-
ing pre-ordering for Chinese-English PBSMT. Al-
though our work focused on Chinese, the ideas can
also be applied to other languages.
In the future, we attempt to create more efficient
pre-ordering rules by exploiting the rich informa-
tion in dependency structures.
Acknowledgments
We thank the anonymous reviewers for their valu-
able comments and suggestions. This work is sup-
ported in part by the International Science & Tech-
nology Cooperation Program of China (Grant No.
2014DFA11350) and Key Lab of Intelligent In-
formation Processing of Chinese Academy of Sci-
ences (CAS), Institute of Computing Technology,
CAS, Beijing 100190, China.
References
Bernd Bohnet. 2010. Very high accuracy and fast de-
pendency parsing is not a contradiction. In Proceed-
159
ings of the 23rd International Conference on Com-
putational Linguistics (COLING 2010).
Daniel Cer, Christopher D. Manning, and Dan Juraf-
sky. 2013. Positive Diversity Tuning for Machine
Translation System Combination. In Proceedings of
the Eighth Workshop on Statistical Machine Trans-
lation (WMT 2013).
Pi-Chuan Chang, Huihsin Tseng, Dan Jurafsky, and
Christopher D. Manning. 2009. Discriminative
reordering with Chinese grammatical relations fea-
tures. In Proceedings of the HLT-NAACL Workshop
on Syntax and Structure in Statistical Translation,
pages 51-59.
Wanxiang Che, Valentin Spitkovsky, and Ting Liu.
2012. A comparison of Chinese parsers for Stan-
ford dependencies. In Proceedings of the 50th An-
nual Meeting of the Association for Computational
Linguistics, pages 11-16.
Michael Collins, Philipp Koehn, and Ivona Kucerova.
2005. Clause restructuring for statistical machine
translation. In Proceedings of the 43rd Annual
Meeting of the Association for Computational Lin-
guistics, pages 531-540.
Dan Han, Katsuhito Sudoh, Xianchao Wu, Kevin Duh,
Hajime Tsukada, and Masaaki Nagata. 2012. Head
Finalization reordering for Chinese-to-Japanese ma-
chine translation. In Proceedings of SSST-6, Sixth
Workshop on Syntax, Semantics and Structure in
Statistical Translation, pages 57-66.
Nizar Habash. 2007. Syntactic preprocessing for sta-
tistical machine translation. In Proceedings of the
11th Machine Translation Summit (MT-Summit).
Hideki Isozaki, Katsuhito Sudoh, Hajime Tsukada, and
Kevin Duh. 2010. Head Finalization: A simple re-
ordering rule for SOV languages. In Proceedings
of the Joint Fifth Workshop on Statistical Machine
Translation and MetricsMATR, pages 250-257.
Jason Katz-Brown, Slav Petrov, Ryan McDonald,
Franz J. Och, David Talbot, Hiroshi Ichikawa,
Masakazu Seno, and Hideto Kazawa. 2011. Train-
ing a parser for machine translation reordering. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, pages 183-
192.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics, pages 423-430.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open source toolkit for statistical machine transla-
tion. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics Com-
panion Volume Proceedings of the Demo and Poster
Sessions, pages 177-180.
Young-Suk Lee, Bing Zhao, and Xiaoqian Luo.
2010. Constituent reordering and syntax models for
English-to-Japanese statistical machine translation.
In Proceedings of the 23rd International of Confer-
ence on Computational Linguistics, pages 626-634.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings
of 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311-318.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and
interpretable tree annotation. In Proceedings of
the 21st International Conference on Computational
Linguistics and 44th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 433-440.
Carl Pollard and Ivan A. Sag. 1994. Head-Driven
Phrase Structure Grammar. University of Chicago
Press.
Chao Wang, Michael Collins, and Philipp Koehn.
2007. Chinese syntactic reordering for statistical
machine translation. In Proceedings of the 2007
Joint Conference on Empirical Methods in Natural
Language Processing and Computational Natural
Language Learning, pages 737-745.
Xianchao Wu, Katsuhito Sudoh, Kevin Duh, Hajime
Tsukada, and Masaaki Nagata. 2011. Extracting
preordering rules from predicate-argument struc-
tures. In Proceedings of 5th International Joint Con-
ference on Natural Language Processing, pages 29-
37.
Fei Xia and Michael McCord. 2004. Improving
a statistical MT system with automatically learned
rewrite patterns. In Proceedings of Coling 2004,
pages 508-514.
Peng Xu, Jaeho Kang, Michael Ringgaard, and Franz J.
Och. 2009. Using a dependency parser to improve
SMT for subject-object-verb languages. In Proceed-
ings of HLT-NAACL, pages 245-253.
Jiajun Zhang, Chengqing Zong, and Shoushan Li.
2008. Sentence type based reordering model for sta-
tistical machine translation. In Proceedings of the
22nd International Conference on Computational
Linguistics, pages 1089-1096.
Yuqi Zhang, Richard Zens, and Hermann Ney. 2011.
Chunk-level reordering of source language sen-
tences with automatically learned rules for statisti-
cal machine translation. In HLT-NAACL Workshop
on Syntax and Structure in Statistical Translation,
pages 1-8.
160
