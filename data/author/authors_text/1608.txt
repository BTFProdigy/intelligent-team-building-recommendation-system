83
84
85
86
87
88
89
90
Robust Knowledge Discovery from Parallel Speech and
Text Sources
F. Jelinek, W. Byrne, S. Khudanpur, B. Hladka?. CLSP, Johns Hopkins University, Baltimore, MD.
H. Ney, F. J. Och. RWTH Aachen University, Aachen, Germany
J. Cur???n. Charles University, Prague, Czech Rep.
J. Psutka. University of West Bohemia, Pilsen, Czech Rep.
1. INTRODUCTION
As a by-product of the recent information explosion, the same
basic facts are often available from multiple sources such as the In-
ternet, television, radio and newspapers. We present here a project
currently in its early stages that aims to take advantage of the re-
dundancies in parallel sources to achieve robustness in automatic
knowledge extraction.
Consider, for instance, the following sampling of actual news
from various sources on a particular day:
CNN: James McDougal, President Bill Clinton?s former business
partner in Arkansas and a cooperating witness in the White-
water investigation, died Sunday while serving a federal prison
term. He was 57.
MSNBC: Fort Worth, Texas, March 8. Whitewater figure James
McDougal died of an apparent heart attack in a private com-
munity hospital in Fort Worth, Texas, Sunday. He was 57.
ABC News: Washington, March 8. James McDougal, a key figure
in Independent Counsel Kenneth Starr?s Whitewater investi-
gation, is dead.
The Detroit News: Fort Worth. James McDougal, a key witness
in Kenneth Starr?s Whitewater investigation of President Clin-
ton and First Lady Hillary Rodham Clinton, died of a heart
attack in a prison hospital Sunday. He was 57.
San Jose Mercury News: James McDougal, the wily Arkansas
banking rogue who drew Bill Clinton and Hillary Rodham
Clinton into real estate deals that have come to haunt them,
died Sunday of cardiac arrest just months before he hoped to
be released from prison. He was 57.
The Miami Herald: Washington. James McDougal, the wily
Arkansas financier and land speculator at the center of the
original Whitewater probe against President Clinton, died
Sunday.
.
Story
Alignment
Speech
RecognitionSpeech Sources
Basic Models:
acoustic
lexical
language
Topic specific
acoustic and language
models
stories
Aligned Sentence
retrieval
Ranked
Answers
Query
Text sources
Figure 1: Information Flow in Alignment and Extraction
We propose to align collections of stories, much like the exam-
ple above, from multiple text and speech sources and then develop
methods that exploit the resulting parallelism both as a tool to im-
prove recognition accuracy and to enable the development of sys-
tems that can reliably extract information from parallel sources.
Our goal is to develop systems that align text sources and rec-
ognize parallel speech streams simultaneously in several languages
by making use of all related text and speech. The initial systems
we intend to develop will process each language independently.
However, our ultimate and most ambitious objective is to align text
sources and recognize speech using a single, integrated multilin-
gual ASR system. Of course, if sufficiently accurate automatic ma-
chine translation (MT) techniques ([1]) were available, we could
address multilingual processing and single language systems in the
same way. However MT techniques are not yet reliable enough
that we expect all words and phrases recognized within languages
to contribute to recognition across languages. We intend to develop
methods that identify the particular words and phrases that both can
be translated reliably and also used to improve story recognition.
As MT technology improves it can be incorporated more exten-
sively within the processing paradigm we propose. We consider
this proposal a framework within which successful MT techniques
can eventually be used for multilingual acoustic processing.
2. PROJECT OBJECTIVES
The first objective is to enhance multi-lingual information sys-
tems by exploiting the processing capabilities for resource-rich lan-
guages to enhance the capabilities for resource-impoverished lan-
guage. The second objective is to advance information retrieval and
knowledge information systems by providing them with consider-
ably improved multi-lingual speech recognition capabilities. Our
research plan proceeds in several steps to (i) collect and (ii) align
multi-lingual parallel speech and text sources, (iii) exploit paral-
lelism for improving ASR within a language, and to (iv) exploit
parallelism for improving ASR across languages. The main infor-
mation flows involved in aligning and exploiting parallel sources
are illustrated in Figure 1. We will initially focus on German, En-
glish and Czech language sources. This section summarizes the
major components of our project.
2.1 Parallel Speech and Text Sources
The monolingual speech and text collections that we will use
to develop techniques to exploit parallelism for improving ASR
within a language are readily available. For instance, the North
American News Text corpus of parallel news streams from 16 US
newspapers and newswire is available from LDC. A 3-year period
yields over 350 million words of multi-source news text.
In addition to data developed within the TIDES and other HLT
programs, we are in the process of identifying and creating our own
multilingual parallel speech and text sources.
FBIS TIDES Multilingual Newstext Collection
For the purposes of developing multilingual alignment techniques,
we intend to use the 240 day, contemporaneous, multilingual news
text collection made available for use to TIDES projects by FBIS.
This corpus contains news in our initial target languages of English,
German, and Czech. The collections are highly parallel, in that
much of the stories are direct translations.
Radio Prague Multilingual Speech and Text Corpus
Speech and news text from Radio Prague was collected under the
direction of J. Psutka with the consent of Radio Prague. The col-
lection contains speech and text in 5 languages: Czech, English,
German, French, and Spanish. The collection began June 1, 2000
and continued for approximately 3 months. The text collection con-
tains the news scripts used for the broadcast; the broadcasts more
or less follow the scripts. The speech is about 3 minutes per day
in each language, which should yield a total of about 5 hours of
speech per language.
Our initial analysis of the Radio Prague corpus suggest that only
approximately 5% of the stories coincide in topic, and that there
is little, if any, direct translation of stories. We anticipate that this
sparseness will make this corpus significantly hard to analyze than
another, highly-parallel corpus. However, we expect this is the
sort of difficulty that will likely be encountered in processing ?real-
world? multilingual news sources.
2.2 Story-level Alignment
Once we have the multiple streams of information we must be
able to align them according to story. A story is the description of
one or more events that happened in a single day and that are re-
ported in a single article by a daily news source the next day. We
expect that we will use the same techniques used in the Topic De-
tection (TDT) field ([5]). Independently of the specific details of
the alignment procedure, there is now substantial evidence that re-
lated stories from parallel streams can be identified using standard
statistical Information Retrieval (IR) techniques.
Sentence Alignment As part of the infrastructure needed to in-
corporate cross-lingual information into language models, we are
employing statistical MT systems to generate English/German and
English/Czech alignments of sentences in the FBIS Newstext Col-
lection. For the English/German sentence and single-word based
alignments, we plan to use statistical models ([4]) [3] which gen-
erate both sentence and word alignments. For English/Czech sen-
tence alignment, we will employ the statistical models trained as
part of the Czech-English MT system developed during the 1999
Johns Hopkins Summer Workshop ([2]).
2.3 Multi-Source Automatic Speech
Recognition
The scenario we propose is extraction of information from paral-
lel text followed by repeated recognition of parallel broadcasts, re-
sulting in a gradual lowering the WER. The first pass is performed
in order to find the likely topics discussed in the story and to iden-
tify the topics relevant to the query. In this process, the acoustic
model will be improved by deriving pronunciation specifications
for out-of-vocabulary words and fixed phrases extracted from the
parallel stories. The language model will be improved by extending
the coverage of the underlying word and phrase vocabulary, and by
specializing the model?s statistics to the narrow topic at hand. As
long as a round of recognition yields new information, the corre-
sponding improvement is incorporated into the recognizer modules
and bootstrapping of the system continues.
Story-specific Language Models from Parallel Speech and Text
Our goal is to create language models combining specific but sparse
statistics, derived from relevant parallel material, with reliable but
unspecific statistics obtainable from large general corpora. We will
create special n-gram language models from the available text, re-
lated or parallel to the spoken stories. We can then interpolate
this special model with a larger pre-existing model, possibly de-
rived from training text associated to the topic of the story. Our
recent STIMULATE work demonstrated success in construction of
topic-specific language models on the basis of hierarchically topic-
organized corpora [8].
Unlike building models from parallel texts, the training of story
specific language models from recognized speech is also affected
by recognition errors in the data which will be used for language
modeling. Confidence measures can be used to estimate the cor-
rectness of individual words or phrases on the recognizer output.
Using this information, n-gram statistics can be extracted from the
recognizer output by selecting those events which are likely to be
correct and which can therefore be used to adjust the original lan-
guage model without introducing new errors to the recognition sys-
tem.
Language Models with Cross-Lingual Lexical Triggers
A trigger language model ([6], [7]) will be constructed for the tar-
get language from the text corpus, where the lexical triggers are not
from the word-history in the target language, but from the aligned
recognized stories in the source language. The trigger informa-
tion becomes most important in those cases in which the baseline
n-gram model in the target language does not supply sufficient in-
formation to predict a word. We expect that content words in the
source language are good predictors for content words in the target
language and that these words are difficult to predict using the tar-
get language alone, and the mutual information techniques used to
identify trigger pairs will be useful here.
Once a spoken source-language story has been recognized, the
words found here there will be used as triggers in the language
model for the recognition of the target-language news broadcasts.
3. SUMMARY
Our goal is to align collections of stories from multiple text and
speech sources in more than one language and then develop meth-
ods that exploit the resulting parallelism both as a tool to improve
recognition accuracy and to enable the development of systems that
can reliably extract information from parallel sources. Much like
a teacher rephrases a concept in a variety of ways to help a class
understand it, the multiple sources, we expect, will increase the po-
tential of success in knowledge extraction. We envision techniques
that will operate repeatedly on multilingual sources by incorporat-
ing newly discovered information in one language into the models
used for all the other languages. Applications of these methods ex-
tend beyond news sources to other multiple-source domains such
as office email and voice-mail, or classroom materials such as lec-
tures, notes and texts.
4. REFERENCES
[1] P. F. Brown, S. A. DellaPietra, V. J. D. Pietra, and R. L.
Mercer. The mathematics of statistical translation.
Computational Linguistics, 19(2), 1993.
[2] K. K. et al Statistical machine translation, WS?99 Final
Report, Johns Hopkins University, 1999.
http://www.clsp.jhu.edu/ws99/projects/mt.
[3] F. J. Och and H. Ney. Improved statistical alignment models.
In ACL?00, pages 440?447, 2000.
[4] F. J. Och, C. Tillmann, and H. Ney. Improved alignment
models for statistical machine translation. In EMNLP/VLC?99,
pages 20?28, 1999.
[5] Proceedings of the Topic Detection and Tracking workshop.
University of Maryland, College Park, MD, October 1997.
[6] C. Tillmann and H. Ney. Selection criteria for word trigger
pairs in language modelling. In ICGI?96, pages 95?106, 1996.
[7] C. Tillmann and H. Ney. Statistical language modeling and
word triggers. In SPECOM?96, pages 22?27, 1996.
[8] D. Yarowsky. Exploiting nonlocal and syntactic word
relationships in language models for conversational speech
recognition, a NSF STIMULATE Project IRI9618874, 1997.
Johns Hopkins University.
Prague Czech-English Dependency Treebank
Any Hopes for a Common Annotation Scheme?
Martin ?Cmejrek, Jan Cur???n, Jir??? Havelka
Institute of Formal and Applied Linguistics
Faculty of Mathematics and Physics, Charles University in Prague
Malostranske? na?m. 25, Praha 1, Czech
{cmejrek,curin,havelka}@ufal.mff.cuni.cz
Abstract
The Prague Czech-English Dependency Tree-
bank (PCEDT) is a new syntactically annotated
Czech-English parallel resource. The Penn
Treebank has been translated to Czech, and its
annotation automatically transformed into de-
pendency annotation scheme. The dependency
annotation of Czech is done from plain text by
automatic procedures. A small subset of cor-
responding Czech and English sentences has
been annotated by humans. We discuss some
of the problems we have experienced during
the automatic transformation between annota-
tion schemes and hint at some of the difficulties
to be tackled by potential guidelines for depen-
dency annotation of English.
1 Introduction
The Prague Czech-English Dependency Treebank
(PCEDT) is a project of creating a Czech-English
syntactically annotated parallel corpus motivated by
research in the field of machine translation. Parallel data
are needed for designing, training, and evaluation of both
statistical and rule-based machine translation systems.
Since Czech is a language with relatively high degree
of word-order freedom, and its sentences contain certain
syntactic phenomena, such as discontinuous constituents
(non-projective constructions), which cannot be straight-
forwardly handled using the annotation scheme of Penn
Treebank (Marcus et al, 1993; Linguistic Data Consor-
tium, 1999), based on phrase-structure trees, we decided
to adopt for the PCEDT the dependency-based annotation
scheme of the Prague Dependency Treebank ? PDT (Lin-
guistic Data Consortium, 2001). The PDT is annotated
on three levels: morphological layer (lowest), analytic
layer (middle) ? surface syntactic annotation, and tec-
togrammatical layer (highest) ? level of linguistic mean-
ing. Dependency trees, representing the sentence struc-
ture as concentrated around the verb and its valency, are
used for the analytical and tectogrammatical levels, as
proposed by Functional Generative Description (Sgall et
al., 1986).
In Section 2, we describe the process of translating the
Penn Treebank into Czech. Section 3 sketches the gen-
eral procedure for transforming phrase topology of Penn
Treebank into dependency structure and describes the
specific conversions into analytical and tectogrammatical
representations. The following Section 4 describes the
automatic process of parsing of Czech into analytical rep-
resentation and its automatic conversion into tectogram-
matical representation. Section 5 briefly discusses some
of the problems of annotation from the point of view of
mutual compatibility of annotation schemes. Section 6
gives an overview of additional resources included in the
PCEDT.
2 English to Czech Translation of Penn
Treebank
When starting the PCEDT project, we chose the latter of
two possible strategies: either the parallel annotation of
already existing parallel texts, or the translation and anno-
tation of an existing syntactically annotated corpus. The
choice of the Penn Treebank as the source corpus was
also pragmatically motivated: firstly it is a widely rec-
ognized linguistic resource, and secondly the translators
were native speakers of Czech, capable of high quality
translation into their native language.
The translators were asked to translate each English
sentence as a single Czech sentence and to avoid unneces-
sary stylistic changes of translated sentences. The trans-
lations are being revised on two levels, linguistic and fac-
tual. About half of the Penn Treebank has been translated
so far (currently 21,628 sentences), the project aims at
translating the whole Wall Street Journal part of the Penn
Treebank.
DT
An
NP-SBJ-1
NN
earthquake
VBD
struck
JJ
Northern
NP
NNP
California
VP
,
,
NP-SBJ
-NONE-
*-1
S
S-ADV
VBG
killing
JJR
more
VP
QP
IN
than
CD
50
NP
NNS
people
.
.
Figure 1: Penn Treebank annotation of the sentence ?An earthquake struck Northern California, killing more than 50
people.?
For the purpose of quantitative evaluation methods,
such as NIST or BLEU, for measuring performance of
translation systems, we selected a test set of 515 sen-
tences and had them retranslated from Czech into En-
glish by 4 different translator offices, two of them from
the Czech Republic and two of them from the U.S.A.
3 Transformation of Penn Treebank
Phrase Trees into Dependency Structure
The transformation algorithm from phrase-structure
topology into dependency one, similar to transformations
described by Xia and Palmer (2001), works as follows:
? Terminal nodes of the phrase are converted to nodes
of the dependency tree.
? Dependencies between nodes are established recur-
sively: The root node of the dependency tree trans-
formed from the head constituent of a phrase be-
comes the governing node. The root nodes of the
dependency trees transformed from the right and left
siblings of the head constituent are attached as the
left and right children (dependent nodes) of the gov-
erning node, respectively.
? Nodes representing traces are removed and their
children are reattached to the parent of the trace.
3.1 Preprocessing of Penn Treebank
Several preprocessing steps preceded the transformation
into both analytical and tectogrammatical representa-
tions.
Marking of Heads in English
The concept of the head of a phrase is important dur-
ing the tranformation described above. For marking
head constituents in each phrase, we used Jason Eisner?s
scripts.
Lemmatization of English
Czech is an inflective language, rich in morphology,
therefore lemmatization (assigning base forms) is indis-
pensable in almost any linguistic application. Mostly for
reasons of symmetry with Czech data and compatibility
with the dependency annotation scheme, the English part
was automatically lemmatized by the morpha tool (Min-
nen et al, 2001) using manually assigned POS tags of the
Penn Treebank.
Unique Identification
For technical reasons, a unique identifier is assigned to
each sentence and to each token of Penn Treebank.
3.2 English Analytical Dependency Trees
This section describes the automatic process of convert-
ing Penn Treebank annotation into analytical representa-
tion.
Sent. #1
AuxS
an
Atr
earthquake
Sb
struck
Pred
Northern
Atr
California
Obj
,
AuxX
killing
Adv
more
Atr
than
AuxP
50
Atr
people
Obj
.
AuxK
Figure 2: Analytical tree for the sentence ?An earthquake
struck Northern California, killing more than 50 people.?
The structural transformation works as described
above. Because the handling of coordination in PDT is
different from the Penn Treebank annotation style and
the output of Jason Eisner?s head assigning scripts, in
the case of a phrase containing a coordinating conjunc-
tion (CC), we consider the rightmost CC as the head. The
treatment of apposition is a more difficult task, since there
is no explicit annotation of this phenomenon in the Penn
Treebank; constituents of a noun phrase enclosed in com-
mas or other delimiters (and not containing CC) are con-
sidered to be in apposition and the rightmost delimiter
becomes the head.
The information from both the phrase tree and the de-
pendency tree is used for the assignment of analytical
functions:
? Penn Treebank function tag to analytical function
mapping: some function tags of a phrase tree corre-
spond to analytic functions in an analytical tree and
can be mapped to them:
SBJ ? Sb,
{DTV, LGS, BNF, TPC, CLR} ? Obj,
{ADV, DIR, EXT, LOC, MNR, PRP, TMP, PUT} ? Adv.
? Assignment of analytical functions using local con-
text of a node: for assigning analytical functions to
the remaining nodes, we use rules looking at the cur-
rent node, its parent and grandparent, taking into ac-
count POS and the phrase marker of the constituent
in the original phrase tree headed by the node. For
example, the rule
mPOS = DT|mAF = Atr
earthquake
ACT
strike
PRED
northern
RSTR
California
PAT
&Cor;
ACT
kill
COMPL
more
CPR
50
RSTR
people
PAT
Figure 3: Tectogrammatical tree for the sentence ?An
earthquake struck Northern California, killing more than
50 people.?
assigns the analytical function Atr to every deter-
miner, the rule
mPOS = MD|pPOS = VB|mAF = AuxV
assigns the function tag AuxV to a modal verb
headed by a verb, etc. The attribute mPOS repre-
senting the POS of a node is obligatory for every
rule. The rules are examined primarily in the order
of the longest prefix of the POS of the given node
and secondarily in the order as they are listed in the
rule file. The ordering of rules is important, since
the first matching rule found assigns the analytical
function and the search is finished.
Specifics of the PDT and Penn Treebank annotation
schemes, mainly the markup of coordinations, apposi-
tions, and prepositional phrases are handled separately:
? Coordinations and appositions: the analytical func-
tion that was originally assigned to the head of a
coordination or apposition is propagated to its child
nodes by attaching the suffix Co or Ap to them, and
the head node gets the analytical function Coord or
Apos, respectively.
? Prepositional phrases: the analytical function origi-
nally assigned to the preposition node is propagated
to its child and the preposition node is labeled AuxP.
? Sentences in the PDT annotation style always con-
tain a root node labeled AuxS, which, as the only one
in the dependency tree, does not correspond to any
terminal of the phrase tree; the root node is inserted
Sent. #1
zem?t?esen?
Sb
zas?hlo
Pred
severn?
Atr
Kalifornii
Obj
a
Coord
usmrtilo
Pred
v?ce
Adv
ne?
AuxC
50
Adv
lid?
Atr
.
AuxK
Figure 4: Analytical tree for the Czech translation
?Zeme?tr?esen?? zasa?hlo severn?? Kalifornii a usmrtilo v??ce
nez? 50 lid??.?
above the original root. While in the Penn Treebank
the final punctuation is a constituent of the sentence
phrase, in the analytical tree it is moved under the
technical sentence root node.
Compare the phrase structure and the analytical repre-
sentation of a sample sentence from the Penn Treebank
in Figures 1 and 2.
3.3 English Tectogrammatical Dependency Trees
The transformation of Penn Treebank phrase trees into
tectogrammatical representation consists of a structural
transformation, and an assignment of a tectogrammat-
ical functor and a set of grammatemes to each node.
At the beginning of the structural transformation, the
initial dependency tree is created by a general transfor-
mation procedure as described above. However, func-
tional (synsemantic) words, such as prepositions, punc-
tuation marks, determiners, subordinating conjunctions,
certain particles, auxiliary and modal verbs are handled
differently. They are marked as ?hidden? and information
about them is stored in special attributes of their govern-
ing nodes (if they were to head a phrase, the head of the
other constituent became the governing node in the de-
pendency tree).
The well-formedness of a tectogrammatical tree struc-
ture requires the valency frames to be complete: apart
from nodes that are realized on surface, there are several
types of ?restored? nodes representing the non-realized
members of valency frames (cf. pro-drop property of
_ _
zem?t?esen?
ACT
earthquake
zas?hnout
PRED CO
strike
severn?
RSTR
northern
Kalifornie
PAT
California
a
CONJ
and
usmrtit
PRED CO
kill
v?ce
EXT
more_than
50
CPR
50
?lov?k
PAT
man
Figure 5: Tectogrammatical tree for the Czech translation
?Zeme?tr?esen?? zasa?hlo severn?? Kalifornii a usmrtilo v??ce
nez? 50 lid??.?
Czech and verbal condensations using gerunds and in-
finitives both in Czech and English). For a partial recon-
struction of such nodes, we can use traces, which allow
us to establish coreferential links, or restore general par-
ticipants in the valency frames.
For the assignment of tectogrammatical functors, we
can use rules taking into consideration POS tags (e.g.
PRP ? APP), function tags (JJ ? RSTR, JJR ? CPR,
etc.) and lemma (?not? ? RHEM, ?both? ? RSTR).
Grammateme Assignment ? morphological gram-
matemes (e.g. tense, degree of comparison) are assigned
to each node of the tectogrammatical tree. The assign-
ment of the morphological attributes is based on Pen-
nTreebank tags and reflects basic morphological proper-
ties of the language. At the moment, there are no auto-
matic tools for the assignment of syntactic grammatemes,
which are designed to capture detailed information about
deep syntactic structure.
The whole procedure is described in detail in
Kuc?erova? and ?Zabokrtsky? (2002).
In order to gain a ?gold standard? annotation, 1,257
sentences have been annotated manually (the 515 sen-
tences from the test set are among them). These data
are assigned morphological gramatemes (the full set of
values) and syntactic grammatemes, and the nodes are
reordered according to topic-focus articulation (informa-
tion structure).
The quality of the automatic transformation procedure
described above, based on comparison with manually an-
JJ
Such
NP-SBJ-1
NNS
loans
VBP
remain
JJ
classified
ADJP-PRD
IN
as
PP
ADJP
JJ
non-accruing
VP
,
,
NP-SBJ
-NONE-
*-1
VBG
costing
S
DT
the
NP
NN
bank
S-ADV
VP
$
$
QP
CD
10
CD
million
NP
-NONE-
*U*
.
.
Figure 6: Penn Treebank annotation of the sentence ?Such loans remain classified as non-accruing, costing the bank
$10 million.?
notated trees, is about 6% of wrongly aimed dependen-
cies and 18% of wrongly assigned functors.
See Figure 3 for the manually annotated tectogrammat-
ical representation of the sample sentence.
4 Automatic Annotation of Czech
The Czech translations of Penn Treebank were auto-
matically tokenized and morphologically tagged, each
word form was assigned a base form ? lemma by
Hajic? and Hladka? (1998) tagging tools.
Czech analytical parsing consists of a statistical
dependency parser for Czech ? either Collins parser
(Collins et al, 1999) or Charniak parser (Charniak,
1999), both adapted to dependency grammar ? and
a module for automatic analytical function assignment
( ?Zabokrtsky? et al, 2002).
When building the tectogrammatical structure, the
analytical tree structure is converted into the tectogram-
matical one. These transformations are described by lin-
guistic rules (Bo?hmova?, 2001). Then, tectogrammatical
functors are assigned by a C4.5 classifier ( ?Zabokrtsky? et
al., 2002).
The test set of 515 sentences (which have been retrans-
lated into English) has been also manually annotated on
tectogrammatical level.
See Figures 4 and 5 for automatic analytical and man-
ual tectogrammatical annotation of the Czech translation
of the sample sentence.
such
RSTR
loan
ACT
remain
PRED
&Cor;
ACT
classify
PAT
non-accruing
CPR
&Cor;
ACT
cost
COMPL
bank
ADDR
$
PAT
10
RSTR
million
RSTR
Figure 7: Tectogrammatical tree for the sentence ?Such
loans remain classified as non-accruing, costing the bank
$10 million.?
5 Problems of Dependency Annotation of
English
The manual annotation of 1,257 English sentences on tec-
togrammatical level was, to our knowledge, the first at-
tempt of its kind, and was based especially on the instruc-
tions for tectogrammatical annotation of Czech. During
the process of annotation, we have experienced both phe-
nomena that do not occur in Czech at all, and phenomena
_ _
&Gen;
ACT
obdobn?
RSTR
such
?v?r
PAT
loan
nad?le
THL
still
klasifikovat
PRED CO
clasify
&Neg;
RHEM
vyn??ej?c?
EFF
accruing
&Comma;
CONJ
co?
ACT
which
banka
PAT
bank
st?t
PRED CO
cost
10
RSTR
10
mili?n
EXT
million
dolar
MAT
dollar
Figure 8: Tectogrammatical tree for the Czech trans-
lation ?Obdobne? u?ve?ry jsou nada?le klasifikova?ny jako
nevyna?s?ej??c??, coz? banku sta?lo 10 milionu? dolaru?.?
whose counterparts in Czech occur rarely, and therefore
are not handeled thoroughly by the guidelines for tec-
togrammatical annotation designed for Czech. To men-
tion just a few, among the former belongs the annotation
of articles, certain aspects of the system of verbal tenses,
and phrasal verbs. A specimen of a roughly correspond-
ing phenomenon occurring both in Czech and English is
the gerund. It is a very common means of condensation
in English, but its counterpart in Czech (usually called
transgressive) has fallen out of use and is nowadays con-
sidered rather obsolete.
The guidelines for Czech require the transgressive to
be annotated with the functor COMPL. The reason why it is
highly problematic to apply them straightforwardly also
to the annotation of English, is that the English gerund
has a much wider range of functions than the Czech trans-
gressive. The gerund can be seen as a means of con-
densing subordinated clauses with in principle adverbial
meaning (as it is analyzed in the phrase-structure annota-
tion of Penn Treebank). Since the range of functors with
adverbial meaning is much more fine-grained, we deem it
inappropriate to mark the gerund clauses in such a simple
way on the tectogrammatical level.
From the point of view of machine translation, the
gerund constructions pose considerable difficulties be-
cause of the many syntactic constructions suitable as their
translations corresponding to their varied syntactic func-
tions.
We present two examples illustrating the issues men-
tioned above. Each example consists of three figures, the
first one presenting the Penn Treebank annotation of a (in
the second case simplified) sentence from the Penn Tree-
bank, the second one giving its tentative tectogrammatic
representation (according to the guidelines for Czech ap-
plied to English), and the third one containing the tec-
JJ
common
ADJP
CC
and
VBN
preferred
NP
NN
stock
NN
purchase
NNS
rights
Figure 9: Penn Treebank annotation of the noun phrase
?common and preferred stock purchase rights?.
togrammatical representation of its translation into Czech
(see Figures 1, 3, 5, and Figures 6, 7, 8). Note that in nei-
ther of the two examples the Czech transgressive is used
as the translation of the English gerund; a coordination
structure is used instead.
On the other hand, we have also experienced phenom-
ena in English whose Penn Treebank style of annotation
is insufficient for a successfull conversion into depen-
dency representation.
In English, the usage of constructions with nominal
premodification is very frequent, and the annotation of
such noun phrases in the Penn Treebank is often flat,
grouping together several constituents without reflecting
finer syntactic and semantic relations among them (see
Figure 9 for an example of such a noun phrase). In fact,
the possible syntactic and especially semantic relations
between the members of the noun phrase can be highly
ambiguous, but when translating such a noun phrase into
Czech, we are not usually able to preserve the ambiguity
and are forced to resolve it by choosing one of the read-
ings (see Figure 10).
Sometimes we even may be forced to insert new words
explicitly expressing the semantic relations within the
nominal group. An example of an English noun phrase
and the tectogrammatical representation of its Czech
translation with an inserted word ?podnikaj??c??? (?operat-
ing?) can be found in Figures 11 and 12.
6 Other Resources Included in PCEDT
6.1 Reader?s Digest Parallel Corpus
Reader?s Digest parallel corpus contains raw text in
53,000 aligned segments in 450 articles from the Reader?s
Digest, years 1993?1996. The Czech part is a free trans-
lation of the English version. The final selection of
data has been done manually, excluding articles whose
translations significantly differ (in length, culture-specific
facts, etc.). Parallel segments on sentential level have
been aligned by Dan Melamed?s aligning tool (Melamed,
DT
a
NNP
San
NNP
Francisco
NN
food
NNS
products
CC
and
NP
NN
building
NNS
materials
NN
marketing
CC
and
NN
distribution
NN
company
Figure 11: Penn Treebank annotation of the noun phrase ?a San Francisco food products and building materials
marketing and distribution company?.
_ _
pr?vo
PAT
right
n?kup
PAT
purchase
oby  ejn?
RSTR
common
akcie
PAT CO
stock
a
CONJ
and
prioritn?
RSTR
preferred
akcie
PAT CO
stock
Figure 10: Tectogrammatical tree for the Czech transla-
tion ?pra?vo na na?kup obyc?ejny?ch a prioritn??ch akci???.
1996). The topology is 1?1 (81%), 0?1 or 1?0 (2%), 1?2
or 2?1 (15%), 2?2 (1%), and others (1%).
6.2 Dictionaries
The PCEDT comprises also a translation dictionary com-
piled from three different Czech-English manual dictio-
naries: two of them were downloaded form the Web and
one was extracted from Czech and English EuroWord-
Nets. Entry-translation pairs were filtered and weighed
taking into account the reliability of the source dictio-
nary, the frequencies of the translations in Czech and En-
glish monolingual corpora, and the correspondence of the
Czech and English POS tags. Furthermore, by training
GIZA++ (Och and Ney, 2003) translation model on the
training part of the PCEDT extended by the manual dic-
tionaries, we obtained a probabilistic Czech-English dic-
tionary, more sensitive to the domain of financial news
specific for the Wall Street Journal.
The resulting Czech-English probabilistic dictionary
_ _
_ _
sanfrancisk?
RSTR
San_Francisco
marketingov?
RSTR CO
marketing
a
CONJ
and
distribu n?
RSTR CO
distribution
spole nost
ACT
company
podnikaj?c?
RSTR
operating
potravina
LOC CO
food_product
a
CONJ
and
stavebn?
RSTR
building
materi?l
LOC CO
material
Figure 12: Tectogrammatical tree for the Czech transla-
tion ?sanfranciska? marketingova? a distribuc?n?? spolec?nost
podnikaj??c?? v potravina?ch a stavebn??ch materia?lech?.
contains 46,150 entry-translation pairs in its lemmatized
version and 496,673 pairs of word forms in the version
where for each entry-translation pair all the correspond-
ing word form pairs have been generated.
6.3 Tools
SMT Quick Run is a package of scripts and instructions
for building statistical machine translation system from
the PCEDT or any other parallel corpus. The system uses
models GIZA++ and ISI ReWrite decoder (Germann et
al., 2001).
TrEd is a graphical editor and viewer of tree structures.
Its modular architecture allows easy handling of diverse
annotation schemes, it has been used as the principal an-
notation environment for the PDT and PCEDT.
Netgraph is a multi-platform client-server application
for browsing, querying and viewing analytical and tec-
togrammatical dependency trees, either over the Internet
or locally.
7 Conclusion
We have described the process of building the first ver-
sion of a parallel treebank for two relatively distant lan-
guages, Czech and English, during which we have also
attempted to reconcile two fairly incompatible linguistic
theories used for their description.
The resulting data collection contains data syntacti-
cally annotated on several layers of analysis. There have
already been experimental machine translation systems
MAGENTA (Hajic? et al, 2002) and DBMT ( ?Cmejrek
et al, 2003) confirming the exploitability of the corpus
and showing that we are capable of performing auto-
matic transformations from phrase structures to depen-
dency representation with an acceptable, though still not
impeccable quality.
However, for both languages, we have presented ex-
amples of phenomena, for which the ?native? annotation
scheme does not provide a sufficiently fine-grained anal-
ysis. In such cases, automatic conversion between anno-
tation schemes is not possible, and the less we can hope
for successfull machine translation.
The question of enhancing the annotation schemes to
allow for a lossless transformation between them remains
still open, and its difficulty presents a yet unfathomed
depth.
8 Acknowledgements
This research was supported by the following
grants: M?SMT ?CR Grants No. LN00A063, No.
MSM113200006, and NSF Grant No. IIS-0121285.
References
Alena Bo?hmova?. 2001. Automatic procedures in tec-
togrammatical tagging. The Prague Bulletin of Math-
ematical Linguistics, 76.
Eugene Charniak. 1999. A maximum-entropy-inspired
parser. Technical Report CS-99-12.
Michael Collins, Jan Hajic?, Lance Ramshaw, and
Christoph Tillmann. 1999. A Statistical Parser for
Czech. In Proceedings of the 37th Annual Meeting of
the Association for Computational Linguistics, College
Park, Maryland.
Ulrich Germann, Michael Jahr, Kevin Knight, Daniel
Marcu, and Kenji Yamada. 2001. Fast decoding and
optimal decoding for machine translation. In Proceed-
ings of the 39th Annual Meeting of the Association for
Computational Linguistics, pages 228?235.
Jan Hajic? and Barbora Hladka?. 1998. Tagging Inflec-
tive Languages: Prediction of Morphological Cate-
gories for a Rich, Structured Tagset. In Proceedings of
COLING-ACL Conference, pages 483?490, Montreal,
Canada.
Jan Hajic?, Martin ?Cmejrek, Bonnie Dorr, Yuan Ding, Ja-
son Eisner, Daniel Gildea, Terry Koo, Kristen Parton,
Gerald Penn, Dragomir Radev, and Owen Rambow.
2002. Natural Language Generation in the Context of
Machine Translation. Technical report. NLP WS?02
Final Report.
Ivona Kuc?erova? and Zdene?k ?Zabokrtsky?. 2002. Trans-
forming Penn Treebank Phrase Trees into (Praguian)
Tectogrammatical Dependency Trees. Prague Bulletin
of Mathematical Linguistics, 78:77?94.
Linguistic Data Consortium. 1999. Penn Treebank 3.
LDC99T42.
Linguistic Data Consortium. 2001. Prague Dependency
Treebank 1. LDC2001T10.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993. Building a Large Annotated Corpus of En-
glish: The Penn Treebank. Computational Linguistics,
19(2):313?330.
I. Dan Melamed. 1996. A geometric approach to map-
ping bitext correspondence. In Proceedings of the First
Conference on Empirical Methods in Natural Lan-
guage Processing.
G. Minnen, J. Carroll, and D. Pearce. 2001. Applied
Morphological Processing of English. Natural Lan-
guage Engineering, 7(3):207?223.
Franz Josef Och and Hermann Ney. 2003. A Systematic
Comparison of Various Statistical Alignment Models.
Computational Linguistics, 29(1):19?51.
Petr Sgall, Eva Hajic?ova?, and Jarmila Panevova?. 1986.
The Meaning of the Sentence and Its Semantic and
Pragmatic Aspects. Academia/Reidel Publishing
Company, Prague, Czech Republic/Dordrecht, Nether-
lands.
Martin ?Cmejrek, Jan Cur???n, and Jir??? Havelka. 2003.
Czech-English Dependency-based Machine Transla-
tion. In Proceedings of the 10th Conference of The Eu-
ropean Chapter of the Association for Computational
Linguistics, pages 83?90, Budapest, Hungary, April.
Zdene?k ?Zabokrtsky?, Petr Sgall, and Dz?eroski Sas?o. 2002.
Machine Learning Approach to Automatic Functor As-
signment in the Prague Dependency Treebank. In Pro-
ceedings of LREC 2002, volume V, pages 1513?1520,
Las Palmas de Gran Canaria, Spain.
Fei Xia and Martha Palmer. 2001. Converting depen-
dency structures to phrase structures. In Proceedings
of HLT 2001, First International Conference on Hu-
man Language Technology Research, San Francisco.
