  
CRF-based Hybrid Model for Word Segmentation, NER and even 
POS Tagging 
Zhiting Xu, Xian Qian, Yuejie Zhang,  Yaqian Zhou 
Department of Computer Science & Engineering, 
Shanghai Key Laboratory of Intelligent Information Processing, 
Fudan University, Shanghai 200433, P. R. China 
 {zhiting, qianxian, yjzhang, zhouyaqian}@fudan.edu.cn 
 
  
Abstract 
This paper presents systems submitted to 
the close track of Fourth SIGHAN Bakeoff. 
We built up three systems based on Condi-
tional Random Field for Chinese Word 
Segmentation, Named Entity Recognition 
and Part-Of-Speech Tagging respectively. 
Our systems employed basic features as 
well as a large number of linguistic features. 
For segmentation task, we adjusted the BIO 
tags according to confidence of each char-
acter. Our final system achieve a F-score of 
94.18 at CTB, 92.86 at NCC, 94.59 at SXU 
on Segmentation, 85.26 at MSRA on 
Named Entity Recognition, and 90.65 at 
PKU on Part-Of-Speech Tagging. 
1 Introduction 
Fourth SIGHAN Bakeoff includes three tasks, that 
is, Word Segmentation, Named Entity Recognition 
(NER) and Part-Of-Speech (POS) Tagging. In the 
POS Tagging task, the testing corpora are pre-
segmented. Word Segmentation, NER and POS 
Tagging could be viewed as classification prob-
lems. In a Segmentation task, each character 
should be classified into three classes, B, I, O, in-
dicating whether this character is the Beginning of 
a word, In a word or Out of a word. For NER, each 
character is assigned a tag indicating what kind of 
Named Entity (NE) this character is (Beginning of 
a Person Name (PN), In a PN, Beginning of a Lo-
cation Name (LN), In a LN, Beginning of an Or-
ganization Name (ON), In an ON or not-a-NE). In 
POS tagging task defined by Fourth SIGHAN Ba-
keoff, we only need to give a POS tag for each 
given word in a context. 
We attended the close track of CTB, NCC, SXU 
on Segmentation, MSRA on NER and PKU on 
POS Tagging. In the close track, we cannot use 
any external resource, and thus we extracted sev-
eral word lists from training corpora to form multi-
ple features beside basic features. Then we trained 
CRF models based on these feature sets. In CRF 
models, a margin of each character can be gotten, 
and the margin could be considered as the confi-
dence of that character. For the Segmentation task, 
we performed the Maximum Probability Segmen-
tation first, through which each character is as-
signed a BIO tag (B represents the Beginning of a 
word, I represents In a word and O represents Out 
of a word). If the confidence of a character is lower 
than the threshold, the tag of that character will be 
adjusted to the tag assigned by the Maximum 
Probability Segmentation (R. Zhang et al, 2006). 
2 Conditional Random Fields 
Conditional Random Fields (CRFs) are a class of 
undirected graphical models with exponent distri-
bution (Lafferty et al, 2001). A common used spe-
cial case of CRFs is linear chain, which has a dis-
tribution of: 
)),,,(exp(1)|(
1
1??
=
?? =
T
t k
ttkk
x
txyyf
Z
xyP rrr
r
?  (1) 
where ),,( 1 txyyf ttk
r
? is a function which is usu-
ally an indicator function; k?  is the learned weight 
of feature kf ; and xZ r is the normalization factor. 
The feature function actually consists of two kinds 
of features, that is, the feature of single state and 
the feature of transferring between states. Features 
will be discussed in section 3. 
167
Sixth SIGHAN Workshop on Chinese Language Processing
  
Several methods (e.g. GIS, IIS, L-BFGS) could 
be used to estimate k? , and L-BFGS has been 
showed to converge faster than GIS and IIS. To 
build up our system, we used Pocket CRF1. 
3 Feature Representation 
We used three feature sets for three tasks respec-
tively, and will describe them respectively. 
3.1 Word Segmentation 
We mainly adopted features from (H. T. Ng et al, 
2004, Y. Shi et al, 2007), as following: 
a) Cn(n=-2, -1, 0, 1, 2) 
b) CnCn+1(n=-2,-1,0,1) 
c) C-1C1 
d) CnCn+1Cn+2 (n=-1, 0, 1) 
e) Pu(C0) 
f) T(C-2)T(C-1)T(C0)T(C1)T(C2) 
g) LBegin(C0), Lend(C0) 
h) Single(C0) 
where C0 represents the current character and Cn 
represents the nst character from the current charac-
ter. Pu(C0) indicates whether current word is a 
punctuation. this feature template helps to indicate 
the end of a sentence. T(C) represents the type of 
character C. There are four types we used: (1) Chi-
nese Number (??/one?, ??/two?, ??/ten?); (2) 
Chinese Dates (??/day?, ??/month?, ??/year?); 
(3) English letters; and (4) other characters. The (f) 
feature template is used to recognize the Chinese 
dates for the construction of Chinese dates may 
cause the sparseness problem. LBegin(C0) represents 
the maximum length of the word beginning with 
the character C0, and Lend(C0) presents the maxi-
mum length of the word ending with the character 
C0. The (g) feature template is used to decide the 
boundary of a word. Single(C0) shows whether cur-
rent character can form a word solely. 
3.2 Named Entity Recognition 
Most features described in (Y. Wu et al, 2005) are 
used in our systems. Specifically, the following is 
the feature templates we used: 
a) Surname(C0): Whether current character is in 
a Surname List, which includes all first char-
acters of PNs in the training corpora. 
                                                 
1 
http://sourceforge.net/project/showfiles.php?group_id=201943 
b) PersonName(C0C1C2, C0C1): Whether C0C1C2, 
C0C1 is in the Person Name List, which con-
tains all PNs in the training corpora. 
c) PersonTitle(C-2C-1): Whether C-2C-1 is in the 
Person Title List, which is extracted from the 
previous two characters of each PN in the 
training corpora. 
d) LocationName(C0C1,C0C1C2,C0C1C2C3): 
Whether C0C1,C0C1C2,C0C1C2C3 is in the Lo-
cation Name List, which includes all LNs in 
the training corpora. 
e) LocationSuffix(C0): Whether current character 
is in the Location Suffix List, which is con-
structed using the last character of each LN in 
the training corpora. 
f) OrgSuffix(C0): Whether current character is in 
the Organization Suffix List, which contains 
the last-two-character of each ON in the train-
ing corpora. 
3.3 Part-Of-Speech Tagging 
We employed part of feature templates described 
in (H. T. Ng et al, 2004, Y. Shi et al, 2007). Since 
we are in the close track, we cannot use morpho-
logical features from external resources such as 
HowNet, and we used features that are available 
just from the training corpora. 
a) Wn, (n=-2,-1,0,1,2) 
b) WnWn+1, (n=-2,-1,0,1) 
c) W-1W1 
d) Wn-1WnWn+1 (n=-1, 1) 
e) Cn(W0) (n=0,1,2,3) 
f) Length(W0) 
where Cn represents the nth character of the current 
word, and Length(W0) indicates the length of the 
current word. 
4 Reliability Evaluation 
In the task of Word Segmentation, the label of each 
character is adjusted according to their reliability. 
For each sentence, we perform Maximum Prob-
ability Segmentation first, through which we can 
get a BIO tagging for each character in the sen-
tence. 
After that, the features are extracted according 
to the feature templates, and the weight of each 
feature has already been estimated in the step of 
training. Then marginal probability for each char-
acter can be computed as follows: 
168
Sixth SIGHAN Workshop on Chinese Language Processing
  
)),(exp(
)(
1)|( yxf
xZ
xyp ii
rr ?=     (2) 
The value of )|( xyp
r
 becomes the original re-
liability value of BIO label y for the current char-
acter under the current contexts. If the probability 
of y  with the largest probability is lower than 0.75, 
which is decided according to the experiment re-
sults, the tag given by Maximum Probability Seg-
mentation will be used instead of tag given by CRF. 
The motivation of this method is to use the Maxi-
mum Probability method to enhance the F-measure 
of In-Vocabulary (IV) Words. According to the 
results reported in (R. Zhang et al, 2006), CRF 
performs relatively better on Out-of-Vocabulary 
(OOV) words while Maximum Probability per-
forms well on IV words, so a model combining the 
advantages of these two methods is appealing. One 
simplest way to combine them is the method we 
described. Besides, there are some complex meth-
ods, such as estimation using Support Vector Ma-
chine (SVM) for CRF, CRF combining boosting 
and combining Margin Infused Relaxed Algorithm 
(MIRA) with CRF, that might perform better. 
However, we did not have enough time to imple-
ment these methods, and we will compare them 
detailedly in the future work. 
5 Experiments 
5.1 Results on Fourth SIGHAN Bakeoff 
We participated in the close track on Word Seg-
mentation on CTB, NCC and SXU corpora, NER 
on MSRA corpora and POS Tagging on PKU cor-
pora. 
For Word Segmentation and NER, our memory 
was enough to use all features. However, for POS 
tagging, we did not have enough memory to use all 
features, and we set a frequency cutoff of 10; that 
is, we could only estimate variables for those fea-
tures that occurred more than ten times. 
Our results of Segmentation are listed in the Ta-
bel 1, the results of NER are listed in the Tabel 2, 
and the results of POS Tagging are listed in the 
Tabel 3. 
 R P F Roov Riv 
CTB 0.9459 0.9418 0.9439 0.6589 0.9628 
NCC 0.9396 0.9286 0.9341 0.5007 0.9614 
SXU 0.9554 0.9459 0.9507 0.6206 0.9735 
Tabel 1. Results of Word Segmentation 
MSRA P R F 
PER 0.8084 0.8557 0.8314 
LOC 0.9138 0.8576 0.8848 
ORG 0.8666 0.773 0.8171 
Overall 0.873 0.8331 0.8526 
Tabel 2. Results of NER 
 
 Total-A IV-R OOV-R MT-R 
PKU 0.9065 0.9259 0.5836 0.8903 
Tabel 3. Results of POS Tagging 
5.2 Errors Analysis 
Observing our results of Word Segmentation and 
POS Tagging, we found that the recall of OOV is 
relatively low, this may be improved through in-
troducing features aiming to enhance the perform-
ance of OOV.  
On NER task, we noticed that precision of PN 
recognition is relative low, and we found that our 
system may classify some ONs as PNs, such as ??
??(Guinness)/ORG? and ?????(World Re-
cord)/)?. Besides, the bound of PN is sometimes 
confusing and may cause problems. For example, 
???/PER ?/ ?/ ??? may be segmented as 
????/PER ?/ ???. Further, some words be-
ginning with Chinese surname, such as ????
??, may be classified as PN.  
For List may not be the real suffix. For example, 
?????? should be a LN, but it is very likely 
that ????? is recognized as a LN for its suffix 
???.  Another problem involves the characters in 
the Location Name list may not a LN all the time. 
In the context ???/ ??/?, for example, ??? 
means Chinese rather than China.  
For ONs, the correlative dictionary also exists. 
Consider sequence ??????, which should be a 
single word, ???? is in the Organization Name 
List and thus it is recognized as an ON in our sys-
tem. Another involves the subsequence of a word. 
For example, the sequence ?????????
??, which should be a person title, but ?????
????? is an ON. Besides, our recall of ON is 
low for the length of an ON could be very long. 
6 Conclusions and Future Works 
We built up our systems based on the CRF model 
and employed multiple linguistics features based 
on the knowledge extracted from training corpora. 
169
Sixth SIGHAN Workshop on Chinese Language Processing
  
We found that these features could greatly improve 
the performance of all tasks. Besides, we adjusted 
the tag of segmentation result according to the reli-
ability of each character, which also helped to en-
hance the performance of segmentation.  
As many other NLP applications, feature plays a 
very important role in sequential labeling tasks. In 
our POS tagging task, we could only use features 
with high frequency, but some low-frequency fea-
tures may also play a vital role in the task; good 
non-redundant features could greatly improve clas-
sification performance while save memory re-
quirement of classifiers. In our further research, we 
will focus on feature selection on CRFs. 
Acknowledgement 
This research was sponsored by National Natural 
Science Foundation of China (No. 60773124, No. 
60503070). 
References 
O. Bender, F. J. Och, and H. Ney. 2003. Maximum En-
tropy Models for Named Entity Recognition. Pro-
ceeding of CoNLL-2003. 
A. L. Berger, S. A. Della Pietra, and V. J. Della Pietra. 
1996. A Maximum Entropy Approach to Natural 
Language Processing. Computational Linguistics, 
22(1). 
H. L. Chieu, H. T. Ng. 2002. Named Entity Recognition: 
A Maximum Entropy Approach Using Global Infor-
mation. International Conference on Computational 
Linguistics (COLING). 
J. N. Darroch and D. Ratcliff. 1972. Generalized Itera-
tive Scaling for Log-Linear Models. The Annals of 
Mathematical Statistics, 43(5). 
J. Lafferty, A McCallum, and F. Pereira..2001. Condi-
tional Random Fields: Probabilistic Models for Seg-
menting and Labeling Sequence Data. In Proceed-
ings of the 18th International Conf. on Machine 
Learning (ICML). 
R. Li, J. Wang, X. Chen, X. Tao, and Y. Hu. 2004. Us-
ing Maximum Entropy Model for Chinese Text 
Categorization. Computer Research and Develop-
ment, 41(4). 
H. T. Ng and J. K. Low. 2004. Chinese Part-Of-Speech 
Tagging: One-at-a-Time or All-at-Once? Word-Base 
or Character-Based? Proceedings of Conference on 
Empirical Methods in Natural Language Processing 
(EMNLP). 
A. Ratnaparkhi. 1997. A Simple Introduction to Maxi-
mum Entropy Models for Natural Language Process-
ing. Institute for Research in Cognitive Science Re-
port, 97(8). 
F. Sha and F.Pereira. 2003. Shallow parsing with condi-
tional random fields. In Proceedings of HLT-NAACL. 
Y. Shi and M. Wang. 2007. A Dual-Layer CRFs Based 
Joint Decoding Method for Cascaded Segmentation 
and Labeling Tasks. In International Joint Confer-
ences on Artificial Intelligence (IJCAI). 
C. A. Sutton, K. Rohanimanesh, A. McCallum. 2004. 
Dynamic conditional random fields: factorized prob-
abilistic models for labeling and segmenting se-
quence data. In International Conference on Machine 
Learning (ICML). 
M. Volk, and S. Clematide. 2001. Learn - Filter - Apply 
-- Forget Mixed Approaches to Named Entity Rec-
ognition. Proceeding of the 6th International Work-
shop on Applications of Natural Language for Infor-
mation Systems. 
Y. Wu, J. Zhao, B. Xu and H. Yu. 2005. Chinese 
Named Entity Recognition Based on Multiple Fea-
tures. Proceedings of Human Language Technology 
Conference and Conference on Empirical Methods in 
Natural Language Processing (HLT/EMNLP). 
H. Zhang, Q. Liu, H. Zhang, and X. Cheng. 2002. Au-
tomatic Recognition of Chinese Unknown Words 
Based on Roles Tagging. Proceeding of the 19th In-
ternational Conference on Computational Linguistics. 
R. Zhang, G. Kikui and E. Sumita. 2006. Subword-
based tagging by conditional random fields for Chi-
neseword segmentation. Companion volume to the-
proceedings of the North American chapter of the 
Association for Computational Linguistics (NAACL). 
Y. Zhou, Y. Guo, X. Huang, and L. Wu. 2003. Chinese 
and English BaseNP Recognition Based on a Maxi-
mum Entropy Model. Journal of Computer Research 
and Development, 40(3). 
 
170
Sixth SIGHAN Workshop on Chinese Language Processing
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 904?912,
Beijing, August 2010
2D Trie for Fast Parsing
Xian Qian, Qi Zhang, Xuanjing Huang, Lide Wu
Institute of Media Computing
School of Computer Science, Fudan University
{xianqian, qz, xjhuang, ldwu}@fudan.edu.cn
Abstract
In practical applications, decoding speed
is very important. Modern structured
learning technique adopts template based
method to extract millions of features.
Complicated templates bring about abun-
dant features which lead to higher accu-
racy but more feature extraction time. We
propose Two Dimensional Trie (2D Trie),
a novel efficient feature indexing structure
which takes advantage of relationship be-
tween templates: feature strings generated
by a template are prefixes of the features
from its extended templates. We apply
our technique to Maximum Spanning Tree
dependency parsing. Experimental results
on Chinese Tree Bank corpus show that
our 2D Trie is about 5 times faster than
traditional Trie structure, making parsing
speed 4.3 times faster.
1 Introduction
In practical applications, decoding speed is very
important. Modern structured learning technique
adopts template based method to generate mil-
lions of features. Such as shallow parsing (Sha
and Pereira, 2003), named entity recognition
(Kazama and Torisawa, ), dependency parsing
(McDonald et al, 2005), etc.
The problem arises when the number of tem-
plates increases, more features generated, mak-
ing the extraction step time consuming. Espe-
cially for maximum spanning tree (MST) depen-
dency parsing, since feature extraction requires
quadratic time even using a first order model. Ac-
cording to Bohnet?s report (Bohnet, 2009), a fast
FeatureGenerationTemplate:p .word+p .pos0 0 Feature:lucky/ADJ
Index:3228~3233
FeatureRetrievalParse Tree
Build lattice, inference etc.
Figure 1: Flow chart of dependency parsing.
p0.word, p0.pos denotes the word and POS tag
of parent node respectively. Indexes correspond
to the features conjoined with dependency types,
e.g., lucky/ADJ/OBJ, lucky/ADJ/NMOD, etc.
feature extraction beside of a fast parsing algo-
rithm is important for the parsing and training
speed. He takes 3 measures for a 40X speedup,
despite the same inference algorithm. One impor-
tant measure is to store the feature vectors in file
to skip feature extraction, otherwise it will be the
bottleneck.
Now we quickly review the feature extraction
stage of structured learning. Typically, it consists
of 2 steps. First, features represented by strings
are generated using templates. Then a feature in-
dexing structure searches feature indexes to get
corresponding feature weights. Figure 1 shows
the flow chart of MST parsing, where p0.word,
p0.pos denote the word and POS tag of parent
node respectively.
We conduct a simple experiment to investi-
gate decoding time of MSTParser, a state-of-the-
art java implementation of dependency parsing 1.
Chinese Tree Bank 6 (CTB6) corpus (Palmer and
1http://sourceforge.net/projects/mstparser
904
Step Feature Index Other Total
Generation Retrieval
Time 300.27 61.66 59.48 421.41
Table 1: Time spent of each step (seconds) of
MSTParser on CTB6 standard test data (2660 sen-
tences). Details of the hardware and corpus are
described in section 5
Xue, 2009) with standard train/development/test
split is used for evaluation. Experimental results
are shown in Table 1. The observation is that time
spent of inference is trivial compared with feature
extraction. Thus, speeding up feature extraction is
critical especially when large template set is used
for high accuracy.
General indexing structure such as Hash and
Trie does not consider the relationships between
templates, therefore they could not speed up fea-
ture generation, and are not completely efficient
for searching feature indexes. For example, fea-
ture string s1 generated by template ?p0.word?
is prefix of feature s2 from template ?p0.word +
c0.word? (word pair of parent and child), hence
index of s1 could be used for searching s2. Fur-
ther more, if s1 is not in the feature set, then s2
must be absent, its generation can be skipped.
We propose Two Dimensional Trie (2D Trie),
a novel efficient feature indexing structure which
takes advantage of relationship between tem-
plates. We apply our technique to Maximum
Spanning Tree dependency parsing. Experimental
results on CTB6 corpus show that our 2D Trie is
about 5 times faster than traditional Trie structure,
making parsing speed 4.3 times faster.
The paper is structured as follows: in section 2,
we describe template tree which represents rela-
tionship between templates; in section 3, we de-
scribe our new 2D Trie structure; in section 4, we
analyze the complexity of the proposed method
and general string indexing structures for parsing;
experimental results are shown in section 5; we
conclude the work in section 6.
2 Template tree
2.1 Formulation of template
A template is a set of template units which are
manually designed: T = {t1, . . . , tm}. For con-
Unit Meaning
p?i/pi the ith node left/right to parent node
c?i/ci the ith node left/right to child node
r?i/ri the ith node left/right to root node
n.word word of node n
n.pos POS tag of node n
n.length word length of node n
|l conjoin current feature with linear distance
between child node and parent node
|d conjoin current feature with direction of de-
pendency (left/right)
Table 2: Template units appearing in this paper
venience, we use another formulation: T = t1 +
. . .+tm. All template units appearing in this paper
are described in Table 2, most of them are widely
used. For example, ?T = p0.word + c0.word|l ?
denotes the word pair of parent and child nodes,
conjoined with their distance.
2.2 Template tree
In the rest of the paper, for simplicity, let si be a
feature string generated by template Ti.
We define the relationship between templates:
T1 is the ancestor of T2 if and only T1 ? T2, and
T2 is called the descendant of T1. Recall that,
feature string s1 is prefix of feature s2. Suppose
T3 ? T1 ? T2, obviously, the most efficient way
to look up indexes of s1, s2, s3 is to search s3 first,
then use its index id3 to search s1, and finally use
id1 to search s2. Hence the relationship between
T2 and T3 can be neglected.
Therefore we define direct ancestor of T1: T2
is a direct ancestor of T1 if T2 ? T1, and there is
no template T ? such that T2 ? T ? ? T1. Corre-
spondingly, T1 is called the direct descendant of
T2.
Template graph G = (V,E) is a directed graph
that represents the relationship between templates,
where V = {T1, . . . , Tn} is the template set, E =
{e1, . . . , eN} is the edge set. Edge from Ti to Tj
exists, if and only if Ti is the direct ancestor of
Tj . For templates having no ancestor, we add an
empty template as their common direct ancestor,
which is also the root of the graph.
The left part of Figure 2 shows a template
graph for templates T1 =p0.word, T2 =p0.pos ,
T3 =p0.word + p0.pos. In this example, T3 has 2
direct ancestors, but in fact s3 has only one prefix
905
p .word0
p .word +p pos0 0.
root
p .word0
root
p .pos0
p .pos0 p .pos0
Figure 2: Left graph shows template graph for
T1 =p0.word, T2 =p0.pos , T3 =p0.word +
p0.pos. Right graph shows the corresponding tem-
plate tree, where each vertex saves the subset of
template units that do not belong to its father
which depends on the order of template units in
generation step. If s3 = s1 + s2, then its prefix is
s1, otherwise its prefix is s2. In this paper, we sim-
ply use the breadth-first tree of the graph for dis-
ambiguation, which is called template tree. The
only direct ancestor T1 of T2 in the tree is called
father of T2, and T2 is a child of T1. The right
part of Figure 2 shows the corresponding template
tree, where each vertex saves the subset of tem-
plate units that do not belong to its father.
2.3 Virtual vertex
Consider the template tree in the left part of Figure
3, red vertex and blue vertex are partially over-
lapped, their intersection is p0.word, if string s
from template T =p0.word is absent in feature set,
then both nodes can be neglected. For efficiently
pruning candidate templates, each vertex in tem-
plate tree is restricted to have exactly one template
unit (except root). Another important reason for
such restriction will be given in the next section.
To this end, virtual vertexes are created for
multi-unit vertexes. For efficient pruning, the new
virtual vertex should extract the most common
template unit. A natural goal is to minimize the
creation number. Here we use a simple greedy
strategy, for the vertexes sharing a common fa-
ther, the most frequent common unit is extracted
as new vertex. Virtual vertexes are iteratively cre-
ated in this way until all vertexes have one unit.
The final template tree is shown in the right part of
Figure 3, newly created virtual vertexes are shown
in dashed circle.
root
p .word+p .word+p .word-1 01
p .word+p pos0 0. c .word+c pos0 0.
root
p .word0
p .pos0 p .word-1
p .word1
c .word0
c .pos0
Figure 3: Templates that are partially overlapped:
Tred ? Tblue =p0.word, virtual vertexes shown in
dashed circle are created to extract the common
unit
root
p .word0
p .pos0
parse tag
VV NN... ... ... ...
.........
Level 0
Level 1
Level 2 VV ...
Figure 4: 2D Trie for single template, alphabets at
level 1 and level 2 are the word set, POS tag set
respectively
3 2D Trie
3.1 Single template case
Trie stores strings over a fixed alphabet, in our
case, feature strings are stored over several alpha-
bets, such as word list, POS tag list, etc. which are
extracted from training corpus.
To illustrate 2D Trie clearly, we first consider a
simple case, where only one template used. The
template tree degenerates to a sequence, we could
use a Trie like structure for feature indexing, the
only difference from traditional Trie is that nodes
at different levels could have different alphabets.
One example is shown in Figure 4. There are 3
feature strings from template ?p0.word + p0.pos?:
{parse/VV, tag/VV, tag/VV}. Alphabets at level
1 and level 2 are the word set, POS tag set re-
spectively, which are determined by correspond-
ing template vertexes.
As mentioned before, each vertex in template
tree has exactly one template unit, therefore, at
each level, we look up an index of a word or POS
906
HehadbeenasalesandmarketingexecutivewithChryslerfor20years
PRPVBDVBNDTNNSCCNNNNINNNPINCDNNS
2648273111210411506406313374192360231560220300566778
21272804130112120613060214
Figure 5: Look up indexes of words and POS tags
beforehand.
tag in sentence, not their combinations. Hence the
number of alphabets is limited, and all the indexes
could be searched beforehand for reuse, as shown
in Figure 5, the token table is converted to a in-
dex table. For example, when generating features
at position i of a sentence, template ?r0.word +
r1.word? requires index of i+1th word in the sen-
tence, which could be reused for generation at po-
sition i+ 1.
3.2 General case
Generally, for vertex in template tree with K chil-
dren, children of corresponding Trie node are ar-
ranged in a matrix of K rows and L columns, L
is the size of corresponding alphabet. If the vertex
is not virtual, i.e., it generates features, one more
row is added at the bottom to store feature indexes.
Figure 6 shows the 2D Trie for a general template
tree.
3.3 Feature extraction
When extracting features for a pair of nodes in a
sentence, template tree and 2D Trie are visited in
breath first traversal order. Each time, an alpha-
bet and a token index j from index table are se-
lected according to current vertex. For example,
POS tag set and the index of the POS tag of par-
ent node are selected as alphabet and token index
respectively for vertex ?p0.pos?. Then children in
the jth column of the Trie node are visited, valid
children and corresponding template vertexes are
saved for further retrieval or generate feature in-
dexes if the child is at the bottom and current Trie
node is not virtual. Two queues are maintained to
been......
... ......
...
VBNp .word+p .pos?been/VBN0 0...
... ......
p .word?been0... ...
root
root
p .word0p .pos0 c .word0
had ......
...
p .word?had0 ...
VBDp .word+p .pos?had/VBD0 0...
... ......
Hep .word+w .wordhad/He0 0?...
...
nmod vmodobj sub
Featureindex array-1 -13327 2510
nmod vmodobj sub-1 7821-1 -1............ ......
...... beenp .word+w .word?had/been0 0 ...
...
invalid
Figure 6: 2D trie for a general template tree.
Dashed boxes are keys of columns, which are not
stored in the structure
save the valid children and Trie nodes. Details of
feature extraction algorithm are described in Al-
gorithm 1.
3.4 Implementation
When feature set is very large, space complexity
of 2D Trie is expensive. Therefore, we use Double
Array Trie structure (Aoe, 1989) for implementa-
tion. Since children of 2D Trie node are arranged
in a matrix, not an array, so each element of the
base array has a list of bases, not one base in stan-
dard structure. For children that store features,
corresponding bases are feature indexes. One ex-
ample is shown in Figure 7. The root node has
3 bases that point to three rows of the child ma-
trix of vertex ?p0.word? respectively. Number of
bases in each element need not to be stored, since
it can be obtained from template vertex in extrac-
tion procedure.
Building algorithm is similarly to Double Array
Trie, when inserting a Trie node, each row of the
child matrix is independently insert into base and
check arrays using brute force strategy. The inser-
907
been
... been
...
...
...
had
...
had had...
... ...
...
...
been ... had had... ... ... ...... ... been hadroot base1 base2
base3
root base 2base
1
base3
...
VBD... ...
VBN...
...
...VBD
VBN
...
base1
base1 base1
base1
-1 -1... ... 3327 2510 ... ......... ......... -1 7821-1 -1... ...
-1 -1... ... 3327 2510 ... ......... ......... -1 7821-1 -1... ...
Basearray
Feature index array
Feature index array
Figure 7: Build base array for 2D Trie in Figure 6. String in the box represents the key of the child.
Blank boxes are the invalid children. The root node has 3 bases that point to three rows of the child
matrix of vertex ?p0.word? respectively
Algorithm 1 Feature extraction using 2D Trie
Input: 2D Trie that stores features, template
tree, template graph, a table storing token in-
dexes, parent and child positions
Output: Feature index set S of dependency
from parent to child.
Create template vertex queue Q1 and Trie
node queue Q2. Push roots of template tree
and Trie into Q1, Q2 respectively. S = ?
while Q1 is not empty, do
Pop a template vertex T from Q1 and a Trie
node N from Q2. Get token index j from
index table according to T .
for i = 1 to child number of T
if child of N at row i column j is valid,
push it into Q2 and push the ith child
of T into Q1.
else
remove decedents of ith child of T
from template tree
end if
end for
if T is not virtual and the last child of N in
column j is valid
Enumerate dependency types, add
valid feature indexes to S
end if
end while
Return S.
tion repeats recursively until all features stored.
4 Complexity analysis
Let
? |T | = number of templates
? |t| = number of template units
? |V | = number of vertexes in template tree,
i.e, |t|+ number of virtual vertexes
? |F | = number of features
? l = length of sentence
? |f | = average length of feature strings
The procedure of 2D Trie for feature extraction
consists of 2 steps: tokens in string table are
mapped to their indexes, then Algorithm 1 is car-
ried out for all node pairs of sentence. In the first
step, we use double array Trie for efficient map-
ping. In fact, time spent is trivial compared with
step 2 even by binary search. The main time spent
of Algorithm 1 is the traversal of the whole tem-
plate tree, in the worst case, no vertexes removed,
so the time complexity of a sentence is l2|V |,
which is proportional to |V |. In other words, mini-
mizing the number of virtual vertexes is important
for efficiency.
For other indexing structures, feature genera-
tion is a primary step of retrieval. For each node
908
Structure Generation Retrieval
2D Trie l2|V |
Hash / Trie l2|t| l2|f ||T |
Binary Search l2|t| l2|T | log |F |
Table 3: Time complexity of different indexing
structures.
pair of sentence, |t| template units are processed,
including concatenations of tokens and split sym-
bols (split tokens in feature strings), boundary
check ( e.g, p?1.word is out of boundary for be-
ginning node of sentence). Thus the generation
requires l2|t| processes. Notice that, time spent of
each process varies on the length of tokens.
For feature string s with length |s|, if perfect
hashing technique is adopted for index retrieval, it
takes |s| calculations to get hash value and a string
comparison to check the string at the calculated
position. So the time complexity is proportional to
|s|, which is the same as Trie. Hence the total time
for a sentence is l2|f ||T |. If binary search is used
instead, log |F | string comparisons are required,
complexity for a sentence is l2|T | log |F |.
Time complexity of these structures is summa-
rized in Table 3.
5 Experiments
5.1 Experimental settings
We use Chinese Tree Bank 6.0 corpus for evalua-
tion. The constituency structures are converted to
dependency trees by Penn2Malt 2 toolkit and the
standard training/development/test split is used.
257 sentences that failed in the conversion were
removed, yielding 23316 sentences for training,
2060 sentences for development and 2660 sen-
tences for testing respectively.
Since all the dependency trees are projective,
a first order projective MST parser is naturally
adopted. Online Passive Aggressive algorithm
(Crammer et al, 2006) is used for fast training, 2
parameters, i.e, iteration number and C, are tuned
on development data. The quality of the parser is
measured by the labeled attachment score (LAS),
i.e., the percentage of tokens with correct head and
dependency type.
2http://w3.msi.vxu.se/ nivre/research/Penn2Malt.html
Group IDs #Temp. #Vert. #Feat. LAS
1 1-2 72 91 3.23M 79.55%
2 1-3 128 155 10.4M 81.38%
3 1-4 240 275 25.0M 81.97%
4 1-5 332 367 34.8M 82.44%
Table 5: Parsing accuracy and number of tem-
plates, vertexes in template tree, features in decod-
ing stage (zero weighted features are excluded) of
each group.
We compare the proposed structure with Trie
and binary search. We do not compare with per-
fect hashing, because it has the same complex-
ity as Trie, and is often used for large data base
retrieval, since it requires only one IO opera-
tion. For easy comparison, all feature indexing
structures and the parser are implemented with
C++. All experiments are carried out on a 64bit
linux platform (CPU: Intel(R) Xeon(R) E5405,
2.00GHz, Memory: 16G Bytes). For each tem-
plate set, we run the parser five times on test data
and the averaged parsing time is reported.
5.2 Parsing speed comparison
To investigate the scalability of our method, rich
templates are designed to generate large feature
sets, as shown in Table 4. All templates are orga-
nized into 4 groups. Each row of Table 5 shows
the details of a group, including parsing accu-
racy and number of templates, vertexes in tem-
plate tree, and features in decoding stage (zero
weighted features are excluded).
There is a rough trend that parsing accuracy
increases as more templates used. Though such
trend is not completely correct, the clear conclu-
sion is that, abundant templates are necessary for
accurate parsing.
Though algorithm described in section 2.3 for
minimizing the number of virtual vertexes is
heuristic, empirical results are satisfactory, num-
ber of newly created vertexes is only 10% as orig-
inal templates. The reason is that complex tem-
plates are often extended from simple ones, their
differences are often one or two template units.
Results of parsing time comparison are shown
in Table 6. We can see that though time com-
plexity of dynamic programming is cubic, pars-
ing time of all systems is consistently dominated
909
ID Templates
1 pi.word pi.pos pi.word+pi.pos
ci.word ci.pos ci.word+ci.pos (|i| ? 2)
pi.length pi.length+pi.pos
ci.length ci.length+ci.pos (|i| ? 1)
p0.length+c0.length|ld p0.length+c0.length+c0.pos|ld p0.length+p0.pos+c0.length|ld
p0.length+p0.pos+c0.pos|ld p0.pos+c0.length+c0.pos|ld p0.length+p0.pos+c0.length+c0.pos|ld
pi.length+pj .length+ck.length+cm.length|ld (|i|+ |j|+ |k|+ |m| ? 2)r0.word r?1.word+r0.word r0.word+r1.word
r0.pos r?1.pos+r0.pos r0.pos+r1.pos
2 pi.pos+cj .pos|d pi.word+cj .word|d pi.pos+cj .word+cj .pos|d
pi.word+pi.pos+cj .pos|d pi.word+pi.pos+cj .word|d pi.word+cj .word+cj .pos|d
pi.word+pi.pos+cj .word+cj .pos|d (|i|+ |j| = 0)
Conjoin templates in the row above with |l
3 Similar with 2 |i|+ |j| = 1
4 Similar with 2 |i|+ |j| = 2
5 pi.word + pj .word + ck.word|d pi.word + cj .word + ck.word|d
pi.pos + pj .pos + ck.pos|d pi.pos + cj .pos + ck.pos|d (|i|+ |j|+ |k| ? 2)
Conjoin templates in the row above with |l
pi.word + pj .word + pk.word + cm.word|d pi.word + pj .word + ck.word + cm.word|d
pi.word + cj .word + ck.word + cm.word|d
pi.pos + pj .pos + pk.pos + cm.pos|d pi.pos + pj .pos + ck.pos + cm.pos|d
pi.pos + cj .pos + ck.pos + cm.pos|d (|i|+ |j|+ |k|+ |m| ? 2)
Conjoin templates in the row above with |l
Table 4: Templates used in Chinese dependency parsing.
by feature extraction. When efficient indexing
structure adopted, i.e, Trie or Hash, time index re-
trieval is greatly reduced, about 4-5 times faster
than binary search. However, general structures
search features independently, their results could
not guide feature generation. Hence, feature gen-
eration is still time consuming. The reason is that
processing each template unit includes a series of
steps, much slower than one integer comparison
in Trie search.
On the other hand, 2D Trie greatly reduces the
number of feature generations by pruning the tem-
plate graph. In fact, no string concatenation oc-
curs when using 2D Trie, since all tokens are con-
verted to indexes beforehand. The improvement
is significant, 2D Trie is about 5 times faster than
Trie on the largest feature set, yielding 13.4 sen-
tences per second parsing speed, about 4.3 times
faster.
Space requirement of 2D Trie is about 2.1 times
as binary search, and 1.7 times as Trie. One possi-
ble reason is that column number of 2D Trie (e.g.
size of words) is much larger than standard double
array Trie, which has only 256 children, i.e, range
of a byte. Therefore, inserting a 2D Trie node is
more strict, yielding sparser double arrays.
5.3 Comparison against state-of-the-art
Recent works on dependency parsing speedup
mainly focus on inference, such as expected
linear time non-projective dependency parsing
(Nivre, 2009), integer linear programming (ILP)
for higher order non-projective parsing (Martins
et al, 2009). They achieve 0.632 seconds per sen-
tence over several languages. On the other hand,
Goldberg and Elhadad proposed splitSVM (Gold-
berg and Elhadad, 2008) for fast low-degree poly-
nomial kernel classifiers, and applied it to transi-
tion based parsing (Nivre, 2003). They achieve
53 sentences per second parsing speed on En-
glish corpus, which is faster than our results, since
transition based parsing is linear time, while for
graph based method, complexity of feature ex-
traction is quadratic. Xavier Llu??s et al (Llu??s
et al, 2009) achieve 8.07 seconds per sentence
speed on CoNLL09 (Hajic? et al, 2009) Chinese
Tree Bank test data with a second order graphic
model. Bernd Bohnet (Bohnet, 2009) also uses
second order model, and achieves 610 minutes on
CoNLL09 English data (2399 sentences, 15.3 sec-
ond per sentence). Although direct comparison
of parsing time is difficult due to the differences
in data, models, hardware and implementations,
910
Group Structure Total Generation Retrieval Other Memory sent/sec
Trie 87.39 63.67 10.33 13.39 402M 30.44
1 Binary Search 127.84 62.68 51.52 13.64 340M 20.81
2D Trie 39.74 26.29 13.45 700M 66.94
Trie 264.21 205.19 39.74 19.28 1.3G 10.07
2 Binary Search 430.23 212.50 198.72 19.01 1.2G 6.18
2D Trie 72.81 53.95 18.86 2.5G 36.53
Trie 620.29 486.40 105.96 27.93 3.2G 4.29
3 Binary Search 982.41 484.62 469.44 28.35 2.9G 2.71
2D Trie 146.83 119.56 27.27 5.9G 18.12
Trie 854.04 677.32 139.70 37.02 4.9G 3.11
4 Binary Search 1328.49 680.36 609.70 38.43 4.1G 2.00
2D Trie 198.31 160.38 37.93 8.6G 13.41
Table 6: Parsing time of 2660 sentences (seconds) on a 64bit linux platform (CPU: Intel(R) Xeon(R)
E5405, 2.00GHz, Memory: 16G Bytes). Title ?Generation? and ?Retrieval? are short for feature gen-
eration and feature index retrieval steps respectively.
System sec/sent
(Martins et al, 2009) 0.63
(Goldberg and Elhadad, 2008) 0.019
(Llu??s et al, 2009) 8.07
(Bohnet, 2009) 15.3
(Galley and Manning, 2009) 15.6
ours group1 0.015
ours group2 0.027
ours group3 0.055
ours group4 0.075
Table 7: Comparison against state of the art, di-
rect comparison of parsing time is difficult due to
the differences in data, models, hardware and im-
plementations.
these results demonstrate that our structure can
actually result in a very fast implementation of a
parser. Moreover, our work is orthogonal to oth-
ers, and could be used for other learning tasks.
6 Conclusion
We proposed 2D Trie, a novel feature indexing
structure for fast template based feature extrac-
tion. The key insight is that feature strings gener-
ated by a template are prefixes of the features from
its extended templates, hence indexes of searched
features can be reused for further extraction. We
applied 2D Trie to dependency parsing task, ex-
perimental results on CTB corpus demonstrate the
advantages of our technique, about 5 times faster
than traditional Trie structure, yielding parsing
speed 4.3 times faster, while using only 1.7 times
as much memory.
7 Acknowledgments
The author wishes to thank the anonymous
reviewers for their helpful comments. This
work was partially funded by 973 Program
(2010CB327906), The National High Technol-
ogy Research and Development Program of China
(2009AA01A346), Shanghai Leading Academic
Discipline Project (B114), Doctoral Fund of Min-
istry of Education of China (200802460066), and
Shanghai Science and Technology Development
Funds (08511500302).
References
Aoe, Jun?ichi. 1989. An efficient digital
search algorithm by using a double-array struc-
ture. IEEE Transactions on software andengineer-
ing, 15(9):1066?1077.
Bohnet, Bernd. 2009. Efficient parsing of syntactic
and semantic dependency structures. In Proceed-
ings of the Thirteenth Conference on Computational
Natural Language Learning (CoNLL 2009): Shared
Task, pages 67?72, Boulder, Colorado, June. Asso-
ciation for Computational Linguistics.
Crammer, Koby, Joseph Keshet, Shai Shalev-Shwartz,
and Yoram Singer. 2006. Online passive-aggressive
algorithms. In JMLR 2006.
911
Galley, Michel and Christopher D. Manning. 2009.
Quadratic-time dependency parsing for machine
translation. In Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL and the
4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP, pages 773?781,
Suntec, Singapore, August. Association for Compu-
tational Linguistics.
Goldberg, Yoav and Michael Elhadad. 2008. splitsvm:
Fast, space-efficient, non-heuristic, polynomial ker-
nel computation for nlp applications. In Proceed-
ings of ACL-08: HLT, Short Papers, pages 237?240,
Columbus, Ohio, June. Association for Computa-
tional Linguistics.
Hajic?, Jan, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Anto`nia Mart??, Llu??s
Ma`rquez, Adam Meyers, Joakim Nivre, Sebastian
Pado?, Jan S?te?pa?nek, Pavel Stran?a?k, Mihai Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The conll-
2009 shared task: Syntactic and semantic dependen-
cies in multiple languages. In Proceedings of the
Thirteenth Conference on Computational Natural
Language Learning (CoNLL 2009): Shared Task,
pages 1?18, Boulder, Colorado, June. Association
for Computational Linguistics.
Kazama, Jun?ichi and Kentaro Torisawa. A new per-
ceptron algorithm for sequence labeling with non-
local features. In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), pages 315?324.
Llu??s, Xavier, Stefan Bott, and Llu??s Ma`rquez. 2009.
A second-order joint eisner model for syntactic and
semantic dependency parsing. In Proceedings of the
Thirteenth Conference on Computational Natural
Language Learning (CoNLL 2009): Shared Task,
pages 79?84, Boulder, Colorado, June. Association
for Computational Linguistics.
Martins, Andre, Noah Smith, and Eric Xing. 2009.
Concise integer linear programming formulations
for dependency parsing. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL
and the 4th International Joint Conference on Natu-
ral Language Processing of the AFNLP, pages 342?
350, Suntec, Singapore, August. Association for
Computational Linguistics.
McDonald, Ryan, Koby Crammer, and Fernando
Pereira. 2005. Online large-margin training of de-
pendency parsers. In Proceedings of the 43rd An-
nual Meeting of the Association for Computational
Linguistics, pages 91?97. Association for Computa-
tional Linguistics.
Nivre, Joakim. 2003. An efficient algorithm for
projective dependency parsing. In Proceedings of
the 11th International Conference on Parsing Tech-
niques, pages 149?160.
Nivre, Joakim. 2009. Non-projective dependency
parsing in expected linear time. In Proceedings of
the Joint Conference of the 47th Annual Meeting of
the ACL and the 4th International Joint Conference
on Natural Language Processing of the AFNLP,
pages 351?359, Suntec, Singapore, August. Asso-
ciation for Computational Linguistics.
Palmer, Martha and Nianwen Xue. 2009. Adding se-
mantic roles to the Chinese Treebank. Natural Lan-
guage Engineering, 15(1):143?172.
Sha, Fei and Fernando Pereira. 2003. Shallow pars-
ing with conditional random fields. In Proceedings
of the 2003 Human Language Technology Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics, pages 134?141,
May.
912
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 187?195,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Joint Training and Decoding Using Virtual Nodes for Cascaded
Segmentation and Tagging Tasks
Xian Qian, Qi Zhang, Yaqian Zhou, Xuanjing Huang, Lide Wu
School of Computer Science, Fudan University
825 Zhangheng Road, Shanghai, P.R.China
{qianxian, qz, zhouyaqian, xjhuang, ldwu}@fudan.edu.cn
Abstract
Many sequence labeling tasks in NLP require
solving a cascade of segmentation and tag-
ging subtasks, such as Chinese POS tagging,
named entity recognition, and so on. Tradi-
tional pipeline approaches usually suffer from
error propagation. Joint training/decoding in
the cross-product state space could cause too
many parameters and high inference complex-
ity. In this paper, we present a novel method
which integrates graph structures of two sub-
tasks into one using virtual nodes, and per-
forms joint training and decoding in the fac-
torized state space. Experimental evaluations
on CoNLL 2000 shallow parsing data set and
Fourth SIGHAN Bakeoff CTB POS tagging
data set demonstrate the superiority of our
method over cross-product, pipeline and can-
didate reranking approaches.
1 Introduction
There is a typical class of sequence labeling tasks
in many natural language processing (NLP) applica-
tions, which require solving a cascade of segmenta-
tion and tagging subtasks. For example, many Asian
languages such as Japanese and Chinese which
do not contain explicitly marked word boundaries,
word segmentation is the preliminary step for solv-
ing part-of-speech (POS) tagging problem. Sen-
tences are firstly segmented into words, then each
word is assigned with a part-of-speech tag. Both
syntactic parsing and dependency parsing usually
start with a textual input that is tokenized, and POS
tagged.
The most commonly approach solves cascaded
subtasks in a pipeline, which is very simple to im-
plement and allows for a modular approach. While,
the key disadvantage of such method is that er-
rors propagate between stages, significantly affect-
ing the quality of the final results. To cope with this
problem, Shi and Wang (2007) proposed a rerank-
ing framework in which N-best segment candidates
generated in the first stage are passed to the tag-
ging model, and the final output is the one with the
highest overall segmentation and tagging probabil-
ity score. The main drawback of this method is that
the interaction between tagging and segmentation is
restricted by the number of candidate segmentation
outputs. Razvan C. Bunescu (2008) presented an
improved pipeline model in which upstream subtask
outputs are regarded as hidden variables, together
with their probabilities are used as probabilistic fea-
tures in the downstream subtasks. One shortcom-
ing of this method is that calculation of marginal
probabilities of features may be inefficient and some
approximations are required for fast computation.
Another disadvantage of these two methods is that
they employ separate training and the segmentation
model could not take advantages of tagging infor-
mation in the training procedure.
On the other hand, joint learning and decoding
using cross-product of segmentation states and tag-
ging states does not suffer from error propagation
problem and achieves higher accuracy on both sub-
tasks (Ng and Low, 2004). However, two problems
arises due to the large state space, one is that the
amount of parameters increases rapidly, which is apt
to overfit on the training corpus, the other is that
the inference by dynamic programming could be in-
efficient. Sutton (2004) proposed Dynamic Con-
ditional Random Fields (DCRFs) to perform joint
training/decoding of subtasks using much fewer pa-
rameters than the cross-product approach. How-
187
ever, DCRFs do not guarantee non-violation of hard-
constraints that nodes within the same segment get
a single consistent tagging label. Another draw-
back of DCRFs is that exact inference is generally
time consuming, some approximations are required
to make it tractable.
Recently, perceptron based learning framework
has been well studied for incorporating node level
and segment level features together (Kazama and
Torisawa, 2007; Zhang and Clark, 2008). The main
shortcoming is that exact inference is intractable
for those dynamically generated segment level fea-
tures, so candidate based searching algorithm is
used for approximation. On the other hand, Jiang
(2008) proposed a cascaded linear model which has
a two layer structure, the inside-layer model uses
node level features to generate candidates with their
weights as inputs of the outside layer model which
captures non-local features. As pipeline models, er-
ror propagation problem exists for such method.
In this paper, we present a novel graph structure
that exploits joint training and decoding in the fac-
torized state space. Our method does not suffer
from error propagation, and guards against viola-
tions of those hard-constraints imposed by segmen-
tation subtask. The motivation is to integrate two
Markov chains for segmentation and tagging sub-
tasks into a single chain, which contains two types of
nodes, then standard dynamic programming based
exact inference is employed on the hybrid struc-
ture. Experiments are conducted on two different
tasks, CoNLL 2000 shallow parsing and SIGHAN
2008 Chinese word segmentation and POS tagging.
Evaluation results of shallow parsing task show
the superiority of our proposed method over tradi-
tional joint training/decoding approach using cross-
product state space, and achieves the best reported
results when no additional resources at hand. For
Chinese word segmentation and POS tagging task, a
strong baseline pipeline model is built, experimental
results show that the proposed method yields a more
substantial improvement over the baseline than can-
didate reranking approach.
The rest of this paper is organized as follows: In
Section 2, we describe our novel graph structure. In
Section 3, we analyze complexity of our proposed
method. Experimental results are shown in Section
4. We conclude the work in Section 5.
2 Multi-chain integration using Virtual
Nodes
2.1 Conditional Random Fields
We begin with a brief review of the Conditional Ran-
dom Fields(CRFs). Let x = x1x2 . . . xl denote the
observed sequence, where xi is the ith node in the
sequence, l is sequence length, y = y1y2 . . . yl is a
label sequence over x that we wish to predict. CRFs
(Lafferty et al, 2001) are undirected graphic mod-
els that use Markov network distribution to learn the
conditional probability. For sequence labeling task,
linear chain CRFs are very popular, in which a first
order Markov assumption is made on the labels:
p(y|x) = 1Z(x)
?
i
?(x,y, i)
,where
?(x,y, i) = exp
(
wT f(x, yi?1, yi, i)
)
Z(x) =
?
y
?
i
?(x,y, i)
f(x, yi?1, yi, i) =
[f1(x, yi?1, yi, i), . . .,fm(x, yi?1, yi, i)]T , each ele-
ment fj(x, yi?1, yi, i) is a real valued feature func-
tion, here we simplify the notation of state feature
by writing fj(x, yi, i) = fj(x, yi?1, yi, i), m is the
cardinality of feature set {fj}. w = [w1, . . . , wm]T
is a weight vector to be learned from the training
set. Z(x) is the normalization factor over all label
sequences for x.
In the traditional joint training/decoding approach
for cascaded segmentation and tagging task, each
label yi has the form si-ti, which consists of seg-
mentation label si and tagging label ti. Let s =
s1s2 . . . sl be the segmentation label sequence over
x. There are several commonly used label sets such
as BI, BIO, IOE, BIES, etc. To facilitate our dis-
cussion, in later sections we will use BIES label set,
where B,I,E represents Beginning, Inside and End of
a multi-node segment respectively, S denotes a sin-
gle node segment. Let t = t1t2 . . . tl be the tagging
label sequence over x. For example, in named entity
recognition task, ti ? {PER, LOC, ORG, MISC,
O} represents an entity type (person name, loca-
tion name, organization name, miscellaneous entity
188
x2
s?t
2
2
x1
s?t
1
1
S-P S-O
x3
s?t
3
3
S-O
x4
s?t
4
4
B-P
x5
s?t
5
5
E-P
Hendrix ?s girlfriend Kathy Etchingham
Figure 1: Graphical representation of linear chain CRFs
for traditional joint learning/decoding
name and other). Graphical representation of lin-
ear chain CRFs is shown in Figure 1, where tagging
label ?P? is the simplification of ?PER?. For nodes
that are labeled as other, we define si =S, ti =O.
2.2 Hybrid structure for cascaded labeling
tasks
Different from traditional joint approach, our
method integrates two linear markov chains for seg-
mentation and tagging subtasks into one that con-
tains two types of nodes. Specifically, we first
regard segmentation and tagging as two indepen-
dent sequence labeling tasks, corresponding chain
structures are built, as shown in the top and mid-
dle sub-figures of Figure 2. Then a chain of twice
length of the observed sequence is built, where
nodes x1, . . . , xl on the even positions are original
observed nodes, while nodes v1, . . . , vl on the odd
positions are virtual nodes that have no content in-
formation. For original nodes xi, the state space is
the tagging label set, while for virtual nodes, their
states are segmentation labels. The label sequence
of the hybrid chain is y = y1 . . . y2l = s1t1 . . . sltl,
where combination of consecutive labels siti repre-
sents the full label for node xi.
Then we let si be connected with si?1 and si+1
, so that first order Markov assumption is made
on segmentation states. Similarly, ti is connected
with ti?1 and ti+1. Then neighboring tagging and
segmentation states are connected as shown in the
bottom sub-figure of Figure 2. Non-violation of
hard-constraints that nodes within the same seg-
ment get a single consistent tagging label is guar-
anteed by introducing second order transition fea-
tures f(ti?1, si, ti, i) that are true if ti?1 6= ti and
si ? {I,E}. For example, fj(ti?1, si, ti, i) is de-
fined as true if ti?1 =PER, si =I and ti =LOC.
In other words, it is true, if a segment is partially
tagging as PER, and partially tagged as LOC. Since
such features are always false in the training corpus,
their corresponding weights will be very low so that
inconsistent label assignments impossibly appear in
decoding procedure. The hybrid graph structure can
be regarded as a special case of second order Markov
chain.
Hendrix ?s girlfriend Kathy Etchingham
x1 x2 x3 x4 x5
s1 s2 s3 s4 s5
S S S B E
x1 x2 x3 x4 x5
t1 t2 t3 t4 t5
P O O P P
x1 x2 x3 x4 x5
t1 t2 t3 t4 t5
P O O P P
s2s1 s3 s4 s5
S S S B E
v1 v2 v3 v4 v5
Integrate
Figure 2: Multi-chain integration using Virtual Nodes
2.3 Factorized features
Compared with traditional joint model that exploits
cross-product state space, our hybrid structure uses
factorized states, hence could handle more flexible
features. Any state feature g(x, yi, i) defined in
the cross-product state space can be replaced by a
first order transition feature in the factorized space:
f(x, si, ti, i). As for the transition features, we
use f(si?1, ti?1, si, i) and f(ti?1, si, ti, i) instead
of g(yi?1, yi, i) in the conventional joint model.
Features in cross-product state space require that
segmentation label and tagging label take on partic-
ular values simultaneously, however, sometimes we
189
want to specify requirement on only segmentation or
tagging label. For example, ?Smith? may be an end
of a person name, ?Speaker: John Smith?; or a sin-
gle word person name ?Professor Smith will . . . ?. In
such case, our observation is that ?Smith? is likely a
(part of) person name, we do not care about its seg-
mentation label. So we could define state feature
f(x, ti, i) = true, if xi is ?Smith? with tagging la-
bel ti=PER.
Further more, we could define features like
f(x, ti?1, ti, i), f(x, si?1, si, i), f(x, ti?1, si, i),
etc. The hybrid structure facilitates us to use
varieties of features. In the remainder of the
paper, we use notations f(x, ti?1, si, ti, i) and
f(x, si?1, ti?1, si, i) for simplicity.
2.4 Hybrid CRFs
A hybrid CRFs is a conditional distribution that fac-
torizes according to the hybrid graphical model, and
is defined as:
p(s, t|x) = 1Z(x)
?
i
?(x, s, t, i)
?
i
?(x, s, t, i)
Where
?(x, s, t, i) = exp
(
wT1 f(x, si?1, ti?1, si)
)
?(x, s, t, i) = exp
(
wT2 f(x, ti?1, si, ti)
)
Z(x) =
?
s,t
(?
i
?(x, s, t, i)
?
i
?(x, s, t, i)
)
Where w1, w2 are weight vectors.
Luckily, unlike DCRFs, in which graph structure
can be very complex, and the cross-product state
space can be very large, in our cascaded labeling
task, the segmentation label set is often small, so
far as we known, the most complicated segmenta-
tion label set has only 6 labels (Huang and Zhao,
2007). So exact dynamic programming based algo-
rithms can be efficiently performed.
In the training stage, we use second order forward
backward algorithm to compute the marginal proba-
bilities p(x, si?1, ti?1, si) and p(x, ti?1, si, ti), and
the normalization factor Z(x). In decoding stage,
we use second order Viterbi algorithm to find the
best label sequence. The Viterbi decoding can be
Table 1: Time Complexity
Method Training Decoding
Pipeline (|S|2cs + |T |2ct)L (|S|2 + |T |2)U
Cross-Product (|S||T |)2cL (|S||T |)2U
Reranking (|S|2cs + |T |2ct)L (|S|2 + |T |2)NU
Hybrid (|S| + |T |)|S||T |cL (|S| + |T |)|S||T |U
used to label a new sequence, and marginal compu-
tation is used for parameter estimation.
3 Complexity Analysis
The time complexity of the hybrid CRFs train-
ing and decoding procedures is higher than that of
pipeline methods, but lower than traditional cross-
product methods. Let
? |S| = size of the segmentation label set.
? |T | = size of the tagging label set.
? L = total number of nodes in the training data
set.
? U = total number of nodes in the testing data
set.
? c = number of joint training iterations.
? cs = number of segmentation training itera-
tions.
? ct = number of tagging training iterations.
? N = number of candidates in candidate rerank-
ing approach.
Time requirements for pipeline, cross-product, can-
didate reranking and hybrid CRFs are summarized
in Table 1. For Hybrid CRFs, original node xi has
features {fj(ti?1, si, ti)}, accessing all label subse-
quences ti?1siti takes |S||T |2 time, while virtual
node vi has features {fj(si?1, ti?1, si)}, accessing
all label subsequences si?1ti?1si takes |S|2|T | time,
so the final complexity is (|S|+ |T |)|S||T |cL.
In real applications, |S| is small, |T | could be
very large, we assume that |T | >> |S|, so for
each iteration, hybrid CRFs is about |S| times slower
than pipeline and |S| times faster than cross-product
190
Table 2: Feature templates for shallow parsing task
Cross Product CRFs Hybrid CRFs
wi?2yi, wi?1yi, wiyi wi?1si, wisi, wi+1si
wi+1yi, wi+2yi wi?2ti, wi?1ti, witi, wi+1ti, wi+2ti
wi?1wiyi, wiwi+1yi wi?1wisi, wiwi+1si
wi?1witi, wiwi+1ti
pi?2yi, pi?1yi, piyi pi?1si, pisi, pi+1si
pi+1yi, pi+2yi pi?2ti, pi?1ti, pi+1ti, pi+2ti
pi?2pi?1yi, pi?1piyi, pipi+1yi,
pi+1pi+2yi
pi?2pi?1si, pi?1pisi, pipi+1si, pi+1pi+2si
pi?3pi?2ti, pi?2pi?1ti, pi?1piti, pipi+1ti,
pi+1pi+2ti, pi+2pi+3ti, pi?1pi+1ti
pi?2pi?1piyi, pi?1pipi+1yi,
pipi+1pi+2yi
pi?2pi?1pisi, pi?1pipi+1si, pipi+1pi+2si
wipiti
wisi?1si
wi?1ti?1ti, witi?1ti, pi?1ti?1ti, piti?1ti
yi?1yi si?1ti?1si, ti?1siti
method. When decoding, candidate reranking ap-
proach requires more time if candidate number N >
|S|.
Though the space complexity could not be com-
pared directly among some of these methods, hybrid
CRFs require less parameters than cross-product
CRFs due to the factorized state space. This is sim-
ilar with factorized CRFs (FCRFs) (Sutton et al,
2004).
4 Experiments
4.1 Shallow Parsing
Our first experiment is the shallow parsing task. We
use corpus from CoNLL 2000 shared task, which
contains 8936 sentences for training and 2012 sen-
tences for testing. There are 11 tagging labels: noun
phrase(NP), verb phrase(VP) , . . . and other (O), the
segmentation state space we used is BIES label set,
since we find that it yields a little improvement over
BIO set.
We use the standard evaluation metrics, which are
precision P (percentage of output phrases that ex-
actly match the reference phrases), recall R (percent-
age of reference phrases returned by our system),
and their harmonic mean, the F1 score F1 = 2PRP+R
(which we call F score in what follows).
We compare our approach with traditional cross-
product method. To find good feature templates,
development data are required. Since CoNLL2000
does not provide development data set, we divide
the training data into 10 folds, of which 9 folds for
training and 1 fold for developing. After selecting
feature templates by cross validation, we extract fea-
tures and learn their weights on the whole training
data set. Feature templates are summarized in Table
2, where wi denotes the ith word, pi denotes the ith
POS tag.
Notice that in the second row, feature templates
of the hybrid CRFs does not contain wi?2si, wi+2si,
since we find that these two templates degrade per-
formance in cross validation. However, wi?2ti,
wi+2ti are useful, which implies that the proper con-
text window size for segmentation is smaller than
tagging. Similarly, for hybrid CRFs, the window
size of POS bigram features for segmentation is 5
(from pi?2 to pi+2, see the eighth row in the sec-
ond column); while for tagging, the size is 7 (from
pi?3 to pi+3, see the ninth row in the second col-
umn). However for cross-product method, their win-
dow sizes must be consistent.
For traditional cross-product CRFs and our hybrid
CRFs, we use fixed gaussian prior ? = 1.0 for both
methods, we find that this parameter does not signifi-
191
Table 3: Results for shallow parsing task, Hybrid CRFs
significantly outperform Cross-Product CRFs (McNe-
mar?s test; p < 0.01)
Method Cross-Product
CRFs
Hybrid
CRFs
Training Time 11.6 hours 6.3 hours
Feature Num-
ber
13 million 10 mil-
lion
Iterations 118 141
F1 93.88 94.31
cantly affect the results when it varies between 1 and
10. LBFGS(Nocedal and Wright, 1999) method is
employed for numerical optimization. Experimen-
tal results are shown in Table 3. Our proposed CRFs
achieve a performance gain of 0.43 points in F-score
over cross-product CRFs that use state space while
require less training time.
For comparison, we also listed the results of pre-
vious top systems, as shown in Table 4. Our pro-
posed method outperforms other systems when no
additional resources at hand. Though recently semi-
supervised learning that incorporates large mounts
of unlabeled data has been shown great improve-
ment over traditional supervised methods, such as
the last row in Table 4, supervised learning is funda-
mental. We believe that combination of our method
and semi-supervised learning will achieve further
improvement.
4.2 Chinese word segmentation and POS
tagging
Our second experiment is the Chinese word seg-
mentation and POS tagging task. To facilitate com-
parison, we focus only on the closed test, which
means that the system is trained only with a des-
ignated training corpus, any extra knowledge is not
allowed, including Chinese and Arabic numbers, let-
ters and so on. We use the Chinese Treebank (CTB)
POS corpus from the Fourth International SIGHAN
Bakeoff data sets (Jin and Chen, 2008). The train-
ing data consist of 23444 sentences, 642246 Chinese
words, 1.05M Chinese characters and testing data
consist of 2079 sentences, 59955 Chinese words,
0.1M Chinese characters.
We compare our hybrid CRFs with pipeline and
candidate reranking methods (Shi and Wang, 2007)
Table 4: Comparison with other systems on shallow pars-
ing task
Method F1 Additional Re-
sources
Cross-Product CRFs 93.88
Hybrid CRFs 94.31
SVM combination 93.91
(Kudo and Mat-
sumoto, 2001)
Voted Perceptrons 93.74 none
(Carreras and Mar-
quez, 2003)
ETL (Milidiu et al,
2008)
92.79
(Wu et al, 2006) 94.21 Extended features
such as token fea-
tures, affixes
HySOL 94.36 17M words unla-
beled
(Suzuki et al, 2007) data
ASO-semi 94.39 15M words unla-
beled
(Ando and Zhang,
2005)
data
(Zhang et al, 2002) 94.17 full parser output
(Suzuki and Isozaki,
2008)
95.15 1G words unla-
beled data
using the same evaluation metrics as shallow pars-
ing. We do not compare with cross-product CRFs
due to large amounts of parameters.
For pipeline method, we built our word segmenter
based on the work of Huang and Zhao (2007),
which uses 6 label representation, 7 feature tem-
plates (listed in Table 5, where ci denotes the ith
Chinese character in the sentence) and CRFs for pa-
rameter learning. We compare our segmentor with
other top systems using SIGHAN CTB corpus and
evaluation metrics. Comparison results are shown
in Table 6, our segmenter achieved 95.12 F-score,
which is ranked 4th of 26 official runs. Except for
the first system which uses extra unlabeled data, dif-
ferences between rest systems are not significant.
Our POS tagging system is based on linear chain
CRFs. Since SIGHAN dose not provide develop-
ment data, we use the 10 fold cross validation de-
scribed in the previous experiment to turning feature
templates and Gaussian prior. Feature templates are
listed in Table 5, where wi denotes the ith word in
192
Table 5: Feature templates for Chinese word segmentation and POS tagging task
Segmentation feature templates
(1.1) ci?2si, ci?1si, cisi, ci+1si, ci+2si
(1.2) ci?1cisi, cici+1si, ci?1ci+1si
(1.3) si?1si
POS tagging feature templates
(2.1) wi?2ti, wi?1ti, witi, wi+1ti, wi+2ti
(2.2) wi?2wi?1ti, wi?1witi, wiwi+1ti, wi+1wi+2ti, wi?1wi+1ti
(2.3) c1(wi)ti, c2(wi)ti, c3(wi)ti, c?2(wi)ti, c?1(wi)ti
(2.4) c1(wi)c2(wi)ti, c?2(wi)c?1(wi)ti
(2.5) l(wi)ti
(2.6) ti?1ti
Joint segmentation and POS tagging feature templates
(3.1) ci?2si, ci?1si, cisi, ci+1si, ci+2si
(3.2) ci?1cisi, cici+1si, ci?1ci+1si
(3.3) ci?3ti, ci?2ti, ci?1ti, citi, ci+1ti, ci+2ti, ci+3ti
(3.4) ci?3ci?2ti, ci?2ci?1ti, ci?1citi, cici+1ti ci+1ci+2ti, ci+2ci+3ti, ci?2citi, cici+2ti
(3.5) cisiti
(3.6) citi?1ti
(3.7) si?1ti?1si, ti?1siti
Table 6: Word segmentation results on Fourth SIGHAN
Bakeoff CTB corpus
Rank F1 Description
1/26 95.89? official best, using extra un-
labeled data (Zhao and Kit,
2008)
2/26 95.33 official second
3/26 95.17 official third
4/26 95.12 segmentor in pipeline sys-
tem
Table 7: POS results on Fourth SIGHAN Bakeoff CTB
corpus
Rank Accuracy Description
1/7 94.29 POS tagger in pipeline sys-
tem
2/7 94.28 official best
3/7 94.01 official second
4/7 93.24 official third
the sentence, cj(wi), j > 0 denotes the jth Chinese
character of word wi, cj(wi), j < 0 denotes the jth
last Chinese character, l(wi) denotes the word length
of wi. We compare our POS tagger with other top
systems on Bakeoff CTB POS corpus where sen-
tences are perfectly segmented into words, our POS
tagger achieved 94.29 accuracy, which is the best of
7 official runs. Comparison results are shown in Ta-
ble 7.
For reranking method, we varied candidate num-
bers n among n ? {10, 20, 50, 100}. For hybrid
CRFs, we use the same segmentation label set as
the segmentor in pipeline. Feature templates are
listed in Table 5. Experimental results are shown
in Figure 3. The gain of hybrid CRFs over the
baseline pipeline model is 0.48 points in F-score,
about 3 times higher than 100-best reranking ap-
proach which achieves 0.13 points improvement.
Though larger candidate number can achieve higher
performance, such improvement becomes trivial for
n > 20.
Table 8 shows the comparison between our work
and other relevant work. Notice that, such com-
parison is indirect due to different data sets and re-
193
0 20 40 60 80 10090.3
90.4
90.5
90.6
90.7
90.8
90.9
candidate number
F s
cor
e
 
 
candidate reranking
Hybrid CRFs
Figure 3: Results for Chinese word segmentation and
POS tagging task, Hybrid CRFs significantly outperform
100-Best Reranking (McNemar?s test; p < 0.01)
Table 8: Comparison of word segmentation and POS tag-
ging, such comparison is indirect due to different data
sets and resources.
Model F1
Pipeline (ours) 90.40
100-Best Reranking (ours) 90.53
Hybrid CRFs (ours) 90.88
Pipeline (Shi and Wang, 2007) 91.67
20-Best Reranking (Shi and Wang,
2007)
91.86
Pipeline (Zhang and Clark, 2008) 90.33
Joint Perceptron (Zhang and Clark,
2008)
91.34
Perceptron Only (Jiang et al, 2008) 92.5
Cascaded Linear (Jiang et al, 2008) 93.4
sources. One common conclusion is that joint mod-
els generally outperform pipeline models.
5 Conclusion
We introduced a framework to integrate graph struc-
tures for segmentation and tagging subtasks into one
using virtual nodes, and performs joint training and
decoding in the factorized state space. Our approach
does not suffer from error propagation, and guards
against violations of those hard-constraints imposed
by segmentation subtask. Experiments on shal-
low parsing and Chinese word segmentation tasks
demonstrate our technique.
6 Acknowledgements
The author wishes to thank the anonymous review-
ers for their helpful comments. This work was
partially funded by 973 Program (2010CB327906),
The National High Technology Research and De-
velopment Program of China (2009AA01A346),
Shanghai Leading Academic Discipline Project
(B114), Doctoral Fund of Ministry of Education of
China (200802460066), National Natural Science
Funds for Distinguished Young Scholar of China
(61003092), and Shanghai Science and Technology
Development Funds (08511500302).
References
R. Ando and T. Zhang. 2005. A high-performance semi-
supervised learning method for text chunking. In Pro-
ceedings of ACL, pages 1?9.
Razvan C. Bunescu. 2008. Learning with probabilistic
features for improved pipeline models. In Proceedings
of EMNLP, Waikiki, Honolulu, Hawaii.
X Carreras and L Marquez. 2003. Phrase recognition by
filtering and ranking with perceptrons. In Proceedings
of RANLP.
Changning Huang and Hai Zhao. 2007. Chinese word
segmentation: A decade review. Journal of Chinese
Information Processing, 21:8?19.
Wenbin Jiang, Liang Huang, Qun Liu, and Yajuan Lu.
2008. A cascaded linear model for joint chinese word
segmentation and part-of-speech tagging. In Proceed-
ings of ACL, Columbus, Ohio, USA.
Guangjin Jin and Xiao Chen. 2008. The fourth interna-
tional chinese language processing bakeoff: Chinese
word segmentation, named entity recognition and chi-
nese pos tagging. In Proceedings of Sixth SIGHAN
Workshop on Chinese Language Processing, India.
Junichi Kazama and Kentaro Torisawa. 2007. A new
perceptron algorithm for sequence labeling with non-
local features. In Proceedings of EMNLP, pages 315?
324, Prague, June.
Taku Kudo and Yuji Matsumoto. 2001. Chunking with
support vector machines. In Proceedings of NAACL.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proceedings
of ICML.
Ruy L. Milidiu, Cicero Nogueira dos Santos, and Julio C.
Duarte. 2008. Phrase chunking using entropy guided
transformation learning. In Proceedings of ACL, pages
647?655.
194
Hwee Tou Ng and Jin Kiat Low. 2004. Chinese part-
ofspeech tagging: One-at-a-time or all-at-once? word-
based or character-based? In Proceedings of EMNLP.
J. Nocedal and S. J. Wright. 1999. Numerical Optimiza-
tion. Springer.
Yanxin Shi and Mengqiu Wang. 2007. A dual-layer crfs
based joint decoding method for cascaded segmenta-
tion and labeling tasks. In Proceedings of IJCAI, pages
1707?1712, Hyderabad, India.
C. Sutton, K. Rohanimanesh, and A. McCallum. 2004.
Dynamic conditional random fields: Factorized prob-
abilistic models for labeling and segmenting sequence
data. In Proceedings of ICML.
Jun Suzuki and Hideki Isozaki. 2008. Semi-supervised
sequential labeling and segmentation using giga-word
scale unlabeled data. In Proceedings of ACL, pages
665?673.
Jun Suzuki, Akinori Fujino, and Hideki Isozaki. 2007.
Semi-supervised structured output learning based on
a hybrid generative and discriminative approach. In
Proceedings of EMNLP, Prague.
Yu-Chieh Wu, Chia-Hui Chang, and Yue-Shi Lee. 2006.
A general and multi-lingual phrase chunking model
based on masking method. In Proceedings of Intel-
ligent Text Processing and Computational Linguistics,
pages 144?155.
Yue Zhang and Stephen Clark. 2008. Joint word seg-
mentation and pos tagging using a single perceptron.
In Proceedings of ACL, Columbus, Ohio, USA.
T. Zhang, F. Damerau, and D. Johnson. 2002. Text
chunking based on a generalization of winnow. ma-
chine learning research. Machine Learning Research,
2:615?637.
Hai Zhao and Chunyu Kit. 2008. Unsupervised segmen-
tation helps supervised learning of character tagging
forword segmentation and named entity recognition.
In Proceedings of Sixth SIGHAN Workshop on Chinese
Language Processing, pages 106?111.
195
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 501?511, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Joint Chinese Word Segmentation, POS Tagging and Parsing
Xian Qian Yang Liu
Computer Science Department
The University of Texas at Dallas
qx,yangl@hlt.utdallas.edu
Abstract
In this paper, we propose a novel decoding al-
gorithm for discriminative joint Chinese word
segmentation, part-of-speech (POS) tagging,
and parsing. Previous work often used a
pipeline method ? Chinese word segmentation
followed by POS tagging and parsing, which
suffers from error propagation and is unable
to leverage information in later modules for
earlier components. In our approach, we train
the three individual models separately during
training, and incorporate them together in a u-
nified framework during decoding. We extend
the CYK parsing algorithm so that it can deal
with word segmentation and POS tagging fea-
tures. As far as we know, this is the first work
on joint Chinese word segmentation, POS tag-
ging and parsing. Our experimental result-
s on Chinese Tree Bank 5 corpus show that
our approach outperforms the state-of-the-art
pipeline system.
1 Introduction
For Asian languages such as Japanese and Chi-
nese that do not contain explicitly marked word
boundaries, word segmentation is an important first
step for many subsequent language processing tasks,
such as POS tagging, parsing, semantic role label-
ing, and various applications. Previous studies for
POS tagging and syntax parsing on these languages
sometimes assume that gold standard word segmen-
tation information is provided, which is not the re-
al scenario. In a fully automatic system, a pipeline
approach is often adopted, where raw sentences are
first segmented into word sequences, then POS tag-
ging and parsing are performed. This kind of ap-
proach suffers from error propagation. For exam-
ple, word segmentation errors will result in tagging
and parsing errors. Additionally, early modules can-
not use information from subsequent modules. In-
tuitively a joint model that performs the three tasks
together should help the system make the best deci-
sions.
In this paper, we propose a unified model for joint
Chinese word segmentation, POS tagging, and pars-
ing. Three sub-models are independently trained
using the state-of-the-art methods. We do not use
the joint inference algorithm for training because of
the high complexity caused by the large amount of
parameters. We use linear chain Conditional Ran-
dom Fields (CRFs) (Lafferty et al2001) to train the
word segmentation model and POS tagging model,
and averaged perceptron (Collins, 2002) to learn the
parsing model. During decoding, parameters of each
sub-model are scaled to represent its importance in
the joint model. Our decoding algorithm is an exten-
sion of CYK parsing. Initially, weights of all possi-
ble words together with their POS tags are calcu-
lated. When searching the parse tree, the word and
POS tagging features are dynamically generated and
the transition information of POS tagging is consid-
ered in the span merge operation.
Experiments are conducted on Chinese Tree Bank
(CTB) 5 dataset, which is widely used for Chinese
word segmentation, POS tagging and parsing. We
compare our proposed joint model with the pipeline
system, both built using the state-of-the-art sub-
models. We also propose an evaluation metric to
501
calculate the bracket scores for parsing in the face of
word segmentation errors. Our experimental results
show that the joint model significantly outperform-
s the pipeline method based on the state-of-the-art
sub-models.
2 Related Work
There is very limited previous work on joint Chinese
word segmentation, POS tagging, and parsing. Pre-
vious joint models mainly focus on word segmenta-
tion and POS tagging task, such as the virtual nodes
method (Qian et al2010), cascaded linear model
(Jiang et al2008a), perceptron (Zhang and Clark,
2008), sub-word based stacked learning (Sun, 2011),
reranking (Jiang et al2008b). These joint models
showed about 0.2 ? 1% F-score improvement over
the pipeline method. Recently, joint tagging and de-
pendency parsing has been studied as well (Li et al
2011; Lee et al2011).
Previous research has showed that word segmen-
tation has a great impact on parsing accuracy in
the pipeline method (Harper and Huang, 2009). In
(Jiang et al2009), additional data was used to im-
prove Chinese word segmentation, which resulted
in significant improvement on the parsing task us-
ing the pipeline framework. Joint segmentation and
parsing was also investigated for Arabic (Green and
Manning, 2010). A study that is closely related to
ours is (Goldberg and Tsarfaty, 2008), where a s-
ingle generative model was proposed for joint mor-
phological segmentation and syntactic parsing for
Hebrew. Different from that work, we use a discrim-
inative model, which benefits from large amounts of
features and is easier to deal with unknown words.
Another main difference is that, besides segmenta-
tion and parsing, we also incorporate the POS tag-
ging model into the CYK parsing framework.
3 Methods
For a given Chinese sentence, our task is to gener-
ate the word sequence, its POS tag sequence, and
the parse tree (constituent parsing). A joint model
is expected to make more optimal decisions than a
pipeline approach; however, such a model will be
too complex and it is difficult to estimate model pa-
rameters. Therefore we do not perform joint infer-
ence for training. Instead, we develop three individ-
ual models independently during training and per-
form joint decoding using them. In this section, we
first describe the three sub-models and then the joint
decoding algorithm.
3.1 Word Segmentation Model
Methods for Chinese word segmentation can be
broadly categorized into character based and word
based models. Previous studies showed that
character-based models are more effective to detect
out-of-vocabulary words while word-based model-
s are more accurate to predict in-vocabulary words
(Zhang et al2006). Here, we use order-0 semi-
Markov model (Sarawagi and Cohen, 2004) to take
advantages of both approaches.
More specifically, given a sentence x =
c1, c2, . . . , cl (where ci is the ith Chinese character,
l is the sentence length), the character-based mod-
el assigns each character with a word boundary tag.
Here we use the BCDIES tag set, which achieved
the best official performance (Zhao and Kit, 2008):
B, C, D, E denote the first, second, third, and last
character of a multi-character word respectively, I
denotes the other characters, and S denotes the s-
ingle character word. We use the same character-
based feature templates as in the best official system,
shown in Table 1 (1.1-1.3), including character un-
igram and bigram features, and transition features.
Linear chain CRFs are used for training.
Feature templates in the word-based model are
shown in Table 1 (1.4-1.6), including word features,
sub-word features, and character bigrams within
words. The word feature is activated if a predicted
word w is in the vocabulary (i.e., appears in train-
ing data). Subword(w) is the longest in-vocabulary
word within w. To use word features, we adopt a K-
best reranking approach. The top K candidate seg-
mentation results for each training sample are gen-
erated using the character-based model, and the gold
segmentation is added if it is not in the candidate set.
We use the Maximum Entropy (ME) model to learn
the weights of word features such that the probabil-
ity of the gold candidate is maximal.
A problem arises when combining the two mod-
els and using it in joint segmentation and parsing,
since the linear chain used in the character-based
model is incompatible with CYK parsing model and
the word-based model due to the transition informa-
502
Character Level Feature Templates
(1.1) ci?2yi, ci?1yi, ciyi, ci+1yi, ci+2yi
(1.2) ci?1ciyi, cici+1yi, ci?1ci+1yi
(1.3) yi?1yi
Word Level Feature Templates
(1.4) word w
(1.5) subword(w)
(1.6) character bigrams within w
Table 1: Feature templates for word segmentation. ci is
the ith character in the sentence, yi is its label, w is a
predicted word.
tion. Thus, we slightly modify the linear chain CRF-
s by fixing the weights of transition features during
training and testing. That is, weights of impossible
transition features (e.g., B?B) are set to ??, and
weights of the other transition features (e.g., E?B)
are set to 0. In this way, the transition feature could
be neglected in testing for two reasons. First, all ille-
gal label assignments are prohibited in prediction, s-
ince their weights are ??; second, because weights
of legal transition features are 0, they do not affec-
t the prediction at all. In the following, transition
features are excluded.
Now we can use order-0 semi Markov model as
the hybrid model. We define the score of a word as
the sum of the weights of all the features within the
word. Formally, the score of a multi-character word
w = ci, . . . , cj is defined as:
scoreseg(x, i, j) = ?CRF ? fCRF (x, yi = B) + . . .
+?CRF ? fCRF (x, yj = E) + ?ME ? fME(x, i, j)
? ?segfseg(x, i, j) (1)
where fCRF and fME are the feature vectors in the
character and word based models respectively, and
?CRF , ?ME are their corresponding weight vectors.
For simplicity, we denote ?seg = ?CRF?ME , fseg =
fCRF?ME , where ?CRF?ME means the concatena-
tion of ?CRF and ?ME . Scores for single character
words are defined similarly. These word scores will
be used in the joint segmentation and parsing task
Section 3.4.
3.2 POS Tagging Model
Though syntax parsing model can directly predict
the POS tag itself, we choose not to use this, but use
an independent POS tagger for two reasons. First,
there is a large amount of data with labeled POS tags
but no syntax annotations, such as the People?s Daily
corpus and SIGHAN bakeoff corpora (Jin and Chen,
2008). Such data can only be used to train POS tag-
gers, but not for training the parsing model. Often
using a larger training set will result in a better POS
tagger. Second, the state-of-the-art POS tagging sys-
tems are often trained by sequence labeling models,
not parsing models.
(2.1) wi?2ti, wi?1ti, witi, wi+1ti, wi+2ti
(2.2) wi?2wi?1ti, wi?1witi, wiwi+1ti,
wi+1wi+2ti wi?1wi+1ti
(2.3) c1(wi)ti, c2(wi)ti, c3(wi)ti, c?2(wi)ti
c?1(wi)ti
(2.4) c1(wi)c2(wi)ti, c?2(wi)c?1(wi)ti
(2.5) l(wi)ti
(2.5) ti?1ti
Table 2: Feature templates for POS tagging. wi is the
ith word in the sentence, ti is its POS tag. For a word w,
cj(w) is its jth character, c?j(w) is the last jth character,
and l(w) is its length.
The POS tagging problem is to assign a POS tag
t ? T to each word in a sentence. We also use lin-
ear chain CRFs for POS tagging. Feature templates
shown in Table 2 are the same as those in (Qian
et al2010), which have been shown effective on
CTB corpus. Three feature sets are considered: (i)
word level features, including surrounding word uni-
grams, bigrams, and word length; (ii) character level
features, such as the first and last characters in the
words; (iii) transition features.
3.3 Parsing Model
We choose discriminative models for parsing since it
is easy to handle unknown words by simply adding
character level features. Online structured learn-
ing algorithms were demonstrated to be effective for
training, such as stochastic optimization (Finkel et
al., 2008). In this study, we use averaged perceptron
algorithm for parameter estimation since it is easier
to implement and has competitive performance.
A Context Free Grammar (CFG) consists of (i) a
set of terminals; (ii) a set of nonterminals {Nk}; (i-
ii) a designated start symbol ROOT; and (iv) a set of
rules, {r = N i ? ?j}, where ?j is a sequence of
terminals and nonterminals. In the parsing task, ter-
503
!" "# $% &'(
Shanghai customs Chongming office
NR NN NR NN
NP
!"
"#
$% &'(
Shanghai
customs
Chongming office
NR
NN
NR NN
NP
NR_NN
NN_NR
Figure 1: Parse tree binarization
minals are the words, and nonterminals are the POS
tags and phrase types. In this paper, nonterminal is
named state for short. A parse tree T of sentence
x can be factorized into several one-level subtrees,
each corresponding to a rule r.
In practice, binarization of rules is necessary to
obtain cubic parsing time. That is, the right hand
side of each rule should contain no more than 2 s-
tates. We used right branching binarization, as il-
lustrated in Figure 1. We did not use parent anno-
tation, since we found it degraded the performance
in our experiments (shown in Section 4). We used
the same preprocessing step as (Harper and Huang,
2009), collapsing all the allowed nonterminal-yield
unary chains to single unary rules. Therefore, all s-
pans in the binarized trees contain no more than one
unary rules. To facilitate decoding, we unify the for-
m of spans so that each span contains exactly one u-
nary rule. This is done by adding identity unary rules
(N ? N ) to spans that have no unary rule. These
identity unary rules will be removed in evaluation.
Hence, there are two states of a span: the top state
N and the bottom state N that correspond to the left
and right hand of the unary rule runary = N ? N
respectively, as shown in Figure 2.
Table 3 lists the feature templates we use for pars-
ing. There are 4 feature sets: (i) bottom state fea-
tures fbottom(i, j,x, N i,j), which depend on the bot-
!" #$
Lastyear realized
VV
VP
IP
CP
!" #$
Last year realized
NP VV
VP
CP
NT VV
top state
bottom state
NP
NT
Figure 2: Unary rule normalization. Nonterminal-yield
unary chains are collapsed to single unary rules. Identity
unary rules are added to spans that have no unary rule.
tom states; (ii) top state features ftop(i, j,x, N i,j);
(iii) unary rule features funary(i, j,x, runaryi,j ), which
extract the transition information from bottom s-
tates to top states; (iv) binary rule features
fbinary(i, j, k,x, rbinaryi,j,k = N i,j ? N i,k?1 +Nk,r),
where N i,k?1, Nk,r are the top states of the left and
right children.
The score function for a sentence xwith parse tree
T is defined as:
score(x, T ) =
?
N i,j?T
?bottom ? fbottom(i, j,x, N i,j)
+
?
N i,j?T
?top ? ftop(i, j,x, N i,j)
+
?
runaryi,j ?T
?unary ? funary(i, j,x, runaryi,j )
+
?
rbinaryi,j,k ?T
?binary ? fbinary(i, j,x, rbinaryi,j,k )
where ?bottom, ?top, ?unary, ?binary are the weight
vectors of the four feature sets.
Given the training corpus {(xi, T?i)}, the learning
task is to estimate the weight vectors so that for each
sentence xi, the gold standard tree T?i achieves the
maximal score among all the possible trees. The per-
ceptron algorithm is guaranteed to find the solution
if it exists.
3.4 Joint Decoding
The three models described above are separately
trained to make parameter estimation feasible as
well as optimize each individual component. In test-
504
(3.1) Binary rule templates
N ? N l +Nr
Xl Xm?1Xr lenllenr Xl Xm Xr lenl lenr
Xl Xm?1 Xr wordm?1(ROOT) Xl + Xm Xr wordm(ROOT)
(3.2) Unary rule templates
N ? N
(3.3) Bottom state templates
Xllen Xrlen
Xl?2Xl?1 Xr+1len Xl?1 Xr+1 Xr+2len
wllwlrXllen wllwlrXrlen XlXrwlllen XlXrwlrlen
wordlwordrXlXrlen wordlwordrXlXr
Xl?1Xl(LEAF) Xl+1Xl(LEAF) Xlwordl(LEAF) Xlwll(LEAF)
Xl+aXr+blen wordl+awordr+b ?1 ? a, b ? 1
(3.3) Top state templates
Xl?1Xl(LEAF) Xl+1Xl(LEAF) Xlwordl(LEAF) Xlwll(LEAF)
Xl+aXr+blen wordl+awordr+b ?1 ? a, b ? 1
Table 3: Feature templates for parsing, where X can be word, first and last character of word, first and last character
bigram of word, POS tag. Xl+a/Xr?a denotes the first/last ath X in the span, while Xl?a/Xr+a denotes the ath X
left/right to span. Xm is the first X of right child, and Xm?1 is the last X of the left child. len, lenl, lenr denote the
length of the span, left child and right child respectively. wl is the length of word. ROOT/LEAF means the template
can only generate the features for the root/initial span.
ing, we perform joint decoding to combine informa-
tion from the three models. Parameters of word seg-
mentation (?seg), POS tagging (?pos), and parsing
models (?parse = ?bottom?top? unary?bianry) are s-
caled by three positive hyper-parameters ?, ?, and
? respectively, which control their contribution in
the joint model. If ? >> ? >> ?, then the join-
t model is equivalent to a pipeline model, in which
there is no feedback from downstream models to up-
stream ones. For well tuned hyper-parameters, we
expect that segmentation and POS tagging results
can be improved by parsing information. The hyper-
parameters are tuned on development data. In the
following sections, for simplicity we drop ?, ?, ?,
and just use ?seg, ?pos, ?parse to represent the scaled
parameters.
The basic idea of our decoding algorithm is to ex-
tend the CYK parsing algorithm so that it can deal
with transition features in POS tagging and segmen-
tation scores in word segmentation.
3.4.1 Algorithm
The joint decoding algorithm is shown in Algo-
rithm 1. Given a sentence x = c1, . . . , cl, Line 0
calculates the scores of all possible words in the sen-
tence using Eq(1). There are l(l + 1)/2 word candi-
dates in total.
Surrounding words are important features for
POS tagging and parsing; however, they are un-
available because segmentation is incomplete before
parsing. Therefore, we adopt pseudo surrounding
features by simply fixing the context words as the s-
ingle most likely ones. Given a word candidate wi,j
from ci to cj , its previous word s? is the rightmost
one in the best word sequence of c1, . . . , ci?1, which
can be obtained by dynamic programming. Recur-
sively, the second word left to wi,j is the previous
word of s?. The next word of wi,j is defined similar-
ly. In Line 1, we use bidirectional Viterbi decoding
to obtain all the surrounding words. In the forward
direction, the algorithm starts from the first charac-
ter boundary to the last, and finds the best previous
word for the ith character boundary bi. In the back-
ward direction, the algorithm starts from right to left,
and finds the best next word of each bi.
In Line 2, for each word candidate, we can calcu-
late the score of each POS tag using state features in
the POS tagging model, since the context words are
available now. The score function of word wi,j with
POS tag t is:
scoreseg?pos(x, i, j, t) =
scoreseg(x, i, j) + ?pos ? fpos(x, wi,j , t) (2)
In Line 3, POS tags of surrounding words can
be obtained similarly using bidirectional decoding.
505
Algorithm 1 Joint Word Segmentation, POS tagging, and Parsing Algorithm
Input: Sentence x = c1, . . . , cl, beam size B, scaled word segmentation model, POS tagging model and
parsing model.
Output: Word sequence, POS tag sequence, and parse tree
0: ?0 ? i ? j ? l ? 1, calculate scoreseg(x, i, j) using Equation (1)
1: For each character boundary bi, 0 ? i ? l, get the best previous and next words of bi using bidirectional
Viterbi decoding
2: ?0 ? i ? j ? l ? 1, t ? T , calculate scoreseg?pos(x, i, j, t) using Equation (2)
3: ?bi, 0 ? i ? l, t ? T , get the best POS tags of words left/right to bi using bidirectional viterbi
decoding.
4: For each word candidate wi,j , 0 ? i ? j ? l ? 1
5: For each bottom state N , POS tag t ? T  step 1 (Line 5-7): get bottom states
6: scorebottom(x, i, j, wi,j , t, N) = scoreseg?pos(x, i, j, t) + ?bottom ? fbottom(x, i, j, wi,j , t, N)
7: Keep B best scorebottom.
8: For each top state N  step 2 (Line 8-9): get top states
9: scoretop(x, i, j, wi,j , t, N) = maxN {scorebottom(x, i, j, wi,j , t, N) + ?top ? ftop(x, i, j, wi,j , t, N)
+?unary ? funary(x, i, j, wi,j , t, N ? N)
}
10: for i = 0, . . . , l ? 1 do
11: for width = 1, . . . , l ? 1 do
12: j = i + width
13: for k = i + 1, . . . , j do
14: scorebottom(x, i, j,w, t, N) = maxl,r
{
scoretop(x, i, k ? 1,wl, tl, N l) + scoretop(x, k, j,wr, tr, Nr)
+?binary ? fbinary(x, i, j, k,w, t, N ? Nr + Nr) + ?pos ? fpos(tlastl ? tfirstr )
+?bottomfbottom(x, i, j,w, t, N)}
15: Keep B best scorebottom  step 1 (Line 14-15): get bottom states
16: For each top state N  step 2 (Line 16-17): get top states
17: scoretop(x, i, j,w, t, N) = maxN {scorebottom(x, i, j,w, t, N)
+?unary ? funary(x, i, j,w, t, N ? N)
}
18: end for
19: end for
20: end for
Line 0 1 2 3 6 9 14 15 Total Bound(w.r.t. l)
Complexity l2 l2 |T |l2 |T |2l2 |T |Ml2 BMl2 l3MB2 BMl2 l3MB2
Table 4: Complexity Analysis of Algorithm 1.
That is, for wi,j with POS tag t, we use Viterbi algo-
rithm to search the optimal POS tags of its left and
right words.
In Lines 4-9, each word was initialized as a basic
span. A span structure in the joint model is a 6-tuple:
S(i, j,w, t, N,N), where i, j are the boundary in-
dices,w, t are the word sequence and POS sequence
within the span respectively, and N,N are the bot-
tom and top states. There are two types of surround-
ing n-grams: one is inside the span, for example, the
first word of a span, which can be obtained from w;
the other is outside the span, for example, the pre-
vious word of a span, which is obtained from the
pseudo context information. The score of a basic s-
pan depends on its corresponding word and POS pair
score, and the weights of the active state and unary
features.
To avoid enumerating the combination of the bot-
tom and top states, initialization for each span is di-
vided into 2 steps. In the first step, the score of ev-
ery bottom state is calculated using bottom state fea-
tures, and only the B best states are maintained (see
Line 6-7). In the second step, top state features and
unary rule features are used to get the score of each
top state (Line 9), and only the top B states are pre-
served.
506
Similarly, there are two steps in the merge opera-
tion: S(i, j,w, t, N,N) = Sl(i, k,wl, tl, Nl, Nl)+
Sr(k + 1, j,wr, tr, Nr, Nr). The score of the bot-
tom state N is calculated using binary features
fbinary(x, i, j, k,w, t, N ? N r+N r), bottom state
features fbottom(x, i, j,w, t, N), and POS tag transi-
tion features that depend on the boundary POS tags
of Sl and Sr. See Line 14 of Algorithm 1, where
tlastl and t
first
r are the POS tags of the last word in
the left child span and the first word in the right child
span respectively.
3.4.2 Complexity analysis
Given a sentence of length l, the complexity for
each line of Algorithm 1 is listed in Table 4, where
|T | is the size of POS tag set, M is the number of
states, and B is the beam size.
4 Experiments
4.1 Data
For comparison with other systems, we use the CT-
B5 corpus, which has been studied for Chinese word
segmentation, POS tagging and parsing. We use the
standard train/develop/test split of the data. Details
are shown in Table 5.
CTB files # sent. # words
Training 1-270 18089 493,939
400-1151
Develop 301-325 350 6,821
Test 271-300 348 8,008
Table 5: Training, development, and test data of CTB 5.
4.2 Evaluation Metric
We evaluate system performance on the individual
tasks, as well as the joint tasks.1 For word segmen-
tation, three metrics are used for evaluation: pre-
cision (P), recall (R), and F-score (F) defined by
2PR/(P+R). Precision is the percentage of correct
words in the system output. Recall is the percent-
age of words in gold standard annotations that are
correctly predicted. For parsing, we use the stan-
dard parseval evaluation metrics: bracketing preci-
sion, recall and F-score.
1Note that the joint task refers to automatic segmentation
and tagging/parsing. It can be achieved using a pipeline system
or our joint decoding method.
For joint word segmentation and POS tagging, a
word is correctly predicted if both the boundaries
and the POS tag are correctly identified. For joint
segmentation, POS tagging, and parsing task, when
calculating the bracket scores using existing parseval
tools, we need to consider possible word segmenta-
tion errors. To do this, we add the word boundary
information in states ? a bracket is correct only if
its boundaries, label and word segmentation are all
correct. One example is shown in Figure 3. Notice
that identity unary rules are removed during evalua-
tion. The basic spans are characters, not words, be-
cause the number of words in reference and predic-
tion may be different. POS tags are removed since
they do not affect the bracket scores. If the segmen-
tation is perfect, then the bracket scores of the mod-
ified tree are exactly the same as the original tree.
This is similar to evaluating parsing performance on
speech transcripts with automatic sentence segmen-
tation (Roark et al2006).
! " # $ %
NP(0,2,5)
!" #$%
Shanghai office
NP
NR NN - - - - -
Shanghai office
Figure 3: Boundary information is added to states to cal-
culate the bracket scores in the face of word segmentation
errors. Left: the original parse tree, Right: the converted
parse tree. The numbers in the brackets are the indices of
the character boundaries based on word segmentation.
4.3 Parameter Estimation
We train three submodels using the gold features,
that is, POS tagger is trained using the perfect seg-
mentation, and parser is trained using perfect seg-
mentation and POS tags. Some studies reported
that better performance may be achieved by train-
ing subsequent models using representative output
of the preceding models (Che et al2009). Hence
for comparison we trained another parser using auto-
matically generated POS tags obtained from 10-fold
cross validation, but did not find significant differ-
ence between these two parsers when testing on the
perfectly segmented development dataset. Therefore
507
we use the parser trained with perfect POS tags for
the joint task.
Three hyper-parameters, ?, ?, and ?, are tuned
on development data using a heuristic search. Pa-
rameters that achieved the best joint parsing result
are selected. In the search, we fixed ? = 1 and
varied ?, ?. First, we set ? = 1, and enumerate
? = 14 ,
1
2 , 1, 2, . . . , and choose the best ?
?. Then,
we set ? = ?? and vary ? = 14 ,
1
2 , 1, 2, . . . , and
select the best ??.
Table 6 lists the parameters we used for training
the submodels, as well as the hyper-parameters for
joint decoding.
Model Parameter Value
Character based Gaussian prior 0.01
word segmentor # Feature 3,875,802
Word based Gaussian prior 0.01
word segmentor # Feature 312,533
POS tagger Gaussian prior 0.1
# Feature 48,608,802
Parser Iteration Number 10
# Feature 49,369,843
Hyper-parameter ? 4
Hyper-parameter ? 0.5
Joint Hyper-parameter ? 1
Beam Size B 20
Table 6: Parameters used in our system.
4.4 Experimental Results
In this section we first show that our sub-models are
better than or comparable to state-of-the-art systems,
and then the joint model is superior to the pipeline
approach.
4.4.1 Evaluating Sub-models
Table 7 shows word segmentation results using
our word segmentation submodel, in comparison to
a few state-of-the-art systems. For our segmentor,
we show results for two variants: one removes tran-
sition features as described in Section 3.1, the other
uses CRFs to learn the weights of transition features.
We can see that our system is competitive with al-
l the others except Sun?s that used additional idiom
resources. Our two word segmentors have similar
performance. Since the one without transition fea-
tures can be naturally integrated into the joint sys-
tem, we use it in the following joint tasks.
System P R F
(Jiang et al2008b) - - 97.74
(Jiang et al2008a) - - 97.85
(Kruengkrai et al2009) 97.46 98.29 97.87
(Zhang and Clark, 2010) - - 97.78
(Zhang and Clark, 2011) - - 97.78
(Sun, 2011) - - 98.17
Ours (w/o transition features) 97.45 98.24 97.85
Ours (with transition features) 97.44 98.23 97.84
Table 7: Word segmentation results.
For the POS tagging only task that takes gold s-
tandard word segmentation as input, we have two
systems. One uses the linear chain CRFs as de-
scribed in Section 3.2, the other is obtained using the
parser described in Section 3.3 ? the parser gener-
ates POS tag hypotheses when POS tag features are
not used. The POS tagging accuracy is 95.53% and
95.10% using these two methods respectively. The
better performance from the former system may be
because the local label dependency is more helpful
for POS tagging than the long distance dependencies
that might be noisy. This result also confirms our
choice of using an independent POS tagger for the
sub-model, rather than relying on a parser for POS
tagging. However, since there are no reported results
for this setup, we demonstrate the competence of our
POS tagger using the joint word segmentation and
POS tagging task. Table 8 shows the performance of
a few systems along with ours, all using the pipeline
approach where automatic segmentation is followed
by POS tagging. We can see that our POS tagger is
comparable to the others.
System P R F
(Jiang et al2008b) - - 93.37
(Jiang et al2008a) - - 93.41
(Kruengkrai et al2009) 93.28 94.07 93.67
(Zhang and Clark, 2010) - - 93.67
(Zhang and Clark, 2011) - - 93.67
(Sun, 2011) - - 94.02
Ours (pipeline) 93.10 93.96 93.53
Table 8: Results for the joint word segmentation and POS
tagging task.
For parsing, Table 9 presents the parsing result
on gold standard segmented sentence. Notice that
the result of (Harper and Huang, 2009; Zhang and
508
Clark, 2011) are not directly comparable to ours, as
they used a different data split. The best published
system result on CTB5 is Petrov and Klein?s, which
used PCFG with latent Variables. Our system per-
forms better mainly because it benefits from a large
amount of features.
System LP LR F
(Petrov and Klein, 2007) 84.8 81.9 83.3
(Jiang et al2009) - - 82.35
(Harper and Huang, 2009)* 83.22 82.84 83.03
(Zhang and Clark, 2011)* 78.6 78.0 78.3
Ours 84.57 83.68 84.13
Ours (w/ parent annotation) 83.35 82.73 83.04
Ours (no POS tag feature) 83.49 82.97 83.23
Table 9: Parsing results using gold standard word seg-
mentation.
For our parser, besides the model described in
Section 3.3, we tried two variations: one does not
use the automatic POS tag features, the other one is
learned on the parent annotated training data. The
results in Table 9 show that there is a performance
degradation when using parent annotation. This may
be due to the introduction of a large number of s-
tates, resulting in sparse features. We also notice
that with the help of the POS tag information, even
automatically generated, the parser gained 0.9% im-
provement in F-score. This demonstrates the advan-
tage of using a better independent POS tagger and
incorporating it in parsing.
Finally Table 10 shows the results for the three
tasks using our joint decoding method in compari-
son to the pipeline method. We can see that the joint
model outperforms the pipeline one. This is mainly
because of a better parsing module as well as join-
t decoding. In the table we also include results of
(Jiang et al2009), which is the only reported join-
t parsing result we found using the same data split
on CTB5. They achieved 80.28% parsing F-score
using automatic word segmentation. Their adapted
system Jiang09+ leveraged additional corpus to im-
prove Chinese word segmentation, resulting in an F-
score of 81.07%. Our system has better performance
than these.
System Task P R F
Jiang09 Parse - - 80.28
Jiang09+ Parse - - 81.07
Ours Seg. 97.45 98.24 97.85
Pipeline POS 93.10 93.96 93.53
Parse 81.87 81.65 81.76
Ours Seg. 97.56 98.36 97.96
Joint POS 93.43 94.20 93.81
Parse 83.03 82.66 82.85
Table 10: Results for the joint segmentation, tagging, and
parsing task using pipeline and joint models.
4.5 Error Analysis
We compared the results from the pipeline and our
joint decoding systems in order to understand the
impact of the joint model on word segmentation and
POS tagging. We notice that the joint model tend to
generate more words than the pipeline model. For
example, ?????? is one word in the pipeline
model, but correctly segmented as two words ??
?/??? in the joint model. This tendency of seg-
mentation also makes it fail to recognize some long
words, especially OOV words. For example, ??
??? is segmented as ???/??. In the data set,
we find that, the joint model corrected 10 missing
boundaries over the pipeline method, and introduced
3 false positive segmentation errors.
For the analysis of POS tags, we only examined
the words that are correctly segmented by both the
pipeline and the joint models. Table 11 shows the
increase and decrease of error patterns of the joint
model over the pipeline POS tagger. An error pat-
tern ?X ? Y? means that the word whose true tag is
?X? is assigned a tag ?Y?. All the patterns are ranked
in descending order of the reduction/increase of the
error number. We can see that the joint model has a
clear advantage in the disambiguation of {VV, NN}
and {DEG, DEC}, which results in the overall im-
proved performance. In contrast, the joint method
performs worse on ambiguous POS pairs such as
{NN,NR}. This observation is similar to those re-
ported by (Li et al2011; Hatori et al2011).
5 Conclusion
In this paper, we proposed a new algorithm for joint
Chinese word segmentation, POS tagging, and pars-
ing. Our algorithm is an extension of the CYK
509
error pattern # ? error pattern # ?
NN? VV 47 19 NN? NR 15 12
VV? NN 42 13 NR? NN 7 5
DEG? DEC 23 10 JJ? P 1 4
NN? JJ 29 8 NN? DT 2 4
DEC? DEG 11 4 P? VV 3 2
JJ? NN 12 4 AD? NN 1 2
Table 11: POS tagging error patterns. # means the error
number of the corresponding pattern made by the pipeline
tagging model. ? and ? mean the error number reduced
or increased by the joint model.
parsing method. The sub-models are independently
trained for the three tasks to reduce model complex-
ity and optimize individual sub-models. Our exper-
iments demonstrate the advantage of the joint mod-
els. In the future work, we will compare this joint
model to the pipeline approach that uses multiple
candidates or soft decisions in the early modules.
We will also investigate methods for joint learning
as well as ways to speed up the joint decoding algo-
rithm.
Acknowledgments The authors thank Zhongqiang
Huang for his help with experiments. This work
is partly supported by DARPA under Contrac-
t No. HR0011-12-C-0016. Any opinions expressed
in this material are those of the authors and do not
necessarily reflect the views of DARPA.
References
Wanxiang Che, Zhenghua Li, Yongqiang Li, Yuhang
Guo, Bing Qin, and Ting Liu. 2009. Multilingual
dependency-based syntactic and semantic parsing. In
Proceedings of CoNLL 09, pages 49?54.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings of
EMNLP 2002, pages 1?8.
Jenny Rose Finkel, Alex Kleeman, and Christopher D.
Manning. 2008. Efficient, feature-based, condition-
al random field parsing. In Proceedings of ACL-08:
HLT, pages 959?967.
Yoav Goldberg and Reut Tsarfaty. 2008. A single gener-
ative model for joint morphological segmentation and
syntactic parsing. In Proceedings of ACL 2008: HLT,
pages 371?379.
Spence Green and Christopher D. Manning. 2010. Better
arbic parsing: Baselines, evaluations, and analysis. In
Proceedings of Coling 2010, pages 394?402.
Mary Harper and Zhongqiang Huang. 2009. Chinese
statistical parsing. In Gale Book.
Jun Hatori, Takuya Matsuzaki, Yusuke Miyao, and
Jun?ichi Tsujii. 2011. Incremental joint pos tagging
and dependency parsing in chinese. In Proceedings of
IJCNLP 2011, pages 1216?1224.
Wenbin Jiang, Liang Huang, Qun Liu, and Yajuan Lu?.
2008a. A cascaded linear model for joint chinese word
segmentation and part-of-speech tagging. In Proceed-
ings of ACL 2008: HLT, pages 897?904.
Wenbin Jiang, Haitao Mi, and Qun Liu. 2008b. Word lat-
tice reranking for chinese word segmentation and part-
of-speech tagging. In Proceedings of Coling 2008,
pages 385?392.
Wenbin Jiang, Liang Huang, and Qun Liu. 2009. Au-
tomatic adaptation of annotation standards: Chinese
word segmentation and pos tagging ? a case study. In
Proceedings of ACL-IJCNLP 2009, pages 522?530.
Guangjin Jin and Xiao Chen. 2008. The fourth interna-
tional chinese language processing bakeoff: Chinese
word segmentation, named entity recognition and chi-
nese pos tagging. In Proceedings of Sixth SIGHAN
Workshop on Chinese Language Processing.
Canasai Kruengkrai, Kiyotaka Uchimoto, Jun?ichi Kaza-
ma, Yiou Wang, Kentaro Torisawa, and Hitoshi Isa-
hara. 2009. An error-driven word-character hybrid
model for joint chinese word segmentation and pos
tagging. In Proceedings of ACL 2009, pages 513?521.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic mod-
els for segmenting and labeling sequence data. In Pro-
ceedings of ICML 2001, pages 282?289.
John Lee, Jason Naradowsky, and David A. Smith. 2011.
A discriminative model for joint morphological disam-
biguation and dependency parsing. In Proceedings A-
CL 2011: HLT, pages 885?894.
Zhenghua Li, Min Zhang, Wanxiang Che, Ting Liu, Wen-
liang Chen, and Haizhou Li. 2011. Joint models for
chinese pos tagging and dependency parsing. In Pro-
ceedings of EMNLP 2011, pages 1180?1191.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Proceedings of NAACL
2007, pages 404?411.
Xian Qian, Qi Zhang, Yaqian Zhou, Xuanjing Huang, and
Lide Wu. 2010. Joint training and decoding using
virtual nodes for cascaded segmentation and tagging
tasks. In Proceedings of EMNLP 2010, pages 187?
195.
Brian Roark, Mary Harper, Eugene Charniak, Bonnie
Dorr, Mark Johnson, Jeremy G. Kahn, Yang Liu, Mari
Ostendorf, John Hale, Anna Krasnyanskaya, Matthew
Lease, Izhak Shafran, Matthew Snover, Robin Stew-
art, Lisa Yung, and Lisa Yung. 2006. Sparseval: E-
510
valuation metrics for parsing speech. In Proceedings
Language Resources and Evaluation (LREC).
Sunita Sarawagi and William W. Cohen. 2004. Semi-
markov conditional random fields for information ex-
traction. In Proceedings of NIPS 2004.
Weiwei Sun. 2011. A stacked sub-word model for join-
t chinese word segmentation and part-of-speech tag-
ging. In Proceedings of ACL 2011, pages 1385?1394.
Yue Zhang and Stephen Clark. 2008. Joint word seg-
mentation and POS tagging using a single perceptron.
In Proceedings of ACL 2008: HLT, pages 888?896.
Yue Zhang and Stephen Clark. 2010. A fast decoder for
joint word segmentation and POS-tagging using a sin-
gle discriminative model. In Proceedings of EMNLP
2010, pages 843?852.
Yue Zhang and Stephen Clark. 2011. Syntactic process-
ing using the generalized perceptron and beam search.
Comput. Linguist., 37(1):105?151.
Ruiqiang Zhang, Genichiro Kikui, and Eiichiro Sumi-
ta. 2006. Subword-based tagging for confidence-
dependent chinese word segmentation. In Proceedings
of the COLING/ACL 2006, pages 961?968.
Hai Zhao and Chunyu Kit. 2008. Unsupervised segmen-
tation helps supervised learning of character tagging
forword segmentation and named entity recognition.
In Proceedings of Sixth SIGHANWorkshop on Chinese
Language Processing, pages 106?111.
511
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1492?1502,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Fast Joint Compression and Summarization via Graph Cuts
Xian Qian and Yang Liu
The University of Texas at Dallas
800 W. Campbell Rd., Richardson, TX, USA
{qx,yangl}@hlt.utdallas.edu
Abstract
Extractive summarization typically uses sen-
tences as summarization units. In contrast,
joint compression and summarization can use
smaller units such as words and phrases, re-
sulting in summaries containing more infor-
mation. The goal of compressive summariza-
tion is to find a subset of words that max-
imize the total score of concepts and cut-
ting dependency arcs under the grammar con-
straints and summary length constraint. We
propose an efficient decoding algorithm for
fast compressive summarization using graph
cuts. Our approach first relaxes the length con-
straint using Lagrangian relaxation. Then we
propose to bound the relaxed objective func-
tion by the supermodular binary quadratic pro-
gramming problem, which can be solved ef-
ficiently using graph max-flow/min-cut. S-
ince finding the tightest lower bound suffers
from local optimality, we use convex relax-
ation for initialization. Experimental results
on TAC2008 dataset demonstrate our method
achieves competitive ROUGE score and has
good readability, while is much faster than the
integer linear programming (ILP) method.
1 Introduction
Automatic multi-document summarization helps
readers get the most important information from
large amounts of texts. Summarization techniques
can be roughly divided into two categories: extrac-
tive and abstractive. Extractive summarization casts
the summarization task as a sentence selection prob-
lem: identifying important summary sentences from
one or multiple documents. Many methods have
been developed in the past decades, including super-
vised approaches that use classifiers to predict sum-
mary sentences, graph based approaches to rank the
sentences, and recent global optimization methods
such as integer linear programming (Gillick et al,
2008) (ILP) and submodular maximization methods
(Lin and Bilmes, 2011). Though extractive summa-
rization is popular because of its simplicity and high
readability, it has limitations in that it selects each
sentence as a whole, and thus may miss informative
partial sentences.
To improve the informativeness, joint com-
pression and summarization was proposed (Berg-
Kirkpatrick et al, 2011), which uses words as sum-
marization units, unlike extractive summarization
where each sentence is a basic undecomposable u-
nit. To achieve better readability, manually defined
grammar constraints or automatically learned mod-
els based on syntax trees are added during the sum-
marization process. Up to now, the state of the art
compressive systems are based on integer linear pro-
gramming (ILP). Because ILP suffers from expo-
nential complexity, word-based compression sum-
marization is an order of magnitude slower than
sentence-based extraction.
One common way to solve an ILP problem is to
use its LP relaxation and round the results. How-
ever Berg-Kirkpatrick et al (2011) found that LP
relaxation gave poor results, finding unacceptably
suboptimal solutions. For speedup, they proposed a
two stage method where they performed some sen-
tence selection in the first step to reduce the number
of candidates. Despite their empirical success, such
1492
a pruning approach has its inherent problem in that
it may eliminate correct sentences in the first step.
Recently, Almeida and Martins (2013) proposed a
fast joint decoding algorithm based on dual decom-
position. For fast convergence, they added quadratic
penalty terms to alleviate the learning rate problem.
In this paper, we propose an efficient decoding al-
gorithm for fast ILP based compressive summariza-
tion using graph cuts. Our assumption is that all con-
cepts are word n-grams and non-negatively scored.
The rationale for the non-negativity assumption is s-
traightforward: the score of a concept reflects its in-
formativeness, hence should be non-negative. Given
a set of documents, each word is associated with a
binary variable, indicating whether the word is se-
lected in the summary. Our idea is to approximate
the ILP as a binary quadratic programming problem
where coefficients of all quadratic terms are non-
negative. It is well known that such binary quadrat-
ic function is supermodular, and its maximum can
be solved efficiently using graph max-flow/min-cut.
Hence the key is to find the coefficients of the super-
modular binary quadratic function (SBQF) so that
its maximum is close to the optimal ILP objective
function. Our solution consists of 3 steps. First,
we show that the subtree deletion model and gram-
mar constraints can be eliminated by adding SBQF-
s to the objective function. Second, we relax the
summary length constraint using Lagrangian relax-
ation. Third, we propose a family of SBQFs that
are lower bounds of the ILP objective function. S-
ince finding the tightest lower bound suffers from
local optimality, we choose to use convex relaxation
for initialization. To demonstrate our technique, we
conduct experiments on Text Analysis Conference
(TAC) datasets using the same train/test splits as pre-
vious work (Berg-Kirkpatrick et al, 2011). We com-
pare our approach with the state-of-the-art ILP based
approach in terms of summary quality (ROUGE s-
cores and sentence quality) and speed. Experimen-
tal results show that our proposed method achieves
competitive performance with ILP, while about 100
times faster.
2 Compressive Summarization
2.1 Extractive Summarization
As our method is an approximation of ILP based
method, we first briefly review the ILP based extrac-
tive summarization and compressive summarization.
Gillick and Favre (2009) introduced the concept-
based ILP for summarization. A concept is a basic
semantic unit. They used word bigrams as such lan-
guage concepts. Their system achieved the highest
ROUGE score on the TAC 2009 evaluation. This
approach selects sentences so that the total score
of language concepts appearing in the summary is
maximized. The association between the language
concepts and sentences serves as the constraints, in
addition to the summary length constraint.
Formally, given a set of sentences S = {sn}Nn=1,
extractive summarization can be represented by a bi-
nary vector y, where yn indicates whether sentence
sn is selected. Let C = {c1, . . . cJ} denote the set
of concepts in S, e.g., word bigrams (Gillick and
Favre, 2009). Each concept cj is associated with a
given score wj and a binary variable vj indicating
if cj is selected in the summary. Let njk denote the
index of the sentence containing the kth occurrence
of concept cj , and ln denote the length of sentence
sn. The ILP based extractive summarization system
can be formulated as below:
max
y,v
J
?
j=1
wjvj
s.t. vj =
?
k
ynjk 1 ? j ? J (1)
N
?
i=1
ynln ? L
v,y are binary
The first constraint is imposed by the relation be-
tween concept selection and sentence selection: s-
electing a sentence leads to the selection of all the
concepts it contains, and selecting a concept only
happens when it is present in at least one of the se-
lected sentences. The second constraint is the sum-
mary length constraint.
As solving an ILP problem is generally NP-hard,
pre-pruning of candidate concepts and sentences is
necessary for efficient summarization. For exam-
1493
ple, the ICSI system (Gillick et al, 2008) removed
the sentences that are too short or have non-overlap
with the queries, and concepts with document fre-
quency less than 3, resulting in 95.8 sentences and
about 80 concepts per topic on the TAC2009 dataset.
Therefore the actual scale of ILP is rather small after
pruning (e.g., 176 variables and 372 constraints per
topic). Empirical studies showed that such small s-
cale ILP can be solved within a few seconds (Gillick
and Favre, 2009).
2.2 Compressive Summarization
The quality of sentence-based extractive summariza-
tion is limited by the informativeness of the orig-
inal sentences and the summary length constraint.
To remove the unimportant part from a long sen-
tence, sentence compression is proposed to generate
more informative summaries (Liu and Liu, 2009; Li
et al, 2013a). Recent studies show that joint sen-
tence compression and extraction, namely compres-
sive summarization, outperforms pipeline systems
that run extractive summarization on the compressed
sentences or compress selected summary sentences
(Martins and Smith, 2009; Berg-Kirkpatrick et al,
2011; Chali and Hasan, 2012). In Berg-Kirkpatrick
et al (2011), compressive summarization inte-
grates the concept model for extractive summariza-
tion (Gillick and Favre, 2009) and subtree deletion
model for sentence compression. The score of a
compressive summary consists of two parts, scores
of selected concepts, and scores of the broken arcs
in the dependency parse trees. The selected word-
s must satisfy the length constraint and grammar
constraints that include subtree constraint and some
manually defined hard constraints.
Formally, let x = x1 . . . xI denote the word se-
quence of documents, where s1 = x1, . . . xl1 corre-
sponds to the first sentence, s2 = xl1+1, . . . , xl1+l2
corresponds to the second sentence, and so on. A
compressive summary can be represented by a bi-
nary vector z, where zi indicates whether word xi
is selected in the summary. Let ahm denote the arc
xh ? xm in the dependency parse tree of the cor-
responding sentence containing words xh and xm,
and A = {ahm} denote the set of dependency arcs.
The subtree constraint ensures that word xm is se-
lected only if its head xh is selected. In order to
guarantee the readability, grammar constraints are
added to prohibit the breaks of some specific arc-
s. For example, Clarke and Lapata (2008) nev-
er deleted an arc whose dependency label is SUB,
OBJ, PMOD, SBAR or VC. In this paper, we use
B ? A to denote the set of these arcs that must
not be broken in summarization. We use ojk to de-
note the indices of words corresponding to the kth
occurrence of cj . For example, suppose the jth
concept European Union appears twice in the doc-
ument: x22x23 = x50x51 =European Union, then
oj1 = {22, 23}, oj2 = {50, 51}.
The compressive summarization model can be
formulated as an integer programming problem
max
z,v
J
?
j=1
wj ? vj +
?
ahm?A
wahmzh(1? zm)
s.t. vj =
?
k
?
i?ojk
zi ?j
?
i
zi ? L
zh ? zm ?ahm ? A (2)
zh = zm ?ahm ? B
z,v are binary
According to the subtree deletion model, the score
of arc ahm is included if zh = 1 and zm = 0, which
can be formulated as wahm ? zh(1 ? zm). The first
constraint is similar to that in extractive summariza-
tion, that is, a concept is selected if and only if any
of its occurrence is selected. The third and fourth
constraints are the subtree constraints and manual-
ly defined grammar constraints respectively. In the
rest of the paper, without loss of generality, we re-
move the fourth constraint by directly substituting
one variable for the other.
Finding the optimal summary is generally NP-
hard. Unlike extractive summarization where the s-
cale of the problem (the number of sentences and
concepts) is small, the number of variables in com-
pressive summarization is linear in the number of
words, which is usually thousands on the TAC
datasets. Hence solving such a problem using ILP
based decoding algorithms is not efficient especially
when the document set is large.
1494
3 Fast Decoding via Graph Cuts
In this section, we introduce our fast decoding al-
gorithm. We assume that all the concepts are word
n-grams, and their scores are non-negative. The non-
negativity assumption can reduce the computational
complexity, but is also reasonable: the score of a
concept denotes its informativeness, hence should
be non-negative. For example, Li et al (2013b)
proposed to use the estimated normalized frequen-
cies of concepts as scores, which are essentially non-
negative. The basic idea of our method is to approx-
imate the above optimization problem (2) by the su-
permodular binary quadratic programming (SBQP)
problem:
max
z
?
i
?izi +
?
ij
?ijzizj
s.t. z is binary (3)
where ?ij ? 0. It is known that such a binary
quadratic function is supermodular, and its maxi-
mum can be solved efficiently using graph max-
flow/min-cut (Billionnet and Minoux, 1985; Kol-
mogorov and Zabih, 2004). Now the problem is to
find the optimal ?, ? for a good approximation.
3.1 Formulate Grammar Constraints and
Subtree Deletion Model by SBQF
We show that the subtree deletion model can be for-
mulated equivalently using SBQF. First, we can e-
liminate the constraint zh ? zm by adding a penalty
term to the objective function. That is,
max f(z)
s.t. zh ? zm
z is binary
is equivalent to
max f(z)??(1? zh)zm
s.t. z is binary
We can see that the penalty term??(1?zh)zm ex-
cludes zh = 0, zm = 1 from the feasible set, and
for zh ? zm, both problems have the same objective
function value. Hence the two problems are equiva-
lent. Notice that the coefficient of quadratic term in
??(1 ? zh)zm is positive, hence the penalty term
is supermodular.
Now we eliminate the third constraint in problem
(2) using the penalized objective function described
above. Note that the fourth constraint has been elim-
inated by variable substitution, we have
max
z,v
J
?
j=1
wj ? vj +
?
ahm?A
wahmzh(1? zm)
??
?
ahm?A
(1? zh)zm
s.t. vj =
?
k
?
i?ojk
zi ?j (4)
?
i
zi ? L
z,v are binary
We can see that for each arc ahm, there must be a
positive quadratic term +?zhzm in the objective
function, which guarantees the supermodularity of
the objective function, no matter what wahm is.
3.2 Eliminate Length Constraint Using
Lagrangian Relaxation
Problem (4) is NP-hard, because for any feasible v,
it is a SBQP with a length constraint. Since size con-
strained minimum cut problem is generally NP-hard
(Nagano et al, 2011), Problem (4) can not be cast
as a SBQP as long as P ?= NP. One popular way to
deal with the size constrained optimization problem
is Lagrangian relaxation. We introduce Lagrangian
multiplier ? to the length constraint in Problem (4),
and get
min
?
max
z,v
J
?
j=1
wj ? vj +
?
ahm?A
wahmzh(1? zm)
??
?
ahm?A
(1? zh)zm
+?(L?
?
i
zi)
s.t. vj =
?
k
?
i?ojk
zi ?j (5)
? ? 0
z,v are binary
We solve the relaxed problem iteratively. In each
iteration, we fix ? and solve the inner maximiza-
tion problem (details described below). The score of
1495
each word is penalized by ? ? with larger ?, few-
er words are selected. Hence the summary length
can be adjusted by ?. The optimal ? can be found
using binary search. We maintain an upper bound
?max , and a lower bound ?min, which is initially 0.
In each iteration, we choose ? = 12(?max + ?min)
and search the optimal z. If the duality gap vanish-
es, i.e., ?(L ?
?
i zi) = 0 and
?
i zi ? L, then we
get the global solution of Problem (4). Otherwise,
if
?
i zi > L, then the current ? is too small, so we
set ?min = ?; otherwise, ? > 0 and
?
i zi < L,
we set ?max = ?. The search process terminates if
?max ? ?min is less than a predefined threshold.
3.3 Eliminate v Using Supermodular
Relaxation
Now we consider the inner maximization Problem
(5). It is still not a SBQP, since the objective func-
tion is not a linear function of zizj . We propose to
approximate the objective function using SBQP. Our
solution consists of two steps. First we relax the first
constraint of Problem (5) by bounding the objec-
tive function with a family of supermodular pseudo
boolean functions (Boros and Hammer, 2002). Sec-
ond we reformulate these pseudo boolean functions
equivalently as quadratic functions.
Similar to the bounding strategy in (Qian and Liu,
2013), we relax the logical disjunction by lineariza-
tion. Using the fact that for any binary vector a, we
have
?
ai = max
p??
?
i
piai
where ? denotes the probability simplex
? = {p|
?
k
pk = 1, pk ? 0}
We have
vj =
?
k
?
i?ojk
zi
= max
pj??
?
k
pjk
?
i?ojk
zi
Plug the equation above into the objective function
of Problem (5), we get the following optimization
problem
max
z,p
J
?
j=1
?
?
?
k
pjkwj
?
i?ojk
zi
?
?
+
?
ahm?A
wahmzh(1? zm)
??
?
ahm?A
(1? zh)zm
+?(L?
?
i
zi)
s.t. z is binary (6)
pj ? ? ?j
LetQ(p, z) denote the objective function of Prob-
lem (6). Given p, we can see that Q is a supermod-
ular pseudo boolean function because coefficients
of all non-linear terms are non-negative. Using the
fact that for any binary vector a = [a1, . . . ar]T ,
ai ? {0, 1}, 1 ? i ? r,
r
?
i=1
ai = max
b?{0,1}
( r
?
i=1
ai ? r + 1
)
b
(Freedman and Drineas, 2005), we get the following
equivalent optimization problem of Problem (6)
max
z,p,q
J
?
j=1
?
k
pjkwjqjk
?
?
?
i?ojk
zi ? |ojk|+ 1
?
?
+
?
ahm?A
wahmzh(1? zm)
??
?
ahm?A
(1? zh)zm
+?(L?
?
i
zi)
s.t. z,q are binary (7)
pj ? ? ?j
where |ojk| is the size of ojk.
Let R(z,p,q) denote the objective function of
Problem (7), to search the optimal point, we al-
ternatively update p and z,q. First we initialize
p = p(0). In each iteration, we first fix p. It is ob-
vious that Problem (7) is a SBQP, hence the optimal
z,q can be solved efficiently using max-flow/min-
cut. Then we fix z,q, and update p using projected
1496
subgradient. That is
pnewj = P?
(
pj +
?R
?pj
?
)
(8)
where ? > 0 is the step size in line search, and func-
tion P?(q) denotes the projection of q onto the fea-
sible set ?
P?(q) = min
p??
?p? q?2
which can be solved efficiently by sorting (Duchi et
al., 2008).
3.4 Initialize p Using Convex Relaxation
Since R is non-concave, searching its maximum us-
ing subgradient method suffers from local optimali-
ty. Though one can use techniques such as branch-
and-bound for exact inference (Qian and Liu, 2013;
Gormley and Eisner, 2013), here for fast decoding,
we use convex relaxation to choose a good seed
p(0). Recall that pjk denotes the percentage of the
kth occurrence contributing to cj . The larger pjk is,
the more likely the kth occurrence is selected. To
estimate such likelihood, we replace the binary con-
straint in extractive summarization (Problem (1)) by
0 ? y,v ? 1, since solving a relaxed LP is much
faster than ILP. Suppose y? is the optimal solution
for such a relaxed LP problem, we initialize p by
pjk =
y?njk
?
k y?njk
(9)
If for all k, y?njk = 0, then we initialize pjk using
uniform distribution
pjk =
1
|oj |
where |oj | is the frequency of cj .
3.5 Summary
For clarity, we summarize our decoding algorithm in
Algorithm 1. Initial ?max can be arbitrarily large. In
our experiments, we set ?max =
?
j wj , which em-
pirically guarantees the summary length
?
i zi ? L
when ? = ?max. The choice of the step size for
updating p is similar to the projected subgradien-
t method in dual decomposition (Koo et al, 2010).
Algorithm 1 Compressive Summarization via
Graph Cuts
Require: Scores of concepts {wj} and arcs {wahm},
max summary length L.
Ensure: Compressive summarization z?, where zi indi-
cates whether the ith word is selected.
Solve the relaxed LP of Problem (1) (replace the binary
constraint by 0 ? y,v ? 1) to get y.
Initialize p(0) using Eq (9).
Initialize sufficient large ?max, and ?min = 0
while ?max ? ?min > ? do
Set ? = 12 (?min + ?max)
Set p = p(0).
repeat
Fix p, solve Problem (7) to get z using max-
flow/min-cut.
Update p using Eq (8).
until convergence
if
?
i zi > L then
?min = ?
else if
?
i zi < L then
?max = ?
else
break
end if
end while
4 Features and Hard Constraints
We choose discriminative models to learn the scores
of concepts and arcs. For concept cj , its score is
wj = ?Tconceptfconcept(cj)
where fconcept(cj) is the feature vector of cj , and
?concept is the corresponding weight vector of fea-
ture fconcept(cj). Similarly, score wahm is defined
as
wahm = ?
T
arcfarc(ahm)
Though our algorithm can handle general word n-
gram concepts, we restrict the concepts to word bi-
grams, which have been widely used recently in the
sentence-based ILP extractive summarization sys-
tems. For a concept cj , we define the following
features, some of which have been used in previous
work (Brandow et al, 1995; Aker and Gaizauskas,
2009; Edmundson, 1969; Radev, 2001; Li et al,
2013b). All of these features are non-negative.
? Term frequency: the frequency of cj in the giv-
en topic.
1497
? Stop word ratio: ratio of stop words in cj . The
value can be {0, 0.5, 1}.
? Similarity with topic title: the number of com-
mon words in these two strings, divided by the
length of the longer string.
? Document ratio: percentage of documents con-
taining cj .
? Sentence ratio: percentage of sentences con-
taining cj .
? Sentence-title similarity: word uni-
gram/bigrams cosine similarity between
the sentence containing cj and the topic title.
For concepts appearing in multiple sentences,
we choose the maximal similarity.
? Sentence-query similarity: word uni-
gram/bigram cosine similarity between
the sentence containing cj and the topic query
(concatenation of topic title and description).
For concepts appearing in multiple sentences,
we choose the maximal similarity.
? Sentence position: position of the sentence
containing cj in the document. For concepts
appearing in multiple sentences, we choose the
minimum.
? Sentence length: length of the sentence con-
taining cj . For concepts appearing in multiple
sentences, we choose the maximum.
? Paragraph starter: binary feature indicating
whether cj appears in the first sentence of a
paragraph.
For subtree deletion model, we define the follow-
ing features for arc ahm.
? POS tags of head word xh and child word xm
and their concatenations.
? Dependency label of arc ahm and its parent arc.
? Word xm if xm is a conjunction word or prepo-
sition word. Word xh if xm is a conjunction
word or preposition word.
? Binary feature indicating whether the modifier
xm is a temporal word such as Friday.
We also define some hard constraints for subtree
deletion to improve the readability of the generated
compressed sentences.
? C0 Arc ahm can be cut only if one of the two
conditions holds: (1) there is a comma, colon,
or semicolon between the head and the modifi-
er; (2) the modifier word is a preposition (POS
tag is IN) or a wh-word, such as what, who,
whose (corresponds to POS tag IN, WDT, WP,
WP$, WRB).
? C1 Arcs with dependency labels SUB, OBJ,
PRD, SBAR or VC can not be cut.
? C2 Arcs in set phrases like so far, more than,
according to can not be cut.
? C3 All arcs in coordinate structures can not be
cut, such as cats and dogs.
Note that compared with previous work, our com-
pression is more conservative. Constraint C0 al-
lows only a small portion of arcs to be cut. This
is based on our observation of the sentence com-
pression corpus: removing preposition phrases (PP)
or sub-clauses can greatly reduce the length of sen-
tence, while hurting the readability little. Cutting
other arcs like NMOD usually removes only one or
two words, and possibly affects the sentence?s read-
ability.
5 Experimental Results
5.1 Experimental Setup
Due to the lack of training data for compressive
summarization, we learn the subtree deletion mod-
el and the concept model separately. Specifically,
the sentence compression dataset (Clarke and La-
pata, 2008) (referred as CL08) is used for subtree
deletion model training (?arc). A sentence pair in
the corpus is kept for training the subtree deletion
model if the compressed sentence can be derived by
deleting subtrees from the parse tree of the origi-
nal sentence. There are 3, 178 out of 5, 739 such
pairs. The concept model (?concept) is learned from
the TAC2009 dataset. We create the oracle extrac-
tive summaries with the maximal bigram recall as
the reference summary. TAC2010 data is used as
1498
Corpus Sent. Words Topics
Train TAC2009 4, 216 117, 304 44
CL08 3, 178 52, 624 N/A
Develop TAC2010 2, 688 72, 609 46
Test TAC2008 4, 518 123, 946 48
Table 1: Corpus statistics. Training data consist of
two parts, TAC2009 for learning the concept mod-
el, CL08 (Clarke and Lapata, 2008) for learning the
subtree deletion model.
development set for various parameter tuning. Table
1 has the descriptions of all the data used.
We choose averaged perceptron for fast training.
The number of iterations is tuned on the develop-
ment data. Remind that our algorithm is based on the
assumption that scores of concepts are non-negative,
?j, wj ? 0. We assume that feature vector fconcept
is non-negative (e.g., term frequency, n-gram fea-
tures), then ?concept ? 0 is required to guarantee the
non-negativity of wj . Therefore, we project ?concept
onto the non-negative space after each iteration. S-
ince training is offline, we use ILP based exact in-
ference for accurate learning. 1
To control the contributions of the concept mod-
el and the subtree deletion model, we introduce a
parameter ?, and modify the original maximization
problem (Problem 2) to:
max
z,v
J
?
j=1
wj ? vj + ??
?
ahm?A
wahmzh(1? zm)
We tune ? on TAC2010 dataset. For max-flow/min-
cut, in our experiments, we implemented the im-
proved shortest augmented path (SAP) method (Ed-
monds and Karp, 1972).
For performance measure of the summaries, we
use both ROUGE and linguistic quality. ROUGE
has been widely used for summarization perfor-
mance and can measure the informativeness of the
summaries (content match between system and ref-
erence summaries). Since joint compression and
summarization tends to pick isolated words to max-
imize the information coverage in the system gener-
ated summaries, it may have poor readability. There-
fore we conduct human evaluation for the linguis-
1we choose the GLPK as our ILP solver, which is used in
(Berg-Kirkpatrick et al, 2011)
tic quality for various systems. The linguistic qual-
ity consists of two parts. One evaluates the gram-
mar quality within a sentence. Annotators marked
if a compressed sentence is grammatically correc-
t. Typical grammar errors include lack of verb or
subordinate clause. The other evaluates the coher-
ence between sentences, including the order of sen-
tences and irrelevant sentences. For compressive
summaries, we removed the sentences with gram-
mar errors when evaluating coherence. The overall
linguistic quality score is the combined score of the
percentage of grammatically correct sentences and
the correct ordering of the summary sentences. The
score is scaled and ranges from 1 (bad) to 10 (good).
5.2 Results on the Development Set
We conducted a series of experiments on the de-
velopment dataset to investigate the effect of the
non-negative score assumption, SBQP approxima-
tion, and initialization. First, we build a stan-
dard ILP based compressive summarizer without the
non-negative score assumption. We varied ? over
{2?4, 2?3, . . . 24} and selected the optimal ? = 2?2
according to both ROUGE-2 score and linguistic
quality. This interpolation weight is used in all the
other experiments.
To study the impact of the non-negative score as-
sumption, we build another summarizer by replac-
ing the concept model with the one trained under the
non-negative constraint. We also compared three d-
ifferent initialization strategies for p. The first one
is uniform initialization, i.e., pjk = 1|oj | . The second
one is a greedy approach, where extractive summa-
rization is obtained by greedy search (i.e., add the
top ranked sentence iteratively), then we use the cor-
responding y and Eq (9) to initialize p. The last one
is our convex relaxation method described in Sec-
tion 3.4.
Table 2 shows the comparison results. For com-
parison, we also include the sentence-based ILP ex-
tractive summarization results. We can see that the
impact of initial p is substantial. Using convex re-
laxation helps our method to survive from local opti-
mality. The non-negativity assumption has very lit-
tle effect on the standard compressive summariza-
tion (comparing the first two rows). This empir-
ical result demonstrates the appropriateness of the
assumption we use in our proposed method.
1499
System R-2 LQ
ILP (? = 2?2) 11.22 6.3
ILP (Non Neg.) 11.18 6.4
Graph Cut (uniform) 9.54 5.9
Graph Cut (greedy) 10.13 6.2
Graph Cut (LP) 11.06 6.1
Sent Extractive 10.11 7.3
Table 2: Experimental results on developmen-
t dataset. R-2 and LQ are short for ROUGE-2 score
multiplied by 100, and linguistic quality respective-
ly.
5.3 Results on Test Dataset
Table 3 shows the summarization results for various
systems on the TAC2008 data set. We show both the
summarization performance and the speed2 of the
system. The presented systems include our graph-
cut based method, the ILP based compression and
summarization, and the sentence-based extractive
summarization. ILP 2-step refers to the 2-step fast
decoding strategy proposed by (Berg-Kirkpatrick et
al., 2011).
We also list the performance of some state-of-the-
art systems, including the two ICSI systems (Gillick
et al, 2008), the compressive summarization sys-
tem of Berg-Kirkpatrick et al (2011) (GBK?11),
the multi-aspect ILP system of Woodsend and Lapa-
ta (2012)(WL?12) and the dual decomposition based
system (Almeida andMartins, 2013) (AM?13). Note
that for these referred systems since the linguistic
quality results are not comparable due to different
judgment methods. For our graph-cut based method,
to study the tradeoff between the readability of the
summary and the ROUGE scores, we present two
versions for this method: one uses all the constraints
(C0-C3), the other does not use C0.
We can see that our proposed method balanced
speed and quality. Compared with ILP, we achieved
competitive ROUGE scores, but with about 100x
speedup. Our method is also faster than the 2-step
ILP system. We also tried another state-of-the-art
LP solver, Gurobi version 5.53, it achieves 0.17 sec-
onds per topic, much faster than GLPK, but stil-
2For fair comparison, we only recode the running time for
decoding. Other time costs like feature extraction, IO opera-
tions are excluded.
3www.gurobi.com
System R-2 R-SU4 LQ sec.
Graph Cut 11.74 14.54 6.5 0.056
Graph Cut w/o C0 12.05 14.71 5.4 0.053
ILP 11.86 14.62 6.6 5.5
ILP (Non Neg.) 11.82 14.60 6.6 5.2
ILP (2-step) 11.72 14.49 6.5 1.1
Sent Extractive 11.06 13.93 7.1 0.13
ICSI-1 11.0 13.4 - 0.38?
ICSI-2 11.1 14.3 - -
BGK?11 11.70 14.38 6.5? -
WL?12 11.37 14.47 - -
AM?13 12.30+ 15.18+ 4.2? 0.41?
Table 3: Experimental results on TAC2008 dataset.
Columns 2-5 are scores of ROUGE-2, ROUGE-
SU4, linguistic quality, and speed (seconds per top-
ic). ROUGE-2 and ROUGE-SU4 scores are multi-
plied by 100. All the experiments are conducted on
the platform Intel Core i5-2500 CPU 3.30GHz. ?
numbers are not directly comparable due to differ-
ent annotations or platforms. + extra resources are
used.
l slower than ours. Regarding the grammar con-
straints used in our system, from the two result-
s for our graph-cut based method, we can see that
adding constraint C0 significantly decreases the R-2
score but improves the language quality. This shows
that word-based joint compression and summariza-
tion can improve ROUGE score; however, we need
to keep in mind about linguistic quality and find a
tradeoff between the ROUGE score and the linguis-
tic quality. Almeida and Martins (2013) trained their
model on extra corpora using multi-task learning,
and achieved better results than ours. The results
of our system and theirs showed that Lagrangian re-
laxation based method combined with combinatorial
optimization algorithms such as dynamic program-
ming or minimum cut can exploit the inner structure
of problems and achieve significant speedup over
ILP.
Four example summaries produced by our system
are shown below. Words in gray are not selected in
the summary.
1500
India?s space agency is ready to send a man to space within sev-
en years if the government gives the nod, while preparations have
lready begun for the launch of an unmanned lunar mission, a top
official said. India will launch more missions to the moon if its
maiden unmanned spacecraft Chandrayaan-1, slated to be launched
by 2008, is successful a top space fficial said Tuesday. The Unit-
ed States, the European Space Agency, China, Japan and India are
all planning lunar missions during the ext decade.India is ?a step
ahead? of China in satellite technology and can surpass Beijing
in space research by tapping the talent of its huge pool of young
scientists, India?s space research chief said Monday. The space
agencies of India and France signed an agreement on Friday to co-
operate in launching a satellite in four years that will help make
climate predictions more accurate. The Indian Space Research Or-
ganization (ISRO) has short-listed experiments from five nation-
s including the United States, Britain and Germany, for a slot on
India?s unmanned moon mission Chandrayaan-1 to be undertaken
by 2006-2007, the Press Trust of India (PTI) reported Monday. A
three-member Afghan delegation is in Bangalore seeking help to
set up a high-tech telemedicine facility in 10 Afghan cities linked
via Indian satellites, Indo-Asian News Service reported Saturday.
A woman was killed in Mississippi when a tree crashed on her car,
becoming the 11th fatality blamed on the powerful Hurricane Kat-
rina that slammed the US Gulf coast after pounding Florida, local
TV reportedMonday. The bill for the Hurricane Katrina disaster ef-
fort has so far reached 2.87 billion dollars, federal officials said on
Tuesday. The official death toll from Hurricane Katrina has risen
to 118 people in and around the swamped city of New Orleans,
officials said Thursday. The Foreign Ministry on Friday reported
the first confirmed death of a Guatemalan due to Hurricane Kat-
rina in the United States. The Ugandan government has pledged
200,000 US dollars toward relief and rebuilding efforts in the after-
math of Hurricane Katrina, local press reported on Friday. Swiss
Reinsurance Co., the world?s second largest reinsurance company
on Monday doubled to 40 billion US dollars its initial estimate of
the global insured losses caused by Hurricane Katrina in the United
States.
The A380 ?superjumbo?, which will be presented to the world in
a lavish ceremony in southern France on Tuesday, will be prof-
itable from 2008, its maker Airbus told the French financial news-
paper La Tribune. The A380 will take over from the Boeing 747
as the biggest jet in the skies. An association of residents living n-
ear Paris?s Charles-de-Gaulle airport on Wednesday denounced the
noise pollution generated by the giant Airbus A380, after the new
airliner?s maiden flight. One problem that Airbus is encountering
with its new A380 is that the craft pushes the envelope on the max-
imum size of a commercial airplane. With a whisper more than a
roar, the largest passenger airliner ever built, the Airbus 380, took
off on its maiden flight Wednesday.
?When she came in, she was in good spirits,? a prison staffer told
the New York Daily News. Martha Stewart, the American celebrity
homemaker who had her own cooking and home improvement TV
show, reported to a federal prison in Alderson, West Virginia, on
Friday to serve a five-month sentence for lying about a stock sale.
Home fashion guru Martha Stewart said on Friday that she has ad-
justed to prison life and is keeping busy behind bars since reporting
a week ago to a federal penal camp in West Virginia, where she
is serving a five-month sentence for lying about a stock sale. The
lawyer said he did not know what she is writing, but Stewart has
suggested since her conviction that she might write a book about
her recent experience with the legal system. Walter Dellinger, the
lawyer leading the appeal, said on NBC?s ?Today? that Stewart is
exploring ?innovative ways to do microwave cooking? The lawyer
said he did not know with her fellow inmates. As Martha Stewart
arrives at the red-brick federal prison in Alderson, W. Va., on Fri-
day to begin a five-month sentence, the company she founded is
focused both on life without her and on life once she returns.
In most cases, the removed phrases do not hurt the
readability of the summaries. The errors are mainly
caused by the break of sub-clauses or main claus-
es that are separated by commas, for example, the
fourth sentence in the last summary, The lawyer said
he did not know what she is writing. The compressed
sentence is grammatically correct, but semantically
incomplete. Other errors are due to the lack of verb,
subject, or object, or incorrect removal of PP, such
as the last sentence of the last summary.
6 Conclusion
In this paper, we propose a fast decoding algorith-
m for compressive summarization using graph cuts.
Our idea is to approximate the original ILP prob-
lem using supermodular binary quadratic program-
ming (SBQP) problem. Under the assumption that
scores of concepts are non-negative, we eliminate
subtree constraints and grammar constraints, and
relax the length constraint and non-supermodular
part of the problem step by step. Our experimen-
tal results showed that the graph cut based method
achieved competitive performance compared to ILP,
while about 100 times faster.
There are several possibilities for further research
involving our graph cut algorithms. One idea is to
apply it to the language model based compression
method (Clarke and Lapata, 2008). The other is
to adapt it to social media text summarization task,
where text is much more noisy. As graph cut is a
general method, applying it to other binary struc-
tured learning tasks is also an interesting direction.
Acknowledgments
We?d like to thank three anonymous reviewers for
their valuable comments. This work is partly sup-
ported by NSF award IIS-0845484 and DARPA un-
der Contract No. FA8750-13-2-0041. Any opinions
expressed in this material are those of the authors
and do not necessarily reflect the views of the fund-
ing agencies.
References
A. Aker and R. Gaizauskas. 2009. Summary generation
for toponym-referenced images using object type lan-
guage models. In Proceedings of RANLP.
1501
Miguel Almeida and Andre Martins. 2013. Fast and ro-
bust compressive summarization with dual decompo-
sition and multi-task learning. In Proceedings of ACL,
pages 196?206, August.
Taylor Berg-Kirkpatrick, Dan Gillick, and Dan Klein.
2011. Jointly learning to extract and compress. In
Proceedings of ACL-HLT, pages 481?490, June.
A. Billionnet and M. Minoux. 1985. Maximizing a su-
permodular pseudoboolean function: A polynomial al-
gorithm for supermodular cubic functions. Discrete
Applied Mathematics, 12(1):1 ? 11.
Endre Boros and Peter L. Hammer. 2002. Pseudo-
boolean optimization. Discrete Applied Mathematics,
123(1C3):155 ? 225.
Ronald Brandow, Karl Mitze, and Lisa F. Rau. 1995.
Automatic condensation of electronic publications by
sentence selection. Information Processing & Man-
agement, 31(5):675 ? 685.
Yllias Chali and Sadid A. Hasan. 2012. On the effective-
ness of using sentence compression models for query-
focused multi-document summarization. In COLING,
pages 457?474.
James Clarke and Mirella Lapata. 2008. Global in-
ference for sentence compression: An integer linear
programming approach. J. Artif. Intell. Res. (JAIR),
31:399?429.
John Duchi, Shai Shalev-Shwartz, Yoram Singer, and
Tushar Chandra. 2008. Efficient projections onto the
L1-ball for learning in high dimensions. In Proceed-
ings of ICML, pages 272?279.
Jack Edmonds and Richard M. Karp. 1972. Theoret-
ical improvements in algorithmic efficiency for net-
work flow problems. J. ACM, 19(2):248?264, April.
H. P. Edmundson. 1969. New methods in automatic ex-
tracting. J. ACM, 16(2):264?285, April.
Daniel Freedman and Petros Drineas. 2005. Energy min-
imization via graph cuts: Settling what is possible. In
CVPR (2), pages 939?946.
Dan Gillick and Benoit Favre. 2009. A scalable global
model for summarization. In Proceedings of the Work-
shop on Integer Linear Programming for Natural Lan-
guage Processing, pages 10?18, June.
Dan Gillick, Benoit Favre, and Dilek Hakkani-Tur. 2008.
The ICSI summarization system at tac 2008. In Pro-
ceedings of the Text Understanding Conference.
Matthew R. Gormley and Jason Eisner. 2013. Noncon-
vex global optimization for latent-variable models. In
Proceedings of ACL, pages 444?454, August.
Vladimir Kolmogorov and Ramin Zabih. 2004. What en-
ergy functions can be minimized via graph cuts. IEEE
Transactions on Pattern Analysis and Machine Intelli-
gence, 26:65?81.
Terry Koo, Alexander M. Rush, Michael Collins, Tommi
Jaakkola, and David Sontag. 2010. Dual decomposi-
tion for parsing with non-projective head automata. In
Proceedings of EMNLP 2010, pages 1288?1298, Oc-
tober.
Chen Li, Fei Liu, Fuliang Weng, and Yang Liu. 2013a.
Document summarization via guided sentence com-
pression. In Proceedings of EMNLP (to appear), Oc-
tober.
Chen Li, Xian Qian, and Yang Liu. 2013b. Using super-
vised bigram-based ILP for extractive summarization.
In Proceedings of ACL, pages 1004?1013, August.
Hui Lin and Jeff Bilmes. 2011. A class of submodular
functions for document summarization. In Proceed-
ings of ACL, pages 510?520, June.
Fei Liu and Yang Liu. 2009. From extractive to abstrac-
tive meeting summaries: Can it be done by sentence
compression? In Proceedings of ACL-IJCNLP 2009,
pages 261?264, August.
Andre? F. T. Martins and Noah A. Smith. 2009. Summa-
rization with a joint model for sentence extraction and
compression. In Proceedings of the Workshop on In-
teger Linear Programming for Natural Langauge Pro-
cessing, ILP ?09, pages 1?9.
Kiyohito Nagano, Yoshinobu Kawahara, and Kazuyuk-
i Aihara. 2011. Size-constrained submodular min-
imization through minimum norm base. In ICML,
pages 977?984.
Xian Qian and Yang Liu. 2013. Branch and bound algo-
rithm for dependency parsing with non-local features.
TACL, 1:105?151.
Dragomir R. Radev. 2001. Experiments in single and
multidocument summarization using mead. In In First
Document Understanding Conference.
Kristian Woodsend and Mirella Lapata. 2012. Mul-
tiple aspect summarization using integer linear pro-
gramming. In Proceedings of EMNLP-CoNLL, pages
233?243, July.
1502
Proceedings of NAACL-HLT 2013, pages 820?825,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Disfluency Detection Using Multi-step Stacked Learning
Xian Qian and Yang Liu
Computer Science Department
The University of Texas at Dallas
{qx,yangl}@hlt.utdallas.edu
Abstract
In this paper, we propose a multi-step stacked
learning model for disfluency detection. Our
method incorporates refined n-gram features
step by step from different word sequences.
First, we detect filler words. Second, edited
words are detected using n-gram features ex-
tracted from both the original text and filler fil-
tered text. In the third step, additional n-gram
features are extracted from edit removed texts
together with our newly induced in-between
features to improve edited word detection. We
useMax-MarginMarkov Networks (M3Ns) as
the classifier with the weighted hamming loss
to balance precision and recall. Experiments
on the Switchboard corpus show that the re-
fined n-gram features from multiple steps and
M3Ns with weighted hamming loss can signif-
icantly improve the performance. Our method
for disfluency detection achieves the best re-
ported F-score 0.841 without the use of addi-
tional resources.1
1 Introduction
Detecting disfluencies in spontaneous speech can
be used to clean up speech transcripts, which help-
s improve readability of the transcripts and make it
easy for downstream language processing modules.
There are two types of disfluencies: filler words in-
cluding filled pauses (e.g., ?uh?, ?um?) and discourse
markers (e.g., ?I mean?, ?you know?), and edited
words that are repeated, discarded, or corrected by
1Our source code is available at
http://code.google.com/p/disfluency-detection/downloads/list
the following words. An example is shown below
that includes edited words and filler words.
I want a flight to Boston
? ?? ?
edited
uh I mean
? ?? ?
filler
to Denver
Automatic filler word detection is much more ac-
curate than edit detection as they are often fixed
phrases (e.g., ?uh?, ?you know?, ?I mean?), hence
our work focuses on edited word detection.
Many models have been evaluated for this task.
Liu et al (2006) used Conditional Random Fields
(CRFs) for sentence boundary and edited word de-
tection. They showed that CRFs significantly out-
performed Maximum Entropy models and HMM-
s. Johnson and Charniak (2004) proposed a TAG-
based noisy channel model which showed great im-
provement over boosting based classifier (Charniak
and Johnson, 2001). Zwarts and Johnson (2011)
extended this model using minimal expected F-loss
oriented n-best reranking. They obtained the best re-
ported F-score of 83.8% on the Switchboard corpus.
Georgila (2009) presented a post-processing method
during testing based on Integer Linear Programming
(ILP) to incorporate local and global constraints.
From the view of features, in addition to tex-
tual information, prosodic features extracted from
speech have been incorporated to detect edited
words in some previous work (Kahn et al, 2005;
Zhang et al, 2006; Liu et al, 2006). Zwarts and
Johnson (2011) trained an extra language model on
additional corpora, and used output log probabili-
ties of language models as features in the reranking
stage. They reported that the language model gained
about absolute 3% F-score for edited word detection
on the Switchboard development dataset.
820
In this paper, we propose a multi-step stacked
learning approach for disfluency detection. In our
method, we first perform filler word detection, then
edited word detection. In every step, we generate
new refined n-gram features based on the processed
text (remove the detected filler or edited words from
the previous step), and use these in the next step.
We also include a new type of features, called in-
between features, and incorporate them into the last
step. For edited word detection, we use Max-Margin
Markov Networks (M3Ns) with weighted hamming
loss as the classifier, as it can well balance the pre-
cision and recall to achieve high performance. On
the commonly used Switchboard corpus, we demon-
strate that our proposed method outperforms other
state-of-the-art systems for edit disfluency detection.
2 Balancing Precision and Recall Using
Weighted M3Ns
We use a sequence labeling model for edit detection.
Each word is assigned one of the five labels: BE (be-
ginning of the multi-word edited region), IE (in the
edited region), EE (end of the edited region), SE (s-
ingle word edited region), O (other). For example,
the previous sentence is represented as:
I/O want/O a/O flight/O to/BE Boston/EE uh/O
I/O mean/O to/O Denver/O
We use the F-score as the evaluation metrics
(Zwarts and Johnson, 2011; Johnson and Charniak,
2004), which is defined as the harmonic mean of the
precision and recall of the edited words:
P = #correctly predicted edited words
#predicted edited words
R = #correctly predicted edited words
#gold standard edited words
F = 2? P ?R
P + R
There are many methods to train the sequence mod-
el, such as CRFs (Lafferty et al, 2001), averaged
structured perceptrons (Collins, 2002), structured
SVM (Altun et al, 2003), online passive aggressive
learning (Crammer et al, 2006). Previous work has
shown that minimizing F-loss is more effective than
minimizing log-loss (Zwarts and Johnson, 2011),
because edited words are much fewer than normal
words.
In this paper, we use Max-margin Markov Net-
works (Taskar et al, 2004) because our preliminary
results showed that they outperform other classifier-
s, and using weighted hamming loss is simple in this
approach (whereas for perceptron or CRFs, the mod-
ification of the objective function is not straightfor-
ward).
The learning task for M3Ns can be represented as
follows:
min
?
1
2
C?
?
x,y
?x,y?f(x, y)?22 +
?
x,y
?x,yL(x, y)
s.t.
?
y
?x,y = 1 ?x
?x,y ? 0, ?x, y
The above shows the dual form for trainingM3Ns,
where x is the observation of a training sample,
y ? Y is a label. ? is the parameter needed
to be optimized, C > 0 is the regularization pa-
rameter. ?f(x, y) is the residual feature vector:
f(x, y?) ? f(x, y), where y? is the true label of x.
L(x, y) is the loss function. Taskar et al (2004) used
un-weighted hamming loss, which is the number
of incorrect components: L(x, y) =
?
t ?(yt, y?t),
where ?(a, b) is the binary indicator function (it is 0
if a = b). In our work, we use the weighted ham-
ming loss:
L(x, y) =
?
t
v(yt, y?t)?(yt, y?t)
where v(yt, y?t) is the weighted loss for the error
when y?t is mislabeled as yt. Such a weighted loss
function allows us to balance the model?s precision
and recall rates. For example, if we assign a large
value to v(O, ?E) (?E denotes SE, BE, IE, EE), then
the classifier is more sensitive to false negative er-
rors (edited word misclassified as non-edited word),
thus we can improve the recall rate. In our work,
we tune the weight matrix v using the development
dataset.
3 Multi-step Stacked Learning for Edit
Disfluency Detection
Rather than just using the above M3Ns with some
features, in this paper we propose to use stacked
learning to incorporate gradually refined n-gram fea-
tures. Stacked learning is a meta-learning approach
(Cohen and de Carvalho, 2005). Its idea is to use two
821
(or more) levels of predictors, where the outputs of
the low level predictors are incorporated as features
into the next level predictors. It has the advantage
of incorporating non-local features as well as non-
linear classifiers. In our task, we do not just use the
classifier?s output (a word is an edited word or not)
as a feature, rather we use such output to remove the
disfluencies and extract new n-gram features for the
subsequent stacked classifiers. We use 10 fold cross
validation to train the low level predictors. The fol-
lowing describes the three steps in our approach.
3.1 Step 1: Filler Word Detection
In the first step, we automatically detect filler word-
s. Since filler words often occur immediately after
edited words (before the corrected words), we ex-
pect that removing them will make rough copy de-
tection easy. For example, in the previous example
shown in Section 1, if ?uh I mean? is removed, then
the reparandum ?to Boston? and repair ?to Denver?
will be adjacent and we can use word/POS based n-
gram features to detect that disfluency. Otherwise,
the classifier needs to skip possible filler words to
find the rough copy of the reparandum.
For filler word detection, similar to edited word
detection, we define 5 labels: BP , IP , EP , SP , O.
We use un-weighted hamming loss to learn M3Ns
for this task. Since for filler word detection, our per-
formance metric is not F-measure, but just the over-
all accuracy in order to generate cleaned text for sub-
sequent n-gram features, we did not use the weight-
ed hamming hoss for this. The features we used are
listed in Table 1. All n-grams are extracted from the
original text.
3.2 Step 2: Edited Word Detection
In the second step, edited words are detected using
M3Ns with the weighted-hamming loss. The fea-
tures we used are listed in Table 2. All n-grams in
the first step are also used here. Besides that, word
n-grams, POS n-grams and logic n-grams extracted
from filler word removed text are included. Feature
templates I(w0, w?i) is to generate features detecting
rough copies separated by filler words.
3.3 Step 3: Refined Edited Word Detection
In this step, we use n-gram features extracted from
the text after removing edit disfluencies based on
unigrams w0, w?1, w1, w?2, w2
p0, p?1, p1, p?2, p2, w0p0
bigrams w?1w0, w0w1, p?1p0, p0p1
trigrams p?2p?1p0, p?1p0p1, p0p1p2
logic unigrams I(wi, w0), I(pi, p0), ?4 ? i ? 4
logic bigrams I(wi?1wi, w?1, w0)
I(pi?1pi, p?1p0)
I(wiwi+1, w0w1)
I(pipi+1, p0p1), ?4 ? i ? 4
transitions y?1y0
Table 1: Feature templates for filler word detection.
w0, p0 denote the current word and POS tag respective-
ly. w?i denotes the ith word to the left, wi denotes the
ith word to the right. The logic function I(a, b) indicates
whether a and b are identical (eigher unigrams or bigram-
s).
All templates in Table 1
unigrams w?1, w?2, w?3, w?4
bigrams p0p?1, p0p?2, p0p?3, p0p?4
w0p?1, w0p?2, w0p?3, w0p?4
w0p1, w0p2, w0p3, w0p4
logic unigrams I(w0, w?i), 1 ? i ? 4
transitions p0y?1y0
Table 2: Feature templates for edit detection (step 2).
w?i, p?i denote the ith word/POS tag to the right in the filler
words removed text. If current word w0 is removed in
step 1, we use its original n-gram features rather than the
refined n-gram features.
the previous step. According to our analysis of the
errors produced by step 2, we observed that many
errors occurred at the boundaries of the disfluen-
cies, and the word bigrams after removing the edited
words are unnatural. The following is an example:
? Ref: The new type is prettier than what
their/SE they used to look like.
? Sys: The new type is prettier than what/BE
their/EE they used to look like.
Using the system?s prediction, we would have bi-
gram than they, which is odd. Usually, the pronoun
following than is accusative case. We expect adding
n-gram features derived from the cleaned-up sen-
tences would allow the new classifier to fix such hy-
pothesis. This kind of n-gram features is similar to
the language models used in (Zwarts and Johnson,
822
2011). They have the benefit of measuring the flu-
ency of the cleaned text.
Another common error we noticed is caused by
the ambiguities of coordinates, because the coordi-
nates have similar patterns as rough copies. For ex-
ample,
? Coordinates: they ca n?t decide which are the
good aspects and which are the bad aspects
? Rough Copies: it/BE ?s/IE a/IE pleasure/IE
to/EE it s good to get outside
To distinguish the rough copies and the coordinate
examples shown above, we analyze the training data
statistically. We extract all the pieces lying between
identical word bigrams AB . . . AB. The observation
is that coordinates are often longer than edited se-
quences. Hence we introduce the in-between fea-
tures for each word. If a word lies between identical
word bigrams, then its in-between feature is the log
length of the subsequence lying between the two bi-
grams; otherwise, it is zero (we use log length to
avoid sparsity). We also used other patterns such as
A . . . A and ABC . . . ABC, but they are too noisy or
infrequent and do not yield much performance gain.
Table 3 lists the feature templates used in this last
step.
All templates in Table 1, Table 2
word n-grams w??1 , w0w??1
in-between LAB , w0bAB , bAB
Table 3: Feature templates for refined edit detection (step
3). w??i denotes the ith word tag to the right in the edit-
ed word removed text. LAB denotes the log length of
the sub-sequence in the pattern AB. . . AB, bAB indicates
whether the current word lies between two identical bi-
grams.
4 Experiments
4.1 Experimental Setup
We use the Switchboard corpus in our experimen-
t, with the same train/develop/test split as the pre-
vious work (Johnson and Charniak, 2004). We al-
so remove the partial words and punctuation from
the training and test data for the reason to simulate
the situation when speech recognizers are used and
such kind of information is not available (Johnson
and Charniak, 2004).
We tuned the weight matrix for hamming loss on
the development dataset using simple grid search.
The diagonal elements are fixed at 0; for false pos-
itive errors, O ? ?E (non-edited word mis-labeled
as edited word), their weights are fixed at 1; for false
negative errors, ?E ? O, we tried the weight from
1 to 3, and increased the weight 0.5 each time. The
optimal weight matrix is shown in Table 4. Note
that we use five labels in the sequence labeling task;
however, for edited word detection evaluation, it is
only a binary task, that is, all of the words labeled
with ?E will be mapped to the class of edited words.
P
P
P
P
P
P
P
truth
predict BE IE EE SE O
BE 0 1 1 1 2
IE 1 0 1 1 2
EE 1 1 0 1 2
SE 1 1 1 0 2
O 1 1 1 1 0
Table 4: Weighted hamming loss for M3Ns.
4.2 Results
We compare several sequence labeling models:
CRFs, structured averaged perceptron (AP), M3Ns
with un-weighted/weighted loss, and online passive-
aggressive (PA) learning. For each model, we tuned
the parameters on the development data: Gaussian
prior for CRFs is 1.0, iteration number for AP is 10,
iteration number and regularization penalty for PA
are 10 and 1. For M3Ns, we use Structured Sequen-
tial Minimal Optimization (Taskar, 2004) for model
training. Regularization penalty is C = 0.1 and iter-
ation number is 30.
Table 5 shows the results using different models
and features. The baseline models use only the n-
grams features extracted from the original text. We
can see that M3Ns with the weighted hamming loss
achieve the best performance, outperforming all the
other models. Regarding the features, the gradually
added n-gram features have consistent improvemen-
t for all models. Using the weighted hamming loss
in M3Ns, we observe a gain of 2.2% after deleting
filler words, and 1.8% after deleting edited words. In
our analysis, we also noticed that the in-between fea-
823
CRF AP PA M3N w. M3N
Baseline 78.8 79.0 78.9 79.4 80.1
Step 2 81.0 81.1 81.1 81.5 82.3
Step 3 82.9 83.0 82.8 83.3 84.1
Table 5: Effect of training strategy and recovered features
for stacked learning. F scores are reported. AP = Aver-
aged Perceptron, PA = online Passive Aggresive, M3N =
un-weighted M3Ns, w. M3N = weighted M3Ns.
tures yield about 1% improvement in F-score for all
models (the gain of step 3 over step 2 is because of
the in-between features and the new n-gram features
extracted from the text after removing previously
detected edited words). We performed McNemar?s
test to evaluate the significance of the difference a-
mong various methods, and found that when using
the same features, weighted M3Ns significantly out-
performs all the other models (p value < 0.001).
There are no significant differences among CRFs,
AP and PA. Using recovered n-gram features and in-
between features significantly improves all sequence
labeling models (p value < 0.001).
We also list the state-of-the-art systems evaluat-
ed on the same dataset, as shown in Table 6. We
achieved the best F-score. The most competitive
system is (Zwarts and Johnson, 2011), which uses
extra resources to train language models.
System F score
(Johnson and Charniak, 2004) 79.7
(Kahn et al, 2005) 78.2
(Zhang et al, 2006)? 81.2
(Georgila, 2009)? 80.1
(Zwarts and Johnson, 2011)+ 83.8
This paper 84.1
Table 6: Comparison with other systems. ? they used
the re-segmented Switchboard corpus, which is not ex-
actly the same as ours. ? they reported the F-score of
BE tag (beginning of the edited sequences). + they used
language model learned from 3 additional corpora.
5 Conclusion
In this paper, we proposed multi-step stacked learn-
ing to extract n-gram features step by step. The first
level removes the filler words providing new ngram-
s for the second level to remove edited words. The
third level uses the n-grams from the original tex-
t and the cleaned text generated by the previous t-
wo steps for accurate edit detection. To minimize
the F-loss approximately, we modified the hamming
loss in M3Ns. Experimental results show that our
method is effective, and achieved the best reported
performance on the Switchboard corpus without the
use of any additional resources.
Acknowledgments
We thank three anonymous reviewers for their valu-
able comments. This work is partly supported by
DARPA under Contract No. HR0011-12-C-0016
and FA8750-13-2-0041. Any opinions expressed in
this material are those of the authors and do not nec-
essarily reflect the views of DARPA.
References
Yasemin Altun, Ioannis Tsochantaridis, and Thomas
Hofmann. 2003. Hidden markov support vector ma-
chines. In Proc. of ICML.
Eugene Charniak and Mark Johnson. 2001. Edit detec-
tion and parsing for transcribed speech. In Proc. of
NAACL.
William W. Cohen and Vitor Rocha de Carvalho. 2005.
Stacked sequential learning. In Proc. of IJCAI.
Michael Collins. 2002. Discriminative training methods
for hidden markov models: Theory and experiments
with perceptron algorithms. In Proc. of EMNLP.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-
Shwartz, Yoram Singer, and Yoram Singer. 2006. On-
line passive-aggressive algorithms. Journal of Ma-
chine Learning Research.
Kallirroi Georgila. 2009. Using integer linear program-
ming for detecting speech disfluencies. In Proc. of
NAACL.
Mark Johnson and Eugene Charniak. 2004. A TAG-
based noisy-channel model of speech repairs. In Proc.
of ACL.
Jeremy G. Kahn, Matthew Lease, Eugene Charniak,
Mark Johnson, and Mari Ostendorf. 2005. Effective
use of prosody in parsing conversational speech. In
Proc. of HLT-EMNLP.
John D. Lafferty, AndrewMcCallum, and Fernando C. N.
Pereira. 2001. Conditional random fields: Probabilis-
tic models for segmenting and labeling sequence data.
In Proc. of ICML.
Yang Liu, E. Shriberg, A. Stolcke, D. Hillard, M. Osten-
dorf, and M. Harper. 2006. Enriching speech recog-
nition with automatic detection of sentence bound-
824
aries and disfluencies. IEEE Transactions on Audio,
Speech, and Language Processing, 14(5).
Ben Taskar, Carlos Guestrin, and Daphne Koller. 2004.
Max-margin markov networks. In Proc. of NIPS.
Ben Taskar. 2004. Learning Structured Prediction Mod-
els: A Large Margin Approach. Ph.D. thesis, Stanford
University.
Qi Zhang, Fuliang Weng, and Zhe Feng. 2006. A pro-
gressive feature selection algorithm for ultra large fea-
ture spaces. In Proc. of ACL.
Simon Zwarts and Mark Johnson. 2011. The impact of
language models and loss functions on repair disfluen-
cy detection. In Proc. of ACL-HLT.
825
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 166?170,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
A Two-step Approach to Sentence Compression of Spoken Utterances
Dong Wang, Xian Qian, Yang Liu
The University of Texas at Dallas
dongwang,qx,yangl@hlt.utdallas.edu
Abstract
This paper presents a two-step approach to
compress spontaneous spoken utterances. In
the first step, we use a sequence labeling
method to determine if a word in the utterance
can be removed, and generate n-best com-
pressed sentences. In the second step, we
use a discriminative training approach to cap-
ture sentence level global information from
the candidates and rerank them. For evalua-
tion, we compare our system output with mul-
tiple human references. Our results show that
the new features we introduced in the first
compression step improve performance upon
the previous work on the same data set, and
reranking is able to yield additional gain, espe-
cially when training is performed to take into
account multiple references.
1 Introduction
Sentence compression aims to preserve the most im-
portant information in the original sentence with
fewer words. It can be used for abstractive summa-
rization where extracted important sentences often
need to be compressed and merged. For summariza-
tion of spontaneous speech, sentence compression
is especially important, since unlike fluent and well-
structured written text, spontaneous speech contains
a lot of disfluencies and much redundancy. The fol-
lowing shows an example of a pair of source and
compressed spoken sentences1 from human annota-
tion (removed words shown in bold):
[original sentence]
1For speech domains, ?sentences? are not clearly defined.
We use sentences and utterances interchangeably when there is
no ambiguity.
and then um in terms of the source the things uh the
only things that we had on there I believe were whether...
[compressed sentence]
and then in terms of the source the only things that we
had on there were whether...
In this study we investigate sentence compres-
sion of spoken utterances in order to remove re-
dundant or unnecessary words while trying to pre-
serve the information in the original sentence. Sen-
tence compression has been studied from formal
text domain to speech domain. In text domain,
(Knight and Marcu, 2000) applies noisy-channel
model and decision tree approaches on this prob-
lem. (Galley and Mckeown, 2007) proposes to use a
synchronous context-free grammars (SCFG) based
method to compress the sentence. (Cohn and La-
pata, 2008) expands the operation set by including
insertion, substitution and reordering, and incorpo-
rates grammar rules. In speech domain, (Clarke and
Lapata, 2008) investigates sentence compression in
broadcast news using an integer linear programming
approach. There is only a few existing work in spon-
taneous speech domains. (Liu and Liu, 2010) mod-
eled it as a sequence labeling problem using con-
ditional random fields model. (Liu and Liu, 2009)
compared the effect of different compression meth-
ods on a meeting summarization task, but did not
evaluate sentence compression itself.
We propose to use a two-step approach in this pa-
per for sentence compression of spontaneous speech
utterances. The contributions of our work are:
? Our proposed two-step approach allows us to
incorporate features from local and global lev-
els. In the first step, we adopt a similar se-
quence labeling method as used in (Liu and
Liu, 2010), but expanded the feature set, which
166
results in better performance. In the second
step, we use discriminative reranking to in-
corporate global information about the com-
pressed sentence candidates, which cannot be
accomplished by word level labeling.
? We evaluate our methods using different met-
rics including word-level accuracy and F1-
measure by comparing to one reference com-
pression, and BLEU scores comparing with
multiple references. We also demonstrate that
training in the reranking module can be tailed
to the evaluation metrics to optimize system
performance.
2 Corpus
We use the same corpus as (Liu and Liu, 2010)
where they annotated 2,860 summary sentences in
26 meetings from the ICSI meeting corpus (Murray
et al, 2005). In their annotation procedure, filled
pauses such as ?uh/um? and incomplete words are
removed before annotation. In the first step, 8 anno-
tators were asked to select words to be removed to
compress the sentences. In the second step, 6 an-
notators (different from the first step) were asked
to pick the best one from the 8 compressions from
the previous step. Therefore for each sentence, we
have 8 human compressions, as well a best one se-
lected by the majority of the 6 annotators in the sec-
ond step. The compression ratio of the best human
reference is 63.64%.
In the first step of our sentence compression ap-
proach (described below), for model training we
need the reference labels for each word, which rep-
resents whether it is preserved or deleted in the com-
pressed sentence. In (Liu and Liu, 2010), they used
the labels from the annotators directly. In this work,
we use a different way. For each sentence, we still
use the best compression as the gold standard, but
we realign the pair of the source sentence and the
compressed sentence, instead of using the labels
provided by annotators. This is because when there
are repeated words, annotators sometimes randomly
pick removed ones. However, we want to keep the
patterns consistent for model training ? we always
label the last appearance of the repeated words as
?preserved?, and the earlier ones as ?deleted?. An-
other difference in our processing of the corpus from
the previous work is that when aligning the original
and the compressed sentence, we keep filled pauses
and incomplete words since they tend to appear to-
gether with disfluencies and thus provide useful in-
formation for compression.
3 Sentence Compression Approach
Our compression approach has two steps: in the
first step, we use Conditional Random Fields (CRFs)
to model this problem as a sequence labeling task,
where the label indicates whether the word should be
removed or not. We select n-best candidates (n = 25
in our work) from this step. In the second step we
use discriminative training based on a maximum En-
tropy model to rerank the candidate compressions,
in order to select the best one based on the quality
of the whole candidate sentence, which cannot be
performed in the first step.
3.1 Generate N-best Candidates
In the first step, we cast sentence compression as
a sequence labeling problem. Considering that in
many cases phrases instead of single words are
deleted, we adopt the ?BIO? labeling scheme, simi-
lar to the name entity recognition task: ?B? indicates
the first word of the removed fragment, ?I? repre-
sents inside the removed fragment (except the first
word), and ?O? means outside the removed frag-
ment, i.e., words remaining in the compressed sen-
tence. Each sentence with n words can be viewed as
a word sequence X1, X2, ..., Xn, and our task is to
find the best label sequence Y1, Y2, ..., Yn where Yi
is one of the three labels. Similar to (Liu and Liu,
2010), for sequence labeling we use linear-chain
first-order CRFs. These models define the condi-
tional probability of each labeling sequence given
the word sequence as:
p(Y |X) ?
exp
Pn
k=1(
P
j ?jfj(yk, yk?1, X) +
P
i ?igi(xk, yk, X))
where fj are transition feature functions (here first-
order Markov independence assumption is used); gi
are observation feature functions; ?j and ?i are their
corresponding weights. To train the model for this
step, we use the best reference compression to obtain
the reference labels (as described in Section 2).
In the CRF compression model, each word is rep-
resented by a feature vector. We incorporate most
of the features used in (Liu and Liu, 2010), includ-
ing unigram, position, length of utterance, part-of-
speech tag as well as syntactic parse tree tags. We
did not use the discourse parsing tree based features
because we found they are not useful in our exper-
iments. In this work, we further expand the feature
set in order to represent the characteristics of disflu-
encies in spontaneous speech as well as model the
adjacent output labels. The additional features we
167
introduced are:
? the distance to the next same word and the next
same POS tag.
? a binary feature to indicate if there is a filled
pause or incomplete word in the following 4-
word window. We add this feature since filled
pauses or incomplete words often appear after
disfluent words.
? the combination of word/POS tag and its posi-
tion in the sentence.
? language model probabilities: the bigram prob-
ability of the current word given the previous
one, and followed by the next word, and their
product. These probabilities are obtained from
the Google Web 1T 5-gram.
? transition features: a combination of the current
output label and the previous one, together with
some observation features such as the unigram
and bigrams of word or POS tag.
3.2 Discriminative Reranking
Although CRFs is able to model the dependency
of adjacent labels, it does not measure the quality
of the whole sentence. In this work, we propose
to use discriminative training to rerank the candi-
dates generated in the first step. Reranking has been
used in many tasks to find better global solutions,
such as machine translation (Wang et al, 2007),
parsing (Charniak and Johnson, 2005), and disflu-
ency detection (Zwarts and Johnson, 2011). We use
a maximum Entropy reranker to learn distributions
over a set of candidates such that the probability of
the best compression is maximized. The conditional
probability of output y given observation x in the
maximum entropy model is defined as:
p(y|x) = 1Z(x)exp
[?k
i=1 ?if(x, y)
]
where f(x, y) are feature functions and ?i are their
weighting parameters; Z(x) is the normalization
factor.
In this reranking model, every compression can-
didate is represented by the following features:
? All the bigrams and trigrams of words and POS
tags in the candidate sentence.
? Bigrams and trigrams of words and POS tags in
the original sentence in combination with their
binary labels in the candidate sentence (delete
the word or not). For example, if the origi-
nal sentence is ?so I should go?, and the can-
didate compression sentence is ?I should go?,
then ?so I 10?, ?so I should 100? are included
in the features (1 means the word is deleted).
? The log likelihood of the candidate sentence
based on the language model.
? The absolute difference of the compression ra-
tio of the candidate sentence with that of the
first ranked candidate. This is because we try
to avoid a very large or small compression ra-
tio, and the first candidate is generally a good
candidate with reasonable length.
? The probability of the label sequence of the
candidate sentence given by the first step CRFs.
? The rank of the candidate sentence in 25 best
list.
For discriminative training using the n-best can-
didates, we need to identify the best candidate from
the n-best list, which can be either the reference
compression (if it exists on the list), or the most
similar candidate to the reference. Since we have
8 human compressions and also want to evaluate
system performance using all of them (see exper-
iments later), we try to use multiple references in
this reranking step. In order to use the same train-
ing objective (maximize the score for the single best
among all the instances), for the 25-best list, if m
reference compressions exist, we split the list into
m groups, each of which is a new sample containing
one reference as positive and several negative can-
didates. If no reference compression appears in 25-
best list, we just keep the entire list and label the in-
stance that is most similar to the best reference com-
pression as positive.
4 Experiments
We perform a cross-validation evaluation where one
meeting is used for testing and the rest of them are
used as the training set. When evaluating the system
performance, we do not consider filled pauses and
incomplete words since they can be easily identi-
fied and removed. We use two different performance
metrics in this study.
? Word-level accuracy and F1 score based on the
minor class (removed words). This was used
in (Liu and Liu, 2010). These measures are ob-
tained by comparing with the best compression.
In evaluation we map the result using ?BIO? la-
bels from the first-step compression to binary
labels that indicate a word is removed or not.
168
? BLEU score. BLEU is a widely used metric
in evaluating machine translation systems that
often use multiple references. Since there is a
great variation in human compression results,
and we have 8 reference compressions, we ex-
plore using BLEU for our sentence compres-
sion task. BLEU is calculated based on the pre-
cision of n-grams. In our experiments we use
up to 4-grams.
Table 1 shows the averaged scores of the cross
validation evaluation using the above metrics for
several methods. Also shown in the table is the com-
pression ratio of the system output. For ?reference?,
we randomly choose one compression from 8 ref-
erences, and use the rest of them as references in
calculating the BLEU score. This represents human
performance. The row ?basic features? shows the
result of using all features in (Liu and Liu, 2010)
except discourse parsing tree based features, and us-
ing binary labels (removed or not). The next row
uses this same basic feature set and ?BIO? labels.
Row ?expanded features? shows the result of our ex-
panded feature set using ?BIO? label set from the
first step of compression. The last two rows show
the results after reranking, trained using one best ref-
erence or 8 reference compressions, respectively.
accuracy F1 BLEU ratio (%)
reference 81.96 69.73 95.36 76.78
basic features (Liu
and Liu, 2010)
76.44 62.11 91.08 73.49
basic features, BIO 77.10 63.34 91.41 73.22
expanded features 79.28 67.37 92.70 72.17
reranking
train w/ 1 ref 79.01 67.74 91.90 70.60
reranking
train w/ 8 refs 78.78 63.76 94.21 77.15
Table 1: Compression results using different systems.
Our result using the basic feature set is similar to
that in (Liu and Liu, 2010) (their accuracy is 76.27%
when compression ratio is 0.7), though the experi-
mental setups are different: they used 6 meetings as
the test set while we performed cross validation. Us-
ing the ?BIO? label set instead of binary labels has
marginal improvement for the three scores. From
the table, we can see that our expanded feature set is
able to significantly improve the result, suggesting
the effectiveness of the new introduced features.
Regarding the two training settings in reranking,
we find that there is no gain from reranking when
using only one best compression, however, train-
ing with multiple references improves BLEU scores.
This indicates the discriminative training used in
maximum entropy reranking is consistent with the
performance metrics. Another reason for the per-
formance gain for this condition is that there is less
data imbalance in model training (since we split the
n-best list, each containing fewer negative exam-
ples). We also notice that the compression ratio af-
ter reranking is more similar to the reference. As
suggested in (Napoles et al, 2011), it is not appro-
priate to compare compression systems with differ-
ent compression ratios, especially when considering
grammars and meanings. Therefore for the com-
pression system without reranking, we generated re-
sults with the same compression ratio (77.15%), and
found that using reranking still outperforms this re-
sult, 1.19% higher in BLEU score.
For an analysis, we check how often our sys-
tem output contains reference compressions based
on the 8 references. We found that 50.8% of sys-
tem generated compressions appear in the 8 refer-
ences when using CRF output with a compression
ration of 77.15%; and after reranking this number
increases to 54.8%. This is still far from the oracle
result ? for 84.7% of sentences, the 25-best list con-
tains one or more reference sentences, that is, there
is still much room for improvement in the reranking
process. The results above also show that the token
level measures by comparing to one best reference
do not always correlate well with BLEU scores ob-
tained by comparing with multiple references, which
shows the need of considering multiple metrics.
5 Conclusion
This paper presents a 2-step approach for sentence
compression: we first generate an n-best list for each
source sentence using a sequence labeling method,
then rerank the n-best candidates to select the best
one based on the quality of the whole candidate sen-
tence using discriminative training. We evaluate the
system performance using different metrics. Our re-
sults show that our expanded feature set improves
the performance across multiple metrics, and rerank-
ing is able to improve the BLEU score. In future
work, we will incorporate more syntactic informa-
tion in the model to better evaluate sentence quality.
We also plan to perform a human evaluation for the
compressed sentences, and use sentence compres-
sion in summarization.
169
6 Acknowledgment
This work is partly supported by DARPA un-
der Contract No. HR0011-12-C-0016 and NSF
No. 0845484. Any opinions expressed in this ma-
terial are those of the authors and do not necessarily
reflect the views of DARPA or NSF.
References
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. In Proceedings of the 43rd Annual Meeting
on Association for Computational Linguistics, pages
173?180, Stroudsburg, PA, USA. Proceedings of ACL.
James Clarke and Mirella Lapata. 2008. Global infer-
ence for sentence compression an integer linear pro-
gramming approach. Journal of Artificial Intelligence
Research, 31:399?429, March.
Trevor Cohn and Mirella Lapata. 2008. Sentence com-
pression beyond word deletion. In Proceedings of
COLING.
Michel Galley and Kathleen R. Mckeown. 2007. Lex-
icalized Markov grammars for sentence compression.
In Proceedings of HLT-NAACL.
Kevin Knight and Daniel Marcu. 2000. Statistics-based
summarization-step one: Sentence compression. In
Proceedings of AAAI.
Fei Liu and Yang Liu. 2009. From extractive to abstrac-
tive meeting summaries: can it be done by sentence
compression? In Proceedings of the ACL-IJCNLP.
Fei Liu and Yang Liu. 2010. Using spoken utterance
compression for meeting summarization: a pilot study.
In Proceedings of SLT.
Gabriel Murray, Steve Renals, and Jean Carletta. 2005.
Extractive summarization of meeting recordings. In
Proceedings of EUROSPEECH.
Courtney Napoles, Benjamin Van Durme, and Chris
Callison-Burch. 2011. Evaluating Sentence Com-
pression: Pitfalls and Suggested Remedies. In Pro-
ceedings of the Workshop on Monolingual Text-To-Text
Generation, pages 91?97, Portland, Oregon, June. As-
sociation for Computational Linguistics.
Wen Wang, A. Stolcke, and Jing Zheng. 2007. Rerank-
ing machine translation hypotheses with structured
and web-based language models. In Proceedings of
IEEE Workshop on Speech Recognition and Under-
standing, pages 159?164, Kyoto.
Simon Zwarts and Mark Johnson. 2011. The impact of
language models and loss functions on repair disflu-
ency detection. In Proceedings of ACL.
170
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1004?1013,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Using Supervised Bigram-based ILP for Extractive Summarization
Chen Li, Xian Qian, and Yang Liu
The University of Texas at Dallas
Computer Science Department
chenli,qx,yangl@hlt.utdallas.edu
Abstract
In this paper, we propose a bigram based
supervised method for extractive docu-
ment summarization in the integer linear
programming (ILP) framework. For each
bigram, a regression model is used to es-
timate its frequency in the reference sum-
mary. The regression model uses a vari-
ety of indicative features and is trained dis-
criminatively to minimize the distance be-
tween the estimated and the ground truth
bigram frequency in the reference sum-
mary. During testing, the sentence selec-
tion problem is formulated as an ILP prob-
lem to maximize the bigram gains. We
demonstrate that our system consistently
outperforms the previous ILP method on
different TAC data sets, and performs
competitively compared to the best results
in the TAC evaluations. We also con-
ducted various analysis to show the im-
pact of bigram selection, weight estima-
tion, and ILP setup.
1 Introduction
Extractive summarization is a sentence selection
problem: identifying important summary sen-
tences from one or multiple documents. Many
methods have been developed for this problem, in-
cluding supervised approaches that use classifiers
to predict summary sentences, graph based ap-
proaches to rank the sentences, and recent global
optimization methods such as integer linear pro-
gramming (ILP) and submodular methods. These
global optimization methods have been shown to
be quite powerful for extractive summarization,
because they try to select important sentences and
remove redundancy at the same time under the
length constraint.
Gillick and Favre (Gillick and Favre, 2009) in-
troduced the concept-based ILP for summariza-
tion. Their system achieved the best result in the
TAC 09 summarization task based on the ROUGE
evaluation metric. In this approach the goal is
to maximize the sum of the weights of the lan-
guage concepts that appear in the summary. They
used bigrams as such language concepts. The as-
sociation between the language concepts and sen-
tences serves as the constraints. This ILP method
is formally represented as below (see (Gillick and
Favre, 2009) for more details):
max ?i wici (1)
s.t. sjOccij ? ci (2)?
j sjOccij ? ci (3)?
j ljsj ? L (4)
ci ? {0, 1} ?i (5)
sj ? {0, 1} ?j (6)
ci and sj are binary variables (shown in (5) and
(6)) that indicate the presence of a concept and
a sentence respectively. wi is a concept?s weight
and Occij means the occurrence of concept i in
sentence j. Inequalities (2)(3) associate the sen-
tences and concepts. They ensure that selecting a
sentence leads to the selection of all the concepts
it contains, and selecting a concept only happens
when it is present in at least one of the selected
sentences.
There are two important components in this
concept-based ILP: one is how to select the con-
cepts (ci); the second is how to set up their weights
(wi). Gillick and Favre (Gillick and Favre, 2009)
used bigrams as concepts, which are selected from
a subset of the sentences, and their document fre-
quency as the weight in the objective function.
In this paper, we propose to find a candidate
summary such that the language concepts (e.g., bi-
grams) in this candidate summary and the refer-
ence summary can have the same frequency. We
expect this restriction is more consistent with the
1004
ROUGE evaluation metric used for summarization
(Lin, 2004). In addition, in the previous concept-
based ILP method, the constraints are with respect
to the appearance of language concepts, hence it
cannot distinguish the importance of different lan-
guage concepts in the reference summary. Our
method can decide not only which language con-
cepts to use in ILP, but also the frequency of these
language concepts in the candidate summary. To
estimate the bigram frequency in the summary,
we propose to use a supervised regression model
that is discriminatively trained using a variety of
features. Our experiments on several TAC sum-
marization data sets demonstrate this proposed
method outperforms the previous ILP system and
often the best performing TAC system.
2 Proposed Method
2.1 Bigram Gain Maximization by ILP
We choose bigrams as the language concepts in
our proposed method since they have been suc-
cessfully used in previous work. In addition, we
expect that the bigram oriented ILP is consistent
with the ROUGE-2 measure widely used for sum-
marization evaluation.
We start the description of our approach for the
scenario where a human abstractive summary is
provided, and the task is to select sentences to
form an extractive summary. Then Our goal is
to make the bigram frequency in this system sum-
mary as close as possible to that in the reference.
For each bigram b, we define its gain:
Gain(b, sum) = min{nb,ref , nb,sum} (7)
where nb,ref is the frequency of b in the reference
summary, and nb,sum is the frequency of b in the
automatic summary. The gain of a bigram is no
more than its frequency in the reference summary,
hence adding redundant bigrams will not increase
the gain.
The total gain of an extractive summary is de-
fined as the sum of every bigram gain in the sum-
mary:
Gain(sum) =
?
b
Gain(b, sum)
=
?
b
min{nb,ref ,
?
s
z(s) ? nb,s} (8)
where s is a sentence in the document, nb,s is
the frequency of b in sentence s, z(s) is a binary
variable, indicating whether s is selected in the
summary. The goal is to find z that maximizes
Gain(sum) (formula (8)) under the length con-
straint L.
This problem can be casted as an ILP problem.
First, using the fact that
min{a, x} = 0.5(?|x ? a| + x + a), x, a ? 0
we have
?
b
min{nb,ref ,
?
s
z(s) ? nb,s} =
?
b
0.5 ? (?|nb,ref ?
?
s
z(s) ? nb,s|+
nb,ref +
?
s
z(s) ? nb,s)
Now the problem is equivalent to:
max
z
?
b
(?|nb,ref ?
?
s
z(s) ? nb,s| +
nb,ref +
?
s
z(s) ? nb,s)
s.t.
?
s
z(s) ? |S| ? L; z(s) ? {0, 1}
This is equivalent to the ILP:
max
?
b
(
?
s
z(s) ? nb,s ?Cb) (9)
s.t.
?
s
z(s) ? |S| ? L (10)
z(s) ? {0, 1} (11)
?Cb ? nb,ref ?
?
s
z(s) ? nb,s ? Cb
(12)
where Cb is an auxiliary variable we introduce that
is equal to |nb,ref ?
?
s z(s) ? nb,s|, and nb,ref is
a constant that can be dropped from the objective
function.
2.2 Regression Model for Bigram Frequency
Estimation
In the previous section, we assume that nb,ref is
at hand (reference abstractive summary is given)
and propose a bigram-based optimization frame-
work for extractive summarization. However, for
the summarization task, the bigram frequency is
unknown, and thus our first goal is to estimate such
frequency. We propose to use a regression model
for this.
Since a bigram?s frequency depends on the sum-
mary length (L), we use a normalized frequency
1005
in our method. Let nb,ref = Nb,ref ? L, where
Nb,ref = n(b,ref)?
b n(b,ref)
is the normalized frequency
in the summary. Now the problem is to automati-
cally estimate Nb,ref .
Since the normalized frequency Nb,ref is a real
number, we choose to use a logistic regression
model to predict it:
Nb,ref =
exp{w?f(b)}?
j exp{w?f(bj)}
(13)
where f(bj) is the feature vector of bigram bj and
w? is the corresponding feature weight. Since even
for identical bigrams bi = bj , their feature vectors
may be different (f(bi) 6= f(bj)) due to their dif-
ferent contexts, we sum up frequencies for identi-
cal bigrams {bi|bi = b}:
Nb,ref =
?
i,bi=b
Nbi,ref
=
?
i,bi=b exp{w?f(bi)}?
j exp{w?f(bj)}
(14)
To train this regression model using the given
reference abstractive summaries, rather than trying
to minimize the squared error as typically done,
we propose a new objective function. Since the
normalized frequency satisfies the probability con-
straint
?
b Nb,ref = 1, we propose to use KL di-
vergence to measure the distance between the es-
timated frequencies and the ground truth values.
The objective function for training is thus to mini-
mize the KL distance:
min
?
b
N?b,ref log
N?b,ref
Nb,ref
(15)
where N?b,ref is the true normalized frequency of
bigram b in reference summaries.
Finally, we replace Nb,ref in Formula (15) with
Eq (14) and get the objective function below:
max
?
b
N?b,ref log
?
i,bi=b exp{w?f(bi)}?
j exp{w?f(bj)}
(16)
This shares the same form as the contrastive es-
timation proposed by (Smith and Eisner, 2005).
We use gradient decent method for parameter esti-
mation, initial w is set with zero.
2.3 Features
Each bigram is represented using a set of features
in the above regression model. We use two types
of features: word level and sentence level features.
Some of these features have been used in previous
work (Aker and Gaizauskas, 2009; Brandow et al,
1995; Edmundson, 1969; Radev, 2001):
? Word Level:
? 1. Term frequency1: The frequency of
this bigram in the given topic.
? 2. Term frequency2: The frequency of
this bigram in the selected sentences1 .
? 3. Stop word ratio: Ratio of stop words
in this bigram. The value can be {0, 0.5,
1}.
? 4. Similarity with topic title: The
number of common tokens in these two
strings, divided by the length of the
longer string.
? 5. Similarity with description of the
topic: Similarity of the bigram with
topic description (see next data section
about the given topics in the summariza-
tion task).
? Sentence Level: (information of sentence
containing the bigram)
? 6. Sentence ratio: Number of sentences
that include this bigram, divided by the
total number of the selected sentences.
? 7. Sentence similarity: Sentence sim-
ilarity with topic?s query, which is the
concatenation of topic title and descrip-
tion.
? 8. Sentence position: Sentence posi-
tion in the document.
? 9. Sentence length: The number of
words in the sentence.
? 10. Paragraph starter: Binary feature
indicating whether this sentence is the
beginning of a paragraph.
3 Experiments
3.1 Data
We evaluate our method using several recent TAC
data sets, from 2008 to 2011. The TAC summa-
rization task is to generate at most 100 words sum-
maries from 10 documents for a given topic query
(with a title and more detailed description). For
model training, we also included two years? DUC
data (2006 and 2007). When evaluating on one
TAC data set, we use the other years of the TAC
data plus the two DUC data sets as the training
data.
1See next section about the sentence selection step
1006
3.2 Summarization System
We use the same system pipeline described in
(Gillick et al, 2008; McDonald, 2007). The key
modules in the ICSI ILP system (Gillick et al,
2008) are briefly described below.
? Step 1: Clean documents, split text into sen-
tences.
? Step 2: Extract bigrams from all the sen-
tences, then select those bigrams with doc-
ument frequency equal to more than 3. We
call this subset as initial bigram set in the fol-
lowing.
? Step 3: Select relevant sentences that contain
at least one bigram from the initial bigram
set.
? Step 4: Feed the ILP with sentences and the
bigram set to get the result.
? Step 5: Order sentences identified by ILP as
the final result of summary.
The difference between the ICSI and our system
is in the 4th step. In our method, we first extract all
the bigrams from the selected sentences and then
estimate each bigram?s Nb,ref using the regression
model. Then we use the top-n bigrams with their
Nb,ref and all the selected sentences in our pro-
posed ILP module for summary sentence selec-
tion. When training our bigram regression model,
we use each of the 4 reference summaries sepa-
rately, i.e., the bigram frequency is obtained from
one reference summary. The same pre-selection of
sentences described above is also applied in train-
ing, that is, the bigram instances used in training
are from these selected sentences and the reference
summary.
4 Experiment and Analysis
4.1 Experimental Results
Table 1 shows the ROUGE-2 results of our pro-
posed system, the ICSI system, and also the best
performing system in the NIST TAC evaluation.
We can see that our proposed system consistently
outperforms ICSI ILP system (the gain is statis-
tically significant based on ROUGE?s 95% confi-
dence internal results). Compared to the best re-
ported TAC result, our method has better perfor-
mance on three data sets, except 2011 data. Note
that the best performing system for the 2009 data
is the ICSI ILP system, with an additional com-
pression step. Our ILP method is purely extrac-
tive. Even without using compression, our ap-
proach performs better than the full ICSI system.
The best performing system for the 2011 data also
has some compression module. We expect that af-
ter applying sentence compression and merging,
we will have even better performance, however,
our focus in this paper is on the bigram-based ex-
tractive summarization.
ICSI Proposed TAC Rank1
ILP System System
2008 0.1023 0.1076 0.1038
2009 0.1160 0.1246 0.1216
2010 0.1003 0.1067 0.0957
2011 0.1271 0.1327 0.1344
Table 1: ROUGE-2 summarization results.
There are several differences between the ICSI
system and our proposed method. First is the
bigrams (concepts) used. We use the top 100
bigrams from our bigram estimation module;
whereas the ICSI system just used the initial bi-
gram set described in Section 3.2. Second, the
weights for those bigrams differ. We used the es-
timated value from the regression model; the ICSI
system just uses the bigram?s document frequency
in the original text as weight. Finally, two systems
use different ILP setups. To analyze which fac-
tors (or all of them) explain the performance dif-
ference, we conducted various controlled experi-
ments for these three factors (bigrams, weights,
ILP). All of the following experiments use the
TAC 2009 data as the test set.
4.2 Effect of Bigram Weights
In this experiment, we vary the weighting methods
for the two systems: our proposed method and the
ICSI system. We use three weighting setups: the
estimated bigram frequency value in our method,
document frequency, or term frequency from the
original text. Table 2 and 3 show the results using
the top 100 bigrams from our system and the ini-
tial bigram set from the ICSI system respectively.
We also evaluate using the two different ILP con-
figurations in these experiments.
First of all, we can see that for both ILP sys-
tems, our estimated bigram weights outperform
the other frequency-based weights. For the ICSI
ILP system, using bigram document frequency
achieves better performance than term frequency
(which verified why document frequency is used
in their system). In contrast, for our ILP method,
1007
# Weight ILP ROUGE-2
1 Estimated value Proposed 0.12462 ICSI 0.1178
3 Document freq Proposed 0.11094 ICSI 0.1132
5 Term freq Proposed 0.11166 ICSI 0.1080
Table 2: Results using different weighting meth-
ods on the top 100 bigrams generated from our
proposed system.
# Weight ILP ROUGE-2
1 Estimated value Proposed 0.11572 ICSI 0.1161
3 Document freq Proposed 0.11014 ICSI 0.1160
5 Term freq Proposed 0.11096 ICSI 0.1072
Table 3: Results using different weighting meth-
ods based on the initial bigram sets. The average
number of bigrams is around 80 for each topic.
the bigram?s term frequency is slightly more use-
ful than its document frequency. This indicates
that our estimated value is more related to bi-
gram?s term frequency in the original text. When
the weight is document frequency, the ICSI?s re-
sult is better than our proposed ILP; whereas when
using term frequency as the weights, our ILP has
better results, again suggesting term frequency fits
our ILP system better. When the weight is esti-
mated value, the results depend on the bigram set
used. The ICSI?s ILP performs slightly better than
ours when it is equipped with the initial bigram,
but our proposed ILP has much better results us-
ing our selected top100 bigrams. This shows that
the size and quality of the bigrams has an impact
on the ILP modules.
4.3 The Effect of Bigram Set?s size
In our proposed system, we use 100 top bigrams.
There are about 80 bigrams used in the ICSI ILP
system. A natural question to ask is the impact
of the number of bigrams and their quality on the
summarization system. Table 4 shows some statis-
tics of the bigrams. We can see that about one
third of bigrams in the reference summary are in
the original text (127.3 out of 321.93), verifying
that people do use different words/bigram when
writing abstractive summaries. We mentioned that
we only use the top-N (n is 100 in previous ex-
periments) bigrams in our summarization system.
On one hand, this is to save computational cost for
the ILP module. On the other hand, we see from
the table that only 127 of these more than 2K bi-
grams are in the reference summary and are thus
expected to help the summary responsiveness. In-
cluding all the bigrams would lead to huge noise.
# bigrams in ref summary 321.93
# bigrams in text and ref summary 127.3
# bigrams used in our regression model 2140.7
(i.e., in selected sentences)
Table 4: Bigram statistics. The numbers are the
average ones for each topic.
Fig 1 shows the bigram coverage (number of bi-
grams used in the system that are also in reference
summaries) when we vary N selected bigrams. As
expected, we can see that as n increases, there
are more reference summary bigrams included in
the system. There are 25 summary bigrams in the
top-50 bigrams and about 38 in top-100 bigrams.
Compared with the ICSI system that has around 80
bigrams in the initial bigram set and 29 in the ref-
erence summary, our estimation module has better
coverage.
0
10
20
30
40
50
60
70
80
90
100
110
120
130
50 500 950 1400 1850 2300 2750 3200
Number of Selected Bigram
N
um
be
r
of
B
ig
ra
m
bo
th
in
Se
le
ct
ed
an
d
R
ef
er
en
ce
Figure 1: Coverage of bigrams (number of bi-
grams in reference summary) when varying the
number of bigrams used in the ILP systems.
Increasing the number of bigrams used in the
system will lead to better coverage, however, the
incorrect bigrams also increase and have a nega-
tive impact on the system performance. To exam-
ine the best tradeoff, we conduct the experiments
by choosing the different top-N bigram set for the
two ILP systems, as shown in Fig 2. For both the
ILP systems, we used the estimated weight value
for the bigrams.
1008
We can see that the ICSI ILP system performs
better when the input bigrams have less noise
(those bigrams that are not in summary). However,
our proposed method is slightly more robust to this
kind of noise, possibly because of the weights we
use in our system ? the noisy bigrams have lower
weights and thus less impact on the final system
performance. Overall the two systems have sim-
ilar trends: performance increases at the begin-
ning when using more bigrams, and after certain
points starts degrading with too many bigrams.
The optimal number of bigrams differs for the two
systems, with a larger number of bigrams in our
method. We also notice that the ICSI ILP system
achieved a ROUGE-2 of 0.1218 when using top
60 bigrams, which is better than using the initial
bigram set in their method (0.1160).
0.109
0.111
0.113
0.115
0.117
0.119
0.121
0.123
0.125
40 50 60 70 80 90 100 110 120 130
Number of selected bigram
R
ou
ge
-2
Proposed ILP
ICSI
Figure 2: Summarization performance when vary-
ing the number of bigrams for two systems.
4.4 Oracle Experiments
Based on the above analysis, we can see the impact
of the bigram set and their weights. The following
experiments are designed to demonstrate the best
system performance we can achieve if we have ac-
cess to good quality bigrams and weights. Here we
use the information from the reference summary.
The first is an oracle experiment, where we use
all the bigrams from the reference summaries that
are also in the original text. In the ICSI ILP
system, the weights are the document frequency
from the multiple reference summaries. In our ILP
module, we use the term frequency of the bigram.
The oracle results are shown in Table 5. We can
see these are significantly better than the automatic
systems.
From Table 5, we notice that ICSI?s ILP per-
forms marginally better than our proposed ILP. We
hypothesize that one reason may be that many bi-
grams in the summary reference only appear once.
Table 6 shows the frequency of the bigrams in the
summary. Indeed 85% of bigram only appear once
ILP System ROUGE-2
Our ILP 0.2124
ICSI ILP 0.2128
Table 5: Oracle experiment: using bigrams and
their frequencies in the reference summary as
weights.
and no bigrams appear more than 9 times. For the
majority of the bigrams, our method and the ICSI
ILP are the same. For the others, our system has
slight disadvantage when using the reference term
frequency. We expect the high term frequency
may need to be properly smoothed/normalized.
Freq 1 2 3 4 5 6 7 8 9
Ave# 277 32 7.5 3.2 1.1 0.3 0.1 0.1 0.04
Table 6: Average number of bigrams for each term
frequency in one topic?s reference summary.
We also treat the oracle results as the gold stan-
dard for extractive summarization and compared
how the two automatic summarization systems
differ at the sentence level. This is different from
the results in Table 1, which are the ROUGE re-
sults comparing to human written abstractive sum-
maries at the n-gram level. We found that among
the 188 sentences in this gold standard, our system
hits 31 and ICSI only has 23. This again shows
that our system has better performance, not just
at the word level based on ROUGE measures, but
also at the sentence level. There are on average
3 different sentences per topic between these two
results.
In the second experiment, after we obtain the
estimated Nb,ref for every bigram in the selected
sentences from our regression model, we only
keep those bigrams that are in the reference sum-
mary, and use the estimated weights for both ILP
modules. Table 7 shows the results. We can
consider these as the upper bound the system
can achieve if we use the automatically estimated
weights for the correct bigrams. In this experi-
ment ICSI ILP?s performance still performs better
than ours. This might be attributed to the fact there
is less noise (all the bigrams are the correct ones)
and thus the ICSI ILP system performs well. We
can see that these results are worse than the pre-
vious oracle experiments, but are better than using
the automatically generated bigrams, again show-
ing the bigram and weight estimation is critical for
1009
summarization.
# Weight ILP ROUGE-2
1 Estimated value Proposed 0.18882 ICSI 0.1942
Table 7: Summarization results when using the es-
timated weights and only keeping the bigrams that
are in the reference summary.
4.5 Effect of Training Set
Since our method uses supervised learning, we
conduct the experiment to show the impact of
training size. In TAC?s data, each topic has two
sets of documents. For set A, the task is a standard
summarization, and there are 4 reference sum-
maries, each 100 words long; for set B, it is an up-
date summarization task ? the summary includes
information not mentioned in the summary from
set A. There are also 4 reference summaries, with
400 words in total. Table 8 shows the results on
2009 data when using the data from different years
and different sets for training. We notice that when
the training data only contains set A, the perfor-
mance is always better than using set B or the com-
bined set A and B. This is not surprising because
of the different task definition. Therefore, for the
rest of the study on data size impact, we only use
data set A from the TAC data and the DUC data as
the training set. In total there are about 233 topics
from the two years? DUC data (06, 07) and three
years? TAC data (08, 10, 11). We incrementally
add 20 topics every time (from DUC06 to TAC11)
and plot the learning curve, as shown in Fig 3. As
expected, more training data results in better per-
formance.
Training Set # Topics ROUGE-2
08 Corpus (A) 48 0.1192
08 Corpus( B) 48 0.1178
08 Corpus (A+B) 96 0.1188
10 Corpus (A) 46 0.1174
10 Corpus (B) 46 0.1167
10 Corpus (A+B) 92 0.1170
11 Corpus (A) 44 0.1157
11 Corpus (B) 44 0.1130
11 Corpus (A+B) 88 0.1140
Table 8: Summarization performance when using
different training corpora.
0.112
0.113
0.114
0.115
0.116
0.117
0.118
0.119
0.12
0.121
0.122
0.123
0.124
0.125
20 40 60 80 100 120 140 160 180 200 220 240
Number of trainning topics
R
ou
ge
-2
Figure 3: Learning curve
4.6 Summary of Analysis
The previous experiments have shown the impact
of the three factors: the quality of the bigrams
themselves, the weights used for these bigrams,
and the ILP module. We found that the bigrams
and their weights are critical for both the ILP se-
tups. However, there is negligible difference be-
tween the two ILP methods.
An important part of our system is the super-
vised method for bigram and weight estimation.
We have already seen for the previous ILP method,
when using our bigrams together with the weights,
better performance can be achieved. Therefore we
ask the question whether this is simply because
we use supervised learning, or whether our pro-
posed regression model is the key. To answer this,
we trained a simple supervised binary classifier
for bigram prediction (positive means that a bi-
gram appears in the summary) using the same set
of features as used in our bigram weight estima-
tion module, and then used their document fre-
quency in the ICSI ILP system. The result for
this method is 0.1128 on the TAC 2009 data. This
is much lower than our result. We originally ex-
pected that using the supervised method may out-
perform the unsupervised bigram selection which
only uses term frequency information. Further ex-
periments are needed to investigate this. From this
we can see that it is not just the supervised meth-
ods or using annotated data that yields the over-
all improved system performance, but rather our
proposed regression setup for bigrams is the main
reason.
5 Related Work
We briefly describe some prior work on summa-
rization in this section. Unsupervised methods
have been widely used. In particular, recently sev-
eral optimization approaches have demonstrated
1010
competitive performance for extractive summa-
rization task. Maximum marginal relevance
(MMR) (Carbonell and Goldstein, 1998) uses a
greedy algorithm to find summary sentences. (Mc-
Donald, 2007) improved the MMR algorithm to
dynamic programming. They used a modified ob-
jective function in order to consider whether the
selected sentence is globally optimal. Sentence-
level ILP was also first introduced in (McDon-
ald, 2007), but (Gillick and Favre, 2009) revised
it to concept-based ILP. (Woodsend and Lapata,
2012) utilized ILP to jointly optimize different as-
pects including content selection, surface realiza-
tion, and rewrite rules in summarization. (Gala-
nis et al, 2012) uses ILP to jointly maximize the
importance of the sentences and their diversity
in the summary. (Berg-Kirkpatrick et al, 2011)
applied a similar idea to conduct the sentence
compression and extraction for multiple document
summarization. (Jin et al, 2010) made a com-
parative study on sentence/concept selection and
pairwise and list ranking algorithms, and con-
cluded ILP performed better than MMR and the
diversity penalty strategy in sentence/concept se-
lection. Other global optimization methods in-
clude submodularity (Lin and Bilmes, 2010) and
graph-based approaches (Erkan and Radev, 2004;
Leskovec et al, 2005; Mihalcea and Tarau, 2004).
Various unsupervised probabilistic topic models
have also been investigated for summarization and
shown promising. For example, (Celikyilmaz and
Hakkani-Tu?r, 2011) used it to model the hidden
abstract concepts across documents as well as the
correlation between these concepts to generate
topically coherent and non-redundant summaries.
(Darling and Song, 2011) applied it to separate
the semantically important words from the low-
content function words.
In contrast to these unsupervised approaches,
there are also various efforts on supervised learn-
ing for summarization where a model is trained to
predict whether a sentence is in the summary or
not. Different features and classifiers have been
explored for this task, such as Bayesian method
(Kupiec et al, 1995), maximum entropy (Osborne,
2002), CRF (Galley, 2006), and recently reinforce-
ment learning (Ryang and Abekawa, 2012). (Aker
et al, 2010) used discriminative reranking on mul-
tiple candidates generated by A* search. Recently,
research has also been performed to address some
issues in the supervised setup, such as the class
data imbalance problem (Xie and Liu, 2010).
In this paper, we propose to incorporate the
supervised method into the concept-based ILP
framework. Unlike previous work using sentence-
based supervised learning, we use a regression
model to estimate the bigrams and their weights,
and use these to guide sentence selection. Com-
pared to the direct sentence-based classification or
regression methods mentioned above, our method
has an advantage. When abstractive summaries
are given, one needs to use that information to au-
tomatically generate reference labels (a sentence
is in the summary or not) for extractive summa-
rization. Most researchers have used the similarity
between a sentence in the document and the ab-
stractive summary for labeling. This is not a per-
fect process. In our method, we do not need to
generate this extra label for model training since
ours is based on bigrams ? it is straightforward to
obtain the reference frequency for bigrams by sim-
ply looking at the reference summary. We expect
our approach also paves an easy way for future au-
tomatic abstractive summarization. One previous
study that is most related to ours is (Conroy et al,
2011), which utilized a Naive Bayes classifier to
predict the probability of a bigram, and applied
ILP for the final sentence selection. They used
more features than ours, whereas we use a discrim-
inatively trained regression model and a modified
ILP framework. Our proposed method performs
better than their reported results in TAC 2011 data.
Another study closely related to ours is (Davis et
al., 2012), which leveraged Latent Semantic Anal-
ysis (LSA) to produce term weights and selected
summary sentences by computing an approximate
solution to the Budgeted Maximal Coverage prob-
lem.
6 Conclusion and Future Work
In this paper, we leverage the ILP method as a core
component in our summarization system. Dif-
ferent from the previous ILP summarization ap-
proach, we propose a supervised learning method
(a discriminatively trained regression model) to
determine the importance of the bigrams fed to
the ILP module. In addition, we revise the ILP to
maximize the bigram gain (which is expected to
be highly correlated with ROUGE-2 scores) rather
than the concept/bigram coverage. Our proposed
method yielded better results than the previous
state-of-the-art ILP system on different TAC data
1011
sets. From a series of experiments, we found that
there is little difference between the two ILP mod-
ules, and that the improved system performance is
attributed to the fact that our proposed supervised
bigram estimation module can successfully gather
the important bigram and assign them appropriate
weights. There are several directions that warrant
further research. We plan to consider the context
of bigrams to better predict whether a bigram is in
the reference summary. We will also investigate
the relationship between concepts and sentences,
which may help move towards abstractive summa-
rization.
Acknowledgments
This work is partly supported by DARPA under
Contract No. HR0011-12-C-0016 and FA8750-
13-2-0041, and NSF IIS-0845484. Any opinions
expressed in this material are those of the authors
and do not necessarily reflect the views of DARPA
or NSF.
References
Ahmet Aker and Robert Gaizauskas. 2009. Summary
generation for toponym-referenced images using ob-
ject type language models. In Proceedings of the
International Conference RANLP.
Ahmet Aker, Trevor Cohn, and Robert Gaizauskas.
2010. Multi-document summarization using a*
search and discriminative training. In Proceedings
of the EMNLP.
Taylor Berg-Kirkpatrick, Dan Gillick, and Dan Klein.
2011. Jointly learning to extract and compress. In
Proceedings of the ACL.
Ronald Brandow, Karl Mitze, and Lisa F. Rau. 1995.
Automatic condensation of electronic publications
by sentence selection. Inf. Process. Manage.
Jaime Carbonell and Jade Goldstein. 1998. The use of
mmr, diversity-based reranking for reordering docu-
ments and producing summaries. In Proceedings of
the SIGIR.
Asli Celikyilmaz and Dilek Hakkani-Tu?r. 2011. Dis-
covery of topically coherent sentences for extractive
summarization. In Proceedings of the ACL.
John M. Conroy, Judith D. Schlesinger, Jeff Kubina,
Peter A. Rankel, and Dianne P. O?Leary. 2011.
Classy 2011 at tac: Guided and multi-lingual sum-
maries and evaluation metrics. In Proceedings of the
TAC.
William M. Darling and Fei Song. 2011. Probabilistic
document modeling for syntax removal in text sum-
marization. In Proceedings of the ACL.
Sashka T. Davis, John M. Conroy, and Judith D.
Schlesinger. 2012. Occams - an optimal combinato-
rial covering algorithm for multi-document summa-
rization. In Proceedings of the ICDM.
H. P. Edmundson. 1969. New methods in automatic
extracting. J. ACM.
Gu?nes Erkan and Dragomir R. Radev. 2004. Lexrank:
graph-based lexical centrality as salience in text
summarization. J. Artif. Int. Res.
Dimitrios Galanis, Gerasimos Lampouras, and Ion An-
droutsopoulos. 2012. Extractive multi-document
summarization with integer linear programming and
support vector regression. In Proceedings of the
COLING.
Michel Galley. 2006. A skip-chain conditional random
field for ranking meeting utterances by importance.
In Proceedings of the EMNLP.
Dan Gillick and Benoit Favre. 2009. A scalable global
model for summarization. In Proceedings of the
Workshop on Integer Linear Programming for Natu-
ral Langauge Processing on NAACL.
Dan Gillick, Benoit Favre, and Dilek Hakkani-Tu?r.
2008. In The ICSI Summarization System at TAC
2008.
Feng Jin, Minlie Huang, and Xiaoyan Zhu. 2010. A
comparative study on ranking and selection strate-
gies for multi-document summarization. In Pro-
ceedings of the COLING.
Julian Kupiec, Jan Pedersen, and Francine Chen. 1995.
A trainable document summarizer. In Proceedings
of the SIGIR.
Jure Leskovec, Natasa Milic-Frayling, and Marko Gro-
belnik. 2005. Impact of linguistic analysis on the
semantic graph coverage and learning of document
extracts. In Proceedings of the AAAI.
Hui Lin and Jeff Bilmes. 2010. Multi-document sum-
marization via budgeted maximization of submodu-
lar functions. In Proceedings of the NAACL.
Chin-Yew Lin. 2004. Rouge: a package for auto-
matic evaluation of summaries. In Proceedings of
the ACL.
Ryan McDonald. 2007. A study of global inference al-
gorithms in multi-document summarization. In Pro-
ceedings of the European conference on IR research.
Rada Mihalcea and Paul Tarau. 2004. Textrank:
Bringing order into text. In Proceedings of the
EMNLP.
Miles Osborne. 2002. Using maximum entropy for
sentence extraction. In Proceedings of the ACL-02
Workshop on Automatic Summarization.
1012
Dragomir R. Radev. 2001. Experiments in single and
multidocument summarization using mead. In In
First Document Understanding Conference.
Seonggi Ryang and Takeshi Abekawa. 2012. Frame-
work of automatic text summarization using rein-
forcement learning. In Proceedings of the EMNLP.
Noah A. Smith and Jason Eisner. 2005. Contrastive
estimation: training log-linear models on unlabeled
data. In Proceedings of the ACL.
Kristian Woodsend and Mirella Lapata. 2012. Mul-
tiple aspect summarization using integer linear pro-
gramming. In Proceedings of the EMNLP.
Shasha Xie and Yang Liu. 2010. Improving supervised
learning for meeting summarization using sampling
and regression. Comput. Speech Lang.
1013
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 327?332,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Polynomial Time Joint Structural Inference for Sentence Compression
Xian Qian and Yang Liu
The University of Texas at Dallas
800 W. Campbell Rd., Richardson, TX, USA
{qx,yangl}@hlt.utdallas.edu
Abstract
We propose two polynomial time infer-
ence algorithms to compress sentences un-
der bigram and dependency-factored ob-
jectives. The first algorithm is exact and
requires O(n
6
) running time. It extend-
s Eisner?s cubic time parsing algorithm
by using virtual dependency arcs to link
deleted words. Two signatures are added
to each span, indicating the number of
deleted words and the rightmost kept word
within the span. The second algorithm is
a fast approximation of the first one. It re-
laxes the compression ratio constraint us-
ing Lagrangian relaxation, and thereby re-
quires O(n
4
) running time. Experimental
results on the popular sentence compres-
sion corpus demonstrate the effectiveness
and efficiency of our proposed approach.
1 Introduction
Sentence compression aims to shorten a sentence
by removing uninformative words to reduce read-
ing time. It has been widely used in compres-
sive summarization (Liu and Liu, 2009; Li et al,
2013; Martins and Smith, 2009; Chali and Hasan,
2012; Qian and Liu, 2013). To make the com-
pressed sentence readable, some techniques con-
sider the n-gram language models of the com-
pressed sentence (Clarke and Lapata, 2008; Mc-
Donald, 2006). Recent studies used a subtree dele-
tion model for compression (Berg-Kirkpatrick et
al., 2011; Morita et al, 2013; Qian and Liu, 2013),
which deletes a word only if its modifier in the
parse tree is deleted. Despite its empirical suc-
cess, such a model fails to generate compressions
that are not subject to the subtree constraint (see
Figure 1). In fact, we parsed the Edinburgh sen-
tence compression corpus using the MSTparser
1
,
1
http://sourceforge.net/projects/mstparser/
Warrensays the economy continues the steady improvementROOT
Warren says steadythe economy continues the improvementROOT
Figure 1: The compressed sentence is not a sub-
tree of the original sentence. Words in gray are
removed.
and found that 2561 of 5379 sentences (47.6%) do
not satisfy the subtree deletion model.
Methods beyond the subtree model are also ex-
plored. Trevor et al proposed synchronous tree
substitution grammar (Cohn and Lapata, 2009),
which allows local distortion of the tree topolo-
gy and can thus naturally capture structural mis-
matches. (Genest and Lapalme, 2012; Thadani
and McKeown, 2013) proposed the joint compres-
sion model, which simultaneously considers the n-
grammodel and dependency parse tree of the com-
pressed sentence. However, the time complexity
greatly increases since the parse tree dynamical-
ly depends on the compression. They used Integer
Linear Programming (ILP) for inference which re-
quires exponential running time in the worst case.
In this paper, we propose a new exact decod-
ing algorithm for the joint model using dynam-
ic programming. Our method extends Eisner?s
cubic time parsing algorithm by adding signa-
tures to each span, which indicate the number of
deleted words and the rightmost kept word with-
in the span, resulting in O(n
6
) time complexity
andO(n
4
) space complexity. We further propose a
faster approximate algorithm based on Lagrangian
relaxation, which has TO(n
4
) running time and
O(n
3
) space complexity (T is the iteration num-
ber in the subgradient decent algorithm). Experi-
ments on the popular Edinburgh dataset show that
327
x x ... x x ... x ...0 (root) 2 i i+1 jx x1 n
w0idep
w i2dep
w ijdep
w i i+1dep
w2ibgr w i i+1bgr w i+1 jbgr
Figure 2: Graph illustration for the objective func-
tion. In this example, words x
2
, x
i
, x
i+1
, x
j
are
kept, others are deleted. The value of the ob-
jective function is w
tok
2
+ w
tok
i
+ w
tok
i+1
+ w
tok
j
+
w
dep
0i
+w
dep
i2
+w
dep
ii+1
+w
dep
ij
+w
bgr
2i
+w
bgr
ii+1
+w
bgr
i+1j
.
the proposed approach is 10 times faster than a
high-performance commercial ILP solver.
2 Task Definition
We define the sentence compression task as: given
a sentence composed of n words, x = x
1
, . . . , x
n
,
and a length L ? n, we need to remove (n ? L)
words from x, so that the sum of the weights of
the dependency tree and word bigrams of the re-
maining part is maximized. Formally, we solve
the following optimization problem:
max
z,y
?
i
w
tok
i
z
i
+
?
i,j
w
dep
ij
z
i
z
j
y
ij
(1)
+
?
i<j
w
bgr
ij
z
i
z
j
?
i<k<j
(1? z
k
)
s.t. z is binary ,
?
i
z
i
= L
y is a projective parse tree over the
subgraph: {x
i
|z
i
= 1}
where z is a binary vector, z
i
indicates x
i
is kep-
t or not. y is a square matrix denoting the pro-
jective dependency parse tree over the remaining
words, y
ij
indicates if x
i
is the head of x
j
(note
that each word has exactly one head). w
tok
i
is the
informativeness of x
i
, w
bgr
ij
is the score of bigram
x
i
x
j
in an n-gram model, w
dep
is the score of de-
pendency arc x
i
? x
j
in an arc-factored depen-
dency parsing model. Hence, the first part of the
objective function is the total score of the kep-
t words, the second and third parts are the scores
of the parse tree and bigrams of the compressed
sentence, z
i
z
j
?
i<k<j
(1? z
k
) = 1 indicates both
x
i
and x
j
are kept, and are adjacent after compres-
sion. A graph illustration of the objective function
is shown in Figure 2.
Warren says steadythe economy continues the improvementROOT
Figure 3: Connect deleted words using virtual arc-
s.
3 Proposed Method
3.1 Eisner?s Cubic Time Parsing Algorithm
Throughout the paper, we assume that all the parse
trees are projective. Our method is a generaliza-
tion of Eisner?s dynamic programming algorithm
(Eisner, 1996), where two types of structures are
used in each iteration, incomplete spans and com-
plete spans. A span is a subtree over a number of
consecutive words, with the leftmost or the right-
most word as its root. An incomplete span denoted
as I
i
j
is a subtree inside a single arc x
i
? x
j
, with
root x
i
. A complete span is denoted as C
i
j
, where
x
i
is the root of the subtree, and x
j
is the furthest
descendant of x
i
.
Eisner?s algorithm searches the optimal tree in
a bottom up order. In each step, it merges two
adjacent spans into a larger one. There are two
rules for merging spans: one merges two complete
spans into an incomplete span, the other merges an
incomplete span and a complete span into a large
complete span.
3.2 Exact O(n
6
) Time Algorithm
First we consider an easy case, where the bigram
scores w
bgr
ij
in the objective function are ignored.
The scores of unigrams w
tok
i
can be transfered
to the dependency arcs, so that we can remove al-
l linear terms w
tok
i
z
i
from the objective function.
That is:
?
i
w
tok
i
z
i
+
?
i,j
w
dep
ij
z
i
z
j
y
ij
=
?
i,j
(w
dep
ij
+ w
tok
j
)z
i
z
j
y
ij
This can be easily verifed. If z
j
= 0, then in both
equations, all terms having z
j
are zero; If z
j
= 1,
i.e., x
j
is kept, since it has exactly one head word
x
k
in the compressed sentence, the sum of the
terms having z
j
is w
tok
j
+ w
dep
kj
for both equations.
Therefore, we only need to consider the scores
of arcs. For any compressed sentence, we could
augment its dependency tree by adding a virtual
328
i i+1i i+1
+ =
i jr+1 j
+ =
i r
i ji+1 j
+ =
i i+1
... ...
i jr j
+ =
i r
Case 1
Case 2
Case 3
Case 4
Figure 4: Merging rules for dependency-factored
sentence compression. Incomplete spans and
complete spans are represented by trapezoids and
triangles respectively.
arc i? 1 ? i for each deleted word x
i
. If the first
word x
1
is deleted, we connect it to the root of the
parse tree x
0
, as shown in Figure 3. In this way,
we derive a full parse tree of the original sentence.
This is a one-to-one mapping. We can reversely
get the the compressed parse tree by removing all
virtual arcs from the full parse tree. We restrict
the score of all the virtual arcs to be zero, so that
scores of the two parse trees are equivalent.
Now the problem is to search the optimal full
parse tree with n? L virtual arcs.
We modify Eisner?s algorithm by adding a sig-
nature to each span indicating the number of vir-
tual arcs within the span. Let I
i
j
(k) and C
i
j
(k)
denote the incomplete and complete spans with k
virtual arcs respectively. When merging two span-
s, there are 4 cases, as shown in Figure 4.
? Case 1 Link two complete spans by a virtual
arc : I
i
i+1
(1) = C
i
i
(0) + C
i+1
i+1
(0).
The two complete spans must be single word-
s, as the length of the virtual arc is 1.
? Case 2 Link two complete spans by a non-
virtual arc: I
i
j
(k) = C
i
r
(k
?
)+C
j
r+1
(k
??
), k
?
+
k
??
= k.
? Case 3 Merge an incomplete span and a com-
plete span. The incomplete span is covered
by a virtual arc: I
i
j
(j ? i) = I
i
i+1
(1) +
C
i+1
j
(j ? i ? 1). The number of the virtu-
al arcs within C
i+1
j
must be j ? i ? 1, since
the descendants of the modifier of a virtual
arc x
j
must be removed.
? Case 4 Merge an incomplete span and a com-
plete span. The incomplete span is covered
by a non-virtual arc: C
i
j
(k) = I
i
r
(k
?
) +
C
r
j
(k
??
), k
?
+ k
??
= k.
The score of the new span is the sum of the two
spans. For case 2, the weight of the dependency
arc i ? j, w
dep
ij
is also added to the final score.
The root node is allowed to have two modifiers:
one is the modifier in the compressed sentence, the
other is the first word if it is removed.
For each combination, the algorithm enumer-
ates the number of virtual arcs in the left and right
spans, and the split position (e.g., k
?
, k
??
, r in case
2), thus it takes O(n
3
) running time. The overall
time complexity is O(n
5
) and the space complex-
ity is O(n
3
).
Next, we consider the bigram scores. The fol-
lowing proposition is obvious.
Proposition 1. For any right-headed span I
i
j
or
C
i
j
, i > j, words x
i
, x
j
must be kept.
Proof. Suppose x
j
is removed, there must be a vir-
tual arc j? 1 ? j which is a conflict with the fact
that x
j
is the leftmost word. As x
j
is a descendant
of x
i
, x
i
must be kept.
When merging two spans, a new bigram is cre-
ated, which connects the rightmost kept words in
the left span and the leftmost kept word in the right
span. According to the proposition above, if the
right span is right-headed, its leftmost word is kep-
t. If the right span is left-headed, there are two
cases: its leftmost word is kept, or no word in the
span is kept. In any case, we only need to consider
the leftmost word in the right span.
Let I
i
j
(k, p) and C
i
j
(k, p) denote the single and
complete span with k virtual arcs and the right-
most kept word x
p
. According to the proposition
above, we have, for any right-headed span p = i.
We slightly modify the two merging rules
above, and obtain:
? Case 2? Link two complete spans by a
non-virtual arc: I
i
j
(k, j) = C
i
r
(k
?
, p) +
C
j
r+1
(k
??
, j), k
?
+ k
??
= k. The score of the
new span is the sum of the two spans plus
w
dep
ij
+ w
bgr
p,r+1
.
329
? Case 4? Merge an incomplete span and a
complete span. The incomplete span is cov-
ered by a non-virtual arc. For left-headed
spans, the rule is C
i
j
(k, q) = I
i
r
(k
?
, p) +
C
r
j
(k
??
, q), k
?
+ k
??
= k, and the score of
the new span is the sum of the two span-
s plus w
bgr
pr
; for right-headed spans, the rule
is C
i
j
(k, i) = I
i
r
(k
?
, i) + C
r
j
(k
??
, r), and the
score of the new span is the sum of the two
spans.
The modified algorithm requires O(n
6
) running
time and O(n
4
) space complexity.
3.3 Approximate O(n
4
) Time Algorithm
In this section, we propose an approximate algo-
rithm where the length constraint
?
i
z
i
= L is re-
laxed by Lagrangian Relaxation. The relaxed ver-
sion of Problem (1) is
min
?
max
z,y
?
i
w
tok
i
z
i
+
?
i,j
w
dep
ij
z
i
z
j
y
ij
(2)
+
?
i<j
w
bgr
ij
z
i
z
j
?
i<k<j
(1? z
k
)
+?(
?
i
z
i
? L)
s.t. z is binary
y is a projective parse tree over the
subgraph: {x
i
|z
i
= 1}
Fixing ?, the optimal z,y can be found using a
simpler version of the algorithm above. We drop
the signature of the virtual arc number from each
span, and thus obtain an O(n
4
) time algorithm. S-
pace complexity is O(n
3
). Fixing z,y, the dual
variable is updated by
? = ? + ?(L?
?
i
z
i
)
where ? > 0 is the learning rate. In this paper, our
choice of ? is the same as (Rush et al, 2010).
4 Experiments
4.1 Data and Settings
We evaluate our method on the data set from
(Clarke and Lapata, 2008). It includes 82
newswire articles with manually produced com-
pression for each sentence. We use the same par-
titions as (Martins and Smith, 2009), i.e., 1,188
sentences for training and 441 for testing.
Our model is discriminative ? the scores of
the unigrams, bigrams and dependency arcs are
the linear functions of features, that is, w
tok
i
=
v
T
f(x
i
), where f is the feature vector of x
i
, and v
is the weight vector of features. The learning task
is to estimate the feature weight vector based on
the manually compressed sentences.
We run a second order dependency parser
trained on the English Penn Treebank corpus to
generate the parse trees of the compressed sen-
tences. Then we augment these parse trees by
adding virtual arcs and get the full parse trees
of their corresponding original sentences. In this
way, the annoation is transformed into a set of
sentences with their augmented parse trees. The
learning task is similar to training a parser. We run
a CRF based POS tagger to generate POS related
features.
We adopt the compression evaluation metric as
used in (Martins and Smith, 2009) that measures
the macro F-measure for the retained unigrams
(F
ugr
), and the one used in (Clarke and Lapata,
2008) that calculates the F1 score of the grammat-
ical relations labeled by RASP (Briscoe and Car-
roll, 2002).
We compare our method with other 4 state-of-
the-art systems. The first is linear chain CRFs,
where the compression task is casted as a bina-
ry sequence labeling problem. It usually achieves
high unigram F1 score but low grammatical rela-
tion F1 score since it only considers the local inter-
dependence between adjacent words. The second
is the subtree deletion model (Berg-Kirkpatrick et
al., 2011) which is solved by integer linear pro-
gramming (ILP)
2
. The third one is the bigram
model proposed by McDonald (McDonald, 2006)
which adopts dynamic programming for efficient
inference. The last one jointly infers tree struc-
tures alongside bigrams using ILP (Thadani and
McKeown, 2013). For fair comparison, system-
s were restricted to produce compressions that
matched their average gold compression rate if
possible.
4.2 Features
Three types of features are used to learn our mod-
el: unigram features, bigram features and depen-
dency features, as shown in Table 1. We also use
the in-between features proposed by (McDonald et
2
We use Gurobi as the ILP solver in the paper.
http://www.gurobi.com/
330
Features for unigram x
i
w
i?2
, w
i?1
, w
i
, w
i+1
, w
i+2
t
i?2
, t
i?1
, t
i
, t
i+1
, t
i+2
w
i
t
i
w
i?1
w
i
, w
i
w
i+1
t
i?2
t
i?1
, t
i?1
t
i
, t
i
t
i+1
, t
i+1
t
i+2
t
i?2
t
i?1
t
i
, t
i?1
t
i
t
i+1
, t
i
t
i+1
t
i+2
whether w
i
is a stopword
Features for selected bigram x
i
x
j
distance between the two words: j ? i
w
i
w
j
, w
i?1
w
j
, w
i+1
w
j
, w
i
w
j?1
, w
i
w
j+1
t
i
t
j
, t
i?1
t
j
, t
i+1
t
j
, t
i
t
j?1
, t
i
t
j+1
Concatenation of the templates above
{t
i
t
k
t
j
|i < k < j}
Dependency Features for arc x
h
? x
m
distance between the head and modifier h?m
dependency type
direction of the dependency arc (left/right)
w
h
w
m
, w
h?1
w
m
, w
h+1
w
m
, w
h
w
m?1
, w
h
w
m+1
t
h
t
m
, t
h?1
t
m
, t
h+1
t
m
, t
h
t
m?1
, t
h
t
m+1
t
h?1
t
h
t
m?1
t
m
, t
h
t
h+1
t
m?1
t
m
t
h?1
t
h
t
m
t
m+1
, t
h
t
h+1
t
m
t
m+1
Concatenation of the templates above
{t
h
t
k
t
m
|x
k
lies between x
h
and x
m
}
Table 1: Feature templates. w
i
denotes the word
form of token x
i
and t
i
denotes the POS tag of x
i
.
al., 2005), which were shown to be very effective
for dependency parsing.
4.3 Results
We show the comparison results in Table 2. As
expected, the joint models (ours and TM13) con-
sistently outperform the subtree deletion model, s-
ince the joint models do not suffer from the sub-
tree restriction. They also outperform McDon-
ald?s, demonstrating the effectiveness of consid-
ering the grammar structure for compression. It
is not surprising that CRFs achieve high unigram
F scores but low syntactic F scores as they do not
System C Rate F
uni
RASP Sec.
Ours(Approx) 0.68 0.802 0.598 0.056
Ours(Exact) 0.68 0.805 0.599 0.610
Subtree 0.68 0.761 0.575 0.022
TM13 0.68 0.804 0.599 0.592
McDonald06 0.71 0.776 0.561 0.010
CRFs 0.73 0.790 0.501 0.002
Table 2: Comparison results under various quality
metrics, including unigram F1 score (F
uni
), syn-
tactic F1 score (RASP), and compression speed
(seconds per sentence). C Rate is the compression
ratio of the system generated output. For fair com-
parison, systems were restricted to produce com-
pressions that matched their average gold com-
pression rate if possible.
consider the fluency of the compressed sentence.
Compared with TM13?s system, our model with
exact decoding is not significantly faster due to the
high order of the time complexity. On the oth-
er hand, our approximate approach is much more
efficient, about 10 times faster than TM13? sys-
tem, and achieves competitive accuracy with the
exact approach. Note that it is worth pointing
out that the exact approach can output compressed
sentences of all lengths, whereas the approximate
method can only output one sentence at a specific
compression rate.
5 Conclusion
In this paper, we proposed two polynomial time
decoding algorithms using joint inference for sen-
tence compression. The first one is an exac-
t dynamic programming algorithm, and requires
O(n
6
) running time. This one does not show
significant advantage in speed over ILP. The sec-
ond one is an approximation of the first algorith-
m. It adopts Lagrangian relaxation to eliminate the
compression ratio constraint, yielding lower time
complexity TO(n
4
). In practice it achieves nearly
the same accuracy as the exact one, but is much
faster.
3
The main assumption of our method is that the
dependency parse tree is projective, which is not
true for some other languages. In that case, our
method is invalid, but (Thadani and McKeown,
2013) still works. In the future, we will study the
non-projective cases based on the recent parsing
techniques for 1-endpoint-crossing trees (Pitler et
al., 2013).
Acknowledgments
We thank three anonymous reviewers for their
valuable comments. This work is partly support-
ed by NSF award IIS-0845484 and DARPA under
Contract No. FA8750-13-2-0041. Any opinion-
s expressed in this material are those of the au-
thors and do not necessarily reflect the views of
the funding agencies.
References
Taylor Berg-Kirkpatrick, Dan Gillick, and Dan Klein.
2011. Jointly learning to extract and compress. In
Proceedings of ACL-HLT, pages 481?490, June.
3
Our code is available at http://code.google.com/p/sent-
compress/
331
T. Briscoe and J. Carroll. 2002. Robust accurate statis-
tical annotation of general text.
Yllias Chali and Sadid A. Hasan. 2012. On the effec-
tiveness of using sentence compression models for
query-focused multi-document summarization. In
Proceedings of COLING, pages 457?474.
James Clarke and Mirella Lapata. 2008. Global in-
ference for sentence compression: An integer linear
programming approach. J. Artif. Intell. Res. (JAIR),
31:399?429.
Trevor Cohn and Mirella Lapata. 2009. Sentence
compression as tree transduction. J. Artif. Int. Res.,
34(1):637?674, April.
Jason M. Eisner. 1996. Three new probabilistic mod-
els for dependency parsing: an exploration. In Pro-
ceedings of COLING.
Pierre-Etienne Genest and Guy Lapalme. 2012. Fully
abstractive approach to guided summarization. In
Proceedings of the ACL, pages 354?358.
Chen Li, Fei Liu, Fuliang Weng, and Yang Liu. 2013.
Document summarization via guided sentence com-
pression. In Proceedings of EMNLP, October.
Fei Liu and Yang Liu. 2009. From extractive to ab-
stractive meeting summaries: Can it be done by
sentence compression? In Proceedings of ACL-
IJCNLP 2009, pages 261?264, August.
Andr?e F. T. Martins and Noah A. Smith. 2009. Sum-
marization with a joint model for sentence extraction
and compression. In Proceedings of the Workshop
on Integer Linear Programming for Natural Lan-
gauge Processing, pages 1?9.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005. Online large-margin training of de-
pendency parsers. In Proceedings of ACL.
Ryan McDonald. 2006. Discriminative Sentence
Compression with Soft Syntactic Constraints. In
Proceedings of EACL, April.
Hajime Morita, Ryohei Sasano, Hiroya Takamura, and
Manabu Okumura. 2013. Subtree extractive sum-
marization via submodular maximization. In Pro-
ceedings of ACL, pages 1023?1032, August.
Emily Pitler, Sampath Kannan, and Mitchell Marcus.
2013. Finding optimal 1-endpoint-crossing trees. In
Transactions of the Association for Computational
Linguistics, 2013 Volume 1.
Xian Qian and Yang Liu. 2013. Fast joint compression
and summarization via graph cuts. In Proceedings
of EMNLP, pages 1492?1502, October.
Alexander M Rush, David Sontag, Michael Collins,
and Tommi Jaakkola. 2010. On dual decomposi-
tion and linear programming relaxations for natural
language processing. In Proceedings of EMNLP.
Kapil Thadani and Kathleen McKeown. 2013. Sen-
tence compression with joint structural inference. In
Proceedings of the CoNLL, August.
332
Transactions of the Association for Computational Linguistics, 1 (2013) 37?48. Action Editor: Ryan McDonald.
Submitted 11/2012; Revised 2/2013; Published 3/2013. c?2013 Association for Computational Linguistics.
Branch and Bound Algorithm for Dependency Parsing
with Non-local Features
Xian Qian and Yang Liu
Computer Science Department
The University of Texas at Dallas
{qx,yangl}@hlt.utdallas.edu
Abstract
Graph based dependency parsing is inefficient
when handling non-local features due to high
computational complexity of inference. In
this paper, we proposed an exact and effi-
cient decoding algorithm based on the Branch
and Bound (B&B) framework where non-
local features are bounded by a linear combi-
nation of local features. Dynamic program-
ming is used to search the upper bound. Ex-
periments are conducted on English PTB and
Chinese CTB datasets. We achieved competi-
tive Unlabeled Attachment Score (UAS) when
no additional resources are available: 93.17%
for English and 87.25% for Chinese. Parsing
speed is 177 words per second for English and
97 words per second for Chinese. Our algo-
rithm is general and can be adapted to non-
projective dependency parsing or other graph-
ical models.
1 Introduction
For graph based projective dependency parsing, dy-
namic programming (DP) is popular for decoding
due to its efficiency when handling local features.
It performs cubic time parsing for arc-factored mod-
els (Eisner, 1996; McDonald et al, 2005a) and bi-
quadratic time for higher order models with richer
sibling and grandchild features (Carreras, 2007; Koo
and Collins, 2010). However, for models with gen-
eral non-local features, DP is inefficient.
There have been numerous studies on global in-
ference algorithms for general higher order parsing.
One popular approach is reranking (Collins, 2000;
Charniak and Johnson, 2005; Hall, 2007). It typi-
cally has two steps: the low level classifier gener-
ates the top k hypotheses using local features, then
the high level classifier reranks these candidates us-
ing global features. Since the reranking quality is
bounded by the oracle performance of candidates,
some work has combined candidate generation and
reranking steps using cube pruning (Huang, 2008;
Zhang and McDonald, 2012) to achieve higher or-
acle performance. They parse a sentence in bottom
up order and keep the top k derivations for each s-
pan using k best parsing (Huang and Chiang, 2005).
After merging the two spans, non-local features are
used to rerank top k combinations. This approach
is very efficient and flexible to handle various non-
local features. The disadvantage is that it tends to
compute non-local features as early as possible so
that the decoder can utilize that information at inter-
nal spans, hence it may miss long historical features
such as long dependency chains.
Smith and Eisner modeled dependency parsing
using Markov Random Fields (MRFs) with glob-
al constraints and applied loopy belief propaga-
tion (LBP) for approximate learning and inference
(Smith and Eisner, 2008). Similar work was done
for Combinatorial Categorial Grammar (CCG) pars-
ing (Auli and Lopez, 2011). They used posterior
marginal beliefs for inference to satisfy the tree con-
straint: for each factor, only legal messages (satisfy-
ing global constraints) are considered in the partition
function.
A similar line of research investigated the use
of integer linear programming (ILP) based parsing
(Riedel and Clarke, 2006; Martins et al, 2009). This
37
method is very expressive. It can handle arbitrary
non-local features determined or bounded by linear
inequalities of local features. For local models, LP is
less efficient than DP. The reason is that, DP works
on a small number of dimensions in each recursion,
while for LP, the popular revised simplex method
needs to solve a m dimensional linear system in
each iteration (Nocedal and Wright, 2006), where
m is the number of constraints, which is quadratic
in sentence length for projective dependency pars-
ing (Martins et al, 2009).
Dual Decomposition (DD) (Rush et al, 2010;
Koo et al, 2010) is a special case of Lagrangian re-
laxation. It relies on standard decoding algorithms
as oracle solvers for sub-problems, together with a
simple method for forcing agreement between the
different oracles. This method does not need to con-
sider the tree constraint explicitly, as it resorts to dy-
namic programming which guarantees its satisfac-
tion. It works well if the sub-problems can be well
defined, especially for joint learning tasks. Howev-
er, for the task of dependency parsing, using various
non-local features may result in many overlapped
sub-problems, hence it may take a long time to reach
a consensus (Martins et al, 2011).
In this paper, we propose a novel Branch and
Bound (B&B) algorithm for efficient parsing with
various non-local features. B&B (Land and Doig,
1960) is generally used for combinatorial optimiza-
tion problems such as ILP. The difference between
our method and ILP is that the sub-problem in ILP
is a relaxed LP, which requires a numerical solution,
while ours bounds the non-local features by a lin-
ear combination of local features and uses DP for
decoding as well as calculating the upper bound of
the objective function. An exact solution is achieved
if the bound is tight. Though in the worst case,
time complexity is exponential in sentence length,
it is practically efficient especially when adopting a
pruning strategy.
Experiments are conducted on English PennTree
Bank and Chinese Tree Bank 5 (CTB5) with stan-
dard train/develop/test split. We achieved 93.17%
Unlabeled Attachment Score (UAS) for English at a
speed of 177 words per second and 87.25% for Chi-
nese at a speed of 97 words per second.
2 Graph Based Parsing
2.1 Problem Definition
Given a sentence x = x1, x2, . . . , xn where xi is
the ith word of the sentence, dependency parsing as-
signs exactly one head word to each word, so that
dependencies from head words to modifiers form a
tree. The root of the tree is a special symbol de-
noted by x0 which has exactly one modifier. In this
paper, we focus on unlabeled projective dependency
parsing but our algorithm can be adapted for labeled
or non-projective dependency parsing (McDonald et
al., 2005b).
The inference problem is to search the optimal
parse tree y?
y? = argmaxy?Y(x)?(x, y)
where Y(x) is the set of all candidate parse trees of
sentence x. ?(x, y) is a given score function which
is usually decomposed into small parts
?(x, y) =
?
c?y
?c(x) (1)
where c is a subset of edges, and is called a factor.
For example, in the all grandchild model (Koo and
Collins, 2010), the score function can be represented
as
?(x, y) =
?
ehm?y
?ehm(x) +
?
egh,ehm?y
?egh,ehm(x)
where the first term is the sum of scores of all edges
xh ? xm, and the second term is the sum of the
scores of all edge chains xg ? xh ? xm.
In discriminative models, the score of a parse tree
y is the weighted sum of the fired feature functions,
which can be represented by the sum of the factors
?(x, y) = wT f(x, y) =
?
c?y
wT f(x, c) =
?
c?y
?c(x)
where f(x, c) is the feature vector that depends on
c. For example, we could define a feature for grand-
child c = {egh, ehm}
f(x, c) =
?
??
??
1 if xg = would ? xh = be
?xm = happy ? c is selected
0 otherwise
38
2.2 Dynamic Programming for Local Models
In first order models, all factors c in Eq(1) contain a
single edge. The optimal parse tree can be derived
by DP with running time O(n3) (Eisner, 1996). The
algorithm has two types of structures: complete s-
pan, which consists of a headword and its descen-
dants on one side, and incomplete span, which con-
sists of a dependency and the region between the
head and modifier. It starts at single word spans, and
merges the spans in bottom up order.
For second order models, the score function
?(x, y) adds the scores of siblings (adjacent edges
with a common head) and grandchildren
?(x, y) =
?
ehm?y
?ehm(x)
+
?
egh,ehm?y
?ehm,egh(x)
+
?
ehm,ehs?y
?ehm,ehs(x)
There are two versions of second order models,
used respectively by Carreras (2007) and Koo et al
(2010). The difference is that Carreras? only con-
siders the outermost grandchildren, while Koo and
Collin?s allows all grandchild features. Both models
permit O(n4) running time.
Third-order models score edge triples such as
three adjacent sibling modifiers, or grand-siblings
that score a word, its modifier and its adjacent grand-
children, and the inference complexity is O(n4)
(Koo and Collins, 2010).
In this paper, for all the factors/features that can
be handled by DP, we call them the local fac-
tors/features.
3 The Proposed Method
3.1 Basic Idea
For general high order models with non-local fea-
tures, we propose to use Branch and Bound (B&B)
algorithm to search the optimal parse tree. A B&B
algorithm has two steps: branching and bounding.
The branching step recursively splits the search s-
pace Y(x) into two disjoint subspaces Y(x) =
Y1
?Y2 by fixing assignment of one edge. For each
subspace Yi, the bounding step calculates the upper
bound of the optimal parse tree score in the sub-
space: UBYi ? maxy?Yi ?(x, y). If this bound is
no more than any obtained parse tree score UBYi ?
?(x, y?), then all parse trees in subspace Yi are no
more optimal than y?, and Yi could be pruned safely.
The efficiency of B&B depends on the branching
strategy and upper bound computation. For exam-
ple, Sun et al (2012) used B&B for MRFs, where
they proposed two branching strategies and a novel
data structure for efficient upper bound computation.
Klenner and Ailloud (2009) proposed a variation of
Balas algorithm (Balas, 1965) for coreference reso-
lution, where candidate branching variables are sort-
ed by their weights.
Our bounding strategy is to find an upper bound
for the score of each non-local factor c containing
multiple edges. The bound is the sum of new scores
of edges in the factor plus a constant
?c(x) ?
?
e?c
?e(x) + ?c
Based on the new scores {?e(x)} and constants
{?c}, we define the new score of parse tree y
?(x, y) =
?
c?y
(?
e?c
?e(x) + ?c
)
Then we have
?(x, y) ? ?(x, y), ?y ? Y(x)
The advantage of such a bound is that, it is the
sum of new edge scores. Hence, its optimum tree
maxy?Y(x) ?(x, y) can be found by DP, which is
the upper bound of maxy?Y(x) ?(x, y), as for any
y ? Y(x), ?(x, y) ? ?(x, y).
3.2 The Upper Bound Function
In this section, we derive the upper bound function
?(x, y) described above. To simplify notation, we
drop x throughout the rest of the paper. Let zc be
a binary variable indicating whether factor c is se-
lected in the parse tree. We reformulate the score
function in Eq(1) as
?(y) ? ?(z) =
?
c
?czc (2)
39
Correspondingly, the tree constraint is replaced by
z ? Z . Then the parsing task is
z? = argmaxz?Z?czc (3)
Notice that, for any zc, we have
zc = mine?c ze
which means that factor c appears in parse tree if and
only if all its edges {e|e ? c} are selected in the tree.
Here ze is short for z{e} for simplicity.
Our bounding method is based on the following
fact: for a set {a1, a2, . . . ar} (aj denotes the jth el-
ement) , its minimum
min{aj} = min
p??
?
j
pjaj (4)
where ? is probability simplex
? = {p|pj ? 0,
?
j
pj = 1}
We discuss the bound for ?czc in two cases: ?c ?
0 and ?c < 0.
If ?c ? 0, we have
?czc = ?cmine?c ze
= ?c minpc??
?
e?c
pecze
= min
pc??
?
e?c
?cpecze
The second equation comes from Eq(4). For sim-
plicity, let
gc(pc, z) =
?
e?c
?cpecze
with domain domgc = {pc ? ?; ze ? {0, 1}, ?e ?
c}. Then we have
?czc = minpc gc(pc, z) (5)
If ?c < 0, we have two upper bounds. One is
commonly used in ILP when all the variables are bi-
nary
a? = min
j
{aj}rj=1
?
a? ? aj
a? ?
?
j
aj ? (r ? 1)
According to the last inequality, we have the upper
bound for negative scored factors
?czc ? ?c
(?
e?c
ze ? (rc ? 1)
)
(6)
where rc is the number of edges in c. For simplicity,
we use the notation
?c(z) = ?c
(?
e?c
ze ? (rc ? 1)
)
The other upper bound when ?c < 0 is simple
?czc ? 0 (7)
Notice that, for any parse tree, one of the upper
bounds must be tight. Eq(6) is tight if c appears
in the parse tree: zc = 1, otherwise Eq(7) is tight.
Therefore
?czc = min {?c(z), 0}
Let
hc(pc, z) = p1c?c(z) + p2c ? 0
with domhc = {pc ? ?; ze ? {0, 1}, ?e ? c}.
According to Eq(4), we have
?czc = minpc hc(pc, z) (8)
Let
?(p, z) =
?
c,?c?0
gc(pc, z) +
?
c,?c<0
hc(pc, z)
Minimize ? with respect to p, we have
min
p
?(p, z)
= min
p
?
? ?
c,?c?0
gc(pc, z) +
?
c,?c<0
hc(pc, z)
?
?
=
?
c,?c?0
min
pc
gc(pc, z) +
?
c,?c<0
min
pc
hc(pc, z)
=
?
c,?c?0
?czc +
?
c,?c<0
?czc
= ?(z)
The second equation holds since, for any two fac-
tors, c and c?, gc (or hc) and gc? (or hc?) are separable.
The third equation comes from Eq(5) and Eq(8).
Based on this, we have the following proposition:
40
Proposition 1. For any p, pc ? ?, and z ? Z ,
?(p, z) ? ?(z).
Therefore, ?(p, z) is an upper bound function of
?(z). Furthermore, fixing p, ?(p, z) is a linear func-
tion of ze , see Eq(5) and Eq(8), variables zc for large
factors are eliminated. Hence z? = argmaxz?(p, z)
can be solved efficiently by DP.
Because
?(p, z?) ? ?(p, z?) ? ?(z?) ? ?(z?)
after obtaining z? , we get the upper bound and lower
bound of ?(z?): ?(p, z?) and ?(z?).
The upper bound is expected to be as tight as pos-
sible. Using min-max inequality, we get
max
z?Z
?(z) = max
z?Z
min
p
?(p, z)
? min
p
max
z?Z
?(p, z)
which provides the tightest upper bound of ?(z?).
Since ? is not differentiable w.r.t p, projected
sub-gradient (Calamai and More?, 1987; Rush et al,
2010) is used to search the saddle point. More
specifically, in each iteration, we first fix p and
search z using DP, then we fix z and update p by
pnew = P?
(
p+ ???p ?
)
where ? > 0 is the step size in line search, function
P?(q) denotes the projection of q onto the proba-
bility simplex ?. In this paper, we use Euclidean
projection, that is
P?(q) = minp?? ?p? q?2
which can be solved efficiently by sorting (Duchi et
al., 2008).
3.3 Branch and Bound Based Parsing
As discussed in Section 3.1, the B&B recursive pro-
cedure yields a binary tree structure called Branch
and Bound tree. Each node of the B&B tree has
some fixed ze, specifying some must-select edges
and must-remove edges. The root of the B&B tree
has no constraints, so it can produce all possible
parse trees including z?. Each node has two chil-
dren. One adds a constraint ze = 1 for a free edge
z =e1 0 1
0 1 0 1z =e2
??=9=4
?<LB
??=8=5 ??=7=4
??=7=4 ??=7=5 ??=4=3 ??=6=2
minp maxz?Z
ze1=0
ze2=1
?(p, z)
6
Figure 1: A part of B&B tree. ?, ? are short for
?(z?) and ?(p?, z?) respectively. For each node,
some edges of the parse tree are fixed. All parse
trees that satisfy the fixed edges compose the subset
of S ? Z . A min-max problem is solved to get the
upper bound and lower bound of the optimal parse
tree over S. Once the upper bound ? is less than
LB, the node is removed safely.
e and the other fixes ze = 0. We can explore the
search space {z|ze ? {0, 1}} by traversing the B&B
tree in breadth first order.
Let S ? Z be subspace of parse trees satisfying
the constraint, i.e., in the branch of the node. For
each node in B&B tree, we solve
p?, z? = argmin
p
max
z?S
?(p, z)
to get the upper bound and lower bound of the best
parse tree in S. A global lower bound LB is main-
tained which is the maximum of all obtained lower
bounds. If the upper bound of the current node is
lower than the global lower bound, the node can be
pruned from the B&B tree safely. An example is
shown in Figure 1.
When the upper bound is not tight: ? > LB, we
need to choose a good branching variable to gener-
ate the child nodes. Let G(z?) = ?(p?, z?) ? ?(z?)
denote the gap between the upper bound and lower
bound. This gap is actually the accumulated gaps of
all factors c. Let Gc be the gap of c
Gc =
{
gc(p?c, z?)? ?cz?c if ?c ? 0
hc(p?c, z?)? ?cz?c if ?c < 0
41
We choose the branching variable heuristically:
for each edge e, we define its gap as the sum of the
gaps of factors that contain it
Ge =
?
c,e?c
Gc
The edge with the maximum gap is selected as the
branching variable.
Suppose there are N nodes on a level of B&B
tree, and correspondingly, we get N branching vari-
ables, among which, we choose the one with the
highest lower bound as it likely reaches the optimal
value faster.
3.4 Lower Bound Initialization
A large lower bound is critical for efficient pruning.
In this section, we discuss an alternative way to ini-
tialize the lower bound LB. We apply the similar
trick to get the lower bound function of ?(z).
Similar to Eq(8), for ?c ? 0, we have
?czc = max{?c
(?
e?c
ze ? (rc ? 1)
)
, 0}
= max{?c(z), 0}
Using the fact that
max{aj} = max
p??
?
j
pjaj
we have
?czc = maxpc??
p1c?c(z) + p2c ? 0
= max
pc
hc(pc, z)
For ?c < 0, we have
?czc = maxe?c {?cze}
= max
pc??
?
e?c
pec?cze
= max
pc
gc(pc, z)
Put the two cases together, we get the lower bound
function
?(p, z) =
?
c,?c?0
hc(pc, z) +
?
c,?c<0
gc(pc, z)
Algorithm 1 Branch and Bound based parsing
Require: {?c}
Ensure: Optimal parse tree z?
Solve p?, z? = argmaxp,z?(p, z)
Initialize S = {Z}, LB = ?(p?, z?)
while S ?= ? do
Set S ? = ?{nodes that survive from pruning}
foreach S ? S
Solve minp maxz ?(p, z) to get LBS , UBS
LB = max{LB,LBS?S}, update z?
foreach S ? S, add S to S ?, if UBS > LB
Select a branching variable ze.
Clear S = ?
foreach S ? S ?
Add S1 = {z|z ? S, ze = 1} to S
Add S2 = {z|z ? S, ze = 0} to S.
end while
For any p, pc ? ?, z ? Z
?(p, z) ? ?(z)
?(p, z) is not concave, however, we could alterna-
tively optimize z and p to get a good approximation,
which provides a lower bound for ?(z?).
3.5 Summary
We summarize our B&B algorithm in Algorithm 1.
It is worth pointing out that so far in the above
description, we have used the assumption that the
backbone DP uses first order models, however, the
backbone DP can be the second or third order ver-
sion. The difference is that, for higher order DP,
higher order factors such as adjacent siblings, grand-
children are directly handled as local factors.
In the worst case, all the edges are selected for
branching, and the complexity grows exponentially
in sentence length. However, in practice, it is quite
efficient, as we will show in the next section.
4 Experiments
4.1 Experimental Settings
The datasets we used are the English Penn Tree
Bank (PTB) and Chinese Tree Bank 5.0 (CTB5). We
use the standard train/develop/test split as described
in Table 1.
We extracted dependencies using Joakim Nivre?s
Penn2Malt tool with standard head rules: Yamada
and Matsumoto?s (Yamada and Matsumoto, 2003)
42
Train Develop Test
PTB sec. 2-21 sec. 22 sec. 23
CTB5 sec. 001-815 sec. 886-931 sec. 816-885
1001-1136 1148-1151 1137-1147
Table 1: Data split in our experiment
for English, and Zhang and Clark?s (Zhang and
Clark, 2008) for Chinese. Unlabeled attachment s-
core (UAS) is used to evaluate parsing quality1. The
B&B parser is implemented with C++. All the ex-
periments are conducted on the platform Intel Core
i5-2500 CPU 3.30GHz.
4.2 Baseline: DP Based Second Order Parser
We use the dynamic programming based second or-
der parser (Carreras, 2007) as the baseline. Aver-
aged structured perceptron (Collins, 2002) is used
for parameter estimation. We determine the number
of iterations on the validation set, which is 6 for both
corpora.
For English, we train the POS tagger using linear
chain perceptron on training set, and predict POS
tags for the development and test data. The parser is
trained using the automatic POS tags generated by
10 fold cross validation. For Chinese, we use the
gold standard POS tags.
We use five types of features: unigram features,
bigram features, in-between features, adjacent sib-
ling features and outermost grand-child features.
The first three types of features are firstly introduced
by McDonald et al (2005a) and the last two type-
s of features are used by Carreras (2007). All the
features are the concatenation of surrounding words,
lower cased words (English only), word length (Chi-
nese only), prefixes and suffixes of words (Chinese
only), POS tags, coarse POS tags which are derived
from POS tags using a simple mapping table, dis-
tance between head and modifier, direction of edges.
For English, we used 674 feature templates to gener-
ate large amounts of features, and finally got 86.7M
non-zero weighted features after training. The base-
line parser got 92.81% UAS on the testing set. For
Chinese, we used 858 feature templates, and finally
got 71.5M non-zero weighted features after train-
1For English, we follow Koo and Collins (2010) and ignore
any word whose gold-standard POS tag is one of { ? ? : , .}. For
Chinese, we ignore any word whose POS tag is PU.
ing. The baseline parser got 86.89% UAS on the
testing set.
4.3 B&B Based Parser with Non-local Features
We use the baseline parser as the backbone of our
B&B parser. We tried different types of non-local
features as listed below:
? All grand-child features. Notice that this fea-
ture can be handled by Koo?s second order
model (Koo and Collins, 2010) directly.
? All great grand-child features.
? All sibling features: all the pairs of edges with
common head. An example is shown in Fig-
ure 2.
? All tri-sibling features: all the 3-tuples of edges
with common head.
? Comb features: for any word with more than 3
consecutive modifiers, the set of all the edges
from the word to the modifiers form a comb.2
? Hand crafted features: We perform cross val-
idation on the training data using the baseline
parser, and designed features that may correc-
t the most common errors. We designed 13
hand-craft features for English in total. One ex-
ample is shown in Figure 3. For Chinese, we
did not add any hand-craft features, as the er-
rors in the cross validation result vary a lot, and
we did not find general patterns to fix them.
4.4 Implementation Details
To speed up the solution of the min-max subprob-
lem, for each node in the B&B tree, we initialize p
with the optimal solution of its parent node, since
the child node fixes only one additional edge, its op-
timal point is likely to be closed to its parent?s. For
the root node of B&B tree, we initialize pec = 1rc for
factors with non-negative weights and p1c = 0 for
2In fact, our algorithm can deal with non-consecutive mod-
ifiers; however, in such cases, factor detection (detect regular
expressions like x1. ? x2. ? . . . ) requires the longest com-
mon subsequence algorithm (LCS), which is time-consuming
if many comb features are generated. Similar problems arise
for sub-tree features, which may contain many non-consecutive
words.
43
c 0 c 1 c 2 c 3h
c 0 c 1h c 0 c 2h c 0 c 3h
c 1 c 2h c 2 c 3h c 1 c 3h
secondorder higher order
Figure 2: An example of all sibling features. Top:
a sub-tree; Bottom: extracted sibling features. Ex-
isting higher order DP systems can not handle the
siblings on both sides of head.
regulation occurs through inaction , rather than through ...
Figure 3: An example of hand-craft feature: for the
word sequence A . . . rather than A, where A is a
preposition, the first A is the head of than, than is
the head of rather and the second A.
negative weighted factors. Step size ? is initialized
with maxc,?c ?=0{ 1|?c|}, as the vector p is bounded ina unit box. ? is updated using the same strategy as
Rush et al (2010). Two stopping criteria are used.
One is 0 ? ?old ??new ? ?, where ? > 0 is a given
precision3. The other checks if the bound is tight:
UB = LB. Because all features are boolean (note
that they can be integer), their weights are integer
during each perceptron update, hence the scores of
parse trees are discrete. The minimal gap between
different scores is 1N?T after averaging, where N isthe number of training samples, and T is the itera-
tion number for perceptron training. Therefore the
upper bound can be tightened as UB = ?NT??NT .
During testing, we use the pre-pruning method as
used in Martins et al (2009) for both datasets to bal-
ance parsing quality and speed. This method uses a
simple classifier to select the top k candidate head-
s for each word and exclude the other heads from
search space. In our experiment, we set k = 10.
3we use ? = 10?8 in our implementation
System PTB CTB
Our baseline 92.81 86.89
B&B +all grand-child 92.97 87.02
+all great grand-child 92.78 86.77
+all sibling 93.00 87.05
+all tri-sibling 92.79 86.81
+comb 92.86 86.91
+hand craft 92.89 N/A
+all grand-child + all sibling + com-
b + hand craft
93.17 87.25
3rd order re-impl. 93.03 87.07
TurboParser (reported) 92.62 N/A
TurboParser (our run) 92.82 86.05
Koo and Collins (2010) 93.04 N/A
Zhang and McDonald (2012) 93.06 86.87
Zhang and Nivre (2011) 92.90 86.00
System integration
Bohnet and Kuhn (2012) 93.39 87.5
Systems using additional resources
Suzuki et al (2009) 93.79 N/A
Koo et al (2008) 93.5 N/A
Chen et al (2012) 92.76 N/A
Table 2: Comparison between our system and the-
state-of-art systems.
4.5 Main Result
Experimental results are listed in Table 2. For com-
parison, we also include results of representative
state-of-the-art systems. For the third order pars-
er, we re-implemented Model 1 (Koo and Collins,
2010), and removed the longest sentence in the CTB
dataset, which contains 240 words, due to theO(n4)
space complexity 4. For ILP based parsing, we used
TurboParser5, a speed-optimized parser toolkit. We
trained full models (which use all grandchild fea-
tures, all sibling features and head bigram features
(Martins et al, 2011)) for both datasets using its de-
fault settings. We also list the performance in its
documentation on English corpus.
The observation is that, the all-sibling features are
most helpful for our parser, as some good sibling
features can not be encoded in DP based parser. For
example, a matched pair of parentheses are always
siblings, but their head may lie between them. An-
4In fact, Koo?s algorithm requires only O(n3) space. Our
implementation is O(n4) because we store the feature vectors
for fast training.
5http://www.ark.cs.cmu.edu/TurboParser/
44
other observation is that all great grandchild features
and all tri-sibling features slightly hurt the perfor-
mance and we excluded them from the final system.
When no additional resource is available, our
parser achieved competitive performance: 93.17%
Unlabeled Attachment Score (UAS) for English at
a speed of 177 words per second and 87.25% for
Chinese at a speed of 97 words per second. High-
er UAS is reported by joint tagging and parsing
(Bohnet and Nivre, 2012) or system integration
(Bohnet and Kuhn, 2012) which benefits from both
transition based parsing and graph based parsing.
Previous work shows that combination of the two
parsing techniques can learn to overcome the short-
comings of each non-integrated system (Nivre and
McDonald, 2008; Zhang and Clark, 2008). Sys-
tem combination will be an interesting topic for our
future research. The highest reported performance
on English corpus is 93.79%, obtained by semi-
supervised learning with a large amount of unla-
beled data (Suzuki et al, 2009).
4.6 Tradeoff Between Accuracy and Speed
In this section, we study the trade off between ac-
curacy and speed using different pre-pruning setups.
In Table 3, we show the parsing accuracy and in-
ference time in testing stage with different numbers
of candidate heads k in pruning step. We can see
that, on English dataset, when k ? 10, our pars-
er could gain 2 ? 3 times speedup without losing
much parsing accuracy. There is a further increase
of the speed with smaller k, at the cost of some ac-
curacy. Compared with TurboParser, our parser is
less efficient but more accurate. Zhang and McDon-
ald (2012) is a state-of-the-art system which adopts
cube pruning for efficient parsing. Notice that, they
did not use pruning which seems to increase parsing
speed with little hit in accuracy. Moreover, they did
labeled parsing, which also makes their speed not
directly comparable.
For each node of B&B tree, our parsing algorithm
uses projected sub-gradient method to find the sad-
dle point, which requires a number of calls to a DP,
hence the efficiency of Algorithm 1 is mainly deter-
mined by the number of DP calls. Figure 4 and Fig-
ure 5 show the averaged parsing time and number of
calls to DP relative to the sentence length with differ-
ent pruning settings. Parsing time grows smoothly
PTB CTB
System UAS w/s UAS w/s
Ours (no prune) 93.18 52 87.28 73
Ours (k = 20) 93.17 105 87.28 76
Ours (k = 10) 93.17 177 87.25 97
Ours (k = 5) 93.10 264 86.94 108
Ours (k = 3) 92.68 493 85.76 128
TurboParser(full) 92.82 402 86.05 192
TurboParser(standard) 92.68 638 85.80 283
TurboParser(basic) 90.97 4670 82.28 2736
Zhang and McDon-
ald (2012)?
93.06 220 86.87 N/A
Table 3: Trade off between parsing accuracy (UAS)
and speed (words per second) with different pre-
pruning settings. k denotes the number of candi-
date heads of each word preserved for B&B parsing.
?Their speed is not directly comparable as they per-
forms labeled parsing without pruning.
when sentence length ? 40. There is some fluctua-
tion for the long sentences. This is because there are
very few sentences for a specific long length (usual-
ly 1 or 2 sentences), and the statistics are not stable
or meaningful for the small samples.
Without pruning, there are in total 132, 161 calls
to parse 2, 416 English sentences, that is, each sen-
tence requires 54.7 calls on average. For Chinese,
there are 84, 645 calls for 1, 910 sentences, i.e., 44.3
calls for each sentence on average.
5 Discussion
5.1 Polynomial Non-local Factors
Our bounding strategy can handle a family of non-
local factors that can be expressed as a polynomial
function of local factors. To see this, suppose
zc =
?
i
?i
?
e?Ei
ze
For each i, we introduce new variable zEi =
mine?Ei ze. Because ze is binary, zEi =
?
e?Ei ze.In this way, we replace zc by several zEi that can be
handled by our bounding strategy.
We give two examples of these polynomial non-
local factors. First is the OR of local factors: zc =
max{ze, z?e}, which can be expressed by zc = ze +
z?e?zez?e. The second is the factor of valency feature
45
0 10 20 30 40 50 600
5
10
parsi
ng tim
e (sec
.)
sentence length
k=3k=5k=10k=20no prune
(a) PTB corpus
0 20 40 60 80 100 120 1400
20
40
60
80
parsi
ng tim
e (sec
.)
sentence length
k=3k=5k=10k=20no prune
(b) CTB corpus
Figure 4 Averaged parsing time (seconds) relative to sentence length with different pruning settings, k
denotes the number of candidate heads of each word in pruning step.
0 10 20 30 40 50 600
100
200
Calls
 to DP
sentence length
k=3k=5k=10k=20no prune
(a) PTB corpus
0 20 40 60 80 100 120 1400
500
1000
Calls
 to DP
sentence length
k=3k=5k=10k=20no prune
(b) CTB corpus
Figure 5 Averaged number of Calls to DP relative to sentence length with different pruning settings, k
denotes the number of candidate heads of each word in pruning step.
(Martins et al, 2009). Let binary variable vik indi-
cate whether word i has k modifiers. Given {ze} for
the edges with head i, then {vik|k = 1, . . . , n ? 1}
can be solved by
?
k
kjvik =
(?
e
ze
)j
0 ? j ? n? 1
The left side of the equation is the linear function of
vik. The right side of the equation is a polynomial
function of ze. Hence, vik could be expressed as a
polynomial function of ze.
5.2 k Best Parsing
Though our B&B algorithm is able to capture a va-
riety of non-local features, it is still difficult to han-
dle many kinds of features, such as the depth of the
parse tree. Hence, a reranking approach may be use-
ful in order to incorporate such information, where
k parse trees can be generated first and then a second
pass model is used to rerank these candidates based
on more global or non-local features. In addition,
k-best parsing may be needed in many applications
to use parse information and especially utilize infor-
mation from multiple candidates to optimize task-
specific performance. We have not conducted any
experiment for k best parsing, hence we only dis-
cuss the algorithm.
According to proposition 1, we have
Proposition 2. Given p and subset S ? Z , let zk
denote the kth best solution of maxz?S ?(p, z). If a
parse tree z? ? S satisfies ?(z?) ? ?(p, zk), then z?
is one of the k best parse trees in subset S.
Proof. Since zk is the kth best solution of ?(p, z),
for zj , j > k, we have ?(p, zk) ? ?(p, zj) ?
?(zj). Since the size of the set {zj |j > k} is
|S| ? k, hence there are at least |S| ? k parse trees
whose scores ?(zj) are less than ?(p, zk). Because
?(z?) ? ?(p, zk), hence z? is at least the kth best
parse tree in subset S.
Therefore, we can search the k best parse trees
in this way: for each sub-problem, we use DP to
derive the k best parse trees. For each parse tree
z, if ?(z) ? ?(p, zk), then z is selected into the k
best set. Algorithm terminates until the kth bound is
tight.
46
6 Conclusion
In this paper we proposed a new parsing algorithm
based on a Branch and Bound framework. The mo-
tivation is to use dynamic programming to search
for the bound. Experimental results on PTB and
CTB5 datasets show that our method is competitive
in terms of both performance and efficiency. Our
method can be adapted to non-projective dependen-
cy parsing, as well as the k best MST algorithm
(Hall, 2007) to find the k best candidates.
Acknowledgments
We?d like to thank Hao Zhang, Andre Martins and
Zhenghua Li for their helpful discussions. We al-
so thank Ryan McDonald and three anonymous re-
viewers for their valuable comments. This work
is partly supported by DARPA under Contract No.
HR0011-12-C-0016 and FA8750-13-2-0041. Any
opinions expressed in this material are those of the
authors and do not necessarily reflect the views of
DARPA.
References
Michael Auli and Adam Lopez. 2011. A comparison of
loopy belief propagation and dual decomposition for
integrated CCG supertagging and parsing. In Proc. of
ACL-HLT.
Egon Balas. 1965. An additive algorithm for solving
linear programs with zero-one variables. Operations
Research, 39(4).
Bernd Bohnet and Jonas Kuhn. 2012. The best of
bothworlds ? a graph-based completion model for
transition-based parsers. In Proc. of EACL.
Bernd Bohnet and Joakim Nivre. 2012. A transition-
based system for joint part-of-speech tagging and la-
beled non-projective dependency parsing. In Proc. of
EMNLP-CoNLL.
Paul Calamai and Jorge More?. 1987. Projected gradien-
t methods for linearly constrained problems. Mathe-
matical Programming, 39(1).
Xavier Carreras. 2007. Experiments with a higher-order
projective dependency parser. In Proc. of EMNLP-
CoNLL.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. In Proc. of ACL.
Wenliang Chen, Min Zhang, and Haizhou Li. 2012. U-
tilizing dependency language models for graph-based
dependency parsing models. In Proc. of ACL.
Michael Collins. 2000. Discriminative reranking for nat-
ural language parsing. In Proc. of ICML.
Michael Collins. 2002. Discriminative training methods
for hidden markov models: Theory and experiments
with perceptron algorithms. In Proc. of EMNLP.
John Duchi, Shai Shalev-Shwartz, Yoram Singer, and
Tushar Chandra. 2008. Efficient projections onto the
l1-ball for learning in high dimensions. In Proc. of
ICML.
Jason M. Eisner. 1996. Three new probabilistic models
for dependency parsing: an exploration. In Proc. of
COLING.
Keith Hall. 2007. K-best spanning tree parsing. In Proc.
of ACL.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In Proc. of IWPT.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proc. of ACL-HLT.
Manfred Klenner and E?tienne Ailloud. 2009. Opti-
mization in coreference resolution is not needed: A
nearly-optimal algorithm with intensional constraints.
In Proc. of EACL.
Terry Koo and Michael Collins. 2010. Efficient third-
order dependency parsers. In Proc. of ACL.
Terry Koo, Xavier Carreras, and Michael Collins. 2008.
Simple semi-supervised dependency parsing. In Proc.
of ACL-HLT.
Terry Koo, Alexander M. Rush, Michael Collins, Tommi
Jaakkola, and David Sontag. 2010. Dual decomposi-
tion for parsing with non-projective head automata. In
Proc. of EMNLP.
Ailsa H. Land and Alison G. Doig. 1960. An automat-
ic method of solving discrete programming problems.
Econometrica, 28(3):497?520.
Andre Martins, Noah Smith, and Eric Xing. 2009. Con-
cise integer linear programming formulations for de-
pendency parsing. In Proc. of ACL.
Andre Martins, Noah Smith, Mario Figueiredo, and Pe-
dro Aguiar. 2011. Dual decomposition with many
overlapping components. In Proc. of EMNLP.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005a. Online large-margin training of dependency
parsers. In Proc. of ACL.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic. 2005b. Non-projective dependency pars-
ing using spanning tree algorithms. In Proc. of HLT-
EMNLP.
Joakim Nivre and Ryan McDonald. 2008. Integrating
graph-based and transition-based dependency parsers.
In Proc. of ACL-HLT.
Jorge Nocedal and Stephen J. Wright. 2006. Numerical
Optimization. Springer, 2nd edition.
47
Sebastian Riedel and James Clarke. 2006. Incremental
integer linear programming for non-projective depen-
dency parsing. In Proc. of EMNLP.
Alexander M Rush, David Sontag, Michael Collins, and
Tommi Jaakkola. 2010. On dual decomposition and
linear programming relaxations for natural language
processing. In Proc. of EMNLP.
David Smith and Jason Eisner. 2008. Dependency pars-
ing by belief propagation. In Proc. of EMNLP.
Min Sun, Murali Telaprolu, Honglak Lee, and Silvio
Savarese. 2012. Efficient and exact MAP-MRF in-
ference using branch and bound. In Proc. of AISTATS.
Jun Suzuki, Hideki Isozaki, Xavier Carreras, andMichael
Collins. 2009. An empirical study of semi-supervised
structured conditional models for dependency parsing.
In Proc. of EMNLP.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statistical
dependency analysis with support vector machines. In
Proc. of IWPT.
Yue Zhang and Stephen Clark. 2008. A tale of t-
wo parsers: Investigating and combining graph-based
and transition-based dependency parsing. In Proc. of
EMNLP.
Hao Zhang and Ryan McDonald. 2012. Generalized
higher-order dependency parsing with cube pruning.
In Proc. of EMNLP.
Yue Zhang and Joakim Nivre. 2011. Transition-based
dependency parsing with rich non-local features. In
Proc. of ACL-HLT.
48
