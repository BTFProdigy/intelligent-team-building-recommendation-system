An HPSG-to-CFG Approximation of Japanese 
Bernd Kiefer, Hans-Ulrich Krieger, Melanie Siegel 
German Research Center for Artificial Intell igence (DFKI )  
Stuhlsatzenhausweg 3, D-66123 Saarbri icken 
{kiefer, krieger, siegel}@dfki, de 
Abstract 
We present asimple approximation method for turn- 
ing a Head-Driven Phrase Structure Grammar into a 
context-free grammar. The approximation method 
can be seen as the construction of the least fixpoint 
of a certain monotonic hmction. We discuss an ex- 
periment with a large HPSG for Japanese. 
1 In t roduct ion  
This paper presents a simple approximation 
method for turning an HPSG (Pollard and Sag, 
1994) into a context-free grmnmar. The the- 
oretical underpinning is established through a 
least fixpoint construction over a certain mono- 
tonic function, similar to the instantiation of 
a rule in a bottom-up passive chart parser or 
to partial evaluation in logic programming; see 
(Kiefer and Krieger, 2000a). 
1.1 Bas ic  Idea  
The intuitive idea underlying our approach is 
to generalize in a first step the set of all lexicon 
entries. The resulting structures form equiv- 
alence classes, since they abstract from word- 
specific information, such as FORN or STEM. The 
abstraction is specified by means of a restrictor 
(Shiet)er, 1985), the so-called lexicon rcstrictor. 
The grammar rules/schemata are then instan- 
tiated via unification, using the abstracted lexi- 
con entries, yielding derivation trees of depth 1. 
We apply the rule restrictor to each resulting 
feature structure, which removes all information 
contained only in the daughters of the rule. Due 
to the Locality Principle of HPSG, this deletion 
does not alter the set of derivable feature struc- 
tures. Since we are interested in a finite fixpoint 
from a practical point of view, the restriction 
also gets rid of information that will lead to in- 
finite growth of feature structures during deriva- 
tion. Additionally, we throw away information 
that will not restrict the search space (typically, 
parts of tile semantics). The restricted fea- 
ture structures (together with older ones) then 
serve as tile basis for the next instantiation step. 
Again, this gives us feature structures encoding 
a derivation, and again we are applying the rule 
restrictor. We proceed with the iteration, until 
we reach a fixpoint, meaning that further itera- 
tion steps will not add (or remove) new (o1" old) 
feature structures. 
Our goal, however, is to obtain a context-fl'ee 
grammar, trot since we have reached a fixpoint, 
we can use the entire feature structures as (com- 
plex) context-free symbols (e.g., by nlapping 
them to integers). By instantiating the HPSG 
rules a final time with feature structures from 
the fixpoint, applying the rule restrictor and 
finally classifying the resulting structure (i.e., 
find tile right structure from the fixpoint), one 
can easily obtain tile desired context-free grain- 
mar (CFG). 
1.2 Why is it Wor th?  
Approximating an HPSG through a CFG ~ is 
interesting for the following practical reason: 
assuming that we have a CFG that comes close 
to an HPSG, we can use the CFG as a cheap fil- 
ter (running time complexity is O(IGI 2 x n 3) for 
an arbitrary sentence of length n). The main 
idea is to use the CFG first and then let the 
HPSG deterministically replay the derivations 
licensed by the CFG. The important point here 
is that one can find for every CF production 
exactly one and only one HPSG rule. (Kasper 
et al, 1996) describe such an approach for word 
graph parsing which employs only the relatively 
unspecific CF backbone of an HPSG-like grmn- 
mar. (Diagne et al, 1995) replaces the CF back- 
bone through a restriction of the original HPSG. 
This grammar, however, is still an unification- 
1046 
based grammar, since it employs coreference 
constraints. 
1..3 Content  of  Paper 
In tile next section, we describe the Japanese 
HPSG that is used in Verbmobil, a project that 
deals with the translation of spontaneously spo- 
ken dialogues between English, German, and 
Japanese speakers. After that, section 3 ex- 
plains a simplified, albeit correct version of the 
implemented algorithm. Section 4 then dis- 
cusses the outcome of the approximation pro- 
cess .  
2 Japanese  Grammar  
The grammar was developed for machine trans- 
lation of spoken dialogues. It is capable of deal- 
ing with spoken language phenomena nd un- 
grammatical or corrupted input. This leads on 
the one hand to the necessity of robustness and 
on the other hand to mnbiguitics that must be 
dealt with. Being used in an MT system for spo- 
ken language, the grammar must firstly accept 
fragmentary input and bc able to deliver partial 
analyses, where no spanning analysis is awdl- 
able. A coinplete fragmentary utterance could, 
e.g., be: 
dai~oubu 
OKay 
This is an adjective without any noun or (cop- 
ula) verb. There is still an analysis available. 
If an utterance is corrupted by not being fully 
recognized~ the grammar delivers analyses for 
those parts that could be understood. An ex- 
ample would be the following transliteration of
input to the MT system: 
son desu ne watakushi 
so COP TAG i 
no hou wa dai~oubu 
GEN side 'FOP okay 
desu da ga kono hi 
COP but this day 
wa kayoubi desu ~te 
TOP Tuesday COP TAG 
(lit.: Well, it is okay for my side, but 
this day is ~l~msday, isn't it?) 
Here, analyses for the following fragments arc 
delivered (where the parser found opera wa in 
the word lattice of the speech recognizer): 
sou dcsu nc watakushi 
so COP TAG I 
no hou wa dai{oubu 
GEN side TOP okay 
dCSlt 
COP 
(Well, it is okay for my side.) 
era  TOP 
(The opera) 
hone hi wa kayoubi 
this day TOP Tlmsday 
desu nc 
COP TAG 
(This (lay is 3hmsday, isn't it?) 
Another necessity for partial analysis comes 
fl'om real-time restrictions imposed by the MT 
system. If tile parser is not allowed to produce 
a spanning analysis, it delivers best partial frag- 
ments. 
rl'tle grammar must also be applicable to phe- 
nomena of spoken language. A typical problem 
is tile extensive use of topicalization and even 
omission of particles. Also serialization of parti- 
cles occur nlore often than in written language, 
as described in (Siegel, 1999). A well-defined 
type hierarchy of Japanese particles is necessary 
here to describe their functions in the dialogues. 
Extensive use of honorification is another sig- 
nificance of spoken Japanese. A detailed de- 
scription is necessary for different purposes in 
an MT system: honorification is a syntactic 
restrictor in subject-verb agreement and com- 
plement sentences, l~lrthermore, it is a very 
useflfl source of information for the solution 
of zero pronominalization (Metzing and Siegel, 
1994). It is finally necessary for Japanese gener- 
ation in order to tind the appropriate honorific 
forms. The sign-based in%rmation structure of 
HPSG (Pollard and Sag, 1994) is predestined 
to describe honorification on the different levels 
of linguistics: on the syntactic level for agree- 
ment phenomena, on tile contextual level for 
anaphora resolution and connection to speaker 
and addressee reference, and via co-indexing on 
the semantic level. Connected to honorification 
is the extensive use of auxiliary and light verb 
constructions that require solutions in the areas 
of morphosyntax, semantics, and context (see 
(Siegel, 2000) for a more detailled description). 
Finally, a severe problem of tile Japanese 
grammar in the MT system is the high po- 
1047 
tential of ambiguity arising from the syntax of 
Japanese itself, and especially from the syntax 
of Japanese spoken language. For example, the 
Japanese particle ga marks verbal argmnents in 
most cases. There are, however, occurrences of 
ga that are assigned to verbal adjuncts. Allow- 
ing g a in any case to mark arguments or ad- 
juncts would lead to a high potential of (spuri- 
ous) ambiguity. Thus, a restriction was set on 
the adjunctive g a, requiring the modified verb 
not to have any unsaturated ga arguments. 
The Japanese language allows many verbal 
arguments to be optional. For example, pro- 
nouns are very often not uttered. This phe- 
nomenon is basic for spoken Japanese, such that 
a syntax urgently needs a clear distinction be- 
tween optional and obligatory (and adjacent) 
arguments. We therefore used a description 
of subcategorization that differs from standard 
HPSG description in that it explicitly states the 
optionality of arguments. 
3 Bas ic  Algor i thm 
We stm't with the description of the top-level 
function HPSG2CFG which initiates the ap- 
proximation process (cf. section 1.1 for the 
main idea). Let 7~ be the set of all rules/rule 
schemata, 12 the set of all lexicon entries, R 
the rule restrictor, and L the lexicon restrictor. 
We begin the approximation by first abstract- 
ing from the lexicon entries /2 with the help of 
the lexicon restrictor L (line 5 of the algorithm). 
This constitutes our initial set To (line 6). Fi- 
nally, we start the fixpoint iteration calling It- 
crate with the necessary parameters. 
1 HPSG2CFG(T~, 12, R, L) :~==~ 
2 local To; 
3 T0 := (~; 
4 for each  l E/2 
5 l :=  L(1); 
6 To := To u {l}; 
7 Iterate(T~, R, To). 
After that, the instantiation of the rule 
schemata with rule/lexicon-restricted elements 
from the previous iteration Ti begins (line 11- 
14). Instantiation via unification is performed 
by Fill-Daughters which takes into account a 
single rule r and Ti, returning successful instan- 
tiations (line 12) to which we apply the rule 
restrictor (line 13). The outcome of this restric- 
tion is added to the actual set of rule-restricted 
feature structures Ti+l iff it is new (remember 
how set union works; line 14). In case that re- 
ally new feature structures have not been added 
during the current iteration (line 15), meaning 
that we have reached a fixpoint, we immediately 
exit with T/ (line 16) from which we generate 
the context-free rules as indicated in section 1.1. 
Otherwise, we proceed with the iteration (line 
17). 
8 Iterate(g, R, Ti) :?==v 
9 local Ti+j; 
10 Ti+~ := Ti; 
11 for each r E T~ 
12 for each t C Fill-Daughters(r, Ti) do 
13 t := R(t); 
14 Ti+I := Ti+I U {t}; 
15 i f  Ti = T/+I 
16 then  re turn  Cornpute-CF-Rules(TG i)
17 else Iterate(7~, R, Ti+l). 
We note here that the pseudo code above is 
only a naYve version of the implemented algo- 
rithm. It is still correct, but not computation- 
ally tractable when dealing with large HPSG 
grammars. Technical details and optimizations 
of the actual algorithm, together with a descrip- 
tion of the theoretical foundations are described 
in (Kiefer and Krieger, 2000a). Due to space 
limitations, we can only give a glimpse of the 
actual implementation. 
Firstly, the most obvious optimization applies 
to the function Fill-Daughters (line 12), where 
the number of unifications is reduced by avoid- 
ing recomputation of combinations of daugh- 
ters and rules that already have been checked. 
To do this in a simple way, we split the set Ti 
into Ti \ T/.-1 and T/_I and fill a rule with only 
those permutations of daughters which contain 
at least one element from T/ \ r / _  1 . This guaran- 
tees checking of only those configurations which 
were enabled by the last iteration. 
Secondly, we use techniques developed in 
(Kiefer et al, 1999), namely the so-called rule 
filter and the quick-check method. The rule fil- 
ter precomputes the applicability of rules into 
each other and thus is able to predict a fail- 
ing unification using a simple and fast table 
lookup. The quick-check method exploits the 
1048 
flint that unification fails snore often at cer- 
tain points in feature structnres than at oth- 
ers. In an off  line stage, we parse a test cor- 
pus, using a special unifier that records all fail- 
ures instead of bailing out after the first one 
in order to determine the most prominent fail- 
ure points/paths. These points constitute the 
so-called quick-check vector. When executing a
unification during approximation, those points 
are efficiently accessed and checked using type 
unification prior to the rest of the structure. Ex- 
actly these quick-check points are used to build 
the lexicon and the rule restrictor as described 
earlier (see fig. 1). During ore: experinmnts, 
nearly 100% of all failing unifications in Fill- 
Daughters could be quickly detected using the 
above two techniques. 
Thirdly, instead of using set union we use 
tlhe more elaborate operation during the addi- 
tion of new feature structures to T/.+I. In fact, 
we add a new structure only if it is not sub- 
sumed by some structure already in tile set. To 
do this efficiently, tile quick-check vectors de- 
scribed above are employed here: before per- 
fl)rming full feature structure subsnmption, we 
pairwise check the elements of the vectors us- 
ing type subsumption and only if this succeeds 
do a full subsmnption test. If we add a new 
structure, we also remove all those structures in 
7)ql that are subsumed by the slew structure 
in order to keep the set small. This does not 
change the language of tile resulting CF gram- 
mar because a more general structure can be 
put into at least those daughter positions which 
can be fillcd by the more specific one. Conse- 
quently, fbr each production that employs the 
more specific structure, there will be a (pos- 
sibly) more general production employing the 
more general structure in the same daughter po- 
sitions. Extending feature structure subsump- 
lion by quick-check subsumption definitely pays 
off: more than 98% of all failing subsumptions 
could be detected early. 
Further optimizations to make the algorithm 
works in practice are described in (Kiefer and 
Krieger, 2000b). 
4 Eva luat ion  
The Japanese HPSG grammar used in our ex- 
periment consists of 43 rule sdmmata (28 unary, 
15 binary), 1,208 types and a test lexicon of 
2,781 highly diverse entries. The lexicon restric- 
tot, as introduced in section 1.1 and depicted in 
figure 1, maps these entries onto 849 lexical ab- 
stractions. This restrictor tells us which parts of 
a feature structure have to be deleted---it s the 
kind of restrictor which we are usually going to 
use. We call this a negative restrictor, contrary 
to tile positive restrictors used in the PATR- 
II system that specii\[y those parts of a feature 
structure which will survive after restricting it. 
Since a restrictor could have reentrance points, 
one can even define a reeursivc (or cyclic) re- 
strictor to foresee recursive mbeddings as is the 
case in HPSG. 
The rule restrictor looks quite silnilar, cut- 
ling off additionally information contained only 
in the daughters. Since both restrictors remove 
the CONTENT feature (and hence the semantics 
which is a source of infinite growth), it hal> 
pened that two very productive head-adjunct 
schemata could be collapsed into a single rule. 
Tiffs has helped to keep the number of feature 
structures in the fixpoint relatively small. 
We reached the fixpoint after 5 iteration 
steps, obtaining 10,058 featnre structures. The 
comtmtation of the fixpoint took about 27.3 
CPU hours on a 400MHz SUN Ultrasparc 2with 
t~?anz Allegro Common Lisp under Solaris 2.5. 
Given tim feature structures from the fixpoint, 
the 43 rules might lead to 28 x 10,058-t- 15 x 
10,058 x 10,058 = 1, 51.7,732,084 CF produc- 
tions in the worst case. Our method produces 
19,198,592 productions, i.e., 1.26% of all pos- 
sible ones. We guess that the enormous et of 
productions is due tile fact that the grammar 
was developed for spoken Japanese (recall sec- 
tion 2 on the mnbiguity of Japanese). Likewise, 
the choice of a 'wrong' restrictor often leads to a 
dramatic increase of structures in the fixpoint, 
and hence of CF rules--we are not sure at this 
point whether our restrictor is a good compro- 
mise between tile specificity of the context-free 
language and the number of context-free rules. 
We are currently implementing a CF parser that 
can handle such an enormous et of CF rules. 
In (Kiefer and Krieger, 2000b), we report on 
a similar experiment that we carried out using 
the English Verbmobil grmnmar, developed at 
CSLI, Stanford. In this paper, we showed that 
the workload on the HPSG side can be drasti- 
cally reduced by using a CFG filter, obtained 
1049 
-PHON 
FORM 
SYNSEM LOCAL 
-CONTENT 
CONTEXT 
HEAD 
CAT 
SUBCAT 
3PEC ~\ ]  
"NONLOCAL 
-CONTENT 
CONTEXT 
LOCAL 
~AT 
M0D 
MARK \[~\] 
FORMAL 
MODUS 
P0S 
PTYPE 
10BJIII I 
I OBJ2 I i I I 
VAL ~ 1 
I SpR l J  ! 
I SUBJ I 1 I I 
SUBCAT 
HEAD 
POS 11 SPEC FORMAl MARK 
MOD 
Figure 1: The lexicon restrictor used during the approximation of the Japanese grammar. In 
addition, the rule restrictor cuts off the DAUGHTERS feature. 
from the HPSG. Our hope is that these results 
can be carried over to the Japanese grammar. 
Acknowledgments  
This research was supported by the German 
Ministry for Education, Science, Research, and 
Technology under grant no. 01 IV 701 V0. 
Re ferences  
Abdel Kader Diagne, Walter Kasper, and Hans- 
Ulrich Krieger. 1995. Distributed parsing with 
HPSG grammars. In Proceedings of the ~th Inter- 
national Workshop on Parsing Technologies~ IW- 
PT'95, pages 79-86. 
Walter Kasper, Hans-Ulrich Krieger, JSrg Spilker, 
and Hans Webcr. 1996. From word hypotheses to
logical form: An efficient interleaved approach. In 
D. Gibbon, editor, Natural Language Processing 
and Speech Technology, pages 77-88. Mouton de 
Gruyter, Berlin. 
Bernd Kiefer and Hans-Ulrich Krieger. 2000a. 
A context-free approximation of Head-Driven 
Phrase Structure Grmnmar. In Proceedings of the 
6th International Workshop on Parsing Technolo- 
gies, IWPT2000, pages 135-146. 
Bernd Kicfcr and Hans-UMch Kricger. 2000b. Ex- 
periments with an HPSG-to-CFG approximation. 
Research report. 
Bernd Kicfcr, Hans-Ulrich Kricger, John Carroll, 
and Rob Malouf. 1999. A bag of useful techniques 
for emcient and robust parsing. In Proceedings of 
the 37th Annual Meeting of the Association for 
Computational Linguistics, pages 473-480. 
Dieter Metzing and Melanin Siegel. 1994. Zero pro- 
noun processing: Some requirements tbr a Verb- 
mobil system. Verbmobil-Memo 46. 
Carl Pollard and Ivan A. Sag. 1994. Head-Driven 
Phrase Structure Grammar. Studies in Contem- 
porary Linguistics. University of Chicago Press, 
Chicago. 
Stuart M. Shiebcr. 1985. Using restriction to extend 
parsing algorithms for complex-feature-based for- 
malisms. In Proceedings of the 23rd Annual Meet- 
ing of the Association for Computational Linguis- 
tics, pages 145-152. 
Melanin Siegel. 1999. The syntactic processing of 
particles in Japanese spoken language. In PTv- 
cecdings of the 13th Pacific Asia Confcrcncc on 
Language, Information and Computation, pages 
313-320. 
Melanic Siegel. 2000. Japanese honorification i an 
HPSG framework. In Proceedings of the l~th Pa- 
cific Asia Conference on Language, Information 
and Computation, pages 289-300. 
1050 
A Novel Disambiguation Method For Unification-Based Grammars Using
Probabilistic Context-Free Approximations
Bernd Kiefer, Hans-Ulrich Krieger, Detlef Prescher
 
kiefer|krieger|prescher  @dfki.de
Language Technology Lab, DFKI GmbH
Stuhlsatzenhausweg 3
66123 Saarbru?cken, Germany
Abstract
We present a novel disambiguation method for
unification-based grammars (UBGs). In contrast to other
methods, our approach obviates the need for probability
models on the UBG side in that it shifts the responsibil-
ity to simpler context-free models, indirectly obtained
from the UBG. Our approach has three advantages:
(i) training can be effectively done in practice, (ii)
parsing and disambiguation of context-free readings
requires only cubic time, and (iii) involved probability
distributions are mathematically clean. In an experiment
for a mid-size UBG, we show that our novel approach is
feasible. Using unsupervised training, we achieve 88%
accuracy on an exact-match task.
1 Introduction
This paper deals with the problem of how to dis-
ambiguate the readings of sentences, analyzed by a
given unification-based grammar (UBG).
Apparently, there are many different approaches
for almost as many different unification-based
grammar formalisms on the market that tackle this
difficult problem. All approaches have in common
that they try to model a probability distribution over
the readings of the UBG, which can be used to
rank the competing analyses of a given sentence;
see, e.g., Briscoe and Carroll (1993), Eisele (1994),
Brew (1995), Abney (1997), Goodman (1997), Bod
and Kaplan (1998), Johnson et al (1999), Riezler et
al. (2000), Osborne (2000), Bouma et al (2001), or
Schmid (2002).
Unfortunately, most of the proposed probability
models are not mathematically clean in that the
probabilities of all possible UBG readings do not
sum to the value 1, a problem which is discussed
intensively by Eisele (1994), Abney (1997), and
Schmid (2002).
In addition, many of the newer approaches use
log-linear (or exponential) models. Schmid (2002)
outlines a serious problem for these models: log-
linear models prevent the application of dynamic
programming methods for the computation of the
most probable parse, if complex features are incor-
porated. Therefore the run-time complexity of the
disambiguation algorithm is linear in the number of
parses of a sentence. If the number of parses grows
exponentially with the length of the sentence, these
approaches are simply impractical.
Our approach obviates the need for such models
on the UBG side in that it shifts the responsibility
to simpler CF models, indirectly obtained from the
UBG. In more detail, the kernel of our novel disam-
biguation method for UBGs consists of the appli-
cation of a context-free approximation for a given
UBG (Kiefer and Krieger, 2000) and the exploita-
tion of the standard probability model for CFGs.
In contrast to earlier approaches to disambigua-
tion for UBGs, our approach has several advantages.
Firstly, probabilistic modeling/training of context-
free grammars is theoretically well-understood and
can be effectively done in practice, using the inside-
outside algorithm (Lari and Young, 1990). Sec-
ondly, the Viterbi algorithm enables CFG pars-
ing and disambiguation in cubic time, exploiting
dynamic programming techniques to specify the
maximum-probability parse of a given sentence.
Thirdly, probability distributions over the CFG trees
are mathematically clean, if some weak conditions
for this desired behaviour are fulfilled (Booth and
Thompson, 1973).
In the rest of the paper, we present the context-
free approximation, our novel disambiguation ap-
proach, and an experiment, showing that the ap-
proach is feasible.
2 Context-Free Approximation
In this section, we briefly review a simple and intu-
itive approximation method for turning unification-
based grammars, such as HPSG (Pollard and Sag,
UBG
CFG approximation
   	
 
    
. . .    	 
   
a
b c
X
S
. . .
a
b c
X
S
a
b c
X
S
. . .
a
b c
X
S
. . . a
b c
X
S
. . .
a
b c
X
S
Figure 1: The readings of a sentence, analyzed by a UBG (top) and its CFG approximation (bottom). The picture
illustrates that (i) each UBG reading of the sentence is associated with a non-empty set of syntax trees according to
the CFG approximation, and (ii) that the sentence may have CFG trees, which can not be replayed by the UBG, since
the CFG overgenerates (or at best is a correct approximation of the UBG).
1994) or PATR-II (Shieber, 1985) into context-free
grammars (CFG). The method was introduced by
Kiefer and Krieger (2000).
The approximation method can be seen as the
construction of the least fixpoint of a certain mono-
tonic function and shares similarities with the in-
stantiation of rules in a bottom-up passive chart
parser or with partial evaluation in logic program-
ming. The basic idea of the approach is as follows.
In a first step, one generalizes the set of all lexicon
entries. The resulting structures form equivalence
classes, since they abstract from word-specific in-
formation, such as FORM or STEM. The abstraction
is specified by means of a restrictor (Shieber, 1985),
the so-called lexicon restrictor. After that, the gram-
mar rules are instantiated by unification, using the
abstracted lexicon entries and resulting in deriva-
tion trees of depth 1. The rule restrictor is applied
to each resulting feature structure (FS), removing
all information contained only in the daughters of a
rule. Additionally, the restriction gets rid of infor-
mation that will either lead to infinite growth of the
FSs or that does not constrain the search space. The
restricted FSs (together with older ones) then serve
as the basis for the next instantiation step. Again,
this gives FSs encoding a derivation, and again the
rule restrictor is applied. This process is iterated un-
til a fixpoint is reached, meaning that further itera-
tion steps will not add (or remove) new (or old) FSs
to the set of computed FSs.
Given the FSs from the fixpoint, it is then easy
to generate context-free productions, using the com-
plete FSs as symbols of the CFG; see Kiefer and
Krieger (2002). We note here that adding (and per-
haps removing) FSs during the iteration can be
achieved in different ways: either by employing
feature structure equivalence  (structural equiva-
lence) or by using FS subsumption  . It is clear that
the resulting CFGs will behave differently (see fig-
ure 4). An in-depth description of the method, con-
taining lots of details, plus a mathematical under-
pinning is presented in (Kiefer and Krieger, 2000)
and (Kiefer and Krieger, 2002). The application of
the method to a mid-size UBG of English, and large-
size HPSGs of English and Japanese is described in
(Kiefer and Krieger, 2002) and (Kiefer et al, 2000).
3 A Novel Disambiguation for UBGs
(Kiefer and Krieger, 2000) suggest that, given a
UBG, the approximated CFG can be used as a cheap
filter during a two-stage parsing approach. The idea
is to let the CFG explore the search space, whereas
the UBG deterministically replays the derivations,
proposed by the CFG. To be able to carry out the
replay, during the creation of the CF grammar, each
CF production is correlated with the UBG rules it
was produced from.
The above mentioned two-stage parsing approach
not only speeds up parsing (see figure 4), but can
also be a starting point for an efficient stochastic
parsing model, even though the UBG might encode
an infinite number of categories. Given a training
corpus, the idea is to move from the approximated
CFG to a PCFG which predicts probabilities for the
CFG trees. Clearly, the probabilities can be used for
disambiguation, and more important, for ranking of
CFG trees. The idea is, that the ranked parsing trees
can be replayed one after another by the UBG (pro-
cessing the most probable CFG trees first), estab-
lishing an order of best UBG parsing trees. Since the
approximation always yields a CFG that is a super-
set of the UBG, it might be possible that derivation
trees proposed by the PCFG can not be replayed by
the UBG. Nevertheless, this behavior does not al-
ter the ranking of reconstructed UBG parsing trees.
Figure 1 gives an overview, displaying the readings
 












v lex
TAKES TIME PP 6
TAKES LOC PP 5
TAKES ATTRIB PP 4
SUBJ SEM N TYPE 3
SUBCAT 7
OBJ SEM N TYPE 8
AGR 2
VP VFORM 9
SEM P TYPE 10
PP SEM PP TYPE 11
INVn
STEM  measure 
VFORM 12













 










vbar rule type
VP VFORM 9
SEM P TYPE 10
PP SEM PP TYPE 11
AGR 2
OBJ SEM N TYPE 8 env
SUBCAT 7 nx0vnx1
SUBJ SEM N TYPE 3
TAKES ATTRIB PP 4
TAKES LOC PP 5
TAKES TIME PP 6
INV 13
VFORM 12











 






nn lex
TAKES LOC PP 26
AGR 24
TAKES DET TYPEnull
TAKES TIME PP 28
TAKES ATTRIB PP 27
STEM  temperature 
N POST MOD TYPEnone
SEM N TYPE 8







 








np rule type
WH 23
SEM N TYPE 8
TAKES REL 25
TAKES LOC PP 26 y
AGR 24
TAKES ATTRIB PP 27 y
TAKES TIME PP 28 y
CONJn
GAPSOUT 1
GAPSIN 1









 


p lex
STEM  at 
SEM P TYPEnot onoff
OBJ SEM N TYPE 16 loc
POSTPOSITIONn
SEM PP TYPE 15



 


d lex
PRENUMBERy
AGR1pl 	 2pl 	 3pl
WHn
STEM  all 
DET TYPEnormal



 

number lex
NUM TYPEdigit
AGR1pl 	 2pl 	 3pl
TIME TYPEhour
STEM  three 


 

d rule type
DET TYPE 21 numeric
AGR 17
PRENUMBERn
WH 22


 






nn lex
N POST MOD TYPEnone
STEM  decks 
AGR 17
TAKES ATTRIB PP 18
TAKES LOC PP 19
TAKES TIME PP 20
TAKES DET TYPE 21
SEM N TYPE 16







 








np rule type
CONJn
TAKES RELy
TAKES TIME PP 20 n
TAKES LOC PP 19 n
TAKES ATTRIB PP 18 n
AGR 17 3pl
SEM N TYPE 16
GAPSIN 14
GAPSOUT 14
WH 22









 

pp rule type
GAPn
WH 22 n
SEM PP TYPE 15 loc
GAPSOUT 14 null
GAPSIN 14


 







np rule type
CONJyes no
TAKES REL 25 y
TAKES TIME PPn
TAKES LOC PPn
TAKES ATTRIB PPn
AGR 24 3sg
SEM N TYPE 8
WH 23 n
GAPSIN 1
GAPSOUT 1








 








vp rule type
VFORM 12 base
INV 13 n
GAPSOUT 1 null
GAPSIN 1
AGR 2 1pl 	 1sg 	 2pl 	 2sg 	 3pl
SUBJ SEM N TYPE 3 agent
TAKES ATTRIB PP 4
TAKES LOC PP 5 y
TAKES TIME PP 6 n
TAKES GAP PPyes no
 








.
.
.
Figure 2: One of the two readings for the sentence measure temperature at all three decks, analyzed by the Gemini
grammar. Note that the vertical dots at the top indicate an incomplete FS derivation tree. Furthermore, the FSs at the
tree nodes are massively simplified.
of a sentence, analyzed by a UBG and its CFG ap-
proximation. Using this figure, it should be clear
that a ranking of CFG trees induces a ranking of
UBG readings, even if not all CFG trees have an
associated UBG reading. We exemplify our idea in
section 4, where we disambiguate a sentence with a
PP-attachment ambiguity.
As a nice side effect, our proposed stochastic
parsing model should usually not explore the full
search space, nor should it statically estimate the
parsing results afterwards, assuming we are in-
terested in the most probable parse (or say, the
two most probable results)?the disambiguation of
UBG results is simply established by the dynamic
ordering of most probable CFG trees during the first
parsing stage.
measure
89
1058
temperature
72
1028
at
7
all
60
three
55
1018
decks
10
1033
951
929
960
873
687
304
1017
S
measure
89
1058
temperature
72
1028
960
at
7
all
60
three
55
1018
decks
10
1033
951
183
873
687
304
1017
S
Figure 3: Alternative readings licensed by the context-free approximation of the Gemini grammar.
4 Experiments
Approximation. (Dowding et al, 2001) com-
pared (Moore, 1999)?s approach to grammar ap-
proximation to (Kiefer and Krieger, 2000). As a ba-
sis for the comparison, they chose an English gram-
mar written in the Gemini/CLE formalism. The mo-
tivation for this enterprise comes from the use of
the resulting CFG as a context-free language model
for the Nuance speech recognizer. John Dowding
kindly provided the Gemini grammar and a corpus
of 500 sentences, allowing us to measure the quality
of our approximation method for a realistic mid-size
grammar, both under  and  (see section 2).1
The Gemini grammar consisted of 57 unification
rules and a small lexicon of 216 entries which ex-
panded into 425 full forms. Since the grammar al-
lows for atomic disjunctions (and makes heavy use
of them), we ended in overall 1,886 type definitions
in our system. Given the 500 sentences, the Gem-
ini grammar licensed 720 readings. We only deleted
the ARGS feature (the daughters) during the iter-
ation and found that the original UBG encodes a
context-free language, due to the fact that the iter-
ation terminates under  . This means that we have
even obtained a correct approximation of the Gem-
ini grammar. Table 4 presents the relevant numbers,
both under  and  , and shows that the ambiguity
rate for  goes up only mildly.
We note, however, that these numbers differ from
those presented in (Dowding et al, 2001). We could
not find out why their implementation produces
worse results than ours. They suggested that the use
of  is the reason for the bad behaviour of the re-
sulting grammar, but, as our figures show, this is not
1A big thank you is due to Mark-Jan Nederhof who has writ-
ten the Gemini-to-   converter and to John Dowding and Ja-
son Baldridge for fruitful discussions.
Gemini  
# readings 720 720 747
ambiguity rate 1.44 1.44 1.494
#terminals ? 152 109
#nonterminals ? 3,158 998
#rules 57 24,101 5,269
#useful rules 57 19,618 4,842
running time (secs) 32.9 14.6 9.5
run time speed-up (%) 0 55.6 71.1
Figure 4: A comparison of the approximated CFGs de-
rived under  and  . The fixpoint for  (  ) was reached
after 9 (8) iteration steps and took 5 minutes (34 seconds)
to be computed, incl. post-processing time to compute
the CF productions. The run time speed-up for two-stage
parsing is given in the last row. The measurements were
conducted on a 833 MHz Linux workstation.
true, at least not for this grammar. Of course, us-
ing  instead of  can lead to substantially less re-
strictive grammars, but when dealing with complex
grammars, there is?at the moment?no alternative
to using  due to massive space and time require-
ments of the approximation process.
Figure 2 displays one of the two readings for the
sentence measure temperature at all three decks, an-
alyzed by the Gemini grammar. The sentence is one
of the 500 sentences provided by John Dowding.
The vertical dots simply indicate that some less rele-
vant nodes of the FS derivation tree have been omit-
ted. The figure shows the reading, where the PP at
all three decks is attached to the NP temperature.
Due to space constraints, we do not show the second
reading, where the PP is attached to the VP measure
temperature.
Figure 3 shows the two syntax trees for the sen-
tence, analyzed by the context-free approximation
of the Gemini grammar, obtained by using  . It
S   1017 (0.995)
1017   304 (0.472)
304   687 (0.980)
687   873 (1.000)
873   960 (0.542)
873   183 (0.330)
960   1058 929 (0.138)
960   1058 1028 (0.335)
183   960 951 (0.042)
1058   89 (1.000)
89   measure (0.941)
929   1028 951 (0.938)
1028   72 (0.278)
72   temperature (0.635)
951   7 1033 (0.286)
7   at (0.963)
1033   1018 10 (0.706)
1018   60 55 (0.581)
60   all (0.818)
55   three (0.111)
10   decks (1.000)
Figure 5: Fragment of the PCFG. The values in paren-
thesis are probabilities for grammar rules, gathered after
two training iterations with the inside-outside algorithm.
is worth noting that both readings of the CFG ap-
proximation differ in PP attachment, in the same
manner as the readings analyzed by the UBG it-
self. In the figure, all non-terminals are simply dis-
played as numbers, but each number represents a
fairly complex feature structure, which is, in gen-
eral, slightly less informative than an associated tree
node of a possible FS derivation tree of the given
Gemini grammar for two reasons. Firstly, the use
of the  operation as a test generalizes informa-
tion during the approximation process. In a more
complex UBG grammar, the restrictors would have
deleted even more information. Secondly, the flow
of information in a local tree from the mother to the
daughter node will not be reflected because the ap-
proximation process works strictly bottom up from
the lexicon entries.
Training of the CFG approximation. A sample
of sentences serves as input to the inside-outside
algorithm, the standard algorithm for unsupervised
training of PCFGs (Lari and Young, 1990). The
given corpus of 500 sentences was divided into a
training corpus (90%, i.e., 450 sentences) and a test-
ing corpus (10%, i.e., 50 sentences). This standard
procedure enables us (i) to apply the inside-outside
algorithm to the training corpus, and (ii) to eval-
uate the resulting probabilistic context-free gram-
mars on the testing corpus. We linguistically eval-
uated the maximum-probability parses of all sen-
tences in the testing corpus (see section 5). For un-
supervised training and parsing, we used the imple-
mentation of Schmid (1999).
Figure 5 shows a fragment of the probabilistic
context-free approximation. The probabilities of the
grammar rules are extracted after several training it-
erations with the inside-outside algorithm using the
training corpus of 450 sentences.
Disambiguation using maximum-probability
parses. In contrast to most approaches to stochas-
tic modeling of UBGs, PCFGs can be very easily
used to assign probabilities to the readings of a
given sentence: the probability of a syntax tree (the
reading) is the product of the probabilities of all
context-free rules occurring in the tree.
For example, the two readings of the sentence
measure temperature at all three decks, as dis-
played in figure 3, have the following probabilities:

	 (first reading on the left-hand side)
and 		 (second reading on the right-hand
side). The maximum-probability parse is therefore
the syntax-tree on the left-hand side of figure 3,
which is the reading, where the PP at all three decks
is attached to the NP temperature.
A closer look on the PCFG fragment shows that
the main contribution to this result comes from the
two rules 929   1028 951 (0.938) and 183   960 951
(0.042). Here, the probabilities encode the statistical
finding that PP-to-NP attachments can be expected
more frequently than PP-to-VP attachments, if the
context-free approximation of the Gemini grammar
is used to analyze the given corpus of 500 sentences.
5 Evaluation
Evaluation task. To evaluate our models, we used
the testing corpus mentioned in section 4. In a next
step, the correct parse was indicated by a human dis-
ambiguator, according to the intended reading. The
average ambiguity of this corpus is about 1.4 parses
per sentence, for sentences with about 5.8 words on
average.
Our statistical disambiguation method was tested
on an exact match task, where exact correspondence
of the manually annotated correct parse and the
most probable parse is checked. Performance on this
evaluation task was assessed according to the fol-
lowing evaluation measure:
precision An Integrated Architecture for Shallow and Deep Processing
Berthold Crysmann, Anette Frank, Bernd Kiefer, Stefan Mu?ller,
Gu?nter Neumann, Jakub Piskorski, Ulrich Scha?fer, Melanie Siegel, Hans Uszkoreit,
Feiyu Xu, Markus Becker and Hans-Ulrich Krieger
DFKI GmbH
Stuhlsatzenhausweg 3
Saarbru?cken, Germany
whiteboard@dfki.de
Abstract
We present an architecture for the integra-
tion of shallow and deep NLP components
which is aimed at flexible combination
of different language technologies for a
range of practical current and future appli-
cations. In particular, we describe the inte-
gration of a high-level HPSG parsing sys-
tem with different high-performance shal-
low components, ranging from named en-
tity recognition to chunk parsing and shal-
low clause recognition. The NLP com-
ponents enrich a representation of natu-
ral language text with layers of new XML
meta-information using a single shared
data structure, called the text chart. We de-
scribe details of the integration methods,
and show how information extraction and
language checking applications for real-
world German text benefit from a deep
grammatical analysis.
1 Introduction
Over the last ten years or so, the trend in application-
oriented natural language processing (e.g., in the
area of term, information, and answer extraction)
has been to argue that for many purposes, shallow
natural language processing (SNLP) of texts can
provide sufficient information for highly accurate
and useful tasks to be carried out. Since the emer-
gence of shallow techniques and the proof of their
utility, the focus has been to exploit these technolo-
gies to the maximum, often ignoring certain com-
plex issues, e.g. those which are typically well han-
dled by deep NLP systems. Up to now, deep natural
language processing (DNLP) has not played a sig-
nificant role in the area of industrial NLP applica-
tions, since this technology often suffers from insuf-
ficient robustness and throughput, when confronted
with large quantities of unrestricted text.
Current information extractions (IE) systems
therefore do not attempt an exhaustive DNLP analy-
sis of all aspects of a text, but rather try to analyse or
?understand? only those text passages that contain
relevant information, thereby warranting speed and
robustness wrt. unrestricted NL text. What exactly
counts as relevant is explicitly defined by means
of highly detailed domain-specific lexical entries
and/or rules, which perform the required mappings
from NL utterances to corresponding domain knowl-
edge. However, this ?fine-tuning? wrt. a particular
application appears to be the major obstacle when
adapting a given shallow IE system to another do-
main or when dealing with the extraction of com-
plex ?scenario-based? relational structures. In fact,
(Appelt and Israel, 1997) have shown that the cur-
rent IE technology seems to have an upper perfor-
mance level of less than 60% in such cases. It seems
reasonable to assume that if a more accurate analy-
sis of structural linguistic relationships could be pro-
vided (e.g., grammatical functions, referential rela-
tionships), this barrier might be overcome. Actually,
the growing market needs in the wide area of intel-
ligent information management systems seem to re-
quest such a break-through.
In this paper we will argue that the quality of cur-
                Computational Linguistics (ACL), Philadelphia, July 2002, pp. 441-448.
                         Proceedings of the 40th Annual Meeting of the Association for
rent SNLP-based applications can be improved by
integrating DNLP on demand in a focussed manner,
and we will present a system that combines the fine-
grained anaysis provided by HPSG parsing with a
high-performance SNLP system into a generic and
flexible NLP architecture.
1.1 Integration Scenarios
Owing to the fact that deep and shallow technologies
are complementary in nature, integration is a non-
trivial task: while SNLP shows its strength in the
areas of efficiency and robustness, these aspects are
problematic for DNLP systems. On the other hand,
DNLP can deliver highly precise and fine-grained
linguistic analyses. The challenge for integration is
to combine these two paradigms according to their
virtues.
Probably the most straightforward way to inte-
grate the two is an architecture in which shallow and
deep components run in parallel, using the results of
DNLP, whenever available. While this kind of ap-
proach is certainly feasible for a real-time applica-
tion such as Verbmobil, it is not ideal for processing
large quantities of text: due to the difference in pro-
cessing speed, shallow and deep NLP soon run out
of sync. To compensate, one can imagine two possi-
ble remedies: either to optimize for precision, or for
speed. The drawback of the former strategy is that
the overall speed will equal the speed of the slow-
est component, whereas in case of the latter, DNLP
will almost always time out, such that overall preci-
sion will hardly be distinguishable from a shallow-
only system. What is thus called for is an integrated,
flexible architecture where components can play at
their strengths. Partial analyses from SNLP can be
used to identify relevant candidates for the focussed
use of DNLP, based on task or domain-specific crite-
ria. Furthermore, such an integrated approach opens
up the possibility to address the issue of robustness
by using shallow analyses (e.g., term recognition)
to increase the coverage of the deep parser, thereby
avoiding a duplication of efforts. Likewise, integra-
tion at the phrasal level can be used to guide the
deep parser towards the most likely syntactic anal-
ysis, leading, as it is hoped, to a considerable speed-
up.
shallow
NLP
components
NLP
deep
components internal repr.
layer
multi
chart
annot.
XML
external repr.
generic OOP
component
interface
WHAM
application
specification
input and
result
Figure 1: The WHITEBOARD architecture.
2 Architecture
The WHITEBOARD architecture defines a platform
that integrates the different NLP components by en-
riching an input document through XML annota-
tions. XML is used as a uniform way of represent-
ing and keeping all results of the various processing
components and to support a transparent software
infrastructure for LT-based applications. It is known
that interesting linguistic information ?especially
when considering DNLP? cannot efficiently be
represented within the basic XML markup frame-
work (?typed parentheses structure?), e.g., linguistic
phenomena like coreferences, ambiguous readings,
and discontinuous constituents. The WHITEBOARD
architecture employs a distributed multi-level repre-
sentation of different annotations. Instead of trans-
lating all complex structures into one XML docu-
ment, they are stored in different annotation layers
(possibly non-XML, e.g. feature structures). Hyper-
links and ?span? information together support effi-
cient access between layers. Linguistic information
of common interest (e.g. constituent structure ex-
tracted from HPSG feature structures) is available in
XML format with hyperlinks to full feature struc-
ture representations externally stored in correspond-
ing data files.
Fig. 1 gives an overview of the architecture of
the WHITEBOARD Annotation Machine (WHAM).
Applications feed the WHAM with input texts and
a specification describing the components and con-
figuration options requested. The core WHAM en-
gine has an XML markup storage (external ?offline?
representation), and an internal ?online? multi-level
annotation chart (index-sequential access). Follow-
ing the trichotomy of NLP data representation mod-
els in (Cunningham et al, 1997), the XML markup
contains additive information, while the multi-level
chart contains positional and abstraction-based in-
formation, e.g., feature structures representing NLP
entities in a uniform, linguistically motivated form.
Applications and the integrated components ac-
cess the WHAM results through an object-oriented
programming (OOP) interface which is designed
as general as possible in order to abstract from
component-specific details (but preserving shallow
and deep paradigms). The interfaces of the actu-
ally integrated components form subclasses of the
generic interface. New components can be inte-
grated by implementing this interface and specifying
DTDs and/or transformation rules for the chart.
The OOP interface consists of iterators that walk
through the different annotation levels (e.g., token
spans, sentences), reference and seek operators that
allow to switch to corresponding annotations on a
different level (e.g., give all tokens of the current
sentence, or move to next named entity starting
from a given token position), and accessor meth-
ods that return the linguistic information contained
in the chart. Similarily, general methods support
navigating the type system and feature structures of
the DNLP components. The resulting output of the
WHAM can be accessed via the OOP interface or as
XML markup.
The WHAM interface operations are not only
used to implement NLP component-based applica-
tions, but also for the integration of deep and shallow
processing components itself.
2.1 Components
2.1.1 Shallow NL component
Shallow analysis is performed by SPPC, a rule-
based system which consists of a cascade of
weighted finite?state components responsible for
performing subsequent steps of the linguistic anal-
ysis, including: fine-grained tokenization, lexico-
morphological analysis, part-of-speech filtering,
named entity (NE) recognition, sentence bound-
ary detection, chunk and subclause recognition,
see (Piskorski and Neumann, 2000; Neumann and
Piskorski, 2002) for details. SPPC is capable of pro-
cessing vast amounts of textual data robustly and ef-
ficiently (ca. 30,000 words per second in standard
PC environment). We will briefly describe the SPPC
components which are currently integrated with the
deep components.
Each token identified by a tokenizer as a poten-
tial word form is morphologically analyzed. For
each token, its lexical information (list of valid read-
ings including stem, part-of-speech and inflection
information) is computed using a fullform lexicon
of about 700,000 entries that has been compiled out
from a stem lexicon of about 120,000 lemmas. Af-
ter morphological processing, POS disambiguation
rules are applied which compute a preferred read-
ing for each token, while the deep components can
back off to all readings. NE recognition is based on
simple pattern matching techniques. Proper names
(organizations, persons, locations), temporal expres-
sions and quantities can be recognized with an av-
erage precision of almost 96% and recall of 85%.
Furthermore, a NE?specific reference resolution is
performed through the use of a dynamic lexicon
which stores abbreviated variants of previously rec-
ognized named entities. Finally, the system splits
the text into sentences by applying only few, but
highly accurate contextual rules for filtering implau-
sible punctuation signs. These rules benefit directly
from NE recognition which already performs re-
stricted punctuation disambiguation.
2.1.2 Deep NL component
The HPSG Grammar is based on a large?scale
grammar for German (Mu?ller, 1999), which was
further developed in the VERBMOBIL project for
translation of spoken language (Mu?ller and Kasper,
2000). After VERBMOBIL the grammar was adapted
to the requirements of the LKB/PET system (Copes-
take, 1999), and to written text, i.e., extended with
constructions like free relative clauses that were ir-
relevant in the VERBMOBIL scenario.
The grammar consists of a rich hierarchy of
5,069 lexical and phrasal types. The core grammar
contains 23 rule schemata, 7 special verb move-
ment rules, and 17 domain specific rules. All rule
schemata are unary or binary branching. The lexicon
contains 38,549 stem entries, from which more than
70% were semi-automatically acquired from the an-
notated NEGRA corpus (Brants et al, 1999).
The grammar parses full sentences, but also other
kinds of maximal projections. In cases where no full
analysis of the input can be provided, analyses of
fragments are handed over to subsequent modules.
Such fragments consist of maximal projections or
single words.
The HPSG analysis system currently integrated
in the WHITEBOARD system is PET (Callmeier,
2000). Initially, PET was built to experiment
with different techniques and strategies to process
unification-based grammars. The resulting sys-
tem provides efficient implementations of the best
known techniques for unification and parsing.
As an experimental system, the original design
lacked open interfaces for flexible integration with
external components. For instance, in the beginning
of the WHITEBOARD project the system only ac-
cepted fullform lexica and string input. In collabora-
tion with Ulrich Callmeier the system was extended.
Instead of single word input, input items can now
be complex, overlapping and ambiguous, i.e. essen-
tially word graphs. We added dynamic creation of
atomic type symbols, e.g., to be able to add arbitrary
symbols to feature structures. With these enhance-
ments, it is possible to build flexible interfaces to
external components like morphology, tokenization,
named entity recognition, etc.
3 Integration
Morphology and POS The coupling between the
morphology delivered by SPPC and the input needed
for the German HPSG was easily established. The
morphological classes of German are mapped onto
HPSG types which expand to small feature struc-
tures representing the morphological information in
a compact way. A mapping to the output of SPPC
was automatically created by identifying the corre-
sponding output classes.
Currently, POS tagging is used in two ways. First,
lexicon entries that are marked as preferred by the
shallow component are assigned higher priority than
the rest. Thus, the probability of finding the cor-
rect reading early should increase without excluding
any reading. Second, if for an input item no entry is
found in the HPSG lexicon, we automatically create
a default entry, based on the part?of?speech of the
preferred reading. This increases robustness, while
avoiding increase in ambiguity.
Named Entity Recognition Writing HPSG gram-
mars for the whole range of NE expressions etc. is
a tedious and not very promising task. They typi-
cally vary across text sorts and domains, and would
require modularized subgrammars that can be easily
exchanged without interfering with the general core.
This can only be realized by using a type interface
where a class of named entities is encoded by a gen-
eral HPSG type which expands to a feature structure
used in parsing. We exploit such a type interface for
coupling shallow and deep processing. The classes
of named entities delivered by shallow processing
are mapped to HPSG types. However, some fine-
tuning is required whenever deep and shallow pro-
cessing differ in the amount of input material they
assign to a named entity.
An alternative strategy is used for complex syn-
tactic phrases containing NEs, e.g., PPs describ-
ing time spans etc. It is based on ideas from
Explanation?based Learning (EBL, see (Tadepalli
and Natarajan, 1996)) for natural language analy-
sis, where analysis trees are retrieved on the basis
of the surface string. In our case, the part-of-speech
sequence of NEs recognised by shallow analysis is
used to retrieve pre-built feature structures. These
structures are produced by extracting NEs from a
corpus and processing them directly by the deep
component. If a correct analysis is delivered, the
lexical parts of the analysis, which are specific for
the input item, are deleted. We obtain a sceletal
analysis which is underspecified with respect to the
concrete input items. The part-of-speech sequence
of the original input forms the access key for this
structure. In the application phase, the underspeci-
fied feature structure is retrieved and the empty slots
for the input items are filled on the basis of the con-
crete input.
The advantage of this approach lies in the more
elaborate semantics of the resulting feature struc-
tures for DNLP, while avoiding the necessity of
adding each and every single name to the HPSG lex-
icon. Instead, good coverage and high precision can
be achieved using prototypical entries.
Lexical Semantics When first applying the origi-
nal VERBMOBIL HPSG grammar to business news
articles, the result was that 78.49% of the miss-
ing lexical items were nouns (ignoring NEs). In
the integrated system, unknown nouns and NEs can
be recognized by SPPC, which determines morpho-
syntactic information. It is essential for the deep sys-
tem to associate nouns with their semantic sorts both
for semantics construction, and for providing se-
mantically based selectional restrictions to help con-
straining the search space during deep parsing. Ger-
maNet (Hamp and Feldweg, 1997) is a large lexical
database, where words are associated with POS in-
formation and semantic sorts, which are organized in
a fine-grained hierarchy. The HPSG lexicon, on the
other hand, is comparatively small and has a more
coarse-grained semantic classification.
To provide the missing sort information when re-
covering unknown noun entries via SPPC, a map-
ping from the GermaNet semantic classification to
the HPSG semantic classification (Siegel et al,
2001) is applied which has been automatically ac-
quired. The training material for this learning pro-
cess are those words that are both annotated with se-
mantic sorts in the HPSG lexicon and with synsets
of GermaNet. The learning algorithm computes a
mapping relevance measure for associating seman-
tic concepts in GermaNet with semantic sorts in the
HPSG lexicon. For evaluation, we examined a cor-
pus of 4664 nouns extracted from business news
that were not contained in the HPSG lexicon. 2312
of these were known in GermaNet, where they are
assigned 2811 senses. With the learned mapping,
the GermaNet senses were automatically mapped to
HPSG semantic sorts. The evaluation of the map-
ping accuracy yields promising results: In 76.52%
of the cases the computed sort with the highest rel-
evance probability was correct. In the remaining
20.70% of the cases, the correct sort was among the
first three sorts.
3.1 Integration on Phrasal Level
In the previous paragraphs we described strategies
for integration of shallow and deep processing where
the focus is on improving DNLP in the domain of
lexical and sub-phrasal coverage.
We can conceive of more advanced strategies for
the integration of shallow and deep analysis at the
length cover- complete LP LR 0CB   2CB
age match
  40 100 80.4 93.4 92.9 92.1 98.9
 40 99.8 78.6 92.4 92.2 90.7 98.5
Training: 16,000 NEGRA sentences
Testing: 1,058 NEGRA sentences
Figure 2: Stochastic topological parsing: results
level of phrasal syntax by guiding the deep syntac-
tic parser towards a partial pre-partitioning of com-
plex sentences provided by shallow analysis sys-
tems. This strategy can reduce the search space, and
enhance parsing efficiency of DNLP.
Stochastic Topological Parsing The traditional
syntactic model of topological fields divides basic
clauses into distinct fields: so-called pre-, middle-
and post-fields, delimited by verbal or senten-
tial markers. This topological model of German
clause structure is underspecified or partial as to
non-sentential constituent boundaries, but provides
a linguistically well-motivated, and theory-neutral
macrostructure for complex sentences. Due to its
linguistic underpinning the topological model pro-
vides a pre-partitioning of complex sentences that is
(i) highly compatible with deep syntactic structures
and (ii) maximally effective to increase parsing ef-
ficiency. At the same time (iii) partiality regarding
the constituency of non-sentential material ensures
the important aspects of robustness, coverage, and
processing efficiency.
In (Becker and Frank, 2002) we present a corpus-
driven stochastic topological parser for German,
based on a topological restructuring of the NEGRA
corpus (Brants et al, 1999). For topological tree-
bank conversion we build on methods and results
in (Frank, 2001). The stochastic topological parser
follows the probabilistic model of non-lexicalised
PCFGs (Charniak, 1996). Due to abstraction from
constituency decisions at the sub-sentential level,
and the essentially POS-driven nature of topologi-
cal structure, this rather simple probabilistic model
yields surprisingly high figures of accuracy and cov-
erage (see Fig.2 and (Becker and Frank, 2002) for
more detail), while context-free parsing guarantees
efficient processing.
The next step is to elaborate a (partial) map-
ping of shallow topological and deep syntactic struc-
tures that is maximally effective for preference-gui-
Topological Structure:
CL-V2
VF-TOPIC LK-FIN MF RK-t
NN VVFIN ADV NN PREP NN VVFIN
[ 	 [ 
	 Peter] [ 
 i?t] [ 
 gerne Wu?rstchen mit Kartoffelsalat] [ Integrated Shallow and Deep Parsing: TopP meets HPSG
Anette Frank, Markus Beckerz, Berthold Crysmann, Bernd Kiefer and Ulrich Scha?fer
DFKI GmbH School of Informaticsz
66123 Saarbru?cken, Germany University of Edinburgh, UK
firstname.lastname@dfki.de M.Becker@ed.ac.uk
Abstract
We present a novel, data-driven method
for integrated shallow and deep parsing.
Mediated by an XML-based multi-layer
annotation architecture, we interleave a
robust, but accurate stochastic topological
field parser of German with a constraint-
based HPSG parser. Our annotation-based
method for dovetailing shallow and deep
phrasal constraints is highly flexible, al-
lowing targeted and fine-grained guidance
of constraint-based parsing. We conduct
systematic experiments that demonstrate
substantial performance gains.1
1 Introduction
One of the strong points of deep processing (DNLP)
technology such as HPSG or LFG parsers certainly
lies with the high degree of precision as well as
detailed linguistic analysis these systems are able
to deliver. Although considerable progress has been
made in the area of processing speed, DNLP systems
still cannot rival shallow and medium depth tech-
nologies in terms of throughput and robustness. As
a net effect, the impact of deep parsing technology
on application-oriented NLP is still fairly limited.
With the advent of XML-based hybrid shallow-
deep architectures as presented in (Grover and Las-
carides, 2001; Crysmann et al, 2002; Uszkoreit,
2002) it has become possible to integrate the added
value of deep processing with the performance and
robustness of shallow processing. So far, integration
has largely focused on the lexical level, to improve
upon the most urgent needs in increasing the robust-
ness and coverage of deep parsing systems, namely
1This work was in part supported by a BMBF grant to the
DFKI project WHITEBOARD (FKZ 01 IW 002).
lexical coverage. While integration in (Grover and
Lascarides, 2001) was still restricted to morphologi-
cal and PoS information, (Crysmann et al, 2002) ex-
tended shallow-deep integration at the lexical level
to lexico-semantic information, and named entity
expressions, including multiword expressions.
(Crysmann et al, 2002) assume a vertical,
?pipeline? scenario where shallow NLP tools provide
XML annotations that are used by the DNLP system
as a preprocessing and lexical interface. The per-
spective opened up by a multi-layered, data-centric
architecture is, however, much broader, in that it en-
courages horizontal cross-fertilisation effects among
complementary and/or competing components.
One of the culprits for the relative inefficiency of
DNLP parsers is the high degree of ambiguity found
in large-scale grammars, which can often only be re-
solved within a larger syntactic domain. Within a hy-
brid shallow-deep platform one can take advantage
of partial knowledge provided by shallow parsers to
pre-structure the search space of the deep parser. In
this paper, we will thus complement the efforts made
on the lexical side by integration at the phrasal level.
We will show that this may lead to considerable per-
formance increase for the DNLP component. More
specifically, we combine a probabilistic topological
field parser for German (Becker and Frank, 2002)
with the HPSG parser of (Callmeier, 2000). The
HPSG grammar used is the one originally developed
by (Mu?ller and Kasper, 2000), with significant per-
formance enhancements by B. Crysmann.
In Section 2 we discuss the mapping problem
involved with syntactic integration of shallow and
deep analyses and motivate our choice to combine
the HPSG system with a topological parser. Sec-
tion 3 outlines our basic approach towards syntactic
shallow-deep integration. Section 4 introduces vari-
ous confidence measures, to be used for fine-tuning
of phrasal integration. Sections 5 and 6 report on
experiments and results of integrated shallow-deep
parsing, measuring the effect of various integra-
tion parameters on performance gains for the DNLP
component. Section 7 concludes and discusses pos-
sible extensions, to address robustness issues.
2 Integrated Shallow and Deep Processing
The prime motivation for integrated shallow-deep
processing is to combine the robustness and effi-
ciency of shallow processing with the accuracy and
fine-grainedness of deep processing. Shallow analy-
ses could be used to pre-structure the search space of
a deep parser, enhancing its efficiency. Even if deep
analysis fails, shallow analysis could act as a guide
to select partial analyses from the deep parser?s chart
? enhancing the robustness of deep analysis, and the
informativeness of the combined system.
In this paper, we concentrate on the usage of shal-
low information to increase the efficiency, and po-
tentially the quality, of HPSG parsing. In particu-
lar, we want to use analyses delivered by an effi-
cient shallow parser to pre-structure the search space
of HPSG parsing, thereby enhancing its efficiency,
and guiding deep parsing towards a best-first analy-
sis suggested by shallow analysis constraints.
The search space of an HPSG chart parser can
be effectively constrained by external knowledge
sources if these deliver compatible partial subtrees,
which would then only need to be checked for com-
patibility with constituents derived in deep pars-
ing. Raw constituent span information can be used
to guide the parsing process by penalizing con-
stituents which are incompatible with the precom-
puted ?shape?. Additional information about pro-
posed constituents, such as categorial or featural
constraints, provide further criteria for prioritis-
ing compatible, and penalising incompatible con-
stituents in the deep parser?s chart.
An obvious challenge for our approach is thus to
identify suitable shallow knowledge sources that can
deliver compatible constraints for HPSG parsing.
2.1 The Shallow-Deep Mapping Problem
However, chunks delivered by state-of-the-art shal-
low parsers are not isomorphic to deep syntactic
analyses that explicitly encode phrasal embedding
structures. As a consequence, the boundaries of
deep grammar constituents in (1.a) cannot be pre-
determined on the basis of a shallow chunk analy-
sis (1.b). Moreover, the prevailing greedy bottom-up
processing strategies applied in chunk parsing do not
take into account the macro-structure of sentences.
They are thus easily trapped in cases such as (2).
(1) a. [
CL
There was [
NP
a rumor [
CL
it was going
to be bought by [
NP
a French company [
CL
that
competes in supercomputers]]]]].
b. [
CL
There was [
NP
a rumor]] [
CL
it was going
to be bought by [
NP
a French company]] [
CL
that competes in supercomputers].
(2) Fred eats [
NP
pizza and Mary] drinks wine.
In sum, state-of-the-art chunk parsing does nei-
ther provide sufficient detail, nor the required accu-
racy to act as a ?guide? for deep syntactic analysis.
2.2 Stochastic Topological Parsing
Recently, there is revived interest in shallow anal-
yses that determine the clausal macro-structure of
sentences. The topological field model of (German)
syntax (Ho?hle, 1983) divides basic clauses into dis-
tinct fields ? pre-, middle-, and post-fields ? delim-
ited by verbal or sentential markers, which consti-
tute the left/right sentence brackets. This model of
clause structure is underspecified, or partial as to
non-sentential constituent structure, but provides a
theory-neutral model of sentence macro-structure.
Due to its linguistic underpinning, the topologi-
cal field model provides a pre-partitioning of com-
plex sentences that is (i) highly compatible with
deep syntactic analysis, and thus (ii) maximally ef-
fective to increase parsing efficiency if interleaved
with deep syntactic analysis; (iii) partiality regarding
the constituency of non-sentential material ensures
robustness, coverage, and processing efficiency.
(Becker and Frank, 2002) explored a corpus-
based stochastic approach to topological field pars-
ing, by training a non-lexicalised PCFG on a topo-
logical corpus derived from the NEGRA treebank of
German. Measured on the basis of hand-corrected
PoS-tagged input as provided by the NEGRA tree-
bank, the parser achieves 100% coverage for length
 40 (99.8% for all). Labelled precision and recall
are around 93%. Perfect match (full tree identity) is
about 80% (cf. Table 1, disamb +).
In this paper, the topological parser was provided
a tagger front-end for free text processing, using the
TnT tagger (Brants, 2000). The grammar was ported
to the efficient LoPar parser of (Schmid, 2000). Tag-
ging inaccuracies lead to a drop of 5.1/4.7 percent-
CL-V2
VF-TOPIC LK-VFIN MF RK-VPART NF
ART NN VAFIN ART ADJA NN VAPP CL-SUBCL
Der,1 Zehnkampf,2 ha?tte,3 eine,4 andere,5 Dimension,6 gehabt,7 ,
The decathlon would have a other dimension had LK-COMPL MF RK-VFIN
KOUS PPER PROAV VAPP VAFIN
wenn,9 er,10 dabei,11 gewesen,12 wa?re,13 .
if he there been had .
<TOPO2HPSG type=?root? id=?5608?>
<MAP CONSTR id=?T1? constr=?v2 cp? conf
ent
=?0.87? left=?W1? right=?W13?/>
<MAP CONSTR id=?T2? constr=?v2 vf? conf
ent
=?0.87? left=?W1? right=?W2?/>
<MAP CONSTR id=?T3? constr=?vfronted vfin+rk? conf
ent
=?0.87? left=?W3? right=?W3?/>
<MAP CONSTR id=?T6? constr=?vfronted rk-complex? conf
ent
=?0.87? left=?W7? right=?W7?/>
<MAP CONSTR id=?T4? constr=?vfronted vfin+vp+rk? conf
ent
=?0.87? left=?W3? right=?W13?/>
<MAP CONSTR id=?T5? constr=?vfronted vp+rk? conf
ent
=?0.87? left=?W4? right=?W13?/>
<MAP CONSTR id=?T10? constr=?extrapos rk+nf? conf
ent
=?0.87? left=?W7? right=?W13?/>
<MAP CONSTR id=?T7? constr=?vl cpfin compl? conf
ent
=?0.87? left=?W9? right=?W13?/>
<MAP CONSTR id=?T8? constr=?vl compl vp? conf
ent
=?0.87? left=?W10? right=?W13?/>
<MAP CONSTR id=?T9? constr=?vl rk fin+complex+finlast? conf
ent
=?0.87? left=?W12? right=?W13?/>
</TOPO2HPSG>
Der
D
Zehnkampf
N?
NP-NOM-SG
haette
V
eine
D
andere
AP-ATT
Dimension
N?
N?
NP-ACC-SG
gehabt
V
EPS
wenn
C
er
NP-NOM-SG
dabei
PP
gewesen
V
waere
V-LE
V
V
S
CP-MOD
EPS
EPS
EPS/NP-NOM-SG
S/NP-NOM-SG
S
Figure 1: Topological tree w/param. cat., TOPO2HPSG map-constraints, tree skeleton of HPSG analysis
dis- cove- perfect LP LR 0CB 2CB
amb rage match in % in % in % in %
+ 100.0 80.4 93.4 92.9 92.1 98.9
  99.8 72.1 88.3 88.2 87.8 97.9
Table 1: Disamb: correct (+) / tagger ( ) PoS input.
Eval. on atomic (vs. parameterised) category labels.
age points in LP/LR, and 8.3 percentage points in
perfect match rate (Table 1, disamb  ).
As seen in Figure 1, the topological trees abstract
away from non-sentential constituency ? phrasal
fields MF (middle-field) and VF (pre-field) directly
expand to PoS tags. By contrast, they perfectly ren-
der the clausal skeleton and embedding structure of
complex sentences. In addition, parameterised cate-
gory labels encode larger syntactic contexts, or ?con-
structions?, such as clause type (CL-V2, -SUBCL,
-REL), or inflectional patterns of verbal clusters (RK-
VFIN,-VPART). These properties, along with their
high accuracy rate, make them perfect candidates for
tight integration with deep syntactic analysis.
Moreover, due to the combination of scrambling
and discontinuous verb clusters in German syntax, a
deep parser is confronted with a high degree of local
ambiguity that can only be resolved at the clausal
level. Highly lexicalised frameworks such as HPSG,
however, do not lend themselves naturally to a top-
down parsing strategy. Using topological analyses to
guide the HPSG will thus provide external top-down
information for bottom-up parsing.
3 TopP meets HPSG
Our work aims at integration of topological and
HPSG parsing in a data-centric architecture, where
each component acts independently2 ? in contrast
to the combination of different syntactic formalisms
within a unified parsing process.3 Data-based inte-
gration not only favours modularity, but facilitates
flexible and targeted dovetailing of structures.
3.1 Mapping Topological to HPSG Structures
While structurally similar, topological trees are not
fully isomorphic to HPSG structures. In Figure 1,
e.g., the span from the verb ?ha?tte? to the end of the
sentence forms a constituent in the HPSG analysis,
while in the topological tree the same span is domi-
nated by a sequence of categories: LK, MF, RK, NF.
Yet, due to its linguistic underpinning, the topo-
logical tree can be used to systematically predict
key constituents in the corresponding ?target? HPSG
2See Section 6 for comparison to recent work on integrated
chunk-based and dependency parsing in (Daum et al, 2003).
3As, for example, in (Duchier and Debusmann, 2001).
analysis. We know, for example, that the span from
the fronted verb (LK-VFIN) till the end of its clause
CL-V2 corresponds to an HPSG phrase. Also, the
first position that follows this verb, here the leftmost
daughter of MF, demarcates the left edge of the tra-
ditional VP. Spans of the vorfeld VF and clause cat-
egories CL exactly match HPSG constituents. Cate-
gory CL-V2 tells us that we need to reckon with a
fronted verb in position of its LK daughter, here 3,
while in CL-SUBCL we expect a complementiser in
the position of LK, and a finite verb within the right
verbal complex RK, which spans positions 12 to 13.
In order to communicate such structural con-
straints to the deep parser, we scan the topological
tree for relevant configurations, and extract the span
information for the target HPSG constituents. The
resulting ?map constraints? (Fig. 1) encode a bracket
type name4 that identifies the target constituent and
its left and right boundary, i.e. the concrete span in
the sentence under consideration. The span is en-
coded by the word position index in the input, which
is identical for the two parsing processes.5
In addition to pure constituency constraints, a
skilled grammar writer will be able to associate spe-
cific HPSG grammar constraints ? positive or neg-
ative ? with these bracket types. These additional
constraints will be globally defined, to permit fine-
grained guidance of the parsing process. This and
further information (cf. Section 4) is communicated
to the deep parser by way of an XML interface.
3.2 Annotation-based Integration
In the annotation-based architecture of (Crysmann
et al, 2002), XML-encoded analysis results of all
components are stored in a multi-layer XML chart.
The architecture employed in this paper improves
on (Crysmann et al, 2002) by providing a central
Whiteboard Annotation Transformer (WHAT) that
supports flexible and powerful access to and trans-
formation of XML annotation based on standard
XSLT engines6 (see (Scha?fer, 2003) for more de-
tails on WHAT). Shallow-deep integration is thus
fully annotation driven. Complex XSLT transforma-
tions are applied to the various analyses, in order to
4We currently extract 34 different bracket types.
5We currently assume identical tokenisation, but could ac-
commodate for distinct tokenisation regimes, using map tables.
6Advantages we see in the XSLT approach are (i) minimised
programming effort in the target implementation language for
XML access, (ii) reuse of transformation rules in multiple mod-
ules, (iii) fast integration of new XML-producing components.
extract or combine independent knowledge sources,
including XPath access to information stored in
shallow annotation, complex XSLT transformations
to the output of the topological parser, and extraction
of bracket constraints.
3.3 Shaping the Deep Parser?s Search Space
The HPSG parser is an active bidirectional chart
parser which allows flexible parsing strategies by us-
ing an agenda for the parsing tasks.7 To compute pri-
orities for the tasks, several information sources can
be consulted, e.g. the estimated quality of the parti-
cipating edges or external resources like PoS tagger
results. Object-oriented implementation of the prior-
ity computation facilitates exchange and, moreover,
combination of different ranking strategies. Extend-
ing our current regime that uses PoS tagging for pri-
oritisation,8 we are now utilising phrasal constraints
(brackets) from topological analysis to enhance the
hand-crafted parsing heuristic employed so far.
Conditions for changing default priorities Ev-
ery bracket pair br
x
computed from the topological
analysis comes with a bracket type x that defines its
behaviour in the priority computation. Each bracket
type can be associated with a set of positive and neg-
ative constraints that state a set of permissible or for-
bidden rules and/or feature structure configurations
for the HPSG analysis.
The bracket types fall into three main categories:
left-, right-, and fully matching brackets. A right-
matching bracket may affect the priority of tasks
whose resulting edge will end at the right bracket
of a pair, like, for example, a task that would
combine edges C and F or C and D in Fig. 2.
Left-matching brackets work analogously. For fully
matching brackets, only tasks that produce an edge
that matches the span of the bracket pair can be af-
fected, like, e.g., a task that combines edges B and C
in Fig. 2. If, in addition, specified rule as well as fea-
ture structure constraints hold, the task is rewarded
if they are positive constraints, and penalised if they
are negative ones. All tasks that produce crossing
edges, i.e. where one endpoint lies strictly inside the
bracket pair and the other lies strictly outside, are
penalised, e.g., a task that combines edges A and B.
This behaviour can be implemented efficiently
when we assume that the computation of a task pri-
7A parsing task encodes the possible combination of a pas-
sive and an active chart edge.
8See e.g. (Prins and van Noord, 2001) for related work.
brxbrx
A
B C
D E
F
Figure 2: An example chart with a bracket pair of
type x. The dashed edges are active.
ority takes into account the priorities of the tasks it
builds upon. This guarantees that the effect of chang-
ing one task in the parsing process will propagate
to all depending tasks without having to check the
bracket conditions repeatedly.
For each task, it is sufficient to examine the start-
and endpoints of the building edges to determine if
its priority is affected by some bracket. Only four
cases can occur:
1. The new edge spans a pair of brackets: a match
2. The new edge starts or ends at one of the brack-
ets, but does not match: left or right hit
3. One bracket of a pair is at the joint of the build-
ing edges and a start- or endpoint lies strictly
inside the brackets: a crossing (edges A and B
in Fig. 2)
4. No bracket at the endpoints of both edges: use
the default priority
For left-/right-matching brackets, a match behaves
exactly like the corresponding left or right hit.
Computing the new priority If the priority of a
task is changed, the change is computed relative to
the default priority. We use two alternative confi-
dence values, and a hand-coded parameter (x), to
adjust the impact on the default priority heuristics.
conf
ent
(br
x
) specifies the confidence for a concrete
bracket pair br
x
of type x in a given sentence, based
on the tree entropy of the topological parse. conf
pr
specifies a measure of ?expected accuracy? for each
bracket type. Sec. 4 will introduce these measures.
The priority p(t) of a task t involving a bracket
br
x
is computed from the default priority ~p(t) by:
p(t) = ~p(t)  (1 conf
ent
(br
x
)  conf
pr
(x) (x))
4 Confidence Measures
This way of calculating priorities allows flexible pa-
rameterisation for the integration of bracket con-
straints. While the topological parser?s accuracy is
high, we need to reckon with (partially) wrong anal-
yses that could counter the expected performance
gains. An important factor is therefore the confi-
dence we can have, for any new sentence, into the
best parse delivered by the topological parser: If
confidence is high, we want it to be fully considered
for prioritisation ? if it is low, we want to lower its
impact, or completely ignore the proposed brackets.
We will experiment with two alternative confi-
dence measures: (i) expected accuracy of particular
bracket types extracted from the best parse deliv-
ered, and (ii) tree entropy based on the probability
distribution encountered in a topological parse, as
a measure of the overall accuracy of the best parse
proposed ? and thus the extracted brackets.9
4.1 Conf
pr
: Accuracy of map-constraints
To determine a measure of ?expected accuracy? for
the map constraints, we computed precision and re-
call for the 34 bracket types by comparing the ex-
tracted brackets from the suite of best delivered
topological parses against the brackets we extracted
from the trees in the manually annotated evalua-
tion corpus in (Becker and Frank, 2002). We obtain
88.3% precision, 87.8% recall for brackets extracted
from the best topological parse, run with TnT front
end. We chose precision of extracted bracket types
as a static confidence weight for prioritisation.
Precision figures are distributed as follows: 26.5%
of the bracket types have precision  90% (93.1%
in avg, 53.5% of bracket mass), 50% have pre-
cision  80% (88.9% avg, 77.7% bracket mass).
20.6% have precision  50% (41.26% in avg, 2.7%
bracket mass). For experiments using a threshold
on conf
pr
(x) for bracket type x, we set a threshold
value of 0.7, which excludes 32.35% of the low-
confidence bracket types (and 22.1% bracket mass),
and includes chunk-based brackets (see Section 5).
4.2 Conf
ent
: Entropy of Parse Distribution
While precision over bracket types is a static mea-
sure that is independent from the structural complex-
ity of a particular sentence, tree entropy is defined as
the entropy over the probability distribution of the
set of parsed trees for a given sentence. It is a use-
ful measure to assess how certain the parser is about
the best analysis, e.g. to measure the training utility
value of a data point in the context of sample selec-
tion (Hwa, 2000). We thus employ tree entropy as a
9Further measures are conceivable: We could extract brack-
ets from some n-best topological parses, associating them with
weights, using methods similar to (Carroll and Briscoe, 2002).
10
20
30
40
50
60
70
80
90
00.20.40.60.81
in
 %
Normalized entropy
precision
recall
coverage
Figure 3: Effect of different thresholds of normal-
ized entropy on precision, recall, and coverage
confidence measure for the quality of the best topo-
logical parse, and the extracted bracket constraints.
We carry out an experiment to assess the effect
of varying entropy thresholds  on precision and re-
call of topological parsing, in terms of perfect match
rate, and show a way to determine an optimal value
for . We compute tree entropy over the full prob-
ability distribution, and normalise the values to be
distributed in a range between 0 and 1. The normali-
sation factor is empirically determined as the highest
entropy over all sentences of the training set.10
Experimental setup We randomly split the man-
ually corrected evaluation corpus of (Becker and
Frank, 2002) (for sentence length  40) into a train-
ing set of 600 sentences and a test set of 408 sen-
tences. This yields the following values for the train-
ing set (test set in brackets): initial perfect match
rate is 73.5% (70.0%), LP 88.8% (87.6%), and LR
88.5% (87.8%).11 Coverage is 99.8% for both.
Evaluation measures For the task of identifying
the perfect matches from a set of parses we give the
following standard definitions: precision is the pro-
portion of selected parses that have a perfect match
? thus being the perfect match rate, and recall is the
proportion of perfect matches that the system se-
lected. Coverage is usually defined as the proportion
of attempted analyses with at least one parse. We ex-
tend this definition to treat successful analyses with
a high tree entropy as being out of coverage. Fig. 3
shows the effect of decreasing entropy thresholds
 on precision, recall and coverage. The unfiltered
set of all sentences is found at =1. Lowering  in-
10Possibly higher values in the test set will be clipped to 1.
11Evaluation figures for this experiment are given disregard-
ing parameterisation (and punctuation), corresponding to the
first row of figures in table 1.
82
84
86
88
90
92
94
96
0.160.180.20.220.240.260.280.3
in
 %
Normalized entropy
precision
recall
f-measure
Figure 4: Maximise f-measure on the training set to
determine best entropy threshold
creases precision, and decreases recall and coverage.
We determine f-measure as composite measure of
precision and recall with equal weighting (=0.5).
Results We use f-measure as a target function on
the training set to determine a plausible . F-measure
is maximal at =0.236 with 88.9%, see Figure 4.
Precision and recall are 83.7% and 94.8% resp.
while coverage goes down to 83.0%. Applying the
same  on the test set, we get the following results:
80.5% precision, 93.0% recall. Coverage goes down
to 80.6%. LP is 93.3%, LR is 91.2%.
Confidence Measure We distribute the comple-
ment of the associated tree entropy of a parse tree tr
as a global confidence measure over all brackets br
extracted from that parse: conf
ent
(br) = 1 ent(tr).
For the thresholded version of conf
ent
(br), we set
the threshold to 1   = 1  0:236 = 0:764.
5 Experiments
Experimental Setup In the experiments we use
the subset of the NEGRA corpus (5060 sents,
24.57%) that is currently parsed by the HPSG gram-
mar.12 Average sentence length is 8.94, ignoring
punctuation; average lexical ambiguity is 3.05 en-
tries/word. As baseline, we performed a run with-
out topological information, yet including PoS pri-
oritisation from tagging.13 A series of tests explores
the effects of alternative parameter settings. We fur-
ther test the impact of chunk information. To this
12This test set is different from the corpus used in Section 4.
13In a comparative run without PoS-priorisation, we estab-
lished a speed-up factor of 1.13 towards the baseline used in
our experiment, with a slight increase in coverage (1%). This
compares to a speed-up factor of 2.26 reported in (Daum et al,
2003), by integration of PoS guidance into a dependency parser.
end, phrasal fields determined by topological pars-
ing were fed to the chunk parser of (Skut and Brants,
1998). Extracted NP and PP bracket constraints are
defined as left-matching bracket types, to compen-
sate for the non-embedding structure of chunks.
Chunk brackets are tested in conjunction with topo-
logical brackets, and in isolation, using the labelled
precision value of 71.1% in (Skut and Brants, 1998)
as a uniform confidence weight.14
Measures For all runs we measure the absolute
time and the number of parsing tasks needed to com-
pute the first reading. The times in the individual
runs were normalised according to the number of
executed tasks per second. We noticed that the cov-
erage of some integrated runs decreased by up to
1% of the 5060 test items, with a typical loss of
around 0.5%. To warrant that we are not just trading
coverage for speed, we derived two measures from
the primary data: an upper bound, where we asso-
ciated every unsuccessful parse with the time and
number of tasks used when the limit of 70000 pas-
sive edges was hit, and a lower bound, where we
removed the most expensive parses from each run,
until we reached the same coverage. Whereas the
upper bound is certainly more realistic in an applica-
tion context, the lower bound gives us a worst case
estimate of expectable speed-up.
Integration Parameters We explored the follow-
ing range of weighting parameters for prioritisation
(see Section 3.3 and Table 2).
We use two global settings for the heuristic pa-
rameter . Setting  to 1
2
without using any confi-
dence measure causes the priority of every affected
parsing task to be in- or decreased by half its value.
Setting  to 1 drastically increases the influence of
topological information, the priority for rewarded
tasks is doubled and set to zero for penalized ones.
The first two runs (rows with  P  E) ignore
both confidence parameters (conf
pr=ent
=1), measur-
ing only the effect of higher or lower influence of
topological information. In the remaining six runs,
the impact of the confidence measures conf
pr=ent
is
tested individually, namely +P  E and  P +E, by
setting the resp. alternative value to 1. For two runs,
we set the resp. confidence values that drop below
a certain threshold to zero (PT, ET) to exclude un-
14The experiments were run on a 700 MHz Pentium III ma-
chine. For all runs, the maximum number of passive edges was
set to the comparatively high value of 70000.
factor msec (1st) tasks
low-b up-b low-b up-b low-b up-b
Baseline     524 675 3813 4749
Integration of topological brackets w/ parameters
 P  E  1
2
2.21 2.17 237 310 1851 2353
 P  E 1 2.04 2.10 257 320 2037 2377
+P  E  1
2
2.15 2.21 243 306 1877 2288
PT  E  1
2
2.20 2.30 238 294 1890 2268
 P +E  1
2
2.27 2.23 230 302 1811 2330
 P ET  1
2
2.10 2.00 250 337 1896 2503
+P  E 1 2.06 2.12 255 318 2021 2360
PT  E 1 2.08 2.10 252 321 1941 2346
PT with chunk and topological brackets
PT  E  1
2
2.13 2.16 246 312 1929 2379
PT with chunk brackets only
PT  E  1
2
0.89 1.10 589 611 4102 4234
Table 2: Priority weight parameters and results
certain candidate brackets or bracket types. For runs
including chunk bracketing constraints, we chose
thresholded precision (PT) as confidence weights
for topological and/or chunk brackets.
6 Discussion of Results
Table 2 summarises the results. A high impact on
bracket constraints (1) results in lower perfor-
mance gains than using a moderate impact ( 1
2
)
(rows 2,4,5 vs. 3,8,9). A possible interpretation is
that for high , wrong topological constraints and
strong negative priorities can mislead the parser.
Use of confidence weights yields the best per-
formance gains (with  1
2
), in particular, thresholded
precision of bracket types PT, and tree entropy
+E, with comparable speed-up of factor 2.2/2.3 and
2.27/2.23 (2.25 if averaged). Thresholded entropy
ET yields slightly lower gains. This could be due to
a non-optimal threshold, or the fact that ? while pre-
cision differentiates bracket types in terms of their
confidence, such that only a small number of brack-
ets are weakened ? tree entropy as a global measure
penalizes all brackets for a sentence on an equal ba-
sis, neutralizing positive effects which ? as seen in
+/ P ? may still contribute useful information.
Additional use of chunk brackets (row 10) leads
to a slight decrease, probably due to lower preci-
sion of chunk brackets. Even more, isolated use of
chunk information (row 11) does not yield signifi-
01000
2000
3000
4000
5000
6000
7000
0 5 10 15 20 25 30 35
baseline
+PT ?(0.5)
12867 12520 11620 9290
0
100
200
300
400
500
600
#sentences
msec
Figure 5: Performance gain/loss per sentence length
cant gains over the baseline (0.89/1.1). Similar re-
sults were reported in (Daum et al, 2003) for inte-
gration of chunk- and dependency parsing.15
For PT -E  1
2
, Figure 5 shows substantial per-
formance gains, with some outliers in the range of
length 25?36. 962 sentences (length >3, avg. 11.09)
took longer parse time as compared to the baseline
(with 5% variance margin). For coverage losses, we
isolated two factors: while erroneous topological in-
formation could lead the parser astray, we also found
cases where topological information prevented spu-
rious HPSG parses to surface. This suggests that
the integrated system bears the potential of cross-
validation of different components.
7 Conclusion
We demonstrated that integration of shallow topo-
logical and deep HPSG processing results in signif-
icant performance gains, of factor 2.25?at a high
level of deep parser efficiency. We show that macro-
structural constraints derived from topological pars-
ing improve significantly over chunk-based con-
straints. Fine-grained prioritisation in terms of con-
fidence weights could further improve the results.
Our annotation-based architecture is now easily
extended to address robustness issues beyond lexical
matters. By extracting spans for clausal fragments
from topological parses, in case of deep parsing fail-
15(Daum et al, 2003) report a gain of factor 2.76 relative to a
non-PoS-guided baseline, which reduces to factor 1.21 relative
to a PoS-prioritised baseline, as in our scenario.
ure the chart can be inspected for spanning anal-
yses for sub-sentential fragments. Further, we can
simplify the input sentence, by pruning adjunct sub-
clauses, and trigger reparsing on the pruned input.
References
M. Becker and A. Frank. 2002. A Stochastic Topological
Parser of German. In Proceedings of COLING 2002,
pages 71?77, Taipei, Taiwan.
T. Brants. 2000. Tnt - A Statistical Part-of-Speech Tag-
ger. In Proceedings of Eurospeech, Rhodes, Greece.
U. Callmeier. 2000. PET ? A platform for experimenta-
tion with efficient HPSG processing techniques. Nat-
ural Language Engineering, 6 (1):99 ? 108.
C. Carroll and E. Briscoe. 2002. High precision extrac-
tion of grammatical relations. In Proceedings of COL-
ING 2002, pages 134?140.
B. Crysmann, A. Frank, B. Kiefer, St. Mu?ller, J. Pisko-
rski, U. Scha?fer, M. Siegel, H. Uszkoreit, F. Xu,
M. Becker, and H.-U. Krieger. 2002. An Integrated
Architecture for Deep and Shallow Processing. In
Proceedings of ACL 2002, Pittsburgh.
M. Daum, K.A. Foth, and W. Menzel. 2003. Constraint
Based Integration of Deep and Shallow Parsing Tech-
niques. In Proceedings of EACL 2003, Budapest.
D. Duchier and R. Debusmann. 2001. Topological De-
pendency Trees: A Constraint-based Account of Lin-
ear Precedence. In Proceedings of ACL 2001.
C. Grover and A. Lascarides. 2001. XML-based data
preparation for robust deep parsing. In Proceedings of
ACL/EACL 2001, pages 252?259, Toulouse, France.
T. Ho?hle. 1983. Topologische Felder. Unpublished
manuscript, University of Cologne.
R. Hwa. 2000. Sample selection for statistical gram-
mar induction. In Proceedings of EMNLP/VLC-2000,
pages 45?52, Hong Kong.
S. Mu?ller and W. Kasper. 2000. HPSG analysis of
German. In W. Wahlster, editor, Verbmobil: Founda-
tions of Speech-to-Speech Translation, Artificial Intel-
ligence, pages 238?253. Springer, Berlin.
R. Prins and G. van Noord. 2001. Unsupervised pos-
tagging improves parsing accuracy and parsing effi-
ciency. In Proceedings of IWPT, Beijing.
U. Scha?fer. 2003. WHAT: An XSLT-based Infrastruc-
ture for the Integration of Natural Language Process-
ing Components. In Proceedings of the SEALTS Work-
shop, HLT-NAACL03, Edmonton, Canada.
H. Schmid, 2000. LoPar: Design and Implementation.
IMS, Stuttgart. Arbeitspapiere des SFB 340, Nr. 149.
W. Skut and T. Brants. 1998. Chunk tagger: statistical
recognition of noun phrases. In ESSLLI-1998 Work-
shop on Automated Acquisition of Syntax and Parsing.
H. Uszkoreit. 2002. New Chances for Deep Linguistic
Processing. In Proceedings of COLING 2002, pages
xiv?xxvii, Taipei, Taiwan.
Proceedings of the ACL-HLT 2011 System Demonstrations, pages 7?13,
Portland, Oregon, USA, 21 June 2011. c?2011 Association for Computational Linguistics
The ACL Anthology Searchbench
Ulrich Scha?fer Bernd Kiefer Christian Spurk Jo?rg Steffen Rui Wang
Language Technology Lab
German Research Center for Artificial Intelligence (DFKI)
D-66123 Saarbru?cken, Germany
{ulrich.schaefer,kiefer,cspurk,steffen,wang.rui}@dfki.de
http://www.dfki.de/lt
Abstract
We describe a novel application for structured
search in scientific digital libraries. The ACL
Anthology Searchbench is meant to become a
publicly available research tool to query the
content of the ACL Anthology. The applica-
tion provides search in both its bibliographic
metadata and semantically analyzed full tex-
tual content. By combining these two features,
very efficient and focused queries are possi-
ble. At the same time, the application serves
as a showcase for the recent progress in nat-
ural language processing (NLP) research and
language technology. The system currently
indexes the textual content of 7,500 anthol-
ogy papers from 2002?2009 with predicate-
argument-like semantic structures. It also
provides useful search filters based on bib-
liographic metadata. It will be extended to
provide the full anthology content and en-
hanced functionality based on further NLP
techniques.
1 Introduction and Motivation
Scientists in all disciplines nowadays are faced with
a flood of new publications every day. In addi-
tion, more and more publications from the past be-
come digitally available and thus even increase the
amount. Finding relevant information and avoiding
duplication of work have become urgent issues to be
addressed by the scientific community.
The organization and preservation of scientific
knowledge in scientific publications, vulgo text doc-
uments, thwarts these efforts. From a viewpoint of
a computer scientist, scientific papers are just ?un-
structured information?. At least in our own sci-
entific community, Computational Linguistics, it is
generally assumed that NLP could help to support
search in such document collections.
The ACL Anthology1 is a comprehensive elec-
tronic collection of scientific papers in our own field
(Bird et al, 2008). It is updated regularly with
new publications, but also older papers have been
scanned and are made available electronically.
We have implemented the ACL Anthology
Searchbench2 for two reasons: Our first aim is to
provide a more targeted search facility in this col-
lection than standard web search on the anthology
website. In this sense, the Searchbench is meant to
become a service to our own community.
Our second motivation is to use the developed
system as a showcase for the progress that has been
made over the last years in precision-oriented deep
linguistic parsing in terms of both efficiency and
coverage, specifically in the context of the DELPH-
IN community3. Our system also uses further NLP
techniques such as unsupervised term extraction,
named entity recognition and part-of-speech (PoS)
tagging.
By automatically precomputing normalized se-
mantic representations (predicate-argument struc-
ture) of each sentence in the anthology, the search
space is structured and allows to find equivalent or
related predicates even if they are expressed differ-
1http://www.aclweb.org/anthology
2http://aclasb.dfki.de
3http://www.delph-in.net ? DELPH-IN stands for
DEep Linguistic Processing with HPSG INitiative.
7
ently, e.g. in passive constructions, using synonyms,
etc. By storing the semantic sentence structure along
with the original text in a structured full-text search
engine, it can be guaranteed that recall cannot fall
behind the baseline of a fulltext search.
In addition, the Searchbench also provides de-
tailed bibliographic metadata for filtering as well as
autosuggest texts for input fields computed from the
corpus ? two further key features one can expect
from such systems today, nevertheless very impor-
tant for efficient search in digital libraries.
We describe the offline preprocessing and deep
parsing approach in Section 2. Section 3 concen-
trates on the generation of the semantic search in-
dex. In Section 4, we describe the search interface.
We conclude in Section 5 and present an outlook to
future extensions.
2 Parsing the ACL Anthology
The basis of the search index for the ACL Anthol-
ogy are its original PDF documents, currently 8,200
from the years 2002 through 2009. To overcome
quality problems in text extraction from PDF, we
use a commercial PDF extractor based on OCR tech-
niques. This approach guarantees uniform and high-
quality textual representations even from older pa-
pers in the anthology (before 2000) which mostly
were scanned from printed paper versions.
The general idea of the semantics-oriented ac-
cess to scholarly paper content is to parse each sen-
tence they contain with the open-source HPSG (Pol-
lard and Sag, 1994) grammar for English (ERG;
Flickinger (2002)) and then distill and index seman-
tically structured representations for search.
To make the deep parser robust, it is embedded
in a NLP workflow. The coverage (percentage of
full deeply parsed sentences) on the anthology cor-
pus could be increased from 65 % to now more
than 85 % through careful combination of several
robustness techniques; for example: (1) chart prun-
ing, directed search during parsing to increase per-
formance, and also coverage for longer sentences
(Cramer and Zhang, 2010); (2) chart mapping, a
novel method for integrating preprocessing informa-
tion in exactly the way the deep grammar expects
it (Adolphs et al, 2008); (3) new version of the
ERG with better handling of open word classes; (4)
more fine-grained named entity recognition, includ-
ing recognition of citation patterns; (5) new, better
suited parse ranking model (WeScience; Flickinger
et al (2010)). Because of limited space, we will fo-
cus on (1) and (2) below. A more detailed descrip-
tion and further results are available in (Scha?fer and
Kiefer, 2011).
Except for a small part of the named entity recog-
nition components (citations, some terminology)
and the parse ranking model, there are no further
adaptations to genre or domain of the text corpus.
This implies that the NLP workflow could be easily
and modularly adapted to other (scientific or non-
scientific) domains?mainly thanks to the generic
and comprehensive language modelling in the ERG.
The NLP preprocessing component workflow is
implemented using the Heart of Gold NLP mid-
dleware architecture (Scha?fer, 2006). It starts
with sentence boundary detection (SBR) and regu-
lar expression-based tokenization using its built-in
component JTok, followed by the trigram-based PoS
tagger TnT (Brants, 2000) trained on the Penn Tree-
bank (Marcus et al, 1993) and the named entity rec-
ognizer SProUT (Droz?dz?yn?ski et al, 2004).
2.1 Precise Preprocessing Integration with
Chart Mapping
Tagger output is combined with information from
the named entity recognizer, e.g. delivering hypo-
thetical information on citation expressions. The
combined result is delivered as input to the deep
parser PET (Callmeier, 2000) running the ERG.
Here, citations, for example, can be treated as either
persons, locations or appositions.
Concerning punctuation, the ERG can make use
of information on opening and closing quotation
marks. Such information is often not explicit in the
input text, e.g. when, as in our setup, gained through
OCR which does not distinguish between ? and ? or ?
and ?. However, a tokenizer can often guess (recon-
struct) leftness and rightness correctly. This infor-
mation, passed to the deep parser via chart mapping,
helps it to disambiguate.
2.2 Increased Processing Speed and Coverage
through Chart Pruning
In addition to a well-established discriminative max-
imum entropy model for post-analysis parse selec-
8
tion, we use an additional generative model as de-
scribed in Cramer and Zhang (2010) to restrict the
search space during parsing. This restriction in-
creases efficiency, but also coverage, because the
parse time was restricted to at most 60 CPU seconds
on a standard PC, and more sentences could now be
parsed within these bounds. A 4 GB limit for main
memory consumption was far beyond what was ever
needed. We saw a small but negligible decrease in
parsing accuracy, 5.4 % best parses were not found
due to the pruning of important chart edges.
Ninomiya et al (2006) did a very thorough com-
parison of different performance optimization strate-
gies, and among those also a local pruning strategy
similar to the one used here. There is an important
difference between the systems, in that theirs works
on a reduced context-free backbone first and recon-
structs the results with the full grammar, while PET
uses the HPSG grammar directly, with subsumption
packing and partial unpacking to achieve a similar
effect as the packed chart of a context-free parser.
 0
 10
 20
 30
 40
 50
 60
 70
 80
 0  20  40  60  80  100
sentences x 1000mean parse time (CPU s)
sentence length ??
Figure 1: Distribution of sentence length and mean parse
times for mild pruning
In total, we parsed 1,537,801 sentences, of which
57,832 (3.8 %) could not be parsed because of lexi-
con errors. Most of them were caused by OCR ar-
tifacts resulting in unexpected punctuation character
combinations. These can be identified and will be
deleted in the future.
Figure 1 displays the average parse time of pro-
cessing with a mild chart pruning setting, together
with the mean quadratic error. In addition, it con-
tains the distribution of input sentences over sen-
tence length. Obviously, the vast majority of sen-
tences has a length of at most 60 words4. The parse
times only grow mildly due to the many optimiza-
tion techniques in the original system, and also the
new chart pruning method. The sentence length dis-
tribution has been integrated into Figure 1 to show
that the predominant part of our real-world corpus
can be processed using this information-rich method
with very low parse times (overall average parse
time < 2 s per sentence).
The large amount of short inputs is at first surpris-
ing, even more so that most of these inputs can not
be parsed. Most of these inputs are non-sentences
such as headings, enumerations, footnotes, table cell
content. There are several alternatives to deal with
such input, one to identify and handle them in a pre-
processing step, another to use a special root con-
dition in the deep analysis component that is able
to combine phrases with well-defined properties for
inputs where no spanning result could be found.
We employed the second method, which has the
advantage that it handles a larger range of phenom-
ena in a homogeneous way. Figure 2 shows the
change in percentage of unparsed and timed out in-
puts for the mild pruning method with and without
the root condition combining fragments.
 0
 10
 20
 30
 40
 50
 60
 70
 80
 90
 100
 0  20  40  60  80  100
strictstrict timeoutstrict+fragmentsstrict+fragments timeout
sentence length ??
Figure 2: Unparsed and timed out sentences with and
without fragment combination
Figure 2 shows that this changes the curve for un-
parsed sentences towards more expected character-
istics and removes the uncommonly high percent-
age of short sentences for which no parse can be
computed. Together with the parses for fragmented
4It has to be pointed out that extremely long sentences also
may be non-sentences resulting from PDF extraction errors,
missing punctuation etc. No manual correction took place.
9
Figure 3: Multiple semantic tuples may be generated for a sentence
input, we get a recall (sentences with at least one
parse) over the whole corpus of 85.9 % (1,321,336
sentences), without a significant change for any of
the other measures, and with potential for further im-
provement.
3 Semantic Tuple Extraction with DMRS
In contrast to shallow parsers, the ERG not only
handles detailed syntactic analyses of phrases, com-
pounds, coordination, negation and other linguistic
phenomena that are important for extracting seman-
tic relations, but also generates a formal semantic
representation of the meaning of the input sentence
in the Minimal Recursion Semantics (MRS) repre-
sentation format (Copestake et al, 2005). It consists
of elementary predications for each word and larger
constituents, connected via argument positions and
variables, from which predicate-argument structure
can be extracted.
MRS representations resulting from deep parsing
are still relatively close to linguistic structures and
contain more detailed information than a user would
like to query and search for. Therefore, an additional
extraction and abstraction step is performed before
storing semantic structures in the search index.
Firstly, MRS is converted to DMRS (Copes-
take, 2009), a dependency-style version of MRS
that eases extraction of predicate-argument struc-
ture using the implementation in LKB (Copestake,
2002). The representation format we devised for the
search index we call semantic tuples, in fact quintu-
ples <subject, predicate, first object, second object,
adjuncts>; example in Figure 3. The basic extrac-
tion algorithm consists of the following three steps:
(1) calculate the closure for each elementary pred-
ication based on the EQ (variable equivalence) re-
lation, and group the predicates and entities in each
closure respectively; (2) extract the relations of the
groups, which results in a graph as a whole; (3) re-
cursively traverse the graph, form one semantic tu-
ple for each predicate, and fill in the corresponding
information under its scope, i.e. subject, object, etc.
In the example shown in Figure 3, entity groups
like ?our systems?, ?the baseline?, and ?good perfor-
mance on the SRL task?, as well as predicate groups
?beating? and ?achieved? are formed at the first step.
In the second step, the graph structure is extracted,
i.e., the relation between the groups. Finally, two
semantic tuples are filled in with both the predicates
and the corresponding information. Notice that the
modifier(s) of the entity belong to the same entity
group, but the modifier(s) of the predicate will be
put into the Adjuncts slot. Similarly, the coordina-
tion of the entities will be put into one entity group,
while the coordination of predicates will form mul-
tiple semantic tuples.
Since we are extracting predicate-argument struc-
ture, syntactic variations such as passive construc-
tions and relative clauses will be all ?normalized?
into the same form. Consequently, ?the book which
I read?, ?I read the book?, and ?the book was read
by me? will form the exact same semantic tuple <I,
read, the book, N/A, N/A>. The resulting tuple
structures along with their associated text are stored
in an Apache Solr/Lucene5 server which receives
queries from the Searchbench user interface.
4 Searchbench User Interface
The Searchbench user interface (UI) is a web appli-
cation running in every modern, JavaScript-enabled
web browser. As can be seen in Figure 4, the UI
is divided into three parts: (1) a sidebar on the left
(Filters View), where different filters can be set that
constrain the list of found documents; (2) a list of
found documents matching the currently set filters
in the upper right part of the UI (Results View); (3)
5http://lucene.apache.org/solr
10
Figure 4: Searchbench user interface with different filters set and currently looking at the debug menu for a sentence.
the Document View in the lower right part of the UI
with different views of the current document.
A focus in the design of the UI has been to al-
low the user to very quickly browse the papers of the
ACL Anthology and then to find small sets of rele-
vant documents based on metadata and content. This
is mainly achieved by these techniques: (i) changes
in the collection of filters automatically update the
Results View; (ii) metadata and searchable content
from both the Results View and the Document View
can easily be used with a single click as new filters;
(iii) filters can easily be removed with a single click;
(iv) manually entering filter items is assisted by sen-
sible autosuggestions computed from the corpus; (v)
accidental filter changes can easily be corrected by
going back in the browser history.
The following kinds of filters are supported:
Statements (filter by semantic statements, i.e., the
actual content of sentences, see Section 4.1), Key-
words (filter by simple keywords with a full-text
search), Topics (filter by topics of the articles that
were extracted with an extended approach of the un-
supervised term extractor of Frantzi et al (1998)),
Publication (filter by publication title/event), Au-
thors (filter by author names), Year (filter by pub-
lication year), Affiliations (filter by affiliation or-
ganizations), Affiliation Sites (filter by affiliation
cities and countries)6. Found papers always match
all currently set filters. For each filter type multi-
ple different filter items can be set; one could search
for papers written jointly by people from different
research institutes on a certain topic, for example.
Matches of the statements filter and the keywords
filter are highlighted in document snippets for each
paper in the Results View and in the currently se-
lected paper of the Document View.
Besides a header displaying the metadata of the
currently selected paper (including the automatically
extracted topics on the right), the Document View
provides three subviews of the selected paper: (1)
the Document Content View is a raw list of the sen-
tences of the paper and provides different kinds of
interaction with these sentences; (2) the PDF View
shows the original PDF version of the paper; (3) the
Citations View provides citation information includ-
6Affiliations have been added using the ACL Anthology
Network data (Radev et al, 2009).
11
ing link to the ACL Anthology Network (Radev et
al., 2009).
Figure 4 shows the search result for a query com-
bining a statement (?obtain improvements?), a topic
?dependency parsing? and the publication year 2008.
As can be seen in the Results View, six papers
match these filters; sentences with semantically sim-
ilar predicates and passive voice are found, too.
4.1 Semantic Search
The main feature which distinguishes the ACL An-
thology Searchbench from other search applications
for scientific papers is the semantic search in paper
content. This enables the search for (semantic) state-
ments in the paper content as opposed to searching
for keywords in the plain text. Our use of the term
?statement? is loosely along the lines of the same
term used in logic. Very simple sentences often
bear a single statement only, while more complex
sentences (especially when having multiple clauses)
contain multiple statements. Each of the semantic
tuples extracted from the papers of the ACL Anthol-
ogy (cf. Section 3) corresponds to a statement.
The Statements filter is responsible for the seman-
tic search. Statements used in filters may be under-
specified, e.g., one may search for statements with a
certain semantic subject but with arbitrary semantic
predicates and objects. There are two ways in which
a new statement filter can be set: (1) entering a state-
ment manually; (2) clicking a sentence in the Doc-
ument Content View and choosing the statements of
this sentence that shall be set as new statement fil-
ters (cf. Figure 5), i.e. it is possible to formulate and
refine queries ?by example?.
Figure 5: Dialog for choosing statements to be used as
new filters (for sentence ?Our systems achieved good per-
formance on the SRL task, easily beating the baseline.?).
Throughout the user interface, no distinction is
made between the different kinds of semantic ob-
jects and adjuncts so as to make it easy also for
non-linguists to use the search and to be more ro-
bust against bad analyses of the parser. Therefore,
the different semantic parts of a statement are high-
lighted in three different colors only, depending on
whether a part is the semantic subject, the semantic
predicate or anything else (object/adjunct).
In order to disengage even further from the con-
crete wording and make the semantic search even
more ?meaning-based?, we additionally search for
synonyms of the semantic predicates in statement
filters. These synonyms have been computed as an
intersection of the most frequent verbs (semantic
predicates) in the anthology corpus with WordNet
synsets (Fellbaum, 1998), the main reason being re-
duction of the number of meanings irrelevant for the
domain. This relatively simple approach could of
course be improved, e.g. by active learning from
user clicks in search results etc.
5 Summary and Outlook
We have described the ACL Anthology Search-
bench, a novel search application for scientific dig-
ital libraries. The system is fully implemented and
indexes 7,500 papers of the 8,200 parsed ones. For
the other 700, bibliographic metadata was missing.
These and the remaining 10,000 papers are currently
being processed and will be added to the search in-
dex. The goal of the Searchbench is both to serve
as a showcase for benefits and improvement of NLP
for text search and at the same time provide a use-
ful tool for researchers in Computational Linguis-
tics. We believe that the tool by now already sup-
ports targeted search in a large collection of digital
research papers better than standard web search en-
gines. An evaluation comparing Searchbench query
results with web search is in progress.
Optionally, the Searchbench runs in a linguistic
debug mode providing NLP output a typical user
would not need. These analyses are accessible from
a context menu on each sentence (cf. Figure 4). Both
a tabular view of the semantic tuples of a sentence
(cf. Figure 3) and different kinds of information re-
lated to the parsing of the sentence (including the
MRS and a parse tree) can be displayed.
Future work, for which we are urgently seek-
ing funding, could include integration of further
12
NLP-based features such as coreference resolution
or question answering, as well as citation classifi-
cation and graphical navigation along the ideas in
Scha?fer and Kasterka (2010).
Acknowledgments
We are indebted to Peter Adolphs, Bart Cramer, Dan
Flickinger, Stephan Oepen, Yi Zhang for their sup-
port with ERG and PET extensions such as chart
mapping and chart pruning. Melanie Reiplinger,
Benjamin Weitz and Leonie Gro?n helped with pre-
processing. We also thank the anonymous review-
ers for their encouraging comments. The work de-
scribed in this paper has been carried out in the
context of the project TAKE (Technologies for Ad-
vanced Knowledge Extraction), funded under con-
tract 01IW08003 by the German Federal Ministry
of Education and Research, and in the context of the
world-wide DELPH-IN consortium.
References
Peter Adolphs, Stephan Oepen, Ulrich Callmeier,
Berthold Crysmann, Daniel Flickinger, and Bernd
Kiefer. 2008. Some fine points of hybrid natural lan-
guage parsing. In Proceedings of LREC-2008, pages
1380?1387, Marrakesh, Morocco.
Steven Bird, Robert Dale, Bonnie Dorr, Bryan Gibson,
Mark Joseph, Min-Yen Kan, Dongwon Lee, Brett
Powley, Dragomir Radev, and Yee Fan Tan. 2008. The
ACL anthology reference corpus: A reference dataset
for bibliographic research. In Proceedings of LREC-
2008, pages 1755?1759, Marrakesh, Morocco.
Torsten Brants. 2000. TnT ? a statistical part-of-speech
tagger. In Proc. of ANLP, pages 224?231, Seattle, WA.
Ulrich Callmeier. 2000. PET ? A platform for experi-
mentation with efficient HPSG processing techniques.
Natural Language Engineering, 6(1):99?108.
Ann Copestake, Dan Flickinger, Ivan A. Sag, and Carl
Pollard. 2005. Minimal recursion semantics: an in-
troduction. Research on Language and Computation,
3(2?3):281?332.
Ann Copestake. 2002. Implementing Typed Feature
Structure Grammars. CSLI publications, Stanford.
Ann Copestake. 2009. Slacker semantics: why superfi-
ciality, dependency and avoidance of commitment can
be the right way to go. In Proc. of EACL, pages 1?9.
Bart Cramer and Yi Zhang. 2010. Constraining robust
constructions for broad-coverage parsing with preci-
sion grammars. In Proceedings of COLING-2010,
pages 223?231, Beijing, China.
Witold Droz?dz?yn?ski, Hans-Ulrich Krieger, Jakub Pisko-
rski, Ulrich Scha?fer, and Feiyu Xu. 2004. Shallow
processing with unification and typed feature struc-
tures ? foundations and applications. Ku?nstliche In-
telligenz, 2004(1):17?23.
Christiane Fellbaum, editor. 1998. WordNet, An Elec-
tronic Lexical Database. MIT Press.
Dan Flickinger, Stephan Oepen, and Gisle Ytrest?l.
2010. WikiWoods: Syntacto-semantic annotation for
English Wikipedia. In Proceedings of LREC-2010,
pages 1665?1671.
Dan Flickinger. 2002. On building a more efficient
grammar by exploiting types. In Dan Flickinger,
Stephan Oepen, Hans Uszkoreit, and Jun?ichi Tsujii,
editors, Collaborative Language Engineering. A Case
Study in Efficient Grammar-based Processing, pages
1?17. CSLI Publications, Stanford, CA.
Katerina T. Frantzi, Sophia Ananiadou, and Jun?ichi Tsu-
jii. 1998. The C-value/NC-value method of automatic
recognition for multi-word terms. In Proceedings of
ECDL, pages 585?604.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English. The Penn Treebank. Computational
Linguistics, 19:313?330.
Takashi Ninomiya, Yoshimasa Tsuruoka, Yusuke Miyao,
Kenjiro Taura, and Jun?ichi Tsujii. 2006. Fast and
scalable HPSG parsing. Traitement automatique des
langues (TAL), 46(2).
Carl Pollard and Ivan A. Sag. 1994. Head-Driven Phrase
Structure Grammar. Studies in Contemporary Lin-
guistics. University of Chicago Press, Chicago.
Dragomir R. Radev, Pradeep Muthukrishnan, and Va-
hed Qazvinian. 2009. The ACL anthology network
corpus. In Proceedings of the ACL-2009 Workshop
on Natural Language Processing and Information Re-
trieval for Digital Libraries, Singapore.
Ulrich Scha?fer and Uwe Kasterka. 2010. Scientific
authoring support: A tool to navigate in typed cita-
tion graphs. In Proceedings of the NAACL-HLT 2010
Workshop on Computational Linguistics and Writing,
pages 7?14, Los Angeles, CA.
Ulrich Scha?fer and Bernd Kiefer. 2011. Advances in
deep parsing of scholarly paper content. In Raffaella
Bernardi, Sally Chambers, Bjo?rn Gottfried, Fre?de?rique
Segond, and Ilya Zaihrayeu, editors, Advanced Lan-
guage Technologies for Digital Libraries, LNCS Hot
Topics Series. Springer. to appear.
Ulrich Scha?fer. 2006. Middleware for creating and
combining multi-dimensional NLP markup. In Pro-
ceedings of the EACL-2006 Workshop on Multi-
dimensional Markup in Natural Language Processing,
pages 81?84, Trento, Italy.
13
