Proceedings of the Demonstrations at the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 61?64,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
DKIE: Open Source Information Extraction for Danish
Leon Derczynski
University of Sheffield
leon@dcs.shef.ac.uk
Camilla Vilhelmsen Field
University of Southern Denmark
cafie13@student.sdu.dk
Kenneth S. B?gh
Aarhus University
ksb@cs.au.dk
Abstract
Danish is a major Scandinavian language
spoken daily by around six million peo-
ple. However, it lacks a unified, open set
of NLP tools. This demonstration will in-
troduce DKIE, an extensible open-source
toolkit for processing Danish text. We im-
plement an information extraction archi-
tecture for Danish within GATE, including
integrated third-party tools. This imple-
mentation includes the creation of a sub-
stantial set of corpus annotations for data-
intensive named entity recognition. The
final application and dataset is made are
openly available, and the part-of-speech
tagger and NER model also operate in-
dependently or with the Stanford NLP
toolkit.
1 Introduction
Danish is primarily spoken in the northern hemi-
sphere: in Denmark, on the Faroe islands, and on
Greenland. Having roots in Old Norse, Danish
bears similarities to other Scandinavian languages,
and shares features with English and German.
Previous tools and language resources for Dan-
ish have suffered from license restrictions, or from
using small or non-reusable datasets. As a result,
it is often difficult to use Danish language tech-
nologies, if anything is available at all. In cases
where quality tools are available, they often have
disparate APIs and input/output formats, making
integration time-consuming and prone to error.
To remedy this, this paper presents an open-
source information extraction toolkit for Danish,
using the established and flexible GATE text pro-
cessing platform (Cunningham et al., 2013). To
this end, there are three main goals:
Adaptation: The application adapts to collo-
quial and formal Danish.
Interoperability: DKIE is internally consistent
and adopts unified, well-grounded solutions to the
problems of processing Danish. Where possible,
DKIE re-uses existing components, and strives for
compatibility with major text processing architec-
tures.
Portability: It is preferable for developed com-
ponents to be readily movable within the chosen
architecture, GATE, and without, usable indepen-
dently.
Openness: The resultant application, and cor-
pora and annotations developed in its creation, are
as freely-available as possible.
The remainder of this paper first discusses con-
siderations specific to the language and prior
work, then introduces the information extraction
pipeline, followed by an evaluation of the tools
provided.
2 Processing Danish
There are a few representational issues for Danish
that are not solved in a unified fashion across exist-
ing technological issues. DKIE builds upon major
standards in general linguistic annotation and in
Danish to unify these solutions.
Danish is written using the Latin alphabet, with
the addition of three vowels: ?, ? and ?a, which
may be transliterated as ae, oe and aa respectively.
It is similar to English in terms of capitalisation
rules and character set.
Over time, the orthography of Danish has
shifted. Among other things, a spelling reform
in 1948 removed the capitalisation of nouns, and
introduced the three vowel characters to repre-
sent existing vowel digraphs. There were also
spelling shifts in this reform (e.g. kj?rlighed to
k?rlighed). In addition, some towns and mu-
nicipalities have changed the spelling of their
name. For example, Denmarks second-largest city
Aarhus changed its name to
?
Arhus with the 1948
61
Figure 1: The ANNIE-based information extraction pipeline for Danish
reform, although Aalborg and Aabenraa did not.
Later, in 2011, the city reverted from
?
Arhus to
Aarhus. The city?s university retained the Aarhus
spelling throughout this period.
The effect of these relatively recent changes is
that there exist digitised texts using a variety of or-
thographies not only to represent the same sound,
as also in English, but also the same actual word.
A language processing toolkit for Danish must ex-
hibit sensitivity to these variances.
In addition, Danish has some word bound-
ary considerations. Compound nouns are com-
mon (e.g. kvindeh?andboldlandsholdet for ?the
women?s national handball team?), as are hyphen-
ated constructions (fugle-fotografering for ?bird
photography?) which are often treated as single to-
kens.
Finally, abbreviations are common in Danish,
and its acronyms can be difficult to disambiguate
without the right context and language resource
(e.g. OB for Odense Boldklub, a football club).
3 Background
The state of the art in Danish information extrac-
tion is not very interoperable or open compared to
that for e.g. English. Previous work, while high-
performance, is not available freely (Bick, 2004),
or domain-restricted.
1
This makes results diffi-
cult to reproduce (Fokkens et al., 2013), and leads
to sub-optimal interoperability (Lee et al., 2010).
Even recent books focusing on the topic are heav-
ily licensed and difficult for the average academic
to access. Further, prior tools are often in the form
of discrete components, hard to extend or to inte-
grate with other systems.
Some good corpus resources are available, most
recently the Copenhagen Dependency Treebank
1
E.g. CST?s non-commercial-only anonymisation tool, at
http://cst.dk/online/navnegenkender/
(CDT) (Buch-Kromann and Korzen, 2010), which
built on and included previously-released corpora
for Danish. This 200K-token corpus is taken
from news articles and editorials, and includes
document structure, tokenisation, lemma, part-of-
speech and dependency relation information.
The application demonstrated, DKIE, draws
only on open corpus resources for annotation, and
the annotations over these corpora are released
openly. Further, the application is also made open-
source, with each component having similar or
better performance when compared with the state-
of-the-art.
4 Information Extraction Pipeline
This section details each step in the DKIE
pipeline. A screenshot of the tool is shown in Fig-
ure 1.
4.1 Tokeniser
We adopt the PAROLE tokenisation scheme (Ke-
son and Norling-Christensen, 1998). This makes
different decisions from Penn Treebank in some
cases, concatenating particular expressions as sin-
gle tokens. For example, the two word phrase i alt
? meaning in total ? is converted to the single to-
ken i alt. A set list of these group formations is
given in the Danish PAROLE guidelines.
Another key difference is in the treatment of
quoted phrases and hyphenation. Phrases con-
nected in this way are often treated as single to-
kens. For example, the phrase ?Se og h?r?-
l?serne (readers of ?See and Hear?, a magazine)
is treated as a single token under this scheme.
4.2 Part-of-Speech tagger
We use a machine-learning based tag-
ger (Toutanova et al., 2003) for Danish part-
of-speech labelling. The original PAROLE
62
Tagger Token accuracy % Sentence acc. %
DKIE 95.3 49.1
TnT 96.2 39.1
Table 1: Part-of-speech labelling accuracy in
DKIE
scheme introduces a set of around 120 tags, many
of which are used only rarely. The scheme com-
prises tags built up of up to nine features. These
features are used to describe information such
as case, degree, gender, number, possessivity,
reflexivity, mood, tense and so on (Keson and
Norling-Christensen, 1998).
The PAROLE data includes morphological en-
coding in tags. We separate this data out in
our corpus, adding morphological features distinct
from part-of-speech data. This data may then be
used by later work to train a morphological anal-
yser, or by other tools that rely on morphological
information.
We combine PAROLE annotations with the re-
duced tagset employed by the Danish Dependency
Treebank (DDT) (Kromann, 2003). This has 25
tags. We adapted the tagger to Danish by in-
cluding internal automatic mapping of ?, ? and
?a to two-letter diphthongs when both training and
labelling, by adding extra sets of features for
handling words and adjusting our unknown word
threshold to compensate for the small corpus (as
in Derczynski et al. (2013)), and by specifying the
closed-class tags for this set and language. We
also prefer a CRF-based classifier in order to get
better whole-sequence accuracy, providing greater
opportunities for later-stage tools such as depen-
dency parsers to accurately process more of the
corpus.
Results are given in Table 1, comparing token-
and sentence-level accuracy to other work using
the DDT and the TnT tagger (Brants, 2000). State-
of-the-art performance is achieved, with whole-
sentence tagging accuracy comparable to that of
leading English taggers.
4.3 Gazetteers
High precision entity recognition can be achieved
with gazetteer-based named entity recognition.
This is a low-cost way of quickly getting decent
performance out of existing toolkits. We include
two special kinds of gazetteer for Danish. Firstly,
it is important to annotation the names of enti-
ties specific to Denmark (e.g. Danish towns).
id expression interpretation
-- ---------- --------------
3 igaa ADD(DCT,day,-1)
13 Num._jul ADD(DATE_MONTH_DAY(DCT, 12, 24),
day, TOKEN(0))
Figure 2: Example normalisation rules in TIMEN.
?DCT? refers to the document creation time.
Secondly, entities outside of Denmark sometimes
have different names specific to the Danish lan-
guage (e.g. Lissabon for Lisboa / Lisbon).
As well as a standard strict-matching gazetteer,
we include a ?fuzzy? gazetteer specific to Dan-
ish that tolerates vowel orthography variation and
the other changes introduced in the 1948 spelling
reform. For locations, we extracted data for
names of Danish towns from DBpedia and a lo-
cal gazetteer, and from Wikipedia the Danish-
language versions of the world?s 1 000 most popu-
lous cities. For organisations, we used Wikipedia
cross-language links to map the international or-
ganisations deemed notable in Wikipedia to their
Danish translation and acroynm (e.g. the United
Nations is referred to as FN). The major Danish
political parties were also added to this gazetteer.
For person names, we build lists of both notable
people,
2
and also populated GATE?s first and last
name lists with common choices in Denmark.
4.4 Temporal Expression Annotation
We include temporal annotation for Danish in this
pipeline, making DKIE the first temporal anno-
tation tool for Danish. We follow the TimeML
temporal annotation standard (Pustejovsky et al.,
2004), completing just the TIMEX3 part.
Danish is interesting in that it permits flexible
temporal anchors outside of reference time (Re-
ichenbach, 1947) and the default structure of a cal-
endar. For example, while in English one may use
numbers to express a distance in days (two days
from now) or into a month (the second of March),
Danish permits these offsets from any agreed time.
As a result, it is common to see expressions of the
form 2. juledag, which in this case is the second
christmas day and refers to 26
th
December.
For this pipeline, we use finite state transducers
to define how Danish timexes may be recognised.
We then use the general-purpose TIMEN (Llorens
et al., 2012) timex normalisation tool to provide
calendar or TIMEX3 values for these expressions.
Example rules are shown in Figure 2.
2
See https://en.wikipedia.org/wiki/List of Danes, minus
musicians due to stage names
63
4.5 Named entities
In addition to gazetteers, we present a machine
learning-based approach to entity recognition and
classification in Danish. We annotated the Copen-
hagen Dependency Treebank for person, location
and organisation entities, according to the ACE
guidelines (or as close as possible). This led
to a total of 100 000 extra tokens annotated for
NEs in Danish, doubling the previously-available
amount. We used three annotators, achieving
inter-annotator agreement of 0.89 on the first
100 000 tokens; annotation is an ongoing effort.
The data was used to learn a model tuned to
Danish with an existing NER tool (Finkel et al.,
2005). We removed word shape conjunctions fea-
tures from the default configuration in an effort to
reduced sensitivities introduced by the group noun
tokenisation issue. This model, and the Stanford
NER tool, were then wrapped as a GATE process-
ing resource, contributing general-purpose Danish
NER to the toolkit.
5 Conclusion
We will demonstrate a modern, interoperable,
open-source NLP toolkit for information extrac-
tion in Danish. The released resources are: a
GATE pipeline for Danish; tools for temporal ex-
pression recognition and normalisation for Dan-
ish; part-of-speech and named entity recognition
models for Danish, that also work in the Stanford
NLP architecture; and named entity corpus an-
notations over the Copenhagen Dependency Tree-
bank.
Acknowledgments
This work was supported by EU funding un-
der grant FP7-ICT-2013-10-611233, Pheme, and
grant agreement No. 296322, AnnoMarket. We
are grateful to Anders S?gaard of Copenhagen
University for comments on an earlier draft and
kind help with gazetteers. The first author would
also like to thank Aarhus University for their kind
provision of research facilities.
References
E. Bick. 2004. A named entity recognizer for Danish.
In Proceedings of LREC.
T. Brants. 2000. TnT: a statistical part-of-speech tag-
ger. In Proceedings of the sixth conference on Ap-
plied natural language processing, pages 224?231.
ACL.
M. Buch-Kromann and I. Korzen. 2010. The unified
annotation of syntax and discourse in the Copen-
hagen Dependency Treebanks. In Proceedings of
the Fourth Linguistic Annotation Workshop, pages
127?131. ACL.
H. Cunningham, V. Tablan, A. Roberts, and
K. Bontcheva. 2013. Getting More Out of
Biomedical Documents with GATE?s Full Lifecycle
Open Source Text Analytics. PLoS computational
biology, 9(2):e1002854.
L. Derczynski, A. Ritter, S. Clark, and K. Bontcheva.
2013. Twitter Part-of-Speech Tagging for All: Over-
coming Sparse and Noisy Data. In Proceedings of
Recent Advances in Natural Language Processing.
Association for Computational Linguistics.
J. R. Finkel, T. Grenager, and C. Manning. 2005. In-
corporating non-local information into information
extraction systems by Gibbs sampling. In Proceed-
ings of the 43rd Annual Meeting on Association for
Computational Linguistics, pages 363?370. ACL.
A. Fokkens, M. van Erp, M. Postma, T. Pedersen,
P. Vossen, and N. Freire. 2013. Offspring from
reproduction problems: What replication failure
teaches us. In Proceedings of the 51st Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 1691?1701. Association for Computa-
tional Linguistics.
B. Keson and O. Norling-Christensen. 1998.
PAROLE-DK. The Danish Society for Language
and Literature.
M. T. Kromann. 2003. The Danish Dependency Tree-
bank and the DTAG treebank tool. In Proceedings
of the Second Workshop on Treebanks and Linguistic
Theories, page 217.
K. Lee, L. Romary, et al. 2010. Towards interoperabil-
ity of ISO standards for Language Resource Man-
agement. Proc. ICGL 2010.
H. Llorens, L. Derczynski, R. J. Gaizauskas, and E. Sa-
quete. 2012. TIMEN: An Open Temporal Ex-
pression Normalisation Resource. In LREC, pages
3044?3051.
J. Pustejovsky, B. Ingria, R. Sauri, J. Castano,
J. Littman, and R. Gaizauskas. 2004. The Specifica-
tion Language TimeML. In The Language of Time:
A Reader, pages 545?557. Oxford University Press.
H. Reichenbach. 1947. The tenses of verbs. In Ele-
ments of Symbolic Logic. Macmillan.
K. Toutanova, D. Klein, C. D. Manning, and Y. Singer.
2003. Feature-rich part-of-speech tagging with a
cyclic dependency network. In Proceedings of the
2003 Conference of the North American Chapter
of the Association for Computational Linguistics,
pages 173?180. ACL.
64
Proceedings of the Demonstrations at the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 97?100,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
The GATE Crowdsourcing Plugin: Crowdsourcing Annotated Corpora
Made Easy
Kalina Bontcheva, Ian Roberts, Leon Derczynski, Dominic Rout
University of Sheffield
{kalina,ian,leon,d.rout}@dcs.shef.ac.uk
Abstract
Crowdsourcing is an increasingly popu-
lar, collaborative approach for acquiring
annotated corpora. Despite this, reuse
of corpus conversion tools and user in-
terfaces between projects is still problem-
atic, since these are not generally made
available. This demonstration will intro-
duce the new, open-source GATE Crowd-
sourcing plugin, which offers infrastruc-
tural support for mapping documents to
crowdsourcing units and back, as well as
automatically generating reusable crowd-
sourcing interfaces for NLP classification
and selection tasks. The entire work-
flow will be demonstrated on: annotating
named entities; disambiguating words and
named entities with respect to DBpedia
URIs; annotation of opinion holders and
targets; and sentiment.
1 Introduction
Annotation science (Hovy, 2010; Stede and
Huang, 2012) and general purpose corpus anno-
tation tools (e.g. Bontcheva et al. (2013)) have
evolved in response to the need for creating high-
quality NLP corpora. Crowdsourcing is a popu-
lar collaborative approach that has been applied
to acquiring annotated corpora and a wide range
of other linguistic resources (Callison-Burch and
Dredze, 2010; Fort et al., 2011; Wang et al., 2012).
Although the use of this approach is intensifying,
especially paid-for crowdsourcing, the reuse of an-
notation guidelines, task designs, and user inter-
faces between projects is still problematic, since
these are generally not made available, despite
their important role in result quality (Khanna et
al., 2010).
A big outstanding challenge for crowdsourc-
ing projects is that the cost to define a single
annotation task remains quite substantial. This
demonstration will introduce the new, open-source
GATE Crowdsourcing plugin, which offers in-
frastructural support for mapping documents to
crowdsourcing units, as well as automatically gen-
erated, reusable user interfaces
1
for NLP classi-
fication and selection tasks. Their use will be
demonstrated on annotating named entities (selec-
tion task), disambiguating words and named enti-
ties with respect to DBpedia URIs (classification
task), annotation of opinion holders and targets
(selection task), as well as sentiment (classifica-
tion task).
2 Crowdsourcing Stages and the Role of
Infrastructural Support
Conceptually, the process of crowdsourcing anno-
tated corpora can be broken down into four main
stages, within which there are a number of largely
infrastructural steps. In particular, data prepara-
tion and transformation into CrowdFlower units,
creation of the annotation UI, creation and upload
of gold units for quality control, and finally map-
ping judgements back into documents and aggre-
gating all judgements into a finished corpus.
The rest of this section discusses in more de-
tail where reusable components and infrastructural
support for automatic data mapping and user inter-
face generation are necessary, in order to reduce
the overhead of crowdsourcing NLP corpora.
2.1 Project Definition
An important part of project definition is the map-
ping of the NLP problem into one or more crowd-
sourcing tasks, which are sufficiently simple to be
carried out by non-experts and with a good qual-
ity. What are helpful here are reusable patterns
for how best to crowdsource different kinds of
NLP corpora. The GATE Crowdsourcing plugin
1
Currently for CrowdFlower, which unlike Amazon Me-
chanical Turk is available globally.
97
currently provides such patterns for selection and
classification tasks.
This stage also focuses on setup of the task pa-
rameters (e.g. number of crowd workers per task,
payment per task) and piloting the project, in order
to tune in its design. With respect to task param-
eters, infrastructural support is helpful, in order
to enable automatic splitting of longer documents
across crowdsourcing tasks.
2.2 Data Preparation
This stage, in particular, can benefit significantly
from infrastructural support and reusable compo-
nents, in order to collect the data (e.g. crawl
the web, download samples from Twitter), pre-
process it with linguistic tools (e.g. tokenisation,
POS tagging, entity recognition), and then map
automatically from documents and sentences to
crowdsourcing micro-tasks.
2.3 Running the Crowdsourcing Project
This is the main phase of each crowdsourcing
project. It consists of three kinds of tasks: task
workflow and management, contributor manage-
ment (including profiling and retention), and qual-
ity control. Paid-for marketplaces like Amazon
Mechanical Turk and CrowdFlower already pro-
vide this support. As with conventional corpus an-
notation, quality control is particularly challeng-
ing, and additional NLP-specific infrastructural
support can help.
2.4 Data Evaluation and Aggregation
In this phase, additional NLP-specific, infrastruc-
tural support is needed for evaluating and aggre-
gating the multiple contributor inputs into a com-
plete linguistic resource, and in assessing the re-
sulting overall quality.
Next we demonstrate how these challenges have
been addressed in our work.
3 The GATE Crowdsourcing Plugin
To address these NLP-specific requirements,
we implemented a generic, open-source GATE
Crowdsourcing plugin, which makes it very easy
to set up and conduct crowdsourcing-based corpus
annotation from within GATE?s visual interface.
3.1 Physical representation for documents
and annotations
Documents and their annotations are encoded in
the GATE stand-off XML format (Cunningham
Figure 1: Classification UI Configuration
et al., 2002), which was chosen for its support
for overlapping annotations and the wide range of
automatic pre-processing tools available. GATE
also has support for the XCES standard (Ide et al.,
2000) and others (e.g. CoNLL) if preferred. An-
notations are grouped in separate annotation sets:
one for the automatically pre-annotated annota-
tions, one for the crowdsourced judgements, and
a consensus set, which can be considered as the fi-
nal resulting corpus annotation layer. In this way,
provenance is fully tracked, which makes it possi-
ble to experiment with methods that consider more
than one answer as potentially correct.
3.2 Automatic data mapping to
CrowdFlower
The plugin expects documents to be pre-
segmented into paragraphs, sentences and word
tokens, using a tokeniser, POS tagger, and sen-
tence splitter ? e.g. those built in to GATE (Cun-
ningham et al., 2002). The GATE Crowdsourcing
plugin allows choice between these of which to
use as the crowdsourcing task unit; e.g., to show
one sentence per unit or one paragraph. In the
demonstration we will show both automatic map-
ping at sentence level (for named entity annota-
tion) and at paragraph level (for named entity dis-
ambiguation).
3.3 Automatic user interface generation
The User Interfaces (UIs) applicable to various
task types tend to fall into a set of categories, the
most commonly used being categorisation, selec-
tion, and text input. The GATE Crowdsourcing
plugin provides generalised and re-usable, auto-
matically generated interfaces for categorisation
98
Figure 2: Classification Interface: Sense Disambiguation Example
Figure 3: Sequential Selection Interface: Named Entity Recognition Example
and selection.
In the first step, task name, instructions, and
classification choices are provided, in a UI config-
uration dialog (see Figure 1). In this example, the
instructions are for disambiguating named entities.
We have configured three fixed choices, which ap-
ply to each entity classification task.
For some categorisation NLP annotation tasks
(e.g. classifying sentiment in tweets into posi-
tive, negative, and neutral), fixed categories are
sufficient. In others, where the available category
choices depend on the text that is being classi-
fied (e.g. the possible disambiguations of Paris
are different from those of London), choices are
defined through annotations on each of the clas-
sification targets. In this case case, the UI gen-
erator then takes these annotations as a parame-
ter and automatically creates the different category
choices, specific to each crowdsourcing unit. Fig-
ure 2 shows an example for sense disambiguation,
which combines two unit-specific classes with the
three fixed classification categories shown before.
Figure 3 shows the CrowdFlower-based user in-
terface for word-constrained sequential selection,
which in this case is parameterised for named en-
tity annotation. In sequential selection, sub-units
are defined in the UI configuration ? tokens, for
this example. The annotators are instructed to
click on all words that constitute the desired se-
quence (the annotation guidelines are given as a
parameter during the automatic user interface gen-
eration).
Since the text may not contain a sequence to be
annotated, we also generate an explicit confirma-
tion checkbox. This forces annotators to declare
that they have made the selection or there is noth-
ing to be selected in this text. CrowdFlower can
then use gold units and test the correctness of the
selections, even in cases where no sequences are
selected in the text. In addition, requiring at least
some worker interaction and decision-making in
every task improves overall result quality.
3.4 Quality control
The key mechanism for spam prevention and qual-
ity control in CrowdFlower is test data, which
we also refer to as gold units. These are com-
pleted examples which are mixed in with the un-
processed data shown to workers, and used to
evaluate worker performance. The GATE Crowd-
sourcing plugin supports automatic creation of
gold units from GATE annotations having a fea-
ture correct. The value of that feature is then
taken to be the answer expected from the human
annotator. Gold units need to be 10%?30% of the
units to be annotated. The minimum performance
threshold for workers can be set in the job config-
uration.
3.5 Automatic data import from
CrowdFlower and adjudication
On completion, the plugin automatically imports
collected multiple judgements back into GATE
99
Figure 4: CrowdFlower Judgements in GATE
and the original documents are enriched with the
crowdsourced information, modelled as multiple
annotations (one per contributor). Figure 4 shows
judgements that have been imported from Crowd-
Flower and stored as annotations on the original
document. One useful feature is the trust metric,
assigned by CrowdFlower for this judgement.
GATE?s existing tools for calculating inter-
annotator agreement and for corpus analysis are
used to gain further insights into the quality of the
collected information. If manual adjudication is
required, GATE?s existing annotations stack edi-
tor is used to show in parallel the annotations im-
ported from CrowdFlower, so that differences in
judgement can easily be seen and resolved. Alter-
natively, automatic adjudication via majority vote
or other more sophisticated strategies can be im-
plemented in GATE as components.
4 Conclusion
This paper described the GATE Crowdsourcing
plugin
2
and the reusable components that it pro-
vides for automatic mapping of corpora to micro-
tasks and vice versa, as well as the generic se-
quence selection and classification user interfaces.
These are easily configurable for a wide range
of NLP corpus annotation tasks and, as part of
this demonstration, several example crowdsourc-
ing projects will be shown.
Future work will focus on expanding the num-
ber of reusable components, the implementation
of reusable automatic adjudication algorithms,
and providing support for crowdsourcing through
games-with-a-purpose (GWAPs).
Acknowledgments This was part of the uComp
project (www.ucomp.eu). uComp receives the
funding support of EPSRC EP/K017896/1, FWF
1097-N23, and ANR-12-CHRI-0003-03, in the
framework of the CHIST-ERA ERA-NET.
2
It is available to download from http://gate.ac.uk/ .
References
Kalina Bontcheva, Hamish Cunningham, Ian Roberts,
Angus. Roberts, Valentin. Tablan, Niraj Aswani, and
Genevieve Gorrell. 2013. GATE Teamware: A
Web-based, Collaborative Text Annotation Frame-
work. Language Resources and Evaluation,
47:1007?1029.
Chris Callison-Burch and Mark Dredze. 2010. Cre-
ating speech and language data with Amazon?s Me-
chanical Turk. In Proceedings of the NAACL HLT
2010 Workshop on Creating Speech and Language
Data with Amazon?s Mechanical Turk, pages 1?12.
Hamish Cunningham, Diana Maynard, Kalina
Bontcheva, and Valentin Tablan. 2002. GATE:
an Architecture for Development of Robust HLT
Applications. In Proceedings of the 40th An-
nual Meeting on Association for Computational
Linguistics, 7?12 July 2002, ACL ?02, pages
168?175, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Karen Fort, Gilles Adda, and K. Bretonnel Cohen.
2011. Amazon mechanical turk: Gold mine or coal
mine? Computational Linguistics, 37(2):413 ?420.
Eduard Hovy. 2010. Annotation. In Tutorial Abstracts
of ACL.
N. Ide, P. Bonhomme, and L. Romary. 2000. XCES:
An XML-based Standard for Linguistic Corpora.
In Proceedings of the second International Confer-
ence on Language Resources and Evaluation (LREC
2000), 30 May ? 2 Jun 2000, pages 825?830,
Athens, Greece.
Shashank Khanna, Aishwarya Ratan, James Davis, and
William Thies. 2010. Evaluating and improving the
usability of Mechanical Turk for low-income work-
ers in India. In Proceedings of the first ACM sympo-
sium on computing for development. ACM.
Manfred Stede and Chu-Ren Huang. 2012. Inter-
operability and reusability: the science of annota-
tion. Language Resources and Evaluation, 46:91?
94. 10.1007/s10579-011-9164-x.
A. Wang, C.D.V. Hoang, and M. Y. Kan. 2012. Per-
spectives on Crowdsourcing Annotations for Natu-
ral Language Processing. Language Resources and
Evaluation, Mar:1?23.
100
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 69?73,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Passive-Aggressive Sequence Labeling with Discriminative Post-Editing
for Recognising Person Entities in Tweets
Leon Derczynski
University of Sheffield
leon@dcs.shef.ac.uk
Kalina Bontcheva
University of Sheffield
kalina@dcs.shef.ac.uk
Abstract
Recognising entities in social media text is
difficult. NER on newswire text is conven-
tionally cast as a sequence labeling prob-
lem. This makes implicit assumptions re-
garding its textual structure. Social me-
dia text is rich in disfluency and often
has poor or noisy structure, and intuitively
does not always satisfy these assumptions.
We explore noise-tolerant methods for se-
quence labeling and apply discriminative
post-editing to exceed state-of-the-art per-
formance for person recognition in tweets,
reaching an F1 of 84%.
1 Introduction
The language of social media text is unusual
and irregular (Baldwin et al., 2013), with mis-
spellings, non-standard capitalisation and jargon,
disfluency and fragmentation. Twitter is one of the
sources of social media text most challenging for
NLP (Eisenstein, 2013; Derczynski et al., 2013).
In particular, traditional approaches to Named
Entity Recognition (NER) perform poorly on
tweets, especially on person mentions ? for exam-
ple, the default model of a leading system reaches
an F1 of less than 0.5 on person entities in a ma-
jor tweet corpus. This indicates a need for ap-
proaches that can cope with the linguistic phe-
nomena apparently common among social media
authors, and operate outside of newswire with its
comparatively low linguistic diversity.
So, how can we adapt? This paper contributes
two techniques. Firstly, it demonstrates that en-
tity recognition using noise-resistant sequence la-
beling outperforms state-of-the-art Twitter NER,
although we find that recall is consistently lower
than precision. Secondly, to remedy this, we intro-
duce a method for automatically post-editing the
resulting entity annotations by using a discrimina-
tive classifier. This improves recall and precision.
2 Background
Named entity recognition is a well-studied prob-
lem, especially on newswire and other long-
document genres (Nadeau and Sekine, 2007; Rati-
nov and Roth, 2009). However, experiments show
that state-of-the-art NER systems from these gen-
res do not transfer well to social media text.
For example, one of the best performing
general-purpose named entity recognisers (hereon
referred to as Stanford NER) is based on linear-
chain conditional random fields (CRF) (Finkel et
al., 2005). The model is trained on newswire
data and has a number of optimisations, includ-
ing distributional similarity measures and sam-
pling for remote dependencies. While excellent
on newswire (overall F1 90%), it performs poorly
on tweets (overall F1 44%) (Ritter et al., 2011).
Rule-based named entity recognition has per-
formed a little better on tweets. Another general-
purpose NER system, ANNIE (Cunningham et al.,
2002), reached F1 of 60% over the same data (Der-
czynski et al., 2013); still a large difference.
These difficulties spurred Twitter-specific NER
research, much of which has fallen into two broad
classes: semi-supervised CRF, and LDA-based.
Semi-supervised CRF: Liu et al. (2011) com-
pare the performance of a person name dictio-
nary (F1 of 33%) to a CRF-based semi-supervised
approach (F1 of 76% on person names), using a
dataset of 12 245 tweets. This, however, is based
on a proprietary corpus, and cannot be compared
to, since the system is also not available.
Another similar approach is TwiNER (Li et al.,
2012), which is focused on a single topic stream
as opposed to general-purpose NER. This leads
to high performance for a topic-sensitive classi-
fier trained to a particular stream. In contrast we
present a general-purpose approach. Further, we
extract a specific entity class, where TwiNER per-
forms entity chunking and no classification.
69
LDA and vocabularies: Ritter et al. (2011)?s
T-NER system uses 2,400 labelled tweets, unla-
belled data and Linked Data vocabularies (Free-
base), as well as co-training. These techniques
helped but did not bring person recognition accu-
racy above the supervised MaxEnt baseline in their
experiments. We use this system as our baseline.
3 Experimental Setup
3.1 Corpus
The experiments combine person annotations
from three openly-available datasets: Ritter et
al. (2011), UMBC (Finin et al., 2010) and
MSM2013 (Basave et al., 2013). In line with pre-
vious research (Ritter et al., 2011), annotations on
@mentions are filtered out. The placeholder to-
kens in MSM data (i.e. MENTION , HASHTAG ,
URL ) are replaced with @Mention, #hashtag,
and http://url/, respectively, to give case and char-
acter n-grams more similar to the original values.
The total corpus has 4 285 tweets, around a third
the size of that in Liu et al. (2011). This dataset
contains 86 352 tokens with 1 741 entity mentions.
Person entity recognition was chosen as it is a
challenging entity type. Names of persons popular
on Twitter change more frequently than e.g. loca-
tions. Person names also tend to have a long tail,
not being confined to just public figures. Lastly,
although all three corpora cover different entity
types, they all have Person annotations.
3.2 Labeling Scheme
Following Li et al. (2009) we used two-class IO la-
beling, where each token is either in-entity or out-
of-entity. In their NER work, this performed better
than the alternative BIO format, since data sparsity
is reduced. The IO scheme has the disadvantage
of being unable to distinguish cases where multi-
ple different entities of the same type follow each
other without intervening tokens. This situation is
uncommon and does not arise in our dataset.
3.3 Features
The Stanford NER tool was used for feature gen-
eration. When required, nominal values were con-
verted to sparse one-hot vectors. Features for
modelling context are included (e.g. ngrams, ad-
joining labels). Our feature sets were:
base: default Stanford NER features, plus the
previous and next token and its word shape.
1
1
Default plus useClassFeature=true, noMidNGrams=true,
Figure 1: Training curve for lem. Diagonal cross
(blue) is CRF/PA, vertical cross (red) SVM/UM.
lem: with added lemmas, lower-case versions
of tokens, word shape, and neighbouring lemmas
(in attempt to reduce feature sparsity & cope better
with lexical and orthographic noise). Word shape
describes the capitalisation and the type of char-
acters (e.g. letters, numbers, symbols) of a word,
without specifying actual character choices. For
example, Capital may become Ww.
These representations are chosen to compare
those that work well for newswire to those with
scope for tolerance of noise, prevalent in Twitter.
3.4 Classifiers
For structured sequence labeling, we experiment
with conditional random fields ? CRF (Lafferty
et al., 2001) ? using the CRFsuite implementa-
tion (Okazaki, 2007) and LBFGS. We also use
an implementation of the passive-aggressive CRF
from CRFsuite, choosing max iterations = 500.
Passive-aggressive learning (Crammer et al.,
2006) demonstrates tolerance to noise in training
data, and can be readily adapted to provide struc-
tured output, e.g. when used in combination with
CRF. Briefly, it skips updates (is passive) when
the hinge loss of a new weight vector during up-
date is zero, but when it is positive, it aggres-
sively adjusts the weight vector regardless of the
required step size. This is integrated into CRF us-
ing a damped loss function and passive-aggressive
(PA) decisions to choose when to update. We ex-
plore the PA-I variant, where the objective func-
tion scales linearly with the slack variable.
maxNGramLeng=6, usePrev=true, useNext=true, usePre-
vSequences=true, maxLeft=1, useTypeSeqs=true, useType-
Seqs2=true, useTypeSeqs3=true, useTypeySequences=true,
wordShape=chris2useLC, useDisjunctive=true, lowercaseN-
Grams=true, useShapeConjunctions=true
70
Approach Precision Recall F1
Stanford 85.88 50.00 63.20
Ritter 77.23 80.18 78.68
MaxEnt 86.92 59.09 70.35
SVM 77.55 59.16 67.11
SVM/UM 73.26 69.63 71.41
CRF 82.94 62.39 71.21
CRF/PA 80.37 65.57 72.22
Table 1: With base features (base)
Approach Precision Recall F1
Stanford 90.60 60.00 72.19
Ritter 77.23 80.18 78.68
MaxEnt 91.10 66.33 76.76
SVM 88.22 66.58 75.89
SVM/UM 81.16 74.97 77.94
CRF 89.52 70.52 78.89
CRF/PA 86.85 74.71 80.32
Table 2: With shape and lemma features (lem)
For independent discriminative classification,
we use SVM, SVM/UM and a maximum entropy
classifier (MegaM (Daum?e III, 2004)). SVM is
provided by the SVMlight (Joachims, 1999) im-
plementation. SVM/UM is an uneven margins
SVM model, designed to deal better with imbal-
anced training data (Li et al., 2009).
3.5 Baselines
The first baseline is the Stanford NER CRF al-
gorithm, the second Ritter?s NER algorithm. We
adapted the latter to use space tokenisation, to
preserve alignment when comparing algorithms.
Baselines are trained and evaluated on our dataset.
3.6 Evaluation
Candidate entity labelings are compared using the
CoNLL NER evaluation tool (Sang and Meulder,
2003), using precision, recall and F1. Following
Ritter, we use 25%/75% splits made at tweet, and
not token, level.
4 Results
The base feature set performs relatively poorly
on all classifiers, with only MaxEnt beating a
baseline on any score (Table 1). However, all
achieve a higher F1 score than the default Stan-
ford NER. Of these classifiers, SVM/UM achieved
the best precision and CRF/PA ? the best F1. This
demonstrates that the noise-tolerance adaptations
to SVM and CRF (uneven margins and passive-
aggressive updates, respectively) did provide im-
provements over the original algorithms.
Results using the extended features (lem) are
shown in Table 2. All classifiers improved, in-
Entity length (tokens) Count
1 610
2 1065
3 51
4 15
Table 3: Distribution of person entity lengths.
cluding the baseline Stanford NER system. The
SVM/UM and CRF/PA adaptations continued to
outperform the vanilla models. With these fea-
tures, MaxEnt achieved highest precision and CRF
variants beat both baselines, with a top F1 of
80.32%. We continue using the lem feature set.
5 Discriminative Post-Editing
Precision is higher than recall for most systems,
especially the best CRF/PA (Table 2). To improve
recall, potential entities are re-examined in post-
editing (Gadde et al., 2011). Manual post-editing
improves machine translation output (Green et al.,
2013); we train an automatic editor.
We adopt a gazetteer-based approach to trig-
gering a discriminative editor, which makes deci-
sions about labels after primary classification. The
gazetteer consists of the top 200 most common
names in English speaking countries. The first
names of popular figures over the past two years
(e.g. Helle, Barack, Scarlett) are also included.
This gives 470 case-sensitive trigger terms.
Often the trigger term is just the first in a se-
quence of tokens that make up the person name.
As can be seen from the entity length statistics
shown in Table 3, examining up to two tokens cov-
ers most (96%) person names in our corpus. Based
on this observation, we look ahead just one extra
token beyond the trigger term. This gives a to-
ken sub-sequence that was marked as out-of-entity
by the original NER classifier. Its constituents be-
come candidate person name tokens.
Candidates are then labeled using a high-recall
classifier. The classifier should be instance-based,
since we are not labeling whole sequences. We
chose SVM with variable cost (Morik et al., 1999),
which can be adjusted to prefer high recall.
To train this classifier, we extract a subset of in-
stances from the current training split as follows.
Each trigger term is included. Also, if the trig-
ger term is labeled as an entity, each subsequent
in-entity token is also included. Finally, the next
out-of-entity token is also included, to give exam-
ples of when to stop. For example, these tokens
are either in or out of the training set:
71
Overall
Method Missed entity F1 P R F1
No editing - plain CRF/PA 0.00 86.85 74.71 80.32
Na??ve: trigger token only 5.82 86.61 78.91 82.58
Na??ve: trigger plus one 6.05 81.26 82.08 81.67
SVM editor, Cost = 0.1 78.26 87.38 79.16 83.07
SVM editor, Cost = 0.5 89.72 87.17 80.30 83.60
SVM editor, Cost = 1.0 90.74 87.19 80.43 83.67
SVM editor, Cost = 1.5 92.73 87.23 80.69 83.83
SVM editor, Cost = 2.0 92.73 87.23 80.69 83.83
Table 4: Post-editing performance. Higher Cost sacrifices precision for recall.
Miley O in
Heights O out
Miley PERSON in
Cyrus PERSON in
is O in
famous O out
When post-editing, the window is any trigger
term and the following token, regardless of initial
label. The features used were exactly the same as
with the earlier experiment, using the lem set. This
is compared with two na??ve baselines: always an-
notating trigger terms as Person, and always anno-
tating trigger terms and the next token as Person.
Results are shown in Table 4. Na??ve editing
baselines had F1 on missed entities of around 6%,
showing that post-editing needs to be intelligent.
At Cost = 1.5, recall increased to 80.69, ex-
ceeding the Ritter recall of 80.18 (raising Cost be-
yond 1.5 had no effect). This setup gave good ac-
curacy on previously-missed entities (second col-
umn) and improved overall F1 to 83.83. It also
gave better precision and recall than the best na??ve
baseline (trigger-only), and 6% absolute higher
precision than trigger plus one. This is a 24.2% re-
duction in error over the Ritter baseline (F1 78.68),
and a 17.84% error reduction compared to the best
non-edited system (CRF/PA+lem).
6 Error Analysis
We examine two types of classification error: false
positives (spurious) and false negatives (missed).
False positives occur most often where non-
person entities are mentioned. This occurred with
mentions of organisations (Huff Post), locations
(Galveston) and products (Exodus Porter). De-
scriptive titles were also sometimes mis-included
in person names (Millionaire Rob Ford). Names of
persons used in other forms also presented as false
positives (e.g. Marie Claire ? a magazine). Pol-
ysemous names (i.e. words that could have other
functions, such as a verb) were also mis-resolved
(Mark). Finally, proper nouns referring to groups
were sometimes mis-included (Haitians).
Despite these errors, precision almost always
remained higher than recall over tweets. We use
in-domain training data, and so it is unlikely that
this is due to the wrong kinds of person being cov-
ered in the training data ? as can sometimes be the
case when applying tools trained on newswire.
False negatives often occurred around incorrect
capitalisation and spelling, with unusual names,
with ambiguous tokens and in low-context set-
tings. Both omitted and added capitalisation gave
false negatives (charlie gibson, or KANYE WEST).
Spelling errors also led to missed names (Rus-
sel Crowe). Ambiguous names caused false neg-
atives and false positives; our approach missed
mark used as a name, and the surname of Jack
Straw. Unusual names with words typically used
for other purposes were also not always correctly
recognised (e.g. the Duck Lady, or the last two
tokens of Spicy Pickle Jr.). Finally, names with
few or no context words were often missed (Video:
Adele 21., and 17-9-2010 Tal al-Mallohi, a 19-).
7 Conclusion
Finding named entities in social media text, par-
ticularly tweets, is harder than in newswire. This
paper demonstrated that adapted to handle noisy
input is useful in this scenario. We achieved the
good results using CRF with passive-aggressive
updates. We used representations rich in word
shape and contextual features and achieved high
precision with moderate recall (65.57?74.71).
To improve recall, we added a post-editing stage
which finds candidate person names based on trig-
ger terms and re-labels them using a cost-adjusted
SVM. This flexible and re-usable approach lead to
a final reduction in error rate of 24.2%, giving per-
formance well above that of comparable systems.
Acknowledgment This work received funding
from EU FP7 under grant agreement No. 611233,
Pheme. We thank Chris Manning and John Bauer
of Stanford University for help with the NER tool.
72
References
T. Baldwin, P. Cook, M. Lui, A. MacKinlay, and
L. Wang. 2013. How noisy social media text,
how diffrnt social media sources. In Proceedings of
the Sixth International Joint Conference on Natural
Language Processing, pages 356?364. ACL.
A. E. C. Basave, A. Varga, M. Rowe, M. Stankovic,
and A.-S. Dadzie. 2013. Making Sense of Micro-
posts (# MSM2013) Concept Extraction Challenge.
In Proceedings of the Concept Extraction Challenge
at the Workshop on ?Making Sense of Microposts?,
volume 1019. CEUR-WS.
K. Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz,
and Y. Singer. 2006. Online passive-aggressive al-
gorithms. Journal of Machine Learning Research,
7:551?585.
H. Cunningham, D. Maynard, K. Bontcheva, and
V. Tablan. 2002. GATE: an Architecture for Devel-
opment of Robust HLT Applications. In Proceed-
ings of the 40th Annual Meeting on Association for
Computational Linguistics, pages 168?175.
H. Daum?e III. 2004. Notes on CG and LM-
BFGS optimization of logistic regression. Pa-
per available at http://pub.hal3.name#
daume04cg-bfgs, implementation available at
http://hal3.name/megam/, August.
L. Derczynski, D. Maynard, N. Aswani, and
K. Bontcheva. 2013. Microblog-Genre Noise and
Impact on Semantic Annotation Accuracy. In Pro-
ceedings of the 24th ACM Conference on Hypertext
and Social Media. ACM.
J. Eisenstein. 2013. What to do about bad language
on the internet. In Proceedings of the 2013 Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 359?369. Association for
Computational Linguistics.
T. Finin, W. Murnane, A. Karandikar, N. Keller, J. Mar-
tineau, and M. Dredze. 2010. Annotating named
entities in Twitter data with crowdsourcing. In Pro-
ceedings of the NAACL HLT 2010 Workshop on Cre-
ating Speech and Language Data with Amazon?s
Mechanical Turk, pages 80?88.
J. Finkel, T. Grenager, and C. Manning. 2005. In-
corporating non-local information into information
extraction systems by Gibbs sampling. In Proceed-
ings of the 43rd Annual Meeting of the Association
for Computational Linguistics, pages 363?370. As-
sociation for Computational Linguistics.
P. Gadde, L. Subramaniam, and T. A. Faruquie. 2011.
Adapting a WSJ trained part-of-speech tagger to
noisy text: preliminary results. In Proceedings of
the 2011 Joint Workshop on Multilingual OCR and
Analytics for Noisy Unstructured Text Data. ACM.
S. Green, J. Heer, and C. D. Manning. 2013. The effi-
cacy of human post-editing for language translation.
In Proceedings of the SIGCHI Conference on Hu-
man Factors in Computing Systems, pages 439?448.
ACM.
T. Joachims. 1999. Svmlight: Support vector machine.
SVM-Light Support Vector Machine http://svmlight.
joachims. org/, University of Dortmund, 19(4).
J. Lafferty, A. McCallum, and F. Pereira. 2001. Condi-
tional Random Fields: Probabilistic Models for Seg-
menting and Labeling Sequence Data. In Proceed-
ings of the Eighteenth International Conference on
Machine Learning, pages 282?289, San Francisco:
Morgan Kaufmann.
Y. Li, K. Bontcheva, and H. Cunningham. 2009.
Adapting SVM for Data Sparseness and Imbalance:
A Case Study on Information Extraction. Natural
Language Engineering, 15(2):241?271.
C. Li, J. Weng, Q. He, Y. Yao, A. Datta, A. Sun, and
B.-S. Lee. 2012. Twiner: named entity recogni-
tion in targeted twitter stream. In Proceedings of
the 35th international ACM SIGIR conference on
Research and development in information retrieval,
pages 721?730. ACM.
X. Liu, S. Zhang, F. Wei, and M. Zhou. 2011. Rec-
ognizing named entities in tweets. In Proceedings
of the 49th Annual Meeting of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 359?367.
K. Morik, P. Brockhausen, and T. Joachims. 1999.
Combining statistical learning with a knowledge-
based approach-a case study in intensive care moni-
toring. In ICML, volume 99, pages 268?277.
D. Nadeau and S. Sekine. 2007. A survey of named
entity recognition and classification. Lingvisticae
Investigationes, 30(1):3?26.
N. Okazaki. 2007. CRFsuite: a fast implementation of
Conditional Random Fields (CRFs).
L. Ratinov and D. Roth. 2009. Design challenges and
misconceptions in named entity recognition. In Pro-
ceedings of the Thirteenth Conference on Computa-
tional Natural Language Learning, pages 147?155.
Association for Computational Linguistics.
A. Ritter, S. Clark, Mausam, and O. Etzioni. 2011.
Named entity recognition in tweets: An experimen-
tal study. In Proc. of Empirical Methods for Natural
Language Processing (EMNLP), Edinburgh, UK.
E. F. T. K. Sang and F. D. Meulder. 2003. Introduc-
tion to the CoNLL-2003 Shared Task: Language-
Independent Named Entity Recognition. In Pro-
ceedings of CoNLL-2003, pages 142?147. Edmon-
ton, Canada.
73
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 645?650,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Temporal Signals Help Label Temporal Relations
Leon Derczynski and Robert Gaizauskas
Natural Language Processing Group
Department of Computer Science
University of Sheffield
211 Portobello, S1 4DP, Sheffield, UK
{leon,robertg}@dcs.shef.ac.uk
Abstract
Automatically determining the temporal order
of events and times in a text is difficult, though
humans can readily perform this task. Some-
times events and times are related through use
of an explicit co-ordination which gives infor-
mation about the temporal relation: expres-
sions like ?before? and ?as soon as?. We in-
vestigate the ro?le that these co-ordinating tem-
poral signals have in determining the type of
temporal relations in discourse. Using ma-
chine learning, we improve upon prior ap-
proaches to the problem, achieving over 80%
accuracy at labelling the types of temporal re-
lation between events and times that are re-
lated by temporal signals.
1 Introduction
It is important to understand time in language. The
ability to express and comprehend expressions of time
enables us to plan, to tell stories, and to discuss change
in the world around us.
When we automatically extract temporal informa-
tion, we are often concerned with events and times ? re-
ferred to collectively as temporal intervals. We might
ask, for example, ?Who is the current President of the
USA?.? In order to extract an answer to this question
from a document collection, we need to identify events
related to persons becoming president and the times of
those events. Crucially, however, we also need to iden-
tify the temporal relations between these events and
times, perhaps, for example, by recognizing a tempo-
ral relation type from a set such as that of Allen (1983).
This last task, temporal relation typing, is challeng-
ing, and is the focus of this paper.
Temporal signals are words or phrases that act as
discourse markers that co-ordinate a pair of events or
times and explicitly state the nature of the temporal re-
lation that holds between them. For example, in ?The
parade reached the town hall before noon?, the word
before is a temporal signal, co-ordinating the event
reached with the time noon. Intuitively, these signal
words act as discourse contain temporal ordering infor-
mation that human readers can readily access, and in-
deed this hypothesis is borne out empirically (Bestgen
and Vonk, 1999). In this paper, we present an in-depth
examination into the role temporal signals can play in
machine learning for temporal relation typing, within
the framework of TimeML (Pustejovsky et al, 2005).
2 Related Work
Temporal relation typing is not a new problem. Clas-
sical work using TimeML is that of Boguraev and
Ando (2005), Mani et al (2007) and Yoshikawa et al
(2009). The TempEval challenge series features re-
lation typing as a key task (Verhagen et al, 2009).
The take-home message from all this work is that tem-
poral relation typing is a hard problem, even using
advanced techniques and extensive engineering ? ap-
proaches rarely achieve over 60% on typing relations
between two events or over 75% accuracy for those be-
tween an event and a time. Recent attempts to include
more linguistically sophisticated features representing
discourse, syntactic and semantic role information have
yielded but marginal improvements, e.g. Llorens et al
(2010); Mirroshandel et al (2011).
Although we focus solely on determining the types
of temporal relations, one must also identify which
pairs of temporal intervals should be temporally re-
lated. Previous work has covered the tasks of identi-
fying and typing temporal relations jointly with some
success (Denis and Muller, 2011; Do et al, 2012). The
TempEval3 challenge addresses exactly this task (Uz-
Zaman et al, 2013).
Investigations into using signals for temporal rela-
tion typing have had promising results. Lapata and
Lascarides (2006) learn temporal structure according
to these explicit signals, then predict temporal order-
ings in sentences without signals. As part of an early
TempEval system, Min et al (2007) automatically an-
notate signals and associate them with temporal rela-
tions. They then include the signal text as a feature
for a relation type classifier. Their definition of sig-
nals varies somewhat from the traditional TimeML sig-
645
Event-event relations Event-time relations
Non-signalled Signalled Overall Non-signalled Signalled Overall
Baseline most-common-class 41.4% 57.4% 43.0% 49.2% 51.6% 49.6%
Maxent classifier 57.7% 58.6% 57.8% 81.4% 59.6% 77.3%
Error reduction 27.8% 2.74% 25.4% 64.5% 16.4% 55.5%
Sample size (number of relations) 3 179 343 3 522 2 299 529 2 828
Table 1: Relation typing performance using the base feature set, for relations with and without a temporal signal.
nal definition, as they include words such as reporting
which would otherwise be annotated as an event. The
system achieves a 22% error reduction on a simplified
set of temporal relation types.
Later, Derczynski and Gaizauskas (2010) saw a 50%
error reduction in assignment of relation types on sig-
nalled relation instances from introducing simple fea-
tures describing a temporal signal?s interaction with the
events or times that it co-ordinates. The features for de-
scribing signals included the signal text itself and the
signal?s position in the document relative to the inter-
vals it co-ordinated. This led to a large increase in re-
lation typing accuracy to 82.19% for signalled event-
event relations, using a maximum entropy classifier.
Previous work has attempted to linguistically charac-
terise temporal signals (Bre?e et al, 1993; Derczynski
and Gaizauskas, 2011). Signal phrases typically fall
into one of three categories: monosemous as temporal
signals (e.g. ?during?, ?when?); bisemous as temporal
or spatial signals (e.g. ?before?); or polysemous with
the temporal sense a minority class (e.g. ?in?, ?fol-
lowing?). Further, a signal phrase may take two argu-
ments, though its arguments need not be in the imme-
diate content and may be anaphoric. We leave the task
of automatic signal annotation to future work, instead
focusing on the impact that signals have on temporal
relation typing.
Our work builds on previous work by expanding the
study to include relations other than just event-event
relations, by extending the feature set, by doing tem-
poral relation labelling over a more carefully curated
version of the TimeBank corpus (see below), and by
providing detailed analysis of the performance of a set
of labelling techniques when using temporal signals.
3 Experimental Setup
We only approach the relation typing task, and we use
existing signal annotations ? that is, we do not attempt
to automatically identify temporal signals.
The corpus used is the signal-curated version of
TimeBank (Pustejovsky et al, 2003). This corpus, TB-
sig,1 adds extra events, times and relations to Time-
Bank, in an effort to correct signal under-annotation in
the original corpus (Derczynski and Gaizauskas, 2011).
Like the original TimeBank corpus, it comprises 183
documents. In these, we are interested only in the tem-
poral relations that use a signal. There are 851 signals
annotated in the corpus, co-ordinating 886 temporal re-
1See http://derczynski.com/sheffield/resources/tb sig.tar.bz2
lations (13.7% of all). For comparison, TimeBank has
688 signal annotations which co-ordinate 718 temporal
relations (11.2%).
When evaluating classifiers, we performed 10-fold
cross-validation, keeping splits at document level.
There are only 14 signalled time-time relations in this
corpus, which is not enough to support any generaliza-
tions, and so we disregard this interval type pairing.
As is common with statistical approaches to tempo-
ral relation typing, we also perform relation folding;
that is, to reduce the number of possible classes, we
sometimes invert argument order and relation type. For
example, A BEFORE B and B AFTER A convey the
same temporal relation, and so we can remove all AF-
TER-type relations by swapping their argument order
and converting them to BEFORE relations. This loss-
less process condenses the labels that our classifier has
to distinguish between, though classification remains a
multi-class problem.
We adopt the base feature set of Mani et al (2007),
which consists mainly of TimeML event and time
annotation surface attributes. These are, for events:
class, aspect, modality, tense, polarity, part
of speech; and, for times: value, type, function
in document, mod, quant. To these are added
same-tense and same-aspect features, as well as
the string values of events/times.
The feature groups we use here are:
? Base ? The attributes of TimeML annotations in-
volved (includes tense, aspect, polarity and so on
as above), as with previous approaches.
? Argument Ordering ? Two features: a boolean
set if both arguments are in the same sentence (as
in Chambers et al (2007)), and the text order of
argument intervals (as in Hepple et al (2007)).
? Signal Ordering ? Textual ordering is important
with temporal signals; compare ?You walk before
you run? and ?Before you walk you run?. We
add features accounting for relative textual posi-
tion of signal and arguments as per Derczynski
and Gaizauskas (2010). To these we add a feature
reporting whether the signal occurs in first, last,
or mid-sentence position, and features to indicate
whether each interval is in the same sentence as
the signal.
? Syntactic ? We add syntactic features: fol-
lowing Bethard et al (2007), the lowest com-
mon constituent label between each argument and
646
Features Classifier Event-event accuracy Event-time accuracy
N/A Baseline most-common-class 57.4% 51.6%
Base Baseline maximum entropy 58.6% 59.6%
Maximum entropy 72.6% 72.4%DG2010 Random forest 76.7% 78.6%
All
Adaptive boosting 70.4% 73.0%
Na??ve Bayes 73.8% 71.5%
Maximum entropy 75.5% 78.1%
Linear SVC / Crammer-Singer 79.3% 75.6%
Linear SVC 80.7% 77.1%
Random forest 80.8% 80.3%
Table 2: Results at temporal relation typing over TB-sig, for relations that use a temporal signal
the signal; following Swampillai and Stevenson
(2011), the syntactic path from each argument
to the signal, using a top-level ROOT node for
cross-sentence paths; and three features indicat-
ing whether there is a temporal function tag (-TMP
between each of the intervals or the signal to the
root note. These features are generated using the
Stanford parser (Klein and Manning, 2003) and a
function tagger (Blaheta and Charniak, 2000).
? Signal Text ? We add the signal?s raw string, as
well as its lower-case version and its lemma.
? DCT ? For event-time relations, whether the time
expression also functions as the document?s cre-
ation timestamp.
Collectively, these feature groups comprise the All
feature set. For comparison, the feature set we reported
in previous work (Derczynski and Gaizauskas, 2010)
is also included, labeled DG2010. This set contains the
base and the signal ordering feature groups only, plus a
single signal feature for the signal raw string.
Using these feature representations we trained multi-
nomial na??ve Bayes (Rennie et al, 2003), maximum
entropy (Daume? III, 2008), adaptive boosting (Fre-
und and Schapire, 1997; Zhu et al, 2009), multi-class
SVM (Crammer and Singer, 2002; Chang and Lin,
2011) and random forest2 (Breiman, 2001) classifiers
via Scikit-learn (Pedregosa et al, 2011).
We use two baselines: most-common-class and a
model trained with no signal features. We also in-
troduce two measures replicating earlier work: one
using the DG2010 features and the classifier used in
that work (maximum entropy), and another using the
DG2010 features with the best-performing classifier
under our All feature set, in order to see if performance
changes are due to features or classifier.
Classifiers were evaluated by determining if the class
they output matched the relation type in TB-sig. Re-
sults are given in Table 2. For comparison with the
general case, i.e. for both signalled and non-signalled
temporal relation instances, we list performance with
a maximum entropy classifier and the base feature set
2With nestimators = 200, a minimum of one sample per
node, and no maximum depth.
Figure 1: Effect of training data size on relation typing
performance.
on TB-sig?s temporal relations. Results are in Table 1.
These are split into those that use a signal and those that
do not, though no features relaying signal information
are included.
In order to assess the adequacy of the dataset in
terms of size, we also examined performance using a
maximum entropy classifier learned from varying sub-
proportions of the training data. This was measured
over event-event relations, using all features. Results
are given in Figure 1. That performance appears to sta-
bilise and level off indicates that the training set is of
sufficient size for these experiments.
4 Analysis
The results in Table 2 echo earlier findings and intu-
ition: temporal signals are useful in temporal relation
typing. Results support that signals are not only helpful
in event-event relation typing but also event-time typ-
ing. For comparison, inter-annotator agreement across
all temporal relation labels, i.e. signalled and non-
signalled relations, in TimeBank is 77%.
Using the maximum entropy classifier, our approach
gives a 2.9% absolute performance increase over the
DG2010 feature set for event-event relations (10.6% er-
ror reduction) and a 5.7% absolute increase for event-
time relations (20.7% error reduction). Random forests
647
Feature sets Evt-evt Evt-time
All 80.8% 80.3%
All-argument order 80.8% 78.3%
All-signal order 79.0% 77.5%
All-syntax 79.2% 79.6%
All-signal text 70.8% 72.7%
All-DCT 79.9% 79.4%
Base 54.2% 53.9%
Base+argument order 56.8% 60.1%
Base+signal order 59.7% 65.0%
Base+syntax 70.0% 71.0%
Base+signal text 75.5% 66.3%
Base+DCT 54.2% 53.9%
Base+signal text+signal order 80.4% 76.9%
Base+signal text+syntax 79.0% 74.1%
Base+arg order+signal order 77.8% 75.2%
Table 3: Relation typing accuracy based on various fea-
ture combinations, using random forests. Bold figures
indicate the largest performance change.
offer better performance under both feature sets, with
the extended features achieving notable error reduction
over DG2010 ? 17.6% for event-event, 7.9% for event-
time relations. Linear support vector classification pro-
vided rapid labelling and comparable performance for
event-event relations but was accuracy was not as good
as random forests for event-time relation labelling.
Note, figures reported earlier in Derczynski and
Gaizauskas (2010) are not directly comparable to the
DG2010 figures reported here, as here we are using the
better-annotated TB-sig corpus, which contains a larger
and more varied set of temporal signal annotations.
Although we are only examining the 13.7% of tem-
poral relations that are co-ordinated with a signal, it
is important to note the performance of conventional
classification approaches on this subset of temporal
relations. Specifically, the error reduction relative to
the baseline that is achieved without signal features is
much lower on relations that use signals than on non-
signalled relations (Table 1). Thus, temporal relations
that use a signal appear to be more difficult to clas-
sify than other relations, unless signal information is
present in the features. This may be due to differences
in how signals are used by authors. One explanation
is that signals may be used in the stead of temporal or-
dering information in surrounding discourse, such as
modulations of dominant tense or aspect (Derczynski
and Gaizauskas, 2013).
Unlike earlier work using maxent, we experiment
with a variety of classifiers, and find a consistent im-
provement in temporal relation typing using signal fea-
tures. With the notable exception of adaptive boost-
ing, classifiers with preference bias (Liu et al, 2002)
? AdaBoost, random trees and SVC ? performed best
in this task. Conversely, those tending toward the in-
dependence assumption (na??ve Bayes and maxent) did
not capitalise as effectively on the training data.
Features Evt-evt Evt-time
All 80.8% 80.3%
All-signal text 70.8% 72.7%
All-signal text-argument order 70.7% 72.2%
All-signal text-signal order 69.5% 71.2%
All-signal text-syntax 59.5% 69.0%
All-signal text-DCT 70.8% 72.8%
Table 4: Feature ablation without signal text features.
Bold figures indicate largest performance change.
We also investigated the impact of each feature
group on the best-performing classifier (random forests
with n = 200) through feature ablation. Results are
given in Table 3. Ablation suggested that the signal text
features (signal string, lower case string, head word and
lemma) had most impact in event-event relation typing,
though were second to syntax features in event-time re-
lations. Removing other feature groups gave only mi-
nor performance decreases.
We also experimented with adding feature groups to
the base set one-by-one. All but DCT features gave
above-baseline improvement, though argument order-
ing features were not very helpful for event-event re-
lation typing. Signal text features gave the strongest
improvement over baseline for event-event relations,
but syntax gave a larger improvement for event-time
relations. Accordingly, it may be useful to distinguish
between event-event and event-time relations when ex-
tracting temporal information using syntax (c.f. the ap-
proach of Wang et al (2010)).
A strong above-baseline performance was still ob-
tained even when signal text features were removed,
which included the signal text itself. This was interest-
ing, as signal phrases can indicate quite different tem-
poral orderings (e.g. ?Open the box while it rains? vs.
?Open the box before it rains?, and the words used are
typically critical to correct interpretation of the tempo-
ral relation. Further, the model is able to generalise
beyond particular signal phrase choices. To investigate
further, we examined the performance impact of each
group sans ?signal text? features (Table 4). In this case,
removing the syntactic features had the greatest (neg-
ative) impact on performance, though the absolute im-
pact on event-event relations (a drop of 11.3%) was far
lower than that on event-time relations (3.7%).
To examine helpful features, we trained a max-
ent classifier on the entire dataset and collected fea-
ture:value pairs. These were then ranked by their
weight. The ten largest-weighted pairings for event-
event relations (the hardest problem in overall temporal
relation typing) are given in Table 5. Prefixes of 1- and
2- correspond to the two interval arguments (events).
Negative values are those where the presence of a par-
ticular feature:value pair suggests the mentioned class
is not applicable.
648
Weight Feature Value Class
9.346 2-polarity POS ENDS
-8.713 1-2-same-sent True BEGINS
-7.861 2-aspect NONE BEGINS
-7.256 1-aspect NONE INCLUDES
6.564 2-sig-synt-path NN-NP-IN INCLUDES
6.519 signal-lower before ENDS
-6.294 2-tense NONE BEGINS
-5.908 2-modality None ENDS
5.643 2-text took BEGINS
-5.580 1-modality None ENDS
Table 5: Top ten largest-weighted feature:value pairs.
It can be seen that BEGINS and INCLUDES rela-
tionships are not indicated if the arguments have no
TimeML aspect assigned; this is what one might ex-
pect, given how aspect is used in English, with these
temporal relation types corresponding to event starts
and the progressive. Also, notice how a particular syn-
tactic path, connecting adjacent nominalised event and
the word in acting as a signal, indicate a temporal inclu-
sion relationship. Temporal polysemy, where a word
has more than one possible temporal interpretation,
is also observable here (Derczynski and Gaizauskas
(2011) examine this polysemy in depth). This is vis-
ible in how the temporal signal phrase ?before? is not,
as one might expect, a strong indicator of a BEFORE or
even AFTER relation, but of an ENDS relationship.
5 Conclusion
This paper set out to investigate the ro?le of temporal
signals in predicting the type of temporal relation be-
tween two intervals. The paper demonstrated the util-
ity of temporal signals in this task, and identified ap-
proaches for using the information these signals con-
tain, which performed consistently better than the state-
of-the-art across a range of machine learning classi-
fiers. Further, it identified the impact that signal text,
signal order and syntax features had in temporal rela-
tion typing of signalled relations.
Two directions of future work are indicated. Firstly,
the utility of signals prompts investigation into detect-
ing which words in a given text occur as temporal sig-
nals. Secondly, it is intuitive that temporal signals ex-
plicitly indicate related pairs of intervals (i.e. events or
times). So, the task of deciding which interval pair(s) a
temporal signal co-ordinates must be approached.
Although we have found a method for achieving
good temporal relation typing performance on a subset
of temporal relations, the greater problem of general
temporal relation typing remains. A better understand-
ing of the semantics of events, times, signals and how
they are related together through syntax may provide
further insights into the temporal relation typing task.
Finally, Bethard et al (2007) reached high temporal
relation typing performance on one a subset of relations
(events and times in the same sentence); we reach high
temporal relation typing performance on another subset
of relations ? those using a temporal signal. Identify-
ing further explicit sources of temporal information ap-
plicable to new sets of relations may reveal promising
paths for investigation.
Acknowledgements
The first author was supported by UK EPSRC grant
EP/K017896/1, uComp (http://www.ucomp.eu/).
References
J. Allen. 1983. Maintaining knowledge about temporal
intervals. Communications of the ACM, 26(11):832?
843.
Y. Bestgen and W. Vonk. 1999. Temporal adverbials as
segmentation markers in discourse comprehension.
Journal of Memory and Language, 42(1):74?87.
S. Bethard, J. Martin, and S. Klingenstein. 2007.
Timelines from text: Identification of syntactic tem-
poral relations. In Proceedings of the International
Conference on Semantic Computing, pages 11?18.
D. Blaheta and E. Charniak. 2000. Assigning function
tags to parsed text. In Proceedings of the meeting
of the North American chapter of the Association for
Computational Linguistics, pages 234?240. ACL.
B. Boguraev and R. K. Ando. 2005. TimeBank-Driven
TimeML Analysis. In G. Katz, J. Pustejovsky, and
F. Schilder, editors, Annotating, Extracting and Rea-
soning about Time and Events, number 05151 in
Dagstuhl Seminar Proceedings, Dagstuhl, Germany.
Internationales Begegnungs- und Forschungszen-
trum fu?r Informatik (IBFI), Schloss Dagstuhl, Ger-
many.
D. Bre?e, A. Feddag, and I. Pratt. 1993. Towards a for-
malization of the semantics of some temporal prepo-
sitions. Time & Society, 2(2):219.
L. Breiman. 2001. Random forests. Machine Learn-
ing, 45(1):5?32.
N. Chambers, S. Wang, and D. Jurafsky. 2007. Clas-
sifying temporal relations between events. In Pro-
ceedings of the 45th meeting of the Association for
Computational Linguistics, pages 173?176. ACL.
C.-C. Chang and C.-J. Lin. 2011. LIBSVM: a library
for support vector machines. ACM Transactions on
Intelligent Systems and Technology, 2(3):27.
K. Crammer and Y. Singer. 2002. On the algorith-
mic implementation of multiclass kernel-based vec-
tor machines. The Journal of Machine Learning Re-
search, 2:265?292.
H. Daume? III. 2008. MegaM: Maximum entropy
model optimization package. ACL Data and Code
Repository, ADCR2008C003, 50.
649
P. Denis and P. Muller. 2011. Predicting globally-
coherent temporal structures from texts via endpoint
inference and graph decomposition. In Proceedings
of the International Joint Conference on Artificial In-
telligence, pages 1788?1793. AAAI Press.
L. Derczynski and R. Gaizauskas. 2010. Using Sig-
nals to Improve Automatic Classification of Tempo-
ral Relations. In Proceedings of 15th Student Ses-
sion of the European Summer School for Logic, Lan-
guage and Information, pages 224?231. FoLLI.
L. Derczynski and R. Gaizauskas. 2011. A Corpus-
based Study of Temporal Signals. In Proceedings of
the Corpus Linguistics Conference.
L. Derczynski and R. Gaizauskas. 2013. Empirical
Validation of Reichenbach?s Tense Framework. In
Proceedings of the 10th International Conference on
Computational Semantics, pages 71?82. ACL.
Q. X. Do, W. Lu, and D. Roth. 2012. Joint infer-
ence for event timeline construction. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing, pages 677?687. ACL.
Y. Freund and R. E. Schapire. 1997. A decision-
theoretic generalization of on-line learning and an
application to boosting. Journal of Computer and
System Sciences, 55(1):119?139.
M. Hepple, A. Setzer, and R. Gaizauskas. 2007.
USFD: preliminary exploration of features and clas-
sifiers for the TempEval-2007 tasks. In Proceedings
of the 4th International Workshop on Semantic Eval-
uations, pages 438?441. ACL.
D. Klein and C. D. Manning. 2003. Accurate unlex-
icalized parsing. In Proceedings of the 41st meet-
ing of the Association for Computational Linguistics,
pages 423?430. ACL.
M. Lapata and A. Lascarides. 2006. Learning
sentence-internal temporal relations. Journal of Ar-
tificial Intelligence Research, 27(1):85?117.
Y. Liu, Y. Yang, and J. Carbonell. 2002. Boosting to
correct inductive bias in text classification. In Pro-
ceedings of the 11th international Conference on In-
formation and Knowledge Management, pages 348?
355. ACM.
H. Llorens, E. Saquete, and B. Navarro. 2010. TIPSem
(English and Spanish): Evaluating CRFs and Se-
mantic Roles in TempEval-2. In Proceedings of
SemEval-2010. ACL.
I. Mani, B. Wellner, M. Verhagen, and J. Pustejovsky.
2007. Three approaches to learning TLINKS in
TimeML. Technical report, CS-07-268, Brandeis
University.
C. Min, M. Srikanth, and A. Fowler. 2007. LCC-TE:
A hybrid approach to temporal relation identification
in news text. In Proceedings of the 4th International
Workshop on Semantic Evaluations, pages 219?222.
ACL.
S. A. Mirroshandel, G. Ghassem-Sani, and
M. Khayyamian. 2011. Using syntactic-based
kernels for classifying temporal relations. Journal
of Computer Science and Technology, 26(1):68?80.
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer,
R. Weiss, V. Dubourg, et al 2011. Scikit-learn: Ma-
chine learning in Python. The Journal of Machine
Learning Research, 12:2825?2830.
J. Pustejovsky, R. Sauri, R. Gaizauskas, A. Setzer,
L. Ferro, et al 2003. The TimeBank Corpus. In
Proceedings of the Corpus Linguistics Conference,
pages 647?656.
J. Pustejovsky, J. Castano, R. Ingria, R. Saur??,
R. Gaizauskas, A. Setzer, G. Katz, and D. Radev.
2005. TimeML: Robust specification of event and
temporal expressions in text. In I. Mani, J. Puste-
jovsky, and R. Gaizauskas, editors, The language of
time: a reader. Oxford University Press.
J. D. Rennie, L. Shih, J. Teevan, and D. Karger. 2003.
Tackling the Poor Assumptions of Naive Bayes Text
Classifiers. In Proceedings of the International Con-
ference on Machine Learning. AAAI Press.
K. Swampillai and M. Stevenson. 2011. Extracting re-
lations within and across sentences. In Proceedings
of the International Conference Recent Advances in
Natural Language Processing, pages 25?32. ACL.
N. UzZaman, H. Llorens, L. Derczynski, M. Verhagen,
J. F. Allen, and J. Pustejovsky. 2013. SemEval-2013
Task 1: TempEval-3: Evaluating Time Expressions,
Events, and Temporal Relations. In Proceedings of
the 7th International Workshop on Semantic Evalu-
ations.
M. Verhagen, R. Gaizauskas, F. Schilder, M. Hep-
ple, J. Moszkowicz, and J. Pustejovsky. 2009.
The TempEval challenge: identifying temporal re-
lations in text. Language Resources and Evaluation,
43(2):161?179.
W. Wang, J. Su, and C. L. Tan. 2010. Kernel based
discourse relation recognition with temporal order-
ing information. In Proceedings of the 48th meet-
ing of the Association for Computational Linguistics,
pages 710?719. ACL.
K. Yoshikawa, S. Riedel, M. Asahara, and Y. Mat-
sumoto. 2009. Jointly identifying temporal relations
with Markov logic. In Proceedings of the Interna-
tional Joint Conference on Natural Language Pro-
cessing, pages 405?413. ACL.
J. Zhu, H. Zou, S. Rosset, and T. Hastie. 2009. Multi-
class AdaBoost. Statistics and Its Interface, 2:349?
360.
650
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 337?340,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
USFD2: Annotating Temporal Expresions and TLINKs for TempEval-2
Leon Derczynski
Dept of Computer Science
University of Sheffield
Regent Court
211 Portobello
Sheffield S1 4DP, UK
leon@dcs.shef.ac.uk
Robert Gaizauskas
Dept of Computer Science
University of Sheffield
Regent Court
211 Portobello
Sheffield S1 4DP, UK
robertg@dcs.shef.ac.uk
Abstract
We describe the University of Sheffield
system used in the TempEval-2 challenge,
USFD2. The challenge requires the au-
tomatic identification of temporal entities
and relations in text.
USFD2 identifies and anchors temporal
expressions, and also attempts two of the
four temporal relation assignment tasks.
A rule-based system picks out and an-
chors temporal expressions, and a max-
imum entropy classifier assigns temporal
link labels, based on features that include
descriptions of associated temporal signal
words. USFD2 identified temporal expres-
sions successfully, and correctly classified
their type in 90% of cases. Determin-
ing the relation between an event and time
expression in the same sentence was per-
formed at 63% accuracy, the second high-
est score in this part of the challenge.
1 Introduction
The TempEval-2 (Pustejovsky and Verhagen,
2009) challenge proposes six tasks. Our system
tackles three of these: task A ? identifying time ex-
pressions, assigning TIMEX3 attribute values, and
anchoring them; task C ? determining the tempo-
ral relation between an event and time in the same
sentence; and task E ? determining the temporal
relation between two main events in consecutive
sentences. For our participation in the task, we
decided to employ both rule- and ML-classifier-
based approaches. Temporal expressions are dealt
with by sets of rules and regular expressions, and
relation labelling performed by NLTK?s1 maxi-
mum entropy classifier with rule-based processing
applied during feature generation. The features
(described in full in Section 2) included attributes
1See http://www.nltk.org/ .
from the TempEval-2 training data annotation,
augmented by features that can be directly derived
from the annotated texts. There are two main aims
of this work: (1) to create a rule-based tempo-
ral expression annotator that includes knowledge
from work published since GUTime (Mani and
Wilson, 2000) and measure its performance, and
(2) to measure the performance of a classifier that
includes features based on temporal signals.
Our entry to the challenge, USFD2, is a succes-
sor to USFD (Hepple et al, 2007). In the rest of
this paper, we will describe how USFD2 is con-
structed (Section 2), and then go on to discuss
its overall performance and the impact of some
internal parameters on specific TempEval tasks.
Regarding classifiers, we found that despite us-
ing identical feature sets across relation classifi-
cation tasks, performance varied significantly. We
also found that USFD2 performance trends with
TempEval-2 did not match those seen when clas-
sifiers were trained on other data while perform-
ing similar tasks. The paper closes with comments
about future work.
2 System Description
The TempEval-2 training and test sets are parti-
tioned into data for entity recognition and descrip-
tion, and data for temporal relation classification.
We will first discuss our approach for temporal ex-
pression recognition, description and anchoring,
and then discuss our approach to two of the re-
lation labelling tasks.
2.1 Identifying, describing and anchoring
temporal expressions
Task A of TempEval-2 requires the identification
of temporal expressions (or timexes) by defining
a start and end boundary for each expression, and
assigning an ID to it. After this, systems should
attempt to describe the temporal expression, de-
termining its type and value (described below).
337
Our timex recogniser works by building a set of
n-grams from the data to be annotated (1 ? n ?
5), and comparing each n-gram against a hand-
crafted set of regular expressions. This approach
has been shown to achieve high precision, with re-
call increasing in proportion to ruleset size (Han
et al, 2006; Mani and Wilson, 2000; Ahn et al,
2005). The recogniser chooses the largest possible
sequence of words that could be a single temporal
expression, discarding any sub-parts that indepen-
dently match any of our set of regular expressions.
The result is a set of boundary-pairs that describe
temporal expression locations within documents.
This part of the system achieved 0.84 precision
and 0.79 recall, for a balanced f1-measure of 0.82.
The next part of the task is to assign a type
to each temporal expression. These can be one
of TIME, DATE, DURATION, or SET. USFD2
only distinguishes between DATE and DURATION
timexes. If the words for or during occur in the
three words before the timex, the timex ends with
an s (such as in seven years), or the timex is a bi-
gram whose first token is a (e.g. in a month), then
the timex is deemed to be of type DURATION; oth-
erwise it is a DATE. These three rules for deter-
mining type were created based on observation of
output over the test data, and are correct 90% of
the time with the evaluation data.
The final part of task A is to provide a value
for the timex. As we only annotate DATEs
and DURATIONs, these will be either a fixed
calendrical reference in the format YYYY-MM-
DD, or a duration in according to the TIMEX2
standard (Ferro et al, 2005). Timex strings of
today or now were assigned the special value
PRESENT REF, which assumes that today is be-
ing used in a literal and not figurative manner, an
assumption which holds around 90% of the time
in newswire text (Ahn et al, 2005) such as that
provided for TempEval-2. In an effort to calcu-
late a temporal distance from the document cre-
ation time (DCT), USFD2 then checks to see if
numeric words (e.g. one, seven hundred) are in
the timex, as well as words like last or next which
determine temporal offset direction. This distance
figure supplies either the second parameter to a
DURATION value, or helps calculate DCT offset.
Strings that describe an imprecise amount, such as
few, are represented in duration values with an X,
as per the TIMEX2 standard. We next search the
timex for temporal unit strings (e.g. quarter, day).
Table 1: Features used by USFD2 to train a tem-
poral relation classifier.
Feature Type
For events
Tense String
Aspect String
Polarity pos or neg
Modality String
For timexes
Type Timex type
Value String
Describing signals
Signal text String
Signal hint Relation type
Arg 1 before signal? Boolean
Signal before Arg 2? Boolean
For every relation
Arguments are same tense Boolean
Arguments are same aspect Boolean
Arg 1 before Arg 2? Boolean
For every interval
Token number in sentence / 5 Integer
Text annotated String
Interval type event or timex
This helps build either a duration length or an off-
set. If we are anchoring a date, the offset is applied
to DCT, and date granularity adjusted according to
the coarsest temporal primitive present ? for ex-
ample, if DCT is 1997-06-12 and our timex is six
months ago, a value of 1997-01 is assigned, as it is
unlikely that the temporal expression refers to the
day precisely six months ago, unless followed by
the word today.
Where weekday names are found, we used
Baldwin?s 7-day window (Baldwin, 2002) to an-
chor these to a calendrical timeline. This tech-
nique has been found to be accurate over 94%
of the time with newswire text (Mazur and Dale,
2008). Where dates are found that do not specify
a year or a clear temporal direction marker (e.g.,
April 17 vs. last July), our algorithm counts the
number of days between DCT and the next oc-
currence of that date. If this is over a limit f ,
then the date is assumed to be last year. This is
a very general rule and does not take into account
the tendency of very-precisely-described dates to
be closer to DCT, and far off dates to be loosely
specified. An f of 14 days gives the highest per-
formance based on the TempEval-2 training data.
Anchoring dates / specifying duration lengths
was the most complex part of task A and our na??ve
rule set was correct only 17% of the time.
338
Table 2: A sample of signals and the TempEval-2
temporal relation they suggest.
Signal phrase Suggested relation
previous AFTER
ahead of BEFORE
so far OVERLAP
thereafter BEFORE
in anticipation of BEFORE
follows AFTER
since then BEFORE
soon after AFTER
as of OVERLAP-OR-AFTER
throughout OVERLAP
2.2 Labelling temporal relations
Our approach for labelling temporal relations (or
TLINKs) is based on NLTK?s maximum en-
tropy classifier, using the feature sets initially pro-
posed in Mani et al (2006). Features that de-
scribe temporal signals have been shown to give
a 30% performance boost in TLINKs that em-
ploy a signal (Derczynski and Gaizauskas, 2010).
Thus, the features in Mani et al (2006) are aug-
mented with those used to describe signals de-
tailed in Derczynski and Gaizauskas (2010), with
some slight changes. Firstly, as there are no spe-
cific TLINK/signal associations in the TempEval-
2 data (unlike TimeBank (Pustejovsky et al,
2003)), USFD2 needs to perform signal identifi-
cation and then associate signals with a temporal
relation between two events or timexes. Secondly,
a look-up list is used to provide TLINK label hints
based on a signal word. A list of features em-
ployed by USFD2 is in Table 1.
We used a simplified version of the approach
in Cheng et al (2007) to identify signal words.
This involved the creation of a list of signal
phrases that occur in TimeBank with a frequency
of 2 or more, and associating a signal from this list
with a temporal entity if it is in the same sentence
and clause. The textually nearest signal is chosen
in the case of conflict.
As this list of signal phrases only contained 42
entries, we also decided to define a ?most-likely?
temporal relation for each signal. This was done
by imagining a short sentence of the form event1
? signal ? event2, and describing the type of re-
lation between event 1 and event 2. An excerpt
from these entries is shown in Table 2. The hint
from this table was included as a feature. Deter-
mining whether or not to invert the suggested rela-
tion type based on word order was left to the clas-
sifier, which is already provided with word order
features. It would be possible to build these sug-
gestions from data such as TimeBank, but a num-
ber of problems stand in the way; the TimeML and
TempEval-2 relation types are not identical, word
order often affects the actual relationship type sug-
gested by a signal (e.g. compare He ran home
before he showered and Before he ran home, he
showered), and noise in mined data is a problem
with the low corpus occurrence frequency of most
signals.
This approach was used for both the intra-
sentence timex/event TLINK labelling task and
also the task of labelling relations between main
events in adjacent sentences.
3 Discussion
USFD2?s rule-based element for timex identifica-
tion and description performs well, even achieving
above-average recall despite a much smaller rule
set than comparable and more complex systems.
However, the temporal anchoring component per-
forms less strongly. The ?all-or-nothing? metric
employed for evaluating the annotation of timex
values gives non-strict matches a zero score (e.g.
if the expected answer is 1990-05-14, no reward is
given for 1990-05) even if values are close, which
many were.
In previous approaches that used a maxi-
mum entropy classifier and comparable feature
set (Mani et al, 2006; Derczynski and Gaizauskas,
2010), the accuracy of event-event relation classi-
fication was higher than that of event-timex clas-
sification. Contrary to this, USFD2?s event-event
classification of relations between main events
of successive sentences (Task E) was less accu-
rate than the classification of event-timex rela-
tions between events and timexes in the same sen-
tence (Task C). Accuracy in Task C was good
(63%), despite the lack of explicit signal/TLINK
associations and the absence of a sophisticated
signal recognition and association mechanism.
This is higher than USFD2?s accuracy in Task
E (45%) though the latter is a harder task, as
most TempEval-2 systems performed significantly
worse at this task than event/timex relation classi-
fication.
Signal information was not relied on by many
TempEval 2007 systems (Min et al (2007) dis-
339
cusses signals to some extent but the system de-
scribed only includes a single feature ? the sig-
nal text), and certainly no processing of this data
was performed for that challenge. USFD2 begins
to leverage this information, and gives very com-
petitive performance at event/timex classification.
In this case, the signals provided an increase from
61.5% to 63.1% predictive accuracy in task C. The
small size of the improvement might be due to the
crude and unevaluated signal identification and as-
sociation system that we implemented.
The performance of classifier based approaches
to temporal link labelling seems to be levelling
off ? the 60%-70% relation labelling accuracy of
work such as Mani et al (2006) has not been
greatly exceeded. This performance level is still
the peak of the current generation of systems. Re-
cent improvements, while employing novel ap-
proaches to the task that rely on constraints be-
tween temporal link types or on complex linguistic
information beyond that describable by TimeML
attributes, still yield marginal improvements (e.g.
Yoshikawa et al (2009)). It seems that to break
through this performance ?wall?, we need to con-
tinue to innovate with and discuss temporal re-
lation labelling, using information and knowl-
edge from many sources to build practical high-
performance systems.
4 Conclusion
In this paper, we have presented USFD2, a novel
system that annotates temporal expressions and
temporal links in text. The system relies on
new hand-crafted rules, existing rule sets, machine
learning and temporal signal information to make
its decisions. Although some of the TempEval-2
tasks are difficult, USFD2 manages to create good
and useful annotations of temporal information.
USFD2 is available via Google Code2.
Acknowledgments
Both authors are grateful for the efforts of the
TempEval-2 team and appreciate their hard work.
The first author would like to acknowledge the
UK Engineering and Physical Science Research
Council for support in the form of a doctoral stu-
dentship.
2See http://code.google.com/p/usfd2/ .
References
D. Ahn, S.F. Adafre, and MD Rijke. 2005. Towards
task-based temporal extraction and recognition. In
Dagstuhl Seminar Proceedings, volume 5151.
J.A. Baldwin. 2002. Learning temporal annotation of
French news. Ph.D. thesis, Georgetown University.
Y. Cheng, M. Asahara, and Y. Matsumoto. 2007.
Temporal relation identification using dependency
parsed tree. In Proceedings of the 4th International
Workshop on Semantic Evaluations, pages 245?248.
L. Derczynski and R. Gaizauskas. 2010. Using sig-
nals to improve automatic classification of temporal
relations. In Proceedings of the ESSLLI StuS. Sub-
mitted.
L. Ferro, L. Gerber, I. Mani, B. Sundheim, and G. Wil-
son. 2005. TIDES 2005 standard for the annotation
of temporal expressions. Technical report, MITRE.
B. Han, D. Gates, and L. Levin. 2006. From language
to time: A temporal expression anchorer. In Tem-
poral Representation and Reasoning (TIME), pages
196?203.
M. Hepple, A. Setzer, and R. Gaizauskas. 2007.
USFD: preliminary exploration of features and clas-
sifiers for the TempEval-2007 tasks. In Proceedings
of SemEval-2007, pages 438?441.
I. Mani and G. Wilson. 2000. Robust temporal pro-
cessing of news. In Proceedings of the 38th Annual
Meeting on ACL, pages 69?76. ACL.
I. Mani, M. Verhagen, B. Wellner, C.M. Lee, and
J. Pustejovsky. 2006. Machine learning of tem-
poral relations. In Proceedings of the 21st Inter-
national Conference on Computational Linguistics,
page 760. ACL.
P. Mazur and R. Dale. 2008. Whats the date? High
accuracy interpretation of weekday. In 22nd Inter-
national Conference on Computational Linguistics
(Coling 2008), Manchester, UK, pages 553?560.
C. Min, M. Srikanth, and A. Fowler. 2007. LCC-TE:
a hybrid approach to temporal relation identification
in news text. In Proceedings of the 4th International
Workshop on Semantic Evaluations, pages 219?222.
J. Pustejovsky and M. Verhagen. 2009. SemEval-2010
task 13: evaluating events, time expressions, and
temporal relations (TempEval-2). In Proceedings of
the Workshop on Semantic Evaluations, pages 112?
116. ACL.
J. Pustejovsky, P. Hanks, R. Sauri, A. See,
R. Gaizauskas, A. Setzer, D. Radev, D. Day,
L. Ferro, et al 2003. The Timebank Corpus. In
Corpus Linguistics, volume 2003, page 40.
K. Yoshikawa, S. Riedel, M. Asahara, and Y. Mat-
sumoto. 2009. Jointly identifying temporal rela-
tions with markov logic. In IJCNLP: Proceedings
of 47th Annual Meeting of the ACL, pages 405?413.
340
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 1?9, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
SemEval-2013 Task 1: TEMPEVAL-3: Evaluating Time Expressions,
Events, and Temporal Relations
Naushad UzZaman?1, Hector Llorens?1, Leon Derczynski?,
Marc Verhagen?, James Allen? and James Pustejovsky?
?: University of Rochester, USA; ?: University of Alicante, Spain
?: Department of Computer Science, University of Sheffield, UK
?: Computer Science Department, Brandeis University, USA
1: Nuance Communications
naushad@cs.rochester.edu, hllorens@dlsi.ua.es, leon@dcs.shef.ac.uk
Abstract
Within the SemEval-2013 evaluation exercise, the
TempEval-3 shared task aims to advance research
on temporal information processing. It follows on
from TempEval-1 and -2, with: a three-part struc-
ture covering temporal expression, event, and tem-
poral relation extraction; a larger dataset; and new
single measures to rank systems ? in each task and in
general. In this paper, we describe the participants?
approaches, results, and the observations from the
results, which may guide future research in this area.
1 Introduction
The TempEval task (Verhagen et al, 2009) was added as a
new task in SemEval-2007. The ultimate aim of research
in this area is the automatic identification of temporal ex-
pressions (timexes), events, and temporal relations within
a text as specified in TimeML annotation (Pustejovsky et
al., 2005). However, since addressing this aim in a first
evaluation challenge was deemed too difficult a staged
approach was suggested.
TempEval (henceforth TempEval-1) was an initial
evaluation exercise focusing only on the categorization of
temporal relations and only in English. It included three
relation types: event-timex, event-dct,1 and relations be-
tween main events in consecutive sentences.
TempEval-2 (Verhagen et al, 2010) extended
TempEval-1, growing into a multilingual task, and con-
sisting of six subtasks rather than three. This included
event and timex extraction, as well as the three relation
tasks from TempEval-1, with the addition of a relation
task where one event subordinates another.
TempEval-3 (UzZaman et al, 2012b) is a follow-up
to TempEval 1 and 2, covering English and Spanish.
TempEval-3 is different from its predecessors in a few
respects:
1DCT stands for document creation time
Size of the corpus: the dataset used has about 600K
word silver standard data and about 100K word gold stan-
dard data for training, compared to around 50K word cor-
pus used in TempEval 1 and 2. Temporal annotation is
a time-consuming task for humans, which has limited
the size of annotated data in previous TempEval exer-
cises. Current systems, however, are performing close to
the inter-annotator reliability, which suggests that larger
corpora could be built from automatically annotated data
with minor human reviews. We want to explore whether
there is value in adding a large automatically created sil-
ver standard to a hand-crafted gold standard.
End-to-end temporal relation processing task: the
temporal relation classification tasks are performed from
raw text, i.e. participants need to extract their own events
and temporal expressions first, determine which ones to
link and then obtain the relation types. In previous Tem-
pEvals, gold timexes, events, and relations (without cate-
gory) were given to participants.
Temporal relation types: the full set of temporal re-
lations in TimeML are used, rather than the reduced set
used in earlier TempEvals.
Platinum test set: A new test dataset has been devel-
oped for this edition. It is based on manual annotations
by experts over new text (unseen in previous editions).
Evaluation: we report a temporal awareness score for
evaluating temporal relations, which helps to rank sys-
tems with a single score.
2 Data
In TempEval-3, we reviewed and corrected existing cor-
pora, and also released new corpora.
2.1 Reviewing Existing Corpora
We considered the existing TimeBank (Pustejovsky et al,
2003) and AQUAINT2 data for TempEval-3. TempEval-
2See http://timeml.org/site/timebank/timebank.html
1
Entity Agreement
Event 0.87
Event class 0.92
Timex 0.87
Timex value 0.88
Table 1: Platinum corpus entity inter-annotator agreement.
Corpus # of words Standard
TimeBank 61,418 Gold
AQUAINT 33,973 Gold
TempEval-3 Silver 666,309 Silver
TempEval-3 Eval 6,375 Platinum
TimeBank-ES Train 57,977 Gold
TimeBank-ES Eval 9,833 Gold
Table 2: Corpora used in TempEval-3.
1 and TempEval-2 had the same documents as TimeBank
but different relation types and events.
For both TimeBank and AQUAINT, we, (i) cleaned up
the formatting for all files making it easy to review and
read, (ii) made all files XML and TimeML schema com-
patible, (iii) added some missing events and temporal ex-
pressions. In TimeBank, we, (i) borrowed the events from
the TempEval-2 corpus and (ii) borrowed the temporal re-
lations from TimeBank corpus, which contains a full set
of temporal relations. In AQUAINT, we added the tem-
poral relations between event and DCT (document cre-
ation time), which was missing for many documents in
that corpus. These existing corpora comprised the high-
quality component of our training set.
2.2 New Corpora
We created two new datasets: a small, manually-
annotated set over new text (platinum); and a machine-
annotated, automatically-merged dataset based on out-
puts of multiple systems (silver).
The TempEval-3 platinum evaluation corpus was anno-
tated/reviewed by the organizers, who are experts in the
area. This process used the TimeML Annotation Guide-
lines v1.2.1 (Saur?? et al, 2006). Every file was anno-
tated independently by at least two expert annotators, and
a third was dedicated to adjudicating between annotations
and merging the final result. Some annotators based their
work on TIPSem annotation suggestions (Llorens et al,
2012b). The GATE Annotation Diff tool was used for
merging (Cunningham et al, 2013), a custom TimeML
validator ensured integrity,3 and CAVaT (Derczynski and
Gaizauskas, 2010) was used to determine various modes
of TimeML mis-annotation and inconsistency that are in-
expressable via XML schema. Post-exercise, that corpus
(TempEval-3 Platinum with around 6K tokens, on com-
pletely new text) is released for the community to review
3See https://github.com/hllorens/TimeML-validator
and improve.4 Inter-annotator agreement (measured with
F1, as per Hripcsak and Rothschild (2005)) and the num-
ber of annotation passes per document were higher than
in existing TimeML corpora, hence the name. Details are
given in Table 1. Attribute value scores are given based
on the agreed entity set. These are for exact matches.
The TempEval-3 silver evaluation corpus is a 600K
word corpus collected from Gigaword (Parker et
al., 2011). We automatically annotated this corpus
by TIPSem, TIPSem-B (Llorens et al, 2013) and
TRIOS (UzZaman and Allen, 2010). These systems were
retrained on the corrected TimeBank and AQUAINT cor-
pus to generate the original TimeML temporal relation
set. We then merged these three state-of-the-art sys-
tem outputs using our merging algorithm (Llorens et al,
2012a). In our selected merged configuration all entities
and relations suggested by the best system (TIPSem) are
added in the merged output. Suggestions from other sys-
tems (TRIOS and TIPSem-B) are added in the merged
output, only if they are also supported by another system.
The weights considered in our configuration are: TIPSem
0.36, TIPSemB 0.32, TRIOS 0.32.
For Spanish, Spanish TimeBank 1.0 corpus (Saur?? and
Badia, 2012) wads used. It is the same corpus that was
used in TempEval-2, with a major review of entity anno-
tation and an important improvement regarding temporal
relation annotation. For TempEval-3, we converted ES-
TimeBank link types to the TimeML standard types based
on Allen?s temporal relations (Allen, 1983).
Table 2 summarizes our released corpora, measured
with PTB-scheme tokens as words. All data produced
was annotated using a well-defined subset of TimeML,
designed for easy processing, and for reduced ambigu-
ity compared to standard TimeML. Participants were en-
couraged to validate their submissions using a purpose-
built tool to ensure that submitted runs were legible. We
called this standard TimeML-strict, and release it sepa-
rately (Derczynski et al, 2013).
3 Tasks
The three main tasks proposed for TempEval-3 focus on
TimeML entities and relations:
3.1 Task A (Timex extraction and normalization)
Determine the extent of the timexes in a text as defined
by the TimeML TIMEX3 tag. In addition, determine the
value of the features TYPE and VALUE. The possible
values of TYPE are time, date, duration, and set; VALUE
is a normalized value as defined by the TIMEX3 standard.
4In the ACL data and code repository, reference ADCR2013T001.
See also https://bitbucket.org/leondz/te3-platinum
2
3.2 Task B (Event extraction and classification)
Determine the extent of the events in a text as defined by
the TimeML EVENT tag and the appropriate CLASS.
3.3 Task ABC (Annotating temporal relations)
This is the ultimate task for evaluating an end-to-end sys-
tem that goes from raw text to TimeML annotation of
entities and links. It entails performing tasks A and B.
From raw text extract the temporal entities (events and
timexes), identify the pairs of temporal entities that have
a temporal link (TLINK) and classify the temporal re-
lation between them. Possible pair of entities that can
have a temporal link are: (i) main events of consecu-
tive sentences, (ii) pairs of events in the same sentence,
(iii) event and timex in the same sentence and (iv) event
and document creation time. In TempEval-3, TimeML
relation are used, i.e.: BEFORE, AFTER, INCLUDES, IS-
INCLUDED, DURING, SIMULTANEOUS, IMMEDIATELY
AFTER, IMMEDIATELY BEFORE, IDENTITY, BEGINS,
ENDS, BEGUN-BY and ENDED-BY.
In addition to this main tasks, we also include two extra
temporal relation tasks:
Task C (Annotating relations given gold entities)
Given the gold entities, identify the pairs of entities that
have a temporal link (TLINK) and classify the temporal
relations between them.
Task C relation only (Annotating relations given gold
entities and related pairs) Given the temporal entities
and the pair of entities that have a temporal link, classify
the temporal relation between them.
4 Evaluation Metrics
The metrics used to evaluate the participants are:
4.1 Temporal Entity Extraction
To evaluate temporal entities (events and temporal ex-
pressions), we need to evaluate, (i) How many entities are
correctly identified, (ii) If the extents for the entities are
correctly identified, and (iii) How many entity attributes
are correctly identified. We use classical precision and
recall for recognition.
How many entities are correctly identified: We evalu-
ate our entities using the entity-based evaluation with the
equations below.
Precision = |Sysentity?Refentity||Sysentity|
Recall = |Sysentity?Refentity||Refentity|
where, Sysentity contains the entities extracted by the
system that we want to evaluate, and Refentity contains
the entities from the reference annotation that are being
compared.
If the extents for the entities are correctly identified:
We compare our entities with both strict match and re-
laxed match. When there is a exact match between the
system entity and gold entity then we call it strict match,
e.g. ?sunday morning? vs ?sunday morning?. When there
is a overlap between the system entity and gold entity
then we call it relaxed match, e.g. ?sunday? vs ?sunday
morning?. When there is a relaxed match, we compare
the attribute values.
How many entity attributes are correctly identified: We
evaluate our entity attributes using the attribute F1-score,
which captures how well the system identified both the
entity and attribute (attr) together.
Attribute Recall =
|{?x | x?(Sysentity?Refentity)?Sysattr(x)==Refattr(x)}|
|Refentity|
Attribute Precision =
|{?x | x?(Sysentity?Refentity)?Sysattr(x)==Refattr(x)}|
|Sysentity|
Attribute F1-score = 2?p?rp+r
Attribute (Attr) accuracy, precision and recall can be
calculated as well from the above information.
Attr Accuracy = Attr F1 / Entity Extraction F1
Attr R = Attr Accuracy * Entity R
Attr P = Attr Accuracy * Entity P
4.2 Temporal Relation Processing
To evaluate relations, we use the evaluation metric pre-
sented by UzZaman and Allen (2011).5 This metric cap-
tures the temporal awareness of an annotation in terms
of precision, recall and F1 score. Temporal awareness
is defined as the performance of an annotation as identi-
fying and categorizing temporal relations, which implies
the correct recognition and classification of the tempo-
ral entities involved in the relations. Unlike TempEval-
2 relation score, where only categorization is evaluated
for relations, this metric evaluates how well pairs of enti-
ties are identified, how well the relations are categorized,
and how well the events and temporal expressions are ex-
tracted.
Precision =
|Sys?relation?Ref
+
relation|
|Sys?relation|
Recall =
|Ref?relation?Sys
+
relation|
|Ref?relation|
where, G+ is the closure of graph G and G? is the
reduced of graph G, where redundant relations are re-
moved.6
We calculate the Precision by checking the number
of reduced system relations (Sys?relation) that can be veri-
fied from the reference annotation temporal closure graph
(Ref+relation), out of number of temporal relations in the
5We used a minor variation of the formula, where we consider the
reduced graph instead of all system or reference relations. Details can
be found in Chapter 6 of UzZaman (2012).
6A relation is redundant if it can be inferred through other relations.
3
strict value
F1 P R F1 F1
HeidelTime-t 90.30 93.08 87.68 81.34 77.61
HeidelTime-bf 87.31 90.00 84.78 78.36 72.39
HeidelTime-1.2 86.99 89.31 84.78 78.07 72.12
NavyTime-1,2 90.32 89.36 91.30 79.57 70.97
ManTIME-4 89.66 95.12 84.78 74.33 68.97
ManTIME-6 87.55 98.20 78.99 73.09 68.27
ManTIME-3 87.06 94.87 80.43 69.80 67.45
SUTime 90.32 89.36 91.30 79.57 67.38
ManTIME-1 87.20 97.32 78.99 70.40 67.20
ManTIME-5 87.20 97.32 78.99 69.60 67.20
ManTIME-2 88.10 97.37 80.43 72.22 66.67
ATT-2 85.25 98.11 75.36 78.69 65.57
ATT-1 85.60 99.05 75.36 79.01 65.02
ClearTK-1,2 90.23 93.75 86.96 82.71 64.66
JU-CSE 86.38 93.28 80.43 75.49 63.81
KUL 83.67 92.92 76.09 69.32 62.95
KUL-TE3RunABC 82.87 92.04 75.36 73.31 62.15
ClearTK-3,4 87.94 94.96 81.88 77.04 61.48
ATT-3 80.85 97.94 68.84 72.34 60.43
FSS-TimEx 85.06 90.24 80.43 49.04 58.24
TIPSem (TE2) 84.90 97.20 75.36 81.63 65.31
Table 3: Task A - Temporal Expression Performance.
reduced system relations (Sys?relation). Similarly, we
calculate the Recall by checking the number of reduced
reference annotation relations (Ref?relation) that can be
verified from the system output?s temporal closure graph
(Sys+relation), out of number of temporal relations in the
reduced reference annotation (Ref?relation).
This metric evaluates Task ABC together. For Task C
and Task C - relation only, all the gold annotation entities
were provided and then evaluated using the above metric.
Our evaluation toolkit that evaluated TempEval-3 par-
ticipants is available online.7
5 Evaluation Results
The aim of this evaluation is to provide a meaningful re-
port of the performance obtained by the participants in
the tasks defined in Section 3.
Furthermore, the results include TIPSem as reference
for comparison. This was used as a pre-annotation system
in some cases. TIPSem obtained the best results in event
processing task in TempEval-2 and offered very compet-
itive results in timex and relation processing. The best
timex processing system in TempEval-2 (HeidelTime) is
participating in this edition as well, therefore we included
TIPSem as a reference in all tasks.
We only report results in main measures. Results are
divided by language and shown per task. Detailed scores
can be found on the task website.8
7See http://www.cs.rochester.edu/u/naushad/temporal
8See http://www.cs.york.ac.uk/semeval-2013/task1/
5.1 Results for English
5.1.1 Task A: Timexes
We had nine participants and 21 unique runs for tem-
poral expression extraction task, Task A. Table 3 shows
the results. Details about participants? approaches can be
found in Table 4.
We rank the participants for Task A on the F1 score
of most important timex attribute ? Value. To get the
attribute Value correct, a system needs to correctly nor-
malise the temporal expression. This score (Value F1)
captures the performance of extracting the timex and
identifying the attribute Value together (Value F1 = Timex
F1 * Value Accuracy).
Participants approached the temporal expression ex-
traction task with rule-engineered methods, machine
learning methods and also hybrid methods. For temporal
expression normalization (identifying the timex attribute
value), all participants used rule-engineered approaches.
Observations: We collected the following observa-
tions from the results and from participants? experiments.
Strategy: Competition was close for timex recogni-
tion and the best systems all performed within 1% of
each other. On our newswire corpus, statistical systems
(ClearTK) performed best at strict matching, and rule-
engineered system best at relaxed matching (NavyTime,
SUTime, HeidelTime).
Strategy: post-processing, on top of machine learning-
base temporal expression extraction, provided a statisti-
cally significant improvement in both precision and recall
(ManTIME).
Data: using the large silver dataset, alone or together
with human annotated data, did not give improvements in
performance for Task A. Human-annotated gold standard
data alone provided the best performance (ManTIME).
Data: TimeBank alone was better than TimeBank and
AQUAINT together for Task A (ClearTK).
Features: syntactic and gazetteers did not provide any
statistically significant increment of performance with re-
spect to the morphological features alone (ManTIME).
Regarding the two sub-tasks of timex annotation,
recognition and interpretation/normalisation, we noticed
a shift in the state of the art. While normalisation is
currently (and perhaps inherently) done best by rule-
engineered systems, recognition is now done well by a
variety of methods. Where formerly, rule-engineered
timex recognition always outperformed other classes of
approach, now it is clear that rule-engineering and ma-
chine learning are equally good at timex recognition.
5.1.2 Task B: Events
For event extraction (Task B) we had seven participants
and 10 unique runs. The results for this task can be found
in Table 6. We rank the participants for TaskB on the F1
score of most important event attribute ? Class. Class
4
Strategy System Training data Classifier used
Data-driven ATT-1, 2, 3 TBAQ + TE3Silver MaxEnt
ClearTK-1, 2 TimeBank SVM, Logit
ClearTK-3, 4 TBAQ SVM, Logit
JU-CSE TBAQ CRF
ManTIME-1 TBAQ + TE3Silver CRF
ManTIME-3 TBAQ CRF
ManTIME-5 TE3Silver CRF
Temp : ESAfeature TBAQ MaxEnt
Temp : WordNetfeature TBAQ MaxEnt
TIPSem (TE2) TBAQ CRF
Rule-based FSS-TimEx (EN) None None
FSS-TimEx (ES) None None
HeidelTime-1.2, bf (EN) None None
HeidelTime-t (EN) TBAQ None
HeidelTime (ES) Gold None
NavyTime-1, 2 None None
SUTime None None
Hybrid KUL TBAQ + TE3Silver Logit + post-processing
KUL-TE3RunABC TBAQ +TE3Silver Logit + post-processing
ManTIME-2 TBAQ + TE3Silver CRF + post-processing
ManTIME-4 TBAQ CRF + post-processing
ManTIME-6 TE3Silver CRF + post-processing
Table 4: Automated approaches for TE3 Timex Extraction
Strategy System Training data Classifier used Linguistic
Knowledge
Data-driven ATT-1, 2, 3 TBAQ + TE3Silver MaxEnt ms, ss
ClearTK-1, 2 TimeBank SVM, Logit ms
ClearTK-3, 4 TBAQ SVM, Logit ms
JU-CSE TBAQ CRF
KUL TBAQ +TE3Silver Logit ms, ls
KUL-TE3RunABC TBAQ +TE3Silver Logit ms, ls
NavyTime-1 TBAQ MaxEnt ms, ls
NavyTime-2 TimeBank MaxEnt ms, ls
Temp : ESAfeature TBAQ MaxEnt ms, ls, ss
Temp : WordNetfeature TBAQ MaxEnt ms, ls
TIPSem (TE2) TBAQ CRF/SVM ms, ls, ss
Rule-based FSS-TimEx (EN) None None ls, ms
FSS-TimEx (ES) None None ls, ms
Table 5: Automated approaches for Event Extraction
5
F1 P R class F1
ATT-1 81.05 81.44 80.67 71.88
ATT-2 80.91 81.02 80.81 71.10
KUL 79.32 80.69 77.99 70.17
ATT-3 78.63 81.95 75.57 69.55
KUL-TE3RunABC 77.11 77.58 76.64 68.74
ClearTK-3,4 78.81 81.40 76.38 67.87
NavyTime-1 80.30 80.73 79.87 67.48
ClearTK-1,2 77.34 81.86 73.29 65.44
NavyTime-2 79.37 80.52 78.26 64.81
Temp:ESAfeature 68.97 78.33 61.61 54.55
JU-CSE 78.62 80.85 76.51 52.69
Temp:WordNetfeature 63.90 78.90 53.69 50.00
FSS-TimEx 65.06 63.13 67.11 42.94
TIPSem (TE2) 82.89 83.51 82.28 75.59
Table 6: Task B - Event Extraction Performance.
F1 P R
ClearTK-2 30.98 34.08 28.40
ClearTK-1 29.77 34.49 26.19
ClearTK-3 28.62 30.94 26.63
ClearTK-4 28.46 29.73 27.29
NavyTime-1 27.28 31.25 24.20
JU-CSE 24.61 19.17 34.36
NavyTime-2 21.99 26.52 18.78
KUL-TE3RunABC 19.01 17.94 20.22
TIPSem (TE2) 42.39 38.79 46.74
Table 7: Task ABC - Temporal Awareness Evaluation (Task C
evaluation from raw text).
F1 captures the performance of extracting the event and
identifying the attribute Class together (Class F1 = Event
F1 * Class Accuracy).
All the participants except one used machine learning
approaches. Details about the participants? approaches
and the linguistic knowledge9 used to solve this problem,
and training data, are in Table 5.
Observations: We collected the following observa-
tions from the results and from participants? experiments.
Strategy: All the high performing systems for event
extraction (Task B) are machine learning-based.
Data: Systems using silver data, along with the hu-
man annotated gold standard data, performed very well
(top three participants in the task ? ATT, KUL, KUL-
TE3RunABC). Additionally, TimeBank and AQUAINT
together performed better than just TimeBank alone
(NavyTime-1, ClearTK-3,4).
Linguistic Features: Semantic features (ls and ss) have
played an important role, since the best systems (TIPSem,
ATT1 and KUL) include them. However, these three are
not the only systems using semantic features.
9Abbreviations used in the table: TBAQ ? TimeBank + AQUAINT
corpus ms ? morphosyntactic information, e.g. POS, lexical informa-
tion, morphological information and syntactic parsing related features;
ls ?lexical semantic information, e.g. WordNet synsets; ss ? sentence-
level semantic information, e.g. Semantic Role labels.
F1 P R
ClearTK-2 36.26 37.32 35.25
ClearTK-4 35.86 35.17 36.57
ClearTK-1 35.19 37.64 33.04
UTTime-5 34.90 35.94 33.92
ClearTK-3 34.13 33.27 35.03
NavyTime-1 31.06 35.48 27.62
UTTime-4 28.81 37.41 23.43
JU-CSE 26.41 21.04 35.47
NavyTime-2 25.84 31.10 22.10
KUL-TE3RunABC 24.83 23.35 26.52
UTTime-1 24.65 15.18 65.64
UTTime-3 24.28 15.10 61.99
UTTime-2 24.05 14.80 64.20
TIPSem (TE2) 44.25 39.71 49.94
Table 8: Task C - TLINK Identification and Classification.
F1 P R
UTTime-1, 4 56.45 55.58 57.35
UTTime-3, 5 54.70 53.85 55.58
UTTime-2 54.26 53.20 55.36
NavyTime-1 46.83 46.59 47.07
NavyTime-2 43.92 43.65 44.20
JU-CSE 34.77 35.07 34.48
Table 9: Task C - relation only: Relation Classification.
5.1.3 Task C: Relation Evaluation
For complete temporal annotation from raw text (Task
ABC - Task C from raw text) and for temporal relation
only tasks (Task C, Task C relation only), we had five
participants in total.
For relation evaluation, we primarily evaluate on Task
ABC (Task C from raw text), which requires joint entity
extraction, link identification and relation classification.
The results for this task can be found in Table 7.
While TIPSem obtained the best results in task ABC,
especially in recall, it was used by some annotators to
pre-label data. In the interest of rigour and fairness, we
separate out this system.
For task C, for provided participants with entities and
participants identified: between which entity pairs a rela-
tion exists (link identification); and the class of that rela-
tion. Results are given in Table 8. We also evaluate the
participants on the relation by providing the entities and
the links (performance in Table 9) ? TIPSem could not be
evaluated in this setting since the system is not prepared
to do categorization only unless the relations are divided
as in TempEval-2. For these Task C related tasks, we had
only one new participant, who didn?t participate in Task
A and B: UTTime.
Identifying which pair of entities to consider for tem-
poral relations is a new task in this TempEval challenge.
The participants approached the problems in data-driven,
rule-based and also in hybrid ways (Table 1010). On
10New abbreviation in the table, e-attr ? entity attributes, e.g. event
class, tense, aspect, polarity, modality; timex type, value.
6
Strategy System Training data Classifier used Linguistic
Knowledge
Data-driven ClearTK-1 TimeBank SVM, Logit e-attr, ms
ClearTK-2 TimeBank + Bethard et al (2007) SVM, Logit e-attr, ms
ClearTK-3 TBAQ SVM, Logit e-attr, ms
ClearTK-4 TBAQ + Muller?s inferences SVM, Logit e-attr, ms
KULRunABC TBAQ SVM, Logit ms
Rule-based JU-CSE None None
UTTime-1, 2 ,3 None None
TIPSem (TE2) None None e-attr, ms, ls, ss
Hybrid NavyTime-1 TBAQ MaxEnt ms
NavyTime-2 TimeBank MaxEnt ms
UTTime-4 TBAQ Logit ms, ls, ss
UTTime-5 TBAQ + inverse relations Logit ms, ls, ss
Table 10: Automated approaches for TE3 TLINK Identification
Strategy System Training data Classifier used Linguistic
Knowledge
Data-driven ClearTK-1 TimeBank SVM, Logit ms, ls
ClearTK-2 TimeBank + Bethard et al (2007) SVM, Logit ms, ls
ClearTK-3 TBAQ SVM, Logit ms, ls
ClearTK-4 TBAQ + Muller?s inferences SVM, Logit ms, ls
JU-CSE TBAQ CRF
KULRunABC TBAQ SVM, Logit ms
NavyTime-1 TBAQ MaxEnt ms, ls
NavyTime-2 TimeBank MaxEnt ms, ls
UTTime-1,4, 2 TBAQ Logit ms, ls, ss
UTTime-3,5 TBAQ + inverse relations Logit ms, ls, ss
TIPSem (TE-2) TBAQ CRF/SVM ms, ls, ss
Table 11: Automated approaches for Relation Classification
the other hand, all the participants used data-driven ap-
proaches for temporal relations (Table 11).
Observations: We collected the following observa-
tions from the results and from participants? experiments.
Strategy: For relation classification, all participants
used partially or fully machine learning-based systems.
Data: None of the participants implemented their sys-
tems training on the silver data. Most of the systems use
the combined TimeBank and AQUAINT (TBAQ) corpus.
Data: Adding additional high-quality relations, either
Philippe Muller?s closure-based inferences or the verb
clause relations from Bethard et al (2007), typically in-
creased recall and the overall performance (ClearTK runs
two and four).
Features: Participants mostly used the morphosyntac-
tic and lexical semantic information. The best perform-
ing systems from TempEval-2 (TIPSem and TRIOS) ad-
ditionally used sentence level semantic information. One
participant in TempEval-3 (UTTime) also did deep pars-
ing for the sentence level semantic features.
Features: Using more Linguistic knowledge is impor-
tant for the task, but it is more important to execute it
properly. Many systems performed better using less lin-
guistic knowledge. Hence a system (e.g. ClearTK) with
basic morphosyntactic features is hard to beat with more
semantic features, if not used properly.
entity extraction
strict relaxed
F1 F1 P R value
HeidelTime 85.3 90.1 96.0 84.9 87.5
TIPSemB-F 82.6 87.4 93.7 81.9 82.0
FSS-TimEx 49.5 65.2 86.6 52.3 62.7
Table 12: Task A: Temporal Expression (Spanish).
class tense aspect
F1 P R F1 F1 F1
FSS-TimEx 57.6 89.8 42.4 24.9 - -
TIPSemB-F 88.8 91.7 86.0 57.6 41.0 36.3
Table 13: Task B: Event Extraction (Spanish).
Classifier: Across the various tasks, ClearTK tried
Mallet CRF, Mallet MaxEnt, OpenNLP MaxEnt, and LI-
BLINEAR (SVMs and logistic regression). They picked
the final classifiers by running a grid search over models
and parameters on the training data, and for all tasks, a
LIBLINEAR model was at least as good as all the other
models. As an added bonus, it was way faster to train
than most of the other models.
6 Evaluation Results (Spanish)
There were two participants for Spanish. Both partici-
pated in task A and only one of them in task B. In this
7
F1 P R
TIPSemB-F 41.6 37.8 46.2
Table 14: Task ABC: Temporal Awareness (Spanish).
entity extraction attributes
strict relaxed val type
F1 F1 P R F1 F1
HeidelTime 86.4 89.8 94.0 85.9 87.5 89.8
FSS-TimEx 42.1 68.4 86.7 56.5 48.7 65.8
TIPSem 86.9 93.7 98.8 89.1 75.4 88.0
TIPSemB-F 84.3 89.9 93.0 87.0 82.0 86.5
Table 15: Task A: TempEval-2 test set (Spanish).
case, TIPSemB-Freeling is provided as a state-of-the-art
reference covering all the tasks. TIPSemB-Freeling is the
Spanish version of TIPSem with the main difference that
it does not include semantic roles. Furthermore, it uses
Freeling (Padro? and Stanilovsky, 2012) to obtain the lin-
guistic features automatically.
Table 12 shows the results obtained for task A. As it
can be observed HeidelTime obtains the best results. It
improves the previous state-of-the-art results (TIPSemB-
F), especially in normalization (value F1).
Table 13 shows the results from event extraction. In
this case, the previous state-of-the-art is not improved.
Table 14 only shows the results obtained in temporal
awareness by the state-of-the-art system since there were
not participants on this task. We observe that TIPSemB-F
approach offers competitive results, which is comparable
to results obtained in TE3 English test set.
6.1 Comparison with TempEval-2
TempEval-2 Spanish test set is included as a subset of this
TempEval-3 test set. We can therefore compare the per-
formance across editions. Furthermore, we can include
the full-featured TIPSem (Llorens et al, 2010), which
unlike TIPSemB-F used the AnCora (Taule? et al, 2008)
corpus annotations as features including semantic roles.
For timexes, as can be seen in Table 15, the origi-
nal TIPSem obtains better results for timex extraction,
which favours the hypothesis that machine learning sys-
tems are very well suited for this task (if the training data
is sufficiently representative). However, for normaliza-
tion (value F1), HeidelTime ? a rule-engineered system ?
obtains better results. This indicates that rule-based ap-
proaches have the upper hand in this task. TIPSem uses
class tense aspect
F1 P R F1 F1 F1
FSS-TimEx 59.0 90.3 43.9 24.6 - -
TIPSemB-F 90.2 92.5 88.0 58.6 39.7 38.1
TIPSem 88.2 90.6 85.8 58.7 84.9 78.7
Table 16: Task B: TempEval-2 test set (Spanish).
a partly data-driven normalization approach which, given
the small amount of training data available, seemed less
suited to the task.
Table 16 shows event extraction performance in TE2
test set. TIPSemB-F and TIPSem obtained a similar per-
formance. TIPSemB-F performed better in extraction and
TIPSem better in attribute classification.
7 Conclusion
In this paper, we described the TempEval-3 task within
the SemEval 2013 exercise. This task involves identify-
ing temporal expressions (timexes), events and their tem-
poral relations in text. In particular participating systems
were required to automatically annotate raw text using
TimeML annotation scheme
This is the first time end-to-end systems are evalu-
ated with a new single score (temporal awareness). In
TempEval-3 participants had to obtain temporal relations
from their own extracted timexes and events which is a
very challenging task and was the ultimate evaluation aim
of TempEval. It was proposed at TempEval-1 but has not
been carried out until this edition.
The newly-introduced silver data proved not so useful
for timex extraction or relation classification, but did help
with event extraction. The new single-measure helped to
rank systems easily.
Future work could investigate temporal annotation in
specific applications. Current annotations metrics evalu-
ate relations for entities in the same consecutive sentence.
For document-level understanding we need to understand
discourse and pragmatic information. Temporal question
answering-based evaluation (UzZaman et al, 2012a) can
help us to evaluate participants on document level tempo-
ral information understanding without creating any addi-
tional training data. Also, summarisation, machine trans-
lation, and information retrieval need temporal annota-
tion. Application-oriented challenges could further re-
search in these areas.
From a TimeML point of view, we still haven?t tack-
led subordinate relations (TimeML SLINKs), aspectual
relations (TimeML ALINKs), or temporal signal anno-
tation (Derczynski and Gaizauskas, 2011). The critical
questions of which links to annotate, and whether the cur-
rent set of temporal relation types are appropriate for lin-
guistic annotation, are still unanswered.
Acknowledgments
We thank the participants ? especially Steven Bethard,
Jannik Stro?tgen, Nate Chambers, Oleksandr Kolomiyets,
Michele Filannino, Philippe Muller and others ? who
helped us to improve TempEval-3 with their valuable
feedback. The third author also thanks Aarhus Univer-
sity, Denmark who kindly provided facilities.
8
References
J. F. Allen. 1983. Maintaining knowledge about temporal in-
tervals. Communications of the ACM, 26(11):832?843.
S. Bethard, J. H. Martin, and S. Klingenstein. 2007. Timelines
from text: Identication of syntactic temporal relations. In
Proceedings of IEEE International Conference on Semantic
Computing.
H. Cunningham, V. Tablan, A. Roberts, and K. Bontcheva.
2013. Getting More Out of Biomedical Documents with
GATE?s Full Lifecycle Open Source Text Analytics. PLoS
computational biology, 9(2):e1002854.
L. Derczynski and R. Gaizauskas. 2010. Analysing Temporally
Annotated Corpora with CAVaT. In Proceedings of the 7th
International Conference on Language Resources and Eval-
uation, pages 398?404.
L. Derczynski and R. Gaizauskas. 2011. A Corpus-based Study
of Temporal Signals. In Proceedings of the 6th Corpus Lin-
guistics Conference.
L. Derczynski, H. Llorens, and N. UzZaman. 2013. TimeML-
strict: clarifying temporal annotation. CoRR, abs/1304.
G. Hripcsak and A. S. Rothschild. 2005. Agreement, the f-
measure, and reliability in information retrieval. Journal of
the American Medical Informatics Association, 12(3):296?
298.
H. Llorens, E. Saquete, and B. Navarro. 2010. TIPSem (En-
glish and Spanish): Evaluating CRFs and Semantic Roles in
TempEval-2. In Proceedings of the 5th International Work-
shop on Semantic Evaluation, pages 284?291. Association
for Computational Linguistics.
H. Llorens, N. UzZaman, and J. Allen. 2012a. Merging Tem-
poral Annotations. In Proceedings of the TIME Conference.
H. Llorens, E. Saquete, and B. Navarro-Colorado. 2012b. Au-
tomatic system for identifying and categorizing temporal re-
lations in natural language. International Journal of Intelli-
gent Systems, 27(7):680?703.
H. Llorens, E. Saquete, and B. Navarro-Colorado. 2013. Ap-
plying Semantic Knowledge to the Automatic Processing of
Temporal Expressions and Events in Natural Language. In-
formation Processing & Management, 49(1):179?197.
L. Padro? and E. Stanilovsky. 2012. Freeling 3.0: Towards wider
multilinguality. In Proceedings of the Language Resources
and Evaluation Conference (LREC 2012), Istanbul, Turkey,
May. ELRA.
R. Parker, D. Graff, J. Kong, K. Chen, and K. Maeda.
2011. English Gigaword Fifth Edition. LDC catalog ref.
LDC2011T07.
J. Pustejovsky, P. Hanks, R. Saur??, A. See, R. Gaizauskas,
A. Setzer, D. Radev, B. Sundheim, D. Day, L. Ferro, et al
2003. The TimeBank corpus. In Corpus Linguistics.
J. Pustejovsky, B. Ingria, R. Saur??, J. Castano, J. Littman,
R. Gaizauskas, A. Setzer, G. Katz, and I. Mani. 2005. The
specification language TimeML. The Language of Time: A
reader, pages 545?557.
R. Saur?? and T. Badia. 2012. Spanish TimeBank 1.0. LDC
catalog ref. LDC2012T12.
R. Saur??, J. Littman, B. Knippen, R. Gaizauskas, A. Setzer, and
J. Pustejovsky. 2006. TimeML Annotation Guidelines Ver-
sion 1.2.1.
M. Taule?, M. A. Mart?, and M. Recasens. 2008. Ancora: Mul-
tilevel annotated corpora for catalan and spanish. In Pro-
ceedings of the 6th International Conference on Language
Resources and Evaluation (LREC-2008).
N. UzZaman and J. Allen. 2010. TRIPS and TRIOS system for
TempEval-2: Extracting temporal information from text. In
Proceedings of the 5th International Workshop on Semantic
Evaluation, pages 276?283. Association for Computational
Linguistics.
N. UzZaman and J. Allen. 2011. Temporal Evaluation. In Pro-
ceedings of The 49th Annual Meeting of the Association for
Computational Linguistics: Human Language Technologies.
N. UzZaman, H. Llorens, and J. Allen. 2012a. Evaluating tem-
poral information understanding with temporal question an-
swering. In Proceedings of IEEE International Conference
on Semantic Computing.
N. UzZaman, H. Llorens, J. F. Allen, L. Derczynski, M. Ver-
hagen, and J. Pustejovsky. 2012b. TempEval-3: Evaluating
Events, Time Expressions, and Temporal Relations. CoRR,
abs/1206.5333.
N. UzZaman. 2012. Interpreting the Temporal Aspects of Lan-
guage. Ph.D. thesis, University of Rochester, Rochester, NY.
M. Verhagen, R. Gaizauskas, F. Schilder, M. Hepple,
J. Moszkowicz, and J. Pustejovsky. 2009. The TempEval
challenge: identifying temporal relations in text. Language
Resources and Evaluation, 43(2):161?179.
M. Verhagen, R. Saur??, T. Caselli, and J. Pustejovsky. 2010.
SemEval-2010 task 13: TempEval-2. In Proceedings of the
5th International Workshop on Semantic Evaluation, pages
57?62. Association for Computational Linguistics.
9
Coling 2008: Proceedings of the 2nd workshop on Information Retrieval for Question Answering (IR4QA), pages 34?41
Manchester, UK. August 2008
A Data Driven Approach to Query Expansion in Question Answering
Leon Derczynski, Jun Wang, Robert Gaizauskas and Mark A. Greenwood
Department of Computer Science
University of Sheffield
Regent Court, 211 Portobello
Sheffield S1 4DP UK
{aca00lad, acp07jw}@shef.ac.uk
{r.gaizauskas, m.greenwood}@dcs.shef.ac.uk
Abstract
Automated answering of natural language
questions is an interesting and useful prob-
lem to solve. Question answering (QA)
systems often perform information re-
trieval at an initial stage. Information re-
trieval (IR) performance, provided by en-
gines such as Lucene, places a bound on
overall system performance. For example,
no answer bearing documents are retrieved
at low ranks for almost 40% of questions.
In this paper, answer texts from previous
QA evaluations held as part of the Text
REtrieval Conferences (TREC) are paired
with queries and analysed in an attempt
to identify performance-enhancing words.
These words are then used to evaluate the
performance of a query expansion method.
Data driven extension words were found
to help in over 70% of difficult questions.
These words can be used to improve and
evaluate query expansion methods. Sim-
ple blind relevance feedback (RF) was cor-
rectly predicted as unlikely to help overall
performance, and an possible explanation
is provided for its low value in IR for QA.
1 Introduction
The task of supplying an answer to a question,
given some background knowledge, is often con-
sidered fairly trivial from a human point of view,
as long as the question is clear and the answer is
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
known. The aim of an automated question answer-
ing system is to provide a single, unambiguous re-
sponse to a natural language question, given a text
collection as a knowledge source, within a certain
amount of time. Since 1999, the Text Retrieval
Conferences have included a task to evaluate such
systems, based on a large pre-defined corpus (such
as AQUAINT, containing around a million news
articles in English) and a set of unseen questions.
Many information retrieval systems perform
document retrieval, giving a list of potentially rel-
evant documents when queried ? Google?s and Ya-
hoo!?s search products are examples of this type of
application. Users formulate a query using a few
keywords that represent the task they are trying to
perform; for example, one might search for ?eif-
fel tower height? to determine how tall the Eiffel
tower is. IR engines then return a set of references
to potentially relevant documents.
In contrast, QA systems must return an exact an-
swer to the question. They should be confident
that the answer has been correctly selected; it is
no longer down to the user to research a set of doc-
ument references in order to discover the informa-
tion themselves. Further, the system takes a natural
language question as input, instead of a few user-
selected key terms.
Once a QA system has been provided with a
question, its processing steps can be described in
three parts - Question Pre-Processing, Text Re-
trieval and Answer Extraction:
1. Question Pre-Processing TREC questions
are grouped into series which relate to a given
target. For example, the target may be ?Hinden-
burg disaster? with questions such as ?What type
of craft was the Hindenburg?? or ?How fast could
it travel??. Questions may include pronouns ref-
34
erencing the target or even previous answers, and
as such require processing before they are suitable
for use.
2. Text Retrieval An IR component will return
a ranked set of texts, based on query terms. At-
tempting to understand and extract data from an
entire corpus is too resource intensive, and so an IR
engine defines a limited subset of the corpus that
is likely to contain answers. The question should
have been pre-processed correctly for a useful set
of texts to be retrieved ? including anaphora reso-
lution.
3. Answer Extraction (AE) Given knowledge
about the question and a set of texts, the AE sys-
tem attempts to identify answers. It should be clear
that only answers within texts returned by the IR
component have any chance of being found.
Reduced performance at any stage will have a
knock-on effect, capping the performance of later
stages. If questions are left unprocessed and full
of pronouns (e.g.,?When did it sink??) the IR com-
ponent has very little chance of working correctly
? in this case, the desired action is to retrieve
documents related to the Kursk submarine, which
would be impossible.
IR performance with a search engine such as
Lucene returns no useful documents for at least
35% of all questions ? when looking at the top
20 returned texts. This caps the AE component
at 65% question ?coverage?. We will measure the
performance of different IR component configura-
tions, to rule out problems with a default Lucene
setup.
For each question, answers are provided in the
form of regular expressions that match answer text,
and a list of documents containing these answers
in a correct context. As references to correct doc-
uments are available, it is possible to explore a
data-driven approach to query analysis. We deter-
mine which questions are hardest then concentrate
on identifying helpful terms found in correct doc-
uments, with a view to building a system than can
automatically extract these helpful terms from un-
seen questions and supporting corpus. The avail-
ability and usefulness of these terms will provide
an estimate of performance for query expansion
techniques.
There are at least two approaches which could
make use of these term sets to perform query ex-
pansion. They may occur in terms selected for
blind RF (non-blind RF is not applicable to the
TREC QA task). It is also possible to build a cata-
logue of terms known to be useful according to cer-
tain question types, thus leading to a dictionary of
(known useful) expansions that can be applied to
previously unseen questions. We will evaluate and
also test blind relevance feedback in IR for QA.
2 Background and Related Work
The performance of an IR system can be quanti-
fied in many ways. We choose and define mea-
sures pertinent to IR for QA. Work has been done
on relevance feedback specific to IR for QA, where
it is has usually be found to be unhelpful. We out-
line the methods used in the past, extend them, and
provide and test means of validating QA relevance
feedback.
2.1 Measuring QA Performance
This paper uses two principle measures to describe
the performance of the IR component. Coverage
is defined as the proportion of questions where at
least one answer bearing text appears in the re-
trieved set. Redundancy is the average number
of answer bearing texts retrieved for each ques-
tion (Roberts and Gaizauskas, 2004).
Both these measures have a fixed limit n on the
number of texts retrieved by a search engine for a
query. As redundancy counts the number of texts
containing correct answers, and not instances of
the answer itself, it can never be greater than the
number of texts retrieved.
The TREC reference answers provide two ways
of finding a correct text, with both a regular expres-
sion and a document ID. Lenient hits (retrievals of
answer bearing documents) are those where the re-
trieved text matches the regular expression; strict
hits occur when the document ID of the retrieved
text matches that declared by TREC as correct and
the text matches the regular expression. Some doc-
uments will match the regular expression but not
be deemed as containing a correct answer (this
is common with numbers and dates (Baeza-Yates
and Ribeiro-Neto, 1999)), in which case a lenient
match is found, but not a strict one.
The answer lists as defined by TREC do not in-
clude every answer-bearing document ? only those
returned by previous systems and marked as cor-
rect. Thus, false negatives are a risk, and strict
measures place an approximate lower bound on
the system?s actual performance. Similarly, lenient
35
matches can occur out of context, without a sup-
porting document; performance based on lenient
matches can be viewed as an approximate upper
bound (Lin and Katz, 2005).
2.2 Relevance Feedback
Relevance feedback is a widely explored technique
for query expansion. It is often done using a spe-
cific measure to select terms using a limited set of
ranked documents of size r; using a larger set will
bring term distribution closer to values over the
whole corpus, and away from ones in documents
relevant to query terms. Techniques are used to
identify phrases relevant to a query topic, in or-
der to reduce noise (such as terms with a low cor-
pus frequency that relate to only a single article)
and query drift (Roussinov and Fan, 2005; Allan,
1996).
In the context of QA, Pizzato (2006) employs
blind RF using the AQUAINT corpus in an attempt
to improve performance when answering factoid
questions on personal names. This is a similar ap-
proach to some content in this paper, though lim-
ited to the study of named entities, and does not
attempt to examine extensions from the existing
answer data.
Monz (2003) finds a negative result when apply-
ing blind feedback for QA in TREC 9, 10 and 11,
and a neutral result for TREC 7 and 8?s ad hoc re-
trieval tasks. Monz?s experiment, using r = 10
and standard Rocchio term weighting, also found
a further reduction in performance when r was
reduced (from 10 to 5). This is an isolated ex-
periment using just one measure on a limited set
of questions, with no use of the available answer
texts.
Robertson (1992) notes that there are issues
when using a whole document for feedback, as
opposed to just a single relevant passage; as men-
tioned in Section 3.1, passage- and document-level
retrieval sets must also be compared for their per-
formance at providing feedback. Critically, we
will survey the intersection between words known
to be helpful and blind RF terms based on initial
retrieval, thus showing exactly how likely an RF
method is to succeed.
3 Methodology
We first investigated the possibility of an IR-
component specific failure leading to impaired
coverage by testing a variety of IR engines and
configurations. Then, difficult questions were
identified, using various performance thresholds.
Next, answer bearing texts for these harder ques-
tions were checked for words that yielded a per-
formance increase when used for query expansion.
After this, we evaluated how likely a RF-based ap-
proach was to succeed. Finally, blind RF was ap-
plied to the whole question set. IR performance
was measured, and terms used for RF compared to
those which had proven to be helpful as extension
words.
3.1 IR Engines
A QA framework (Greenwood, 2004a) was origi-
nally used to construct a QA system based on run-
ning a default Lucene installation. As this only
covers one IR engine in one configuration, it is
prudent to examine alternatives. Other IR engines
should be tested, using different configurations.
The chosen additional engines were: Indri, based
on the mature INQUERY engine and the Lemur
toolkit (Allan et al, 2003); and Terrier, a newer en-
gine designed to deal with corpora in the terabyte
range and to back applications entered into TREC
conferences (Ounis et al, 2005).
We also looked at both passage-level and
document-level retrieval. Passages can be de-
fined in a number of ways, such as a sentence,
a sliding window of k terms centred on the tar-
get term(s), parts of a document of fixed (and
equal) lengths, or a paragraph. In this case,
the documents in the AQUAINT corpus contain
paragraph markers which were used as passage-
level boundaries, thus making ?passage-level?
and ?paragraph-level? equivalent in this paper.
Passage-level retrieval may be preferable for AE,
as the number of potential distracters is some-
what reduced when compared to document-level
retrieval (Roberts and Gaizauskas, 2004).
The initial IR component configuration was with
Lucene indexing the AQUAINT corpus at passage-
level, with a Porter stemmer (Porter, 1980) and an
augmented version of the CACM (Jones and van
Rijsbergen, 1976) stopword list.
Indri natively supports document-level indexing
of TREC format corpora. Passage-level retrieval
was done using the paragraph tags defined in the
corpus as delimiters; this allows both passage- and
document-level retrieval from the same index, ac-
cording to the query.
All the IR engines were unified to use the Porter
36
Coverage Redundancy
Year Len. Strict Len. Strict
Lucene
2004 0.686 0.636 2.884 1.624
2005 0.703 0.566 2.780 1.155
2006 0.665 0.568 2.417 1.181
Indri
2004 0.690 0.554 3.849 1.527
2005 0.694 0.512 3.908 1.056
2006 0.691 0.552 3.373 1.152
Terrier
2004 - - - -
2005 - - - -
2006 0.638 0.493 2.520 1.000
Table 1: Performance of Lucene, Indri and Terrier at para-
graph level, over top 20 documents. This clearly shows the
limitations of the engines.
stemmer and the same CACM-derived stopword
list.
The top n documents for each question in the
TREC2004, TREC2005 and TREC2006 sets were
retrieved using every combination of engine, and
configuration1 . The questions and targets were
processed to produce IR queries as per the default
configuration for the QA framework. Examining
the top 200 documents gave a good compromise
between the time taken to run experiments (be-
tween 30 and 240 minutes each) and the amount
one can mine into the data. Tabulated results are
shown in Table 1 and Table 2. Queries have had
anaphora resolution performed in the context of
their series by the QA framework. AE compo-
nents begin to fail due to excess noise when pre-
sented with over 20 texts, so this value is enough to
encompass typical operating parameters and leave
space for discovery (Greenwood et al, 2006).
A failure analysis (FA) tool, an early version
of which is described by (Sanka, 2005), provided
reporting and analysis of IR component perfor-
mance. In this experiment, it provided high level
comparison of all engines, measuring coverage
and redundancy as the number of documents re-
trieved, n, varies. This is measured because a per-
fect engine will return the most useful documents
first, followed by others; thus, coverage will be
higher for that engine with low values of n.
3.2 Identification of Difficult Questions
Once the performance of an IR configuration over
a question set is known, it?s possible to produce
a simple report listing redundancy for each ques-
tion. A performance reporting script accesses the
1Save Terrier / TREC2004 / passage-level retrieval;
passage-level retrieval with Terrier was very slow using our
configuration, and could not be reliably performed using the
same Terrier instance as document-level retrieval.
Coverage Redundancy
Year Len. Strict Len. Strict
Indri
2004 0.926 0.837 7.841 2.663
2005 0.935 0.735 7.573 1.969
2006 0.882 0.741 6.872 1.958
Terrier
2004 0.919 0.806 7.186 2.380
2005 0.928 0.766 7.620 2.130
2006 0.983 0.783 6.339 2.067
Table 2: Performance of Indri and Terrier at document level
IR over the AQUAINT corpus, with n = 20
FA tool?s database and lists all the questions in
a particular set with the strict and lenient redun-
dancy for selected engines and configurations. En-
gines may use passage- or document-level config-
urations.
Data on the performance of the three engines is
described in Table 2. As can be seen, the cover-
age with passage-level retrieval (which was often
favoured, as the AE component performs best with
reduced amounts of text) languishes between 51%
and 71%, depending on the measurement method.
Failed anaphora resolution may contribute to this
figure, though no deficiencies were found upon vi-
sual inspection.
Not all documents containing answers are noted,
only those checked by the NIST judges (Bilotti
et al, 2004). Match judgements are incomplete,
leading to the potential generation of false nega-
tives, where a correct answer is found with com-
plete supporting information, but as the informa-
tion has not been manually flagged, the system will
mark this as a failure. Assessment methods are
fully detailed in Dang et al (2006). Factoid per-
formance is still relatively poor, although as only
1.95 documents match per question, this may be an
effect of such false negatives (Voorhees and Buck-
land, 2003). Work has been done into creating
synthetic corpora that include exhaustive answer
sets (Bilotti, 2004; Tellex et al, 2003; Lin and
Katz, 2005), but for the sake of consistency, and
easy comparison with both parallel work and prior
local results, the TREC judgements will be used to
evaluate systems in this paper.
Mean redundancy is also calculated for a num-
ber of IR engines. Difficult questions were those
for which no answer bearing texts were found by
either strict or lenient matches in any of the top n
documents, using a variety of engines. As soon as
one answer bearing document was found by an en-
gine using any measure, that question was deemed
non-difficult. Questions with mean redundancy of
37
zero are marked difficult, and subjected to further
analysis. Reducing the question set to just diffi-
cult questions produces a TREC-format file for re-
testing the IR component.
3.3 Extension of Difficult Questions
The documents deemed relevant by TREC must
contain some useful text that can help IR engine
performance. Such words should be revealed by
a gain in redundancy when used to extend an ini-
tially difficult query, usually signified by a change
from zero to a non-zero value (signifying that rele-
vant documents have been found where none were
before). In an attempt to identify where the use-
ful text is, the relevant documents for each difficult
question were retrieved, and passages matching the
answer regular expression identified. A script is
then used to build a list of terms from each passage,
removing words in the question or its target, words
that occur in the answer, and stopwords (based on
both the indexing stopword list, and a set of stems
common within the corpus). In later runs, num-
bers are also stripped out of the term list, as their
value is just as often confusing as useful (Baeza-
Yates and Ribeiro-Neto, 1999). Of course, answer
terms provide an obvious advantage that would not
be reproducible for questions where the answer is
unknown, and one of our goals is to help query ex-
pansion for unseen questions. This approach may
provide insights that will enable appropriate query
expansion where answers are not known.
Performance has been measured with both the
question followed by an extension (Q+E), as well
as the question followed by the target and then
extension candidates (Q+T+E). Runs were also
executed with just Q and Q+T, to provide non-
extended reference performance data points. Ad-
dition of the target often leads to gains in perfor-
mance (Roussinov et al, 2005), and may also aid
in cases where anaphora resolution has failed.
Some words are retained, such as titles, as in-
cluding these can be inferred from question or tar-
get terms and they will not unfairly boost redun-
dancy scores; for example, when searching for a
?Who? question containing the word ?military?,
one may want to preserve appellations such as
?Lt.? or ?Col.?, even if this term appears in the an-
swer.
This filtered list of extensions is then used to cre-
ate a revised query file, containing the base ques-
tion (with and without the target suffixed) as well
as new questions created by appending a candidate
extension word.
Results of retrievals with these new question are
loaded into the FA database and a report describ-
ing any performance changes is generated. The
extension generation process also creates custom
answer specifications, which replicate the informa-
tion found in the answers defined by TREC.
This whole process can be repeated with vary-
ing question difficulty thresholds, as well as alter-
native n values (typically from 5 to 100), different
engines, and various question sets.
3.4 Relevance Feedback Performance
Now that we can find the helpful extension words
(HEWs) described earlier, we?re equipped to eval-
uate query expansion methods. One simplistic ap-
proach could use blind RF to determine candidate
extensions, and be considered potentially success-
ful should these words be found in the set of HEWs
for a query. For this, term frequencies can be
measured given the top r documents retrieved us-
ing anaphora-resolved query Q. After stopword
and question word removal, frequent terms are ap-
pended to Q, which is then re-evaluated. This
has been previously attempted for factoid ques-
tions (Roussinov et al, 2005) and with a limited
range of r values (Monz, 2003) but not validated
using a set of data-driven terms.
We investigated how likely term frequency (TF)
based RF is to discover HEWs. To do this, the
proportion of HEWs that occurred in initially re-
trieved texts was measured, as well as the propor-
tion of these texts containing at least one HEW.
Also, to see how effective an expansion method is,
suggested expansion terms can be checked against
the HEW list.
We used both the top 5 and the top 50 documents
in formulation of extension terms, with TF as a
ranking measure; 50 is significantly larger than the
optimal number of documents for AE (20), without
overly diluting term frequencies.
Problems have been found with using entire
documents for RF, as the topic may not be the
same throughout the entire discourse (Robertson
et al, 1992). Limiting the texts used for RF to
paragraphs may reduce noise; both document- and
paragraph-level terms should be checked.
38
Engine
Year Lucene
Para
Indri
Para
Indri
Doc
Terrier
Doc
2004 76 72 37 42
2005 87 98 37 35
2006 108 118 59 53
Table 3: Number of difficult questions, as defined by those
which have zero redundancy over both strict and lenient mea-
sures, at n = 20. Questions seem to get harder each year.
Document retrieval yields fewer difficult questions, as more
text is returned for potential matching.
Engine
Lucene Indri Terrier
Paragraph 226 221 -
Document - 121 109
Table 4: Number of difficult questions in the 2006 task, as de-
fined above, this time with n = 5. Questions become harder
as fewer chances are given to provide relevant documents.
4 Results
Once we have HEWs, we can determine if these
are going to be of significant help when chosen as
query extensions. We can also determine if a query
expansion method is likely to be fruitful. Blind RF
was applied, and assessed using the helpful words
list, as well as RF?s effect on coverage.
4.1 Difficult Question Analysis
The number of difficult questions found at n =
20 is shown in Table 3. Document-level retrieval
gave many fewer difficult questions, as the amount
of text retrieved gave a higher chance of finding
lenient matches. A comparison of strict and lenient
matching is in Table 5.
Extensions were then applied to difficult ques-
tions, with or without the target. The performance
of these extensions is shown in Table 6. Results
show a significant proportion (74.4%) of difficult
questions can benefit from being extended with
non-answer words found in answer bearing texts.
4.2 Applying Relevance Feedback
Identifying HEWs provides a set of words that
are useful for evaluating potential expansion terms.
Match type
Strict Lenient
Year
2004 39 49
2005 56 66
2006 53 49
Table 5: Common difficult questions (over all three engines
mentioned above) by year and match type; n = 20.
Difficult questions used 118
Variations tested 6683
Questions that benefited 87 (74.4%)
Helpful extension words (strict) 4973
Mean helpful words per question 42.144
Mean redundancy increase 3.958
Table 6: Using Terrier Passage / strict matching, retrieving 20
docs, with TREC2006 questions / AQUAINT. Difficult ques-
tions are those where no strict matches are found in the top 20
IRT from just one engine.
2004 2005 2006
HEW found in IRT 4.17% 18.58% 8.94%
IRT containing HEW 10.00% 33.33% 34.29%
RF words in HEW 1.25% 1.67% 5.71%
Table 7: ?Helpful extension words?: the set of extensions that,
when added to the query, move redundancy above zero. r =
5, n = 20, using Indri at passage level.
Using simple TF based feedback (see Section 3.4),
5 terms were chosen per query. These words had
some intersection (see Table 7) with the exten-
sion words set, indicating that this RF may lead to
performance increases for previously unseen ques-
tions. Only a small number of the HEWs occur in
the initially retrieved texts (IRTs), although a no-
ticeable proportion of IRTs (up to 34.29%) contain
at least one HEW. However, these terms are prob-
ably not very frequent in the documents and un-
likely to be selected with TF-based blind RF. The
mean proportion of RF selected terms that were
HEWs was only 2.88%. Blind RF for question an-
swering fails here due to this low proportion. Strict
measures are used for evaluation as we are inter-
ested in finding documents which were not pre-
viously being retrieved rather than changes in the
distribution of keywords in IRT.
Document and passage based RF term selection
is used, to explore the effect of noise on terms, and
document based term selection proved marginally
superior. Choosing RF terms from a small set of
documents (r = 5) was found to be marginally
better than choosing from a larger set (r = 50).
In support of the suggestion that RF would be un-
r
5 50 Baseline
Rank Doc Para Doc Para
5 0.253 0.251 0.240 0.179 0.312
10 0.331 0.347 0.331 0.284 0.434
20 0.438 0.444 0.438 0.398 0.553
50 0.583 0.577 0.577 0.552 0.634
Table 8: Coverage (strict) using blind RF. Both document-
and paragraph-level retrieval used to determine RF terms.
39
Question:
Who was the nominal leader after the overthrow?
Target: Pakistani government overthrown in 1999
Extension word Redundancy
Kashmir 4
Pakistan 4
Islamabad 2.5
Question: Where did he play in college?
Target: Warren Moon
Extension word Redundancy
NFL 2.5
football 1
Question: Who have commanded the division?
Target: 82nd Airborne division
Extension word Redundancy
Gen 3
Col 2
decimated 2
officer 1
Table 9: Queries with extensions, and their mean redundancy
using Indri at document level with n = 20. Without exten-
sions, redundancy is zero.
likely to locate HEWs, applying blind RF consis-
tently hampered overall coverage (Table 8).
5 Discussion
HEWs are often found in answer bearing texts,
though these are hard to identify through sim-
ple TF-based RF. A majority of difficult questions
can be made accessible through addition of HEWs
present in answer bearing texts, and work to deter-
mine a relationship between words found in initial
retrieval and these HEWs can lead to coverage in-
creases. HEWs also provide an effective means
of evaluating other RF methods, which can be de-
veloped into a generic rapid testing tool for query
expansion techniques. TF-based RF, while finding
some HEWs, is not effective at discovering exten-
sions, and reduces overall IR performance.
There was not a large performance change
between engines and configurations. Strict
paragraph-level coverage never topped 65%, leav-
ing a significant number of questions where no
useful information could be provided for AE.
The original sets of difficult questions for in-
dividual engines were small ? often less than the
35% suggested when looking at the coverage fig-
ures. Possible causes could include:
Difficult questions being defined as those for
which average redundancy is zero: This limit
may be too low. To remedy this, we could increase
the redundancy limit to specify an arbitrary num-
ber of difficult questions out of the whole set.
The use of both strict and lenient measures: It
is possible to get a lenient match (thus marking a
question as non-difficult) when the answer text oc-
curs out of context.
Reducing n from 20 to 5 (Table 4) increased
the number of difficult questions produced. From
this we can hypothesise that although many search
engines are succeeding in returning useful docu-
ments (where available), the distribution of these
documents over the available ranks is not one that
bunches high ranking documents up as those im-
mediately retrieved (unlike a perfect engine; see
Section 3.1), but rather suggests a more even dis-
tribution of such documents over the returned set.
The number of candidate extension words for
queries (even after filtering) is often in the range
of hundreds to thousands. Each of these words
creates a separate query, and there are two varia-
tions, depending on whether the target is included
in the search terms or not. Thus, a large number
of extended queries need to be executed for each
question run. Passage-level retrieval returns less
text, which has two advantages: firstly, it reduces
the scope for false positives in lenient matching;
secondly, it is easier to scan result by eye and de-
termine why the engine selected a result.
Proper nouns are often helpful as extensions.
We noticed that these cropped up fairly regularly
for some kinds of question (e.g. ?Who?). Espe-
cially useful were proper nouns associated with
locations - for example, adding ?Pakistani? to
a query containing the word Pakistan lifted re-
dundancy above zero for a question on President
Musharraf, as in Table 9. This reconfirms work
done by Greenwood (2004b).
6 Conclusion and Future Work
IR engines find some questions very difficult and
consistently fail to retrieve useful texts even with
high values of n. This behaviour is common over
many engines. Paragraph level retrieval seems to
give a better idea of which questions are hard-
est, although the possibility of false negatives is
present from answer lists and anaphora resolution.
Relationships exist between query words and
helpful words from answer documents (e.g. with
a military leadership themes in a query, adding the
term ?general? or ?gen? helps). Identification of
HEWs has potential use in query expansion. They
could be used to evaluate RF approaches, or asso-
ciated with question words and used as extensions.
Previous work has ruled out relevance feedback
40
in particular circumstances using a single ranking
measure, though this has not been based on analy-
sis of answer bearing texts. The presence of HEWs
in IRT for difficult questions shows that guided RF
may work, but this will be difficult to pursue. Blind
RF based on term frequencies does not increase IR
performance. However, there is an intersection be-
tween words in initially retrieved texts and words
data driven analysis defines as helpful, showing
promise for alternative RF methods (e.g. based on
TFIDF). These extension words form a basis for
indicating the usefulness of RF and query expan-
sion techniques.
In this paper, we have chosen to explore only
one branch of query expansion. An alternative data
driven approach would be to build associations be-
tween recurrently useful terms given question con-
tent. Question texts could be stripped of stopwords
and proper nouns, and a list of HEWs associated
with each remaining term. To reduce noise, the
number of times a particular extension has helped
a word would be counted. Given sufficient sample
data, this would provide a reference body of HEWs
to be used as an aid to query expansion.
References
Allan, J., J. Callan, K. Collins-Thompson, B. Croft,
F. Feng, D. Fisher, J. Lafferty, L. Larkey, TN Truong,
P. Ogilvie, et al 2003. The Lemur Toolkit for Lan-
guage Modeling and Information Retrieval.
Allan, J. 1996. Incremental Relevance Feedback for
Information Filtering. In Research and Development
in IR, pages 270?278.
Baeza-Yates, R. and B. Ribeiro-Neto. 1999. Modern
Information Retrieval. Addison Wesley.
Bilotti, M.W., B. Katz, and J. Lin. 2004. What Works
Better for Question Answering: Stemming or Mor-
phological Query Expansion. Proc. IR for QA Work-
shop at SIGIR 2004.
Bilotti, M.W. 2004. Query Expansion Techniques for
Question Answering. Master?s thesis, Massachusetts
Institute of Technology.
Dang, H.T., J. Lin, and D. Kelly. 2006. Overview of
the TREC 2006 QA track. Proc. 15th Text REtrieval
Conf..
Greenwood, M.A., M. Stevenson, and R. Gaizauskas.
2006. The University of Sheffield?s TREC 2006
Q&A Experiments. In Proc. 15th Text REtrieval
Conference
Greenwood, M.A. 2004a. AnswerFinder: Question
Answering from your Desktop. In Proc. 7th Annual
Colloquium for the UK SIG for Computational Lin-
guistics (CLUK ?04).
Greenwood, M.A. 2004b. Using Pertainyms to Im-
prove Passage Retrieval for Questions Requesting
Information about a Location. In Proc. Workshop
on IR for QA (SIGIR 2004).
Jones, K.S. and C.J. van Rijsbergen. 1976. IR Test
Collections. J. of Documentation, 32(1):59?75.
Lin, J. and B. Katz. 2005. Building a Reusable Test
Collection for Question Answering. J. American So-
ciety for Information Science and Technology.
Monz, C. 2003. From Document Retrieval to Question
Answering. ILLC Dissertation Series 2003, 4.
Ounis, I., G. Amati, V. Plachouras, B. He, C. Macdon-
ald, and D. Johnson. 2005. Terrier IR Platform.
Proc. 27th European Conf. on IR (ECIR 05), San-
tiago de Compostela, Spain, pages 517?519.
Pizzato, L.A., D. Molla, and C. Paris. 2006. Pseudo-
Relevance Feedback using Named Entities for Ques-
tion Answering. Australasian Language Technology
Workshop (ALTW2006), pages 83?90.
Porter, M. 1980. An Algorithm for Suffix Stripping
Program. Program, 14(3):130?137.
Roberts, I and R Gaizauskas. 2004. Evaluating Passage
Retrieval Approaches for Question Answering. In
Proc. 26th European Conf. on IR.
Robertson, S.E., S. Walker, M. Hancock-Beaulieu,
A. Gull, and M. Lau. 1992. Okapi at TREC. In
Text REtrieval Conf., pages 21?30.
Roussinov, D. and W. Fan. 2005. Discretization Based
Learning Approach to Information Retrieval. In
Proc. 2005 Conf. on Human Language Technologies.
Roussinov, D., M. Chau, E. Filatova, and J.A. Robles-
Flores. 2005. Building on Redundancy: Fac-
toid Question Answering, Robust Retrieval and the
?Other?. In Proc. 14th Text REtrieval Conf.
Sanka, Atheesh. 2005. Passage Retrieval for Question
Answering. Master?s thesis, University of Sheffield.
Tellex, S., B. Katz, J. Lin, A. Fernandes, and G. Marton.
2003. Quantitative Evaluation of Passage Retrieval
Algorithms for Question Answering. Proc. 26th An-
nual Int?l ACM SIGIR Conf. on R&D in IR, pages
41?47.
Voorhees, E. and L. P. Buckland, editors. 2003. Proc.
12th Text REtrieval Conference.
41
