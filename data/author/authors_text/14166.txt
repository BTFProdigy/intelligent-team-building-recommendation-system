Coling 2010: Poster Volume, pages 1238?1246,
Beijing, August 2010
Semi-Supervised WSD in Selectional Preferences
with Semantic Redundancy 
Xuri TANG1,5  , Xiaohe CHEN1 , Weiguang QU2,3 and Shiwen YU4
1. School of Chinese Language and Literature, Nanjing Normal University 
{xrtang,chenxiaohe5209}@126.com
2. Jiangsu Research Center of Information Security & Privacy Technology
3. School of Computer Science, Nanjing Normal University 
 wgqu_nj@163.com 
4. Institute of Computational Linguistics , Peking University 
yusw@pku.edu.cn
5. College of Foreign Studies, Wuhan Textile University 
Abstract
This paper proposes a semi-supervised 
approach for WSD in Word-Class 
based selectional preferences. The 
approach exploits syntagmatic and 
paradigmatic semantic redundancy in 
the semantic system and uses 
association computation and minimum 
description length for the task of WSD. 
Experiments on Predicate-Object 
collocations and Subject-Predicate 
collocations with polysemous 
predicates in Chinese show that the 
proposed approach achieves a precision 
which is 8% higher than the semantic-
association based baseline. The semi-
supervised nature of the approach 
makes it promising for constructing 
large scale selectional preference 
knowledge base. 
1 Introduction 
This paper addresses word sense 
disambiguation (WSD) which is required in 
the construction of selectional preference (SP) 
knowledge database. In previous literature of 
SP, four different types of formalization 
models are explicitly or implicitly employed. 
Two types are distinguished in Li and 
Abe(1998):
Word Model: ?=),|( rvnP     (1) 
Class Model: ?=),|( rvCP         (2) 
where v stands for verb, n for noun, C for the 
semantic class of n, r for the grammatical 
relation between v and n, and P for the 
preference strength. Most of the 
researches(Resnik 1996; Li and Abe 1998; 
Ciaramita and Johnson 2000; Brockmann and 
Lapata 2003; Light and Greiff 2002) uses the 
class model, and a few(Erk 2007) uses the 
word model. The other two types of model are 
given as below: 
Class-Only Model: ?=),|( rCCP vn        (3) 
Word-Class Model: ?=),,|,( rCvCnP vn  (4) 
where ,  are semantic classes for the 
noun and verb respectively. Class-Only model 
considers solely the semantic classes, while 
Word-Class model considers both words and 
semantic classes. Agirre and Martinez(2001) 
and Zheng et al2007) adopted the Class-only 
Model in research, while in McCarthy and  
Carroll(2003) and Merlo and Stevenson(2001) 
the Word-Class Model is employed. 
nC vC
Among the four models, the Word-Class 
Model is the type which possesses the most 
granulated knowledge and is the most potential 
in applications. McCarthy and Carroll(2003) 
reports that the Word-Class Model performs 
well in unsupervised WSD. In other NLP tasks 
such as metaphor recognition, this model may 
be indispensable. For instance, to distinguish 
the  predicate verb  ???(float)?  in  Ex(1a) as 
Ex. 1
a.
? ? ? ?
leaf floats b.
? ? ? ?
price floats
1238
literal and Ex(1b) as metaphorical requires 
different interpretations of the verb.  
The present research is concerned with 
WSD as in the Word-Class model. Particularly, 
it aims at disambiguating predicates in subject-
predicate (Subj-Pred) and predicate-object 
(Pred-Obj) constructions. The motivations 
behind the research are two folds. Firstly, 
semi-supervised and unsupervised WSD in SP 
are not fully explored. Merlo and 
Stevenson(Merlo and Stevenson 2001) 
employs supervised learning from large 
annotated corpus, which is difficult to obtain. 
One known unsupervised learning approach 
for WSD in SP is McCarthy and Carroll(2003) 
which addresses the issue via conditional 
probability. The other motivation derives from 
the fact few research is done on selectional 
preferences in languages other than English, as 
is stated in Brockmann and Lapata(2003). For 
instance, studies on construction of SP 
knowledge database in Chinese can only be 
found in Wu et al2005), Zhen et al2007), Jia 
and Yu(2008) and some others.  
The basic idea of the approach proposed for 
WSD in the paper is that the most acceptable 
interpretation of senses for a given 
construction is the pair of senses which 
encodes the most redundant information in the 
semantic system of the language. Two 
principles, namely Syntagmatic Redundancy 
Principle and Paradigmatic Redundancy 
Principle, are proposed in the paper to capture 
the intuition. Two corresponding devices are 
employed to model the two principles: 
Association for Syntagmatic Redundancy 
Principle and Minimum Description Length for 
Paradigmatic Redundancy Principle. Two 
experiments are conducted in the paper. The 
first is based on semantic association, 
achieving a 61.98% precision for predicates in 
Subj-Preds and 62.54% in Pred-Objs. This 
experiment is used as baseline as the approach 
is also used in McCarthy and Carroll(2003) for 
verb and adjective disambiguation. In the 
second experiment, both semantic association 
and MDL are employed, the precision of WSD 
amounts to 69.88% and 69.09% for predicates 
in Subj-Preds and Pred-Objs respectively, 
indicating that a combination of the two 
devices are fairly effective in disambiguating 
word senses for SP. 
The rest of the paper is organized as below. 
The second part gives further illustration of the 
rationale for the approach. The third part 
describes the procedure and the fourth part 
discusses the experiment result. The thesis 
concludes with some speculations in further 
researches. 
2 Rationale
2.1 Task Formalization 
Consider a Subj-Pred or Pred-Obj 
collocation C=< , > , where  is 
the word of predicate and  is the word of 
argument.  has M senses, denoted by set 
.  has N senses, denoted by .
The possible interpretation of C has M*N 
possibilities, denoted by 
={ | =< , >},
where  is called a sense collocation. The 
task of WSD is to search for a particular sense 
collocation in  and assign it to C as its 
interpretation. At the initial stage, each sense 
collocation in  is considered to have an 
even number of frequency, namely 
. Accordingly, for each 
, , For each 
, .
predW
arg ?
CS
CS
)M?
sf ipred(
s i 1)arg =
argW
W
i
j
i
j
?
) =
N/
predW
argS
jsarg
arg
M/1
predW
arg
S?
/(1 N
pred
f (
predS
SC =
(f ij?
i
preds
args i ?
W
S pred
i
j
?
=
S
argS
i
preds
)
?
2.2 Syntagmatic Redundancy Principle 
Syntagmatic Redundancy Principle (SRP) 
can be stated as following: among all possible 
sense collocations for a word collocation, the 
most appropriate is the one in which senses 
exhibit the most redundant information 
between each other. 
 The syntagmatic redundancy between 
words has been noticed very early by linguists 
and has been applied in WSD. Firth(1957) 
argues that there exists ?mutual expectancy? 
between words in collocations, and the 
meaning of word is partially encoded in its 
juxtaposition. Lyons(1977:261) comments that 
Porzig has noticed in 1934 the ?essential 
meaning relation? between words of 
collocations like ?dog barks? and ?tree fells? 
1239
and emphasizes that the meanings of 
collocationally restricted lexemes such as 
?bark? and ?fell? can only be explained by 
taking into account the collocates they occur 
with. This notion is also employed in 
Yarowsky(1995) for WSD, in which the key is 
the ?one-sense-per-collocation? statement. 
McCarthy and Carroll(2003) also uses this 
type of redundancy for disambiguation in SP.  
SRP can be explained as a statistic 
correlation between  and . The more 
co-relevant these two senses are, the more 
likely the pair is to be accepted as the 
appropriate interpretation.  This can be 
described as below: 
preds args
),(maxarg arg
ji
pred
i
j ssAssoc=?    (5) 
where is the function for 
sense association. Four methods can be 
considered for association computation: 
conditional probability (Formula 6 and 7), 
Lift(Han and Kamber 2006:261) (Formula 8), 
All-Confidence(Han and Kamber 2006:263)  
(Formula 9) and cosine (Formula 10). Note 
that two versions of conditional probability are 
considered, as are denoted in Formula 6 and 7. 
The first version, Cond-Prob 1, takes argument 
sense as condition, while the second version 
Cond-Prob 2 takes predicate sense as condition. 
  ),( arg
ji
pred ssAssoc
)(
),(
)|(
arg
arg
arg j
ij
predji
pred sp
ssp
ssp =             (6) 
)(
),(
)|( argarg i
pred
ij
predj
pred
i
sp
ssp
ssp =                 (7) 
)(*)(
),(
),(
arg
arg
arg ji
pred
ji
predji
pred spsp
ssp
sslift =              (8) 
))(),(max(
),(
),(_
arg
arg
arg j
pred
i
ji
predji
pred cfsf
ssf
ssconfall =         (9) 
)(*)(
),(
),(cos
arg
arg
arg ji
pred
ji
predji
pred
spsp
ssp
ssine =            (10) 
2.3 Paradigmatic Redundancy Principle 
Paradigmatic Redundancy Principle (PRP) 
can be stated as following: among all possible 
sense collocations for a word collocation, the 
most appropriate is the one which is also 
implicitly or explicitly expressed by other 
synonymous, metonymic or metaphorical word 
collocations.
Ex(2) illustrates the explicit redundancy in 
synonymous and metaphorical ways, in which 
the sense collocation ?[Price| ? ? ]
[QuantityChange|??]? is expressed by five 
word collocations, each with a different 
predicate : ?? (change), ?? (float), ??
(adjust),??(go up and down), ??(alter).  
Ex 2.
a.
????
price changes     b.
????
price floats     c.
? ? ? ?
price adjusts
d.
? ? ? ?
SULFHJRHVXSDQGGRZQe. ????price alters
Ex(3) reveals the implicit redundancy in 
metonymic way, in which the meaning ??
(human) ? ? (is eased)? is implicitly 
expressed in all the six collocations, 
established by  semantic relatedness among the 
arguments ????? (Maradona)?, ???
(student)?, ???(work)?, ???(labour)?, ??
?(driving)?, and ???(life)?.
Ex 3. 
a.
???????
Maradona is eased         b.
?? ?? ?
Student is eased
c.
?????
work is eased               d. 
?? ?? ?
labour is eased
e.
?? ?? ?
driving is eased             f. 
?????
Life is eased
To apply PRP, WSD in SP is casted as an 
issue of model selection. Given a set of word 
collocations , the process of WSD is to 
assign to each word collocation one sense 
collocation from a number of possibilities. 
Those assigned sense collocations form a set, 
or a model for ? . The goal of WSD in SP is 
to select from all those models the one which 
best interprets ? . For this purpose, Miminum 
Description Length(Barron et al 1998; Michell  
2003; MacKay 2003) can be used. MDL 
selects models by relying on induction bias 
based on Occam?s Razor, which stipulates that 
the simplest solution is usually the correct one. 
One way to interpret MDL in Bays? analysis is 
as below(Michell 2003:124): 
?
)|()(minarg' mDLmLm DM +=            (11) 
In (11)?  is the model description 
length when model m  is considered, 
 is the data description length when 
model  is used for description. The model 
with minimum length is the best model. 
)(mLM
)|( mDLD
m
1240
For model description length, we have 
adopted the method used in (Li and Abe 1998) 
which considers only the size of the model: 
)log(
2
1)(
)( NmsizemLM
?
=                   (12) 
where size(m) is the number of sense 
collocation contained in model m , and N is 
the number of word collocation in 
consideration. In this study, the set of word 
collocation with the same predicate word, 
denoted by ? , is used as the unit for model 
description length calculation instead of the 
whole corpus, so as to reduce computation 
complexity. Accordingly, each word 
collocation in ?  can be assigned one and only 
one sense collocation in the model m , out of 
all the potential sense collocations as is 
explained in section 2.1. 
Data description length is calculated on 
model and , as is denoted in formulas 
(13),  (14)  and (15) below. The  calculation  is   
m ?
??
?
?=?=? )
)(
log())(log()|(
2Num
f
pmL
i
ji
j
?
?
         (13) 
?
?
+=
m
k
l
i
j
i
j
i
j
i
j
k
l
i
j
wfff
??
?????
,
),(*)()()(         (14) 
??
?
?
?
?
=
=
><><=
k
pred
k
pred
ilj
lk
pred
ji
pred
k
l
i
j
s
sssrel
ssssrelw
i
pred
predargarg
argarg
s if                                 0
s if                ),(
             
),,,(),( ??
 (15) 
based on the probability of sense collocation 
, which in turn is calculated on a 
modified frequency of the collocation 
>=< jipred
i
j ss arg,?
)( ijf ? .
The frequency is modified by counting the 
explicit occurrence of the sense collocation 
itself and the implicit occurrence expressed by 
other sense collocations in ? . This idea is 
equivalent to enlarge the corpus by 1 fold, thus 
the overall collocation number is the two times 
of the original number.  
The modified frequency is a sum of two 
parts, denoted in formula (14). The first part is 
, the frequency of . The second part is 
the weighted frequency of . The weight is 
determined by the relatedness of the sense 
collocation  and all the other sense 
collocation in the model m. According to
this formula, if the sense collocation is found 
to be more similar to other sense collocations, 
it should obtain a higher modified frequency, 
and thus more likely to be the correct one for 
the word collocation.
)( ijf ?
i
j?
i
j?
i
j?
k
l?
The way to calculate the weight is given in 
formula (15). If two sense collocations have 
identical predicate sense, namely ,
then the weight between the two sense 
collocations is measured by rel , the 
semantic relatedness between the argument 
sense and . Otherwise, 0 is returned. 
There are different ways to measure sense 
relatedness. The present study has used 
semantic similarity based on HowNet(Liu and 
i 2002) to calculate the semantic relatedness. 
k
pred
i s=preds
),( argarg
lj ss
jsarg lsarg
L
3 Procedure
Figure 1 maps out the procedure for WSD in 
SP in the present study. The procedure is 
divided into two phases: data collection and 
disambiguation. The collocation data are 
collected from three sources: Sketch Engine, 
Collocation Dictionary and HowNet Examples. 
Two types of collocation data are collected: 
subject-predicate collocations (Subj-Pred) and 
predicate-object collocations (Pred-Obj) from 
Sketch Engine and Collocation Dictionary. 
Collocation Retriever reduces HowNet 
examples into Subj-Preds and Pred-Objs using 
simple heuristic methods. As a result, about 
70,000 subject-predicate collocations and 
106,000 predicate-object collocations are 
obtained.
Figure 1. WSD Procedure 
In disambiguation phase, two devices are
employed to filter out unlikely sense 
collocations: Association-Based Sense 
Collocation Filter, following SRP, and MDL-
Based Sense Collocation Filter, following PRP. 
Colloc Dict. 
HowNet Examples
MDL-Based Sense Colloc Filter
Assoc-Based Sense Colloc Filter
Collocation Retriever
Data Combination
Sketch Engine
Output
1241
In this phase, Subj-Preds and Pred-Objs are 
processed independently but following the 
same route.   
Each phase alone can perform WSD 
independently. Accordingly, two experiments 
are conducted to evaluate the method proposed 
in this paper. The first experiment uses 
association-based filter for word sense 
disambiguation, which is also used as the 
baseline. The approach is also used in 
(McCarthy and Carroll 2003) to disambiguate 
verbs and adjectives in collocations. To be 
particular, the method used by McCarthy and 
Carroll(2003) is formula (6). The second 
experiment is based on the result of the first 
one so as to observe the improvement obtained 
by MDL-Based approach. In the second 
experiment, unsupervised and semi-supervised 
WSD are also investigated by including some 
annotated collocations in the evaluation data. 
Two corpora are constructed for evaluation. 
One corpus is a set of 1034 subject-predicate 
constructions. The other is a set of 1841 
predicate-object constructions. Both are 
manually annotated by the authors with sense 
definitions defined in HowNet(Dong 2006). 
All together there are 52 highly ambiguous 
predicates involved in the study. 
4 Experiments and Discussion 
4.1 Collocation Retriever 
The major task in data collocation is in 
Collocation Retriever, which retrieves 
collocations from HowNet examples. Ex(4) 
gives a partial entry structure in HowNet,  
Ex 4. 
W_C=??
E_C=??~???~???????
?~
DEF=[change|?]
in which W_C stands for Chinese Word, DEF 
for definition, E_C for Examples of Chinese, 
and the wave ?~? for the word in question. 
From E_C, possible Subj-Preds such as ???
(public opinion) ??(floats)?, ???(index) 
??(floats)? can be retrieved, in which the 
sense of ???(float)? is annotated with DEF. 
But there are also noises. A simple heuristic 
method is applied to automatically filter out 
unwanted collocations. The heuristic method 
checks whether the collocation retrieved from 
HowNet share possible sense collocations with 
collocations in Collocation Dictionary. If yes, 
it is accepted as a collocation of the type, 
otherwise, it is rejected. Procedures are given 
below:
(a) Use Subj-Pred collocations and Pred-Obj 
collocations in Collocation Dictionary to build 
sense collocation set edSubj Pr?? and Objed??Pr ;
(b) For each example sentence in E_C, 
segment it using ICTCLAS1 to obtain an array 
of words. Words before ?~? forms potential 
Subj-Pred collocations and Words 
after form potential Pred-Obj collocations 
.
edSubj Pr??
ObjedB ?Pr
(c) For each or ,
construct possible sense collocation set 
edSubja Pr??? edSubjBb Pr??
a?  or 
b? , if ?????
? edSubja Pr
 or ?????
?Objedb Pr , add 
it as a Subj-Pred collocation or Pred-Obj 
collocation.
Evaluation on partial retrieved collocations 
shows that about 70% of obtained collocations 
are valid collocations, while about 30% are 
errors. Thus manual edition has been applied 
to rid those invalid collocations. 
4.2 Association-Based Filter 
Association-Based Sense Collocation Filter 
filters out those sense collocations that are very 
unlikely to be the right interpretation for a 
word collocation. Table 1 gives association 
computation result for the six senses related to 
the predicate ? ? (rough)? in Subj-Pred 
collocation ??? (personality) ? (rough)?. 
The 2nd , 3rd, 4th, and 6th are very unlikely 
interpretations and should be filtered, while the 
5th seems to be the most appropriate. 
Table 1. Association-Based Filter Example 
No.Pred Sense Arg Sense Assoc. Dgr
1 [Behavior|??][careless|??] 0.0019
2 [Behavior|??][coarse|?] 0.0002
3 [Behavior|??][hoarse|??] 0.0004
4 [Behavior|??][roughly|??] 0.0002
5 [Behavior|??][vulgar|?] 0.0071
6 [Behavior|??][widediameter|?] 0.0002
Following the procedure in Figure 1, to filter 
out those unlikely sense collocations, average 
1 A Chinese segmentation system, please refer to 
http://www.ictclas.org for further information. 
1242
association value is used as the filter and those 
below the average are dropped and those above 
are chosen for MDL-Based Filter. In Table 1, 
the average is 0.0017, and the 1rd and 5th are 
chosen.
However, in order to obtain a baseline and 
to decide which association computation 
model to use, we have followed the definition 
in Formula 5 and perform WSD test by 
choosing the sense collocation with highest 
association as the correct sense tags. for used 
this step solely for WSD, as is defined in 
Formula 4. Table 2 gives the experiment 
results for Subj-Pred and Pred-Obj collocations 
with all the association computation models 
denoted in Formula 6-10.
Table 2. WSD Result by Association 
Subj-Pred(%) Pred-Obj(%)
Cond-Prob 1 61.98 62.54
Cond-Prob 2 55.15 42.4
Lift 63.09 40.84
All_Conf 56.16 48.54
Cosine 58.83 55.72
One interesting phenomenon about all the 
five models is null-invariance. In selecting 
models for association computation, null-
invariance is an important feature to be 
considered(Han and Kamber 2006). A model 
with null-invariance is not influenced by 
additional irrelevant data and thus is more 
stable. In the experiment, the model Lift is the 
only one not featured with null-invariance. The 
experiments show that Lift is not stable in 
different collocation types, achieving high 
precision in Subj-Pred but low precision in 
Pred_Obj.
A second interesting phenomenon is 
collocation directionality exposed by the 
experiments, which can be observed in the two 
models of conditional probability: Cond-Prob 
1, with argument as condition, and Cond-Prob 
2, with predicate as condition. Directionality in 
collocation has been noticed earlier in some 
researches, for example Qu(2008). Our 
experiment shows that when using Cond-Prob 
1, we are able to get a precision of 61.98% and 
62.54% for Subj-Pred and Pred-Obj 
respectively, while Cond-Prob 2 gets a much 
lower precision. This fact can be interpreted 
that arguments tend to have a stronger 
selectional preference strength, and the 
possible selection range is comparatively 
narrower, while predicates have weaker 
selectional preference strength and a wider 
selectional range. 
4.3 MDL-Based Filter 
MDL-Based Filter takes as input result from 
Association-Based Filter using Cond-Prob 1 
for association computation and average 
association as filter. Table 3 and 4 give the 
final experiment outcome for Pred-Obj and 
Subj-Pred constructions and individual 
predicates.
It can be seen in Table 3 that MDL-Based 
Filter Several inferences can be made from the 
experiments. Firstly, comparison between 
Association-Based WSD (Table 2) and MDL 
WSD (Table 3) shows that MDL can improve 
overall performance up to 8%. As is mentioned 
earlier, Association-Based WSD is used as 
baseline in the present study. Given the fact 
that the average number of senses for word in 
question is fairly high, the improvement is 
considered as significant.  
Table 3. General WSD Results2
Ave. 
N.O.S.
Assoc.  
WSD (%) 
MDL
WSD (%) 
Subj-Pred 4.16 61.98 69.09
Pred-Obj 5.03 62.54 69.88
Analysis on the individual predicates in 
Table 4 gives a clearer picture of WDL-based 
WSD. Firstly, it can be seen that MDL is 
especially effective when the demarcation of 
word senses is clear-cut. Predicate words such 
as ??? (quiet)?, ??? (dirty)?, ???
(difficult)? in Subj-Preds and ??? (beat)?, 
???(touch)? and ???(break)? in Pred-Objs 
are successfully disambiguated in Table 4. 
These words generally have 2 or 3 senses, and 
the   senses    generally    differ    in   terms   of 
abstractness and concreteness, as is indicated 
in table 5. This is due to the fact that the 
arguments in these collocations are clearly 
delimitated in HowNet and this delimitation is 
well captured by the modified frequency 
calculation defined in formula (14). Via the 
formula, the concrete sense collocations can  
2 In Table 3 and 4, Ave. N.O.S stands for average number 
of senses of predicates, N.O.S stands for number of 
senses of the predicate, Assoc. WSD stands for 
Association-based WSD, and MDL WSD stands for 
MDL-based WSD. 
1243
Table 4. Detailed WSD Experiment Results 
Results for Pred-Obj. Results for Subj-Pred. 
Pred. 
N. 
O. 
S
Assoc. 
WSD
(%)
MDL
WSD
(%)
Pred. 
N. 
O. 
S. 
Assoc.
WSD
(%)
MDL
WSD
(%)
?(v) 5 69.23 80.77 ??(a) 2 61.14 92.00
?(v) 14 70.59 70.59 ?(v) 2 72.73 86.36
?(v) 6 56.25 90.62 ??(a) 2 47.83 58.7
??(v) 3 72.22 88.89 ?(a/v) 5 52.17 78.26
?(v) 9 50 60.53 ??(a) 3 56.76 81.08
?(v) 8 86.67 93.33 ?(a) 5 40 40 
?(v) 5 68.75 62.5 ??(v) 2 55.17 41.38
??(v) 3 73.91 81.16 ??(a) 3 75.76 93.94
?(v) 17 55.93 44.07 ?(a) 4 96.3 66.67
??(v) 3 80.36 78.57 ??(a) 3 47.37 42.11
??(v) 2 66.67 92.31 ?(a) 6 88.24 88.24
??(v) 2 57.14 80.95 ?(a) 6 46 60 
?(v) 6 76.27 79.66 ??(v) 3 44.44 44.44
??(v) 3 83.33 100 ??(a) 2 38.46 65.38
?(v) 8 63.64 63.64 ??(a) 2 93.33 53.33
?(v) 3 77.14 80 ??(v) 3 85.19 88.89
??(v) 2 88.24 100 ?(a) 10 50 50 
??(v) 2 83.87 80.65 ??(v) 2 60.53 63.16
?(v) 9 61.84 68.42 ?(a/v) 9 39.66 53.45
??(v) 3 40.28 51.39 ?(a) 6 59.46 51.35
?(v) 4 48.08 53.85 ?(v) 6 48.72 74.36
??(v) 3 73.49 73.49 ??(v) 3 48.15 44.44
??(v) 2 15.32 40 ??(a) 2 88.57 57.14
??(v) 2 84.91 83.02 ?(a) 6 68.18 40.91
?(v) 3 86.54 85.58 ?(v) 8 52.03 65.04
?(v) 4 72.51 72.99 ??(a) 2 95.35 95.35
Table 5. Word Sense Distinction 
Pred Concrete  Sense Abstract Sense(s) 
?? [quiet|?] [calm|??], 
[peaceful|?]
?? [dirty|?] [despicable|??], 
[immoral|???]
?? [difficult|?] [poor|?]
?? [beat|?] [MakeBetter|??], 
[cultivate|??]
?? [touch|?] [excite|??]
?? [break|??] [obstruct|??]
increase the  modified  frequency  of  concrete 
sense collocations, and the abstract sense 
collocation can increase the modified 
frequency of abstract sense collocations, thus 
leading to the clear demarcation of abstract 
senses and concrete senses. 
The role of semantic relevance can also be 
clearly noticed in the predicates which have a 
decreased precision in MDL in Table 4. Via 
Paradigmatic Redundancy Principle, the 
information encoded in one collocation are 
diffused to other collocations. Consequently, 
errors can be diffused. This explains why the 
precisions of some predicates such as ??
(sink)?, ???(dumb)?, ???(dark)? in Subj-
Pred and ??(open)?, ??(harness)? in Pred-
Objs decrease after MDL. Further analysis 
shows that this is because MDL has diffused 
the errors produced by Association Filter. For 
instance, at Association Filter phase, the 
collocation ???(box) ?(sink)? is assigned 
with the only sense collocation ?[tool|?? ]
[very| ? ]? and all other potential sense 
collocations are filtered. When MDL is applied, 
other collocations such as ???(machine) ?
(heavy)?, ???(pick)?(heavy)?, ???(chaw) 
?(heavy)?, ???(basket) ?(heavy)?, ???
(box) ?(heavy)?, ???(furniture) ?(heavy)?, 
in which the arguments are tightly correlated 
with that of ???(box) ?(sink)?? all takes 
the sense ?[very|? ]?, thus leading to the 
decrease of precision. 
The diffusion of senses can also best seen in 
the comparison between those predicates 
whose WSD are semi-supervised and those 
whose WSD are not supervised. Some 
predicates have collocations successfully 
retrieved from HowNet examples in which the 
word sense is already identified. These 
collocations are diffused in MDL filtering and 
play important roles in improving precision, 
while some other predicates do not have such 
resource. In Table 4, those unsupervised 
predicates are ???(fall)?, ??(collapse)?, ??
?(exquisite)?, ???(dumb)?, ???(wide)?, 
??? (develop)? in Subj-Preds and ???
(spread)?, ??? (brush)?, ??? (get into)?, 
??(bring)?, and ???(mar)? in Pred-Objs. 
The other predicates are semi-supervised. As 
can be seen in Table 4, most of these 
unsupervised predicates generally have a 
precision of 40%-60%, while those semi-
supervised predicates enjoy are much higher 
precision between 50%-100%. The explanation 
1244
for the result is straight forward. When one 
sense collocation of one word collocation is 
correctly identified, by way of Paradigmatic 
Redundancy Principle, the sense collocation 
which is similar to the correctly identified will 
have a higher modified frequency and is thus 
singled out as the best choice. This feature of 
MDL has great significance in the process of 
annotating large scale collocation data. With 
only a small number of annotated collocations 
for each predicate, a fairly high precision can 
be achieved for all the rest of the data through 
MDL.
5 Conclusion
The present paper believes that the Word-
Class Model gives the fullest description for 
selectional preference and thus makes efforts 
to disambiguate predicates in selectional 
preferences. From the perspective of semantic 
system, two principles of semantic redundancy, 
namely the Syntagmatic Redundancy Principle 
and Paradigmatic Redundancy Principle, are 
proposed in the paper and are applied in WSD 
in SP via Association Computation and 
Minimum Description Length. The 
experiments show that the approach proposed 
is fairly encouraging in disambiguation of 
polysemous predicates, especially under semi-
supervised conditions when a small portion of 
data is annotated. With such a tool, we are able 
to build large scale selectional preference 
knowledge database based on Word-Class 
Models, which can be applied in various tasks, 
of which metaphor recognition is the particular 
one we bear in mind.  
Acknowledgement 
This work is supported by Chinese National 
Fund of Social Science under Grant 
07BYY050 and Chinese National Science 
Fund under Grant 60773173 and Chinese 
National Fund of Social Science under Grant 
10CYY021. We are also grateful to the 
autonomous reviewers for their valuable 
advice and suggestions. 
References 
Agirre, E., and D. Mart?nez. 2001. Learning class-
to-class selectional preferences. Paper read at 
Proceedings of the Conference on Natural 
Language Learning, at Toulouse, France. 
Barron, A. R., J. Rissanen, and B. Yu. 1998. The 
Minimum Description Length Principle in 
coding and modeling. IEEE Transactions on 
Information Theory 44 (6):2743-2760. 
Brockmann, C., and M. Lapata. 2003. Evaluating 
and combining approaches to selectional 
preference acquisition. Paper read at Proceedings 
of the European Association for Computational 
Linguistics, at Budapest, Hungary. 
Ciaramita, M., and M. Johnson. 2000. Explaining 
away ambiguity: Learning verb selectional 
preference with Bayesian networks. In 
Proceedingsofthe18thInternationalConferenceon
ComputationalLinguistics (COLING 2000), 187-
193. 
Dong, Z. 2006. HowNet and the Computation of 
Meaning. River Edge, NJ: World Scientific. 
Erk, K. 2007. A Simple, Similarity-based Model for 
Selectional Preferences. Paper read at 
Proceedings of the 45th Annual Meeting of the 
Association of Computational Linguistics, at 
Prague, Czech Republic. 
Firth, J. R. 1957. A Synopsis of Linguistic Theory, 
1930-1955. In Studies in Linguistic Analysis. 
Oxford: Blackwell, 1-32. 
Han, J., and M. Kamber. 2006. Data Ming: 
Concepts and Techniques. Singapore: Elsevier. 
Jia Yuxiang and Yu Shiwen. 2008. Automatic 
Acquisition of Selectional Preference and Its 
Application to Metaphor Processing. Paper read 
at the Fourth National Student Conference on 
Computationl Linguistics, at Taiyuan, Shangxi, 
China. 
Li, H., and N. Abe. 1998. Generalizing Case 
Frames Using a Thesaurus and the MDL 
Principle. Computational Linguistics 24 (2):217-
244. 
Light, M., and W. Greiff. 2002. Statistical models 
for the induction and use of selectional 
preferences. Cognitive Science 87:1-13. 
Liu, Qun and Li Sujian. 2002. Word Similarity 
Computation Based on HowNet. In Proceedings 
of the 3rd Chinese Lexical Semantics. Taibei, 
China. 
Lyons, J. 1977. Semantics. Cambridge: Cambridge 
University Press. 
1245
MacKay, D. J. C. 2003. Information Theory, 
Inference, and Learning Algorithms. Cambridge: 
Cambridge University Press. 
McCarthy, D., and J. Carroll. 2003. Disambiguating 
Nouns, Verbs, and Adjectives Using 
Automatically Acquired Selectional Preferences. 
Computational Linguistics 29 (4):639-654. 
Merlo, P., and S. Stevenson. 2001. Automatic Verb 
Classification Based on Statistical Distributions 
of Argument Structure. Computational 
Linguistics 27 (3):374-408. 
Michell, Tom M.. Machine Learning. Translated by 
Zen Huajun and Zhang Yinkui. Beijing: China 
Machine Press. 
Resnik, P. 1996. Selectional constraints: an 
information-theoretic model and its 
computational realization. Cognition 61:127-159. 
Qu, Weiguang. 2008. Lexical Sense 
Disambiguation in Modern Chinese. Beijing: 
Science Press. 
Wu, Yunfang, Duan Huiming and Yu Shiwen. 2005. 
Verb?s Selectional Preference on Object. Spoken 
and Written Language in Practice 2005(2):121-
128. 
Yarowsky, D. 1995. Unsupervised Word Sense 
Disambiguation Rivaling Supervised Methods. 
Paper read at Proceedings of the 33rd Annual 
Meeting of the Association for Computational 
Linguistics, at Cambridge, MA. 
Zheng, Xuling, Zhou Changle, Li Tangqiu and 
Chen Yidong. 2007. Automatic Acquisition of 
Chinese Semantic Collocation Rules Based on 
Association Rule Mining Technique. Journal of 
Xiamen University (Natural Science) 46(3):331-
336. 
1246
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 557?567, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
 
 
 
Exploiting Chunk-level Features to Improve Phrase Chunking 
 
Junsheng Zhou    Weiguang Qu     Fen Zhang 
Jiangsu Research Center of Information Security & Privacy Technology 
School of Computer Science and Technology 
Nanjing Normal University. Nanjing, China, 210046 
Email:{zhoujs,wgqu}@njnu.edu.cn  zf9646@126.com 
  
  
Abstract 
Most existing systems solved the phrase 
chunking task with the sequence labeling 
approaches, in which the chunk candidates 
cannot be treated as a whole during parsing 
process so that the chunk-level features 
cannot be exploited in a natural way. In this 
paper, we formulate phrase chunking as a 
joint segmentation and labeling task. We 
propose an efficient dynamic programming 
algorithm with pruning for decoding, 
which allows the direct use of the features 
describing the internal characteristics of 
chunk and the features capturing the 
correlations between adjacent chunks. A 
relaxed, online maximum margin training 
algorithm is used for learning. Within this 
framework, we explored  a variety of 
effective feature representations for 
Chinese phrase chunking. The 
experimental results show that the use of 
chunk-level features can lead to significant 
performance improvement, and that our 
approach achieves state-of-the-art 
performance. In particular, our approach is 
much better at recognizing long and 
complicated phrases. 
1 Introduction 
Phrase chunking is a Natural Language Processing 
task that consists in dividing a text into 
syntactically correlated parts of words. Theses 
phrases are non-overlapping, i.e., a word can only 
be a member of one chunk (Abney, 1991). 
Generally speaking, there are two phrase chunking 
tasks, including text chunking (shallow parsing), 
and noun phrase (NP) chunking. Phrase chunking 
provides a key feature that helps on more 
elaborated NLP tasks such as parsing, semantic 
role tagging and information extraction.  
    There is a wide range of research work on 
phrase chunking based on machine learning 
approaches. However, most of the previous work 
reduced phrase chunking to sequence labeling 
problems either by using the classification models, 
such as SVM (Kudo and Matsumoto, 2001), 
Winnow and voted-perceptrons (Zhang et al2002; 
Collins, 2002), or by using the sequence labeling 
models, such as Hidden Markov Models (HMMs) 
(Molina and Pla, 2002) and Conditional Random 
Fields (CRFs) (Sha and Pereira, 2003). When 
applying the sequence labeling approaches to 
phrase chunking, there exist two major problems. 
Firstly, these models cannot treat globally a 
sequence of continuous words as a chunk 
candidate, and thus cannot inspect the internal 
structure of the candidate, which is an important 
aspect of information in modeling phrase chunking. 
In particular, it makes impossible the use of local 
indicator function features of the type "the chunk 
consists of POS tag sequence p1...,pk". For example, 
the Chinese NP "?? /NN(agriculture) ??
/NN(production) ?/CC(and) ??/NN(rural) ??
/NN(economic) ?? /NN(development)" seems 
relatively difficult to be correctly recognized by a 
sequence labeling approach due to its length. But if 
we can treat the sequence of words as a whole and 
describe the formation pattern of POS tags of this 
chunk with a regular expression-like form 
"[NN]+[CC][NN]+", then it is more likely to be 
correctly recognized, since this pattern might better 
express the characteristics of its constituents. As 
another example, consider the recognition of 
special terms. In Chinese corpus, there exists a 
kind of NPs called special terms, such as "? ??
557
  
 
(Life) ?? (Forbidden Zone) ? ", which are 
bracketed with the particular punctuations like "
? , ? , ? , ? , ? , ?". When recognizing the 
special terms, it is difficult for the sequence 
labeling approaches to guarantee the matching of 
particular punctuations appearing at the starting 
and ending positions of a chunk. For instance, the 
chunk candidate "? ??(Life) ??(Forbidden 
Zone)? is considered to be an invalid chunk. But 
it is easy to check this kind of punctuation 
matching in a single chunk by introducing a chunk-
level feature. 
Secondly, the sequence labeling models cannot 
capture the correlations between adjacent chunks, 
which should be informative for the identification 
of chunk boundaries and types. In particular, we 
find that some headwords in the sentence are 
expected to have a stronger dependency relation 
with their preceding headwords in preceding 
chunks than with their immediately preceding 
words within the same chunk. For example, in the 
following sentence: 
" [??/PN(Bilateral)]_NP [??/NN(economic 
and trade) ??/NN(relations)]_NP [?/AD(just) 
??/AD(steadily) ??/VV(develop)]_VP " 
if we can find the three headwords "??", "??" 
and "??" located in the three adjacent chunks 
with some head-finding rules, then the headword 
dependency expressed by headword bigrams or 
trigrams should be helpful to recognize these 
chunks in this sentence.  
In summary, the inherent deficiency in applying 
the sequence labeling approaches to phrase 
chunking is that the chunk-level features one 
would expect to be very informative cannot be 
exploited in a natural way.  
In this paper, we formulate phrase chunking as a 
joint segmentation and labeling problem, which 
offers advantages over previous learning methods 
by providing a natural formulation to exploit the 
features describing the internal structure of a chunk 
and the features capturing the correlations between 
the adjacent chunks.  
Within this framework, we explored  a variety of 
effective feature representations for Chinese phrase 
chunking. The experimental results on Chinese 
chunking corpus as well as English chunking 
corpus show that the use of chunk-level features 
can lead to significant performance improvement, 
and that our approach performs better than other 
approaches based on the sequence labeling models. 
2 Related Work 
In recent years, many chunking systems based on 
machine learning approaches have been presented. 
Some approaches rely on k-order generative 
probabilistic models, such as HMMs (Molina and 
Pla, 2002). However, HMMs learn a generative 
model over input sequence and labeled sequence 
pairs. It has difficulties in modeling multiple non-
independent features of the observation sequence. 
To accommodate multiple overlapping features on 
observations, some other approaches view the 
phrase chunking as a sequence of classification 
problems, including support vector machines 
(SVMs) (Kudo and Matsumoto 2001) and a variety 
of other classifiers (Zhang et al2002). Since these 
classifiers cannot trade off decisions at different 
positions against each other, the best classifier 
based shallow parsers are forced to resort to 
heuristic combinations of multiple classifiers. 
Recently, CRFs were widely employed for phrase 
chunking, and presented comparable or better 
performance than other state-of-the-art models 
(Sha and Pereira 2003; McDonald et al005). 
Further, Sun et al2008) used the latent-dynamic 
conditional random fields (LDCRF) to explicitly 
learn the hidden substructure of shallow phrases, 
achieving state-of-the-art performance over the 
NP-chunking task on the CoNLL data. 
Some similar approaches based on classifiers or 
sequence labeling models were also used for 
Chinese chunking (Li et al2003; Tan et al2004; 
Tan et al2005). Chen et al2006) conducted an 
empirical study of Chinese chunking on a corpus, 
which was extracted from UPENN Chinese 
Treebank-4 (CTB4). They compared the 
performances of the state-of-the-art machine 
learning models for Chinese chunking, and 
proposed some Tag-Extension and novel voting 
methods to improve performance.  
In this paper, we model phrase chunking with a 
joint segmentation and labeling approach, which 
offer advantages over previous learning methods 
by explicitly incorporating the internal structural 
feature and the correlations between the adjacent 
chunks. To some extent, our model is similar to 
Semi-Markov Conditional Random Fields (called a 
Semi-CRF), in which the segmentation and 
558
  
 
labeling can also be done directly (Sarawagi and 
Cohen, 2004). However, Semi-CRF just models 
label dependency, and it cannot capture more 
correlations between adjacent chunks, as is done in 
our approach. The limitation of Semi-CRF leads to 
its relatively low performance.   
3 Problem Formulation 
3.1 Chunk Types 
Unlike English chunking, there is not a 
benchmarking corpus for Chinese chunking. We 
follow the studies in (Chen et al006) so that a 
more direct comparison with state-of-the-art 
systems for Chinese chunking would be possible. 
There are 12 types of chunks: ADJP, ADVP, CLP, 
DNP, DP, DVP, LCP, LST, NP, PP, QP and VP in 
the chunking corpus (Xue et al2000). The 
training and test corpus can be extracted from 
CTB4 with a public tool, as depicted in (Chen et al
2006). 
3.2 Sequence Labeling Approaches to Phrase 
Chunking 
The standard approach to phrase chunking is to use 
tagging techniques with a BIO tag set. Words in 
the input text are tagged with one of B for the 
beginning of a contiguous segment, I for the inside 
of a contiguous segment, or O for outside a 
segment. For instance, the sentence (word 
segmented and POS tagged) "?/NR(He) ??
/VV(reached) ? ? /NR(Beijing) ? ?
/NN(airport) ?/PU" will be tagged as follows: 
Example 1: 
S1: [NP ?][VP ??][NP ??/??][O ?] 
S2: ?/B-NP ??/B-VP ??/B-NP ??/I-
NP ?/O 
Here S1 denotes that the sentence is tagged with 
chunk types, and S2 denotes that the sentence is 
tagged with chunk tags based on the BIO-based 
model. With the data representation like the S2, the 
problem of phrase chunking can be reduced to a 
sequence labeling task. 
3.3 Phrase Chunking via a Joint 
Segmentation and Labeling Approach 
To tackle the problems with the sequence labeling 
approaches to phrase chunking, we formulate it as 
a joint problem, which maps a Chinese sentence x 
with segmented words and POS tags to an output y 
with tagged chunk types, like the S1 in Example 1. 
The joint model considers all possible chunk 
boundaries and corresponding chunk types in the 
sentence, and chooses the overall best output. This 
kind of parser reads the input sentences from left to 
right, predicts whether current segment of 
continuous words is some type of chunk. After one 
chunk is found, parser move on and search for next 
possible chunk. 
Given a sentence x, let y denote an output tagged 
with chunk types, and GEN a function that 
enumerates a set of segmentation and labeling 
candidates GEN(x) for x. A parser is to solve the 
following ?argmax? problem: 
( )
| |
[1.. ]( ) 1
? arg max ( )
arg max ( )
T
y GEN x
y
T
iy GEN x i
y w y
  w yf
?
? =
= ?F
= ??
 
 
where F  and f  are global and local feature maps 
and w is the parameter vector to learn. The inner 
product [1.. ]( )T iw yf?  can be seen as the confidence 
score of whether yi is a chunk. The parser takes into 
account confidence score of each chunk, by using 
the sum of local scores as its criteria. Markov 
assumption is necessary for computation, so f  is 
usually defined on a limited history. 
The main advantage of the joint segmentation 
and labeling approach to phrase chunking is to 
allow for integrating both the internal structural 
features and the correlations between the adjacent 
chunks for prediction. The two basic components 
of our model are decoding and learning algorithms, 
which are described in the following sections. 
4 Decoding 
The inference technique is one of the most 
important components for a joint segmentation and 
labeling model. In this section, we propose a 
dynamic programming algorithm with pruning to 
efficiently produce the optimal output. 
4.1 Algorithm Description 
Given an input sentence x, the decoding algorithm 
searches for the highest-scored output with 
recognized chunks. The search space of combined 
candidates in the joint segmentation and labeling 
task is very large, which is an exponential growth 
(1)
559
  
 
in the number of possible candidates with 
increasing sentence size. The rate of growth is 
O(2nTn) for the joint system, where n is the length 
of the sentence and T is the number of chunk types. 
It is natural to use some greedy heuristic search 
algorithms for inference in some similar joint 
problems (Zhang and Clark, 2008; Zhang and 
Clark, 2010). However, the greedy heuristic search 
algorithms only explore a fraction of the whole 
space (even with beam search) as opposed to 
dynamic programming. Additionally, a specific 
advantage of the dynamic programming algorithm 
is that constraints required in a valid prediction 
sequence can be handled in a principled way. We 
show that dynamic programming is in fact possible 
for this joint problem, by introducing some 
effective pruning schemes. 
   To make the inference tractable, we first make a 
first-order Markov assumption on the features used 
in our model. In other words, we assume that the 
chunk ci and the corresponding label ti are only 
associated with the preceding chunk ci-1 and the 
label ti-1. Suppose that the input sentence has n 
words and the constant M is the maximum chunk 
length in the training corpus. Let V(b,e,t) denote 
the highest-scored segmentation and labeling with 
the last chunk starting at word index b, ending at 
word index e and the last chunk type being t. One 
way to find the highest-scored segmentation and 
labeling for the input sentence is to first calculate 
the V(b,n-1,t) for all possible start position b?(n-
M)..n-1, and all possible chunk type t, respectively, 
and then pick the highest-scored one from these 
candidates. In order to compute V(b,n-1,t), the last 
chunk needs to be combined with all possible 
different segmentations of words (b-M)..b-1 and all 
possible different chunk types so that the highest-
scored can be selected. According to the principle 
of optimality, the highest-scored among the 
segmentations of words (b-M)..b-1 and all possible 
chunk types with the last chunk being word b? ..b-
1 and the last chunk type being t ?  will also give the highest score when combined with the word 
b..n-1 and tag t. In this way, the search task is 
reduced recursively into smaller subproblems, 
where in the base case the subproblems V(0,e,t) for 
e?0..M-1, and each possible chunk type t, are 
solved in straightforward manner. And the final 
highest-scored segmentation and labeling can be 
found by solving all subproblems in a bottom-up 
fashion. 
   The pseudo code for this algorithm is shown in 
Figure 1. It works by filling an n by n by T table 
chart, where n is the number of words in the input 
sentence sent, and T is the number of chunk types. 
chart[b,e,t] records the value of subproblem 
V(b,e,t). chart[0, e, t] can be computed directly for 
e = 0..M-1 and for chunk type t=1..T. The final 
output is the best among chart[b,n-1,t], with b= 
n-M..n-1, and t=1..T.  
Inputs: sentence sent (word segmented and POS 
tagged) 
Variables:  
word index b for the start of chunk; 
word index e for the end of chunk; 
word index p for the start of the previous chunk. 
chunk type index t for the current chunk; 
chunk type index t ?  for the previous chunk; 
Initialization: 
for e = 0.. M-1: 
   for t =1..T: 
     chart[0,e,t] ?single chunk sent[0,e] and type t 
Algorithm: 
for e = 0..n-1: 
  for b = (e-M)..e: 
    for t =1..T: 
       chart[b,e,t]?the highest scored segmentation            
               and labeling among those derived by 
               combining chart[p,b-1, t ? ] with sent[b,e] 
               and chunk type t, for p = (b-M)..b-1, 
                t ? =1..T. 
Outputs: the highest scored segmentation and 
labeling among chart[b,n-1,t], for b=n-M..n-1, t 
=1..T. 
Figure 1: A dynamic-programming algorithm for 
phrase chunking. 
4.2 Pruning 
The time complexity of the above algorithm is 
O(M2T2n), where M is the maximum chunk size. It 
is linear in the length of sentence. However, the 
constant in the O is relatively large. In practice, the 
search space contains a large number of invalid 
partial candidates, which make the algorithm slow. 
In this section we describe three partial output 
pruning schemes which are helpful in speeding up 
the algorithm. 
560
  
 
Firstly, we collect chunk type transition 
information between chunk types by observing 
every pair of adjacent chunks in the training corpus, 
and record a chunk type transition matrix. For 
example, from the Chinese Treebank that we used 
for our experiments, a transition from chunk type 
ADJP to ADVP does not occur in the training 
corpus, the corresponding matrix element is set to 
false, true otherwise. During decoding, the chunk 
type transition information is used to prune 
unlikely combinations between current chunk and 
the preceding chunk by their chunk types. 
Secondly, a POS tag dictionary is used to record 
POS tags associated with each chunk type. 
Specifically, for each chunk type, we record all 
POS tags appearing in this type of chunk in the 
training corpus. During decoding, a segment of 
continuous words that contains only allowed POS 
tags according to the POS tag dictionary will be 
considered to be a valid chunk candidate. 
Finally, the system records the maximum 
number of words for each type of chunk in the 
training corpus. For example, in the Chinese 
Treebank, most types of chunks have one to three 
words. The few chunk types that are seen with 
length bigger than ten are NP, QP and ADJP. 
During decoding, the chunk candidate whose 
length is greater than the maximum chunk length 
associated with its chunk type will be discarded. 
For the above pruning schemes, development 
tests show that it improves the speed significantly, 
while having a very small negative influence on 
the accuracy. 
5 Learning 
5.1 Discriminative Online Training 
By defining features, a candidate output y is 
mapped into a global feature vector, in which each 
dimension represents the count of a particular 
feature in the sentence. The learning task is to set 
the parameter values w using the training examples 
as evidence. 
   Online learning is an attractive method for the 
joint model since it quickly converges within a few 
iterations (McDonald, 2006). We focus on an 
online learning algorithm called MIRA, which is a 
relaxed, online maximum margin training 
algorithm with the desired accuracy and scalability 
properties (Crammer, 2004). Furthermore, MIRA 
is very flexible with respect to the loss function. 
Any loss function on the output is compatible with 
MIRA since it does not require the loss to factor 
according to the output, which enables our model 
to be optimized with respect to evaluation metrics 
directly. Figure 2 outlines the generic online 
learning algorithm (McDonald, 2006) used in our 
framework. 
MIRA updates the parameter vector w with two 
constraints: (1) the positive example must have a 
higher score by a given margin, and (2) the change 
to w should be minimal. This second constraint is 
to reduce fluctuations in w. In particular, we use a 
generalized version of MIRA (Crammer et al
2005; McDonald, 2006) that can incorporate k-best 
decoding in the update procedure.  
Input: Training set 1{( , )}Tt t tS x y ==  
1: w(0) = 0; v = 0; i = 0 
2: for iter = 1 to N do 
3:    for t = 1 to T do 
4:       w(i+1) = update w(i) according to (xt, yt) 
5:       v = v + w(i+1) 
6:       i = i + 1 
7:    end for 
8: end for 
9: w = v/(N ? T) 
Output: weight vector w 
Figure 2: Generic Online Learning Algorithm 
In each iteration, MIRA updates the weight 
vector w by keeping the norm of the change in the 
weight vector as small as possible. Within this 
framework, we can formulate the optimization 
problem as follows (McDonald, 2006): 
( 1) ( )
( )
argmin
. . ( ; ) :
( ) ( ) ( , )
i i
w
i
k t
T T
t t
w w w
s t   y best x w
     w y w y L y y
+ = -
?" ?
? ??F - ?F ?
 
where ( )( ; )ik tbest x w  represents a set of top k-best 
outputs for xt given the weight vector w(i). In our 
implementation, the top k-best outputs are obtained 
with a straightforward k-best extension to the 
decoding algorithm in section 4.1. The above 
quadratic programming (QP) problem can be 
solved using Hildreth?s algorithm (Yair Censor, 
1997). Replacing Eq. (2) into line 4 of the 
algorithm in Figure 2, we obtain k-best MIRA. 
As shown in (McDonald, 2006), parameter 
averaging can effectively avoid overfitting. The 
(2)
561
  
 
final weight vector w is the average of the weight 
vectors after each iteration. 
5.2 Loss Function 
For the joint segmentation and labeling task, there 
are two alternative loss functions: 0-1 loss and F1 
loss. 0-1 loss gives credit only when the entire 
output sequence is correct: there is no notion of 
partially correct solutions. The most common loss 
function for joint segmentation and labeling 
problems is F1 measure over chunks. This is the 
geometric mean of precision and recall over the 
(properly-labeled) chunk identification task, 
defined as follows. 
2 | |?( , ) 1 | | | |
F y yL y y y y
??- ?+?
 
where the cardinality of y is simply the number of 
chunks identified. The cardinality of the 
intersection is the number of chunks in common. 
As can be seen in the definition, one is penalized 
both for identifying too many chunks (penalty in 
the denominator) and for identifying too few 
(penalty in the numerator).  
In our experiments, we will compare the 
performance of the systems with different loss 
functions. 
5.3 Features 
Table 1 shows the feature templates for the joint 
segmentation and labeling model. In the row for 
feature templates, c, t, w and p are used to 
represent a chunk, a chunk type, a word and a POS 
tag, respectively. And c0 and c?1 represent the 
current chunk and the previous chunk respectively. 
Similarly, w?1, w0 and w1 represent the previous 
word, the current word and the next word, 
respectively.  
Although it is slightly less natural to do so, part 
of the features used in the sequence labeling 
models can also be represented in our approach. 
Therefore the features employed in our model can 
be divided into three types: the features similar to 
those used in the sequence labeling models (called 
SL-type features), the features describing internal 
structure of a chunk (called Internal-type features), 
and the features capturing the correlations between 
the adjacent chunks (called Correlation-type 
features). 
Firstly, some features associated with a single 
label (here refers to label "B" and "I") used in the 
sequence labeling models are also represented in 
our model. In Table 1, templates 1-4 are SL-type 
features, where label(w) denotes the label 
indicating the position of the word w in the current 
chunk; len(c) denotes the length of chunk c. For 
example, given an NP chunk "??(Beijing) ??
(Airport)", which includes two words, the value of 
label("??") is "B" and the value of label("??") 
is "I". Bigram(w) denotes the word bigrams formed 
by combining the word to the left of w and the one 
to the right of w. And the same meaning is for 
biPOS(w). Template specitermMatch(c) is used to 
check the punctuation matching within chunk c for 
the special terms, as illustrated in section 1. 
Secondly, in our model, we have a chance to 
treat the chunk candidate as a whole during 
decoding, which means that we can employ more 
expressive features in our model than in the 
sequence labeling models. In Table 1, templates 5-
13 concern the Internal-type features, where 
start_word(c) and end_word(c) represent the first 
word and the last word of chunk c, respectively. 
Similarly, start_POS(c) and end_POS(c) represent 
the POS tags associated with the first word and the 
last word of chunk c, respectively. These features 
aim at expressing the formation patterns of the 
current chunk with respect to words and POS tags. 
Template internalWords(c) denotes the 
concatenation of words in chunk c, while 
internalPOSs(c) denotes the sequence of POS tags 
in chunk c using regular expression-like form, as 
illustrated in section 1.  
Finally, in Table 1, templates 14-28 concern the 
Correlation-type features, where head(c) denotes 
the headword extracted from chunk c, and 
headPOS(c) denotes the POS tag associated with 
the headword in chunk c. These features take into 
account various aspects of correlations between 
adjacent chunks. For example, we extracted the 
headwords located in adjacent chunks to form 
headword bigrams to express semantic dependency 
between adjacent chunks. To find the headword 
within every chunk, we referred to the head-
finding rules from (Bikel, 2004), and made a 
simple modification to them.  For instance, the 
head-finding rule for NP in (Bikel, 2004) is as 
follows:  
(NP (r NP NN NT NR QP) (r))  
Since the phrases are non-overlapping in our task, 
we simply remove the overlapping phrase tags NP 
    (3)
562
  
 
and QP from the rule, and then the rule is modified 
as follows:  
    (NP (r NN NT NR) (r))   
Additionally, the different bigrams formed by 
combining the first word (or POS) and last word 
(or POS) located in two adjacent chunks can also 
capture some correlations between adjacent chunks, 
and templates 17-22 are designed to express this 
kind of bigram information. 
ID Feature template 
1 wlabel(w) t0  
for all w in c0 
2 bigram (w) label(w)t0  
for all w in c0 
3 biPOS(w) label(w)t0  
for all w in c0 
4 w-1w1label(w0) t0 ,  where len(c0)=1 
5 start_word(c0)t0 
6 start_POS(c0)t0 
7 end_word(c0)t0 
8 end_POS(c0)t0 
9 wend_word (c0) t0 
 where 0 0_ ( )w c  and w end word c? ?  
10 pend_POS (c0) t0 
 where 0 0_ ( )p c  and p end POS c? ?  
11 internalPOSs(c0) t0 
12 internalWords(c0) t0 
13 specitermMatch(c0) 
14 t-1t0 
15 head(c-1)t-1head(c0)t0 
16 headPOS(c-1)t-1headPOS(c0)t0 
17 end_word(c-1)t-1start_word(c0)t0 
18 end_POS(c-1)t-1start_POS(c0)t0 
19 end_word(c-1)t-1end_word(c0)t0 
20 end_POS(c-1)t-1end_POS(c0)t0 
21 start_word(c-1)t-1start_word(c0)t0 
22 start_POS(c-1)t-1start_POS(c0)t0 
23 end_word(c-1)t0 
24 end_POS(c-1)t0 
25 t-1t0start_word(c0) 
26 t-1t0start_POS(c0) 
27 internalWords(c-1) t-1 internalWords(c0) t0
28 internalPOSs(c-1) t-1 internalPOSs(c0) t0 
Table 1: Feature templates. 
6 Experiments 
6.1 Data Sets and Evaluation 
Following previous studies on Chinese chunking in 
(Chen et al2006), our experiments were 
performed on the CTB4 dataset. The dataset 
consists of 838 files. In the experiments, we used 
the first 728 files (FID from chtb 001.fid to chtb 
899.fid) as training data, and the other 110 files 
(FID from chtb 900.fid to chtb 1078.fid) as testing 
data. The training set consists of 9878 sentences, 
and the test set consists of 5920 sentences. The 
standard evaluation metrics for this task are 
precision p (the fraction of output chunks matching 
the reference chunks), recall r (the fraction of 
reference chunks returned), and the F-measure 
given by F = 2pr/(p + r). 
Our model has two tunable parameters: the 
number of training iterations N; the number of top 
k-best outputs. Since we were interested in finding 
an effective feature representation at chunk-level 
for phrase chunking, we fixed N = 10 and k = 5 for 
all experiments. In the following experiments, our 
model has roughly comparable training time to the 
sequence labeling approach based on CRFs. 
6.2 Chinese NP chunking 
NP is the most important phrase in Chinese 
chunking and about 47% phrases in the CTB4 
Corpus are NPs. In this section, we present the 
results of our approach to NP recognition.  
Table 2 shows the results of the two systems 
using the same feature representations as defined 
in Table 1, but using different loss functions for 
learning. As shown, learning with F1 loss can 
improve the F-score by 0.34% over learning with 
0-1 loss. It is reasonable that the model optimized 
with respect to evaluation metrics directly can 
achieve higher performance. 
Loss Function Precision Recall F1 
0-1 loss 91.39 90.93 91.16 
F1 loss 92.03 90.98 91.50 
Table 2: Experimental results on Chinese NP 
chunking. 
6.3 Chinese Text Chunking 
There are 12 different types of phrases in the 
chunking corpus. Table 3 shows the results from 
563
  
 
two different systems with different loss functions 
for learning. Observing the results in Table 3, we 
can see that learning with F1 loss can improve the 
F-score by 0.36% over learning with 0-1 loss, 
similar to the case in NP recognition. More 
specifically, learning with F1 loss provides much 
better results for ADJP, ADVP, DVP, NP and VP, 
respectively. And it yields equivalent or 
comparable results to 0-1 loss in other categories.
 
 F1 loss 0-1 loss 
precision recall F1 precision recall F1 
ADJP 87.86 87.09 87.47 86.74 86.55 86.64 
ADVP 90.66 78.73 84.27 91.91 76.68 83.61 
CLP 0.00 0.00 0.00 1.32 5.88 2.15 
DNP 99.42 99.93 99.68 99.42 99.95 99.69 
DP 99.46 99.76 99.61 99.46 99.76 99.61 
DVP 99.61 99.61 99.61 99.22 99.61 99.42 
LCP 99.74 99.96 99.85 99.74 99.93 99.84 
LST 87.50 52.50 65.63 87.50 52.50 65.63 
NP 91.87 91.01 91.44 91.34 90.52 90.93 
PP 99.57 99.77 99.67 99.57 99.77 99.67 
QP 96.45 96.64 96.55 96.45 97.07 96.76 
VP 90.14 90.39 90.26 89.92 89.79 89.85 
ALL 92.54 91.68 92.11 92.30 91.20 91.75 
Table 3: Experimental results on Chinese text chunking. 
6.4 Comparison with Other Models 
Chen et al2006) compared the performance of 
the state-of-the-art machine learning models for 
Chinese chunking, and found that the SVMs 
approach yields higher accuracy than respective 
CRFs, Transformation-based Learning (TBL) 
(Megyesi, 2002), and Memory-based Learning 
(MBL) (Sang, 2002) approaches.  
In this section, we give a comparison and 
analysis between our model and other state-of-the-
art machine learning models for Chinese NP 
chunking and text chunking tasks. Performance of 
our model and some of the best results from the 
state-of-the-art systems are summarized in Table 4. 
Row "Voting" refers to the phrase-based voting 
methods based on four basic systems, which are 
respectively SVMs, CRFs, TBL and MBL, as 
depicted in (Chen et al2006). Observing the 
results in Table 4, we can see that for both NP 
chunking and text chunking tasks, our model 
achieves significant performance improvement 
over those state-of-the-art systems in terms of the 
F1-score, even for the voting methods. For text 
chunking task, our approach improves performance 
by 0.65% over SVMs, and 0.43% over the voting 
method, respectively. 
 Method F1 
NP 
chunking 
CRFs 89.72 
SVMs 90.62 
Voting 91.13 
Ours 91.50 
Text 
chunking 
CRFs 90.74 
SVMs 91.46 
Voting 91.68 
Ours 92.11 
Table 4: Comparisons of chunking performance for 
Chinese NP chunking and text chunking. 
In particular, for NP chunking task, the F1-score 
of our approach is improved by 0.88% in 
comparison with SVMs, the best single system. 
Further, we investigated the likely cause for 
performance improvement by comparing the 
recognized results from our system and SVMs 
564
  
 
respectively. We first sorted NPs by their length, 
and then calculated the F1-scores associated with 
different lengths for the two systems respectively. 
Figure 3 shows the comparison of F1-scores of the 
two systems by the chunk length. In the Chinese 
chunking corpus, the max NP length is 27, and 
the mean NP length is 1.5. Among all NPs, the 
NPs with the length 1 account for 81.22%. For the 
NPs with the length 1, our system gives slight 
improvement by 0.28% over SVMs. From the 
figure, we can see that the performance gap grows 
rapidly with the increase of the chunk length. In 
particular, the gap between the two systems is 
27.73% when the length hits 4. But the gap begins 
to become smaller with further growth of the 
chunk length. The reasons may include the 
following two aspects. First, the number of NPs 
with the greater length is relatively small in the 
corpus. Second, the NPs with greater length in 
Chinese corpus often exhibit some typical rules.  
For example, an NP with length 8 is given as 
follows. 
 "??/NN(cotton) ?/PU ??/NN(oil) ?/PU ?
?/NN(drug) ?/PU ??/NN(vegetable) ?/ETC 
(et al  
The NP consists of a sequence of nouns simply 
separated by a punctuation "?". So it is also easy 
to be recognized by the sequence labeling 
approach based on SVMs. In summary, the above 
investigation indicates that our system is better at 
recognizing the long and complicated phrases 
compared with the sequence labeling approaches. 
35
45
55
65
75
85
95
1 2 3 4 5 6 7 8 >8The length of NP
F-
sc
or
e
our system
SVM
 
Figure 3: Comparison of F1-scores of NP 
recognition on Chinese corpus by the chunk length. 
6.5 Impact of Different Types of Features 
Our phrase chunking model is highly dependent 
upon chunk-level information. To establish the 
impact of each type of feature (SL-type, Internal-
type, Correlation-type), we look at the 
improvement in F1-score brought about by adding 
each type of features. Table 5 shows the accuracy 
with various features added to the model.  
First consider the effect of the SL-type features. 
If we use only the SL-type features, the system 
achieves slightly lower performance than CRFs or 
SVMs, as shown in Table 4. Since the SL-type 
features consist of the features associated with 
single label, not including the features associated 
with label bigrams. Then, adding the Internal-type 
features to the system results in significant 
performance improvement on NP chunking and on 
text chunking, achieving 2.53% and 1.37%, 
respectively. Further, if Correlation-type features 
are used, the F1-scores on NP chunking and on text 
chunking are improved by 1.01% and 0.66%, 
respectively. The results show a significant impact 
due to the use of Internal-type features and 
Correlation-type features for both NP chunking 
and text chunking. 
Task Type Feature Type F1 
NP chunking 
SL-type 87.96 
+Internal-type 90.49 
+Correlation-type 91.50 
Text chunking
SL-type 90.08 
+Internal-type 91.45 
+Correlation-type 92.11 
Table 5: Test F1-scores for different types of 
features on Chinese corpus. 
6.6 Performance on Other Languages 
We mainly focused on Chinese chunking in this 
paper. However, our approach is generally 
applicable to other languages including English, 
except that the definition of feature templates may 
be language-specific. To validate this point, we 
evaluated our system on the CoNLL 2000 data set, 
a public benchmarking corpus for English 
chunking (Sang and Buchholz 2000). The training 
set consists of 8936 sentences, and the test set 
consists of 2012 sentences. 
We conducted both the NP-chunking and text 
chunking experiments on this data set with our 
approach, using the same feature templates as in 
Chinese chunking task excluding template 13. To 
find the headword within every chunk, we referred 
to the head-finding rules from (Collins, 1999), and 
made a simple modification to them in a similar 
way as in Chinese. As we can see from Table 6, 
565
  
 
our model is able to achieve better performance 
compared with state-of-the-art systems. Table 6 
also shows state-of-the-art performance for both 
NP-chunking and text chunking tasks. LDCRF's 
results presented in (Sun et al2008) are the state-
of-the-art for the NP chunking task, and SVM's 
results presented in (Wu et al2006) are the state-
of-the-art for the text chunking task. 
Moreover, the performance should be further 
improved if some additional features tailored for 
English chunking are employed in our model. For 
example, we can introduce an orthographic feature 
type called Token feature and the affix feature into 
the model, as used in  (Wu et al2006). 
 Method Precision Recall F1 
NP 
chunking 
Ours 94.79 94.65 94.72
LDCRF 94.65 94.03 94.34
Text 
chunking 
Ours 94.31 94.12 94.22
SVMs 94.12 94.13 94.12
Table 6: Performance on English corpus. 
7 Conclusions and Future Work 
In this paper we have presented a novel approach 
to phrase chunking by formulating it as a joint 
segmentation and labeling problem. One important 
advantage of our approach is that it provides a 
natural formulation to exploit chunk-level features. 
The experimental results on both Chinese chunking 
and English chunking tasks show that the use of 
chunk-level features can lead to significant 
performance improvement and that our approach 
outperforms the best in the literature. 
Future work mainly includes the following two 
aspects. Firstly, we will explore applying external 
information, such as semantic knowledge, to 
represent the chunk-level features, and then 
incorporate them into our model to improve the 
performance. Secondly, we plan to apply our 
approach to other joint segmentation and labeling 
tasks, such as clause identification and named 
entity recognition. 
Acknowledgments 
This research is supported by Projects 61073119, 
60773173 under the National Natural Science 
Foundation of China, and project BK2010547 
under the Jiangsu Natural Science Foundation of 
China. We would also like to thank the excellent 
and insightful comments from the three 
anonymous reviewers. 
References  
Steven P. Abney. 1991. Parsing by chunks. In Robert C. 
Berwick, Steven P. Abney, and Carol Tenny, editors, 
Principle-Based Parsing , pages 257-278. Kluwer 
Academic Publishers. 
Daniel M, Bikel. 2004. On the Parameter Space of 
Generative Lexicalized Statistical Parsing Models. 
Ph.D. thesis, University of Pennsylvania. 
Wenliang Chen, Yujie Zhang, and Hitoshi Isahara. 2006. 
An empirical study of Chinese chunking. In 
Proceedings of the COLING/ACL 2006 Main 
Conference Poster Sessions, pages 97-104. 
Michael Collins. 2002. Discriminative training methods 
for hidden Markov models: Theory and experiments 
with perceptron algorithms. In Proc. EMNLP-02. 
Michael Collins. 1999. Head-Driven Statistical Models 
for Natural Language Parsing. Ph.D. thesis, 
University of Pennsylvania. 
Koby Crammer. 2004. Online Learning of Complex 
Categorial Problems. Hebrew University of 
Jerusalem, PhD Thesis.  
Taku Kudo and Yuji Matsumoto. 2001. Chunking with 
support vector machines. In Proceedings of 
NAACL01.  
Koby Crammer, Ryan McDonald, and Fernando Pereira. 
2005. Scalable large-margin online learning for 
structured classification. In NIPS Workshop on 
Learning With Structured Outputs. 
Heng Li, Jonathan J. Webster, Chunyu Kit, and 
Tianshun Yao. 2003. Transductive hmm based 
chinese text chunking. In Proceedings of IEEE 
NLPKE2003, pages 257-262, Beijing, China. 
Ryan McDonald, Femando Pereira, Kiril Ribarow, and 
Jan Hajic. 2005. Non-projective dependency parsing 
using spanning tree algorithms. In Proceedings of 
HLT/EMNLP, pages 523-530.  
Ryan. McDonald, K. Crammer, and F. Pereira, 2005. 
Flexible Text Segmentation with Structured 
Multilabel Classification. In Proceedings 
HLT/EMNLP, pages 987- 994. 
Ryan McDonald. 2006. Discriminative Training and 
Spanning Tree Algorithms for Dependency Parsing. 
University of Pennsylvania, PhD Thesis. 
Beata Megyesi. 2002. Shallow parsing with pos taggers 
and linguistic features. Journal of Machine Learning 
Research, 2:639-668. 
566
  
 
Antonio Molina and Ferran Pla. 2002. Shallow parsing 
using specialized hmms. Journal of Machine 
Learning Research., 2:595- 613. 
 E.F.T.K Sang and S. Buchholz. 2000. Introduction to 
the CoNLL-2000 shared task: Chunking. In 
Proceedings CoNLL-00, pages 127-132. 
Sunita Sarawagi and W. Cohen. 2004. Semi-markov 
conditional random fields for information extraction. 
In Proceedings of NIPS 17, pages 1185?1192.  
Fei Sha and Fernando Pereira. 2003. Shallow parsing 
with conditional random fields. In Proceedings of 
HLT-NAACL03. 
Xu Sun, Louis-Philippe Morency, Daisuke Okanohara, 
and Jun?ichi Tsujii. 2008. Modeling Latent-Dynamic 
in Shallow Parsing: A Latent Conditional Model with 
Improved Inference. In Proceedings of the 22nd 
International Conference on Computational 
Linguistics, pages 841?848. 
Yongmei Tan, Tianshun Yao, Qing Chen, and Jingbo 
Zhu. 2004. Chinese chunk identification using svms 
plus sigmoid. In IJCNLP, pages 527-536. 
Yongmei Tan, Tianshun Yao, Qing Chen, and Jingbo 
Zhu. 2005. Applying conditional random fields to 
chinese shallow parsing. In Proceedings of CICLing-
2005, pages 167-176. 
Erik F. Tjong Kim Sang. 2002. Memory-based shallow 
parsing. JMLR, 2(3):559-594. 
Yu-Chieh Wu, Chia-Hui Chang, and Yue-Shi Lee. 2006. 
A general and multi-lingual phrase chunking model 
based on masking method. In Proceedings of 7th 
International Conference on Intelligent Text 
Processing and Computational Linguistics, pages 
144-155. 
Nianwen Xue, Fei Xia, Shizhe Huang, and Anthony 
Kroch. 2000. The bracketing guidelines for the penn 
chinese treebank. Technical report, University of 
Pennsylvania. 
Stavros A. Zenios Yair Censor. 1997. Parallel 
Optimization: Theory, Algorithms, and Applications. 
Oxford University Press. 
Tong Zhang, F. Damerau, and D. Johnson. 2002. Text 
chunking based on a generalization of winnow. 
Journal of Machine Learning Research, 2:615-637.  
Yue Zhang and Stephen Clark. 2008. Joint word 
segmentation and POS tagging using a single 
perceptron. In Proceedings of ACL/HLT, pages 888-
896. 
Yue Zhang and Stephen Clark. 2010. A fast decoder for 
joint word segmentation and POS-tagging using a 
single discriminative model. In Proceedings of 
EMNLP, pages 843-852. 
 
567
Proceedings of the ACL 2010 Conference Short Papers, pages 296?300,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
A Semi-Supervised Key Phrase Extraction Approach: Learning from 
Title Phrases through a Document Semantic Network 
 
Decong Li1, Sujian Li1, Wenjie Li2, Wei Wang1, Weiguang Qu3 
1Key Laboratory of Computational Linguistics, Peking University  
2Department of Computing, The Hong Kong Polytechnic University 
3 School of Computer Science and Technology, Nanjing Normal University 
{lidecong,lisujian, wwei }@pku.edu.cn   cswjli@comp.polyu.edu.hk  wgqu@njnu.edu.cn 
  
  
Abstract 
It is a fundamental and important task to ex-
tract key phrases from documents. Generally, 
phrases in a document are not independent in 
delivering the content of the document. In or-
der to capture and make better use of their re-
lationships in key phrase extraction, we sug-
gest exploring the Wikipedia knowledge to 
model a document as a semantic network, 
where both n-ary and binary relationships 
among phrases are formulated. Based on a 
commonly accepted assumption that the title 
of a document is always elaborated to reflect 
the content of a document and consequently 
key phrases tend to have close semantics to the 
title, we propose a novel semi-supervised key 
phrase extraction approach in this paper by 
computing the phrase importance in the se-
mantic network, through which the influence 
of title phrases is propagated to the other 
phrases iteratively. Experimental results dem-
onstrate the remarkable performance of this 
approach. 
1 Introduction 
Key phrases are defined as the phrases that ex-
press the main content of a document. Guided by 
the given key phrases, people can easily under-
stand what a document describes, saving a great 
amount of time reading the whole text. Conse-
quently, automatic key phrase extraction is in 
high demand. Meanwhile, it is also fundamental 
to many other natural language processing appli-
cations, such as information retrieval, text clus-
tering and so on.  
Key phrase extraction can be normally cast as 
a ranking problem solved by either supervised or 
unsupervised methods. Supervised learning re-
quires a large amount of expensive training data, 
whereas unsupervised learning totally ignores 
human knowledge. To overcome the deficiencies 
of these two kinds of methods, we propose a 
novel semi-supervised key phrase extraction ap-
proach in this paper, which explores title phrases 
as the source of knowledge.  
It is well agreed that the title has a similar role 
to the key phrases. They are both elaborated to 
reflect the content of a document. Therefore, 
phrases in the titles are often appropriate to be 
key phrases. That is why position has been a 
quite effective feature in the feature-based key 
phrase extraction methods (Witten, 1999), i.e., if 
a phrase is located in the title, it is ranked higher.  
However, one can only include a couple of 
most important phrases in the title prudently due 
to the limitation of the title length, even though 
many other key phrases are all pivotal to the un-
derstanding of the document. For example, when 
we read the title ?China Tightens Grip on the 
Web?, we can only have a glimpse of what the 
document says. On the other hand, the key 
phrases, such as ?China?, ?Censorship?, ?Web?, 
?Domain name?, ?Internet?, and ?CNNIC?, etc. 
can tell more details about the main topics of the 
document. In this regard, title phrases are often 
good key phrases but they are far from enough. 
If we review the above example again, we will 
find that the key phrase ?Internet? can be in-
ferred from the title phrase ?Web?. As a matter 
of fact, key phrases often have close semantics to 
title phrases. Then a question comes to our minds: 
can we make use of these title phrases to infer 
the other key phrases?  
To provide a foundation of inference, a seman-
tic network that captures the relationships among 
phrases is required. In the previous works (Tur-
dakov and Velikhov, 2008), semantic networks 
are constructed based on the binary relations, and 
the semantic relatedness between a pair of phras-
es is formulated by the weighted edges that con-
nects them. The deficiency of these approaches is 
the incapability to capture the n-ary relations 
among multiple phrases. For example, a group of 
296
phrases may collectively describe an entity or an 
event.  
In this study, we propose to model a semantic 
network as a hyper-graph, where vertices 
represent phrases and weighted hyper-edges 
measure the semantic relatedness of both binary 
relations and n-ary relations among phrases. We 
explore a universal knowledge base ? Wikipedia 
? to compute the semantic relatedness. Yet our 
major contribution is to develop a novel semi-
supervised key phrase extraction approach by 
computing the phrase importance in the semantic 
network, through which the influence of title 
phrases is propagated to the other phrases itera-
tively.  
The goal of the semi-supervised learning is to 
design a function that is sufficiently smooth with 
respect to the intrinsic structure revealed by title 
phrases and other phrases. Based on the assump-
tion that semantically related phrases are likely 
to have similar scores, the function to be esti-
mated is required to assign title phrases a higher 
score and meanwhile locally smooth on the con-
structed hyper-graph. Zhou et al?s work (Zhou 
2005) lays down a foundation for our semi-
supervised phrase ranking algorithm introduced 
in Section 3. Experimental results presented in 
Section 4 demonstrate the effectiveness of this 
approach. 
2 Wikipedia-based Semantic Network 
Construction  
Wikipedia1 is a free online encyclopedia, which 
has unarguably become the world?s largest col-
lection of encyclopedic knowledge. Articles are 
the basic entries in the Wikipedia, with each ar-
ticle explaining one Wikipedia term. Articles 
contain links pointing from one article to another. 
Currently, there are over 3 million articles and 90 
million links in English Wikipedia. In addition to 
providing a large vocabulary, Wikipedia articles 
also contain a rich body of lexical semantic in-
formation expressed via the extensive number of 
links. During recent years, Wikipedia has been 
used as a powerful tool to compute semantic re-
latedness between terms in a good few of works 
(Turdakov 2008).   
We consider a document composed of the 
phrases that describe various aspects of entities 
or events with different semantic relationships. 
We then model a document as a semantic net-
work formulated by a weighted hyper-graph 
                                               
1 www.wikipedia.org 
 
G=(V, E, W), where each vertex vi?V (1?i?n) 
represents a phrase, each hyper-edge ej?E 
(1?j?m) is a subset of V, representing binary re-
lations or n-ary relations among phrases, and the 
weight w(ej) measures the semantic relatedness 
of ej.  
By applying the WSD technique proposed by 
(Turdakov and Velikhov, 2008), each phrase is 
assigned with a single Wikipedia article that de-
scribes its meaning. Intuitively, if the fraction of 
the links that the two articles have in common to 
the total number of the links in both articles is 
high, the two phrases corresponding to the two 
articles are more semantically related. Also, an 
article contains different types of links, which are 
relevant to the computation of semantic related-
ness to different extent. Hence we adopt the 
weighted Dice metric proposed by (Turdakov 
2008) to compute the semantic relatedness of 
each binary relation, resulting in the edge weight  
w(eij), where eij is an edge connecting the phrases 
vi and vj. 
To define the n-ary relations in the semantic 
network, a proper graph clustering technique is 
needed. We adopt the weighted Girvan-Newman 
algorithm (Newman 2004) to cluster phrases (in-
cluding title phrases) by computing their bet-
weenness centrality. The advantage of this algo-
rithm is that it need not specify a pre-defined 
number of clusters. Then the phrases, within 
each cluster, are connected by a n-ary relation. n-
ary relations among the phrases in the same clus-
ter are then measured based on binary relations. 
The weight of a hyper-edge e is defined as: 
( ) ( )| | ij ije e
w e w ee
?
?
? ?
  (1) 
where |e| is the number of the vertices in e, eij is 
an edge with two vertices included in e and ? ? 0 
is a parameter balancing the relative importance 
of n-ary hyper-edges compared with binary ones.  
3 Semi-supervised Learning from Title 
Given the document semantic network 
represented as a phrase hyper-graph, one way to 
make better use of the semantic information is to 
rank phrases with a semi-supervised learning 
strategy, where the title phrases are regarded as 
labeled samples, while the other phrases as unla-
beled ones. That is, the information we have at 
the beginning about how to rank phrases is that 
the title phrases are the most important phrases. 
Initially, the title phrases are assigned with a pos-
itive score of 1 indicating its importance and oth-
297
er phrases are assigned zero. Then the impor-
tance scores of the phrases are learned iteratively 
from the title phrases through the hyper-graph. 
The key idea behind hyper-graph based semi-
supervised ranking is that the vertices which 
usually belong to the same hyper-edges should 
be assigned with similar scores. Then, we have 
the following two constraints: 
1. The phrases which have many incident hy-
per-edges in common should be assigned similar 
scores. 
2. The given initial scores of the title phrases 
should be changed as little as possible. 
Given a weighted hyper-graph G, assume a 
ranking function f over V, which assigns each 
vertex v an importance score f(v). f can be 
thought as a vector in Euclid space R|V|. For the 
convenience of computation, we use an inci-
dence matrix H to represent the hypergraph, de-
fined as: 
0, if ( , ) 1, if 
v eh v e v e
??? ? ??
   (2) 
Based on the incidence matrix, we define the 
degrees of the vertex v and the hyper-edge e as 
   (3) 
and 
   (4) 
Then, to formulate the above-mentioned con-
straints, let  denote the initial score vector, then 
the importance scores of the phrases are learned 
iteratively by solving the following optimization 
problem: 
| |
2arg min { ( ) }Vf R f f y?? ? ? ?
 (5) 
2
{ , }
1 1 ( ) ( )( ) ( )2 ( ) ( ) ( )e E u v e
f u f vf w ee d u d v?? ?
? ?? ? ?? ?
? ?? ?
 (6) 
where ?> 0 is the parameter specifying the 
tradeoff between the two competitive items. Let 
Dv and De denote the diagonal matrices contain-
ing the vertex and the hyper-edge degrees re-
spectively, W denote the diagonal matrix con-
taining the hyper-edge weights, f* denote the so-
lution of (6).  Zhou has given the solution (Zhou, 
2005) as. 
* * (1 )f f y? ?? ? ? ?   (7) 
where 
1/2 1 1/2Tv e vD HWD H D? ? ?? ?  and 1/ ( 1)? ?? ? . 
Using an approximation algorithm (e.g. Algo-
rithm 1), we can finally get a vector f 
representing the approximate phrase scores. 
Algorithm 1: PhraseRank(V, T, a, b) 
Input: Title phrase set = {v1,v2,?,vt},the set of other 
phrases ={vt+1,vt+2,?,vn}, parameters ? and ?, con-
vergence threshold ? 
Output: The approximate phrase scores f  
Construct a document semantic network for all the 
phrases {v1,v2,?,vn} using the method described  in 
section 2. 
Let 
1/2 1 1/2Tv e vD HWD H D? ? ? ?? ;  
Initialize the score vector y as 1,1iy i t? ? ? , and  
0,jy t j n? ? ?
; 
Let , k = 0; 
REPEAT  
1 (1 )k kf f y?? ?? ? ? ?; 
, ; 
; 
UNTIL  
END 
Finally we rank phrases in descending order of 
the calculated importance scores and select those 
highest ranked phrases as key phrases. Accord-
ing to the number of all the candidate phrases, 
we choose an appropriate proportion, i.e. 10%, of 
all the phrases as key phrases. 
4 Evaluation 
4.1 Experiment Set-up  
We first collect all the Wikipedia terms to com-
pose of a dictionary. The word sequences that 
occur in the dictionary are identified as phrases. 
Here we use a finite-state automaton to accom-
plish this task to avoid the imprecision of pre-
processing by POS tagging or chunking. Then, 
we adopt the WSD technique proposed by (Tur-
dakov and Velikhov 2008) to find the corres-
ponding Wikipedia article for each phrase. As 
mentioned in Section 2, a document semantic 
network in the form of a hyper-graph is con-
structed, on which Algorithm 1 is applied to rank 
the phrases.  
To evaluate our proposed approach, we select 
200 pieces of news from well-known English 
media. 5 to 10 key phrases are manually labeled 
in each news document and the average number 
of the key phrases is 7.2 per document. Due to 
the abbreviation and synonymy phenomena, we 
construct a thesaurus and convert all manual and 
automatic phrases into their canonical forms 
when evaluated. The traditional Recall, Precision 
and F1-measure metrics are adopted for evalua-
tion. This section conducts two sets of experi-
ment: (1) to examine the influence of two para-
meters: ? and ?, on the key phrase extraction 
performance; (2) to compare with other well 
known state-of-art key phrase extraction ap-
proaches. 
298
4.2 Parameter tuning  
The approach involves two parameters: ? (??0) 
is a relation factor balancing the influence of n-
ary relations and binary relations; ? (0???1) is a 
learning factor tuning the influence from the title 
phrases. It is hard to find a global optimized so-
lution for the combination of these two factors. 
So we apply a gradient search strategy. At first, 
the learning factor is set to ?=0.8. Different val-
ues of ? ranging from 0 to 3 are examined. Then, 
given that ? is set to the value with the best per-
formance, we conduct experiments to find an 
appropriate value for ?. 
4.2.1 ?: Relation Factor 
First, we fix the learning factor ? as 0.8 random-
ly and evaluate the performance by varying ? 
value from 0 to 3. When ?=0, it means that the 
weight of n-ary relations is zero and only binary 
relations are considered. As we can see from 
Figure 1, the performance is improved in most 
cases in terms of F1-measure and reaches a peak 
at ?=1.8. This justifies the rational to incorpo-
rate n-ary relations with binary relations in the 
document semantic network. 
 
Figure 1. F1-measures with ? in [0 3] 
4.2.2 ?: Learning factor  
Next, we set the relation factor ?=1.8, we in-
spect the performance with the learning factor ? 
ranging from 0 to 1. ?=1 means that the ranking 
scores learn from the semantic network without 
any consideration of title phrases. As shown in 
Figure 2, we find that the performance almost 
keep a smooth fluctuation as ? increases from 0 
to 0.9, and then a diving when ?=1. This proves 
that title phrases indeed provide valuable infor-
mation for learning.  
 
Figure 2. F1-measure with ? in [0,1] 
4.3 Comparison with Other Approaches  
Our approach aims at inferring important key 
phrases from title phrases through a semantic 
network. Here we take a method of synonym 
expansion as the baseline, called WordNet ex-
pansion here. The WordNet2 expansion approach 
selects all the synonyms of the title phrases in the 
document as key phrases. Afterwards, our ap-
proach is evaluated against two existing ap-
proaches, which rely on the conventional seman-
tic network and are able to capture binary rela-
tions only. One approach combines the title in-
formation into the Grineva?s community-based 
method (Grineva et al, 2009), called title-
community approach. The title-community ap-
proach uses the Girvan-Newman algorithm to 
cluster phrases into communities and selects 
those phrases in the communities containing the 
title phrases as key phrases. We do not limit the 
number of key phrases selected. The other one is 
based on topic-sensitive LexRank (Otterbacher et 
al., 2005), called title-sensitive PageRank here. 
The title-sensitive PageRank approach makes use 
of title phrases to re-weight the transitions be-
tween vertices and picks up 10% top-ranked 
phrases as key phrases.  
Approach Precision Recall F1 
Title-sensitive Pa-
geRank (d=0.15) 
34.8% 39.5% 37.0% 
Title-community 29.8% 56.9% 39.1% 
Our approach 
(?=1.8, ?=0.5) 
39.4% 44.6% 41.8% 
WordNet expansion 
(baseline) 
7.9%  32.9% 12.5% 
Table 1. Comparison with other approaches 
Table 1 summarizes the performance on the 
test data. The results presented in the table show 
that our approach exhibits the best performance 
among all the four approaches. It follows that the 
key phrases inferred from a document semantic 
network are not limited to the synonyms of title 
phrases. As the title-sensitive PageRank ap-
                                               
2 http://wordnet.princeton.edu 
299
proach totally ignores the n-ary relations, its per-
formance is the worst. Based on binary relations, 
the title-community approach clusters phrases 
into communities and each community can be 
considered as an n-ary relation. However, this 
approach lacks of an importance propagation 
process. Consequently, it has the highest recall 
value but the lowest precision. In contrast, our 
approach achieves the highest precision, due to 
its ability to infer many correct key phrases using 
importance propagation among n-ary relations.  
5 Conclusion  
This work is based on the belief that key phrases 
tend to have close semantics to the title phrases. 
In order to make better use of phrase relations in 
key phrase extraction, we explore the Wikipedia 
knowledge to model one document as a semantic 
network in the form of hyper-graph, through 
which the other phrases learned their importance 
scores from the title phrases iteratively. Experi-
mental results demonstrate the effectiveness and 
robustness of our approach. 
 
Acknowledgments 
The work described in this paper was partially 
supported by NSFC programs (No: 60773173, 
60875042 and 90920011), and Hong Kong RGC 
Projects (No: PolyU5217/07E).  We thank the 
anonymous reviewers for their insightful com-
ments. 
References  
David Milne, Ian H. Witten. 2008. An Effective, 
Low-Cost Measure of Semantic Relatedness 
Obtained from Wikipedia Links. In Wikipedia 
and AI workshop at the AAAI-08 Conference, 
Chicago, US. 
Dengyong Zhou, Jiayuan Huang and Bernhard 
Sch?lkopf. 2005. Beyond Pairwise Classifica-
tion and Clustering Using Hypergraphs. MPI 
Technical Report, T?bingen, Germany. 
Denis Turdakov and Pavel Velikhov. 2008. Semantic 
relatedness metric for wikipedia concepts 
based on link analysis and its application to 
word sense disambiguation. In Colloquium on 
Databases and Information Systems (SYRCoDIS). 
Ian H. Witten, Gordon W. Paynter, Eibe Frank , Carl 
Gutwin , Craig G. Nevill-Manning. 1999.  KEA: 
practical automatic keyphrase extraction, In 
Proceedings of the fourth ACM conference on Dig-
ital libraries, pp.254-255, California, USA. 
Jahna Otterbacher, Gunes Erkan and Dragomir R. 
Radev. 2005. Using Random Walks for Ques-
tion-focused Sentence Retrieval. In Proceedings 
of HLT/EMNLP 2005, pp. 915-922, Vancouver, 
Canada. 
Maria Grineva, Maxim Grinev and Dmitry Lizorkin. 
2009. Extracting key terms from noisy and 
multitheme documents, In Proceedings of the 
18th international conference on World wide web, 
pp. 661-670, Madrid, Spain.  
Michael Strube and Simone Paolo Ponzetto. 
2006.WikiRelate! Computing Semantic Rela-
tedness using Wikipedia. In Proceedings of the 
21st National Conference on Artificial Intelligence, 
pp. 1419?1424, Boston, MA. 
M. E. J. Newman. 2004. Analysis of Weighted Net-
works.  Physical Review E 70, 056131. 
 
300
