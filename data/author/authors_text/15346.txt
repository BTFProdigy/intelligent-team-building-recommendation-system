Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 873?883,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Semantic Frames to Predict Stock Price Movement
Boyi Xie, Rebecca J. Passonneau, Leon Wu
Center for Computational Learning Systems
Columbia University
New York, NY USA
(bx2109|becky|leon.wu)@columbia.edu
Germa?n G. Creamer
Howe School of Technology Management
Stevens Institute of Technology
Hoboken, NJ USA
gcreamer@stevens.edu
Abstract
Semantic frames are a rich linguistic re-
source. There has been much work
on semantic frame parsers, but less that
applies them to general NLP problems.
We address a task to predict change in
stock price from financial news. Seman-
tic frames help to generalize from spe-
cific sentences to scenarios, and to de-
tect the (positive or negative) roles of spe-
cific companies. We introduce a novel tree
representation, and use it to train predic-
tive models with tree kernels using sup-
port vector machines. Our experiments
test multiple text representations on two
binary classification tasks, change of price
and polarity. Experiments show that fea-
tures derived from semantic frame pars-
ing have significantly better performance
across years on the polarity task.
1 Introduction
A growing literature evaluates the financial effects
of media on the market (Tetlock, 2007; Engel-
berg and Parsons, 2011). Recent work has applied
NLP techniques to various financial media (con-
ventional news, tweets) to detect sentiment in con-
ventional news (Devitt and Ahmad, 2007; Haider
and Mehrotra, 2011) or message boards (Chua
et al, 2009), or discriminate expert from non-
expert investors in financial tweets (Bar-Haim et
al., 2011). With the exception of Bar-Haim et al
(2011), these NLP studies have relied on small
corpora of hand-labeled data for training or evalu-
ation, and the connection to market events is done
indirectly through sentiment detection. We hy-
pothesize that conventional news can be used to
predict changes in the stock price of specific com-
panies, and that the semantic features that best
represent relevant aspects of the news vary across
On Wednesday, April 11th, 2012, Google Inc announced
its first
 quarterly earnings report, a week before the April20 options contracts expiration in contrast to its history
of reporting a day before monthly options expirations.
The stock price of Google surged 3.85% from April
10th?s $626.86 to 12th?s $651.01. On Friday, April 13th,
news reported Oracle Corp would sue
 Google Inc ,
claiming Google?s Android operating system tramples its intellectual property rights . Jury selection was set for
the next Monday. Google?s stock price tumbled 4.06% on
Friday, and continued to drop in the following week.
Figure 1: Summary of financial news items per-
taining to Google in April, 2012.
market sectors. To test this hypothesis, we use
price information to label data from six years of
financial news. Our experiments test several doc-
ument representations for two binary classification
tasks, change of price and polarity. Our main con-
tribution is a novel tree representation based on
semantic frame parses that performs significantly
better than enriched bag-of-words vectors.
Figure 1 shows a constructed example based
on extracts from financial news about Google in
April, 2012. It illustrates how a series of events
reported in the news precedes and potentially
predicts a large change in Google?s stock price.
Google?s early announcement of quarterly earn-
ings possibly presages trouble, and its stock price
falls soon after reports of a legal action against
Google by Oracle. To produce a coherent story,
the original sentences were edited for Figure 1,
but they are in the style of actual sentences from
our dataset. Accurate detection of events and re-
lations that might have an impact on stock price
should benefit from document representation that
captures sentiment in lexical items (e.g., aggres-
sive) combined with the conceptual relations cap-
tured by FrameNet (Ruppenhofer and Rehbein,
2012). A frame is a lexical semantic representa-
873
tion of the conceptual roles played by parts of a
clause, and relates different lexical items (e.g., re-
port, announce) to the same situation type. In the
figure, some of the words that evoke frames have
been underlined, and role fillers are outlined by
boxes or ovals. Sentiment words are in italics.
To the best of our knowledge, this paper is
the first to apply semantic frames in this do-
main. On the polarity task, the semantic frame fea-
tures encoded as trees perform significantly better
across years and sectors than bag-of-words vectors
(BOW), and outperform BOW vectors enhanced
with semantic frame features, and a supervised
topic modeling approach. The results on the price
change task show the same trend, but are not sta-
tistically significant, possibly due to the volatility
of the market in 2007 and the following several
years. Yet even modest predictive performance
on both tasks could have an impact, as discussed
below, if incorporated into financial models such
as Rydberg and Shephard (2003). We first dis-
cuss the motivation and related work. Section 4
presents vector-based and tree-based features from
semantic frame parses, and section 5 describes our
dataset. The experimental design and results ap-
pear in the following section, followed by discus-
sion and conclusions.
2 Motivation
Financial news is a rich vein for NLP applica-
tions to mine. Many news organizations that fea-
ture financial news, such as Reuters, the Wall
Street Journal and Bloomberg, devote significant
resources to the analysis of corporate news.
Much of the data that would support studies of
a link between the news media and the market are
publicly available. As pointed out by Tetlock et
al. (2008), linguistic communication is a poten-
tially important source of information about firms?
fundamental values. Because very few stock mar-
ket investors directly observe firms? production ac-
tivities, they get most of their information sec-
ondhand. Their three main sources are analysts?
forecasts, quantifiable publicly disclosed account-
ing variables, and descriptions of firms? current
and future profit-generating activities. If analyst
and accounting variables are incomplete or biased
measures of firms? fundamental values, linguis-
tic variables may have incremental explanatory
power for firms? future earnings and returns.
Consider the following sentences:
Oracle sued Google in August 2010, saying
Google?s Android mobile operating system in-
fringes its copyrights and patents for the Java pro-
gramming language. (a)
Oracle has accused Google of violating its in-
tellectual property rights to the Java programming
language. (b)
Oracle has blamed Google and alleged that the
latter has committed copyright infringement re-
lated to Java programming language held by Ora-
cle. (c)
Oracle?s Ellison says couldn?t sway Google on
Java. (d)
Sentences a, b and c are semantically similar,
but lexically rather distinct: the shared words are
the company names and Java (programming lan-
guage). Bag-of-Words (BOW) document repre-
sentation is difficult to surpass for many document
classification tasks, but cannot capture the de-
gree of semantic similarity among these sentences.
Methods that have proven successful for para-
phrase detection (Deerwester et al, 1990; Dolan
et al, 2004), as in the main clauses of b and
c, include latent variable models that simultane-
ously capture the semantics of words and sen-
tences, such as latent semantic analysis (LSA) or
latent Dirichlet alocation (LDA). However, our
task goes beyond paraphrase detection. The first
three sentences all indicate an adversarial relation
of Oracle to Google involving a negative judge-
ment. It would be useful to capture the similarities
among all three of these sentences, and to distin-
guish the role of each company (who is suing and
who is being sued). Further, these three sentences
potentially have a greater impact on market per-
ception of Google in contrast to a sentence like d,
that refers to the same conflict more indirectly, and
whose main clause verb is say. We hypothesize
that semantic frames can address these issues.
Most of the NLP literature on semantic frames
addresses how to build robust semantic frame
parsers, with intrinsic evaluation against gold stan-
dard parses. There have been few applications of
semantic frame parsing for extrinsic tasks. To test
for measurable benefits of semantic frame parsing,
this paper poses the following questions:
1. Are semantic frames useful for document
representation of financial news?
2. What aspects of frames are most useful?
3. What is the relative performance of document
representation that relies on frames?
874
4. What improvements could be made to best
exploit semantic frames?
Our work is not aimed at investment profit.
Rather, we investigate whether computational lin-
guistic methodologies can improve our under-
standing of a company?s fundamental market
value, and whether linguistic information derived
from news produces a consistent enough result to
benefit more comprehensive financial models.
3 Related Work
NLP has recently been applied to financial text
for market analysis, primarily using bag-of-
words (BOW) document representation. Luss
and d?Aspremont (2008) use text classification to
model price movements of financial assets on a
per-day basis. They try to predict the direction
of return, and abnormal returns, defined as an ab-
solute return greater than a predefined threshold.
Kogan et al (2009) address a text regression prob-
lem to predict the financial risk of investment in
companies. They analyze 10-K reports to predict
stock return volatility. They also predict whether
a company will be delisted following its 10-K re-
port. Ruiz et al (2012) correlate text with finan-
cial time series volume and price data. They find
that graph centrality measures like page rank and
degree are more strongly correlated to both price
and traded volume for an aggregation of similar
companies, while individual stocks are less corre-
lated. Lavrenko et al (2000) present an approach
to identify news stories that influence the behavior
of financial markets, and predict trends in stock
prices based on the content of news stories that
precede the trends. Luss and d?Aspremont (2008)
and Lavrenko et al (2000) both point out the de-
sire for document feature engineering as future re-
search directions. We explore a rich feature space
that relies on frame semantic parsing.
Sentiment analysis figures strongly in NLP
work on news. General Inquirer (GI), a content
analysis program, is used to quantify pessimism of
news in Tetlock (2007) and Tetlock et al (2008).
Other resources for sentiment detection include
the Dictionary of Affect in Language (DAL) to
score the prior polarity of words, as in Agarwal
et al (2011) on social media data. Our study in-
corporates DAL scores along with other features.
FrameNet is a rich lexical resource (Fillmore et
al., 2003), based on the theory of frame seman-
tics (Fillmore, 1976). There is active research
Category Features Value type
Frame F, FT, FE N
attributes wF, wFT, wFE R?0
BOW UniG, BiG, TriG N
wUniG, wBiG, wTriG R?0
pDAL all-Pls, all-Act, all-Img R??=0,std=1
VB-Pls, VB-Act, VB-Img R??=0,std=1
JJ-Pls, JJ-Act, JJ-Img R??=0,std=1
RB-Pls, RB-Act, RB-Img R??=0,std=1
Table 1: FWD features (Frame, bag-of-Words,
part-of-speech DAL score) and their value types.
to build more accurate parsers (Das and Smith,
2011; Das and Smith, 2012). Semantic role label-
ing using FrameNet has been used to identify an
opinion with its holder and topic (Kim and Hovy,
2006). For deep representation of sentiment anal-
ysis, Ruppenhofer and Rehbein (2012) propose
SentiFrameNet.
Our work addresses classification tasks that
have potential relevance to an influential financial
model (Rydberg and Shephard, 2003). This model
decomposes stock price analysis of financial data
into a three-part ADS model - activity (a binary
process modeling the price move or not), direction
(another binary process modeling the direction of
the moves) and size (a number quantifying the size
of the moves). Our two binary classification tasks
for news, price change and polarity, are analogous
to their activity and direction. In contrast to the
ADS model, our approach does not calculate the
conditional probability of each factor. At present,
our goal is limited to the determination of whether
NLP features can uncover information from news
that could help predict stock price movement or
support analysts? investigations.
4 Methods
We propose two approaches for the use of seman-
tic frames. The first is a rich vector space based
on semantic frames, word forms and DAL affect
scores. The second is a tree representation that
encodes semantic frame features, and depends on
tree kernel measures for support vector machine
classification. The semantic parses of both meth-
ods are derived from SEMAFOR1 (Das and Smith,
2012; Das and Smith, 2011), which solves the se-
mantic parsing problem by rule-based target iden-
tification, log-linear model based frame identifica-
tion and frame element filling.
1http://www.ark.cs.cmu.edu/SEMAFOR.
875
Frame (F) Judgment comm. Commerce buy
accuse buy
Target (FT) sue purchase
charge bid
Frame COMMUNICATOR BUYER
Element EVALUEE SELLER
(FE) REASON GOODS
Table 2: Sample frames.
4.1 Semantic Frame based FWD Features
Table 1 lists 24 types of features, including seman-
tic Frame attributes, bag-of-Words, and scores for
words in the Dictionary of Affect in Language by
part of speech (pDAL). We refer to these features
as FWD features throughout the paper. FWD fea-
tures are used alone and in combinations.
FrameNet defines hundreds of frames, each of
which represents a scenario associated with se-
mantic roles, or frame elements, that serve as
participants in the scenario the frame signifies.
Table 2 shows two frames. The frame Judg-
ment communication (JC or Judgment comm. in
the rest of the paper) represents a scenario in
which a COMMUNICATOR communicates a judg-
ment of an EVALUEE for some REASON. It is
evoked by (target) words such as accuse or sue.
Here we use F for the frame name, FT for the
target words, and FE for frame elements. We use
both frequency and weighted scores. For exam-
ple, we define idf -adjusted weighted frame fea-
tures, such as wF for attribute F in document d as
wFF,d = f(F, d) ? log |D||d?D:F?d| , where f(F, d)is the frequency of frame F in d, D is the whole
document set and |?| is the cardinality operator.
Bag-of-Words features include term frequency
and tfidf of unigrams, bigrams, and trigrams.
DAL (Dictionary of Affect in Language) is a
psycholinguistic resource to measure the emo-
tional meaning of words and texts (Whissel,
1989). It includes 8,742 words that were anno-
tated for three dimensions: Pleasantness (Pls), Ac-
tivation (Act), and Imagery (Img). Agarwal et
al. (2009) introduced part-of-speech specific DAL
features for sentiment analysis. We follow their
approach by averaging the scores for all words,
verb only, adjective only, and adverb only words.
Feature values are normalized to mean of zero and
standard deviation of one.
4.2 SemTree Feature Space and Kernels
We propose SemTree as another feature space to
encode semantic information in trees. SemTree
can distinguish the roles of each company of in-
terest, or designated object (e.g. who is suing and
who is being sued).
4.2.1 Construction of Tree Representation
The semantic frame parse of a sentence is a forest
of trees, each of which corresponds to a semantic
frame. SemTree encodes the original frame struc-
ture and its leaf words and phrases, and highlights
a designated object at a particular node as follows.
For each lexical item (target) that evokes a frame, a
backbone is found by extracting the path from the
root to the role filler mentioning a designated ob-
ject; the backbone is then reversed to promote the
designated object. If multiple frames have been
assigned to the same designated object, their back-
bones are merged. Lastly, the frame elements and
frame targets are inserted at the frame root.
The top of Figure 2 shows the semantic parse
for sentence a from section 2; we use it to illus-
trate tree construction for designated object Ora-
cle. The parse has two frames (Figure 2-(1)&(2)),
one corresponding to the main clause (verb sue),
and the other for the tenseless adjunct (verb say).
The reversed paths extracted from each frame root
to the designated object Oracle become the back-
bones (Figures 2-(3)&(4)). After merging the two
backbones we get the resulting SemTree, as shown
in Figure 2-(5). By the same steps, this sentence
would also yield a SemTree with Google at the
root, in the role of EVALUEE.
4.2.2 Kernels and Tree Substructures
The tree kernel (Moschitti, 2006; Collins and
Duffy, 2002) is a function of tree similarity, based
on common substructures (tree fragments). There
are two types of substructures. A subtree (ST) is
defined as any node of a tree along with all its de-
scendants. A subset tree (SST) is defined as any
node along with its immediate children and, op-
tionally, part or all of the children?s descendants.
Each tree is represented by a d dimensional vec-
tor where the i?th component counts the number
of occurrences of the i?th tree fragment.
Define the function hi(T ) as the number of
occurrences of the i?th tree fragment in tree
T , so that T is now represented as h(T ) =
(h1(T ), h2(T ), ..., hd(T )). We define the set of
nodes in tree T1 and T2 as NT1 and NT2 respec-
tively. We define the indicator function Ii(n) to be
1 if subtree i is seen rooted at node n, and 0 oth-
erwise. It follows that hi(T1) = ?n1?NT1 Ii(n1)
876
Designated object: Oracle (ORCL)
Sentence: Oracle sued Google in August 2010, saying Google?s Android mobile operating system infringes its copyrights and patents for the Java pro-
gramming language.
SRL: [OracleJC.FE.Communicator,Stmt.FE.Speaker] [suedJC.Target] [GoogleJC.FE.Evaluee] in August 2010, [sayingStmt.Target]
[Googles? Android mobile operating system infringes its copyrights and patents for the Java programming languageStmt.FE.Message].
(1) Judgment comm.
FE.Evaluee
GOOG
FE.Communicator
ORCL
Judgment comm.Target
sue
(2) Statement
FE.Message
GOOG?s Android ... language
FE.Speaker
ORCL
Statement.Target
say
(3) ORCL
FE.Communicator
Judgment comm.
(4) ORCL
FE.Speaker
Statement
(5) ORCL
Speaker
Statement
FE.MessageFE.SpeakerStatement.Target
say
Communicator
Judgment comm.
FE.EvalueeFE.CommunicatorJudgment comm.Target
sue
Figure 2: Constructing a tree representation for the designated object Oracle in sentence shown.
and hi(T2) = ?n2?NT2 Ii(n2). Their similaritycan be efficiently computed by the inner product,
K(T1, T2) = h(T1) ? h(T2)
=
?
i hi(T1)hi(T2)
=
?
i(
?
n1?NT1
Ii(n1))(
?
n2?NT2
Ii(n2))
=
?
n1?NT1
?
n2?NT2
?
i Ii(ni)Ii(n2)
=
?
n1?NT1
?
n2?NT2
?(n1, n2)
where ?(n1, n2) is the number of common frag-
ments rooted in the nodes n1 and n2. If the pro-
ductions of these two nodes (themselves and their
immediate children) differ, ?(n1, n2) = 0; other-
wise iterate their children recursively to evaluate
?(n1, n2) =
?|children|
j (?+?(c
j
n1 , cjn2)) , where
? = 0 for ST kernel and ? = 1 for SST kernel.
The kernel computational complexity is
O(|NT1 | ? |NT2 |), where all pairwise compar-
isons are carried out between T1 and T2. However,
there are fast algorithms for kernel computation
that run in linear time on average, either by
dynamic programming (Collins and Duffy, 2002),
or pre-sorting production rules before training
(Moschitti, 2006). We use the latter.
5 Dataset
We use publicly available financial news from
Reuters from January 2007 through August 2012.
This time frame includes a severe economic down-
turn in 2007-2010 followed by a modest recovery
in 2011-2012.
An information extraction pipeline is used to
pre-process the data. News full text is extracted
from HTML. The timestamp of the news is ex-
tracted for a later alignment with stock price infor-
mation, which will be discussed in section 6. The
company mentioned is identified by a rule-based
matching of a finite list of companies.
There are a total of 10 sectors in the Global In-
dustry Classification Standard (GICS), an industry
taxonomy used by the S&P 500.2 To explore our
approach for this domain, we select three sectors
for our experiment: Telecommunication Services
(TS, the sector with the smallest number of com-
panies), Information Technology (IT), and Con-
sumer Staples (CS), due to our familiarity with the
companies in these sectors and an expectation of
different characteristics they may exhibit. In the
expectation there would be semantic differences
associated with these sectors, experiments are per-
formed independently for each sector. There are
also differences in the number of companies in the
sector, and the amount of news.
We bin news articles by sector. We remove ar-
ticles that only list stock prices or only show ta-
bles of accounting reports. The first preprocess-
ing step is to extract sentences that mention the
2Standard & Poor?s 500 is an equity market index that
includes 500 U.S. leading companies in leading industries.
877
CS (N=40) IT (N=69) TS (N=8)
avg # news 5,702?749 13446?1,272 2,177?188
avg # sentences 16,090?2,316 48,929?5,927 6,970?1,383
avg # com./sent. 1.07?0.01 1.06?0.20 1.14?0.03
avg # total 17,131?2,339 51,306?8,637 7,947?1,576
Table 3: Data statistics of mean and standard devi-
ation by year from January 2007 to August 2012,
for three sectors, with the number of companies.
relevant companies. Each data instance is a sen-
tence and one of the target companies it mentions.
Table 3 summarizes the data statistics. For exam-
ple, the consumer staples sector has 40 companies.
It has an average of 5,702 news articles (16,090
sentences) per year. Each sentence that mentions
a consumer staple company mentions 1.07 com-
panies on average. On average, this sector has
17,131 instances per year.
6 Experiments
Our current experiments are carried out for each
year, training on one year and testing on the next.
The choice to use a coarse time interval with no
overlap was an expedience to permit more numer-
ous exploratory experiments, given the computa-
tional resources these experiments require. We test
the influence of news to predict (1) a change in
stock price (change task), and (2) the polarity of
change (increase vs. decrease; polarity task). Ex-
periments evaluate the FWD and SemTree feature
spaces compared to two baselines: bag-of-words
(BOW) and supervised latent Dirichlet alocation
(sLDA) (Blei and McAuliffe, 2007). BOW in-
cludes features of unigram, bigram and trigram.
sLDA is a statistical model to classify documents
based on LDA topic models, using labeled data. It
has been applied to and shown good performance
in topical text classification, collaborative filter-
ing, and web page popularity prediction problems.
6.1 Labels, Evaluation Metrics, and Settings
We align publicly available daily stock price data
from Yahoo Finance with the Reuters news us-
ing a method to avoid back-casting. In particular,
we use the daily adjusted closing price - the price
quoted at the end of a trading day (4PM US East-
ern Time), then adjusted by dividends, stock split,
and other corporate actions. We create two types
of labels for news documents using the price data,
to label the existence of a change and the direc-
tion of change. Both tasks are treated as binary
classification problems. Based on the finding of
a one-day delay of the price response to the in-
formation embedded in the news by Tetlock et al
(2008), we use ?t = 1 in our experiment. To
constrain the number of parameters, we also use a
threshold value (r) of a 2% change, based on the
distribution of price changes across our data. In
future work, this could be tuned to sector or time.
change=
{
+1 if |pt(0)+?t?pt(?1)|pt(?1) > r
?1 otherwise
polarity=
{
+1 if pt(0)+?t > pt(?1) and change = +1
?1 if pt(0)+?t < pt(?1) and change = +1
pt(?1) is the adjusted closing price at the end of
the last trading day, and pt(0)+?t is the price of
the end of the trading day after the ?t day delay.
Only the instances with changes are included in
the polarity task.
There is high variance across years in the pro-
portion of positive labels, and often highly skewed
classes in one direction or the other. The average
ratios of +/- classes for change and polarity over
the six years? data are 0.73 (std=0.35) and 1.12
(std=0.25), respectively. Because the time frame
for our experiments includes an economic crisis
followed by a recovery period, we note that the
ratio between increase and decrease of price flips
between 2007, where it is 1.40, and 2008, where it
is 0.71. Accuracy is very sensitive to skew: when a
class has low frequency, accuracy can be high us-
ing a baseline that makes prediction on the major-
ity class. Given the high data skew, and the large
changes from year to year in positive versus nega-
tive skew, we use a more robust evaluation metric.
Our evaluation relies on the Matthews corre-
lation coefficient (MCC, also known as the ?-
coefficient) (Matthews, 1975) to avoid the bias of
accuracy due to data skew, and to produce a ro-
bust summary score independent of whether the
positive class is skewed to the majority or minor-
ity. In contrast to f-measure, which is a class-
specific weighted average of precision and recall,
and whose weighted version depends on a choice
of whether the class-specific weights should come
from the training or testing data, MCC is a sin-
gle summary value that incorporates all 4 cells of
a 2 ? 2 confusion matrix (TP, FP, TN and FN for
True or False Positive or Negative). We have also
observed that MCC has a lower relative standard
deviation than f-measure.
For a 2 ? 2 contingency table, MCC corre-
sponds to the square root of the average ?2 statis-
tic ??2/n, with values in [-1,1]. It has been sug-
878
Change
test years BOW sLDA FWD SemTreeFWD
Consumer Staples
2008-2010 0.1015 0.0774 0.1079 0.1426
2011-2012 0.1663 0.1203 0.1664 0.1736
5 years 0.1274 0.0945 0.1313 0.1550
Information Technology
2008-2010 0.0580 0.0585 0.0701 0.0846
2011-2012 0.0894 0.0681 0.1076 0.1273
5 years 0.0705 0.0623 0.0851 0.1017
Telecommunication Services
2008-2010 0.1501 0.1615 0.1497 0.2409
2011-2012 0.2256 0.2084 0.2191 0.4009
5 years 0.1803 0.1803 0.1774 0.3049
Polarity
Consumer Staples
2008-2010 0.0359 0.0383 0.0956 0.1054
2011-2012 0.0938 0.0270 0.1131 0.1285
5 years 0.0590 0.0338 0.1026 0.1147
p-value >>0.1000 0.0918 0.0489
Information Technology
2008-2010 0.0551 0.0332 0.0697 0.0763
2011-2012 0.0591 0.0516 0.0764 0.0857
5 years 0.0567 0.0405 0.0723 0.0801
p-value 0.0626 0.0948 0.0103
Telecommunication Services
2008-2010 0.0402 0.0464 0.0821 0.0745
2011-2012 0.0366 0.0781 0.0611 0.0809
5 years 0.0388 0.0591 0.0737 0.0770
p-value >>0.1000 0.0950 0.0222
Table 4: Average MCC for the change and polarity
tasks by feature representation, for 2008-2010; for
2011-2012; for all 5 years and associated p-values
of ANOVAs for comparison to BOW.
gested as one of the best methods to summarize
into a single value the confusion matrix of a binary
classification task (Jurman and Furlanello, 2010;
Baldi et al, 2000). Given the confusion matrix(TP FN
FP TN
) :
MCC = TP ?TN?FP ?FN?
(TP+FP )(TP+FN)(TN+FP )(TN+FN)
.
All sentences with at least one company men-
tion are used for the experiment. We remove
stop words and use Stanford CoreNLP for part-
of-speech tagging and named entity recognition.
Models are constructed using linear kernel sup-
port vector machines for both classification tasks.
SVM-light with tree kernels3 (Joachims, 2006;
Moschitti, 2006) is used for both the FWD and
SemTree feature spaces.
6.2 Results
Table 4 shows the mean MCC values for each task,
for each sector. Separate means are shown for
the test years of financial crisis (2008-2010) and
economic recovery (2011-2012) to highlight the
differences in performance that might result from
market volatility.
3SVM-light: http://svmlight.joachims.org and Tree
Kernels in SVM-light: http://disi.unitn.it/moschitti/Tree-
Kernel.htm.
pos. 1 dow, investors, index, retail, data
pos. 2 costs, food, price, prices, named entity 4
neu. 1 q3, q1, nov, q2, apr
neu. 2 cents, million, share, year, quarter
neg. 1 cut, sales, prices, hurt, disappointing
neg. 2 percent, call, company, fell, named entity 7
Table 5: Sample sLDA topics for consumer staples
for test year 2010 (train on 2009), polarity task.
SemTree combined with FWD (SemTreeFWD)
generally gives the best performance in both
change and polarity tasks. SemTree results here
are based on the subset tree (SST) kernel, be-
cause of its greater precision in computing com-
mon frame structures and consistently better per-
formance over the subtree (ST) kernel. SemTree
also provides interpretable features for manual
analysis as discussed in the next section.
Analysis of Variance (ANOVA) tests were per-
formed on the full 5 years for each sector, to com-
pare each feature representation as a predictor of
MCC score with the baseline BOW. The ANOVAs
yield the p-values shown in Table 4. There were no
significant differences from BOW on the change
task. For polarity detection, SemTreeFWD was
significantly better than BOW for each sector (see
boldface p-values). No other method was sig-
nificantly better than BOW, although FWD ap-
proaches significance on all sectors, and sLDA ap-
proaches significance on IT.
sLDA has promising MCC scores for the
telecommunication sector, which has only 8 com-
panies, thus many fewer data instances. Table 5
displays a sample of sLDA topics with good per-
formance on polarity for the consumer staples sec-
tor for training year 2009. The positive topics are
related to stock index details and retail data. The
negative topics contain many words with negative
sentiment (e.g., hurt, disappointing).
7 Discussion
7.1 Semantic Parse Quality
In general, SEMAFOR parses capture most of
the important frames for our purposes. There is,
however, significant room for improvement. On
a small, randomly selected sample of sentences
from all three sectors, two of the authors working
independently evaluated the semantic parses, with
approximately 80% agreement. Some of the in-
accuracies in frame parses result from errors prior
to the SEMAFOR parse, such as tokenization or
879
+ (Target(jump))
+ (RECIPIENT(Receiving))
+ (VICTIM(Defend))
+ (PERCEIVER AGENTIVE(Perception active(Target)
(PERCEIVER AGENTIVE)(PHENOMENON)))
+ (DONOR(Giving(Target)(THEME)(DONOR)))
+ (Target(beats))
...
- (PHENOMENON(Perception active(Target)(PERCEIVER
AGENTIVE)(PHENOMENON)))
- (TRIGGER(Response))
- (Target(cuts))
- (VICTIM(Cause harm(Target(hurt))(VICTIM)))
Figure 3: Best performing SemTree fragments for
increase (+) and decrease (-) of price for consumer
staples sector across training years.
dependency parsing errors. The average sentence
length for the sample was 33.3 words, with an av-
erage of 14 frames per sentence, 3 of them with a
GICS company as a role filler. Because SemTree
encodes only the frames containing a designated
object (company), these are the frames we eval-
uated. On average, about half the frames with
a designated object were correct, and two thirds
of those frames we judged to be important. Be-
sides errors due to incorrect tokenization or depen-
dency parsing, we observed that about 8% to 10%
of frames were incorrectly assigned to due word
sense ambiguity.
7.2 Feature Analysis
The experimental results show the SemTree space
to be the one representation tested here that is sig-
nificantly better than BOW, but only for the po-
larity task. Post hoc analysis indicates this may
be due to the aptness of semantic frame parsing
for polarity. Limitations in our treatment of time
point to directions for improvement regarding the
change task.
Some strengths of our approach are the separate
treatment of different sectors, and the benefits of
SemTree features. To analyze which were the best
performing features within sectors, we extracted
the best performing frame fragments for the po-
larity task using a tree kernel feature engineering
method presented in Pighin and Moschitti (2009).
The algorithm selects the most relevant features in
accordance with the weights estimated by SVM,
and uses these features to build an explicit repre-
sentation of the kernel space. Figure 3 shows the
best performing SemTree fragments of the polar-
ity task for the consumer staples sector.
Recall that we hypothesized differences in
semantic frame features across sectors. This
shows up as large differences in the strength
of features across sectors. More strikingly, the
same feature can differ in polarity across sec-
tors. For example, in consumer staples, (EVAL-
UEE(Judgment communication)) has positive po-
larity, compared with negative polarity in informa-
tion technology sector. The examples we see indi-
cate that the positive cases pertain to aggressive re-
tail practices that lead to lawsuits with only small
fines, but whose larger impact benefits the bottom
line. A typical case is the sentence, The plaintiffs
accused Wal-Mart of discriminating against dis-
abled customers by mounting ?point-of-sale? ter-
minals in many stores at elevated heights that can-
not be reached. Lawsuits in the IT sector, on the
other hand, are often about technology patent dis-
putes, and are more negative, as illustrated by our
example sentence in Figure 2.
SemTree features capture the differences be-
tween semantic roles for the same frame, and be-
tween the same semantic role in different frames.
For example, the PERCEIVER AGENTIVE role of
the Perception active frame contributes to predic-
tion of an increase in price, as in R.J. Reynolds
is watching this situation closely and will respond
as appropriate. Conversely, a company that fills
the PHENOMENON role of the same frame con-
tributes to prediction of a price decrease, as in In-
vestors will get a clearer look at how the market
values the Philip Morris tobacco businesses when
Altria Group Inc. ?when-issued? shares begin
trading on Tuesday. When a company fills the
VICTIM role in the Cause harm frame, this can
predict a decrease in price, as in Hershey has
been hurt by soaring prices for cocoa, energy and
other commodities, whereas filling the VICTIM
role in the Defend frame is associated with an in-
crease in price, as in At Berkshire?s annual share-
holder meeting earlier this month, Warren Buffett
defended Wal-Mart , saying the scandal did not
change his opinion of the company.
One weakness of our approach that we dis-
cussed above is that there is a strong effect of
time that we do not address. The same SemTree
feature can be predictive for one time period and
not for another. (GOODS(Commerce sell)) is re-
lated to a decrease in price for 2008 and 2009 but
to an increase in price for 2010-2012. There is
clearly an influence of the overall economic con-
text that we do not take into account. For example,
880
the practices of acquiring or selling a business are
different in downturning versus recovering mar-
kets. An important observation of the MCC val-
ues, especially in the case of SemTreeFWD is that
MCC increases during the years 2011-2012. We
attribute this change to the difficulty of predicting
stock price trends when there is the high volatil-
ity typical of a financial crisis. The effect of news
on volatility, however, can be explored indepen-
dently. For example, Creamer et al (2012) detect
a strong association.
Another weakness of our approach is that we
take sentences out of context, which can lead
to prediction errors. For example, the sentence
Longs? real estate assets alone are worth some
$2.9 billion, or $71.50 per share, Ackman wrote,
meaning that CVS would essentially be paying
for real estate, but gaining Longs? pharmacy ben-
efit management business and retail operations for
free is treated as predicting a positive polarity for
CVS. This would be accurate if CVS was actually
going to acquire Longs? business. Later in the
same news item, however, there is a sentence indi-
cating that the sale will not go through, which pre-
dicts negative polarity for CVS: Pershing Square
Capital Management said on Thursday it won?t
support a tender offer from CVS Caremark Corp
for rival Longs Drug Stores Corp because the of-
fer price ?materially understates the fair value of
the company,? according to a filing.
8 Conclusion
We have presented a model for predicting stock
price movement from news. We proposed FWD
(Frames, BOW, and part-of-speech specific DAL)
features and SemTree data representations. Our
semantic frame-based model benefits from tree
kernel learning using support vector machines.
The experimental results for our feature represen-
tation perform significantly better than BOW on
the polarity task, and show promise on the change
task. It also facilitates human interpretable analy-
sis to understand the relation between a company?s
market value and its business activities. The sig-
nals generated by this algorithm could improve the
prediction of a financial time series model, such as
ADS (Rydberg and Shephard, 2003).
Our future work will consider the contextual in-
formation for sentence selection, and an aggrega-
tion of weighted news content based on the decay
effect over time for individual companies. We plan
to use a moving window for training and testing.
We will also explore different labeling methods,
such as a threshold for price change tuned by sec-
tors and background economics.
9 Acknowledgements
The authors thank the anonymous reviewers for
their insightful comments.
References
Apoorv Agarwal, Fadi Biadsy, and Kathleen Mckeown.
2009. Contextual phrase-level polarity analysis us-
ing lexical affect scoring and syntactic N-grams. In
Proceedings of the 12th Conference of the Euro-
pean Chapter of the ACL (EACL 2009), pages 24?
32, Athens, Greece, March. Association for Compu-
tational Linguistics.
Apoorv Agarwal, Boyi Xie, Ilia Vovsha, Owen Ram-
bow, and Rebecca Passonneau. 2011. Sentiment
analysis of twitter data. In Proceedings of the Work-
shop on Languages in Social Media, LSM ?11, pages
30?38. Association for Computational Linguistics.
Pierre Baldi, S?ren Brunak, Yves Chauvin, Claus A. F.
Andersen, and Henrik Nielsen. 2000. Assessing the
accuracy of prediction algorithms for classification:
an overview. Bioinformatics, 16:412 ? 424.
Roy Bar-Haim, Elad Dinur, Ronen Feldman, Moshe
Fresko, and Guy Goldstein. 2011. Identifying
and following expert investors in stock microblogs.
In Proceedings of the 2011 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1310?1319, Edinburgh, Scotland, UK., July. Asso-
ciation for Computational Linguistics.
David M. Blei and Jon D. McAuliffe. 2007. Super-
vised topic models. In Advances in Neural Informa-
tion Processing Systems, Proceedings of the Twenty-
First Annual Conference on Neural Information
Processing Systems, Vancouver, British Columbia,
Canada, December 3-6.
Christopher Chua, Maria Milosavljevic, and James R.
Curran. 2009. A sentiment detection engine for
internet stock message boards. In Proceedings of
the Australasian Language Technology Association
Workshop 2009, pages 89?93, Sydney, Australia,
December.
Michael Collins and Nigel Duffy. 2002. New rank-
ing algorithms for parsing and tagging: kernels over
discrete structures, and the voted perceptron. In Pro-
ceedings of the 40th Annual Meeting on Association
for Computational Linguistics, ACL ?02, pages 263?
270, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
881
Germa?n G. Creamer, Yong Ren, and Jeffrey V. Nicker-
son. 2012. A Longitudinal Analysis of Asset Re-
turn, Volatility and Corporate News Network. In
Business Intelligence Congress 3 Proceedings.
Dipanjan Das and Noah A. Smith. 2011. Semi-
supervised frame-semantic parsing for unknown
predicates. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguis-
tics: Human Language Technologies - Volume 1,
HLT ?11, pages 1435?1444, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Dipanjan Das and Noah A. Smith. 2012. Graph-based
lexicon expansion with sparsity-inducing penalties.
In HLT-NAACL, pages 677?687. The Association
for Computational Linguistics.
Scott Deerwester, Susan T. Dumais, George W. Furnas,
Thomas K. Landauer, and Richard Harshman. 1990.
Indexing by latent semantic analysis. Journal of the
American Society for Information Science.
Ann Devitt and Khurshid Ahmad. 2007. Sentiment
polarity identification in financial news: A cohesion-
based approach. In Proceedings of the 45th Annual
Meeting of the Association of Computational Lin-
guistics, pages 984?991, Prague, Czech Republic,
June. Association for Computational Linguistics.
William Dolan, Chris Quirk, and Chris Brockett. 2004.
Unsupervised construction of large paraphrase cor-
pora: Exploiting massively parallel news sources.
Proceedings of the 20th International Conference on
Computational Linguistics.
Joseph Engelberg and Christopher A. Parsons. 2011.
The causal impact of media in financial markets.
Journal of Finance, 66(1):67?97.
Charles J. Fillmore, Christopher R. Johnson, and
Miriam R. L. Petruck. 2003. Background to
Framenet. International Journal of Lexicography,
16(3):235?250, September.
Charles J. Fillmore. 1976. Frame semantics and
the nature of language. Annals of the New York
Academy of Sciences, 280(1):20?32.
Syed Aqueel Haider and Rishabh Mehrotra. 2011.
Corporate news classification and valence predic-
tion: A supervised approach. In Proceedings of
the 2nd Workshop on Computational Approaches to
Subjectivity and Sentiment Analysis (WASSA 2.011),
pages 175?181, Portland, Oregon, June. Association
for Computational Linguistics.
Thorsten Joachims. 2006. Training linear svms in lin-
ear time. In Proceedings of the 12th ACM SIGKDD
international conference on Knowledge discovery
and data mining, KDD ?06, pages 217?226, New
York, NY, USA. ACM.
Giuseppe Jurman and Cesare Furlanello. 2010. A uni-
fying view for performance measures in multi-class
prediction. ArXiv e-prints.
Soo-Min Kim and Eduard Hovy. 2006. Extracting
opinions, opinion holders, and topics expressed in
online news media text. In Proceedings of the Work-
shop on Sentiment and Subjectivity in Text, SST ?06,
pages 1?8, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Shimon Kogan, Dimitry Levin, Bryan R. Routledge,
Jacob S. Sagi, and Noah A. Smith. 2009. Pre-
dicting risk from financial reports with regression.
In Proceedings of Human Language Technologies:
The 2009 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, NAACL ?09, pages 272?280, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Victor Lavrenko, Matt Schmill, Dawn Lawrie, Paul
Ogilvie, David Jensen, and James Allan. 2000.
Mining of concurrent text and time series. In In pro-
ceedings of the 6th ACM SIGKDD Int?l Conference
on Knowledge Discovery and Data Mining Work-
shop on Text Mining, pages 37?44.
Ronny Luss and Alexandre d?Aspremont. 2008. Pre-
dicting abnormal returns from news using text clas-
sification. CoRR, abs/0809.2792.
Brian W. Matthews. 1975. Comparison of the pre-
dicted and observed secondary structure of t4 phage
lysozyme. Biochimica et Biophysica Acta (BBA) -
Protein Structure, 405(2):442 ? 451.
Alessandro Moschitti. 2006. Making tree kernels prac-
tical for natural language learning. In In Proceed-
ings of the 11th Conference of the European Chapter
of the Association for Computational Linguistics.
Daniele Pighin and Alessandro Moschitti. 2009. Re-
verse engineering of tree kernel feature spaces. In
Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing, EMNLP
2009, 6-7 August 2009, Singapore, pages 111?120.
Eduardo J. Ruiz, Vagelis Hristidis, Carlos Castillo,
Aristides Gionis, and Alejandro Jaimes. 2012. Cor-
relating financial time series with micro-blogging
activity. In Proceedings of the fifth ACM interna-
tional conference on Web search and data mining,
WSDM ?12, pages 513?522, New York, NY, USA.
ACM.
Josef Ruppenhofer and Ines Rehbein. 2012. Se-
mantic frames as an anchor representation for sen-
timent analysis. In Proceedings of the 3rd Work-
shop in Computational Approaches to Subjectivity
and Sentiment Analysis, WASSA ?12, pages 104?
109, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Tina H. Rydberg and Neil Shephard. 2003. Dynam-
ics of Trade-by-Trade Price Movements: Decompo-
sition and Models. Journal of Financial Economet-
rics, 1(1):2?25.
882
Paul C. Tetlock, Maytal Saar-Tsechansky, and Sofus
Macskassy. 2008. More than Words: Quantifying
Language to Measure Firms? Fundamentals. The
Journal of Finance.
Paul C. Tetlock. 2007. Giving Content to Investor Sen-
timent: The Role of Media in the Stock Market. The
Journal of Finance.
Cynthia M. Whissel. 1989. The dictionary of affect in
language. Emotion: Theory, Research, and Experi-
ence, 39(4):113?131.
883
Proceedings of the Workshop on Language in Social Media (LSM 2011), pages 30?38,
Portland, Oregon, 23 June 2011. c?2011 Association for Computational Linguistics
Sentiment Analysis of Twitter Data
Apoorv Agarwal Boyi Xie Ilia Vovsha Owen Rambow Rebecca Passonneau
Department of Computer Science
Columbia University
New York, NY 10027 USA
{apoorv@cs, xie@cs, iv2121@, rambow@ccls, becky@cs}.columbia.edu
Abstract
We examine sentiment analysis on Twitter
data. The contributions of this paper are: (1)
We introduce POS-specific prior polarity fea-
tures. (2) We explore the use of a tree kernel to
obviate the need for tedious feature engineer-
ing. The new features (in conjunction with
previously proposed features) and the tree ker-
nel perform approximately at the same level,
both outperforming the state-of-the-art base-
line.
1 Introduction
Microblogging websites have evolved to become a
source of varied kind of information. This is due to
nature of microblogs on which people post real time
messages about their opinions on a variety of topics,
discuss current issues, complain, and express posi-
tive sentiment for products they use in daily life. In
fact, companies manufacturing such products have
started to poll these microblogs to get a sense of gen-
eral sentiment for their product. Many times these
companies study user reactions and reply to users on
microblogs. One challenge is to build technology to
detect and summarize an overall sentiment.
In this paper, we look at one such popular mi-
croblog called Twitter and build models for classify-
ing ?tweets? into positive, negative and neutral senti-
ment. We build models for two classification tasks:
a binary task of classifying sentiment into positive
and negative classes and a 3-way task of classi-
fying sentiment into positive, negative and neutral
classes. We experiment with three types of models:
unigram model, a feature based model and a tree
kernel based model. For the feature based model
we use some of the features proposed in past liter-
ature and propose new features. For the tree ker-
nel based model we design a new tree representa-
tion for tweets. We use a unigram model, previously
shown to work well for sentiment analysis for Twit-
ter data, as our baseline. Our experiments show that
a unigram model is indeed a hard baseline achieving
over 20% over the chance baseline for both classifi-
cation tasks. Our feature based model that uses only
100 features achieves similar accuracy as the uni-
gram model that uses over 10,000 features. Our tree
kernel based model outperforms both these models
by a significant margin. We also experiment with
a combination of models: combining unigrams with
our features and combining our features with the tree
kernel. Both these combinations outperform the un-
igram baseline by over 4% for both classification
tasks. In this paper, we present extensive feature
analysis of the 100 features we propose. Our ex-
periments show that features that have to do with
Twitter-specific features (emoticons, hashtags etc.)
add value to the classifier but only marginally. Fea-
tures that combine prior polarity of words with their
parts-of-speech tags are most important for both the
classification tasks. Thus, we see that standard nat-
ural language processing tools are useful even in
a genre which is quite different from the genre on
which they were trained (newswire). Furthermore,
we also show that the tree kernel model performs
roughly as well as the best feature based models,
even though it does not require detailed feature en-
gineering.
We use manually annotated Twitter data for our
30
experiments. One advantage of this data, over pre-
viously used data-sets, is that the tweets are col-
lected in a streaming fashion and therefore represent
a true sample of actual tweets in terms of language
use and content. Our new data set is available to
other researchers. In this paper we also introduce
two resources which are available (contact the first
author): 1) a hand annotated dictionary for emoti-
cons that maps emoticons to their polarity and 2)
an acronym dictionary collected from the web with
English translations of over 5000 frequently used
acronyms.
The rest of the paper is organized as follows. In
section 2, we discuss classification tasks like sen-
timent analysis on micro-blog data. In section 3,
we give details about the data. In section 4 we dis-
cuss our pre-processing technique and additional re-
sources. In section 5 we present our prior polarity
scoring scheme. In section 6 we present the design
of our tree kernel. In section 7 we give details of our
feature based approach. In section 8 we present our
experiments and discuss the results. We conclude
and give future directions of research in section 9.
2 Literature Survey
Sentiment analysis has been handled as a Natural
Language Processing task at many levels of gran-
ularity. Starting from being a document level classi-
fication task (Turney, 2002; Pang and Lee, 2004), it
has been handled at the sentence level (Hu and Liu,
2004; Kim and Hovy, 2004) and more recently at
the phrase level (Wilson et al, 2005; Agarwal et al,
2009).
Microblog data like Twitter, on which users post
real time reactions to and opinions about ?every-
thing?, poses newer and different challenges. Some
of the early and recent results on sentiment analysis
of Twitter data are by Go et al (2009), (Bermingham
and Smeaton, 2010) and Pak and Paroubek (2010).
Go et al (2009) use distant learning to acquire senti-
ment data. They use tweets ending in positive emoti-
cons like ?:)? ?:-)? as positive and negative emoti-
cons like ?:(? ?:-(? as negative. They build mod-
els using Naive Bayes, MaxEnt and Support Vec-
tor Machines (SVM), and they report SVM outper-
forms other classifiers. In terms of feature space,
they try a Unigram, Bigram model in conjunction
with parts-of-speech (POS) features. They note that
the unigram model outperforms all other models.
Specifically, bigrams and POS features do not help.
Pak and Paroubek (2010) collect data following a
similar distant learning paradigm. They perform a
different classification task though: subjective ver-
sus objective. For subjective data they collect the
tweets ending with emoticons in the same manner
as Go et al (2009). For objective data they crawl
twitter accounts of popular newspapers like ?New
York Times?, ?Washington Posts? etc. They re-
port that POS and bigrams both help (contrary to
results presented by Go et al (2009)). Both these
approaches, however, are primarily based on ngram
models. Moreover, the data they use for training and
testing is collected by search queries and is therefore
biased. In contrast, we present features that achieve
a significant gain over a unigram baseline. In addi-
tion we explore a different method of data represen-
tation and report significant improvement over the
unigram models. Another contribution of this paper
is that we report results on manually annotated data
that does not suffer from any known biases. Our
data is a random sample of streaming tweets unlike
data collected by using specific queries. The size
of our hand-labeled data allows us to perform cross-
validation experiments and check for the variance in
performance of the classifier across folds.
Another significant effort for sentiment classifica-
tion on Twitter data is by Barbosa and Feng (2010).
They use polarity predictions from three websites as
noisy labels to train a model and use 1000 manually
labeled tweets for tuning and another 1000 manu-
ally labeled tweets for testing. They however do
not mention how they collect their test data. They
propose the use of syntax features of tweets like
retweet, hashtags, link, punctuation and exclamation
marks in conjunction with features like prior polar-
ity of words and POS of words. We extend their
approach by using real valued prior polarity, and by
combining prior polarity with POS. Our results show
that the features that enhance the performance of our
classifiers the most are features that combine prior
polarity of words with their parts of speech. The
tweet syntax features help but only marginally.
Gamon (2004) perform sentiment analysis on
feeadback data from Global Support Services sur-
vey. One aim of their paper is to analyze the role
31
of linguistic features like POS tags. They perform
extensive feature analysis and feature selection and
demonstrate that abstract linguistic analysis features
contributes to the classifier accuracy. In this paper
we perform extensive feature analysis and show that
the use of only 100 abstract linguistic features per-
forms as well as a hard unigram baseline.
3 Data Description
Twitter is a social networking and microblogging
service that allows users to post real time messages,
called tweets. Tweets are short messages, restricted
to 140 characters in length. Due to the nature of this
microblogging service (quick and short messages),
people use acronyms, make spelling mistakes, use
emoticons and other characters that express special
meanings. Following is a brief terminology associ-
ated with tweets. Emoticons: These are facial ex-
pressions pictorially represented using punctuation
and letters; they express the user?s mood. Target:
Users of Twitter use the ?@? symbol to refer to other
users on the microblog. Referring to other users in
this manner automatically alerts them. Hashtags:
Users usually use hashtags to mark topics. This
is primarily done to increase the visibility of their
tweets.
We acquire 11,875 manually annotated Twitter
data (tweets) from a commercial source. They have
made part of their data publicly available. For infor-
mation on how to obtain the data, see Acknowledg-
ments section at the end of the paper. They collected
the data by archiving the real-time stream. No lan-
guage, location or any other kind of restriction was
made during the streaming process. In fact, their
collection consists of tweets in foreign languages.
They use Google translate to convert it into English
before the annotation process. Each tweet is labeled
by a human annotator as positive, negative, neutral
or junk. The ?junk? label means that the tweet can-
not be understood by a human annotator. A man-
ual analysis of a random sample of tweets labeled
as ?junk? suggested that many of these tweets were
those that were not translated well using Google
translate. We eliminate the tweets with junk la-
bel for experiments. This leaves us with an unbal-
anced sample of 8,753 tweets. We use stratified sam-
pling to get a balanced data-set of 5127 tweets (1709
tweets each from classes positive, negative and neu-
tral).
4 Resources and Pre-processing of data
In this paper we introduce two new resources for
pre-processing twitter data: 1) an emoticon dictio-
nary and 2) an acronym dictionary. We prepare
the emoticon dictionary by labeling 170 emoticons
listed on Wikipedia1 with their emotional state. For
example, ?:)? is labeled as positive whereas ?:=(? is
labeled as negative. We assign each emoticon a label
from the following set of labels: Extremely-positive,
Extremely-negative, Positive, Negative, and Neu-
tral. We compile an acronym dictionary from an on-
line resource.2 The dictionary has translations for
5,184 acronyms. For example, lol is translated to
laughing out loud.
We pre-process all the tweets as follows: a) re-
place all the emoticons with a their sentiment po-
larity by looking up the emoticon dictionary, b) re-
place all URLs with a tag ||U ||, c) replace targets
(e.g. ?@John?) with tag ||T ||, d) replace all nega-
tions (e.g. not, no, never, n?t, cannot) by tag ?NOT?,
and e) replace a sequence of repeated characters by
three characters, for example, convert coooooooool
to coool. We do not replace the sequence by only
two characters since we want to differentiate be-
tween the regular usage and emphasized usage of the
word.
Acronym English expansion
gr8, gr8t great
lol laughing out loud
rotf rolling on the floor
bff best friend forever
Table 1: Example acrynom and their expansion in the
acronym dictionary.
We present some preliminary statistics about the
data in Table 3. We use the Stanford tokenizer (Klein
and Manning, 2003) to tokenize the tweets. We use
a stop word dictionary3 to identify stop words. All
the other words which are found in WordNet (Fell-
baum, 1998) are counted as English words. We use
1http://en.wikipedia.org/wiki/List of emoticons
2http://www.noslang.com/
3http://www.webconfs.com/stop-words.php
32
Emoticon Polarity
:-) :) :o) :] :3 :c) Positive
:D C: Extremely-Positive
:-( :( :c :[ Negative
D8 D; D= DX v.v Extremely-Negative
: | Neutral
Table 2: Part of the dictionary of emoticons
the standard tagset defined by the Penn Treebank for
identifying punctuation. We record the occurrence
of three standard twitter tags: emoticons, URLs and
targets. The remaining tokens are either non English
words (like coool, zzz etc.) or other symbols.
Number of tokens 79,152
Number of stop words 30,371
Number of English words 23,837
Number of punctuation marks 9,356
Number of capitalized words 4,851
Number of twitter tags 3,371
Number of exclamation marks 2,228
Number of negations 942
Number of other tokens 9047
Table 3: Statistics about the data used for our experi-
ments.
In Table 3 we see that 38.3% of the tokens are stop
words, 30.1% of the tokens are found in WordNet
and 1.2% tokens are negation words. 11.8% of all
the tokens are punctuation marks excluding excla-
mation marks which make up for 2.8% of all tokens.
In total, 84.1% of all tokens are tokens that we ex-
pect to see in a typical English language text. There
are 4.2% tags that are specific to Twitter which in-
clude emoticons, target, hastags and ?RT? (retweet).
The remaining 11.7% tokens are either words that
cannot be found in WordNet (like Zzzzz, kewl) or
special symbols which do not fall in the category of
Twitter tags.
5 Prior polarity scoring
A number of our features are based on prior po-
larity of words. For obtaining the prior polarity of
words, we take motivation from work by Agarwal
et al (2009). We use Dictionary of Affect in Lan-
guage (DAL) (Whissel, 1989) and extend it using
WordNet. This dictionary of about 8000 English
language words assigns every word a pleasantness
score (? R) between 1 (Negative) - 3 (Positive). We
first normalize the scores by diving each score my
the scale (which is equal to 3). We consider words
with polarity less than 0.5 as negative, higher than
0.8 as positive and the rest as neutral. If a word is not
directly found in the dictionary, we retrieve all syn-
onyms from Wordnet. We then look for each of the
synonyms in DAL. If any synonym is found in DAL,
we assign the original word the same pleasantness
score as its synonym. If none of the synonyms is
present in DAL, the word is not associated with any
prior polarity. For the given data we directly found
prior polarity of 81.1% of the words. We find po-
larity of other 7.8% of the words by using WordNet.
So we find prior polarity of about 88.9% of English
language words.
6 Design of Tree Kernel
We design a tree representation of tweets to combine
many categories of features in one succinct conve-
nient representation. For calculating the similarity
between two trees we use a Partial Tree (PT) ker-
nel first proposed by Moschitti (2006). A PT ker-
nel calculates the similarity between two trees by
comparing all possible sub-trees. This tree kernel
is an instance of a general class of convolution ker-
nels. Convolution Kernels, first introduced by Haus-
sler (1999), can be used to compare abstract objects,
like strings, instead of feature vectors. This is be-
cause these kernels involve a recursive calculation
over the ?parts? of abstract object. This calculation
is made computationally efficient by using Dynamic
Programming techniques. By considering all possi-
ble combinations of fragments, tree kernels capture
any possible correlation between features and cate-
gories of features.
Figure 1 shows an example of the tree structure
we design. This tree is for a synthesized tweet:
@Fernando this isn?t a great day for playing the
HARP! :). We use the following procedure to con-
vert a tweet into a tree representation: Initialize the
main tree to be ?ROOT?. Then tokenize each tweet
and for each token: a) if the token is a target, emoti-
con, exclamation mark, other punctuation mark, or a
negation word, add a leaf node to the ?ROOT? with
33
VBGfor
EW
POSplayingPOS
STOP
NN dayPOS
EW EW
NN CAPS harp
EXC ||P||STOP
this
||T||
great
ROOT
NOTSTOP EW
is JJ
VB
GG forEW POS
VB
GG f orE
VBGf
orEW VBG ffoff ffrff
Figure 1: Tree kernel for a synthesized tweet: ?@Fernando this isn?t a great day for playing the HARP! :)?
the corresponding tag. For example, in the tree in
Figure 1 we add tag ||T || (target) for ?@Fernando?,
add tag ?NOT? for the token ?n?t?, add tag ?EXC?
for the exclamation mark at the end of the sentence
and add ||P || for the emoticon representing positive
mood. b) if the token is a stop word, we simply add
the subtree ? (STOP (?stop-word?))? to ?ROOT?. For
instance, we add a subtree corresponding to each of
the stop words: this, is, and for. c) if the token is
an English language word, we map the word to its
part-of-speech tag, calculate the prior polarity of the
word using the procedure described in section 5 and
add the subtree (EW (?POS? ?word? ?prior polarity?))
to the ?ROOT?. For example, we add the subtree
(EW (JJ great POS)) for the word great. ?EW? refers
to English word. d) For any other token <token>
we add subtree ?(NE (<token>))? to the ?ROOT?.
?NE? refers to non-English.
The PT tree kernel creates all possible subtrees
and compares them to each other. These subtrees
include subtrees in which non-adjacent branches be-
come adjacent by excising other branches, though
order is preserved. In Figure 1, we show some of
the tree fragments that the PT kernel will attempt to
compare with tree fragments from other trees. For
example, given the tree (EW (JJ) (great) (POS)), the
PT kernel will use (EW (JJ) (great) (POS)), (EW
(great) (POS)), (EW (JJ) (POS)), (EW (JJ) (great)),
(EW (JJ)), (EW (great)), (EW (POS)), (EW), (JJ),
(great), and (POS). This means that the PT tree ker-
nel attempts to use full information, and also ab-
stracts away from specific information (such as the
lexical item). In this manner, it is not necessary to
create by hand features at all levels of abstraction.
7 Our features
We propose a set of features listed in Table 4 for our
experiments. These are a total of 50 type of features.
We calculate these features for the whole tweet and
for the last one-third of the tweet. In total we get
100 additional features. We refer to these features as
Senti-features throughout the paper.
Our features can be divided into three broad cat-
egories: ones that are primarily counts of various
features and therefore the value of the feature is a
natural number ? N. Second, features whose value
is a real number ? R. These are primarily features
that capture the score retrieved from DAL. Thirdly,
features whose values are boolean ? B. These are
bag of words, presence of exclamation marks and
capitalized text. Each of these broad categories is
divided into two subcategories: Polar features and
Non-polar features. We refer to a feature as polar
if we calculate its prior polarity either by looking
it up in DAL (extended through WordNet) or in the
emoticon dictionary. All other features which are
not associated with any prior polarity fall in the Non-
polar category. Each of Polar and Non-polar features
is further subdivided into two categories: POS and
Other. POS refers to features that capture statistics
about parts-of-speech of words and Other refers to
all other types of features.
In reference to Table 4, row f1 belongs to the cat-
egory Polar POS and refers to the count of number
of positive and negative parts-of-speech (POS) in a
tweet, rows f2, f3, f4 belongs to the category Po-
34
lar Other and refers to count of number of negation
words, count of words that have positive and neg-
ative prior polarity, count of emoticons per polarity
type, count of hashtags, capitalized words and words
with exclamation marks associated with words that
have prior polarity, row f5 belongs to the category
Non-Polar POS and refers to counts of different
parts-of-speech tags, rows f6, f7 belong to the cat-
egory Non-Polar Other and refer to count of num-
ber of slangs, latin alphabets, and other words with-
out polarity. It also relates to special terms such as
the number of hashtags, URLs, targets and newlines.
Row f8 belongs to the category Polar POS and cap-
tures the summation of prior polarity scores of words
with POS of JJ, RB, VB and NN. Similarly, row f9
belongs to the category Polar Other and calculates
the summation of prior polarity scores of all words,
row f10 refers to the category Non-Polar Other and
calculates the percentage of tweet that is capitalized.
Finally, row f11 belongs to the category Non-
Polar Other and refers to presence of exclamation
and presence of capitalized words as features.
8 Experiments and Results
In this section, we present experiments and results
for two classification tasks: 1) Positive versus Nega-
tive and 2) Positive versus Negative versus Neutral.
For each of the classification tasks we present three
models, as well as results for two combinations of
these models:
1. Unigram model (our baseline)
2. Tree kernel model
3. 100 Senti-features model
4. Kernel plus Senti-features
5. Unigram plus Senti-features
For the unigram plus Senti-features model, we
present feature analysis to gain insight about what
kinds of features are adding most value to the model.
We also present learning curves for each of the mod-
els and compare learning abilities of models when
provided limited data.
Experimental-Set-up: For all our experiments we
use Support Vector Machines (SVM) and report av-
eraged 5-fold cross-validation test results. We tune
the C parameter for SVM using an embedded 5-fold
cross-validation on the training data of each fold,
i.e. for each fold, we first run 5-fold cross-validation
only on the training data of that fold for different
values of C. We pick the setting that yields the best
cross-validation error and use that C for determin-
ing test error for that fold. As usual, the reported
accuracies is the average over the five folds.
8.1 Positive versus Negative
This is a binary classification task with two classes
of sentiment polarity: positive and negative. We use
a balanced data-set of 1709 instances for each class
and therefore the chance baseline is 50%.
8.1.1 Comparison of models
We use a unigram model as our baseline. Re-
searchers report state-of-the-art performance for
sentiment analysis on Twitter data using a unigram
model (Go et al, 2009; Pak and Paroubek, 2010).
Table 5 compares the performance of three models:
unigram model, feature based model using only 100
Senti-features, and the tree kernel model. We report
mean and standard deviation of 5-fold test accuracy.
We observe that the tree kernels outperform the uni-
gram and the Senti-features by 2.58% and 2.66% re-
spectively. The 100 Senti-features described in Ta-
ble 4 performs as well as the unigram model that
uses about 10,000 features. We also experiment with
combination of models. Combining unigrams with
Senti-features outperforms the combination of ker-
nels with Senti-features by 0.78%. This is our best
performing system for the positive versus negative
task, gaining about 4.04% absolute gain over a hard
unigram baseline.
8.1.2 Feature Analysis
Table 6 presents classifier accuracy and F1-
measure when features are added incrementally. We
start with our baseline unigram model and subse-
quently add various sets of features. First, we add
all non-polar features (rows f5, f6, f7, f10, f11 in Ta-
ble 4) and observe no improvement in the perfor-
mance. Next, we add all part-of-speech based fea-
tures (rows f1, f8) and observe a gain of 3.49% over
the unigram baseline. We see an additional increase
in accuracy by 0.55% when we add other prior po-
larity features (rows f2, f3, f4, f9 in Table 4). From
35
N
Polar
POS # of (+/-) POS (JJ, RB, VB, NN) f1
Other # of negation words, positive words, negative words f2
# of extremely-pos., extremely-neg., positive, negative emoticons f3
# of (+/-) hashtags, capitalized words, exclamation words f4
Non-Polar
POS # of JJ, RB, VB, NN f5
Other # of slangs, latin alphabets, dictionary words, words f6
# of hashtags, URLs, targets, newlines f7
R
Polar
POS For POS JJ, RB, VB, NN,
?
prior pol. scores of words of that POS f8
Other
?
prior polarity scores of all words f9
Non-Polar Other percentage of capitalized text f10
B Non-Polar Other exclamation, capitalized text f11
Table 4: N refers to set of features whose value is a positive integer. They are primarily count features; for example,
count of number of positive adverbs, negative verbs etc. R refers to features whose value is a real number; for example,
sum of the prior polarity scores of words with part-of-speech of adjective/adverb/verb/noun, and sum of prior polarity
scores of all words. B refers to the set of features that have a boolean value; for example, presence of exclamation
marks, presence of capitalized text.
Model Avg. Acc (%) Std. Dev. (%)
Unigram 71.35 1.95
Senti-features 71.27 0.65
Kernel 73.93 1.50
Unigram +
Senti-features
75.39 1.29
Kernel +
Senti-features
74.61 1.43
Table 5: Average and standard deviation for test accuracy
for the 2-way classification task using different models:
Unigram (baseline), tree kernel, Senti-features, unigram
plus Senti-features, and tree kernel plus senti-features.
these experiments we conclude that the most impor-
tant features in Senti-features are those that involve
prior polarity of parts-of-speech. All other features
play a marginal role in achieving the best performing
system. In fact, we experimented by using unigrams
with only prior polarity POS features and achieved a
performance of 75.1%, which is only slightly lower
than using all Senti-features.
In terms of unigram features, we use Information
Gain as the attribute evaluation metric to do feature
selection. In Table 7 we present a list of unigrams
that consistently appear as top 15 unigram features
across all folds. Words having positive or negative
prior polarity top the list. Emoticons also appear as
important unigrams. Surprisingly though, the word
for appeared as a top feature. A preliminary analy-
Features Acc.
F1 Measure
Pos Neg
Unigram baseline 71.35 71.13 71.50
+ f5, f6, f7, f10, f11 70.1 69.66 70.46
+ f1, f8 74.84 74.4 75.2
+ f2, f3, f4, f9 75.39 74.81 75.86
Table 6: Accuracy and F1-measure for 2-way classifica-
tion task using Unigrams and Senti-features. All fi refer
to Table 4 and are cumulative.
Positive words love, great, good, thanks
Negative words hate, shit, hell, tired
Emoticons ||P || (positive emoticon),
||N || (negative emoticon)
Other for, ||U || (URL)
Table 7: List of top unigram features for 2-way task.
sis revealed that the word for appears as frequently
in positive tweets as it does in negative tweets. How-
ever, tweets containing phrases like for you and for
me tend to be positive even in the absence of any
other explicit prior polarity words. Owing to previ-
ous research, the URL appearing as a top feature is
less surprising because Go et al (2009) report that
tweets containing URLs tend to be positive.
8.1.3 Learning curve
The learning curve for the 2-way classification
task is in Figure 2. The curve shows that when lim-
36
0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 162
64
66
68
70
72
74
76
Percentage of training data
Ac
cur
acy
 (%
)
 
 
Unigram
Unigram + Our Features
Tree Kernel
Figure 2: Learning curve for two-way classification task.
ited data is used the advantages in the performance
of our best performing systems is even more pro-
nounced. This implies that with limited amount of
training data, simply using unigrams has a critical
disadvantage, while both tree kernel and unigram
model with our features exhibit promising perfor-
mance.
8.2 Positive versus Negative versus Neutral
This is a 3-way classification task with classes
of sentiment polarity: positive, negative and neu-
tral. We use a balanced data-set of 1709 instances
for each class and therefore the chance baseline is
33.33%.
8.2.1 Comparison of models
For this task the unigram model achieves a gain
of 23.25% over chance baseline. Table 8 compares
the performance of our three models. We report
mean and standard deviation of 5-fold test accuracy.
We observe that the tree kernels outperform the un-
igram and the Senti-features model by 4.02% and
4.29% absolute, respectively. We note that this dif-
ference is much more pronounced comparing to the
two way classification task. Once again, our 100
Senti-features perform almost as well as the unigram
baseline which has about 13,000 features. We also
experiment with the combination of models. For
this classification task the combination of tree ker-
nel with Senti-features outperforms the combination
of unigrams with Senti-features by a small margin.
Model Avg. Acc (%) Std. Dev. (%)
Unigram 56.58 1.52
Senti-features 56.31 0.69
Kernel 60.60 1.00
Unigram +
Senti-features
60.50 2.27
Kernel +
Senti-features
60.83 1.09
Table 8: Average and standard deviation for test accuracy
for the 3-way classification task using different models:
Unigram (baseline), tree kernel, Senti-features, unigram
plus Senti-features, and Senti-features plus tree kernels.
This is our best performing system for the 3-way
classification task, gaining 4.25% over the unigram
baseline.
The learning curve for the 3-way classification
task is similar to the curve of the 2-way classifica-
tion task, and we omit it.
8.2.2 Feature Analysis
Table 9 presents classifier accuracy and F1-
measure when features are added incrementally. We
start with our baseline unigram model and subse-
quently add various sets of features. First, we add all
non-polar features (rows f5, f6, f7, f10 in Table 4)
and observe an small improvement in the perfor-
mance. Next, we add all part-of-speech based fea-
tures and observe a gain of 3.28% over the unigram
baseline. We see an additional increase in accuracy
by 0.64% when we add other prior polarity features
(rows f2, f3, f4, f9 in Table 4). These results are in
line with our observations for the 2-way classifica-
tion task. Once again, the main contribution comes
from features that involve prior polarity of parts-of-
speech.
Features Acc.
F1 Measure
Pos Neu Neg
Unigram baseline 56.58 56.86 56.58 56.20
+
f5, f6, f7, f10, f11
56.91 55.12 59.84 55
+ f1, f8 59.86 58.42 61.04 59.82
+ f2, f3, f4, f9 60.50 59.41 60.15 61.86
Table 9: Accuracy and F1-measure for 3-way classifica-
tion task using unigrams and Senti-features.
The top ranked unigram features for the 3-way
37
classification task are mostly similar to that of the
2-way classification task, except several terms with
neutral polarity appear to be discriminative features,
such as to, have, and so.
9 Conclusion
We presented results for sentiment analysis on Twit-
ter. We use previously proposed state-of-the-art un-
igram model as our baseline and report an overall
gain of over 4% for two classification tasks: a binary,
positive versus negative and a 3-way positive versus
negative versus neutral. We presented a comprehen-
sive set of experiments for both these tasks on manu-
ally annotated data that is a random sample of stream
of tweets. We investigated two kinds of models:
tree kernel and feature based models and demon-
strate that both these models outperform the unigram
baseline. For our feature-based approach, we do fea-
ture analysis which reveals that the most important
features are those that combine the prior polarity of
words and their parts-of-speech tags. We tentatively
conclude that sentiment analysis for Twitter data is
not that different from sentiment analysis for other
genres.
In future work, we will explore even richer lin-
guistic analysis, for example, parsing, semantic
analysis and topic modeling.
10 Acknowledgments
Agarwal and Rambow are funded by NSF grant
IIS-0713548. Vovsha is funded by NSF grant
IIS-0916200. We would like to thank NextGen
Invent (NGI) Corporation for providing us with
the Twitter data. Please contact Deepak Mit-
tal (deepak.mittal@ngicorportion.com) about ob-
taining the data.
References
Apoorv Agarwal, Fadi Biadsy, and Kathleen Mckeown.
2009. Contextual phrase-level polarity analysis using
lexical affect scoring and syntactic n-grams. Proceed-
ings of the 12th Conference of the European Chapter
of the ACL (EACL 2009), pages 24?32, March.
Luciano Barbosa and Junlan Feng. 2010. Robust senti-
ment detection on twitter from biased and noisy data.
Proceedings of the 23rd International Conference on
Computational Linguistics: Posters, pages 36?44.
Adam Bermingham and Alan Smeaton. 2010. Classify-
ing sentiment in microblogs: is brevity an advantage is
brevity an advantage? ACM, pages 1833?1836.
C. Fellbaum. 1998. Wordnet, an electronic lexical
database. MIT Press.
Michael Gamon. 2004. Sentiment classification on cus-
tomer feedback data: noisy data, large feature vectors,
and the role of linguistic analysis. Proceedings of the
20th international conference on Computational Lin-
guistics.
Alec Go, Richa Bhayani, and Lei Huang. 2009. Twit-
ter sentiment classification using distant supervision.
Technical report, Stanford.
David Haussler. 1999. Convolution kernels on discrete
structures. Technical report, University of California
at Santa Cruz.
M Hu and B Liu. 2004. Mining and summarizing cus-
tomer reviews. KDD.
S M Kim and E Hovy. 2004. Determining the sentiment
of opinions. Coling.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. Proceedings of the 41st Meet-
ing of the Association for Computational Linguistics,
pages 423?430.
Alessandro Moschitti. 2006. Efficient convolution ker-
nels for dependency and constituent syntactic trees. In
Proceedings of the 17th European Conference on Ma-
chine Learning.
Alexander Pak and Patrick Paroubek. 2010. Twitter as
a corpus for sentiment analysis and opinion mining.
Proceedings of LREC.
B. Pang and L. Lee. 2004. A sentimental education: Sen-
timent analysis using subjectivity analysis using sub-
jectivity summarization based on minimum cuts. ACL.
P. Turney. 2002. Thumbs up or thumbs down? seman-
tic orientation applied to unsupervised classification of
reviews. ACL.
C M Whissel. 1989. The dictionary of Affect in Lan-
guage. Emotion: theory research and experience,
Acad press London.
T. Wilson, J. Wiebe, and P. Hoffman. 2005. Recognizing
contextual polarity in phrase level sentiment analysis.
ACL.
38
