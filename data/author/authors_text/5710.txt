Word Sense Disambiguation using a dictionary for sense similarity measure
Bruno Gaume Nabil Hathout Philippe Muller
IRIT ? CNRS, UPS & INPT ERSS ? CNRS & UTM IRIT ? CNRS, UPS & INPT
Toulouse, France Toulouse, France Toulouse, France
gaume@irit.fr hathout@univ-tlse2.fr muller@irit.fr
Abstract
This paper presents a disambiguation
method in which word senses are deter-
mined using a dictionary. We use a seman-
tic proximity measure between words in the
dictionary, taking into account the whole
topology of the dictionary, seen as a graph
on its entries. We have tested the method on
the problem of disambiguation of the dic-
tionary entries themselves, with promising
results considering we do not use any prior
annotated data.
1 Introduction
Various tasks dealing with natural language
data have to cope with the numerous different
senses possessed by every lexical item: ma-
chine translation, information retrieval, infor-
mation extraction ... This very old issue is far
from being solved, and evaluation of methods
addressing it is far from obvious (Resnik and
Yarowsky, 2000). This problem has been tack-
led in a number of ways1: by looking at con-
texts of use (with supervised learning or un-
supervised sense clustering) or by using lexi-
cal resources such as dictionaries or thesauri.
The first kind of approach relies on data that
are hard to collect (supervised) or very sensitive
to the type of corpus (unsupervised). The sec-
ond kind of approach tries to exploit the lexical
knowledge that is represented in dictionaries or
thesaurus, with various results from its incep-
tion up to now (Lesk, 1986; Banerjee and Ped-
ersen, 2003). In all cases, a distance between
words or word senses is used as a way to find
the right sense in a given context. Dictionary-
based approaches usually rely on a comparison
of the set of words used in sense definitions and
1A good introduction is (Ide and V?ronis, 1998), or
(Manning and Sch?tze, 1999), chap. 7.
in the context to disambiguate2.
This paper presents an algorithm which uses
a dictionary as a network of lexical items (cf.
sections 2 and 3) to compute a semantic simi-
larity measure between words and word senses.
It takes into account the whole topology of the
dictionary instead of just the entry of target
words. This arguably gives a certain robustness
of the results with respect to the dictionary. We
have begun testing this approach on word sense
disambiguation on definitions of the dictionary
itself (section 5), but the method is expected
to be more general, although this has not been
tested yet. Preliminary results are quite encour-
aging considering that the method does not re-
quire any prior annotated data, while operating
on an unconstrained vocabulary.
2 Building the graph of a
dictionary
The experiment we describe here has been
achieved on a dictionary restricted to nouns and
verbs only: we considered dictionary entries
classified as nouns and verbs and noun and verb
lemmas occurring within those entries. The
basic idea is to consider the dictionary as an
undirected graph whose nodes are noun entries,
and an edge exists between two nodes when-
ever one of them occur in the definition of the
other. More precisely, the graph of the dictio-
nary encodes two types of lexicographical in-
formations: (1) the definitions of the entries
sub-senses and (2) the structure of the entries
that is the hierarchical organisation of their sub-
senses. The graph then includes two types of
nodes: w-nodes used for the words that occur
2With the exceptions of the methods of (Kozima and
Furugori, 1993; Ide and V?ronis, 1990), both based on
models of activation of lexical relations, but who present
no quantified results.
in the definitions and ?-nodes used for the def-
initions of the sub-senses of the entries. The
graph is created in three phases:
1. For each dictionary entry, there is a ?-
node for the entry as a whole and there is
one ?-node for each of the sub-senses of
the entry. Then an edge is added between
each ?-node and the ?-nodes which rep-
resent the sub-senses of the next lower
level. In other words, the graph includes
a tree of ?-nodes which encodes the hier-
archical structure of each entry.
2. A w-node is created in the graph for each
word occurring in a definition and an edge
is added between the w-node and the ?-
node of that definition.
3. An edge is added between each w-node
and the top ?-node representing the dic-
tionary entry for that word.
For instance, given the entry for "tooth"3:
1. (Anat.) One of the hard, bony appendages
which are borne on the jaws, or on other
bones in the walls of the mouth or pharynx
of most vertebrates, and which usually aid
in the prehension and mastication of food.
2. Fig.: Taste; palate.
These are not dishes for thy dainty tooth.
?Dryden.
3. Any projection corresponding to the tooth
of an animal, in shape, position, or office;
as, the teeth, or cogs, of a cogwheel; a
tooth, prong, or tine, of a fork; a tooth, or
the teeth, of a rake, a saw, a file, a card.
4. (a) A projecting member resembling a
tenon, but fitting into a mortise that
is only sunk, not pierced through.
(b) One of several steps, or offsets, in a
tusk. See Tusk.
We would consider one ?-node for tooth as the
top-level entry, let us call it ?0. ?0 is con-
nected with an edge to the ?-nodes ?1, ?2 ,?3
and ?4 corresponding to the senses 1., 2., 3.
3Source: Webster?s Revised Unabridged Dictionary,
1996. The experiment has actually been done on a
French dictionary, Le Robert.
and 4.; the latter will have an edge towards the
two ?-nodes ?4.1 and ?4.2 for the sub-senses
4.a. and 4.b.; ?4.1 will have an edge to each w-
node built for nouns and verbs occurring in its
definition (member, resemble, tenon, fit, mor-
tise, sink, pierce). Then the w-node for tenon
will have an edge to the ?-node of the top-
level entry of tenon. We do not directly con-
nect ?4.1to the ?-nodes of the top-level entries
because these may have both w- and ?-node
daughters.
In the graph, ?-nodes have tags which indi-
cates their homograph number and their loca-
tion in the hierarchical structure of the entry.
These tags are sequences of integers where the
first one gives the homograph number and the
next ones indicate the rank of the sense-number
at each level. For instance, the previous nodes
?4.1 is tagged (0, 4, 1).
3 Prox, a distance between graph
nodes
We describe here our method (dubbed Prox) to
compute a distance between nodes in the kind
of graph described in the previous section. It is
a stochastic method for the study of so-called
hierarchical small-world graphs (Gaume et al,
2002) (see also the next section). The idea is to
see a graph as a Markov chain whose states are
the graph nodes and whose transitions are its
edges, with equal probabilities. Then we send
random particles walking through this graph,
and their trajectories and the dynamics of their
trajectories reveal their structural properties. In
short, we assume the average distance a parti-
cle has made between two nodes after a given
time is an indication of the semantic distance
between these nodes. Obviously, nodes located
in highly clustered areas will tend to be sepa-
rated by smaller distance.
Formally, if G = (V, E) is an irreflexive
graph with |V | = n, we note [G] the n ? n ad-
jacency matrix of G that is such that [G]i,j (the
ith row and jth column) is 1 if there is an edge
between node i and node j and 0 otherwise.
We note [G?] the Markovian matrix of G, such
that [G?]r,s = [G]r,s?
x?V ([G]r,x)
.
In the case of graphs built from a dictionary
as above,[G?]r,s is 0 if there is no edge between
nodes r and s, and 1/D otherwise, where D
is the number of neighbours of r. This is in-
deed a Markovian transition matrix since the
sum of each line is one (the graph considered
being connected).
We note [G?]i the matrix [G?] multiplied i
times by itself.
Let now PROX(G,i,r,s) be [G?]ir,s. This is thus
the probability that a random particle leaving
node r will be in node s after i time steps. This
is the measure we will use to determine if a
node s is closer to a node r than another node
t. Now we still have to find a proper value for
i. The next section explains the choice we have
made.
4 Dictionaries as hierarchical
small-worlds
Recent work in graph theory has revealed a set
of features shared by many graphs observed
"in the field" These features define the class
of "hierarchical small world" networks (hence-
forth HSW) (Watts and Strogatz, 1998; New-
man, 2003). The relevant features of a graph in
this respect are the following:
D the density of the network. HSWs typically
have a low D, i.e. they have rather few
edges compared to their number of ver-
tices.
L the average shortest path between two nodes.
It is also low in a HSW.
C the clustering rate. This is a measure of how
often neighbours of a vertex are also con-
nected in the graph. In a HSW, this feature
is typically high.
I the distribution of incidence degrees (i.e. the
number of neighbours) of vertices accord-
ing to the frequency of nodes (how many
nodes are there that have an incidence de-
gree of 1, 2, ... n). In a HSW network, this
distribution follows a power law: the prob-
ability P(k) that a given node has k neigh-
bours decreases as k??, with lambda > 0.
It means also that there are very few nodes
with a lot of neighbours, and a lot more
nodes with very few neighbours.
As a mean of comparison, table 1 shows the
differences between randoms graphs (nodes
are given, edges are drawn randomly between
nodes), regular graphs and HSWs.
The graph of a dictionary belongs to the class
of HSW. For instance, on the dictionary we
used, D=7, C=0.183, L=3.3. Table 2 gives a
few characteristics of the graph of nouns only
on the dictionary we used (starred columns in-
dicate values for the maximal self-connected
component).
We also think that the hierarchical aspect of
dictionaries (reflected in the distribution of in-
cidence degrees) is a consequence of the role
of hypernymy associated to the high polysemy
of some entries, while the high clustering rate
define local domains that are useful for dis-
ambiguation. We think these two aspects de-
termine the dynamics of random walks in the
graph as explained above, and we assume they
are what makes our method interesting for
sense disambiguation.
5 Word sense disambiguation
using Prox semantic distance
We will now present a method for disambiguat-
ing dictionary entries using the semantic dis-
tance introduced section (3).
The task can be defined as follows: we con-
sider a word lemma ? occurring in the defini-
tion of one of the senses of a word ?. We want
to tag ? with the most likely sense it has in
this context. Each dictionary entry is coded as
a tree of senses in the graph of the dictionary,
with a number list associated to each sub-entry.
For instance for a given word sense of word W,
listed as sub-sense II.3.1 in the dictionary, we
would record that sense as a node W(2,3,1) in
the graph. In fact, to take homography into ac-
count we had to add another level to this, for
instance W(1,1,2) is sense 1.2 of the first ho-
mograph of word W. In the absence of an ho-
mograph, the first number for a word sense will
conventionally be 0.
Let G=(V,E) the graph of words built as ex-
plained section 2, [G] is the adjacency matrix
of G, and [G?] is the corresponding Markovian
matrix . The following algorithm has then been
applied:
1. Delete all neighbours of ? in G, i.e. make
?x ? V, [G]?,x = [G]x,? = 0
2. Compute the new [G?]i where i is taken to
be 6
(with equal D) L C I
Random graphs small L small C Poisson Law
HSW small L high C power law
Regular graphs high L high C constant
Table 1: Comparing classes of graphs
nb nodes nb edges nb N* nb E* Diam* C* L*
Nouns 53770 392161 51511 392142 7 0.1829 3.3249
Nouns and sub-senses 140080 399969 140026 399941 11 0.0081 5.21
Table 2: Dictionary used
3. Let L be the line ? in the result. ?k, L[k] =
[G?]i?,k
4. Let E = {x1, x2, ..., xn} be the nodes cor-
responding to all the sub-senses induced
by the definition of ?.
Then take xk = argmaxx?E(L[x])
Then xk is the sub-sense with the best rank ac-
cording to the Prox distance.
The following steps needs a little explana-
tion:
1 This neighbours are deleted because other-
wise there is a bias towards the sub-senses
of ?, which then form a sort of "artificial"
cluster with respect to the given task. This
is done to allow the random walk to really
go into the larger network of senses.
2 Choosing a good value for the length of
the random walk through the graph is not
a simple matter. If it is too small, only lo-
cal relations appear (near synonyms, etc)
which might not appear in contexts to dis-
ambiguate (this is the main problem of
Lesk?s method); if it is too large, the dis-
tances between words will converge to a
constant value. So it has to be related in
some way to the average length between
two senses in the graph. A reasonable as-
sumption is therefore to stay close to this
average length. Hence we took i = 6 since
the average length is 5.21 (in the graph
with a node for every sub-sense, the graph
with a node for each entry having L=3.3)
6 Evaluating the results
The following methodology has been followed
to evaluate the process.
We have randomly taken about a hundred
of sub-entries in the chosen dictionary (out
of about 140,000 sub-entries), and have hand-
tagged all nouns and verbs in the entries with
their sub-senses (and homography number), ex-
cept those with only one sense, for a total of
about 350 words to disambiguate. For all pair
of (context,target), the algorithm gives a ranked
list of all the sub-senses of the target. Although
we have used both nouns and verbs to build the
graph of senses, we have tested disambiguation
first on nouns only, for a total of 237 nouns. We
have looked how the algorithm behaves when
we used both nouns and verbs in the graph of
senses.
To compare the human annotation to the au-
tomated one, we have applied the following
measures, where (h1, h2, , ...) is the human tag,
and (s1, s2, ..) is the top-ranked system output
for a context i defined as the entry and the target
word to disambiguate:
1. if h1 = 0 then do nothing else the homo-
graph score is 1 if h1 = s1, 0 otherwise;
2. in all cases, coarse polysemy count = 1 if
h2 = s2, 0 otherwise;
3. in all cases, fine polysemy count = 1 if ?i
hi = si
Thus, the coarse polysemy score computes how
many times the algorithm gives a sub-sense that
has the same "main" sense as the human tag
(the main-sense corresponds to the first level in
the hierarchy of senses as defined above). The
fine polysemy score gives the number of times
the algorithm gives exactly the same sense as
the human.
To give an idea of the difficult of the task,
we have computed the average number of main
entry target system output human tag
correct bal#n._m.*0_3 lieu#n. 1_1_3 1_1_1
correct van#n._m.*2_0_0_0_0 voiture#n. 0_2 0_2_3
error phon?tisme#n._m.*0 moyen#n. 1_1_1 2_1
error cr?ativit?#n._f.*0 pouvoir#n. 2_3 2_1
error acm?#n._m._ou_f.*0_1 phase#n. 0_1 0_4
Table 3: Detailed, main-sense evaluation of a couple of examples.
sub-senses and the number of all senses, for
each target word. This corresponds to a ran-
dom algorithm, choosing between all senses of
a given word. The expected value of this base-
line is thus:
? homograph score=?x 1/(number of ho-
mographs of x)
? coarse polysemy = ?x 1/(number of main
sub-senses of x)
? fine polysemy = ?x 1/(number of all sub-
senses of x)
A second baseline consists in answering al-
ways the first sense of the target word, since
this is often (but not always) the most common
usage of the word. We did not do this for homo-
graphs since the order in which they are given
in the dictionary does not seem to reflect their
importance.
Table 4 sums up the results.
7 Discussion
The result for homographs is very good but not
very significant given the low number of occur-
rences; this all the more true as we used a part-
of-speech tagger to disambiguate homographs
with different part-of-speech beforehand (these
have been left out of the computation of the
score).
The scores we get are rather good for coarse
polysemy, given the simplicity of the method.
As a means of comparison, (Patwardhan et
al., 2003) applies various measures of seman-
tic proximity (due to a number of authors), us-
ing the WordNet hierarchy, to the task of word
sense disambiguation of a few selected words
with results ranging from 0.2 to 0.4 with respect
to sense definition given in WordNet (the aver-
age of senses for each entry giving a random
score of about 0.2).
Our method already gives similar results
on the fine polysemy task (which has an
even harder random baseline) when using both
nouns and verbs as nodes, and does not focus
on selected targets.
A method not evaluated by (Patwardhan et
al., 2003) and using another semantic related-
ness measure ("conceptual density") is (Agirre
and Rigau, 1996). It is also based on a dis-
tance within the WordNet hierarchy. They used
a variable context size for the task and present
results only for the best size (thus being a
not fully unsupervised method). Their random
baseline is around 30%, and their precision is
43% for 80% attempted disambiguations.
Another study of disambiguation using a se-
mantic similarity derived from WordNet is (Ab-
ney and Light, 1999); it sees the task as a Hid-
den Markov Model, whose parameters are es-
timated from corpus data, so this is a mixed
model more than a purely dictionary-based
model. With a baseline of 0.285, they reach a
score of 0.423. Again, the method we used is
much simpler, for comparable or better results.
Besides, by using all connections simultane-
ously between words in the context to disam-
biguate and the rest of the lexicon, this method
avoids the combinatorial explosion of methods
purely based on a similarity measure, where ev-
ery potential sense of every meaningful word
in the context must be considered (unless ev-
ery word sense of words other than the target is
known beforehand, which is not a very realis-
tic assumption), so that only local optimization
can be achieved. In our case disambiguating
a lot of different words appearing in the same
context may result in poorer results than with
only a few words, but it will not take longer.
The only downside is heavy memory usage, as
with any dictionary-based method.
We have made the evaluation on dictionary
entries because they are already part of the net-
random first sense algorithm(n+v)
homographs 0.49 - 0.875 (14/16)
coarse polysemy 0.35 0.493 0.574 (136/237)
fine polysemy 0.18 0.40 0.346 (82/237)
Table 4: Results
work of senses, to avoid raising other issues too
early. Thus, we are not exactly in the context
of disambiguating free text. It could then be
argued that our task is simpler than standard
disambiguation, because dictionary definitions
might just be written in a more constrained and
precise language. That is why we give the score
when taking always the first sense for each en-
try, as an approximation of the most common
sense (since the dictionary does not have fre-
quency information). We can see that this score
is about 50% only for the coarse polysemy, and
40% for the fine polysemy, compared to a typ-
ical 70-80% in usual disambiguation test sets,
for similar sense dispersion (given by the ran-
dom baseline); in (Abney and Light, 1999), the
first-sense baseline gives 82%. So we could
in fact argue that disambiguating dictionary en-
tries seems harder. This fact remains however
to be confirmed with the actual most frequent
senses. Let us point out again that our al-
gorithm does not make use of the number of
senses in definitions.
Among the potential sources of improvement
for the future, or sources of errors in the past,
we can list at least the following:
? overlapping of some definitions for sub-
senses of an entry. Some entries of the
dictionary we used have sub-senses that
are very hard to distinguish. In order to
measure the impact of this, we should have
multiple annotations of the same data and
measure inter-annotator agreement, some-
thing that has not been done yet.
? part of speech tagging generates a few er-
rors when confusing adjectives and nouns
or adjectives and verbs having the same
lemma; this should be compensated when
we enrich the graph with entries for adjec-
tives.
? some time should be spent studying the
precise influence of the length of the ran-
dom walk considered; we have chosen a
value a priori to take into account the aver-
age length of a path in the graph, but once
we have more hand-tagged data we should
be able to have a better idea of the best
suited value for that parameter.
8 Conclusion
We have presented here an algorithm giving a
measure of lexical similarity, built from infor-
mation found in a dictionary. This has been
used to disambiguate dictionary entries, with a
method that needs no other source of informa-
tion (except part-of-speech tagging), no anno-
tated data. The coverage of the method depends
only on the lexical coverage of the dictionary
used. It seems to give promising results on dis-
ambiguating nouns, using only nouns or nouns
and verbs. We intend to try the method after
enriching the network of senses with adjectives
and/or adverbs. We also intend, of course, to try
the method on disambiguating verbs and adjec-
tives.
Moreover, the method can be rather straight-
forwardly extended to any type of disambigua-
tion by considering a context with a target
word as a node added in the graph of senses
(a kind of virtual definition). We have not
tested this idea yet. Since our method gives a
ranked list of sense candidates, we also con-
sider using finer performance measures, taking
into account confidence degrees, as proposed in
(Resnik and Yarowsky, 2000).
References
Steven Abney and Marc Light. 1999. Hiding
a semantic hierarchy in a markov model. In
ACL?99 Workshop Unsupervised Learning in
Natural Language Processing, University of
Maryland.
E. Agirre and G. Rigau. 1996. Word sense
disambiguation using conceptual density. In
Proceedings of COLING?96, pages 16?22,
Copenhagen (Denmark).
S. Banerjee and T. Pedersen. 2003. Extended
gloss overlaps as a measure of semantic re-
latedness. In Proceedings of the Eighteenth
International Conference on Artificial Intel-
ligence (IJCAI-03), Acapulco, Mexico.
B. Gaume, K. Duvignau, O. Gasquet, and M.-
D. Gineste. 2002. Forms of meaning, mean-
ing of forms. Journal of Experimental and
Theoretical Artificial Intelligence, 14(1):61?
74.
N. Ide and J. V?ronis. 1998. Introduction to
the special issue on word sense disambigua-
tion: The state of the art. Computational
Linguistics, 24(1).
N. Ide and J. V?ronis. 1990. Word sense dis-
ambiguation with very large neural networks
extracted from machine readable dictionar-
ies. In Proceedings of the 14th International
Conference on Computational Linguistics
(COLING?90), volume 2, pages 389?394.
H. Kozima and T. Furugori. 1993. Similarity
between words computed by spreading acti-
vation on an english dictionary. In Proceed-
ings of the conference of the European chap-
ter of the ACL, pages 232?239.
M. Lesk. 1986. Automatic sense disambigua-
tion using machine readable dictionaries:
how to tell a pine code from an ice cream
cone. In Proceedings of the 5th annual in-
ternational conference on Systems documen-
tation, pages 24?26, Toronto, Canada.
C. Manning and H. Sch?tze. 1999. Founda-
tions of Statistical Natural Language Pro-
cessing. MIT Press.
M. E. J. Newman. 2003. The structure and
function of complex networks. SIAM Re-
view, 45:167?256.
S. Patwardhan, S. Banerjee, and T. Pedersen.
2003. Using measures of semantic related-
ness for word sense disambiguation. In Pro-
ceedings of the Fourth International Confer-
ence on Intelligent Text Processing and Com-
putational Linguistics (CICLING-03).
P. Resnik and D. Yarowsky. 2000. Distinguish-
ing systems and distinguishing senses: New
evaluation methods for word sense disam-
biguation. Natural Language Engineering,
5(2):113?133.
D.J. Watts and S.H Strogatz. 1998. Collective
dynamics of ?small-world? networks. Na-
ture, (393):440?442.
Workshop on TextGraphs, at HLT-NAACL 2006, pages 65?72,
New York City, June 2006. c?2006 Association for Computational Linguistics
Synonym Extraction Using a Semantic Distance on a Dictionary
Philippe Muller
IRIT ? CNRS, UPS & INPT
Toulouse, France
muller@irit.fr
Nabil Hathout
ERSS ? CNRS & UTM
Toulouse, France
hathout@univ-tlse2.fr
Bruno Gaume
IRIT ? CNRS, UPS & INPT
Toulouse, France
gaume@irit.fr
Abstract
Synonyms extraction is a difficult task to
achieve and evaluate. Some studies have
tried to exploit general dictionaries for
that purpose, seeing them as graphs where
words are related by the definition they ap-
pear in, in a complex network of an ar-
guably semantic nature. The advantage
of using a general dictionary lies in the
coverage, and the availability of such re-
sources, in general and also in specialised
domains. We present here a method ex-
ploiting such a graph structure to compute
a distance between words. This distance
is used to isolate candidate synonyms for
a given word. We present an evaluation of
the relevance of the candidates on a sam-
ple of the lexicon.
1 Introduction
Thesaurus are an important resource in many natural
language processing tasks. They are used to help in-
formation retrieval (Zukerman et al, 2003), machine
or semi-automated translation, (Ploux and Ji, 2003;
Barzilay and McKeown, 2001; Edmonds and Hirst,
2002) or generation (Langkilde and Knight, 1998).
Since the gathering of such lexical information is a
delicate and time-consuming endeavour, some effort
has been devoted to the automatic building of sets of
synonyms words or expressions.
Synonym extraction suffers from a variety of
methodological problems, however. Synonymy it-
self is not an easily definable notion. Totally equiv-
alent words (in meaning and use) arguably do not
exist, and some people prefer to talk about near-
synonyms (Edmonds and Hirst, 2002). A near-
synonym is a word that can be used instead of
another one, in some contexts, without too much
change in meaning. This leaves of lot of freedom
in the degree of synonymy one is ready to accept.
Other authors include ?related? terms in the build-
ing of thesaurus, such as hyponyms and hypernyms,
(Blondel et al, 2004) in a somewhat arbitrary way.
More generally, paraphrase is a preferred term re-
ferring to alternative formulations of words or ex-
pressions, in the context of information retrieval or
machine translation.
Then there is the question of evaluating the results.
Comparing to already existing thesaurus is a de-
batable means when automatic construction is sup-
posed to complement an existing one, or when a spe-
cific domain is targeted, or when simply the auto-
matic procedure is supposed to fill a void. Manual
verification of a sample of synonyms extracted is a
common practice, either by the authors of a study
or by independent lexicographers. This of course
does not solve problems related to the definition of
synonymy in the ?manual? design of a thesaurus,
but can help evaluate the relevance of synonyms ex-
tracted automatically, and which could have been
forgotten. One can hope at best for a semi-automatic
procedure were lexicographers have to weed out bad
candidates in a set of proposals that is hopefully not
too noisy.
A few studies have tried to use the lexical informa-
tion available in a general dictionary and find pat-
terns that would indicate synonymy relations (Blon-
65
del et al, 2004; Ho and C?drick, 2004). The general
idea is that words are related by the definition they
appear in, in a complex network that must be seman-
tic in nature (this has been also applied to word sense
disambiguation, albeit with limited success (Veronis
and Ide, 1990; H.Kozima and Furugori, 1993)).
We present here a method exploiting the graph struc-
ture of a dictionary, where words are related by the
definition they appear in, to compute a distance be-
tween words. This distance is used to isolate can-
didate synonyms for a given word. We present an
evaluation of the relevance of the candidates on a
sample of the lexicon.
2 Semantic distance on a dictionary graph
We describe here our method (dubbed Prox) to com-
pute a distance between nodes in a graph. Basi-
cally, nodes are derived from entries in the dictio-
nary or words appearing in definitions, and there are
edges between an entry and the word in its definition
(more in section 3). Such graphs are "small world"
networks with distinguishing features and we hypo-
thetize these features reflect a linguistic and seman-
tic organisation that can be exploited (Gaume et al,
2005).
The idea is to see a graph as a Markov chain whose
states are the graph nodes and whose transitions are
its edges, valuated with probabilities. Then we send
random particles walking through this graph, and
their trajectories and the dynamics of their trajec-
tories reveal their structural properties. In short, we
assume the average distance a particle has made be-
tween two nodes after a given time is an indication
of the semantic distance between these nodes. Ob-
viously, nodes located in highly clustered areas will
tend to be separated by smaller distance.
Formally, if G = (V,E) is a reflexive graph (each
node is connected to itself) with |V | = n, we note
[G] the n ? n adjacency matrix of G that is such
that [G]i,j (the ith row and jth column) is non null
if there is an edge between node i and node j and
0 otherwise. We can have different weights for
the edge between nodes (cf. next section), but the
method will be similar.
The first step is to turn the matrix into a Markovian
matrix. We note [G?] the Markovian matrix of G,
such that
[G?]r,s =
[G]r,s
?
x?V ([G]r,x)
The sum of each line of G is different from 0 since
the graph is reflexive.
We note [G?]i the matrix [G?] multiplied i times by it-
self.
Let now PROX(G, i, r, s) be [G?]ir,s. This is thus
the probability that a random particle leaving node r
will be in node s after i time steps. This is the mea-
sure we will use to determine if a node s is closer
to a node r than another node t. The choice for i
will depend on the graph and is explained later (cf.
section 4).
3 Synonym extraction
We used for the experiment the XML tagged MRD
Tr?sor de la Langue Fran?aise informatis? (TLFi)
from ATILF (http://atilf.atilf.fr/), a
large French dictionary with 54,280 articles, 92,997
entries and 271,166 definitions. The extraction of
synonyms has been carried out only for nouns, verbs
and adjectives. The basic assumption is that words
with semantically close definitions are likely to be
synonyms. We then designed a oriented graph
that brings closer definitions that contain the same
words, especially when these words occur in the be-
ginning. We selected the noun, verb and adjective
definitions from the dictionary and created a record
for each of them with the information relevant to
the building of the graph: the word or expression
being defined (henceforth, definiendum); its gram-
matical category; the hierarchical position of the de-
fined (sub-)sense in the article; the definition proper
(henceforth definiens).
Definitions are made of 2 members: a definiendum
and a definiens and we strongly distinguish these 2
types of objects in the graph. They are represented
by 2 types of nodes: a-type nodes for the words be-
ing defined and for their sub-senses; o-type nodes
for the words that occur in definiens.
For instance, the noun nostalgie ?nostalgia? has 6 de-
fined sub-senses numbered A.1, A.2, B., C., C. ? and
D.:
66
NOSTALGIE, subst. f?m.
A. 1. ?tat de tristesse [...]
2. Trouble psychique [...]
B. Regret m?lancolique [...] d?sir d?un retour dans
le pass?.
C. Regret m?lancolique [...] d?sir insatisfait.
? Sentiment d?impuissance [...]
D. ?tat de m?lancolie [...]
The 6 sub-senses yield 6 a-nodes in the graph plus
one for the article entry:
a.S.nostalgie article entry
a.S.nostalgie.1_1 sub-sense A. 1.
a.S.nostalgie.1_2 sub-sense A. 2.
a.S.nostalgie.2 sub-sense B.
a.S.nostalgie.3 sub-sense C.
a.S.nostalgie.3_1 sub-sense C. ?
a.S.nostalgie.4 sub-sense D.
A-node tags have 4 fields: the node type (namely a);
its grammatical category (S for nouns, V for verbs
and A for adjectives); the lemma that correponds to
the definiendum; a representation of the hierarchi-
cal position of the sub-sense in the dictionary arti-
cle. For instance, the A. 2. sub-sense of nostalgie
corresponds to the hierarchical position 1_2.
O-nodes represent the types that occur in definiens.1
A second example can be used to present them. The
adjective jonceux ?rushy? has two sub-senses ?re-
sembling rushes? and ?populated with rushes?:
Jonceux, -euse,
a) Qui ressemble au jonc.
b) Peupl? de joncs.
Actually, TLFi definitions are POS-tagged and lem-
matized:
Jonceux/S
a) qui/Pro ressembler/V au/D jonc/S ./X
b) peupl?/A de/Prep jonc/S ./X 2
The 2 definiens yield the following o-type nodes in
the graph:
o.Pro.qui; o.V.ressembler; o.D.au;
o.S.jonc; o.X..; o.A.peupl?; o.Prep.de
1The tokens are represented by edges.
2In this sentence, peupl? is an adjective and not a verb.
All the types that occur in definiens are represented,
including the function words (pronouns, deter-
miners...) and the punctuation. Function words
play an important role in the graph because they
bring closer the words that belong to the same
semantical referential classes (e.g. the adjectives
of resemblance), that is words that are likely to
be synonyms. Their role is also reinforced by the
manner edges are weighted.
A large number of TLFi definitions concerns
phrases and locutions. However, these definitions
have been removed from the graph because:
? their tokens are not identified in the definiens;
? their grammatical categories are not given in
the articles and are difficult to calculate;
? many lexicalized phrases are not sub-senses of
the article entry.
O-node tags have 3 fields: the node type (namely o);
the grammatical category of the word; its lemma.
The oriented graph built for the experiment then
contains one a-node for each entry and each entry
sub-sense (i.e. each definiendum) and one o-node
for each type that occurs in a definition (i.e. in a
definiens). These nodes are connected as follows:
1. The graph is reflexive;
2. Sub-senses are connected to the words of their
definiens and vice versa (e.g. there is an edge
between a.A.jonceux.1 and o.Pro.qui,
and another one between o.Pro.qui and
a.A.jonceux.1).
3. Each a-node is connected to the a-nodes
of the immediately lower hierarchical
level but there is no edge between an
a-node and the a-nodes of higher hier-
archical levels (e.g. a.S.nostalgie
is connected to a.S.nostalgie.1_1,
a.S.nostalgie.1_2,
a.S.nostalgie.2, a.S.nostalgie.3
and a.S.nostalgie.4, but none of the
sub-senses is connected to the entry).
67
4. Each o-node is connected to the a-node that
represents its entry, but there is no edge be-
tween the a-node representing an entry and the
corresponding o-node (e.g. there is an edge be-
tween o.A.jonceux and a.A.jonceux,
but none between a.A.jonceux and
o.A.jonceux).
All edge weights are 1 with the exception of
the edges representing the 9 first words of each
definiens. For these words, the edge weight takes
into account their position in the definiens. The
weight of the edge that represent the first token is
10; it is 9 for the second word; and so on down to
1.3
These characteristics are illustrated by the fragment
of the graph representing the entry jonceux in table
1.
4 Experiment and results
Once the graph built, we used Prox to compute a se-
mantic similarity between the nodes. We first turned
the matrix G that represent the graph into a Marko-
vian matrix [G?] as described in section 2 and then
computed [G?]5, that correspond to 5-steps paths in
the Markovian graph.4 For a given word, we have
extracted as candidate synonyms the a-nodes (i) of
the same category as the word (ii) that are the clos-
est to the o-node representing that word in the dictio-
nary definitions. Moreover, only the first a-node of
each entry is considered. For instance, the candidate
synonyms of the verb accumuler ?accumulate? are
the a-nodes representing verbs (i.e. their tags begin
in a.V) that are the closer to the o.V.accumuler
node.
5-steps paths starting from an o-node representing a
word w reach six groups of a-nodes:
A1 the a-nodes of the sub-senses which have w in
their definition;
3Lexicographic definitions usually have two parts: a genus
and a differentia. This edge weight is intended to favour the
genus part of the definiens.
4The path length has been determined empirically.
A2 the a-nodes of the sub-senses with definiens
containing the same words as those of A1;
A3 the a-nodes of the sub-senses with definiens
containing the same words as those of A2;
B1 the a-nodes of the sub-senses of the article of w.
(These dummy candidates are not kept.)
B2 the a-nodes of the sub-senses with definiens
containing the same words as those of B1;
B3 the a-nodes of the sub-senses with definiens
containing the same words as those of B2;
The three first groups take advantage of the fact
that synonyms of the definiendum are often used in
definiens.
The question of the evaluation of the extraction of
synonyms is a difficult one, as was already men-
tioned in the introduction. We have at our disposal
several thesauri for French, with various coverages
(from about 2000 pairs of synonyms, to 140,000),
and a lot of discrepancies.5 If we compare the the-
saurus with each other and restrict the comparison
to their common lexicon for fairness, we still have
a lot of differences. The best f-score is never above
60%, and it raises the question of the proper gold
standard to begin with. This is all the more distress-
ing as the dictionary we used has a larger lexicon
than all the thesaurus considered together (roughly
twice as much). As our main purpose is to build a set
of synonyms from the TLF to go beyond the avail-
able thesaurus, we have no other way but to have
lexicographers look at the result and judge the qual-
ity of candidate synonyms. Before imposing this
workload on our lexicographer colleagues, we took
a sample of 50 verbs and 50 nouns, and evaluated
the first ten candidates for each, using the ranking
method presented above, and a simpler version with
equal weights and no distinction between sense lev-
els or node types. The basic version of the graph
also excludes nodes with too many neighbours, such
as "?tre" (be), "avoir" (have), "chose" (thing), etc. ).
Two of the authors separately evaluated the candi-
dates, with the synonyms from the existing thesauri
5These seven classical dictionaries of synonyms are all
available from http://www.crisco.unicaen.fr/dicosyn.html.
68
o
.A
.jo
nc
eu
x
a.
A
.jo
nc
eu
x
a.
A
.jo
nc
eu
x.1
a.
A
.jo
nc
eu
x.2
o
.P
ro
.q
ui
o
.V
.
re
ss
em
bl
er
o
.D
.a
u
o
.S
.jo
nc
o
.X
..
o
.A
.p
eu
pl
?
o
.P
re
p.
de
o.A.jonceux 1 1
a.A.jonceux 1 1 1
a.A.jonceux.1 1 1 1 1 1 1
a.A.jonceux.2 1 1 1 1 1
o.Pro.qui 10 1
o.V.ressembler 9 1
o.D.au 8 1
o.S.jonc 7 8 1
o.X.. 6 7 1
o.A.peupl? 10 1
o.Prep.de 9 1
Table 1: A fragment of the graph, presented as a matrix.
already marked. It turned out one of the judge was
much more liberal than the other about synonymy,
but most synonyms accepted by the first were ac-
cepted by the second judge (precision of 0.85).6
We also considered a few baselines inspired by the
method. Obviously a lot of synonyms appear in the
definition of a word, and words in a definition tend
to be consider close to the entry they appear in. So
we tried two different baselines to estimate this bias,
and how our method improves or not from this.
The first baseline considers as synonyms of a word
all the words of the same category (verbs or nouns
in each case) that appear in a definition of the word,
and all the entry the word appear in. Then we se-
lected ten words at random among this base.
The second baseline was similar, but restricted to the
first word appearing in a definition of another word.
Again we took ten words at random in this set if it
was larger than ten, and all of them otherwise.
We show the results of precision for the first can-
didate ranked by prox, the first 5, and the first 10
(always excluding the word itself). In the case of
the two baselines, results for the first ten are a bit
6The kappa score between the two annotators was 0.5 for
both verbs and nouns, which only moderately satisfactory.
misleading, since the average numbers of candidates
proposed by these methods were respectively 8 and
6 for verbs and 9 and 5.6 for nouns (Table 2). Also,
nouns had an average of 5.8 synonyms in the exist-
ing thesauri (when what was considered was the min
between 10 and the number of synonyms), and verbs
had an average of 8.9.
We can see that both baselines outperforms
weighted prox on the existing thesaurus for verbs,
and that the simpler prox is similar to baseline 2 (first
word only). For nouns, results are close between B2
and the two proxs. It is to be noted that a lot of
uncommon words appear as candidates, as they are
related with very few words, and a lot of these do
not appear in the existing thesauri.
By looking precisely at each candidate (see judges?
scores), we can see that both baselines are slightly
improved (and still close to one another), but are
now beaten by both prox for the first and the first
5 words. There is a big difference between the two
judges, so Judge 2 has better scores than Judge 1 for
the baselines, but in each case, prox was better. It
could be troubling to see how good the second base-
line is for the first 10 candidates, but one must re-
member this baseline actually proposes 6 candidates
on average (when prox was always at 10), making
it actually nothing more than a variation on the 5
69
Existing Thesauri (V) Judge 1 Judge 2 ET (N) J1 J2
baseline-1 1 0.30 0.42 0.38 0.06 0.12 0.12
5 0.29 0.39 0.375 0.08 0.12 0.13
10 0.31 0.41 0.39 0.10 0.14 0.15
baseline-2 1 0.32 0.52 0.44 0.21 0.22 0.23
5 0.36 0.50 0.446 0.21 0.24 0.25
10 0.28 0.51 0.46 0.19 0.245 0.255
simple prox 1 0.35 0.67 NA 0.27 0.415 0.417
5 0.34 0.52 NA 0.137 0.215 0.237
10 0.247 0.375 NA 0.123 0.17 0.19
weighted prox 1 0.22 0.56 0.76 0.18 0.44 0.5
5 0.196 0.44 0.58 0.148 0.31 0.39
10 0.17 0.36 0.47 0.10 0.22 0.3
Table 2: Experimental results on a sample, V=verbs, N=nouns,
candidate baseline, to which it should be compared
in all fairness (and we see that prox is much better
there). The difference between the two versions of
prox shows that a basic version is better for verbs
and the more elaborate one is better for nouns, with
overall better results for verbs than for nouns.
One could wonder why there was some many more
candidates marked as synonyms by both judges,
compared to the original compilation of thesaurus.
Mainly, it seemed to us that it can be accounted for
by a lot of infrequent words, or old senses of words
absent for more restricted dictionaries. We are cur-
rently investigating this matter. It could also be that
our sample picked out a lot of not so frequent words
since they outnumber frequent words in such a large
dictionary as the TLF. An indication is the average
frequency of words in a corpus of ten years of the
journal "Le Monde". The 50 words picked out in
our sample have an average frequency of 2000 oc-
currences, while when we consider all our about 430
candidates for synonymy, the average frequency is
5300.
The main conclusion to draw here is that our method
is able to recover a lot of synonyms that are in the
definition of words, and some in definitions not di-
rectly related, which seems to be an improvement on
previous attempts from dictionaries. There is some
arbitrariness in the method that should be further
investigated (the length of the random walk for in-
stance), but we believe the parameters are rather in-
tuitive wrt to graph concepts. We also have an as-
sessment of the quality of the method, even though
it is still on a sample. The precision seems fair on
the first ten candidates, enough to be used in a semi-
automatic way, coupled with a lexicographic analy-
sis.
5 Related work
Among the methods proposed to collect synonymy
information, two families can be distinguished ac-
cording to the input they consider. Either a gen-
eral dictionary is used (or more than one (Wu and
Zhou, 2003)), or a corpus of unconstrained texts
from which lexical distributions are computed (sim-
ple collocations or syntactic dependencies) (Lin,
1998; Freitag et al, 2005) . The approach of (Barzi-
lay and McKeown, 2001) uses a related kind of re-
source: multiple translations of the same text, with
additional constraints on availability, and problems
of text alignment, for only a third of the results be-
ing synonyms (when compared to Wordnet).
A measure of similarity is almost always used to
rank possible candidates. In the case of distribu-
tional approaches, similarity if determined from the
appearance in similar contexts (Lin, 1998); in the
case of dictionary-based methods, lexical relations
are deduced from the links between words expressed
in definitions of entries.
Approaches that rely on distributional data have two
major drawbacks: they need a lot of data, gener-
ally syntactically parsed sentences, that is not al-
ways available for a given language (English is an
exception), and they do not discriminate well among
lexical relations (mainly hyponyms, antonyms, hy-
pernyms) (Weeds et al, 2004) . Dictionary-based
70
approaches address the first problem since dictionar-
ies are readily available for a lot of language, even
electronically, and this is the raison d??tre of our ef-
fort. As we have seen here, it is not an obvious task
to sort related terms with respect to synonymy, hy-
pernymy, etc, just as with distribution approaches.
A lot of work has been done to extract lexical rela-
tions from the definitions taken in isolation (mostly
for ontology building), see recently (Nichols et al,
2005), with a syntactic/semantic parse, with usually
results around 60% of precision (that can be com-
pared with the same baseline we used, all words in
the definition with the same category), on dictionar-
ies with very small definitions (and thus a higher
proportions of synonyms and hypernyms). Estimat-
ing the recall of such methods have not been done.
Using dictionaries as network of lexical items or
senses has been quite popular for word sense dis-
ambiguation (Veronis and Ide, 1990; H.Kozima and
Furugori, 1993; Niwa and Nitta, 1994) before los-
ing ground to statistical approaches, even though
(Gaume et al, 2004; Mihalcea et al, 2004) tried a re-
vival of such methods. Both (Ho and C?drick, 2004)
and (Blondel et al, 2004) build a graph of lexical
items from a dictionary in a manner similar to ours.
In the first case, the method used to compute similar-
ity between two concepts (or words) is restricted to
neighbors, in the graph, of the two concepts; in the
second case, only directly related words are consid-
ered as potential candidates for synonymy: for two
words to be considered synonyms, one has to appear
in the definition of another. In both cases, only 6
or 7 words have been used as a test of synonymy,
with a validation provided by the authors with "re-
lated terms" (an unclear notion) considered correct.
The similarity measure itself was evaluated on a set
of related terms from (Miller and Charles, 1991), as
in (Budanitsky and Hirst, 2001; Banerjee and Ped-
ersen, 2003), with seemingly good results, but se-
mantically related terms is a very different notion
("car" and "tire" for instance are semantically related
terms, and thus considered similar).
We do not know of any dictionary-based graph ap-
proach which have been given a larger evaluation of
its results. Parsing definitions in isolation prevents a
complete coverage (we estimated that only 30% of
synonyms pairs in the TLF can be found from defi-
nitions).
As for distributional approaches, (Barzilay and
McKeown, 2001) gets a very high precision (around
90%) on valid paraphrases as judged by humans,
among which 35% are synonymy relations in Word-
net, 32% are hypernyms, 18% are coordinate terms.
Discriminating among the paraphrases types is not
addressed. Other approaches usually consider either
given sets of synonyms among which one is to be
chosen (for a translation for instance) (Edmonds and
Hirst, 2002) or must choose a synonym word against
unrelated terms in the context of a synonymy test
(Freitag et al, 2005), a seemingly easier task than
actually proposing synonyms. (Lin, 1998) proposes
a different methodology for evaluation of candidate
synonyms, by comparing similarity measures of the
terms he provides with the similarity measures be-
tween them in Wordnet, using various semantic dis-
tances. This makes for very complex evaluation pro-
cedures without an intuitive interpretation, and there
is no assessment of the quality of the automated the-
saurus.
6 Conclusion
We have developed a general method to extract near-
synonyms from a dictionary, improving on the two
baselines. There is some arbitrariness in the param-
eters we used, but we believe the parameters are
rather intuitive wrt to graph concepts.7 There is
room for improvement obviously, also for a combi-
nation with other methods to filter synonyms (with
frequency estimates for instance, such as tf.idf or
mutual information measures).
Clearly the advantage of using a dictionary is re-
tained: there is no restriction of coverage, and we
could have used a specialised dictionary to build a
specialised thesaurus. We have provided an assess-
ment of the quality of the results, although there
is not much to compare it to (to the best of our
knowledge), since previous accounts only had cur-
sory evaluation.
7The lexical graph can be explored at http://prox.irit.fr.
71
Acknowledgments
This research has been supported by the CNRS pro-
gram TCAN 04N85/0025. We sincerely thank the
ATILF laboratory and Pr. Jean-Marie Pierrel for
the opportunity they gave us to use the Tr?sor de la
Langue Fran?aise informatis?. We would also like
to thank Jean-Marc Destabeaux for his crucial help
in extracting the definitions from the TLFi.
References
S. Banerjee and T. Pedersen. 2003. Extended gloss over-
laps as a measure of semantic relatedness. In Proceed-
ings of IJCAI-03, Acapulco, Mexico.
Regina Barzilay and Kathleen R. McKeown. 2001. Ex-
tracting paraphrases from a parallel corpus. In Proceed-
ings of the 39th ACL, pages 00?00, Toulouse.
Vincent D. Blondel, Anah? Gajardo, Maureen Heymans,
Pierre Senellart, and Paul Van Dooren. 2004. A mea-
sure of similarity between graph vertices: Applications
to synonym extraction and web searching. SIAM Review,
46(4):647?666.
A. Budanitsky and G. Hirst. 2001. Semantic distance
in wordnet: An experimental, application-oriented eval-
uation of five measures. In Workshop on WordNet and
Other Lexical Resources, NAACL 2001, Pittsburgh.
Philip Edmonds and Graeme Hirst. 2002. Near-
Synonymy and lexical choice. Computational Linguis-
tics, 28(2):105?144.
Dayne Freitag, Matthias Blume, John Byrnes, Edmond
Chow, Sadik Kapadia, Richard Rohwer, and Zhiqiang
Wang. 2005. New experiments in distributional repre-
sentations of synonymy. In Proceedings of CoNLL, pages
25?32, Ann Arbor, Michigan, June. Association for Com-
putational Linguistics.
B. Gaume, N. Hathout, and P. Muller. 2004. Word
sense disambiguation using a dictionary for sense similar-
ity measure. In Proceedings of Coling 2004, volume II,
pages 1194?1200, Gen?ve.
B. Gaume, F. Venant, and B. Victorri. 2005. Hierarchy
in lexical organization of natural language. In D. Pumain,
editor, Hierarchy in natural and social sciences, Metho-
dos series, pages 121?143. Kluwer.
H.Kozima and T. Furugori. 1993. Similarity between
words computed by spreading activation on an english
dictionary. In Proceedings of the EACL, pages 232?239.
Ngoc-Diep Ho and Fairon C?drick. 2004. Lexical simi-
larity based on quantity of information exchanged - syn-
onym extraction. In Proc. of Intl. Conf. RIVF?04, Hanoi.
Irene Langkilde and Kevin Knight. 1998. Generation
that exploits corpus-based statistical knowledge. In Pro-
ceedings of COLING-ACL ?98, volume 1, pages 704?
710, Montreal.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of COLING-ACL ?98,
volume 2, pages 768?774, Montreal.
Rada Mihalcea, Paul Tarau, and Elizabeth Figa. 2004.
PageRank on semantic networks, with application to
word sense disambiguation. In Proceedings of Coling
2004, Geneva.
GA Miller and WG Charles. 1991. Contextual corre-
lates of semantic similarity. Language and Cognitive
Processes, 6(1):1?28.
Eric Nichols, Francis Bond, and Daniel Flickinger. 2005.
Robust ontology acquisition from machine-readable dic-
tionaries. In Proceedings of IJCAI?05.
Yoshiki Niwa and Yoshihiko Nitta. 1994. Co-occurrence
vectors from corpora vs. distance vectors from dictionar-
ies. In Proceedings of Coling 1994.
Sabine Ploux and Hyungsuk Ji. 2003. A
model for matching semantic maps between languages
(French/English, English/French). Computational Lin-
guistics, 29(2):155?178.
J. Veronis and N.M. Ide. 1990. Word sense disambigua-
tion with very large neural networks extracted from ma-
chine readable dictionaries. In COLING-90: Proceed-
ings of the 13th International Conference on Computa-
tional Linguistics, volume 2, pages 389?394, Helsinki,
Finland.
Julie Weeds, David Weir, and Diana McCarthy. 2004.
Characterising measures of lexical distributional similar-
ity. In Proceedings of Coling 2004, pages 1015?1021,
Geneva, Switzerland, Aug 23?Aug 27. COLING.
Hua Wu and Ming Zhou. 2003. Optimizing synonyms
extraction with mono and bilingual resources. In Pro-
ceedings of the Second International Workshop on Para-
phrasing, Sapporo, Japan. Association for Computational
Linguistics.
Ingrid Zukerman, Sarah George, and Yingying Wen.
2003. Lexical paraphrasing for document retrieval and
node identification. In Proceedings of the Second Inter-
national Workshop on Paraphrasing, Sapporo, Japan. As-
sociation for Computational Linguistics.
72
Annotating and measuring temporal relations in texts
Philippe Muller Xavier Tannier
IRIT, Universit? Paul Sabatier IRIT, Universit? Paul Sabatier
Toulouse, France Toulouse, France
muller@irit.fr tannier@emse.fr
Abstract
This paper focuses on the automated processing
of temporal information in written texts, more
specifically on relations between events intro-
duced by verbs in finite clauses. While this
problem has been largely studied from a the-
oretical point of view, it has very rarely been
applied to real texts, if ever, with quantified re-
sults. The methodology required is still to be
defined, even though there have been proposals
in the strictly human annotation case. We pro-
pose here both a procedure to achieve this task
and a way of measuring the results. We have
been testing the feasibility of this on newswire
articles, with promising results.
1 Annotating temporal information
This paper focuses on the automated annotation of
temporal information in texts, more specifically re-
lations between events introduced by finite verbs.
While the semantics of temporal markers and the
temporal structure of discourse are well-developed
subjects in formal linguistics (Steedman, 1997),
investigation of quantifiable annotation of unre-
stricted texts is a somewhat recent topic. The is-
sue has started to generate some interest in com-
putational linguistics (Harper et al, 2001), as it is
potentially an important component in information
extraction or question-answer systems. A few tasks
can be distinguished in that respect:
? detecting dates and temporal markers
? detecting event descriptions
? finding the date of events described
? figuring out the temporal relations between
events in a text
The first task is not too difficult when looking for
dates, e.g. using regular expressions (Wilson et
al., 2001), but requires some syntactic analysis in a
larger framework (Vazov, 2001; Shilder and Habel,
2001). The second one raises more difficult, onto-
logical questions; what counts as an event is not un-
controversial (Setzer, 2001): attitude reports, such
as beliefs, or reported speech have an unclear status
in that respect. The third task adds another level of
complexity: a lot of events described in a text do
not have an explicit temporal stamp, and it is not
always possible to determine one, even when tak-
ing context into account (Filatova and Hovy, 2001).
This leads to an approach more suited to the level of
underspecification found in texts: annotating rela-
tions between events in a symbolic way (e.g. that an
event e1 is before another one e2). This is the path
chosen by (Katz and Arosio, 2001; Setzer, 2001)
with human annotators. This, in turn, raises new
problems. First, what are the relations best suited to
that task, among the many propositions (linguistic
or logical) one can find for expressing temporal lo-
cation ? Then, how can an annotation be evaluated,
between annotators, or between a human annotator
and an automated system ? Such annotations can-
not be easy to determine automatically anyway, and
must use some level of discourse modeling (cf. the
work of (Grover et al, 1995)).
We want to show here the feasibility of such an
effort, and we propose a way of evaluating the suc-
cess or failure of the task. The next section will
precise why evaluating this particular task is not a
trivial question. Section 3 will explain the method
used to extract temporal relations, using also a form
of symbolic inference on available temporal infor-
mation (section 4). Then section 5 discusses how
we propose to evaluate the success of the task, be-
fore presenting our results (section 6).
2 Evaluating annotations
What we want to annotate is something close to the
temporal model built by a human reader of a text; as
such, it may involve some form of reasoning, based
on various cues (lexical or discursive), and may be
expressed in several ways. As was noticed by (Set-
zer, 2001), it is difficult to reach a good agreement
between human annotators, as they can express re-
lations between events in different, yet equivalent,
ways. For instance, they can say that an event e1
happens during another one e2, and that e2 happens
before e3, leaving implicit that e1 too is before e3,
while another might list explicitly all relations. One
option could be to ask for a relation between all
pairs of events in a given text, but this would be
demanding a lot from human subjects, since they
would be asked for n? (n? 1)/2 judgments, most
of which would be hard to make explicit. Another
option, followed by (Setzer, 2001) (and in a very
simplified way, by (Katz and Arosio, 2001)) is to
use a few rules of inference (similar to the exam-
ple seen in the previous paragraph), and to compare
the closures (with respect to these rules) of the hu-
man annotations. Such rules are of the form "if r1
holds between x and y, and r2 holds between y and
z, then r3 holds between x and z". Then one can
measure the agreement between annotations with
classical precision and recall on the set of triplets
(event x,event y,relation). This is certainly an im-
provement, but (Setzer, 2001) points out that hu-
mans still forget available information, so that it is
necessary to help them spell out completely the in-
formation they should have annotated. Setzer esti-
mates that an hour is needed on average for a text
with a number of 15 to 40 events.
Actually, this method has two shortcomings.
First, the choice of temporal relations proposed to
annotators, i.e. "before", "after", "during", and "si-
multaneously". The latter is all the more difficult
to judge as it lacks a precise semantics, and is de-
fined as "roughly at the same time" ((Setzer, 2001),
p.81). The second problem is related to the infer-
ential model considered, as it is only partial. Even
though the exact mental processing of such infor-
mation is still beyond reach, and thus any claim to
cognitive plausibility is questionable, there are more
precise frameworks for reasoning about temporal
information. For instance the well-studied Allen?s
relations algebra (see Figure 2). Here, relations be-
tween two time intervals are derived from all the
possibilities for the respective position of those in-
tervals endpoints (before, after or same), yielding
13 relations. What this framework can also express
are more general relations between events, such as
disjunctive relations (relation between event 1 and
event 2 is relation A or relation B), and reasoning
on such knowledge. We think it is important at
least to relate annotation relations to a clear tem-
poral model, even if this model is not directly used.
Besides, we believe that measuring agreement on
the basis of a more complete "event calculus" will
be more precise, if we accept to infer disjunctive re-
lation. Then we want to give a better score to the
annotation "A or B" when A is true, than to an an-
notation where nothing is said. Section 5 gives more
details about this problem.
We will now present our method to achieve the
task of annotating automatically event relations.
This has been tested on a small set of French
newswire texts from the Agence France Press.
3 A method for annotating temporal
relations
We will now present our method to achieve the task
of annotating automatically event relations. This
has been tested on a small set of French newswire
texts from the Agence France Press. The starting
point was raw text plus its broadcast date. We then
applied the following steps:
? part of speech tagging with Treetagger
(Schmid, 1994), with some post-processing to
locate some lexicalised prepositional phrases;
? partial parsing with a cascade of regular ex-
pressions analyzers (cf. (Abney, 1996); we
also used Abney?s Cass software to apply the
rules)1. This was done to extract dates, tem-
poral adjuncts, various temporal markers, and
to achieve a somewhat coarse clause-splitting
(one finite verb in each clause) and to attach
temporal adjuncts to the appropriate clause
(this is of course a potentially large source of
errors). Relative clauses are extracted and put
at the end of their sentence of origin, in a way
similar to (Filatova and Hovy, 2001). Table
1 gives an idea of the kind of temporal infor-
mation defined and extracted at this step and
for which potentially different temporal inter-
pretations are given (for now, temporal focus
is always the previously detected event; this is
obviously an over-simplification).
? date computation to precise temporal locations
of events associated with explicit, yet impre-
cise, temporal information, such as dates rela-
tive to the time of the text (e.g. last Monday).
? for each event associated to a temporal adjunct,
a temporal relation is established (with a date
when possible).
? a set of discourse rules is used to establish
possible relations between two events appear-
ing consecutively in the text, according to
the tenses of the verbs introducing the events.
These rules for French are similar to rules for
English proposed in (Grover et al, 1995; Song
and Cohen, 1991; Kameyama et al, 1993), but
1We have defined 89 rules, divided in 29 levels.
are expressed with Allen relations instead of a
set of ad hoc relations (see Table 1 for a sub-
set of the rules). These rules are only applied
when no temporal marker indicates a specific
relation between the two events.
? the last step consists in computing a fixed point
on the graph of relations between events recog-
nized in the text, and dates. We used a classi-
cal path-consistency algorithm (Allen, 1984).
More explanation is given section 4.
Allen relations are illustrated Figure 2. In the fol-
lowing (and Table 1) they will be abbreviated with
their first letters, adding an "i" for their inverse re-
lations. So, for instance, "before" is "b" and "after"
is "bi" (b(x,y)? bi(y,x)). Table 1 gives the disjunc-
tion of possible relations between an event e1 with
tense X and a event e2 with tense Y following e1 in
the text. This is considered as a first very simplified
discourse model. It only tries to list plausible rela-
tions between two consecutive events, when there is
no marker than could explicit that relation. For in-
stance a simple past e1 can be related with e, b, m,
s, d, f, o to a following simple past event e2 in such
a context (roughly saying that e1 is before or dur-
ing e2 or meets or overlaps it). This crude model is
only intended as a basis, which will be refined once
we have a larger set of annotated texts. This will be
enriched later with a notion of temporal focus, fol-
lowing for instance (Kameyama et al, 1993; Song
and Cohen, 1991), and a notion of temporal per-
spective necessary to capture more complex tense
interactions.
The path consistency algorithm is detailed in the
next section.
4 Inferring relations between events
X
Y
X
X
X
Y
Y
Yfinishes
before
meets
overlaps
X
X
Y
Y
equals
during
starts
X
Y
Figure 2: Allen Relations between two intervals X
and Y (Time flies from left to right)
We have argued in favor of the use of Allen rela-
tions for defining annotating temporal relations, not
only because they have a clear semantics, but also
because a lot of work has been done on inference
procedures over constraints expressed with these re-
lations. We therefore believe that a good way of
avoiding the pitfalls of choosing relations for hu-
man annotation and of defining inference patterns
for these relations is to define them from Allen rela-
tions and use relational algebra computation to infer
all possible relations between events of a text (that is
saturate the constraint graph, see below), both from
a human annotation and an annotation given by a
system, and then to compare the two. In this per-
spective, any event is considered to correspond to a
convex time interval.
The set of all relations between pairs of events is
then seen as a graph of constraints, which can be
completed with inference rules. The saturation of
the graph of relations is not done with a few hand-
crafted rules of the form (relation between e1 and
e2) + (relation between e2 and e3) gives (a simple
relation between e1 and e3) (Setzer, 2001; Katz and
Arosio, 2001) but with the use of the full algebra of
Allen relation. This will reach a more complete de-
scription of temporal information, and also gives a
way to detect inconsistencies in an annotation.
An algebra of relation can be defined on any set of
relations that are mutually exclusive (two relations
cannot hold at the same time between two entities)
and exhaustive (at least one relation must hold be-
tween two given entities). The algebra starts from a
set of base relations U= {r1, r2, ...}, and a general
relation is a subset of U, interpreted as a disjunction
of the relations it contains. From there we can de-
fine union and intersection of relations as classical
set union and intersection of the base relations they
consist of. Moreover, one can define a composition
of relations as follows:
(r1 ? r2)(x, z) ? ?y r1(x, y) ? r2(y, z)
By computing beforehand the 13?13 compositions
of base relations of U, we can compute the composi-
tion of any two general relations (because r?r ? =?
when r, r? are basic and r6= r?):
{r1, r2, ...rk} ? {s1, s2, ...sm} =
?
i,j
(ri ? sj)
Saturating the graph of temporal constraints means
applying these rules to all compatible pairs of
constraints in the graph and iterating until a fix-
point is reached. The following, so-called "path-
consistency" algorithm (Allen, 1984) ensures this
fixpoint is reached:
date(1/2) : non absolute date ("march 25th", "in June").
dateabs : absolute date "July 14th, 1789".
daterelST : date, relative to utterance time ("two years
ago").
daterelTF : date, relative to temporal focus ("3 days later").
datespecabs : absolute date, with imprecise reference ("in
the beginning of the 80s").
datespecrel : relative date, special forms (months, seasons).
dur : basic duration ("during 3 years").
dur2 : duration with two dates (from February, 11 to Octo-
ber, 27. . . ).
durabs : absolute duration ("starting July 14").
durrelST : relative duration, w.r.t utterance time ("for a
year").
durrelTF : relative duration, w.r.t temporal focus ("since").
tatom : temporal atom (three days, four years, . . . ).
Figure 1: Temporal elements extracted by shallow parsing (with examples translated from French)
e1/e2 imp pp pres sp
imp o, e, s, d, f, si, di, fi bi, mi, oi e, b o, d, s, f, e, si, di, fi
pp b, m, o, e, s, d, f b, m, o, e, s, d, f, bi, mi e, b b, m, o
pres U U b, m, o, si, di, fi, e U
sp b, m, o, e, s, d, f e, s, d, f, bi, mi e, b e, b, m, s, d, f, o
Table 1: Some Discursive temporal constraints for the main relevant tenses, sp=simple past and perfect,
imp=French imparfait, pp=past perfect, pres=present
Let
{
A = the set of all edges of the graph
N = the set of vertices of the graph
U = the disjunction of all 13 Allen relations
Rm,n = the current relation between
nodes m and n
1. changed = 0
2. for all pair of nodes (i, j) ? N ?N and for all
k ? N such that ((i, k) ? A ? (k, j) ? A)
(a) R1i,j = (Ri,k ? Rk,j)
(b) if no edge (a relation R2i,j) existed before
between i and j, then R2i,j = U
(c) intersect: Ri,j = R1i,j ? R2i,j
(d) if Ri,j = ? (inconsistency detected)
then : error
(e) if Ri,j = U (=no information) do nothing
else update edge
changed = 1
3. if changed = 1, then go back to 1.
It is to be noted that this algorithm is correct: if
it detects an inconsistency then there is really one,
but it is incomplete in general (it does not neces-
sarily detect an inconsistent situation). There are
sub-algebras for which it is also complete, but it re-
mains to be seen if any of them can be enough for
our purpose here.
5 Measuring success
In order to validate our method, we have compared
the results given by the system with a "manual" an-
notation. It is not really realistic to ask humans
(whether they are experts or not) for Allen relations
between events. They are too numerous and some
are too precise to be useful alone, and it is prob-
ably dangerous to ask for disjunctive information.
But we still want to have annotation relations with a
clear semantics, that we could link to Allen?s alge-
bra to infer and compare information about tempo-
ral situations. So we have chosen relations similar
to that of (Bruce, 1972) (as in (Li et al, 2001)), who
inspired Allen; these relations are equivalent to cer-
tain sets of Allen relations, as shown Table 2. We
thought they were rather intuitive, seem to have an
appropriate level of granularity, and since three of
them are enough to describe situations (the other 3
being the converse relations), they are not to hard to
use by naive annotators.
To abstract away from particulars of a given an-
notation for some text, and thus to be able to com-
pare the underlying temporal model described by an
annotation, we try to measure a similarity between
annotations given by a system and human annota-
tions, from the saturated graph of detected tempo-
ral relations in each case (the human graph is satu-
rated after annotation relations have been translated
as equivalent disjunctions of Allen relations). We do
not want to limit the comparison to "simple" (base)
relations, as in (Setzer, 2001), because it makes the
evaluation very dependent on the choice of rela-
tions, and we also want to have a gradual measure
of the imprecision of the system annotation. For in-
stance, finding there is a "before or during" relation
between two events is better than proposing "after"
is the human put down "before", and it is less good
BEFORE ? i ? j (i before j ? ((i b j) ? (i m j)))
AFTER ? i ? j (i after j ? ((i bi j) ? (i mi j)))
OVERLAPS ? i ? j (i overlaps j ? ((i o j)))
IS_OVERLAPPED ? i ? j (i is_overlapped j ? ((i oi j)))
INCLUDES ? i ? j (i includes j ? ((i di j) ? (i si j) ? (i fi j) ? (i e j)))
IS_INCLUDED ? i ? j (i is_included j ? ((i d j) ? (i s j) ? (i f j) ? (i e j)))
Table 2: Relations proposed for annotation
than the correct answer "before".
Actually we are after two different notions. The
first one is the consistency of the system?s annota-
tion with the human?s: the information in the text
is compatible with the system?s annotation, i.e. the
former implies the latter. The second notion is how
precise the information given by the system is. A
very disjunctive information is less precise than a
simple one, for instance (a or b or c) is less precise
than (a or b) if a correct answer is (a).
In order to measure these, we propose two ele-
mentary comparison functions between two sets of
relations S and H, where S is the annotation pro-
posed by the system and H is the annotation inferred
from what was proposed by the human.
finesse = |S?H||S| coherence =
|S?H|
|H|
The global finesse score of an annotation is the aver-
age of a measure on all edges that have information
according to the human annotation (this excludes
edges with the universal disjunction U) once the
graph is saturated, while coherence is averaged on
the set of edges that bear information according to
the system annotation.
Finesse is intended to measure the quantity of in-
formation the system gets, while coherence gives
an estimate of errors the system makes with re-
spect to information in the text. Finesse and coher-
ence thus are somewhat similar respectively to re-
call and precision, but we decided to use new terms
to avoid confusion ("precision" being an ambigu-
ous term when dealing with gradual measures, as
it could mean how close the measure is to the max-
imum 1).
Obviously if S=H on all edges, all measures are
equal to 1. If the system gives no information at
all, S is a disjunction of all relations so H ? S,
H ? S = H and coherence=1, but then finesse is
very low.
These measures can of course be used to estimate
agreement between annotators.
6 Results
In order to see whether the measures we propose
are meaningful, we have looked at how the mea-
sures behave on a text "randomly" annotated in the
following way: we have selected at random pairs of
events in a text, and for each pair we have picked a
random annotation relation. Then we have saturated
the graph of constraints and compared with the hu-
man annotation. Results are typically very low, as
shown on a newswire message taken as example Ta-
ble 3.
We have then made two series of measures: one
on annotation relations (thus disjunctions of Allen
relations are re-expressed as disjunctions of annota-
tion relations that contains them), and one on equiv-
alent Allen relations (which arguably reflects more
the underlying computation, while deteriorating the
measure of the actual task). In the first case, an
Allen relation answer equals to b or d or s between
two events is considered as ?before or is_included?
(using relations used by humans) and is compared
to an annotation of the same form.
We then used finesse and coherence to estimate
our annotation made according to the method de-
scribed in the previous sections. We tried it on a
still limited2 set of 8 newswire texts (from AFP),
for a total of 2300 words and 160 events, compar-
ing to the English corpus of (Setzer, 2001), which
has 6 texts for less than 2000 words and also about
160 events. Each one of these texts has between 10
and 40 events. The system finds them correctly with
precision and recall around 97%. We made the com-
parison only on the correctly recognized events, in
order to separate the problems. This course limits
the influence of errors on coherence, but handicaps
finesse as less information is available for inference.
The measures we used were then averaged on the
number of texts. This departs from what could be
considered a more standard practice, summing ev-
erything and dividing by the number of comparisons
made. The reason behind this is we think compar-
ing two graphs as comparing two temporal models
of a text, not just finding a list of targets in a set
of texts. It might be easier to accept this if one re-
members that the number of possible relations be-
tween n events is n(n?1)/2. A text t1 with k more
2We are still annotating more texts manually to give more
significance to the results.
Finesse Coherence
annotation relations 0.114 0.011
Allen relations 0.083 0.094
Table 3: Example of evaluation on a "random" annotation
events than a text t2 will thus have about k2 times
more importance in a global score, and we find con-
fusing this non-linear relation between the size of a
text and its weight in the evaluation process. There-
fore, both finesse and coherence are generalized as
global measure of a temporal model of a text. It
could then be interesting to relate temporal infor-
mation and other features of a given text (size being
only one factor).
Results are shown Table 4. These results seem
promising when considering the simplifications we
have made on every step of the process. Caution is
necessary though, given the limited number of texts
we have experimented on, and the high variation we
have observed between texts. At this stage we be-
lieve the quality of our results is not that important.
Our main objective, above all, was to show the fea-
sibility of a robust method to annotate temporal re-
lations, and provide useful tools to evaluate the task,
in order to improve each step separately later. Our
focus was on the design of a good methodology.
If we try a first analysis of the results, sources
of errors fall on various categories. First, a number
of temporal adverbials were attached to the wrong
event, or were misinterpreted. This should be fine-
tuned with a better parser than what we used. Then,
we have not tried to take into account the specific
narrative style of newswire texts. In our set of texts,
the present tense was for instance used in a lot of
places, sometimes to refer to events in the past,
sometimes to refer to events that were going to hap-
pen at the time the text was published. However,
given the method we adopted, one could have ex-
pected better coherence results than finesse results.
It means we have made decisions that were not cau-
tious enough, for reasons we still have to analyze.
One potential reason is that relations offered to hu-
mans are maybe too vague in the wrong places: a
lot of information in a text can be asserted to be
"strictly before" something else (based on dates for
instance), while human annotators can only say that
events are "before or meets" some other event; each
time this is the case, coherence is only 0.5.
It is important to note that there are few points
of comparison on this problem. To the best of our
knowledge, only (Li et al, 2001) and (Mani and
Wilson, 2000) mention having tried this kind of an-
notation, as a side job for their temporal expressions
mark-up systems. The former considers only rela-
tions between events within a sentence, and the lat-
ter did not evaluate their method.
Finally, it is worth remembering that human an-
notation itself is a difficult task, with potentially a
lot of disagreement between annotators. For now,
our texts have been annotated by the two authors,
with an a posteriori resolution of conflicts. We
therefore have no measure of inter-annotator agree-
ment which could serve as an upper bound of the
performance of the system, although we are plan-
ning to do this at a later stage.
7 Conclusion
The aim of this study was to show the feasibility of
annotating temporal relations in a text and to pro-
pose a methodology for the task. We thus define a
way of evaluating the results, abstracting away from
variations of human descriptions for similar tempo-
ral situations. Our preliminary results seem promis-
ing in this respect. Obviously, parts of the method
need some polishing, and we need to extend the
study to a larger data set. It remains to be seen how
improving part of speech tagging, syntactic analy-
sis and discourse modeling can influence the out-
come of the task. Specifically, some work needs to
be done to evaluate the detection of temporal ad-
juncts, a major source of information in the process.
We could also try to mix our symbolic method with
some empirical learning. Provided we can collect
more annotated data, it would be easy to improve
the discourse model by (at least local) optimization
on the space of possible rules, starting with our own
set. We hope that the measures of temporal infor-
mation we have used will help in all these aspects,
but we are also planning to further investigate their
properties and that of other candidate measures not
considered here.
References
Steven Abney, 1996. Corpus-Based Methods in
Language and Speech, chapter Part-of-Speech
Tagging and Partial Parsing. Kluwer Academic
Publisher.
J. Allen. 1984. Towards a general theory of action
and time. Artificial Intelligence, 23:123?154.
B. Bruce. 1972. A model for temporal references
Finesse Standard Deviation Coherence SD
annotation relations 0.477499 0.286781 0.449899 0.175922
Allen relations 0.448222 0.289856 0.495755 0.204974
Table 4: Evaluation
and its application in a question answering pro-
gram. Artificial Intelligence, 3(1-3):1?25.
Elena Filatova and Eduard Hovy. 2001. Assign-
ing time-stamps to event-clauses. In Harper et al
(Harper et al, 2001).
Claire Grover, Janet Hitzeman, and Marc Moens.
1995. Algorithms for analysing the temporal
structure of discourse. In Sixth International
Conference of the European Chapter of the As-
sociation for Computational Linguistics. ACL.
Lisa Harper, Inderjeet Mani, and Beth Sundheim,
editors. 2001. ACL Workshop on Temporal
and Spatial Information Processing, 39th Annual
Meeting and 10th Conference of the European
Chapter. Association for Computational Linguis-
tics.
M. Kameyama, R. Passonneau, and M. Poesio.
1993. Temporal centering. In Proceedings of
ACL 1993, pages 70?77.
Graham Katz and Fabrizio Arosio. 2001. The an-
notation of temporal information in natural lan-
guage sentences. In Harper et al (Harper et al,
2001), pages 104?111.
W. Li, K-F. Wong, and C. Yuan. 2001. A model
for processing temporal reference in chinese. In
Harper et al (Harper et al, 2001).
I. Mani and G. Wilson. 2000. Robust temporal pro-
cessing of news. In Proceedings of ACL 2000.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of
the International Conference on New Methods in
Language Processing.
Andrea Setzer. 2001. Temporal Information in
Newswire Articles: an Annotation Scheme and
Corpus Study. Ph.D. thesis, University of
Sheffield, UK.
Franck Shilder and Christopher Habel. 2001. From
temporal expressions to temporal information:
Semantic tagging of news messages. In Harper
et al (Harper et al, 2001), pages 65?72.
F. Song and R. Cohen. 1991. Tense interpretation
in the context of narrative. In Proceedings of
AAAI?91, pages 131?136.
Mark Steedman. 1997. Temporality. In J. Van Ben-
them and A. ter Meulen, editors, Handbook of
Logic and Language. Elsevier Science B.V.
Nikolai Vazov. 2001. A system for extraction of
temporal expressions from french texts based on
syntactic and semantic constraints. In Harper
et al (Harper et al, 2001).
George Wilson, Inderjeet Mani, Beth Sundheim,
and Lisa Ferro. 2001. A multilingual approach to
annotating and extracting temporal information.
In Harper et al (Harper et al, 2001).
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 250?258,
Beijing, August 2010
Comparison of different algebras for inducing
the temporal structure of texts
Pascal Denis?
? Alpage Project-Team
INRIA & Universit? Paris 7
pascal.denis@inria.fr
Philippe Muller?,
 IRIT
Universit? de Toulouse
muller@irit.fr
Abstract
This paper investigates the impact of us-
ing different temporal algebras for learn-
ing temporal relations between events.
Specifically, we compare three interval-
based algebras: Allen (1983) algebra,
Bruce (1972) algebra, and the algebra de-
rived from the TempEval-07 campaign.
These algebras encode different granular-
ities of relations and have different infer-
ential properties. They in turn behave dif-
ferently when used to enforce global con-
sistency constraints on the building of a
temporal representation. Through various
experiments on the TimeBank/AQUAINT
corpus, we show that although the TempE-
val relation set leads to the best classifica-
tion accuracy performance, it is too vague
to be used for enforcing consistency. By
contrast, the other two relation sets are
similarly harder to learn, but more use-
ful when global consistency is important.
Overall, the Bruce algebra is shown to
give the best compromise between learn-
ability and expressive power.
1 Introduction
Being able to recover the temporal relations (e.g.,
precedence, inclusion) that hold between events
and other time-denoting expressions in a docu-
ment is an essential part of natural language un-
derstanding. Success in this task has important
implications for other NLP applications, such as
text summarization, information extraction, and
question answering.
Interest for this problem within the NLP com-
munity is not new (Passonneau, 1988; Webber,
1988; Lascarides and Asher, 1993), but has been
recently revived by the creation of the TimeBank
corpus (Pustejovsky et al, 2003), and the orga-
nization of the TempEval-07 campaign (Verhagen
et al, 2007). These have seen the development
of machine learning inspired systems (Bramsen et
al., 2006; Mani et al, 2006; Tatu and Srikanth,
2008; Chambers and Jurafsky, 2008).
Learning the temporal stucture from texts is a
difficult problem because there are numerous in-
formation sources at play (in particular, seman-
tic and pragmatic ones) (Lascarides and Asher,
1993). An additional difficulty comes from the
fact that temporal relations have logical proper-
ties that restrict the consistent graphs that can be
built for a set of temporal entities (for instance
the transitivity of inclusion and temporal prece-
dence). Previous work do not attempt to directly
predict globally coherent temporal graphs, but in-
stead focus on the the simpler problem of label-
ing pre-selected pairs of events (i.e., a task that
directly lends itself to the use of standard classifi-
cation techniques). That is, they do not consider
the problem of linking pairs of events (i.e., of de-
termining which pairs of events are related).
Given the importance of temporal reasoning
for determining the temporal structure of texts,
a natural question is how to best use it within
a machine-based learning approach. Following
(Mani et al, 2006), prior approaches exploit tem-
poral inferences to enrich the set of training in-
stances used for learning. By contrast, (Bramsen
et al, 2006) use temporal relation compositions to
provide constraints in a global inference problem
(on the slightly different task of ordering passages
in medical history records). (Tatu and Srikanth,
2008) and (Chambers and Jurafsky, 2008) com-
bine both approaches and use temporal reasoning
both during training and decoding. Interestingly,
these approaches use different inventories of re-
lations: (Mani et al, 2006) use the TimeML 13
relation set, while (Chambers and Jurafsky, 2008;
250
Bramsen et al, 2006) use subset of these relations,
namely precedence and the absence of relation.
This paper adopts a more systematic perspec-
tive and directly assesses the impact of differ-
ent relation sets (and their underlying algebras)
in terms of learning and inferential properties.
Specifically, we compare three interval-based al-
gebras for building classification-based systems,
namely: Allen (1983)?s 13 relation algebra, Bruce
(1972)?s 7 relations algebra, and the algebra
underlying Tempeval-07 3 relations (henceforth,
TempEval algebra). We wish to determine the
best trade-off between: (i) how easy it is to learn
a given set of relations, (ii) how informative are
the representations produced by each relation set,
and (iii) how much information can be drawn from
the predicted relations using knowledge encoded
in the representation. These algebras indeed dif-
fer in the number of relations they encode, and in
turn in how expressive each of these relations is.
From a machine learning point of view of learn-
ing, it is arguably easier to learn a model that
has to decide among fewer relations (i.e., that has
fewer classes). But from a representational point
of view, it is better to predict relations that are as
specific as possible, for composing them may re-
strict the prediction to more accurate descriptions
of the situation. However, while specific relations
potentially trigger more inferences, they are also
more likely to predict inconsistent constraints. In
order to evaluate these differences, we design a set
of experiments on the Timebank/AQUAINT cor-
pus, wherein we learn precise relations and vaguer
ones, and evaluate them with respect to each other
(when a correspondence is possible).
Section 2 briefly presents the Time-
bank/AQUAINT corpus. In section 3, we
describe the task of temporal ordering through an
example, and discuss how it should be evaluated.
Section 4 then goes into more detail about the
different representation possibilities for temporal
relations, and some of their formal properties.
Section 5 presents our methods for building tem-
poral structures, that combines relation classifiers
with global constraints on whole documents.
Finally, we discuss our experimental results in
section 6.
2 The Timebank/AQUAINT corpus
Like (Mani et al, 2006) and (Chambers and Ju-
rafsky, 2008), we use the so-called OTC corpus,
a corpus of 259 documents obtained by com-
bining the Timebank corpus (Pustejovsky et al,
2003) (we use version 1.1 of the corpus) and the
AQUAINT corpus.1 The Timebank corpus con-
sists of 186 newswire articles (and around 65, 000
words), while AQUAINT has 73 documents (and
around 40, 000 words).
Both corpora are annotated using the TimeML
scheme for tagging eventualities (events and
states), dates/times, and their temporal relations.
Eventualities can be denoted by verbs, nouns, and
some specific constructions. The temporal rela-
tions (i.e., the so-called TLINKS) encode topolog-
ical information between the time intervals of oc-
curring eventualities. TimeML distinguishes three
types of TLINKS: event-event, event-time, and
time-time, giving rise to different subtasks. In this
paper, we will focus on predicting event-event re-
lations (see (Filatova and Hovy, 2001; Boguraev
and Ando, 2005) for work on the other tasks). The
set of temporal relations used in TLINKS mirrors
the 13 Allen relations (see next section), and in-
cludes the following six relations: before, begins,
ends, ibefore, includes, simultaneous and their in-
verses. The combined OTC corpus comprises a
total of 6, 139 annotated event-event TLINKS. We
also make use of the additional TLINKS indepen-
dently provided by (Bethard et al, 2007) for 129
of the 186 Timebank documents.
3 Task presentation and evaluation
3.1 An example
We illustrate the task of event ordering using a
small fabricated, simplified example:
Fortis bank investede1 in junk bonds
before the financial crisise2 , but
got ride3 of most of them during
the crisise2bis . However, the insti-
tution still went bankrupte4 a year
later.
1Both corpora are freely available from http://www.
timeml.org/site/timebank/timebank.html.
251
The annotation for this temporal structure would
include the following relations: e1 is temporally
before e2, e3 is temporally included in e2, and e3
is before e4. The coreference relation between e2
and e2bis implies the equality of their temporal ex-
tension. Of course all these events may in theory
be related temporally to almost any other event in
the text. Events are also anchored to temporal ex-
pressions explicitly, and this is usually considered
as a separate, much easier task. We will use this
example throughout the rest of our presentation.
3.2 Comparing temporal annotations
Due to possible inferences, there are often many
equivalent ways to express the same ordering of
events, so comparisons between annotation and
reference event-event pairs cannot rely on simple
precision/recall measures.
Consider the above example and assume the
following annotation: e1 is before e2, e3 is in-
cluded in e2, and e3 is before e3. Without going
into too much detail about the semantics of the re-
lations used, one expects annotators to agree with
the fact that it entails that e1 is before e3, among
other things. So the annotation is equivalent to a
larger set of relations. In some cases, the inferred
information is disjunctive (the relation holding be-
tween two events is a subset of possible ?simple?
relations, such as ?before or included?).
Nowadays, the given practice is to compute
some sort of transitive closure over the network of
constraints on temporal events (usually expressed
in the well-studied Allen algebra (Allen, 1983)),
and compute agreements over the saturated struc-
tures. Specifically, we can compare the sets of
simple temporal relations that are deduced from
it (henceforth, the ?strict? metric), or measure the
agreement between the whole graphs, including
disjunctions (Verhagen et al, 2007) (henceforth,
the ?relaxed? metric).2 Under this latter met-
ric, precision (resp. recall) of a prediction for a
pair of events consisting of a set S of relations
with respect to a set of relations R inferred from
the reference, is computed as |S ? R|/|S| (resp.
|S ?R|/|R|).
2Taking into account disjunctions means giving partial
credit to disjunctions approximating the reference relation
(possibly disjunctive itself), see next section.
e1
e2
e3
e4
b
di
b
e1
e2
e3
e4
b
b
di,fi,o,m
e1
e2
e3
e4
b
di
b
b
e1
e2
e3
e4
b
b
Figure 1: Two non-equivalent annotations of the
same situations (left) and their transitive closure
in Allen?s algebra (right, with new relations only).
b stands for Allen?s before relation, m for meet, o
for overlap, di and fi for the inverses of during and
finish, respectively.
Figure 1 illustrates the point of these ?satu-
rated? representations, showing two raw annota-
tions of our example on the left (top and bottom)
and their closures on the right. The raw annota-
tions share only 2 relations (between e1 and e2,
and e3 and e4), but their transitive closures agree
also on the relations between e1 and e3, e1 and
e4, and e3 and e4. They still differ on the rela-
tion between e2 and e4, but only because one is
much more specific than the other, something that
can only be taken into account by a partial credit
scoring function.
For this example, the ?strict? metric yields pre-
cision and recall scores of 5/5 and 5/6, when
comparing the top annotation against the bottom
one. By contrast, the ?relaxed? metric (introduced
in the TempEval-07) yields precision and recall
scores of (5+0.2)/6 and 6/6, respectively.
We now turn to the issue of the set of relations
chosen for the task of expressing temporal infor-
mation in texts.
4 Temporal representations
Because of the inferential properties of temporal
relations, we have seen that the same situation can
be expressed in different ways, and some rela-
tions can be deduced from others. The need for
252
a precise reasoning framework has been present
in previous attempts at the task (Setzer et al,
2006), and people have moved to a set of hand-
made rules over ad hoc relations to more widely
accepted temporal reasoning frameworks, such as
algebras of temporal relations, the most famous
being Allen?s interval algebra.
An algebra of relations can be defined on any
set of relations that are mutually exclusive (two
relations cannot hold at the same time between
two entities) and exhaustive (at least one relation
must hold between two given entities). The alge-
bra starts from a set of simple, atomic, relations
U = {r1, r2, ...}, and a general relation is a sub-
set of U , interpreted as a disjunction of the rela-
tions it contains. From there, we can define union
and intersection of relations as classical set union
and intersection of the base relations they consist
of. Moreover, one can define a composition of re-
lations as follows:
(r1 ? r2)(x, z)? ?y r1(x, y) ? r2(y, z)
In words, a relation between x and z can be
computed from what is known between (x and
y) and (y and z). By computing beforehand the
n?n compositions of base relations of U , we can
compute the composition of any two general rela-
tions (because r ? r? =? when r, r? are basic and
r 6= r?):
{r1, r2, ...rk} ? {s1, s2, ...sm} =
?
i,j
(ri ? sj)
Saturating the graph of temporal constraints
means applying these rules to all compatible pairs
of constraints in the graph and iterating until a
fixpoint is reached. In Allen?s algebra there are
13 relations, determined by the different relations
that can hold between two intervals endpoints (be-
fore, equals, after). These relations are: b (be-
fore), m (meet), o (overlap), s (start), f (finish), d
(during), their inverses (bi, mi, oi, si, fi, di) and =
(equal), see figure 2.3
It is important to see that a general approach
to temporal ordering of events cannot restrict it-
self to a subset of these and still use the power of
3TimeML uses somewhat different names, with obvious
mappings, except ibefore (?immediately before?) for m, and
iafter (?immediately after?) for mi.
X
Y
X
X
X
Y
Y
Yfinishes
before
meets
overlaps
X
X
Y
Y
equals
during
starts
X
Y
Figure 2: Allen?s thirteen relations between two
temporal intervals
inferences to complete a situation, because com-
position of information is stable only on restricted
subsets. And using all of them means generating
numerous disjunctions of relations.
Allen relations are convenient for reason-
ing purposes, but might too precise for rep-
resenting natural language expressions, and
that?s why recent evaluation campaigns such as
TempEval-07 have settled on vaguer representa-
tions. TempEval-07 uses three relations called be-
fore, overlaps and after, which we note bt, ot,
and bit.4 These all correspond to disjunctions
of Allen relations: {b,m}a, {o,d,s,=,f}a and its
inverse, and {bi,mi}a, respectively. These rep-
resentations can be converted to Allen relations,
over which the same inference procedures can be
applied, and then expressed back as (potentially
disjunctive) TempEval relations. They thus form
a sub-algebra of Allen?s algebra, if we add their
possible disjunctions.
In fact, starting from the base relations, only
{b,o}t, {bi,o}t, and vague (i.e., the disjunction of
all relations) can be inferred (besides the base re-
lations). This is a consequence of the stability of
so-called convex relations in Allen algebra. Note
that an even simpler schema is used in (Chambers
and Jurafsky, 2008), where only TempEval before
and after and the vague relation are used.
We propose to consider yet another set of rela-
tion, namely relations from (Bruce, 1972). These
provide an intermediate level of representation,
since they include 7 simple relations. These are
4When it is not obvious, we will use subscript symbols
to indicate the particular algebra that is used (e.g., bt is the
before relation in TempEval).
253
also expressible as disjunctions of Allen relations;
they are: before (bb), after (bib) (with the same
semantics as TempEval?s bt and bit), equals (=b,
same as =a), includes (i, same as Allen?s {s,d,f}a),
overlaps (ob, same as oa), included (ii) and is-
overlapped (oib), their inverse relations. The
equivalences between the three algebras is shown
table 1.
Allen Bruce Tempeval
before before beforemeet
overlaps overlaps
overlaps
starts
includedduring
finishes
overlapsi is-overlapped
startsi
includesduringi
finishesi
meeti after afterbeforei
equals equals equals
Table 1: Correspondances between temporal al-
gebras. A relation ranging over multiple cells
is equivalent to a disjunction of all the relations
within these cells.
Considering a vaguer set is arguably more ad-
equate for natural language expressions while at
the same time this specific set preserves at least
the notions of temporal order and inclusion (con-
trary to the TempEval scheme), which have strong
inferential properties: they are both transitive, and
their composition yields simple relations; over-
lap allows for much weaker inferences. Figure 3
shows part of our example from the introduction
expressed in the three cases: with Allen relations,
the most precise, with Bruce relations and Tem-
pEval relations, with dotted lines showing the ex-
tent of the vagueness of the temporal situations in
each case (with respect to the most precise Allen
description). We can see that TempEval relations
lose quickly all information that is not before or
after, while Bruce preserves inference combining
precedence and temporal inclusion.
Information can be converted from one algebra
to the other, since vaguer algebras are based on re-
lations equivalent to disjunctions in Allen algebra.
But conversion from a precise relation to a vaguer
one and back to a more precise algebra leads to
information loss. Hence on figure 3, the original
Allen relation: e3 da e2 is converted to: e3 ot e2
in TempEval, which converts back into the much
less informative: e3 {o,d, s,=, f,oi, si, fi,di}a e2.
We will use these translations during our system
evaluation to have a common comparison point
between representations.
5 Models
5.1 Algebra-based classifiers
In order to compare the impact of the different al-
gebras described in section 4, we build three event
pair classification models corresponding to each
relation set. The resulting Allen-based, Bruce-
based, and Tempeval-based models therefore con-
tain 13, 7, and 3 class labels, respectively.5 For
obvious sparsity issues, we did not include classes
corresponding to disjunctive relations, as there are
2|R| possible disjunctions for each relation set R.
For training our models, we experiment with 4
various configurations that correspond to ways of
expanding the set of training examples. Specifi-
cally, these configurations vary in: (i) whether or
not we added the additional ?Bethard relations? to
the initial OTC annotations (Bethard et al, 2007),
(ii) whether or not we applied saturation over the
set of annotated relations.
5.2 Features
Our feature set for the various models is similar
to that used by previous work, including binary
features that encode event string as well as the five
TimeML attributes and their possible values:
? aspect: none, prog, perfect, prog perfect
? class: report, aspectual, state, I-state I-
action, perception, occurrence
? modality: none, to, should, would, could
can, might
? polarity: positive, negative
? tense: none, present, past, future
5Our TempEval model actually has a fourth label for the
identity relation. The motivations behind the inclusion of this
extra label are: (i) this relation is linguistically motivated and
comparatively easy to learn (for a lot of instances of this rela-
tion are cases of anaphora, which are often signaled by iden-
tical strings) (ii) this relation triggers a lot of specific infer-
ences.
254
e1
Time
e3
e2
(a) Allen:
(e1bae2 ? e3dae2) ? e1bae3
e1
Time
e3
e2
(b) Bruce:
(e1bbe2 ? e3dbe2) ? e1bbe3
e1
Time
e3
e2
(c) Tempeval:
(e1bte2 ? e3ote2) ? e1{bt, ot}e3
Figure 3: Comparing loss of inferential power in algebras: hard lines show the actual temporal
model, exactly expressed in Allen relations (a); dotted lines show the vagueness induced by alterna-
tive schemes, and the inference that can or cannot still be made in each algebra, (b) and (c).
Additional binary features check agreement for
same attribute (e.g., the same tense). Finally, we
add features that represent the distance between
two events (in number of sentences, and in num-
ber of intervening events). 6
5.3 Training set generation
Our generic training procedure works as follows.
For each document, we scan events in their order
of appearance in the text. We create a training
instance inst(ei,ej) for each ordered pair of events
(ei, ej): if (ei, ej) (resp. (ej , ei)) corresponds to
an annotated relation r, then we label inst(ei,ej)
with the label r (resp. its inverse r?1).
5.4 Parameter estimation
All of these classifiers are maximum entropy mod-
els (Berger et al, 1996). Parameter estimation
was performed with the Limited Memory Variable
Metric algorithm (Malouf, 2002) implemented in
the Megam package.7
5.5 Decoding
We consider two different decoding procedures.
The first one simply mirrors the training proce-
dure just described, scanning pairs of events in the
order of the text, and sending each pair to the clas-
sifier. The pair is then labeled with the label out-
putted by the classifier (i.e., the label receiving the
6These were also encoded as binary features, and the var-
ious feature values were binned in order to avoid sparseness.
7Available from http://www.cs.utah.edu/
~hal/megam/.
highest probability). No attempt is made to guar-
antee the consistency of the final temporal graph.
Our second inference procedure works as fol-
lows. As in the previous method, we scan the
events in the order of the text, and create ordered
pairs of events that we then submit to the classifier.
But the difference is that we saturate the graph af-
ter each classification decision to make sure that
the graph created so far is coherent. In case where
the classifier predicts a relation whose addition re-
sults in an incoherent graph, we try the next high-
est probability relation, and so on, until we find
a coherent graph. This greedy procedure is simi-
lar to the Natural Reading Order (NRO) inference
procedure described by (Bramsen et al, 2006).
6 Experiments and results
We perform two main series of experiments for
comparing our different models. In the first series,
we measure the accuracy of the Allen-, Bruce-
, and Tempeval-based models on predicting the
correct relation for the event-event TLINKS an-
notated in the corpus. In the second series, we
saturate the event pair relations produced by the
classifiers (combined with NRO search to en-
force global coherence) and compare the pre-
dicted graphs against the saturated event-event
TLINKS.
6.1 Experiment settings
All our models are trained and tested with 5-fold
cross-validation on the OTC documents. For eval-
255
uation, we use simple accuracy for the first se-
ries of experiments, and two ?strict? and ?relaxed?
precision/recall measures described in section 3
for the other series. For each type of measures,
we report scores with respect to both Allen and
TemEval relation sets. All scores are reported
using macro-averaging. Out of the 259 tempo-
ral graphs present in OTC, we found that 54 of
them were actually inconsistent when saturated;
the corresponding documents were therefore left
out of the evaluation.8 Given the rather expensive
procedure involved in the NRO decoding (saturat-
ing an inconsistent graph ?erases? all relations),
we skipped 8 documents wich were much longer
than the rest, leaving us with 197 documents for
our final experiments.
6.2 Event-event classification
Table 2 summarizes the accuracy scores of the
different classifiers on the event-event TLINKS
of OTC. We only report the best configuration
for each model. For the TempEval-based model,
we found that the best training setting was when
Bethard annotations were added to the original
TimeML annotations, but with no saturation.9 For
Allen and Bruce models, neither Bethard?s re-
lations nor saturation helps improve classifica-
tion accuracy. In fact, saturation degrades per-
formance, which can be explained by the fact
that saturation reinforces the bias towards already
over-represented relations.10 The best accuracy
performances are obtained by the Allen-based and
TempEval-based classifiers, each one performing
better in its own algebra (with 47.0% and 54.0%).
This is not surprising, since these classifiers were
specifically trained to optimize their respective
metrics. The Bruce-based classifier is slightly bet-
ter than the Allen-based one in TempEval, but also
slightly worse than TempEval-based classifier in
Allen.
8Because there is no way to trace the relation(s) respon-
sible for an inconsistency without analysing the whole set of
annotations of a text, and considering that it usually happens
on very long texts, we did not attempt to manually correct
the annotations.
9This is actually consistent with similar findings made by
(Chambers and Jurafsky, 2008).
10For instance, for Allen relations, there are roughly 50%
of before-after relations before saturation but 73% of them
after saturation.
Allen Acc. TempEval Acc.
Allen 47.0 48.9
Bruce N/A 49.3
TempEval N/A 54.0
Table 2: Accuracy scores for Allen, Bruce, and
TempEval classifiers on event-event TLINKS, ex-
pressed in Allen or TempEval algebra. Scores for
Bruce and TempEval models into Allen are left
out, since they predict (through conversion) dis-
junctive relations for all relations but equality.
Our accuracy scores for Allen, and TempEval-
based classifiers are somewhat lower than the ones
reported for similar systems by (Mani et al, 2006)
and (Chambers and Jurafsky, 2008), respectively.
These differences are likely to come from the fact
that: (i) (Mani et al, 2006) perform a 6-way clas-
sification, and not a 13-way classification11, and
(ii) (Chambers and Jurafsky, 2008) use a relation
set that is even more restrictive than TempEval?s.
6.3 Saturated graphs
Table 3 summarizes the various precision/recall
scores of the graph obtained by saturating the clas-
sifiers predictions (potentially altered by NRO)
against the event-event saturated graph. These re-
sults contrast with the accuracy results presented
in table 2: while the TempEval-based model was
the best model in classification accuracy in Tem-
pEval, it is now outperformed by both the Allen-
and Bruce-based systems (this with or with us-
ing NRO). The best system in TempEval is actu-
ally Bruce-based system, with 52.9 and 62.8 for
the strict/relaxed metrics, respectively. The re-
sults suggest that this algebra might actually of-
fer the best trade-off between learnanility and ex-
pressive power. The use of NRO to restore global
coherence yields important gains (10 points) in
the relaxed metric for both Allen- and Bruce-
based systems (although they do not convert into
gains in the strict metric). Unsuprisingly, the
best model on the Allen set remains Allen-based
model (and this time the use of NRO results in
gains on the strict metric). Predictions without
11This is only possible because they order the event-event
pairs before submitting them to the classifier.
256
System Allen Tempeval
RELAX STRICT RELAX STRICT
R P F1 R P F1 R P F1 R P F1
Allen 57.5 46.7 51.5 49.6 56.2 52.7 62.0 50.3 55.5 50.4 57.1 53.6
Bruce 46.0 39.0 42.1 18.0 44.0 25.9 62.9 52.6 57.3 50.9 57.0 53.8
Tempeval 37.1 35.9 36.5 14.0 44.0 21.2 49.3 47.1 48.2 21.7 44.2 29.1
AllenNRO 44.8 60.1 51.3 57.2 62.9 59.9 63.8 67.0 65.3 45.2 60.6 51.8
BruceNRO 46.3 53.1 49.5 13.9 45.3 21.2 65.5 71.8 68.5 46.6 61.1 52.9
TempevalNRO 37.1 35.9 36.5 13.9 44.3 21.2 49.3 47.1 48.2 21.7 44.2 29.1
Table 3: Comparing Allen-, Bruce-, Tempeval-based classifiers saturated predictions on saturated event-
event graph. The NRO subscript indicates whether the system uses NRO or not. Evaluation are given
with respect to both Allen and Tempeval relation sets.
NRO yielded between 7.5 and 9% of inconsistent
saturated graphs that were ignored by the evalua-
tion, which means this impacted recall measures
only.
7 Related work
Early work on temporal ordering (Passonneau,
1988; Webber, 1988; Lascarides and Asher, 1993)
concentrated on studying the knowledge sources
at play (such as tense, aspect, lexical semantics,
rhetorical relations). The development of anno-
tated resources like the TimeBank corpus (Puste-
jovsky et al, 2003) has triggered the development
of machine learning systems (Mani et al, 2006;
Tatu and Srikanth, 2008; Chambers and Jurafsky,
2008).
More recent work uses automatic classifica-
tion methods, based on the TimeBank and Ac-
quaint corpus, either as is, with inferential enrich-
ment for training (Mani et al, 2006; Chambers
et al, 2007), or supplied with the corrections of
(Bethard et al, 2007), or are restricted to selected
contexts, such as intra-sentential event relations
(Li et al, 2004; Lapata and Lascarides, 2006). All
of these assume that event pairs are preselected,
so the task is only to determine what is the most
likely relation between them. The best scores
are obtained with the added assumption that the
event-event pair can be pre-ordered (thus reduc-
ing the number of possible labels by 2).
More recently, (Bramsen et al, 2006) and sub-
sequently (Chambers and Jurafsky, 2008) pro-
pose to use an Integer Linear Programming solver
to enforce the consistency of a network of con-
straints while maximizing the score of local clas-
sification decisions. But these are restricted to the
relations BEFORE and AFTER, which have very
strong inference properties that cannot be gener-
alised to other relations. The ILP strategy is not
likely to scale up very well for richer relation sets,
for the number of possible relations between two
events (and thus the number of variables to put in
the LP solver for each pair) is the order of 2|R|
(where R is the relation set), and each transitiv-
ity constraints generates an enormous amount of
constraints.
8 Conclusion
We have investigated the role played by ontolog-
ical choices in temporal representations by com-
paring three algebras with different granularities
of relations and inferential powers. Our experi-
ments on the Timebank/AQUAINT reveal that the
TempEval relation set provides the best overall
classification accuracy, but it provides much less
informative temporal structures, and it does not
provide enough inferences for being useful for en-
forcing consistency. By contrast, the other two
relation sets are significantly harder to learn, but
provide more richer inferences and are therefore
more useful when global consistency is important.
Bruce?s 7 relations-based model appears to per-
form best in the TempEval evaluation, suggesting
that this algebra provides the best trade-off be-
tween learnability and expressive power.
257
References
Allen, James. 1983. Maintaining Knowledge about
Temporal Intervals. Communications of the ACM,
pages 832?843.
Berger, A., S. Della Pietra, and V. Della Pietra. 1996.
A maximum entropy approach to natural language
processing. Computational Linguistics, 22(1):39?
71.
Bethard, Steven, James H. Martin, and Sara Klingen-
stein. 2007. Timelines from text: Identification of
syntactic temporal relations. In International Con-
ference on Semantic Computing, pages 11?18, Los
Alamitos, CA, USA. IEEE Computer Society.
Boguraev, Branimir and Rie Ando. 2005. TimeML-
compliant text analysis for temporal reasoning. In
Kaelbling, Leslie Pack and Fausto Giunchiglia, edi-
tors, Proceedings of IJCAI05, pages 997?1003.
Bramsen, Philip, Pawan Deshpande, Yoong Keok Lee,
and Regina Barzilay. 2006. Inducing temporal
graphs. In Proceedings of the 2006 Conference on
Empirical Methods in Natural Language Process-
ing, pages 189?198, Sydney, Australia, July. Asso-
ciation for Computational Linguistics.
Bruce, B. 1972. A model for temporal references and
its application in a question answering program. Ar-
tificial Intelligence, 3(1-3):1?25.
Chambers, Nathanael and Daniel Jurafsky. 2008.
Jointly combining implicit constraints improves
temporal ordering. In Proceedings of the 2008 Con-
ference on Empirical Methods in Natural Language
Processing, pages 698?706, Honolulu, Hawaii, Oc-
tober. Association for Computational Linguistics.
Chambers, Nathanael, Shan Wang, and Daniel Juraf-
sky. 2007. Classifying temporal relations between
events. In ACL. The Association for Computer Lin-
guistics.
Filatova, Elena and Eduard Hovy. 2001. Assigning
time-stamps to event-clauses. In Mani, I., J. Puste-
jovsky, and R Gaizauskas, editors, The Language of
Time: A Reader. Oxford University Press.
Lapata, Maria and Alex Lascarides. 2006. Learning
sentence-internal temporal relations. J. Artif. Intell.
Res. (JAIR), 27:85?117.
Lascarides, Alex and Nicholas Asher. 1993. Tem-
poral interpretation, discourse relations and com-
mon sense entailment. Linguistics and Philosophy,
16:437?493.
Li, Wenjie, Kam-Fai Wong, Guihong Cao, and Chunfa
Yuan. 2004. Applying machine learning to chi-
nese temporal relation resolution. In Proceedings
of the 42nd Meeting of the Association for Compu-
tational Linguistics (ACL?04), Main Volume, pages
582?588, Barcelona, Spain, July.
Malouf, Robert. 2002. A comparison of algorithms
for maximum entropy parameter estimation. In Pro-
ceedings of the Sixth Workshop on Natural Lan-
guage Learning, pages 49?55, Taipei, Taiwan.
Mani, Inderjeet, Marc Verhagen, Ben Wellner,
Chong Min Lee, and James Pustejovsky. 2006. Ma-
chine learning of temporal relations. In Proceedings
of the 21st International Conference on Computa-
tional Linguistics and 44th Annual Meeting of the
Association for Computational Linguistics, pages
753?760, Sydney, Australia, July. Association for
Computational Linguistics.
Passonneau, Rebecca J. 1988. A computational model
of the semantics of tense and aspect. Computational
Linguistics, 14(2):44?60.
Pustejovsky, James, Patrick Hanks, Roser Saur?,
Andrew See, Robert Gaizauskas, Andrea Setzer,
Dragomir Radev, Beth Sundheim, David Day, Lisa
Ferro, and Marcia Lazo. 2003. The TIMEBANK
Corpus. In Proceedings of Corpus Linguistics,
pages 647?656, Lancaster University, UK, March.
Setzer, Andrea, Robert Gaizauskas, and Mark Hepple.
2006. The Role of Inference in the Temporal An-
notation and Analysis of Text. Language Resources
and Evaluation, 39:243?265.
Tatu, Marta and Munirathnam Srikanth. 2008. Ex-
periments with reasoning for temporal relations be-
tween events. In Proceedings of the 22nd Inter-
national Conference on Computational Linguistics
(Coling 2008), pages 857?864, Manchester, UK,
August. Coling 2008 Organizing Committee.
Verhagen, Marc, Robert Gaizauskas, Franck Schilder,
Mark Hepple, Graham Katz, and James Puste-
jovsky. 2007. SemEval-2007 - 15: TempEval Tem-
poral Relation Identification. In Proceedings of Se-
mEval workshop at ACL 2007, Prague, Czech Re-
public, June. Association for Computational Lin-
guistics, Morristown, NJ, USA.
Webber, Bonnie Lynn. 1988. Tense as discourse
anaphor. Computational Linguistics, 14(2):61?73.
258
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 2184?2194, Dublin, Ireland, August 23-29 2014.
Unsupervised extraction of semantic relations using discourse cues
Juliette Conrath Stergos Afantenos Nicholas Asher Philippe Muller
IRIT, Universit? Toulouse & CNRS, Univ. Paul Sabatier, 118 Route de Narbonne, 31062 Toulouse
{firstname.lastname@irit.fr}
Abstract
This paper presents a knowledge base containing triples involving pairs of verbs associated with
semantic or discourse relations. The relations in these triples are marked by discourse connectors
between two adjacent instances of the verbs in the triple in the large French corpus, frWaC.
We detail several measures that evaluate the relevance of the triples and the strength of their
association. We use manual annotations to evaluate our method, and also study the coverage of
our resource with respect to the discourse annotated corpus Annodis. Our positive results show
the potential impact of our resource for discourse analysis tasks as well as other semantically
oriented tasks like temporal and causal information extraction.
1 Introduction
Relational lexical resources, which describe semantic relations between lexical items, have tradition-
ally focused on relations like synonymy or similarity in thesauri, perhaps including some hierarchical
semantic relations like hyperonymy or hyponomy or part-whole relations as in the resource Wordnet (Fel-
baum, 1998). Some distributional thesauri contain more varied relations, see e.g. (Grefenstette, 1994),
however these relations are not typed. The lexical semantics given by FrameNet (Baker et al., 1998) does
include causal and temporal relations, as does Verbocean (Chklovski and Pantel, 2004), but coverage is
limited and empirical validation of these resources is partial and still largely remains to be done.
Lexical relations, in particular between verbs, are nevertheless crucial for understanding natural lan-
guage and for many information processing tasks. They are needed for textual inference, in which one
has to infer certain relations between eventualities (Hashimoto et al., 2009; Tremper and Frank, 2013),
for information extraction tasks, like finding temporal relations between eventualities mentioned in a text
(UzZaman et al., 2013), for automatic summarization (Liu et al., 2007), and for discourse parsing in the
absence of explicit discourse markers (Sporleder and Lascarides, 2008).
In this paper we report on our efforts to extract semantic relations essential to the analysis of discourse
and its interpretation, in which links are made between units of text or rather their semantic representa-
tions as in (1) in virtue of semantic information about the two main verbs of those clauses.
(1) The candidate demonstrated his expertise during the interview. The committee was completely
convinced.
We follow similar work on the extraction of causal, temporal, entailment and presuppositional relations
from corpora (Do et al., 2011; Chambers and Jurafsky, 2008; Hashimoto et al., 2009; Tremper and Frank,
2013), though our goals and validation methods are different. While one of our goals is to use this
information to improve performance in predicting discourse relations between clauses, we believe that
such a lexical resource will have other uses in other tasks in which semantic information is needed.
Discourse analysis is a difficult task. Rhetorical relations are frequently implicit and require for their
identification inference using diverse sources of lexical and compositional semantic information. In the
Penn Discourse Treebank corpus for example, 52% of the discourse relations are unmarked (Prasad et
This work has been supported by the French agency Agence Nationale de la Recherche (ANR-12-CORD-0004).
It is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added
by the organizers. License details : http://creativecommons.org/licenses/by/4.0/.
2184
al., 2008). Accordingly, annotation with discourse structure is a slow and error prone task, and relatively
little annotated data is currently available ; and so machine learning approaches have had limited suc-
cess in this area. Our approach addresses this problem, using non annotated data with features that can
be automatically detected to find typical contexts (pairs of discourse units) in which various discourse
relations occur. We suppose with (Sporleder and Lascarides, 2008; Braud and Denis, 2013) that such
contexts display regular lexical associations, in particular with verbs in those discourse units. An ex-
plicit, manually compiled list of all possible associations between two verbs and the semantic relations
they suggest is infeasible, so we present here an automatic method for compiling such a list, inspired by
the Verbocean project (Chklovski and Pantel, 2004).
Our hypothesis, supported by existing corpora, is that adjacent clauses are often arguments of discourse
relations. When these clauses contain certain adverbs or other discourse connectors, we can recover
automatically one or more discourse relations that we associate with the main verbs of those clauses. We
extract triples consisting of the two verbs and a semantic relation from a large corpus with the aim of
inferring that such a pair of verbs can suggest the semantic relation even in the absence of an explicit
discourse marker. We thus also suppose, with (Sporleder and Lascarides, 2008; Braud and Denis, 2013),
that such discourse markers are at least partially redundant ; inferring a discourse relation between two
clauses relies not only the marker but on the two verbs in the related clauses as well. All of our work has
been done on French data.
Our paper is organized as follows. We describe first the knowledge base of verb semantic relation
triples that we have constructed (section 2) ; we then present our methods for isolating verb pairs impli-
cating discourse or temporal information (section 3). A third section describes our methods of evaluation
(section 4) and a fourth discusses related work (section 5).
2 Exploring relations between verbs in a corpus
We built a knowledge base (V
2
R)
1
using the frWaC corpus(Baroni et al., 2009). frWaC contains about
1.6 billion words and was collected on the Web on the .fr domain. We first parsed the documents in our
corpus using BONSAI
2
, which first produced a morpho-syntactic labeling using MElt (Denis and Sagot,
2012) and then a syntactic analysis in the form of dependency trees via a French version of the MaltParser
(Nivre et al., 2007).
Our goal is to find pairs of verbs linked by a relation explicitly marked by a discourse connector
in the corpus, as an indication of a regular semantic relation between the two verbs. The relations we
have considered are common to most theories of discourse analysis, and they can be grouped into four
classes (Prasad et al., 2008) : causal (contingency) relations, temporal relations, comparison relations
(mainly contrast type relations), and expansion relations (e.g. elaboration or continuation).
To find explicitly marked relations, we used a lexicon of discourse connectors for French, the man-
ually constructed LEXCONN resource (Roze et al., 2012)
3
. LEXCONN includes 358 connectors and
gives their syntactic category as well as associated discourse relations inspired from (Asher and Las-
carides, 2003). Some connectors are ambiguous in that they are associated with several relations. We
used only the unambiguous connectors (263 in all) in LEXCONN, as a first step. We regrouped the
LEXCONN relations into classes
4
: explanation relations (parce que/because) and result (ainsi/thus)
form the causal class ; temporal relations (puis, apr?s que/then,after that) form the narration group. We
also considered other relations like contrast (mais/but), continuation (et, encore/and,again), background
(alors que/while), temporal location (quand, pendant que/when), detachment (de toutes fa?ons/anyway),
elaboration (en particulier/in particular), alternation (ou/or), commentary (au fait/by the way), rephras-
ing (du moins/at least), and evidence (effectivement/indeed).
We searched our syntactically parsed corpus for connectors. When a connector is found and its syn-
tactic category verified, if it is close enough to the root of the sentence (at most one dependency link
from the root), we look for an inter-sentential link. The first verb of our pair corresponds in this case
1. Available as an SQLite database at https://dl.dropboxusercontent.com/u/78938139/v2r_db
2. http://alpage.inria.fr/statgram/frdep/fr_stat_dep_parsing.html or (Candito et al., 2010)
3. Freely available at : https://gforge.inria.fr/frs/download.php/31052/lexconn.tar.gz.
4. We illustrate each relation with examples of potentially ambiguous markers.
2185
to the last verb of the previous sentence in the case of connectors for narration, or to its main verb for
all the other relations. We search for the second verb in the pair within a window of two dependency
links after the connector. If the connector is not close enough to the root of the sentence, we look for
a intra-sentential link. In this case, we look for the two verbs of the pair in the same sentence within a
forward and backward window of two dependency links.
If two verbs are found, we examine their local context to better characterize their usage and to improve
our results. If one of the verbs is a modal or support verb, we look for the verb dependent on the modal
or support verb and use that as the verb in our pair (if it exists), while keeping the presence of the
support verb in memory. Unlike support verbs, we use the presence of a negation or a reflexive particle
in the local context to distinguish verbs with different meanings ; e.g., comprendre/understand vs. ne pas
comprendre/not understand, agir/act vs. s?agir/concern are all distinct entries. To get at different verb
senses, we search for idiomatic usage of prepositions using the Dicovalence resource (Van Den Eynde
and Mertens, 2010), which contains valency frames for more than 3700 simple French verbs. We also
use the Lefff resource (Sagot, 2010) to find idiomatic verbal locutions. We also encode other information
that do not lead to distinct lexical entries : tense, and voice.
Once we have obtained a list of verb pairs associated with
Relation Distribution
contrast 50,104%
cause 33,108%
continuation 8,243%
narration 6,362%
background 1,853%
temporal localisation 0.177%
detachement 0.149%
elaboration 0.002%
alternation 0.002%
TABLE 1 ? Distribution of relations in
V
2
R ;commentary, reformulation and
evidence occur with negligible fre-
quency.
a connector, we aggregate this data to get a list of triple types
(verb1, verb2, relation). Given that we have used only unam-
biguous connectors (so classified by LEXCONN), the associ-
ation of a relation with a connector is immediate. We asso-
ciate to each triple type the number of intra-sentential, inter-
sentential and total number of occurrences. The other features
mentioned above are stored in a separate table.
Our method has isolated more than 1 million distinct types
of triples for V
2
R and 2 million occurrences, of which 95% are
intra-sentential
5
. Among these triples, 6.2% have 5 or more
occurrences.
Table 1 summarizes the distribution of triples by relation
in V
2
R. Note that triples with contrast and causal relations
comprise the majority. This does not mean that these are the
most frequent relations in the corpus but only that they are the
most frequently marked by the connectors we considered. This
makes for a very different distribution than that of the French manually annotated discourse corpus Ann-
odis (Afantenos et al., 2012).
3 Measuring the association of a pair of verbs with a relation
In the last section we presented our extraction method. We now present the measures we have used to
rank verb pairs with respect to the strength of their association with a particular discourse relation. We
adapted versions of standard lexical association measures like PMI (pointwise mutual information) and
their variants, as well as some measures specific to the association of a causal relation between items (Do
et al., 2011). We also experimented with a new measure specifically designed for our knowledge base.
Measures of lexical association used in research on co-occurrences in distributional semantics pick
out significant associations, taking into account the frequency of the related items. We examined over
10 measures ; we discuss the ones with the best results (see section 4). One simple measure, PMI, and
its variants, normalized, local (Evert, 2005), discounted (Lin and Pantel, 2002), which are designed
to reduce biases in the original measure, work well. The idea behind PMI is to estimate whether the
probability of the co-occurrence of two items is greater than the a priori probability of the two items
appearing independently. In distributional semantics, the measure is also used to estimate the significance
of two items co-occurring with a particular grammatical dependency relation like the subject or object
relation between an NP and a verb. This use of PMI measures over triples in distributional semantics
fits perfectly with our task of measuring the significance of triples consisting of a pair of verbs and
5. The low proportion of inter-sentential occurrences comes from our conservative scheme for finding these occurrences,
which uses only those connectors at the beginning of the second sentence. Other schemes are possible but would, we fear,
introduce too much noise into the data.
2186
a particular semantic or discourse relation ; our PMI measures estimate whether the co-occurrence of
two items with a particular discourse relation is higher than the a priori probability of the three items
occurring independently. Our measures consider co-occurrences of two lexical items in a certain relation
denoted by an explicit discourse marker. PMI and normalized PMI are defined as :
PMI = log(
P (V
1
, V
2
, R)
P (V
1
)? P (V
2
)? P (R)
)
PMI _normalized =
PMI
?2 log(P (V
1
, V
2
, R))
Indeed, when we have a complete co-occurrence of the three items, we have : P (V
1
) = P (V
2
) =
P (R) = P (V
1
, V
2
, R), and PMI = ?2 log(P (V
1
, V
2
, R)). The values of normalized PMI lie between
?1 and 1, approaching ?1 when the items never appear together, taking the value 0 in the case of
independence, and the value 1 when they always appear together. We also considered a weighted PMI
measure (Lin and Pantel, 2002) that corrects the bias of PMI for rare triples.
A specificity measure (Mirroshandel et al., 2013), originally used to measure the precision of subcat-
egorization frames, also performed well :
specificity =
1
3
? (
P (V
1
, V
2
, R)
?
i
P (V
1
, V
i
, R)
+
P (V
1
, V
2
, R)
?
i
P (V
i
, V
2
, R)
+
P (V
1
, V
2
, R)
?
i
P (V
1
, V
2
, R
i
)
)
A version of Do et al. (2011)?s measure for triples involving causal relations did not fare so well on
other types of relation. The definition of the measure can be found in (Do et al., 2011).
6
Finally, we investigated a measure that evaluates the contribution of each element in the triple to the
significance measure (this measure is similar to specificity).
W
combined
(V
1
, V
2
, R) =
1
3
(w
V
1
+ w
V
2
+ w
R
)
with : w
V
1
=
P (V
1
,V
2
,R)
max
i
(P (V
i
,V
2
,R))
, w
V
2
=
P (V
1
,V
2
,R)
max
i
(P (V
1
,V
i
,R))
, and w
R
=
P (V
1
,V
2
,R)
max
i
(P (V
1
,V
2
,R
i
))
.
4 Evaluating extracted relations
We evaluated V
2
R in several ways ; we provided : (i) an intrinsic evaluation of the relations between
verbs (section 4.1) and (ii) an extrinsic evaluation where we evaluated the coverage of the resource on a
discourse annotated corpus and its potential to help in predicting discourse relations in contexts with no
explicit marking (section 4.2).
4.1 Intrinsic evaluation
Our intrinsic evaluation first evaluates the feasibility of assigning an ?inherent? semantic link to a verb
pair, independently of any linguistic context. For example, is it possible to judge that there is a typical
causality link between push and fall, in scenarios where they share some arguments (subject, object, ...),
these scenarios being left to the annotator?s imagination (section 4.1.1). In a second stage, we selected
several verb pairs linked with different relations in V
2
R, and 40 contexts in which these verbs occur
together in the original corpus, to judge the semantic link in context (section 4.1.2).
In both cases we restricted the study to three relation groups : causal, contrastive, and narrative. These
are the most often marked relations and correspond to different types of links with a meaningful semantic
aspect (as opposed to the ?continuation? relation for instance, which is often marked too).
4.1.1 Out of context evaluation
For out of context judgments, we adopted the following protocol : one of the authors chose for each
relation 100 verbs with equivalent proportions of good and bad normalized PMI scores. Then the other
6. We simplified their measure by ignoring IDF (inverse document frequency) and the distance between the verbs, as neither
measure applies to our task.
2187
three authors judged the validity of associating each of the 300 pairs with the corresponding relation,
without any knowledge of the source of these pairs.
We measured the inter-annotator agreements with Cohen?s Kappa (Carletta, 1996), which resulted in :
0.17 for cause, 0.42 for narration and 0.56 for contrast as mean values. If a 0.6 kappa serves a measure for
a feasible semantic judgment task, out of context judgments appear very difficult, with only contrastive
pairs as a relative exception. We decided to only consider judgments about contrast, after an adjudication
phase, and we evaluated the measures presented in section 3 to see if they could discriminate between
the two verb groups, those judged positively or negatively according to human annotations. A Mann-
Whitney U statistical test showed all of our measures to be discriminative, with the exception of raw
co-occurrence counts for which p>0.05.
4.1.2 In context evaluation
We also judged associations in context.
Verb pair translation association
/human
Cause
inviter/souhaiter invite/wish 12.8%
promettre/?lire promise/elect 25.6%
aimer/trouver like/find 38.5%
b?n?ficier/cr?er benefit/create 51.3%
aider/gagner help/win 53.8%
Contrast
proposer/refuser propose/refuse 59.0%
augmenter/diminuer increase/decrease 64.1%
tenter/?chouer try/fail 64.1%
gagner/perdre win/lose 71.8%
autoriser/interdire authorize/forbid 74.4%
Narration
parler/r?fl?chir speak/think 42.5%
acheter/essayer buy/try 70.0%
atteindre/traverser reach/cross 77.5%
commencer/finir begin/end 80.0%
envoyer/transmettre send/transmit 82.5%
TABLE 2 ? For each relation, the list of verb pairs manu-
ally evaluated in context (and an approximate translation),
and the association percentage resulting from the adjudi-
cated human annotation.
This task was easier and also gave more
fine-grained results, because with it we can
quantify the degree of association, and the
typicality of the link, as a proportion of con-
texts where the two verbs appear together
in a given semantic relation. We can then
observe if this proportion is correlated with
the association measures we already pre-
sented. Nevertheless, this is a costly way of
evaluating a verb pair, as we require a num-
ber of judgments on each pair. It is also not
easy to sample the possible pairs with dif-
ferent values to be able to observe signif-
icant correlations, because we cannot pre-
dict in advance how they will be judged by
the annotators.
We selected 40 contexts for each of the
15 pairs of verbs we chose, 5 for each of the
target relation (cause, narration, contrast).
Selected pairs range over different values of
normalized PMI, again chosen by one of the
authors independently of the others, who
annotated the 600 contexts. Prior to adjudi-
cation, raw agreement was 78% on average,
for an average kappa of 0.46 (and a max-
imum of 0.49). These values seem moder-
ately good, as the task is also rather diffi-
cult.
Table 2 shows the results after adjudication : for each pair, the proportion of contexts in which the
considered relation is judged to appear.
We computed two correlation values between the association ratio in contexts manually annotated
and each association measure considered : one based on all annotated contexts, and one on the subset
of contexts devoid of explicit markers of a semantic relation (implicit contexts). The latter is important
to quantify the actual impact of the method, since explicit marking is already used as the basis of verb
association in the same corpus. Implicit contexts, however, never appeared in the computation of the verb
pair associations.
2188
normalized
PMI
specificity W_combined
discounted
PMI
PMI
local
PMI
U_do
raw fre-
quency
Global
correlation
0.749 0.747 0.720 0.716 0.709 0.434 0.376 0.170
Correlation
for implicit
instances
0.806 0.760 0.738 0.761 0.756 0.553 0.499 0.242
TABLE 3 ? Pearson correlation for the 15 pairs considered and measures from section 3, in decreasing
order.
Table 3 shows that mutual information measures are well correlated with human annotations, and
that our W_combined seems useful too. We also observed results on each relation separately, although
one should be careful drawing conclusions from these results since the correlations are then computed
on 5 points only. These results (not shown here) show a lot of variation between relations. The U_do
measure, designed for causal relations, does indeed produce good results for these relations, but does not
generalize well to our other chosen relations.
Also, local PMI seems to work very well on narration and causal relations. This needs to be confirmed
with more verb pairs.
We conclude that the best three measures are : normalized PMI, specificity, and W_combined. The last
two assign their maximal value to several pairs, so we used them in a lexicographical ordering to sort all
associated pairs, using normalized PMI to break ties.
Verb pair Translation Relation
abandonner / mener abandon / lead background
ne pas s?arr?ter / rouler not stop / drive narration
donner satisfaction sur / r??lire give satisfaction concerning / re-elect continuation
emporter / ne pas cesser take away / not stop summary
emprunter / assurer borrow / insure cause
ne pas manquer / prolonger not miss / prolong detachment
ratifier / trembler ratify / tremble background
avoir honte / faire piti? be ashamed / cause pity cause
avoir droit / cotiser pour be entitled / contribute to temploc
ne pas repr?senter / st?r?otyper not represent / stereotype temploc
TABLE 4 ? Ten best triples in the database.
Table 4 shows the best triples with our lexicographical ranking.
4.2 Extrinsic evaluation
In order to evaluate the performance of our resource relative to its main intended application?
predicting rhetorical relations in text, we intend to use our association measures as additional features
to an inductive prediction model. Whether this evaluation produces results depends on the proportion
of cases in which this information could help and on the coverage of our resource with respect to these
cases. We used the Annodis corpus (Afantenos et al., 2012), a set of French texts annotated with rhetori-
cal relations, for our study.
To improve existing models, a significant number of the predictions to be made must involve a verb
pair for which we have information in the resource. A first indication of its usefulness is also that the
verb pair appears most frequently with the relation group to which the annotation belongs, for instance
the fact that two verbs are related with a causal relation whenever we want to predict an explanation. This
is interesting only in the absence of an explicit marking of the target relation, i.e for implicit relations.
2189
Beyond that, it should be interesting to use all the available information about other semantic relations
too : for instance a potential causal link between two events could indicate the relevance of a temporal link
for the prediction of a relation. We relied again on the Lexconn marker database. As an approximation
we considered that a relation between two discourse units is explicit when a Lexconn marker is present
in any of the two segments, and one of the potential senses of the marker is the annotated relation.
This may overestimate the number of explicit instances but ensures that all implicit instances are indeed
implicit (assuming a good enough coverage of the marker resource). The Annodis corpus lists rhetorical
relations between elementary discourse units (EDUs), typically clauses, and complex discourse units
(sets of EDUs) ; as a simplification we only consider EDUs, since the question of what is a main verb of
a complex unit is difficult to answer. This is a relatively small corpus, as it includes about 2000 instances
of relations between elementary discourse units.
Table 5 present results for coverage, for the main relations in the annotated corpus. Note that only a
small part of the set of relations between EDUs is considered when we restrict instances to both EDUs
with verbs (about 20% of the whole). It turns out that a lot of EDUs in Annodis are short segments
(incises, detached segments, ...).
global narration cause contrast elab. cont. BG other
Annodis pairs 427 73 67 41 96 92 24 16
Annodis pairs ? V
2
R 68.9 71.2 70.8 78.0 68.3 61.9 74.1 62.5
Annodis triples ? V
2
R 26.5 34.2 50.0 70.7 0.0 20.6 11.1 0.0
Implicit Annodis pairs 83.4 71.2 79.2 36.6 99.0 94.8 88.9 100.0
Implicit Annodis pairs ? V
2
R
(any relation)
56.9 52.1 54.2 31.7 67.3 58.8 66.7 62.5
Implicit Annodis triples ?
V
2
R (with correct relation)
17.7 24.7 40.3 31.7 0.0 19.6 11.1 0.0
TABLE 5 ? Coverage of verb pairs in V
2
R with respect to EDU pairs in the Annodis corpus containing
two verbs. Except for the first line, all numbers are percentages. Pair = verb pairs in the EDUs linked
by a rhetorical relation R, Triple=verb pair associated with a relation R in V
2
R, BG = Background,
cont.=continuation, elab.=elaboration.
Our table includes : the proportion of verb pairs found in Annodis EDUs that appear in V
2
R, the
proportion of triples from Annodis that appear in V
2
R (with the correct relation), and the restriction
of these proportions to implicit contexts in Annodis. Except for a few exceptions due to lemmatisation
errors, all verbs in Annodis are in V
2
R in at least one pair, and we can see that the pairs in V
2
R cover
most of the pairs appearing in Annodis (almost 70% globally and between 60 and 80% depending on the
relation), and a little less of implicit cases (around 55% on average). We note that a high proportion of the
implicit cases contains verb pairs that have been collected in a marked context, even for rarely marked
relations like elaboration or continuation?contexts with these relations are the majority in Annodis.
Furthermore more than half of these contexts are associated with the right relation in V
2
R. Thus the
hypothesis of the partial redundancy of connectors appears useful when isolating verbal associations
relevant for discourse from a large corpus. We also looked at semantic neighbors of the verbs in V
2
R but
this did not increase coverage significantly.
A good test of the predictive power of the semantic information we gathered is also to include the
association measures as additional features to a predictive model, to improve classically low results
on implicit discourse relations. The only available discursive corpus in French, Annodis, is small, and
as shown above only about 400 instances have a verb in both related EDUs. We trained and tested
a maximum entropy model with and without the association measures as features, on top of features
presented in Muller et al. (2012), who trained a relation model on the same corpus. We did a 10-fold
cross-validation on the 400 instance subset as evaluation, and did not find a significant difference between
the two set-ups (F1 score was in the range .40?.42, similar to the cited paper), which is unsurprising
2190
given the size of the subset. We plan to evaluate our method relative to discourse parsing by building an
English resource like V
2
R ; we will then be able to use the much larger PDTB corpus (10 times as large
as Annodis) as a source of implicit discourse relations. This should prove a much more telling evaluation
of the usefulness of association measures in predicting implicit discourse relations.
5 Related work
There are two different groups of related work. The first group aims to alleviate the lack of annotated
data for discourse parsing by using a weakly supervised approach, exploiting the presence of discourse
connectors in a large non-annotated corpus. Each pair of elementary discourse units is automatically
annotated with the discourse relation triggered by the presence of the connector (connectors are often
filtered for non-discursive uses). Those connectors are afterwards eliminated from the corpus so that the
model trained on this dataset will not be informed by the presence of those connectors. The pioneering
article in this group is Marcu and Echihabi (2002). Such learning methods with such ?artificial data?
obtain low scores, barely above chance as shown in Sporleder and Lascarides (2008). Braud and Denis
(2013) observe that the performance of a classifier for the prediction of implicit relations is much lower
when using ?artificial? data than on ?natural? data (implicit relations annotated by a human being). They
propose a method which exploits these two different kinds of datasets together in various mixtures and
on the level of the prediction algorithm, obtaining thus a significant improvement on the Annodis corpus.
Our approach is different and complementary ; we isolate the semantic relations between pairs of verbs.
We can use that as a feature on discourse units for discourse parsing but it has other uses as well.
A second group aims at identifying discourse relations (implicit or not) by focusing on the use of fine-
grained lexical relations as another feature during the training phase. Most of this work focuses mainly
on the use of lexical relations between two verbs. Chklovski and Pantel (2004), for example, rely on
specific patterns constructed manually for each semantic relation between (similarity, strength, antonymy,
enablement and temporal happens-before). They use the web as a corpus in order to estimate the PMI
between a pattern and a pair of verbs (a precise measurement cannot be achieved over the web since the
probability of a pattern is not precisely known over all the web). A threshold on the value of the PMI
(manually fixed) permits thus to determine the pairs of verbs that are related to the relation denoted by the
pattern. In the same spirit, Kozareva (2012) is using a weakly supervised approach for the extraction of
pairs of verbs that are potentially implied in a cause-effect relation. Her method consists in using patterns
applied to the web in order to extract pairs and generate new seeds. Do et al. (2011) focus on causal
relations and take into account not only verbs but also event denoting nouns. According to this paper,
an event is denoted by a predicate with a specific number of arguments and thus the association of the
events is the sum of the association between predicates, between predicates and arguments and between
arguments. Their association measures are based on PMI and are quite complex. Our results show that
their measures do not generalize well to association with all discourse relations. Using Gigaword as a
corpus and a reimplementation of Lin et al. (2014) they have extracted discourse relations. An inductive
logic programming approach is finally used exploiting the interaction between causal pairs and discourse
relations in order to extract causal links. Those papers focus on specific relations with the exception of
Chklovski and Pantel (2004) who do not present a systematic evaluation of their results. An important
difference of our approach is also to consider predicates and their negation as separate entries.
Finally, we mention the approaches which while focusing on the learning of discourse structures,
nonetheless enrich their systems with lexical information. Feng and Hirst (2012) have used HILDA (Her-
nault et al., 2010) adding more features. A specific family of features represents lexical similarity based
on the hierarchical distance in VERBNET and WORDNET. In a similar fashion, Wellner et al. (2006) fo-
cus on intra-sentential discourse relations adding lexical information on the features based on measures
proposed by Lin (1998) calculated on the British National Corpus. Those approaches use thus only infor-
mation on lexical similarity without semantically typing this link. The impact of this information seems
limited. As far as evaluation is concerned, our method is similar to that followed in Tremper and Frank
(2013) for implication relations combining in and out of context evaluation for verbal associations. Their
inter-annotator agreement is similar to ours (0.42-0.44 of Kappa) with very different choices : the anno-
2191
tators were supposed to discriminate verbal links between the different possible sub-cases. The pairs of
verbs were identified by the system of Lin and Pantel. These authors also present a classification model
among the different types of relationships, assuming that two verbs are semantically related.
6 Conclusions
We have presented a knowledge base of triples involving pairs of verbs associated with semantic or
discourse relations. We extracted these triples from the large French corpus, frWaC, using discourse con-
nectors as markers of relations between two adjacent clauses containing verbs. We investigated several
measures to give the strength of association of a pair of verbs with a relation. We used manual annotations
to evaluate our method and select the best measures, and we also studied the coverage of our resource on
the discourse annotated corpus Annodis. Our positive results show our resource has the potential to help
discourse analysis as well as other semantically oriented tasks.
2192
References
Stergos Afantenos, Nicholas Asher, Farah Benamara, Myriam Bras, Cecile Fabre, Mai Ho-Dac, Anne Le Draoulec,
Philippe Muller, Marie-Paul Pery-Woodley, Laurent Prevot, Josette Rebeyrolles, Ludovic Tanguy, Marianne
Vergez-Couret, and Laure Vieu. 2012. An empirical resource for discovering cognitive principles of discourse
organisation : the ANNODIS corpus. In Nicoletta Calzolari, Khalid Choukri, Thierry Declerck, Mehmet U?gur
Do?gan, Bente Maegaard, Joseph Mariani, Jan Odijk, and Stelios Piperidis, editors, Proceedings of the Eight
International Conference on Language Resources and Evaluation (LREC?12), Istanbul, Turkey, may. European
Language Resources Association (ELRA).
Nicholas Asher and Alex Lascarides. 2003. Logics of Conversation. Studies in Natural Language Processing.
Cambridge University Press, Cambridge, UK.
Collin F. Baker, Charles J. Fillmore, and John B. Lowe. 1998. The Berkeley FrameNet Project. In Proceedings of
the COLING-ACL, Montreal, Canada.
Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and Eros Zanchetta. 2009. The wacky wide web : a collection
of very large linguistically processed web-crawled corpora. Language resources and evaluation, 43(3) :209?
226.
Chlo? Braud and Pascal Denis. 2013. Identification automatique des relations discursives "implicites" ? partir
de donn?es annot?es et de corpus bruts. In TALN - 20?me conf?rence du Traitement Automatique du Langage
Naturel 2013, volume 1, pages 104?117, Sables d?Olonne, France, June.
Marie Candito, Beno?t Crabb?, and Pascal Denis. 2010. Statistical french dependency parsing : Treebank conver-
sion and first results. In LREC.
Jean Carletta. 1996. Assessing agreement on classification tasks : the kappa statistic. Computational linguistics,
22(2) :249?254.
Nathanael Chambers and Dan Jurafsky. 2008. Unsupervised Learning of Narrative Event Chains. In Proceedings
of ACL-08 : HLT, pages 789?797, Columbus, Ohio, June. Association for Computational Linguistics, Morris-
town, NJ, USA.
Timothy Chklovski and Patrick Pantel. 2004. Verbocean : Mining the web for fine-grained semantic verb relations.
In Dekang Lin and Dekai Wu, editors, Proceedings of EMNLP 2004, pages 33?40, Barcelona, Spain, July.
Association for Computational Linguistics.
P. Denis and B. Sagot. 2012. Coupling an annotated corpus and a lexicon for state-of-the-art pos tagging. Lan-
guage Resources and Evaluation, (46) :721?736.
Quang Do, Yee Seng Chan, and Dan Roth. 2011. Minimally supervised event causality identification. In Proceed-
ings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 294?303, Edinburgh,
Scotland, UK., July. Association for Computational Linguistics.
Stefan Evert. 2005. The statistics of word cooccurrences. Ph.D. thesis, Stuttgart University.
C. Felbaum. 1998. Wordnet, an Electronic Lexical Database for English. Cambridge : MIT Press.
Vanessa Wei Feng and Graeme Hirst. 2012. Text-level discourse parsing with rich linguistic features. In Proceed-
ings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1 : Long Papers),
pages 60?68, Jeju Island, Korea, July. Association for Computational Linguistics.
G. Grefenstette. 1994. Explorations in automatic thesaurus discovery. Springer.
Chikara Hashimoto, Kentaro Torisawa, Kow Kuroda, Stijn De Saeger, Masaki Murata, and Jun?ichi Kazama. 2009.
Large-scale verb entailment acquisition from the Web. In Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing, pages 1172?1181, Singapore, August. Association for Computational
Linguistics.
Hugo Hernault, Helmut Prendinger, David A. duVerle, and Mitsuru Ishizuka. 2010. HILDA : A Discourse Parser
Using Support Vector Machine Classification. Dialogue and Discourse, 1(3) :1?33.
Zornitsa Kozareva. 2012. Cause-effect relation learning. In Workshop Proceedings of TextGraphs-7 : Graph-
based Methods for Natural Language Processing, pages 39?43, Jeju, Republic of Korea, July. Association for
Computational Linguistics.
Dekang Lin and Patrick Pantel. 2002. Concept discovery from text. In Proceedings of Coling 2002, pages 1?7.
Association for Computational Linguistics.
Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2014. A PDTB-styled end-to-end discourse parser. Natural
Language Engineering, 20(2) :151?184.
Dekang Lin. 1998. Automatic retrieval and clustering of similar words. In Proceedings of the 36th ACL and 17th
COLING joint conference, volume 2, pages 768?774, Montreal.
Maofu Liu, Wenjie Li, Mingli Wu, and Qin Lu. 2007. Extractive summarization based on event term clustering. In
Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume
Proceedings of the Demo and Poster Sessions, pages 185?188, Prague, Czech Republic, June. Association for
Computational Linguistics.
2193
Daniel Marcu and Abdessamad Echihabi. 2002. An Unsupervised Approach to Recognizing Discourse Relations.
In Proceedings of ACL, pages 368?375.
Seyed Abolghasem Mirroshandel, Alexis Nasr, and Beno?t Sagot. 2013. Enforcing subcategorization constraints in
a parser using sub-parses recombining. In Proceedings of the 2013 Conference of the North American Chapter
of the Association for Computational Linguistics : Human Language Technologies, pages 239?247, Atlanta,
Georgia, June. Association for Computational Linguistics.
Philippe Muller, Stergos Afantenos, Pascal Denis, and Nicholas Asher. 2012. Constrained decoding for text-
level discourse parsing. In Proceedings of COLING 2012, pages 1883?1900, Mumbai, India, December. The
COLING 2012 Organizing Committee.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev, G?lsen Eryigit, Sandra K?bler, Svetoslav Marinov, and
Erwin Marsi. 2007. Maltparser : A language-independent system for data-driven dependency parsing. Natural
Language Engineering, 13(2) :95?135.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Miltsakaki, Livio Robaldo, Aravind Joshi, and Bonnie L. Webber.
2008. The Penn Discourse TreeBank 2.0. In Proceedings of LREC 2008.
Charlotte Roze, Laurence Danlos, and Philippe Muller. 2012. Lexconn : A french lexicon of discourse connectives.
Discours, (10).
Beno?t Sagot. 2010. The lefff, a freely available and large-coverage morphological and syntactic lexicon for
french. In 7th international conference on Language Resources and Evaluation (LREC 2010).
Caroline Sporleder and Alex Lascarides. 2008. Using Automatically Labelled Examples to Classify Rhetorical
Relations : An Assessment. Natural Language Engineering, 14(3) :369?416, July.
Galina Tremper and Anette Frank. 2013. A discriminative analysis of fine-grained semantic relations including
presupposition : Annotation and classification. Dialogue & Discourse, 4(2) :282?322.
Naushad UzZaman, Hector Llorens, Leon Derczynski, James Allen, Marc Verhagen, and James Pustejovsky. 2013.
Semeval-2013 task 1 : Tempeval-3 : Evaluating time expressions, events, and temporal relations. In Second
Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2 : Proceedings of the Seventh
International Workshop on Semantic Evaluation (SemEval 2013), pages 1?9, Atlanta, Georgia, USA, June.
Association for Computational Linguistics.
K. Van Den Eynde and P. Mertens, 2010. Le dictionnaire de valence : Dicovalence. Leuven : Universit? de
Leuven. [http ://bach. arts. kuleuven. be/dicovalence/].
Ben Wellner, James Pustejovsky, Catherine Havasi, Anna Rumshisky, and Roser Saur?. 2006. Classification of
discourse coherence relations : an exploratory study using multiple knowledge sources. In Proceedings of
the 7th SIGdial Workshop on Discourse and Dialogue, SigDIAL ?06, pages 117?125, Stroudsburg, PA, USA.
Association for Computational Linguistics.
2194
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 479?488,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Predicting the relevance of distributional semantic similarity with
contextual information
Philippe Muller
IRIT, Toulouse University
Universit?e Paul Sabatier
118 Route de Narbonne
31062 Toulouse Cedex 04
philippe.muller@irit.fr
C
?
ecile Fabre
CLLE, Toulouse University
Universit?e Toulouse-Le Mirail
5 alles A. Machado
31058 Toulouse Cedex
cecile.fabre@univ-tlse2.fr
Cl
?
ementine Adam
CLLE, Toulouse University
Universit?e Toulouse-Le Mirail
5 alles A. Machado
31058 Toulouse Cedex
clementine.adam@univ-tlse2.fr
Abstract
Using distributional analysis methods to
compute semantic proximity links be-
tween words has become commonplace
in NLP. The resulting relations are often
noisy or difficult to interpret in general.
This paper focuses on the issues of eval-
uating a distributional resource and filter-
ing the relations it contains, but instead
of considering it in abstracto, we focus
on pairs of words in context. In a dis-
course, we are interested in knowing if the
semantic link between two items is a by-
product of textual coherence or is irrele-
vant. We first set up a human annotation
of semantic links with or without contex-
tual information to show the importance of
the textual context in evaluating the rele-
vance of semantic similarity, and to assess
the prevalence of actual semantic relations
between word tokens. We then built an ex-
periment to automatically predict this rel-
evance, evaluated on the reliable reference
data set which was the outcome of the first
annotation. We show that in-document in-
formation greatly improve the prediction
made by the similarity level alone.
1 Introduction
The goal of the work presented in this paper is to
improve distributional thesauri, and to help evalu-
ate the content of such resources. A distributional
thesaurus is a lexical network that lists semantic
neighbours, computed from a corpus and a simi-
larity measure between lexical items, which gen-
erally captures the similarity of contexts in which
the items occur. This way of building a seman-
tic network has been very popular since (Grefen-
stette, 1994; Lin, 1998), even though the nature of
the information it contains is hard to define, and
its evaluation is far from obvious. A distributional
thesaurus includes a lot of ?noise? from a seman-
tic point of view, but also lists relevant lexical pairs
that escape classical lexical relations such as syn-
onymy or hypernymy.
There is a classical dichotomy when evaluat-
ing NLP components between extrinsic and in-
trinsic evaluations (Jones, 1994), and this applies
to distributional thesauri (Curran, 2004; Poibeau
and Messiant, 2008). Extrinsic evaluations mea-
sure the capacity of a system in which a resource
or a component to evaluate has been used, for in-
stance in this case information retrieval (van der
Plas, 2008) or word sense disambiguation (Weeds
and Weir, 2005). Intrinsic evaluations try to mea-
sure the resource itself with respect to some hu-
man standard or judgment, for instance by com-
paring a distributional resource with respect to an
existing synonym dictionary or similarity judg-
ment produced by human subjects (Pado and La-
pata, 2007; Baroni and Lenci, 2010). The short-
comings of these methods have been underlined
in (Baroni and Lenci, 2011). Lexical resources
designed for other objectives put the spotlight on
specific areas of the distributional thesaurus. They
are not suitable for the evaluation of the whole
range of semantic relatedness that is exhibited by
distributional similarities, which exceeds the lim-
its of classical lexical relations, even though re-
searchers have tried to collect equivalent resources
manually, to be used as a gold standard (Weeds,
2003; Bordag, 2008; Anguiano et al, 2011). One
advantage of distributional similarities is to exhibit
a lot of different semantic relations, not necessar-
ily standard lexical relations. Even with respect
to established lexical resources, distributional ap-
proaches may improve coverage, complicating the
evaluation even more.
The method we propose here has been de-
signed as an intrinsic evaluation with a view to
validate semantic proximity links in a broad per-
479
spective, to cover what (Morris and Hirst, 2004)
call ?non classical lexical semantic relations?.
For instance, agentive relations (author/publish,
author/publication) or associative relations (ac-
tor/cinema) should be considered. At the same
time, we want to filter associations that can be
considered as accidental in a semantic perspective
(e.g. flag and composer are similar because they
appear a lot with nationality names). We do this
by judging the relevance of a lexical relation in a
context where both elements of a lexical pair oc-
cur. We show not only that this improves the relia-
bility of human judgments, but also that it gives a
framework where this relevance can be predicted
automatically. We hypothetize that evaluating and
filtering semantic relations in texts where lexical
items occur would help tasks that naturally make
use of semantic similarity relations, but assessing
this goes beyond the present work.
In the rest of this paper, we describe the re-
source we used as a case study, and the data we
collected to evaluate its content (section 2). We
present the experiments we set up to automatically
filter semantic relations in context, with various
groups of features that take into account informa-
tion from the corpus used to build the thesaurus
and contextual information related to occurrences
of semantic neighbours 3). Finally we discuss
some related work on the evaluation and improve-
ment of distributional resources (section 4).
2 Evaluation of lexical similarity in
context
2.1 Data
We use a distributional resource for French, built
on a 200M word corpus extracted from the French
Wikipedia, following principles laid out in (Bouri-
gault, 2002) from a structured model (Baroni
and Lenci, 2010), i.e. using syntactic con-
texts. In this approach, contexts are triples (gover-
nor,relation,dependent) derived from syntactic de-
pendency structures. Governors and dependents
are verbs, adjectives and nouns. Multiword units
are available, but they form a very small subset
of the resulting neighbours. Base elements in the
thesaurus are of two types: arguments (depen-
dents? lemma) and predicates (governor+relation).
This is to keep the predicate/argument distinction
since similarities will be computed between pred-
icate pairs or argument pairs, and a lexical item
can appear in many predicates and as an argument
(e.g. interest as argument, interest for as one pred-
icate). The similarity of distributions was com-
puted with Lin?s score (Lin, 1998).
We will talk of lexical neighbours or distribu-
tional neighbours to label pairs of predicates or ar-
guments, and in the rest of the paper we consider
only lexical pairs with a Lin score of at least 0.1,
which means about 1.4M pairs. This somewhat
arbitrary level is an a priori threshold to limit the
resulting database, and it is conservative enough
not to exclude potential interesting relations. The
distribution of scores is given figure 1; 97% of the
selected pairs have a score between 0.1 and 0.29.
Figure 1: Histogram of Lin scores for pairs con-
sidered.
To ease the use of lexical neighbours in our ex-
periments, we merged together predicates that in-
clude the same lexical unit, a posteriori. Thus
there is no need for a syntactic analysis of the con-
text considered when exploiting the resource, and
sparsity is less of an issue
1
.
2.2 Annotation
In order to evaluate the resource, we set up an an-
notation in context: pairs of lexical items are to
be judged in their context of use, in texts where
they occur together. To verify that this method-
ology is useful, we did a preliminary annotation
to contrast judgment on lexical pairs with or with-
out this contextual information. Then we made a
larger annotation in context once we were assured
of the reliability of the methodology.
For the preliminary test, we asked three annota-
tors to judge the similarity of pairs of lexical items
without any context (no-context), and to judge the
1
Whenever two predicates with the same lemma have
common neighbours, we average the score of the pairs.
480
[...] Le ventre de l?impala de m?eme que ses l`evres et sa queue sont blancs. Il faut aussi mentionner leurs lignes noires uniques
`a chaque individu au bout des oreilles , sur le dos de la queue et sur le front. Ces lignes noires sont tr`es utiles aux impalas
puisque ce sont des signes qui leur permettent de se reconnaitre entre eux. Ils poss`edent aussi des glandes s?ecr?etant des odeurs
sur les pattes arri`eres et sur le front. Ces odeurs permettent ?egalement aux individus de se reconnaitre entre eux. Il a ?egalement
des coussinets noirs situ?es, `a l?arri`ere de ses pattes . Les impalas m?ales et femelles ont une morphologie diff?erente. En effet,
on peut facilement distinguer un m?ale par ses cornes en forme de S qui mesurent de 40 `a 90 cm de long.
Les impalas vivent dans les savanes o`u l? herbe (courte ou moyenne) abonde. Bien qu?ils appr?ecient la proximit?e d?une source
d?eau, celle-ci n?est g?en?eralement pas essentielle aux impalas puisqu?ils peuvent se satisfaire de l?eau contenue dans l? herbe
qu?ils consomment. Leur environnement est relativement peu accident?e et n?est compos?e que d? herbes , de buissons ainsi que
de quelques arbres.
[...]
Figure 2: Example excerpt during the annotation of lexical pairs: annotators focus on a target item (here
corne, horn, in blue) and must judge yellow words (pending: oreille/queue, ear/tail), either validating
their relevance (green words: pattes, legs) or rejecting them (red words: herbe, grass). The text describes
the morphology of the impala, and its habitat.
similarity of pairs presented within a paragraph
where they both occur (in context). The three an-
notators were linguists, and two of them (1 and
3) knew about the resource and how it was built.
For each annotation, 100 pairs were randomly se-
lected, with the following constraints:
? for the no-context annotation, candidate pairs
had a Lin score above 0.2, which placed them
in the top 14% of lexical neighbours with re-
spect to the similarity level.
? for the in context annotation, the only con-
straint was that the pairs occur in the same
paragraph somewhere in the corpus used to
build the resource. The example paragraph
was chosen at random.
The guidelines given in both cases were the
same: ?Do you think the two words are seman-
tically close ? In other words, is there a seman-
tic relation between them, either classical (syn-
onymy, hypernymy, co-hyponymy, meronymy, co-
meronymy) or not (the relation can be paraphrased
but does not belong to the previous cases) ??
For the pre-test, agreement was rather moderate
without context (the average of pairwise kappas
was .46), and much better with a context (aver-
age = .68), with agreement rates above 90%. This
seems to validate the feasability of a reliable anno-
tation of relatedness in context, so we went on for
a larger annotation with two of the previous anno-
tators.
For the larger annotation, the protocol was
slightly changed: two annotators were given 42
full texts from the original corpus where lexical
neighbours occurred. They were asked to judge
the relation between two items types, regardless of
the number of occurrences in the text. This time
there was no filtering of the lexical pairs beyond
the 0.1 threshold of the original resource. We fol-
lowed the well-known postulate (Gale et al, 1992)
that all occurrences of a word in the same dis-
course tend to have the same sense (?one sense
per discourse?), in order to decrease the annotator
workload. We also assumed that the relation be-
tween these items remain stable within the docu-
ment, an arguably strong hypothesis that needed to
be checked against inter-annotator agreement be-
fore beginning the final annotation . It turns out
that the kappa score (0.80) shows a better inter-
annotator agreement than during the preliminary
test, which can be explained by the larger context
given to the annotator (the whole text), and thus
more occurrences of each element in the pair to
judge, and also because the annotators were more
experienced after the preliminary test. Agreement
measures are summed-up table 1. An excerpt of an
example text, as it was presented to the annotators,
is shown figure 2.
Overall, it took only a few days to annotate
9885 pairs of lexical items. Among the pairs that
were presented to the annotators, about 11% were
judged as relevant by the annotators. It is not
easy to decide if the non-relevant pairs are just
noise, or context-dependent associations that were
not present in the actual text considered (for pol-
ysemy reasons for instance), or just low-level as-
sociations. An important aspect is thus to guar-
antee that there is a correlation between the sim-
481
Annotators Non-contextual Contextual
Agreement rate Kappa Agreement rate Kappa
N1+N2 77% 0.52 91% 0.66
N1+N3 70% 0.36 92% 0.69
N2+N3 79% 0.50 92% 0.69
Average 75, 3% 0,46 91, 7% 0,68
Experts NA NA 90.8% 0.80
Table 1: Inter-annotator agreements with Cohen?s Kappa for contextual and non-contextual annotations.
N1, N2, N3 were annotators during the pre-test; expert annotation was made on a different dataset from
the same corpus, only with the full discourse context.
ilarity score (Lin?s score here), and the evaluated
relevance of the neighbour pairs. Pearson corre-
lation factor shows that Lin score is indeed sig-
nificantly correlated to the annotated relevance of
lexical pairs, albeit not strongly (r = 0.159).
The produced annotation
2
can be used as a ref-
erence to explore various aspects of distributional
resources, with the caveat that it is as such a bit
dependent on the particular resource used. We
nonetheless assume that some of the relevant pairs
would appear in other thesauri, or would be of in-
terest in an evaluation of another resource.
The first thing we can analyse from the anno-
tated data is the impact of a threshold on Lin?s
score to select relevant lexical pairs. The resource
itself is built by choosing a cut-off which is sup-
posed to keep pairs with a satisfactory similar-
ity, but this threshold is rather arbitrary. Figure
3 shows the influence of the threshold value to se-
lect relevant pairs, when considering precision and
recall of the pairs that are kept when choosing the
threshold, evaluated against the human annotation
of relevance in context. In case one wants to opti-
mize the F-score (the harmonic mean of precision
and recall) when extracting relevant pairs, we can
see that the optimal point is at .24 for a threshold
of .22 on Lin?s score. This can be considered as a
baseline for extraction of relevant lexical pairs, to
which we turn in the following section.
3 Experiments: predicting relevance in
context
The outcome of the contextual annotation pre-
sented above is a rather sizeable dataset of val-
idated semantic links, and we showed these lin-
guistic judgments to be reliable. We used this
2
Freely available here http://www.irit.fr/
?
Philippe.Muller/resources.html.
Figure 3: Precision and recall on relevant links
with respect to a threshold on the similarity mea-
sure (Lin?s score)
dataset to set up a supervised classification exper-
iment in order to automatically predict the rele-
vance of a semantic link in a given discourse. We
present now the list of features that were used for
the model. They can be divided in three groups,
according to their origin: they are computed from
the whole corpus, gathered from the distributional
resource, or extracted from the considered text
which contains the semantic pair to be evaluated.
3.1 Features
For each pair neighbour
a
/neighbour
b
, we com-
puted a set of features from Wikipedia (the corpus
used to derive the distributional similarity): We
first computed the frequencies of each item in the
corpus, freq
a
and freq
b
, from which we derive
? freq
min
, freq
max
: the min and max of
freq
a
and freq
b
;
? freq
?
: the combination of the two, or
log(freq
a
? freq
b
)
482
We also measured the syntagmatic association of
neighbour
a
and neighbour
b
, with a mutual infor-
mation measure (Church and Hanks, 1990), com-
puted from the cooccurrence of two tokens within
the same paragraph in Wikipedia. This is a rather
large window, and thus gives a good coverage
with respect to the neighbour database (70% of all
pairs).
A straightforward parameter to include to pre-
dict the relevance of a link is of course the simi-
larity measure itself, here Lin?s information mea-
sure. But this can be complemented by additional
information on the similarity of the neighbours,
namely:
? each neighbour productivity : prod
a
and
prod
b
are defined as the numbers of
neighbours of respectively neighbour
a
and
neighbour
b
in the database (thus related to-
kens with a similarity above the threshold),
from which we derive three features as for
frequencies: the min, the max, and the log
of the product. The idea is that neighbours
whith very high productivity give rise to less
reliable relations.
? the ranks of tokens in other related items
neighbours: rank
a?b
is defined as the rank of
neighbour
a
among neighbours of neighbour
b
ordered with respect to Lin?s score; rang
b?a
is defined similarly and again we consider
as features the min, max and log-product of
these ranks.
We add two categorial features, of a more linguis-
tic nature:
? cats is the pair of part-of-speech for the re-
lated items, e.g. to distinguish the relevance
of NN or VV pairs.
? predarg is related to the predicate/argument
distinction: are the related items predicates or
arguments ?
The last set of features derive from the occur-
rences of related tokens in the considered dis-
courses:
First, we take into account the frequencies of
items within the text, with three features as before:
the min of the frequencies of the two related items,
the max, and the log-product. Then we consider a
tf?idf (Salton et al, 1975) measure, to evaluate the
specificity and arguably the importance of a word
Feature Description
freq
min
min(freq
a
, freq
b
)
freq
max
max(freq
a
, freq
b
)
freq
?
log(freq
a
? freq
b
)
im im = log
P (a,b)
P (a)?P (b)
lin Lin?s score
rank
min
min(rank
a?b
, rank
b?a
)
rank
max
max(rank
a?b
, rank
b?a
)
rank
?
log(rank
a?b
? rank
b?a
)
prod
min
min(prod
a
, prod
b
)
prod
max
max(prod
a
, prod
b
)
prod
?
log(prod
a
? prod
b
)
cats neighbour pos pair
predarg predicate or argument
freqtxt
min
min(freqtxt
a
, freqtxt
b
)
freqtxt
max
max(freqtxt
a
, freqtxt
b
)
freqtxt
?
log(freqtxt
a
? freqstxt
b
)
tf?ipf tf?ipf (neighbour
a
)?tf?ipf (neighbour
b
)
copr
ph
copresence in a sentence
copr
para
copresence in a paragraph
sd smallest distance between
neighbour
a
and neighbour
b
gd highest distance between neighbour
a
and neighbour
b
ad average distance between neighbour
a
and neighbour
b
prodtxt
min
min(prod
a
, prod
b
)
prodtxt
max
max(prod
a
, prod
b
)
prodtxt
?
log(prod
a
? prod
b
)
cc belong to the same lexical connected
component
Table 2: Summary of features used in the super-
vised model, with respect to two lexical items a
and b. The first group is corpus related, the second
group is related to the distributional database, the
third group is related to the textual context. Freq
is related to the frequencies in the corpus, Freqtext
the frequencies in the considered text.
483
in a document or within a document. Several vari-
ants of tf?idf have been proposed to adapt the mea-
sure to more local areas in a text with respect to the
whole document. For instance (Dias et al, 2007)
propose a tf?isf (term frequency ? inverse sentence
frequency), for topic segmentation. We similarly
defined a tf?ipf measure based on the frequency of
a word within a paragraph with respect to its fre-
quency within the text. The resulting feature we
used is the product of this measure for neighbour
a
and neighbour
b
.
A few other contextual features are included in
the model: the distances between pairs of related
items, instantiated as:
? distance in words between occurrences of re-
lated word types:
? minimal distance between two occur-
rences (sd)
? maximal distance between two occur-
rences (gd)
? average distance (ad) ;
? boolean features indicating whether
neighbour
a
and neighbour
b
appear in
the same sentence (copr
s
) or the same
paragraph (copr
para
).
Finally, we took into account the network of re-
lated lexical items, by considering the largest sets
of words present in the text and connected in the
database (self-connected components), by adding
the following features:
? the degree of each lemma, seen as a node
in this similarity graph, combined as above
in minimal degree of the pair, maximal de-
gree, and product of degrees (prodtxt
min
,
prodtxt
max
, prodtxt
?
). This is the number
of pairs (present in the text) where a lemma
appears in.
? a boolean feature cc saying whether a lexi-
cal pair belongs to a connected component of
the text, except the largest. This reflects the
fact that a small component may concern a
lexical field which is more specific and thus
more relevant to the text.
Figure 4 shows examples of self-connected
components in an excerpt of the page on Go-
rille (gorilla), e.g. the set {pelage, dos, four-
rure} (coat, back, fur).
The last feature is probably not entirely indepen-
dent from the productivity of an item, or from the
tf.ipf measure.
Table 2 sums up the features used in our model.
3.2 Model
Our task is to identify relevant similarities between
lexical items, between all possible related pairs,
and we want to train an inductive model, a clas-
sifier, to extract the relevant links. We have seen
that the relevant/not relevant classification is very
imbalanced, biased towards the ?not relevant? cat-
egory (about 11%/89%), so we applied methods
dedicated to counter-balance this, and will focus
on the precision and recall of the predicted rele-
vant links.
Following a classical methodology, we made a
10-fold cross-validation to evaluate robustly the
performance of the classifiers. We tested a few
popular machine learning methods, and report on
two of them, a naive bayes model and the best
method on our dataset, the Random Forest clas-
sifier (Breiman, 2001). Other popular methods
(maximum entropy, SVM) have shown slightly in-
ferior combined F-score, even though precision
and recall might yield more important variations.
As a baseline, we can also consider a simple
threshold on the lexical similarity score, in our
case Lin?s measure, which we have shown to yield
the best F-score of 24% when set at 0.22.
To address class imbalance, two broad types of
methods can be applied to help the model focus
on the minority class. The first one is to resam-
ple the training data to balance the two classes,
the second one is to penalize differently the two
classes during training when the model makes a
mistake (a mistake on the minority class being
made more costly than on the majority class). We
tested the two strategies, by applying the classical
Smote method of (Chawla et al, 2002) as a kind
of resampling, and the ensemble method Meta-
Cost of (Domingos, 1999) as a cost-aware learn-
ing method. Smote synthetizes and adds new in-
stances similar to the minority class instances and
is more efficient than a mere resampling. Meta-
Cost is an interesting meta-learner that can use
any classifier as a base classifier. We used Weka?s
implementations of these methods (Frank et al,
2004), and our experiments and comparisons are
thus easily replicated on our dataset, provided with
this paper, even though they can be improved by
484
Le gorille est apr`es le bonobo et le chimpanz?e , du point de vue g?en?etique , l? animal le plus proche
de l? humain . Cette parent?e a ?et?e confirm?ee par les similitudes entre les chromosomes et les groupes
sanguins . Notre g?enome ne diff`ere que de 2 % de celui du gorille .
Redress?es , les gorilles atteignent une taille de 1,75 m`etre , mais ils sont en fait un peu plus grands car
ils ont les genoux fl?echis . L? envergure des bras d?epasse la longueur du corps et peut atteindre 2,75
m`etres .
Il existe une grande diff?erence de masse entre les sexes : les femelles p`esent de 90 `a 150 kilogrammes
et les m?ales jusqu? `a 275. En captivit?e , particuli`erement bien nourris , ils atteignent 350 kilogrammes
.
Le pelage d?epend du sexe et de l? ?age . Chez les m?ales les plus ?ag?es se d?eveloppe sur le dos une
fourrure gris argent?e , d? o`u leur nom de ?dos argent?es? . Le pelage des gorilles de montagne est
particuli`erement long et soyeux .
Comme tous les anthropodes , les gorilles sont d?epourvus de queue . Leur anatomie est puissante , le
visage et les oreilles sont glabres et ils pr?esentent des torus supra-orbitaires marqu?es .
Figure 4: A few connected lexical components of the similarity graph, projected on a text, each in a
different color. The groups are, in order of appearance of the first element: {genetic, close, human},
{similarity, kinship}, {chromosome, genome}, {male, female}, {coat, back, fur}, {age/N, aged/A},
{ear, tail, face}. The text describes the gorilla species, more particularly its morphology. Gray words are
other lexical elements in the neighbour database.
refinements of these techniques. We chose the
following settings for the different models: naive
bayes uses a kernel density estimation for numer-
ical features, as this generally improves perfor-
mance. For Random Forests, we chose to have ten
trees, and each decision is taken on a randomly
chosen set of five features. For resampling, Smote
advises to double the number of instances of the
minority class, and we observed that a bigger re-
sampling degrades performances. For cost-aware
learning, a sensible choice is to invert the class ra-
tio for the cost ratio, i.e. here the cost of a mistake
on a relevant link (false negative) is exactly 8.5
times higher than the cost on a non-relevant link
(false positive), as non-relevant instances are 8.5
times more present than relevant ones.
3.3 Results
We are interested in the precision and recall for
the ?relevant? class. If we take the best simple
classifier (random forests), the precision and re-
call are 68.1% and 24.2% for an F-score of 35.7%,
and this is significantly beaten by the Naive Bayes
method as precision and recall are more even (F-
score of 41.5%). This is already a big improve-
ment on the use of the similarity measure alone
(24%). Also note that predicting every link as rel-
evant would result in a 2.6% precision, and thus a
5% F-score. The random forest model is signifi-
cantly improved by the balancing techniques: the
overall best F-score of 46.3% is reached with Ran-
dom Forests and the cost-aware learning method.
Table 3 sums up the scores for the different con-
figurations, with precision, recall, F-score and the
confidence interval on the F-score. We analysed
the learning curve by doing a cross-validation on
reduced set of instances (from 10% to 90%); F1-
scores range from 37.3% with 10% of instances
and stabilize at 80%, with small increment in ev-
ery case.
The filtering approach we propose seems to
yield good results, by augmenting the similarity
built on the whole corpus with signals from the lo-
cal contexts and documents where related lexical
items appear together.
To try to analyse the role of each set of fea-
tures, we repeated the experiment but changed the
set of features used during training, and results are
shown table 4 for the best method (RF with cost-
aware learning).
We can see that similarity-related features (mea-
sures, ranks) have the biggest impact, but the other
ones also seem to play a significant role. We can
draw the tentative conclusion that the quality of
distributional relations depends on the contextual-
izing of the related lexical items, beyond just the
similarity score and the ranks of items as neigh-
bours of other items.
485
Method Precision Recall F-score CI
Baseline (Lin threshold) 24.0 24.0 24.0
RF 68.1 24.2 35.7 ? 3.4
NB 34.8 51.3 41.5 ? 2.6
RF+resampling 56.6 32.0 40.9 ? 3.3
NB+resampling 32.8 54.0 40.7 ? 2.5
RF+cost aware learning 40.4 54.3 46.3 ? 2.7
NB+cost aware learning 27.3 61.5 37.8 ? 2.2
Table 3: Classification scores (%) on the relevant class. CI is the confidence interval on the F-score (RF
= Random Forest, NB= naive bayes).
Features Prec. Recall F-score
all 40.4 54.3 46.3
all ? corpus feat. 37.4 52.8 43.8
all ? similarity feat. 36.1 49.5 41.8
all ? contextual feat. 36.5 54.8 43.8
Table 4: Impact of each group of features on the best scores (%) : the lowest the results, the bigger the
impact of the removed group of features.
4 Related work
Our work is related to two issues: evaluating dis-
tributional resources, and improving them. Eval-
uating distributional resources is the subject of a
lot of methodological reflection (Sahlgren, 2006),
and as we said in the introduction, evaluations can
be divided between extrinsic and intrinsic evalua-
tions. In extrinsic evaluations, models are evalu-
ated against benchmarks focusing on a single task
or a single aspect of a resource: either discrimina-
tive, TOEFL-like tests (Freitag et al, 2005), anal-
ogy production (Turney, 2008), or synonym selec-
tion (Weeds, 2003; Anguiano et al, 2011; Fer-
ret, 2013; Curran and Moens, 2002). In intrin-
sic evaluations, associations norms are used, such
as the 353 word-similarity dataset (Finkelstein et
al., 2002), e.g. (Pado and Lapata, 2007; Agirre et
al., 2009), or specifically designed test cases, as
in (Baroni and Lenci, 2011). We differ from all
these evaluation procedures as we do not focus on
an essential view of the relatedness of two lexical
items, but evaluate the link in a context where the
relevance of the link is in question, an ?existential?
view of semantic relatedness.
As for improving distributional thesauri, out-
side of numerous alternate approaches to the
construction, there is a body of work focusing
on improving an existing resource, for instance
reweighting context features once an initial the-
saurus is built (Zhitomirsky-Geffet and Dagan,
2009), or post-processing the resource to filter bad
neighbours or re-ranking neighbours of a given
target (Ferret, 2013). They still use ?essential?
evaluation measures (mostly synonym extraction),
although the latter comes close to our work since
it also trains a model to detect (intrinsically) bad
neighbours by using example sentences with the
words to discriminate. We are not aware of any
work that would try to evaluate differently seman-
tic neighbours according to the context they ap-
pear in.
5 Conclusion
We proposed a method to reliably evaluate distri-
butional semantic similarity in a broad sense by
considering the validation of lexical pairs in con-
texts where they both appear. This helps cover non
classical semantic relations which are hard to eval-
uate with classical resources. We also presented a
supervised learning model which combines global
features from the corpus used to built a distribu-
tional thesaurus and local features from the text
where similarities are to be judged as relevant or
not to the coherence of a document. It seems
from these experiments that the quality of distri-
butional relations depends on the contextualizing
of the related lexical items, beyond just the simi-
486
larity score and the ranks of items as neighbours of
other items. This can hopefully help filter out lex-
ical pairs when word lexical similarity is used as
an information source where context is important:
lexical disambiguation (Miller et al, 2012), topic
segmentation (Guinaudeau et al, 2012). This can
also be a preprocessing step when looking for sim-
ilarities at higher levels, for instance at the sen-
tence level (Mihalcea et al, 2006) or other macro-
textual level (Agirre et al, 2013), since these are
always aggregation functions of word similarities.
There are limits to what is presented here: we need
to evaluate the importance of the level of noise in
the distributional neighbours database, or at least
the quantity of non-semantic relations present, and
this depends on the way the database is built. Our
starting corpus is relatively small compared to cur-
rent efforts in this framework. We are confident
that the same methodology can be followed, even
though the quantitative results may vary, since it
is independent of the particular distributional the-
saurus we used, and the way the similarities are
computed.
References
E. Agirre, E. Alfonseca, K. Hall, J. Kravalova,
M. Pas?ca, and A. Soroa. 2009. A study on similar-
ity and relatedness using distributional and wordnet-
based approaches. In Proceedings of Human Lan-
guage Technologies: The 2009 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics, pages 19?27. Asso-
ciation for Computational Linguistics.
Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-
Agirre, and Weiwei Guo. 2013. *sem 2013 shared
task: Semantic textual similarity. In Second Joint
Conference on Lexical and Computational Seman-
tics (*SEM), Volume 1: Proceedings of the Main
Conference and the Shared Task: Semantic Textual
Similarity, pages 32?43, Atlanta, Georgia, USA,
June. Association for Computational Linguistics.
E.H. Anguiano, P. Denis, et al 2011. FreDist: Au-
tomatic construction of distributional thesauri for
French. In Actes de la 18eme conf?erence sur
le traitement automatique des langues naturelles,
pages 119?124.
M. Baroni and A. Lenci. 2010. Distributional mem-
ory: A general framework for corpus-based seman-
tics. Computational Linguistics, 36(4):673?721.
M. Baroni and A. Lenci. 2011. How we BLESSed dis-
tributional semantic evaluation. GEMS 2011, pages
1?10.
Stefan Bordag. 2008. A comparison of co-occurrence
and similarity measures as simulations of context.
In Alexander F. Gelbukh, editor, CICLing, volume
4919 of Lecture Notes in Computer Science, pages
52?63. Springer.
D. Bourigault. 2002. UPERY : un outil
d?analyse distributionnelle tendue pour la construc-
tion d?ontologies partir de corpus. In Actes de la
9e confrence sur le Traitement Automatique de la
Langue Naturelle, pages 75?84, Nancy.
Leo Breiman. 2001. Random forests. Machine Learn-
ing, 45(1):5?32.
Nitesh V. Chawla, Kevin W. Bowyer, Lawrence O.
Hall, and W. Philip Kegelmeyer. 2002. Smote: Syn-
thetic minority over-sampling technique. J. Artif. In-
tell. Res. (JAIR), 16:321?357.
Kenneth Church and Patrick Hanks. 1990. Word as-
sociation norms, mutual information, and lexicogra-
phy. Computational Linguistics, 16(1):pp. 22?29.
James R. Curran and Marc Moens. 2002. Improve-
ments in automatic thesaurus extraction. In Pro-
ceedings of the ACL-02 Workshop on Unsupervised
Lexical Acquisition, pages 59?66.
J.R. Curran. 2004. From distributional to semantic
similarity. Ph.D. thesis, University of Edinburgh.
Ga?el Dias, Elsa Alves, and Jos?e Gabriel Pereira Lopes.
2007. Topic segmentation algorithms for text sum-
marization and passage retrieval: an exhaustive eval-
uation. In Proceedings of the 22nd national confer-
ence on Artificial intelligence - Volume 2, AAAI?07,
pages 1334?1339. AAAI Press.
Pedro Domingos. 1999. Metacost: A general method
for making classifiers cost-sensitive. In Usama M.
Fayyad, Surajit Chaudhuri, and David Madigan, ed-
itors, KDD, pages 155?164. ACM.
Olivier Ferret. 2013. Identifying bad semantic neigh-
bors for improving distributional thesauri. In Pro-
ceedings of the 51st Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 1: Long
Papers), pages 561?571, Sofia, Bulgaria, August.
Association for Computational Linguistics.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Ey-
tan Ruppin. 2002. Placing search in context: the
concept revisited. ACM Trans. Inf. Syst., 20(1):116?
131.
Eibe Frank, Mark Hall, , and Len Trigg. 2004.
Weka 3.3: Data mining software in java.
www.cs.waikato.ac.nz/ml/weka/.
Dayne Freitag, Matthias Blume, John Byrnes, Ed-
mond Chow, Sadik Kapadia, Richard Rohwer, and
Zhiqiang Wang. 2005. New experiments in distri-
butional representations of synonymy. In Proceed-
ings of CoNLL, pages 25?32, Ann Arbor, Michigan,
June. Association for Computational Linguistics.
487
W. Gale, K. Church, and D. Yarowsky. 1992. One
sense per discourse. In In Proceedings of the 4th
DARPA Speech and Natural Language Workshop,
New-York, pages 233?237.
G. Grefenstette. 1994. Explorations in automatic the-
saurus discovery. Kluwer Academic Pub., Boston.
Camille Guinaudeau, Guillaume Gravier, and Pascale
S?ebillot. 2012. Enhancing lexical cohesion measure
with confidence measures, semantic relations and
language model interpolation for multimedia spo-
ken content topic segmentation. Computer Speech
& Language, 26(2):90?104.
Karen Sparck Jones. 1994. Towards better NLP sys-
tem evaluation. In Proceedings of the Human Lan-
guage Technology Conference, pages 102?107. As-
sociation for Computational Linguistics.
D. Lin. 1998. An information-theoretic definition of
similarity. In Proceedings of the 15th International
Conference on Machine Learning, pages 296?304,
Madison.
Rada Mihalcea, Courtney Corley, and Carlo Strappa-
rava. 2006. Corpus-based and knowledge-based
measures of text semantic similarity. In Proceedings
of the 21st national conference on Artificial intel-
ligence, AAAI06, volume 1, pages 775?780. AAAI
Press.
Tristan Miller, Chris Biemann, Torsten Zesch, and
Iryna Gurevych. 2012. Using distributional similar-
ity for lexical expansion in knowledge-based word
sense disambiguation. In Proceedings of COLING
2012, pages 1781?1796, Mumbai, India, December.
The COLING 2012 Organizing Committee.
J. Morris and G. Hirst. 2004. Non-classical lexical se-
mantic relations. In Proceedings of the HLT Work-
shop on Computational Lexical Semantics, pages
46?51, Boston.
Sebastian Pado and Mirella Lapata. 2007.
Dependency-based construction of semantic space
models. Computational Linguistics, 33(2):161?199.
Thierry Poibeau and C?edric Messiant. 2008. Do we
still Need Gold Standards for Evaluation? In Pro-
ceedings of the Language Resource and Evaluation
Conference.
Magnus Sahlgren. 2006. Towards pertinent evalua-
tion methodologies for word-space models. In In
Proceedings of the 5th International Conference on
Language Resources and Evaluation.
G. Salton, C. S. Yang, and C. T. Yu. 1975. A theory
of term importance in automatic text analysis. Jour-
nal of the American Society for Information Science,
26(1):33?44.
Peter D. Turney. 2008. A uniform approach to analo-
gies, synonyms, antonyms, and associations. In
Proceedings of the 22nd International Conference
on Computational Linguistics - Volume 1, COLING
?08, pages 905?912, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
L. van der Plas. 2008. Automatic Lexico-Semantic Ac-
quisition for Question Answering. Ph.D. thesis, Uni-
versity of Groningen.
J. Weeds and D. Weir. 2005. Co-occurrence retrieval:
A flexible framework for lexical distributional simi-
larity. Computational Linguistics, 31(4):439?475.
Julie Elizabeth Weeds. 2003. Measures and Appli-
cations of Lexical Distributional Similarity. Ph.D.
thesis, University of Sussex.
Maayan Zhitomirsky-Geffet and Ido Dagan. 2009.
Bootstrapping distributional feature vector quality.
Computational Linguistics, 35(3):435?461.
488
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 98?102, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
MELODI: Semantic Similarity of Words and Compositional Phrases
using Latent Vector Weighting
Tim Van de Cruys
IRIT, CNRS
tim.vandecruys@irit.fr
Stergos Afantenos
IRIT, Toulouse University
stergos.afantenos@irit.fr
Philippe Muller
IRIT, Toulouse University
philippe.muller@irit.fr
Abstract
In this paper we present our system for the
SemEval 2013 Task 5a on semantic similar-
ity of words and compositional phrases. Our
system uses a dependency-based vector space
model, in combination with a technique called
latent vector weighting. The system computes
the similarity between a particular noun in-
stance and the head noun of a particular noun
phrase, which was weighted according to the
semantics of the modifier. The system is en-
tirely unsupervised; one single parameter, the
similarity threshold, was tuned using the train-
ing data.
1 Introduction
In the course of the last two decades, vector space
models have gained considerable momentum for se-
mantic processing. Initially, these models only dealt
with individual words, ignoring the context in which
these words appear. More recently, two different but
related approaches emerged that take into account
the interaction between different words within a par-
ticular context. The first approach aims at building a
joint, compositional representation for larger units
beyond the individual word level (e.g., the com-
posed, semantic representation of the noun phrase
crispy chips). The second approach, different but re-
lated to the first one, computes the specific meaning
of a word within a particular context (e.g. the mean-
ing of the noun bank in the context of the adjective
bankrupt).
In this paper, we describe our system for the Sem-
Eval 2013 Task 5a: semantic similarity of words and
compositional phrases ? which follows the latter ap-
proach. Our system uses a dependency-based vector
space model, in combination with a technique called
latent vector weighting (Van de Cruys et al, 2011).
The system computes the similarity between a par-
ticular noun instance and the head noun of a par-
ticular noun phrase, which was weighted according
to the semantics of the modifier. The system is en-
tirely unsupervised; one single parameter, the simi-
larity threshold, was tuned using the training data.
2 Related work
In recent years, a number of methods have been de-
veloped that try to capture the compositional mean-
ing of units beyond the individual word level within
a distributional framework. One of the first ap-
proaches to tackle compositional phenomena in a
systematic way is Mitchell and Lapata?s (2008) ap-
proach. They explore a number of different mod-
els for vector composition, of which vector addition
(the sum of each feature) and vector multiplication
(the elementwise multiplication of each feature) are
the most important. Baroni and Zamparelli (2010)
present a method for the composition of adjectives
and nouns. In their model, an adjective is a linear
function of one vector (the noun vector) to another
vector (the vector for the adjective-noun pair). The
linear transformation for a particular adjective is rep-
resented by a matrix, and is learned automatically
from a corpus, using partial least-squares regression.
Coecke et al (2010) present an abstract theoretical
framework in which a sentence vector is a function
of the Kronecker product of its word vectors, which
allows for greater interaction between the different
98
word features. And Socher et al (2012) present a
model for compositionality based on recursive neu-
ral networks.
Closely related to the work on compositionality
is research on the computation of word meaning in
context. Erk and Pado? (2008, 2009) make use of
selectional preferences to express the meaning of
a word in context. And Dinu and Lapata (2010)
propose a probabilistic framework that models the
meaning of words as a probability distribution over
latent factors. This allows them to model contex-
tualized meaning as a change in the original sense
distribution.
Our work takes the latter approach of computing
word meaning in context, and is described in detail
below.
3 Methodology
Our method uses latent vector weighting (Van de
Cruys et al, 2011) in order to compute a se-
mantic representation for the meaning of a word
within a particular context. The method relies
upon a factorization model in which words, together
with their window-based context features and their
dependency-based context features, are linked to la-
tent dimensions. The factorization model allows us
to determine which dimensions are important for a
particular context, and adapt the dependency-based
feature vector of the word accordingly. The mod-
ified feature vector is then compared to the target
noun feature vector with the cosine similarity func-
tion.
This following sections describe our model in
more detail. In section 3.1, we describe non-
negative matrix factorization ? the factorization
technique that our model uses. Section 3.2 describes
our way of combining dependency-based context
features and window-based context features within
the same factorization model. Section 3.3, then, de-
scribes our method of computing the meaning of a
word within a particular context.
3.1 Non-negative Matrix Factorization
Our latent model uses a factorization technique
called non-negative matrix factorization (Lee and
Seung, 2000) in order to find latent dimensions. The
key idea is that a non-negative matrix A is factorized
into two other non-negative matrices, W and H
Ai? j ?Wi?kHk? j (1)
where k is much smaller than i, j so that both in-
stances and features are expressed in terms of a few
components. Non-negative matrix factorization en-
forces the constraint that all three matrices must be
non-negative, so all elements must be greater than or
equal to zero.
Using the minimization of the Kullback-Leibler
divergence as an objective function, we want to find
the matrices W and H for which the divergence
between A and WH (the multiplication of W and
H) is the smallest. This factorization is carried
out through the iterative application of update rules.
Matrices W and H are randomly initialized, and the
rules in 2 and 3 are iteratively applied ? alternating
between them. In each iteration, each vector is ade-
quately normalized, so that all dimension values sum
to 1.
Ha? ?Ha?
?i Wia
Ai?
(WH)i?
?k Wka
(2)
Wia?Wia
?? Ha?
Ai?
(WH)i?
?v Hav
(3)
3.2 Combining syntax and context words
Using an extension of non-negative matrix fac-
torization (Van de Cruys, 2008), it is possible
to jointly induce latent factors for three different
modes: nouns, their window-based context words,
and their dependency-based context features. The
intuition is that the window-based context words
inform us about broad, topical similarity, whereas
the dependency-based features get at a tighter,
synonym-like similarity. As input to the algo-
rithm, two matrices are constructed that capture the
pairwise co-occurrence frequencies for the different
modes. The first matrix contains co-occurrence fre-
quencies of words cross-classified by dependency-
based features, and the second matrix contains co-
occurrence frequencies of words cross-classified by
words that appear in the word?s context window.
NMF is then applied to the two matrices, and the
separate factorizations are interleaved (i.e. matrix
W, which contains the nouns by latent dimensions,
99
is shared between both factorizations). A graphical
representation of the interleaved factorization algo-
rithm is given in figure 1. The numbered arrows in-
dicate the sequence of the updates.
= W=
U
I
V
K
I
Anouns xdependencies
Bnouns xcontext wordsI
HK U
3
21
4 GK V
Figure 1: A graphical representation of the interleaved
NMF
When the factorization is finished, the three dif-
ferent modes (words, window-based context words
and dependency-based context features) are all rep-
resented according to a limited number of latent fac-
tors.
The factorization that comes out of the NMF
model can be interpreted probabilistically (Gaussier
and Goutte, 2005; Ding et al, 2008). More specifi-
cally, we can transform the factorization into a stan-
dard latent variable model of the form
p(wi,d j) =
K
?
z=1
p(z)p(wi|z)p(d j|z) (4)
by introducing two K?K diagonal scaling matrices
X and Y, such that Xkk = ?i Wik and Ykk = ? j Hk j.
The factorization WH can then be rewritten as
WH = (WX?1X)(YY?1H)
= (WX?1)(XY)(Y?1H)
(5)
such that WX?1 represents p(wi|z), (Y?1H)T rep-
resents p(d j|z), and XY represents p(z). Using
Bayes? theorem, it is now straightforward to deter-
mine p(z|d j).
p(z|d j) =
p(d j|z)p(z)
p(d j)
(6)
3.3 Meaning in Context
3.3.1 Overview
Using the results of the factorization model de-
scribed above, we can now adapt a word?s feature
vector according to the context in which it appears.
Intuitively, the context of the word (in our case,
the dependency-based context feature that acts as an
adjectival modifier to the head noun) pinpoint the
important semantic dimensions of the particular in-
stance, creating a probability distribution over latent
factors. The required probability vector, p(z|d j), is
yielded by our factorization model. This probabil-
ity distribution over latent factors can be interpreted
as a semantic fingerprint of the passage in which the
target word appears. Using this fingerprint, we can
now determine a new probability distribution over
dependency features given the context.
p(d|d j) = p(z|d j)p(d|z) (7)
The last step is to weight the original probability
vector of the word according to the probability vec-
tor of the dependency features given the word?s con-
text, by taking the pointwise multiplication of prob-
ability vectors p(d|wi) and p(d|d j).
p(d|wi,d j) = p(d|wi) ? p(d|d j) (8)
Note that this final step is a crucial one in our
approach. We do not just build a model based on
latent factors, but we use the latent factors to de-
termine which of the features in the original word
vector are the salient ones given a particular context.
This allows us to compute an accurate adaptation of
the original word vector in context.
3.3.2 Example
Let us exemplify the procedure with an example.
Say we want to compute the distributionally similar
words to the noun instrument within the phrases (1)
and (2), taken from the task?s test set:
(1) musical instrument
(2) optical instrument
First, we extract the context feature for both in-
stances, in this case C1 = {musicalad j} for phrase
(1), and C2 = {opticalad j} for phrase (2). Next, we
100
look up p(z|C1) and p(z|C2) ? the probability distri-
butions over latent factors given the context ? which
are yielded by our factorization model. Using these
probability distributions over latent factors, we can
now determine the probability of each dependency
feature given the different contexts ? p(d|C1) and
p(d|C2) (equation 7).
The former step yields a general probability dis-
tribution over dependency features that tells us how
likely a particular dependency feature is given the
context that our target word appears in. Our last step
is now to weight the original probability vector of
the target word (the aggregate of dependency-based
context features over all contexts of the target word)
according to the new distribution given the context
in which the target word appears (equation 8).
We can now return to our original matrix A and
compute the top similar words for the two adapted
vectors of instrument given the different contexts,
which yields the results presented below.
1. instrumentN , C1: percussion, flute, violin,
melody, harp
2. instrumentN , C2: sensor, detector, amplifier,
device, microscope
3.4 Implementational details
Our model has been trained on the UKWaC cor-
pus (Baroni et al, 2009). The corpus has been
part of speech tagged and lemmatized with Stan-
ford Part-Of-Speech Tagger (Toutanova and Man-
ning, 2000; Toutanova et al, 2003), and parsed with
MaltParser (Nivre et al, 2006) trained on sections
2-21 of the Wall Street Journal section of the Penn
Treebank extended with about 4000 questions from
the QuestionBank1, so that dependency triples could
be extracted.
The matrices needed for our interleaved NMF fac-
torization are extracted from the corpus. Our model
was built using 5K nouns, 80K dependency relations,
and 2K context words2 (excluding stop words) with
highest frequency in the training set, which yields
matrices of 5K nouns ? 80K dependency relations,
and 5K nouns ? 2K context words.
1http://maltparser.org/mco/english_parser/
engmalt.html
2We used a fairly large, paragraph-like window of four sen-
tences.
model accuracy precision recall F1
dist .69 .83 .48 .61
lvw .75 .84 .61 .71
Table 1: Results of the distributional model (dist) and la-
tent vector weighting model (lvw) on the SemEval task
5a
The interleaved NMF model was carried out using
K = 600 (the number of factorized dimensions in the
model), and applying 100 iterations. The interleaved
NMF algorithm was implemented in Matlab; the pre-
processing scripts and scripts for vector computation
in context were written in Python.
The model is entirely unsupervised. The only pa-
rameter to set, the cosine similarity threshold ? , is
induced from the training set. We set ? = .049.
4 Results
Table 1 shows the evaluation results of the simple
distributional model (which only takes into account
the head noun) and our model that uses latent vector
weighting. The results indicate that our model based
on latent vector weighting performs quite a bit bet-
ter than a standard dependency-based distributional
model. The lvw model attains an accuracy of .75 ?
a 6% improvement over the distributional model ?
and an F-measure of .71 ? a 10% improvement over
the distributional model.
5 Conclusion
In this paper we presented an entirely unsuper-
vised system for the assessment of the similarity of
words and compositional phrases. Our system uses a
dependency-based vector space model, in combina-
tion with latent vector weighting. The system com-
putes the similarity between a particular noun in-
stance and the head noun of a particular noun phrase,
which was weighted according to the semantics of
the modifier. Using our system yields a substantial
improvement over a simple dependency-based dis-
tributional model, which only takes the head noun
into account.
101
References
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Proceedings of the 2010 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1183?1193, Cambridge, MA, October. Association for
Computational Linguistics.
Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and
Eros Zanchetta. 2009. The wacky wide web: A
collection of very large linguistically processed web-
crawled corpora. Language Resources and Evalua-
tion, 43(3):209?226.
Bob Coecke, Mehrnoosh Sadrzadeh, and Stephen Clark.
2010. Mathematical foundations for a compositional
distributed model of meaning. Lambek Festschrift,
Linguistic Analysis, vol. 36, 36.
Chris Ding, Tao Li, and Wei Peng. 2008. On the equiv-
alence between non-negative matrix factorization and
probabilistic latent semantic indexing. Computational
Statistics & Data Analysis, 52(8):3913?3927.
Georgiana Dinu and Mirella Lapata. 2010. Measuring
distributional similarity in context. In Proceedings of
the 2010 Conference on Empirical Methods in Natural
Language Processing, pages 1162?1172, Cambridge,
MA, October.
Katrin Erk and Sebastian Pado?. 2008. A structured
vector space model for word meaning in context. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, pages 897?906,
Waikiki, Hawaii, USA.
Katrin Erk and Sebastian Pado?. 2009. Paraphrase as-
sessment in structured vector space: Exploring param-
eters and datasets. In Proceedings of the Workshop on
Geometrical Models of Natural Language Semantics,
pages 57?65, Athens, Greece.
Eric Gaussier and Cyril Goutte. 2005. Relation between
PLSA and NMF and implications. In Proceedings of
the 28th annual international ACM SIGIR conference
on Research and development in information retrieval,
pages 601?602, Salvador, Brazil.
Daniel D. Lee and H. Sebastian Seung. 2000. Algo-
rithms for non-negative matrix factorization. In Ad-
vances in Neural Information Processing Systems 13,
pages 556?562.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. proceedings of ACL-
08: HLT, pages 236?244.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2006. Malt-
parser: A data-driven parser-generator for dependency
parsing. In Proceedings of LREC-2006, pages 2216?
2219.
Richard Socher, Brody Huval, Christopher D. Manning,
and Andrew Y. Ng. 2012. Semantic compositional-
ity through recursive matrix-vector spaces. In Pro-
ceedings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning, pages 1201?
1211, Jeju Island, Korea, July. Association for Com-
putational Linguistics.
Kristina Toutanova and Christopher D. Manning. 2000.
Enriching the knowledge sources used in a maximum
entropy part-of-speech tagger. In Proceedings of the
Joint SIGDAT Conference on Empirical Methods in
Natural Language Processing and Very Large Corpora
(EMNLP/VLC-2000), pages 63?70.
Kristina Toutanova, Dan Klein, Christopher Manning,
and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In Pro-
ceedings of HLT-NAACL 2003, pages 252?259.
Tim Van de Cruys, Thierry Poibeau, and Anna Korho-
nen. 2011. Latent vector weighting for word meaning
in context. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Processing,
pages 1012?1022, Edinburgh, Scotland, UK., July. As-
sociation for Computational Linguistics.
Tim Van de Cruys. 2008. Using three way data for word
sense discrimination. In Proceedings of the 22nd In-
ternational Conference on Computational Linguistics
(Coling 2008), pages 929?936, Manchester.
102
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 144?147, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
MELODI: A Supervised Distributional Approach
for Free Paraphrasing of Noun Compounds
Tim Van de Cruys
IRIT, CNRS
tim.vandecruys@irit.fr
Stergos Afantenos
IRIT, Toulouse University
stergos.afantenos@irit.fr
Philippe Muller
IRIT, Toulouse University
philippe.muller@irit.fr
Abstract
This paper describes the system submitted
by the MELODI team for the SemEval-2013
Task 4: Free Paraphrases of Noun Compounds
(Hendrickx et al, 2013). Our approach com-
bines the strength of an unsupervised distri-
butional word space model with a supervised
maximum-entropy classification model; the
distributional model yields a feature represen-
tation for a particular compound noun, which
is subsequently used by the classifier to induce
a number of appropriate paraphrases.
1 Introduction
Interpretation of noun compounds is making explicit
the relation between the component nouns, for in-
stance that running shoes are shoes used in running
activities, while leather shoes are made from leather.
The relations can have very different meanings, and
existing work either postulates a fixed set of rela-
tions (Tratz and Hovy, 2010) or relies on appropri-
ate descriptions of the relations, through constrained
verbal paraphrases (Butnariu et al, 2010) or uncon-
strained paraphrases as in the present campaign. The
latter is much simpler for annotation purposes, but
raises difficult challenges involving not only com-
pound interpretation but also paraphrase evaluation
and ranking.
In terms of constrained verbal paraphrases
Wubben (2010), for example, uses a supervised
memory-based ranker using features from the
Google n-gram corpus as well as WordNet. Nulty
and Costello (2010) rank paraphrases of compounds
according to the number of times they co-occurred
with other paraphrases for other compounds. They
use these co-occurrences to compute conditional
probabilities estimating is-a relations between para-
phrases. Li et al (2010) provide a hybrid sys-
tem which combines a Bayesian algorithm exploit-
ing Google n-grams, a score which captures human
preferences at the tail distribution of the training
data, as well as a metric that captures pairwise para-
phrase preferences.
Our methodology consists of two steps. First,
an unsupervised distributional word space model is
constructed, which yields a feature representation
for a particular compound. The feature representa-
tion is then used by a maximum entropy classifier to
induce a number of appropriate paraphrases.
2 Methodology
2.1 Distributional word space model
In order to induce appropriate feature representa-
tions for the various noun compounds, we start by
constructing a standard distributional word space
model for nouns. We construct a co-occurrence
matrix of the 5K most frequent nouns1 by the 2K
most frequent context words2, which occur in a win-
dow of 5 words to the left and right of the target
word. The bare frequencies of the word-context ma-
trix are weighted using pointwise mutual informa-
tion (Church and Hanks, 1990).
Next, we compute a joint, compositional repre-
sentation of the noun compound, combining the se-
1making sure all nouns that appear in the training and test
set are included
2excluding the 50 most frequent context words as stop words
144
mantics of the head noun with the modifier noun. To
do so, we make use of a simple vector-based multi-
plicative model of compositionality, as proposed by
Mitchell and Lapata (2008). In order to compute the
compositional representation of a compound noun,
this model takes the elementwise multiplication of
the vectors for the head noun and the modifier noun,
i.e.
pi = uivi
for each feature i. The resulting features are used as
input to our next classification step.
We compare the performance of the abovemen-
tioned compositional model with a simpler model
that only takes into account the semantics of the
head noun. This model only uses the context fea-
tures for the head noun as input to our second clas-
sification step. This means that the model only takes
into account the semantics of the head noun, and ig-
nores the semantics of the modifier noun.
2.2 Maximum entropy classification
The second step of our paraphrasing system consists
of a supervised maximum entropy classification ap-
proach. Training vectors for each noun compound
from the training set are constructed according to
the approach described in the previous section. The
(non-zero) context features yielded by the first step
are used as input for the maximum entropy classi-
fier, together with the appropriate paraphrase labels
and the label counts (used to weight the instances),
which are extracted from the training set.
We then deploy the model in order to induce a
probability distribution over the various paraphrase
labels. Every paraphrase label above a threshold ? is
considered an appropriate paraphrase. Using a por-
tion of held-out training data (20%), we set ? = 0.01
for our official submission. In this paper, we show a
number of results using different thresholds.
2.3 Set of paraphrases labels
For our classification approach to work, we need to
extract an appropriate set of paraphrase labels from
the training data. In order to create this set, we
substitute the nouns that appear in the training set?s
paraphrases by dummy variables. Table 1 gives an
example of three different paraphrases and the re-
sulting paraphrase labels after substitution. Note
that we did not apply any NLP techniques to prop-
erly deal with inflected words.
We apply a frequency threshold of 2 (counted over
all the instances), so we discard paraphrase labels
that appear only once in the training set. This gives
us a total of 285 possible paraphrase labels.
One possible disadvantage of this supervised ap-
proach is a loss of recall on unseen paraphrases. A
rough estimation shows that our set of training labels
accounts for only 25% of the similarly constructed
labels extracted from the test set. However, the most
frequently used paraphrase labels are present in both
training and test set, so this does not prevent our
system to come up with a number of suitable para-
phrases for the test set.
2.4 Implementational details
All frequency co-occurrence information has been
extracted from the ukWaC corpus (Baroni et al,
2009). The corpus has been part of speech tagged
and lemmatized with Stanford Part-Of-Speech Tag-
ger (Toutanova and Manning, 2000; Toutanova et
al., 2003). Distributional word space algorithms
have been implemented in Python. The maximum
entropy classifier was implemented using the Maxi-
mum Entropy Modeling Toolkit for Python and C++
(Le, 2004).
3 Results
Table 2 shows the results of the different systems in
terms of the isomorphic and non-isomorphic evalu-
ation measures defined by the task organizers (Hen-
drickx et al, 2013). For comparison, we include a
number of baselines. The first baseline assigns the
two most frequent paraphrase labels (Y of X, Y for
X) to each test instance; the second baseline assigns
the four most frequent paraphrase labels (Y of X, Y
for X, Y on X, Y in X); and the third baseline assigns
all of the possible 285 paraphrase labels as correct
answer for each test instance.
For both our primary system (the multiplicative
model) and our contrastive system (the head noun
model), we vary the threshold used to select the final
set of paraphrases. A threshold ? = 0.01 results in
a smaller set of paraphrases, whereas a threshold of
? = 0.001 results in a broad set of paraphrases. Our
official submission uses the former threshold.
145
compound paraphrase paraphrase label
textile company company that makes textiles Y that makes Xs
textile company company that produces textiles Y that produces Xs
textile company company in textile industry Y in X industry
Table 1: Example of induced paraphrase labels
model ? isomorphic non-isomorphic
baseline (2) ? .058 .808
baseline (4) ? .090 .633
baseline (all) ? .332 .200
multiplicative .01 .130 .548
.001 .270 .259
head noun .01 .136 .536
.001 .277 .302
Table 2: Results
First of all, we note that the different baseline
models are able to obtain substantial scores for the
different evaluation measures. The first two base-
lines, which use a limited number of paraphrase
labels, perform very well in terms of the non-
isomorphic evaluation measure. The third baseline,
which uses a very large number of candidate para-
phrase labels, gets more balanced results in terms of
both the isomorphic and non-isomorphic measure.
Considering our different thresholds, the results
of our models are in line with the baseline re-
sults. A larger threshold, which results in a smaller
number of paraphrase labels, reaches a higher non-
isomorphic score. A smaller threshold, which re-
sults in a larger number of paraphrase labels, gives
more balanced results for the isomorphic and non-
isomorphic measure.
There does not seem to be a significant difference
between our primary system (multiplicative) and our
contrastive system (head noun). For ? = 0.01, the
results of both models are very similar; for ? =
0.001, the head noun model reaches slightly better
results, in particular for the non-isomorphic score.
Finally, we note that our models do not seem to
improve significantly on the baseline scores. For
? = 0.001, the results of our models seem somewhat
more balanced compared to the all baseline, but the
differences are not very large. In general, our sys-
tems (in line with the other systems participating in
the task) seem to have a hard time beating a num-
ber of simple baselines, in terms of the evaluation
measures defined by the task.
4 Conclusion
We have presented a system for producing free para-
phrases of noun compounds. Our methodology con-
sists of two steps. First, an unsupervised distribu-
tional word space model is constructed, which is
used to compute a feature representation for a par-
ticular compound. The feature representation is then
used by a maximum entropy classifier to induce a
number of appropriate paraphrases.
Although our models do seem to yield slightly
more balanced scores than the baseline models, the
differences are not very large. Moreover, there is
no substantial difference between our primary mul-
tiplicative model, which takes into account the se-
mantics of both head and modifier noun, and our
contrastive model, which only uses the semantics of
the head noun.
References
Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and
Eros Zanchetta. 2009. The wacky wide web: A
collection of very large linguistically processed web-
crawled corpora. Language Resources and Evalua-
tion, 43(3):209?226.
Cristina Butnariu, Su Nam Kim, Preslav Nakov, Diar-
muid O? Se?aghdha, Stan Szpakowicz, and Tony Veale.
2010. Semeval-2 task 9: The interpretation of noun
compounds using paraphrasing verbs and prepositions.
In Proceedings of the 5th International Workshop on
Semantic Evaluation, pages 39?44, Uppsala, Sweden,
July. Association for Computational Linguistics.
Kenneth W. Church and Patrick Hanks. 1990. Word as-
sociation norms, mutual information & lexicography.
Computational Linguistics, 16(1):22?29.
146
Iris Hendrickx, Zornitsa Kozareva, Preslav Nakov, Diar-
muid O? Se?aghdha, Stan Szpakowicz, and Tony Veale.
2013. SemEval-2013 task 4: Free paraphrases of noun
compounds. In Proceedings of the International Work-
shop on Semantic Evaluation, SemEval ?13, June.
Zhang Le. 2004. Maximum entropy modeling toolkit for
python and c++. http://homepages.inf.ed.ac.
uk/lzhang10/maxent_toolkit.html.
Guofu Li, Alejandra Lopez-Fernandez, and Tony Veale.
2010. Ucd-goggle: A hybrid system for noun com-
pound paraphrasing. In Proceedings of the 5th In-
ternational Workshop on Semantic Evaluation, pages
230?233, Uppsala, Sweden, July. Association for
Computational Linguistics.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. proceedings of ACL-
08: HLT, pages 236?244.
Paul Nulty and Fintan Costello. 2010. Ucd-pn: Select-
ing general paraphrases using conditional probability.
In Proceedings of the 5th International Workshop on
Semantic Evaluation, pages 234?237, Uppsala, Swe-
den, July. Association for Computational Linguistics.
Kristina Toutanova and Christopher D. Manning. 2000.
Enriching the knowledge sources used in a maximum
entropy part-of-speech tagger. In Proceedings of the
Joint SIGDAT Conference on Empirical Methods in
Natural Language Processing and Very Large Corpora
(EMNLP/VLC-2000), pages 63?70.
Kristina Toutanova, Dan Klein, Christopher Manning,
and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In Pro-
ceedings of HLT-NAACL 2003, pages 252?259.
Stephen Tratz and Eduard Hovy. 2010. A taxonomy,
dataset, and classifier for automatic noun compound
interpretation. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics, pages 678?687, Uppsala, Sweden, July. Associa-
tion for Computational Linguistics.
Sander Wubben. 2010. Uvt: Memory-based pairwise
ranking of paraphrasing verbs. In Proceedings of the
5th International Workshop on Semantic Evaluation,
pages 260?263, Uppsala, Sweden, July. Association
for Computational Linguistics.
147
Proceedings of the SIGDIAL 2013 Conference, pages 2?11,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
Expressivity and comparison of models of discourse structure
Antoine Venant1 Nicholas Asher2 Philippe Muller1 Pascal Denis3 Stergos Afantenos1
(1) IRIT, Toulouse University, France, (2) IRIT, CNRS, France (3) Mostrare, INRIA, France ?
Abstract
Several discourse annotated corpora now ex-
ist for NLP. But they use different, not eas-
ily comparable annotation schemes: are the
structures these schemes describe incompati-
ble, incomparable, or do they share interpre-
tations? In this paper, we relate three types
of discourse annotation used in corpora or dis-
course parsing: (i) RST, (ii) SDRT, and (iii)
dependency tree structures. We offer a com-
mon language in which their structures can be
defined and furnished a range of interpreta-
tions. We define translations between RST and
DT preserving these interpretations, and intro-
duce a similarity measure for discourse repre-
sentations in these frameworks. This will en-
able researchers to exploit different types of
discourse annotated data for automated tasks.
1 Introduction
Computer scientists and linguists now largely agree
that representing discourse structure as a hierarchical
relational structure over discourse units linked by dis-
course relations is appropriate to account for a variety
of interpretative tasks. There is also some agreement
over the taxonomy of discourse relations ?almost all
current theories include expressions that refer to rela-
tions like Elaboration, Explanation, Result, Narration,
Contrast, Attribution. Sanders, Spooren, and Noord-
man 1992; Bateman and Rondhuis 1997 discuss corre-
spondences between different taxonomies.
Different theories, however, assume different sets of
constraints that govern these representations; some ad-
vocate trees: RST Mann and Thompson 1987, DLTAG
Webber et al 1999; others, graphs of different sorts:
SDRT Asher and Lascarides 2003, Graphbank Wolf
and Gibson 2005. Consider:
(1) [?he was a very aggressive firefighter.]C1 [he
loved the work he was in,?]C2 [said acting fire
chief Lary Garcia.]C3 . [?He couldn?t be bested
in terms of his willingness and his ability to do
something to help you survive.?]C4 (from Egg
and Redeker 2010)
Using RST, Egg and Redeker 2010 provide the tree an-
notated with nuclearity features for this example (given
by the linear encoding in (s1)), while SDRT provides
?This research was supported by ERC grant 269427.
a different kind of structure (s2). Dependency trees
(DTs), similar to syntactic dependency trees and used
in Muller et al 2012 for automated parsing, give yet an-
other representation (s3). Elab stands for elaboration,
Attr for attribution, and Cont for continuation.
Elab1(Attr(Elab2(C1N ,C2S )N ,C3S )N ,C4S ) (s1)
Attr(pi,C3) ? pi :Elab(C1, pi1) ? pi1 :Cont(C2,C4) (s2)
Elab1(C1,C2) ? Attr(C1,C3) ? Elab(C1,C4) (s3)
Several corpora now exist annotated with such struc-
tures: RSTTB Carlson, Marcu, and Okurowski 2002,
Discor Baldridge, Asher, and Hunter 2007, Graph-
Bank1. But how exactly do these annotations compare?
In the illustrative example chosen and for the relation
types they agree on (Elaboration and Attribution), dif-
ferent annotation models and theoretical frameworks
invoke different numbers of instances of these relations
and assign the instances different arguments or differ-
ent scopes, at least on the surface. In this paper we de-
velop a method of comparing the scopes of relations in
different types of structures by developing a notion of
interpretation shared between different structures. This
interpretation specifies the set of possible scopes of re-
lations compatible with a given structure. This theoret-
ical work is important for furthering empirical research
on discourse. Discourse annotations are expensive. It
behooves researchers to use as much data as they can,
annotated in several formalisms, while pursuing pre-
diction or evaluation in their chosen theory. This paper
provides a theoretical basis to do this.
What a given structure expresses exactly is often not
clear; some discourse theories are not completely for-
malized or lack a worked out semantics. Neverthe-
less, in all of them rhetorical relations have semantic
consequences bearing on tasks like text summarization,
textual entailment, anaphora resolution, as well as the
temporal, spatial and thematic organization of a text
Hobbs, Stickel, and Martin 1993; Kehler 2002; Asher
1993; Lascarides and Asher 1993; Hobbs, Stickel, and
Martin 1993; Hitzeman, Moens, and Grover 1995, inter
alia. Theories like SDRT or Polanyi et al 2004 adopt a
conception of discourse structure as logical form. Dis-
course structures are like logical formulae and relations
1The Penn Discourse Treebank Prasad et al 2008 could
also be considered as a corpus with partial dependency struc-
tures.
2
function like logical operators on the meaning of their
arguments. Hence their exact scope has great semantic
impact on the phenomena we have mentioned, in ex-
actly the way the relative scope of quantifiers make a
great semantic difference in first order logic. By con-
centrating on exact meaning representations, however,
the syntax-semantics interface becomes quite complex:
as happens with quantifiers at the intra sentential level,
discourse relations might semantically require a scope
that is, at least a priori, not determined by syntactic
considerations alone and violates surface order (see s2).
Other theories like Polanyi?s Linguistic Discourse
Model (LDM) of Polanyi 1985; Polanyi and Scha 1984,
and DLTAG Webber et al 1999 explicitly adopt a
syntactic point of view, and RST with strongly con-
strained (tree-shaped) structures is subject to parsing
approaches duVerle and Prendinger 2009; Sagae 2009;
Subba and Di Eugenio 2009 that adhere to the syntac-
tic approach in adopting decoding strategies of syntac-
tic parsing. In such theories, discourse structure repre-
sentations, subject to syntactic constraints (e.g. domi-
nance of spans of text one over another) respect surface
order but do not always and unproblematically yield a
semantic interpretation that fits intuitions. According
to Marcu 1996, an RST tree is not by itself sufficient to
generate desired predictions; he employs the nuclearity
principle, NP, as an additional interpretation principle
on scopes of relations.
We focus on two theories: RST, which offers the
model for the annotations of the RST treebank Carl-
son, Marcu, and Okurowski 2002 and the Potsdam
commentary corpus Stede 2004, and on SDRT, which
counts several small corpora annotated with semantic
scopes, Discor Baldridge, Asher, and Hunter 2007 and
Annodis Afantenos et al 2012. We describe these the-
ories in section 2. We will also compare these two the-
ories to dependency tree representations of discourse
Muller et al 2012. Section 3 introduces a language for
describing semantics scopes of relations that is power-
ful enough to: i) compare the expressiveness (in terms
of what different scopes can be expressed) of the dif-
ferent formalisms considered; ii) give a formal target
language that will provide comparable interpretations
of the different structures at stake. Section 4 discusses
Marcu?s nuclearity principle and proposes an alterna-
tive way to interpret an RST tree as a set of different
possible scopes expressed in our language. Section 5
provides intertranslability results between the different
formalisms. Section 6 defines a measure of similarity
over discourse structures in different formalisms.
2 Discourse formalisms
These formalisms we introduce here all require the in-
put text to be segmented into elementary units (EDUs).
The definition of what an EDU is varies slightly with
the formalism, but roughly corresponds to the clause
level in RST, SDRT and other theories. We assume a
segmentation common to the different formalisms and
use examples with a non controversial and intuitive
segmentation.
Rhetorical Structure Theory (RST), the theory un-
derlying the RST-Treebank is the most used corpus for
discourse parsing, cf. duVerle and Prendinger 2009,
Subba and Di Eugenio 2009, inter alia.
In its Mann and Thompson 1987 formulation, RST
builds a descriptive tree for the discourse by the recur-
sive application of schemata in a bottom-up procedure.
Each schema application ideally reflects the most plau-
sible relation the writer intended between two contigu-
ous spans of text, as well as hierarchical information
about the arguments of the relation, distinguishing be-
tween nuclei as essential arguments of a relation and
satellites as more contingent parts. The set of RS Trees
is inductively defined as follows:
1- An EDU is a RS Tree.
2- if R is a nucleus-statellite relation symbol, s1 and
s2 are both RS Trees with contiguous spans (the left-
most leaf in s2 is textually located right after the right-
most one in s1), and ?a1, a2? ? {?N, S ?; ?S ,N?} then
R(t1 a1, t2 a2) is an RS Tree.
3- if R is a multinuclear relation symbol and
?s1, . . . , sn? are n RS Trees with contiguous spans then
R(s1 N, . . . , sn N) is an RS Tree.
Following Mann and Thompson 1987 a complete RS
tree makes explicit the content the author intended to
communicate. RS Trees are graphically represented
Marcu 1996 with intermediate nodes labelled with re-
lation names, leaves with symbols referring to EDUs,
and edges with nucleus/satellite distinctions.
Segmented Discourse Representation Theory
(SDRT), our second case-study theory, inherits a
framework from dynamic semantics and enriches
it with rhetorical relations. The set of SDRSs is
inductively defined as follows:
Assume a set of rhetorical relations R, distinguished
between coordinating and subordinating relations.
- Any EDU is an SDRS.
- Any Complex Discourse Unit (CDU) is a SDRS.
- a CDU is an acyclic labelled graph (A, E) where
every node is a discourse unit (DU) or SDRS and each
labelled edge is a discourse relation such that:
(i) every node is connected to some other node;
(ii) no two nodes are linked by subordinating and co-
ordinating relations,
(iii) given EDUs a1, . . . , an+1 in their textual order
that yield a CDU (A, E) = G, each EDU a j+1 j < n is
linked either: (a) to nodes on the right frontier of the
CDU G? a subgraph of G constructed from a1, . . . , a j;
or (b) to one or more nodes in G? = (A?,G?), a subgraph
of G, which linked to one or more nodes on the right
frontier of the graph G?, and where G? is constructed
from a subset of a j+2, . . . an.
The right frontier of a graph G consists of the nodes
a that are not the left arguments to any coordinating
relation and for which if any node b is linked to some
node dominating a, then there is a path of subordinating
3
relations from b to a.
A Segmented Discourse Representation Structure
(SDRS), is assigned a recursively computed meaning
in terms of context-change potential (relation between
pairs of ? world, assignation function ?) in the tradi-
tion of dynamic semantics. The semantics of a complex
constituent is compositionally defined from the seman-
tics of rhetorical relations and the interpretation of its
subconstituents. In the base case of an EDU, the se-
mantics is given in dynamic semantics.
We also consider dependency trees (DTs). Muller
et al 2012 derive DTs from the SDRSs of the ANN-
ODIS corpus to get a reduced search space, simplify-
ing automated discourse parsing. A DT is an SDRS
in which there are no CDUs and there is a unique arc
between any two nodes. Muller et al 2012 provide
a procedure from SDRSs to DTs, which we slightly
modify to respect the Frontier Contraint that they use.
? works in a bottom-up fashion replacing every CDU
X that is an argument of a rhetorical relation in ? by
their top-most immediate sub-constituent which do not
appear on the right of any relation in X, or distributing
the top relation when necessary to preserve projectivity.
To give a simple example: ?(R([R?(a, [R??(b, c)])], d)) =
?(R([R?(a, b) ? R??(b, c)], d)) = R(a, d) ? R?(a, b) ?
R??(b, c). (1) provides a more complicated example we
discuss in Section 6).
3 Describing the scope of relations
We provide here a language expressive and general
enough to express the structures of the 3 theories. All
our case-study theories involve structures described by
a list of rhetorical relations and their arguments. Two
things may vary: first, the nature of the arguments.
SDRT for instance, introduces complex constituents
as arguments of relations (e.g.
{
pi : Rsubord(b, c)
Rsubord(a, pi) ),
which finds a counterpart within RS Trees, where a
relation may directly appear as argument of another
(R(aN ,R(bN , cS )S )) but not within dependency trees.
Second, the set of constraints that restrict the possi-
ble lists of such relations can vary across theories (e.g.
right frontier, or requirement for a tree structure).
To deal with the first point above, we remark that
it suffices to list, for each instance of a discourse rela-
tion, the set of elementary constituents that belong to its
left and right scope in order to express the three kinds
of structures. We do this in a way that an isomorphic
structure can always be recovered. Models of our com-
mon language will be a list of relation instances and el-
ementary constituents, together with a set of predicates
stating what is in the scope of what. As for the second
point, we axiomatize each constraint in our common
language, thereby describing each of the 3 types of dis-
course structures as a theory in our language.
Our language contains only binary relations. Among
discourse formalisms, only RST makes serious (and
empirical) use of n?ary discourse relations. Neverthe-
less, such RST structures are expressible in our frame-
work, if we assume certain semantic equivalences.
RST allows for two cases of non-binary trees: (i) nu-
cleus with n satellites, each one linked to the nucleus
by some relation Rn. Such a structure is semantically
equivalent to the conjunction of n-binary relations Rn
between the nucleus and the nth satellite, which is ex-
pressible in our framework. (ii) RST also allows for n-
ary multinuclear relations such as List and Sequence. In
our understanding, multinuclear relations R(a1, . . . an),
essentially serve a purpose of expressiveness, and such
an n-ary tree is an equivalent to the split non-tree
shaped structure R(a1, a2) ? R(a2, a3) . . .R(a(n?1), an).
This seems clear for the Sequence relation, which
states that a1 . . . an are in temporal sequence and can
be equivalently formulated as ?each ai precedes ai+1?.
This might appear less obvious for the List relation.
The semantics (as it appears on the RST website http:
//www.sfu.ca/rst/) of this relation requires the ai to
be ?comparable?, and as far as this is a transitive prop-
erty, we can split the relation into a set of binary ones.
Formally, our scope language Lscopes is a fragment of
that of monadic second order logic with two sorts of in-
dividuals: relation instances (i), and elementary consti-
tuants (l). Below, we assume R is the set of all relation
names (elaboration, narration, justification, . . . ).
Definition 1 (Scoping language). Let S be the set {i, l}.
The set of primitive, disjoint types of Lscopes consists of
i, l and t (type of formulae). For each of the types in
S , we have a countable set of variable symbols Vi (Vl).
Two additional countable sets of variable symbols V?i,t?
and V?l,t? range over sets of individuals. These four sets
of variable symbols are pairwise disjoint.
The alphabet of our language is constituted by Vi, Vs,
a set of predicates, equality, connector and quantifier
symbols. The set of predicate symbols is as follows:
1) For each relation symbol r in R, LR is a unary
predicate of type ?i, t??i.e., LR : ?i, t? .
2) unary predicates, sub, coord and sub?1 : ?i, t?.
3) binary predicates ?l and ?r : ?i, l, t?.
4) two equality relations, =s : ?s, s, t? for s ? {i, l}.
Logical connectors, and quantifiers are as usual.
The sets of terms ?i,?l and ?t are recursively defined:
1. Vi ? ?i, Varl ? ?l. 2. For v ? Vs,t, v : ?s, t?. 3. For
each symbol ? of type ?u1, . . . , un? in the alphabet, for
all (t1, . . . , tn?1) ? ?u1?? ? ???un?1, ?[t1, . . . , tn?1] ? ?un .
?t is the set of well formed formulae of the scope lan-
guage.
The predicates ?l and ?r take a relation instance r of
type i and a elementary constituent x of type l as argu-
ments. Intuitively, they mean that x has to be included
in the left (for ?l) or right (for ?r) scope of r. For each
relation symbol R such as justification or elaboration,
the predicate LR takes a relation instance r has argu-
ment and states that r is an instance of the rhetorical re-
lation R. Predicates sub, coord and sub?1 apply to a re-
lation instance r, respectively specifying that r?s left ar-
gument hierarchically dominate its right argument, that
4
both are of equal hierarchical importance, or that the
left one is subordinate to the right one.
Definition 2 (Scope structure and Interpretation).
A scope structure is an Lscopes-structure M =
?Di,Dl, |.|M?. Di and Dl are disjoint sets of individu-
als for the sorts i and l respectively, and |.|M assigns to
each predicate symbol P of type ?u1, . . . , un, t? a func-
tion |.|P : Du1?? ? ??Dun 7? {0, 1}. Variables of type ?i, t?
are assigned subsets of Di and similarly for variables of
type ?l, t?, The predicates =i and =s are interpreted as
equality over Di and Dl respectively.
The interpretation ~?Mv of a formula ? ? ?S is the
standard interpretation of a monadic second order for-
mula w.r.t to a model and a valuation (interpretation of
first order quantifiers and connectors is as usual, quan-
tification over sets is over all sets of individuals). Va-
lidity |= also follows the standard definition.
These scope structures offer a common framework
for different discourse formalisms. Given one of the
three formalisms, we say that two structures S 1 and S 2
are equivalent iff there is an encoding from one struc-
ture into a scoped structure or set of scoped structures
and a decoding back from the scoped structure or set of
scoped structures into S 2
Fact 1. One can define two algorithms I and E such
that:
? from a given structure s which is a RS Tree, a
SDRS or a DT, I computes a scope structure I(s).
? given such a computed structure, E allow to re-
trieve the original structure s (E(I(s)) = s).
RST Encoding and Decoding To flesh out I and E
for RST, we need to define dominance. Set lArgs(r) =
{e ? Dl | (r, e) ? |?l|M}; rArgs(r) is defined analogously
(where ?r replaces ?l). The left and right dominance
relations vl and vr are defined as follows: r vl r? iff
(Args(r) ? lArgs(r?)).
- r vl r? ? ?z : l((z ?l r)? z ?r r))? z ?l r?) with r vr r?
defined analogously.
Dominance v is: v=vl ? vr.
- lArgs(r, X)??z : l(z ?l r) ? z ? X), with rArgs(r, X)
similar and
-Args(r, X)? ?z : l((z ?l r) ? z ?r r))? z ? X).
The NS, NN and NS schemes of RST will be re-
spectively encoded by the predicates sub, coord and
sub?1. We proceed recursively. If t is an EDU e, re-
turn Mt = ?Di = ?,Dl = {e}, ? where  is the inter-
pretation that assigns the empty set to each predicate
symbol. If the root of t is a binary node instantiating
a relation R(t1a1 , t2a2 ), let Tr ? {sub, coord, sub?1} bethe predicate that encodes the schema a1a2, let Mt1 =
?D1i ,D1l , |.|1? and Mt2 = ?D2i ,D2l , |.|2?. The algorithm re-turns Mt = ?D1i ? D2i ? {r},D1l ? D2l , |.|Mt ? where r is a?fresh? relation instance variable not in D1i or D2i , and
|.|Mt is updated in the appropriate fashion to reflect the
left and right arguments of r. Finally, if the root of t is
an n-ary node, split it into a sequence of binary relation
R1(t1, t2),R2(t2, t3), . . . , proceed to recursively compute
the scope-structures Mi for each of the relations using
2 (take care to introduce a ?fresh? relation instance in-
dividual for each relation of the sequence), then return
the union of the models Mi.
RST Decoding Given a finite scope structure M =
?Di,Dl, |.|M?, for each relation instance r compute the
left arguments of r and its right arguments. We then
identify L(r), the unique relation symbol R such that
r ? |LR|M. If that fails, the algorithm fails. Similarly
retrieve the right nuclearity schema from the adequate
predicate that applies to r. Then compute the domi-
nance relations for r. If the input structure M = I(t)
for some RS Tree t then there is at least one maximal
relation instance for the dominance relation. If t the
root node of t is a binary relation, there is exactly one
maximal element in the dominance relation. If there
is none, then we return fail. If there is exactly one,
recursively compute the two RS Trees obtained from
the models computed from the left and right arguments
and descendants of r. If there is more than one, the root
node of the encoded RS Tree was a n-ary relation and
one has to reconstruct the n-ary node if that is possi-
ble; if not the algorithm fails (but that means the input
structure was not obtained from a valid RS Tree).
SDRT Encoding and Decoding: This is similar to
the RST encoding and decoding; for the encoding al-
gorithm, we proceed recursively top down. A SDRS
s is a complex constituent that contents a graph g =
?V, E? whose edges are relations holding between sub-
constituents, simple or complex as well. First come
up with an encoding of the set E of all edges that
hold between two sub-constituents of s, i.e. a struc-
ture M = ?Di = Ei,Dl = V, {LR}, ?l, ?r?, where, for
each edge e ? Ei, LR encodes its relation type, and
?l1 and ?r1 consists of all the pairs (x, e) of left and
right nodes x of the edges e ? E. Finally, for each
complex immediate sub-constituent of s in Dl, update
M as follows: for c such a subconsituent, recursively
compute its encoding Mc, then add everything of Mc
to M, finally remove c from M but add instead for
each relation r scoping over c to the right (left), all
the pairs {(r, x) | x is a constituent in Mc}. The decod-
ing works again similarly to the one for RST, top-down
once again: one recursively retrieves immediate con-
tent of the current complex constituent at each level
then moves to inner constituents.
DT: Dependency trees are syntactically a special case
of SDRSs; there is only one CDU whose domain is
only EDUs.
The scope language allows us to axiomatize three
classes of scope structures corresponding to RS Trees,
SDRSs and DTs. Not every scope structure will yield
a RS Tree when fed to the RST decoding algorithm,
only those obtainable from encoding an RS tree. As not
all scope structures obey these axioms, our language is
5
strictly more expressive than any of these discourse for-
malisms.
As an example of an axiom, the following formula
expresses that a relation cannot have both left and right
scope over the same elementary constituent:
Strong Irreflexivity:
?r : i?x : l?(x ?l r ? x ?r r)) (A0)
Strong irreflexivity entails irreflexivity; a given relation
instance cannot have the same (complete) left and right
scopes. All discourse theories validate A0.
In the Appendix, we define left and right strong dom-
inance relations vl(r) as well as n-ary RS trees and
CDUs of SDRT. We exploit these facts in the Appendix
to express axioms (A1-A9) that axiomatize the struc-
tures corresponding to RST, SDRT and DTs. Axiom
A1 says that every discourse unit is linked via some dis-
course relation instance. Axiom A2 insures that all our
relation instances have the right number of arguments;
Axioms A3 and A4 ensure acyclicity and no crossing
dependencies. A5a and A5b restrict structures to a tree-
like dominance relation with a maximal dominating el-
ement, while A6 defines the Right Frontier constraint
for SDRT, and A7 fixes the domain for SDRT con-
straints on CDUs. A8 ensures that no coordinating and
subordinating relations have the same left and right ar-
guments, while A9 provide the restrictions needed to
define the set of DTs. We use the encoding and decod-
ing maps to show:
Fact 2.
1. The theory TRS T ={A0, A1, A2, A3, A4, A5a, A5b, A8}
characterizes RST structures in the sense that:
- E applied to any structure M such that M |= TRS T
yield an RST Tree.
- for any RST Tree t, I(t) |= TRS T .
2. The theory TS DRT ={A0, A1, A2, A3, A6, A7, A8}
similarly characterizes SDRSs.
3. The theory TDT =TS DRT ? {A9a, A9b} similarly
characterizes Dependency Trees structures.
4 Different Interpretations of Scope
The previous section defined the set of scope structures
as well as the means to import, and then retrieve, RS
trees, DTs, or SDRs into, and from, this set. Some of
these scope structures export both into RST and SDRT,
yielding a 1 ? 1 correspondence between a subset of
SDRT and RST structures. But what does this corre-
spondence actually tell us about these two structures?
In mathematics, the existence of an isomorphism relies
on a bijection that preserves structure. Our correspon-
dence preserves the immediate interpretation of the se-
mantic scopes of relations.
Immediate Interpretation Consider a scope struc-
tureM (validating A0, A1, A2). The predicates lArgs(r)
and rArgs(r) are the sets of all units in the left or right
scope of a relation instance r. Whether r, labelled by
relation name R holds of two discourse units or not
in M, depends on the semantic content of its left and
right arguments, recursively described by lArgs(r) and
all relations r? such that r? @l r, and rArgs(r) and all
relations r? such that r? @r r. Algorithm I computes
what we call the immediate interpretation of an input
structure. Intuitively, in this interpretation the semantic
scope of relations is directly read from the structures
themselves; a node R(t1, t2) in a RS Tree expresses that
R holds between contents expressed by the whole sub-
structures t1 and t2. Similarly, for SDRT and DTs, im-
mediate interpretation of an edge pi1 ?R pi2 is that R
holds between the whole content of pi1 and pi2.
While this immediate interpretation is standard in
SDRT, it is not in RST. Consider again (1) from the
introduction or:
(2) [In 1988, Kidder eked out a $ 46 mil-
lion profit,]31 [mainly because of severe cost
cutting.]32 [Its 1,400-member brokerage oper-
ation reported an estimated $ 5 million loss last
year,]33 [although Kidder expects to turn a profit
this year]34 (RST Treebank, wsj 0604).
(3) [Suzanne Sequin passed away Saturday at the
communal hospital of Bar-le-Duc,]3 [where she
had been admitted a month ago.]4 [. . . ] [Her fu-
neral will be held today at 10h30 at the church
of Saint-Etienne of Bar-le-Duc.]5 (annodis cor-
pus).
These examples involve what are called long distance
attachments. (2) involves a relation of contrast, or com-
parison between 31 and 33, but which does not involve
the contribution of 32 (the costs cutting of 1988). (3)
displays something comparable. A causal relation like
result, or at least a temporal narration holds between
3 and 5, but it should not scope over 4 if one does
not wish to make Sequin?s admission to the hospital
a month ago a consequence of her death last Saturday.
Finally in (1) C4 elaborates on C1, but not on the fact
that C1 is attributed to chief Garcia, so the correspond-
ing elaboration relation should not scope over C3.
It is impossible however, to account for long distance
attachment using the immediate interpretation of RST
trees. (2), for instance, also involves an explanation
relation between 31 and 32, which should include none
of 33 or 34 in its scope. Since 31 is in the scope of both
the explanation and the contrast relation, Axiom A5a of
the previous section entails than an RST tree involving
the two relations has to make one of the two relations
dominates the other.
Marcu?s Nuclearity Principle (NP) Marcu 1996 pro-
vides an alternative to the immediate interpretation and
captures some long distance attachments Danlos 2008;
Egg and Redeker 2010. According to the NP, a rela-
6
tion between two spans of text, expressed at a node of
a RS Tree should hold between the most salient parts
of these spans. Most salient part is recursively defined:
the most salient part of an elementary constituent is it-
self, for a multinuclear relation R(t1N , . . . , tkN) its most
salient part is the union of the most salient parts of the
ti2. Following Egg and Redeker 2010, the NP, or weak
NP is a constraint on which RST trees may correctly
characterize an input text; it is not a mechanism for
computing scopes. Given their analysis of (1) given in
the introduction, NP entails that Elab1 holds between
C1 and C4, accounting for the long distance attach-
ment, and that Attribution holds between C1 and C4
which meets intuition in this case. There is however no
requirement that Attribution do not hold between the
wider span [C1,C2] and C3, as there is no requirement
that Elab1 does not hold between [C1,C2,C3] and C4.
In order to accurately account for (1), the former must
be true and the latter false.
However, this interpretation of NP together with an
RST tree does not determine the semantic scope of all
relations. Danlos 2008 reformulates NP as a Mixed
Nuclearity Principle (MNP) that outputs determinate
scopes for a given structure. The MNP requires for a
given node, that the most salient parts of his daughters
furnish the exact semantic scope for the relation at that
node. The MNP transforms an RST tree t into a scope
structureMt, which validates A0 ? A3 but also A6.3, A7
and A8. HenceM could be exported back to SDRT and
the MNP would yield a translation from RST-trees to
SDRSs.
But when applied to the RST Treebank, the MNP
yields wrong, or at least incomplete, semantic scopes
for intuitively correct RS Trees. The mixed principle
applied to the tree of s1 gives the Attribution scope
over C1 only, but not C2, which is incorrect. Focus-
ing on the attribution relation which is the second most
frequent in the RST Treebank, we find out that, regard-
less of whether we assign Attribution?s arguments S
and N or N and S, this principle makes wrong predic-
tions 86% of the time in a random sampling of 50 cases
in which we have attributions with multi-clause second
argument spans. Consider the following example from
the RST Treebank:
(4) [Interprovincial Pipe Line Co. said]1 [it will de-
lay a proposed two-step, 830 million Canadian-
dollar [(US$705.6 million)]3 expansion of its
system]2 [because Canada?s output of crude oil
is shrinking.]4
Applied to the annotated RS Tree for this example (fig-
2Except for Sequence which only retains the most salient
part of tk
3That A6 is valid in the resulting model is not immediate.
Assume a multinuclear (coordinating) relation instance r has
scope over xn and xn+k later in the textual order. Then it is
impossible to attach with r? a later found constituent xn+k+l to
xn alone, for it would require that xn+1 escapes the scope of r?
from the MNP which it will not do by multinuclearity of r.
attribution
1
S
reason
restatement
2
N
3SN
5S
N
Figure 1: Annotated RST Tree for example (4).
ure 1), the MNP yields an incorrect scope of the attribu-
tion relation over 2 only, regardless of whether the at-
tribution is annotated N-S or S -N. The idea behind the
weak NP provides a better fit with intuitions. The prin-
ciple gives minimal semantic requirements for scoping
relations; everything beyond those requirements is left
underspecified. We formalize this as the relaxed Nu-
clearity Principle (RNP), which does not compute one
structure where each relation is given its exact scope,
but a set of such structures.
The target structures are not trees any more, but we
want them to still reflect the dominance information
present in the RS Tree. We therefore define a notion
of weak dominance over structures of the scoping lan-
guage: for two sets of constituents, X  Y iff X ? Y or
there is a subordinating relation whose left argument is
X and right one Y . Weak dominance is given by tran-
sitive closure ? of . For two relations, r ?l r? iff theleft argument of r weakly dominates both arguments
of r?. ?r is symmetrically defined. Finally, structures
computed by the RNP have to validate the weakened
version of A5: if two relations scope over the same el-
ementary constituent one has to weakly dominates the
other. Let AW5 denote this axiom.
Definition 3 (Relaxed Nuclearity Principle). One can
assign to an RS Tree t a formula of the scoping lan-
guage ?t = ?x??r??t ? ?t such that:
1? ?t is a formula specifying that all individuals
quantified in x? and r? are pairwise distinct, and that there
is no other individuals that the ones just mentioned. ?t
also specifies for each intermediate node n that the cor-
responding relation instance rn is labelled with the ad-
equate relation symbol R and relation type (subordinat-
ing if N-S . . . ).
2? ?t encodes the nuclearity principle applied to t:
for all intermediate nodes ni and n j in t such that nl is
the left (resp. right) daughter of ni, ?t specifies that ni
must scope to the left (resp. right) over the nucleus of
n j.
The interpretation ~t is defined as the set of struc-
turesM that validate ?t and A0, A1, A2, A3, AW5 (they allhave |t| individuals, as fixed by ?t). Moreover, it can
be shown that each model of this set validates TS DRT ;
so we have a interpretation of an RS-Tree into a set of
SDRSs.
5 Intertranslability between RST/DTs
DTs are a restriction of SDRSs to structures without
complex constituents. So the ? function of section 2
7
can transform distinct SDRSs transform into the same
DT with a consequent loss of information.
a?R1 pi
pi : b?R2 c | a?R1 b?R2 c |
pi?R2 b
pi : a?R1 b (1)
Each of the SDRSs above yields the same DT after sim-
plification, namely the second one a?R1 b?R2 c.
The natural interpretation of a DT g describes the
set of fully scoped SDRS structures that are compat-
ible with these minimal requirements, i.e that would
yield g by simplification. To get this set, every edge
r(x, y) in g, r, must be assigned left scope among the
descendants of x in g (and right scope among those of
y); this is a consequence of i) x and y being heads of the
left and right arguments of r and ii) the SDRSs that are
compatible with g do not admit relations with a right
argument in one constituent and a left one outside of it.
Definition 4. Assume that we map each node4 x of g
into a unique variable vx ? Vl and each edge e into a
unique variable symbol re ? Vi. Define x? and r? in an
analogous way as in definition 3.
For a given dependency tree g, we compute a for-
mula ?g = ?x??r? ?g ? ?g such that
? ?g is defined analogously as in definition 3, defin-
ing the set of relation instances and EDUs.
? ?g is the formula stating the minimal scopes for
each relation instance: for all edge in e = R(x, y)
in g, ?g entails i) re has vx in its left scope and
vy in its right scope and ii) let Des(x) be the set
of variable symbols for all the descendants of x in
g, ?g entails that if re has left scope over some vz
then vz is in Des(x) (symmetrically for y and right
scope).
The interpretation ~g of a DT is: {M | M |=
?g, A0-A3, A6, A7}. The DT a ?R1 b ?R2 c for in-
stance, is interpreted as a set of three structures iso-
morphic to the ones in (1) above.
We now relate DTs to RS Trees interpreted with the
RNP. To this aim, we focus on a restricted class of DTs,
those who involve i) coordinating chains of 3 edus or
more only if they involve a single coordinating relation:
x1 ?R1 x2 ?R2 ? ? ? ?Rn?1 xn may appear only for n >
2 if all the Ri are the same coordinating relation, and
ii) subordinating nests of 3 edus or more only if they
involve a single subordinating relation:
x
y1
R1
. . . yn
Rn
is allowed for n > 1 only if all Ri
are labelled with the same subor-
dinating relation.
This restricted class of DTs corresponds exactly with
the set of RS-Trees interpreted with the RNP, provided
that we restrict the interpretation of a DT in the fol-
lowing way: a principle called Continuing Discourse
Pattern, CDP Asher and Lascarides 2003 must apply,
4Recall that unlike RS Trees, DTs have EDUs as nodes
and relations as edges.
who states that whenever a sequence of coordinating
relation Ric originates as a node which appear to be
also in the right scope of a subordinating relation Rs,
Rs must totally include all the Ric in its right scope. A
second principle is required, who states that whenever
two subordinating relations R0s and R?s originate at the
same node in the DT, and the right argument of R?s is
located after the right argument of Rs, any structure in
the interpretation of the DT must verify R?s l Rs. The
translation needs these requirements to work, because:
i) with the NP a relation scoping over a multinuclear
one must includes all the nucleus in RST, and ii)a node
in a RS Tree cannot scope over something that is not its
descendant). Let CDP+ denote these requirements.
Using the restricted interpretation of a DT g;
~gCDP = {M | M |= A0-A3, A6, A7,CDP+}, we trans-
form an RS Tree t into a dependency graph G(t) such
that ~t = ~G(t)CDP:
Definition 5 (RS Trees to dependency graphs). The
translation G takes a RS Tree t as input and outputs
a pair ?G, n?, where G = ?Nodes, Edges? is the corre-
sponding dependency graph, and n an attachment point
used along the recursive definition of G.
? If t is an EDU x then (G)(t) = ?({x}, {}), x?.
? If t = R(t1N , t2S ) then let ?G1, n1? = G(t1) and
?G2, n2? = G(t2).
G(t) = ?(G1 ?G2 ? {Rsubord(n1, n2))}; n1?
? If t = R(t1S , t2N) then G(t) = G(R(t2N , t1S ))
? If t = R(t1N , . . . , tkN) (multinuclear), let ?Gi, ni? =
G(ti), let G be the result of adding a chain
n1 ?Rcoord ? ? ? ?Rcoord nk to the union of the Gi,
G(t) = ?G; n1?
? If t is a nuclear satellite relation with several satel-
lites R(t1S , . . . t jN , . . . tkS ), compute the Gi has inthe previous case, then add to the union of the Gi
the nest of k ? 1 subordinating relations R linking
n j to each of the ni, i , j.
Recall RS Tree (s1). Applying G to this tree yields
the dependency tree (s3): Elab1(C1,C2)?Attr(C1,C3)?
Elab2(C1,C4). ~s3 supports any reading of (s1) pro-
vided by RNP, but also an additional one where Attr
scopes over [C1,C2,C4]. This is however forbidden
by CDP+ for C4 is after C3 in the textual order but
Elab(C1,C4) l Attr(C1,C3).
6 Similarities and distances
The framework we have presented yields a notion of
similarity that applies to structures of different for-
malisms. To motivate our idea, recall example (1);
the structure in (s3) in which Attribution just scopes
over C1 differs from the intuitively correct interpreta-
tion only in that Attribution should also scope over C2
8
as in (s2), while a structure that does this but in which
C3 is in the scope of the Elaboration relation is intu-
itively further away from the correct interpretation.
Our similarity measure Sim over structures M1 and
M2 assumes a common set of elementary constituents
and a correspondence between relation types in the
structures. We measure similarity in terms of the
scopes given to the relations. The intuition, is that given
a map f from elements of relation instances inM1 re-
lation instances in M2, we achieve a similarity score
by counting for each relation instance r the number of
EDUs that are both in the left scope of one element of
r and in f (r), then divide this number by the total num-
ber of diffrents constituents in the left scope of r1 and
r2, and do the same for right scopes as well. The global
similarity is given by the correspondence which yields
the best score.
Given a relation r1 ? M1 and a relation r2?M2, let
?(r1, r2) =
{ 1 if r1 and r2 have the same label
0 otherwise . De-
fine Cl(r1, r2) = |{x : l | M1 |= x ?l r1 ?M2 |= x ?l r2}|,
the number of constituents over which r1 and r2 scope
and Dl(r1, r2) = |{x : l |M1 |= x ?l r1?M2 |= x ?l r2}|.
Define Cr and Dr analogously and assume thatM1 has
less relation instances thanM2. Let Inj(D1i ,D2i ) be theset of injections of relations instances of M1 to those
ofM2.
S im(M1,M2) = 12Max(|M1|, |M2|)?
Max
f?Inj(D1i ,D2i )
?
r:i
?(r, f (r)) ? ( Cl(r, f (r))Dl(r, f (r)) +
Cr(r, f (r))
Dr(r, f (r)) )
If M2 has more relation instances, Invert arguments
and use the definition above. If they have same number
of instances, both directions coincide.
d(M1,M2)=1 ? S im(M1,M2)
For a discourse structureM, S im(M,M) = 1; Sim
ranges between 1 and 0. d is a Jaccard-like met-
ric obeying symmetry, d(x, x) = 0 d(x, y) , 0 for
x , y, and the triangle equality. One can further define
the maximal or average similarity between any pair of
structures of two sets S 1 and S 2. This gives an idea
of the similarity between two underspecified interpre-
tations, such as the ones provided by RNP of section 4.
For example, the maximal similarity between (s2) in-
terpreted as itself (immediate interpretation) and a pos-
sible scope structure for the DT (s3), interpreted with
the underspecified ~ of section 5, is 7/12. It is pro-
vided by the interpretation of (s3) where Attr is given
left scope over C1,C2,C4, Elab1 holds between C1 and
C2, and the second Elab fails to match the continua-
tion of (s3). sim(~s2, ~?(s2) = 7/12 also, because
? must distribute [2, 4] in s2 to avoid crossing depen-
dencies; so ~?(s2)  ~s3. The maximal similarity
between the RS tree in (s1) with RNP (or equivalently,
(3) with ~CDP+) and (s2) is 19/36, achieved when both
C1 and C2 are left argument of Attr (though not C4).
With MNP, the similarity is 17/36.
Given our results in sections 4 and 5, we have:
Fact 3. (i) For any DT g without a > 3 length flat se-
quence and interpreted using CDP+, there an RS tree
t interpreted with RNP such that S im(g, t) = 1. (ii)
For any RS tree with RNP there is a DT g such that
S im(t, g) = 1.
To prove (i) construct a model using Definition 4 and
then use RST decoding. To prove (ii) construct a model
given Definition 3 and use DT encoding. Our similarity
measure provides general results for SDRSs and DTTs
(and a fortiori SDRSs and RS trees) (See Appendix).
7 Related Work
Our work shares a motivation with Blackburn, Gardent,
and Meyer-Viol 1993: Blackburn, Gardent, and Meyer-
Viol 1993 provides a modal logic framework for for-
malizing syntactic structures; we have used MSO and
our scope language to formalize discourse structures.
While many concepts of discourse structure admit of
a modal formalization, the fact that discourse relations
can have scope over multiple elementary nodes either
in their first or second argument makes an MSO treat-
ment more natural. Danlos 2008 compares RST, SDRT
and Directed Acyclic Graphs (DAGs) in terms of their
strong generative capacity in a study of structures and
examples involving 3 EDUS. We do not consider gen-
erative capacity, but we have given a generic and gen-
eral axiomatization of RST, SDRT and DT in a formal
interpreted language. We can translate any structure of
these theories into this language, independent of their
linguistic realization. We agree with Danlos that the
NP does not yield an accurate semantic representation
of some discourses. We agree with Egg and Redeker
2010 that the NP is rather a constraint on structures, and
we formalize this with the relaxed principle and show
how it furnishes a translation from RS trees to sets of
scoped structures. Danlos?s interesting correspondence
between restricted sets of RST trees, SDRSs and DAGs
assumes an already fixed scope-interpretation for each
kind of structure: SDRSs and DAGs are naturally in-
terpreted as themselves, and RS Trees are interpreted
with the mixed NP Our formalism allows us both to
describe the structures themselves and various ways of
computing alternate scopes for relations.
With regard to the discussion in Egg and Redeker
2008; Wolf and Gibson 2005 of tree vs. graph struc-
tures, we show exactly how tree based structures
like RST with or without the NP compare to graph
based formalisms like SDRT. We have not investigated
Graphbank here, but the scope language can axioma-
tize Graphbank (with A0-A3, A8).
8 Conclusions
We have investigated how to determine the semantic
scopes of discourse relations in various formalisms by
9
developing a canonical formalism that encodes scopes
of relations regardless of particular assumptions about
discourse structure. This provides a lingua franca for
comparing discourse formalisms and a way to measure
similarity between structures, which can help to com-
pare different annotations of a same text.
References
Afantenos, S. et al (2012). ?An empirical resource for
discovering cognitive principles of discourse organ-
isation: the ANNODIS corpus?. In: Proceedings of
LREC 2012. ELRA.
Asher, N. and A. Lascarides (2003). Logics of Conver-
sation. Cambridge University Press.
Asher, N. (1993). Reference to Abstract Objects in Dis-
course. Studies in Linguistics and Philosophy 50.
Dordrecht: Kluwer.
Baldridge, J., N. Asher, and J. Hunter (2007). ?Anno-
tation for and Robust Parsing of Discourse Structure
on Unrestricted Texts?. In: Zeitschrift fr Sprachwis-
senschaft 26, pp. 213?239.
Bateman, J. and K. J. Rondhuis (1997). ?Coherence re-
lations : Towards a general specification?. In: Dis-
course Processes 24.1, pp. 3?49.
Blackburn, P., C. Gardent, and W. Meyer-Viol (1993).
?Talking about Trees?. In: EACL 6, pp. 21?29.
Carlson, L., D. Marcu, and M. E. Okurowski (2002).
RST Discourse Treebank. Linguistic Data Consor-
tium, Philadelphia.
Danlos, L. (2008). ?Strong generative capacity of RST,
SDRT and discourse dependency DAGSs?. English.
In: Constraints in Discourse. Ed. by A. Benz and P.
Kuhnlein. Benjamins, pp. 69?95.
duVerle, D. and H. Prendinger (2009). ?A Novel Dis-
course Parser Based on Support Vector Machine
Classification?. In: Proceedings of ACL-IJCNLP
2009. ACL, pp. 665?673.
Egg, M. and G. Redeker (2008). ?Underspecified dis-
course representation?. In: PRAGMATICS AND BE-
YOND NEW SERIES 172, p. 117.
? (2010). ?How Complex is Discourse Structure?? In:
Proceedings of LREC?10. Ed. by N. Calzolari et al
ELRA.
Hitzeman, J., M. Moens, and C. Grover (1995). ?Algo-
rithms for Analyzing the Temporal Structure of Dis-
course?. In: Proceedings of the 7th Meeting of the
European Chapter of the Association for Computa-
tional Linguistics, pp. 253?260.
Hobbs, J. R., M. Stickel, and P. Martin (1993). ?In-
terpretation as Abduction?. In: Artificial Intelligence
63, pp. 69?142.
Kehler, A. (2002). Coherence, Reference and the The-
ory of Grammar. CSLI Publications.
Lascarides, A. and N. Asher (1993). ?Temporal In-
terpretation, Discourse Relations and Commonsense
Entailment?. In: Linguistics and Philosophy 16,
pp. 437?493.
Mann, W. C. and S. A. Thompson (1987). ?Rhetorical
Structure Theory: A Framework for the Analysis of
Texts?. In: International Pragmatics Association Pa-
pers in Pragmatics 1, pp. 79?105.
Marcu, D. (1996). ?Building up rhetorical structure
trees?. In: Proceedings of the thirteenth national
conference on Artificial intelligence - Volume 2.
AAAI?96. Portland, Oregon: AAAI Press, pp. 1069?
1074. isbn: 0-262-51091-X.
Muller, P. et al (2012). ?Constrained decoding for
text-level discourse parsing?. Anglais. In: COLING
- 24th International Conference on Computational
Linguistics. Mumbai, Inde.
Polanyi, L. (1985). ?A Theory of Discourse Structure
and Discourse Coherence?. In: Papers from the Gen-
eral Session at the 21st Regional Meeting of the
Chicago Linguistics Society. Ed. by P. D. K. W. H.
Eilfort and K. L. Peterson.
Polanyi, L. and R. Scha (1984). ?A Syntactic Ap-
proach to Discourse Semantics?. In: Proceedings of
the 10th International Conference on Computational
Linguistics (COLING84). Stanford, pp. 413?419.
Polanyi, L. et al (2004). ?A Rule Based Approach
to Discourse Parsing?. In: Proceedings of the 5th
SIGDIAL Workshop in Discourse and Dialogue,
pp. 108?117.
Prasad, R. et al (2008). ?The penn discourse tree-
bank 2.0?. In: Proceedings of the 6th International
Conference on Language Resources and Evaluation
(LREC 2008), p. 2961.
Sagae, K. (2009). ?Analysis of Discourse Structure
with Syntactic Dependencies and Data-Driven Shift-
Reduce Parsing?. In: Proceedings of IWPT?09. ACL,
pp. 81?84.
Sanders, T., W. Spooren, and L. Noordman (1992).
?Toward a taxonomy of coherence relations?. In:
Discourse processes 15.1, pp. 1?35.
Stede, M. (2004). ?The Potsdam Commentary Cor-
pus?. In: ACL 2004 Workshop on Discourse Annota-
tion. Ed. by B. Webber and D. K. Byron. Barcelona,
Spain: Association for Computational Linguistics,
pp. 96?102.
Subba, R. and B. Di Eugenio (2009). ?An effective Dis-
course Parser that uses Rich Linguistic Information?.
In: Proceedings of HLT-NAACL. ACL, pp. 566?574.
Webber, B. et al (1999). ?Discourse Relations: A
Structural and Presuppositional Account Using Lex-
icalised TAG?. In: Proceedings of the 37th ACL
Conference. College Park, Maryland, USA: Associ-
ation for Computational Linguistics, pp. 41?48. doi:
10.3115/1034678.1034695.
Wolf, F. and E. Gibson (2005). ?Representing Dis-
course Coherence: A Corpus Based Study?. In:
Computational Linguistics 31.2, pp. 249?287.
Appendix
In what follows, let @ denotes the irreflexive part of
v We assume that we have access to the textual order
10
of EDUs as a function f : EDUs ? N with an associ-
ated strict linear ordering < over EDUs. We also ap-
peal to the notion of a chain over EDUs {x1, x2, . . . xn}
with a set of relation instances r1, . . . , rn} all of which
are instances of an n-ary relation type, of the form
x1 ?r1 x2 ?r2 . . . ?rn xn which can be defined in
MSO. To handle RST relations with multiple satellites,
we define a nest: Nest(X,R) iff all r ? R have the same
left argument in X but take different right arguments in
X. Finally, we define CDUs:
cdu(X,R)? ?rArgs(r, X)?
?r? (?x x ?r r? ? x ? X)? r? ? R
Axiomatization
?x : l ?r : i (x ?l r) ? (x ?r r)
(A1:Weak Connectedness)
?r?x, y(x ?r r) ? y ?l r))
(A2 :Properness of the relation)
?X : (l, t)(X , 0? ?y?X ?n?y ?l n
(A3 :Acyclicity or Well Foundedness)
No crossing dependencies using the textual order < of
EDUs:
?x, y, z,w((x < y < z < w) ?
?m, n?(x ?l n ? z ?r n
? y ?l m ? w ?r m)) (A4)
Tree Structures. Define scopes(r, x) := x ?l r ? x ?r r.
?r, r? ((?(?X,R r, r? ? R ? chain(X,R) ? nest(X,R))
? (?x scopes(r, x) ? scopes(r?, x)))
? (r v r? ? r? v r))
(A5a)
?R : (i, t)?!r : i ?r? ? R r? v r (A5b)
Right Frontier:
?n, xn, xn+1?r ((xn+1 ?r r)? (xn ?l r) ? (?xn ?l r
? ?X,R(chain(X,R) ? ?r?(r? ? R? sub(r?))
? ?y ? X?z?k ?m, j ? R (scopes( j, y) ? acc(z, y)
? scopes(m, xn) ? z ?l k ? k ? ?xn+1)))) (A6)
(The definition of SDRS accessibility acc is easy)
CDUs or EDUs and no overlapping CDUs:
?!x : l ? ?X,R cdu(X,R)?
?X,Y,R,R? (cdu(X,R) ? cdu(Y,R?)?
(R ? R? , 0? (R ? R? ? R? ? R))
(A7)
The same arguments cannot be linked by subordinating
and coordinating relations. The formal axiom is evi-
dent.
Finally, two axioms for restricting SDRSs to depen-
dency trees:
?r?x, y((x ?l r) ? y ?l r))
? (x ?r r) ? y ?r r)))? x = y
(A9a : NoCDUs.)
?r?r??X,Y(lArgs(r, X) ? rArgs(r,Y)
? lArgs(r?, X) ? rArgs(r?,Y))
? r = r?
(A9b :unique arc)
We note that as a consequence of A5a and A5b we have
no danglers or contiguous spans:
?x, y, n (x ?l n ? y ?l n ? x , y)
? ??m?z (x ?l m ? z ?r m
? ?(z ?l n ? z ?r n))
We also note that A5a and A5b entail A7, A8 and A9b,
though not vice-versa.
Fact 4. Where ? is any SDRS and ? : S DRS ? DT as
in section 2, set R1 = {r : i : |{x : M? |= x ?l r)}| > 1},
R2 = {r : i : |{x : M? |= x ?r r)}| > 1}, and
R{x,y} = {r|?r? : i(x ?l r? ? y ?r r? ? r? , r}. Assume the
immediate interpretation of ? and ?(?):
S im(?, ?(?))=
2|I| ? |(R1 ? R2) ??x,y?D2l X{x,y}|
2|I|
+
1
2|I| {?r?R1
1
|x : M? |= x ?l r)}|
+?r?R2
1
|x : M? |= x ?r r)}| }
Explanation: We suppose that I is the number of re-
lation instances in the SDRS. ? removes CDUs in an
SDRS and attaches all incoming arcs to the CDUs to
the head of the CDU. It also removes multiple arcs
into any given node. So for any node m such that
|{r : m ?r r}| = a > 1, then the information contained
in the a ? 1 arcs will be lost. In addition ? will restrict
that one incoming arc that in the SDRS has in its scope
all the elements in the CDU to just the head. So the
scope information concerning all the other elements in
the CDU will be lost.
11
