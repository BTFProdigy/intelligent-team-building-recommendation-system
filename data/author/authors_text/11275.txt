Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 477?485,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Predicting Response to Political Blog Posts with Topic Models
Tae Yano William W. Cohen Noah A. Smith
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{taey,wcohen,nasmith}@cs.cmu.edu
Abstract
In this paper we model discussions in online
political blogs. To do this, we extend Latent
Dirichlet Allocation (Blei et al, 2003), in var-
ious ways to capture different characteristics
of the data. Our models jointly describe the
generation of the primary documents (posts)
as well as the authorship and, optionally, the
contents of the blog community?s verbal reac-
tions to each post (comments). We evaluate
our model on a novel comment prediction task
where the models are used to predict which
blog users will leave comments on a given
post. We also provide a qualitative discussion
about what the models discover.
1 Introduction
Web logging (blogging) and its social impact have
recently attracted considerable public and scientific
interest. One use of blogs is as a community dis-
cussion forum, especially for political discussion
and debate. Blogging has arguably opened a new
channel for huge numbers of people to express their
views with unprecedented speed and to unprece-
dented audiences. Their collective behavior in the
blogosphere has already been noted in the Ameri-
can political arena (Adamic and Glance, 2005). In
this paper we attempt to deliver a framework useful
for analyzing text in blogs quantitatively as well as
qualitatively. Better blog text analysis could lead to
better automated recommendation, organization, ex-
traction, and retrieval systems, and might facilitate
data-driven research in the social sciences.
Apart from the potential social utility of text pro-
cessing for this domain, we believe blog data is wor-
thy of scientific study in its own right. The sponta-
neous, reactive, and informal nature of the language
in this domain seems to defy conventional analytical
approaches in NLP such as supervised text classifi-
cation (Mullen and Malouf, 2006), yet the data are
rich in argumentative, topical, and temporal struc-
ture that can perhaps be modeled computationally.
We are especially interested in the semi-causal struc-
ture of blog discussions, in which a post ?spawns?
comments (or fails to do so), which meander among
topics and asides and show the personality of the
participants and the community.
Our approach is to develop probabilistic mod-
els for the generation of blog posts and comments
jointly within a blog site. The model is an extension
of Latent Dirichlet Allocation (Blei et al, 2003).
Unsupervised topic models can be applied to collec-
tions of unannotated documents, requiring very lit-
tle corpus engineering. They can be easily adapted
to new problems by altering the graphical model,
then applying standard probabilistic inference algo-
rithms. Different models can be compared to ex-
plore the ramifications of different hypotheses about
the data. For example, we will explore whether the
contents of posts a user has commented on in the
past and the words she has used can help predict
which posts she will respond to in the future.
The paper is organized as follows. In ?2 we re-
view prior work on topic modeling for document
collections and studies of social media like political
blogs. We then provide a qualitative characterization
of political blogs, highlighting some of the features
we believe a computational model should capture
and discuss our new corpus of political blogs (?3).
We present several different candidate topic models
that aim to capture these ideas in ?4. ?5 shows our
empirical evaluation on a new comment prediction
task and a qualitative analysis of the models learned.
2 Related Work
Network analysis, including citation analysis, has
been applied to document collections on the Web
(Cohn and Hofmann, 2001). Adamic and Glance
(2005) applied network analysis to the political bl-
477
ogosphere. The study modeled the large, complex
structure of the political blogosphere as a network
of hyperlinks among the blog sites, demonstrated the
viability of link structure for information discovery,
though their analysis of text content was less exten-
sive. In contrast, the text seems to be of interest
to social scientists studying blogs as an artifact of
the political process. Although attempts to quanti-
tatively analyze the contents of political texts have
been made, results from classical, supervised text
classification experiments are mixed (Mullen and
Malouf, 2006; Malouf and Mullen, 2007). Also, a
consensus on useful, reliable annotation or catego-
rization schemes for political texts, at any level of
granularity, has yet to emerge.
Meanwhile, latent topic modeling has become a
widely used unsupervised text analysis tool. The ba-
sic aim of those models is to discover recurring pat-
terns of ?topics? within a text collection. LDA was
introduced by Blei et al (2003) and has been espe-
cially popular because it can be understood as a gen-
erative model and because it discovers understand-
able topics in many scenarios (Steyvers and Grif-
fiths, 2007). Its declarative specification makes it
easy to extend for new kinds of text collections. The
technique has been applied to Web document collec-
tions, notably for community discovery in social net-
works (Zhang et al, 2007), opinion mining in user
reviews (Titov and McDonald, 2008), and sentiment
discovery in free-text annotations (Branavan et al,
2008). Dredze et al (2008) applied LDA to a collec-
tion of email for summary keyword extraction. The
authors evaluated the model with proxy tasks such as
recipient prediction. More closely related to the data
considered in this work, Lin et al (2008) applied a
variation of LDA to ideological discourse.
A notable trend in the recent research is to aug-
ment the models to describe non-textual evidence
alongside the document collection. Several such
studies are especially relevant to our work. Blei and
Jordan (2003) were one of the earliest results in this
trend. The concept was developed into more general
framework by Blei and McAuliffe (2008). Steyvers
et al (2004) and Rosen-Zvi et al (2004) first ex-
tended LDA to explicitly model the influence of au-
thorship, applying the model to a collection of aca-
demic papers from CiteSeer. The model combined
the ideas from the mixture model proposed by Mc-
Callum (1999) and LDA. In this model, an abstract
notion ?author? is associated with a distribution over
topics. Another approach to the same document col-
lection based on LDA was used for citation network
analysis. Erosheva et al (2004), following Cohn and
Hofmann (2001), defined a generative process not
only for each word in the text, but also its citation
to other documents in the collection, thereby cap-
turing the notion of relations between the document
into one generative process. Nallapati and Cohen
(2008) introduced the Link-PLSA-LDA model, in
which the contents of the citing document and the
?influences? on the document (its citations to exist-
ing literature), as well as the contents of the cited
documents, are modeled together. They further ap-
plied the Link-PLSA-LDA model to a blog corpus
to analyze its cross citation structure via hyperlinks.
In this work, we aim to model the data within blog
conversations, focusing on comments left by a blog
community in response to a blogger?s post.
3 Political Blog Data
We discuss next the dataset used in our experiments.
3.1 Corpus
We have collected blog posts and comments from
40 blog sites focusing on American politics during
the period November 2007 to October 2008, con-
temporaneous with the presidential elections. The
discussions on these blogs focus on American poli-
tics, and many themes appear: the Democratic and
Republican candidates, speculation about the results
of various state contests, and various aspects of
international and (more commonly) domestic poli-
tics. The sites were selected to have a variety of
political leanings. From this pool we chose five
blogs which accumulated a large number of posts
during this period: Carpetbagger (CB),1 Daily Kos
(DK),2 Matthew Yglesias (MY),3 Red State (RS),4
and Right Wing News (RWN).5 CB and MY ceased
as independent bloggers in August 2008.6 Because
1http://www.thecarpetbaggerreport.com
2http://www.dailykos.com
3http://matthewyglesias.theatlantic.com
4http://www.redstate.com
5http://www.rightwingnews.com
6The authors of those blogs now write for larger on-
line media, CB for Washingon Monthly at http://www.
478
MY RWN CB RS DK
Time span (from 11/11/07) ?8/2/08 ?10/10/08 ?8/25/08 ?6/26/08 ?4/9/08
# training posts 1607 1052 1080 2045 2146
# words (total) 110,788 194,948 183,635 321,699 221,820
(on average per post) (68) (185) (170) (157) (103)
# comments 56,507 34,734 34,244 59,687 425,494
(on average per post) (35) (33) (31) (29) (198)
(unique commenters, on average) (24) (13) (24) (14) (93)
# words in comments (total) 2,287,843 1,073,726 1,411,363 1,675,098 8,359,456
(on average per post) (1423) (1020) (1306) (819) (3895)
(on average per comment) (41) (31) (41) (27) (20)
Post vocabulary size 6,659 9,707 7,579 12,282 10,179
Comment vocabulary size 33,350 22,024 24,702 25,473 58,591
Size of user pool 7,341 963 5,059 2,789 16,849
# test posts 183 113 121 231 240
Table 1: Details of the blog data used in this paper.
our focus in this paper is on blog posts and their
comments, we discard posts on which no one com-
mented within six days. We also remove posts with
too few words: specifically, we retain a post only
if it has at least five words in the main entry, and
at least five words in the comment section. All
posts are represented as text only (images, hyper-
links, and other non-text contents are ignored). To
standardize the texts, we remove from the text 670
commonly used stop words, non-alphabet symbols
including punctuation marks, and strings consisting
of only symbols and digits. We also discard infre-
quent words from our dataset: for each word in a
post?s main entry, we kept it only if it appears at
least one more time in some main entry. We ap-
ply the same word pruning to the comment section
as well. The corpus size and the vocabulary size of
the five datasets are listed in Table 1. In addition,
each user?s handle is replaced with a unique inte-
ger. The dataset is available for download at http:
//www.ark.cs.cmu.edu/blog-data.
3.2 Qualitative Properties of Blogs
We believe that readers? reactions to blog posts are
an integral part of blogging activity. Often com-
ments are much more substantial and informative
than the post. While circumspective articles limit
themselves to allusions or oblique references, read-
ers? comments may point to heart of the matter more
washingtonmonthly.com and MY for Think Progress
athttp://yglesias.thinkprogress.org.
boldly. Opinions are expressed more blatantly in
comments. Comments may help a human (or au-
tomated) reader to understand the post more clearly
when the main text is too terse, stylized, or technical.
Although the main entry and its comments are
certainly related and at least partially address similar
topics, they are markedly different in several ways.
First of all, their vocabulary is noticeably different.
Comments are more casual, conversational, and full
of jargon. They are less carefully edited and there-
fore contain more misspellings and typographical er-
rors. There is more diversity among comments than
within the single-author post, both in style of writing
and in what commenters like to talk about. Depend-
ing on the subjects covered in a blog post, different
types of people are inspired to respond. We believe
that analyzing a piece of text based on the reaction
it causes among those who read it is a fascinating
problem for NLP.
Blog sites are also quite distinctive from each
other. Their language, discussion topics, and col-
lective political orientations vary greatly. Their vol-
umes also vary; multi-author sites (such as DK, RS)
may consistently produce over twenty posts per day,
while single-author sites (such asMY, CB) may have
a day with only one post. Single author sites also
tend to have a much smaller vocabulary and range
of interests. The sites are also culturally different
in commenting styles; some sites are full of short
interjections, while others have longer, more analyt-
ical comments. On some sites, users appear to be
479
??
z z?
u
?
w
?
D
N M
1
?
?
z z?
w? u
? ? ?
w
?
D
MN
1
Figure 1: Left:
LinkLDA (Erosheva
et al, 2004), with
variables reassigned.
Right:
CommentLDA. In
training, w, u, and
(in CommentLDA)
w? are observed. D is
the number of blog
posts, and N and M
are the word counts
in the post and the all
of its comments,
respectively. Here we
?count by verbosity.?
close-knit, while others have high turnover.
In the next section, we describe how we apply
topic models to political blogs, and how these prob-
abilistic models can put to use to make predictions.
4 Generative Models
The first model we consider is LinkLDA, which is
analogous to the model of Erosheva et al (2004),
though the variables are given different meanings
here.7 The graphical model is depicted in Fig. 1
(left). As in LDA and its many variants, this model
postulates a set of latent ?topic? variables, where
each topic k corresponds to a multinomial distribu-
tion ?k over the vocabulary. In addition to gener-
ating the words in the post from its topic mixture,
this model also generates a bag of users who respond
to the post, according to a distribution ? over users
given topics. In this model, the topic distribution ?
is all that determines the text content of the post and
which users will respond to the post.
LinkLDA models which users are likely to re-
spond to a post, but it does not model what they
will write. Our new model, CommentLDA, gen-
erates the contents of the comments (see Fig. 1,
right). In order to capture the differences in lan-
guage style between posts and comments, however,
we use a different conditional distribution over com-
ment words given topics, ??. The post text, comment
text, and commenter distributions are all interdepen-
dent through the (latent) topic distribution ?, and a
topic k is defined by:
7Instead of blog commenters, they modeled citations.
? A multinomial distribution ?k over post words;
? A multinomial distribution ??k over comment
words; and
? A multinomial distribution ?k over blog com-
menters who might react to posts on the topic.
Formally, LinkLDA and CommentLDA generate
blog data as follows: For each blog post (1 to D):
1. Choose a distribution ? over topics according
to Dirichlet distribution ?.
2. For i from 1 to Ni (the length of the post):
(a) Choose a topic zi according to ?.
(b) Choose a word wi according to the topic?s
post word distribution ?zi .
3. For j from 1 to Mi (the length of the comments
on the post, in words):
(a) Choose a topic z?j .
(b) Choose an author uj from the topic?s com-
menter distribution ?z?j .
(c) (CommentLDA only) Choose a word w?j
according to the topic?s comment word
distribution ??z?j .
4.1 Variations on Counting Users
As described, CommentLDA associates each com-
ment word token with an independent author. In
both LinkLDA and CommentLDA, this ?counting
by verbosity? will force ? to give higher probabil-
ity to users who write longer comments with more
480
words. We consider two alternative ways to count
comments, applicable to both LinkLDA and Com-
mentLDA. These both involve a change to step 3 in
the generative process.
Counting by response (replaces step 3): For j from
1 to Ui (the number of users who respond to the
post): (a) and (b) as before. (c) (CommentLDA only)
For ` from 1 to `i,j (the number of words in uj?s
comments), choose w?` according to the topic?s com-
ment word distribution ??z?j . This model collapses allcomments by a user into a single bag of words on a
single topic.8
Counting by comments (replaces step 3): For j
from 1 to Ci (the number of comments on the post):
(a) and (b) as before. (c) (CommentLDA only) For `
from 1 to `i,j (the number of words in comment j),
choose w?` according to the topic?s comment word
distribution ??z?j . Intuitively, each comment has atopic, a user, and a bag of words.
The three variations?counting users by ver-
bosity, response, or comments?correspond to dif-
ferent ways of thinking about topics in political blog
discourse. Counting by verbosity will let garrulous
users define the topics. Counting by response is
more democratic, letting every user who responds
to a blog post get an equal vote in determining what
the post is about, no matter how much that user says.
Counting by comments gives more say to users who
engage in the conversation repeatedly.
4.2 Implementation
We train our model using empirical Bayesian esti-
mation. Specifically, we fix ? = 0.1, and we learn
the values of word distributions ? and ?? and user
distribution ? by maximizing the likelihood of the
training data:
p(w,w?,u | ?, ?, ??, ?) (1)
(Obviously, ?? is not present in the LinkLDA mod-
els.) This requires an inference step that marginal-
izes out the latent variables, ?, z, and z?, for which
we use Gibbs sampling as implemented by the Hier-
archical Bayes Compiler (Daume?, 2007). The Gibbs
8The counting-by-response models are deficient, since they
assume each user will only be chosen once per blog post, though
they permit the same user to be chosen repeatedly.
sampling inference algorithm for LDA was first in-
troduced by Griffiths and Steyvers (2004) and has
since been used widely.
5 Empirical Evaluation
We adopt a typical NLP ?train-and-test? strategy that
learns the model parameters on a training dataset
consisting of a collection of blog posts and their
commenters and comments, then considers an un-
seen test dataset from a later time period. Many
kinds of predictions might be made about the test
set and then evaluated against the true comment re-
sponse. For example, the likelihood of a user to
comment on the post, given knowledge of ? can be
estimated as:9
p(u | wN1 , ?, ?) =
K?
z=1
p(u | z, ?)p(z | wN1 , ?)
=
K?
z=1
?z,u ? ?z (2)
The latter is in a sense a ?guessing game,? a pre-
diction on who is going to comment on a new blog
post. A similar task was used by Nallapati and Co-
hen (2008) for assessing the performance of Link-
PLSA-LDA: they predicted the presence or absence
of citation links between documents. We report the
performance on this prediction task using our six
blog topic models (LinkLDA and CommentLDA,
with three counting variations each).
Our aim is to explore and compare the effective-
ness of the different models in discovering topics
that are useful for a practical task. We also give a
qualitative analysis of topics learned.
5.1 Comment Prediction
For each political blog, we trained the three varia-
tions each of LinkLDA and CommentLDA. Model
parameters ?, ?, and (in CommentLDA) ?? were
learned by maximizing likelihood, with Gibbs sam-
pling for inference, as described in ?4.2. The num-
ber of topics K was fixed at 15.
A simple baseline method makes a post-
independent prediction that ranks users by their
comment frequency. Since blogs often have a ?core
constituency? of users who post frequently, this is a
9Another approach would attempt to integrate out ?.
481
n=5 n=10 n=20 n=30 oracle
MY
Freq. 23.93 18.68 14.20 11.65 13.18
NB 25.13 19.28 14.20 11.63 13.54
Link-v 20.10 14.04 11.17 9.23 11.32
Link-r 26.77 18.63 14.64 12.47 14.03
Link-c 25.13 18.85 14.61 11.91 13.84
Com-v 22.84 17.15 12.75 10.69 12.77
Com-r 27.54 20.54 14.61 12.45 14.35
Com-c 22.40 18.50 14.83 12.56 14.20
Max 94.75 89.89 73.63 58.76 92.60
RWN
Freq. 32.56 30.17 22.61 19.7 27.19
NB 25.63 34.86 27.61 22.03 18.28
Link-v 28.14 21.06 17.34 14.51 19.81
Link-r 32.92 29.29 22.61 18.96 26.32
Link-c 32.56 27.43 21.15 17.43 25.09
Com-v 29.02 24.07 19.07 16.04 22.71
Com-r 36.10 29.64 23.8 19.26 25.97
Com-c 32.03 27.43 19.82 16.25 23.88
Max 90.97 76.46 52.56 37.05 96.16
CB
Freq. 33.38 28.84 24.17 20.99 21.63
NB 36.36 31.15 25.08 21.40 23.22
Link-v 32.06 26.11 19.79 17.43 18.31
Link-r 37.02 31.65 24.62 20.85 22.34
Link-c 36.03 32.06 25.28 21.10 23.44
Com-v 32.39 26.36 20.95 18.26 19.85
Com-r 35.53 29.33 24.33 20.22 22.02
Com-c 33.71 29.25 23.80 19.86 21.68
Max 99.66 98.34 88.88 72.53 95.58
RS
Freq. 25.45 16.75 11.42 9.62 17.15
NB 22.07 16.01 11.60 9.76 16.50
Link-v 14.63 11.9 9.13 7.76 11.38
Link-r 25.19 16.92 12.14 9.82 17.98
Link-c 24.50 16.45 11.49 9.32 16.76
Com-v 14.97 10.51 8.46 7.37 11.3 0
Com-r 15.93 11.42 8.37 6.89 10.97
Com-c 17.57 12.46 8.85 7.34 12.14
Max 80.77 62.98 40.95 29.03 91.86
DK
Freq. 24.66 19.08 15.33 13.34 9.64
NB 35.00 27.33 22.25 19.45 13.97
Link-v 20.58 19.79 15.83 13.88 10.35
Link-r 33.83 27.29 21.39 19.09 13.44
Link-c 28.66 22.16 18.33 16.79 12.60
Com-v 22.16 18.00 16.54 14.45 10.92
Com-r 33.08 25.66 20.66 18.29 12.74
Com-c 26.08 20.91 17.47 15.59 11.82
Max 100.00 100.00 100.00 99.09 98.62
Table 2: Comment prediction results on 5 blogs. See text.
strong baseline. We also compared to a Na??ve Bayes
classifier (with word counts in the post?s main en-
try as features). To perform the prediction task with
our models, we took the following steps. First, we
removed the comment section (both the words and
the authorship information) from the test data set.
Then, we ran a Gibbs sampler with the partial data,
fixing the model parameters to their learned values
and the blog post words to their observed values.
This gives a posterior topic mixture for each post (?
in the above equations).10 We then computed each
user?s comment prediction score for each post as in
Eq. 2. Users are ordered by their posterior probabil-
ities. Note that these posteriors have different mean-
ings for different variations:
? When counting by verbosity, the value is the prob-
ability that the next (or any) comment word will
be generated by the user, given the blog post.
? When counting by response, the value is the prob-
ability that the user will respond at all, given the
blog post. (Intuitively, this approach best matches
the task at hand.)
? When counting by comments, the value is the
probability that the next (or any) comment will be
generated by the user, given the blog post.
We compare our commenter ranking-by-
likelihood with the actual commenters in the test
set. We report in Tab. 2 the precision (macro-
averaged across posts) of our predictions at various
cut-offs (n). The oracle column is the precision
where it is equal to the recall, equivalent to the
situation when the true number of commenters
is known. (The performance of random guessing
is well below 1% for all sites at cut-off points
shown.) ?Freq.? and ?NB? refer to our baseline
methods. ?Link? refers to LinkLDA and ?Com? to
CommentLDA. The suffixes denote the counting
methods: verbosity (?-v?), response (?-r?), and
comments (?-c?). Recall that we considered only
the comments by the users seen at least once in the
training set, so perfect precision, as well as recall, is
impossible when new users comment on a post; the
Max row shows the maximum performance possible
given the set of commenters recognizable from the
training data.
10For a few cases we checked the stability of the sampler and
found results varied by less than 1% precision across ten runs.
482
Our results suggest that, if asked to guess 5 peo-
ple who would comment on a new post given some
site history, we will get 25?37% of them right, de-
pending on the site, given the content of a new post.
We achieved some improvement over both the
baseline and Na??ve Bayes for some cut-offs on three
of the five sites, though the gains were very small
for and RS and CB. LinkLDA usually works slightly
better than CommentLDA, except for MY, where
CommentLDA is stronger, and RS, where Com-
mentLDA is extremely poor. Differences in com-
menting style are likely to blame: MY has relatively
long comments in comparison to RS, as well as DK.
MY is the only site where CommentLDA variations
consistently outperformed LinkLDA variations, as
well as Na??ve Bayes classifiers. This suggests that
sites with more terse comments may be too sparse
to support a rich model like CommentLDA.
In general, counting by response works best,
though counting by comments is a close rival in
some cases. We observe that counting by response
tends to help LinkLDA, which is ignorant of the
word contents of the comment, more than it helps
CommentLDA. Varying the counting method can
bring as much as 10% performance gain.
Each of the models we have tested makes differ-
ent assumptions about the behavior of commenters.
Our results suggest that commenters on different
sites behave differently, so that the same modeling
assumptions cannot be made universally. In future
work, we hope to permit blog-specific properties
to be automatically discovered during learning, so
that, for example, the comment words can be ex-
ploited when they are helpful but assumed indepen-
dent when they are not. Of course, improved per-
formance might also be obtained with more topics,
richer priors over topic distributions, or models that
take into account other cues, such as the time of the
post, pages it links to, etc. It is also possible that bet-
ter performance will come from more sophisticated
supervised models that do not use topics.
5.2 Qualitative Evaluation
Aside from prediction tasks such as above, the
model parameters by themselves can be informative.
? defines which words are likely to occur in the post
body for a given topic. ?? tells which words are
likely to appear in the collective response to a partic-
ular topic. Similarity or divergence of the two dis-
tributions can tell us about differences in language
used by bloggers and their readers. ? expresses
users? topic preferences. A pair or group of par-
ticipants may be seen as ?like-minded? if they have
similar topic preferences (perhaps useful in collabo-
rative filtering).
Following previous work on LDA and its exten-
sions, we show words most strongly associated with
a few topics, arguing that some coherent clusters
have been discovered. Table 3 shows topics discov-
ered in MY using CommentLDA (counting by com-
ments). This is the blog site where our models most
consistently outperformed the Na??ve Bayes classi-
fiers and LinkLDA, therefore we believe the model
was a good fit for this dataset.
Since the site is concentrated on American pol-
itics, many of the topics look alike. Table 3 shows
the most probable words in the posts, comments, and
both together for five hand-picked topics that were
relatively transparent. The probabilistic scores of
those words are computed with the scoring method
suggested by Blei and Lafferty (in press).
The model clustered words into topics pertain-
ing to religion and domestic policy (first and last
topics in Table 3) quite reasonably. Some of the
religion-related words make sense in light of cur-
rent affairs.11 Some words in the comment sec-
tion are slightly off-topic from the issue of religion,
such as dawkins12 or wright,13 but are relevant in
the context of real-world events. Notice those words
rank highly only in the comment section, showing
differences between discussion in the post and the
comments. This is also noticeable, for example, in
the ?primary? topic (second in Table 3), where the
Republican primary receives more discussion in the
main post, and in the ?Iraq war? and ?energy? top-
ics, where bloggers discuss strategy and commenters
11Mitt Romney was a candidate for the Republican nomi-
nation in 2008 presidential election. He is a member of The
Church of Jesus Christ of Latter-Day Saints. Another candi-
date, Mike Huckabee, is an ordained Southern Baptist minister.
Moktada al-Sadr is an Iraqi theologian and political activist, and
John Hagee is an influential televangelist.
12Richard Dawkins is a well known evolutionary biologist
who is a vocal critic of intelligent design.
13We believe this is a reference to Rev. Jeremiah Wright of
Trinity United Church of Christ, whose inflammatory rhetoric
was negatively associated with then-candidate Barack Obama.
483
religion; in both: people, just, american, church, believe, god, black, jesus, mormon, faith, jews, right, say,
mormons, religious, point
in posts: romney, huckabee, muslim, political, hagee, cabinet, mitt, consider, true, anti, problem,
course, views, life, real, speech, moral, answer, jobs, difference, muslims, hardly, going,
christianity
in comments: religion, think, know, really, christian, obama, white, wright, way, said, good, world, science,
time, dawkins, human, man, things, fact, years, mean, atheists, blacks, christians
primary; in both: obama, clinton, mccain, race, win, iowa, delegates, going, people, state, nomination, primary,
hillary, election, polls, party, states, voters, campaign, michigan, just
in posts: huckabee, wins, romney, got, percent, lead, barack, point, majority, ohio, big, victory, strong,
pretty, winning, support, primaries, south, rules
in comments: vote, think, superdelegates, democratic, candidate, pledged, delegate, independents, votes,
white, democrats, really, way, caucuses, edwards, florida, supporters, wisconsin, count
Iraq war; in
both:
american, iran, just, iraq, people, support, point, country, nuclear, world, power, military,
really, government, war, army, right, iraqi, think
in posts: kind, united, forces, international, presence, political, states, foreign, countries, role, need,
making, course, problem, shiite, john, understand, level, idea, security, main
in comments: israel, sadr, bush, state, way, oil, years, time, going, good, weapons, saddam, know, maliki,
want, say, policy, fact, said, shia, troops
energy; in both: people, just, tax, carbon, think, high, transit, need, live, going, want, problem, way, market,
money, income, cost, density
in posts: idea, public, pretty, course, economic, plan, making, climate, spending, economy, reduce,
change, increase, policy, things, stimulus, cuts, low, fi nancial, housing, bad, real
in comments: taxes, fuel, years, time, rail, oil, cars, car, energy, good, really, lot, point, better, prices, pay,
city, know, government, price, work, technology
domestic policy;
in both:
people, public, health, care, insurance, college, schools, education, higher, children, think,
poor, really, just, kids, want, school, going, better
in posts: different, things, point, fact, social, work, large, article, getting, inequality, matt, simply,
percent, tend, hard, increase, huge, costs, course, policy, happen
in comments: students, universal, high, good, way, income, money, government, class, problem, pay, amer-
icans, private, plan, american, country, immigrants, time, know, taxes, cost
Table 3: The most probable words for some CommentLDA topics (MY).
focus on the tangible (oil, taxes, prices, weapons).
While our topic-modeling approach achieves
mixed results on the prediction task, we believe it
holds promise as a way to understand and summa-
rize the data. Without CommentLDA, we would not
be able to easily see the differences noted above in
blogger and commenter language. In future work,
we plan to explore models with weaker indepen-
dence assumptions among users, among blog posts
over time, and even across blogs. This line of re-
search will permit a more nuanced understanding
of language in the blogosphere and in political dis-
course more generally.
6 Conclusion
In this paper we applied several probabilistic topic
models to discourse within political blogs. We in-
troduced a novel comment prediction task to assess
these models in an objective evaluation with possi-
ble practical applications. The results show that pre-
dicting political discourse behavior is challenging,
in part because of considerable variation in user be-
havior across different blog sites. Our results show
that using topic modeling, we can begin to make rea-
sonable predictions as well as qualitative discoveries
about language in blogs.
Acknowledgments
This research was supported by a gift from Microsoft
Research and NSF IIS-0836431. The authors appreciate
helpful comments from the anonymous reviewers, Ja-Hui
Chang, Hal Daume?, and Ramesh Nallapati. We thank
Shay Cohen for his help with inference algorithms and
the members of the ARK group for reviewing this paper.
484
References
L. Adamic and N. Glance. 2005. The political blogo-
sphere and the 2004 U.S. election: Divided they blog.
In Proceedings of the 2nd Annual Workshop on the We-
blogging Ecosystem: Aggregation, Analysis and Dy-
namics.
D. Blei and M. Jordan. 2003. Modeling annotated data.
In Proceedings of the 26th Annual International ACM
SIGIR Conference on Research and Development in
Informaion Retrieval.
D. Blei and J. Lafferty. In press. Topic models. In A. Sri-
vastava and M. Sahami, editors, Text Mining: Theory
and Applications. Taylor and Franci.
D. Blei and J. McAuliffe. 2008. Supervised topic mod-
els. In Advances in Neural Information Processing
Systems 20.
D. Blei, A. Ng, and M. Jordan. 2003. Latent Dirich-
let alocation. Journal of Machine Learning Research,
3:993?1022.
S. R. K. Branavan, H. Chen, J. Eisenstein, and R. Barzi-
lay. 2008. Learning document-level semantic prop-
erties from free-text annotations. In Proceedings of
ACL-08: HLT.
D. Cohn and T. Hofmann. 2001. The missing link?a
probabilistic model of document content and hyper-
text connectivity. In Neural Information Processing
Systems 13.
H. Daume?. 2007. HBC: Hierarchical Bayes compiler.
http://www.cs.utah.edu/?hal/HBC.
M. Dredze, H. M. Wallach, D. Puller, and F. Pereira.
2008. Generating summary keywords for emails us-
ing topics. In Proceedings of the 13th International
Conference on Intelligent User Interfaces.
E. Erosheva, S. Fienberg, and J. Lafferty. 2004. Mixed
membership models of scientific publications. Pro-
ceedings of the National Academy of Sciences, pages
5220?5227, April.
T. L. Griffiths and M. Steyvers. 2004. Finding scien-
tific topics. Proceedings of the National Academy of
Sciences, 101 Suppl. 1:5228?5235, April.
W.-H. Lin, E. Xing, and A. Hauptmann. 2008. A joint
topic and perspective model for ideological discourse.
In Proceedings of 2008 European Conference on Ma-
chine Learning and Principles and Practice of Knowl-
edge Discovery in Databases.
R. Malouf and T. Mullen. 2007. Graph-based user clas-
sification for informal online political discourse. In
Proceedings of the 1st Workshop on Information Cred-
ibility on the Web.
A. McCallum. 1999. Multi-label text classification with
a mixture model trained by EM. In AAAI Workshop on
Text Learning.
T. Mullen and R. Malouf. 2006. A preliminary investi-
gation into sentiment analysis of informal political dis-
course. In Proceedings of AAAI-2006 Spring Sympo-
sium on Computational Approaches to Analyzing We-
blogs.
R. Nallapati and W. Cohen. 2008. Link-PLSA-LDA: A
new unsupervised model for topics and influence of
blogs. In Proceedings of the 2nd International Con-
ference on Weblogs and Social Media.
M. Rosen-Zvi, T. Griffiths, M. Steyvers, and P Smyth.
2004. The author-topic model for authors and docu-
ments. In Proceedings of the 20th Conference on Un-
certainty in Artificial Intelligence.
M. Steyvers and T. Griffiths. 2007. Probabilistic topic
models. In T. Landauer, D. Mcnamara, S. Dennis,
and W. Kintsch, editors, Handbook of Latent Semantic
Analysis. Lawrence Erlbaum Associates.
M. Steyvers, P. Smyth, M. Rosen-Zvi, and T. L. Grif-
fiths. 2004. Probabilistic author-topic models for in-
formation discovery. In Proceedings of the tenth ACM
SIGKDD international conference on Knowledge dis-
covery and data mining.
I. Titov and R. McDonald. 2008. A joint model of text
and aspect ratings for sentiment summarization. In
Proceedings of ACL-08: HLT.
H. Zhang, B. Qiu, C. L. Giles, H. C. Foley, and J. Yen.
2007. An LDA-based community structure discovery
approach for large-scale social networks. In Proceed-
ings of the IEEE International Conference on Intelli-
gence and Security Informatics.
485
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 793?802,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Textual Predictors of Bill Survival in Congressional Committees
Tae Yano Noah A. Smith
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{taey,nasmith}@cs.cmu.edu
John D. Wilkerson
Department of Political Science
University of Washington
Seattle, WA 98195, USA
jwilker@u.washington.edu
Abstract
A U.S. Congressional bill is a textual artifact
that must pass through a series of hurdles to
become a law. In this paper, we focus on one
of the most precarious and least understood
stages in a bill?s life: its consideration, behind
closed doors, by a Congressional committee.
We construct predictive models of whether a
bill will survive committee, starting with a
strong, novel baseline that uses features of the
bill?s sponsor and the committee it is referred
to. We augment the model with information
from the contents of bills, comparing different
hypotheses about how a committee decides a
bill?s fate. These models give significant re-
ductions in prediction error and highlight the
importance of bill substance in explanations of
policy-making and agenda-setting.
1 Introduction
In representative governments, laws result from a
complex social process. Central to that process is
language. Text data emerging from the process in-
clude debates among legislators (Laver et al, 2003;
Quinn et al, 2010; Beigman Klebanov et al, 2008),
press releases (Grimmer, 2010), accounts of these
debates in the press, policy proposals, and laws.
In the work reported here, we seek to exploit text
data?specifically, the text of Congressional bills?
to understand the lawmaking process. We consider
an especially murky part of that process that is dif-
ficult to study because it happens largely behind
closed doors: the handling of bills by Congressional
committees. This early stage of a bill?s life is precar-
ious: roughly 85% of bills do not survive commit-
tee. By contrast, nearly 90% of bills that are recom-
mended by a committee (i.e., survive the committee
and are introduced for debate on the floor) will sur-
vive a roll call vote by the legislature. Because fil-
tering by these powerful Congressional committees
is both more opaque and more selective than the ac-
tions of the legislature as a whole, we believe that
text-based models can play a central role in under-
standing this stage of lawmaking.
This paper?s contributions are: (i) We formu-
late computationally the prediction of which bills
will a survive Congressional committee, presenting
a (baseline) model based on observable features as-
sociated with a bill, the committee(s) it is assigned
to, members of that committee, the Congress as a
whole, and expert combinations of those features.
The task formulation and baseline model are novel.
(ii) We propose several extensions of that strong
baseline with information derived from the text of
a bill. (iii) We validate our models on a hard predic-
tive task: predicting which bills will survive com-
mittee. Text is shown to be highly beneficial. (iv)
We present a discussion of the predictive features se-
lected by our model and what they suggest about the
underlying political process. (v) We release our cor-
pus of over 50,000 bills and associated metadata to
the research community for further study.1
We give brief background on how bills become
U.S. laws in ?2. We describe our data in ?3. The
modeling framework and baseline are then intro-
duced (?4), followed by our text-based models with
experiments (?5), then further discussion (?6).
1http://www.ark.cs.cmu.edu/bills
793
2 How Bills Become Laws
In the U.S., federal laws are passed by the
U.S. Congress, which consists of two ?chambers,?
the House of Representatives (commonly called the
?House?) and the Senate. To become law, a bill (i.e.,
a proposed law) must pass a vote in both chambers
and then be signed by the U.S. President. If the Pres-
ident refuses to sign a bill (called a ?veto?), it may
still become law if both chambers of Congress over-
rides the veto through a two-thirds majority.
Much less discussed is the process by which bills
come into existence. A bill is formally proposed
by a member of Congress, known as its sponsor.
Once proposed, it is routed to one or more (usu-
ally just one) of about twenty subject-specializing
committees in each chamber. Unlike floor proceed-
ings, transcripts of the proceedings of Congressional
committees are published at the discretion of the
committee and are usually publicly unavailable.
Each committee has a chairman (a member of the
majority party in the chamber) and is further divided
into subcommittees. Collectively a few thousand
bills per year are referred to Congress? committees
for consideration. Committees then recommend (re-
port) only about 15% for consideration and voting
by the full chamber.
The U.S. House is larger (435 voting members
compared to 100 in the Senate) and, in recent his-
tory, understood to be more polarized than the Sen-
ate (McCarty et al, 2006). All of its seats are up
for election every two years. A ?Congress? often
refers to a two-year instantiation of the body with a
particular set of legislators (e.g., the 112th Congress
convened on January 3, 2011 and adjourns on Jan-
uary 3, 2013). In this paper, we limit our attention
to bills referred to committees in the House.
3 Data
We have collected the text of all bills introduced in
the U.S. House of Representatives from the 103rd
to the 111th Congresses (1/3/1993?1/3/2011). Here
we consider only the version of the bill as originally
introduced. After introduction, a bill?s title and con-
tents can change significantly, which we ignore here.
These bills were downloaded directly from the
Library of Congress?s Thomas website.2 Informa-
2http://thomas.loc.gov/home/thomas.php
Cong. Maj.
Total Survival Rate (%)
Introduced Total Rep. Dem.
103 Dem. 5,311 11.7 3.4 16.2
104 Rep. 4,345 13.7 19.7 6.1
105 Rep. 4,875 13.2 19.0 5.4
106 Rep. 5,682 15.1 20.9 7.0
107 Rep. 5,768 12.1 17.5 5.8
108 Rep. 5,432 14.0 21.0 5.9
109 Rep. 6,437 11.8 16.9 5.1
110 Dem. 7,341 14.5 8.5 18.0
111 Dem. 6,571 12.6 8.1 14.5
Total 51,762 13.2 15.9 10.7
Table 1: Count of introduced bills per Congress, along
with survival rate, and breakdown by the bill sponsor?s
party affiliation. Note that the probability of survival in-
creases by a factor of 2?5 when the sponsor is in the ma-
jority party. Horizontal lines delineate presidential ad-
ministrations (Clinton, Bush, and Obama).
tion about the makeup of House committees was
obtained from Charles Stewart?s resources at MIT,3
while additional sponsor and bill information (e.g.,
sponsor party affiliation and bill topic) was obtained
from E. Scott Adler and John Wilkerson?s Congres-
sional Bills Project at the University of Washing-
ton.4
In our corpus, each bill is associated with its title,
text, committee referral(s), and a binary value indi-
cating whether or not the committee reported the bill
to the chamber. We also extracted metadata, such as
sponsor?s name, from each bill?s summary page pro-
vided by the Library of Congress.
There were a total of 51,762 bills in the House
during this seventeen-year period, of which 6,828
survived committee and progressed further. See Ta-
ble 1 for the breakdown by Congress and party.
In this paper, we will consider a primary train-test
split of the bills by Congress, with the 103rd?110th
Congresses serving as the training dataset and the
111th as the test dataset. This allows us to simulate
the task of ?forecasting? which bills will survive in a
future Congress. In ?5.5, we will show that a similar
result is obtained on different data splits.
These data are, in principle, ?freely available?
to the public, but they are not accessible in a uni-
3http://web.mit.edu/17.251/www/data_
page.html
4http://congressionalbills.org
794
fied, structured form. Considerable effort must
be expended to align databases from a variety of
sources, and significant domain knowledge about
the structure of Congress and its operation is re-
quired to disambiguate the data. Further exploration
of the deeper relationships among the legislators,
their roles in past Congresses, their standing with
their constituencies, their political campaigns, and
so on, will require ongoing effort in joining data
from disparate sources.
When we consider a larger goal of understanding
legislative behavior across many legislative bodies
(e.g., states in the U.S., other nations, or interna-
tional bodies), the challenge of creating and main-
taining such reliable, clean, and complete databases
seems insurmountable.
We view text content?noisy and complex as it
is?as an attractive alternative, or at least a comple-
mentary information source. Though unstructured,
text is made up of features that are relatively easy
for humans to interpret, offering a way to not only
predict, but also explain legislative outcomes.
4 A Predictive Model
We next consider a modeling framework for predict-
ing bill survival or death in committee. We briefly
review logistic regression models (section 4.1), then
turn to the non-textual features that form a baseline
and a starting point for the use of text (section 4.2).
4.1 Modeling Framework
Our approach to predicting a bill?s survival is logis-
tic regression. Specifically, let X be a random vari-
able associated with a bill, and let f be a feature vec-
tor function that encodes observable features of the
bill. Let Y be a binary random variable correspond-
ing to bill survival (Y = 1) or death (Y = 0). Let:
pw(Y = 1 | X = x) =
expw>f(x)
1 + expw>f(x)
(1)
where w are ?weight? parameters associating each
feature in the feature vector f(x) with each outcome.
This leads to the predictive rule:
y?(x) =
{
1 if w>f(x) > 0
0 otherwise
(2)
We train the model by maximizing log-likelihood
plus a a sparsity-inducing log-prior that encourages
many weights to go to zero:
maxw
?
i log pw(yi | xi)? ??w?1 (3)
where i indexes training examples (specifically, each
training instance is a bill referred to a single com-
mittee). The second term is an `1 norm, equivalent
to a Laplacian prior on the weights. The value of
?, which controls sparsity, is chosen on a held-out
subset of the training data.
Linear models like this one, commonly called
?exponential? or ?max ent? models, are attractive
because they are intelligible. The magnitude of a
weight indicates a feature?s importance in the pre-
diction, and its sign indicates the direction of the ef-
fect.
We note that the `1 regularizer is not ideal for
identifying predictive features. When two features
are strongly correlated, it tends to choose one of
them to include in the model and eliminate the other,
despite the fact that they are both predictive. It is
therefore important to remember that a weight of
zero does not imply that the corresponding feature
is unimportant. We chose to cope with this poten-
tial elimination of good features so that our models
would be compact and easily interpretable.
4.2 Features
In American politics, the survival or death of many
bills can be explained in terms of expertise, en-
trepreneurship, and procedural control, which are
manifest in committee membership, sponsor at-
tributes, and majority party affiliation. We there-
fore begin with a strong baseline that includes fea-
tures encoding many expected effects on bill suc-
cess. These include basic structural features and
some interactions.
The basic features are all binary. The value of
the random variable X includes information about
the bill, its sponsor, and the committee to which the
bill is referred. In addition to a bias feature (always
equal to 1), we include the following features:
1. For each party p, is the bill?s sponsor affiliated with
p?
2. Is the bill?s sponsor in the same party as the com-
mittee chair? Equivalently, is the bill?s sponsor in
the majority party of the House?
3. Is the bill?s sponsor a member of the committee?
795
4. Is the bill?s sponsor a majority member of the com-
mittee? (This feature conjoins 2 and 3.)
5. Is the bill?s sponsor the chairman of the committee?
6. For each House member j, did j sponsor the bill?
7. For each House member j, is the bill sponsored by j
and referred to a committee he chairs? (This feature
conjoins 5 and 6.)
8. For each House member j, is the bill sponsored by
j and is j in the same party as the committee chair?
(This feature conjoins 2 and 6.)
9. For each state s, is the bill?s sponsor from s?
10. For each month m, is the bill introduced during m?
11. For v ? {1, 2}, is the bill introduced during the vth
year of the (two-year) Congress?
The features above were engineered in prelimi-
nary model development, before text was incorpo-
rated.5
4.3 Experiment
Performance. Considering the 111th Congress as a
test set (6,571 instances), a most-frequent-class pre-
dictor (i.e., a constant prediction that no bill will
survive committee) achieves an error rate of 12.6%
(more details in Table 3). A model trained on
the 103rd?110th Congresses (45,191 bills) contains
3,731 instantiated features above achieved 11.8% er-
ror (again, see Table 3).
Discussion. When inspecting linear models, consid-
ering feature weights can be misleading, since (even
with regularization) large weights often correspond
to small effects in the training data. Our method-
ology for inspecting models is therefore as follows:
we calculate the impact of each feature on the final
decision for class y, defined for feature j as
wj
N
?N
i=1 fj(xi) (4)
where i indexes test examples (of which there are
N ). Impact is the average effect of a feature on the
model?s score for class y. Note that it is not affected
5One surprisingly detrimental feature, omitted here, was
the identity of the committee. Bill success rates vary greatly
across committees (e.g., Appropriations recommends about half
of bills, while Ways and Means only 7%). We suspect that
this feature simply has poor generalization ability across Con-
gresses. (In ?5.2 we will consider preferences of individuals on
committees, based on text, which appears to benefit predictive
performance.)
Bill Survival
sponsor is in the majority party (2) 0.525
sponsor is in the majority party and on the
committee (4)
0.233
sponsor is a Democrat (1) 0.135
sponsor is on the committee (3) 0.108
bill introduced in year 1 (11) 0.098
sponsor is the referred committee?s chair (5) 0.073
sponsor is a Republican (1) 0.069
Bill Death
bill?s sponsor is from NY (9) -0.036
sponsor is Ron Paul (Rep., TX) (6) -0.023
bill introduced in December (10) -0.018
sponsor is Bob Filner (Dem., CA) (6) -0.013
Table 2: Baseline model: high-impact features associated
with each outcome and their impact scores (eq. 4).
by the true label for an example. Impact is addi-
tive, which allows us to measure and compare the
influence of sets of features within a model on model
predictions. Impact is not, however, directly compa-
rable across models.
The highest impact features are shown in Table 2.
Unsurprisingly, the model?s predictions are strongly
influenced (toward survival) when a bill is sponsored
by someone who is on the committee and/or in the
majority party. Feature 2, the sponsor being on the
committee, accounted for nearly 27% of all (abso-
lute) impact, followed by the member-specific fea-
tures (6?8, 19%), the sponsor being in the majority
and on the committee (4, 12%), and the party of the
sponsor (1, 10%).
We note that impact as a tool for interpreting mod-
els has some drawbacks. If a large portion of bills
in the test set happen to have a particular feature,
that feature may have a high impact score for the
dominant class (death). This probably explains the
high impact of ?sponsor is a Democrat? (Table 2);
Democrats led the 111th Congress, and introduced
more bills, most of which died.
5 Adding Text
We turn next to the use of text data to augment the
predictive power of our baseline model. We will
propose three ways of using the title and/or text of
a bill to create features. From a computational per-
spective, each approach merely augments the base-
line model with features that may reduce predictive
796
errors?our measure of the success of the hypothe-
sis. From a political science perspective, each pro-
posal corresponds to a different explanation of how
committees come to decisions.
5.1 Functional Bill Categories
An important insight from political science is that
bills can be categorized in general ways that are re-
lated to their likelihood of success. In their study on
legislative success, Adler and Wilkerson (2005) dis-
tinguish Congressional bills into several categories
that capture bills that are on the extremes in terms
of the importance and/or urgency of the issue ad-
dressed. We expect to find that distinguishing bills
by their substance will reduce prediction errors.
? bills addressing trivial issues, such as those nam-
ing a federal building or facility or coining com-
memorative medals;
? bills that make technical changes to existing laws,
usually at the request of the executive agency re-
sponsible for its implementation;
? bills addressing recurring issues, such as annual
appropriations or more sporadic reauthorizations
of expiring federal programs or laws; and
? bills addressing important, urgent issues, such as
bills introduced in response to the 9/11 terrorist
attacks or a sharp spike in oil prices.
Adler and Wilkerson (2005) annotated House bills
for the 101st?105th Congresses using the above cat-
egories (all other bills were deemed to be ?discre-
tionary?). Out of this set we use the portion that
overlaps with our bill collection (103rd?105th). Of
14,528 bills, 1,580 were labeled as trivial, 119 as
technical, 972 as recurring, and 1,508 as important.
Our hypothesis is that these categories can help ex-
plain which bills survive committees.
To categorize the bills in the other Congresses
of our dataset, we trained binary logistic regression
models to label bills with each of the three most fre-
quent bill types above (trivial, recurring, and impor-
tant) based on unigram features of the body of bill
text. (There is some overlap among categories in the
annotated data, so we opted for three binary clas-
sifiers rather than multi-class.) In a ten-fold cross-
validated experiment, this model averaged 83% ac-
curacy across the prediction tasks. We used the man-
ually annotated labels for the bills in the 103rd?
105th Congresses; for other bills, we calculated each
model?s probability that the bill belonged to the tar-
get category.6 These values were used to define bi-
nary indicators for each classifier?s probability re-
gions: [0, 0.3); [0.3, 0.4); [0.4, 0.5); [0.5, 1.0]. For
each of the three labels, we included two classifiers
trained with different hyperparameter settings, giv-
ing a total of 24 additional features. All baseline
features were retained.
Performance. Including functional category fea-
tures reduces the prediction error slightly but signif-
icantly relative to the baseline (just over 1% relative
error reduction)?see Table 3.7
Discussion. Considering the model?s weights, the
log-odds are most strongly influenced toward bill
success by bills that seem ?important? according to
the classifiers. 55% of this model?s features had non-
zero impact on test-set predictions; compare this to
only 36% of the baseline model?s features.8 Further,
the category features accounted for 66% of the total
(absolute) impact of all features. Taken altogether,
these observations suggest that bill category features
are a more compact substitute for many of the base-
line features,9 but that they do not offer much ad-
ditional predictive information beyond the baseline
(error is only slightly reduced). It is also possi-
ble that our categories do not perfectly capture the
perceptions of committees making decisions about
bills. Refinement of the categories within the pre-
6In preliminary experiments, we used the 103rd?105th data
to measure the effect of automatic vs. manual categories.
Though the particulars of the earlier model and the smaller
dataset size make controlled comparison impossible, we note
that gold-standard annotations achieved 1?2% lower absolute
error across cross-validation folds.
7We note that preliminary investigations conjoining the bill
category features with baseline features did not show any gains.
Prior work by Adler and Wilkerson (2012) suggests that bill cat-
egory interacts with the sponsor?s identity, but does not consider
bill success prediction; we leave a more careful exploration of
this interaction in our framework to future work.
8Note that `1-regularized models make global decisions
about which features to include, so the new features influence
which baseline features get non-zero weights. Comparing the
absolute number of features in the final selected models is not
meaningful, since it depends on the hyperparameter ?, which is
tuned separately for each model.
9This substitutability is unsurprising in some scenarios; e.g.,
successful reauthorization bills are often sponsored by commit-
tee leadership.
797
Model Error (%) False + False ? True + # Feats. Size Effective
most frequent class 12.6 0 828 0 ? ? ?
?4.2 baseline (no text) 11.8 69 709 119 3,731 1,284 460
?5.1 bill categories 11.7 52 716 112 3,755 274 152
?5.2
proxy vote, chair only 10.8 111 596 232 3,780 1,111 425
proxy vote, majority 11.3 134 606 222 3,777 526 254
proxy vote, whole committee 10.9 123 596 232 3,777 1,131 433
proxy vote, all three 10.9 110 606 222 3,872 305 178
?5.3 unigram & bigram 9.8 106 541 287 28,246 199 194
?5.4 full model (all of the above) 9.6 120 514 314 28,411 1,096 1,069
Table 3: Key experimental results; models were trained on the 103rd?110th Congresses and tested on the 111th.
Baseline features are included in each model listed below the baseline. ?# Feats.? is the total number of features
available to the model; ?Size? is the number of features with non-zero weights in the final selected sparse model;
?Effective? is the number of features with non-zero impact (eq. 4) on test data. Each model?s improvement over the
baseline is significant (McNemar?s test, p < 0.0001 except bill categories, for which p < 0.065).
dictive framework we have laid out here is left to
future research.
5.2 Textual Proxy Votes
We next consider a different view of text: as a means
of profiling the preferences and agendas of legisla-
tors. Our hypothesis here is that committees oper-
ate similarly to the legislature as a whole: when a
bill comes to a committee for consideration, mem-
bers of the committee vote on whether it will sur-
vive. Of course, deliberation and compromise may
take place before such a vote; our simple model does
not attempt to account for such complex processes,
instead merely positing a hidden roll call vote.
Although the actions of legislators on commit-
tees are hidden, their voting behavior on the floor
is observed. Roll call data is frequently used in po-
litical science to estimate spatial models of legis-
lators and legislation (Poole and Rosenthal, 1985;
Poole and Rosenthal, 1991; Jackman, 2001; Clinton
et al, 2004). These models help visualize politics in
terms of intuitive, low-dimensional spaces which of-
ten correspond closely to our intuitions about ?left?
and ?right? in American politics. Recently, Gerrish
and Blei (2011) showed how such models could nat-
urally be augmented with models of text. Such mod-
els are based on observed voting; it is left to future
work to reduce the dimensionality of hidden votes
within the survival prediction model here.
Our approach is to construct a proxy vote; an es-
timate of a roll call vote by members of the com-
mittee on the bill. We consider three variants, each
based on the same estimate of the individual com-
mittee members? votes:
? Only the committee chairman?s vote matters.
? Only majority-party committee members vote.
? All committee members vote.
We will compare these three versions of the proxy
vote feature experimentally, but abstractly they can
all be defined the same way. Let C denote the set of
committee members who can vote on a bill x. Then
the proxy vote equals:
1
|C|
?
j?C E[Vj,x] (5)
(If x is referred to more than one committee, we av-
erage the above feature across committees.) We treat
the vote by representative j on bill x as a binary ran-
dom variable Vj,x corresponding to a vote for (1) or
against (0) the bill. We do not observe Vj,x; instead
we estimate its expected value, which will be be-
tween 0 and 1. Note that, by linearity of expecta-
tion, the sum in equation 5 is the expected value of
the number of committee members who ?voted? for
the bill; dividing by |C| gives a value that, if our esti-
mates are correct, should be close to 1 when the bill
is likely to be favored by the committee and 0 when
it is likely to be disfavored.
To estimate E[Vj,x], we use a simple probabilis-
tic model of Vj,x given the bill x and the past vot-
ing record of representative j.10 Let Rj be a set of
10We note that the observable roll call votes on the floor of
798
bills that representative j has publicly voted on, on
the floor of the House, in the past.11 For x ? Rj ,
let Vj,x be 1 if j voted for the bill and 0 if j voted
against it. Further, define a similarity measure be-
tween bills; here we use cosine similarity of two
bills? tfidf vectors.12 We denote by sim(x, x?) the
similarity of bills x and x?.
The probabilistic model is as follows. First, the
representative selects a bill he has voted on previ-
ously; he is likely to choose a bill that is similar to
x. More formally, given representative j and bill x,
randomly choose a bill X ? from Rj according to:
p(X ? = x? | j, x) = exp sim(x,x
?)P
x???Rj
exp sim(x,x??) (6)
An attractive property of this distribution is that it
has no parameters to estimate; it is defined entirely
by the text of bills in Rj . Second, the representa-
tive votes on x identically to how he voted on X ?.
Formally, let Vj,x = Vj,x? , which is observed.
The above model gives a closed form for the ex-
pectation of Vj,x:
E[Vj,x] =
?
x??Rj p(X
? = x? | j, x) ? Vj,x? (7)
In addition to the proxy vote score in eq. 5, we cal-
culate a similar expected vote based on ?nay? votes,
and consider a second score that is the ratio of the
?yea? proxy vote to the ?nay? proxy vote. Both
of these scores are continuous values; we quantize
them into bins, giving 141 features.13
Performance. Models built using the baseline fea-
tures plus, in turn, each of the three variations of the
proxy vote feature (C defined to include the chair
the U.S. House consist of a very different sample of bills than
those we consider in this study; indeed, votes on the floor cor-
respond to bills that survived committee. We leave attempts to
characterize and control for this bias to future work.
11To simplify matters, we use all bills from the training pe-
riod that j has voted on. For future predictions (on the test set),
these are all in the past, but in the training set they may include
bills that come later than a given training example.
12We first eliminated punctutation and numbers from the
texts, then removed unigrams which occured in more than 75%
or less than 0.05% of the training documents. Tfidf scores were
calculated based on the result.
13We discretized the continuous values by 0.01 increment for
proxy vote score, and 0.1 increment for proxy vote rate scores.
We further combined outlier bins (one for exremely large val-
ues, one for extremely small values).
only, majority party members, or the full commit-
tee), and all three sets of proxy vote features, were
compared?see Table 3. All three models showed
improvement over the baseline. Using the chairman-
only committee (followed closely by whole commit-
tee and all three) turned out to be the best performing
among them, with a 8% relative error reduction.
Discussion. Nearly 58% of the features in the com-
bined model had non-zero impact at test time, and
38% of total absolute impact was due to these fea-
tures. Comparing the performance of these four
models suggests that, as is widely believed in polit-
ical science, the preferences of the committee chair
are a major factor in which bills survive.
5.3 Direct Use of Content: Bag of Words
Our third hypothesis is that committees make collec-
tive decisions by considering the contents of bills di-
rectly. A sensible starting point is to treat our model
as a document classifier and incorporate standard
features of the text directly into the model, rather
than deriving functional categories or proxy votes
from the text.14 Perhaps unsurprisingly, this ap-
proach will perform better than the previous two.
Following Pang and Lee (2004), who used word
and bigram features to model an author?s sentiment,
and Kogan et al (2009), who used word and bigram
features to directly predict a future outcome, we in-
corporate binary features for the presence or absence
of terms in the body and (separately) in the title of
the bill. We include unigram features for the body
and unigram and bigram features for the title.15 The
result is 28,246 features, of which 24,515 are lexical.
Performance. Combined with baseline features,
word and bigram features led to nearly 18% relative
error reduction compared to the baseline and 9% rel-
ative to the best model above (Table 3). The model
is very small (under 200 features), and 98% of the
features in the model impacted test-time predictions.
The model?s gain over the baseline is not sensitive to
the score threshold; see Figure 1.
A key finding is that the bag of words model out-
14The models from ?5.1 and ?5.2 can be understood from
a machine learning perspective as task-specific dimensionality
reduction methods on the words.
15Punctuation marks are removed from the text, and numbers
are collapsed into single indicator. We filtered terms appearing
in fewer than 0.5% and more than 30% of training documents.
799
Bill Survival Bill Death
Contents Title Contents Title
resources 0.112 title as 0.052 percent -0.074 internal -0.058
ms 0.056 other purposes 0.041 revenue -0.061 the internal 0.024
authorization 0.053 for other 0.028 speaker -0.050 revenue -0.022
information 0.049 amended by 0.017 security -0.037 prohibit -0.020
authorize 0.030 of the 0.017 energy -0.037 internal revenue -0.019
march 0.029 for the 0.014 make -0.030 the social -0.018
amounts 0.027 public 0.012 require -0.029 amend title -0.016
its 0.026 extend 0.011 human -0.029 to provide -0.015
administration 0.026 designate the 0.010 concerned -0.029 establish -0.015
texas 0.024 as amended 0.009 department -0.027 SYMBOL to -0.014
interior 0.023 located 0.009 receive -0.025 duty on -0.013
judiciary 0.021 relief 0.009 armed -0.024 revenue code -0.013
Table 4: Full model:
text terms with
highest impact
(eq. 4). Impact
scores are not
comparable across
models, so for com-
parison, the impacts
for the features from
Table 2 here are,
respectively: 0.534,
0.181, 10?4, 0.196,
0.123, 0.063, 0.053;
-0.011, 0, 0.003, 0.
0.0 0.2 0.4 0.6 0.8
0.2
0.4
0.6
0.8
1.0
Recall
Pre
cis
ion
Bag of Words
Baseline
Figure 1: Precision-recall curve (survival is the target
class) comparing the bag of words model to the baseline.
performs the bill categories and proxy vote models.
This suggests that there is more information in the
text contents than either the functional categories or
similarity to past bills.16
5.4 Full Model
Finally, we considered a model using all three kinds
of text features. Shown in Table 3, this reduces error
only 2% relative to the bag of words model. This
leads us to believe that direct use of text captures
most of what functional bill category and proxy vote
features capture about bill success.
16We also experimented with dimensionality reduction with
latent Dirichlet alocation (Blei et al, 2003). We used the topic
posteriors as features in lieu of words during training and test-
ing. The symmetric Dirichlet hyperparameter was fixed at 0.1,
and we explored 10?200 topics. Although this offered speedups
in training time, the performance was consistently worse than
the bag of words model, for each number of topics.
Table 4 shows the terms with greatest impact.
When predicting bills to survive, the model seems
to focus on explanations for minor legislation. For
example, interior and resources may indicate non-
controversial local land transfer bills. In titles, des-
ignate and located have to do with naming federal
buildings (e.g., post offices).
As for bills that die, the model appears to have
captured two related facts about proposed legisla-
tion. One is that legislators often sponsor bills to
express support or concern about an issue with little
expectation that the bill will become a law. If such
?position-taking? accounts for many of the bills pro-
posed, then we would expect features with high im-
pact toward failure predictions to relate to such is-
sues. This would explain the terms energy, security,
and human (if used in the context of human rights or
human cloning). The second fact is that some bills
die because committees ultimately bundle their con-
tents into bigger bills. There are many such bills re-
lating to tax policy (leading to the terms contained in
the trigram Internal Revenue Service, the American
tax collection agency) and Social Security policy (a
collection of social welfare and social insurance pro-
grams), for example.17
17The term speaker likely refers to the first ten bill numbers,
which are ?reserved for the speaker,? which actually implies that
no bill was introduced. Our process for marking bills that sur-
vive (based on committee recommendation data) leaves these
unmarked, hence they ?died? in our gold-standard data. The
experiments revealed this uninteresting anomaly.
800
Model
Error (%)
109th 110th
most frequent class 11.8 14.5
?4.2 baseline (no text) 11.1 13.9
?5.1 bill categories 10.9 13.6
?5.2 proxy vote, all three 9.9 12.7
?5.3 unigram & bigram 8.9 10.6
?5.4 full model 8.9 10.9
Table 5: Replicated results on two different data splits.
Columns are marked by the test-set Congress. See ?5.5.
5.5 Replication
To avoid drawing conclusions based on a single,
possibly idiosyncratic Congress, we repeated the ex-
periment using the 109th and 110th Congresses as
test datasets, training only on bills prior to the test
set. The error patterns are similar to the primary
split; see Table 5.
6 Discussion
From a political science perspective, our experimen-
tal results using text underscore the importance of
considering the substance of policy proposals (here,
bills) when attempting to explain their progress. An
important research direction in political science, one
in which NLP must play a role, is how different
types of issues are managed in legislatures. Our re-
sults also suggest that political considerations may
induce lawmakers to sponsor certain types of bills
with no real expectation of seeing them enacted into
law.
Considerable recent work has modeled text along-
side data about social behavior. This includes pre-
dictive settings (Kogan et al, 2009; Lerman et
al., 2008), various kinds of sentiment and opin-
ion analysis (Thomas et al, 2006; Monroe et al,
2008; O?Connor et al, 2010; Das et al, 2009), and
exploratory models (Steyvers and Griffiths, 2007).
In political science specifically, the ?text as data?
movement (Grimmer and Stewart, 2012; O?Connor
et al, 2011) has leveraged tools from NLP in quan-
titative research. For example, Grimmer (2010) and
Quinn et al (2006) used topic models to study, re-
spectively, Supreme Court proceedings and Senate
speeches. Closest to this work, Gerrish and Blei
(2011) combined topic models with spatial roll call
models to predict votes in the legislature from text
alone. Their best results, however, came from a
text regression model quite similar to our direct text
model.
7 Conclusions
We presented a novel task: predicting whether a
Congressional bill will be recommended by a com-
mittee. We introduced a strong, expert-informed
baseline that uses basic social features, then demon-
strated substantial improvents on the task using text
in a variety of ways. Comparison leads to insights
about American lawmaking. The data are available
to the research community.
Acknowledgments
We thank the anonymous reviewers, David Bamman,
Justin Grimmer, Michael Heilman, Brendan O?Connor,
Dani Yogatama, and other members of the ARK research
group for helpful feedback. This research was supported
by DARPA grant N10AP20042.
References
E. Scott Adler and John Wilkerson. 2005. The scope
and urgency of legislation: Reconsidering bill success
in the house of representatives. Paper presented at the
annual meeting of the American Political Science As-
sociation.
E. Scott Adler and John Wilkerson. 2012. Congress and
the Politics of Problem Solving. Cambridge University
Press, London.
Beata Beigman Klebanov, Daniel Diermeier, and Eyal
Beigman. 2008. Lexical cohesion analysis of politi-
cal speech. Political Analysis, 16(4):447?463.
David Blei, Andrew Ng, and Michael Jordan. 2003. La-
tent Dirichlet alocation. Journal of Machine Learning
Research, 3:993?1022.
Joshua Clinton, Simon Jackman, and Doug Rivers. 2004.
The statistical analysis of roll-call data. American Po-
litical Science Review, 98(2):355?370.
Pradipto Das, Rohini Srihari, and Smruthi Mukund.
2009. Discovering voter preferences in blogs using
mixtures of topic models. In Proceedings of the Third
Workshop on Analytics for Noisy Unstructured Text
Data.
Sean Gerrish and David Blei. 2011. Predicting legisla-
tive roll calls from text. In Proc. of ICML.
Justin Grimmer and Brandon Stewart. 2012. Text as
data: The promise and pitfalls of automatic content
analysis methods for political documents. http://
www.stanford.edu/?jgrimmer/tad2.pdf.
801
Justin Grimmer. 2010. A Bayesian hierarchical topic
model for political texts: Measuring expressed agen-
das in Senate press releases. Political Analysis,
18(1):1?35.
Simon Jackman. 2001. Multidimensional analysis of
roll call data via Bayesian simulation: Identification,
estimation, inference, and model checking. Political
Analysis, 9(3):227?241.
Shimon Kogan, Dimitry Levin, Bryan R. Routledge, Ja-
cob S. Sagi, and Noah A. Smith. 2009. Predicting
risk from financial reports with regression. In Proc. of
NAACL.
Michael Laver, Kenneth Benoit, and John Garry. 2003.
Extracting policy positions from political texts using
words as data. American Political Science Review,
97(2):311?331.
Kevin Lerman, Ari Gilder, Mark Dredze, and Fernando
Pereira. 2008. Reading the markets: Forecasting pub-
lic opinion of political candidates by news analysis. In
Proc. of COLING.
Nolan McCarty, Howard Rosenthal, and Keith T. Poole.
2006. Polarized America: The Dance of Ideology and
Unequal Riches. MIT Press.
Burt Monroe, Michael Colaresi, and Kevin M. Quinn.
2008. Fightin? words: Lexical feature selection and
evaluation for identifying the content of political con-
flict. Political Analysis, 16(4):372?403.
Brendan O?Connor, Ramnath Balasubramanyan,
Bryan R. Routledge, and Noah A. Smith. 2010.
From tweets to polls: Linking text sentiment to public
opinion time series. In Proc. of ICWSM.
Brendan O?Connor, David Bamman, and Noah A. Smith.
2011. Computational text analysis for social science:
Model complexity and assumptions. In Proc. of the
NIPS Workshop on Comptuational Social Science and
the Wisdom of Crowds.
Bo Pang and Lillian Lee. 2004. A sentimental education:
Sentiment analysis using subjectivity summarization
based on minimum cuts. In Proc. of ACL.
Keith T. Poole and Howard Rosenthal. 1985. A spatial
model for legislative roll call analysis. American Jour-
nal of Political Science, 29(2):357?384.
Keith T. Poole and Howard Rosenthal. 1991. Patterns of
congressional voting. American Journal of Political
Science, 35(1):228?278.
Kevin M. Quinn, Burt L. Monroe, Michael Colaresi,
Michael H. Crespin, and Dragomir R. Radev. 2006.
An automated method of topic-coding legislative
speech over time with application to the 105th?108th
U.S. Senate. Paper presented at the meeting of the
Midwest Political Science Association.
Kevin M. Quinn, Burt L Monroe, Michael Colaresi,
Michael H. Crespin, and Dragomir R. Radev. 2010.
How to analyze political attention with minimal as-
sumptions and costs. American Journal of Political
Science, 54(1):209?228.
Mark Steyvers and Tom Griffiths. 2007. Probabilistic
topic models. In T. Landauer, D. McNamara, S. Den-
nis, and W. Kintsch, editors, Handbook of Latent Se-
mantic Analysis. Lawrence Erlbaum.
Matt Thomas, Bo Pang, and Lillian Lee. 2006. Get
out the vote: Determining support or opposition from
Congressional floor-debate transcripts. In Proc. of
EMNLP.
802
Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon?s Mechanical Turk, pages 152?158,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Shedding (a Thousand Points of) Light on Biased Language
Tae Yano
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
taey@cs.cmu.edu
Philip Resnik
Department of Linguistics and UMIACS
University of Maryland
College Park, MD 20742, USA
resnik@umiacs.umd.edu
Noah A. Smith
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
nasmith@cs.cmu.edu
Abstract
This paper considers the linguistic indicators of bias
in political text. We used Amazon Mechanical Turk
judgments about sentences from American political
blogs, asking annotators to indicate whether a sen-
tence showed bias, and if so, in which political di-
rection and through which word tokens. We also
asked annotators questions about their own political
views. We conducted a preliminary analysis of the
data, exploring how different groups perceive bias in
different blogs, and showing some lexical indicators
strongly associated with perceived bias.
1 Introduction
Bias and framing are central topics in the study of com-
munications, media, and political discourse (Scheufele,
1999; Entman, 2007), but they have received relatively
little attention in computational linguistics. What are the
linguistic indicators of bias? Are there lexical, syntactic,
topical, or other clues that can be computationally mod-
eled and automatically detected?
Here we use Amazon Mechanical Turk (MTurk) to en-
gage in a systematic, empirical study of linguistic indi-
cators of bias in the political domain, using text drawn
from political blogs. Using the MTurk framework, we
collected judgments connected with the two dominant
schools of thought in American politics, as exhibited in
single sentences. Since no one person can claim to be an
unbiased judge of political bias in language, MTurk is an
attractive framework that lets us measure perception of
bias across a population.
2 Annotation Task
We drew sentences from a corpus of American political
blog posts from 2008. (Details in Section 2.1.) Sentences
were presented to participants one at a time, without con-
text. Participants were asked to judge the following (see
Figure 1 for interface design):
? To what extent a sentence or clause is biased (none,
somewhat, very);
? The nature of the bias (very liberal, moderately lib-
eral, moderately conservative, very conservative, bi-
ased but not sure which direction); and
? Which words in the sentence give away the author?s
bias, similar to ?rationale? annotations in Zaidan et
al. (2007).
For example, a participant might identify a moderate
liberal bias in this sentence,
Without Sestak?s challenge, we would have
Specter, comfortably ensconced as a Democrat
in name only.
adding checkmarks on the underlined words. A more
neutral paraphrase is:
Without Sestak?s challenge, Specter would
have no incentive to side more frequently with
Democrats.
It is worth noting that ?bias,? in the sense we are us-
ing it here, is distinct from ?subjectivity? as that topic
has been studied in computational linguistics. Wiebe
et al (1999) characterize subjective sentences as those
that ?are used to communicate the speaker?s evaluations,
opinions, and speculations,? as distinguished from sen-
tences whose primary intention is ?to objectively com-
municate material that is factual to the reporter.? In con-
trast, a biased sentence reflects a ?tendency or preference
towards a particular perspective, ideology or result.?1 A
subjective sentence can be unbiased (I think that movie
was terrible), and a biased sentence can purport to com-
municate factually (Nationalizing our health care system
1http://en.wikipedia.org/wiki/Bias as of 13 April,
2010.
152
is a point of no return for government interference in the
lives of its citizens2).
In addition to annotating sentences, each participant
was asked to complete a brief questionnaire about his or
her own political views. The survey asked:
1. Whether the participant is a resident of the United
States;
2. Who the participant voted for in the 2008 U.S.
presidential election (Barack Obama, John McCain,
other, decline to answer);
3. Which side of political spectrum he/she identified
with for social issues (liberal, conservative, decline
to answer); and
4. Which side of political spectrum he/she identified
with for fiscal/economic issues (liberal, conserva-
tive, decline to answer).
This information was gathered to allow us to measure
variation in bias perception as it relates to the stance of
the annotator, e.g., whether people who view themselves
as liberal perceive more bias in conservative sources, and
vice versa.
2.1 Dataset
We extracted our sentences from the collection of blog
posts in Eisenstein and Xing (2010). The corpus con-
sists of 2008 blog posts gathered from six sites focused
on American politics:
? American Thinker (conservative),3
? Digby (liberal),4
? Hot Air (conservative),5
? Michelle Malkin (conservative),6
? Think Progress (liberal),7 and
? Talking Points Memo (liberal).8
13,246 posts were gathered in total, and 261,073 sen-
tences were extracted using WebHarvest9 and OpenNLP
1.3.0.10 Conservative and liberal sites are evenly rep-
resented (130,980 sentences from conservative sites,
130,093 from liberal sites). OpenNLP was also used for
tokenization.
2Sarah Palin, http://www.facebook.com/note.php?
note_id=113851103434, August 7, 2009.
3http://www.americanthinker.com
4http://digbysblog.blogspot.com
5http://hotair.com
6http://michellemalkin.com
7http://thinkprogress.org
8http://www.talkingpointsmemo.com
9http://web-harvest.sourceforge.net
10http://opennlp.sourceforge.net
Liberal Conservative
thinkprogress org exit question
video thinkprogress hat tip
et rally ed lasky
org 2008 hot air
gi bill tony rezko
wonk room ed morrissey
dana perino track record
phil gramm confirmed dead
senator mccain american thinker
abu ghraib illegal alien
Table 1: Top ten ?sticky? partisan bigrams for each side.
2.2 Sentence Selection
To support exploratory data analysis, we sought a di-
verse sample of sentences for annotation, but we were
also guided by some factors known or likely to correlate
with bias. We extracted sentences from our corpus that
matched at least one of the categories below, filtering to
keep those of length between 8 and 40 tokens. Then, for
each category, we first sampled 100 sentences without re-
placement. We then randomly extracted sentences up to
1,100 from the remaining pool. We selected the sentences
this way so that the collection has variety, while including
enough examples for individual categories. Our goal was
to gather at least 1,000 annotated sentences; ultimately
we collected 1,041. The categories are as follows.
?Sticky? partisan bigrams. One likely indicator of
bias is the use of terms that are particular to one side or
the other in a debate (Monroe et al, 2008). In order to
identify such terms, we independently created two lists
of ?sticky? (i.e., strongly associated) bigrams in liberal
and conservative subcorpora, measuring association us-
ing the log-likelihood ratio (Dunning, 1993) and omitting
bigrams containing stopwords.11 We identified a bigram
as ?liberal? if it was among the top 1,000 bigrams from
the liberal blogs, as measured by strength of association,
and was also not among the top 1,000 bigrams on the con-
servative side. The reverse definition yielded the ?conser-
vative? bigrams. The resulting liberal list contained 495
bigrams, and the conservative list contained 539. We then
manually filtered cases that were clearly remnant HTML
tags and other markup, arriving at lists of 433 and 535,
respectively. Table 1 shows the strongest weighted bi-
grams.
As an example, consider this sentence (with a preced-
ing sentence of context), which contains gi bill. There is
no reason to think the bigram itself is inherently biased
(in contrast to, for example, death tax, which we would
11We made use of Pedersen?s N -gram Statistics Package (Banerjee
and Pedersen, 2003).
153
perceive as biased in virtually any unquoted context), but
we do perceive bias in the full sentence.
Their hard fiscal line softens in the face of
American imperialist adventures. According to
CongressDaily the Bush dogs are also whining
because one of their members, Stephanie Her-
seth Sandlin, didn?t get HERGI Bill to the floor
in favor of Jim Webb?s .
Emotional lexical categories. Emotional words might
be another indicator of bias. We extracted four categories
of words from Pennebaker?s LIWC dictionary: Nega-
tive Emotion, Positive Emotion, Causation, and Anger.12
The following is one example of a biased sentence in our
dataset that matched these lexicons, in this case the Anger
category; the match is in bold.
A bunch of ugly facts are nailing the biggest
scare story in history.
The five most frequent matches in the corpus for each
category are as follows.13
Negative Emotion: war attack* problem* numb* argu*
Positive Emotion: like well good party* secur*
Causation: how because lead* make why
Anger: war attack* argu* fight* threat*
Kill verbs. Greene and Resnik (2009) discuss the rel-
evance of syntactic structure to the perception of senti-
ment. For example, their psycholinguistic experiments
would predict that when comparing Millions of people
starved under Stalin (inchoative) with Stalin starved mil-
lions of people (transitive), the latter will be perceived as
more negative toward Stalin, because the transitive syn-
tactic frame tends to be connected with semantic prop-
erties such as intended action by the subject and change
of state in the object. ?Kill verbs? provide particularly
strong examples of such phenomena, because they ex-
hibit a large set of semantic properties canonically as-
sociated with the transitive frame (Dowty, 1991). The
study by Greene and Resnik used 11 verbs of killing and
similar action to study the effect of syntactic ?packag-
ing? on perceptions of sentiment.14 We included mem-
bership on this list (in any morphological form) as a se-
lection criterion, both because these verbs may be likely
12http://www.liwc.net. See Pennebaker et al (2007) for de-
tailed description of background theory, and how these lexicons were
constructed. Our gratitude to Jamie Pennebaker for the use of this dic-
tionary.
13Note that some LIWC lexical entries are specified as pre-
fixes/stems, e.g. ugl*, which matches ugly uglier, etc.
14The verbs are: kill, slaughter, assassinate, shoot, poison, strangle,
smother, choke, drown, suffocate, and starve.
to appear in sentences containing bias (they overlap sig-
nificantly with Pennebaker?s Negative Emotion list), and
because annotation of bias will provide further data rel-
evant to Greene and Resnik?s hypothesis about the con-
nections among semantic propeties, syntactic structures,
and positive or negative perceptions (which are strongly
connected with bias).
In our final 1,041-sentence sample, ?sticky bigrams?
occur 235 times (liberal 113, conservative 122), the lexi-
cal category features occur 1,619 times (Positive Emotion
577, Negative Emotion 466, Causation 332, and Anger
244), and ?kill? verbs appear as a feature in 94 sentences.
Note that one sentence often matches multiple selection
criteria. Of the 1,041-sentence sample, 232 (22.3%) are
from American Thinker, 169 (16.2%) from Digby, 246
(23.6%) from Hot Air, 73 (7.0%) from Michelle Malkin,
166 (15.9%) from Think Progress, and 155 (14.9%) from
Talking Points Memo.
3 Mechanical Turk Experiment
We prepared 1,100 Human Intelligence Tasks (HITs),
each containing one sentence annotation task. 1,041 sen-
tences were annotated five times each (5,205 judgements
total). One annotation task consists of three bias judge-
ment questions plus four survey questions. We priced
each HIT between $0.02 and $0.04 (moving from less
to more to encourage faster completion). The total cost
was $212.15 We restricted access to our tasks to those
who resided in United States and who had above 90% ap-
proval history, to ensure quality and awareness of Amer-
ican political issues. We also discarded HITs annotated
by workers with particularly low agreement scores. The
time allowance for each HIT was set at 5 minutes.
3.1 Annotation Results
3.1.1 Distribution of Judgments
Overall, more than half the judgments are ?not biased,?
and the ?very biased? label is used sparingly (Table 2).
There is a slight tendency among the annotators to assign
the ?very conservative? label, although moderate bias is
distributed evenly on both side (Table 3). Interestingly,
there are many ?biased, but not sure? labels, indicating
that the annotators are capable of perceiving bias (or ma-
nipulative language), without fully decoding the intent of
the author, given sentences out of context.
Bias 1 1.5 2 2.5 3
% judged 36.0 26.6 25.5 9.4 2.4
Table 2: Strength of perceived bias per sentence, averaged over
the annotators (rounded to nearest half point). Annotators rate
bias on a scale of 1 (no bias), 2 (some bias), and 3 (very biased).
15This includes the cost for the discarded annotations.
154
Figure 1: HIT: Three judgment questions. We first ask for the strength of bisa, then the direction. For the word-level annotation
question (right), workers are asked to check the box to indicate the region which ?give away? the bias.
Bias type VL ML NB MC VC B
% judged 4.0 8.5 54.8 8.2 6.7 17.9
Table 3: Direction of perceived bias, per judgment (very lib-
eral, moderately liberal, no bias, moderately conservative, very
conservative, biased but not sure which).
Economic
L M C NA
So
ci
al
L 20.1 10.1 4.9 0.7
M 0.0 21.9 4.7 0.0
C 0.1 0.4 11.7 0.0
NA 0.1 0.0 11.2 14.1
Table 4: Distribution of judgements by annotators? self-
identification on social issues (row) and fiscal issue (column);
{L, C, M, NA} denote liberal, conservative, moderate, and de-
cline to answer, respectively.
3.1.2 Annotation Quality
In this study, we are interested in where the wisdom of
the crowd will take us, or where the majority consensus
on bias may emerge. For this reason we did not contrive a
gold standard for ?correct? annotation. We are, however,
mindful of its overall quality?whether annotations have
reasonable agreement, and whether there are fraudulent
responses tainting the results.
To validate our data, we measured the pair-wise Kappa
statistic (Cohen, 1960) among the 50 most frequent work-
ers16 and took the average over all the scores.17. The
average of the agreement score for the first question is
0.55, and the second 0.50. Those are within the range of
reasonable agreement for moderately difficult task. We
also inspected per worker average scores for frequent
workers18 and found one with consistently low agreement
scores. We discarded all the HITs by this worker from our
results. We also manually inspected the first 200 HITs for
apparent frauds. The annotations appeared to be consis-
tent. Often annotators agreed (many ?no bias? cases were
unanimous), or differed in only the degree of strength
(?very biased? vs. ?biased?) or specificity (?biased but I
am not sure? vs. ?moderately liberal?). The direction of
bias, if specified, was very rarely inconsistent.
Along with the annotation tasks, we asked workers
how we could improve our HITs. Some comments were
16258 workers participated; only 50 of them completed more than 10
annotations.
17Unlike traditional subjects for a user-annotation study, our annota-
tors have not judged all the sentences considered in the study. There-
fore, to compute the agreement, we considered only the case where two
annotators share 20 or more sentences.
18We consider only those with 10 or more annotations.
155
insightful for our study (as well as for the interface de-
sign). A few pointed out that an impolite statement or
a statement of negative fact is not the same as bias, and
therefore should be marked separately from bias. Others
mentioned that some sentences are difficult to judge out
of context. These comments will be taken into account in
future research.
4 Analysis and Significance
In the following section we report some of the interesting
trends we found in our annotation results. We consider a
few questions and report the answers the data provide for
each.
4.1 Is a sentence from a liberal blog more likely be
seen as liberal?
In our sample sentence pool, conservatives and liberals
are equally represented, though each blog site has a dif-
ferent representation.19 We grouped sentences by source
site, then computed the percentage representation of each
site within each bias label; see Table 5. In the top row,
we show the percentage representation of each group in
overall judgements.
In general, a site yields more sentences that match its
known political leanings. Note that in our annotation
task, we did not disclose the sentence?s source to the
workers. The annotators formed their judgements solely
based on the content of the sentence. This result can
be taken as confirming people?s ability to perceive bias
within a sentence, or, conversely, as confirming our a pri-
ori categorizations of the blogs.
at ha mm db tp tpm
Overall 22.3 23.6 7.0 16.2 15.9 14.9
NB 23.7 22.3 6.1 15.7 17.0 15.3
VC 24.8 32.3 19.3 6.9 7.5 9.2
MC 24.4 33.6 8.0 8.2 13.6 12.2
ML 16.6 15.2 3.4 21.1 22.9 20.9
VL 16.7 9.0 4.3 31.0 22.4 16.7
B 20.1 25.4 7.2 19.5 12.3 13.7
Table 5: Percentage representation of each site within bias label
pools from question 2 (direction of perceived bias): very liberal,
moderately liberal, no bias, moderately conservative, very con-
servative, biased but not sure which. Rows sum to 100. Bold-
face indicates rates higher than the site?s overall representation
in the pool.
4.2 Does a liberal leaning annotator see more
conservative bias?
In Table 5, we see that blogs are very different from each
other in terms of the bias annotators perceive in their lan-
19Posts appear on different sites at different rates.
1
3
10
32
100
very conservative no bias very liberal
not sure
LL
MM
CC
Overall
Figure 2: Distribution of bias labels (by judgment) for social
and economic liberals (LL), social and economic moderates
(MM), and social and economic conservatives (CC), and over-
all. Note that this plot uses a logarithmic scale, to tease apart
the differences among groups.
guage. In general, conservative sites seemingly produced
much more identifiable partisan bias than liberal sites.20
This impression, however, might be an artifact of the
distribution of the annotators? own bias. As seen in Ta-
ble 4, a large portion of our annotators identified them-
selves as liberal in some way. People might call a state-
ment biased if they disagree with it, while showing le-
niency toward hyperbole more consistent with their opin-
ions.
To answer this question, we break down the judgement
labels by the annotators? self-identification, and check
the percentage of each bias type within key groups (see
Figure 2). In general, moderates perceive less bias than
partisans (another useful reality check, in the sense that
this is to be expected), but conservatives show a much
stronger tendency to label sentences as biased, in both
directions. (We caution that the underrepresentation of
self-identifying conservatives in our worker pool means
that only 608 judgments from 48 distinct workers were
used to estimate these statistics.) Liberals in this sample
are less balanced, perceiving conservative bias at double
the rate of liberal bias.
4.3 What are the lexical indicators of perceived
bias?
For a given word type w, we calculate the frequency that
it was marked as indicating bias, normalized by its total
number of occurrences. To combine the judgments of dif-
ferent annotators, we increment w?s count by k/n when-
ever k judgments out of n marked the word as showing
bias. We perform similar calculations with a restriction
to liberal and conservative judgments on the sentence as a
20Liberal sites cumulatively produced 64.9% of the moderately lib-
eral bias label and 70.1 % of very liberal, while conservative sites pro-
duced 66.0% of moderately conservative and 76.4% of very conserva-
tive, respectively.
156
Overall Liberal Conservative Not Sure Which
bad 0.60 Administration 0.28 illegal 0.40 pass 0.32
personally 0.56 Americans 0.24 Obama?s 0.38 bad 0.32
illegal 0.53 woman 0.24 corruption 0.32 sure 0.28
woman 0.52 single 0.24 rich 0.28 blame 0.28
single 0.52 personally 0.24 stop 0.26 they?re 0.24
rich 0.52 lobbyists 0.23 tax 0.25 happen 0.24
corruption 0.52 Republican 0.22 claimed 0.25 doubt 0.24
Administration 0.52 union 0.20 human 0.24 doing 0.24
Americans 0.51 torture 0.20 doesn?t 0.24 death 0.24
conservative 0.50 rich 0.20 difficult 0.24 actually 0.24
doubt 0.48 interests 0.20 Democrats 0.24 exactly 0.22
torture 0.47 doing 0.20 less 0.23 wrong 0.22
Table 6: Most strongly biased words, ranked by relative frequency of receiving a bias mark, normalized by total frequency. Only
words appearing five times or more in our annotation set are ranked.
whole. Top-ranked words for each calculation are shown
in Table 6.
Some of the patterns we see are consistent with what
we found in our automatic method for proposing biased
bigrams. For example, the bigrams tended to include
terms that refer to members or groups on the opposing
side. Here we find that Republican and Administration
(referring in 2008 to the Bush administration) tends to
show liberal bias, while Obama?s and Democrats show
conservative bias.
5 Discussion and Future Work
The study we have conducted here represents an initial
pass at empirical, corpus-driven analysis of bias using the
methods of computational linguistics. The results thus far
suggest that it is possible to automatically extract a sam-
ple that is rich in examples that annotators would con-
sider biased; that na??ve annotators can achieve reason-
able agreement with minimal instructions and no train-
ing; and that basic exploratory analysis of results yields
interpretable patterns that comport with prior expecta-
tions, as well as interesting observations that merit further
investigation.
In future work, enabled by annotations of biased and
non-biased material, we plan to delve more deeply into
the linguistic characteristics associated with biased ex-
pression. These will include, for example, an analysis
of the extent to which explicit ?lexical framing? (use of
partisan terms, e.g., Monroe et al, 2008) is used to con-
vey bias, versus use of more subtle cues such as syntactic
framing (Greene and Resnik, 2009). We will also explore
the extent to which idiomatic usages are connected with
bias, with the prediction that partisan ?memes? tend to be
more idiomatic than compositional in nature.
In our current analysis, the issue of subjectivity was not
directly addressed. Previous work has shown that opin-
ions are closely related to subjective language (Pang and
Lee, 2008). It is possible that asking annotators about
sentiment while asking about bias would provide a deeper
understanding of the latter. Interestingly, annotator feed-
back included remarks that mere negative ?facts? do not
convey an author?s opinion or bias. The nature of subjec-
tivity as a factor in bias perception is an important issue
for future investigation.
6 Conclusion
This paper considered the linguistic indicators of bias in
political text. We used Amazon Mechanical Turk judg-
ments about sentences from American political blogs,
asking annotators to indicate whether a sentence showed
bias, and if so, in which political direction and through
which word tokens; these data were augmented by a po-
litical questionnaire for each annotator. Our preliminary
analysis suggests that bias can be annotated reasonably
consistently, that bias perception varies based on personal
views, and that there are some consistent lexical cues for
bias in political blog data.
Acknowledgments
The authors acknowledge research support from HP
Labs, help with data from Jacob Eisenstein, and help-
ful comments from the reviewers, Olivia Buzek, Michael
Heilman, and Brendan O?Connor.
References
Satanjeev Banerjee and Ted Pedersen. 2003. The design, implementa-
tion and use of the ngram statistics package. In the Fourth Interna-
tional Conference on Intelligent Text Processing and Computational
Linguistics.
Jacob Cohen. 1960. A coefficient of agreement for nominal scales.
Educational and Psychological Measurement, 20(1):37?46.
David Dowty. 1991. Thematic Proto-Roles and Argument Selection.
Language, 67:547?619.
157
Ted Dunning. 1993. Accurate methods for the statistics of surprise and
coincidence. Computational Linguistics, 19(1):61?74.
Jacob Eisenstein and Eric Xing. 2010. The CMU 2008 political blog
corpus. Technical report CMU-ML-10-101.
Robert M. Entman. 2007. Framing bias: Media in the distribution of
power. Journal of Communication, 57(1):163?173.
Stephan Greene and Philip Resnik. 2009. More than words: Syntactic
packaging and implicit sentiment. In NAACL, pages 503?511, June.
Burt L. Monroe, Michael P. Colaresi, and Kevin M. Quinn. 2008.
Fightin? words: Lexical feature selection and evaluation for identi-
fying the content of political conflict. Political Analysis, 16(4):372?
403, October.
Bo Pang and Lillian Lee. 2008. Opinion mining and sentiment analysis.
Foundations and Trends in Information Retrieval, 2(1-2):1?135.
J.W. Pennebaker, C.K Chung, M. Ireland, A Gonzales, and R J. Booth,
2007. The development and psychometric properties of LIWC2007.
Dietram A. Scheufele. 1999. Framing as a theory of media effects.
Journal of Communication, 49(1):103?122.
Janyce M. Wiebe, Rebecca F. Bruce, and Thomas P. O?Hara. 1999.
Development and use of a gold standard data set for subjectivity
classifications. In Proceedings of the Association for Computational
Linguistics (ACL), pages 246?253.
Omar Zaidan, Jason Eisner, and Christine Piatko. 2007. Using ?anno-
tator rationales? to improve machine learning for text categorization.
In NAACL, pages 260?267, April.
158
Proceedings of EMNLP 2011, Conference on Empirical Methods in Natural Language Processing, pages 2?12,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Structured Databases of Named Entities from Bayesian Nonparametrics
Jacob Eisenstein Tae Yano William W. Cohen Noah A. Smith Eric P. Xing
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{jacobeis,taey,wcohen,nasmith,epxing}@cs.cmu.edu
Abstract
We present a nonparametric Bayesian ap-
proach to extract a structured database of enti-
ties from text. Neither the number of entities
nor the fields that characterize each entity are
provided in advance; the only supervision is
a set of five prototype examples. Our method
jointly accomplishes three tasks: (i) identify-
ing a set of canonical entities, (ii) inferring a
schema for the fields that describe each entity,
and (iii) matching entities to their references in
raw text. Empirical evaluation shows that the
approach learns an accurate database of enti-
ties and a sensible model of name structure.
1 Introduction
Consider the task of building a set of structured
records from a collection of text: for example, ex-
tracting the names of people or businesses from
blog posts, where each full name decomposes into
fields corresponding to first-name, last-name, title,
etc. To instruct a person to perform this task, one
might begin with a few examples of the records to
be obtained; assuming that the mapping from text to
records is relatively straightforward, no additional
instruction would be necessary. In this paper, we
present a method for training information extraction
software in the same way: starting from a small table
of partially-complete ?prototype? records (Table 1),
our system learns to add new entries and fields to
the table, while simultaneously aligning the records
to text.
We assume that the dimensionality of the database
is unknown, so that neither the number of entries
John McCain Sen. Mr.
George Bush W. Mr.
Hillary Clinton Rodham Mrs.
Barack Obama Sen.
Sarah Palin
Table 1: A set of partially-complete prototype records,
which constitutes the only supervision for the system.
nor the number of fields is specified in advance. To
accommodate this uncertainty, we apply a Bayesian
model which is nonparametric along three dimen-
sions: the assignment of text mentions to entities
(making popular entries more likely while always al-
lowing new entries); the alignment of individual text
tokens to fields (encouraging the re-use of common
fields, but permitting the creation of new fields); and
the assignment of values to entries in the database
itself (encouraging the reuse of values across entries
in a given field). By adaptively updating the con-
centration parameter of stick-breaking distribution
controlling the assignment of values to entries in the
database, our model can learn domain-specific infor-
mation about each field: for example, that titles are
often repeated, while names are more varied.
Our system?s input consists of a very small proto-
type table and a corpus of text which has been au-
tomatically segmented to identify names. Our de-
sired output is a set of structured records in which
each field contains a single string ? not a distribu-
tion over strings, which would be more difficult to
interpret. This requirement induces a tight proba-
bilistic coupling between the assignment of text to
cells in the table, so special care is required to ob-
2
tain efficient inference. Our procedure alternates
between two phases. In the first phase, we per-
form collapsed Gibbs sampling on the assignments
of string mentions to rows and columns in the table,
while marginalizing the values of the table itself. In
the second phase, we apply Metropolis-Hastings to
swap the values of columns in the table, while simul-
taneously relabeling the affected strings in the text.
Our model performs three tasks: it constructs a
set of entities from raw text, matches mentions in
text with the entities to which they refer, and discov-
ers general categories of tokens that appear in names
(such as titles and first names). We are aware of
no existing system that performs all three of these
tasks jointly. We evaluate on a dataset of political
blogs, measuring our system?s ability to discover
a set of reference entities (recall) while maintain-
ing a compact number of rows and columns (pre-
cision). With as few as five partially-complete pro-
totype examples, our approach gives accurate tables
that match well against a manually-annotated refer-
ence list. Our method outperforms a baseline single-
link clustering approach inspired by one of the most
successful entries (Elmacioglu et al, 2007) in the
SEMEVAL ?Web People Search? shared task (Ar-
tiles et al, 2007).
2 Task Definition
In this work, we assume that a bag of M mentions
in text have been identified. The mth mention wm
is a sequence of contiguous word tokens (its length
is denoted Nm) understood to refer to a real-world
entity. The entities (and the mapping of mentions to
entities) are not known in advance. While our focus
in this paper is names of people, the task is defined
in a more generic way.
Formally, the task is to construct a table x where
rows correspond to entities and columns to func-
tional fields. The number of entities and the num-
ber of fields are not prespecified. x?,j denotes the
jth column of x, and xi,j is a single word type fill-
ing the cell in row i, column j. An example is Ta-
ble 1, where the fields are first-name, last-name, ti-
tle, middle-name, and so on. In addition to the table,
we require that each mention be mapped to an en-
tity (i.e., a row in the table). Success at this task
therefore requires (i) identifying entities, (ii) discov-
ering the internal structure of mentions (effectively
canonicalizing them), and (iii) mapping mentions
to entities (therefore resolving coreference relation-
ships among mentions). Note that this task differs
from previous work on knowledge base population
(e.g., McNamee, 2009) because the schema is not
formally defined in advance; rather, the number of
fields and their meaning must be induced from just
a few prototype examples.
To incorporate partial supervision, a subset of the
table x is specified manually by an annotator. We
denote this subset of ?prototypes? by x?; for entries
that are unspecified by the user, we write x?i,j = ?.
Prototypes are not assumed to provide complete in-
formation for any entity.
3 Model
We now craft a nonparametric generative story that
explains both the latent table and the observed men-
tions. The model incorporates three nonparamet-
ric components, allowing an unbounded number of
rows (entities) and columns (fields), as well as an un-
bounded number of values per column (field values).
A plate diagram for the graphical model is shown in
Figure 1.
A key point is that the column distributions ?
range over possible values at the entity level, not
over mentions in text. For example, ?2 might be
the distribution over possible last names and ?3 the
distribution over elected office titles. Note that ?2
would contain a low value for the last name Obama
? which indicates that few people have this last
name ? even though a very high proportion of men-
tions in our data include the string Obama.
The user-generated entries (x?) can still be treated
as the outcome of the generative process: using ex-
changeability, we treat these entries as the first sam-
ples drawn in each column. In this work, we treat
them as fully observed, but it is possible to treat
them as noisy and incorporate a stochastic depen-
dency between xi,j and x?i,j .
4 Inference
We now develop sampling-based inference for the
model described in the previous section. We be-
gin with a token-based collapsed Gibbs sampler, and
then add larger-scale Metropolis-Hastings moves.
3
? ?2
x ? ?
w r ?r
c ?r
?c ?c
Figure 1: A plate diagram for the
text-and-tables graphical model.
The upper plate is the table x, and
the lower plate is the set of textual
mentions. Notation is defined in the
generative model to the right.
? Generate the table entries. For each column j,
? Draw a concentration parameter ?j from a log-normal distribution,
log?j ? N (?, ?2).
? Draw a distribution over strings from a Dirichlet process ?j ?
DP(?j , G0), where the base distribution G0 is a uniform distribution
over strings in a fixed character alphabet, up to an arbitrary finite length.
? For each row i, draw the entry xi,j ? ?j .
? Generate the text mentions.
? Draw a prior distribution over rows from a stick-breaking distribution,
?r ? Stick(?r).
? Draw a prior distribution over columns from a stick-breaking distribu-
tion, ?c ? Stick(?c).
? For each mention wm,
? Draw a row in the table rm ? ?r.
? For each word token wm,n (n ? {1, . . . , Nm}),
? Draw a column in the table cm,n ? ?c.
? Set the text wm,n = xrm,cm,n .
4.1 Gibbs sampling
A key aspect of the generative process is that the
word token wm,n is completely determined by the
table x and the row and column indicators rm and
cm,n: given that a token was generated by row i
and column j of the table, it must be identical to
the value of xi,j . Using Bayes? rule, we can reverse
this deterministic dependence: given the values for
the row and column indices, the entries in the table
are restricted to exact matches with the text men-
tions that they generate. This allows us to marginal-
ize the unobserved entries in the table. We can also
marginalize the distributions ?r, ?c, and ?j , using
the standard collapsed Gibbs sampling equations for
Dirichlet processes. Thus, sampling the row and col-
umn indices is all that is required to explore the en-
tire space of model configurations.
4.1.1 Conditional probability for word tokens
The conditional sampling distributions for both
rows and columns will marginalize the table (be-
sides the prototypes x?). To do this, we must be
able to compute P (wm,n | rm = i, cm,n =
j, x?,w?(m,n), r?m, c?(m,n), ?j), which represents
the probability of generating word wm,n, given
rm = i and cm,n = j. The notation w?(m,n), r?m,
and c?m,n represent the words, row indices, and col-
umn indices for all mentions besides wm,n. For sim-
plicity, we will elide these variables in much of the
subsequent notation.
We first consider the case where we have a user-
specified entry for the row and column ?i, j?? that
is, if x?ij 6= ?. Then the probability is simply,
P (wm,n | rm = i, cm,n = j, x?, . . .) =
{
1, if x?ij = wm,n
0, if x?ij 6= wm,n.
(1)
Because the table cell xij is observed, we do not
marginalize over it; we have a generative probability
of one if the word matches, and zero otherwise. If
the table cell xij is not specified by the user, then we
marginalize over its possible values. For any given
xij , the probability P (wm,n | xij , rm = i, cm,n =
j) is still a delta function, so we have:
?
P (wm,n | xrm,cm,n)P (xrm,cm,n | . . .) dxrm,cm,n
= P (x = wm,n | w?(m,n), r?m, c?(m,n), x?, . . .)
The integral is equal to the probability of the value
of the cell xrm,cm,n being identical to the string
wm,n, given assignments to all other variables. To
compute this probability, we again must consider
two cases: if the cell xi,j has generated some other
string wm?,n? then its value must be identical to that
4
string; otherwise it is unknown. More formally, for
any cell ?i, j?, if ?wm?,n? : rm? = i ? cm?,n? =
j ? ?m?, n?? 6= ?m,n?, then P (xi,j = wm?,n?) = 1;
all other strings have zero probability. If xi,j has not
generated any other entry, then its probability is con-
ditioned on the other elements of the table x. The
known elements of this table are themselves deter-
mined by either the user entries x? or the observa-
tionsw?(m,n). We can define these known elements
as x?, where x?ij = ? if x?ij = ? ? @?m,n? : rm =
i ? cm,n = j. Then we can apply the standard Chi-
nese restaurant process marginalization to obtain:
P (xij | x??(i,j), ?) =
{ N(x??(i,j)=xij)
N(x??(i,j) 6=?)+?
, N(x??(i,j) = xij) > 0
?
N(x??(i,j) 6=?)+?
, N(x??(i,j) = xij) = 0
(2)
In our implementation, we maintain the table x?,
updating it as we resample the row and column as-
signments. To construct the conditional distribution
for any given entry, we first consult this table, and
then compute the probability in Equation 2 for en-
tries where x?ij = ?.
4.1.2 Sampling columns
We can now derive sampling equations for the
column indices cm,n. We first apply Bayes? rule
to obtain P (cm,n | wm,n, rm, . . .) ? P (cm,n |
c?(m,n), ?c)?P (wm,n | cm,n, rm, x?, . . .). The like-
lihood term P (wm,n | cm,n, . . .) is defined in the
previous section; we can compute the first factor us-
ing the standard Dirichlet process marginalization
over ?c. Writing N(c?(m,n) = j) for the count of
occurrences of column j in the set c?(m,n), we ob-
tain
P (cm,n = j | c?(m,n), ?c) =
{ N(c?(m,n)=j)
N(c?(m,n))+?c
, if N(c?(m,n) = j) > 0
?c
N(c?(m,n))+?c
, if N(c?(m,n) = j) = 0
(3)
4.1.3 Sampling rows
In principle the row indicators can be sampled
identically to the columns, with the caveat that the
generative probability P (wm | rm, . . .) is a product
across all Nm tokens in wm.1 However, because of
1This relies on the assumption that the values of {cm,n} are
mutually independent given c?m. Future work might apply
the tight probabilistic coupling between the row and
column indicators, straightforward Gibbs sampling
mixes slowly. Instead, we marginalize the column
indicators while sampling r. Only the likelihood
term is affected by this change:
P (wm | rm,w?m, r?m, . . .)
=
?
j
P (c = j | c?m, ?c)P (wm,n | cm,n = j, rm, x?, ?).
(4)
The tokens are conditionally independent given the
row, so we factor and then explicitly marginalize
over each cm,n. The chain rule gives the form in
Equation 4, which contains terms for the prior over
columns and the likelihood of the word; these are
defined in Equations 2 and 3. Note that neither the
inferred table x? nor the heldout column counts c?m
include counts from any of the cells in row m.
4.2 Column swaps
Suppose that during initialization, we encounter the
string Barry Obama before encountering Barack
Obama. We would then put Barry in the first-name
column, and put Barack in some other column for
nicknames. After making these initial decisions,
they would be very difficult to undo using Gibbs
sampling ? we would have to first shift all instances
of Barry to another column, then move an instance
of Barack to the first-name column, and then move
the instances of Barry to the nickname column. To
rectify this issue, we perform sampling on the table
itself, swapping the columns of entries in the table,
while simultaneously updating the relevant column
indices of the mentions.
In the proposal, we select at random a row t and
indices i and j. In the table, we will swap xt,i with
xt,j ; in the text we will swap the values of each cm,n
whenever rm = t and cm,n = i or j. This pro-
posal is symmetric, so no Hastings correction is re-
quired. Because we are simultaneously updating the
table and the column indices, the generative likeli-
hood of the words is unchanged; the only changes
a more structured model of the ways that fields are combined
when mentioning an entity. For example, a first-order Markov
model could learn that family names often follow given names,
but the reverse rarely occurs (in English).
5
in the overall likelihood come from the column in-
dices and the values of the cells in the table. Letting
x?, c? indicate the state of the table and column in-
dices after the proposed move, we will accept with
probability,
Paccept(x? x?) = min
(
1,
P (c?)P (x?)
P (c)P (x)
)
(5)
We first consider the ratio of the table probabili-
ties, P (x
?|?)
P (x|?) . Recall that each column of x is drawn
from a Dirichlet process; appealing to exchangeabil-
ity, we can treat the row t as the last element drawn,
and compute the probabilities P (xt,i | x?(t,i), ?i),
with x?(t,i) indicating the elements of the column i
excluding row t. This probability is given by Equa-
tion 2. For a swap of columns i and j, we compute
the ratio:
P (xt,i | x?(t,j), ?j)P (xt,j | x?(t,i), ?i)
P (xt,i | x?(t,i), ?i)P (xt,j | x?(t,j), ?j)
(6)
Next we consider the ratio of the column proba-
bilities, P (c
?)
P (c) . Again we can apply exchangeabil-
ity, P (c) = P ({cm : rm = t} | {cm? : rm? 6=
t})P ({cm? : rm? 6= t}). The second term P ({cm? :
rm? 6= t}) is unaffected by the move, and so is iden-
tical in both the numerator and denominator of the
likelihood ratio; probabilities from columns other
than i and j also cancel in this way. The remaining
ratio can be simplified to,
(
P (c = j | c?t, ?c)
P (c = i | c?t, ?c)
)N(r=t?c=i)?N(r=t?c=j)
(7)
where the counts N() are from the state of the sam-
pler before executing the proposed move. The prob-
ability P (c = i | c?t, ?c) is defined in Equation 3,
and the overall acceptance ratio for column swaps is
the product of (6) and (7).
4.3 Hyperparameters
The concentration parameters ?r and ?c help to con-
trol the number of rows and columns in the ta-
ble, respectively. These parameters are updated to
their maximum likelihood values using gradient-
based optimization, so our overall inference pro-
cedure is a form of Monte Carlo Expectation-
Maximization (Wei and Tanner, 1990).
The concentration parameters ?j control the di-
versity of each column in the table: if ?j is low then
we expect a high degree of repetition, as with titles;
if ?j is high then we expect a high degree of diver-
sity. When the sampling procedure adds a new col-
umn, there is very little information for how to set
its concentration parameter, as the conditional like-
lihood will be flat. Consequently, greater care must
be taken to handle these priors appropriately.
We place a log-normal hyperprior on the col-
umn concentration parameters, log?j ? N (?, ?2).
The parameters of the log-normal are shared across
columns, which provides additional information to
constrain the concentration parameters of newly-
created columns. We then use Metropolis-Hastings
to sample the values of each ?j , using the joint like-
lihood,
P (?j , x?(j) | ?, ?2) ?
exp(?(log?j ? ?)2)?
kj
j ?(?j)
2?2?(nj + ?j)
,
where x?(j) is column j of the inferred table, nj is
the number of specified entries in column j of the
table x? and kj is the number of unique entries in
the column; see Rasmussen (2000) for a derivation.
After repeatedly sampling several values of ?j for
each column in the table, we update ? and ?2 to their
maximum-likelihood estimates.
5 Temporal Prominence
Andy Warhol predicted, ?in the future, everyone will
be world-famous for fifteen minutes.? A model of
temporal dynamics that accounts for the fleeting and
fickle nature of fame might yield better performance
for transient entities, like Joe the Plumber. Among
several alternatives for modeling temporal dynamics
in latent variable models, we choose a simple non-
parametric approach: the recurrent Chinese restau-
rant process (RCRP; Ahmed and Xing, 2008). The
core idea of the RCRP is that time is partitioned into
epochs, with a unique Chinese restaurant process in
each epoch. Each CRP has a prior which takes the
form of pseudo-counts computed from the counts in
previous epochs. We employ the simplest version of
the RCRP, a first-order Markov model in which the
prior for epoch t is equal to the vector of counts for
epoch t? 1:
6
P (r(t)m = i|r
(t)
1...m?1, r
(t?1), ?r) ?
{
N(r(t)1...m?1 = i) + N(r
(t?1) = i), if > 0;
?r, otherwise.
(8)
The count of row i in epoch t ? 1 is written
N(r(t?1) = i); the count in epoch t for mentions
1 to m ? 1 is written N(r(t)1...m?1 = i). As before,
we can apply exchangeability to treat each mention
as the last in the epoch, so during inference we can
replace this with the count N(r(t)?m). Note that there
is zero probability of drawing an entity that has no
counts in epochs t or t ? 1 but exists in some other
epoch; the probability mass ?r is reserved for draw-
ing a new entity, and the chance of this matching
some existing entity from another epoch is vanish-
ingly small.
During Gibbs sampling, we also need to consider
the effect of r(t)m on the subsequent epoch t + 1.
While space does not permit a derivation, the result-
ing probability is proportional to
P (r(t+1)|r(t)?m, r
(t)
m = i, ?r) ?
?
???
???
1 if N(r(t+1) = i) = 0,
N(r(t+1)=i)
?r
if N(r(t)?m = i) = 0,
1 + N(r
(t+1)=i)
N(r(t)?m=i)
if N(r(t)?m = i) > 0.
(9)
This favors entities which are frequent in epoch
t+ 1 but infrequent in epoch t.
The move to a recurrent Chinese restaurant pro-
cess does not affect the sampling equations for the
columns c, nor the concentration parameters of the
table, ?. The only part of the inference procedure
that needs to be changed is the optimization of the
hyperparameter ?r; the log-likelihood is now the
sum across all epochs, and each epoch makes a con-
tribution to the gradient.
6 Evaluation Setup
Our model jointly performs three tasks: identifying
a set of entities, discovering the set of fields, and
matching mention strings with the entities and fields
to which they refer. We are aware of no prior work
that performs these tasks jointly, nor any dataset that
is annotated for all three tasks.2 Consequently, we
focus our quantitative evaluation on what we take to
be the most important subtask: identifying the enti-
ties which are mentioned in raw text. We annotate
a new dataset of blog text for this purpose, and de-
sign precision and recall metrics to reward systems
that recover as much of the reference set as possi-
ble, while avoiding spurious entities and fields. We
also perform a qualitative analysis, noting the areas
where our method outperforms string matching ap-
proaches, and where there is need for further im-
provement.
Data Evaluation was performed on a corpus
of blogs describing United States politics in
2008 (Eisenstein and Xing, 2010). We ran the Stan-
ford Named Entity Recognition system (Finkel et
al., 2005) to obtain a set of 25,000 candidate men-
tions which the system judged to be names of peo-
ple. We then pruned strings that appeared fewer than
four times and eliminated strings with more than
seven tokens (these were usually errors). The result-
ing dataset has 19,247 mentions comprising 45,466
word tokens, and 813 unique mention strings.
Gold standard We develop a reference set of 100
entities for evaluation. This set was created by sort-
ing the unique name strings in the training set by fre-
quency, and manually merging strings that reference
the same entity. We also manually discarded strings
from the reference set if they resulted from errors in
the preprocessing pipeline (tokenization and named
entity recognition). Each entity is represented by
the set of all word tokens that appear in its refer-
ences; there are a total of 231 tokens for the 100 en-
tities. Most entities only include first and last names,
though the most frequent entities have many more:
for example, the entity Barack Obama has known
names: {Barack, Obama, Sen., Mr.}.
Metrics We evaluate the recall and precision of
a system?s response set by matching against the
reference set. The first step is to create a bipar-
tite matching between response and reference enti-
ties.3 Using a cost function that quantifies the sim-
2Recent work exploiting Wikipedia disambiguation pages
for evaluating cross-document coreference suggests an appeal-
ing alternative for future work (Singh et al, 2011).
3Bipartite matchings are typical in information extraction
evaluation metrics (e.g., Doddington et al, 2004).
7
ilarity of response and reference entities, we opti-
mize the matching using the Kuhn-Munkres algo-
rithm (Kuhn, 1955). For recall, the cost function
counts the number of shared word tokens, divided
by the number of word tokens in the reference enti-
ties; the recall is one minus the average cost of the
best matching (with a cost of one for reference enti-
ties that are not matched, and no cost for unmatched
response entities). Precision is computed identically,
but we normalize by the number of word tokens in
the response entity. Precision assigns a penalty of
one to unmatched response entities and no penalty
for unmatched reference entities.
Note that this metric grossly underrates the preci-
sion of all systems: the reference set is limited to 100
entities, but it is clear that our text mentions many
other people. This is harsh but fair: all systems are
penalized equally for identifying entities that are not
present in the reference set, and the ideal system will
recover the fifty reference entities (thus maximizing
recall) while keeping the table as compact as possi-
ble (thus maximizing precision). However, the raw
precision values have little meaning outside the con-
text of a direct comparison under identical experi-
mental conditions.
Systems The initial seed set for our system con-
sists of a partial annotation of five entities (Table 1)
? larger seed sets did not improve performance. We
run the inference procedure described in the previ-
ous section for 20,000 iterations, and then obtain a
final database by taking the intersection of the in-
ferred tables x? obtained at every 100 iterations, start-
ing with iteration 15,000. To account for variance
across Markov chains, we perform three different
runs. We evaluate a non-temporal version of our
model (as described in Sections 3 and 4), and a tem-
poral version with 5 epochs. For the non-temporal
version, a non-parallel C implementation had a wall
clock sampling time of roughly 16 hours; the tem-
poral version required 24 hours.
We compare against a baseline that incrementally
clusters strings into entities using a string edit dis-
tance metric, based on the work of Elmacioglu et
al. (2007). Starting from a configuration in which
each unique string forms its own cluster, we incre-
mentally merge clusters using the single-link crite-
rion, based on the minimum Jaccard edit distance
0.2 0.3 0.4 0.5 0.6 0.70
0.1
0.2
0.3
recall
pre
cis
ion
 
 
baseline
atemporal model
temporal model
Figure 2: The precision and recall of our models, as com-
pared to the curve defined by the incremental clustering
baseline. Each point indicates a unique sampling run.
Bill Clinton Benazir Bhutto
Nancy Pelosi Speaker
John Kerry Sen. Roberts
Martin King Dr. Jr. Luther
Bill Nelson
Table 2: A subset of the entity database discovered by
our model, hand selected to show highlight interesting
success and failure cases.
between each pair of clusters. This yields a series of
outputs that move along the precision-recall curve,
with precision increasing as the clusters encompass
more strings. There is prior work on heuristics for
selecting a stopping point, but we compare our re-
sults against the entire precision-recall curve (Man-
ning et al, 2008).
7 Results
The results of our evaluation are shown in Figure 2.
All sampling runs from our models lie well beyond
the precision-recall curve defined by the baseline
system, demonstrating the ability to achieve reason-
able recall with a far more compact database. The
baseline system can achieve nearly perfect recall by
creating one entity per unique string, but as it merges
strings to improve precision, its recall suffers sig-
nificantly. As noted above, perfect precision is not
possible on this task, because the reference set cov-
ers only a subset of the entities that appear in the
data. However, the numbers do measure the ability
to recover the reference entities in the most compact
table possible, allowing a quantitative comparison of
our models and the baseline approach.
8
Table 2 shows a database identified by the atem-
poral version of our model. The most densely-
populated columns in the table correspond to well-
defined name parts: columns 1 and 2 are almost
exclusively populated with first and last names re-
spectively, and column 3 is mainly populated by ti-
tles. The remaining columns are more of a grab
bag. Column 4 correctly captures Jr. for Martin
Luther King; column 5 correctly captures Luther,
but mistakenly contains Roberts (thus merging the
John Kerry and John Roberts entities), and Bhutto
(thus helping to merge the Bill Clinton and Benazir
Bhutto entities).
The model successfully distinguishes some, but
not all, of the entities that share tokens. For example,
the model separates Bill Clinton from Bill Nelson;
it also separates John McCain from John Kerry
(whom it mistakenly merges with John Roberts).
The ability to distinguish individuals who share first
names is due in part to the model attributing a low
concentration parameter to first names, meaning that
some repetition in the first name column is expected.
The model correctly identifies several titles and al-
ternative names, including the rare title Speaker for
Nancy Pelosi; however, it misses others, such as the
Senator title for Bill Nelson. This may be due in
part to the sample merging procedure used to gener-
ate this table, which requires that a cell contain the
same value in at least 80% of the samples.
Many errors may be attributed to slow mixing.
After mistakenly merging Bhutto and Clinton at
an early stage, the Gibbs sampler ? which treats
each mention independently ? is unable to sep-
arate them. Given that several other mentions of
Bhutto are already in the row occupied by Clin-
ton, the overall likelihood would benefit little from
creating a new row for a single mention, though
moving all such mentions simultaneously would re-
sult in an improvement. Larger scale Metropolis-
Hastings moves, such as split-merge or type-based
sampling (Liang et al, 2010) may help.
8 Related Work
Information Extraction A tradition of research
in information extraction focuses on processing raw
text to fill in the fields of manually-defined tem-
plates, thus populating databases of events or re-
lations (McNamee and Dang, 2009). While early
approaches focused on surface-level methods such
as wrapper induction (Kushmerick et al, 1997),
more recent work in this area includes Bayesian
nonparametrics to select the number of rows in the
database (Haghighi and Klein, 2010a). However,
even in such nonparametric work, the form of the
template and the number of slots are fixed in ad-
vance. Our approach differs in that the number of
fields and their meaning is learned from data. Recent
work by Chambers and Jurafsky (2011) approaches
a related problem, applying agglomerative cluster-
ing over sentences to detect events, and then clus-
tering syntactic constituents to induce the relevant
fields of each event entity. As described in Section 6,
our method performs well against an agglomerative
clustering baseline, though a more comprehensive
comparison of the two approaches is an important
step for future work.
Name Segmentation and Structure A related
stream of research focuses specifically on names:
identifying them in raw text, discovering their struc-
ture, and matching names that refer to the same en-
tity. We do not undertake the problem of named en-
tity recognition (Tjong Kim Sang, 2002), but rather
apply an existing NER system as a preprocessing
step (Finkel et al, 2005). Typical NER systems
do not attempt to discover the internal structure of
names or a database of canonical names, although
they often use prefabricated ?gazetteers? of names
and name parts as features to improve performance
(Borthwick et al, 1998; Sarawagi and Cohen, 2005).
Charniak (2001) shows that it is possible to learn a
model of name structure, either by using coreference
information as labeled data, or by leveraging a small
set of hand-crafted constraints. Elsner et al (2009)
develop a nonparametric Bayesian model of name
structure using adaptor grammars, which they use to
distinguish types of names (e.g., people, places, and
organizations). Li et al (2004) use a set of manually-
crafted ?transformations? of name parts to build a
model of how a name might be rendered in multi-
ple different ways. While each of these approaches
bears on one or more facets of the problem that we
consider here, none provides a holistic treatment of
name disambiguation and structure.
9
Resolving Mentions to Entities The problem of
resolving mentions to entities has been approach
from a variety of different perspectives. There is
an extensive literature on probabilistic record link-
age, in which database records are compared to de-
termine if they are likely to have the same real-world
referents (e.g., Felligi and Sunter, 1969; Bilenko
et al, 2003). Most approaches focus on pairwise
assessments of whether two records are the same,
whereas our method attempts to infer a single coher-
ent model of the underlying relational data. Some
more recent work in record linkage has explicitly
formulated the task of inferring a latent relational
model of a set of observed datasets (e.g., Cohen
et al, 2000; Pasula et al, 2002; Bhattacharya and
Getoor, 2007); however, to our knowledge, these
prior models have all exploited some predefined
database schema (i.e., set of columns), which our
model does not require. Many of these prior mod-
els have been applied to bibliographic data, where
different conventions and abbreviations lead to im-
perfect matches in different references to the same
publication. In our task, we consider name mentions
in raw text; such mentions are short, and may not
offer as many redundant clues for linkage as biblio-
graphic references.
In natural language processing, coreference res-
olution is the task of grouping entity mentions
(strings), in one or more documents, based on their
common referents in the world. Although much of
coreference resolution has on the single document
setting, there has been some recent work on cross-
document coreference resolution (Li et al, 2004;
Haghighi and Klein, 2007; Poon and Domingos,
2008; Singh et al, 2011). The problem we consider
is related to cross-document coreference, although
we take on the additional challenge of providing
a canonicalized name for each referent (the corre-
sponding table row), and in inferring a structured
representation of entity names (the table columns).
For this reason, our evaluation focuses on the in-
duced table of entities, rather than the clustering of
mention strings. The best coreference systems de-
pend on carefully crafted, problem-specific linguis-
tic features (Bengtson and Roth, 2008) and exter-
nal knowledge (Haghighi and Klein, 2010b). Future
work might consider how to exploit such features for
the more holistic information extraction setting.
9 Conclusion
This paper presents a Bayesian nonparametric ap-
proach to recover structured records from text. Us-
ing only a small set of prototype records, we are able
to recover an accurate table that jointly identifies en-
tities and internal name structure. In our view, the
main advantage of a Bayesian approach compared
to more heuristic alternatives is that it facilitates in-
corporation of additional information sources when
available. In this paper, we have considered one
such additional source, incorporating temporal con-
text using the recurrent Chinese restaurant process.
We envision enhancing the model in several other
respects. One promising direction is the incorpo-
ration of name structure, which could be captured
using a first-order Markov model of the transitions
between name parts. In the nonparametric setting,
a transition matrix is unbounded along both dimen-
sions, and this can be handled by a hierarchical
Dirichlet process (HDP; Teh et al2006).4 We en-
vision other potential applications of the HDP: for
example, learning ?topics? of entities which tend to
appear together (i.e., given a mention of Mahmoud
Abbas in the American press, a mention of Ben-
jamin Netanyahu is likely), and handling document-
specific burstiness (i.e., given that an entity is men-
tioned once in a document, it is much more likely
to be mentioned again). Finally, we would like
to incorporate lexical context from the sentences in
which each entity is mentioned, which might help to
distinguish, say, computer science researchers who
share names with former defense secretaries or pro-
fessional basketball players.
Acknowledgments This research was enabled
by AFOSR FA95501010247, DARPA grant
N10AP20042, ONR N000140910758, NSF DBI-
0546594, IIS-0713379, IIS-0915187, IIS-0811562,
an Alfred P. Sloan Fellowship, and Google?s support
of the Worldly Knowledge project at CMU. We
thank the reviewers for their thoughtful feedback.
4One of the reviewers proposed to draw entire column se-
quences from a Dirichlet process. Given the relatively small
number of columns and canonical name forms, this may be a
straightforward and effective alternative to the HDP.
10
References
Amr Ahmed and Eric P. Xing. 2008. Dynamic non-
parametric mixture models and the recurrent Chinese
restaurant process with applications to evolutionary
clustering. In International Conference on Data Min-
ing.
Javier Artiles, Julio Gonzalo, and Satoshi Sekine. 2007.
The SemEval-2007 WePS evaluation: establishing a
benchmark for the web people search task. In Pro-
ceedings of the 4th International Workshop on Seman-
tic Evaluations, SemEval ?07, pages 64?69. Associa-
tion for Computational Linguistics.
Eric Bengtson and Dan Roth. 2008. Understanding
the value of features for coreference resolution. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP ?08,
pages 294?303, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Indrajit Bhattacharya and Lise Getoor. 2007. Collec-
tive entity resolution in relational data. ACM Trans.
Knowl. Discov. Data, 1(1), March.
Mikhail Bilenko, William W. Cohen, Stephen Fien-
berg, Raymond J. Mooney, and Pradeep Ravikumar.
2003. Adaptive name-matching in information in-
tegration. IEEE Intelligent Systems, 18(5):16?23,
September/October.
A. Borthwick, J. Sterling, E. Agichtein, and R. Grishman.
1998. Exploiting diverse knowledge sources via max-
imum entropy in named entity recognition. In Sixth
Workshop on Very Large Corpora New Brunswick,
New Jersey. Association for Computational Linguis-
tics.
Nathanael Chambers and Dan Jurafsky. 2011. Template-
based information extraction without the templates. In
Proceedings of ACL.
Eugene Charniak. 2001. Unsupervised learning of name
structure from coreference data. In Proceedings of the
Second Meeting of the North American Chapter of the
Association for Computational Linguistics.
William W. Cohen, Henry Kautz, and David McAllester.
2000. Hardening soft information sources. In Pro-
ceedings of the Sixth International Conference on
Knowledge Discovery and Data Mining, pages 255?
259.
George Doddington, Alexis Mitchell, Mark Przybocki,
Lance Ramshaw, Stephanie Strassel, and Ralph
Weischedel. 2004. The automatic content extraction
(ace) program: Tasks, data, and evaluation. In 4th
international conference on language resources and
evaluation (LREC?04).
Jacob Eisenstein and Eric Xing. 2010. The CMU 2008
political blog corpus. Technical report, Carnegie Mel-
lon University.
Ergin Elmacioglu, Yee Fan Tan, Su Yan, Min-Yen Kan,
and Dongwon Lee. 2007. Psnus: Web people name
disambiguation by simple clustering with rich features.
In Proceedings of the Fourth International Workshop
on Semantic Evaluations (SemEval-2007), pages 268?
271, Prague, Czech Republic, June. Association for
Computational Linguistics.
Micha Elsner, Eugene Charniak, and Mark Johnson.
2009. Structured generative models for unsupervised
named-entity clustering. In Proceedings of Human
Language Technologies: The 2009 Annual Conference
of the North American Chapter of the Association for
Computational Linguistics, pages 164?172, Boulder,
Colorado, June. Association for Computational Lin-
guistics.
I. P. Felligi and A. B. Sunter. 1969. A theory for record
linkage. Journal of the American Statistical Society,
64:1183?1210.
Jenny R. Finkel, Trond Grenager, and Christopher Man-
ning. 2005. Incorporating non-local information
into information extraction systems by Gibbs sam-
pling. In Proceedings of the 43rd Annual Meeting on
Association for Computational Linguistics, ACL ?05,
pages 363?370, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Aria Haghighi and Dan Klein. 2007. Unsupervised
coreference resolution in a nonparametric bayesian
model. In Proceedings of the 45th Annual Meeting
of the Association of Computational Linguistics, pages
848?855, Prague, Czech Republic, June. Association
for Computational Linguistics.
Aria Haghighi and Dan Klein. 2010a. An entity-level
approach to information extraction. In Proceedings
of the ACL 2010 Conference Short Papers, ACLShort
?10, pages 291?295, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
Aria Haghighi and Dan Klein. 2010b. Coreference reso-
lution in a modular, entity-centered model. In Human
Language Technologies: The 2010 Annual Conference
of the North American Chapter of the Association for
Computational Linguistics, pages 385?393, Los An-
geles, California, June. Association for Computational
Linguistics.
Harold W. Kuhn. 1955. The Hungarian method for the
assignment problem. Naval Research Logistic Quar-
terly, 2:83?97.
Nicholas Kushmerick, Daniel S. Weld, and Robert
Doorenbos. 1997. Wrapper induction for information
extraction. In Proceedings of IJCAI.
Xin Li, Paul Morie, and Dan Roth. 2004. Identification
and tracing of ambiguous names: Discriminative and
generative approaches. In Proceedings of AAAI, pages
419?424.
11
Percy Liang, Michael I. Jordan, and Dan Klein. 2010.
Type-Based MCMC. In Human Language Technolo-
gies: The 2010 Annual Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics, pages 573?581, Los Angeles, California,
June. Association for Computational Linguistics.
Christopher D. Manning, Prabhakar Raghavan, and Hin-
rich Schu?tze. 2008. Introduction to Information Re-
trieval. Cambridge University Press, 1 edition, July.
Paul McNamee and Hoa Trang Dang. 2009. Overview
of the TAC 2009 knowledge base population track. In
Proceedings of the Text Analysis Conference (TAC).
Hanna Pasula, Bhaskara Marthi, Brian Milch, Stuart Rus-
sell, and Ilya Shpitser. 2002. Identity uncertainty and
citation matching. In Advances in Neural Processing
Systems 15, Vancouver, British Columbia. MIT Press.
Hoifung Poon and Pedro Domingos. 2008. Joint un-
supervised coreference resolution with Markov Logic.
In Proceedings of the 2008 Conference on Empirical
Methods in Natural Language Processing, pages 650?
659, Honolulu, Hawaii, October. Association for Com-
putational Linguistics.
Carl E. Rasmussen. 2000. The Infinite Gaussian Mixture
Model. In In Advances in Neural Information Process-
ing Systems 12, volume 12, pages 554?560.
Sunita Sarawagi and William W. Cohen. 2005. Semi-
Markov conditional random fields for information ex-
traction. In Lawrence K. Saul, Yair Weiss, and Le?on
Bottou, editors, Advances in Neural Information Pro-
cessing Systems 17, pages 1185?1192. MIT Press,
Cambridge, MA.
Sameer Singh, Amarnag Subramanya, Fernando Pereira,
and Andrew McCallum. 2011. Large-scale cross-
document coreference using distributed inference and
hierarchical models. In Association for Computa-
tional Linguistics: Human Language Technologies
(ACL HLT).
Yee W. Teh, Michael I. Jordan, Matthew J. Beal, and
David M. Blei. 2006. Hierarchical dirichlet pro-
cesses. Journal of the American Statistical Associa-
tion, 101:1566?1581, December.
Erik F. Tjong Kim Sang. 2002. Introduction to
the CoNLL-2002 shared task: Language-independent
named entity recognition. In Proceedings of the Sixth
Conference on Natural Language Learning.
Greg C. G. Wei and Martin A. Tanner. 1990. A Monte
Carlo Implementation of the EM Algorithm and the
Poor Man?s Data Augmentation Algorithms. Journal
of the American Statistical Association, 85(411):699?
704.
12
