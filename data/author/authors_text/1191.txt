BioNLP 2007: Biological, translational, and clinical language processing, pages 81?88,
Prague, June 2007. c?2007 Association for Computational Linguistics
ConText: An Algorithm for Identifying Contextual Features 
from Clinical Text 
Wendy W. Chapman, David Chu, John N. Dowling 
Department of Biomedical Informatics 
University of Pittsburgh 
Pittsburgh, PA 
chapman@cbmi.pitt.edu 
 
Abstract 
Applications using automatically indexed 
clinical conditions must account for con-
textual features such as whether a condition 
is negated, historical or hypothetical, or 
experienced by someone other than the pa-
tient. We developed and evaluated an algo-
rithm called ConText, an extension of the 
NegEx negation algorithm, which relies on 
trigger terms, pseudo-trigger terms, and 
termination terms for identifying the values 
of three contextual features. In spite of its 
simplicity, ConText performed well at 
identifying negation and hypothetical status. 
ConText performed moderately at identify-
ing whether a condition was experienced 
by someone other than the patient and 
whether the condition occurred historically.  
1 Introduction 
Natural language processing (NLP) techniques can 
extract variables from free-text clinical records 
important for medical informatics applications per-
forming decision support, quality assurance, and 
biosurveillance [1-6]. Many applications have fo-
cused on identifying individual clinical conditions 
in textual records, which is the first step in making 
the conditions available to computerized applica-
tions. However, identifying individual instances of 
clinical conditions is not sufficient for many medi-
cal informatics tasks?the context surrounding the 
condition is crucial for integrating the information 
within the text to determine the clinical state of a 
patient.  
For instance, it is important to understand 
whether a condition is affirmed or negated, acute 
or chronic, or mentioned hypothetically. We refer 
to these as contextual features, because the infor-
mation is not usually contained in the lexical repre-
sentation of the clinical condition itself but in the 
context surrounding the clinical condition. We de-
veloped an algorithm called ConText for identify-
ing three contextual features relevant for biosur-
veillance from emergency department (ED) reports 
and evaluated its performance compared to physi-
cian annotation of the features. 
2 Background 
2.1 Encoding Contextual Information from-
Clinical Texts 
NLP systems designed to encode detailed informa-
tion from clinical reports, such as MedLEE [1], 
MPLUS [7], and MedSyndikate [4], encode con-
textual features such as negation, uncertainty, 
change over time, and severity. Over the last ten 
years, several negation algorithms have been de-
scribed in the literature [8-12]. Recently, research-
ers at Columbia University have categorized tem-
poral expressions in clinical narrative text and 
evaluated a temporal constraint structure designed 
to model the temporal information for discharge 
summaries [13, 14]. 
ConText differs from most other work in this 
area by providing a stand-alone algorithm that can 
be integrated with any application that indexes 
clinical conditions from text.  
2.2 Biosurveillance from ED Data 
Biosurveillance and situational awareness are im-
perative research issues in today?s world. State-of-
the-art surveillance systems rely on chief com-
plaints and ICD-9 codes, which provide limited 
clinical information and have been shown to per-
form with only fair to moderate sensitivity [15-18]. 
ED reports are a timely source of clinical informa-
81
tion that may be useful for syndromic surveillance. 
We are developing NLP-based methods for identi-
fying clinical conditions from ED reports. 
2.3 SySTR 
We are developing an NLP application called 
SySTR (Syndromic Surveillance from Textual Re-
cords). It currently uses free-text descriptions of 
clinical conditions in ED reports to determine 
whether the patient has an acute lower respiratory 
syndrome.  We previously identified 55 clinical 
conditions (e.g. cough, pneumonia, oxygen desatu-
ration, wheezing) relevant for determining whether 
a patient has an acute lower respiratory condition 
[19]. SySTR identifies instances of these 55 clini-
cal conditions in ED reports to determine if a pa-
tient has an acute lower respiratory syndrome. 
SySTR has four modules: 
(1) Index each instance of the 55 clinical condi-
tions in an ED report; 
(2) For each indexed instance of a clinical condi-
tion, assign values to three contextual features; 
(3) Integrate the information from indexed in-
stances to determine whether each of the 55 
conditions are acute, chronic, or absent; 
(4) Use the values of the 55 conditions to deter-
mine whether a patient has an acute lower res-
piratory syndrome. 
We built SySTR on top of an application 
called caTIES [20], which comprises a GATE 
pipeline of processing resources (http://gate.ac.uk/). 
Module 1 uses MetaMap [5] to index UMLS con-
cepts in the text and then maps the UMLS concepts 
to the 55 clinical conditions. For instance, Module 
1 would identify the clinical condition Dyspnea in 
the sentence ?Patient presents with a 3 day history 
of shortness of breath.? For each instance of the 55 
conditions identified by Module 1, Module 2 as-
signs values to three contextual features: Negation 
(negated, affirmed); Temporality (historical, re-
cent, hypothetical); and Experiencer (patient, 
other). For the sentence above, Module 2 would 
assign Dyspnea the following contextual features 
and their values: Negation?affirmed; Temporal-
ity?recent; Experiencer?patient. Module 3, as 
described in Chu and colleagues [21], resolves 
contradictions among multiple instances of clinical 
conditions, removes conditions not experienced by 
the patient, and assigns a final value of acute, 
chronic, or absent to each of the 55 conditions. 
Module 4 uses machine learning models to deter-
mine whether a patient has acute lower respiratory 
syndrome based on values of the conditions.  
The objective of this study was to evaluate an 
algorithm for identifying the contextual informa-
tion generated by Module 2. 
3 Methods 
We developed an algorithm called ConText for 
determining the values for three contextual features 
of a clinical condition: Negation, Temporality, and 
Experiencer. The same algorithm is applied to all 
three contextual features and is largely based on a 
regular expression algorithm for determining 
whether a condition is negated or not (NegEx [9]). 
ConText relies on trigger terms, pseudo-trigger 
terms, and scope termination terms that are specific 
to the type of contextual feature being identified. 
Below we describe the three contextual features 
addressed by the algorithm, details of how Con-
Text works, and our evaluation of ConText. 
3.1 Three Contextual Features 
Determining whether a patient had an acute epi-
sode of a clinical condition, such as cough, poten-
tially involves information described in the context 
of the clinical condition in the text. We performed 
a pilot study to learn which contextual features af-
fected classification of 55 clinical conditions as 
acute, chronic, or absent [21]. The pilot study 
identified which contextual features were critical 
for our task and reduced the number of values we 
initially used.  
The contextual features for each indexed clinical 
condition are assigned default values. ConText 
changes the values if the condition falls within the 
scope of a relevant trigger term. Below, we de-
scribe the contextual features (default values are in 
parentheses). 
(1) Negation (affirmed): ConText determines 
whether a condition is negated, as in ?No fe-
ver.? 
(2) Temporality (recent): ConText can change 
Temporality to historical or hypothetical. In its 
current implementation, historical is defined as 
beginning at least 14 days before the visit to 
the ED, but the algorithm can easily be modi-
fied to change the length of time. ConText 
would mark Fever in ?Patient should return if 
she develops fever? as hypothetical.   
82
(3) Experiencer (patient): ConText assigns condi-
tions ascribed to someone other than the pa-
tient an Experiencer of other, as in ?The pa-
tient?s father has a history of CHF.?  
3.2 Contextual Feature Algorithm 
As we examined how the contextual features were 
manifested in ED reports, we discovered similar 
patterns for all features and hypothesized that an 
existing negation algorithm, NegEx [9], may be 
applicable for all three features.  
NegEx uses two regular expressions (RE) to de-
termine whether an indexed condition is negated: 
RE1: <trigger term> <5w> <indexed term> 
RE2: <indexed term> <5w> <trigger term> 
<5w> represents five words (a word can be a sin-
gle word or a UMLS concept), and the text 
matched by this pattern is called the scope. NegEx 
relies on three types of terms to determine whether 
a condition is negated: trigger terms, pseudo-
trigger terms, and termination terms. Trigger terms 
such as ?no? and ?denies? indicate that the clinical 
conditions that fall within the scope of the trigger 
term should be negated. Pseudo-trigger terms, such 
as ?no increase,? contain a negation trigger term 
but do not indicate negation of a clinical concept.  
A termination term such as ?but? can terminate the 
scope of the negation before the end of the win-
dow, as in ?She denies headache but complains of 
dizziness.?  
ConText is an expansion of NegEx. It relies on 
the same basic algorithm but applies different term 
lists and different windows of scope depending on 
the contextual feature being annotated.  
3.3 ConText Term Lists  
Each contextual feature has a unique set of trigger 
terms and pseudo-trigger terms, as shown in Table 
1. The complete list of terms can be found at 
http://web.cbmi.pitt.edu/chapman/ConText.html. 
Most of the triggers apply to RE1, but a few 
(marked in table) apply to RE2. ConText assigns a 
default value to each feature, then changes that 
value if a clinical condition falls within the scope 
of a relevant trigger term.  
Although trigger terms are unique to the contex-
tual feature being identified, termination terms 
Table 1. Examples of trigger and pseudo-trigger terms for the three contextual features. If all terms are not 
represented in the table, we indicate the number of terms used by ConText in parentheses. 
Temporality (default = recent) 
Trigger terms for 
hypothetical 
Pseudo-trigger 
terms Trigger terms for historical Pseudo-trigger terms (10) 
if 
return 
should [he|she] 
should there 
should the patient 
as needed 
come back [for|to] 
if negative 
 
General triggers 
history 
previous^ 
History Section title^^ 
Temporal Measurement triggers^^^ 
  <time> of 
  [for|over] the [last|past] <time> 
  since (last) [day-of-week|week|month| 
    season|year] 
history, physical 
history taking 
poor history 
history and examination 
history of present illness 
social history 
family history 
sudden onset of 
Experiencer (default = patient)  Negation (default = affirmed) 
Trigger terms 
for other (12) 
Pseudo-trigger 
terms 
 Trigger terms for negated (125) Pseudo-trigger terms (16) 
father(?s) 
mother(?s) 
aunt(?s) 
  no 
not 
denies 
without 
no increase 
not extend 
gram negative 
^  the scope for ?previous? only extends one term forward (e.g., ?for previous headache?) 
^^Currently the only history section title we use is PAST MEDICAL HISTORY. 
^^^ <time> includes the following regular expression indicating a temporal quantification: x[-|space]   
[day(s)|hour(s)|week(s)|month(s)|year(s)]. x = any digit; words in brackets are disjunctions; items in parentheses are 
optional. The first two temporal measurement triggers are used with RE1; the third is used with RE2. For our 
current application, a condition lasting 14 days or more is considered historical.  
 
83
may be common to multiple contextual features. 
For instance, a termination term indicating that the 
physician is speaking about the patient can indicate 
termination of scope for the features Temporality 
and Experiencer. In the sentence ?History of 
COPD, presenting with shortness of breath,? the 
trigger term ?history? indicates that COPD is his-
torical, but the term ?presenting? terminates the 
scope of the temporality trigger term, because the 
physician is now describing the current patient 
visit. Therefore, the condition Dyspnea (?shortness 
of breath?) should be classified as recent. Simi-
larly, in the sentence ?Mother has CHF and patient 
presents with chest pain,? Experiencer for CHF 
should be other, but Experiencer for Chest Pain 
should be patient.  
We compiled termination terms into conceptual 
groups, as shown in Table 2.  
Table 2. ConText?s termination terms. Column 1 lists 
the type of termination term, the number of terms used 
by Context, and the contextual feature values using that 
type of termination term. Column 2 gives examples of 
the terms. 
Type of Term Examples 
Patient (5) 
Temporal (hypothetical) 
Experiencer (other) 
Patient, who, his, her, pa-
tient?s 
Presentation (12) 
Temporal (historical) 
Experiencer (other) 
Presents, presenting, com-
plains, was found, states, 
reports, currently, today 
Because (2) 
Temporal (hypothetical) Since, because 
Which (1) 
Experiencer (other) Which 
ED (2) 
Temporal (historical) Emergency department, ED 
But (8) 
Negation (negated) 
But, however, yet, though, 
although, aside from 
3.4 ConText Algorithm 
The input to ConText is an ED report with in-
stances of the 55 clinical concepts already indexed. 
For each clinical condition, ConText assigns val-
ues to the three contextual features. ConText?s al-
gorithm is as follows1: 
                                                 
1 This algorithm applies to RE1. The algorithm for RE2 
is the same, except that it works backwards from the 
trigger term and does not look for pseudo-trigger terms. 
Go to first trigger term in sentence 
If term is a pseudo-trigger term, 
   Skip to next trigger term  
Determine scope of trigger term 
If termination term within scope, 
   Terminate scope before termination term 
Assign appropriate contextual feature value to 
all indexed clinical concepts within scope.  
The scope of a trigger term depends on the con-
textual feature being classified. The default scope 
includes all text following the indexed condition 
until the end of the sentence. Thus, in the sentence 
?He should return for fever? the scope of the Tem-
porality (hypothetical) trigger term ?return? in-
cludes the segment ?for fever,? which includes an 
indexed condition Fever. The default scope is over-
ridden in a few circumstances. First, as described 
above, the scope can be terminated by a relevant 
termination term. Second, if the trigger term is a 
<section title>, the scope extends throughout the 
entire section, which is defined previous to Con-
Text?s processing. Third, a trigger term itself can 
require a different scope. The Temporality (histori-
cal) term ?previous? only extends one term for-
ward in the sentence. 
3.5 Evaluation 
We evaluated ConText?s ability to assign correct 
values to the three contextual features by compar-
ing ConText?s annotations with annotations made 
by a physician. 
Setting and Subjects. The study was conducted on 
reports for patients presenting to the University of 
Pittsburgh Medical Center Presbyterian Hospital 
ED during 2002. The study was approved by the 
University of Pittsburgh?s Institutional Review 
Board. We randomly selected 120 reports for pa-
tients with respiratory-related ICD-9 discharge di-
agnoses for manual annotation. For this study, we 
used 30 reports as a development set and 90 re-
ports as a test set. In addition to the annotated de-
velopment set, we used a separate set of 100 unan-
notated ED reports to informally validate our term 
lists. 
Reference Standard. A physician board-certified 
in internal medicine and infectious diseases with 
30 years of experience generated manual annota-
tions for the development and test reports. He used 
GATE (http://gate.ac.uk/) to highlight every indi-
84
vidual annotation in the text referring to any of the 
55 clinical conditions. For every annotation, he 
assigned values to the three contextual features, as 
shown in Figure 1.  
Previous experience in annotating the 55 condi-
tions showed that a single physician was inade-
quate for generating a reliable reference standard 
[19]. The main mistake made by a single physician 
was not marking a concept that existed in the text. 
We used NLP-assisted review to improve physi-
cian annotations by comparing the single physi-
cian?s annotations to those made by SySTR. The 
physician reviewed disagreements and made 
changes to his original annotations if he felt his 
original annotation was incorrect. A study by Mey-
stre and Haug [22] used a similar NLP-assisted 
review methodology and showed that compared to 
a reference standard not using NLP-assisted re-
view, their system had higher  recall and the same 
precision. 
Outcome Measures. For each contextual feature 
assigned to an annotation, we compared ConText?s 
value to the value assigned by the reference stan-
dard. We classified the feature as a true positive 
(TP) if ConText correctly changed the condition?s 
default value and a true negative (TN) if ConText 
correctly left the default value. We then calculated 
recall and precision using the following formulas: 
)(
:Recall
FNofnumberTPofnumber
TPofnumber
+
 
)(
:Precision
FPofnumberTPofnumber
TPofnumber
+
 
For the Temporality feature, we calculated recall 
and precision separately for the values historical 
and hypothetical. We calculated the 95% confi-
dence intervals (CI) for all outcome measures. 
4 Results 
Using NLP-assisted review, the reference standard 
physician made several changes to his initial anno-
tations. He indexed an additional 82 clinical condi-
tions and changed the title of the clinical condition 
for 48 conditions, resulting in a total of 1,620 in-
dexed clinical conditions in the 90 test reports. The 
reference standard physician also made 35 changes 
to Temporality values and 4 changes to Negation. 
The majority of Temporality changes were from 
historical to recent (17) and from hypothetical to 
recent (12).   
Table 3 shows ConText?s recall and precision 
values compared to the reference standard annota-
tions. About half of the conditions were negated 
(773/1620). Fewer conditions were historical 
(95/1620), hypothetical (40/1620), or experienced 
by someone other than the patient (8/1620). In 
spite of low frequency for these contextual feature 
values, identifying them is critical to understanding 
a patient?s current state. ConText performed best 
on Negation, with recall and precision above 97%. 
ConText performed well at assigning the Tempo-
rality value hypothetical, but less well on the Tem-
porality value historical. Experiencer had a small 
sample size, making results difficult to interpret. 
 
Table 3. Outcome measures for ConText on test set of 90 ED reports. 
Feature TP TN FP FN Recall 95% CI 
Precision 
95% CI 
Negation 750 824 23 23 97.0 96-98 
97.0 
96-98 
Temporality 
(historical) 66 1499 23 32 
67.4 
58-76 
74.2 
64-82 
Temporality 
(hypothetical) 33 1578 2 7 
82.5 
68-91 
94.3 
81-98 
Experiencer 4 1612 0 4 50.00 22-78 
100 
51-100 
5 Discussion 
We evaluated an extension of the NegEx algorithm 
for determining the values of two additional con-
textual features?Temporality and Experiencer. 
ConText performed with very high recall and pre-
cision when determining whether a condition was 
negated, and demonstrated moderate to high per-
formance on the other features. 
Figure 1. When the physician highlights text, 
GATE provides a drop-down menu to select the 
Clinical Condition and the values of the Contex-
tual Features. 
85
We performed an informal error analysis, which 
not only isolates ConText?s errors but also points 
out future research directions in contextual feature 
identification.  
5.1 Negation 
ConText?s negation identification performed sub-
stantially better than NegEx?s published results [9], 
even though ConText is very similar to NegEx and 
uses the same trigger terms.  Several possible ex-
planations exist for this boost in performance. First, 
our study evaluated negation identification in ED 
reports, whereas the referenced study on NegEx 
applied to discharge summaries. Second, ConText 
only applied to 55 clinical conditions, rather than 
the large set of UMLS concepts in the NegEx 
study. Third, the conditions indexed by SySTR that 
act as input to ConText are sometimes negated or 
affirmed before ConText sees them. For some con-
ditions, SySTR addresses internal negation in a 
word (e.g., ?afebrile? is classified as Fever with the 
Negation value negated). Also, SySTR assigns 
Negation values to some conditions with numeric 
values, such as negating Tachycardia from ?pulse 
rate 75.? Fourth, ConText does not use NegEx?s 
original scope of five words, but extends the scope 
to the end of the sentence. It would be useful to 
compare ConText?s scope difference directly 
against NegEx to determine which scope assign-
ment works better, but our results suggest the in-
creased scope may work well for ED reports. 
ConText?s errors in assigning the Negation 
value were equally distributed between FN?s and 
FP?s (23 errors each). Some false negatives re-
sulted from missing trigger terms (e.g., ?denying?). 
Several false negatives resulted from the interac-
tion between ConText and SySTR?s mapping rules. 
For example, in the sentence ?chest wall is without 
tenderness,? SySTR maps the UMLS concepts for 
?chest wall? and ?tenderness? to the condition 
Chest Wall Tenderness. In such a case, the nega-
tion trigger term ?without? is caught between the 
two UMLS concepts. Therefore, RE1 does not 
match, and ConText does not change the default 
from affirmed. False positive negations resulted 
from our not integrating the rule described in 
NegEx that a concept preceded by a definite article 
should not be negated [23] (e.g., ?has not been on 
steroids for his asthma?) and from descriptions in 
the text whose Negation status is even difficult for 
humans to determine, such as ?no vomiting with-
out having the cough? and ?patient does not know 
if she has a fever.? 
5.2 Temporality 
Historical. ConText identified historical condi-
tions with 67% sensitivity and 74% precision. 
Identifying historical conditions appears simple on 
the surface, but is a complex problem. The single 
trigger term ?history? is used for many of the his-
torical conditions, but the word ?history? is a rela-
tive term that can indicate a history of years (as in 
?history of COPD?) or of only a few days (as in 
?ENT: No history of nasal congestion?). The error 
analysis showed that ConText is missing trigger 
terms that act equivalently to the word ?history? 
such as ?in the past? (?has not been on steroids in 
the past for his asthma?) and ?pre-existing? (?pre-
existing shortness of breath?).  
Some conditions that the reference standard 
classified as historical had no explicit trigger in the 
text, as in the sentence ?When he sits up in bed, he 
develops pain in the chest.? It may be useful to 
implement rules involving verb tense for these 
cases. 
The most difficult cases for ConText were those 
with temporal measurement triggers. The few tem-
poral quantifier patterns we used were fairly suc-
cessful, but the test set contained multiple varia-
tions on those quantifiers, and a new dataset would 
probably introduce even more variations. For in-
stance, ConText falsely classified Non-pleuritic 
Chest Pain as historical in ?awoken at approxi-
mately 2:45 with chest pressure,? because Con-
Text?s temporal quantifiers do not account for time 
of the day. Also, even though ConText?s temporal 
quantifiers include the pattern ?last x weeks,? x 
represents a digit and thus didn?t match the phrase 
?intermittent cough the last couple of weeks.?  
We were hoping that identifying historical con-
ditions would not require detailed modeling of 
temporal information, but our results suggest oth-
erwise. We will explore the temporal categories 
derived by Hripcsak and Zhou [13] for discharge 
summaries to expand ConText?s ability to identify 
temporal measurement triggers.  
Hypothetical. ConText demonstrated 83% recall 
and 94% precision when classifying a condition as 
hypothetical rather than recent. Again, missing 
trigger terms (e.g., ?returning? and ?look out for?) 
and termination terms (e.g., ?diagnosis?) caused 
errors. The chief cause of false negatives was ter-
86
minating the scope of a trigger term too early. For 
instance, in the sentence ?She knows to return to 
the ED if she has anginal type chest discomfort 
which was discussed with her, shortness of breath, 
and peripheral edema? the scope of the trigger ?re-
turn? was terminated by ?her.? The major limita-
tion of regular expressions is evident in this exam-
ple in which ?her? is part of a relative clause modi-
fying ?chest discomfort,? not ?shortness of breath.?  
5.3 Experiencer 
ConText?s ability to identify an experiencer other 
than the patient suffered from low prevalence. In 
the test set of 90 reports, only 8 of the 1620 condi-
tions were experienced by someone other than the 
patient, and ConText missed half of them. Two of 
the false negatives came from not including the 
trigger term ?family history.? A more difficult er-
ror to address is recognizing that bronchitis is ex-
perienced by someone other than the patient in 
??due to the type of bronchitis that is currently 
being seen in the community.? ConText made no 
false positive classifications for Experiencer. 
5.4 Limitations and Future Work 
Some of ConText?s errors can be resolved by refin-
ing the trigger and termination terms. However, 
many of the erroneous classifications are due to 
complex syntax and semantics that cannot be han-
dled by simple regular expressions. Determining 
the scope of trigger terms in sentences with relative 
clauses and coordinated conjunctions is especially 
difficult. We believe ConText?s approach involv-
ing trigger terms, scope, and termination terms is 
still a reasonable model for this problem and hope 
to improve ConText?s ability to identify scope with 
syntactic information.  
A main limitation of our evaluation was the ref-
erence standard, which was comprised of a single 
physician. We used NLP-assisted review to in-
crease the identification of clinical conditions and 
decrease noise in his classifications. It is possible 
that the NLP-assisted review biased the reference 
standard toward ConText?s classifications, but the 
majority of changes made after NLP-assisted re-
view involved indexing the clinical conditions, 
rather than changing the values of the contextual 
features. Moreover, most of the changes to contex-
tual feature values involved a change in our anno-
tation schema after the physician had completed 
his first round of annotations. Specifically, we al-
lowed the physician to use the entire report to de-
termine whether a condition was historical, which 
caused him to mark recent exacerbations of his-
torical conditions as historical. A second physician 
is in the process of annotating the test set. The two 
physicians will come to consensus on their classi-
fications in generating a new reference standard.  
How good contextual feature identification has 
to be depends largely on the intended application. 
We tested SySTR?s ability to determine whether 
the 55 clinical conditions were acute, chronic, or 
absent on a subset of 30 test reports [24]. SySTR 
made 51 classification errors, 22 of which were 
due to ConText?s mistakes. In spite of the errors, 
SySTR demonstrated a kappa of 0.85 when com-
pared to physician classifications, suggesting that 
because of redundancy in clinical reports, Con-
Text?s mistakes may not have a substantial adverse 
effect on SySTR?s final output.  
5.5 Conclusion 
We evaluated a regular-expression-based algorithm 
for determining the status of three contextual fea-
tures in ED reports and found that ConText per-
formed very well at identifying negated conditions, 
fairly well at determining whether conditions were 
hypothetical or historical, and moderately well at 
determining whether a condition was experienced 
by someone other than the patient. ConText?s algo-
rithm is based on the negation algorithm NegEx, 
which is a frequently applied negation algorithm in 
biomedical informatics applications due to its sim-
plicity, availability, and generalizability to various 
NLP applications. Simple algorithms for identify-
ing contextual features of indexed conditions is 
important in medical language processing for im-
proving the accuracy of information retrieval and 
extraction applications and for providing a baseline 
comparison for more sophisticated algorithms. 
ConText accepts any indexed clinical conditions as 
input and thus may be applicable to other NLP ap-
plications. We do not know how well ConText will 
perform on other report types, but see similar con-
textual features in discharge summaries, progress 
notes, and history and physical exams. Currently, 
ConText only identifies three contextual features, 
but we hope to extend the algorithm to other fea-
tures in the future, such as whether a condition is 
mentioned as a radiology finding or as a diagnosis 
(e.g., Pneumonia).  
87
Over and above negation identification, which 
can be addressed by NegEx or other algorithms, 
ConText could be useful for a variety of NLP tasks, 
including flagging historical findings and eliminat-
ing indexed conditions that are hypothetical or 
were not experienced by the patient. Ability to 
modify indexed conditions based on their contex-
tual features can potentially improve precision in 
biosurveillance, real-time decision support, and 
information retrieval. 
Acknowledgments. This work was supported by 
NLM grant K22 LM008301, ?Natural language 
processing for respiratory surveillance.? 
References 
1. Friedman C. A broad-coverage natural language 
processing system. Proc AMIA Symp 2000:270-4. 
2. Fiszman M, Chapman WW, Aronsky D, Evans RS, 
Haug PJ. Automatic detection of acute bacterial pneu-
monia from chest X-ray reports. J Am Med Inform 
Assoc 2000;7(6):593-604. 
3. Taira R, Bashyam V, Kangarloo H. A field theory 
approach to medical natural language processing. IEEE 
Transactions in Inform Techn in Biomedicine 
2007;11(2). 
4. Hahn U, Romacker M, Schulz S. MEDSYNDI-
KATE-a natural language system for the extraction of 
medical information from findings reports. Int J Med Inf 
2002;67(1-3):63-74. 
5. Aronson AR. Effective mapping of biomedical text to 
the UMLS Metathesaurus: the MetaMap program. Proc 
AMIA Symp 2001:17-21. 
6. Hazlehurst B, Frost HR, Sittig DF, Stevens VJ. 
MediClass: A system for detecting and classifying en-
counter-based clinical events in any electronic medical 
record. J Am Med Inform Assoc 2005;12(5):517-29. 
7. Christensen L, Haug PJ, Fiszman M. MPLUS: a 
probabilistic medical language understanding system. 
Proc Workshop on Natural Language Processing in the 
Biomedical Domain 2002:29-36. 
8. Mutalik PG, Deshpande A, Nadkarni PM. Use of 
general-purpose negation detection to augment concept 
indexing of medical documents: a quantitative study 
using the UMLS. J Am Med Inform Assoc 
2001;8(6):598-609. 
9. Chapman WW, Bridewell W, Hanbury P, Cooper GF, 
Buchanan BG. A simple algorithm for identifying ne-
gated findings and diseases in discharge summaries. J 
Biomed Inform 2001;34(5):301-10. 
10. Elkin PL, Brown SH, Bauer BA, Husser CS, Carruth 
W, Bergstrom LR, et al A controlled trial of automated 
classification of negation from clinical notes. BMC Med 
Inform Decis Mak 2005;5(1):13. 
11. Herman T, Matters M, Walop W, Law B, Tong W, 
Liu F, et al Concept negation in free text components of 
vaccine safety reports. AMIA Annu Symp Proc 
2006:1122. 
12. Huang Y, Lowe HJ. A Novel Hybrid Approach to 
Automated Negation Detection in Clinical Radiology 
Reports. J Am Med Inform Assoc 2007. 
13. Hripcsak G, Zhou L, Parsons S, Das AK, Johnson 
SB. Modeling electronic discharge summaries as a sim-
ple temporal constraint satisfaction problem. J Am Med 
Inform Assoc 2005;12(1):55-63. 
14. Zhou L, Melton GB, Parsons S, Hripcsak G. A tem-
poral constraint structure for extracting temporal infor-
mation from clinical narrative. J Biomed Inform 2005. 
15. Chapman WW, Dowling JN, Wagner MM. Classifi-
cation of emergency department chief complaints into 
seven syndromes:  a retrospective analysis of 527,228 
patients. Ann Emerg Med 2005;46(5):445-455. 
16. Ivanov O, Wagner MM, Chapman WW, Olszewski 
RT. Accuracy of three classifiers of acute gastrointesti-
nal syndrome for syndromic surveillance. Proc AMIA 
Symp 2002:345-9. 
17. Chang HG, Cochrane DG, Tserenpuntsag B, Allegra 
JR, Smith PF. ICD9 as a surrogate for chart review in 
the validation of a chief complaint syndromic surveil-
lance system. In: Syndromic Surveillance Conference 
Seattle, Washington; 2005. 
18. Beitel AJ, Olson KL, Reis BY, Mandl KD. Use of 
emergency department chief complaint and diagnostic 
codes for identifying respiratory illness in a pediatric 
population. Pediatr Emerg Care 2004;20(6):355-60. 
19. Chapman WW, Fiszman M, Dowling JN, Chapman 
BE, Rindflesch TC. Identifying respiratory findings in 
emergency department reports for biosurveillance using 
MetaMap. Medinfo 2004;2004:487-91. 
20. Mitchell KJ, Becich MJ, Berman JJ, Chapman WW, 
Gilbertson J, Gupta D, et al Implementation and 
evaluation of a negation tagger in a pipeline-based sys-
tem for information extraction from pathology reports. 
Medinfo 2004;2004:663-7. 
21. Chu D, Dowling JN, Chapman WW. Evaluating the 
effectiveness of four contextual features in classifying 
annotated clinical conditions in emergency department 
reports. AMIA Annu Symp Proc 2006:141-5. 
22. Meystre S, Haug PJ. Natural language processing to 
extract medical problems from electronic clinical docu-
ments: performance evaluation. J Biomed Inform 
2006;39(6):589-99. 
23. Goldin I, Chapman WW. Learning to detect nega-
tion with 'not' in medical texts. In: Proc Workshop on 
Text Analysis and Search for Bioinformatics at the 26th 
Annual International ACM SIGIR Conference (SIGIR-
2003); 2003. 
24. Chu D. Clinical feature extraction from emergency 
department reports for biosurveillance [Master's Thesis]. 
Pittsburgh: University of Pittsburgh; 2007. 
88
BioNLP 2008: Current Trends in Biomedical Natural Language Processing, pages 106?107,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Temporal Annotation of Clinical Text  Danielle L. Mowery MS, Henk Harkema PhD, Wendy W. Chapman PhD Department of Biomedical Informatics University of Pittsburgh, Pittsburgh, PA 15260, USA dlm31@pitt.edu, heh23@pitt.edu, wec6@pitt.edu    Abstract 
We developed a temporal annotation schema that provides a structured method to capture contextual and temporal features of clinical conditions found in clinical reports. In this poster we describe the elements of the annota-tion schema and provide results of an initial annotation study on a document set compris-ing six different types of clinical reports.  
1 Introduction Distinguishing between historical and recent con-ditions is important for most tasks involving re-trieval of patients or extraction of information from textual clinical records. Various approaches can be used to determine whether a condition is historical or recent. Chapman et al (2007) developed an al-gorithm called ConText that uses trigger terms like ?history? to predict whether a condition is histori-cal. Studies of ConText show that this approach is inadequate for determining whether a condition is historical, achieving recall of 67% and precision 74% on emergency department reports. Temporal modeling methods commonly reason about the temporality of an event with respect to absolute time and other temporally related events (Zhou et al, 2006; Chambers et al, 2007). Knowing the relative or absolute time the condition occurred can be useful in determining whether the condition is historical. However, we hypothesize that many clinical conditions in clinical reports are not modi-fied by explicit temporal references. To test this hypothesis and explore other types of information that may be useful in automatically distinguishing historical from recent clinical condi-tions in dictated clinical records, we developed a temporal annotation schema that accounts for ex-plicit temporal expressions, temporal trigger terms, 
and clinical reporting acts described in reports. Three annotators applied the schema to six types of reports. We measured inter-annotator agreement scores and obtained prevalence and distribution figures for the three annotation types. 
2 Methods 2.1 Dataset Our dataset is comprised of 24 clinical reports of six types dictated at the University of Pittsburgh Medical Center during 2007: discharge summaries, surgical pathology, radiology, echocardiograms, operative gastrointestinal, and emergency depart-ment reports. A physician pre-annotated the 518 clinical conditions in the reports and marked each one as recent or historical. We developed our annotation schema using one of each report type (six reports). Annotators (authors HH, DM and WC) annotated the remain-ing 18 reports as described below.  2.2 Annotation Schema  For our temporal annotation study, each pre-annotated clinical condition was annotated with three types of information: temporal expression, trigger term, and clinical reporting act. The set of temporal expressions (TEs) is taken from Zhou et al (2006) and includes categories such as DATE AND TIME for explicit TEs and KEY EVENTS for TEs relative to significant clinical events. A given clinical condition is annotated with the category of the TE it is modified by. For exam-ple, in the sentence ?The stroke occurred on 1/5/2000?, the condition ?stroke? is annotated with category DATE AND TIME. There is also a category NO TEMPORAL EXPRESSION for annotating condi-tions that are not linked to a TE. Trigger terms (TTs) are explicit signals (words and phrases) in text other than TEs that indicate 
106
whether a condition is recent or historical (Chap-man et al, 2007). If a condition co-occurs with a TT, it is annotated with TRIGGER: YES. For exam-ple, ?pneumonia? in the sentence ?Films indicate pneumonia, which is new for this patient? is anno-tated as TRIGGER: YES because ?new? is a TT.  Error analyses of our previous studies indicate that the context in which a condition is mentioned in a report is potentially useful for prediction of a condition as recent or historical. Clinical reports consist of statements that group into segments ac-cording to the clinical reporting act (CRA) they describe, such as noting a past history and consid-ering a diagnosis. CRAs are tightly correlated with report sections; however, sections are not consis-tent, and different CRAs can occur within a single section. We distinguish 16 CRAs. Each clinical condition is annotated with one CRA. For exam-ple, the condition ?smoker? in the sentence ?She was a smoker? is annotated SOCIAL HISTORY.  2.3 Analysis  To establish the level of inter-annotator agreement, we iteratively annotated groups of six reports (one of each type). After each iteration, we refined our annotation schema and guidelines. We analyzed annotations, overall and by report type, in the fol-lowing way: 1) calculate inter-annotator kappa score, 2) measure prevalence of TT and TE catego-ries, and 3) observe distribution of CRAs. 3 Results and Discussion As shown in figure 1, average inter-annotator scores as measured by Cohen's kappa for TE, TT, and CRA (.68, .82 and .72 respectively) reached acceptable levels after three iterations and are ex-pected to rise further with increased annotation experience and understanding of the guidelines. Table 1 shows the prevalence of TEs and TTs across six report types, where prevalence is defined as the frequency of TE or TT found in a given re-port. Use of TEs across report types ranged from 0% to 52% whereas TTs were found less often at 0% to 34% by report genre. Table 2 plots the cor-relation between the CRA assigned to a clinical condition and the condition's classification as re-cent or historical. We found that there is a strong correlation for the most commonly occurring clini-cal reporting acts (PH, PR, and PO). We are there-fore optimistic that CRAs can serve as an 
informative feature for a statistical recent/historical classifier. 
kappa 
0
1
1 2 3
i t e r a t i o n
TE
TT
CRA
 Figure 1. Average Cohen?s kappa agreement for 3 iterations.   DS E ED GI RAD SP O TE 48(52) 0(0) 51(20) 2(10) 1(5) 8(36) 110(21) TT 32(34) 0(0) 54(21) 1(5) 0(0) 6(27) 93(17)  Table 1. Prevalence, count (%), of TE and TT across report types, overall. DS: discharge summary, E: echocardiogram, ED: emergency department, GI: operative gastrointestinal, RAD: radiology, SP: surgical pathology and O: overall.   
0%
100%
 
P
H
P
R
H
P
I
P
O
A
l
l
 
C
C
S
H
P
F
P
M
x
D
x
P
T
x
M
d
x
R
P
R
M
D
C
D
x
C R A
Recent
Historical
 Table 2. Historical/recent distribution of CRAs. PH: Past his-tory, PR, Patient reporting, HPI: History of present illness, PO: Physician observing, All: Allergies, CC: Chief complaint, SH: Social history, FH: Family history, PF: Past Finding, PMx, Past medication, Dx: Diagnosis, PTx: Plan treatment, Mdx: Prescribing medication, RP: Referring problem, RMD: Refer to MD, CDx: Considering diagnosis.  The finding that many conditions are associated with neither a TE nor a TT and study of ConText?s limitations with such categories at the scope of the sentence suggests that additional features are nec-essary to discern a condition as recent or historical. Whereas temporality in discourse may follow a sequential chronology as narrative unfolds, refer-ences to past instances within clinical text are not easily resolved. We are optimistic that CRAs may help this issue and will focus our study to evaluate whether these three features are sufficient together. References  L. Zhou, G. B. Melton, S. Parsons, G. Hripcsak. 2006. A temporal constraint structure for extracting tempo-ral information from clinical narrative. Journal of Biomedical Informatics, 39(4):424-439 N. Chambers, S. Wang, D. Jurafsky. 2007. Classifying Temporal Relations Between Events. In: ACL-07.  W. Chapman, D. Chu, J. N. Dowling. 2007. ConText: An Algorithm for Identifying Contextual Features from Clinical Text. In: ACL-07. 
107
Proceedings of the Workshop on BioNLP, pages 10?18,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Distinguishing Historical from Current Problems in Clinical      
Reports?Which Textual Features Help?  
 
Danielle L. Mowery MS, Henk Harkema PhD, John N. Dowling MS MD,  
Jonathan L. Lustgarten PhD, Wendy W. Chapman PhD 
Department of Biomedical Informatics 
University of Pittsburgh, Pittsburgh, Pa 15260, USA 
dlm31@pitt.edu, heh23@pitt.edu, dowling@pitt.edu, jll47@pitt.edu, wec6@pitt.edu 
 
 
Abstract 
Determining whether a condition is historical 
or recent is important for accurate results in 
biomedicine. In this paper, we investigate four 
types of information found in clinical text that 
might be used to make this distinction. We 
conducted a descriptive, exploratory study us-
ing annotation on clinical reports to determine 
whether this temporal information is useful 
for classifying conditions as historical or re-
cent. Our initial results suggest that few of 
these feature values can be used to predict 
temporal classification. 
1 Introduction 
Clinical applications for decision support, biosur-
veillance and quality of care assessment depend on 
patient data described in unstructured, free-text 
reports.  For instance, patient data in emergency 
department reports contain valuable indicators for 
biosurveillance applications that may provide early 
signs and symptoms suggestive of an outbreak. 
Quality assurance departments can use free-text 
medical record data to assess adherence to quality 
care guidelines, such as determining whether an 
MI patient was given an aspirin within twenty-four 
hours of arrival. In either application, one must 
consider how to address the question of time, but 
each of the applications requires a different level of 
temporal granularity: the biosurveillance system 
needs a coarse-grained temporal model that dis-
cerns whether the signs and symptoms are histori-
cal or recent. In contrast, the quality assurance 
system needs a fine-grained temporal model to 
identify the admission event, when (or if) aspirin 
was given, and the order and duration of time be-
tween these events. One important problem in nat-
ural language processing is extracting the appro-
priate temporal granularity for a given task. 
Many solutions exist for extracting temporal in-
formation, and each is designed to address ques-
tions of various degrees of temporal granularity, 
including determining whether a condition is his-
torical or recent, identifying explicit temporal ex-
pressions, and identifying temporal relations 
among events in text. (Chapman et al, 2007; Zhou 
et al, 2008; Irvine et al, 2008;  Verhagen and Pus-
tejovsky, 2008; Bramsen et al, 2006). We pre-
viously extended the NegEx algorithm in ConText, 
a simple algorithm that relies on lexical cues to 
determine whether a condition is historical or re-
cent (Chapman et al, 2007). However, ConText 
performs with moderate recall (76%) and precision 
(75%) across different report types implying that 
trigger terms and simple temporal expressions are 
not sufficient for the task of identifying historical 
conditions.  
In order to extend work in identifying historical 
conditions, we conducted a detailed annotation 
study of potentially useful temporal classification 
features for conditions found in six genres of clini-
cal text. Our three main objectives were: (1) cha-
racterize the temporal similarity and differences 
found in different genres of clinical text; (2) de-
termine which features successfully predict wheth-
er a condition is historical, and (3) compare 
ConText to machine learning classifiers that ac-
count for this broader set of temporal features. 
2 Temporality in Clinical Text 
For several decades, researchers have been study-
ing temporality in clinical records (Zhou and 
Hripcsak, 2007). Readers use a variety of clues to 
distinguish temporality from the clinical narrative, 
and we wanted to identify features from other tem-
10
poral models that may be useful for determining 
whether a condition is historical or recent.  
There are a number of automated systems for 
extracting, representing, and reasoning time in a 
variety of text. One system that emerged from the 
AQUAINT workshops for temporal modeling of 
newspaper articles is TARSQI. TARSQI processes 
events annotated in text by anchoring and ordering 
them with respect to nearby temporal expressions 
(Verhagen and Pustejovsky, 2008). A few recent 
applications, such as TimeText and TN-TIES 
(Zhou et al, 2008; Irvine et al, 2008), identify 
medically relevant events from clinical texts and 
use temporal expressions to order the events. One 
method attempts to order temporal segments of 
clinical narratives (Bramsen et al, 2006). One key 
difference between these previous efforts and our 
work is that these systems identify all temporal 
expressions from the text and attempt to order all 
events. In contrast, our goal is to determine wheth-
er a clinical condition is historical or recent, so we 
focus only on temporal information related to the 
signs, symptoms, and diseases described in the 
text. Therefore, we ignore explicit temporal ex-
pressions that do not modify clinical conditions. If 
a condition does not have explicit temporal mod-
ifiers, we still attempt to determine the historical 
status for that condition (e.g., ?Denies cough?). In 
order to improve the ability to determine whether a 
condition is historical, we carried out this annota-
tion study to identify any useful temporal informa-
tion related to the clinical conditions in six clinical 
genres. Building on work in this area, we explored 
temporal features used in other temporal annota-
tion studies. 
TimeML is a well-known standard for complex, 
temporal annotation. TimeML supports the annota-
tion of events defined as ?situations that happen or 
occur? and temporal expressions such as dates and 
durations in order to answer temporal questions 
about these events and other entities in news text 
(Saur??, et al, 2006). One notable feature of the 
TimeML schema is its ability to capture verb tense 
such as past or present and verb aspect such as 
perfective or progressing. We annotated verb tense 
and aspect in medical text according to the Time-
ML standard. 
Within the medical domain, Zhou et al (2006) 
developed an annotation schema used to identify 
temporal expressions and clinical events. They 
measured the prevalence of explicit temporal ex-
pressions and key medical events like admission or 
transfer found in discharge summaries. We used 
the Zhou categorization scheme to explore tempor-
al expressions and clinical events across genres of 
reports. 
A few NLP systems rely on lexical cues to ad-
dress time. MediClass is a knowledge-based sys-
tem that classifies the content of an encounter 
using both free-text and encoded information from 
electronic medical records (Hazelhurst et al, 
2005). For example, MediClass classifies smoking 
cessation care delivery events by identifying the 
status of a smoker as continued, former or history 
using words like continues. ConText, an extension 
of the NegEx algorithm, temporally classifies con-
ditions as historical, recent, or hypothetical using 
lexical cues such as history, new, and if, respec-
tively (Chapman et al, 2007). Drawing from these 
applications, we used state and temporal trigger 
terms like active, unchanged, and history to cap-
ture coarse, temporal information about a condi-
tion.  
Temporal information may also be implied in 
the document structure, particularly with regards to 
the section in which the condition appears. SecTag 
marks explicit and implicit sections found 
throughout patient H&P notes (Denny et al, 2008). 
We adopted some section headers from the SecTag 
terminology to annotate sections found in reports.  
Our long-term goal is to build a robust temporal 
classifier for information found in clinical text 
where the output is classification of whether a con-
dition is historical or recent (historical categoriza-
tion). An important first step in classifying 
temporality in clinical text is to identify and cha-
racterize temporal features found in clinical re-
ports. Specifically, we aim to determine which 
expressions or features are predictive of historical 
categorization of clinical conditions in dictated 
reports. 
3 Historical Assignment and Temporal 
Features 
We conducted a descriptive, exploratory study of 
temporal features found across six genres of clini-
cal reports. We had three goals related to our task 
of determining whether a clinical condition was 
historical or recent. First, to develop a temporal 
classifier that is generalizable across report types, 
we compared temporality among different genres 
11
of clinical text. Second, to determine which fea-
tures predict whether a condition is historical or 
recent, we observed common rules generated by 
three different rule learners based on manually an-
notated temporal features we describe in the fol-
lowing section. Finally, we compared the 
performance of ConText and automated rule learn-
ers and assessed which features may improve the 
ConText algorithm.  
Next, we describe the temporal features we as-
sessed for identification of historical signs, symp-
toms, or diseases, including temporal expressions, 
lexical cues, verb tense and aspect, and sections.  
(1) Temporal Expressions: Temporal expres-
sions are time operators like dates (May 5th 2005) 
and durations (for past two days), as well as clini-
cal processes related to the encounter (discharge, 
transfer). For each clinical condition, we annotated 
whether a temporal expression modified it and, if 
so, the category of temporal expression. We used 
six major categories from Zhou et al (2006) in-
cluding: Date and Time, Relative Date and Time, 
Durations, Key Events, Fuzzy Time, and No Tem-
poral Expression. These categories also have 
types. For instance, Relative Date and Time has a 
type Yesterday, Today or Tomorrow.  For the con-
dition in the sentence ?The patient had a stroke in 
May 2006?, the temporal expression category is 
Date and Time with type Date. Statements without 
a temporal expression were annotated No Tempor-
al Expression with type N/A. 
(2) Tense and Aspect: Tense and aspect define 
how a verb is situated and related to a particular 
time. We used TimeML Specification 1.2.1 for 
standardization of tense and aspect where exam-
ples of tense include Past or Present and aspect 
may be Perfective, Progressive, Both or None as 
found in Saur??, et al (2006). We annotated the 
verb that scoped a condition and annotated its tense 
and aspect. The primary verb may be a predicate 
adjective integral to interpretation of the condition 
(Left ventricle is enlarged), a verb preceding the 
condition (has hypertension), or a verb following a 
condition (Chest pain has resolved). In ?her chest 
pain has resolved,? we would mark ?has resolved? 
with tense Present and aspect Perfective. State-
ments without verbs (e.g., No murmurs) would be 
annotated Null for both.  
(3) Trigger Terms: We annotated lexical cues 
that provide temporal information about a condi-
tion. For example, in the statement, ?Patient has 
past history of diabetes,? we would annotate ?his-
tory? as Trigger Term: Yes and would note the ex-
act trigger term. 
     (4) Sections: Sections are ?clinically meaning-
ful segments which act independently of the 
unique narrative? for a patient (Denny et al 2008). 
Examples of report sections include Review of Sys-
tems (Emergency Department), Findings (Opera-
tive Gastrointestinal and Radiology) and 
Discharge Diagnosis (Emergency Department and 
Discharge Summary).  
We extended Denny?s section schema with ex-
plicit, report-specific section headers not included 
in the original terminology. Similar to Denny, we 
assigned implied sections in which there was an 
obvious change of topic and paragraph marker. For 
instance, if the sentence ?the patient is allergic to 
penicillin? followed the Social History section, we 
annotated the section as Allergies, even if there 
was not a section heading for allergies. 
4 Methods 
4.1 Dataset Generation 
We randomly selected seven reports from each of 
six genres of clinical reports dictated at the Univer-
sity of Pittsburgh Medical Center during 2007 
These included Discharge Summaries, Surgical 
Pathology, Radiology, Echocardiograms, Opera-
tive Gastrointestinal, and Emergency Department 
reports. The dataset ultimately contained 42 clini-
cal reports and 854 conditions. Figure 1 show our 
annotation process, which was completed in 
GATE, an open-source framework for building 
NLP systems (http://gate.ac.uk/). A physician 
board-certified in internal medicine and infectious 
diseases annotated all clinical conditions in the set 
and annotated each condition as either historical or 
recent. He used a general guideline for annotating 
a condition as historical if the condition began 
more than 14 days before the current encounter and 
as recent if it began or occurred within 14 days or 
during the current visit. However, the physician 
was not bound to this definition and ultimately 
used his own judgment to determine whether a 
condition was historical. 
Provided with pre-annotated clinical conditions 
and blinded to the historical category, three of the 
authors annotated the features iteratively in groups 
of six (one of each report type) using guidelines we 
12
developed for the first two types of temporal fea-
tures (temporal expressions and trigger terms.) 
Between iterations, we resolved disagreements 
through discussion and updated our guidelines. 
Cohen?s kappa for temporal expressions and trig-
ger terms by the final iteration was at 0.66 and 0.69 
respectively. Finally, one author annotated sec-
tions, verb tense, and aspect.  Cases in which as-
signing the appropriate feature value was unclear 
were resolved after consultation with one other 
author-annotator.  
4.2 Data Analysis 
 
We represented each condition as a vector with  
temporal features and their manually-assigned val-
ues as input features for predicting the binary out-
come value of historical or recent. We trained three 
rule learning algorithms to classify each condition 
as historical or recent: J48 Decision Tree, Ripper, 
and Rule Learner (RL) (Witten and Frank, 2005; 
Clearwater and Provost, 1990). Rule learners per-
form well at classification tasks and provide expli-
cit rules that can be viewed, understood, and 
potentially implemented in existing rule-based ap-
plications. We used Weka 3.5.8, an openly-
available machine learning application for predic-
tion modeling, to implement the Decision Tree 
(J48) and Ripper (JRip) algorithms, and we applied 
an in house version of RL retrieved from 
www.dbmi.pitt.edu\probe. For all rule learners, we 
used the default settings and ran ten-fold cross-
validation. The J48 algorithm produces mutually 
exclusive rules for predicting the outcome value. 
Thus, two rules cannot cover or apply to any one 
case. In contrast, both JRip and RL generate non-
mutually-exclusive rules for predicting the out-
come value. Although J48 and JRip are sensitive to 
bias in outcome values, RL accounts for skewed 
distribution of the data.  
We also applied ConText to the test cases to 
classify them as historical or recent. ConText looks 
for trigger terms and a limited set of temporal ex-
pressions within a sentence. Clinical conditions 
within the scope of the trigger terms are assigned 
the value indicated by the trigger terms (e.g., his-
torical for the term history). Scope extends from 
the trigger term to the end of the sentence or until 
the presence of a termination term, such as pre-
senting. For instance, in the sentence ?History of 
CHF, presenting with chest pain,? CHF would be 
annotated as historical.  
5 Evaluation 
To characterize the different reports types, we es-
tablished the overall prevalence and proportion of 
conditions annotated as historical for each clinical 
report genre.  We assessed the prevalence of each 
feature (temporal expressions, trigger terms, tense 
and aspect, and sections) by report genre to deter-
mine the level of similarity or difference between 
genres. To determine which features values are 
predictive of whether a condition is historical or 
recent, we observed common rules found by more 
than one rule learning algorithm. Amongst com-
mon rules, we identified new rules that could im-
prove the ConText algorithm.  
We also measured predictive performance with 
95% confidence intervals of the rule learners and 
ConText by calculating overall accuracy, as well as 
recall and precision for historical classifications 
and recall and precision for recent classifications.  
Table 1 describes equations for the evaluation me-
trics. 
 
Table 1. Description of evaluation metrics. RLP = rule 
learner prediction. RS = Reference Standard 
 
 
Figure 1. Annotation process for dataset and objectives 
for evaluation. 
13
Recall:                 number of TP              
(number of TP + number of FN) 
 
Precision:           number of TP              
(number of TP + number of FP) 
 
Accuracy:   number of instances correctly classified 
                      total number of possible instances  
6 Results 
Overall, we found 854 conditions of interest across 
all six report genre. Table 2 illustrates the preva-
lence of conditions across report genres. Emergen-
cy Department reports contained the highest 
concentration of conditions. Across report genres, 
87% of conditions were recent (741 conditions). 
All conditions were recent in Echocardiograms, in 
contrast to Surgical Pathology reports in which 
68% were recent.  
 
Table 2. Prevalence and count of conditions by temporal 
category and report genre. DS = Discharge Summary, 
Echo = Echocardiogram, ED = Emergency Department, 
GI = Operative Gastrointestinal, RAD = Radiology and 
SP = Surgical Pathology. (%) = percent; Ct = count.  
 
6.1 Prevalence of Temporal Features 
Table 3 shows that most conditions were not mod-
ified by a temporal expression or a trigger term. 
Conditions were modified by a temporal expres-
sion in Discharge Summaries more often than in 
other report genres. Similarly, Surgical Pathology 
had the highest prevalence of conditions modified 
by a trigger term. Operative Gastrointestinal and 
Radiology reports showed the lowest prevalence of 
both temporal expressions and trigger terms. Nei-
ther temporal expressions nor trigger terms oc-
curred in Echocardiograms. Overall, the 
prevalence of conditions scoped by a verb varied 
across report types ranging from 46% (Surgical 
Pathology) to 81% (Echocardiogram). 
Table 3. Prevalence of conditions modified by temporal 
features. All conditions were assigned a section and are 
thereby excluded. TE = temporal expression; TT = trig-
ger term; V = scoped by verb.  
 
6.2 Common Rules 
Rule learners generated a variety of rules. The J48 
Decision Tree algorithm learned 27 rules, six for 
predicting conditions as historical and the remain-
ing for classifying the condition as recent. The 
rules predominantly incorporated the trigger term 
and verb tense and aspect feature values. JRip 
learned nine rules, eight for classifying the histori-
cal temporal category and one ?otherwise? rule for 
the majority class. The JRip rules most heavily 
incorporated the section feature. The RL algorithm 
found 79 rules, 18 of which predict the historical 
category. Figure 2 illustrates historical rules 
learned by each rule learner. JRip and RL pre-
dicted the following sections alone can be used to 
predict a condition as historical: Past Medical His-
tory, Allergies and Social History. Both J48 and 
RL learned that trigger terms like previous, known 
and history predict historical. There was only one 
common, simple rule for the historical category 
found amongst all three learners: the trigger term 
no change predicts the historical category. All al-
gorithms learned a number of rules that include 
two features values; however, none of the com-
pound rules were common amongst all three algo-
rithms.    
 
Figure 2. Historical rules learned by each rule learner 
algorithm. Black dots represent simple rules whereas 
triangles represent compound rules. Common rules 
shared by each algorithm occur in the overlapping areas 
of each circle. 
14
6.3 Predictive Performance 
Table 4 shows predictive performance for each 
rule learner and for ConText. The RL algorithm 
outperformed all other algorithms in almost all 
evaluation measures. The RL scores were com-
puted based on classifying the 42 cases (eight his-
torical) for which the algorithm did not make a 
prediction as recent. ConText and J48, which ex-
clusively relied on trigger terms, had lower recall 
for the historical category.  
All of the rule learners out-performed ConText. 
JRip and RL showed substantially higher recall for 
assigning the historical category, which is the most 
important measure in a comparison with ConText, 
because ConText assigns the default value of re-
cent unless there is textual evidence to indicate a 
historical classification. Although the majority 
class baseline shows high accuracy due to high 
prevalence of the recent category, all other classifi-
ers show even higher accuracy, achieving fairly 
high recall and precision for the historical cases 
while maintaining high performance on the recent 
category. 
 
Table 4. Performance results with 95% confidence in-
tervals for three rule learners trained on manually anno-
tated features and ConText, which uses automatically 
generated features. Bolded values do not have overlap-
ping confidence intervals with ConText. MCB = Ma-
jority Class Baseline (recent class)   
 
7 Discussion 
Our study provides a descriptive investigation of 
temporal features found in clinical text. Our first 
objective was to characterize the temporal similari-
ties and differences amongst report types. We 
found that the majority of conditions in all report 
genres were recent conditions, indicating that a 
majority class classifier would produce an accura-
cy of about 87% over our data set.  According to 
the distributions of temporal category by report 
genre (Table 2), Echocardiograms exclusively de-
scribe recent conditions. Operative Gastrointestinal 
and Radiology reports contain similar proportions 
of historical conditions (9% and 6%). Echocardio-
grams appear to be most similar to Radiology re-
ports and Operative Gastrointestinal reports, which 
may be supported by the fact that these reports are 
used to document findings from tests conducted 
during the current visit. Emergency Department 
reports and Discharge Summaries contain similar 
proportions of historical conditions (17% and 19% 
respectively), which might be explained by the fact 
that both reports describe a patient?s temporal pro-
gression throughout the stay in the Emergency De-
partment or the hospital.  
Surgical Pathology reports may be the most 
temporally distinct report in our study, showing the 
highest proportion of historical conditions. This 
may seem counter-intuitive given that Surgical 
Pathology reports also facilitate the reporting of 
findings described from a recent physical speci-
men. However, we had a small sample size (28 
conditions in seven reports), and most of the his-
torical conditions were described in a single ad-
dendum report. Removing this report decreased the 
prevalence of historical conditions to 23% (3/13).  
Discharge Summaries and Emergency Depart-
ment reports displayed more variety in the ob-
served types of temporal expressions (9 to 14 
subtypes) and trigger terms (10 to 12 terms) than 
other report genres. This is not surprising consider-
ing the range of events described in these reports. 
Other reports tend to have between zero and three 
subtypes of temporal expressions and zero and 
seven different trigger terms. In all report types, 
temporal expressions were mainly subtype past, 
and the most frequent trigger term was history. 
Our second objective was to identify which fea-
tures predict whether a condition is historical or 
recent. Due to high prevalence of the recent cate-
gory, we were especially interested in discovering 
temporal features that predict whether a condition 
is historical. With one exception (date greater than 
four weeks prior to the current visit), temporal ex-
pression features always occurred in compound 
rules in which the temporal expression value had to 
co-occur with another feature value. For instance, 
any temporal expression in the category key event 
had to also occur in the secondary diagnosis sec-
tion to classify the condition as historical. For ex-
15
ample, in ?SECONDARY DIAGNOSIS: Status 
post Coronary artery bypass graft with complica-
tion of mediastinitis? the key event is the coronary 
artery bypass graft, the section is secondary diag-
nosis, and the correct classification is historical.  
Similarly, verb tense and aspect were only use-
ful in conjunction with other feature values. One 
rule predicted a condition as historical if the condi-
tion was modified by the trigger term history and 
fell within the scope of a present tense verb with 
no aspect. An example of this is ?The patient is a 
50 year old male with history of hypertension.? 
Intuitively, one would think that a past tense verb 
would always predict historical; however, we 
found the presence of a past tense verb with no 
aspect was a feature only when the condition was 
in the Patient History section.  Sometimes the ab-
sence of a verb in conjunction with another feature 
value predicted a condition as historical. For ex-
ample, in the sentences ?PAST MEDICAL 
HISTORY: History of COPD. Also diabetes?? 
also functioned as a trigger term that extended the 
scope of a previous trigger term, history, in the 
antecedent sentence.  
A few historical trigger terms were discovered 
as simple rules by the rule learners: no change, 
previous, known, status post, and history. A few 
rules incorporated both a trigger term and a partic-
ular section header value. One rule predicted his-
torical if the trigger term was status post and the 
condition occurred in the History of Present Illness 
section. This rule would classify the condition 
CABG as historical in ?HISTORY OF PRESENT 
ILLNESS: The patient is...status post CABG.? 
One important detail to note is that a number of the 
temporal expressions categorized as Fuzzy Time 
also act as trigger terms, such as history and status 
post?both of which were learned by J48. A histor-
ical trigger term did not always predict the catego-
ry historical. In the sentence ?No focal sensory or 
motor deficits on history,? history may suggest that 
the condition was not previously documented, but 
was interpreted as not presently identified during 
the current physical exam.   
Finally, sections appeared in the majority of 
JRip and RL historical rules: 4/8 simple rules and 
13/18 compound rules. A few sections were con-
sistently classified as historical: Past Medical His-
tory, Allergies, and Social History.  One important 
point to address is that these sections were manual-
ly annotated.  
Our results revealed a few unexpected observa-
tions. We found at least two trigger terms indicated 
in the J48 rules, also and status post, which did not 
have the same predictive ability across report ge-
nres.  For instance, in the statement ?TRANSFER 
DIAGNOSIS: status post coiling for left posterior 
internal carotid artery aneurysm,? status post indi-
cates the reason for the transfer as an inpatient 
from the Emergency Department and the condition 
is recent. In contrast, status post in a Surgical Pa-
thology report was interpreted to mean historical 
(e.g., PATIENT HISTORY: Status post double 
lung transplant for COPD.) In these instances, 
document knowledge of the meaning of the section 
may be useful to resolve these cases.  
One other unexpected finding was that the trig-
ger term chronic was predictive of recent rather 
than historical. This may seem counterintuitive; 
however, in the statement ?We are treating this as 
chronic musculoskeletal pain with oxycodone?, the 
condition is being referenced in the context of the 
reason for the current visit. Contextual information 
surrounding the condition, in this case treating or 
administering medication for the condition, may 
help discriminate several of these cases.  
Our third objective was to assess ConText in re-
lation to the rules learned from manually annotated 
temporal features. J48 and ConText emphasized 
the use of trigger terms as predictors of whether a 
condition was historical or recent and performed 
with roughly the same overall accuracy. JRip and 
RL learned rules that incorporated other feature 
values including sections and temporal expres-
sions, resulting in a 12% increase in historical re-
call over ConText and a 31% increase in historical 
recall over J48. 
Many of the rules we learned can be easily ex-
tracted and incorporated into ConText (e.g., trigger 
terms previous and no change). The ConText algo-
rithm largely relies on the use of trigger terms like 
history and one section header, Past Medical His-
tory. By incorporating additional section headers 
that may strongly predict historical, ConText could 
potentially predict a condition as historical when a 
trigger term is absent and the header title is the 
only predictor as in the case of ?ALLERGIES: 
peanut allergy?. Although these sections header 
may only be applied to Emergency Department 
and Discharge Summaries, trigger terms and tem-
poral expressions may be generalizable across ge-
nre of reports.  Some rules do not lend themselves 
16
to ConText?s trigger-term-based approach, particu-
larly those that require sophisticated representation 
and reasoning. For example, ConText only reasons 
some simple durations like several day history. 
ConText cannot compute dates from the current 
visit to reason that a condition occurred in the past 
(e.g., stroke in March 2000).  The algorithm per-
formance would gain from such a function; how-
ever, such a task would greatly add to its 
complexity.   
8 Limitations 
The small sample size of reports and few condi-
tions found in three report genres (Operative Ga-
strointestinal, Radiology, and Surgical Pathology) 
is a limitation in this study. Also, annotation of 
conditions, temporal category, sections, verb tense 
and aspect were conducted by a single author, 
which may have introduced bias to the study. Most 
studies on temporality in text focus on the temporal 
features themselves. For instance, the prevalence 
of temporal expressions reported by Zhou et al 
(2006) include all temporal expressions found 
throughout a discharge summary, whereas we an-
notated only those expressions that modified the 
condition. This difference makes comparing our 
results to other published literature challenging.  
9 Future Work  
Although our results are preliminary, we be-
lieve our study has provided a few new insights 
that may help improve the state of the art for his-
torical categorization of a condition. The next step 
to building on this work includes automatically 
extracting the predictive features identified by the 
rule learners. Some features may be easier to ex-
tract than others. Since sections appear to be strong 
indicators for historical categorization we may start 
by implementing the SecTag tagger. Often a sec-
tion header does not exist between text describing 
the past medical history and a description of the 
current problem, so relying merely on the section 
heading is not sufficient. The SecTag tagger identi-
fies both implicit and explicit sections and may 
prove useful for this task. To our knowledge, Sec-
Tag was only tested on Emergency Department 
reports, so adapting it to other report genres will be 
necessary. Both JRip and RL produced high per-
formance, suggesting a broader set of features may 
improve historical classification; however, because 
these features do not result in perfect performance, 
there are surely other features necessary for im-
proving historical classification. For instance, hu-
mans use medical knowledge about conditions that 
are inherently chronic or usually experienced over 
the course of a patient?s life (i.e., HIV, social ha-
bits like smoking, allergies etc). Moreover, physi-
cians are able to integrate knowledge about chronic 
conditions with understanding of the patient?s rea-
son for visit to determine whether a chronic condi-
tion is also a recent problem. An application that 
imitated experts would need to integrate this type 
of information. We also need to explore adding 
features captured at the discourse level, such as 
nominal and temporal coreference. We have begun 
work in these areas and are optimistic that they 
will improve historical categorization.  
10 Conclusion 
Although most conditions in six clinical report ge-
nres are recent problems, identifying those that are 
historical is important in understanding a patient?s 
clinical state. A simple algorithm that relies on lex-
ical cues and simple temporal expressions can 
classify the majority of historical conditions, but 
our results indicate that the ability to reason with 
temporal expressions, to recognize tense and as-
pect, and to place conditions in the context of their 
report sections will improve historical classifica-
tion. We will continue to explore other features to 
predict historical categorization. 
 
Acknowledgments 
 
This work was funded by NLM grant 1 
R01LM009427-01, ?NLP Foundational Studies 
and Ontologies for Syndromic Surveillance from 
ED Reports?.  
References 
 
Philip Bramsen, Pawan Deshpande, Yoong Keok Lee, 
and Regina Barzilay. 2006. Finding Temporal Order 
in Discharge Summaries. AMIA Annu Symp Proc. 
2006; 81?85 
Wendy W Chapman, David Chu, and John N. Dowling. 
2007. ConText: An Algorithm for Identifying Contex-
tual Features from Clinical Text. Association for 
Computational Linguistics, Prague, Czech Republic 
17
Scott H. Clearwater and Foster J. Provost. 1990. RL4: A 
Tool for Knowledge-Based Induction. Tools for Ar-
tificial Intelligence, 1990. Proc of the 2nd Intern 
IEEE Conf: 24-30. 
Joshua C. Denny, Randolph A. Miller, Kevin B. John-
son, and Anderson Spickard III. 2008. Development 
and Evaluation of a Clinical Note Section Header 
Terminology. SNOMED. AMIA 2008 Symp. Pro-
ceedings: 156-160. 
Brian Hazlehurst, H. Robert Frost, Dean F. Sittig, and 
Victor J. Stevens. 2005. MediClass: A system for de-
tecting and classifying encounter-based clinical 
events in any electronic medical record. J Am Med 
Inform Assoc 12(5): 517-29 
Ann K. Irvine, Stephanie W. Haas, and Tessa Sullivan. 
2008. TN-TIES: A System for Extracting Temporal 
Information from Emergency Department Triage 
Notes. AMIA 2008 Symp Proc: 328-332. 
Roser Saur??, Jessica Littman, Bob Knippen, Robert 
Gaizauskas, Andrea Setzer, and James Pustejovsky. 
2006. TimeML Annotation Guidelines Version 1.2.1. 
at: 
http://www.timeml.org/site/publications/timeMLdocs
/annguide_1.2.1.pdf 
Marc Verhagen and James Pustejovsky. 2008. Temporal 
Processing with TARSQI Toolkit. Coling 2008: Com-
panion volume ? Posters and Demonstrations, Man-
chester, 189?192 
Ian H. Witten and Eibe Frank. 2005. Data Mining: 
Practical machine learning tools and techniques, 2nd 
Edition, Morgan Kaufmann, San Francisco, 2005. 
Li Zhou, Genevieve B. Melton, Simon Parsons and 
George Hripcsak. 2006. A temporal constraint struc-
ture for extracting temporal information from clini-
cal narrative. J Biomed Inform 39(4): 424-439. 
Li Zhou and George Hripcsak. 2007. Temporal reason-
ing with medical data--a review with emphasis on 
medical natural language processing. J Biomed In-
form Apr; 40(2):183-202. 
Li Zhou, Simon Parson, and George Hripcsak. 2008. 
The Evaluation of a Temporal Reasoning System in 
Processing Discharge Summaries. J Am Med Inform 
Assoc 15(1): 99?106.  
 
18
Proceedings of the Workshop on BioNLP, pages 19?27,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
ONYX: A System for the Semantic Analysis of Clinical Text  Lee M. Christensen, Henk Harkema, Peter J. Haug,  Jeannie Y. Irwin, Wendy W. Chapman Department of Biomedical Informatics University of Pittsburgh University of Utah Pittsburgh, PA 15214, USA Salt Lake City, Utah, 84143, USA lmc61 heh23 rey3 wec6 @pitt.edu Peter.Haug@intermountainmail.org     Abstract 
This paper introduces ONYX, a sentence-level text analyzer that implements a number of innovative ideas in syntactic and semantic analysis. ONYX is being developed as part of a project that seeks to translate spoken dental examinations directly into chartable findings. ONYX integrates syntax and semantics to a high degree. It interprets sentences using a combination of probabilistic classifiers, graphical unification, and semantically anno-tated grammar rules. In this preliminary evaluation, ONYX shows inter-annotator agreement scores with humans of 86% for as-signing semantic types to relevant words, 80% for inferring relevant concepts from words, and 76% for identifying relations between concepts. 
1 Introduction This paper describes ONYX, a sentence-level medical language analyzer currently under devel-opment at the University of Pittsburgh. Since ONYX contains a number of innovative ideas at an early stage of development, the objective of this paper is to paint a broad picture of ONYX and to present preliminary evaluation results rather than analyzing any single aspect in detail.  ONYX is being developed as part of a project aimed at extracting information from spoken dental examinations. Currently, dental findings must be charted after an exam is completed or may be charted by an assistant who acts as a transcription-ist during the exam. Our goal is to design a system capable of automatically extracting chartable find-
ings directly from spoken exams, potentially also supporting automated decision support and quality control. We are also developing tools to enable the system to be ported to other clinical domains and settings.   Extracting information from unedited speech tran-scriptions presents a number of challenges. Sen-tences may be fragmented or telegraphic, and much of the speech may be irrelevant for our pur-poses. The following example illustrates some of these difficulties:  "Okay. Okay. Open. Okay. No. 1 is missing. Two oc-clusal distal amalgam. Actually, make that occlusal. Also, one palatal amalgam. Can you close just slightly? And perfect. Okay, now open again."  The relevant findings in this example are that tooth number one is missing and tooth number two has amalgam fillings on the occlusal and palatal sur-faces. Our ultimate challenge is to create a system that can recognize relevant sentences and perform competently in the face of the inherent ambiguity and noise commonly found in conversational speech. ONYX does not yet address all of these challenges, although we have clear directions we are pursuing as described in the Future Work sec-tion of this paper. Our goal in this paper is to de-scribe the current state of ONYX and the innovations we feel will enable it to be adapted to complex NLP tasks in the future. 2 Overview of ONYX  ONYX is the middle component of a pipelined architecture as illustrated in figure 1. The entry point to this architecture is a speech-to-text ana-lyzer, which takes input from a microphone worn 
19
by the dentist and produces a transcription that ONYX analyzes for semantic content. ONYX's output is then passed to a discourse analyzer that applies dental knowledge to assemble ONYX's sentence-level semantic representations into chartable exam findings.  
 Figure 1. Speech-to-chart pipeline.  ONYX looks for dental conditions such as caries, fractures and translucencies; restorations such as fillings and crowns; tooth locations; and modifiers such as tooth part, tooth surface, and condition ex-tent. It produces templates of words and concepts. Table 1 shows a summary of four templates (Den-tal Condition, Tooth Location, Surface and State) representing the meaning of "eight mesio might have a slight translucency."   
 ONYX?s interpretations are represented as binary predicates that take the templates as arguments (for convenience, only the summary concepts from the templates are shown): 
ConditionAt(*translucency, *numberEight) & LocationHasSurface(*numberEight, *mesial) &  StateOf(*translucency, *possible)   ONYX builds on ideas from MPLUS (Christensen et al 2002), which was used primarily to interpret radiology reports. MPLUS uses Bayesian networks (BNs) to produce filled templates. Through a train-ing process, words from the corpus of training documents are manually associated with states of terminal nodes in a BN, and concepts are associ-ated with states of nonterminal nodes. When MPLUS interprets a sentence, it instantiates the BNs with words from the sentence and infers the most probable concepts consistent with those words. It then generates templates filled with those words and concepts.   BNs have proven useful in semantic analysis (e.g. Ranum 1989, Koehler 1998, Christensen 2002): their performance degrades gracefully in the face of various types of lexical and syntactic noise. The main disadvantage with using BNs is their inherent computational complexity. ONYX employs a se-mantics-intensive form of parsing, interpreting each phrase as it is constructed rather than waiting until the syntactic analysis is completed to do the interpretation. For this reason we have developed an experimental probabilistic classifier for ONYX called a Concept Model (CM). CMs support a tree-structured representation of related words and con-cepts (figure 5), structurally similar to the BNs used by MPLUS, but using a more efficient model of computation. In essence CMs are trees of Na?ve Bayes classifiers, although they contain enhance-ments, not described in this study, which in general make them more accurate than strict Na?ve Bayes. Each node together with its children constitutes a single classifier. When a CM is applied to words in a sentence, word-level CM states are assigned a probability based on training data. Probabilities are propagated upwards through the CM, calculating probabilities for all concepts that depend directly or indirectly on the words of the sentence.  3 ONYX Syntactic Analyzer For this project we desired a parser that was fast, flexible and robust. We designed a variation on a bottom-up chart parser (Kay, 1980) and hand-crafted an initial set of 52 context-free grammar 
Dental Condition   Condition Concept *translucency   Condition Term "translucency"   Severity Concept *superficial   Severity Term "slight" Tooth Location   Location Concept *numberEight   Tooth Number "eight" Surface   Surface Concept *mesial   Front/Back Term "mesio" State   State Concept *possible   State Term "might" Table 1: ONYX templates for "eight mesio might have a slight translucency." Terms with an * are in-ferred concepts. 
20
rules. Chart parsers based on Kay?s algorithm maintain an agenda of ?edges,? which correspond to partially or completely instantiated grammar rules. In the original algorithm, for each new phrase added to the chart an edge is created for each rule that can begin with that phrase. In addi-tion, each existing edge that abuts and can be ex-tended with that phrase is duplicated with a pointer to the new phrase. When an edge has no more un-matched components, it is regarded as a new phrase that can begin or extend other edges. Since edges are used to anticipate all possible continua-tions of phrases vis-?-vis the grammar, the number of edges grows quickly relative to the number of words in the sentence. Charniak et al (1998) noted that exhaustively parsing maximum-40-word sen-tences from the Penn II treebank requires an aver-age of 1.2 million edges per sentence.   ONYX?s parse algorithm replaces edges with bi-nary links. We briefly describe this new algorithm. A set of binary link templates is defined for each grammar rule. For instance, the rule S->NP AUX VP (labeled S1) would produce the templates [s1:np,aux] and [s1:aux,vp]. When a phrase is added to the chart, binary links for all applicable rules are added from that phrase to juxtaposed phrases to the left and right on the chart. When a right or left-terminating link is added (all links for rules with two or three components are right or left terminating), a quick search is done in the other direction for links belonging to the same rule. Each complete set of links defines a new phrase of the target type, as shown in figure 2.  
 Figure 2. Binary links for the rule S1:S->NP AUX VP used to generate new phrases of type S1 from juxta-posed NP, AUX, and VP phrases on the chart.  Although we have not analyzed the time and space complexity of this algorithm, it has proven to be 
more efficient than the edge-based parser used by MPLUS. Time and space complexity for chart parsers is calculated based on the number of edges produced, which has been shown to be O(n3), with n words in a sentence. Since binary links, unlike edges, are only used to record grammatical rela-tions between juxtaposed phrases on the chart (rather than anticipating possible continuations), are not duplicated, and can participate in the crea-tion of multiple new phrases, the number of binary links grows more slowly than the number of edges.  On the other hand, the need to search for com-pleted link sets increases processing time. We plan to formally analyze the time and space require-ments of this algorithm in a future study. 4  ONYX Semantic Analyzer In ONYX syntax and semantics are highly inte-grated. Rather than waiting for a completed parse tree to begin the interpretation process, ONYX semantically interprets each phrase as it is created and before it is placed on the chart. Each phrase is assigned a ?goodness? score based in part on the goodness of its semantic interpretation, and this score is used in determining the order in which phrases are expanded, resulting in a semantically guided best-first search.  To represent semantic relations between templates, ONYX uses a custom-built first-order predicate language with a syntax based roughly on the Knowledge Interchange Format (Genesareth & Fikes, 1992). ONYX interpretations are conjuncts of binary predicates formulated in this language, with templates as arguments. This language is for internal use only; ONYX will use standard lan-guage protocols for communicating with external systems. We decided to implement our own lan-guage rather than using an existing implementation in order to have access to the underlying data structures, which we use in three ways not tradi-tionally applied to symbolic languages: 1- We have extended our language to include Java objects as constants and Java methods as functions and rela-tions. In particular, CM templates are treated as constants in the language, and CMs are semanti-cally typed functions that map words to templates. 2- As described next, ONYX's default mode of semantic interpretation is based on a form of graph unification. Binary predicates are treated as unifi-
21
able links in a graph as shown in figure 3. 3- ONYX uses the predicate structure of an interpre-tation to pass information between CMs. For in-stance, if an interpretation contains the relation ConditionAt(Condition, Location), ONYX inserts the summary concept from the Location CM into the Condition CM. This allows the Condition CM to factor tooth location into its determination of the most probable Condition concept.  Figure 3 illustrates ONYX?s unification-based in-terpretation process. ONYX relies on a semantic network that defines types and relations in the den-tal domain (figure 4). As dental concepts are brought together in a phrase, links connecting those concepts are extracted from the semantic network and formulated into binary predicates in an interpretation. As phrases are joined together in larger phrases, their relations and templates are merged, resulting in an interpretation tree denoting a dental object (e.g. dental condition, tooth loca-tion, tooth surface) with possibly multiple levels of modifiers. For instance, the interpretation for "eight mesio might have a slight translucency" can be generated from the partial interpretations of the phrases "eight mesio", "might" and "slight translu-cency" as shown in figure 3.  
 Figure 3. Interpreting ?eight mesio might have a slight trans-lucency? using graph unification.   There are two primary justifications for using uni-fication in this way. First, conjoined phrases, par-ticularly noun phrases, often contain unifiable partial descriptions of a single object. Second, if concepts appear together in a phrase, there is a good chance that relations connecting those con-cepts in the semantic network are captured, explic-itly or implicitly, in the meaning of the phrase.  
The dental semantic network is shown in figure 4. Terminal (white) nodes define concrete semantic types associated with dental CMs. For instance, the DentalCondition type is associated with the con-cept model shown in figure 5. 
 Figure 4. Semantic network for dental exams. Nonterminal (gray) nodes represent abstract types with no associated CMs. A concrete type may have more than one abstract parent type. For instance, a Restoration, such as a crown, is both a Condition and a Location. As such, it can exist at a tooth lo-cation, e.g., "the crown on tooth 5," and it can be the location of condition, e.g., "the crack on the crown on tooth 5." Since a concrete type can have multiple parent types, ONYX often produces mul-tiple alternative interpretations over words of a sentence. For instance, ONYX may produce two interpretations for "mesial amalgam"?one refer-ring to the mesial surface of an amalgam filling, and one referring to an amalgam filling on the me-sial surface of some unspecified tooth. ONYX uses probabilities derived from training cases to prefer the latter interpretation, which is the more likely of the two.  
 Figure 5. Dental Condition Concept Model. Each concept model has a tree structure as illus-trated in figure 5, which shows the structure of the Dental Condition CM. Nonterminal nodes repre-
22
sent concepts, and terminal nodes represent words, with the exception of stub nodes. The value of a stub node is the summary concept (i.e., root node) from the CM of the same name.   One problem with ONYX?s graph-based model of interpretation is that the semantic network does not capture all relations that might be expressed in a dental exam. The network was deliberately kept simple by including mostly relations that are cate-gorically true (e.g., all teeth have surfaces) or that are frequently talked about (e.g., restorations are frequently mentioned as being locations of other conditions). This restriction helps keep the unifica-tion process tractable and minimizes ambiguity, but interpretations may miss important points. For instance, the ONYX interpretation of "15 occlusal amalgam" is ConditionAt(*filling, *toothFifteen) & Loca-tionHasSurface(*toothFifteen, *occlusal) which can be paraphrased as "a filling at tooth 15 and tooth 15 has an occlusal surface". This interpretation misses the important fact that the filling is on the occlusal surface of tooth 15, which we would normally in-fer from the fact that ?occlusal? adjectivally modi-fies ?amalgam.? Another limitation is that although the semantic network as it stands can describe sin-gle objects with their modifiers, it cannot be used to build up complex descriptions involving multi-ple objects of the same type.  To address these limitations we have added a sec-ond, more specialized mode of interpretation that is contingent on lexical and syntactic information from the parse and that can introduce into an inter-pretation predicates that do not exist in the seman-tic network. This mode of interpretation uses semantic types and patterns attached to grammar rules. As an example, the rule NP -> AP NP can be semantically annotated thus:     NP<Restoration> -> AP<Surface> NP<Restoration>  => OnSurface(Restoration, Surface)  This rule captures the idea that if a Surface-type adjectival phrase modifies a Restoration-type noun phrase, the restoration exists on that surface. Ap-plied to ?occlusal amalgam? this rule would pro-duce an interpretation OnSurface(*filling, *occlusal), which is the relation missing from the previous example. Semantically annotated grammar rules 
can also connect objects of the same semantic type. For instance, we might define a rule      NP<Condition> -> NP<Condition1> "caused by"     NP<Condition2>      => CausedBy(condition1, condition2)  This rule can match phrases such as "leakage caused by a crack along the lingual surface", and link the two conditions (leakage and crack) with a CausedBy relation. This mechanism enables ONYX to construct complex descriptions with multiple objects.  We have added a mechanism to the ONYX train-ing tool that allows semantically annotated gram-mar rules to be generated semi-automatically during training. A human annotator with sufficient linguistic background can view the parse trees generated by ONYX for corpus sentences, repair those parse trees and/or add new semantic relations if necessary, then apply a function that creates cop-ies of the rules embodied in those trees with se-mantic types and predicates attached. 5  Integrating Syntax and Semantics Although most NLP systems apply semantic analy-sis to completed parse trees, in humans the two processes are more integrated. Syntactic expecta-tions are greatly influenced by word meanings, as illustrated by ?garden path? sentences such as ?The man whistling tunes pianos.? In ONYX, syntax and semantics are highly interleaved. This is ac-complished in several ways:  1- ONYX?s parse algorithm permits words to be processed in any order, rather than strictly left-to-right, since binary grammar links can be added to the phrase chart in any order. This allows ONYX to be instructed to focus on semantically interest-ing words first, which can be used, among other things, to gather useful information from ungram-matical speech or run-on sentences where attempt-ing to look for complete sentences in strict left-to-right fashion would be unsuccessful.  2- ONYX implements a variation on a probabilistic context free grammar (PCFG) (Charniak, 1997) that associates grammar rules with semantic types. Based on training, a conditional probability is cal-culated for each <rule, type> pair given specific 
23
<rule, type> assignments to the rule?s components. The probability of a phrase is then calculated as the product of the probabilities of the phrase rule and its semantic type, given the rule and type of each of its child phrases. ONYX is then able to prefer phrases that best accommodate the semantic types of their constituents. Specifically,  prob(phrase) =  ?(prob(rule(phrase) + semtype(phrase) |         rule(childPhrase) + semtype(childPhrase)))  3- One hard problem in parsing is determining the correct structure of conjunctive noun phrases. ONYX applies semantic guidance to solve this problem. For instance, in a chest radiology report the words "right and left lower lobe opacity" can be grouped in several different ways, and different groupings can produce different interpretations. The correct grouping should be something like: [[[right and left] [lower lobe]] opacity], rather than [[right and [left lower]] [lobe opacity]]. ONYX currently employs a simplistic representation of the meaning of a conjunctive phrase as a list of inter-pretations. The correct interpretations for "right and left lower lobe opacity" would be two predi-cate expressions covering the words (right, lower, lobe, opacity) and (left, lower, lobe, opacity). ONYX generates a measure of the similarity of these expressions based on the cosine similarity of the lists of non-null nodes in their CM templates. This measure is factored into the phrase's goodness score under the heuristic that semantically bal-anced conjunctive phrases are more likely to be correct than imbalanced ones.  4- As mentioned earlier, ONYX can utilize gram-mar rules annotated with semantic types and pat-terns. Semantically annotated rules constrain phrases to match particular semantic types, and can contribute predicates to the interpretation of those phrases. This gives ONYX's grammar the character of a semantic grammar.  5- Phrases are weighted and preferred by ONYX according to their goodness score, which is based on three measures: the probability of the phrase as determined by the PCFG formula, the conjunct cosine similarity score, if applicable, and the goodness score of the phrase's semantic interpreta-tion. The PCFG and conjunct similarity formulas 
are based on semantic criteria, as mentioned ear-lier. Interpretation goodness scores are calculated as a simple product of the probabilities of the se-mantic relation predicates they contain. Relation probabilities are in turn derived from training data, and are conditioned on the concepts they contain. The probability of a relation is calculated as the number of times a pair of concepts appears to-gether in the target relation divided by the number of times they appear together in any set of rela-tions. The goodness score of a phrase is thus highly semantically determined.  goodness(phrase) = F(prob(phrase, PCFG),     conjunctSimilarity(phrase),     goodness(interp(phrase)) goodness(interp(phrase)) =    ?prob(relations(interp(phrase))) prob(relation) =   count(relation + concepts(relation)) /  count(anyConnection(concepts(relation))) 6  Evaluation We performed a preliminary evaluation of ONYX for the extraction of relevant dental concepts and relations on a set of twelve documents in our cur-rent training corpus.   Reference Standard. Each document was inde-pendently annotated by three human annotators (authors LC, JI and HH), who used the ONYX training tool to fill in templates representing dental conditions, tooth locations and other relevant con-cepts, as well as to select the semantic relations linking those templates. The annotators then re-viewed disagreements and by consensus created a reference standard set of templates and relations. Where the annotators did not have sufficient dental knowledge to reach an agreement they consulted dental clinicians.  Outcome Metrics. To evaluate ONYX on the rela-tively small corpus of documents, we applied a leave-one-out approach: for each sentence in the reference standard, ONYX was trained using the templates from the remaining reference standard sentences. ONYX was then applied to the target sentence, and the resulting templates and relations were compared to the reference standard. We measured inter-annotator agreement (IAA) be-tween ONYX and the reference standard using the formula described in Roberts et al(2007): 
24
 IAA = (2 * correct) / (spurious + missing + correct)  We calculated IAA separately for CM words, con-cepts, and semantic relations. A correct match is a word, concept or relation generated by both the reference standard and ONYX; a spurious item is one ONYX generated that did not exist in the ref-erence standard; and a missing item is one that ex-isted in the reference standard but was not generated by ONYX. In addition to IAA we identi-fied the concepts and relations most commonly in error and calculated percentages for those errors.   We compared ONYX?s performance on the target documents with that of a simple baseline parser we created for this purpose. The baseline parser proc-esses the words of a sentence from left to right, creating phrases for sets of juxtaposed words that can be interpreted together using the semantic net-work. No grammar rules are employed, there is no analysis of conjunctive phrases, and goodness scores are not calculated. Our goal was to get a feel for how much these factors contribute to generat-ing correct interpretations. There is no precedence for this particular approach as far as we are aware, so we regard this comparison as informative but not definitive. 7 Results IAA results for ONYX and the baseline parser are shown in table 2. ONYX performs best at inserting words into appropriate nodes in the CMs, with IAA of 86%, and less well for inferring the best concept (80%) and identifying relations among concepts (76%). ONYX consistently out-performs the baseline parser.  Table 2: IAA for assignment of words, concepts, and relations.  IAA ONYX  86% Words (n = 904) Baseline  57% ONYX  80% Concepts (n = 1186) Baseline  53% ONYX  76% Relations (n = 297) Baseline  41%  Although this study does not examine all the rea-sons for the differences in performance between ONYX and the baseline parser, some reasons can 
be illustrated with an example. Conjunctive phrases are common in dental discourse, and a failure to handle conjuncts can result in both con-cept and relation errors. For instance, given the sentence "4, 5, 6, 7 fine" ONYX generates separate interpretations covering the word groupings (4, fine), (5, fine), (6, fine), and (7, fine), which would yield four ConditionAt relations, four Location concepts (*numberFour, *numberFive, *numberSix, *numberSeven) and one Condition concept (*normalTooth) appearing in each relation. The baseline parser in contrast does not discover this distribution of terms and so omits all but the ConditionAt relation over (7, fine). Trying to merge juxtaposed tooth numbers, the baseline parser also infers that at least some of these denote tooth ranges instead of individual teeth (e.g. inter-preting ?4, 5? as ?4 to 5? instead of ?4 and 5?), which causes it to misclassify Location concepts. The ability to generate correct parse trees and to use the structure of those parse trees in the inter-pretation process is important in generating correct interpretations.  Tables 3 and 4 show breakdowns by percentage of the concepts and relations most commonly in error in ONYX?s interpretations (errors accounting for more than 15%).  Table 3: Per-concept error percentages  Dental Condition Summary Concept 18% Tooth Location Summary Concept 17% Dental Condition Intermediate Concept 16% Surface Summary Concept 15% Total  66%  Table 4: Per-relation error percentages. Surface of Part 47% Location of Condition 23% Total 70%  8  Related Work ONYX is a new application inspired by SPRUS (Ranum, 1989), Symtext (Koehler, 1998), and MPLUS (Christensen, 2002), which all used Baye-sian Networks to infer relevant findings from text. Other medical language processing systems im-plement different approaches to encode clinical concepts and their modifiers, along with relations between concepts, including MedLEE (Friedman, 
25
1994), a largely statistical system by Taira and col-leagues (Taira, 2007), and MedSyndikate (Hahn, 2002).   Many of ONYX?s components leverage research in the general and clinical NLP domains, including the use of chart parsing (Kay, 1980) and probabil-istic context free grammars (Charniak, 1997). ONYX's use of semantically annotated grammar rules was inspired in part by MedLEE (Friedman et al 1994), which uses a semantic grammar.   Although incorporating ideas and approaches from others, we feel that ONYX is unique in several ways, including its high level of syntactic/semantic integration and the ways in which it blends sym-bolic and probabilistic representations of domain knowledge. We plan to make ONYX available through open source when the system is more complete.  9 Limitations There are several limitations to this study. Al-though ONYX introduces several innovations, these are not described in detail in this study and are not individually evaluated for their effect on ONYX?s performance. Instead, this study presents a broad overview of ONYX and evaluates ONYX's overall performance against a reference standard on a small test sample. Another limitation of our study is the baseline system?because similar sys-tems generate different output than ONYX and do not model the same domain, finding a competitive baseline application is difficult. In spite of its im-perfection, we believe the baseline we imple-mented to be reasonable. 10 Future Work One limitation of a system like ONYX is the over-head of manually creating complex training cases. To address this shortcoming, the ONYX training tool invokes ONYX to automatically create tem-plates and relations for corpus sentences, and hu-man trainers correct any mistakes. A semi-automated approach greatly speeds up the training process and facilitates agreement among human trainers. We plan to further automate this process using an approach derived from Thelen & Riloff (2002), which uses a classifier with features based 
on extraction patterns derived from Autoslog (Riloff, 1996). We plan to adapt this approach to automatically classify CM word assignments, and also to automatically classify semantic relations between CM templates. We will add this function-ality to the training tool to enable it to find and an-notate relevant sentences automatically where possible. We will also apply this functionality to enable ONYX to recognize relevant sentences in new documents based on their similarity to training sentences, and we will use semantic patterns stored with training sentences to aid in interpreting noisy segments of text that ONYX cannot parse. We plan to compare the performance of grammar-based and feature-based semantic analysis in future studies. With more fully automated training, we also hope to make ONYX more easily portable to new do-mains and clinical settings in the future.   Conclusions  This paper describes ONYX, which is being devel-oped as part of a system for extracting chartable findings from spoken dental examinations. ONYX contains a number of innovative ideas including a novel adaptation of Kay's (1980) parse algorithm; a symbolic language extended to include probabilis-tic and procedural elements; an integration of syn-tax and semantics that includes a semantically weighted probabilistic context free grammar and interpretation based both on a semantic network and a semantic grammar. Considering ONYX?s early stage of development it performed reasonably well in this limited evaluation but must be ex-tended to address challenges in extracting findings from spoken dental exams. Acknowledgments  This work was funded by NIDCR 1 R21DE018158-01A1 ?Feasibility of a Natural Language Processing-based Dental Charting Application. References  E. Charniak. 1997. Statistical parsing with a context-free grammar and word statistics. In Proceedings of the Fourteenth National Conference on Artificial In-telligence, pp. 598-603. E. Charniak, S. Goldwater and M. Johnson. 1998. Edge-Based Best-First Chart Parsing. In Proceedings of 
26
the Sixth Workshop on Very Large Corpora, pp. 127-133. Lee M. Christensen, Peter J. Haug, and Marcelo Fisz-man. 2002. MPLUS: A Probabilistic Medical Lan-guage Understanding System. Proceedings of the Workshop on Natural Language Processing in the Biomedical Domain, Philadelphia, pp. 29 ? 36. Carol Friedman, Phil Alderson, John Austin, James Ci-mino, & Stephen Johnson. 1994. A general natural language text processor for clinical radiology. Jour-nal of American Medical Informatics Association 1(2), pp. 161?174.  M. R. Genesereth and R. E. Fikes. Knowledge Inter-change Format, Version 3.0 Reference Manual. Technical Report Logic-92-1, Stanford, CA, USA, 1992.  Hahn U, Romacker M, Schulz S. 2002. Medsyndikate-a natural language system for the extraction of medical information from findings reports. Int J Med Inf. 67(1-3), pp. 63-74.  M. Kay. 1980. Algorithm schemata and data structures in syntactic parsing. In Readings in Natural Lan-guage Processing, pp. 35 ? 70. Morgan Kaufmann Publishers Inc.  Koehler, S. B. 1998. SymText: A natural language un-derstanding system for encoding free text medical data. Ph.D. Dissertation, University of Utah.  Ranum D.L. 1989. Knowledge-based understanding of radiology text. Comput Methods ProBiomed. Oct-Nov;30(2-3) pp. 209-215. Ellen Riloff, 1996. Automatically Generating Extraction Patterns from Untagged Text. Proceedings of the Thirteenth National Conference on Artiticial Intelli-gence, pp. 1044 ? 1049. The AAAI Press/MIT Press. Angus Roberts, Robert Gaizauskas, Mark Hepple, Neil Davis, George Demetriou, Yikun Guo, Jay Kola, Ian Roberts, Andrea Setzer, Archana Tapuria, Bill Wheeldin. 2007. The CLEF Corpus: Semantic Anno-tation of Clinical Text. AMIA 2007, pp. 625 ? 629.  Taira R, Bashyam V, Kangarloo H. 2007. A field theory approach to medical natural language processing. IEEE Transactions in Inform Techn in Biomedicine 11(2). Michael Thelen and Ellen Riloff. 2002. A Bootstrapping Method for Learning Semantic Lexicons using Ex-traction Pattern Contexts. Proceedings of the ACL-02 conference on Empirical methods in natural lan-guage processing, pp. 214 ? 221. 
27
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 54?62,
Dublin, Ireland, August 23-24, 2014.
SemEval-2014 Task 7: Analysis of Clinical Text
Sameer Pradhan
1
, No
?
emie Elhadad
2
, Wendy Chapman
3
,
Suresh Manandhar
4
and Guergana Savova
1
1
Harvard University, Boston, MA,
2
Columbia University, New York, NY
3
University of Utah, Salt Lake City, UT,
4
University of York, York, UK
{sameer.pradhan,guergana.savova}@childrens.harvard.edu, noemie.elhadad@columbia.edu,
wendy.chapman@utah.edu, suresh@cs.york.ac.uk
Abstract
This paper describes the SemEval-2014,
Task 7 on the Analysis of Clinical Text
and presents the evaluation results. It fo-
cused on two subtasks: (i) identification
(Task A) and (ii) normalization (Task B)
of diseases and disorders in clinical reports
as annotated in the Shared Annotated Re-
sources (ShARe)
1
corpus. This task was
a follow-up to the ShARe/CLEF eHealth
2013 shared task, subtasks 1a and 1b,
2
but
using a larger test set. A total of 21 teams
competed in Task A, and 18 of those also
participated in Task B. For Task A, the
best system had a strict F
1
-score of 81.3,
with a precision of 84.3 and recall of 78.6.
For Task B, the same group had the best
strict accuracy of 74.1. The organizers
have made the text corpora, annotations,
and evaluation tools available for future re-
search and development at the shared task
website.
3
1 Introduction
A large amount of very useful information?both
for medical researchers and patients?is present
in the form of unstructured text within the clin-
ical notes and discharge summaries that form a
patient?s medical history. Adapting and extend-
ing natural language processing (NLP) techniques
to mine this information can open doors to bet-
ter, novel, clinical studies on one hand, and help
patients understand the contents of their clini-
cal records on the other. Organization of this
1
http://share.healthnlp.org
2
https://sites.google.com/site/shareclefehealth/
evaluation
3
http://alt.qcri.org/semeval2014/task7/
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
shared task helps establish state-of-the-art bench-
marks and paves the way for further explorations.
It tackles two important sub-problems in NLP?
named entity recognition and word sense disam-
biguation. Neither of these problems are new to
NLP. Research in general-domain NLP goes back
to about two decades. For an overview of the
development in the field through roughly 2009,
we refer the refer to Nadeau and Sekine (2007).
NLP has also penetrated the field of bimedical
informatics and has been particularly focused on
biomedical literature for over the past decade. Ad-
vances in that sub-field has also been documented
in surveys such as one by Leaman and Gonza-
lez (2008). Word sense disambiguation also has
a long history in the general NLP domain (Nav-
igli, 2009). In spite of word sense annotations in
the biomedical literature, recent work by Savova
et al. (2008) highlights the importance of annotat-
ing them in clinical notes. This is true for many
other clinical and linguistic phenomena as the var-
ious characteristics of the clinical narrative present
a unique challenge to NLP. Recently various ini-
tiatives have led to annotated corpora for clini-
cal NLP research. Probably the first comprehen-
sive annotation performed on a clinical corpora
was by Roberts et al. (2009), but unfortunately
that corpus is not publicly available owing to pri-
vacy regulations. The i2b2 initiative
4
challenges
have focused on such topics as concept recog-
nition (Uzuner et al., 2011), coreference resolu-
tion (Uzuner et al., 2012), temporal relations (Sun
et al., 2013) and their datasets are available to the
community. More recently, the Shared Annotated
Resources (ShARe)
1
project has created a corpus
annotated with disease/disorder mentions in clini-
cal notes as well as normalized them to a concept
unique identifier (CUI) within the SNOMED-CT
subset of the Unified Medical Language System
5
4
http://www.i2b2.org
5
https://uts.nlm.nih.gov/home.html
54
Train Development Test
Notes 199 99 133
Words 94K 88K 153K
Disorder mentions 5,816 5,351 7,998
CUI-less mentions 1,639 (28%) 1,750 (32%) 1,930 (24%)
CUI-ied mentions 4,117 (72%) 3,601 (67%) 6,068 (76%)
Contiguous mentions 5,165 (89%) 4,912 (92%) 7,374 (92%)
Discontiguous mentions 651 (11%) 439 (8%) 6,24 (8%)
Table 1: Distribution of data in terms of notes and disorder mentions across the training, development
and test sets. The disorders are further split according to two criteria ? whether they map to a CUI or
whether they are contiguous.
(UMLS) (Campbell et al., 1998). The task of nor-
malization is a combination of word/phrase sense
disambiguation and semantic similarity where a
phrase is mapped to a unique concept in an on-
tology (based on the description of that concept in
the ontology) after disambiguating potential am-
biguous surface words, or phrases. This is espe-
cially true with abbreviations and acronyms which
are much more common in clinical text (Moon et
al., 2012). The SemEval-2014 task 7 was one of
nine shared tasks organized at the SemEval-2014.
It was designed as a follow up to the shared tasks
organized during the ShARe/CLEF eHealth 2013
evaluation (Suominen et al., 2013; Pradhan et al.,
2013; Pradhan et al., 2014). Like the previous
shared task, we relied on the ShARe corpus, but
with more data for training and a new test set. Fur-
thermore, in this task, we provided the options to
participants to utilize a large corpus of unlabeled
clinical notes. The rest of the paper is organized as
follows. Section 2 describes the characteristics of
the data used in the task. Section 3 describes the
tasks in more detail. Section 4 explains the evalu-
ation criteria for the two tasks. Section 5 lists the
participants of the task. Section 6 discusses the re-
sults on this task and also compares them with the
ShARe/CLEF eHealth 2013 results, and Section 7
concludes.
2 Data
The ShARe corpus comprises annotations over
de-identified clinical reports from a US intensive
care department (version 2.5 of the MIMIC II
database
6
) (Saeed et al., 2002). It consists of
discharge summaries, electrocardiogram, echocar-
diogram, and radiology reports. Access to data
was carried out following MIMIC user agreement
requirements for access to de-identified medical
6
http://mimic.physionet.org ? Multiparameter Intelligent
Monitoring in Intensive Care
data. Hence, all participants were required to reg-
ister for the evaluation, obtain a US human sub-
jects training certificate
7
, create an account to the
password-protected MIMIC site, specify the pur-
pose of data usage, accept the data use agree-
ment, and get their account approved. The anno-
tation focus was on disorder mentions, their var-
ious attributes and normalizations to an UMLS
CUI. As such, there were two parts to the annota-
tion: identifying a span of text as a disorder men-
tion and normalizing (or mapping) the span to a
UMLS CUI. The UMLS represents over 130 lex-
icons/thesauri with terms from a variety of lan-
guages and integrates resources used world-wide
in clinical care, public health, and epidemiology.
A disorder mention was defined as any span of text
which can be mapped to a concept in SNOMED-
CT and which belongs to the Disorder semantic
group
8
. It also provided a semantic network in
which every concept is represented by its CUI
and is semantically typed (Bodenreider and Mc-
Cray, 2003). A concept was in the Disorder se-
mantic group if it belonged to one of the follow-
ing UMLS semantic types: Congenital Abnormal-
ity; Acquired Abnormality; Injury or Poisoning;
Pathologic Function; Disease or Syndrome; Men-
tal or Behavioral Dysfunction; Cell or Molecu-
lar Dysfunction; Experimental Model of Disease;
Anatomical Abnormality; Neoplastic Process; and
Signs and Symptoms. The Finding semantic type
was left out as it is very noisy and our pilot study
showed lower annotation agreement on it. Follow-
ing are the salient aspects of the guidelines used to
7
The course was available free of charge on the Internet, for example,
via the CITI Collaborative Institutional Training Initiative at
https://www.citiprogram.org/Default.asp
or, the US National Institutes of Health (NIH) at
http://phrp.nihtraining.com/users.
8
Note that this definition of Disorder semantic group did not include the
Findings semantic type, and as such differed from the one of UMLS Seman-
tic Groups, available at http://semanticnetwork.nlm.nih.gov/
SemGroups
55
annotate the data.
? Annotations represent the most specific dis-
order span. For example, small bowel ob-
struction is preferred over bowel obstruction.
? A disorder mention is a concept in the
SNOMED-CT portion of the Disorder se-
mantic group.
? Negation and temporal modifiers are not con-
sidered part of the disorder mention span.
? All disorder mentions are annotated?even
the ones related to a person other than the pa-
tient and including acronyms and abbrevia-
tions.
? Mentions of disorders that are coreferen-
tial/anaphoric are also annotated.
Following are a few examples of disorder men-
tions from the data.
Patient found to have lower extremity DVT. (E1)
In example (E1), lower extremity DVT is marked
as the disorder. It corresponds to CUI C0340708
(preferred term: Deep vein thrombosis of lower
limb). The span DVT can be mapped to CUI
C0149871 (preferred term: Deep Vein Thrombo-
sis), but this mapping would be incorrect because
it is part of a more specific disorder in the sen-
tence, namely lower extremity DVT.
A tumor was found in the left ovary. (E2)
In example (E2), tumor ... ovary is annotated as a
discontiguous disorder mention. This is the best
method of capturing the exact disorder mention
in clinical notes and its novelty is in the fact that
either such phenomena have not been seen fre-
quently enough in the general domain to gather
particular attention, or the lack of a manually
curated general domain ontology parallel to the
UMLS.
Patient admitted with low blood pressure. (E3)
There are some disorders that do not have a rep-
resentation to a CUI as part of the SNOMED CT
within the UMLS. However, if they were deemed
important by the annotators then they were anno-
tated as CUI-less mentions. In example (E3), low
blood pressure is a finding and is normalized as
a CUI-less disorder. We constructed the annota-
tion guidelines to require that the disorder be a
reasonable synonym of the lexical description of a
SNOMED-CT disorder. There are a few instances
where the disorders are abbreviated or shortened
in the clinical note. One example is w/r/r, which
is an abbreviation for concepts wheezing (CUI
C0043144), rales (CUI C0034642), and ronchi
(CUI C0035508). This abbreviation is also some-
times written as r/w/r and r/r/w. Another is gsw for
gunshot wound and tachy for tachycardia. More
details on the annotation scheme is detailed in the
guidelines
9
and in a forthcoming manuscript. The
annotations covered about 336K words. Table 1
shows the quantity of the data and the split across
the training, development and test sets as well as
in terms of the number of notes and the number of
words.
2.1 Annotation Quality
Each note in the training and development set was
annotated by two professional coders trained for
this task, followed by an open adjudication step.
By the time we reached annotating the test data,
the annotators were quite familiar with the anno-
tation and so, in order to save time, we decided
to perform a single annotation pass using a senior
annotator. This was followed by a correction pass
by the same annotator using a checklist of frequent
annotation issues faced earlier. Table 2 shows the
inter-annotator agreement (IAA) statistics for the
adjudicated data. For the disorders we measure the
agreement in terms of the F
1
-score as traditional
agreement measures such as Cohen?s kappa and
Krippendorf?s alpha are not applicable for measur-
ing agreement for entity mention annotation. We
computed agreements between the two annotators
as well as between each annotator and the final ad-
judicated gold standard. The latter is to give a
sense of the fraction of corrections made in the
process of adjudication. The strict criterion con-
siders two mentions correct if they agree in terms
of the class and the exact string, whereas the re-
laxed criteria considers overlapping strings of the
9
http://goo.gl/vU8KdW
Disorder CUI
Relaxed Strict Relaxed Strict
F
1
F
1
Acc. Acc.
A1-A2 90.9 76.9 77.6 84.6
A1-GS 96.8 93.2 95.4 97.3
A2-GS 93.7 82.6 80.6 86.3
Table 2: Inter-annotator (A1 and A2) and gold
standard (GS) agreement as F
1
-score for the Dis-
order mentions and their normalization to the
UMLS CUI.
56
Institution User ID Team ID
University of Pisa, Italy attardi UniPI
University of Lisbon, Portugal francisco ULisboa
University of Wisconsin, Milwaukee, USA ghiasvand UWM
University of Colorado, Boulder, USA gung CLEAR
University of Guadalajara, Mexico herrera UG
Taipei Medical University, Taiwan hjdai TMU
University of Turku, Finland kaewphan UTU
University of Szeged, Hungary katona SZTE-NLP
Queensland University of Queensland, Australia kholghi QUT AEHRC
KU Leuven, Belgium kolomiyets KUL
Universidade de Aveiro, Portugal nunes BioinformaticsUA
University of the Basque Country, Spain oronoz IxaMed
IBM, India parikh ThinkMiners
easy data intelligence, India pathak ezDI
RelAgent Tech Pvt. Ltd., India ramanan RelAgent
Universidad Nacional de Colombia, Colombia riveros MindLab-UNAL
IIT Patna, India sikdar IITP
University of North Texas, USA solomon UNT
University of Illinois at Urbana Champaign, USA upadhya CogComp
The University of Texas Health Science Center at Houston, USA wu UTH CCB
East China Normal University, China yi ECNU
Table 3: Participant organization and the respective User IDs and Team IDs.
same class as correct. The reason for checking
the class is as follows. Although we only use the
disorder mention in this task, the corpus has been
annotated with some other UMLS types as well
and therefore there are instances where a differ-
ent UMLS type is assigned to the same character
span in the text by the second annotator. If exact
boundaries are not taken into account then the IAA
agreement score is in the mid-90s. For the task of
normalization to CUIs, we used accuracy to assess
agreement. For the relaxed criterion, all overlap-
ping disorder spans with the same CUI were con-
sidered correct. For the strict criterion, only disor-
der spans with identical spans and the same CUI
were considered correct.
3 Task Description
The participants were evaluated on the following
two tasks:
? Task A ? Identification of the character spans
of disorder mentions.
? Task B ? Normalizing disorder mentions to
SNOMED-CT subset of UMLS CUIs.
For Task A, participants were instructed to develop
a system that predicts the spans for disorder men-
tions. For Tasks B, participants were instructed
to develop a system that predicts the UMLS CUI
within the SNOMED-CT vocabulary. The input to
Task B were the disorder mention predictions from
Task A. Task B was optional. System outputs ad-
hered to the annotation format. Each participant
was allowed to submit up to three runs. The en-
tire set of unlabeled MIMIC clinical notes (exclud-
ing the test notes) were made available to the par-
ticipants for potential unsupervised approaches to
enhance the performance of their systems. They
were allowed to use additional annotations in their
systems, but this counted towards the total allow-
able runs; systems that used annotations outside
of those provided were evaluated separately. The
evaluation for all tasks was conducted using the
blind, withheld test data. The participants were
provided a training set containing clinical text as
well as pre-annotated spans and named entities for
disorders (Tasks A and B).
4 Evaluation Criteria
The following evaluation criteria were used:
? Task A ? The system performance was eval-
uated against the gold standard using the
F
1
-score of the Precision and Recall values.
There were two variations: (i) Strict; and (ii)
Relaxed. The formulae for computing these
metrics are mentioned below.
Precision = P =
D
tp
D
tp
+ D
fp
(1)
Recall = R =
D
tp
D
tp
+ D
fn
(2)
Where, D
tp
= Number of true positives dis-
order mentions; D
fp
= Number of false pos-
itives disorder mentions; D
fn
= Number of
false negative disorder mentions. In the strict
case, a span was counted as correct if it was
identical to the gold standard span, whereas
57
Task A
Strict Relaxed
Team ID User ID Run P R F
1
P R F
1
Data
(%) (%) (%) (%) (%) (%)
UTH CCB wu 0 84.3 78.6 81.3 93.6 86.6 90.0 T+D
UTH CCB wu 1 80.8 80.5 80.6 91.6 90.7 91.1 T+D
UTU kaewphan 1 76.5 76.7 76.6 88.6 89.9 89.3 T+D
UWM ghiasvand 0 78.7 72.6 75.5 91.1 85.6 88.3 T+D
UTH CCB wu 2 68.0 84.9 75.5 83.8 93.5 88.4 T+D
UTU kaewphan 0 77.3 72.4 74.8 90.1 85.6 87.8 T
IxaMed oronoz 1 68.1 78.6 73.0 87.2 89.0 88.1 T+D
UWM ghiasvand 0 77.5 67.9 72.4 90.9 81.2 85.8 T
RelAgent ramanan 0 74.1 70.1 72.0 89.5 84.0 86.7 T+D
IxaMed oronoz 0 72.9 70.1 71.5 88.5 80.8 84.5 T+D
ezDI pathak 1 75.0 68.2 71.4 91.5 82.7 86.9 T
CLEAR gung 0 80.7 63.6 71.2 92.0 72.3 81.0 T
ezDI pathak 0 75.0 67.7 71.2 91.4 81.9 86.4 T
ULisboa francisco 0 75.3 66.3 70.5 91.4 81.5 86.2 T
ULisboa francisco 1 75.2 66.0 70.3 90.9 80.6 85.5 T
ULisboa francisco 2 75.2 66.0 70.3 90.9 80.6 85.5 T
BioinformaticsUA nunes 0 81.3 60.5 69.4 92.9 69.3 79.4 T+D
ThinkMiners parikh 0 73.4 65.0 68.9 89.2 80.2 84.4 T
ThinkMiners parikh 1 74.9 61.7 67.7 90.7 75.8 82.6 T
ECNU yi 0 75.4 61.1 67.5 89.8 72.2 80.0 T+D
UniPI attardi 2 71.2 60.1 65.2 89.7 76.6 82.6 T+D
UNT solomon 0 64.7 62.8 63.8 81.5 79.9 80.7 T+D
UniPI attardi 1 65.9 61.2 63.5 90.2 77.5 83.4 T+D
BioinformaticsUA nunes 2 75.3 53.8 62.8 86.5 62.1 72.3 T+D
BioinformaticsUA nunes 1 60.0 62.1 61.0 69.8 72.3 71.0 T+D
UniPI attardi 0 53.9 68.4 60.2 77.8 88.5 82.8 T+D
CogComp upadhya 1 63.9 52.9 57.9 82.3 68.3 74.6 T+D
CogComp upadhya 2 64.1 52.0 57.4 82.9 67.5 74.4 T+D
CogComp upadhya 0 63.6 51.5 56.9 81.9 66.5 73.4 T+D
TMU hjdai 0 52.4 57.6 54.9 91.4 76.5 83.3 T+D
MindLab-UNAL riveros 2 56.1 53.4 54.7 76.9 67.7 72.0 T
MindLab-UNAL riveros 1 57.8 51.5 54.5 77.7 65.4 71.0 T
TMU hjdai 1 62.2 42.9 50.8 89.9 65.2 75.6 T+D
IITP sikdar 0 50.0 47.9 48.9 81.5 79.7 80.6 T+D
IITP sikdar 1 47.3 45.8 46.5 78.9 77.6 78.2 T+D
IITP sikdar 2 45.0 48.1 46.5 76.9 82.6 79.6 T+D
MindLab-UNAL riveros 0 32.1 56.5 40.9 43.9 72.5 54.7 T
SZTE-NLP katona 1 54.7 25.2 34.5 88.4 40.1 55.1 T
SZTE-NLP katona 2 54.7 25.2 34.5 88.4 40.1 55.1 T
QUT AEHRC kholghi 0 38.7 29.8 33.7 90.6 70.9 79.5 T+D
SZTE-NLP katona 0 57.1 20.5 30.2 91.8 32.5 48.0 T
KUL kolomiyets 0 65.5 17.8 28.0 72.1 19.6 30.8 P
UG herrera 0 11.4 23.4 15.3 25.9 49.0 33.9 P
Table 4: Performance on test data for participating systems on Task A ? Identification of disorder men-
tions.
Task A
Strict Relaxed
Team ID User ID Run P R F
1
P R F
1
Data
(%) (%) (%) (%) (%) (%)
hjdai TMU 1 0.687 0.922 0.787 0.952 1.000 0.975 T
wu UTH CCB 0 0.877 0.710 0.785 0.962 0.789 0.867 T
wu UTH CCB 1 0.828 0.747 0.785 0.941 0.853 0.895 T
Best ShARe/CLEF-2013 performance 0.800 0.706 0.750 0.925 0.827 0.873 T
ghiasvand UWM 0 0.827 0.675 0.743 0.958 0.799 0.871 T
pathak ezDI 0 0.813 0.670 0.734 0.954 0.800 0.870 T
pathak ezDI 1 0.809 0.667 0.732 0.954 0.801 0.871 T
wu UTH CCB 2 0.657 0.790 0.717 0.806 0.893 0.847 T
francisco ULisboa 1 0.803 0.646 0.716 0.954 0.781 0.858 T
francisco ULisboa 2 0.803 0.646 0.716 0.954 0.781 0.858 T
francisco ULisboa 0 0.796 0.642 0.711 0.959 0.793 0.868 T
oronoz IxaMed 0 0.766 0.650 0.703 0.936 0.752 0.834 T
oronoz IxaMed 1 0.660 0.721 0.689 0.899 0.842 0.870 T
hjdai TMU 0 0.667 0.414 0.511 0.912 0.591 0.717 T
sikdar IITP 0 0.525 0.430 0.473 0.862 0.726 0.788 T
sikdar IITP 2 0.467 0.440 0.453 0.812 0.775 0.793 T
sikdar IITP 1 0.493 0.410 0.448 0.828 0.706 0.762 T
Table 5: Performance on development data for participating systems on Task A ? Identification of disor-
der mentions.
58
in the relaxed case, a span overlapping with
the gold standard span was also considered
correct.
? Task B ? Accuracy was used as the perfor-
mance measure for Task 1b. It was defined as
follows:
Accuracy
strict
=
D
tp
?N
correct
T
g
(3)
Accuracy
relaxed
=
D
tp
?N
correct
D
tp
(4)
Where, D
tp
= Number of true positive disor-
der mentions with identical spans as in the
gold standard; N
correct
= Number of cor-
rectly normalized disorder mentions; and T
g
= Total number of disorder mentions in the
gold standard. For Task B, the systems were
only evaluated on annotations they identified
in Task A. Relaxed accuracy only measured
the ability to normalize correct spans. There-
fore, it was possible to obtain very high val-
ues for this measure by simply dropping any
mention with a low confidence span.
5 Participants
A total of 21 participants from across the world
participated in Task A and out of them 18 also par-
ticipated in Task B. Unfortunately, although inter-
ested, the ThinkMiners team (Parikh et al., 2014)
could not participate in Task B owing to some
UMLS licensing issues. The participating organi-
zations along with the contact user?s User ID and
their chosen Team ID are mentioned in Table 3.
Eight teams submitted three runs, six submitted
two runs and seven submitted just one run. Out
of these, only 13 submitted system description pa-
pers. We based our analysis on those system de-
scriptions.
6 System Results
Tables 4 and 6 show the performance of the sys-
tems on Tasks A and B. None of the systems used
any additional annotated data so we did not have
to compare them separately. Both tables mention
performance of all the different runs that the sys-
tems submitted. Given the many variables, we de-
liberately left the decision on how many and how
to define these runs to the individual participant.
They used various different ways to differentiate
their runs. Some, for example, UTU (Kaewphan et
al., 2014), did it based on the composition of train-
ing data, i.e., whether they used just the training
data or both the training and the development data
for training the final system, which highlighted
the fact that adding development data to training
bumped the F
1
-score on Task A by about 2 percent
points. Some participants, however, did not make
use of the development data in training their sys-
tems. This was partially due to the fact that we had
not explicitly mentioned in the task description
that participants were allowed to use the develop-
ment data for training their final models. In order
to be fair, we allowed some users an opportunity
to submit runs post evaluation where they used the
exact same system that they used for evaluation
but used the development data as well. We added
a column to the results tables showing whether the
participant used only the training data (T) or both
training and development data (T+D) for training
their system. It can be seen that even though the
addition of development data helps, there are still
systems that perform in the lower percentile who
have used both training and development data for
training, indicating that both the features and the
machine learning classifier contribute to the mod-
els. A novel aspect of the SemEval-2014 shared
task that differentiates it from the ShARE/CLEF
task?other than the fact that it used more data and
a new test set?is the fact that SemEval-2014 al-
lowed the use of a much larger set of unlabeled
MIMIC notes to inform the models. Surprisingly,
only two of the systems (ULisboa (Leal et al.,
2014) and UniPi (Attardi et al., 2014)) used the
unlabeled MIMIC corpus to generalize the lexical
features. Another team?UTH CCB(Zhang et al.,
2014)?used off-the-shelf Brown clusters
10
as op-
posed to training them on the unlabeled MIMIC
II data. For Task B, the accuracy of a system
using the strict metric was positively correlated
with its recall on the disorder mentions that were
input to it (i.e., recall for Task A), and did not
get penalized for lower precision. Therefore one
could essentially gain higher accuracy in Task B
by tuning a system to provide the highest men-
tion recall in Task A potentially at the cost of pre-
cision and the overall F
1
-score and using those
mentions as input for Task B. This can be seen
from the fact that the run 2 for UTH CCB (Zhang
et al., 2014) system with the lowest F
1
-score has
10
Personal conversation with the participants as it was not
very clear in the system description paper.
59
Task B
Strict Relaxed
Team ID User ID Run Acc. Acc. Data
(%) (%)
UTH CCB wu 2 74.1 87.3 T+D
UTH CCB wu 1 70.8 88.0 T+D
UTH CCB wu 0 69.4 88.3 T+D
UWM ghiasvand 0 66.0 90.9 T+D
RelAgent ramanan 0 63.9 91.2 T+D
UWM ghiasvand 0 61.7 90.8 T
IxaMed oronoz 0 60.4 86.2 T+D
UTU kaewphan 1 60.1 78.3 T+D
ezDI pathak 1 59.9 87.8 T
ezDI pathak 0 59.2 87.4 T
UTU kaewphan 0 57.7 79.7 T
BioinformaticsUA nunes 1 53.1 85.5 T+D
BioinformaticsUA nunes 0 52.7 87.0 T+D
CLEAR gung 0 52.5 82.5 T
TMU hjdai 0 48.9 84.9 T+D
UNT solomon 0 47.0 74.8 T+D
UniPI attardi 0 46.7 68.3 T+D
BioinformaticsUA nunes 2 46.3 86.1 T+D
MindLab-UNAL riveros 2 46.1 86.3 T
IxaMed oronoz 1 43.9 55.8 T+D
MindLab-UNAL riveros 0 43.5 77.1 T
UniPI attardi 1 42.8 69.9 T+D
UniPI attardi 2 41.7 69.3 T+D
MindLab-UNAL riveros 1 41.1 79.7 T
ULisboa francisco 2 40.5 61.5 T
ULisboa francisco 1 40.4 61.2 T
ULisboa francisco 0 40.2 60.6 T
ECNU yi 0 36.4 59.5 T+D
TMU hjdai 1 35.8 83.4 T+D
IITP sikdar 0 33.3 69.6 T+D
IITP sikdar 2 33.2 69.1 T+D
IITP sikdar 1 31.9 69.6 T+D
CogComp upadhya 1 25.3 47.9 T+D
CogComp upadhya 2 24.8 47.7 T+D
CogComp upadhya 0 24.4 47.3 T+D
KUL kolomiyets 0 16.5 92.8 P
UG herrera 0 12.5 53.4 P
Table 6: Performance on test data for participat-
ing systems on Task B ? Normalization of disorder
mentions to UMLS (SNOMED-CT subset) CUIs.
Task B
Strict Relaxed
Team ID User ID Run Acc. Acc. Data
(%) (%)
TMU hjdai 0 0.716 0.777 T
TMU hjdai 1 0.716 0.777 T
UTH CCB wu 2 0.713 0.903 T
UTH CCB wu 1 0.680 0.910 T
UTH CCB wu 0 0.647 0.910 T
UWM ghiasvand 0 0.623 0.923 T
ezDI pathak 0 0.603 0.900 T
ezDI pathak 1 0.600 0.899 T
Best ShARe/CLEF-2013 performance 0.589 0.895 T
IxaMed oronoz 0 0.556 0.855 T
IxaMed oronoz 1 0.421 0.584 T
ULisboa francisco 2 0.388 0.601 T
ULisboa francisco 1 0.385 0.596 T
ULisboa francisco 0 0.377 0.588 T
IITP sikdar 2 0.318 0.724 T
IITP sikdar 0 0.312 0.725 T
IITP sikdar 1 0.299 0.730 T
Table 7: Performance on development data
for some participating systems on Task B ?
Normalization of disorder mentions to UMLS
(SNOMED-CT subset) CUIs.
the best accuracy for Task B and vice-versa for
run 0 with run 1 in between the two. In order to
fairly compare the performance between two sys-
tems one would have to provide perfect mentions
as input to Task B. One of the systems?UWM
Ghiasvand and Kate (2014)?did run some abla-
tion experiments using gold standard mentions as
input to Task B and obtained a best performance
of 89.5F
1
-score (Table 5 of Ghiasvand and Kate
(2014)) as opposed to 62.3 F
1
-score (Table 7) in
the more realistic setting which is a huge differ-
ence. In the upcoming SemEval-2014 where this
same evaluation is going to carried out under Task
14, we plan to perform supplementary evaluation
where gold disorder mentions would be input to
the system while attempting Task B. An inter-
esting outcome of planning a follow-on evalua-
tion to the ShARe/CLEF eHealth 2013 task was
that we could, and did, use the test data from the
ShARe/CLEF eHealth 2013 task as the develop-
ment set for this evaluation. After the main eval-
uation we asked participants to provide the sys-
tem performance on the development set using the
same number and run convention that they submit-
ted for the main evaluation. These results are pre-
sented in Tables 5 and 7. We have inserted the best
performing system score from the ShARe/CLEF
eHealth 2013 task in these tables. For Task A, re-
ferring to Tables 4 and 5, there is a boost of 3.7
absolute percent points for the F
1
-score over the
same task (Task 1a) in the ShARe/CLEF eHealth
2013. For Task B, referring to Tables 6 and 7, there
is a boost of 13.7 percent points for the F
1
-score
over the same task (Task 1b) in the ShARe/CLEF
eHealth 2013 evaluation. The participants used
various approaches for tackling the tasks, rang-
ing from purely rule-based/unsupervised (RelA-
gent (Ramanan and Nathan, 2014), (Matos et
al., 2014), KUL
11
) to a hybrid of rules and ma-
chine learning classifiers. The top performing sys-
tems typically used the latter. Various versions
of the IOB formulation were used for tagging the
disorder mentions. None of the standard varia-
tions on the IOB formulation were explicitly de-
signed or used to handle discontiguous mentions.
Some systems used novel variations on this ap-
proach. Probably the simplest variation was ap-
plied by the UWM team (Ghiasvand and Kate,
2014). In this formulation the following labeled
sequence ?the/O left/B atrium/I is/O moderately/O
11
Personal communication with participant.
60
dilated/I? can be used to represent the discontigu-
ous mention left atrium...dilated, and can be con-
structed as such from the output of the classifica-
tion. The most complex variation was the one used
by the UTH CCB team (Zhang et al., 2014) where
they used the following set of tags?B, I, O, DB,
DI, HB, HI. This variation encodes discontiguous
mentions by adding four more tags to the I, O and
B tags. These are variations of the B and I tags
with either a D or a H prefix. The prefix H indi-
cates that the word or word sequence is the shared
head, and the prefix D indicates otherwise. An-
other intermediate approach used by the ULisboa
team (Leal et al., 2014) with the tagset?S, B, I,
O, E and N. Here, S represents the single token
entity to be recognized, E represents the end of an
entity (which is part of one of the prior IOB vari-
ations) and an N tag to identify non-contiguous
mentions. They don?t provide an explicit exam-
ple usage of this tag set in their paper. Yet another
variation was used by the SZTE-NLP team (Ka-
tona and Farkas, 2014). This used tags B, I, L, O
and U. Here, L is used for the last token similar to
E earlier, and U is used for a unit-token mention,
similar to S earlier. We believe that the only ap-
proach that can distinguish between discontiguous
disorders that share the same head word/phrase is
the one used by the UTH CCB team (Zhang et
al., 2014). The participants used various machine
learning classifiers such as MaxEnt, SVM, CRF in
combination with rich syntactic and semantic fea-
tures to capture the disorder mentions. As men-
tioned earlier, a few participants used the avail-
able unlabeled data and also off-the-shelf clusters
to better generalize features. The use of vector
space models such as cosine similarities as well
as continuous distributed word vector representa-
tions was useful in the normalization task. They
also availed of tools such as MetaMap and cTakes
to generate features as well as candidate CUIs dur-
ing normalizations.
7 Conclusion
We have created a reference standard with high
inter-annotator agreement and evaluated systems
on the task of identification and normalization
of diseases and disorders appearing in clinical
reports. The results have demonstrated that an
NLP system can complete this task with reason-
ably high accuracy. We plan to annotate another
evaluation using the same data as part of the in
the SemEval-2015, Task 14
12
adding another task
of template filling where the systems will iden-
tify and normalize ten attributes the identified dis-
ease/disorder mentions.
Acknowledgments
We greatly appreciate the hard work and feed-
back of our program committee members and an-
notators David Harris, Jennifer Green and Glenn
Zaramba. Danielle Mowery, Sumithra Velupillai
and Brett South for helping prepare the manuscript
by summarizing the approaches used by various
systems. This shared task was partially sup-
ported by Shared Annotated Resources (ShARe)
project NIH 5R01GM090187 and Temporal His-
tories of Your Medical Events (THYME) project
(NIH R01LM010090 and U54LM008748).
References
Giuseppe Attardi, Vitoria Cozza, and Daniele Sartiano.
2014. UniPi: Recognition of mentions of disorders
in clinical text. In Proceedings of the International
Workshop on Semantic Evaluations, Dublin, Ireland,
August.
Olivier Bodenreider and Alexa McCray. 2003. Ex-
ploring semantic groups through visual approaches.
Journal of Biomedical Informatics, 36:414?432.
Keith E. Campbell, Diane E. Oliver, and Edward H.
Shortliffe. 1998. The Unified Medical Language
System: Towards a collaborative approach for solv-
ing terminologic problems. J Am Med Inform Assoc,
5(1):12?16.
Omid Ghiasvand and Rohit J. Kate. 2014. UWM: Dis-
order mention extraction from clinical text using crfs
and normalization using learned edit distance pat-
terns. In Proceedings of the International Workshop
on Semantic Evaluations, Dublin, Ireland, August.
Suwisa Kaewphan, Kai Hakaka1, and Filip Ginter.
2014. UTU: Disease mention recognition and nor-
malization with crfs and vector space representa-
tions. In Proceedings of the International Workshop
on Semantic Evaluations, Dublin, Ireland, August.
Melinda Katona and Rich?ard Farkas. 2014. SZTE-
NLP: Clinical text analysis with named entity recog-
nition. In Proceedings of the International Work-
shop on Semantic Evaluations, Dublin, Ireland, Au-
gust.
Andr?e Leal, Diogo Gonc?alves, Bruno Martins, and
Francisco M. Couto. 2014. ULisboa: Identifica-
tion and classification of medical concepts. In Pro-
ceedings of the International Workshop on Semantic
Evaluations, Dublin, Ireland, August.
12
http://alt.qcri.org/semeval2015/task14
61
Robert Leaman and Graciela Gonzalez. 2008. Ban-
ner: an executable survey of advances in biomedical
named entity recognition. In Pacific Symposium on
Biocomputing, volume 13, pages 652?663.
S?ergio Matos, Tiago Nunes, and Jos?e Lu??s Oliveira.
2014. BioinformaticsUA: Concept recognition in
clinical narratives using a modular and highly ef-
ficient text processing framework. In Proceedings
of the International Workshop on Semantic Evalua-
tions, Dublin, Ireland, August.
Sungrim Moon, Serguei Pakhomov, and Genevieve B
Melton. 2012. Automated disambiguation of
acronyms and abbreviations in clinical texts: Win-
dow and training size considerations. In AMIA Annu
Symp Proc, pages 1310?1319.
David Nadeau and Satoshi Sekine. 2007. A sur-
vey of named entity recognition and classification.
Lingvisticae Investigationes, 30(1):3?26.
Roberto Navigli. 2009. Word sense disambiguation.
ACM Computing Surveys, 41(2):1?69, February.
Ankur Parikh, Avinesh PVS, Joy Mustafi, Lalit Agar-
walla, and Ashish Mungi. 2014. ThinkMiners:
SemEval-2014 task 7: Analysis of clinical text. In
Proceedings of the International Workshop on Se-
mantic Evaluations, Dublin, Ireland, August.
Sameer Pradhan, No?emie Elhadad, Brett South, David
Martinez, Lee Christensen, Amy Vogel, Hanna
Suominen, Wendy W. Chapman, and Guergana
Savova. 2013. Task 1: ShARe/CLEF eHealth
Evaluation Lab 2013. In Working Notes of CLEF
eHealth Evaluation Labs.
Sameer Pradhan, No?emie Elhadad, Brett South, David
Martinez, Lee Christensen, Amy Vogel, Hanna
Suominen, Wendy W. Chapman, and Guergana
Savova. 2014. Evaluating the state of the art in
disorder recognition and normalization of the clin-
ical narrative. In Journal of the American Medical
Informatics Association (to appear).
S. V. Ramanan and P. Senthil Nathan. 2014. RelA-
gent: Entity detection and normalization for diseases
in clinical records: a linguistically driven approach.
In Proceedings of the International Workshop on Se-
mantic Evaluations, Dublin, Ireland, August.
Angus Roberts, Robert Gaizauskas, Mark Hepple,
George Demetriou, Yikun Guo, Ian Roberts, and
Andrea Setzer. 2009. Building a semantically an-
notated corpus of clinical texts. J Biomed Inform,
42(5):950?66.
Mohammed Saeed, C. Lieu, G. Raber, and R.G. Mark.
2002. MIMIC II: a massive temporal ICU patient
database to support research in intelligent patient
monitoring. Comput Cardiol, 29.
Guergana K. Savova, A. R. Coden, I. L. Sominsky,
R. Johnson, P. V. Ogren, P. C. de Groen, and C. G.
Chute. 2008. Word sense disambiguation across
two domains: Biomedical literature and clinical
notes. J Biomed Inform, 41(6):1088?1100, Decem-
ber.
Weiyi Sun, Anna Rumshisky, and
?
Ozlem Uzuner.
2013. Evaluating temporal relations in clinical text:
2012 i2b2 Challenge. Journal of the American Med-
ical Informatics Association, 20(5):806?13.
Hanna Suominen, Sanna Salanter?a, Sumithra Velupil-
lai, Wendy W. Chapman, Guergana Savova,
Noemie Elhadad, Sameer Pradhan, Brett R. South,
Danielle L. Mowery, Gareth J. F. Jones, Johannes
Leveling, Liadh Kelly, Lorraine Goeuriot, David
Martinez, and Guido Zuccon. 2013. Overview of
the ShARe/CLEF eHealth evaluation lab 2013. In
Working Notes of CLEF eHealth Evaluation Labs.
?
Ozlem Uzuner, Brett R South, Shuying Shen, and
Scott L DuVall. 2011. 2010 i2b2/VA challenge on
concepts, assertions, and relations in clinical text.
Journal of the American Medical Informatics Asso-
ciation, 18(5):552?556.
?
Ozlem Uzuner, Andreea Bodnari, Shuying Shen, Tyler
Forbush, John Pestian, and Brett R South. 2012.
Evaluating the state of the art in coreference res-
olution for electronic medical records. Jour-
nal of American Medical Informatics Association,
19(5):786?791, September.
Yaoyun Zhang, Jingqi Wang, Buzhou Tang, Yonghui
Wu, Min Jiang, Yukun Chen, and Hua Xu. 2014.
UTH CCB: A report for SemEval 2014 task 7 anal-
ysis of clinical text. In Proceedings of the Interna-
tional Workshop on Semantic Evaluations, Dublin,
Ireland, August.
62
Proceedings of the 6th Workshop on Ontologies and Lexical Resources (Ontolex 2010), pages 58?66,
Beijing, August 2010
Developing a Biosurveillance Application Ontology for
Influenza-Like-Illness
Mike Conway, John Dowling and Wendy Chapman
Department of Biomedical Informatics
University of Pittsburgh
{conwaym|dowling|wec6}@pitt.edu
Abstract
Increasing biosurveillance capacity is a
public health priority in both the devel-
oped and the developing world. Effec-
tive syndromic surveillance is especially
important if we are to successfully iden-
tify and monitor disease outbreaks in their
early stages. This paper describes the
construction and preliminary evaluation
of a syndromic surveillance orientated ap-
plication ontology designed to facilitate
the early identification of Influenza-Like-
Illness syndrome from Emergency Room
clinical reports using natural language
processing.
1 Introduction and Motivation
Increasing biosurveillance capacity is a public
health priority in both the developed and devel-
oping world, both for the early identification of
emerging diseases and for pinpointing epidemic
outbreaks (Chen et al, 2010). The 2009 Mexican
flu outbreak provides an example of how an out-
break of a new disease (in this case a new vari-
ant of H1N1 influenza) can spend some weeks
spreading in a community before it is recognized
as a threat by public health officials.
Syndromic surveillance is vital if we are to de-
tect outbreaks at an early stage (Henning, 2004;
Wagner et al, 2006). The United States Cen-
ter for Disease Control (CDC) defines syndromic
surveillance as ?surveillance using health-related
data that precede diagnosis and signal a sufficient
probability of a case or outbreak to warrant fur-
ther public health response.?1 That is, the focus of
1www.webcitation.org/5pxhlyaxX
syndromic surveillance is the identification of dis-
ease outbreaks before the traditional public health
apparatus of confirmatory laboratory testing and
official diagnosis can be used. Data sources for
syndromic surveillance have included, over the
counter pharmacy sales (Tsui et al, 2003), school
absenteeism records (Lombardo et al, 2003), calls
to NHS Direct (a nurse led information and advice
service in the United Kingdom) (Cooper, 2007),
and search engine queries (Eysenbach, 2006).
However, in this paper we concentrate on min-
ing text based clinical records for outbreak data.
Clinical interactions between health workers and
patients generate large amounts of textual data ?
in the form of clinical reports, chief complaints,
and so on ? which provide an obvious source of
pre-diagnosis information. In order to mine the
information in these clinical reports we are faced
with two distinct problems:
1. How should we define a syndrome of inter-
est? That is, how are signs and symptoms
mapped to syndromes?
2. Given that we have established such a set
of mappings, how then do we map from the
text in our clinical reports to the signs and
symptoms that constitute a syndrome, given
the high level of terminological variability in
clinical reports.
This paper presents an application ontology that
attempts to address both these issues for the do-
main of Influenza-Like-Illness Syndrome (ILI).
The case definition for ILI, as defined by the
United States Center for Disease Control is ?fever
greater than or equal to 100 degrees Fahrenheit
58
and either cough or sore throat.?2 In contrast
to the CDC?s straightforward definition, the syn-
drome is variously described as a cluster of symp-
toms and findings, including fever and cold symp-
toms, cough, nausea, vomiting, body aches and
sore throat (Scholer, 2004). In constructing an ap-
plication specific syndrome definition for this on-
tology, we used a data driven approach to defining
ILI, generating a list of terms through an analysis
of Emergency Room reports.
The remainder of the paper is divided into five
parts. First, we briefly describe related work, be-
fore going on to report on the ontology develop-
ment process. We then set forth an evaluation of
the ontology with respect to its coverage of terms
in the target domain. We go on to outline areas for
future work, before finally presenting some con-
cluding comments.
2 Related Work
In recent years there has been significant progress
in interfacing lexical resources (in particular
WordNet (Miller, 1995)) and upper level ontolo-
gies (like the Descriptive Ontology for Linguistic
and Cognitive Engineering (DOLCE) (Gangemi
et al, 2002) and the Suggested Upper Merged On-
tology (SUMO) (Niles and Pease, 2003)). How-
ever, as our domain of interest employs a highly
specialized terminology, the use of general lin-
guistic resources like WordNet was inappropriate.
Our work has focused on the representation of
ILI relevant concepts that occur in clinical re-
ports in order to facilitate syndromic surveillance.
While the widely used medical taxonomies and
nomenclatures (for example Unified Medical Lan-
guage System3 and the Systematized Nomencla-
ture of Medicine Clinical Terms4) contain many
of the ILI relevant concepts found in clinical texts,
these general resources do not have the specific re-
lations (and lexical information) relevant to syn-
dromic surveillance from clinical reports. Cur-
rently, there are at least four major terminological
resources available that focus on the public health
domain: PHSkb, SSO, and the BioCaster Ontol-
ogy.
2www.webcitation.org/5q22KTcHx
3www.nlm.nih.gov/research/umls/
4www.ihtsdo.org/snomed-ct/
2.1 PHSkb
The Public Health Surveillance knowledge base
PHSkb (Doyle et al, 2005) developed by the CDC
is a coding system for the communication of no-
tifiable disease5 findings for public health profes-
sionals at the state and federal level in the United
States. There are however several difficulties in
using the PHSkb directly in an NLP orientated
syndromic surveillance context:
1. Syndromic surveillance requires that syn-
dromes and signs are adequately represented.
The PHSkb emphasizes diagnosed diseases.
That is, the PHSKb is focused on post diag-
nosis reporting, when laboratory tests have
been conducted and the presence of a disease
is confirmed. This approach is not suitable
for syndromic surveillance where we seek to
identify clusters of symptoms and signs be-
fore a diagnosis.
2. PHSkb is no longer under active develop-
ment.
2.2 SSO
The Syndromic Surveillance Ontology (SSO)
(Okhmatovskaia et al, 2009) was developed to
address a pressing problem for system develop-
ers and public health officials. How can we inte-
grate outbreak information when every site uses
different syndrome definitions? For instance, if
State X defines sore throat as part of ILI, yet State
Y does not, syndromic surveillance results from
each state will not be directly comparable. When
we apply this example to the wider national scene,
with federal regional and provincial public health
agencies attempting to share data with each other,
and international agencies, we can see the scale of
the problem to be addressed.
In order to manage this data sharing problem,
a working group of eighteen researchers, repre-
senting ten functional syndromic surveillance sys-
tems in the United States (for example, Boston
Public Health Department and the US Depart-
ment of Defense) convened to develop standard
5A notifiable disease is a disease (or by extension, con-
dition) that must, by law, be reported to the authorities for
monitoring purposes. In the United States, examples of noti-
fiable diseases are: Shigellosis, Anthrax and HIV infection.
59
definitions for four syndromes of interest (respi-
ratory, gastro-intestinal, constitutional and ILI)6
and constructed an OWL ontology based on these
definitions. While the SSO is a useful starting
points, there are several reasons why ? on its own
? it is insufficient for clinical report processing:
1. SSO is centered on chief complaints. Chief
complaints (or ?presenting complaints?) are
phrases that briefly describe a patient?s pre-
senting condition on first contact with a med-
ical facility. They usually describe symp-
toms, refrain from diagnostic speculation
and employ frequent abbreviations and mis-
spellings (for example ?vom + naus? for
?vomiting and nausea?). Clinical texts ?
the focus of attention in this paper ? are
full length documents, normally using cor-
rect spellings (even if they are somewhat
?telegraphic? in style). Furthermore, clini-
cal reports frequently list physical findings
(that is, physical signs elicited by the physi-
cian, like, for instance reflex tests) which are
not present in symptom orientated chief com-
plaints.
2. The range of syndromes represented in SSO
is limited to four. Although we are starting
out with ILI, we have plans (and data) to ex-
tend our resource to four new syndromes (see
Section 5 for details of further work).
3. The most distinctive feature of the SSO is
that the knowledge engineering process was
conducted in a face-to-face committee con-
text. Currently, there is no process in place
to extend the SSO to new syndromes, symp-
toms or domains.
2.3 BioCaster Ontology
The BioCaster application ontology was built to
facilitate text mining of news articles for disease
outbreaks in several different Pacific Rim lan-
guages (including English, Japanese, Thai and
Vietnamese) (Collier et al, 2006). However, the
6A demonstration chief complaint classifier based on
SSO is available at:
http://onto-classifier.dbmi.pitt.edu
/onto classify.html
ontology, as it stands, is not suitable for support-
ing text mining clinical reports, for the following
reasons:
1. The BioCaster ontology concentrates on the
types of concepts found in published news
outlets for a general (that is, non medical)
readership. The level of conceptual granular-
ity and degree of terminological sophistica-
tion is not always directly applicable to that
found in documents produced by health pro-
fessionals.
2. The BioCaster ontology, while it does repre-
sent syndromes (for example, constitutional
and hemorrhagic syndromes) and symptoms,
does not represent physical findings, as these
are beyond its scope.
In addition to the application ontologies de-
scribed above, the Infectious Disease Ontology
provides an Influenza component (and indeed
wide coverage of many diseases relevant to syn-
dromic surveillance). In Section 5 we describe
plans to link to other ontologies.
3 Constructing the Ontology
Work began with the identification of ILI terms
from clinical reports by author JD (a board-
certified infectious disease physician with thirty
years experience of clinical practice) supported by
an informatician [author MC]. The term identifi-
cation process involved the project?s domain ex-
pert reading multiple reports,7 searching through
appropriate textbooks, and utilizing professional
knowledge. After a provisional list of ILI con-
cepts had been identified, we compared our list
to the list of ILI concepts generated by the SSO
ILI component (see Section 2.2) and attempted to
reuse SSO concepts where possible. The resulting
ILI concept list consisted of 40 clinical concepts
taken from SSO and 15 new concepts. Clinical
concepts were divided into three classes: Disease
(15 concepts), Finding (21 concepts) and Symp-
tom (19 concepts). Figure 1 shows the clinical
7De-identified (that is, anonymized) clinical reports were
obtained through partnership with the University of Pitts-
burgh Medical Center.
60
concepts covered. As part of our knowledge en-
gineering effort, we identified concepts and as-
sociated relations for several different syndromes
which we plan to add to our ontology at a later
date.8
Early on in the project development process, we
took the decision to design our ontology in such a
way as to maintain consistency with the BioCaster
ontology. We adopted the BioCaster ontology as
a model for three reasons:
1. A considerable knowledge engineering effort
has been invested in BioCaster since 2006,
and both the domain (biosurveillance) and
application area (text mining) are congruent
to our own.
2. The BioCaster ontology has proven utility in
its domain (biosurveillance from news texts)
for driving NLP systems.
3. We plan to import BioCaster terms and re-
lations, and thus settled on a structure that
facilitated this goal.
The BioCaster ontology (inspired by the struc-
ture of EuroWordNet9) uses root terms as interlin-
gual pivots for the multiple languages represented
in the ontology.10 One consequence of following
this structure is that all clinical concepts are in-
stances.11 Additionally, all specified relations are
relations between instances.
Relations relevant to the syndromic surveil-
lance domain generally were identified by our
physician in conjunction with an informatician
(MC). Although some of these relations (like
is bioterrorismDisease) are less relevant
to ILI syndrome, they were retained in order to
maintain consistency with planned future work.
Additionally, we have added links to other ter-
minological resources (for example, UMLS and
Snomed-CT)
8Note that finer granularity was used in the initial knowl-
edge acquisition efforts (for example, we distinguished sign
from physical finding).
9http://www.illc.uva.nl/EuroWordNet/
10Note that we are using root term instead of the equivalent
EuroWordNet term Inter Lingual Index.
11Note that from a formal ontology perspective, concepts
are instantiated in text. For example, ?Patient X presents with
nausea and high fever? instantiates the concepts nausea and
high fever.
Lexical resources and regular expressions are
a vital component of our project, as the ontology
has been built with the public health audience in
mind (in practice, state or city public health IT
personnel). These users have typically had lim-
ited exposure to NLP pipelines, named entity rec-
ognizers, and so on. They require an (almost) ?off
the shelf? product that can easily be plugged into
existing systems for text analysis.
The ontology currently includes 484 English
keywords and 453 English regular expression.
The core classes and relations were developed in
Protege-OWL, and the populated ontology is gen-
erated from data stored in a spreadsheet (using a
Perl script). Version control was managed using
Subversion, and the ontology is available from a
public access Google code site.12 Figure 2 pro-
vides a simplified example of relations for the
clinical concept instance fever.
4 Evaluation
In recent years, significant research effort has
centered on the evaluation of ontologies and
ontology-like lexical resources, with a smorgas-
bord of techniques available (Zhu et al, 2009;
Brank et al, 2005). Yet no single evaluation
method has achieved ?best practice? status for all
contexts. As our ontology is an application on-
tology designed to facilitate NLP in a highly con-
strained domain (that is, text analysis and infor-
mation extraction from clinical reports) the notion
of coverage is vital. There are two distinct ques-
tions here:
1. Can we map between the various textual in-
stantiations of ILI concepts clinical reports
and our ontology concepts? That is, are
the NLP resources available in the ontology
(keywords, regular expressions) adequate for
the mapping task?
2. Do we have the right ILI concepts in our on-
tology? That is, do we adequately represent
all the ILI concepts that occur in clinical re-
ports?
Inspired by Grigonyte et al (2010), we at-
tempted to address these two related issues using
12http://code.google.com/p/ss-ontology
61
ClinicalConcept
Disease
SymptomFinding
Instances:
 - athma
 - bronchiolitis
 - croup
 - ili
 - influenza
 - pertussis
 - pharyngitis
 - pneumonia
 - pneumonitis
 - reactiveAirways
 - respiratorySyncytialVirus
Instances:
 - chill
 - conjunctivitis
 - coryza
 - cyanosis
 - dyspnea
 - elevatedTemperature
 - failureToThrive
 - fever
 - hemoptysis
 - infiltrate
 - lethargy
 - nasalObstruction
 - persistentNonProductiveCough
 - photophobia
 - rales
 - rhinorrhea
 - rigor
 - somnolent
 - throatSwelling
 - wheezing
Instances:
 - anorexia
 - arthralgia
 - asthenia
 - bodyAche
 - coldSymptom
 - cough
 - diarrhea
 - fatigue
 - generalizedMuscleAche
 - headache
 - hoarseness
 - malaise
 - myalgia
 - nausea
 - painOnEyeMovement
 - productiveCough
 - soreThroat
 - substernalDiscomfortOrBurning
 - viralSymptom
 
is_a
is_a
is_a
Figure 1: Clinical concepts.
techniques derived from terminology extraction
and corpus linguistics. Our method consisted of
assembling a corpus of twenty Emergency Room
clinical reports which had been flagged by ex-
perts (not the current authors) as relevant to ILI.
Note that these articles were not used in the initial
knowledge engineering phase of the project. We
then identified the ?best? twenty five terms from
these clinical reports using two tools, Termine and
KWExT.
1. Termine (Frantzi et al, 2000) is a term ex-
traction tool hosted by Manchester Univer-
sity?s National Centre for Text Mining which
can be accessed via web services.13 It uses
a method based on linguistic preprocessing
and statistical methods. We extracted 231
terms from our twenty ILI documents (using
Termine?s default configuration). Then we
identified the twenty-five highest ranked dis-
ease, finding and symptom terms (that is, dis-
carding terms like ?hospital visit? and ?chief
complaint?).
13www.nactem.ac.uk/software/termine/
2. KWExT (Keyword Extraction Tool) (Con-
way, 2010) is a Linux based statistical key-
word extraction tool.14 We used KWExT
to extract 1536 unigrams, bigrams and tri-
grams using the log-likelihood method (Dun-
ning, 1993). The log-likelihood method is
designed to identify n-grams that occur with
the most frequency compared to some ref-
erence corpus. We used the FLOB cor-
pus,15 a one million multi-genre corpus con-
sisting of American English from the early
1990s as our reference corpus. We ranked
all n-grams according to their statistical sig-
nificance and then manually identified the
twenty-five highest ranked disease, finding
and symptom terms.
Term lists derived using the Termine and
KWExT tools are presented in Tables 1 and 2 re-
spectively. For both tables, column two (?Term?)
details each of the twenty-five ?best? terms (with
respect to each term recognition algorithm) ex-
14http://code.google.com/p/kwext/
15www.webcitation.org/5q1aKtnf3
62
Thing
ClinicalConcept
Syndrome
Keyword
Link
Regular
Expression
SymptomFinding
Disease
UmlsLink
English
Keyword
EnglishRegular
Expression
ILI
fever
elevated
Temperature
chill
"febrile"
"fever"
\bfiebre\b
\bfeel.*?\s+hot\b
is_a
is_a
is_a
is_a
is_a
is_a
is_a
is_a
i
is_a
is_a
is_a
instance
instance
instance
instance
instance
fever
instance
instance class
is_a
(class to class)
instance
(instance of a class)
relation
(instance to instance relation)
hasAssociatedSyndrome
hasKeyword
hasKeyword
isSynonymous
hasRegularExpression
hasRegularExpression
hasLink
isRelatedTo
instance
instance
instance
Figure 2: Example of clinical concept ?fever? and its important relations (note the diagram is simpli-
fied).
tracted from our twenty document ILI corpus.
Column three (?Concept?) specifies the concept in
our ontology to which the term maps (that is, the
lexical resources in the ontology ? keywords and
regular expressions ? can map the term in col-
umn two to the clinical concept in column three).
For instance the extracted term slight crackles can
be mapped to the clinical concept RALE using the
keyword ?crackles.? Note that ?-? in column three
indicates that no mapping was possible. Under-
lined terms are those that should be mapped to
concepts in the ontology, but currently are not (ad-
ditional concepts and keywords will be added in
the next iteration of the ontology).
There are two ways that mappings can fail here
(mirroring the two questions posed at the begin-
ning of this section). ?Shortness of breath? should
map to the concept DYSPNEA, but there is no key-
word or regular expression that can bridge be-
tween text and concept. For the terms ?edema?
and ?lymphadenopathy? however, no suitable can-
didate concept exists in the ontology.
5 Further Work
While the current ontology covers only ILI, we
have firm plans to extend the current work along
several different dimensions:
? Developing new relations, to include model-
ing DISEASE ? SYMPTOM, and DISEASE
? FINDING relations (for example TONSIL-
LITIS hasSymptom SORE THROAT).
? Extend the application ontology beyond ILI
to several other syndromes of interest to the
biosurveillance community. These include:
? Rash Syndrome
? Hemorrhagic Syndrome
? Botulic Syndrome
? Neurological Syndrome
? Currently, we have links to UMLS (and also
Snomed-CT and BioCaster). We intend to
extend our coverage to the MeSH vocabu-
lary (to facilitate mining PubMed) and also
the Infectious Disease Ontology.
63
Term Concept
1 abdominal pain -
2 chest pain -
3 urinary tract infection -
4 sore throat SORE THROAT
5 renal disease -
6 runny nose CORYZA
7 body ache MYALGIA
8 respiratory distress PNEUMONIA
9 neck stiffness -
10 yellow sputum -
11 mild dementia -
12 copd -
13 viral syndrome VIRAL SYN.
14 influenza INFLUENZA
15 febrile illness FEVER
16 lung problem -
17 atrial fibrillation -
18 severe copd -
19 mild cough COUGH
20 asthmatic bronchitis BRONCHIOLITIS
21 coronary disease -
22 dry cough COUGH
23 neck pain -
24 bronchial pneumonia PNEUMONIA
25 slight crackles RALE
Table 1: Terms generated using the Termine tool
? Currently evaluation strategies have concen-
trated on coverage. We plan to extend our
auditing to encompass both intrinsic evalu-
ation (for example, have our relations eval-
uated by external health professionals using
some variant of the ?laddering? technique
(Bright et al, 2009)) and extrinsic evaluation
(for example, plugging the application ontol-
ogy into an NLP pipeline for Named Entity
Recognition and evaluating its performance
in comparison to other techniques).
In addition to these ontology development and
evaluation goals, we intend to use the ontology as
a ?gold standard? against which to evaluate au-
tomatic term recognition and taxonomy construc-
tion techniques for the syndromic surveillance do-
main. Further, we seek to integrate the resulting
ontology with the BioCaster ontology allowing
the potential for limited interlingual processing in
priority languages (in the United States, Spanish).
Currently we are considering two ontology in-
tegration strategies. First, using the existing map-
pings we have created between the ILI ontology
and BioCaster to access multi-lingual information
(using OWL datatype properties). Second, fully
Term Concept
1 cough COUGH
2 fever FEVER
3 pain -
4 shortness of breath -
5 vomiting -
6 influenza INFLUENZA
7 pneumonia PNEUMONIA
8 diarrhea DIARRHEA
9 nausea NAUSEA
10 chills CHILL
11 abdominal pain -
12 chest pain -
13 edema -
14 cyanosis CYANOSIS
15 lymphadenopathy -
16 dysuria -
17 dementia -
18 urinary tract inf -
19 sore throat SORE THROAT
20 wheezing WHEEZING
21 rhonchi -
22 bronchitis BRONCHIOLITIS
23 hypertension -
24 tachycardia -
25 respiratory distress PNEUMONIA
Table 2: Terms generated using the KWExT tool
integrating ? that is, merging ? the two on-
tologies and creating object property relations be-
tween them.
For example (using strategy 1), we could move
from the string ?flu? in a clinical report (iden-
tified by the \bflu\b regular expression) to
the ILI ontology concept ili:influenza. In
turn, ili:influenza could be linked (using
a datatype property) to the BioCaster root term
biocaster:DISEASE 378 (which has the la-
bel ?Influenza (Human).?) From the BioCaster
root term, we can ? for example ? generate the
translation ?Gripe (Humano)? (Spanish).
6 Conclusion
The ILI application ontology developed from the
need for knowledge resources for the text mining
of clinical documents (specifically, Emergency
Room clinical reports). Our initial evaluation in-
dicates that we have good coverage of our domain,
although we plan to incrementally work on im-
proving any gaps in coverage through a process of
active and regular updating. We have described
our future plans to extend the ontology to new
syndromes in order to provide a general commu-
64
nity resource to facilitate data sharing and inte-
gration in the NLP based syndromic surveillance
domain. Finally, we actively solicit feedback on
the design, scope and accuracy of the ontology.
Acknowledgments
This project was partially funded by Grant Num-
ber 3-R01-LM009427-02 (NLM) from the United
States National Institute of Health.
References
Brank, J., Grobelnik, M., and Mladenic?, D.
(2005). A Survey of Ontology Evaluation Tech-
niques. In Proceedings of the Conference on
Data Mining and Data Warehouses (SiKDD
2005), pages 166?170.
Bright, T., Furuya, E., Kuperman, G., and Bakken,
S. (2009). Laddering as a Technique for On-
tology Evaluation. In American Medical Infor-
matics Symposium (AMIA 2009).
Chen, H., Zeng, D., and Dang, Y. (2010). Infec-
tious Disease Informatics: Syndromic Surveil-
lance for Public Health and Bio-Defense.
Springer, New York.
Collier, N., Shigematsu, M., Dien, D., Berrero,
R., Takeuchi, K., and Kawtrakul, A. (2006).
A Multilingual Ontology for Infectious Dis-
ease Surveillance: Rationale, Design and Chal-
lenges. Language Resources and Evaluation,
40(3):405?413.
Conway, M. (2010). Mining a Corpus of Bio-
graphical Texts Using Keywords. Literary and
Linguistic Computing, 25(1):23?35.
Cooper, D. (2007). Disease Surveillance: A Pub-
lic Health Informatics Approach, chapter Case
Study: Use of Tele-health Data for Syndromic
Surveillance in England and Wales, pages 335?
365. Wiley, New York.
Doyle, T., Ma, H., Groseclose, S., and Hopkins,
R. (2005). PHSkb: A Knowledgebase to Sup-
port Notifiable Disease Surveillance. BMC Med
Inform Decis Mak, 5:27.
Dunning, T. (1993). Accurate Methods for the
Statistics of Surprise and Coincidence. Com-
putational Linguistics, 19(1):61?74.
Eysenbach, G. (2006). Infodemiology: Track-
ing Flu-Related Searches on the Web for Syn-
dromic Surveillance. In American Medical In-
formatics Association Annual Symposium Pro-
ceedings (AMIA 2006), pages 244?248.
Frantzi, K., Ananiadou, S., and Mima, H.
(2000). Automatic Recognition for Multi-word
Terms. International Journal of Digital Li-
braries, 3(2):117?132.
Gangemi, A., Guarino, N., Masolo, C., Oltramari,
A., and Schneider, L. (2002). Sweetening On-
tologies with DOLCE. In Proceedings of the
13th International Conference on Knowledge
Engineering and Knowledge Management. On-
tologies and the Semantic Web, pages 166?181.
Grigonyte, G., Brochhausen, M., Martin, L., Tsik-
nakis, M., and Haller, J. (2010). Evaluating
Ontologies with NLP-Based Terminologies - A
Case Study on ACGT and its Master Ontol-
ogy. In Formal Ontology in Information Sys-
tems: Proceedings of the Sixth International
Conference (FOIS 2010), pages 331?344.
Henning, K. (2004). What is Syndromic Surveil-
lance? MMWR Morb Mortal Wkly Rep, 53
Suppl:5?11.
Lombardo, J., Burkom, H., Elbert, E., Ma-
gruder, S., Lewis, S. H., Loschen, W., Sari,
J., Sniegoski, C., Wojcik, R., and Pavlin, J.
(2003). A Systems Overview of the Electronic
Surveillance System for the Early Notification
of Community-Based Epidemics (ESSENCE
II). J Urban Health, 80(2 Suppl 1):32?42.
Miller, G. (1995). WordNet: A Lexical Database
for English. Communications of the Associa-
tion for Computing Machinary, 38(11):39?41.
Niles, I. and Pease, A. (2003). Linking Lexicons
and Ontologies: Mapping WordNet to the Sug-
gested Upper Merged Ontology. In Proceed-
ings of the 2003 International Conference on
Information and Knowledge Engineering (IKE
03), pages 23?26.
Okhmatovskaia, A., Chapman, W., Collier, N.,
Espino, J., and Buckeridge, D. (2009). SSO:
The Syndromic Surveillance Ontology. In Pro-
ceedings of the International Society for Dis-
ease Surveillance.
65
Scholer, M. (2004). Development of a Syndrome
Definition for Influenza-Like-Illness. In Pro-
ceedings of American Public Health Associa-
tion Meeting (APHA 2004).
Tsui, F., Espino, J., Dato, V., Gesteland, P., Hut-
man, J., and Wagner, M. (2003). Technical De-
scription of RODS: a Real-Time Public Health
Surveillance System. J Am Med Inform Assoc,
10(5):399?408.
Wagner, M., Gresham, L., and Dato, V. (2006).
Handbook of Biosurveillance, chapter Case
Detection, Outbreak Detection, and Outbreak
Characterization, pages 27?50. Elsevier Aca-
demic Press.
Zhu, X., Fan, J.-W., Baorto, D., Weng, C., and
Cimino, J. (2009). A Review of Auditing
Methods Applied to the Content of Controlled
Biomedical Terminologies. Journal of Biomed-
ical Informatics, 42(3):413 ? 425.
66
Proceedings of the 2012 Workshop on Biomedical Natural Language Processing (BioNLP 2012), pages 56?64,
Montre?al, Canada, June 8, 2012. c?2012 Association for Computational Linguistics
Medical diagnosis lost in translation ? Analysis of uncertainty and negation
expressions in English and Swedish clinical texts
Danielle L Mowery
University of Pittsburgh
200 Meyran Ave
Pittsburgh, PA 15260
dlm31@pitt.edu
Sumithra Velupillai
Stockholm University
164 40 Kista
Stockholm, Sweden
sumithra@dsv.su.se
Wendy W Chapman
University of California San Diego
10100 Hopkins Dr
La Jolla, CA 92093
wwchapman@ucsd.edu
Abstract
In the English clinical and biomedical text do-
mains, negation and certainty usage are two
well-studied phenomena. However, few stud-
ies have made an in-depth characterization
of uncertainties expressed in a clinical set-
ting, and compared this between different an-
notation efforts. This preliminary, qualita-
tive study attempts to 1) create a clinical un-
certainty and negation taxonomy, 2) develop
a translation map to convert annotation la-
bels from an English schema into a Swedish
schema, and 3) characterize and compare two
data sets using this taxonomy. We define
a clinical uncertainty and negation taxonomy
and a translation map for converting annota-
tion labels between two schemas and report
observed similarities and differences between
the two data sets.
1 Introduction and Background
Medical natural language processing techniques are
potentially useful for extracting information con-
tained in clinical texts, such as emergency depart-
ment reports (Meystre et al, 2008). One impor-
tant aspect to take into account when developing ac-
curate information extraction tools is the ability to
distinguish negated, affirmed, and uncertain infor-
mation (Chu et al, 2006). Several research stud-
ies have targeted this problem and created anno-
tation schemas and manually annotated reference
standards for uncertainty and negation occurrence
in news documents (Saur?? and Pustejovsky (2009),
Wiebe et al (2001), Rubin et al (2006)), biomedical
research articles (Wilbur et al (2006), Vincze et al
(2008)), and clinical narratives (Uzuner et al (2011)
and Uzuner et al (2009)). There are encoding tools
developed for automatic identification of uncertainty
and negation in English, such as ConText (Harkema
et al, 2009), which relies on heuristics and keyword
lists, and MITRE?s CARAFE (Clark et al, 2011),
which combines heuristic and statistical techniques.
However, most relevant annotation schemas, ref-
erence standards, and encoding tools are built for
English documents. For smaller languages, such as
Swedish, resources are scarce.
We present a pilot, qualitative study to compare
two different annotation schemas and subsequent
annotated corpora for uncertainty modeling of dis-
order mentions, e.g., signs, symptoms, and diseases,
in clinical texts, for two different languages: English
and Swedish. We compare these annotation schemas
and their instantiation in the two languages in an at-
tempt to gain a deeper understanding of how uncer-
tainty and negation are expressed in different clini-
cal texts with an emphasis on creating a portable un-
certainty and negation application that generalizes
among clinical texts of different languages.
This pilot study is motivated for at least two
reasons. First, little attention has been given to
mapping, characterizing, or comparing annotation
schemas built for different languages or to character-
izing different types of uncertainty expressions and
the intention underlying those expressions. Such
knowledge is needed for building information ex-
traction tools that can accurately identify or track
differential diagnoses over time, particularly when
medical reasoning can be laden with uncertainty
about a disorder?s existence or change over time.
56
Second, building new resources for small lan-
guages is time consuming. Utilizing existing tools
and techniques already developed for one language,
such as English, could be an efficient way of devel-
oping new useful tools for other less exploited lan-
guages, such as Swedish.
Our overall goal is to move towards improving au-
tomatic information extraction from clinical texts by
leveraging language differences and similarities. In
order to address this issue, our aims in this study
are to 1) create a taxonomy for deepened charac-
terization of how uncertainty and negation is ex-
pressed in clinical texts, 2) compare two existing un-
certainty and negation annotation schemas from this
perspective, and 3) compare differences and similar-
ities in expressions of uncertainty and negation be-
tween two languages: English and Swedish.
2 Methods
In this pilot, qualitative comparison study, we used
grounded theory (Strauss and Corbin, 1990) to in-
ductively identify themes that characterize clini-
cal uncertainty and negation expressed in both En-
glish (University of Pittsburgh Medical Center) and
Swedish (Karolinska University Hospital) research
data sets derived from emergency department re-
ports.
2.1 Uncertainty/negation annotation schemas
Two independently developed annotation schemas
were used to annotate disorder mentions in the
clinical texts: a schema developed for English re-
ports (Mowery et al (2012)) and one for Swedish
(Velupillai et al (2011)). Each disorder mention
was pre-annotated and constituted the input to a sep-
arate set of annotators, who assigned values to a set
of attributes defined in the schema. For instance, in
the sentence ?Patient with possible pneumonia.?, an-
notators for the English data set assigned values to
four attributes for the instance of pneumonia:
? Existence(yes, no): whether the disorder was ever present
? AspectualPhase(initiation, continuation, culmination, un-
marked): the stage of the disorder in its progression
? Certainty(low, moderate, high, unmarked): amount of certainty
expressed about whether the disorder exists
? MentalState(yes, no): whether an outward thought or feeling
about the disorder?s existence is mentioned
In the Swedish schema, annotators assigned val-
ues to two attributes:
? Polarity(positive, negative): whether a disorder mention is in the
positive or negative polarity, i.e., affirmed (positive) or negated
(negative)
? Certainty(possibly, probably, certainly): gradation of certainty
for a disorder mention, to be assigned with a polarity value.
2.2 Data Sets
The English data set included 30 de-identified, full-
length emergency department reports annotated with
283 disorders related to influenza-like illnesses by
a board-certified infectious disease physician. Each
disorder was annotated with four attributes ? exis-
tence, aspectual phase, certainty and mental state ?
by two independent annotators (including DM) who
came to consensus after reviewing disagreements.
The Swedish data set included 1,297 assessment
sections from emergency department reports anno-
tated with approx. 2,000 disorders, automatically
marked from a manually created list of approx-
imately 300 unique disorders by two physicians.
The two physicians annotated each disorder mention
with attributes of polarity and certainty. A random
subset of approx. 200 annotated disorder mentions
from the data set were used for this qualitative study.
2.3 Study Process
In order to better understand how physicians de-
scribe uncertainty of the presence or absence of a
disorder, we evaluated the annotations from the two
data sets as follows: 1) created a clinical uncertainty
and negation taxonomy, 2) developed a translation
map for mapping attributes and values from the En-
glish schema into the Swedish schema, and 3) char-
acterized and compared both data sets and languages
using the taxonomy.
To create the uncertainty and negation taxonomy,
we conducted a literature review of recent annota-
tion schemas (e.g. Vincze et al (2008)), assignment
applications (e.g. Uzuner et al (2011), Harkema
et al (2009), Clark et al (2011), Chapman et al
(2011)), and observational studies (Lingard et al,
2003) about uncertainty or negation in the clinical
domain. From our review, we created a clinical tax-
onomy describing notable characteristics of uncer-
tainty and negation, which were added to and re-
fined using grounded theory, by inspecting the dis-
order annotations in our data sets and documenting
57
emerging themes consistent with issues found from
the literature review. For instance, one characteristic
of negation annotations found in the literature and in
our data sets is the existence of a lexical cue indicat-
ing that a disorder is negated, and the lexical cue can
occur before, within, or after the disorder mention.
The characteristics included in the taxonomy repre-
sent features describing the attributes of uncertainty
and negation in the data sets (see Section 3.1).
To develop the translation map between certainty
and negation values from each annotation schema,
authors DM and SV jointly reviewed each annotated
disorder mention from the English data set and as-
signed a Swedish polarity and certainty label, then
devised a map from the English schema into the
Swedish schema.
To characterize and compare manifestations of
uncertainty and negation using annotations from the
two data sets, DM and SV annotated each disorder
mention in both data sets with the features in the
clinical uncertainty and negation taxonomy. In the
English data set, each disorder was annotated by DM
and adjudicated by SV. In the Swedish data set, each
disorder was annotated by SV then translated into
English for adjudication by DM.
3 Results
3.1 Clinical Uncertainty and Negation
Taxonomy
We developed a clinical uncertainty and negation
taxonomy to characterize the linguistic manifesta-
tions of uncertainty and negation in clinical text
(Figure 1). We found three high-level features in
the literature and in our data sets: position of lexical
cue (i.e., position of the lexical expression indicat-
ing uncertainty or negation with respect to the dis-
order), opinion source (i.e. person believing there
is absence, presence, or uncertainty), and evidence
evaluation (i.e., reason for the uncertainty or nega-
tion belief).
Position of lexical cue demonstrated itself in the
data sets in three non-mutually exclusive ways:
? pre-disorder (lexical cue precedes the disorder) ?Patient denies
chest pain.?
? intra-disorder (lexical cue occurs within the name of the disor-
der) ?x-ray...possibly be indicative of pneumonia.?
? post-disorder (lexical cue occurs after the disorder)
?abdominal cramping..is unlikely.?
Opinion source exhibited the following values:
? dictating physician (dictating physician alone expressed pres-
ence, absence, or uncertainty regarding the disorder) ?I suspect
bacterial pneumonia.?
? dictating physician with consultation (dictating physician explic-
itly includes other clinical professional in statement) ?Discussing
with Dr. **NAME**, pneumonia can not be excluded.?
? other clinical care providers (other clinical team members ex-
plicitly stated as expressing presence, absence or uncertainty re-
garding the disorder) ?per patient?s primary doctor, pneumonia
is suspected.?
? patient (patient expressed presence, absence, or uncertainty re-
garding the disorder) ?Pt doesn?t think she has pneumonia.?
? unknown (ambiguous who is expressing presence, absence, or
uncertainty regarding the disorder) ?there was a short episode of
coughing.?
Evidence evaluation includes a modified subset
of values found in the model of uncertainty pro-
posed by Lingard et al (2003) to connote perceived
reasons for the provider uncertainty (and negation)
about the disorder mention as used in our data sets.
? limits of evidence (data limitations for hypothesis testing), one
diagnosis
? evidence contradicts (data contradicts expected hypothe-
sis), ?Blood test normal, but we still think Lyme disease.?
? evidence needed (evidence unavailable to test hypoth-
esis) ?Waiting for x-ray results to determine if it?s a
femur fracture.?
? evidence not convincing, but diagnosis asserted (data
doesn?t fully support proposed hypothesis), ?Slightly el-
evated levels of WBCs suggests infection.?
? limits of evidence, more than one diagnosis
? differential diagnoses enumerated (competing diagnoses
reasoned), ?bacterial infection vs. viral infection.?
? limits in source of evidence (untrusted evidence)
? non-clinical source (from non-provider source), ?Pt can?t
remember if she was diagnosed with COPD.?
? clinical source (from provider source), ?I do not agree
with Dr. X?s diagnosis of meningitis.?
? test source (from test e.g., poor quality), ?We cannot de-
termine from the x-ray if the mass is fluid or a tumor.?
? limitless possibilities (large number of likely diagnoses so diag-
nosis defaulted to most likely), ?This is probably an infection of
some sort.?
? other (no evidence limitation)
? asserting a diagnosis or disorder as affirmed (positive
case), ?Confirms nausea.?
? asserting a diagnosis or disorder as negated (negative
case), ?No vomiting.?
58
Figure 1: Uncertainty and negation taxonomy with features ? Position of lexical cue, Opinion source and Evidence evaluation ?
with corresponding values (nested lines and sub-lines).
3.2 Translation Map
In order to compare annotations between the data
sets, we developed a mapping procedure for convert-
ing the four annotated attribute values from the En-
glish schema into the two annotated attribute values
from the Swedish schema. This mapping procedure
uses two normalization steps, negation and certainty
(see Figure 2).
Using Figure 2, we explain the mapping proce-
dure to convert English annotations into Swedish
annotations. Our steps and rules are applied with
precedence, top down and left to right. For ?I have
no suspicion for bacterial infection for this patient?,
English annotations are Existence(no) AND Aspec-
tualPhase(null) AND Certainty(high) AND Men-
talState(yes), and Swedish annotations are Polar-
ity(negative) AND Certainty(probably). The map-
ping procedure applies two normalization steps,
negation and uncertainty, with the following rules.
The first step is negation normalization to convert
Existence and Aspectual Phase into Polarity anno-
tations. In this example, Existence(no) ? Polar-
ity(negative).
The second step is certainty normalization with
up to two sub steps. For Certainty mapping, in sum-
mary, map English NOT Certainty(unmarked) to
Swedish Certainty level, e.g., Certainty(high)
? Certainty(probably). For MentalState
mapping, if English Certainty(unmarked) AND
MentalState(yes), map to either Swedish Cer-
tainty(probably) OR Certainty(possibly) using
your best judgment; otherwise, map to Cer-
tainty(certainly). For our example sentence,
Certainty mapping was sufficient to map from the
English to the Swedish Certainty levels.
We found that these two schemas were mappable.
Despite the binary mapping splits from English Cer-
tainty(Moderate) ? Swedish Certainty(possibly)
OR Certainty(probably) and judgment calls neces-
sary for MentalState mapping, few annotations were
not easily mapped.
3.3 Characterization of English and Swedish
Data sets
In this study, we characterized our data sets accord-
ing to a clinical uncertainty and negation taxonomy
comprised of three concepts ? position of lexical
cue, opinion source, and evidence evaluation.
3.3.1 Position of lexical cue
In Table 1, we show examples of phrases from each
data set representing the Polarity and Certainty lev-
els in the taxonomy. In our data set, we did not
explicitly annotate markers for the highest certainty
levels in the positive polarity, such as ?definitely
has?. We did not encounter any of these cases in the
59
Figure 2: Map between values for attributes in Swedish and English schemas. Bolded rules indicate the rules used to assign values
to the example sentence (English sentence on top).
data set. We observed that most uncertainty expres-
sions precede a disorder mention. Few expressions
both precede and follow the disorder mention, or
within the disorder mention itself. We observed that
most expressions of uncertainty are conveyed using
positive polarity gradations such as ?probably? and
?possibly?, for example ?likely?, ?appears to have?,
?signs of?. Lexical cues of low levels of certainty in
the negative polarity were rare.
3.3.2 Opinion source
In Table 2, we report examples of the various in-
dividuals ? dictating physician, dictating physician
with consultation, other clinical care providers, pa-
tient, unknown ? that are the source of the belief
state for uncertainty about a disorder. We observed
explicit judgments or mental postulations e.g., ?I
judge? or implied speculations in which the physi-
cian was not the subject and passive expressions
were used e.g., ?patient appears to have?. In cases
of dictating physician with consultation, the physi-
cian speculated about the disorder using references
to other providers consulted to strengthen the as-
sessment e.g., ?Discussing with Dr...?. In cases of
other clinical care providers, there was no owner-
ship on the part of the dictating physician, but of
other members of the clinical care team e.g., ?Con-
sulting Attending (Infection) thinks...?. In cases for
patient, the patient is conveying statements of con-
fusion with respect to self-diagnosing e.g., ?Pat. re-
ports that she finds it difficult to discern...?. We ob-
served no expressions of uncertainty owned by the
patient in the English data set or by a relative in the
Swedish data set. In the unknown case, it is unclear
from the context of the report whether the specu-
lation is on the part of the physician to believe the
symptom reported or the relative unsure about re-
porting the symptoms e.g., ?there was apparently?.
3.3.3 Evidence evaluation
Below we list examples of the different rea-
sons for uncertainties that were identified. Not all
types were observed in both corpora (Not observed).
limits of evidence, one diagnosis
- evidence contradicts ? English: ?Likely upper GI bleed
with elevated bun, but normal h and h.?; Swedish: ?Kon-
sulterar infektionsjour som anser viros vara osannolikt
med tanke pa? normalt leverstatus. (Consulting Attend-
ing (infection) who thinks that virosis is improbable given
normal liver status.)?
- evidence needed ? English: ?chest x-ray was ordered
to rule out TB.?; Swedish: ?Diskuterar med RAH-jour;
vi bo?rjar utredning med CT-skalle med kontrast pa? mis-
stanke om metastaser och na?gon form av epileptiskt anfall
(Discussion with Attendant [CLINIC]; we start inves-
60
Table 1: Common lexical cues and their relative position to the disorder mention: Pre-disorder: uncertainty marker before disor-
der, Intra-disorder: uncertainty marker inside disorder, Post-disorder: uncertainty marker after disorder, }= schema compatibil-
ity/neutral case.
Table 2: Opinion source of uncertainty or negation types with English and Swedish examples.
tigation with CT-brain with contrast on suspicion for
metastasis and some form of epileptic seizure.)?
- evidence not convincing, but diagnosis asserted ? En-
glish: Not observed; Swedish: ?Fo?rmodligen en viros
eftersom man kan se en viss lymfocytopeni i diff (Proba-
bly a virosis since there is some lymphocyte in blood cell
count.)?
limits of evidence, more than one diagnosis
- differential diagnoses enumerated ? English: ?ques-
tionable right-sided increased density on the right side
of the chest x-ray that could possibly be indicative of
a pneumonia versus increased pulmonary vasculature?;
Swedish: ?Fo?refaller neurologiskt, blo?dning? Infarkt?
(Appears neurological, bleeding? Infarction?)?
limits in source of evidence
- non-clinical source ? English: ?I am not convinced that
he is perfectly clear on his situation..?; Swedish: ?Pat
uppger att hon har sva?rt att skilja pa? panika?ngest och an-
dra symtom. (Pat. reports that she finds it difficult to
discern panick disorder from other symptoms...)?
- clinical source ? English: ?there was no definite diagno-
sis and they thought it was a viral syndrome of unknown
type..?; Swedish: Not observed
- test source ? English: ?..confusion was possible related
a TIA without much facial droop appreciated on my
physical exam?; Swedish: ?Ter sig mest sannolikt som
reumatoid artrit ba?de klinisk och lab-ma?ssigt (Seems like
it most probably is rheumatoid arthritis both clinically
and lab-wise.)?
limitless possibilities ? English: ?I think this is probably a
viral problem.?; Swedish: ?Pat bedo?mes ha en fo?rkylning,
troligen virusinfektion. (Patient is evaluated as having a cold,
probably a virus infection.)?
other
61
- asserting dx or disorder as affirmed ? English: ?I sus-
pect that colon cancer is both the cause of the patient?s
bleeding..?; Swedish: Not observed
- asserting dx or disorder as negated ? English: ?...her
fever has abated.?; Swedish: Not observed
In many cases, the local context was sufficient for
understanding the evidential origins for uncertainty.
When a single disorder was mentioned, uncertainty
was due to data insufficient to make a definitive di-
agnosis because it contradicted a hypothesis, was
unavailable, or was not convincing. For instance,
data was to be ordered and the opportunity to inter-
pret it had not presented itself, such as ?..was or-
dered to rule out TB? or ?..start investigation with
CT-brain with contrast..?. In few cases, more than
one diagnosis was being enumerated due to a lim-
itation in the evidence or data gathered e.g., ?Ap-
pears neurological, bleeding? Infarction??. We ob-
served cases in which the source of the evidence pro-
duced uncertainty including both non-clinical and
clinical sources (care providers consulted and tests
produced). In cases of limitless possibilities, the
physician resorted to a common, default diagnosis
e.g., ?probably a virus infection?. Limitations of ev-
idence from a clinical source were not found in the
Swedish data set and few were found in the English
data set. We expect that more examples of this cat-
egory would be found in e.g. radiology reports in
which the quality of the image is a critical factor in
making an interpretation.
4 Discussion and Conclusion
From the resulting clinical taxonomy and charac-
terization, we observe some general differences and
similarities between the two data sets and languages.
The Swedish assessment entries are more verbose
compared to the English medical records in terms
of a more detailed account of the uncertainty and
what is being done by whom to derive a diagnosis
from a disorder mention. This might reflect cultural
differences in how documentation is both produced
and used. Differential diagnoses are often listed with
question marks (???) in the Swedish set, e.g., ?Dis-
order 1? Disorder 2? Disorder 3??, whereas in the
English data set enumerations are either listed or
competing, e.g., ?disorder 1 vs. disorder 2?. De-
spite these differences, there are many similarities
between the two data sets.
Mapping observations from the English schema
into the Swedish schema was not complicated
despite the difference in the modeled attributes.
In most cases, we determined that designating
attribute-value rules for negation and certainty nor-
malization steps was sufficient to accurately map ob-
servations between the language schemas without
changing an observation?s semantics. This finding
suggests that simple heuristics can be used to trans-
late annotations made from English trained tools
into the Swedish schema values.
The majority of the lexical markers are pre-
positioned in both languages, and the majority of
these markers are similar across the two languages,
e.g., ?likely?, ?possible?, ?suspicion for?. How-
ever, inflections and variants are more common in
Swedish, and the language allows for free word or-
der, this relation needs to be studied further. The
default case, i.e. affirmed, or certainly positive, was
rarely expressed through lexical markers.
When it comes to the opinion source of an un-
certainty or negation, we observed a pattern in the
use of passive voice, e.g. ?it was felt?, indicating
avoidance to commitment in a statement. Accurate
extraction of the opinion source of an expression
has important implications for a system that, for in-
stance, tracks the reasoning about a patient case over
time by source. This has been recognized and incor-
porated in other annotation efforts, for example for
news documents (Saur?? and Pustejovsky, 2009). In
the English data set, no cases of self-diagnosing are
found, i.e. a patient owning the expressed uncer-
tainty. In both data sets, an implicit dictating physi-
cian source is most common, i.e. there is no explicit
use of pronouns indicating the opinion holder. In
most cases it is clear that it is the writer?s (i.e. the
dictating physician?s) opinion that is expressed, but
in some cases, a larger context is needed for this
knowledge to be resolved.
Reviewing the evidential origins or reason for ex-
pressed uncertainty, for both the Swedish and En-
glish data sets, the category ?limits of evidence? is
most common. This reflects a clinical reality, where
many disorders require test results, radiology find-
ings and other similar procedures before ascertain-
ing a diagnosis. Although most cases of uncertainty
are manifested and strengthened through a lexical
62
marker, there are also instances where the uncer-
tainty is evident without such explicit markers, e.g.
the ordering of a test may in itself indicate uncer-
tainty.
4.1 Limitations
There are several limitations of this study. The
Swedish data set only contains parts of the medi-
cal record and the English data set is very small.
In the creation of the taxonomy and characteristics,
we have not focused on studying uncertainty lev-
els, i.e. distinctions between ?possibly? and ?prob-
ably?. The values of our taxonomy are preliminary
and may change as we develop the size of our data
set. Additionally, we only studied emergency de-
partment reports. We need to study other report
types to evaluate the generalizability of the taxon-
omy.
The two compared languages both origin from the
same language family (Germanic), which limits gen-
eralizability for other languages. Furthermore, the
definitions of disorders in the two sets differ to some
extent, i.e., English disorders are related to specific
influenza-like illnesses and Swedish to more general
disorders found in emergency departments.
4.2 Comparison to related work
Annotation schemas and reference standards for un-
certainty and negation have been created from dif-
ferent perspectives, for different levels and pur-
poses. The BioScope Corpus, for instance, contains
sentence-level uncertainty annotations with token-
level annotations for speculation and negation cues,
along with their linguistic scope (Vincze et al,
2008). In Wilbur et al (2006), five qualitative di-
mensions for characterizing biomedical articles are
defined, including levels of certainty. In the 2010
i2b2/VA Challenge on concepts, assertions and re-
lations in clinical texts, medical problem concepts
were annotated. The assertion task included six an-
notation classes (present, absent, possible, hypothet-
ical, conditional, not associated with the patient),
to be assigned to each medical problem concept
(Uzuner et al, 2011). Vincze et al (2011) present
a quantitative comparison of the intersection of two
English corpora annotated for negation and specula-
tion (BioScope and Genia Event) from two different
perspectives (linguistic and event-oriented).
We extend these schemas by characterizing the
underlying meaning and distinctions evident by the
linguistic expressions used to indicate uncertainty
and negation in the clinical domain and by exploring
the relationship between uncertainty and negation,
through an analysis and comparison of two differ-
ent annotation schemas. However, this study is not a
proposal for mapping to these schemas or others.
From an application perspective, uncertainty and
negation handling have been included in rule-based
systems such as NegEx and ConText, applied on dis-
order mentions. In Chapman et al (2011), a gener-
alized version of ConText is presented, with uncer-
tainty values (probably, definitely) linked to either a
positive or negative assertion, with an added indeter-
minate value. A previous study has shown promis-
ing results for adapting NegEx to Swedish (Skepp-
stedt, 2011), indicating that further extensions and
adaptations between the two languages for e.g. un-
certainty modeling should be viable. Machine-
learning based approaches outperform rule-based
for assertion classification according to results pre-
sented in Uzuner et al (2009). A machine-learning
approach was also used in the top performing sys-
tem in the 2010 i2b2/VA Challenge assertion task
(de Bruijn et al, 2011).
4.3 Implications and future work
With uncertainty lexicons for both Swedish and En-
glish, we hypothesize that we will be able to ex-
tend ConText to handle uncertainties in English as
well as in Swedish. This enables both improve-
ments over the existing system and the possibilities
of further comparing system performances between
languages. We will also experiment with machine-
learning approaches to detect and annotate uncer-
tainty and negation. We plan to extend both data
sets, the English data set using semi-automatically
translated disorders marked in the Swedish data set
to encode new disorder mentions, and the Swedish
data set by extracting the full medical records, thus
creating a larger set for comparison. We will extend
the taxonomy as needed e.g., syntactic and semantic
patterns, and investigate how to integrate the clini-
cal taxonomy to inform ConText by providing more
granular descriptions of the motivation behind the
uncertainty, thus bringing us closer to natural lan-
guage understanding.
63
Acknowledgments
For the English and Swedish data sets, we obtained
approval from the University of Pittsburgh IRB and
the Regional Ethical Review Board in Stockholm
(Etikpro?vningsna?mnden i Stockholm). The study is
part of the Interlock project, funded by the Stock-
holm University Academic Initiative and partially
funded by NLM Fellowship 5T15LM007059. Lex-
icons and probabilities will be made available and
updated on the iDASH NLP ecosystem under Re-
sources: http://idash.ucsd.edu/nlp/natural-language-
processing-nlp-ecosystem.
References
B. E. Chapman, S. Lee, H. Peter Kang, and W. W. Chap-
man. 2011. Document-level Classification of CT Pul-
monary Angiography Reports Based on an Extension
of the ConText Algorithm. Journal of Biomedical In-
formatics, 44:728?737.
D. Chu, J.N. Dowling, and WW Chapman. 2006. Eval-
uating the Effectiveness of Four Contextual Features
in Classifying Annotated Clinical Conditions in Emer-
gency Department Reports. In AMIA Annu Symp Proc,
pages 141?145.
C. Clark, J. Aberdeen, M. Coarr, D. Tresner-Kirsh,
B. Wellner, A. Yeh, and L. Hirschman. 2011. MITRE
system for Clinical Assertion Status Classification. J
Am Med Inform Assoc, 11(18):563?567.
B. de Bruijn, C. Cherry, S. Kiritchenko, J. Martin, and
X. Zhu. 2011. Machine-learned Solutions for Three
Stages of Clinical Information Extraction: The State of
the Art at i2b2 2010. Journal of the American Medical
Informatics Association, 18:557?562.
H. Harkema, J. N. Dowling, T. Thornblade, and W. W.
Chapman. 2009. ConText: An Algorithm for De-
termining Negation, Experiencer, and Temporal Status
from Clinical Reports. Journal of Biomedical Infor-
matics, 42:839?851.
L. Lingard, K. Garwood, C. F. Schryer, and M. M. Spaf-
ford. 2003. A Certain Art of Uncertainty: Case Pre-
sentation and the Development of Professional Iden-
tity. Social science medicine, 56(3):603?616.
S. M. Meystre, G. K. Savova, K. C. Kipper-Schuler, and
John E. Hurdle. 2008. Extracting Information from
Textual Documents in the Electronic Health Record: A
Review of Recent Research. IMIA Yearbook of Medi-
cal Informatics 2008. 47 Suppl 1:138-154.
D. Mowery, P. Jordan, J.M. Wiebe, H. Harkema, and
W.W. Chapman. 2012. Semantic Annotation of Clini-
cal Text: A Pilot Study. Unpublished.
V. L. Rubin, E. D. Liddy, and N. Kando. 2006. Cer-
tainty Identification in Texts: Categorization Model
and Manual Tagging Results. In Computing Affect and
Attitutde in Text: Theory and Applications. Springer.
R. Saur?? and J. Pustejovsky. 2009. FactBank: A Corpus
Annotated with Event Factuality. Language Resources
and Evaluation, 43(3):227?268?268, September.
M. Skeppstedt. 2011. Negation Detection in Swedish
Clinical Text: An Adaptation of NegEx to Swedish.
Journal of Biomedical Semantics, 2(Suppl. 3):S3.
A. L. Strauss and J. Corbin. 1990. Basics of Qual-
itative Research: Grounded Theory Procedures and
Techniques. Sage.
O?. Uzuner, X. Zhang, and T. Sibanda. 2009. Ma-
chine Learning and Rule-based Approaches to Asser-
tion Classification. Journal of the American Medical
Informatics Association, 16(1):109?115.
O?. Uzuner, B. R. South, S. Shen, and S. L. DuVall. 2011.
2010 i2b2/VA Challenge on Concepts, Assertions, and
Relations in Clinical Text. JAMIA, 18(5):552?556.
S. Velupillai, H. Dalianis, and M. Kvist. 2011. Factual-
ity Levels of Diagnoses in Swedish Clinical Text. In
A. Moen, S. K. Andersen, J. Aarts, and P. Hurlen, ed-
itors, Proc. XXIII International Conference of the Eu-
ropean Federation for Medical Informatics (User Cen-
tred Networked Health Care), pages 559 ? 563, Oslo,
August. IOS Press.
V. Vincze, G. Szarvas, R. Farkas, G. Mo?ra, and J. Csirik.
2008. The BioScope Corpus: Biomedical Texts An-
notated for Uncertainty, Negation and Their Scopes.
BMC Bioinformatics, 9(S-11).
V. Vincze, G. Szarvas, G. M?ora, T. Ohta, and R. Farkas.
2011. Linguistic Scope-based and Biological Event-
based Speculation and Negation Annotations in the
BioScope and Genia Event Corpora. Journal of
Biomedical Semantics, 2(Suppl. 5):S8.
J. Wiebe, R. Bruce, M. Bell, M. Martin, and T. Wilson.
2001. A Corpus Study of Evaluative and Specula-
tive Language. In Proceedings of the Second SIG-
dial Workshop on Discourse and Dialogue - Volume
16, SIGDIAL ?01, pages 1?10, Stroudsburg, PA, USA.
Association for Computational Linguistics.
J. W. Wilbur, A. Rzhetsky, and H. Shatkay. 2006.
New Directions in Biomedical Text Annotation: Def-
initions, Guidelines and Corpus Construction. BMC
Bioinformatics, 7:356+, July.
64
Proceedings of the 2012 Workshop on Biomedical Natural Language Processing (BioNLP 2012), pages 130?139,
Montre?al, Canada, June 8, 2012. c?2012 Association for Computational Linguistics
A Prototype Tool Set to Support Machine-Assisted Annotation 
Brett R. South1,2, Shuying Shen1,2, Jianwei Leng2, Tyler B. Forbush4,  Scott L. DuVall3,4, Wendy W. Chapman5 Departments of 1Biomedical Informatics, 2Internal Medicine, and 3Radiology University of Utah, Salt Lake City, Utah, USA  4IDEAS Center SLCVA Healthcare System, Salt Lake City, Utah, USA  5University of California, San Diego, Division of Biomedical Informatics, La Jolla, California, USA brett.south@hsc.utah.edu,  shuying.shen@hsc.utah.edu, jianwei.leng@utah.edu,  tyler.forbush@utah.edu, scott.duvall@utah.edu,  wendy.w.chapman@gmail.com 
Abstract 
Manually annotating clinical document corpora to generate reference standards for Natural Language Processing (NLP) sys-tems or Machine Learning (ML) is a time-consuming and labor-intensive endeavor. Although a variety of open source annota-tion tools currently exist, there is a clear opportunity to develop new tools and assess functionalities that introduce efficiencies into the process of generating reference standards. These features include: man-agement of document corpora and batch as-signment, integration of machine-assisted verification functions, semi-automated cu-ration of annotated information, and sup-port of machine-assisted pre-annotation. The goals of reducing annotator workload and improving the quality of reference standards are important considerations for development of new tools. An infrastruc-ture is also needed that will support large-scale but secure annotation of sensitive clinical data as well as crowdsourcing which has proven successful for a variety of annotation tasks. We introduce the Ex-tensible Human Oracle Suite of Tools  (eHOST) http://code.google.com/p/ehost that provides such functionalities that when coupled with server integration offer an end-to-end solution to carry out small or large scale as well as crowd sourced anno-tation projects. 1 Introduction Supervised learning methods benefit from a ref-erence standard that is used to train and evaluate 
the performance of Natural Language Processing (NLP) or Machine Learning (ML) systems for information extraction and classification. Ideal-ly, generating a reference standard involves the review of more than one annotator with an ac-companying adjudication step to resolve dis-crepancies (Roberts et al, 2007; Roberts et al, 2009). However, manual annotation of clinical, texts is time-consuming, expensive, and requires considerable effort. Reducing the time and costs required for manual annotation could be achieved by developing new tools that integrate methods to more efficiently annotate clinical texts and integrate a management interface that allows administration of large or small scale an-notation projects. Such a tool could also inte-grate methods to pre-annotate entities such as noun phrases or clinical concepts mapped to a standard vocabulary. Efficiencies could be real-ized via reduction in human workload, modifica-tion of annotation tasks that could include crowd sourcing, and implementation of machine-assisted approaches.  Typically annotation of clinical texts requires human reviewers to identify information classes of interest called ?markables?. These tasks may also require reviewers to assign attributes to those information classes and build relations between spans of annotated text. For each anno-tation task there may be one or many types of markables and each markable class may be asso-ciated with one or more spans of text and may include single or even multiple tokens. These tasks may occur simultaneously, or may also be done in different steps and by multiple review-ers. Furthermore, these activities require written guidelines that clearly explicate what infor-
130
mation to annotate, specifics about each marka-ble class, such as how much information to in-clude in annotated spans, or syntactic rules to provide further guidance on annotated spans. Annotation tasks may benefit by incorporating rules or guidelines as part of the annotation task itself in the form of machine-assisted verifica-tion. There are many annotation tools available, and the majority of them were designed for lin-guistic or gene annotation. Linguistic annotation tools such as Callisto and WordFreak are stand-alone clients suitable for small to medium scale tasks where collaborative effort is not empha-sized. Functionality integrated with eHOST was inspired by existing features of these tools with the intent of providing a more efficient means of reference standard generation in a large collabo-rative environment. One annotation tool called Knowtator, a plug-in for Prot?g? (Musen, M.A., et al 1995) developed by Ogren (2006) has been widely used to annotate clinical texts and gener-ate reference standards. However, no stand-alone system exists that can provide end users with the ability to manually or semi-automatically edit, curate, and easily navigate annotated information. There are also specific functionalities that are missing from open source annotation tools in the clinical and biomedical domains that would introduce efficiencies into manual annotation tasks. These functionalities include: annotation of clinical texts along with database storage of stand-off annotations, the ability to interactively annotate texts in a way that allows users to react to either pre-annotations imported from NLP or ML systems or use exact string matching across an active corpus to identify similar spans of text to those already annotated. Additionally, these systems do not generally support crowd sourcing, ma-chine-assisted pre-annotation or verification ap-proaches integrated directly with the annotation tool. This paper discusses development of a proto-type open source system designed to provide functionality that supports these activities and offers an end-to-end solution when coupled with server integration to reduce both annotator and administrative workload associated with refer-ence standard. We introduce the Extensible Hu-
man Oracle Suite of Tools (eHOST) created with these expectations in mind.  2 Background Our goal for these development efforts was to build a prototype open source system that im-proves upon existing tools by including new functions and refining capabilities available in other annotation tools. The resulting GUI inter-face provides a means of visually representing annotated information, its attributes, and rela-tions between annotated mentions. These efforts also focused integrating various machine-assisted approaches that can be used to easily curate and navigate annotated information with-in a document corpus, pre-annotate information, and also verify annotations based on rules checks that correspond with annotation guide-lines or linguistic and syntactic cues.   The eHOST provides basic functionality in-cluding manual annotation of information repre-senting markable classes and assignment of information attributes and relationships between markable classes. Annotations exported from eHOST are written using the XML format as Knowtator thus allowing integration of inputs and outputs to and from Knowtator and indirect-ly to Prot?g? 3.3.1. Coupling eHOST with an integrated server package such as the one under development by the VA Informatics and Com-puting Infrastructure (VINCI) called the Chart Administration Server for Patient Review (CASPR) provides one method of increasing efficiencies for small or large-scale annotation efforts that could also include crowd sourcing.  2.1 System Features Development In the domains of computational linguistics and biomedical informatics various approaches that can be used to improve annotation efficiencies have been evaluated for a variety of tasks in-cluding information extraction and classifica-tion. While several methods may help reduce the time and costs required to create reference standards, one of the simplest approaches may include integrating machine-assisted methods to pre-annotate relevant spans of text allowing the annotator to add missing annotations, modify spans, or delete spurious annotations. Neveol (2011) evaluated use of automatic semantic pre-
131
annotation of PubMed queries. This study showed a significant reduction in the number of required annotations when using pre-annotations, reduction in annotation time with higher inter-annotator agreement. Pre-annotation using simple approaches such as regular expres-sions coupled with dictionaries (South et al, 2010a) based on the UMLS as a source of lexi-cal knowledge (Friedman, 2001) and  pre-annotation of information representing protected health information (South et al, 2010b). In both cases finding that annotators preferred particular types of pre-annotation over others, but im-provements in reference standard quality occur when pre-annotation was provided. Others have explored the use of third party tools for the pre-annotation task for UMLS concepts (Savova, 2008) and pre-annotation using an algorithmic approach (Chapman, et al, 2007) combined with domain expert annotations reused for temporal relation annotation (Mowery, 2008). Savova (2008) suggests limited utility when a third party tool is used for pre-annotation and Mowery (2008) suggest that even with domain expert pre-annotations, additional features are required to discern temporality. Finally, Fort and Sagot (2008) evaluated using pre-annotation for part-of-speech tagging on the Penn Tree bank corpus and demonstrate a gain in quality and annotation speed even with a not so accurate tagger. Semi-automated curation has been explored as a means to build custom dictionaries for in-formation extraction tasks (Riloff, 1993). More recently this approach was spurred on by the BioCreative II competition (Yeh et al, 2003). Alex et al, (2008), explored the use of NLP-assisted text mining to speed up curation of bi-omedical texts. Settles et al, (2008) estimates true labeling costs and provides a review of ac-tive and interactive learning approaches as a means of providing labels and reducing the cost of obtaining training data (Settles, 2010). Alt-hough eHOST does not yet include an active learning module it does provide one means of interactive annotation so these are important considerations for future development efforts.  In the biomedical informatics domain crowd sourcing has been evaluated as part of the 2009 i2b2 Medication Challenge (Uzuner, 2010). Nowak and Ruger (2010) provide estimates of annotation reliability from crowd sourcing of 
image annotation. Hsueh et al, (2009) provide estimates of the quality of crowd sourcing for sentiment classification using both experts and non-expert annotators. In all three cases the re-sulting annotation set was of comparable quality to that derived from expert annotators. Wang et al, (2008) make general recommendations for best approaches to crowd sourcing that include closer interactions between human and machine methods in ways that more efficiently connect domain expertise with the annotation task.  Subsequent sections in this paper walk the reader through the various basic and advanced features eHOST provides. These features have been developed in a way that provides flexibility to add additional modules that support im-provements in annotation workflow and effi-ciency for a variety of annotation scenarios applicable to computational linguistics and bio-medical informatics. Some of these features may be useful for crowd-sourced efforts whereas oth-ers may simply represent an improvement in the way annotation is visualized or how manual ef-fort can be reduced. Figures in this paper use a set of synthetic clinical documents and a demon-stration annotation project based on the 2010 and 2011 i2b2/VA annotation tasks as examples available from http://code.google.com/p/ehost. 2.2 Systems Architecture The eHOST is a client application that can run on most operating systems that supports Java including, most Microsoft Windows x86/x64 platforms, Apple Mac OS X, Sun Solaris, and Linux. The application uses standardized for-mats including a file folder system, and struc-tured XML inputs and outputs. These capabilities also support integration with other open source tools for annotation and knowledge management including Knowtator and Prot?g?. An Extract-Transform-Load process (ETL) is used by the system to import concept infor-mation from different sources, such as XML or Prot?g? PINS files. These inputs sources are normalized for loading into eHOST. All data that exists in the data pool can be transformed into various output formats. Raw input data doc-uments in a single text file or sequential text files in a file folder system. Information representing an annotation in-
132
cluding concept attributes such as the annotated span, attributes, and relationships between anno-tations are inserted into a common data pool us-ing a dynamic structured storage space. The data pool ensures that eHOST has capabilities to add new functions easily without making major changes to system architecture.  2.3 Annotation Project Workspace In eHOST each project has its own user assigned  workspace that includes an annotation schema and document corpus. Annotation schema can also be imported from an existing Prot?g? PINS file. Project settings can be inherited from exist-ing projects for similar annotations tasks using eHOST. Other workspace functions include quickly switching between up to five of the most recently used workspaces. A workspace can be assigned for each annotation layer or document batch. In these situations, an annotator would receive a pre-compiled project that specifies all settings including any text documents and the annotation schema. Defining a workspace is a particularly useful function in situations where annotations may be crowd sourced and there may be multiple layers of annotation that are potentially fielded to many annotators. 2.3.1 Corpus Management For any annotation task, the end user must man-age the document corpus, which can originate from a server or a file folder system that con-tains individual text files. Using the stand-alone eHOST client tool, corpus management is ac-complished via the current workspace (Figure 1). When the user initializes a new project, docu-ments are placed in a ?corpus? folder that is as-sociated with the newly created annotation project. All text files, are copied to the ?corpus? folder at the time of workspace assignment. Therefore, there is no risk of deleting the origi-nal documents associated with each new annota-tion project. This feature makes distribution of projects easier, because of the consistency be-tween the workspace, corpus assignment and annotation output folders. For crowd-sourced projects eHOST can be integrated with a backend server via web services using an admin-istrative module called CASPR.    
 Figure 1. eHOST corpus management  2.3.2 Viewer/Editor Panels Figure 2 shows an annotation for ?full body pain?, (shown with black bar above and below the active annotation) and information for that annotation including the annotated span, the class assignment and an assertion for the 2010 and 2011 i2b2/VA Challenge annotation tasks (Uzuner et al, 2011 and Uzuner et al, 2012). The result editor tab and its associated panels serve as the central place for basic annotation features. These functionalities include: assigning an annotator, creating new annotations or adjust-ing annotated spans of text and assignment of attributes or creating relationships between an-notated spans of text. Other functions in the re-sults editor tab include navigation between documents in the active corpus, resizing the text displayed in the document viewer, and ?save? and ?save as? functions that assigns a path for XML output files. The end user can easily re-move all annotations in a document or remove specific kinds of annotations by deleting a ?markable? class as well as remove attributes, and relationships between all annotations.  From the navigator screen in the stand-alone eHOST client tool a user can build annotation schema specifying markable classes, their asso-ciated attributes, and any allowed relationships. The navigator interface allows the user to review all annotated spans either within the current document or across the entire document corpus, toggle the view of each class on or off, see counts for all unique annotations and all annota-tions for each class, and choose a class for a fast annotate mode.  An annotation editor panel allows the user to view more detailed information for each selected 
133
annotation. This includes the time stamp of when the annotation was created, annotator as-signment, comments on the annotation and class, attribute and relationship information.  Annotations can be created using several ap-proaches from the result editor. In the normal mode, a class assignment window appears when the user selects a span of text, new annotations are generated by selecting any one of the marka-ble classes.  Activating a ?one click annotate? mode is possible by checking the box next to a class of markables. Under this mode, any text 
selected is automatically annotated as that mark-able class. This feature improves task efficien-cies when categories of markables are low or annotations of the same category cluster in small sections. Keyboard shortcuts have also been in-tegrated with eHOST to reduce annotator click burden and dependence on a mouse. These shortcuts are available for tasks such as modifi-cation of spans, deletion of annotations, and nav-igation between annotations.  
 
 Figure 2. Example annotations using the eHOST interface  2.3.3 Server Integration Annotation projects of any scale benefit from an automated means of building and distributing batches of texts to annotators, managing stand-off XML files generated from annotation tasks or written directly to a database and getting and submitting assignments with minimal user input. Coupling eHOST with server components that comply with the web services API defined for eHOST allows these functionalities. The CASPR module under development by VINCI provides a means to automate the administration of annotation efforts that could include crowd-sourced annotation projects.  Clicking on the sync assignments tab in the eHOST client (Figure 2) brings up a GUI that 
allows annotators to sync with a server location, enter credentials, see documents assigned, and designate documents as on hold, in process, or completed. When a user syncs and gets assign-ments from CASPR, a project folder is created that contains the annotation schema, text docu-ments, and annotations sent from the server.  The CASPR module allows an annotator to open the project and complete their task without need-ing to manage files or folders.  Once completed, annotations can be synced to the server, and the next assignment will be loaded.  The CASPR module allows iterative distribution of annota-tion batches without sending large sets of docu-ments to annotators that may contain sensitive data, decreasing the risk of breaches in privacy and data security. 
134
2.3.4 Additional Features The document viewer panel employs visual cues to display relationships between annotations us-ing color coding representing a parent and child node and line indicator between them showing the relationship. An ?annotation profiler? to the right of the scroll bar shows the density of anno-tations color-coded to their categories, as well as relative to their positions in the document. This type of data visualization is useful to see the rel-
ative location of annotations within a single document or across an en tire document corpus.  An adjudication mode is also included in the stand-alone eHOST client that allows difference matching and side-by-side comparison of anno-tations for efficient adjudication of discrepancies between annotations. Standard reporting metrics can be calculated including Inter-Annotator Agreement (IAA), Recall, Precision and F1-Measure.  
 Figure 3. eHOST adjudication mode showing discrepant annotations between annotators A7 and B4
In Adjudication mode discrepant annotations are shown using a wavy red underline in the editor window and by a red bolded outline in a side by side two panel view between the annotation edi-tor and comparator (Figure 3). These metrics and comparison tables between annotator results on the same documents can be output as HTML formatted reports that can be used by an adjudi-cator to quickly identify discrepancies between 
annotators (Figure 4). These reports and the edi-tor window display can also be used to quickly train annotators on new clinical domains using a reference standard created by domain experts for training purposes. Using these features error analysis can also be done by importing outputs from an NLP system that have been converted into the XML format used by eHOST. 
    
135
Figure 4. HTML Formatted report showing discrepant annotations between annotators A7 and B4  3 Advanced eHOST Features  There are also other more advanced features that have been integrated with eHOST. These in-clude an ?Oracle? mode that allows semi-automated annotation of similar spans of text across a document corpus, a means to easily and quickly curate annotated spans of text to create custom dictionaries, and machine-assisted pre-annotation integrated with the annotation tool itself.  3.1 Oracle Mode Also implemented with eHOST is an ?Oracle? mode which uses exact string matching allowing the user to annotate all spans of text that are 
identical to a new annotation. The oracle lists where these candidate annotations are found along with the surrounding context. The annota-tor can then accept or reject candidate spans an-notated with the same markable class. Oracle mode can run within the current document or across the entire document corpus. This type of functionality is useful for annotation tasks that may involve identifying and marking spans of text that are repetitive or follow the same format For example, the 2011 i2b2/VA annotation task in which annotation of pronominal information was required for co-reference resolution (Figure 5). 
 
 Figure 5. Example annotations generated using the eHOST ?Oracle? mode 
136
3.2 Semi-Automated Curation and    Dictionary Management Using the navigator window users can navigate to all annotations in either a single document or across an entire document corpus (Figure 6). The end user can curate annotations directly, create classes on the fly, or add attributes to an-notations found from the navigator pane. These functions also allow users to easily identify spu-rious annotations introduced from machine-assisted approaches correct misclassification errors, and quickly curate all annotations within a single document or across an entire document corpus. 
 Figure 6. Semi-Automated curation within the  document corpus  One task often associated with development of NLP systems involves manually creating or en-hancing some existing representation of lexical knowledge that can be used as a domain specific dictionary. Using eHOST users can export anno-tations to create a dictionary of terms, phrases, or individual tokens that have been identified by human annotators and assigned to markable in-formation classes. Once curated, annotated in-formation can be exported as a new dictionary. User created dictionaries can be integrated with 
a database or exported and used in the creation of some ontologic representation of information using Prot?g?. Output from a dictionary manager is in the form of a delimited text file and can therefore be modified to fit any standardized information model or used to pre-annotate sub-sequent document batches. 3.3 Machine-Assisted Pre-Annotation An interface is provided in eHOST that can be used for machine-assisted pre-annotation of documents in the active project corpus using either dictionaries or regular expressions based approaches. Users can import libraries of regular expressions or build their own regular expres-sions using a custom regular expression builder. Users can build and modify dictionaries created as part of annotation tasks that may include semi-automated curation steps. Dictionaries and regular expressions can also be coupled with the ConText algorithm (Chapman et al, 2007) to identify concept attributes such as negation, ex-periencer, and temporality. Pre-annotations de-rived from some external third party source such as an NLP system written as Knowtator XML outputs may also be imported into eHOST or passed to eHOST using CASPR. Computational speed required for pre-annotation can be improved by selecting an op-tion to use an internal statistical dictionary in-dexing function. This feature is particularly useful in situations where pre-annotation dic-tionaries are extremely large, such as where a subset of some standard vocabulary may be used to pre-annotate documents. Using the result edi-tor and its associated functions annotators can add missed annotations, modifying existing an-notations and delete spurious annotations. Han-dling pre-annotations in this way allows troubleshooting and error analysis of NLP sys-tem outputs imported into eHOST that can be shown to a reviewer in context and also facili-tates interactive annotator training.   3.4 Machine-Assisted Verification One of the more innovative features integrated with eHOST is the ability to verify and produce recommendations that help human annotators comply with syntactic and lexical rules that are specified by annotation task guidelines. Ma-
137
chine-Assisted verification is most useful when used on lexical or syntax rules to ensure that candidate phrases generated by automated sys-tems are similar to those marked by humans. These rules rely more on adherence to patterns than on decision-making, so the strengths of human review with machine approaches to semi-automated verification can be leveraged. When identifying medical concepts, it is common that noun phrases are marked as candidates. The de-termination of how much of a noun phrase to mark (inclusion of articles, adjectives, noun-modifiers, prepositional phrases) and at what granularity (simple nouns or complex noun phrases) may vary with each project. The verifier allows portions of an annotation guideline to be programmed into rules that check for consistency. Rules check whether a word appears within a user-defined window before and after an annotation. Each rule can be linked to text that describes why the annotation was flagged. Annotators are then provided sugges-tions on the correct span based on the rule. Us-ing the surrounding text, the guideline text, and the suggestion, the annotator can determine the final span for an annotation. These machine-assisted verifier functions help support reference standard generation by providing the context of annotations that seem to fail syntactic and lexi-cal rules while allowing human annotators to focus on domain expertise required to identify and classify information found in clinical texts.  Conclusion Our prototype system provides functionalities that have been created to more efficiently sup-port reference standard generation including ma-chine-assisted annotation approaches. It is our hope that these system features will serve as the basis for the further development efforts that will be part of an enterprise level system. Out-puts of such an annotation tool could be used as inputs for pipeline NLP systems or as one com-ponent of a common workbench of tools used for clinical NLP development tasks.  We have implemented and tested eHOST for the 2010 and 2011 i2b2/VA challenge annota-tion tasks and annotation projects for the Con-sortium for Healthcare Informatics Research (CHIR). The stand-alone eHOST client tool is 
available from http://code.google.com/p/ehost along with a demonstration project, a users guide, API documentation, and source code. The eHOST/CASPR interfaces will be used to sup-port a large-scale crowd sourced annotation task used for annotation of disorders, temporal ex-pressions, uncertainty, and negation along with data standardization. These efforts will include more rigorous analysis and usability assessment of eHOST/CASPR for crowd sourcing and other small and large-scale annotation projects.  Acknowledgments Support and funding was provided by the VA Salt Lake City HealthCare System and the VA Consortium for Healthcare Informatics Research (CHIR), VA HSR HIR 08-374, the VA Infor-matics and Computing Infrastructure (VINCI), VA HIR 08-204, and NIH Grant U54 HL 108460 for integrating Data for Analysis, An-nonymization and Sharing (iDASH), NIGMS 7R01GM090187.  References  Beatrice Alex, Claire Grover, Barry Haddow, Mijail Kabadjov, Ewan Klein, Michael Matthews, Stuart Roebuck, Richard Tobin, and Xinglong Wang. 2008. Assisted curation: does text mining really help? In: Proceedings of the Pacific Symposium on Biocomputing.  Wendy W. Chapman, David Chu, John N. Dowling. 2007. ConText: An Algorithm for Identifying Contextual Features from Clinical Text. In: ACL-07 2007.  Carol Friedman, Hongfang Liu, Lyudmila Shagina, Stephen Johnson, George Hripcsak. 2001. Evaluat-ing the UMLS as a source of lexical knowledge for medical language processing. In: Proc AMIA Symp, 2001: 189-93. Karen Fort and Saggot B. 2010. Influence of Pre-Annotation on POS-tagged Corpus Development. In: Proceedings of the Fourth Linguistic Annota-tion Workshop. ACL 2010: 56-63.  Pei-Yun Hsueh, Prem Melville, Vikas Sindhwani. 2009. Data Quality from Crowdsourcing: A study of Annotation Selection Criteria. In: Proceedings of the NAACL HLT Workshop on Active Learning for Natural Language Processing. June 2009: 27-35.  Danielle Mowery, Henk Harkema, Wendy W. Chap-man. 2008. Temporal Annotation of Clinical Text. In: ACL-08 2008. 
138
Mark A. Musen, John Gennari, Henrik Eriksson, Samson W. Tu, and Angel R. Puerta. 1995. PROTEGE-II: computer support for development of intelligent systems from libraries of compo-nents. In: Medinfo 1995.  Aur?lie N?v?ol, Rezarta Islamaj-Do?an, Zhiyong Lu. 2011. Semi-automatic semantic annotation of PubMed queries: a study on quality, efficiency, satisfaction. In: J Biomed Inform. 2011 Apr; 44(2):310-8.  Stefanie Nowak and Stefan Ruger. 2010. How Relia-ble are Annotations via Crowdsourcing? A Study about Inter-Annotator Agreement for Multi-label Image Annotation. In: MIR 10  2010. Philip V. Ogren. 2006. Knowtator a protege plug-in for annotated corpus construction. In: Proceedings of the 2006 Conference of the North American Chapter of the Association for Computational Lin-guistics on Human Language Technology, 2006: 273-5. Philip V. Ogren, Guergana K. Savova, Christopher G. Chute. 2008. Constructing Evaluation Corpora for Automated Clinical Named Entity Recogni-tion. In: Proceedings of the sixth international conference on Language Resources and Evalua-tion LREC 2008: 3143-3150. Angus Roberts, Robert Gaizauskas, Mark Hepple, Neil Davis, George Demetriou, Yikun Guo, Jay Kola, Ian Roberts, Andrea Setzer, Archana Tapuria, Bill Wheeldin. 2007. The CLEF corpus: semantic annotation of clinical text. In: AMIA An-nu Symp Proc, 625-9. Angus Roberts, Robert Gaizauskas, Mark Hepple, George Demetriou, Yikun Guo, Ian Roberts, An-drea Setzer. 2009. Building a semantically anno-tated corpus of clinical texts. In: J Biomed Inform, 42(5): 950-66.  Ellen Riloff. 1993. Automatically constructing a dic-tionary for information extraction tasks.  In: Pro-ceedings of the Eleventh National Conference on Artificial Intelligence, 811-816.  
Burr Settles, Mark Craven, and Lewis Friedland. 2008. Active Learning with Real Annotation Costs. In: Proceedings of the NIPS Workshop on Cost-Sensitive Learning. 2008. Burr Settles. 2009. Active Learning Literature Sur-vey. In: Computer Sciences Technical Report 1648. University of Wisconsin-Madison. 2009. Brett R. South, Shuying Shen, F. Jeff Friedlin, Mat-thew H. Samore, and Stephane M. Meystre. 2010. Enhancing Annotation of Clinical Text using Pre-Annotation of Common PHI. In: AMIA Annu Symp Proc. 2010.  Brett R. South, Shuying Shen, Robyn Barrus, Scott L. DuVall, Ozlem Uzuner, and Charlene Weir. 2011. Qualitative analysis of workflow modifications used to generate the reference standard for the 2010 i2b2/VA challenge. In: AMIA Annu Symp Proc. 2011.  ?zlem Uzuner, Imre Solti, Fei Xia, and Eithon Cadag. 2010. Community annotation experiment for ground truth generation for the i2b2 medica-tion challenge. In: J Am Med Inform Assoc, 2010. 17(5):519-23. ?zlem Uzuner, Brett R. South, Shuying Shen, and Scott L. DuVall. 2011. 2010 i2b2/VA challenge on concepts, assertions, and relations in clinical text. In: JAMIA 18(5): 552-556. ?zlem Uzuner, Andreea Bodnari, Shuying Shen, Tyler B. Forbush, John Pestian, and Brett R. South. 2012. Evaluating the state of the art in co-reference resolution for electronic medical records. In: JAMIA doi: 10.1136/amiajnl-2011-000784. Aobo Wang, Cong Duy Vu Hoang, and Min-Yen Kan. 2010. Perspectives on Crowdsourcing Anno-tations for Natural Language Processing. In: CSIDM Project No. CSIDM-200805. Alexander S. Yeh, Lynette Hirschman, and Alexan-der A. Morgan. 2003. Evaluation of text data min-ing for database curation: Lessons learned from the KDD challenge cup. In: Bioinformatics, 19(Suppl 1): i331?339, 2003. 
 
139
Proceedings of the 2014 Workshop on Biomedical Natural Language Processing (BioNLP 2014), pages 54?58,
Baltimore, Maryland USA, June 26-27 2014.
c?2014 Association for Computational Linguistics
Generating Patient Problem Lists from the ShARe Corpus using
SNOMED CT/SNOMED CT CORE Problem List
Danielle Mowery
Janyce Wiebe
University of Pittsburgh
Pittsburgh, PA
dlm31@pitt.edu
wiebe@cs.pitt.edu
Mindy Ross
University of California
San Diego
La Jolla, CA
mkross@ucsd.edu
Sumithra Velupillai
Stockholm University
Stockholm, SE
sumithra@dsv.su.se
Stephane Meystre
Wendy W Chapman
University of Utah
Salt Lake City, UT
stephane.meystre,
wendy.chapman@utah.edu
Abstract
An up-to-date problem list is useful for
assessing a patient?s current clinical sta-
tus. Natural language processing can help
maintain an accurate problem list. For in-
stance, a patient problem list from a clin-
ical document can be derived from indi-
vidual problem mentions within the clin-
ical document once these mentions are
mapped to a standard vocabulary. In
order to develop and evaluate accurate
document-level inference engines for this
task, a patient problem list could be gen-
erated using a standard vocabulary. Ad-
equate coverage by standard vocabularies
is important for supporting a clear rep-
resentation of the patient problem con-
cepts described in the texts and for interop-
erability between clinical systems within
and outside the care facilities. In this
pilot study, we report the reliability of
domain expert generation of a patient
problem list from a variety of clinical
texts and evaluate the coverage of anno-
tated patient problems against SNOMED
CT and SNOMED Clinical Observation
Recording and Encoding (CORE) Prob-
lem List. Across report types, we learned
that patient problems can be annotated
with agreement ranging from 77.1% to
89.6% F1-score and mapped to the CORE
with moderate coverage ranging from
45%-67% of patient problems.
1 Introduction
In the late 1960?s, Lawrence Weed published
about the importance of problem-oriented medi-
cal records and the utilization of a problem list
to facilitate care provider?s clinical reasoning by
reducing the cognitive burden of tracking cur-
rent, active problems from past, inactive problems
from the patient health record (Weed, 1970). Al-
though electronic health records (EHR) can help
achieve better documentation of problem-specific
information, in most cases, the problem list is
manually created and updated by care providers.
Thus, the problem list can be out-of-date con-
taining resolved problems or missing new prob-
lems. Providing care providers with problem list
update suggestions generated from clinical docu-
ments can improve the completeness and timeli-
ness of the problem list (Meystre and Haug, 2008).
In recent years, national incentive and standard
programs have endorsed the use of problem lists
in the EHR for tracking patient diagnoses over
time. For example, as part of the Electronic Health
Record Incentive Program, the Center for Medi-
care and Medicaid Services defined demonstra-
tion of Meaningful Use of adopted health infor-
mation technology in the Core Measure 3 objec-
tive as ?maintaining an up-to-date problem list of
current and active diagnoses in addition to histor-
ical diagnoses relevant to the patients care? (Cen-
ter for Medicare and Medicaid Services, 2013).
More recently, the Systematized Nomenclature of
Medicine Clinical Terms (SNOMED CT) has be-
come the standard vocabulary for representing and
documenting patient problems within the clinical
record. Since 2008, this list is iteratively refined
four times each year to produce a subset of gen-
eralizable clinical problems called the SNOMED
CT CORE Problem List. This CORE list repre-
sents the most frequent problem terms and con-
cepts across eight major healthcare institutions in
the United States and is designed to support in-
teroperability between regional healthcare institu-
tions (National Library of Medicine, 2009).
In practice, there are several methodologies ap-
plied to generate a patient problem list from clin-
ical text. Problem lists can be generated from
coded diagnoses such as the International Statis-
tical Classification of Disease (ICD-9 codes) or
54
concept labels such as Unified Medical Language
System concept unique identifiers (UMLS CUIs).
For example, Meystre and Haug (2005) defined 80
of the most frequent problem concepts from coded
diagnoses for cardiac patients. This list was gen-
erated by a physician and later validated by two
physicians independently. Coverage of coded pa-
tient problems were evaluated against the ICD-9-
CM vocabulary. Solti et al. (2008) extended the
work of Meystre and Haug (2005) by not limit-
ing the types of patient problems from any list
or vocabulary to generate the patient problem list.
They observed 154 unique problem concepts in
their reference standard. Although both studies
demonstrate valid methods for developing a pa-
tient problem list reference standard, neither study
leverages a standard vocabulary designed specifi-
cally for generating problem lists.
The goals of this study are 1) determine how
reliably two domain experts can generate a pa-
tient problem list leveraging SNOMED CT from
a variety of clinical texts and 2) assess the cover-
age of annotated patient problems from this corpus
against the CORE Problem List.
2 Methods
In this IRB-approved study, we obtained the
Shared Annotated Resource (ShARe) corpus
originally generated from the Beth Israel Dea-
coness Medical Center (Elhadad et al., un-
der review) and stored in the Multiparameter
Intelligent Monitoring in Intensive Care, ver-
sion 2.5 (MIMIC II) database (Saeed et al.,
2002). This corpus consists of discharge sum-
maries (DS), radiology (RAD), electrocardiogram
(ECG), and echocardiogram (ECHO) reports from
the Intensive Care Unit (ICU). The ShARe cor-
pus was selected because it 1) contains a variety of
clinical text sources, 2) links to additional patient
structured data that can be leveraged for further
system development and evaluation, and 3) has en-
coded individual problem mentions with semantic
annotations within each clinical document that can
be leveraged to develop and test document-level
inference engines. We elected to study ICU pa-
tients because they represent a sensitive cohort that
requires up-to-date summaries of their clinical sta-
tus for providing timely and effective care.
2.1 Annotation Study
For this annotation study, two annotators - a physi-
cian and nurse - were provided independent train-
ing to annotate clinically relevant problems e.g.,
signs, symptoms, diseases, and disorders, at the
document-level for 20 reports. The annotators
were given feedback based on errors over two it-
erations. For each patient problem in the remain-
ing set, the physician was instructed to review the
full text, span the a problem mention, and map the
problem to a CUI from SNOMED-CT using the
extensible Human Oracle Suite of Tools (eHOST)
annotation tool (South et al., 2012). If a CUI did
not exist in the vocabulary for the problem, the
physician was instructed to assign a ?CUI-less? la-
bel. Finally, the physician then assigned one of
five possible status labels - Active, Inactive, Re-
solved, Proposed, and Other - based on our pre-
vious study (Mowery et al., 2013) to the men-
tion representing its last status change at the con-
clusion of the care encounter. Patient problems
were not annotated as Negated since patient prob-
lem concepts are assumed absent at a document-
level (Meystre and Haug, 2005). If the patient
was healthy, the physician assigned ?Healthy - no
problems? to the text. To reduce the cognitive bur-
den of annotation and create a more robust refer-
ence standard, these annotations were then pro-
vided to a nurse for review. The nurse was in-
structed to add missing, modify existing, or delete
spurious patient problems based on the guidelines.
We assessed how reliably annotators agreed
with each other?s patient problem lists using inter-
annotator agreement (IAA) at the document-level.
We evaluated IAA in two ways: 1) by problem
CUI and 2) by problem CUI and status. Since
the number of problems not annotated (i.e., true
negatives (TN)) are very large, we calculated F1-
score as a surrogate for kappa (Hripcsak and Roth-
schild, 2005). F1-score is the harmonic mean of
recall and precision, calculated from true posi-
tive, false positive, and false negative annotations,
which were defined as follows:
true positive (TP) = the physician and nurse prob-
lem annotation was assigned the same CUI
(and status)
false positive (FP) = the physician problem anno-
tation (and status) did not exist among the
nurse problem annotations
55
false negative (FN) = the nurse problem anno-
tation (and status) did not exist among the
physician problem annotations
Recall =
TP
(TP + FN)
(1)
Precision =
TP
(TP + FP )
(2)
F1-score =
2
(Recall ? Precision)
(Recall + Precision)
(3)
We sampled 50% of the corpus and determined
the most common errors. These errors with
examples were programmatically adjudicated
with the following solutions:
Spurious problems: procedures
solution: exclude non-problems via guidelines
Problem specificity: CUI specificity differences
solution: select most general CUIs
Conflicting status: negated vs. resolved
solution: select second reviewer?s status
CUI/CUI-less: C0031039 vs. CUI-less
solution: select CUI since clinically useful
We split the dataset into about two-thirds train-
ing and one-third test for each report type. The re-
maining data analysis was performed on the train-
ing set.
2.2 Coverage Study
We characterized the composition of the reference
standard patient problem lists against two stan-
dard vocabularies SNOMED-CT and SNOMED-
CT CORE Problem List. We evaluated the cover-
age of patient problems against the SNOMED CT
CORE Problem List since the list was developed
to support encoding clinical observations such as
findings, diseases, and disorders for generating pa-
tient summaries like problem lists. We evaluated
the coverage of patient problems from the corpus
against the SNOMED-CT January 2012 Release
which leverages the UMLS version 2011AB. We
assessed recall (Eq 1), defining a TP as a patient
problem CUI occurring in the vocabulary and a
FN as a patient problem CUI not occurring in the
vocabulary.
3 Results
We report the results of our annotation study on
the full set and vocabulary coverage study on the
training set.
3.1 Annotation Study
The full dataset is comprised of 298 clinical doc-
uments - 136 (45.6%) DS, 54 (18.1%) ECHO,
54 (18.1%) RAD, and 54 (18.1%) ECG. Seventy-
four percent (221) of the corpus was annotated by
both annotators. Table 1 shows agreement overall
and by report, matching problem CUI and prob-
lem CUI with status. Inter-annotator agreement
for problem with status was slightly lower for all
report types with the largest agreement drop for
DS at 15% (11.6 points).
Report Type CUI CUI + Status
DS 77.1 65.5
ECHO 83.9 82.8
RAD 84.7 82.8
ECG 89.6 84.8
Table 1: Document-level IAA by report type for problem
(CUI) and problem with status (CUI + status)
We report the most common errors by frequency
in Table 2. By report type, the most common er-
rors for ECHO, RAD, and ECG were CUI/CUI-
less, and DS was Spurious Concepts.
Errors DS ECHO RAD ECG
SP 423 (42%) 26 (23%) 30 (35%) 8 (18%)
PS 139 (14%) 31 (27%) 8 (9%) 0 (0%)
CS 318 (32%) 9 (8%) 8 (9%) 14 (32%)
CC 110 (11%) 34 (30%) 37 (44%) 22 (50%)
Other 6 (>1%) 14 (13%) 2 (2%) 0 (0%)
Table 2: Error types by frequency - Spurious Problems (SP),
Problem Specificity (PS), Conflicting status (CS), CUI/CUI-
less (CC)
3.2 Coverage Study
In the training set, there were 203 clinical docu-
ments - 93 DS, 37 ECHO, 38 RAD, and 35 ECG.
The average number of problems were 22?10 DS,
10?4 ECHO, 6?2 RAD, and 4?1 ECG. There
are 5843 total current problems in SNOMED-CT
CORE Problem List. We observed a range of
unique SNOMED-CT problem concept frequen-
cies: 776 DS, 63 ECHO, 113 RAD, and 36 ECG
56
by report type. The prevalence of covered prob-
lem concepts by CORE is 461 (59%) DS, 36
(57%) ECHO, 71 (63%) RAD, and 16 (44%)
ECG. In Table 3, we report coverage of patient
problems for each vocabulary. No reports were
annotated as ?Healthy - no problems?. All reports
have SNOMED CT coverage of problem mentions
above 80%. After mapping problem mentions to
CORE, we observed coverage drops for all report
types, 24 to 36 points.
Report Patient Annotated with Mapped to
Type Problems SNOMED CT CORE
DS 2000 1813 (91%) 1335 (67%)
ECHO 349 300 (86%) 173 (50%)
RAD 190 156 (82%) 110 (58%)
ECG 95 77(81%) 43 (45%)
Table 3: Patient problem coverage by SNOMED-CT and
SNOMED-CT CORE
4 Discussion
In this feasibility study, we evaluated how reliably
two domain experts can generate a patient problem
list and assessed the coverage of annotated patient
problems against two standard clinical vocabular-
ies.
4.1 Annotation Study
Overall, we demonstrated that problems can be re-
liably annotated with moderate to high agreement
between domain experts (Table 1). For DS, agree-
ment scores were lowest and dropped most when
considering the problem status in the match crite-
ria. The most prevalent disagreement for DS was
Spurious problems (Table 2). Spurious problems
included additional events (e.g., C2939181: Mo-
tor vehicle accident), procedures (e.g., C0199470:
Mechanical ventilation), and modes of administra-
tion (e.g., C0041281: Tube feeding of patient) that
were outside our patient problem list inclusion cri-
teria. Some pertinent findings were also missed.
These findings are not surprising given on average
more problems occur in DS and the length of DS
documents are much longer than other document
types. Indeed, annotators are more likely to miss
a problem as the number of patient problems in-
crease.
Also, status differences can be attributed to mul-
tiple status change descriptions using expressions
of time e.g., ?cough improved then? and modal-
ity ?rule out pneumonia?, which are harder to
track and interpret over a longer document. The
most prevalent disagreements for all other doc-
ument types were CUI/CUI-less in which iden-
tifying a CUI representative of a clinical obser-
vation proved more difficult. An example of
Other disagreement was a sidedness mismatch
or redundant patient problem annotation. For
example, C0344911: Left ventricular dilatation
vs. C0344893: Right ventricular dilatation or
C0032285: Pneumonia was recorded twice.
4.2 Coverage Study
We observed that DS and RAD reports have higher
counts and coverage of unique patient problem
concepts. We suspect this might be because other
document types like ECG reports are more likely
to have laboratory observations, which may be
less prevalent findings in CORE. Across document
types, coverage of patient problems in the corpus
by SNOMED CT were high ranging from 81%
to 91% (Table 3). However, coverage of patient
problems by CORE dropped to moderate cover-
ages ranging from 45% to 67%. This suggests that
the CORE Problem List is more restrictive and
may not be as useful for capturing patient prob-
lems from these document types. A similar report
of moderate problem coverage with a more restric-
tive concept list was also reported by Meystre and
Haug (2005).
5 Limitations
Our study has limitations. We did not apply a tra-
ditional adjudication review between domain ex-
perts. In addition, we selected the ShARe corpus
from an ICU database in which vocabulary cover-
age of patient problems could be very different for
other domains and specialties.
6 Conclusion
Based on this feasibility study, we conclude that
we can generate a reliable patient problem list
reference standard for the ShARe corpus and
SNOMED CT provides better coverage of patient
problems than the CORE Problem List. In fu-
ture work, we plan to evaluate from each ShARe
report type, how well these patient problem lists
can be derived and visualized from the individ-
ual disease/disorder problem mentions leveraging
temporality and modality attributes using natu-
ral language processing and machine learning ap-
proaches.
57
Acknowledgments
This work was partially funded by NLM
(5T15LM007059 and 1R01LM010964), ShARe
(R01GM090187), Swedish Research Council
(350-2012-6658), and Swedish Fulbright Com-
mission.
References
Center for Medicare and Medicaid Services. 2013.
EHR Incentive Programs-Maintain Problem
List. http://www.cms.gov/Regulations-and-
Guidance/Legislation/EHRIncentivePrograms/
downloads/3 Maintain Problem ListEP.pdf.
Noemie Elhadad, Wendy Chapman, Tim OGorman,
Martha Palmer, and Guergana. Under Review
Savova. under review. The ShARe Schema for
the Syntactic and Semantic Annotation of Clinical
Texts.
George Hripcsak and Adam S. Rothschild. 2005.
Agreement, the F-measure, and Reliability in In-
formation Retrieval. J Am Med Inform Assoc,
12(3):296?298.
Stephane Meystre and Peter Haug. 2005. Automation
of a Problem List using Natural Language Process-
ing. BMC Medical Informatics and Decision Mak-
ing, 5(30).
Stephane M. Meystre and Peter J. Haug. 2008. Ran-
domized Controlled Trial of an Automated Problem
List with Improved Sensitivity. International Jour-
nal of Medical Informatics, 77:602?12.
Danielle L. Mowery, Pamela W. Jordan, Janyce M.
Wiebe, Henk Harkema, John Dowling, and
Wendy W. Chapman. 2013. Semantic Annotation
of Clinical Events for Generating a Problem List. In
AMIA Annu Symp Proc, pages 1032?1041.
National Library of Medicine. 2009. The
CORE Problem List Subset of SNOMED-
CT. Unified Medical Language System 2011.
http://www.nlm.nih.gov/research/umls/SNOMED-
CT/core subset.html.
Mohammed Saeed, C. Lieu, G. Raber, and Roger G.
Mark. 2002. MIMIC II: a massive temporal ICU
patient database to support research in intelligent pa-
tient monitoring. Comput Cardiol, 29.
Imre Solti, Barry Aaronson, Grant Fletcher, Magdolna
Solti, John H. Gennari, Melissa Cooper, and Thomas
Payne. 2008. Building an Automated Problem List
based on Natural Language Processing: Lessons
Learned in the Early Phase of Development. pages
687?691.
Brett R. South, Shuying Shen, Jianwei Leng, Tyler B.
Forbush, Scott L. DuVall, and Wendy W. Chapman.
2012. A prototype tool set to support machine-
assisted annotation. In Proceedings of the 2012
Workshop on Biomedical Natural Language Pro-
cessing, BioNLP ?12, pages 130?139. Association
for Computational Linguistics.
Lawrence Weed. 1970. Medical Records, Med-
ical Education and Patient Care: The Problem-
Oriented Record as a Basic Tool. Medical Pub-
lishers: Press of Case Western Reserve University,
Cleveland: Year Book.
58
