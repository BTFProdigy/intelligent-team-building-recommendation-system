Extraction of Chinese Compound Words - 
An Experimental Study on a Very Large Corpus 
Jian Zhang 
Department of Computer Science and 
Technology of Tsinghua University, China 
ajian@sl000e.cs.tsinghua.edu.cn 
Jianfeng Gao, Ming Zhou 
Microsoft Research China 
{jfgao, mingzhou }@microsoft.corn 
Abstract 
This paper is to introduce a statistical 
method to extract Chinese compound words 
from a very large corpusL This method is 
based on mutual information and context 
dependency. Experimental results show that 
this method is efficient and robust 
compared with other approaches. We also 
examined the impact of different parameter 
settings, corpus size and heterogeneousness 
on the extraction results. We finally present 
results on information retrieval to show the 
usefulness of extracted compounds. 
1 Introduction 
Almost all techniques to statistical anguage 
processing, including speech recognition, machine 
translation and information retrieval, are based on 
words. Although word-based approaches work 
very well for western languages, where words are 
well defined, it is difficult to apply to Chinese. 
Chinese sentences are written as characters strings 
with no spaces between words. Therefore, words 
in Chinese are actually not well marked in 
sentences, and there does not exist a commonly 
accepted Chinese lexicon. 
Furthermore, since new compounds (words 
formed with at least wo characters) are constantly 
created, it is impossible to list them exhaustively 
in a lexicon. Therefore, automatic extraction of 
compounds is an important issue. Traditional 
extraction approaches used rules. However, 
compounds extracted in this way are not always 
desirable. So, human effort is still required to find 
the preferred compounds from a large compound 
l This work was done while the author worked for 
Microsoft Research China as a visiting student. 
candidate list. Some statistical approaches to 
extract Chinese compounds from corpus have 
been proposed (Lee-Feng Chien 1997, WU Dekai 
and Xuanyin XIA 1995, Ming-Wen Wu and Keh- 
Yih Su 1993) as well, but almost all experiments 
are based on relatively small corpus, it is not clear 
whether these methods till work well with large 
corpus. 
In this paper, we investigate statistical 
approaches to Chinese compound extraction from 
very large corpus by using statistical features, 
namely mutual information and context 
dependency. There are three main contributions in
this paper. First, we apply our procedure on a very 
large corpus while other experiments were based 
on small or medium size corpora. We show that 
better esults can be obtained with a large corpus. 
Second, we examine how the results can be 
influenced by parameter settings including mutual 
information and context dependency restrictions. 
It turns out that mutual information mainly affects 
precision while context dependency affects the 
count of extracted items. Third, we test the 
usefulness of the extracted compounds for 
information retrieval. Our experimental results on 
IR show that the new compounds have a positive 
effect on IR. 
The rest of this paper is structured as follows. In 
section 2, we describe the techniques we used. In 
section 3, we present several sets of experimental 
results. In section 4, we outline the related works 
as well as their results. Finally, we give our 
conclusions in section 5. 
2 Technique description 
Statistical extraction of Chinese compounds has 
been used in (Lee-Feng Chien 1997)(WU Dekai 
and Xuanyin XIA 1995) and (Ming-Wen Wu and 
Keh-Yih Su 1993). The basic idea is that a 
132 
Chinese compound should appear as a stable 
sequence in corpus. That is, the components in the 
compound are strongly correlated, while the 
components lie at both ends should have low 
correlations with otiter words. 
The method consists of two steps. At fast, a fist 
of candidate compounds i extracted from a very 
large corpus by using mutual information. Then, 
context dependency is used to remove undesirable 
compounds. In what follows, we will describe 
them in more detail. 
2.1 Mutual Information 
According to our study on Chinese corpora, 
most compounds are of length less than 5 
characters. The average length of words in the 
segmented-corpus i of approximately 1.6 
characters. Therefore, only word bi-gram, tn'- 
gram, and quad-gram in the corpus are of interest 
to us in compound extraction. 
We use a criterion, called mutual inform~on, 
to evaluate the correlation of different components 
in the compound. Mutual information Ml(x,y) of a 
bi-gram (x, y) is estimated by: 
Ml(x ,y )  = f (x ,y )  
f (x )  + f (y ) -  f (x ,y )  
Where f(x) is the occurrence frequency of word 
x in the corpus, and fix, y) is the occurrence 
frequency of the word pair (x,y) in the corpus. The 
higher the value of MI is, the more likely x and y 
are to form a compound. 
The mutual information MI(x,y,z) of tri-gram 
(x,y,z) is estimated by: 
Ml(x,  y, z) = f (x ,  y, z) 
f (x)  + f (y)  + f ( z ) -  f(x,  y, z) 
The estimation of mutual information of quad- 
grams is similar to that of tri-grams. The extracted 
compounds should be of higher value of MI than a 
pre-set threshold. 
2.2 Context Dependency 
Figure 1 
The extracted Chinese compounds hould be 
complete. That is, we should generate a whole 
word, not a part of it. For example, 
~,~-~-~t'~J(missi le defense plan) is a 
complete word, and -~.~0-1~J~ (missile defense) is
not, although both have relatively high value of 
mutual information. 
Therefore, we use another feature, called 
context dependency. The contexts of the word 
l~(defense)  are illustrated by figure 1. 
A compound X has NO left context dependency 
if 
LSize -~ L I > tl or 
f (ctX)  MaxL = MAX ~ - -  < t2 
f (X )  
Where tl, t2 are threshold value, j\[.) is 
frequency, L is the set of left adjacent strings of X, 
tz~L and ILl means the number of unique left 
adjacent strings. Similarly, a compound X has NO 
right context dependency if 
RSize ~ R l> t3 or 
f ( /~)  < t4 MaxR = MAX a f ( X ) 
Where tl, t2, t3, t4 are threshold value, f(.) is 
frequency, R is the set of right adjacent strings of 
X, tiER and \[R I means the number of unique left 
adjacent strings. 
The extracted complete compounds should have 
neither left nor fight context dependency. 
3 Experimental results 
In our experiments, three corpora were used to 
test the performance of the presented approach. 
These corpora re described in table 1. Corpus A 
consists of local news with more than 325 million 
characters. Corpus B consists of documents from 
different domains of novel, news, technique 
report, etc., with approximately 650 million 
characters. Corpus C consists of People's Daily 
news and Xinhua news from TREC5 and TREC6 
(Harman and Voorhees, 1996) with 75 million 
characters. 
Table 1: Characteristics of Corpora 
!Corpus Source Size(char#) 
Corpus political, economic news 325 M 
A 
Corpus Corpus A + novels + 650 M 
B technique reports, etc. 
133 
'cCOrpus \[TREC 5/6 Chinese 75 M 
,, ,coMus I I 
In the first experiment, we test the perfomaance 
of our method on corpus A, which is homogeneity 
in style. We then use corpus B in the second 
experiment totest if the method works as well on 
the corpus that is heterogeneity n style. We also 
use different parameter settings in order to figure 
out the best combination of the two statistical 
features, i.e. mutual information and context 
dependency. In the third experiment, we apply the 
results of the method to information retrieval 
system. We extract new compounds on corpus C, 
and add them to the indexing lexicon, and we 
achieve a higher average precision-recall. In all 
experiments, corpora re segmented automatically 
into words using a lexicon consisting of 65,502 
entries. 
3.1 Compounds Extraction from Homogeneous 
Corpus 
Corpus A contains political and economic news. 
In this series of tests, we gradually loosen the 
conditions to form a compound, i.e. MI threshold 
becomes smaller and MaxL/MaxR becomes larger. 
Results for quad-graras, tri-graras and bi-grams 
are shown in tables 2,3,4. Some compounds 
extracted are fisted in table 5. 
1 
2 
3 
4 
5 
Table2: Performance of quad-sram compounds 
Parameter setting 
(MI, LSize, MaxL, RSize, MaxR) compounds found 
0.01 1 0.75 1 0.75 ! 10 
extraction 
Number of New Precision 
27 
(correct 
compounds/compounds checked) 
0% (27/27) 
0.005 1 0.85 1 0.85 92 98.9% (91/92) 
0.002 1 0.90 1 0.90 513 95.8% (113/118) 
0.001 1 0.95 1 0.95 1648 96.2% (179/186) 
0.0005 1 0.95 1 0.95 4707 96.7% (206/213) 
1 
2 
3 
4 
5 
Table3: Performance of tri-sram compounds extraction 
Parameter setting 
(MI, LSize, MaxL, RSize, MaxR) 
0.02 2 0.70 2 0.70 
Number of New 
compounds found 
167 
Precision (correct 
compounds/compounds checked) 
100% (167/167) 
0.01 2 0.75 2 0.75 538 100% (205/205) 
0.005 2 0.80 2 0.80 1607 100% (262/262) 
0.003 2 0.80 2 0.80 3532 98.3% (341/347) 
0.001 2 0.80 2 0.80 16849 96.6% (488/501) 
1 
2 
3 
4 
5 
Table4: Performance of bi-sram compounds extraction 
Parameter setting Number of New Precision 
(MI, LSize, MaxL, RSize, MaxR) compounds found compound# 
0.05 3 0.5 3 0.5 1622 
(correct 
:~ s/compounds checked) 
98.9% (184/186) 
0.05 3 0.6 3 0.6 1904 98.6% (309/212) 
0.03 3 0.6 3 0.6 3938 97.8% (218/223) 
0.01 3 0.5 3 0.5 14666 97.5% (354/363) 
0.005 3 0.5 3 0.5 32899 97.3% (404/415) 
N-gram 
N=2 
N=3 
N=4 
Table 5: Some N-gram compounds found by our method 
Extracted Compounds 
~\]Jg~(graindepot), ~~J~(CD-ROM Driver), ~ \ [~(B i l l  Gates) 
(XuanWu Gate), (asynchronous t ransfer  model), 
(Amazon) 
~\[~\ [~\ [~ (Eiysee). ~\]~\[~i\[~H (Ohio), ~\ [~\ [~\ [~ (Mr. Dong Jianhua) 
134 
It turns out that our algorithm successfully 
extracted a large number of new compounds 
(>50000) from raw texts. Compared with previous 
methods described in the next section, the 
precision is very high. We can also find that there 
is little precision loss when we loose restriction. 
The result may be due to three reasons. First, the 
two statistical features really characterize the 
nature of compounds, and provide a simple and 
efficient way to estimate the possibility of a word 
sequence being a compound. Second, the corpus 
we use is very large. It is always true that more 
data leads to better esults. Third, the corpus we 
used in this experiment is homogeneity in style. 
The raw corpus is composed of news on politics, 
economy, science and technology. These are 
formal articles, and the sentences and compounds 
are well normalized and strict. This is very helpful 
for compound extraction. 
3.2 Compounds Extraction from Heterogeneous 
Corpus 
In this experiment, we use a heterogeneous 
corpus. It is a combination of corpus A, and some 
other novels, technique reports, etc. For 
simplicity, we discuss the extraction of bi-gram 
compounds only. In comparison with the first 
experiment, we find that the precision is strongly 
affected by the corpus we used. As shown in table 
6, for each corpus, we use the same parameter 
setting, say MI >0.005, LSize >3, MaxL <0.5, 
RSize>3 and MaxR<0.5. 
Table 6: Impact of heterogeneousness of corpora 
Corpus Compounds Extract 
extracted precision 
Corpus A 32899 97.3% 
(4041415) 
Corpus B 36383 88.3% 
(362/410) 
As we mentioned early, the larger the corpus we 
use, the better results we obtain. Therefore, we 
intuitively expect better esult on corpus B, which 
is larger than corpus A. But, the result shown in 
table 6 is just the opposite. 
There are mainly two reasons for this. The first 
one is that our method works better on 
homogeneous corpus than on heterogeneous 
corpus. The second one is that it might not be 
suitable to use the same parameter settings on two 
different corpora. We then try different parameter 
settings on corpus B. 
There are two groups of parameters. MI 
measures the correlation between adjacent words, 
and other four parameters, namely LSize, RSize, 
MaxL, and MaxR, measure the context 
dependency. Therefore, each time, we fix one 
parameter, and relax another from fight to loose to 
see what happens. The Number of extracted 
compounds and precision of each parameter 
setting are shown in table 7. 
MRCD 
0.0002 
0.0004 
0.0006 
0.0008 
0.0010 
0.0012 
0.0014 
Table 7: Extraction results with different parameter settings 
(Ml=Mutual Information, CD = Context Dependency=(LSize, MaxL, RSize, MaxR 
(2, 0.8, 2, (6, 0.7, 6, (10, 0.6, (14, 0.5, 4, (18, 0.4, (22, 0.3, 
0.8) 
1457781 
(39.06%) 
784082 
(48.98%) 
530723 
(51.28%) 
396602 
(54.63%) 
313868 
(59.11%) 
257990 
(58.94%) 
217766 
(58.93%) 
0.7) 
809502 
(42.24%) 
485143 
(46.84%) 
349882 
(53.96%) 
273231 
(58.00%) 
223827 
(66.51%) 
189014 
(59.50%) 
163189 
(67.91%) 
10, 0.6) 
570601 
(43.98%) 
359499 
(52.53%) 
266068 
(60.39%) 
211044 
(55.19%) 
175050 
(61.14%) 
149315 
(60.98%) 
129978 
(60.19%) 
0.5) 
426223 
(44.67%) 
277673 
(49.25%) 
208921 
(52.48%) 
167660 
(65.24%) 
140197 
(57.66%) 
120312 
(65.28%) 
105334 
(65.84%) 
18, 0.4) 
314810 
(43.96%) 
209634 
(53.92%) 
159363 
(49.49%) 
128819 
(60.54%) 
108322 
(67.38%) 
93323 
(70.47%) 
82083 
(66.83%) 
22, 0.3) 
209910 
(43.38%) 
141215 
(49.55%) 
108120 
(63.35%) 
87869 
(64.40%) 
74104 
(63.08%) 
64079 
(65.32%) 
56582 
(67.50%) 
(26, 0.2, 6, 
0.2) 
96383 
(40.93%) 
63907/ 
(52.53%) 
48683 
(61.65%) 
39502 
(54.86%) 
33354 
(67.50%) 
28879 
(64.65%) 
25486 
(65.46%) 
135 
Table 7 shows the extraction results with 
different parameters. These results fit our 
intuition. While parameters become more and 
more strict, less and less compounds are found 
and precisions become higher. This phenomena is
also illustrated in figure 2 and 3, in which the 
"correct compounds extracted" is an estimation 
from tableT, i.e. number of compounds found x 
precision. (These two figures are very useful for 
one who wants to automatically extract a new 
lexicon with pre-defined size from a large corpus.) 
600 
500 
400 
300 
~ 2oo 
~ loo 
o 
o 
2 3 4 5 6 7 
mutual information 
Figure 2 Impact of Parameter Mutual Information 
-(2 0.8 2 0.8) 
-(6 0.7 6 0.7) 
? (I0 0.6 I0 0.6) 
-(14 0.5 14 0.5) 
-(18 0.4 18 0.4) 
-(22 0.3 22 0.3) 
-(26 0.2 26 0.2) 
o 
O) 
o~ 
g 
g 
600 
500 
400 
300 
200 
100 
0 
2 3 4 5 6 7 
context dependency 
ix=0.0002 
ix=0.0004 
ix=0.0006 
ix=0.0008 
ix=0.0010 
ix=0.0012 
ix=0.0014 
Figure 3 Impact of Parameter Context Dependency 
136 
The precision of extraction is estimated in the 
following way. We extract a set of compounds 
based on a seres of pre-defined parameter set. For 
each set of compotinds, we randomly select 200 
compounds. Then we merge those selected 
compounds to a new file for manually check. This 
file consists of about 9,800 new compounds 
because there are 49 compounds lists. One person 
will group these 'compounds' into two sets, say 
set A and set B. Set A contains the items that are 
considered to be correct, and set B contains 
incorrect ones. Then for each original group of 
about 200 compounds we select in the first step, 
we check how many items that also appear in set 
A and how many items in set B. Suppose these 
two values are al and bl, then we estimate the 
precision as al/(al+bl). 
So, there are two important points in our 
evaluation process. First, it is difficult to give a 
definition of the term "compound" to be accepted 
popularly. Different people may have different 
judgement. Only one person takes part in the 
evaluation in our experiment. This can eliminate 
the effect of divergence among different persons. 
Second, we merge those items together. This can 
eliminate the effect of different ime period. One 
may feel tired after checked too many items. If he 
checks those 49 files one by one, the latter results 
are incomparable with the previous one. 
The precisions estimated by the above method 
are not exactly correct. However, as described 
above, the precisions of different parameter 
settings are comparable. In this experiment, what 
we want to show is how the parameter settings 
affect he results. 
Both MI and CD can affect number of extracted 
compounds, as shown in table 7. Compared with 
MI, CD has stronger effect in this aspect. For each 
row in table 7, numbers of extracted compounds 
finally decrease to 10% of that showed in the first 
column. For each column, while MI changes from 
0.0002 to 0.0014, the number is decreased of 
about 20%. This may be explained by the fact that 
it is difficult for candidate to fulfill all four 
restrictions in CD simultaneously. Many 
disqualified candidates are cut off. Table 7 lists 
the precisions of extracted results. It shows that 
there is no clear increasing/decreasing pattern in 
each row. That is to say, CD doesn't strongly 
affect he precision. When we check each column, 
we can see that precision is in a growing progress. 
As we defined above, MI and CD are two 
different measurements. What role they play in 
our extraction procedure? Our conclusion is that 
mutual information mainly affects the precision 
while context dependency mainly affects the count 
of extracted items. This conclusion is also 
confirmed by Fig2 and Fig3. That is, the curves in 
Fig2 are more fiat than corresponding curves in 
Fig3. 
3.3 Testing the Extracted Compounds in 
Information Retrieval 
In this experiment, we apply our method to 
improve information retrieval results. We use 
SMART system (Buckley 1985) for our 
experiments. SMART is a robust, efficient and 
flexible information retrieval system. The corpus 
used in this experiment is TREC Chinese corpus 
(Harman and Voorhees, 1996). The corpus 
contains about 160,000 articles, including articles 
published in the People's Daily from 1991 to 
1993, and a part of the news released by the 
Xinhua News Agency in 1994 and 1995. A set of 
54 queries has been set up and evaluated by 
people in NIST(Nafional Institute of Standards 
and Technology). 
We first use an initial lexicon consisting of 
65,502 entries to segment the corpus. When 
running SMART on the segmented corpus, we 
obtain an average precision of 42.90%. 
Then we extract new compounds from the 
segmented corpus, and add them into the initial 
lexicon. With the new lexicon, the TREC Chinese 
corpus is re-segmented. When running SMART 
on this re-segmented corpus, we obtain an average 
precision of 43.42%, which shows a slight 
improvement of 1.2%. 
Further analysis shows that the new lexicon 
brings positive effect to 10 queries and negative 
effect to 4 queries. For other 40 queries, there is 
no obvious effect. Some improved queries are 
listed in table 8 as well as new compounds being 
contained. 
As an example, we give the segmentation 
results with the two lexicons for query 23 in table 
9. 
137 
Query 
ID 
9 
23 
Base line 
precision 
0.3648 
0.3940 
New 
precision 
0.4173 
0.5154 
Table 8: Improved Query Samples 
Improvement Extracted compounds 
14.4% 
30.8% 
ME(drugs 
sale), ~ \[\] :~li~ Ih\] ~(Drug Problems 
in China) 
I~ -~- \ [ \ ] : '~( the  UN Security 
Council),~l\] ~1~ ,~(peace proposal) 
30 0.3457 0.3639 5.3% 
46 0.3483 0.4192 20.4% ~ ~(Claina nd Vietnam) 
47 0.5369 0.5847 8.9% /~ 1~ ~-~k~ tl.l (Mount 
Minatubo),~U-~(ozone layer), 
~ ~(Subic) 
Table 9: Se~rnented Corpus with the Two Lexicons for Query 23 
Query 23 segment with small lexicon 
, bk 
Query 23 segment with new lexicon 
Another interesting example is query 30. There 
is no new compound extracted from that query. Its 
result is also improved significantly because its 
relevant documents are segmented better than 
before. 
Because the compounds extracted from the 
corpus are not exactly correct, the new lexicon 
will bring negative ffect o some queries, such as 
query 10. The retrieval precision changes from 
0.3086 to 0.1359. The main reason is that 
"~ \ [ \ ]~" (Ch inese  XinJiang) is taken as a new 
compound in the query. 
4 Related works 
Several methods have been proposed for 
extracting compounds from corpus by statistical 
approaches. In this section, we will briefly 
describe some of them. 
(Lee-Feng Chien 1997) proposed an approach 
based on PAT-Tree to automatically extracting 
domain specific terms from online text 
collections. Our method is primary derived from 
(Lee-Feng Chien 1997), and use the similar 
statistical features, i.e. mutual informan'on and 
context dependency. The difference is that we use 
n-gram instead of PAT-Tree, due to the efficiency 
issue. Another difference lies in the experiments. 
In Chien's work, only domain specific terms are 
extracted from domain specific corpus, and the 
size of the corpus is relatively small, namely 
1,872 political news abstracts. 
(Cheng-Huang Tung and His-Jian Lee 1994) 
also presented an efficient method for identifying 
unknown words from a large corpus. The 
statistical features used consist of string (character 
sequence) frequency and entropy of left/fight 
neighbonng characters (similar to left/fight 
context dependency). The corpus consists of 
178,027 sentences, representing a total of more 
than 2 million Chinese characters. 8327 unknown 
words were identified and 5366 items of them 
were confirmed manually. 
(Ming-Wen Wu and Keh-Yih Su 1993) 
presented a method using mutual information and 
relative frequency. 9,124 compounds are extracted 
from the corpus consists of 74,404 words, with the 
precision of 47.43%. In this method, the 
compound extraction problem is formulated as 
classification problem. Each bi-grarn (tri-grarn) is 
assigned to one of those two clusters. It also needs 
a training corpus to estimate parameters for 
classification model. In our method, we didn't 
138 
make use of any training corpus. Another 
difference is that they use the method for English 
compounds extraction while we extract Chinese 
compounds in our experiments. 
(Pascale Fung 1998) presented two simple 
systems for Chinese compound extraction---- 
CXtract. CXtract uses predominantly statistical 
lexical information to find term boundaries in 
large text. Evaluations on the corpus consisting of 
2 million characters show that the average 
precision is 54.09%. 
We should note that since the experiment setup 
and evaluation systems of the methods mentioned 
above are not identical, the results are not 
comparable. However, by showing our 
experimental results on much larger and 
heterogenous corpus, we can say that our method 
is an efficient and robust one. 
5 Conclusion 
In this paper, we investigate a statistical 
approach to Chinese compounds extraction from 
very large corpora using mutual information and 
context dependency. 
We explained how the performance can be 
influenced by different parameter settings, corpus 
size, and corpus heterogeneousness. We also 
refine the lexicon with information retrieval 
system by adding compounds obtained by our 
methods, and achieve 1.2% improvements on 
precision of IR. 
Through our experiments, we conclude that 
statistical method based on mutual information 
and context dependency is efficient and robust for 
Chinese compounds extraction. And, mutual 
information mainly affects the precision while 
context dependency mainly affects the count of 
extracted items. 
Reference 
Lee-Feng Chien, (1997) "PAT-tree-based keyword 
extraction for Chinese Information retrieval", ACM 
SIGIR'97, Philadelphia, USA, 50-58 
WU, Dekai and Xuanyin XIA. (1995). "Large-scale 
automatic extraction of an English-Chinese l xicon", 
Machine Translation 9(3-4), pp.285-313. 
Ming-Wen Wu and Keh-Yih Su. (1993). "Corpus- 
based Automatic Compound Extraction with Mutual 
Information and Relative Frequency Count," 
Proceedings of R. 0. C. Computational Linguistics 
Conference V I .  Nantou, Taiwan, R. O. C., pp.207- 
216. 
Pascale Fung. (1998). "Extracting Key Terms from 
Chinese and Japanese texts ". The International 
Journal on Computer Processing of Oriental 
Language, Special Issue on Information Retrieval on 
Oriental Languages, pp.99-121. 
Cheng-Huang Tung and His-Jian Lee. (1994). 
"Identification of Unknown Words From a Corpus". 
Compouter Processing of Chinese and Oriental 
Languages Vol.8, pp. 131 -145. 
Buckley, C. (1985). Implementation f the SMART 
information retrieval system, Technical report, #85- 
686, Cornell University. 
Harman, D. K. and Voorhees, E. M., Eds. (1996). 
Information Technology: The Fifth Text Retrieval 
Conference(TREC5), NIST SP 500-238. 
Gaithersburg, National Institute fo standards and 
Technology. 
139 
Proceedings of NAACL HLT 2007, Companion Volume, pages 213?216,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Speech Summarization Without Lexical Features
for Mandarin Broadcast News
Jian Zhang
Human Language Technology Center
Electronic and Computer Engineering
University of Science and Technology
Clear Water Bay,Hong Kong
zjustin@ust.hk
Pascale Fung
Human Language Technology Center
Electronic and Computer Engineering
University of Science and Technology
Clear Water Bay,Hong Kong
pascale@ee.ust.hk
Abstract
We present the first known empirical study
on speech summarization without lexi-
cal features for Mandarin broadcast news.
We evaluate acoustic, lexical and struc-
tural features as predictors of summary
sentences. We find that the summarizer
yields good performance at the average F-
measure of 0.5646 even by using the com-
bination of acoustic and structural features
alone, which are independent of lexical
features. In addition, we show that struc-
tural features are superior to lexical fea-
tures and our summarizer performs sur-
prisingly well at the average F-measure
of 0.3914 by using only acoustic features.
These findings enable us to summarize
speech without placing a stringent demand
on speech recognition accuracy.
1 Introduction
Speech summarization, a technique of extracting
key segments that convey the main content from
a spoken document or audio document, has be-
come a new area of study in the last few years.
There has been much significant progress made in
speech summarization for English or Japanese text
and audio sources (Hori and Furui, 2003; Inoue et
al., 2004; Koumpis and Renals, 2005; Maskey and
Hirschberg, 2003; Maskey and Hirschberg, 2005).
Some research efforts have focused on summariz-
ing Mandarin sources (Chen et al, 2006; Huang
et al, 2005), which are dependent on lexical fea-
tures. Considering the difficulty in obtaining high
quality transcriptions, some researchers proposed
speech summarization systems with non-lexical fea-
tures (Inoue et al, 2004; Koumpis and Renals,
2005; Maskey and Hirschberg, 2003; Maskey and
Hirschberg, 2006). However, there does not exist
any empirical study on speech summarization with-
out lexical features for Mandarin Chinese sources.
In this paper, we construct our summarizer with
acoustic and structural features, which are indepen-
dent of lexical features, and compare acoustic and
structural features against lexical features as predic-
tors of summary sentences.
In Section 2 we review previous work on broad-
cast news summarization. We describe the Mandarin
broadcast news corpus on which our system operates
in Section 3. In Section 4 we describe our summa-
rizer and these features used in experiments. We set
up our experiments and evaluate the results in Sec-
tion 5, followed by our conclusion in Section 6.
2 Previous Work
There have been many research efforts on speech
summarization. Some methods dependent on lexi-
cal features are presented (Inoue et al, 2004; Chen
et al, 2006; Huang et al, 2005). (Inoue et al,
2004) uses statistical methods to identify words to
include in a summary, based on linguistic and acous-
tic/prosodic features of the Japanese broadcast news
transcriptions; while (Chen et al, 2006) proposes
the use of probabilistic latent topical information for
extractive summarization of Mandarin spoken docu-
ments. (Huang et al, 2005) presents Mandarin spo-
213
ken document summarization scheme using acous-
tic, prosodic, and semantic information. Alterna-
tively, some methods which are independent of lex-
ical features are presented (Maskey and Hirschberg,
2003; Maskey and Hirschberg, 2006). (Maskey
and Hirschberg, 2003) extracts structural informa-
tion from audio documents to help summarization.
(Maskey and Hirschberg, 2006) focuses on how to
use acoustic information alone to help predict sen-
tences to be included in a summary and shows a
novel way of using continuous HMMs for summa-
rizing speech documents without transcriptions.
It is advantageous to build speech summarization
models without using lexical features: we can sum-
marize speech data without placing a stringent de-
mand on the speech recognition accuracy. In this pa-
per, we propose one such model on Mandarin broad-
cast news and compare the effectiveness of acous-
tic and structural features against lexical features as
predictors of summary sentences.
3 The Corpus and Manual Summaries
We use a portion of the 1997 Hub4 Mandarin corpus
available via LDC as experiment data. The related
audio data were recorded from China Central Tele-
vision(CCTV) International News programs. They
include 23-day broadcast from 14th January, 1997
to 21st April, 1997, which contain 593 stories and
weather forecasts. Each broadcast lasts approxi-
mately 32 minutes, and has been hand-segmented
into speaker turns. For evaluation, we manually
annotated these broadcast news, and extracted seg-
ments as reference summaries. We divide these
broadcast news stories into 3 types: one-turn news,
weather forecast, and several-turns news. The con-
tent of each several-turn news is presented by more
than one reporter, and sometimes interviewees. We
evaluate our summarizer on the several-turns news
corpus. The corpus has 347 stories which contain
4748 sentences in total.
4 Features and Methodology
4.1 Acoustic/Prosodic Features
Acoustic/prosodic features in speech summarization
system are usually extracted from audio data. Re-
searchers commonly use acoustic/prosodic variation
? changes in pitch, intensity, speaking rate ? and du-
ration of pause for tagging the important contents
of their speeches (Hirschberg, 2002). We also use
these features for predicting summary sentences on
Mandarin broadcast news.
Our acoustic feature set contains thirteen features:
DurationI, DurationII, SpeakingRate, F0I, F0II,
F0III, F0IV, F0V, EI, EII, EIII, EIV and EV. Du-
rationI is the sentence duration. DurationII is the
average phoneme duration. General phonetic stud-
ies consider that the speaking rate of sentence is re-
flected in syllable duration. So we use average syl-
lable duration for representing SpeakingRate. F0I is
F0?s minimum value. F0II is F0?s maximum value.
F0III equals to the difference between F0II and F0I.
F0IV is the mean of F0. F0V is F0 slope. EI is min-
imum energy value. EII is maximum energy value.
EIII equals to the difference between EII and EI.
EIV is the mean of energy value. EV is energy slope.
We calculate DurationI from the annotated manual
transcriptions that align the audio documents. We
then obtain DurationII and SpeakingRate by pho-
netic forced alignment. Next we extract F0 fea-
tures and energy features from audio data by using
Praat (Boersma and Weenink, 1996).
4.2 Structural Features
Each broadcast news of the 1997 Hub4 Mandarin
corpus has similar structure, which starts with an an-
chor, followed by the formal report of the story by
other reporters or interviewees.
Our structural feature set consists of 4 features:
Position, TurnI, TurnII and TurnIII. Position is de-
fined as follows: one news has k sentences, then we
set (1? (0/k)) as Position value of the first sentence
in the news, and set (1?((i?1)/k)) as Position value
of the ith sentence. TurnI is defined as follows: one
news has m turns, then we set (1? (0/m)) as TurnI
value of the sentences which belong to the first turn?s
content, and set (1?((j?1)/m)) as TurnI values of
the sentences which belong to the jth turn?s content.
TurnII is the previous turn?s TurnI value. TurnIII is
the next turn?s TurnI value.
4.3 Reference Lexical Features
Most methods for text summarization mainly utilize
lexical features. We are interested in investigating
the role of lexical features in comparison to other
features. All reference lexical features are extracted
214
from the manual transcriptions.
Our lexical feature set contains eight features:
LenI, LenII, LenIII, NEI, NEII, NEIII, TFIDF
and Cosine. For a sentence, we set the number of
words in the sentence as LenI value. LenII is the
previous sentence?s LenI value. LenIII is the next
sentence?s LenI value. For a sentence, we set the
number of Named Entities in the sentence as the
NEI value. We define the number of Named Enti-
ties which appear in the sentence at the first time in
a news as NEII value. NEIII value equals to the ra-
tio of the number of unique Named Entities to the
number of all Named Entities.
TFIDF is the product of tf and idf. tf is the frac-
tion: the numerator is the number of occurrences
of the considered word and the denominator is the
number of occurrences of all words in a story. idf is
the logarithm of the fraction: the numerator is the to-
tal number of sentences in the considered news and
the denominator is the number of sentences where
the considered word appears. Cosine means cosine
similarity measure between two sentence vectors.
4.4 Summarizer
Our summarizer contains the preprocessing stage
and the estimating stage. The preprocessing stage
extracts features and normalizes all features by
equation (1).
Nj = wj ?mean(wj)dev(wj) (1)
Here, wj is the original value of feature j which is
used to describe sentence i; mean(wj) is the mean
value of feature j in our training set or test set;
dev(wj) is the standard deviation value of feature
j in our training set or test set.
The estimating stage predicts whether each sen-
tence of the broadcast news is in a summary or not.
We use Radial Basis Function(RBF) kernel for con-
structing SVM classifier as our estimator referring to
LIBSVM (Chang and Lin, 2001), which is a library
for support vector machines.
5 Experiments and Evaluation
We use the several-turn news corpus, described in
Section 3, in our experiments. We use 70% of the
corpus consisting of 3294 sentences as training set
Table 1: Feature set Evaluation by F-measure
Feature Set SR10% SR15% SR20% Ave
Ac+St+Le .5961 .546 .5544 .5655
Ac+St .5888 .5489 .5562 .5646
St .5951 .5616 .537 .5645
Le .5175 .5219 .5329 .5241
Ac .3068 .4092 .4582 .3914
Baseline .21 .32 .43 .32
Ac: Acoustic; St: Structural; Le: Lexical
and the remaining 1454 sentences as held-out test
set, upon which our summarizer is tested.
We measure our summarizer?s performance by
precision, recall, and F-measure (Jing et al, 1998).
We explain these metrics as follows:
precision = Sman
?Ssum
Ssum (2)
recall = Sman
?Ssum
Sman (3)
F-Measure = 2? precision ? recallprecision + recall (4)
In equation (2), (3) and (4), Sman is the sentence
set of manual summaries or reference summaries;
Ssum is the sentence set of predicted summaries pro-
vided by our summarizer.
We have three versions of reference summaries
based on summarization ratio(SR): 10%, 15% and
20% respectively. So we build three baselines re-
ferring to different versions of reference summaries.
When using SR 10% summaries, we build the base-
lines by choosing the first 10% of sentences from
each story. Our baseline results in F-measure score
are given in Table 1.
We perform three sets of experiments with differ-
ent summarization ratios.
By using acoustic and structural features alone,
the summarizer produces the same performance as
by using all features. We can find the evidence from
Table 1 and Figure 1. On average, the combination
of acoustic and structural features yields good per-
formance: F-measure of 0.5646, 24.46% higher than
the baseline, only 0.09% lower than the average F-
measure produced by using all features. This find-
ing makes it possible to summarize speech without
215
A L AS S ALS
0.00
0.05
0.10
0.15
0.20
0.25
0.30
0.35
0.40
0.45
0.50
0.55
0.60
0.65
A:Acoustic, S: Structural, L:Lexical
 Positive Precision
 Positive Recall
 Positive F-measure
S
c
o
r
e
Feature sets
Figure 1: Performance comparison on SR10%
placing a stringent demand on the speech recogni-
tion accuracy.
In the same Mandarin broadcast program, the dis-
tribution and flow of summary sentences are rela-
tively consistent. Therefore, compared with speech
summarization on English sources, we can achieve
the different finding that structural features play
a key role in speech summarization for Mandarin
broadcast news. Table 1 shows the evidence. On
average, structural features are superior to lexical
features: F-measure of 0.5645, 24.45% higher than
the baseline and 4,04% higher than the average F-
measure produced by using lexical features.
Another conclusion we can draw from Table 1
is that acoustic features are important for speech
summarization on Mandarin broadcast news. On
average, even by using acoustic features alone our
summarizer yields competitive result: F-measure of
0.3914, 7.14% higher than the baseline. The similar
conclusion also holds for speech summarization on
English sources (Maskey and Hirschberg, 2006).
6 Conclusion
In this paper, we have presented the results of an
empirical study on speech summarization for Man-
darin broadcast news. From these results, we found
that by using acoustic and structural features alone,
the summarizer produces good performance: aver-
age F-measure of 0.5646, the same as by using all
features. We also found that structural features make
more important contribution than lexical features to
speech summarization because of the relatively con-
sistent distribution and flow of summary sentences
in the same Mandarin broadcast program. Moreover,
we have shown that our summarizer performed sur-
prisingly well by using only acoustic features: av-
erage F-measure of 0.3914, 7.14% higher than the
baseline. These findings also suggest that high qual-
ity speech summarization can be achieved without
stringent requirement on speech recognition accu-
racy.
References
P. Boersma and D. Weenink. 1996. Praat, a system for doing
phonetics by computer, version 3.4. Institute of Phonetic
Sciences of the University of Amsterdam, Report, 132:182.
Chih-Chung Chang and Chih-Jen Lin, 2001. LIBSVM: a library
for support vector machines.
B. Chen, Y.M. Yeh, Y.M. Huang, and Y.T. Chen. 2006. Chi-
nese Spoken Document Summarization Using Probabilistic
Latent Topical Information. Proc. ICASSP.
J. Hirschberg. 2002. Communication and prosody: Functional
aspects of prosody. Speech Communication, 36(1):31?43.
C. Hori and S. Furui. 2003. A new approach to automatic
speech summarization. Multimedia, IEEE Transactions on,
5(3):368?378.
C.L. Huang, C.H. Hsieh, and C.H. Wu. 2005. Spoken Docu-
ment Summarization Using Acoustic, Prosodic and Seman-
tic Information. Multimedia and Expo, 2005. ICME 2005.
IEEE International Conference on, pages 434?437.
A. Inoue, T. Mikami, and Y. Yamashita. 2004. Improvement of
Speech Summarization Using Prosodic Information. Proc.
of Speech Prosody.
H. Jing, R. Barzilay, K. McKeown, and M. Elhadad. 1998.
Summarization evaluation methods: Experiments and anal-
ysis. AAAI Symposium on Intelligent Summarization.
K. Koumpis and S. Renals. 2005. Automatic summariza-
tion of voicemail messages using lexical and prosodic fea-
tures. ACM Transactions on Speech and Language Process-
ing (TSLP), 2(1):1?24.
S. Maskey and J. Hirschberg. 2003. Automatic summarization
of broadcast news using structural features. Proceedings of
Eurospeech 2003.
S. Maskey and J. Hirschberg. 2005. Comparing lexical, acous-
tic/prosodic, structural and discourse features for speech
summarization. Interspeech 2005 (Eurospeech).
S. Maskey and J. Hirschberg. 2006. Summarizing Speech
Without Text Using Hidden Markov Models. Proc. NAACL.
216
Proceedings of the 2nd Workshop on Building and Using Comparable Corpora, ACL-IJCNLP 2009, pages 23?26,
Suntec, Singapore, 6 August 2009. c?2009 ACL and AFNLP
Active Learning of Extractive Reference Summaries
for Lecture Speech Summarization
Justin Jian Zhang and Pascale Fung
Human Language Technology Center
Department of Electronic and Computer Engineering
University of Science and Technology (HKUST)
Clear Water Bay,Hong Kong
{zjustin,pascale}@ece.ust.hk
Abstract
We propose using active learning for tag-
ging extractive reference summary of lec-
ture speech. The training process of
feature-based summarization model usu-
ally requires a large amount of train-
ing data with high-quality reference sum-
maries. Human production of such sum-
maries is tedious, and since inter-labeler
agreement is low, very unreliable. Ac-
tive learning helps assuage this problem by
automatically selecting a small amount of
unlabeled documents for humans to hand
correct. Our method chooses the unla-
beled documents according to the similar-
ity score between the document and the
comparable resource?PowerPoint slides.
After manual correction, the selected doc-
uments are returned to the training pool.
Summarization results show an increasing
learning curve of ROUGE-L F-measure,
from 0.44 to 0.514, consistently higher
than that of using randomly chosen train-
ing samples.
Index Terms: active learning, summarization
1 Introduction
The need for the summarization of classroom lec-
tures, conference speeches, political speeches is
ever increasing with the advent of remote learning,
distributed collaboration and electronic archiving.
These user needs cannot be sufficiently met by
short abstracts. In recent years, virtually all sum-
marization systems are extractive - compiling bul-
let points from the document using some saliency
criteria. Reference summaries are often manu-
ally compiled by one or multiple human annota-
tors (Fujii et al, 2008; Nenkova et al, 2007). Un-
like for speech recognition where the reference
sentence is clear and unambiguous, and unlike
for machine translation where there are guidelines
for manual translating reference sentences, there
is no clear guideline for compiling a good ref-
erence summary. As a result, one of the most
important challenges in speech summarization re-
mains the difficulty to compile, evaluate and thus
to learn what a good summary is. Human judges
tend to agree on obviously good and very bad
summaries but cannot agree on borderline cases.
Consequently, annotator agreement is low. Refer-
ence summary generation is a tedious and low ef-
ficiency task. On the other hand, supervised learn-
ing of extractive summarization requires a large
amount of training data of reference summaries.
To reduce the amount of human annotation effort
and improve annotator agreement on the reference
summaries, we propose that active learning (selec-
tive sampling) is one possible solution.
Active learning has been applied to NLP tasks
such as spoken language understanding (Tur et al,
2005), information extraction (Shen et al, 2004),
and text classification (Lewis and Catlett, 1994;
McCallum and Nigam, 1998; Tong and Koller,
2002). Different from supervised learning which
needs the entire corpus with manual labeling re-
sult, active learning selects the most useful exam-
ples for labeling and requires manual labeling of
training dataset to re-train model.
In this paper, we suggest a framework of refer-
ence summary annotation with relatively high in-
ter labeler agreement based on the rhetorical struc-
ture in presentation slides. Based on this frame-
work, we further propose a certainty-based active
learning method to alleviate the burden of human
annotation of training data.
The rest of this paper is organized as follows:
Section 2 depicts the corpus for our experiments,
the extractive summarizer, and outlines the acous-
tic/prosodic, and linguistic feature sets for repre-
senting each sentence. Section 3 depicts how to
23
compile reference summaries with high inter la-
beler agreement by using the RDTW algorithm
and our active learning algorithm for tagging ex-
tractive reference summary. We describe our ex-
periments and evaluate the results in Section 4.
Our conclusion follows in Section 5.
2 Experimental Setup
2.1 The Corpus
Our lecture speech corpus (Zhang et al, 2008)
contains 111 presentations recorded from the
NCMMSC2005 and NCMMSC2007 conferences
for evaluating our approach. The man-
ual transcriptions and the comparable corpus?
PowerPoint slides are also collected. Each presen-
tation lasts for 15 minutes on average. We select
71 of the 111 presentations with well organized
PowerPoint slides that always have clear sketches
and evidently aligned with the transcriptions. We
use about 90% of the lecture corpus from the 65
presentations as original unlabeled data U and the
remaining 6 presentations as held-out test set. We
randomly select 5 presentations from U as our
seed presentations. Reference summaries of the
seed presentations and the presentations of test set
are generated from the PowerPoint slides and pre-
sentation transcriptions using RDTW followed by
manual correction, as described in Section 3.
2.2 SVM Classifier and the Feature Set
While (Ribeiro and de Matos, 2007) has shown
that MMR (maximum marginal relevance) ap-
proach is superior to feature-based classifica-
tion for summarizing Portuguese broadcast news
data, another work on Japanese lecture speech
drew the opposite conclusion (Fujii et al, 2008)
that feature-based classification method is bet-
ter. Therefore we continue to use the feature-
based method in our work. We consider the ex-
tractive summarization as a binary classification
problem, we predict whether each sentence of the
lecture transcription should be in a summary or
not. We use Radial Basis Function (RBF) ker-
nel for constructing SVM classifier, which is pro-
vided by LIBSVM, a library for support vector
machines (Chang and Lin, 2001). We represent
each sentence by a feature vector which consists of
acoustic features: duration of the sentence, aver-
age syllable Duration, F0 information features, en-
ergy information features; and linguistic features:
length of the sentence counted by word and TFIDF
information features, as shown in (Zhang et al,
2008). We then build the SVM classifier as our
summarizer based on these sentence feature vec-
tors.
3 Active Learning for Tagging Reference
Summary and Summarization
Similar to (Hayama et al, 2005; Kan, 2007), we
have previously proposed how presentation slides
are used to compile reference summaries automat-
ically (Zhang et al, 2008). The motivations be-
hind this procedure are:
? presentation slides are compiled by the au-
thors themselves and therefore provide a
good standard summary of their work;
? presentation slides contain the hierarchical
rhetorical structure of lecture speech as the ti-
tles, subtitles, page breaks, bullet points pro-
vide an enriched set of discourse information
that are otherwise not apparent in the spoken
lecture transcriptions.
We propose a Relaxed Dynamic Time Warping
(RDTW) procedure, which is identical to Dy-
namic Programming and Edit Distance, to align
sentences from the slides to those in the lecture
speech transcriptions, resulting in automatically
extracted reference summaries.
We calculate the similarity scores
matrix Sim = (sij), where sij =
similarity(Senttrans[i], Sentslides[j]), be-
tween the sentences in the transcription and
the sentences in the slides. We then obtain
the distance matrix Dist = (dij), where
dij = 1?sij . We calculate the initial warp path P:
P = (pini1 , ..., pinin , ..., piniN ) by DTW, where pinin
is represented by sentence pair(iinin , jinin ): one
from transcription, the other from slides. Con-
sidering that the lecturer often doesn?t follow the
flow of his/her slides strictly, we adopt Relaxed
Dynamic Time Warping (RDTW) for finding the
optimal warp path, by the following equation.
?
??
??
ioptn = iinin
joptn =
jinin +Cargmin
j=jinin ?C
dioptn ,j
(1)
We consider the transcription sentences on this
path as reference summary sentences. We then
obtain the optimal path (popt1 , ..., poptn , ..., poptN ),
where poptn is represented by (ioptn , joptn ) and C
24
is the capacity to relax the path. We then select
the sentences ioptn of the transcription whose sim-
ilarity scores of sentence pairs: (ioptn , joptn ), are
higher than the pre-defined threshold as the refer-
ence summary sentences. The advantage of using
these summaries as references is that it circum-
vents the disagreement between multiple human
annotators.
We have compared these reference summaries
to human-labeled summaries. When asked to ?se-
lect the most salient sentences for a summary?, we
found that inter-annotator agreement ranges from
30% to 50% only. Sometimes even a single per-
son might choose different sentences at different
times (Nenkova et al, 2007). However, when in-
structed to follow the structure and points in the
presentation slides, inter-annotator agreement in-
creased to 80%. The agreement between auto-
matically extracted reference summary and hu-
mans also reaches 75%. Based on this high degree
of agreement, we generate reference summaries
by asking a human to manually correct those ex-
tracted by the RDTW algorithm. Our reference
summaries therefore make for more reliable train-
ing and test data.
For a transcribed presentation D with a se-
quence of recognized sentences {s1, s2, ..., sN},
we want to find the sentences to be classified
as summary sentences by using the salient sen-
tence classification function c(). In a probabilis-
tic framework, the extractive summarization task
is equivalent to estimating P (c(??s n) = 1|D) of
each sentence sn, where ??s n is the feature vec-
tor with acoustic and linguistic features of the sen-
tence sn.
We propose an active learning approach where a
small set of transcriptions as seeds with reference
summaries, created by the RDTW algorithm and
human correction, are used to train the seed model
for the summarization classifier, and then the clas-
sifier is used to label data from a unlabel pool. At
each iteration, human annotators choose the unla-
beled documents whose similarity scores between
the extracted summary sentences and the Power-
Point slides sentences are top-N highest for label-
ing summary sentences. Formally, this approach
is described in Algorithm 1.
Given document D: {s1, s2, ..., sN}, we cal-
culate the similarity score between the extracted
summary sentences: {s?1, s?2, ..., s?K} and the Pow-
erPoint slide sentences: {ppts1, ppts2, ..., pptsL},
by equation 2.
Scoresim(D) = 1K
K?
n=1
L?
j=1
Sim(s?n, pptsj) (2)
4 Experimental Results and Evaluation
Algorithm 1 Active learning for tagging extrac-
tive reference summary and summarization
Initialization
For an unlabeled data set: Uall, i = 0
(1) Randomly choose a small set of data X{i}
from Uall; U{i} = Uall ?X{i}
(2) Manually label each sentence in X{i} as
summary or non-summary by RDTW and hu-
man correction and save these sentences and
their labels in L{i}
Active Learning Process
(3) X{i} = null
(4) Train the classifier M{i} using L{i}
(5) Test U{i} by M{i}
(6) Calculate similarity score of given docu-
ment D between the extracted summary sen-
tences and the PowerPoint slides sentences by
equation 2
(7) Select the documents with top-five highest
similarity scores from U{i}
(8) Save selected samples into X{i}
(9) Manually correct each sentence label in
X{i} as summary or non-summary
(10) L{i + 1} = L{i} + X{i}
(11) U{i + 1} = U{i} ?X{i}
(12) Evaluate M{i} on the testing set E
(13) i = i+1, and repeat from (3) until U{i} is
empty or M{i} obtains satisfying performance
(14) M{i} is produced and the process ends
We start our experiments by randomly choosing
six documents for manual labeling. We gradually
increase the training data pool by choosing five
more documents each time for manual correction.
We carry out two sets of experiments for compar-
ing our algorithm and random selection. We evalu-
ate the summarizer by ROUGE-L (summary-level
Longest Common Subsequence) F-measure (Lin,
2004).
The performance of our algorithm is illustrated
by the increasing ROUGE-L F-measure curve in
Figure 1. It is shown to be consistently higher than
25
Figure 1: Active learning vs. random selection
using randomly chosen samples. We also find that
by using only 51 documents for training, the per-
formance of the summarization model achieved
by our approach is better than that of the model
trained by random selection using all 65 presen-
tations (0.514 vs. 0.512 ROUGE-L F-measure).
This shows that our active learning approach re-
quires 22% less training data. Besides, acoustic
features can improve the performance of active
learning of speech summarization. Without acous-
tic features, our summarizer only performs 0.47
ROUGE-L F-measure.
5 Conclusion and Discussion
In this paper, we propose using active learning re-
duce the need for human annotation for tagging
extractive reference summary of lecture speech
summarization. We use RDTW to extract sen-
tences from transcriptions according to Power-
Point slides, and these sentences are then hand
corrected as reference summaries. The unlabeled
documents are selected whose similarity scores
between the extracted summary sentences and the
PowerPoint slides sentences are top-N highest for
labeling summary sentences. We then use an SVM
classifier to extract summary sentences. Summa-
rization results show an increasing learning curve
of F-measure, from 0.44 to 0.514, consistently
higher than that of using randomly chosen train-
ing data samples. Besides, acoustic features play
a significant role in active learning of speech sum-
marization. In our future work, we will try to ap-
ply different criteria, such as uncertainty-based or
committee-based criteria, for selecting samples to
be labeled, and compare the effectiveness of them.
6 Acknowledgements
This work is partially supported by GRF612806 of
the Hong Kong RGC.
References
C.C. Chang and C.J. Lin. 2001. LIBSVM: a library for sup-
port vector machines. Software available at http://www.
csie. ntu. edu. tw/cjlin/libsvm, 80:604?611.
Y. Fujii, K. Yamamoto, N. Kitaoka, and S. Nakagawa. 2008.
Class Lecture Summarization Taking into Account Con-
secutiveness of Important Sentences. In Proceedings of
Interspeech, pages 2438?2441.
T. Hayama, H. Nanba, and S. Kunifuji. 2005. Alignment
between a technical paper and presentation sheets using
a hidden markov model. In Active Media Technology,
2005.(AMT 2005). Proceedings of the 2005 International
Conference on, pages 102?106.
M.Y. Kan. 2007. SlideSeer: A digital library of aligned
document and presentation pairs. In Proceedings of the
7th ACM/IEEE-CS joint conference on Digital libraries,
pages 81?90. ACM New York, NY, USA.
D.D. Lewis and J. Catlett. 1994. Heterogeneous uncertainty
sampling for supervised learning. In Proceedings of the
Eleventh International Conference on Machine Learning,
pages 148?156. Morgan Kaufmann.
C.Y. Lin. 2004. Rouge: A Package for Automatic Evalua-
tion of Summaries. Proceedings of the Workshop on Text
Summarization Branches Out (WAS 2004), pages 25?26.
A. McCallum and K. Nigam. 1998. Employing EM in Pool-
based Active Learning for Text Classification. In Proceed-
ings of ICML, pages 350?358.
A. Nenkova, R. Passonneau, and K. McKeown. 2007. The
Pyramid Method: Incorporating human content selection
variation in summarization evaluation. ACM Transactions
on Speech and Language Processing (TSLP), 4(2).
R. Ribeiro and D.M. de Matos. 2007. Extractive Summa-
rization of Broadcast News: Comparing Strategies for Eu-
ropean Portuguese. Lecture Notes in Computer Science,
4629:115.
D. Shen, J. Zhang, J. Su, G. Zhou, and C.L. Tan. 2004.
Multi-criteria-based Active Learning for Named Entity
Recognition. In Proceedings of 42th Annual Meeting of
the Association for Computational Linguistics. Associa-
tion for Computational Linguistics Morristown, NJ, USA.
S. Tong and D. Koller. 2002. Support vector machine ac-
tive learning with applications to text classification. The
Journal of Machine Learning Research, 2:45?66.
G. Tur, D. Hakkani-Tr, and R. E. Schapiro. 2005. Combin-
ing Active and Semi-supervised Learning for Spoken Lan-
guage Understanding. Speech Communications, 45:171?
186.
J.J. Zhang, S. Huang, and P. Fung. 2008. RSHMM++ for
extractive lecture speech summarization. In IEEE Spoken
Language Technology Workshop, 2008. SLT 2008, pages
161?164.
26
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1299?1307,
Beijing, August 2010
 	
 	   
 	Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 69?71,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
PLUTO: Automated Solutions for Patent Translation
i
 
John Tinsley, Alexandru Ceausu, Jian Zhang 
 
Centre for Next Generation Localisation 
School of Computing 
Dublin City University, Ireland 
{jtinsley;aceausu;jzhang}@computing.dcu.ie 
  
1 Introduction 
PLUTO is a commercial development project 
supported by the European Commission as part 
of the FP7 programme which aims to eliminate 
the language barriers that exist worldwide in the 
provision of multilingual access to patent infor-
mation. The project consortium comprises four 
partners: the Centre for Next Generation Locali-
sation at Dublin City University,1 ESTeam AB,2 
CrossLang, 3  and the Dutch Patent Information 
User Group (WON).4 Research and development 
is carried out in close collaboration with user 
groups and intellectual property (IP) profession-
als to ensure solutions and software are delivered 
that meet actual user needs. 
1.1 The need for patent translation 
The number of patent applications filed 
worldwide is continually increasing, with over 
1.8 million new filings in 2010 alone. Yet De-
spite the fact that patents are filed in dozens of 
different languages, language barriers are no ex-
cuse in the case of infringement. When carrying 
our prior-art and other searches IP professionals 
must ensure they include collections which en-
compass all potential relevant patents. Such 
searches will typically return results ? a set of 
patent documents ? 30% of which will be in a 
foreign language. 
As professional translation for patents is such 
a specialist task, translators command a premium 
fee for this service, often up to ?0.50 per word 
for Asian languages. This often results in high or 
unworkable translation costs for innovators. 
While free machine translation (MT) tools such 
as Google translate have unquestionably been 
beneficial in helping to reduce the need to resort 
                                                     
1 www.cngl.ie 
2 www.esteam.se 
3 www.crosslang.com 
4 www.won-nl.com 
to expensive human translation, the quality is 
still often inadequate as the models are too gen-
eral to cope with the intricacies of patent text. 
In what follows, we will provide an overview 
of some of the technologies being developed in 
PLUTO to address the need for higher quality 
MT solutions for patents and how these are de-
ployed for the benefit of IP professionals. 
2 Language Technology for Patents 
Patent translation is a unique task given the 
style of language used in patent documents. This 
language, so-called ?patentese?, typically com-
prises a mixture of highly-specific technical ter-
minology and legal jargon and is often written 
with the express purpose of obfuscating the in-
tended meaning. For example, in 2001 an inno-
vation was granted in Australia for a ?Circular 
Transportation Facilitation Device?, i.e. a wheel.5 
Patents are also characterised by a prolifera-
tion of extremely long sentences, complex chem-
ical formula, and other constructs which make 
the task for MT more difficult. 
2.1 Domain-specfic machine translation 
The patent translation systems used in 
PLUTO have been built using the MaTrEx MT 
framework (Armstrong et al, 2006). The systems 
are domain specific in that they have been 
trained exclusively using parallel patent corpora. 
A number of experiments related to domain ad-
aptation of the language and translation models 
have been carried out in the context of these sys-
tems. The principal findings from this work were 
that systems combining all available patent data 
for a given language were preferable (Ceausu et 
al. 2011). 
                                                     
5  
http://pericles.ipaustralia.gov.au/aub/pdf/nps/2002/0808/200
1100012A4/2001100012.pdf 
69
Significant pre-processing techniques are also 
applied to the input text to account for specific 
features of patent language. For instance, sen-
tence splitting based on the marker hypothesis 
(Green, 1979) is used to reduce long sentences to 
more manageable lengths, while named-entity 
recognition is applied to isolate certain struc-
tures, such as chemical compounds and refer-
ences to figures, in order to treat them in a 
specific manner. 
Additionally, various language-specific tech-
niques are used for relevant MT systems. For 
example, a technique called word packing (Ma et 
al., 2007), is exploited for Chinese?English. 
This is a bilingually motivated task which im-
proves the precision of word alignment by ?pack-
ing? several consecutive words together which 
correspond to a single word in the corresponding 
language.  
Japanese?English is a particularly challeng-
ing pair due to the divergent word ordering be-
tween the two languages. To overcome this, we 
employ preordering of the input text (Talbot et 
al. 2011) in order to harmonise the word ordering 
between the two languages and reduce the likeli-
hood of ordering errors. This is done using a 
rule-based technique called head-finalisation 
(Isozaki et al, 2010) which moves the English 
syntactic head towards the end of the phrase to 
emulate the Japanese word order. 
Finally, we use compound splitting and true 
casing modules for our English?German MT 
systems in order to reduce the occurrence of out-
of-vocabulary words. 
2.2 Translation memory integration 
In order to further improve the translation 
quality, we are developing an engine to automat-
ically combine the outputs of the MT system and 
a translation memory (TM). 
The engine works by taking a patent docu-
ment as input and searching for full matches on 
paragraph, sentence, and segment (sub-
sentential) level in the TM. If no full matches are 
found, fuzzy matches are sought above a prede-
termined threshold and combined with the output 
of the MT system using phrase- and word-level 
alignment information. 
For patents, most leverage from the TM is 
seen at segment level, particularly as the patent 
claims are often written using quite a rigid struc-
ture. This is due to that fact that, as patents typi-
cally describe something novel which may never 
have been written about previously, there is often 
little repetition of full sentences. 
2.3 Evaluation 
The performance of the patent MT systems in 
PLUTO is evaluated using a range of methods 
aimed not only at gauging general quality, but 
also identifying areas for improvement and rela-
tive performance against similar systems. 
In addition to assessing the MT systems using 
automatic evaluation metrics such as BLEU 
(Papineni et al, 2002) and METEOR (Banerjee 
et al 2005), large-scale human evaluations are 
also carried out. MT system output is ranked 
from 1?5 based on the overall quality of transla-
tion, and individual translation errors are identi-
fied and classified in an error categorisation task. 
On top of this standalone evaluation, the 
PLUTO MT systems are also benchmarked 
against leading commercial systems across two 
MT paradigms: Google Translate for statistical 
MT and Systran (Enterprise) for rule-based MT. 
A comparative analysis is carried out using both 
the automatic and human evaluation techniques 
described above. This comparison is also applied 
to the output of the PLUTO MT systems and the 
output of the integrated TM/MT system in order 
to quantify the improvements achieved using the 
translation memories. 
The main findings from the first round of 
evaluations for our French?English and Portu-
guese?English systems showed that our MT 
systems score relatively high based on human 
judgments -- 3.8 out of 5 on average -- while be-
ing ranked higher than the commercial systems 
approximately 75% of the time. More details on 
these experiments can be found in Ceausu et al 
(2011). 
3 Patent Translation Web Service 
The PLUTO MT systems are deployed as a 
web service (Tinsley et al, 2010). The main en-
try point for end users is through a web browser 
plugin which allows them to access translations 
on-the-fly regardless of the search engine being 
used to find relevant patents. In addition to the 
browser plugin, users also have the option to in-
put text directly or upload patent documents in a 
number of formats including PDF and MS Word. 
A number of further natural language pro-
cessing techniques are exploited to improve the 
user experience. N-gram based language identifi-
cation is used to send input to the correct MT 
system; while frequency based keyword extrac-
70
tion provides users with potentially important 
terms with which to carry out subsequent search-
es. 
Corresponding source and target segments are 
highlighted on both word and phrase level, while 
users have the option of post-editing translations 
which are stored in a personal terminology data-
base and applied to future translations. 
The entire framework has been designed to 
facilitate the patent professional in their daily 
workflow. It provides them with a consistency of 
translation quality and features regardless of the 
search tools being used to locate relevant patents. 
This has been validated through extensive us-
er experience testing which included a usability 
evaluation of the translation output.  
4 Looking Forward 
The PLUTO project has been running for just 
over two years and is scheduled to end in March 
2013. Our goal by that time is to have established 
a viable commercial offering to capitalize on the 
state-of-the-art research and development into 
automated patent translation. 
In the meantime, we will continue to build 
upon our existing work by building MT systems 
for additional language pairs and iteratively im-
proving upon our baseline translation perfor-
mance. Significant effort will also be spent on 
optimising the integration of translation memo-
ries with MT using techniques such as those de-
scribed in He et al (2011). 
Acknowledgements 
 
The PLuTO project (ICT-PSP-250416) is 
funded under the European Union's ICT Policy 
Support Programme as part of the Competitive-
ness and Innovation Framework Programme 
References 
Armstrong, S., M. Flanagan, Y. Graham, D. 
Groves, B. Mellebeek, S. Morrissey, N. 
Stroppa and A. Way. 2006. MaTrEx: Ma-
chine Translation Using Examples. TC-STAR 
OpenLab on Speech Translation. Trento, Ita-
ly. 
Banerjee, S. and Lavie, A. (2005). METEOR: An 
Automatic Metric for MT Evaluation with Im-
proved Correlation with Human Judgments. 
In Proceedings of Workshop on Intrinsic and 
Extrinsic Evaluation Measures for MT and/or 
Summarization at the 43th Annual Meeting of 
the Association of Computational Linguistics 
(ACL-05), Ann Arbor, MI. 
Ceausu, Alexandru, John Tinsley, Andrew Way, 
Jian Zhang, Paraic Sheridan, Experiments on 
Domain Adaptation for Patent Machine 
Translation in the PLuTO project, The 15th 
Annual Conference of the European Associa-
tion for Machine Translation, EAMT-2011, 
Leuven, Belgium 
Green, T., The necessity of syntax markers. two 
experiments with artificial languages. Journal 
of Verbal Learning and Behavior, 
18:481{496}, 1979. 
Isozaki, H., Sudoh, K., Tsukada, H., and Duh, K. 
Head finalization: A simple reordering rule 
for SOV languages. In Proceedings of the 5th 
Workshop on Machine Translation (WMT), 
Upsala, Sweden. 
Ma, Yanjun, Nicolas Stroppa, and Andy Way. 
2007. Boostrapping Word Alignment via 
Word Packing. In Proceedings of the 45th An-
nual Meeting of the Association for Computa-
tional Linguistics (ACL 2007), Prague, Czech 
Republic, pp.304?311 
Papineni, K., Roukos, S.,Ward, T., and Zhu,W.-
J. (2002). BLEU: a Method for Automatic 
Evaluationof Machine Translation. In Pro-
ceedings of the 40th Annual Meeting of the 
Association for Computational Linguistics 
(ACL-02), pages 311?318, Philadelphia, PA. 
Talbot, David, Hideto Kazawa, Hiroshi Ichikwa, 
Jason Katz-Brown, Masakazu Seno, Franz 
Och A Lightweight Evaluation Framework for 
Machine Translation Reordering, In Proceed-
ings of the Sixth Workshop on Statistical Ma-
chine Translation (July 2011), Edinburgh, 
Scotland.pp. 12-21 
Tinsley, J., A. Way and P. Sheridan 
2010. PLuTO: MT for Online Patent Transla-
tion In Proceedings of the 9th Conferences of 
the Association for Machine Translation in the 
Americas. Denver, CO, USA. 
 
                                                     
i This paper is an extended abstract intended to accom-
pany an oral presentation. It is not intended to be a 
standalone scientific article. 
71
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 260?265,
Baltimore, Maryland USA, June 26?27, 2014.
c?2014 Association for Computational Linguistics
Experiments in Medical Translation Shared Task at WMT 2014
Jian Zhang, Xiaofeng Wu,
Iacer Calixto, Ali Hosseinzadeh Vahid, Xiaojun Zhang,
Andy Way, Qun Liu
The CNGL Centre for Global Intelligent Content
School of Computing
Dublin City University, Ireland
{zhangj,xiaofengwu,
icalixto,avahid,xzhang,
away,qliu}@computing.dcu.ie
Abstract
This paper describes Dublin City Uni-
versity?s (DCU) submission to the WMT
2014 Medical Summary task. We re-
port our results on the test data set in
the French to English translation direction.
We also report statistics collected from the
corpora used to train our translation sys-
tem. We conducted our experiment on the
Moses 1.0 phrase-based translation system
framework. We performed a variety of ex-
periments on translation models, reorder-
ing models, operation sequence model and
language model. We also experimented
with data selection and removal the length
constraint for phrase-pair extraction.
1 System Description
1.1 Training Data Statistics and Preparation
The training corpora provided to the medical
translation shared task can be divided into 3 cat-
egories:
Medical in-domain corpora: these corpora
contain documents, patents, articles, terminology
lists, and titles that are representative of the same
medical domain as the development and test data
sets (Table 1, second column).
Medical out-of-domain corpora: these cor-
pora also contain medical documents, patents, ar-
ticles, terminologies lists and titles, but describe
a different domain from the development and test
data sets (Table 1, third column).
General domain corpora: these corpora con-
sist of general-domain text (WMT 2014 general
translation subtask corpora), and encompass vari-
ous domains. (We did not use these corpora in our
system).
Corpus In-domain Out-of-domain
parallel sentence parallel sentence
number number
EMEA 1,092,568 0
COPPA 664,658 2,841,849
PatTR-title 408,502 2,096,270
PatTR-abstract 688,147 3,009,523
PatTR-claims 1,105,230 5,861,621
UMLS 85,705 0
Wikipedia 8,448 0
TOTAL 4,053,258 13,809,263
Table 1: WMT 2014 Medical Translation shared
task parallel training data before preprocessing.
Within all the provided training corpora from
WMT 2014, 70.72% of the medical in domain
bilingual sentences, and 100% of the medical
out-of-domain bilingual sentences were obtained
from patent document collections. Motivated by
these percentages, we view the WMT 2014 med-
ical translation shared task as similar to training
a patent-specific translation system. The mono-
lingual corpora are taken from 9 different cor-
pora collections, and there is no clear demarca-
tion of the in/out-of-domain boundaries (except
the PatTR collection). Our method of differenti-
ating between the in/out-of-domain monolingual
corpora is that only English sentences from the
third column of Table 1, and the patent descrip-
tion documents from PatTR collection, are out-of-
domain monolingual corpora. All other English
260
sentences are treated as an in-domain monolingual
resource.
A patent document usually comprises title, ab-
stract, claims and description fields. The docu-
ments often use its unique formatting and con-
tain linguistic idiosyncrasies, which distinguish
patent-specific translation systems from general
translation systems, in both training and transla-
tion phases (Ceaus?u et al., 2011). We have also
found that some common writing styles are con-
stantly used, especially for long sentences. For
example, a typical patent claim begins with
Method of [X], which comprising:
followed by a numbered list. The abstract
field normally contains one paragraph only, but
with multiple sentences. Those long sentences
are necessarily filtered out to facilitate efficient
word alignment, using a tool such as GIZA++
(Och, 2003) word aligner with the default param-
eter settings. However, because statistical ma-
chine translation depends on the training data to
estimate translation probability, more high qual-
ity training data often leads a better translation re-
sult. One possible method of including long sen-
tences into the training cycle is to change the word
aligner?s parameter settings to handle longer sen-
tences; however, aligning long sentences is time
consuming. Our solution is to capture the styled
long sentences and attempt to split them on both
source and target side simultaneously according
to the numbered list or sentence boundary indica-
tions. If the sentence number after splitting are
matching in both source and target sides, and each
sentence pair is within the token length ratio of
3, we assume the split attempt is successful, oth-
erwise the sentences are kept unchanged and will
be filtered out eventually. We applied our splitting
attempt approach on the patent documents at the
data preparation step which consequently results
in 19.35% and 7.1% increase in the number of
sentence pairs compared with the original medical
in-domain (from 4053258 to 4837382) and over-
all medical (from 17862521 to 19124142) datasets
respectively.
Another finding from the training corpora is that
the titles of the patent documents are often capital-
ized in the training corpora. Since we are training
a true-cased translation system, and the transla-
tion inputs contain non-title sentences, capitalized
training sentences will contribute biased weights
to our true-case model. We addressed this issue by
creating a lowercase version of the title corpora,
then we trained our true-case model with the low-
ercased titles corpora and other non-title corpora.
We also included the lowercased title corpora in
the translation system training.
We tokenized the training corpora using the
tokenizer script distributed in the Moses 1.0
framework with additional patent document non-
breaking preferences observed during data prepa-
ration, such as Figs and FIGS etc., and a modified
aggressive setting (split hyphen character in all
cases). Other data preparation steps included char-
acter normalization, character/token based foreign
language detection, HTML/XML tag removal,
case insensitive duplication removal, longer sen-
tence removal (2-80, length ratio 9), resulting in
the preprocessed data shown in Table 2.
Corpus In-domain Out-of-domain
parallel sentence parallel sentence
number number
EMEA 273,532 0
COPPA 1,374,371 6,075,599
PatTR-title 63,856 3,457,164
PatTR-abstract 599,435 2,595,515
PatTR-claims 876,603 4,244,324
UMLS 85,683 0
Wikipedia 8,438 0
TOTAL 3,956,478 16,372,602
Table 2: WMT 2014 Medical Translation shared
task parallel training data after preprocessing
steps.
1.2 Training Data Selection
It is an open secret that high quality and large
quantity of the parallel corpus are the two most
important factors for a high-quality SMT system.
These factors assist the word aligner in producing
a precise alignment model, which in turn brings
benefits to the other SMT training steps.
The quantity factor also helps the SMT system
to cover more translation input variations. In order
to efficiently use the training corpora listed in Ta-
ble 2, we explored some data selection methodolo-
gies. We used the feature decay algorithm (Bicici
et al., 2014) to select the training instances trans-
ductively, using the source side of the test set. We
built systems with the pre-defined selection pro-
portions in token number, 1/64, 1/32, 1/16, 1/8,
1/2, 3/4 and 1 of all the in-domain medical train-
ing data, then searched for the best performing
261
system using the test data set as our baseline (Ta-
ble 3). For the purpose of making the potential
baseline systems comparable, instance selection
was employed after word alignment using word
aligner MGIZA++ (Gao and Vogel, 2008) on all
the available data. The transductive learning uses
features extracted from the source data of the de-
velopment set with the default feature decay algo-
rithm weight settings. All of systems were trained
using the default phrase-based training parameter
settings of Moses 1.0 framework, with additional
msd-bidirectional-fe reordering model (Koehn et
al., 2005). We extract phrase pairs based on grow-
diag-final-and (Koehn et al., 2003) heuristics.
The language model was created with open source
IRSTLM toolkit (Federico et al., 2008) using all
the English in-domain data (monolingual and par-
allel). We used 5-gram with modied Kneser-Ney
smoothing (Kneser and Ney, 1995). The tuning
step used minimum error rate training (MERT)
(Och, 2003). The performance was measured by
the test data set in case insensitive BLEU score.
Proportions Test set
case insensitive BLEU
1/64 0.4374
1/32 0.4409
1/16 0.4370
1/8 0.4419
1/4 0.4390
1/2 0.4399
3/4 0.4397
1 0.4260
Table 3: Feature decay algorithm transductive
learning selection on all in-domain data using ex-
tracted features from the source side of the test
data set. We choose system uses 1/8 proportions
of the in-domain data as our baseline system.
Our results show that the system trained with
1/8 proportion of the in-domain medical training
data (398,098 sentence pairs) selected by FDA
outperformed the others. We chose this system as
our baseline system.
2 Experiments
2.1 Maximum Phrase Length
While extracting phrase pairs, collecting longer
phrases is not guaranteed to produce a better qual-
ity phrase table than the shorter settings, even
setting the maximum phrase length to three can
achieve top performance (Koehn et al., 2003).
We take this WMT 2014 opportunity to study the
capability of long phrase lengths ( >=10 ). We
trained translation models with phrase length set-
ting from 10 to 15, employed them to our base-
line system and compared the performance with
the default setting (length = 7).
Phrase Length Phrase Table Test set
Entries case insensitive
BLEU
7 (Baseline) 19.31 0.4419
10 29.67 0.4400
11 32.87 0.4416
12 35.95 0.4444*
13 38.91 0.4448*
14 41.75 0.4444*
15 44.47 0.4362
Table 4: -max-phrase-length setting experiment,
where phrase table entries is in millions. * indi-
actes statistically significant improvement at the p
= 0.05 level.
1
As stated in (Koehn et al., 2003) and expected,
the size of the phrase table is linear with respect to
the maximum phrase length restriction. Surpris-
ingly, we also found the performance can still im-
prove after the default length setting, until a peak
point (Table 4).
It is also interesting to see the effect for each
sentence in the test set when the default phrase
length setting in Moses framework is changed. We
first evaluated the sentence level BLEU scores for
the systems listed in Table 4, then compared them
with our baseline system sentence level BLEU
scores and categorised the compared results into
increased, decreased or unaffected groups (Fig-
ure 1). We found that system with -max-phrase-
length set to 12 is influenced the least (158, 118
and 724 sentences have BLEU score increased,
decreased and unaffected respectively) and with
-max-phrase-length sets to 10 is influenced the
most (261, 257 and 482 sentences have BLEU
score increased, decreased and unaffected respec-
tively).
We then looked into the decoding phase and
tried to discover the actual phrase length that was
used to generate the translation outputs. We ex-
posed the translation segmentations by trigger-
ing the -report-segmentation decoding parameter
1
The same notation is used for the rest of the tables in this
paper
262
10 11 12 13 14 15
200
250
300
350
400
450
500
550
600
650
700
750
2
6
1
2
0
8
1
5
8
2
5
8
2
1
7
2
1
8
2
5
7
1
8
0
1
1
8
2
1
3
1
6
3
2
2
7
4
8
2
6
1
2
7
2
4
5
2
9
6
2
0
5
5
5
-max-phrase-length from 10 to 15
t
e
s
t
s
e
t
(
1
0
0
0
s
e
n
t
e
n
c
e
s
)
increased decreased unaffected
Figure 1: Sentence level BLEU score affects when
enlarge -max-phrase-length
7 10 11 12 13 14 15
5 ? 10
?4
5 ? 10
?2
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
0.55
0.6
baseline and -max-phrase-length from 10 to 15
S
o
u
r
c
e
p
h
r
a
s
e
u
s
e
d
i
n
d
e
c
o
d
i
n
g
(
%
)
pl=1 pl=2 pl=3 pl=4 pl=5 pl=6 pl=7 pl=8
Figure 2: Phrase length (pl) distribution used in
decoding
in the Moses framework and computed the per-
centage of different phrases used according to the
phrase token number (Figure 2). The transla-
tion is mostly generated from short source phrases
(length<4) in all the systems during decoding,
which we think is the reason that setting phrase ex-
traction to length 3 can achieve top performance.
We did not carry out more experiments in this
case, as we think there is no absolute maximum
phrase length setting which can fit into all experi-
ments and such experiments depend on many fac-
tors, such as the similarity between the training
corpus and then testing data. The choice to set
-max-phrase-length to 13 is purely directed by the
BLEU score shown in Table 4.
2.2 Reordering Models
Ceaus?u et al. (2011) also found that long-range re-
ordering is one of the characteristics of patent doc-
uments; however, long-range reordering increases
the difficulty of SMT training and decoding. We
experimented two approaches to address this chal-
lenge. Apart from the msd-bidirectional-fe lexi-
cal reordering model (Koehn et al., 2005) in our
baseline system, the phrase-based orientation and
hierarchical orientation reordering models (Gal-
ley and Manning, 2008) can capture long distance
dependencies. The phrase-based orientation re-
ordering model is similar to the lexical reordering
approach, the only difference between these two
models is the phrase-based reordering model per-
forms reordering only on the phrase level, but the
hierarchical reordering model does not have such
constraint - it does not require phrases to be ad-
jacent. OSM (Durrani, 2011) (Durrani, 2013b)
is a sequence model integrating the N-gram-based
translation model and reordering model. It de-
fines three operations for reordering and consid-
ers all reordering possibilities within a fixed win-
dow while searching. We experimented with both
reordering models, and found that the system de-
fined with three reordering models performs bet-
ter (Table 5) than OSM. We then tried to use both
OSM and the reordering models together, which
produced the best system at this point.
Systems Test set
case insensitive BLEU
Baseline + 13 0.4448
+ OSM 0.4472
+ pho-ho 0.4551*
+ pho-ho + OSM 0.4561*
Table 5: Reordering Model or/and OSM results
2.3 Two Translation Models
The back-off model aims to produce translations
for the unknown words or unknown phrases in the
primary translation table by yielding the phrase ta-
ble translation probability from primary transla-
tion table to the back-off table, as in (Koehn et
al., 2012a)
p
BO
(e|f) =
{
p
1
(e|f) if count
1
(f) > 0
p
2
(e|f) otherwise
Moreover, we look at using the back off model
263
as a domain adaptation approach, which is to con-
strain the translation options within the target do-
main unless no options can be found, in which
case the translation will be selected from the back-
off model.
Phrase table fill-up (Bisazza et al., 2011) is a
very similar approach with back-off models, it col-
lects and uses the phrase pairs from the out-of-
domain phrase table only when the input is un-
available at the in-domain phrase table. It merges
the in-domain and out-of-domain translation mod-
els into one, where the scores are taken from more
reliable source. To distinguish the source of a
phrase pair entry, fill-up assigns a binary value as
an additional feature at the merged phrase table.
We trained our out-of-domain translation model
separately using all of the out-of-domain medi-
cal data listed at Table 2 with the same parame-
ter settings as our baseline system, then employed
Moses?s back-off model feature to pass the pri-
mary and back-off translation models to the de-
coder at tuning and translation time. The fill-up
tool was sourced from (Bisazza et al., 2011) at
Moses?s distribution. Our experiment results (Ta-
ble 6) show that the fill-up approach performed
better than the back-off model approach.
Systems Test set
case insensitive BLEU
Baseline + 13 + pho-ho + OSM 0.4561
Back-off 0.4573
Fill-up 0.4599*
Table 6: Back-off and fill-up experiment results
2.4 Language Model
Until now, we have reported our results using a
language model trained with all in-domain medi-
cal data only. We also took the similar approach
to (Koehn et al., 2007) and carried out language
model experiments. We trained our out-of-domain
language model with all the out-of-domain En-
glish sentences mentioned in section 1.1, then in-
terpolated the in-domain and out-of-domain lan-
guage model by optimizing the perplexity to the
development data set. We received a similar pic-
ture to (Koehn et al., 2007), where the language
model trained with only in-domain data performed
the best (Table 7).
Our final submission for WMT 2014 Medical
Translation shared task is the * system at Table 7.
Systems Test set
case insensitive BLEU
Baseline + 13 + pho-ho
+ OSM + Fill-up* 0.4599
out-of-domain LM 0.4461
interpolated LM 0.4592
Table 7: Language model experiment results
3 Conclusion
In this paper, we report our results on the WMT
2014 in the French to English translation direc-
tion. We shared our statistics for the bilingual
corpora used to train our translation system. All
systems were trained using the open source Moses
1.0 translation framework. Based on the feature
set of Moses phrased-based translation system, we
carried out our experiments on translation models,
reordering models, operation sequence model and
language model. We also experimented on data
selection and releasing the length restriction while
extracting phrase pairs.
4 Acknowledgements
This research is supported by the Science Foun-
dation Ireland (Grant 12/CE/I2267) as part of
the Centre for Next Generation Localisation
(www.cngl.ie) at Dublin City University. We
would also like to acknowledge Ergun Bicici who
gives suggestions at the data selection approach.
References
Alexandru Ceaus?u, John Tinsley, Jian Zhang and Andy
Way. 2011. Experiments on domain adaptation for
patent machine translation in the PLuTO project,
The 15th conference of the European Association
for Machine Translation, Leuven, Belgium.
Arianna Bisazza, Nick Ruiz, and Marcello Fed-
erico. 2011. Fill-up versus Interpolation Meth-
ods for Phrase-based SMT Adaptation., In Interna-
tional Workshop on Spoken Language Translation
(IWSLT), San Francisco, CA.
Durrani, N., Schmid, H., and Fraser, A. 2011. A
Joint Sequence Translation Model with Integrated
Reordering., The 49th Annual Meeting of the As-
sociation for Computational Linguistics, Portland,
Oregon, USA.
Durrani, N., Fraser, A., Schmid, H., Hoang, H., and
Koehn, P. 2013b. Can Markov Models Over Min-
imal Translation Units Help Phrase-Based SMT,
The 51th Annual Meeting of the Association for
Computational Linguistics, Sofia, Bulgaria.
264
Ergun Bicici and Deniz Yuret. 2014. Optimizing In-
stance Selection for Statistical Machine Translation
with Feature Decay Algorithms, IEEE/ACM Trans-
actions On Audio, Speech, and Language Process-
ing (TASLP).
Franz J. Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models,
Computational Linguistics, 29(1):1951.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation, The 41th Annual
Meeting of the Association for Computational Lin-
guistics, Sapporo, Japan.
Marcello Federico, Nicola Bertoldi, and Mauro Cet-
tolo. 2008. IRSTLM: an open source toolkit for
handling large scale language models, Interspeech,
Brisbane, Australia.
Michel Galley and Christopher D. Manning. 2008. A
simple and effective hierarchical phrase reordering
model. , The 2008 Conference on Empirical Meth-
ods in Natural Language Processing,pages 848856,
Honolulu, Hawaii, October. Association for Compu-
tational Linguistics.
Qin Gao and Stephan Vogel. 2008. Parallel implemen-
tations of word alignment tool, In Software Engi-
neering, Testing, and Quality Assurance for Natu-
ral Language Processing, SETQA-NLP 2008, pages
49-57, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Philipp Koehn, Amittai Axelrod, Alexandra Birch
Mayne, Chris Callison-Burch, Miles Osborne and
David Talbot. 2005. Edinburgh System Description
for the 2005 IWSLT Speech Translation Evaluation,
International Workshop on Spoken Language Trans-
lation.
Philipp Koehn and Josh Schroeder. 2007. Experi-
ments in Domain Adaptation for Statistical Machine
Translation, The Second Workshop on Statistical
Machine Translation, pages 224227, Prague.
Philipp Koehn, Amittai Axelrod, Alexandra Birch
Mayne, Chris Callison-Burch, Miles Osborne and
David Talbot. 2003. Statistical phrase-based trans-
lation, 2003 Conference of the North American
Chapter of the Association for Computational Lin-
guistics on Human Language Technology, pages
4854, Edmonton, Canada.
Philipp Koehn, and Barry Haddow. 2012. Interpolated
backoff for factored translation models., The 10th
Conference of the Association for Machine Transla-
tion in the Americas (AMTA).
Reinhard Kneser and Hermann Ney 1995. Improved
backing-off for m-gram language modeling., IEEE
International Conference on Acoustics, Speech and
Signal Processing, pages 181184.
265
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 329?334,
Baltimore, Maryland USA, June 26?27, 2014.
c?2014 Association for Computational Linguistics
Target-Centric Features for Translation Quality Estimation
Chris Hokamp and Iacer Calixto and Joachim Wagner and Jian Zhang
CNGL Centre for Global Intelligent Content
Dublin City University
School of Computing
Dublin, Ireland
{chokamp|icalixto|jwagner|zhangj}@computing.dcu.ie
Abstract
We describe the DCU-MIXED and DCU-
SVR submissions to the WMT-14 Quality
Estimation task 1.1, predicting sentence-
level perceived post-editing effort. Fea-
ture design focuses on target-side features
as we hypothesise that the source side has
little effect on the quality of human trans-
lations, which are included in task 1.1
of this year?s WMT Quality Estimation
shared task. We experiment with features
of the QuEst framework, features of our
past work, and three novel feature sets.
Despite these efforts, our two systems per-
form poorly in the competition. Follow up
experiments indicate that the poor perfor-
mance is due to improperly optimised pa-
rameters.
1 Introduction
Translation quality estimation tries to predict the
quality of a translation given the source and target
text but no reference translations. Different from
previous years (Callison-Burch et al., 2012; Bo-
jar et al., 2013), the WMT 2014 Quality Estima-
tion shared task is MT system-independent, i. e. no
glass-box features are available and translations in
the training and test sets are produced by different
MT systems and also by human translators.
This paper describes the CNGL@DCU team
submission to task 1.1 of the WMT 2014 Quality
Estimation shared task.
1
The task is to predict the
perceived post-editing effort given a source sen-
tence and its raw translation. Due to the inclusion
of human translation in the task, we focus our ef-
forts on target-side features as we expect that the
quality of a translation produced by a human trans-
lator is much less affected by features of the source
1
A CNGL system based on referential translation ma-
chines is submitted separately (Bic?ici and Way, 2014).
than by extrinsic factors such as time pressure and
familiarity with the domain.
To build our quality estimation system, we use
and extend the QuEst framework for translation
quality estimation
2
(Shah et al., 2013; Specia
et al., 2013). QuEst provides modules for fea-
ture extraction and machine learning. We modify
both the feature extraction framework and the ma-
chine learning components to add functionality to
QuEst.
The novel features we add to our systems are
(a) a language model on a combination of stop
words and POS tags, (b) inverse glass-box fea-
tures for translating the translation, and (c) ran-
dom indexing (Sahlgren, 2005) for measuring the
semantic similarity of source and target side across
languages. Furthermore, we integrated (d) source-
side pseudo-reference features (Soricut and Echi-
habi, 2010) and (e) error grammar features (Wag-
ner, 2012), which were used first in MT quality
estimation by (Rubino et al., 2012; Rubino et al.,
2013).
The remaining sections are organised as fol-
lows. Section 2 gives details on the features we
use. Section 3 describes how we set up our ex-
periments. Results are presented in Section 4 and
conclusions are drawn in Section 5 together with
pointers to future work.
2 Features
This section describes the features we extract from
source and target sentences in order to train predic-
tion models and to make predictions in addition to
the baseline features provided for the task.
We focus on the target side as we assume that
the quality of the source side has little predictive
power for human translations, which are included
in task 1.1.
2
http://www.quest.dcs.shef.ac.uk/
329
2.1 QuEst Black-Box Features and Baseline
Features
We use the QuEst framework to extract 47 ba-
sic black-box features from both source and tar-
get side, such as the ratio of the number of to-
kens, punctuation statistics, number if mismatched
brackets and quotes, language model perplexity,
n-gram frequency quartile statistics (n = 1, 2, 3),
and coarse-grained POS frequency ratios. 17 of
the 47 features are identical to the baseline fea-
tures from the shared task website, i. e. 30 fea-
tures are new. To train the language models and
to extract frequency information, we use the News
Commentary corpus (Bojar et al., 2013).
2.2 POS and Stop Word Language Model
Features
For all languages, we extract probability and per-
plexity features from language models trained on
POS tagged corpora. POS tagging is performed
using the IMS Tree Tagger (Schmid, 1994).
We also experiment with language models built
from a combination of stop words
3
and POS tags.
Starting with a tokenised corpus, and its POS-
tagged counterpart, we create a new representation
of the corpus by replacing POS tags for stop words
with the literal stop word that occurred in the orig-
inal corpus, leaving non-stop word tags intact.
4
The intuition behind the approach is that the com-
bined POS and stop word model should encode
the distributional tendencies of the most common
words in the language.
The log-probability and the perplexity of the
target side are used as features. The development
of these features was motivated by manual exam-
ination of the common error types in the train-
ing data. We noted that stop word errors (omis-
sion, mistranslation, mis-translation of idiom), are
prevalent in all language pairs, indicating that fea-
tures which focus on stop word usage could be
useful for predicting the quality of machine trans-
lation. We implement POS and stop word lan-
guage models inside the QuEst framework.
2.3 Source-Side Pseudo-Reference Features
We extract source-side pseudo-reference features
(Albrecht and Hwa, 2008; Soricut and Echihabi,
3
We use the stop word lists from Apache Lucene (McCan-
dless et al., 2010).
4
The News Commentary corpus from WMT13 was used
to build these models, same as for the black-box features
(Section 2.1).
2010; Rubino et al., 2012), for English to German
quality prediction using a highly-tuned German to
English translation system (Li et al., 2014) work-
ing in the reverse direction. The MT system trans-
lates the German target side, the quality of which
is to be predicted, back into English, and we ex-
tract pseudo-reference features on the source side:
? BLEU score (Papineni et al., 2002) be-
tween back-translation and original source
sentence, and
? TER score (Snover et al., 2006).
For the 5th English to German test set item, for
example, the translation
(1) Und belasse sie dort eine Woche.
is translated back to English as
(2) and leave it there for a week .
and compared to the original source sentence
(3) Leave for a week.
producing a BLEU score of 0.077 using the
Python interface to the cdec toolkit (Chahuneau et
al., 2012).
2.4 Inverse Glass-Box Features for
Translating the Translation
In the absence of direct glass-box features, we ob-
tain glass-box features from translating the raw
translation back to the source language using the
same MT system that we use for the source-side
pseudo-reference features. We extract features
from the following components of the Moses de-
coder: distortion model, language model, lexi-
cal reordering, lexical translation probability, op-
erational sequence model (Durrani et al., 2013),
phrase translation probability, and the decoder
score.
The intuition for this set of features is that back-
translating an incorrect translation will give low
system-internal scores, e. g. a low phrase transla-
tion score, and produce poor output with low lan-
guage model scores (garbage in, garbage out).
We are not aware of any previous work using
inverse glass-box features of translating the target
side to another language for quality estimation.
330
2.5 Semantic Similarity Using Random
Indexing
These features try to measure the semantic sim-
ilarity of source and target side of a translation
unit for quality estimation using random index-
ing (Sahlgren, 2005). We experiment with adding
the similarity score of the source and target ran-
dom vectors.
For each source and target pair in the English-
Spanish portion of the Europarl corpus (Koehn,
2005), we initialize a sparse random vector. We
then create token vectors for each source and tar-
get token by summing the vectors for all of the
segments where the token occurs. To extract the
similarity feature for new source and target pairs,
we map them into the vector space by taking the
centroid of the token vectors for the source side
and the target side, and computing their cosine
similarity.
2.6 Error Grammar Parsing
We obtain features from monolingual parsing with
three grammars:
1. the vanilla grammar shipped with the Blipp
parser (Charniak, 2000; Charniak and John-
son, 2005) induced from the Penn-Treebank
(Marcus et al., 1994),
2. an error grammar induced from Penn-Tree-
bank trees distorted according to an error
model (Foster, 2007), and
3. a grammar induced from the union of the
above two treebanks.
Features include the log-ratios between the prob-
ability of the best parse obtained with each gram-
mar and structural differences measured with Par-
seval (Black et al., 1991) and leaf-ancestor (Samp-
son and Babarczy, 2003) metrics. These features
have been shown to be useful for judging the
grammaticality of sentences (Wagner et al., 2009;
Wagner, 2012) and have been used in MT quality
estimation before (Rubino et al., 2012; Rubino et
al., 2013).
3 Experimental Setup
This section describes how we set up our experi-
ments.
3.1 Cross-Validation
Decisions about parameters are made in 10-fold
cross-validation on the training data provided for
the task. As the datasets for task 1.1 include
three to four translations for each source segment,
we group segments by their source side and split
the data for cross-validation between segments to
ensure that a source segment does not occur in
both training and test data for any of the cross-
validation runs.
We implement these modifications to cross-
validation and randomisation in the QuEst frame-
work.
3.2 Training
We use the QuEst framework to train our models.
Support vector regression (SVR) meta-parameters
are optimised using QuEst?s default settings, ex-
ploring RBF kernels with two possible values for
each of the three meta-parameters C, ? and .
5
The two final models are trained on the
full training set with the meta-parameters that
achieved the best average cross-validation score.
3.3 Classifier Combination
We experiment with combining logistic regression
(LR) and support vector regression (SVR) by first
choosing the instances where LR classification is
confident and using the LR class label (1, 2, or
3) as predicted perceived post-editing effort, and
falling back to SVR for all other instances.
We employ several heuristics to decide whether
to use the output of LR or SVR. As the LR classi-
fier learns a decision function for each of the three
classes, we can exploit the scores of the classes to
measure the confidence of the LR classifier about
its decision. If the LR classifier is confident, we
use its prediction directly, otherwise we use the
SVR prediction.
For the cases where one of the three decision
functions for the LR classifier is positive, we select
the prediction directly, falling back to SVR when
the classifier is not confident about any of the three
classes. We implement the LR+SVR classifier
combination inside the QuEst framework.
4 Results
Table 1 shows cross-validation results for the 17
baseline features, the combination of all features
and target-side features only. We do not show
combinations of individual feature sets and base-
line features that do not improve over the base-
5
We only discovered this limitation of the default config-
uration after the system submission, see Sections 4 and 5.
331
Features Classifier RMSE MAE
Basel.17 LR+SVR 0.75 0.62
ALL LR+SVR 0.74 0.59
ALL LR> 0.5+SVR 0.75 0.58
Target LR+SVR 0.75 0.59
ALL LR> 0.5+SVR-r 0.78 0.55
Table 1: Cross-validation results for English to
German. LR > 0.5 indicates that we require the
LR decision function to be > 0.5. SVR-r rounds
the output to the nearest natural number.
line. Several experiments, including those with the
semantic similarity feature sets, are thus omitted.
Furthermore, we only exemplify one language pair
(English to German), as the other language pairs
show similar patterns. The feature set target con-
tains the subset of the QuEst black-box features
(Section 2.1) which only examine the target side.
Our best results for English to German in the
cross-validation experiments are achieved by com-
bining a logistic regression (LR) classifier with
support vector regression (SVR). Furthermore,
performance on the cross-validation is slightly im-
proved for the mean absolute error (MAE) by
rounding SVR scores to the nearest integer. For
the root-mean-square error (RMSE), rounding has
the opposite effect.
Performing a more fine-grained grid search for
the meta-parameters C, ? and  after system sub-
mission, we were able to match the scores for
the baseline features published on the shared task
website.
4.1 Parameters for the Final Models
The final two models for system submission are
trained on the full data set. We submit our best sys-
tem according to MAE in cross-validation com-
bining LR, SVR and rounding with all features
(ALL) as DCU-MIXED. For our second submis-
sion, we choose SVR on its own (system DCU-
SVR). For English-Spanish, we only submit DCU-
SVR.
5 Conclusions and Future Work
We identified improperly optimised parameters of
the SVR component as the cause, or at least as a
contributing factor, for the placement of our sys-
tems below the official baseline system. Other po-
tential factors may be an error in our experimen-
tal setup or over-fitting. Therefore, we plan to re-
peat the experiments with a more fine-grained grid
search for optimal parameters and/or will try an-
other machine learning toolkit.
Unfortunately, due to the above problems with
our system so far, we cannot draw conclusions
about the effectiveness of our novel feature sets.
A substantial gain is achieved on the MAE met-
ric with the rounding method, indicating that the
majority of prediction errors are below 0.5.
6
Fu-
ture work should account for this effect. Two ideas
are: (a) round all predictions before evaluation
and (b) use more fine-grained gold values, e. g. the
(weighted) average over multiple annotations as in
the WMT 2012 quality estimation task (Callison-
Burch et al., 2012).
For the error grammar method, the next step
will be to adjust the error model to errors found in
translations. It may be possible to do this without a
time-consuming analysis of errors: Wagner (2012)
suggests to use parallel data of authentic errors and
corrections to build the error grammar, first pars-
ing the corrections and then guiding the error cre-
ation procedure with the edit operations inverse to
the corrections. Post-editing corpora can play this
role and have recently become available (Potet et
al., 2012).
Furthermore, future work should explore the
inverse glass-box feature idea with arbitrary tar-
get languages for the MT system. (There is no
requirement that the glass-box system translates
back to the original source language).
Finally, we would like to integrate referential
translation machines (Bic?ici, 2013; Bic?ici and
Way, 2014) into our system as they performed well
in the WMT quality estimation tasks this and last
year.
Acknowledgments
This research is supported by the European Com-
mission under the 7th Framework Programme,
specifically its Marie Curie Programme 317471,
and by the Science Foundation Ireland (Grant
12/CE/I2267) as part of CNGL (www.cngl.ie) at
Dublin City University. We thank the anonymous
reviewers and Jennifer Foster for their comments
on earlier versions of this paper.
6
The simultaneous increase on RMSE can be explained if
there is a sufficient number of errors above 0.5: After squar-
ing, these errors are still quite small, e. g. 0.36 for an error of
0.6, but after rounding, the square error becomes 1.0 or 4.0.
332
References
Joshua Albrecht and Rebecca Hwa. 2008. The role
of pseudo references in MT evaluation. In Proceed-
ings of the Third Workshop on Statistical Machine
Translation, pages 187?190, Columbus, Ohio, June.
Association for Computational Linguistics.
Ergun Bic?ici and Andy Way. 2014. Referential trans-
lation machines for predicting translation quality. In
Proceedings of the Nineth Workshop on Statistical
Machine Translation, Baltimore, USA, June. Asso-
ciation for Computational Linguistics.
Ergun Bic?ici. 2013. Referential translation machines
for quality estimation. In Proceedings of the Eighth
Workshop on Statistical Machine Translation, pages
343?351, Sofia, Bulgaria, August. Association for
Computational Linguistics.
Ezra Black, Steve Abney, Dan Flickinger, Claudia
Gdaniec, Robert Grishman, Philip Harrison, Donald
Hindle, Robert Ingria, Fred Jelinek, Judith Klavans,
Mark Liberman, Mitchell Marcus, Salim Roukos,
Beatrice Santorini, and Tomek Strzalkowski. 1991.
A procedure for quantitatively comparing the syn-
tactic coverage of English grammars. In E. Black,
editor, Proceedings of the HLT Workshop on Speech
and Natural Language, pages 306?311, Morristown,
NJ, USA. Association for Computational Linguis-
tics.
Ond?rej Bojar, Christian Buck, Chris Callison-Burch,
Christian Federmann, Barry Haddow, Philipp
Koehn, Christof Monz, Matt Post, Radu Soricut, and
Lucia Specia. 2013. Findings of the 2013 Work-
shop on Statistical Machine Translation. In Pro-
ceedings of the Eighth Workshop on Statistical Ma-
chine Translation, pages 1?44, Sofia, Bulgaria, Au-
gust. Association for Computational Linguistics.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 Workshop on Statistical Ma-
chine Translation. In Proceedings of the Seventh
Workshop on Statistical Machine Translation, pages
10?51, Montr?eal, Canada, June. Association for
Computational Linguistics.
Victor Chahuneau, Noah A. Smith, and Chris Dyer.
2012. pycdec: A python interface to cdec. Prague
Bull. Math. Linguistics, 98:51?62.
Eugene Charniak and Mark Johnson. 2005. Course-
to-fine n-best-parsing and maxent discriminative
reranking. In Proceedings of the 43rd Annual Meet-
ing of the ACL (ACL-05), pages 173?180, Ann Ar-
bor, Michigan, June. Association for Computational
Linguistics.
Eugene Charniak. 2000. A maximum entropy inspired
parser. In Proceedings of the First Annual Meeting
of the North American Chapter of the Association
for Computational Linguistics (NAACL-00), pages
132?139, Seattle, WA.
Nadir Durrani, Barry Haddow, Kenneth Heafield, and
Philipp Koehn. 2013. Edinburgh?s machine trans-
lation systems for European language pairs. In Pro-
ceedings of the Eighth Workshop on Statistical Ma-
chine Translation, pages 114?121, Sofia, Bulgaria,
August. Association for Computational Linguistics.
Jennifer Foster. 2007. Treebanks gone bad: Parser
evaluation and retraining using a treebank of un-
grammatical sentences. International Journal on
Document Analysis and Recognition, 10(3-4):129?
145.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In MT summit, vol-
ume 5, pages 79?86.
Liangyou Li, Xiaofeng Wu, Santiago Cort?es Va??llo, Jun
Xie, Jia Xu, Andy Way, and Qun Liu. 2014. The
DCU-ICTCAS-Tsinghua MT system at WMT 2014
on German-English translation task. In Proceed-
ings of the Nineth Workshop on Statistical Machine
Translation, Baltimore, USA, June. Association for
Computational Linguistics.
Mitchell Marcus, Grace Kim, Mary Ann
Marcinkiewicz, Robert MacIntyre, Ann Bies,
Mark Ferguson, Karen Katz, and Britta Schas-
berger. 1994. The Penn Treebank: Annotating
predicate argument structure. In Proceedings of
the 1994 ARPA Speech and Natural Language
Workshop, pages 114?119.
Michael McCandless, Erik Hatcher, and Otis Gospod-
netic. 2010. Lucene in Action, Second Edition:
Covers Apache Lucene 3.0. Manning Publications
Co., Greenwich, CT, USA.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings of
the 40th Annual Meeting on Association for Compu-
tational Linguistics (ACL02), pages 311?318, Mor-
ristown, NJ, USA. Association for Computational
Linguistics.
Marion Potet, Emmanuelle Esperanc?a-Rodier, Laurent
Besacier, and Herv?e Blanchon. 2012. Collection
of a large database of French-English SMT output
corrections. In Proceedings of the 8th International
Conference on Language Resources and Evaluation
(LREC).
Raphael Rubino, Jennifer Foster, Joachim Wagner, Jo-
hann Roturier, Rasul Samad Zadeh Kaljahi, and
Fred Hollowood. 2012. Dcu-symantec submis-
sion for the wmt 2012 quality estimation task. In
Proceedings of the Seventh Workshop on Statisti-
cal Machine Translation, pages 138?144, Montr?eal,
Canada, June. Association for Computational Lin-
guistics.
Raphael Rubino, Joachim Wagner, Jennifer Foster, Jo-
hann Roturier, Rasoul Samad Zadeh Kaljahi, and
Fred Hollowood. 2013. DCU-Symantec at the
333
WMT 2013 quality estimation shared task. In Pro-
ceedings of the Eighth Workshop on Statistical Ma-
chine Translation, pages 392?397, Sofia, Bulgaria,
August. Association for Computational Linguistics.
Magnus Sahlgren. 2005. An introduction to random
indexing. In Methods and Applications of Seman-
tic Indexing Workshop at the 7th International Con-
ference on Terminology and Knowledge Engineering
(TKE), volume 5, Copenhagen, Denmark.
Geoffrey Sampson and Anna Babarczy. 2003. A test of
the leaf-ancestor metric for parse accuracy. Natural
Language Engineering, 9(4):365?380.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of the
International Conference on New Methods in Lan-
guage Processing, Manchester, United Kingdom.
Kashif Shah, Eleftherios Avramidis, Ergun Bic?ici, and
Lucia Specia. 2013. QuEst - design, implemen-
tation and extensions of a framework for machine
translation quality estimation. The Prague Bulletin
of Mathematical Linguistics, 100.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of association for machine transla-
tion in the Americas, pages 223?231.
Radu Soricut and Abdessamad Echihabi. 2010.
Trustrank: Inducing trust in automatic translations
via ranking. In Proceedings of the 48th Annual
Meeting of the Association for Computational Lin-
guistics, pages 612?621, Uppsala, Sweden, July. As-
sociation for Computational Linguistics.
Lucia Specia, Kashif Shah, Jose G.C. de Souza, and
Trevor Cohn. 2013. QuEst - a translation quality es-
timation framework. In Proceedings of the 51st An-
nual Meeting of the Association for Computational
Linguistics: System Demonstrations, pages 79?84,
Sofia, Bulgaria, August. Association for Computa-
tional Linguistics.
Joachim Wagner, Jennifer Foster, and Josef van Gen-
abith. 2009. Judging grammaticality: Experiments
in sentence classification. CALICO Journal (Special
Issue of the 2008 CALICO Workshop on Automatic
Analysis of Learner Language), 26(3):474?490.
Joachim Wagner. 2012. Detecting grammatical errors
with treebank-induced, probabilistic parsers. Ph.D.
thesis, Dublin City University, Dublin, Ireland.
334
