Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1298?1307,
Singapore, 6-7 August 2009.
c
?2009 ACL and AFNLP
Refining Grammars for Parsing with Hierarchical Semantic Knowledge
Xiaojun Lin, Yang Fan, Meng Zhang, Xihong Wu
?
, Huisheng Chi
Speech and Hearing Research Center
Key Laboratory of Machine Perception (Ministry of Education)
School of Electronics Engineering and Computer Science
Peking University, Beijing, 100871, China
{linxj, fanyang, zhangm, wxh}@cis.pku.edu.cn, chi@pku.edu.cn
Abstract
This paper proposes a novel method to
refine the grammars in parsing by utiliz-
ing semantic knowledge from HowNet.
Based on the hierarchical state-split ap-
proach, which can refine grammars au-
tomatically in a data-driven manner, this
study introduces semantic knowledge into
the splitting process at two steps. Firstly,
each part-of-speech node will be anno-
tated with a semantic tag of its termi-
nal word. These new tags generated in
this step are semantic-related, which can
provide a good start for splitting. Sec-
ondly, a knowledge-based criterion is used
to supervise the hierarchical splitting of
these semantic-related tags, which can al-
leviate overfitting. The experiments are
carried out on both Chinese and English
Penn Treebank show that the refined gram-
mars with semantic knowledge can im-
prove parsing performance significantly.
Especially with respect to Chinese, our
parser achieves an F
1
score of 87.5%,
which is the best published result we are
aware of.
1 Introduction
At present, most high-performance parsers are
based on probabilistic context-free grammars
(PCFGs) in one way or another (Collins, 1999;
Charniak and Johnson, 2005; Petrov and Klein,
2007). However, restricted by the strong context-
free assumptions, the original PCFG model which
simply takes the grammars and probabilities off a
treebank, does not perform well. Therefore, a va-
riety of techniques have been developed to enrich
and generalize the original grammar, ranging from
lexicalization to symbol annotation.
?
Corresponding author: Xihong Wu.
Lexicalized PCFGs use the structural features
on the lexical head of phrasal node in a tree, and
get significant improvements for parsing (Collins,
1997; Charniak, 1997; Collins, 1999; Charniak,
2000). However, they suffer from the problem of
fundamental sparseness of the lexical dependency
information. (Klein and Manning, 2003).
In order to deal with this limitation, a variety
of unlexicalized parsing techniques have been pro-
posed. Johnson (1998) annotates each node by
its parent category in a tree, and gets significant
improvements compared with the original PCFGs
on the Penn Treebank. Then, some manual and
automatic symbol splitting methods are presented,
which get comparable performance with lexical-
ized parsers (Klein and Manning, 2003; Matsuzaki
et al, 2005). Recently, Petrov et al (2006) in-
troduces an automatic hierarchical state-split ap-
proach to refine the grammars, which can alter-
nately split and merge the basic nonterminals by
the Expectation-Maximization (EM) algorithm. In
this method, the nonterminals are split to differ-
ent degrees, as appropriate to the actual complex-
ity in the data. The grammars refined in this way
are proved to be much more accurate and compact
than previous work on automatic annotation. This
data-driven method still suffers from the overfit-
ting problem, which may be improved by integrat-
ing other external information.
In this paper, we propose a novel method that
combines the strengths of both data-driven and
knowledge-driven strategies to refine grammars.
Based on the work proposed by Petrov et al
(2006), we use the semantic knowledge from
HowNet (Dong and Dong, 2000) to supervise
the hierarchical state-split process at the part-of-
speech(POS) level. At first, we define the most
general hypernym in HowNet as the semantic class
of a word, and then use this semantic class to ini-
tialize the tag of each POS node. In this way, a
new set of semantic-related tags is generated, and
1298
a good starting annotation is provided to reduce
the search space for the EM algorithm in the split-
ting process. Then, in order to mitigate the overfit-
ting risk, the hierarchical hypernym-hyponym re-
lation between hypernyms in HowNet is utilized
to supervise the splitting of these new semantic-
related tags. By introducing a knowledge-based
criterion, these new tags are decided whether or
not to split into subcategories from a semantic per-
spective. To investigate the effectiveness of the
presented approach, several experiments are con-
duced on both Chinese and English. They reveal
that the semantic knowledge is potentially useful
to parsing.
The remainder of this paper is organized as
follows. Section 2 reviews some closely related
works, including the lexical semantic related pars-
ing and the hierarchical state-split unlexicalized
parsing. In section 3, the presented method for
grammar refining is described in detail, and sev-
eral experiments are carried out for evaluation in
Section 4. Conclusions are drawn in Section 5.
2 Background
This paper tries to refine the grammars through
an improved hierarchical state-split process in-
tegrated with semantic knowledge. The related
works are reviewed as follows.
2.1 Lexical Semantic Related Parsing
Semantic knowledge is useful to resolving syntac-
tic ambiguities, and a variety of researches focus
on how to utilize it. Especially in recent years,
a conviction arose that semantic knowledge could
be incorporated into the lexicalized parsing.
Based on the lexicalized grammars, Bikel
(2000) attempts at combining parsing and word
sense disambiguation in a unified model, using a
subset of SemCor (Miller et al, 1994). Bikel
(2000) evaluates this model in a parsing context
with sense information from WordNet, but does
not get improvements on parsing performance.
Xiong et al (2005) combines word sense from
CiLin and HowNet (two Chinese semantic re-
sources) in a generative parsing model, which gen-
eralizes standard bilexical dependencies to word-
class dependencies, and indeed help to tackle the
sparseness problem in lexicalized parsing. The
experiments show that the parse model combined
with word sense and the most special hypernyms
achieves a significant improvement on Penn Chi-
nese Treebank. This work only considers the most
special hypernym of a word, rather than other
hypernyms at different levels of the hypernym-
hyponym hierarchy.
Then, Fujita et al (2007) uses the Hinoki tree-
bank as training data to train a discriminative parse
selection model combining syntactic features and
word sense information. Instead of utilizing the
most special hypernym, the word sense informa-
tion in this model is embodied with more general
concepts. Based on the hand-craft sense informa-
tion, this model is proved to be effective for parse
selection.
Recently, Agirre et al (2008) train two lexical-
ized models (Charniak, 2000; Bikel, 2004) on pre-
processed inputs, where content words are substi-
tuted with semantic classes from WordNet. By in-
tegrating the word semantic classes into the pro-
cess of parser training directly, these two models
obtain significant improvements in both parsing
and prepositional phrase attachment tasks. Zhang
(2008) does preliminary work on integrating POS
with semantic class of words directly, which can
not only alleviate the confusion in parsing, but also
infer syntax and semantic information at the same
time.
2.2 The Hierarchical State-split Parsing
In order to alleviate the context-free assumptions,
Petrov et al (2006) proposes a hierarchical state-
split approach to refine and generalize the orig-
inal grammars, and achieves state-of-the-art per-
formance. Starting with the basic nonterminals,
this method repeats the split-merge (SM) cycle to
increase the complexity of grammars. That is, it
splits every symbol into two, and then re-merges
some new subcategories based on the likelihood
computation.
Splitting
In each splitting stage, the previous syntactic sym-
bol is split into two subcategories, and the EM al-
gorithm is adopted to learn probability of the rules
for these latent annotations to maximize the like-
lihood of trees in the training data. Finally, each
symbol generates a series of new subcategories in
a hierarchical fashion. With this method, the split-
ting strategy introduces more context information,
and the refined grammars cover more linguistic in-
formation which helps resolve the syntactic ambi-
guities.
1299
However, it is worth noting that the EM algo-
rithm does not guarantee a global optimal solution,
and often gets stuck in a suboptimal configuration.
Therefore, a good starting annotation is expected
to help alleviate this problem, as well as reduce the
search space for EM.
Merging
It is obvious that using more derived subcategories
can increase accuracy, but the refined grammars fit
tighter to the training data, and may lead to over-
fitting to some extent. In addition, different sym-
bols should have their specific numbers of subcat-
egories. For example, the comma POS tag should
have only one subcategory, as it always produces
the terminal comma. On the contrary, the noun
POS tag and the verb POS tag are expected to have
much more subcategories to express their context
dependencies. Therefore, it is not reasonable to
split them in the same way.
The symbol merging stage is introduced to al-
leviate this defect. This approach splits symbols
only where needed, and it is implemented by split-
ting each symbol first and then measure the loss in
likelihood incurred when removing this subcate-
gory. If the loss is small, it means that this subcate-
gory does not take enough information and should
be removed. In general it is hard to decide the
threshold of the likelihood loss, and this merging
stage is often executed by removing a certain pro-
portion of subcategories, as well as giving priority
to the most informative subcategories.
By splitting and merging alternately, this
method can refine the grammars step by step to
mitigate the overfitting risk to some extent. How-
ever, this data-driven method can not solve this
problem completely, and we need to find other ex-
ternal information to improve it.
Analysis
The hierarchical state-split approach is used to
split all the symbols in the same way. Table 1 cites
the subcategories for several POS tags, along with
their two most frequent words. Results show that
the words in the same subcategory of POS tags are
semantic consistent in some cases. Therefore, it
is expected to optimize the splitting and merging
process at the POS level with semantic knowledge.
NR
NR-0 ???(Daja river) ???(Nepal)
NR-1 ??(Sony) ???(Bole Co.)
NR-2 ??(C. Hua) ???(T. Wen)
NR-3 ???(S. Yue) ?(Shang)
LC
LC-0 ??(middle) ??(right)
LC-1 ??(before) ??(since)
LC-2 ??(start) ?(end)
LC-3 ??(till) ?(end)
P
P-0 ??(whenever) ??(as for)
P-1 ??(like) ??(as)
P-2 ??(look to) ??(according to)
P-3 ?(be close to) ??(contrast)
Table 1: The two most frequent words in the sub-
categories of several POS tag.
3 Integration with Semantic Knowledge
In this paper, the semantic knowledge is used to re-
fine grammars by improving the automatic hierar-
chical state-split approach. At first, in order to pro-
vide good starting annotations to reduce the search
space for the EM algorithm, we try to annotate the
tag of each POS node with the most general hyper-
nym of its terminal word. In this way, we generate
a new set of semantic-related tags. And then, in-
stead of splitting and merging all symbols together
automatically, we propose a knowledge-based cri-
terion with hierarchical semantic knowledge to su-
pervise the splitting of these new semantic-related
tags.
3.1 HowNet
The semantic knowledge resource we use is
HowNet, which is a common sense knowledge
base unveiling concepts and inter-conceptual re-
lations in Chinese and English.
As a knowledge base of graph structure,
HowNet is devoted to demonstrating the proper-
ties of concepts through sememes and relations
between sememes. Broadly speaking, a sememe
refers to the smallest basic semantic unit that can-
not be reduced further, which can be represented
in English and their Chinese equivalents, such as
the sememe institution|??. The relations expli-
cated in HowNet include hypernym-hyponym re-
lations, location-event relations, time-event rela-
tions and so on. In this work, we mainly focus on
1300
.vitality
is full ofThe goveronmentThe goveronment is full of
.
IP
NP VP PU
NN VV NP ?
?? ?? NN
vitality
a.
??
IP
NP VP PU
NN-Entity VV-Event NP ?
?? ?? NN-Attribute
b.
??
Figure 1: The two syntax trees of the sentence "The government is full of vitality". a. is the original
syntax tree, b. is the syntax tree in which each tag of the POS node is annotated with the most general
hypernym of its terminal word.
the hypernym-hyponym relations. Take the word
??(government) as an example, its hypernyms with
the hierarchical hypernym-hyponym relations are
listed below from speciality to generality, which
we call hierarchical semantic information in this
paper.
institution|???group|???thing|???entity|??
It is clear that this word ??(government) has hy-
pernyms from the most special hypernym institu-
tion|?? to the most general hypernym entity|??
in a hierarchical way.
In HowNet(Update 2008), there are 173535
concepts, with 2085 sememes. The sememes are
categorized into entity, event, attribute, attribute
value, etc., each corresponding to a sememe hi-
erarchy tree.
3.2 Annotating the Training Data
One of the original motivations for the grammar
refinement is that the original symbols, especially
the POS tags, are usually too general to distin-
guish the context dependencies. Take the sentence
in Figure 1 for example, the word ??(government)
should have different context dependencies com-
pared with the word ??(vitality), although both of
them have the same POS tag "NN". In fact, the
two words are defined in HowNet with different
hypernyms. The word ??(government) is defined
as a kind of objective things, while the word ?
?(vitality) is defined as a property that is often used
to describe things. It is obvious that the different
senses can represent their different syntax struc-
tures, and we expect to refine the POS tags with
semantic knowledge.
In the automatic hierarchical state-split ap-
proach introduced above, the EM algorithm is
used to search for the maximum of the likelihood
during the splitting process, which can generate
subcategories for POS tags to express the context
dependencies. However, this method often gets
stuck in a suboptimal configuration, which varies
depending on the start point. Therefore, a good
start of the annotations is very important. As it is
displayed in Figure 1, we annotate the tag of each
POS node with the hypernym of its terminal word
as the starting annotation. There are two problems
that we have to consider in this process: a) how to
choose the appropriate semantic granularity, and
b) how to deal with the polysemous words.
As mentioned above, the semantic information
of each word can be represented as hierarchi-
cal hypernym-hyponym relations among its hyper-
nyms. In general, it is hard to decide the appro-
priate level of granularity to represent the word.
The semantic class is only used as the starting an-
notations of POS tags to reduce the search space
for EM in our method. It is followed by the hi-
erarchical state-split process to further refine the
starting annotations based on the structural infor-
mation. If more special kinds of semantic classes
are chosen, it will make the structural information
weaker. As annotations with the special hyper-
nym always defeat some of the advantage of au-
tomatically latent annotations learning, we anno-
tate the training data with the most general hyper-
nym. For example, as shown in Figure 1, the POS
tag "NN" of ??(government) is annotated as "NN-
Entity", and "NN" of ??(energy) is annotated as
"NN-Attribute".
Another problem is how to deal with the polyse-
mous words in HowNet. In fact, when we choose
the most general hypernym as the word?s semantic
1301
??(beast)
??(insect)
??(banana) 
??(orange)
??(noon) 
??(forenoon)
??(north)
??(south)
noon forenoon 
north south
noon forenoon north southbeast insect banana orange
entity| ??
thing| ?? time| ?? direction| ??
animal| ? fruit| ??
Continue Splitting...
Having hyponyms...
NN-Entity HowNet
beast insect
banana orange
Figure 2: A schematic figure for the hierarchical state-split process of the semantic-related tag "NN-
Entity". Each subcategory of this tag has its own word set, and corresponds to one hypernym at the
appropriate level in HowNet.
representation, this problem has been alleviated to
a large extent. In this paper we adopt the first sense
option as our word sense disambiguation (WSD)
strategy to determine the sense of each token in-
stance of a target word. That is to say, all token in-
stances of a given word are tagged with the sense
that occurs most frequently in HowNet. In addi-
tion, we keep the tag of the POS node whose ter-
minal word is not defined in HowNet unchanged.
3.3 Supervising the Hierarchical State-split
Process
With the method proposed above, we can produce
a good starting annotation with semantic knowl-
edge, which is of great use to constraining the au-
tomatic splitting process. Our parser is trained on
the good starting annotations with the automatic
hierarchical state-split process, and gets improve-
ments compared with the original training data.
However, during this process, only the most gen-
eral hypernyms are used as the semantic repre-
sentation of words, and the hierarchical semantic
knowledge is not explored. In addition, the auto-
matic process tries to refine all symbols together
through a data-driven manner, which suffers the
overfitting risk.
After annotating the training data with hyper-
nyms, a new set of semantic-related tags such as
"NN-Entity" is produced. We treat the refining
process of these semantic-related tags as the spe-
cializing process of hypernym with hierarchical
semantic knowledge. Each subcategory of these
tags corresponds to a appropriate special level of
hypernym in the HowNet. For example, every sub-
category of "NN-Entity" could corresponds to a
appropriate hyponym of entity|??.
We integrate the hierarchical semantic knowl-
edge into the original hierarchical state-split pro-
cess to refine these semantic-related tags. First
of all, it is necessary to establish the mapping
from each subcategory of these semantic-related
tags to the hypernym at the appropriate level in
HowNet. Then, instead of likelihood judgment, a
knowledge-based criterion is proposed, to decide
whether or not to remove the new subcategories
of these tags. That is to say, once the parent tag
of this new subcategory is mapped onto the most
special hypernym without any hyponym, it should
be removed immediately.
The schematic Figure 2 demonstrates this se-
mantically supervised splitting process. The left
part of this figure is the subcategories of the
semantic-related tag "NN-Entity", which is split
hierarchically. As expressed by the dashed line,
each subcategory corresponds to one hypernym in
the right part of this figure. If the hypernym node
has no hyponym, the corresponding subcategory
will stop splitting.
The mapping from each subcategory of these
semantic-related tags to the hypernym at the ap-
propriate level is implemented with the word set
related to this subcategory. As it is shown in Fig-
1302
DataSet
Chinese English
Xue et al (2002) Marcus et al (1993)
TrainSet Art. 1-270,400-1151 Sections 2-21
DevSet Articles 301-325 Section 22
TestSet Articles 271-300 Section 23
Table 2: Experimental setup.
ure 2, the original tag "NN-Entity" treats all the
words it products as its word set. Once the orig-
inal category is split into two subcategories, its
word set is also split, through forcedly dividing
each word in the word set into one subcategory
which is most frequent with this word. And then,
each subcategory is mapped onto the most specific
hypernym that contains its related word set en-
tirely in HowNet. On this basis, a new knowledge-
based criterion is introduced to enrich and gener-
alize these semantic-related tags, with purpose of
fitting to the hierarchical semantic structure rather
than the training data.
4 Experiments
In this section, we designed several experiments to
investigate the validity of refining grammars with
semantic knowledge.
4.1 Experimental Setup
We did experiments on Chinese and English. In
order to make a fair comparison with previous
works, we split the standard corpora as shown
in Table 2. Our parsers were evaluated by the
EVALB parseval reference implementation
1
. The
Berkeley parser
2
was used to train the models with
the original automatic hierarchical state-split pro-
cess. The semantic resource we used to improve
parsing was HowNet, which has been introduced
in Subsection 3.1. Statistical significance was
checked using Dan Bikel?s randomized parsing
evaluation comparator with the default setting of
10,000 iterations
3
.
4.2 Semantic Representation Experiments
First of all, we ran experiments with different se-
mantic representation methods on Chinese. The
polysemous words in the training set were anno-
tated with the WSD strategy of first sense option,
1
http://nlp.cs.nyu.edu/evalb/.
2
http://code.google.com/p/berkeleyparser/.
3
http://www.cis.upenn.edu/ dbikel/software.html.
which was proved to be useful in Agirre et al
(2008).
As mentioned in Subsection 3.1, the semantic
information of each word can be represented as
a hierarchical relation among its hypernyms from
specialty to generalization in HowNet. In order to
choose the appropriate level of granularity to rep-
resent words, we annotated the training set with
different levels of granularity as semantic repre-
sentation. In our experiments, the automatic hier-
archical state-split process is used to train models
on these training sets with different level of seman-
tic representation.
We tried two kinds of semantic representations,
one is using the most general hypernym, and the
other is using the most special hypernym. Results
in Figure 3 proved the effectiveness of our method
in Subsection 3.2. When we annotated the tag of
each POS node with the most general hypernym of
its terminal word, the parser performs much bet-
ter than both the baseline and the one annotated
with the most special hypernym. Moreover, the F
1
score starts dropping after 3 training iterations on
the training set annotated with the most special hy-
pernyms, while it is still improving with the most
general one, indicating overfitting.
1 2 3 4
68
70
72
74
76
78
80
82
84
86
88
 
 
P
a
r
s
i
n
g
 
a
c
c
u
r
a
c
y
 
(
F
1
)
Times of split-merge iteration
 Baseline
 Most Special Hypernym
 Most General Hypernym
Figure 3: Performances on Chinese with different
semantic representations: the training set without
semantic representation, the training set annotated
with the most special hypernyms, and the training
set annotated with the most general hypernyms.
When the training set was annotated with the
most general hypernyms, there were only 57 new
semantic-related tags such as "NN-Entity", "NN-
Attribute" and so on. However, when the train-
ing set was annotated with the most special hyper-
nyms, 4313 new tags would be introduced. Ob-
1303
viously, it introduces too many tags at once and is
difficult to refine appropriate grammars in the sub-
sequent step starting with this over-splitting train-
ing set.
4.3 Grammar Refinement Experiments
Several experiments were carried out on Chinese
and English to verify the effectiveness of refining
grammars with semantic knowledge. We took the
most general hypernym as the semantic represen-
tation, and the polysemous words in the training
set were annotated with the WSD strategy of first
sense option.
In our experiments, three kinds of method were
compared. The baseline was trained on the raw
training set with the automatic hierarchical state-
split approach. Then, we improved it with the se-
mantic annotation, which annotated the raw train-
ing set with the most general hypernyms as se-
mantic representations, while keeping the train-
ing approach used in the baseline unchanged.
Further, our knowledge-based criterion was in-
troduced to supervise the automatic hierarchical
state-split process with semantic knowledge.
In this section, since most of the parsers (includ-
ing the baseline parser and our advanced parsers)
had the same behavior on development set that the
accuracy continued increasing in the five begin-
ning iterations and then dropped at the sixth iter-
ation, we chose the results at the fifth iteration as
our final test set parsing performance.
Performances on Chinese
Figure 4 shows that refining grammars with se-
mantic knowledge can help improve parsing per-
formance significantly on Chinese (sentences of
length 40 or less). Benefitting from the good start-
ing annotations, our parser achieved significant
improvements compared with the baseline (86.8%
vs. 86.1%, p<.08). It proved that the good start-
ing annotations with semantic knowledge were ef-
fective in the splitting process. Further, we su-
pervised the splitting of the new semantic-related
tags from the semantic annotations, and achieved
the best results at the fifth iteration. The best F
1
score reached 87.5%, with an error rate reduction
of 10.1%, relative to the baseline (p<.004).
Table 3 compared our methods with the best
previous works on Chinese. The result showed
that refining grammars integrated with semantic
knowledge could resolve syntactic ambiguities re-
LP LR F1
82
83
84
85
86
87
88
89
90
91
92
 
  
87.5
86.8
86.1
86.0
85.7
84.9
88.9
88.0
Evaluation Criterion
 Baseline
 Semantic Annotation
 Semantic Annotation & Knowledge-based Criterion
87.3
Figure 4: Performances at the fifth iteration on
Chinese (sentences of length 40 or less) with three
methods: the baseline, the parser trained on the
semantic annotations with automatic method, and
the parser trained on the semantic annotations with
knowledge-based criterion.
Parser
? 40 words all
LP LR F
1
LP LR F
1
Chiang and
81.1 78.8 79.9 78.0 75.2 76.6
Bikel (2002)
Petrov and
86.9 85.7 86.3 84.8 81.9 83.3
Klein (2007)
This Paper 88.9 86.0 87.5 86.0 83.1 84.5
Table 3: Our final parsing performance compared
with the best previous works on Chinese.
markably and achieved the state-of-the-art perfor-
mance on Chinese.
Performances on English
In order to verify the effectiveness of our method
on other languages, we carried out some experi-
ments on English. HowNet is a common sense
knowledge base in Chinese and English, there-
fore, it was still utilized as the knowledge source
in these experiments.
The same three methods were compared on En-
glish (sentences of length 40 or less), and the re-
sults were showed in Table 4. Compared with the
baseline (90.1%), the parsers trained with the se-
mantic annotation, while using different splitting
methods introduced in Section 3, achieved an F
1
score of 90.2% and 90.3% respectively. The re-
sults showed that our methods could get a small
but stable improvements on English (p<.08).
1304
Subcategory Refined from the Original Training Set
PN-0
??(aid foreign),??(aunt),??(self),?(you),?(donate),??(those),??(appearence),
???(self),??(we),?(that),??(the above),??(there),??(other),??(below)
Subcategories Fefined from the Good Starting Annotations
PN-0 ??(aunt),??(self),???(self),??(we),?(you)
PN-Event-0 ??(aid foreign),?(donate)
PN-AttributeValue-2 ??(the above),??(those),?(that),??(other),??(below)
Table 5: Several subcategories that generated from the original training set and the good starting annota-
tions respectively.
Method F
1
Baseline 90.1
Semantic Annotations 90.2
Semantic Annotations &
90.3
Knowledge-based Criterion
Table 4: Performances at the fifth iteration on En-
glish (sentences of length 40 or less) with three
methods: the baseline, the parser trained on the
semantic annotations with automatic method, and
the parser trained on the semantic annotations with
knowledge-based criterion.
These results on English were preliminary, and
we did not introduce any language dependent op-
eration such as morphological processing. Since
only the lemma of English words can be found
in HowNet, we just annotated two kinds of POS
tags "VB"(Verb, base form) and "NN"(Noun, sin-
gular or mass) with semantic knowledge, on the
contrary, we annotated almost all POS tags whose
corresponding words could be found in HowNet
on Chinese. This might be the reason that the
improvement on the English Treebank was much
smaller than that of Chinese. It is expected to
achieve more improvements through some mor-
phological analysis in the future.
4.4 Results and Analysis
So far, a new strategy has been introduced to re-
fine the grammars in two steps, and achieved sig-
nificant improvements on parsing performance. In
this section, we analyze the grammars learned at
different steps, attempting to explain how the se-
mantic knowledge works.
It is hard to inspect all the grammars by hand.
Since the semantic knowledge is mainly used for
generating and splitting new semantic-related tags
in our method, we focus on the refined subcate-
gories of these tags.
First, we examine the refined subcategories of
POS tags, which are generated from the original
training set and the good starting annotations re-
spectively. Several subcategories are listed and
compared in Table 5, along with their frequent
words. It can be seen that the subcategories refined
with semantic knowledge are more consistent than
the previous one. For example, the subcategory
"PN-0", which is refined from the original training
set, produces a lot of words without semantic con-
sistence. In contrast, we refine the subcategories
"PN-0", "PN-Event-0" and "PN-AttributeValue-2"
from the good starting annotations. Each of them
produces a small but semantic consistent word set.
In order to inspect the difference between the
automatic splitting process and the semantic based
one, we compare the numbers of subcategories re-
fined in these two processes. Since it is hard to list
all the semantic-related tags here, three parts of
the semantic-related tags were selected and listed
in Table 6, along with the number of their subcat-
egories. The first part is the noun and verb related
tags, which are most heavily split in both two pro-
cesses. It is clear that the semantic based split-
ting process can generate more subcategories than
the automatic one, because the semantic structures
of noun and verb are sophisticated. The second
part lists the tags that have much more subcate-
gories (? 4) from the automatic splitting process
than the semantic based one, and the third part
vice verse. It can be seen that most of the sub-
categories in the second part are functional cate-
gories, while most of the subcategories in the third
part are content categories. It means that the se-
mantic based splitting process is prone to generat-
ing less subcategory for the functional categories,
but more subcategories for the content categories.
This tendency is in accordance with the linguis-
tic intuition. We believe that it is the main effect
1305
Semantic-related
Automatic split Semantic based
tag number split numebr
NN-Attribute 30 30
NN-AttributeValue 25 27
NN-Entity 32 32
NN-Event 31 30
VV-Attribute 2 2
VV-AttributeValue 27 27
VV-Entity 22 26
VV-Event 29 32
BA-event 13 5
CS-AttributeValue 29 16
CS-entity 22 15
OD-Attribute 13 7
PN-Attribute 26 22
AS-AttributeValue 2 7
JJ-event 4 8
NR-AttributeValue 9 13
NT-event 12 18
VA-AttributeValue 22 27
VA-event 7 11
Table 6: The number of subcategories learned
from two approaches: the automatic hierarchical
state-splitting, and the semantic based splitting.
of our knowledge-based criterion, because it ad-
justs the splitting results dynamically with seman-
tic knowledge, which can alleviate the overfitting
risk.
5 Conclusions
In this paper, we present a novel approach to in-
tegrate semantic knowledge into the hierarchical
state-split process for grammar refinement, which
yields better accuracies on Chinese than previ-
ous methods. The improvements are mainly ow-
ing to two aspects. Firstly, the original treebank
is initialized by annotating the tag of each POS
node with the most general hypernym of its ter-
minal word, which reduces the search space for
the EM algorithm and brings an initial restrict to
the following splitting step. Secondly, the splitting
process is supervised by a knowledge-based crite-
rion with the new semantic-related tags. Benefit-
ting from the hierarchical semantic knowledge, the
proposed approach alleviates the overfitting risk in
a knowledge-driven manner. Experimental results
reveal that the semantic knowledge is of great use
to syntactic disambiguation. The further analysis
on the refined grammars shows that, our method
tends to split the content categories more often
than the baseline method and the function classes
less often.
Acknowledgments
We thank Yaozhong Zhang for the enlighten-
ing discussions. We also thank the anony-
mous reviewers who gave very helpful com-
ments. The work was supported in part by the
National Natural Science Foundation of China
(60535030; 60605016), the National High Tech-
nology Research and Development Program of
China (2006AA010103), the National Key Ba-
sic Research Program of China (2004CB318005,
2004CB318105).
References
E. Agirre, T. Baldwin and D. Martinez. 2008. Improv-
ing parsing and PP attachment performance with
sense information. In Proc. of ACL?08, pages 317-
325.
D. Bikel. 2000. A statistical model for pars-
ing and word-sense disambiguation. In Proc. of
EMNLP/VLC?2000, pages 155-163.
D. Bikel. 2004. Intricacies of Collins? parsing model.
Computational Linguistics, 30(4):479-511.?
E. Charniak. 1997. Statistical parsing with a context-
free grammar and word statistics. In Proc. of
AAAI?97, pages 598-603.
E. Charniak. 2000. A maximum-entropy-inspired
parser. In Proc. of NAACL?00, pages 132-139.
E Charniak and M. Johnson. 2005. Coarse-to-fine n-
best parsing and maxEnt discriminative reranking.
In Proc. of ACL?05, pages 173-180.
D. Chiang and D. Bikel. 2002. Recovering latent infor-
mation in treebanks. In Proc. of COLING?02, pages
183-189.
M. Collins. 1997. Three generative, lexicalised models
for statistical parsing. In Proc. of ACL?97, pages 16-
23.
M. Collins. 1999. Head-driven statistical models for
natural language parsing. Ph.D. thesis, U. of Penn-
sylvania.
Z. Dong and Q. Dong. 2000. HowNet Chinese-
English conceptual database. Technical Re-
port Online Software Database, Released at ACL.
http://www.keenage.com.
1306
S. Fujita, F. Bond, S. Oepen and T. Tanaka 2007. Ex-
ploiting semantic information for HPSG parse se-
lection. In ACL 2007 Workshop on Deep Linguistic
Processing, pages 25-32.
M. Johnson. 1998. PCFG models of linguistic tree rep-
resentations. Computational Linguistics, 24(4):613-
631.
D. Klein and C. Manning. 2003. Accurate unlexical-
ized parsing. In Proc. of ACL?03, pages 423-430.
M. Marcus, B. Santorini, and M. Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish: The Penn Treebank. Computational Linguis-
tics, 19(2):313-330.
T. Matsuzaki, Y. Miyao, and J. Tsujii. 2005. Prob-
abilistic CFG with latent annotations. In Proc. of
ACL?05, pages 75-82.
George A. Miller, Martin Chodorow, Shari Landes,
Claudia Leacock, and Robert G. Thomas. 1994. Us-
ing a semantic concordance for sense identification.
In Proc. of ARPA-HLT Workshop., pages 240-243.
S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006.
Learning accurate, compact, and interpretable tree
annotation. In Proc. of COLING-ACL?06, pages
443?440.
S. Petrov and D. Klein. 2007. Improved inference for
unlexicalized parsing. In Proc. of HLT-NAACL?07,
pages 404-411.
D. Xiong, S. Li, Q. Liu, S. Lin, and Y. Qian. 2005.
Parsing the Penn Chinese treebank with semantic
knowledge. In Proc. of IJCNLP?05, pages 70-81.
N. Xue, F.-D. Chiou, and M. Palmer. 2002. Building
a large scale annotated Chinese corpus. In Proc. of
COLING?02, pages 1-8.
Y. Zhang. 2008. The Study and Realization of Chinese
Parsing with Semantic and Sentence Type Informa-
tion. Master thesis, Peking University.
1307
An Improved CRF based Chinese Language Processing System for SIGHAN
Bakeoff 2007
Xihong Wu, Xiaojun Lin, Xinhao Wang, Chunyao Wu, Yaozhong Zhang and Dianhai Yu
Speech and Hearing Research Center
State Key Laboratory of Machine Perception,
Peking University, China, 100871
{wxh,linxj,wangxh,wucy,zhangyaoz,yudh}@cis.pku.edu.cn
Abstract
This paper describes three systems: the
Chinese word segmentation (WS) system,
the named entity recognition (NER) sys-
tem and the Part-of-Speech tagging (POS)
system, which are submitted to the Fourth
International Chinese Language Processing
Bakeoff. Here, Conditional Random Fields
(CRFs) are employed as the primary mod-
els. For the WS and NER tracks, the n-
gram language model is incorporated in our
CRFs based systems in order to take into ac-
count the higher level language information.
Furthermore, to improve the performances
of our submitted systems, a transformation-
based learning (TBL) technique is adopted
for post-processing.
1 Introduction
Among 24 closed and open tracks in this bakeoff, we
participated in 23 tracks, except the open NER track
of MSRA. Our systems are ranked 1st in 6 tracks,
and get close to the top level in several other tracks.
Recently, Maximum Entropy model(ME) and
CRFs (Low et al, 2005)(Tseng et al, 2005) (Hai
Zhao et al, 2006) turned out to be promising in natu-
ral language processing tracks, and obtain excellent
performances on most of the test corpora of Bake-
off 2005 and Bakeoff 2006. Compared to the gen-
erative models, like HMM, the primary advantage
of CRFs is that it relaxes the independence assump-
tions, which makes it able to handle multiple inter-
acting features between observation elements (Wal-
lach et al, 2004).
However, the ME and CRFs emphasize the rela-
tion of the basic units of sequence, like the Chinese
characters in these tracks. While, the higher level
information, like the relationship of the words is ig-
nored. From this point of view, the n-gram language
model is incorporated in our CRFs based systems in
order to cover the word level language information.
Based on several pilot-experimental results, we
found that the tagging errors always follow some
patterns. In order to find those error patterns and cor-
rect the similar errors, we integrated the TBL post-
processor in our systems. In addition, extra train-
ing data, which is transformed from People Daily
Corpus (Shiwen Yu et al, 2000) with some auto-
extracted transition rules, is used in each corpus for
the open tracks of WS.
The remainder of this paper is organized as fol-
lows. The scheme of our three developed systems
are described in section 2, 3 and 4, respectively. In
section 5, evaluation results based on these systems
are enumerated and discussed. Finally some conclu-
sions are drawn in section 6.
2 Word Segmentation
The WS system mainly consists of three compo-
nents, CRFs, n-gram language model and post-
processing strategies.
2.1 Conditional Random Fields
Conditional Random Fields, as the statistical se-
quence labeling models, achieve great success in
natural language processing, such as chunking (Fei
Sha et al, 2003) and word segmentation (Hai Zhao
et al, 2006). Different from traditional generative
155
Sixth SIGHAN Workshop on Chinese Language Processing
model, CRFs relax the constraint of the indepen-
dence assumptions, and therefore turn out to be more
suitable for natural language tasks.
CRFs model the conditional distribution p(Y |X)
of the labels Y given the observations X directly
with the formulation:
P?(Y |X) = 1Z(X)exp{
?
c?C
?
k
?kfk(Yc, X, c)}
(1)
Y is the label sequence, X is the observation se-
quence, Z(X) is a normalization term, fk is a fea-
ture function, and c is the set of cliques in Graphic.
In our tasks, C = {(yi?1, yi)}, X is the Chinese
character sequence of a sentence.
To label a Chinese character, we need to define
the label tags. Here we have six types of tags ac-
cording to character position in a word (Hai Zhao et
al., 2006):
tag = {B1, B2, B3, I, E, S}
?B1, B2, B3, I, E? represent the first, second, third,
continue, and end character positions in a multi-
character word, and ?S? is the single-character word
tag.
The unigram feature templates used here are:
Cn (n = ?2,?1, 0, 1, 2)
CnCn+1 (n = ?2,?1, 0)
CnCn+1Cn+2 (n = ?1)
Where C0 refers to the current character and
C?n(Cn) is the nth character to the left(right) of the
current character. We also use the basic bigram fea-
ture template which denotes the dependency on the
previous tag and current tag.
2.2 Multi-Model Integration
In order to integrate multi-model information, we
use a log-linear model(Och et al, 2002) to compute
the posterior probability:
Pr (W |C) = p?M1 (W |C)
= exp[
?M
m=1 ?mhm(W,C)]
?
W ? exp[
?M
m=1 ?mhm(W ?, C)]
(2)
Where W is the word sequence, and C is the char-
acter sequence. The decision rule here is:
W0 = argmaxW {Pr(W |C)}
= argmaxW {
M
?
m=1
?mhm(W,C)} (3)
The parameters ?M1 of this model can be opti-
mized by standard approaches, such as the Mini-
mum Error Rate Training used in machine transla-
tion (Och, 2003). In fact, the CRFs approach is
a special case of this framework when we define
M = 1 and use the following feature function:
h1(W,C) = logP?(Y |X) (4)
In our approach, the logarithms of the scores gen-
erated by the two kinds of models are used as feature
functions:
h1(W,C) = logPcrf (W,C)
= log
?
w
i
P?(wi|C) (5)
h2(W,C) = logPlm(W ) (6)
The first feature function(Eq.5) comes from CRFs.
Instead of computing the score of the whole la-
bel sequence Y with character sequence X through
P?(Y |X) directly, we try to get the posterior prob-
ability of a sub-sequence to be tagged as one whole
word P?(wi|C). Then we combine all the score of
words together. The second feature function(Eq.6)
comes from n-gram language model, which aims to
catch the words information.
The log-linear model with the feature functions
described above allows the dynamic programming
search algorithm for efficient decoding. The system
generates the word lattice with posterior probability
P?(wi|C). Then the best word sequence is searched
on the word lattice with the decision rule(Eq.3).
Since arbitrary sub-sequence can be viewed as a
candidate word in word lattice, we need to deal with
the problem of OOV words. The unigram of an OOV
word is estimated as:
Unigram(OOV Word) = pl (7)
where p is the minimal value of unigram scores in
the language model; l is the length of the OOV
word, which is used as a punishment factor to
avoid overemphasizing the long OOV words (Xin-
hao Wang et al, 2006).
2.3 Post-Processing Strategies
The division and combination rule, which has been
proved to be useful in our system of Bakeoff 2006
(Xinhao Wang et al, 2006), is adopted for the post-
processing in the system.
156
Sixth SIGHAN Workshop on Chinese Language Processing
2.4 Training Data Transition
For the WS open tracks, the unique difference from
closed tracks is that the additional training data is
supplemented for model refinement.
For the Simplified Chinese tracks, the additional
training data are collected from People Daily Cor-
pus with a set of auto-extracted transition rules. This
process is performed in a heuristic strategy and con-
tains five steps as follows:
(1) Segment the raw People Daily texts with the cor-
responding system for the closed track of each cor-
pus.
(2) Compare the result of step 1 with People Daily
Corpus to get the conflict pairs. For example,
{pair1: ??? vs. ???}
(Zhemin Jiang)
{pair2: ??? vs. ???}
(catch with two hands)
In each pair, the left phrase follows the People Daily
Corpus segmentation guideline, while the right one
is the phrase obtained from step 1.
(3) Divide the pairs into two sets: the first set con-
tains the pairs with right phrase appearing in the tar-
get training data; the other pairs are in the second
set.
(4) Select sentences which contain the left phrase of
the pairs in the second set from People Daily Cor-
pus.
(5) Transform these selected sentences by replacing
their phrase in the left side of the pair in the first set
to the right one. This is used as our transition rules.
3 Named Entity Recognition
The named entity recognition track is viewed as a
character sequence tagging problem in our NER sys-
tem and the log-linear model mentioned above is
employed again to integrate multi-model informa-
tion. To find the error patterns and correct them,
a TBL strategy is then used in the post-processing
module.
3.1 Model Description
In this NER track, we employe the log-linear model
and use the logarithms of the scores generated by the
two types of models as feature functions. Besides
CRFs, another model is the class-based n-gram lan-
guage model:
h1(Y, X) = logPcrf (Y, X)
= logP?(Y |X) (8)
h2(Y, X) = logPclm(Y, X) (9)
Y is the label sequence and X is the character se-
quence.
CRFs are used to generate the N-best tagging re-
sults with the scores of whole label sequence Y on
character sequence X by P?(Y |X). And then, the
log-linear model is used to reorder the N-best tag-
ging results by integrating the CRFs score and the
class-based n-gram language model score together.
CRFs
In this track, one Chinese character is labeled by
a tag of ten classes, which denoting the beginning,
continue, ending character of a specified named en-
tity or a non-entity character. There are three types
of named entities in these tracks, including person
name, location name and organization name.
In CRFs, the basic features used here are:
Cn (n = ?2,?1, 0, 1, 2)
CnCn+1 (n = ?2,?1, 0, 1)
CnCn+2 (n = ?1)
Besides basic unigram features, the bigram transi-
tion features considering the previous tag is adopted
with template Cn (n = ?2,?1, 0, 1, 2).
Class-Based N-gram Language Model
For the class-based n-gram language model, we
define that each character is a single class, while
each type of named entity is viewed as a single class.
With the character sequence and label sequence, the
class sequence can be generated. Take this sentence
for instance:
???????????
(But Ibrahimov is not satisfied)
Table 1 shows its class sequence. Class-based n-
gram language model can be trained with class se-
quence.
3.2 TBL
Since the analysis on our experiments shows that the
tagging errors always follow some patterns in NER
track, TBL strategy is adopted in our system to find
these patterns and correct the similar errors.
157
Sixth SIGHAN Workshop on Chinese Language Processing
character sequence ? ? ? ? ? ? ? ? ? ? ?
label sequence N Per-B Per-C Per-C Per-C Per-C Per-E N N N N
class sequence ? PERSON ? ? ? ?
Table 1: A class sequence example
Transformation-based learning is a symbolic ma-
chine learning method, introduced by (Eric Brill,
1995). The main idea in TBL is to generate a set of
transformation rules that can correct tagging errors
produced by the initial process.
There are four main procedures in our TBL
framework: An initial state assignment which is op-
erated by the system we described above; a set of al-
lowable templates for rules, ranging from words in
a 3 positions windows and name entity information
in a 3-word window with their combinations consid-
ered, and rules which are learned according to the
tagging differences between training data and results
generated by our system, at last, those rules are in-
troduced to correct similar errors.
4 POS Tagging
The POS tagging track is to assign the part-of-
speech sequence for the correctly segmented word
sequence. In our system, for the CTB corpus, the
CRFs are adopted; however for the other four cor-
pora, considering the limitations of resources and
time, the ME model is adopted. To improve the per-
formance of ME model, the POS tag of the previous
word is taken as a feature and the dynamic program-
ming strategy is used in decoding.
In the closed track, the features include the basic
features and their combined features. Firstly the pre-
vious and next words of the current word are taken
as the basic features. Secondly, based on the anal-
ysis of the OOV words, the first and last characters
of the current word, as well as the length of the cur-
rent word are proven to be effective features for the
OOV POS. Furthermore since the long distance con-
straint word may impact the POS of current word
(Yan Zhao et al, 2006), in the open track, a Chi-
nese parser is imported and the word depended on
the current word is extracted as feature.
5 Experiments and Results
We have participated in 23 tracks, except the open
NER track of MSRA. CRFs, ME model and n-gram
language model are adopted in these systems. Our
implementation uses the CRF++ package1 provided
by Taku Kudo, the Maximum Entropy Toolkit2 pro-
vided by Zhang Le, and the SRILM Toolkit provided
by Andreas Stolcke (Andreas Stolcke et al, 2002).
5.1 Chinese Word Segmentation
In the closed tracks, CRFs and bigram language
model are trained on the given training data for each
corpus. In order to integrate these two models, it is
necessary to train the corresponding parameter ?M1
with Minimum Error Rate Training approache based
on a development data. Since the development data
is not provided in this bakeoff, a ten-fold cross val-
idation approach is employed to implement the pa-
rameter training. A set of parameters can be trained
independently, and then the mean value is calculated
as the estimation of each parameter.
Table 2 gives the results of our WS system for
closed tracks.
baseline +LM +LM+Post
CTB 94.7 94.7 94.8
NCC 92.6 92.4 92.9
SXU 94.7 95.7 95.8
CITYU 92.9 93.7 93.9
CKIP 93.2 93.7 93.7
Table 2: Word segmentation performance on F-
value with different approach for the closed tracks
In the open tracks, as we do not have enough time
to finish the parameter estimation on the new data,
our system adopt the same parameters ?M1 used in
closed tracks. The unique difference from closed
1http://chasen.org/taku/software/CRF++
2http://homepages.inf.ed.ac.uk/s0450736/maxent
toolkit.html
158
Sixth SIGHAN Workshop on Chinese Language Processing
tracks is that extra training data is added for each
corpus to improve the performance. For the Sim-
plified Chinese tracks, additional data comes from
People Daily Corpus which is transformed by our
transition strategy. At the same time, for the Tra-
ditional Chinese tracks, additional data comes from
the training and testing data used in the early Bake-
off. However, we implement two systems for the
CTB open track. The system (a) takes the training
and testing data used in the early Bakeoff as addi-
tional data, and System (b) takes the translated Peo-
ple Daily Corpus as additional data. Table 3 gives
the results of our open WS system.
baseline +LM +LM+Post
CTB(a) 99.2 99.2 99.3
CTB(b) 95.6 95.1 97.0
NCC 93.7 93.0 92.9
SXU 96.4 87.0 95.8
CITYU 95.8 90.6 91.0
CKIP 94.5 94.8 95.1
Table 3: Word segmentation performance on F-
value with different approach for the open tracks
The result shows that the system performance is
sensitive to the parameters ?M1 . Although we train
the useful parameter for closed tracks, it plays a bad
role in open tracks as we do not adapt it for the ad-
ditional training data.
5.2 Named Entity Recognition
In the closed NER tracks, CRFs and class-based tri-
gram language model are trained on the given train-
ing data for each corpus. The same approach em-
ployed in the WS tracks is adopted to train the corre-
sponding parameter ?M1 in our NER systems. Mean-
while, the TBL rules trained via five-fold cross val-
idation approach are also used in post-processing
procedure. Table 4 reports the results of our closed
NER system.
5.3 POS Tagging
The experiments show that the CRFs/ME method is
superior to the TBL method, and the concurrent er-
rors for these two methods are less than 60%. There-
fore we adopted TBL to correct the output results
of CRFs/ME: If the output tags of CRFs/ME and
baseline +LM +LM+Post
MSRA 89.3 89.7 89.9
CITYU 79.3 80.6 80.5
Table 4: Named entity recognition F-value through
different approaches for the closed tracks
TBL are not consistent and the output probability
of CRFs/ME is below a certain threshold, the TBL
results are fixed. Here the 90% of the training set
is taken as the training data and remained 10% is
separated as the development data to get the thresh-
old, which is 0.60 for the CRFs, and 0.90 for the
ME. In addition, the POS tagged corpus of the Chi-
nese Treebank 5.0 from LDC is added to the training
data for CTB open track. In our system, the Berke-
ley Parser (Slav Petrov et al, 2006) is adopted to
obtain the long distance constraint words. The per-
formance achieved by the methods described above
on each corpus are reported in Table 5.
CRFs/ME CRFs/ME
CRFs/ME TBL +TBL +TBL
+Syntax
CTIYU 88.7 87.7 89.1 89.0
CKIP 91.8 91.4 92.2 92.1
CTB 94.0 92.7 94.3 96.5
NCC 94.6 94.3 94.9 95.0
PKU 93.5 93.2 94.0 94.1
Table 5: POS tagging performance on total-accuracy
with different approach
6 Conclusion
In this paper, we have briefly described our systems
participating in the Bakeoff 2007. In the WS and
NER systems, the log-linear model is adopted to in-
tegrate CRFs and language model, which improves
the system performances effectively. At the same
time, system integration approach used in the POS
system also proves its validity. In addition, a heuris-
tic strategy is imported to generate additional train-
ing data for the open WS tracks. Finally, several
post-processing strategies are used to further im-
prove our systems.
159
Sixth SIGHAN Workshop on Chinese Language Processing
References
Jin Kiat Low, Hwee Tou Ng and Wenyuan Guo. 2005.
A Maximum Entropy Approach to Chinese Word Seg-
mentation. Proceedings of the Fourth SIGHAN Work-
shop on Chinese Language Processing. pp. 161-164.
Jeju Island, Korea.
Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel
Jurafsky, Christopher Manning. 2005. A Conditional
Random Field Word Segmenter for Sighan Bakeoff
2005. Proceedings of the Fourth SIGHAN Workshop
on Chinese Language Processing. pp. 168-171. Jeju
Island, Korea.
Hai Zhao, Chang-Ning Huang and Mu Li. 2006. An
Improved Chinese Word Segmentation System with
Conditional Random Field. Proceedings of the Fifth
SIGHAN Workshop on Chinese Language Processing.
pp. 162-165. Sydney, Australia.
Hanna M. Wallach. 2004. Conditional Random Fields:
An Introduction. Technical Report, UPenn CIS TR
MS-CIS-04-21.
Shiwen Yu, Xuefeng Zhu and Huiming Duan. 2000.
Specification of large-scale modern Chinese corpus.
Proceedings of ICMLP?2001. pp. 18-24. Urumqi,
China.
Fei Sha and Fernando Pereira. 2003. Shallow Parsing
with Conditional Random Fields. Proceedings of Hu-
man Language Technology/NAACL. pp. 213-220. Ed-
monton, Canada.
Franz Josef Och and Hermann Ney. 2002. Discrimi-
native training and maximum entropy models for sta-
tistical machine translation. Proceedings of the 40th
Annual Meeting of the Association for Computational
Linguistics (ACL). pp. 295-302. Philadelphia, PA.
Franz Josef Och. 2003. Minimum Error Rate Train-
ing in Statistical Machine Translation. Proceedings of
the 41th Annual Meeting of the Association for Com-
putational Linguistics (ACL). pp. 160-167. Sapporo,
Japan.
Xinhao Wang, Xiaojun Lin, Dianhai Yu, Hao Tian, Xi-
hong Wu. 2006. Chinese Word Segmentation with
Maximum Entropy and N-gram Language Model. the
Fifth SIGHAN Workshop on Chinese Language Pro-
cessing. pp. 138-141. Sydney, Australia.
Eric Brill. 1995. Transformation-based error-driven
learning and natural language processing: a case study
in Part-of-Speech tagging. Computational Lingusitics.
21(4).
Yan Zhao, Xiaolong Wang, Bingquan Liu, and Yi Guan.
2006. Fusion of Clustering Trigger-Pair Features for
POS Tagging Based on Maximum Entropy Model.
Journal of Computer Research and Development.
43(2). pp. 268-274.
Andreas Stolcke. 2002. SRILM - An Extensible Lan-
guage Modeling Toolkit. Proceedings of International
Conference on Spoken Language Processing. pp. 901-
904. Denver, Colorado.
Slav Petrov, Leon Barrett, Romain Thibaux and Dan
Klein. 2006. Learning Accurate, Compact, and Inter-
pretable Tree Annotation. Proceedings of the 21st In-
ternational Conference on Computational Linguistics
and the 44th annual meeting of the ACL. pp. 433-440.
Sydney, Australia.
160
Sixth SIGHAN Workshop on Chinese Language Processing
Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 138?141,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Chinese Word Segmentation with Maximum Entropy
and N-gram Language Model
Wang Xinhao, Lin Xiaojun, Yu Dianhai, Tian Hao, Wu Xihong
National Laboratory on Machine Perception,
School of Electronics Engineering and Computer Science,
Peking University, China, 100871
{wangxh,linxj,yudh,tianhao,wxh}@cis.pku.edu.cn
Abstract
This paper presents the Chinese word seg-
mentation systems developed by Speech
and Hearing Research Group of Na-
tional Laboratory on Machine Perception
(NLMP) at Peking University, which were
evaluated in the third International Chi-
nese Word Segmentation Bakeoff held by
SIGHAN. The Chinese character-based
maximum entropy model, which switches
the word segmentation task to a classi-
fication task, is adopted in system de-
veloping. To integrate more linguistics
information, an n-gram language model
as well as several post processing strate-
gies are also employed. Both the closed
and open tracks regarding to all four cor-
pora MSRA, UPUC, CITYU, CKIP are
involved in our systems? evaluation, and
good performance are achieved. Espe-
cially, in the closed track on MSRA, our
system ranks 1st.
1 Introduction
Chinese word segmentation is one of the core tech-
niques in Chinese language processing and attracts
lots of research interests in recent years. Sev-
eral promising methods are proposed by previous
researchers, in which Maximum Entropy (ME)
model has turned out to be a successful way for
this task (Hwee Tou Ng et al, 2004; Jin Kiat
Low et al, 2005). By employing Maximum En-
tropy (ME) model, the Chinese word segmentation
task is regarded as a classification problem, where
each character will be classified to one of the four
classes, i.e., the beginning, middle, end of a multi-
character word and a single-character word.
However, in a high degree, ME model pays its
emphasis on Chinese characters while debases the
consideration on the relationship of the context
words. Motivated by this view, several strategies
used for reflecting the context words? relationship
and integrating more linguistics information, are
employed in our systems.
As known, an n-gram language model could ex-
press the relationship of the context words well, it
therefore as a desirable choice is imported in our
system to modify the scoring of the ME model.
An analysis on our preliminary experiments shows
the combination ambiguity is another issue that
should be specially tackled, and a division and
combination strategy is then adopted in our sys-
tem. To handle the numeral words, we also intro-
duce a number conjunction strategy. In addition,
to deal with the long organization names problem
in MSRA corpus, a post processing strategy for
organization name is presented.
The remainder of this paper is organized as fol-
lows. Section 2 describes our system in detail.
Section 3 presents the experiments and results.
And in last section, we draw our conclusions.
2 System Description
With the ME model, n-gram language model, and
several post processing strategies, our systems are
established. And detailed description on these
components are given in following subsections.
2.1 Maximum Entropy Model
The ME model used in our system is based on the
previous works (Jin Kiat Low et al, 2005; Hwee
Tou Ng et al, 2004). As mentioned above, the
ME model based word segmentation is a 4-classes
learning process. Here, we remarked four classes,
i.e. the beginning, middle, end of a multi-character
138
word and a single-character word, as b, m, e and s
respectively.
In ME model, the following features (Jin Kiat
Low et al, 2005) are selected:
a) cn (n = ?2,?1, 0, 1, 2)
b) cncn+1 (n = ?2,?1, 0, 1)
c) c?1c+1
where cn indicates the character in the left or right
position n relative to the current character c0.
For the open track especially, three extended
features are extracted with the help of an external
dictionary as follows:
d) Pu (c0)
e) L and t0
f) cnt0 (n = ?1, 0, 1)
where Pu(c0) denotes whether the current charac-
ter is a punctuation, L is the length of word W that
conjoined from the character and its context which
matching a word in the external dictionary as long
as possible. t0 is the boundary tag of the character
in W.
With the features, a ME model is trained which
could output four scores for each character with
regard to four classes. Based on scores of all char-
acters, a completely segmented semiangle matrix
can be constructed. Each element wji in this ma-
trix represents a word that starts at the ith charac-
ter and ends at jth character, and its value ME(j, i),
the score for these (j ? i+1) characters to form a
word, is calculated as follow:
ME[j, i] = ? log p(w = ci...cj)
= ? log[p(bci)p(mci+1)...
p(mcj?1)p(ecj )]
(1)
As a consequence, the optimal segmentation re-
sults corresponding to the best path with the low-
est overall score could be reached via a dynamic
programming algorithm. For example:
@?c????(I was 19 years old that year)
Table 1 shows its corresponding matrix. In this
example, the ultimate segmented result is:
@ ?c ? ???
2.2 Language Model
N-gram language model, a widely used method
in natural language processing, can represent the
context relation of words. In our systems, a bi-
gram model is integrated with ME model in the
phase of calculating the path score. In detail, the
score of a path will be modified by adding the bi-
gram of words with a weight ? at the word bound-
aries. The approach used for modifying path score
is based on the following formula.
V [j, i] = ME[j, i]
+mini?1k=1{[(V [i ? 1, k]
+?Bigram(wk,i?1, wi,j)}
(2)
where V[j,i] is the score of local best path which
ends at the jth character and the last word on the
path is wi,j = ci...cj , the parameter ? is optimized
by the test set used in the 2nd International Chi-
nese Word Segmentation Bakeoff. When scoring
the path, if one of the words wk,i?1 and wi,j is out
of the vocabulary, their bigram will backoff to the
unigram. And the unigram of the OOV word will
be calculated as:
Unigram(OOV Word) = pl (3)
where p is the minimal unigram value of words in
vocabulary; l is the length of the word acting as
a punishment factor to avoid overemphasizing the
long OOV words.
2.3 Post Processing Strategies
The analysis on preliminary experiments, where
the ME model and n-gram language model are in-
volved, lead to several post processing strategies
in developing our final systems.
2.3.1 Division and Combination Strategy
To handle the combination ambiguity issue,
we introduce a division and combination strategy
which take in use of unigram and bigram. For
each two words A and B, if their bigrams does
not exist while there exists the unigram of word
AB, then they can be conjoined as one word. For
example, ??ff(August)? and ???(revolution)?
are two segmented words, and in training set the
bigram of ??ff? and ???? is absent, while
the word ??ff??(the August Revolution)? ap-
peares, then the character string ??ff??? is
conjoined as one word. On the other hand, for a
word C which can be divided as AB, if its uni-
gram does not exit in training set, while the bigram
of its subwords A and B exits, then it will be re-
segmented. For example, Taking the word ??L
N?U?(economic system reform)? for instance,
if its corresponding unigram is absent in training
set, while the bigram of two subwords ??LN
139
@ ? c ? ? ? ?
1 2 3 4 5 6 7
@ 1 6.3180e-07
? 2 33.159 7.5801
c 3 26.401 0.0056708 5.2704
? 4 71.617 45.221 49.934 3.1001e-07
? 5 83.129 56.734 61.446 33.869 7.0559
? 6 90.021 63.625 68.337 40.760 12.525 12.534
? 7 77.497 51.101 55.813 28.236 0.0012012 10.077 10.055
Table 1: A completely segmented matrix
?(economic system)? and ?U?(reform)? exists,
as a consequence, it will be segmented into two
words ??LN?? and ?U??.
2.3.2 Numeral Word Processing Strategy
The ME model always segment a numeral
word into several words. For instance, the word
?4.34(RMB Yuan 4.34)?, may be segmented
into two words ?4.? and ?34?. To tackle this
problem, a numeral word processing strategy is
used. Under this strategy, those words that contain
Arabic numerals are manually marked in the train-
ing set firstly, then a list of high frequency charac-
ters which always appear alone between the num-
bers in the training set can be extracted, based on
which numeral word issue can be tackled as fol-
lows. When segmenting one sentence, if two con-
joint words are numeral words, and the last char-
acter of the former word is in the list, then they are
combined as one word.
2.3.3 Long Organization Name Processing
Strategy
Since an organization name is usually an OOV,
it always will be segmented as several words, es-
pecially for a long one, while in MSRA corpus, it
is required to be recognized as one word. In our
systems, a corresponding strategy is presented to
deal with this problem. Firstly a list of organiza-
tion names is manually selected from the training
set and stored in the prefix-tree based on charac-
ters. Then a list of prefixes is extracted by scan-
ning the prefix-tree, that is, for each node, if the
frequencies of its child nodes are all lower than the
predefined threshold k and half of the frequency of
the current node, the string of the current node will
be extracted as a prefix; otherwise, if there exists
a child node whose frequency is higher than the
threshold k, scan the corresponding subtree. In the
same way, the suffixes can also be extracted. The
only difference is that the order of characters is in-
verse in the lexical tree.
During recognizing phase, to a successive
words string that may include 2-5 words, will be
combined as one word, if all of the following con-
ditions are satisfied.
a) Does not include numbers, full stop or comma.
b) Includes some OOV words.
c) Has a tail substring matching some suffix.
d) Appears more than twice in the test data.
e) Has a higher frequency than any of its substring which
is an OOV word or combined by multiple words.
f) Satisfy the condition that for any two successive words
w1 w2 in the strings, freq(w1w2)/freq(w1)?0.1, unless w1
contains some prefix in its right.
3 Experiments and Results
We have participated in both the closed and open
tracks of all the four corpora. For MSRA corpus
and other three corpora, we build System I and
System II respectively. Both systems are based on
the ME model and the Maximum Entropy Toolkit
1, provided by Zhang Le, is adopted.
Four systems are derived from System I with re-
gard to whether or not the n-gram language model
and three post processing strategies are used on the
closed track of MSRA corpus. Table 2 shows the
results of four derived systems.
System R P F ROOV RIV
IA 95.0 95.7 95.3 66.0 96.0
IB 96.0 95.6 95.8 60.3 97.3
IC 96.4 96.0 96.2 60.3 97.7
ID 96.4 96.1 96.3 61.2 97.6
Table 2: The effect of MEmodel, n-gram language
model and three post processing strategies on the
closed track of MSRA corpus.
System IA only adopts the ME model. System
IB integrates the ME model and the bigram lan-
guage model. System IC integrates the division
and combination strategy and the numeral words
1http://homepages.inf.ed.ac.uk/s0450736
/maxent toolkit.html
140
processing strategy. System ID adds the long or-
ganization name processing strategy.
For the open track of MSRA, an external dictio-
nary is utilized to extract the e and f features. The
external dictionary is built from six sources, in-
cluding the Chinese Concept Dictionary from In-
stitute of Computational Linguistics, Peking Uni-
versity(72,716 words), the LDC dictionary(43,120
words), the Noun Cyclopedia(111,633), the word
segmentation dictionary from Institute of Com-
puting Technology, Chinese Academy of Sci-
ences(84,763 words), the dictionary from Insti-
tute of Acoustics, and the dictionary from Insti-
tute of Computational Linguistics, Peking Univer-
sity(68,200 words) and a dictionary collected by
ourselves(63,470 words).
The union of the six dictionaries forms a big
dictionary, and those words appearing in five or
six dictionaries are extracted to form a core dic-
tionary. If a word belongs to one of the following
dictionaries or word sets, it is added into the exter-
nal dictionary.
a) The core dictionary.
b) The intersection of the big dictionary and the training
data.
c) The words appearing in the training data twice or more
times.
Those words in the external dictionaries will be
eliminated, if in most cases they are divided in
the training data. Table 3 shows the effect of ME
model, n-gram language model, three post pro-
cessing strategies on the open track of MSRA.
Here System IO only adopts the basic features,
while the external dictionary based features are
used in four derived systems related to open track:
IA, IB, IC, ID.
System R P F ROOV RIV
IO 96.0 96.5 96.3 71.1 96.9
IA 97.5 96.9 97.2 65.9 98.6
IB 97.6 96.8 97.2 64.8 98.7
IC 97.7 97.0 97.4 66.8 98.8
ID 97.7 97.1 97.4 67.5 98.8
Table 3: The effect of MEmodel, n-gram language
model, three post processing strategies on the open
track of MSRA.
System II only adopts ME model, the division
and combination strategy and the numeral word
processing strategy. In the open track of the cor-
pora CKIP and CITYU, the training set and test set
from the 2nd Chinese Word Segmentation Backoff
are used for training. For the corpora UPUC and
CITYU, the external dictionaries are used, which
is constructed in the same way as that in the open
track of MSRA Corpus. Table 4 shows the official
results of system II on UPUC, CKIP and CITYU.
Corpus R P F ROOV RIV
UPUC-C 93.6 92.3 93.0 68.3 96.1
UPUC-O 94.0 90.7 92.3 56.1 97.6
CKIP-C 95.8 94.8 95.3 64.6 97.2
CKIP-O 95.8 94.8 95.3 64.7 97.2
CITYU-C 96.9 97.0 97.0 77.3 97.8
CITYU-O 97.9 97.6 97.7 81.3 98.5
Table 4: Official results of our systems on UPUC
CKIP and CITYU
On the UPUC corpus, an interesting observation
is that the performance of the open track is worse
than the closed track. The investigation and analy-
sis lead to a possible explanation. That is, the seg-
mentation standard of the dictionaries, which are
used to construct the external dictionary, is differ-
ent from that of the UPUC corpus.
4 Conclusion
In this paper, a detailed description on several Chi-
nese word segmentation systems are presented,
where ME model, n-gram language model as well
as three post processing strategies are involved. In
the closed track of MSRA, the integration of bi-
gram language model greatly improves the recall
ratio of the words in vocabulary, although it will
impairs the performance of system in recognizing
the words out of vocabulary. In addition, three
strategies are introduced to deal with combination
ambiguity, numeral word, long organization name
issues. And the evaluation results reveal the valid-
ity and effectivity of our approaches.
References
Jin Kiat Low, Hwee Tou Ng and Wenyuan Guo.
A maximum Entropy Approach to Chinese Word
Segmentation. 2005. Preceedings of the Fourth
SIGHAN Workshop on Chinese Language Process-
ing, pp. 161-164.
Hwee Tou Ng and Jin Kiat Low. Chinese part-of-
speech tagging: One-at-a-time or all-at-once? word-
based or character-based? 2004. Preceedings of the
2004 Conference on Empirical Methods in Natural
Language Processing(EMNLP), pp. 277-284.
Zhang Huaping and Liu Qun. Model of Chinese
Words Rough Segmentation Based on N-Shortest-
Paths Method. 2002. Journal of Chinese Informa-
tion Processing, 28(1):pp. 1-7.
141
