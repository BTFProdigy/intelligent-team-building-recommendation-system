Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 102?112,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Learning to Freestyle: Hip Hop Challenge-Response Induction via
Transduction Rule Segmentation
Dekai Wu Karteek Addanki Markus Saers Meriem Beloucif
Human Language Technology Center
Department of Computer Science
HKUST, Clear Water Bay, Hong Kong
{dekai|vskaddanki|masaers|mbeloucif}@cs.ust.hk
Abstract
We present a novel model, Freestyle, that
learns to improvise rhyming and fluent re-
sponses upon being challenged with a line of
hip hop lyrics, by combining both bottom-
up token based rule induction and top-down
rule segmentation strategies to learn a stochas-
tic transduction grammar that simultaneously
learns both phrasing and rhyming associations.
In this attack on the woefully under-explored
natural language genre of music lyrics, we
exploit a strictly unsupervised transduction
grammar induction approach. Our task is par-
ticularly ambitious in that no use of any a pri-
ori linguistic or phonetic information is al-
lowed, even though the domain of hip hop
lyrics is particularly noisy and unstructured.
We evaluate the performance of the learned
model against a model learned only using
the more conventional bottom-up token based
rule induction, and demonstrate the superi-
ority of our combined token based and rule
segmentation induction method toward gen-
erating higher quality improvised responses,
measured on fluency and rhyming criteria as
judged by human evaluators. To highlight
some of the inherent challenges in adapting
other algorithms to this novel task, we also
compare the quality of the responses generated
by our model to those generated by an out-of-
the-box phrase based SMT system. We tackle
the challenge of selecting appropriate training
data for our task via a dedicated rhyme scheme
detection module, which is also acquired via
unsupervised learning and report improved
quality of the generated responses. Finally,
we report results with Maghrebi French hip
hop lyrics indicating that our model performs
surprisingly well with no special adaptation to
other languages.
1 Introduction
The genre of lyrics in music has been severely under-
studied from the perspective of computational lin-
guistics despite being a form of language that has
perhaps had the most impact across almost all human
cultures. With the motivation of spurring further re-
search in this genre, we apply stochastic transduc-
tion grammar induction algorithms to address some
of the modeling issues in song lyrics. An ideal start-
ing point for this investigation is hip hop, a genre
that emphasizes rapping, spoken or chanted rhyming
lyrics against strong beats or simple melodies. Hip
hop lyrics, in contrast to poetry and other genres of
music, present a significant number of challenges for
learning as it lacks well-defined structure in terms of
rhyme scheme, meter, or overall meaning making it
an interesting genre to bring to light some of the less
studied modeling issues.
The domain of hip hop lyrics is particularly un-
structured when compared to classical poetry, a do-
main on which statistical methods have been applied
in the past. Hip hop lyrics are unstructured in the
sense that a very high degree of variation is permit-
ted in the meter of the lyrics, and large amounts of
colloquial vocabulary and slang from the subculture
are employed. The variance in the permitted me-
ter makes it hard to make any assumptions about
the stress patterns of verses in order to identify the
rhyming words used when generating output. The
broad range of unorthodox vocabulary used in hip
hop make it difficult to use off-the-shelf NLP tools
for doing phonological and/or morphological analy-
sis. These problems are further exacerbated by dif-
ferences in intonation of the same word and lack of
robust transcription (Liberman, 2010).
102
We argue that stochastic transduction grammars,1
given their success in the area of machine transla-
tion and efficient unsupervised learning algorithms,
are ideal for capturing the structural relationship be-
tween lyrics. Hence, our Freestyle system mod-
els the problem of improvising a rhyming response
given any hip hop lyric challenge as transducing
a challenge line into a rhyming response. We
use a stochastic transduction grammar induced in
a completely unsupervised fashion using a combi-
nation of token based rule induction and segment-
ing (Saers et al, 2013) as the underlying model to
fully-automatically learn a challenge-response sys-
tem and compare its performance against a simpler
token based transduction grammar model. Both our
models are completely unsupervised and use no prior
phonetic or linguistic knowledge whatsoever despite
the highly unstructured and noisy domain.
We believe that the challenge-response system
based on an interpolated combination of token based
rule induction and rule segmenting transduction
grammars will generate more fluent and rhyming re-
sponses compared to one based on token based trans-
duction grammars models. This is based on the ob-
servation that token based transduction grammars
suffer from a lack of fluency; a consequence of the
degree of expressivity they permit. Therefore, as a
principal part of our investigation we compare the
quality of responses generated using a combination
of token based rule induction and top-down rule seg-
menting transduction grammars to those generated
by pure token based transduction grammars.
We also hypothesize that in order to generate flu-
ent and rhyming responses, it is not sufficient to train
the transduction grammars on all adjacent lines of a
hip hop verse. Therefore, we propose a data selec-
tion scheme using a rhyme scheme detector acquired
through unsupervised learning to generate the train-
ing data for the challenge-response systems. The
rhyme scheme detector segments each verse of a hip
hop song into stanzas and identifies the lines in each
stanza that rhyme with each other which are then
added as training instances. We demonstrate the su-
periority of our training data selection method by
comparing the quality of the responses generated by
the models trained on data selected with and without
1Also known in SMT as ?synchronous grammars?.
using the rhyme scheme detector.
Unlike conventional spoken and written language,
disfluencies and backing vocals2 occur very fre-
quently in the domain of hip hop lyrics which af-
fect the performance of NLP models designed for
processing well-formed sentences. We propose two
strategies to mitigate the effect of disfluencies on our
model performance and compare their efficacy using
human evaluations. Finally, in order to illustrate the
challenges faced by other NLP algorithms, we con-
trast the performance of our model against a conven-
tional, widely used phrase-based SMT model.
A brief terminological note: ?stanza? and ?verse?
are frequently confused and sometimes conflated.
Worse yet, their usage for song lyrics is often con-
tradictory to that for poetry. To avoid ambiguity
we consistently follow these technical definitions for
segments in decreasing size of granularity:
verse a large unit of a song?s lyrics. A song typi-
cally contains several verses interspersed with
choruses. In the present work, we do not differ-
entiate choruses from verses. In song lyrics, a
verse is most commonly represented as a sepa-
rate paragraph.
stanza a segment within a verse which has a me-
ter and rhyme scheme. Stanzas often consist of
2, 3, or 4 lines, but stanzas of more lines are
also common. Particularly in hip hop, a single
verse often contains many stanzas with differ-
ent rhyme schemes and meters.
line a segment within a stanza consisting of a single
line. In poetry, strictly speaking this would be
called a ?verse?, which however conflicts with
the conventional use of ?verse? in song lyrics.
In Section 2, we discuss some of the previous
work that applies statistical NLP methods to less
conventional domains and problems. We describe
our experimental conditions in Section 3. We com-
pare the performance of token and segment based
transduction grammar models in Section 4. We com-
pare our data selection schemes and disfluency han-
dling strategies in Sections 5 and 6. Finally, in
2Particularly the repetitive chants, exclamations, and inter-
jections in hip hop ?hype man? style backing vocals.
103
Section 7 we describe some preliminary results ob-
tained using our approach on improvising hip hop
responses in French and conclude in Section 8.
2 Related work
Although a few attempts have been made to apply
statistical NLP learning methods to unconventional
domains, Freestyle is among the first to tackle the
genre of hip hop lyrics (Addanki and Wu, 2013; Wu
et al, 2013a,b). Our preliminary work suggested the
need for further research to identify models that cap-
ture the correct generalizations to be able to gener-
ate fluent and rhyming responses. As a step towards
this direction, we contrast the performance of inter-
polated bottom-up token based rule induction and
top-down segmenting transduction grammar models
and token based transduction grammar models. We
briefly describe some of the past work in statistical
NLP on unconventional domains below.
Most of the past work either uses some form of
prior linguistic knowledge or enforces harsher con-
straints such as set number of words in a line, or a set
meter which are warranted by more structured do-
mains such as poetry. However, in hip hop lyrics it
is hard to make any linguistic or structural assump-
tions. For example, words such as sho, flo, holla
which frequently appear in the lyrics are not part of
any standard lexicon and hip hop does not require a
set number of syllables in a line, unlike poems. Also,
surprising and unlikely rhymes in hip hop are fre-
quently achieved via intonation and assonance, mak-
ing it hard to apply prior phonological constraints.
A phrase based SMT systemwas trained to ?trans-
late? the first line of a Chinese couplet or duilian
into the second by Jiang and Zhou (2008). The most
suitable next line was selected by applying linguistic
constraints to the n best output of the SMT system.
However in contrast to Chinese couplets, which ad-
here to strict rules requiring, for example, an identi-
cal number of characters in each line and one-to-one
correspondence in their metrical length, the domain
of hip hop lyrics is far more unstructured and there
exists no clear constraint that would ensure fluent
and rhyming responses to hip hop challenge lyrics.
Barbieri et al (2012) use controlled Markov pro-
cesses to semi-automatically generate lyrics that sat-
isfy the structural constraints of rhyme and meter.
Tamil lyrics were automatically generated given a
melody using conditional random fields by A. et al
(2009). The lyrics were represented as a sequence
of labels using the KNM system where K, N and M
represented the long vowels, short vowels and con-
sonants respectively.
Genzel et al (2010) used SMT in conjunction
with stress patterns and rhymes found in a pronun-
ciation dictionary to produce translations of poems.
Although many constraints were applied in translat-
ing full verses of poems, it was challenging to sat-
isfy all the constraints. Stress patterns were assigned
to words given the meter of a line in Shakespeare?s
sonnets by Greene et al (2010), which were then
combined with a language model to generate poems.
Sonderegger (2011) attempted to infer the pronun-
ciation of words in old English by identifying the
rhyming patterns using graph theory. However, their
heuristic of clustering words with similar IPA end-
ings resulted in large clusters of false positives such
as bloom and numb. A language-independent gener-
ative model for stanzas in poetry was proposed by
Reddy and Knight (2011) via which they could dis-
cover rhyme schemes in French and English poetry.
3 Experimental conditions
Before introducing our Freestyle models, we first
detail our experimental assumptions and the evalua-
tion scheme under which the responses generated by
different models are compared against one another.
We describe our training data as well as a phrase-
based SMT (PBSMT) contrastive baseline. We also
define the evaluation scheme used to compare the re-
sponses of different systems on criteria of fluency
and rhyming.
3.1 Training data
We used freely available user generated hip hop
lyrics on the Internet to provide training data for our
experiments. We collected approximately 52,000
English hip hop song lyrics amounting to approxi-
mately 800Mb of raw HTML content. The data was
cleaned by stripping HTML tags, metadata and nor-
malized for special characters and case differences.
The processed corpus contained 22 million tokens
with 260,000 verses and 2.7 million lines of hip hop
lyrics. As human evaluation using expert hip hop
104
listeners is expensive, a small subset of 85 lines was
chosen as the test set to provide challenges for com-
paring the quality of responses generated by different
systems.
3.2 Evaluation scheme
The performance of various Freestyle versions
was evaluated on the task of generating a improvised
fluent and rhyming response given a single line of a
hip hop verse as a challenge. The output of all the
systems on the test set was given to three indepen-
dent frequent hip hop listeners for manual evalua-
tion. They were asked to evaluate the system out-
puts according to fluency and the degree of rhyming.
They were free to choose the tune to make the lyrics
rhyme as the beats of the song were not used in the
training data. Each evaluator was asked to score the
response of each system on the criterion of fluency
and rhyming as being good, acceptable or bad.
3.3 Phrase-based SMT baseline
In order to evaluate the performance of an out-of-
the-box phrase-based SMT (PBSMT) system toward
this novel task of generating rhyming and fluent re-
sponses, a standard Moses baseline (Koehn et al,
2007) was also trained in order to compare its per-
formance with our transduction grammar induction
model. A 4-gram language model which was trained
on the entire training corpus using SRILM (Stolcke,
2002) was used to generate responses in conjunction
with the phrase-based translation model. As no au-
tomatic quality evaluation metrics exist for hip hop
responses analogous to BLEU for SMT, the model
weights cannot be tuned in conventional ways such
asMERT (Och, 2003). Instead, a slightly higher than
typical language model weight was empirically cho-
sen using a small development set to produce fluent
outputs.
4 Interpolated segmenting model vs. token
based model
We compare the performance of transduction gram-
mars induced via interpolated token based and rule
segmenting (ISTG) versus token based transduction
grammars (TG) on the task of generating a rhyming
and fluent response to hip hop challenges. We use
the framework of stochastic transduction grammars,
specifically bracketing ITGs (inversion transduction
grammars) (Wu, 1997), as our translation model for
?transducing? any given challenge into a rhyming
and fluent response. Our choice is motivated by
the significant amount of empirical evidence for the
representational capacity of transduction grammars
across a spectrum of natural language tasks such as
textual entailment (Wu, 2006), mining parallel sen-
tences (Wu and Fung, 2005) and machine translation
(Zens and Ney, 2003). Further, existence of effi-
cient learning algorithms (Saers et al, 2012; Saers
and Wu, 2011) that make no language specific as-
sumptions, make inversion transduction grammars a
suitable framework for our modeling needs. Exam-
ples of lexical transduction rules can be seen in Ta-
bles 3 and 5. In addition, the grammar also includes
structural transduction rules for the straight case
A? [A A] and also the inverted case A? <A A>.
4.1 Token based vs. segmental ITGs
The degenerate case of ITGs are token based ITGs
wherein each translation rule contains at most one
token in input and output languages. Efficient induc-
tion algorithmswith polynomial run time exist for to-
ken based ITGs and the expressivity they permit has
been empirically determined to capture most of the
word alignments that occur across natural languages.
The parameters of the token based ITGs can be es-
timated using expectation maximization through an
efficient dynamic programming algorithm in con-
junction with beam pruning (Saers and Wu, 2011).
In contrast to token based ITGs, each rule in a seg-
mental ITG grammar can contain more than one to-
ken in both input and output languages. In machine
translation applications, segmental models produce
translations that are more fluent as they can capture
lexical knowledge at a phrasal level. However, only
a handful of purely unsupervised algorithms exist
for learning segmental ITGs under matched training
and testing assumptions. Most other approaches in
SMT use a variety of ad hoc heuristics for extracting
segments from token alignments, justified purely by
short term improvements in automatic MT evalua-
tion metrics such as BLEU (Papineni et al, 2002)
which cannot be transferred to our current task. In-
stead, we use a completely unsupervised learning al-
gorithm for segmental ITGs that stays strictly within
the transduction grammar optimization framework
for both training and testing as proposed in Saers
105
et al (2013).
Saers et al (2013) induce a phrasal inversion
transduction grammar via interpolating the bottom-
up rule chunking approach proposed in Saers et al
(2012) with a top-down rule segmenting approach
driven by a minimum description length objective
function (Solomonoff, 1959; Rissanen, 1983) that
trades off the maximum likelihood against model
size. Saers et al (2013) report improvements in
BLEU score (Papineni et al, 2002) on their transla-
tion task. In our current approach instead of using a
bottom-up rule chunking approach we use a simpler
token based grammar instead. Given two grammars
(Ga and Gb) and an interpolation parameter ? the
probability function of the interpolated grammar is
given by:
pa+b (r) = ?pa (r) + (1? ?)pb (r)
for all rules r in the union of the two rule sets, and
where pa+b is the rule probability function of the
combined grammar and pa and pb are the rule prob-
ability functions of Ga and Gb respectively. The
pseudocode for the top-down rule segmenting algo-
rithm is shown in 1. The algorithm uses the methods
collect_biaffixes, eval_dl, sort_by_delta and
make_segmentations. These methods collect all the
biaffixes in an ITG, evaluate the difference in de-
scription length, sort candidates by these differences,
and commit to a given set of candidates, respectively.
The suitable interpolation parameter is chosen em-
pirically based on the responses generated on a small
development set.
We compare the performance of inducing a token
based ITG versus inducing a segmental ITG using in-
terpolated bottom-up token based rule induction and
top-down rule segmentation. To highlight some of
the inherent challenges in adapting other algorithms
to this novel task, we also compare the quality of the
responses generated by our model to those generated
by an off-the-shelf phrase based SMT system.
4.2 Decoding heuristics
We use our in-house ITG decoder implemented ac-
cording to the algorithm mentioned in Wu (1996)
for the generating responses to challenges by decod-
ing with the trained transduction grammars. The de-
coder uses a CKY-style parsing algorithm (Cocke,
Algorithm 1 Iterative rule segmenting learning
driven by minimum description length.
1: ? ? The ITG being induced
2: repeat
3: ?sum ? 0
4: bs? collect_biaffixes(?)
5: b? ? []
6: for all b ? bs do
7: ? ? eval_dl(b,?)
8: if ? < 0 then
9: b? ? [b?, ?b, ??]
10: sort_by_delta(b?)
11: for all ?b, ?? ? b? do
12: ?? ? eval_dl(b,?)
13: if ?? < 0 then
14: ?? make_segmentations(b,?)
15: ?sum ? ?sum + ??
16: until ?sum ? 0
17: return ?
1969) with cube pruning (Chiang, 2007). The de-
coder builds an efficient hypergraph structure which
is then scored using the induced grammar. The
trained transduction grammar model was decoded
using the 4-gram language model and the model
weights determined as described in 3.3.
In our decoding algorithm, we restrict the reorder-
ing to only be monotonic as we want to produce out-
put that follows the same rhyming order of the chal-
lenge. Interleaved rhyming order is harder to evalu-
ate without the larger context of the song and we do
not address that problem in our current model. We
also penalize singleton rules to produce responses of
similar length as successive lines in a stanza are typ-
ically of similar length. Finally, we add a penalty to
reflexive translation rules that map the same surface
form to itself such as A ? yo/yo. We obtain these
rules with a high probability due to the presence of
sentence pairs where both the input and output are
identical strings as many stanzas in our data contain
repeated chorus lines.
4.3 Results: Rule segmentation improves
responses
Results in Table 1 indicate that the ISTG outperforms
the TG model towards the task of generating fluent
and rhyming responses. On the criterion of fluency,
106
Table 1: Percentage of ?good and ?acceptable (i.e., either good or acceptable) responses on fluency and rhyming
criteria. PBSMT, TG and ISTG models trained using corpus generated from all adjacent lines in a verse. PBSMT+RS,
TG+RS, ISTG+RS are models trained on rhyme scheme based corpus selection strategy. Disfluency correction strategy
was used in all cases.
model fluency (?good) fluency (?acceptable) rhyming (?good) rhyming (?acceptable)
PBSMT 3.14% 4.70% 1.57% 4.31%
TG 21.18% 54.51% 23.53% 39.21%
ISTG 26.27% 57.64% 27.45% 48.23%
PBSMT+RS 30.59% 43.53% 1.96% 9.02%
TG+RS 34.12% 60.39% 20.00% 42.74%
ISTG+RS 30.98% 61.18% 30.98% 53.72%
Table 2: Transduction rules learned by ISTG model.
transduction grammar rule log prob.
A? long/wrong -11.6747
A? rhyme/time -11.6604
A? felt bad/couldn't see what i really had -11.3196
A? matter what you say/leaving anyway -11.8792
A? arhythamatic/this rhythm is sick -12.3492
the ISTGmodel produces a significantly higher frac-
tion of sentences rated good (26.27% vs. 21.18%)
and ?acceptable (57.64% vs. 54.51%). Higher frac-
tion of responses generated by the ISTG model are
rated as good (27.45% vs. 23.53%) and ?acceptable
(57.64% vs. 54.51%) compared to the TG model.
Both TG and ISTG model perform significantly bet-
ter than the PBSMT baseline. Upon inspecting the
learned rules, we noticed that the ISTG models cap-
ture rhyming correspondences both at the token and
segmental levels. Table 2 shows some examples
of the transduction rules learned by ISTG grammar
trained using rhyme scheme detection.
5 Data selection via rhyme scheme
detection vs. adjacent lines
We now compare two data selection approaches
for generating the training data for transduction
grammar induction via a rhyme scheme detection
module and choosing all adjacent lines in a verse.
We also briefly describe the training of the rhyme
scheme detection module and determine the efficacy
of our data selection scheme by training the ISTG
model, TG model and the PBSMT baseline on train-
ing data generated with and without employing the
rhyme scheme detection module. As the rule seg-
menting approach was intended to improve the flu-
ency as opposed to the rhyming nature of the re-
sponses, we only train the rule segmenting model
on the randomly chosen subset of all adjacent lines
in the verse. Further, adding adjacent lines as the
training data to the segmenting model maintains the
context of the responses generated thereby produc-
ing higher quality responses. The segmental trans-
duction grammar model was combined with the to-
ken based transduction grammar model trained on
data selected with and without using rhyme scheme
detection model.
5.1 Rhyme scheme detection
Although our approach adapts a transduction gram-
mar induction model toward the problem of generat-
ing fluent and rhyming hip hop responses, it would
be undesirable to train the model directly on all the
successive lines of the verses?as done by Jiang and
Zhou (2008)?due to variance in hip hop rhyming
patterns. For example, adding successive lines of a
stanza which follows ABAB rhyme scheme as train-
ing instances to the transduction grammar causes in-
correct rhyme correspondences to be learned. The
fact that a verse (which is usually represented as
a separate paragraph) may contain multiple stanzas
of varying length and rhyme schemes worsens this
problem. Adding all possible pairs of lines in a verse
as training examples not only introduces a lot of
noise but also explodes the size of the training data
due to the large size of the verse.
We employ a rhyme scheme detection model (Ad-
danki and Wu, 2013) in order to select training in-
stances that are likely to rhyme. Lines belonging to
107
the same stanza and marked as rhyming according
to the rhyme scheme detection model are added to
the training corpus. We believe that this data selec-
tion scheme will improve the rhyming associations
learned during the transduction grammar induction
thereby biasing the model towards producing fluent
and rhyming output.
The rhyme scheme detection model proposes a
HMM based generative model for a verse of hip hop
lyrics similar to Reddy and Knight (2011). However,
owing to the lack of well-defined verse structure in
hip hop, a number of hidden states corresponding to
stanzas of varying length are used to automatically
obtain a soft-segmentation of the verse. Each state
in the HMM corresponds to a stanza with a particu-
lar rhyme scheme such asAA,ABAB,AAAAwhile
the emissions correspond to the final words in the
stanza. We restrict the maximum length of a stanza
to be four to maintain a tractable number of states
and further only use states to represent stanzas whose
rhyme schemes could not be partitioned into smaller
schemes without losing a rhyme correspondence.
The parameters of the HMM are estimated using
the EM algorithm (Devijer, 1985) using the corpus
generated by taking the final word of each line in the
hip hop lyrics. The lines from each stanza that rhyme
with each other according to the Viterbi parse using
the trained model are added as training instances for
transduction grammar induction. As the source and
target languages are identical, each selected pair gen-
erates two training instances: a challenge-response
and a response-challenge pair.
The training data for the rhyme scheme detector
was obtained by extracting the end-of-line tokens
from each verse. However, upon data inspection we
noticed that shorter lines in hip hop stanzas are typi-
cally joined with a comma and represented as a sin-
gle line of text and hence all the tokens before the
commas were also added to the training corpus. We
obtained a corpus containing 4.2 million tokens cor-
responding to potential rhyming candidates compris-
ing of around 153,000 unique token types.
We evaluated the performance of our rhyme
scheme detector on the task of correctly labeling a
given verse with rhyme schemes. As our model is
completely unsupervised, we chose a random sam-
ple of 75 verses from our training data as our test set.
Two native English speakers who were frequent hip
hop listeners were asked to partition the verse into
stanzas and assign them with a gold standard rhyme
scheme. Precision and recall were aggregated for the
Viterbi parse of each verse against this gold standard
and f-score was calculated. The rhyme scheme de-
tection module employed in our data selection ob-
tained a precision of 35.81% and a recall of 57.25%,
giving an f-score of 44.06%.
5.2 Training data selection via rhyme scheme
detection
We obtained around 600,000 training instances upon
extracting a training corpus using rhyme scheme de-
tection module as described in Section 5.1. We
added those lines that were adjacent and labeled as
rhyming by the rhyme scheme detector as training in-
stances resulting in a training corpus of size 200,000.
5.3 Training data selection via adjacent lines
Considering all adjacent lines in a verse resulted in
a corpus with over 5 million training instances. In
order to ensure fair comparison of models trained
on data selected using rhyme scheme detection, we
randomly chose 200,000 training instances from the
generated corpus. The training corpus thus gener-
ated shared around 15% of training instances with
the corpus generated through our proposed data se-
lection scheme.
5.4 Results: Rhyme scheme detection helps
Results in Table 1 indicate that using the rhyme
scheme detector for training data selection helps
produce significantly more fluent responses com-
pared to using adjacent lines. A possible explana-
tion for this could be that adding all adjacent lines as
training instances introduces a lot of noise into the
model which hurts the fluency of the responses gen-
erated. Also, the cumulative fraction of sentences
that were labeled good or ?acceptable on the crite-
rion of rhyming is larger when rhyme scheme detec-
tion was used to generate the training data (although
the TG model trained on the corpus generated us-
ing adjacent lines produces a higher percentage of
rhyming responses that were rated good). Given the
significantly higher rate of response fluency when
using rhyme scheme detection, we argue that using
rhyme scheme detector for data selection is benefi-
cial.
108
Table 3: English hip hop challenge-response examples.
challenge man i die to see em all thun i just don't care
tg+rs in the sky and me the in polla and the you there
tg and the ride the me the and white the i the air
pbsmt+rs man i live to see em all i just don't care
challenge did a twelve year bid in the streets and held it down
tg+rs to the girls here kid the and to the thought the now
tg to the p's here did the a the i was the the
pbsmt+rs did a year in the streets and it down
challenge oh i believe in yesterday
tg+rs can you see the day
tg now you see the way
pbsmt+rs oh i believe in tomorrow
challenge what would i do
tg+rs just me and you
tg and you and you
pbsmt+rs what would you do
challenge cause you ain't going home till the early morn
tg+rs and the you this alone i i gotta on
tg and i you my on the a home we
pbsmt+rs cause you and your friends aint nothing but
It is also interesting to note from Table 1 that
ISTG+RS performs better than TG+RS indicating
that transduction grammar induced via interpolating
token based grammar and rule segmenting produces
better responses than token based transduction gram-
mar on both data selection schemes. Although the
average fraction of responses rated good on fluency
are slightly lower for ISTG+RS compared to TG+RS
(34.12% vs. 30.98%), the fraction of responses rated
?acceptable are higher (61.18% vs. 57.64%). It is
important to note that the fraction of sentences rated
good and ?acceptable on rhyming are much larger
for ISTG+RS model. Although the fluency of the
responses generated by PBSMT+RS drastically im-
proves compared to PBSMT it still lags behind the
TG+RS and ISTG+RS models on both fluency and
rhyming. The results in Table 1 confirm our hypoth-
esis that off-the-shelf SMT systems are not guaran-
teed to be effective on our novel task.
5.5 Challenge-response examples
Table 3 shows some of the challenges and the cor-
responding responses of PBSMT+RS, TG+RS and
TG model. While PBSMT+RS and TG+RS mod-
els generate responses reflecting a high degree of
fluency, the output of the TG contains a lot of ar-
ticles. It is interesting to note that TG+RS produces
responses comparable to PBSMT+RS despite being
a token based transduction grammar. However, PB-
SMT tends to produce responses that are too simi-
lar to the challenge. Moreover, TG models produce
responses that indeed rhyme better (shown in bold-
face). In fact, TG tries to rhymewords not only at the
end but also in middle of the lines, as our transduc-
tion grammar model captures structural associations
more effectively than the phrase-based model.
6 Disfluency handling via disfluency
correction and filtering
In this section, we compare the effect of two dis-
fluency mitigating strategies on the quality of the re-
sponses generated by the PBSMT baseline and token
based transduction grammar model with and without
using rhyme scheme detection.
6.1 Correction vs. filtering
Error analysis of our initial runs showed a dis-
turbingly high proportion of responses generated by
our system that contained disfluencies with succes-
sive repetitions of words such as the and I. Upon in-
spection of data we noticed that the training lyrics
actually did contain such disfluencies and backing
vocal lines, amounting to 10% of our training data.
We therefore compared two alternative strategies to
tackle this problem. The first strategy involved fil-
tering out all lines from our training corpus which
contained such disfluencies. In the second strategy,
we implemented a disfluency detection and correc-
tion algorithm (for example, the the the, which fre-
quently occurred in the training corpus, was cor-
rected to simply the). The PBSMT baseline and the
TG model were trained on both the filtered and cor-
rected versions of the training corpus and the quality
of the responses were compared.
6.2 Results: Disfluency correction helps
The results in Table 4 indicate that the disfluency
correction strategy outperforms the filtering strategy
for both TG and TG+RS models. For the model
TG+RS, disfluency correction generated 34.12%
good responses in terms of fluency, while the filter-
ing strategy produced only 28.63% good responses.
Similarly for the model TG, disfluency correction
produced 21.8% of responses with good fluency and
the filtering strategy produced only 17.25%. Dis-
fluency correction strategy produces higher fraction
of responses with ?acceptable fluency compared to
the filtering strategy for both TG and TG+RS mod-
els. This result is not surprising, as harshly pruning
109
Table 4: Effect of the disfluency correction strategies on fluency of the responses generated for the TG induction
models vs PBSMT baselines using both rhyme scheme detection and adjacent lines as the corpus selection method.
model+disfluency strat. fluency (good) fluency (?acceptable) rhyming (good) rhyming (?acceptable)
PBSMT+filtering 4.3% 13.72% 3.53% 7.06%
PBSMT+correction 3.14% 4.70% 1.57% 4.31%
PBSMT+RS+filtering 31.76% 43.91% 12.15% 21.17%
PBSMT+RS+correction 30.59% 43.53% 1.96% 9.02%
TG+filtering 17.25% 46.27% 18.04% 33.33%
TG+correction 21.18% 54.51% 23.53% 39.21%
TG+RS+filtering 28.63% 56.86% 14.90% 34.51%
TG+RS+correction 34.12% 60.39% 20.00% 42.74%
the training corpus causes useful word association
information necessary for rhyming to be lost. Sur-
prisingly, for both PBSMT and PBSMT+RSmodels,
the disfluency correction has a negative effect on the
fluency level of the response but still falls behind TG
and TG+RS models. As disfluency correction yields
more fluent responses for TG and TG+RS models,
the results for ISTG and ISTG+RS models in Table
1 were obtained using disfluency correction strategy.
7 Maghrebi French hip hop
We have begun to apply Freestyle to rap in lan-
guages other than English, taking advantage of
the language independence and linguistics-light ap-
proach of our unsupervised transduction grammar
induction methods. With no special adaption our
transduction grammar based model performs sur-
prisingly well, even with significantly smaller train-
ing data size and noisier data. These results across
different languages are encouraging as they can be
used to discover truly language independence as-
sumptions. We briefly describe our initial experi-
ments on Maghrebi French hip hop lyrics below.
7.1 Dataset
We collected freely available French hip hop lyrics
of approximately 1300 songs. About 85% of the
songs were by Maghrebi French artists of Alge-
rian, Moroccan, or Tunisian cultural backgrounds,
while the remaining were by artists from the rest
of Francophonie. As the large majority of songs
are in Maghrebi French, the lyrics are sometimes
interspersed with romanized Arabic such as ?De la
travers?e du d?sert au bon couscous de Y?ma? (Y?ma
meansMy mother). Some songs also contain Berber
phrases, for instance ?a yemmi ino, a thizizwith?
(which means my son, a bee). Furthermore, some
songs also contained English phrases in the style of
gangster rap such as ?T'es game over, game over... Le
son de Chicken wings?. As mentioned earlier, it is
complexity like this which dissuaded us from mak-
ing language specific assumptions in our model.
We extracted the end-of-line words and obtained
a corpus containing 120,000 tokens corresponding
to potential rhyming candidates with around 29,000
unique token types which was used as the training
data for the rhyme scheme detector module. For the
transduction grammar induction, the training data
contained about 47,000 sentence pairs selected us-
ing rhyme scheme detection.
7.2 Results
After human evaluation by native French speak-
ers and frequent hip hop listeners, our transduction
grammar based model generates about 9.2% and
14.5% of the responses that are rated good by the
human evaluators on the criterion of fluency and
rhyming respectively. About 30.2% and 38% of
the responses are rated as ?acceptable. These num-
bers are encouraging given the noisy lyrics andmuch
smaller amount of training data. Some examples of
the challenge-response pairs and learned transduc-
tion rules in French are shown in Tables 5 and 6.
From Table 5, we can see that responses gener-
ated by the system rhyme with the challenges. The
first response is fluent and not only perfectly rhymes
with the challenge but also semantically valid. In the
second example, the model realizes a less common
AABA rhyme scheme through the response. The re-
110
Table 5: French hip hop challenge-response examples.
challenge Si je me trompe
response faut que je raconte
challenge Un jour je suis un livre
response et ce que je de vivre
challenge Pacha mama ils ne voient pas ta souffrance
response Combat ni leur de voulait de la d?cadence
challenge la palestine n'etait pas une terre sans peuple.
response le darfour d'autre de la guerre on est
challenge Une banlieue qui meut
response les yeux et
Table 6: Transduction rules for Maghrebi French hip hop.
transduction grammar rule log prob.
A? terre/la guerre -9.4837
A? haine/peine -9.77056
A? mal/pays natal -10.6877
A? je frissonne/mi corazon -11.0931
A? gratteurs/rappeurs -11.7306
sponse in the third example, exhibits strong rhyming
with the challenge and both the challenge and the
response contain words like souffrance, combat and
d?cadence which are related. Similarly in the fourth
example, the challenge and response also contain se-
mantically related tokens which also rhyme. These
examples illustrate that our transduction grammar
formalism coupled with our rhyme scheme detection
module does capture the necessary correspondences
between lines of hip hop lyrics without assuming any
language specific resources.
8 Conclusion
We presented a new machine learning approach for
improvising hip hop responses to challenge lyrics
by inducing stochastic transduction grammars, and
demonstrated that inducing the transduction rules by
interpolating bottom-up token based rule induction
and rule segmentation strategies outperforms a token
based baseline. We compared the performance of
our Freestylemodel against a widely used off-the-
shelf phrase-based SMT model, showing that PB-
SMT falls short in tackling the noisy and highly un-
structured domain of hip hop lyrics. We showed that
the quality of responses improves when the training
data for the transduction grammar induction is se-
lected using a rhyme scheme detector. Several do-
main related oddities such as disfluencies and back-
ing vocals have been identified and some strategies
for alleviating their effects have been compared. We
also reported results on Maghrebi French hip hop
lyrics which indicate that our model works surpris-
ingly well with no special adaptation for languages
other than English. In the future, we plan to inves-
tigate alternative training data selection techniques,
disfluency handling strategies, search heuristics, and
novel transduction grammar induction models.
Acknowledgements
This material is based upon work supported in part by
the Hong Kong Research Grants Council (RGC) research
grants GRF620811, GRF621008, GRF612806; by the
Defense Advanced Research Projects Agency (DARPA)
under BOLT contract no. HR0011-12-C-0016, and GALE
contract nos. HR0011-06-C-0022 and HR0011-06-C-
0023; and by the European Union under the FP7 grant
agreement no. 287658. Any opinions, findings and con-
clusions or recommendations expressed in this material
are those of the authors and do not necessarily reflect the
views of the RGC, EU, or DARPA.
References
Ananth Ramakrishnan A., Sankar Kuppan, and
Lalitha Devi Sobha. ?Automatic generation of
Tamil lyrics for melodies.? Workshop on Computa-
tional Approaches to Linguistic Creativity (CALC-09).
2009.
Karteek Addanki and DekaiWu. ?Unsupervised rhyme
scheme identification in hip hop lyrics using hidden
Markov models.? 1st International Conference on Sta-
tistical Language and Speech Processing (SLSP 2013).
2013.
Gabriele Barbieri, Fran?ois Pachet, Pierre Roy, and
Mirko Degli Esposti. ?Markov constraints for gen-
erating lyrics with style.? 20th European Conference
on Artificial Intelligence, (ECAI 2012). 2012.
David Chiang. ?Hierarchical phrase-based translation.?
Computational Linguistics, 33(2), 2007.
John Cocke. Programming languages and their compil-
ers: Preliminary notes. Courant Institute ofMathemat-
ical Sciences, New York University, 1969.
P.A. Devijer. ?Baum?s forward-backward algorithm re-
visited.? Pattern Recognition Letters, 3(6), 1985.
D. Genzel, J. Uszkoreit, and F. Och. ?Poetic statisti-
cal machine translation: rhyme and meter.? 2010 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP 2010). Association for Computa-
tional Linguistics, 2010.
E. Greene, T. Bodrumlu, and K. Knight. ?Auto-
matic analysis of rhythmic poetry with applications
111
to generation and translation.? 2010 Conference on
Empirical Methods in Natural Language Processing
(EMNLP 2010). Association for Computational Lin-
guistics, 2010.
Long Jiang and Ming Zhou. ?Generating Chinese
couplets using a statistical MT approach.? 22nd In-
ternational Conference on Computational Linguistics
(COLING 2008). 2008.
Philipp Koehn, Hieu Hoang, Alexandra Birch,
Chris Callison-Burch, Marcello Federico, Nicola
Bertoldi, Brooke Cowan, Wade Shen, Christine
Moran, Richard Zens, Chris Dyer, Ondrej Bojar,
Alexandra Constantin, and Evan Herbst. ?Moses:
Open source toolkit for statistical machine translation.?
Interactive Poster and Demonstration Sessions of the
45th Annual Meeting of the Association for Computa-
tional Linguistics (ACL 2007). June 2007.
MarkLiberman. ?Rap scholarship, rapmeter, and the an-
thology of mondegreens.? http://languagelog.ldc.
upenn.edu/nll/?p=2824, December 2010. Accessed:
2013-06-30.
Franz Josef Och. ?Minimum error rate training in sta-
tistical machine translation.? 41st Annual Meeting of
the Association for Computational Linguistics (ACL-
2003). July 2003.
Kishore Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. ?BLEU: a method for automatic evalu-
ation of machine translation.? 40th Annual Meeting of
the Association for Computational Linguistics (ACL-
02). July 2002.
S. Reddy and K. Knight. ?Unsupervised discovery of
rhyme schemes.? 49th Annual Meeting of the Associa-
tion for Computational Linguistics: Human Language
Technologies (ACL HLT 2011), vol. 2. Association for
Computational Linguistics, 2011.
Jorma Rissanen. ?A universal prior for integers and es-
timation by minimum description length.? The Annals
of Statistics, 11(2), June 1983.
Markus Saers, KarteekAddanki, andDekaiWu. ?From
finite-state to inversion transductions: Toward un-
supervised bilingual grammar induction.? 24th In-
ternational Conference on Computational Linguistics
(COLING 2012). December 2012.
Markus Saers, Karteek Addanki, and Dekai Wu.
?Combining top-down and bottom-up search for un-
supervised induction of transduction grammars.? Sev-
enth Workshop on Syntax, Semantics and Structure in
Statistical Translation (SSST-7). June 2013.
Markus Saers and Dekai Wu. ?Reestimation of reified
rules in semiring parsing and biparsing.? Fifth Work-
shop on Syntax, Semantics and Structure in Statistical
Translation (SSST-5). Association for Computational
Linguistics, June 2011.
Ray J. Solomonoff. ?A new method for discov-
ering the grammars of phrase structure languages.?
International Federation for Information Processing
Congress (IFIP). 1959.
M. Sonderegger. ?Applications of graph theory to an
English rhyming corpus.? Computer Speech & Lan-
guage, 25(3), 2011.
Andreas Stolcke. ?SRILM ? an extensible language
modeling toolkit.? 7th International Conference on
Spoken Language Processing (ICSLP2002 - INTER-
SPEECH 2002). September 2002.
Dekai Wu. ?A polynomial-time algorithm for statisti-
cal machine translation.? 34th Annual Meeting of the
Association for Computational Linguistics (ACL96).
1996.
DekaiWu. ?Stochastic inversion transduction grammars
and bilingual parsing of parallel corpora.? Computa-
tional Linguistics, 23(3), 1997.
Dekai Wu. ?Textual entailment recognition using inver-
sion transduction grammars.? Joaquin Qui?onero-
Candela, Ido Dagan, Bernardo Magnini, and Flo-
rence d?Alch? Buc (eds.),Machine Learning Chal-
lenges, Evaluating Predictive Uncertainty, Visual Ob-
ject Classification and Recognizing Textual Entail-
ment, First PASCAL Machine Learning Challenges
Workshop (MLCW 2005), vol. 3944 of Lecture Notes
in Computer Science. Springer, 2006.
Dekai Wu, Karteek Addanki, and Markus Saers.
?FREESTYLE: A challenge-response system for hip
hop lyrics via unsupervised induction of stochastic
transduction grammars.? 14th Annual Conference of
the International Speech Communication Association
(Interspeech 2013). 2013a.
Dekai Wu, Karteek Addanki, and Markus Saers.
?Modeling hip hop challenge-response lyrics as ma-
chine translation.? 14th Machine Translation Summit
(MT Summit XIV). 2013b.
Dekai Wu and Pascale Fung. ?Inversion transduc-
tion grammar constraints for mining parallel sentences
from quasi-comparable corpora.? Second Interna-
tional Joint Conference on Natural Language Process-
ing (IJCNLP 2005). Springer, 2005.
Richard Zens and HermannNey. ?A comparative study
on reordering constraints in statistical machine trans-
lation.? 41st Annual Meeting of the Association for
Computational Linguistics (ACL-2003). Association
for Computational Linguistics, 2003.
112
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 765?771,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
XMEANT: Better semantic MT evaluation without reference translations
Lo, Chi-kiu Beloucif, Meriem Saers, Markus Wu, Dekai
HKUST
Human Language Technology Center
Department of Computer Science and Engineering
Hong Kong University of Science and Technology
{jackielo|mbeloucif|masaers|dekai}@cs.ust.hk
Abstract
We introduce XMEANT?a new cross-lingual
version of the semantic frame based MT
evaluation metric MEANT?which can cor-
relate even more closely with human ade-
quacy judgments than monolingual MEANT
and eliminates the need for expensive hu-
man references. Previous work established
that MEANT reflects translation adequacy
with state-of-the-art accuracy, and optimiz-
ing MT systems against MEANT robustly im-
proves translation quality. However, to go
beyond tuning weights in the loglinear SMT
model, a cross-lingual objective function that
can deeply integrate semantic frame crite-
ria into the MT training pipeline is needed.
We show that cross-lingual XMEANT out-
performs monolingual MEANT by (1) replac-
ing the monolingual context vector model in
MEANT with simple translation probabilities,
and (2) incorporating bracketing ITG con-
straints.
1 Introduction
We show that XMEANT, a new cross-lingual ver-
sion of MEANT (Lo et al, 2012), correlates with
human judgment even more closely than MEANT
for evaluating MT adequacy via semantic frames,
despite discarding the need for expensive human
reference translations. XMEANT is obtained by
(1) using simple lexical translation probabilities,
instead of the monolingual context vector model
used in MEANT for computing the semantic role
fillers similarities, and (2) incorporating bracket-
ing ITG constrains for word alignment within the
semantic role fillers. We conjecture that the rea-
son that XMEANT correlates more closely with
human adequacy judgement than MEANT is that
on the one hand, the semantic structure of the
MT output is closer to that of the input sentence
than that of the reference translation, and on the
other hand, the BITG constraints the word align-
ment more accurately than the heuristic bag-of-
word aggregation used in MEANT. Our results
suggest that MT translation adequacy is more ac-
curately evaluated via the cross-lingual semantic
frame similarities of the input and the MT output
which may obviate the need for expensive human
reference translations.
The MEANT family of metrics (Lo and Wu,
2011a, 2012; Lo et al, 2012) adopt the princi-
ple that a good translation is one where a human
can successfully understand the central meaning
of the foreign sentence as captured by the basic
event structure: ?who did what to whom, when,
where and why? (Pradhan et al, 2004). MEANT
measures similarity between the MT output and
the reference translations by comparing the simi-
larities between the semantic frame structures of
output and reference translations. It is well estab-
lished that the MEANT family of metrics corre-
lates better with human adequacy judgments than
commonly used MT evaluation metrics (Lo and
Wu, 2011a, 2012; Lo et al, 2012; Lo and Wu,
2013b; Mach?a?cek and Bojar, 2013). In addition,
the translation adequacy across different genres
(ranging from formal news to informal web fo-
rum and public speech) and different languages
(English and Chinese) is improved by replacing
BLEU or TER with MEANT during parameter
tuning (Lo et al, 2013a; Lo and Wu, 2013a; Lo
et al, 2013b).
In order to continue driving MT towards better
translation adequacy by deeply integrating seman-
tic frame criteria into theMT training pipeline, it is
necessary to have a cross-lingual semantic objec-
tive function that assesses the semantic frame sim-
ilarities of input and output sentences. We there-
fore propose XMEANT, a cross-lingual MT evalu-
ation metric, that modifies MEANT using (1) sim-
ple translation probabilities (in our experiments,
765
from quick IBM-1 training), to replace the mono-
lingual context vector model in MEANT, and (2)
constraints from BITGs (bracketing ITGs). We
show that XMEANT assesses MT adequacy more
accurately than MEANT (as measured by correla-
tion with human adequacy judgement) without the
need for expensive human reference translations in
the output language.
2 Related Work
2.1 MT evaluation metrics
Surface-form oriented metrics such as BLEU (Pa-
pineni et al, 2002), NIST (Doddington, 2002),
METEOR (Banerjee and Lavie, 2005), CDER
(Leusch et al, 2006), WER (Nie?en et al, 2000),
and TER (Snover et al, 2006) do not correctly re-
flect the meaning similarities of the input sentence.
In fact, a number of large scale meta-evaluations
(Callison-Burch et al, 2006; Koehn and Monz,
2006) report cases where BLEU strongly dis-
agrees with human judgments of translation ade-
quacy.
This has caused a recent surge of work to de-
velop better ways to automatically measure MT
adequacy. Owczarzak et al (2007a,b) improved
correlation with human fluency judgments by us-
ing LFG to extend the approach of evaluating syn-
tactic dependency structure similarity proposed by
Liu and Gildea (2005), but did not achieve higher
correlation with human adequacy judgments than
metrics like METEOR. TINE (Rios et al, 2011) is
a recall-oriented metric which aims to preserve the
basic event structure but it performs comparably
to BLEU and worse than METEOR on correlation
with human adequacy judgments. ULC (Gim?enez
and M`arquez, 2007, 2008) incorporates several
semantic features and shows improved correla-
tion with human judgement on translation quality
(Callison-Burch et al, 2007, 2008) but no work
has been done towards tuning an SMT system us-
ing a pure form of ULC perhaps due to its expen-
sive run time. Similarly, SPEDE (Wang and Man-
ning, 2012) predicts the edit sequence for match-
ing the MT output to the reference via an inte-
grated probabilistic FSM and PDA model. Sagan
(Castillo and Estrella, 2012) is a semantic textual
similarity metric based on a complex textual en-
tailment pipeline. These aggregated metrics re-
quire sophisticated feature extraction steps, con-
tain several dozens of parameters to tune, and em-
ploy expensive linguistic resources like WordNet
Figure 1: Monolingual MEANT algorithm.
or paraphrase tables; the expensive training, tun-
ing, and/or running time makes them hard to in-
corporate into the MT development cycle.
2.2 The MEANT family of metrics
MEANT (Lo et al, 2012), which is the weighted f-
score over the matched semantic role labels of the
automatically aligned semantic frames and role
fillers, that outperforms BLEU, NIST, METEOR,
WER, CDER and TER in correlation with human
adequacy judgments. MEANT is easily portable
to other languages, requiring only an automatic se-
mantic parser and a large monolingual corpus in
the output language for identifying the semantic
structures and the lexical similarity between the
semantic role fillers of the reference and transla-
tion.
Figure 1 shows the algorithm and equations for
computing MEANT. q
0
i,j
and q
1
i,j
are the argument
of type j in frame i in MT and REF respectively.
w
0
i
and w
1
i
are the weights for frame i in MT/REF
respectively. These weights estimate the degree of
contribution of each frame to the overall meaning
of the sentence. w
pred
and w
j
are the weights of
the lexical similarities of the predicates and role
fillers of the arguments of type j of all frame be-
tween the reference translations and the MT out-
766
Figure 2: Examples of automatic shallow semantic parses. The input is parsed by a Chinese automatic
shallow semantic parser. The reference and MT output are parsed by an English automatic shallow
semantic parser. There are no semantic frames for MT3 since the system decided to drop the predicate.
put. There is a total of 12 weights for the set
of semantic role labels in MEANT as defined in
Lo and Wu (2011b). For MEANT, they are de-
termined using supervised estimation via a sim-
ple grid search to optimize the correlation with
human adequacy judgments (Lo and Wu, 2011a).
For UMEANT (Lo and Wu, 2012), they are es-
timated in an unsupervised manner using relative
frequency of each semantic role label in the refer-
ences and thus UMEANT is useful when human
judgments on adequacy of the development set are
unavailable.
s
i,pred
and s
i,j
are the lexical similarities based
on a context vector model of the predicates and
role fillers of the arguments of type j between the
reference translations and the MT output. Lo et al
(2012) and Tumuluru et al (2012) described how
the lexical and phrasal similarities of the semantic
role fillers are computed. A subsequent variant of
the aggregation function inspired by Mihalcea et
al. (2006) that normalizes phrasal similarities ac-
cording to the phrase length more accurately was
used in more recent work (Lo et al, 2013a; Lo
and Wu, 2013a; Lo et al, 2013b). In this paper,
we employ a newer version of MEANT that uses
f-score to aggregate individual token similarities
into the composite phrasal similarities of seman-
tic role fillers, as our experiments indicate this is
more accurate than the previously used aggrega-
tion functions.
Recent studies (Lo et al, 2013a; Lo and Wu,
2013a; Lo et al, 2013b) show that tuning MT sys-
tems against MEANT produces more robustly ad-
equate translations than the common practice of
tuning against BLEU or TER across different data
genres, such as formal newswire text, informal
web forum text and informal public speech.
2.3 MT quality estimation
Evaluating cross-lingual MT quality is similar to
the work of MT quality estimation (QE). Broadly
speaking, there are two different approaches to
QE: surface-based and feature-based.
Token-based QE models, such as those in Gan-
drabur et al (2006) and Ueffing and Ney (2005)
fail to assess the overall MT quality because trans-
lation goodness is not a compositional property. In
contrast, Blatz et al (2004) introduced a sentence-
level QE system where an arbitrary threshold is
used to classify the MT output as good or bad.
The fundamental problem of this approach is that
it defines QE as a binary classification task rather
than attempting to measure the degree of goodness
of the MT output. To address this problem, Quirk
(2004) related the sentence-level correctness of the
QE model to human judgment and achieved a high
correlation with human judgement for a small an-
notated corpus; however, the proposed model does
not scale well to larger data sets.
Feature-based QE models (Xiong et al, 2010;
He et al, 2011; Ma et al, 2011; Specia, 2011;
Avramidis, 2012; Mehdad et al, 2012; Almaghout
and Specia, 2013; Avramidis and Popovi?c, 2013;
Shah et al, 2013) throw a wide range of linguis-
tic and non-linguistic features into machine learn-
767
Figure 3: Cross-lingual XMEANT algorithm.
ing algorithms for predicting MT quality. Al-
though the feature-based QE system of Avramidis
and Popovi?c (2013) slightly outperformed ME-
TEOR on correlation with human adequacy judg-
ment, these ?black box? approaches typically lack
representational transparency, require expensive
running time, and/or must be discriminatively re-
trained for each language and text type.
3 XMEANT: a cross-lingual MEANT
Like MEANT, XMEANT aims to evaluate how
well MT preserves the core semantics, while
maintaining full representational transparency.
But whereas MEANT measures lexical similar-
ity using a monolingual context vector model,
XMEANT instead substitutes simple cross-lingual
lexical translation probabilities.
XMEANT differs only minimally from
MEANT, as underlined in figure 3. The same
weights obtained by optimizing MEANT against
human adequacy judgement were used for
XMEANT. The weights can also be estimated in
unsupervised fashion using the relative frequency
of each semantic role label in the foreign input, as
in UMEANT.
To aggregate individual lexical translation prob-
abilities into phrasal similarities between cross-
lingual semantic role fillers, we compared two nat-
ural approaches to generalizing MEANT?s method
of comparing semantic parses, as described below.
3.1 Applying MEANT?s f-score within
semantic role fillers
The first natural approach is to extend MEANT?s
f-score based method of aggregating semantic
parse accuracy, so as to also apply to aggregat-
ing lexical translation probabilities within seman-
tic role filler phrases. However, since we are miss-
ing structure information within the flat role filler
phrases, we can no longer assume an injective
mapping for aligning the tokens of the role fillers
between the foreign input and the MT output. We
therefore relax the assumption and thus for cross-
lingual phrasal precision/recall, we align each to-
ken of the role fillers in the output/input string
to the token of the role fillers in the input/output
string that has the maximum lexical translation
probability. The precise definition of the cross-
lingual phrasal similarities is as follows:
e
i,pred
? the output side of the pred of aligned frame i
f
i,pred
? the input side of the pred of aligned frame i
e
i,j
? the output side of the ARG j of aligned frame i
f
i,j
? the input side of the ARG j of aligned frame i
p(e, f) =
?
t (e|f) t (f |e)
prec
e,f
=
?
e?e
max
f?f
p(e, f)
|e|
rec
e,f
=
?
f?f
max
e?e
p(e, f)
|f|
s
i,pred
=
2 ? prec
e
i,pred
,f
i,pred
? rec
e
i,pred
,f
i,pred
prec
e
i,pred
,f
i,pred
+ rec
e
i,pred
,f
i,pred
s
i,j
=
2 ? prec
e
i,j
,f
i,j
? rec
e
i,j
,f
i,j
prec
e
i,j
,f
i,j
+ rec
e
i,j
,f
i,j
where the joint probability p is defined as the har-
monized the two directions of the translation table
t trained using IBM model 1 (Brown et al, 1993).
prec
e,f
is the precision and rec
e,f
is the recall of
the phrasal similarities of the role fillers. s
i,pred
and s
i,j
are the f-scores of the phrasal similarities
of the predicates and role fillers of the arguments
of type j between the input and the MT output.
3.2 Applying MEANT?s ITG bias within
semantic role fillers
The second natural approach is to extend
MEANT?s ITG bias on compositional reorder-
ing, so as to also apply to aggregating lexical
translation probabilities within semantic role filler
phrases. Addanki et al (2012) showed empiri-
cally that cross-lingual semantic role reordering of
the kind that MEANT is based upon is fully cov-
ered within ITG constraints. In Wu et al (2014),
we extend ITG constraints into aligning the tokens
within the semantic role fillers within monolingual
MEANT, thus replacing its previous monolingual
phrasal aggregation heuristic. Here we borrow the
768
idea for the cross-lingual case, using the length-
normalized inside probability at the root of a BITG
biparse (Wu, 1997; Zens and Ney, 2003; Saers and
Wu, 2009) as follows:
G ? ?{A} ,W
0
,W
1
,R,A?
R ? {A ? [AA] ,A ? ?AA?,A ? e/f}
p ([AA] |A) = p (?AA?|A) = 0.25
p (e/f |A) =
1
2
?
t (e|f) t (f |e)
s
i,pred
=
1
1?
ln
(
P
(
A
?
?e
i,pred
/f
i,pred
|G
))
max(|e
i,pred
|,|f
i,pred
|)
s
i,j
=
1
1?
ln
(
P
(
A
?
?e
i,j
/f
i,j
|G
))
max(|e
i,j
|,|f
i,j
|)
where G is a bracketing ITG, whose only nonter-
minal is A, and where R is a set of transduction
rules where e ? W
0
? {?} is an output token
(or the null token), and f ? W
1
? {?} is an in-
put token (or the null token). The rule probabil-
ity function p is defined using fixed probabilities
for the structural rules, and a translation table t
trained using IBM model 1 in both directions. To
calculate the inside probability of a pair of seg-
ments, P
(
A
?
? e/f|G
)
, we use the algorithm de-
scribed in Saers et al (2009). s
i,pred
and s
i,j
are
the length normalized BITG parsing probabilities
of the predicates and role fillers of the arguments
of type j between the input and the MT output.
4 Results
Table 1 shows that for human adequacy judgments
at the sentence level, the f-score based XMEANT
(1) correlates significantly more closely than other
commonly used monolingual automatic MT eval-
uation metrics, and (2) even correlates nearly as
well as monolingual MEANT. This suggests that
the semantic structure of the MT output is indeed
closer to that of the input sentence than that of the
reference translation.
Furthermore, the ITG-based XMEANT (1) sig-
nificantly outperforms MEANT, and (2) is an au-
tomatic metric that is nearly as accurate as the
HMEANT human subjective version. This indi-
cates that BITG constraints indeed provide a more
robust token alignment compared to the heuris-
tics previously employed in MEANT. It is also
consistent with results observed while estimating
word alignment probabilities, where BITG con-
straints outperformed alignments from GIZA++
(Saers and Wu, 2009).
Table 1: Sentence-level correlation with HAJ
(GALE phase 2.5 evaluation data)
Metric Kendall
HMEANT 0.53
XMEANT (BITG) 0.51
MEANT (f-score) 0.48
XMEANT (f-score) 0.46
MEANT (2013) 0.46
NIST 0.29
BLEU/METEOR/TER/PER 0.20
CDER 0.12
WER 0.10
5 Conclusion
We have presented XMEANT, a new cross-lingual
variant of MEANT, that correlates even more
closely with human translation adequacy judg-
ments thanMEANT, without the expensive human
references. This is (1) accomplished by replacing
monolingual MEANT?s context vector model with
simple translation probabilities when computing
similarities of semantic role fillers, and (2) fur-
ther improved by incorporating BITG constraints
for aligning the tokens in semantic role fillers.
While monolingual MEANT alone accurately re-
flects adequacy via semantic frames and optimiz-
ing SMT against MEANT improves translation,
the new cross-lingual XMEANT semantic objec-
tive function moves closer toward deep integration
of semantics into the MT training pipeline.
The phrasal similarity scoring has only been
minimally adapted to cross-lingual semantic role
fillers in this first study of XMEANT. We expect
further improvements to XMEANT, but these first
results already demonstrate XMEANT?s potential
to drive research progress toward semantic SMT.
6 Acknowledgments
This material is based upon work supported
in part by the Defense Advanced Research
Projects Agency (DARPA) under BOLT con-
tract nos. HR0011-12-C-0014 and HR0011-12-
C-0016, and GALE contract nos. HR0011-06-C-
0022 and HR0011-06-C-0023; by the European
Union under the FP7 grant agreement no. 287658;
and by the Hong Kong Research Grants Council
(RGC) research grants GRF620811, GRF621008,
and GRF612806. Any opinions, findings and
conclusions or recommendations expressed in this
material are those of the authors and do not nec-
essarily reflect the views of DARPA, the EU, or
RGC.
769
References
Karteek Addanki, Chi-Kiu Lo, Markus Saers, and
Dekai Wu. LTG vs. ITG coverage of cross-lingual
verb frame alternations. In 16th Annual Conference
of the European Association for Machine Transla-
tion (EAMT-2012), Trento, Italy, May 2012.
Hala Almaghout and Lucia Specia. A CCG-based qual-
ity estimation metric for statistical machine transla-
tion. In Machine Translation Summit XIV (MT Sum-
mit 2013), Nice, France, 2013.
Eleftherios Avramidis and Maja Popovi?c. Machine
learning methods for comparative and time-oriented
quality estimation of machine translation output. In
8th Workshop on Statistical Machine Translation
(WMT 2013), 2013.
Eleftherios Avramidis. Quality estimation for machine
translation output using linguistic analysis and de-
coding features. In 7th Workshop on Statistical Ma-
chine Translation (WMT 2012), 2012.
Satanjeev Banerjee and Alon Lavie. METEOR: An
automatic metric for MT evaluation with improved
correlation with human judgments. In Workshop on
Intrinsic and Extrinsic Evaluation Measures for Ma-
chine Translation and/or Summarization, Ann Ar-
bor, Michigan, June 2005.
John Blatz, Erin Fitzgerald, George Foster, Simona
Gandrabur, Cyril Goutte, Alex Kulesza, Alberto
Sanchis, and Nicola Ueffing. Confidence estimation
for machine translation. In 20th international con-
ference on Computational Linguistics, 2004.
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, and Robert L. Mercer. The mathemat-
ics of machine translation: Parameter estimation.
Computational Linguistics, 19(2):263?311, 1993.
Chris Callison-Burch, Miles Osborne, and Philipp
Koehn. Re-evaluating the role of BLEU in machine
translation research. In 11th Conference of the Euro-
pean Chapter of the Association for Computational
Linguistics (EACL-2006), 2006.
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz, and Josh Schroeder. (meta-)
evaluation of machine translation. In Second Work-
shop on Statistical Machine Translation (WMT-07),
2007.
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz, and Josh Schroeder. Further
meta-evaluation of machine translation. In Third
Workshop on Statistical Machine Translation (WMT-
08), 2008.
Julio Castillo and Paula Estrella. Semantic textual sim-
ilarity for MT evaluation. In 7th Workshop on Sta-
tistical Machine Translation (WMT 2012), 2012.
George Doddington. Automatic evaluation of machine
translation quality using n-gram co-occurrence
statistics. In The second international conference on
Human Language Technology Research (HLT ?02),
San Diego, California, 2002.
Simona Gandrabur, George Foster, and Guy Lapalme.
Confidence estimation for nlp applications. ACM
Transactions on Speech and Language Processing,
2006.
Jes?us Gim?enez and Llu??s M`arquez. Linguistic features
for automatic evaluation of heterogenous MT sys-
tems. In Second Workshop on Statistical Machine
Translation (WMT-07), pages 256?264, Prague,
Czech Republic, June 2007.
Jes?us Gim?enez and Llu??s M`arquez. A smorgasbord
of features for automatic MT evaluation. In Third
Workshop on Statistical Machine Translation (WMT-
08), Columbus, Ohio, June 2008.
Yifan He, Yanjun Ma, Andy Way, and Josef van
Genabith. Rich linguistic features for translation
memory-inspired consistent translation. In 13th Ma-
chine Translation Summit (MT Summit XIII), 2011.
Philipp Koehn and Christof Monz. Manual and auto-
matic evaluation of machine translation between eu-
ropean languages. In Workshop on Statistical Ma-
chine Translation (WMT-06), 2006.
Gregor Leusch, Nicola Ueffing, and Hermann Ney.
CDer: Efficient MT evaluation using block move-
ments. In 11th Conference of the European Chap-
ter of the Association for Computational Linguistics
(EACL-2006), 2006.
Ding Liu and Daniel Gildea. Syntactic features for
evaluation of machine translation. In Workshop on
Intrinsic and Extrinsic Evaluation Measures for Ma-
chine Translation and/or Summarization, Ann Ar-
bor, Michigan, June 2005.
Chi-kiu Lo and Dekai Wu. MEANT: An inexpensive,
high-accuracy, semi-automatic metric for evaluating
translation utility based on semantic roles. In 49th
Annual Meeting of the Association for Computa-
tional Linguistics: Human Language Technologies
(ACL HLT 2011), 2011.
Chi-kiu Lo and Dekai Wu. SMT vs. AI redux: How
semantic frames evaluate MT more accurately. In
Twenty-second International Joint Conference on
Artificial Intelligence (IJCAI-11), 2011.
Chi-kiu Lo and Dekai Wu. Unsupervised vs. super-
vised weight estimation for semantic MT evalua-
tion metrics. In Sixth Workshop on Syntax, Seman-
tics and Structure in Statistical Translation (SSST-
6), 2012.
Chi-kiu Lo and Dekai Wu. Can informal genres be
better translated by tuning on automatic semantic
metrics? In 14th Machine Translation Summit (MT
Summit XIV), 2013.
Chi-kiu Lo and Dekai Wu. MEANT at WMT 2013:
A tunable, accurate yet inexpensive semantic frame
based mt evaluation metric. In 8th Workshop on Sta-
tistical Machine Translation (WMT 2013), 2013.
Chi-kiu Lo, Anand Karthik Tumuluru, and Dekai Wu.
Fully automatic semantic MT evaluation. In 7th
Workshop on Statistical Machine Translation (WMT
2012), 2012.
Chi-kiu Lo, Karteek Addanki, Markus Saers, and
Dekai Wu. Improving machine translation by train-
ing against an automatic semantic frame based eval-
770
uation metric. In 51st Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL 2013),
2013.
Chi-kiu Lo, Meriem Beloucif, and Dekai Wu. Im-
proving machine translation into Chinese by tun-
ing against Chinese MEANT. In International
Workshop on Spoken Language Translation (IWSLT
2013), 2013.
Yanjun Ma, Yifan He, Andy Way, and Josef van Gen-
abith. Consistent translation using discriminative
learning: a translation memory-inspired approach.
In 49th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies (ACL HLT 2011). Association for Computa-
tional Linguistics, 2011.
Matou?s Mach?a?cek and Ond?rej Bojar. Results of the
WMT13 metrics shared task. In Eighth Workshop on
Statistical Machine Translation (WMT 2013), Sofia,
Bulgaria, August 2013.
Yashar Mehdad, Matteo Negri, and Marcello Federico.
Match without a referee: evaluating mt adequacy
without reference translations. In 7th Workshop on
Statistical Machine Translation (WMT 2012), 2012.
Rada Mihalcea, Courtney Corley, and Carlo Strappar-
ava. Corpus-based and knowledge-based measures
of text semantic similarity. In The Twenty-first Na-
tional Conference on Artificial Intelligence (AAAI-
06), volume 21. Menlo Park, CA; Cambridge, MA;
London; AAAI Press; MIT Press; 1999, 2006.
Sonja Nie?en, Franz Josef Och, Gregor Leusch, and
Hermann Ney. A evaluation tool for machine trans-
lation: Fast evaluation for MT research. In The
Second International Conference on Language Re-
sources and Evaluation (LREC 2000), 2000.
Karolina Owczarzak, Josef van Genabith, and Andy
Way. Dependency-based automatic evaluation for
machine translation. In Syntax and Structure in Sta-
tistical Translation (SSST), 2007.
Karolina Owczarzak, Josef van Genabith, and Andy
Way. Evaluating machine translation with LFG de-
pendencies. Machine Translation, 21:95?119, 2007.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. BLEU: a method for automatic evalua-
tion of machine translation. In 40th Annual Meet-
ing of the Association for Computational Linguistics
(ACL-02), pages 311?318, Philadelphia, Pennsylva-
nia, July 2002.
Sameer Pradhan, Wayne Ward, Kadri Hacioglu,
James H. Martin, and Dan Jurafsky. Shallow seman-
tic parsing using support vector machines. In Hu-
man Language Technology Conference of the North
American Chapter of the Association for Computa-
tional Linguistics (HLT-NAACL 2004), 2004.
Christopher Quirk. Training a sentence-level machine
translation confidence measure. In Fourth Interna-
tional Conference on Language Resources and Eval-
uation (LREC 2004), Lisbon, Portugal, May 2004.
Miguel Rios, Wilker Aziz, and Lucia Specia. Tine: A
metric to assess MT adequacy. In Sixth Workshop on
Statistical Machine Translation (WMT 2011), 2011.
Markus Saers and Dekai Wu. Improving phrase-based
translation via word alignments from stochastic in-
version transduction grammars. In Third Workshop
on Syntax and Structure in Statistical Translation
(SSST-3), Boulder, Colorado, June 2009.
Markus Saers, Joakim Nivre, and Dekai Wu. Learning
stochastic bracketing inversion transduction gram-
mars with a cubic time biparsing algorithm. In 11th
International Conference on Parsing Technologies
(IWPT?09), Paris, France, October 2009.
Kashif Shah, Trevor Cohn, and Lucia Specia. An in-
vestigation on the effectiveness of features for trans-
lation quality estimation. In Machine Translation
Summit XIV (MT Summit 2013), Nice, France, 2013.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. A study of trans-
lation edit rate with targeted human annotation. In
7th Biennial Conference Association for Machine
Translation in the Americas (AMTA 2006), pages
223?231, Cambridge, Massachusetts, August 2006.
Lucia Specia. Exploiting objective annotations for
measuring translation post-editing effort. In 15th
Conference of the European Association for Ma-
chine Translation, pages 73?80, 2011.
Anand Karthik Tumuluru, Chi-kiu Lo, and Dekai Wu.
Accuracy and robustness in measuring the lexical
similarity of semantic role fillers for automatic se-
mantic MT evaluation. In 26th Pacific Asia Confer-
ence on Language, Information, and Computation
(PACLIC 26), 2012.
Nicola Ueffing and Hermann Ney. Word-level con-
fidence estimation for machine translation using
phrase-based translation models. In Proceedings of
the conference on Human Language Technology and
Empirical Methods in Natural Language Process-
ing, pages 763?770, 2005.
Mengqiu Wang and Christopher D. Manning. SPEDE:
Probabilistic edit distance metrics for MT evalua-
tion. In 7th Workshop on Statistical Machine Trans-
lation (WMT 2012), 2012.
Dekai Wu, Chi-kiu Lo, Meriem Beloucif, and Markus
Saers. IMEANT: Improving semantic frame based
MT evaluation via inversion transduction grammars.
Forthcoming, 2014.
Dekai Wu. Stochastic inversion transduction grammars
and bilingual parsing of parallel corpora. Computa-
tional Linguistics, 23(3):377?403, 1997.
Deyi Xiong, Min Zhang, and Haizhou Li. Error detec-
tion for statistical machine translation using linguis-
tic features. In 48th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL 2010),
2010.
Richard Zens and Hermann Ney. A comparative
study on reordering constraints in statistical machine
translation. In 41st Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL-2003),
pages 144?151, Stroudsburg, Pennsylvania, 2003.
771
Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 22?33,
October 25, 2014, Doha, Qatar. c?2014 Association for Computational Linguistics
Better Semantic Frame Based MT Evaluation
via Inversion Transduction Grammars
Dekai Wu Lo Chi-kiu Meriem Beloucif Markus Saers
HKUST
Human Language Technology Center
Department of Computer Science and Engineering
Hong Kong University of Science and Technology
{jackielo|mbeloucif|masaers|dekai}@cs.ust.hk
Abstract
We introduce an inversion transduc-
tion grammar based restructuring of
the MEANT automatic semantic frame
based MT evaluation metric, which,
by leveraging ITG language biases, is
able to further improve upon MEANT?s
already-high correlation with human
adequacy judgments. The new metric,
called IMEANT, uses bracketing ITGs to
biparse the reference and machine transla-
tions, but subject to obeying the semantic
frames in both. Resulting improvements
support the presumption that ITGs, which
constrain the allowable permutations
between compositional segments across
the reference and MT output, score the
phrasal similarity of the semantic role
fillers more accurately than the simple
word alignment heuristics (bag-of-word
alignment or maximum alignment) used
in previous version of MEANT. The
approach successfully integrates (1) the
previously demonstrated extremely high
coverage of cross-lingual semantic frame
alternations by ITGs, with (2) the high
accuracy of evaluating MT via weighted
f-scores on the degree of semantic frame
preservation.
1 Introduction
There has been to date relatively little use of in-
version transduction grammars (Wu, 1997) to im-
prove the accuracy of MT evaluation metrics, de-
spite long empirical evidence the vast majority of
translation patterns between human languages can
be accommodated within ITG constraints (and the
observation that most current state-of-the-art SMT
systems employ ITG decoders). We show that
ITGs can be used to redesign the MEANT seman-
tic frame based MT evaluation metric (Lo et al.,
2012) to produce improvements in accuracy and
reliability. This work is driven by the motiva-
tion that especially when considering semanticMT
metrics, ITGs would be seem to be a natural basis
for several reasons.
To begin with, it is quite natural to think of
sentences as having been generated from an ab-
stract concept using a rewriting system: a stochas-
tic grammar predicts how frequently any particu-
lar realization of the abstract concept will be gen-
erated. The bilingual analogy is a transduction
grammar generating a pair of possible realizations
of the same underlying concept. Stochastic trans-
duction grammars predict how frequently a partic-
ular pair of realizations will be generated, and thus
represent a good way to evaluate how well a pair
of sentences correspond to each other.
The particular class of transduction gram-
mars known as ITGs tackle the problem that
the (bi)parsing complexity for general syntax-
directed transductions (Aho and Ullman, 1972)
is exponential. By constraining a syntax-directed
transduction grammar to allow only monotonic
straight and inverted reorderings, or equivalently
permitting only binary or ternary rank rules, it is
possible to isolate the low end of that hierarchy into
a single equivalence class of inversion transduc-
tions. ITGs are guaranteed to have a two-normal
form similar to context-free grammars, and can
be biparsed in polynomial time and space (O
(
n
6
)
time andO
(
n
4
)
space). It is also possible to do ap-
proximate biparsing in O
(
n
3
)
time (Saers et al.,
2009). These polynomial complexities makes it
feasible to estimate the parameters of an ITG us-
ing standard machine learning techniques such as
expectation maximization (Wu, 1995b) .
At the same time, inversion transductions have
also been directly shown to be more than sufficient
to account for the reordering that occur within se-
mantic frame alternations (Addanki et al., 2012).
This makes ITGs an appealing alternative for eval-
22
uating the possible links between both semantic
role fillers in different languages as well as the
predicates, and how these parts fit together to form
entire semantic frames. We believe that ITGs are
not only capable of generating the desired struc-
tural correspondences between the semantic struc-
tures of two languages, but also provide meaning-
ful constraints to prevent alignments from wander-
ing off in the wrong direction.
In this paper we show that IMEANT, a newmet-
ric drawing from the strengths of both MEANT
and inversion transduction grammars, is able to
exploit bracketing ITGs (also known as BITGs
or BTGs) which are ITGs containing only a sin-
gle non-differentiated non terminal category (Wu,
1995a), so as to produce even higher correlation
with human adequacy judgments than any auto-
matic MEANT variants, or other common auto-
matic metrics. We argue that the constraints pro-
vided by BITGs over the semantic frames and ar-
guments of the reference and MT output sentences
are essential for accurate evaluation of the phrasal
similarity of the semantic role fillers.
In common with the various MEANT semantic
MT evaluation metrics (Lo and Wu, 2011a, 2012;
Lo et al., 2012; Lo and Wu, 2013b), our proposed
IMEANT metric measures the degree to which
the basic semantic event structure is preserved
by translation?the ?who did what to whom, for
whom, when, where, how and why? (Pradhan et
al., 2004)?emphasizing that a good translation
is one that can successfully be understood by a
human. In the other versions of MEANT, sim-
ilarity between the MT output and the reference
translations is computed as a modified weighted f-
score over the semantic predicates and role fillers.
Across a variety of language pairs and genres, it
has been shown thatMEANT correlates better with
human adequacy judgment than both n-gram based
MT evaluation metrics such as BLEU (Papineni
et al., 2002), NIST (Doddington, 2002), and ME-
TEOR (Banerjee and Lavie, 2005), as well as edit-
distance based metrics such as CDER (Leusch et
al., 2006), WER (Nie?en et al., 2000), and TER
(Snover et al., 2006) when evaluating MT output
(Lo and Wu, 2011a, 2012; Lo et al., 2012; Lo and
Wu, 2013b; Mach??ek and Bojar, 2013). Further-
more, tuning the parameters of MT systems with
MEANT instead of BLEU or TER robustly im-
proves translation adequacy across different gen-
res and different languages (English and Chinese)
(Lo et al., 2013a; Lo and Wu, 2013a; Lo et al.,
2013b). This has motivated our choice of MEANT
as the basis on which to experiment with deploying
ITGs into semantic MT evaluation.
2 Related Work
2.1 ITGs and MT evaluation
Relatively little investigation into the potential
benefits of ITGs is found in previous MT eval-
uation work. One exception is invWER, pro-
posed by Leusch et al. (2003) and Leusch and Ney
(2008). The invWER metric interprets weighted
BITGs as a generalization of the Levenshtein edit
distance, in which entire segments (blocks) can be
inverted, as long as this is done strictly compo-
sitionally so as not to violate legal ITG biparse
tree structures. The input and output languages
are considered to be those of the reference and ma-
chine translations, and thus are over the same vo-
cabulary (say,English). At the sentence level, cor-
relation of invWER with human adequacy judg-
ments was found to be among the best.
Our current approach differs in several key
respects from invWER. First,invWER operates
purely at the surface level of exact token match,
IMEANT mediates between segments of refer-
ence translation andMT output using lexical BITG
probabilities.
Secondly, there is no explicit semantic model-
ing in invWER. Providing they meet the BITG
constraints, the biparse trees in invWER are com-
pletely unconstrained. In contrast, IMEANT em-
ploys the same explicit, strong semantic frame
modeling as MEANT, on both the reference and
machine translations. In IMEANT, the semantic
frames always take precedence over pure BITG
biases. Compared to invWER, this strongly con-
strains the space of biparses that IMEANT permits
to be considered.
2.2 MT evaluation metrics
Like invWER, other common surface-form ori-
ented metrics like BLEU (Papineni et al., 2002),
NIST (Doddington, 2002), METEOR (Banerjee
and Lavie, 2005; Denkowski and Lavie, 2014),
CDER (Leusch et al., 2006), WER (Nie?en et
al., 2000), and TER (Snover et al., 2006) do
not correctly reflect the meaning similarities of
the input sentence. There are in fact several
large scale meta-evaluations (Callison-Burch et
al., 2006; Koehn and Monz, 2006) reporting cases
23
where BLEU strongly disagrees with human judg-
ments of translation adequacy.
Such observations have generated a recent surge
of work on developing MT evaluation metrics that
would outperform BLEU in correlation with hu-
man adequacy judgment (HAJ). Like MEANT, the
TINE automatic recall-oriented evaluation metric
(Rios et al., 2011) aims to preserve basic event
structure. However, its correlation with human ad-
equacy judgment is comparable to that of BLEU
and not as high as that of METEOR. Owczarzak
et al. (2007a,b) improved correlation with human
fluency judgments by using LFG to extend the ap-
proach of evaluating syntactic dependency struc-
ture similarity proposed by Liu and Gildea (2005),
but did not achieve higher correlation with hu-
man adequacy judgments than metrics like ME-
TEOR. Another automatic metric, ULC (Gim?nez
and M?rquez, 2007, 2008), incorporates several
semantic similarity features and shows improved
correlation with human judgement of translation
quality (Callison-Burch et al., 2007; Gim?nez
and M?rquez, 2007; Callison-Burch et al., 2008;
Gim?nez and M?rquez, 2008) but no work has
been done towards tuning an SMT system using
a pure form of ULC perhaps due to its expensive
run time. Likewise, SPEDE (Wang and Manning,
2012) predicts the edit sequence needed to match
the machine translation to the reference translation
via an integrated probabilistic FSM and probabilis-
tic PDA model. The semantic textual similarity
metric Sagan (Castillo and Estrella, 2012) is based
on a complex textual entailment pipeline. These
aggregated metrics require sophisticated feature
extraction steps, contain many parameters that
need to be tuned, and employ expensive linguis-
tic resources such asWordNet or paraphrase tables.
The expensive training, tuning and/or running time
renders these metrics difficult to use in the SMT
training cycle.
3 IMEANT
In this section we give a contrastive description
of IMEANT: we first summarize the MEANT ap-
proach, and then explain how IMEANT differs.
3.1 Variants of MEANT
MEANT and its variants (Lo et al., 2012) measure
weighted f-scores over corresponding semantic
frames and role fillers in the reference andmachine
translations. The automatic versions of MEANT
replace humans with automatic SRL and align-
ment algorithms. MEANT typically outperforms
BLEU, NIST, METEOR, WER, CDER and TER
in correlation with human adequacy judgment, and
is relatively easy to port to other languages, re-
quiring only an automatic semantic parser and a
monolingual corpus of the output language, which
is used to gauge lexical similarity between the se-
mantic role fillers of the reference and translation.
MEANT is computed as follows:
1. Apply an automatic shallow semantic parser
to both the reference and machine transla-
tions. (Figure 1 shows examples of auto-
matic shallow semantic parses on both refer-
ence and MT.)
2. Apply the maximum weighted bipartite
matching algorithm to align the semantic
frames between the reference and machine
translations according to the lexical similari-
ties of the predicates. (Lo and Wu (2013a)
proposed a backoff algorithm that evaluates
the entire sentence of theMT output using the
lexical similarity based on the context vector
model, if the automatic shallow semantic
parser fails to parse the reference or machine
translations.)
3. For each pair of the aligned frames, apply the
maximum weighted bipartite matching algo-
rithm to align the arguments between the ref-
erence andMT output according to the lexical
similarity of role fillers.
4. Compute the weighted f-score over the
matching role labels of these aligned predi-
cates and role fillers according to the follow-
ing definitions:
q
0
i,j
? ARG j of aligned frame i in MT
q
1
i,j
? ARG j of aligned frame i in REF
w
0
i
?
#tokens filled in aligned frame i of MT
total #tokens in MT
w
1
i
?
#tokens filled in aligned frame i of REF
total #tokens in REF
wpred ? weight of similarity of predicates
w
j
? weight of similarity of ARG j
e
i,pred ? the pred string of the aligned frame i of MT
f
i,pred ? the pred string of the aligned frame i of REF
e
i,j
? the role fillers of ARG j of the aligned frame i of MT
f
i,j
? the role fillers of ARG j of the aligned frame i of REF
s(e, f) = lexical similarity of token e and f
24
[IN] ?? ? ? ?? ?? ?? ? ? ? ? ? ? ????? ?? ?? ?? ?? ?  
[REF] Until after their sales had ceased in mainland China for almost two months , sales of the complete range of SK ? II products have now been resumed . 
ARGM-TMP PRED ARGM-LOC PRED ARG1 
ARGM-LOC PRED ARG1 PRED ARG1 
ARG0 ARGM-TMP 
[MT1] So far , nearly two months sk - ii the sale of products in the mainland of China to resume sales .  
PRED ARG0 ARG1 
[MT2] So far , in the mainland of China to stop selling nearly two months of SK - 2 products sales resumed .  
ARGM-TMP ARG1 PRED PRED ARG1 
[MT3] So far , the sale in the mainland of China for nearly two months of SK - II line of products .  
PRED 
PRED ARG0 
ARG1 ARGM-TMP 
ARGM-ADV 
ARG0 
ARGM-EXT 
Figure 1: Examples of automatic shallow semantic parses. Both the reference and machine translations
are parsed using automatic English SRL. There are no semantic frames forMT3 since there is no predicate
in the MT output.
prece,f =
?
e?e max
f?f
s(e, f)
| e |
rece,f =
?
f?f max
e?e
s(e, f)
| f |
s
i,pred =
2 ? prece
i,pred,fi,pred ? recei,pred,fi,pred
prece
i,pred,fi,pred + recei,pred,fi,pred
s
i,j
=
2 ? prece
i,j
,f
i,j
? rece
i,j
,f
i,j
prece
i,j
,f
i,j
+ rece
i,j
,f
i,j
precision =
?
i
w
0
i
wpredsi,pred+
?
j
w
j
s
i,j
wpred+
?
j
w
j
|q
0
i,j
|
?
i
w
0
i
recall =
?
i
w
1
i
wpredsi,pred+
?
j
w
j
s
i,j
wpred+
?
j
w
j
|q
1
i,j
|
?
i
w
1
i
MEANT = 2 ? precision ? recallprecision + recall
where q0
i,j
and q1
i,j
are the argument of type j in
frame i inMT andREF respectively.w0
i
andw1
i
are
the weights for frame i in MT/REF respectively.
These weights estimate the degree of contribution
of each frame to the overall meaning of the sen-
tence. wpred and wj are the weights of the lexical
similarities of the predicates and role fillers of the
arguments of type j of all frame between the ref-
erence translations and the MT output.There is a
total of 12 weights for the set of semantic role la-
bels in MEANT as defined in Lo and Wu (2011b).
For MEANT, they are determined using super-
vised estimation via a simple grid search to opti-
mize the correlation with human adequacy judg-
ments (Lo andWu, 2011a). For UMEANT (Lo and
Wu, 2012), they are estimated in an unsupervised
manner using relative frequency of each semantic
role label in the references and thus UMEANT is
useful when human judgments on adequacy of the
development set are unavailable.
s
i,pred and si,j are the lexical similarities based
on a context vectormodel of the predicates and role
fillers of the arguments of type j between the ref-
erence translations and the MT output. Lo et al.
(2012) and Tumuluru et al. (2012) described how
the lexical and phrasal similarities of the semantic
role fillers are computed. A subsequent variant of
the aggregation function inspired by Mihalcea et
al. (2006) that normalizes phrasal similarities ac-
cording to the phrase length more accurately was
used in more recent work (Lo et al., 2013a; Lo and
Wu, 2013a; Lo et al., 2013b). In this paper, we
will assess IMEANT against the latest version of
MEANT (Lo et al., 2014) which, as shown, uses
f-score to aggregate individual token similarities
into the composite phrasal similarities of semantic
role fillers,since this has been shown to bemore ac-
curate than the previously used aggregation func-
tions.
Recent studies (Lo et al., 2013a; Lo and Wu,
2013a; Lo et al., 2013b) show that tuning MT sys-
tems against MEANT produces more robustly ad-
equate translations than the common practice of
tuning against BLEU or TER across different data
genres, such as formal newswire text, informal
web forum text and informal public speech.
25
In an alternative quality-estimation oriented line
of research, Lo et al. (2014) describe a cross-
lingual variant called XMEANT capable of eval-
uating translation quality without the need for ex-
pensive human reference translations, by utiliz-
ing semantic parses of the original foreign in-
put sentence instead of a reference translation.
Since XMEANT?s results could have been due
to either (1) more accurate evaluation of phrasal
similarity via cross-lingual translation probabili-
ties, or (2) better match of semantic frames with-
out reference translations, there is no direct evi-
dencewhether ITGs contribute to the improvement
in MEANT?s correlation with human adequacy
judgment. For the sake of better understanding
whether ITGs improve semantic MT evaluation,
we will also assess IMEANT against cross-lingual
XMEANT.
3.2 The IMEANT metric
Although MEANT was previously shown to pro-
duce higher correlation with human adequacy
judgments compared to other automatic metrics,
our error analyses suggest that it still suffers from a
common weakness among metrics employing lex-
ical similarity, namely that word/token alignments
between the reference and machine translations
are severely under constrained. No bijectivity or
permutation restrictions are applied, even between
compositional segments where this should be nat-
ural. This can cause role fillers to be aligned even
when they should not be. IMEANT, in contrast,
uses a bracketing inversion transduction grammar
to constrain permissible token alignment patterns
between aligned role filler phrases. The semantic
frames above the token level also fits ITG com-
positional structure, consistent with the aforemen-
tioned semantic frame alternation coverage study
of Addanki et al. (2012). Figure 2 illustrates how
the ITG constraints are consistent with the needed
permutations between semantic role fillers across
the reference and machine translations for a sam-
ple sentence from our evaluation data, which as
we will see leads to higher HAJ correlations than
MEANT.
Subject to the structural ITG constraints,
IMEANT scores sentence translations in a spirit
similar to the way MEANT scores them: it utilizes
an aggregated score over the matched semantic
role labels of the automatically aligned semantic
frames and their role fillers between the reference
and machine translations. Despite the structural
differences, like MEANT, at the conceptual level
IMEANT still aims to evaluate MT output in
terms of the degree to which the translation has
preserved the essential ?who did what to whom,for
whom, when, where, how and why? of the foreign
input sentence.
Unlike MEANT, however, IMEANT aligns and
scores under ITG assumptions. MEANT uses a
maximum alignment algorithm to align the tokens
in the role fillers between the reference and ma-
chine translations, and then scores by aggregating
the lexical similarities into a phrasal similarity us-
ing an f-measure. In contrast, IMEANT aligns and
scores by utilizing a length-normalized weighted
BITG (Wu, 1997; Zens and Ney, 2003; Saers and
Wu, 2009; Addanki et al., 2012). To be precise in
this regard, we can see IMEANT as differing from
the foregoing description of MEANT in the defi-
nition of s
i,pred and si,j , as follows.
G ? ?{A} ,W0,W1,R,A?
R ? {A ? [AA] ,A ? ?AA?,A ? e/f}
p ([AA] |A) = p (?AA?|A) = 1
p (e/f |A) = s(e, f)
s
i,pred = lg?1
?
?
lg
(
P
(
A ?? e
i,pred/fi,pred|G
))
max(| e
i,pred |, | fi,pred |)
?
?
s
i,j
= lg?1
?
?
lg
(
P
(
A ?? e
i,j
/f
i,j
|G
))
max(| e
i,j
|, | f
i,j
|)
?
?
where G is a bracketing ITG whose only non ter-
minal is A, andR is a set of transduction rules with
e ? W
0
?{?} denoting a token in theMToutput (or
the null token) and f ? W1?{?} denoting a token
in the reference translation (or the null token). The
rule probability (or more accurately, rule weight)
function p is set to be 1 for structural transduction
rules, and for lexical transduction rules it is de-
fined using MEANT?s context vector model based
lexical similarity measure. To calculate the inside
probability (or more accurately, inside score) of a
pair of segments, P
(
A ?? e/f|G
)
, we use the al-
gorithm described in Saers et al. (2009). Given
this, s
i,pred and si,j now represent the length nor-
malized BITG parse scores of the predicates and
role fillers of the arguments of type j between the
reference and machine translations.
4 Experiments
In this section we discuss experiments indicating
that IMEANT further improves upon MEANT?s
26
[REF] The reduction in hierarchy helps raise the efficiency of inspection and supervisory work .  
[MT2] The level of reduction is conducive to raising the inspection and supervision work efficiency .  
ARG0 ARG1 PRED 
ARG0 PRED ARG1 
The 
level 
of 
reduction 
is 
conducive 
to 
raising 
the 
inspection 
and 
supervision 
work 
efficiency 
. 
Th
e 
red
uct
ion
 in 
hie
rar
chy
 
hel
ps rais
e the
 
effi
cie
ncy
 of 
ins
pec
tion
 
and
 
sup
erv
iso
ry . 
wo
rk 
pred 
ARG0 
ARG1 
pr
ed
 
AR
G0
 
AR
G1
 
Figure 2: An example of aligning automatic shallow semantic parses under ITGs, visualized using both
biparse tree and alignment matrix depictions, for the Chinese input sentence ????????????
????????. Both the reference and machine translations are parsed using automatic English SRL.
Compositional alignments between the semantic frames and the tokens within role filler phrases obey
inversion transduction grammars.
already-high correlation with human adequacy
judgments.
4.1 Experimental setup
We perform the meta-evaluation upon two differ-
ent partitions of the DARPA GALE P2.5 Chinese-
English translation test set. The corpus includes
the Chinese input sentences, each accompanied by
one English reference translation and three partic-
ipating state-of-the-art MT systems? output.
For the sake of consistent comparison, the first
evaluation partition, GALE-A, is the same as the
one used in Lo and Wu (2011a), and the second
evaluation partition, GALE-B, is the same as the
one used in Lo and Wu (2011b).
For both reference and machine translations, the
ASSERT (Pradhan et al., 2004) semantic role la-
beler was used to automatically predict semantic
parses.
27
Table 1: Sentence-level correlation with human
adequacy judgements on different partitions of
GALE P2.5 data. IMEANT always yields top
correlations, and is more consistent than either
MEANT or its recent cross-lingual XMEANT
quality estimation variant. For reference, the hu-
man HMEANT upper bound is 0.53 for GALE-A
and 0.37 for GALE-B?thus, the fully automated
IMEANT approximation is not far from closing the
gap.
metric GALE-A GALE-B
IMEANT 0.51 0.33
XMEANT 0.51 0.20
MEANT 0.48 0.33
METEOR 1.5 (2014) 0.43 0.10
NIST 0.29 0.16
METEOR 0.4.3 (2005) 0.20 0.29
BLEU 0.20 0.27
TER 0.20 0.19
PER 0.20 0.18
CDER 0.12 0.16
WER 0.10 0.26
4.2 Results
The sentence-level correlations in Table 1 show
that IMEANT outperforms other automatic met-
rics in correlation with human adequacy judgment.
Note that this was achieved with no tuning what-
soever of the default rule weights (suggesting that
the performance of IMEANT could be further im-
proved in the future by slightly optimizing the ITG
weights).
On the GALE-A partition, IMEANT shows 3
points improvement over MEANT, and is tied
with the cross-lingual XMEANT quality estimator
discussed earlier.IMEANT produces much higher
HAJ correlations than any of the other metrics.
On the GALE-B partition, IMEANT is tied with
MEANT, and is significantly better correlated with
HAJ than the XMEANT quality estimator. Again,
IMEANT produces much higher HAJ correlations
than any of the other metrics.
We note that we have also observed this pattern
consistently in smaller-scale experiments?while
the monolingual MEANT metric and its cross-
lingual XMEANT cousin vie with each other on
different data sets, IMEANT robustly and consis-
tently produces top HAJ correlations.
In both the GALE-A and GALE-B partitions,
IMEANT comes within a few points of the human
upper bound benchmark HAJ correlations com-
puted using the human labeled semantic frames
and alignments used in the HMEANT.
Data analysis reveals two reasons that IMEANT
correlates with human adequacy judgement more
closely than MEANT. First, BITG constraints in-
deed provide more accurate phrasal similarity ag-
gregation, compared to the naive bag-of-words
based heuristics employed in MEANT. Similar re-
sults have been observed while trying to estimate
word alignment probabilities where BITG con-
straints outperformed alignments from GIZA++
(Saers and Wu, 2009).
Secondly, the permutation and bijectivity con-
straints enforced by the ITG provide better lever-
age to reject token alignments when they are not
appropriate, compared with the maximal align-
ment approach which tends to be rather promiscu-
ous. A case of this can be seen in Figure 3, which
shows the result on the same example sentence as
in Figure 1. Disregarding the semantic parsing er-
rors arising from the current limitations of auto-
matic SRL tools, the ITG tends to provide clean,
sparse alignments for role fillers like the ARG1
of the resumed PRED, preferring to leave tokens
like complete and range unaligned instead of aligning
them anyway as MEANT?s maximal alignment al-
gorithm tends to do. Note that it is not simply a
matter of lowering thresholds for accepting token
alignments: Tumuluru et al. (2012) showed that
the competitive linking approach (Melamed, 1996)
which also generally produces sparser alignments
does not work as well inMEANT, whereas the ITG
appears to be selective about the token alignments
in a manner that better fits the semantic structure.
For contrast, Figure 4 shows a case where
IMEANT appropriately accepts dense alignments.
5 Conclusion
We have presented IMEANT, an inversion trans-
duction grammar based rethinking of the MEANT
semantic frame based MT evaluation approach,
that achieves higher correlation with human ad-
equacy judgments of MT output quality than
MEANT and its variants, as well as other com-
mon evaluation metrics. Our results improve upon
previous research showing that MEANT?s explicit
use of semantic frames leads to state-of-the-art au-
tomatic MT evaluation. IMEANT achieves this
by aligning and scoring semantic frames under a
simple, consistent ITG that provides empirically
28
[REF] Until after their sales had ceased in mainland China for almost two months , sales of the complete range of SK ? II products have now been resumed .  
ARG0 PRED ARGM-LOC PRED ARG1 
[MT2] So far , in the mainland of China to stop selling nearly two months of SK - 2 products sales resumed .  
ARGM-TMP ARG1 PRED PRED ARG1 PRED 
ARGM-TMP ARGM-TMP 
So 
far 
, 
in 
the 
mainland 
of 
China 
to 
stop 
selling 
nearly 
two 
months 
of 
SK 
- 
2 
products 
sales 
resumed 
. 
Un
til 
afte
r 
the
ir 
sal
e had
 
cea
sed
 in 
ma
inla
nd 
Ch
ina
 for 
alm
ost
 
two
 , 
sal
es of the
 
com
ple
te 
PRED 
PRED 
ARG1 
ARGM-TMP 
ARG1 
PRED 
PR
ED
 
PR
ED
 
AR
G1
 
AR
GM
-L
OC
 
AR
G0
 
ran
ge of SK
 - II 
pro
duc
ts 
hav
e 
now
 
bee
n 
res
um
ed . 
mo
nth
s 
AR
GM
-T
MP
 
AR
GM
-T
MP
 
Figure 3: An example where the ITG helps produce correctly sparse alignments by rejecting inappro-
priate token alignments in the ARG1 of the resumed PRED, instead of wrongly aligning tokens like the,
complete, and range as MEANT tends to do. (The semantic parse errors are due to limitations of automatic
SRL.)
informative permutation and bijectivity biases, in-
stead of the maximal alignment and bag-of-words
assumptions used by MEANT. At the same time,
IMEANT retains the Occam?s Razor style simplic-
ity and representational transparency characteris-
tics of MEANT.
Given the absence of any tuning of ITG weights
in this first version of IMEANT, we speculate that
29
[REF] Australian Prime Minister Howard said the government could cancel AWB 's monopoly in the wheat business next week . 
[MT2] Australian Prime Minister John Howard said that the Government might cancel the AWB company wheat monopoly next week . 
ARG0 
ARG0 
PRED 
ARG0 PRED ARG1 PRED 
ARG0 
ARGM-MOD ARGM-TMP 
ARG1 
PRED ARGM-MOD ARG1 ARGM-TMP 
ARG1 
Australian 
Prime 
Minister 
John 
Howard 
said 
the 
Government 
might 
cancel 
the 
AWB 
company 
wheat 
monopoly 
Au
str
alia
n 
Pri
me
 
Min
iste
r 
Ho
wa
rd sai
d the
 
go
ver
nm
en
t 
cou
ld 
can
cel
 
AW
B 's 
mo
no
po
ly the
 in 
next 
week 
that 
. 
wh
ea
t 
bu
sin
ess
 
ne
xt 
we
ek . 
pr
ed
 
pr
ed
 
AR
G0
 
AR
G0
 
AR
GM
-M
OD
 
AR
G1
 
AR
GM
-T
MP
 
AR
G1
 
pred 
pred 
ARG0 
ARG0 
ARGM-MOD 
ARGM-TMP 
ARG1 
ARG1 
Figure 4: An example of dense alignments in IMEANT, for the Chinese input sentence ???????
?????????????? AWB?????????? (The semantic parse errors are due to limitations
of automatic SRL.)
IMEANT could perform even better than it already
does here.We plan to investigate simple hyperpa-
rameter optimizations in the near future.
30
6 Acknowledgments
This material is based upon work supported
in part by the Defense Advanced Research
Projects Agency (DARPA) under BOLT contract
nos. HR0011-12-C-0014 and HR0011-12-C-0016,
and GALE contract nos. HR0011-06-C-0022 and
HR0011-06-C-0023; by the European Union un-
der the FP7 grant agreement no. 287658; and by
the Hong Kong Research Grants Council (RGC)
research grants GRF620811, GRF621008, and
GRF612806. Any opinions, findings and conclu-
sions or recommendations expressed in this mate-
rial are those of the authors and do not necessar-
ily reflect the views of DARPA, the EU, or RGC.
Thanks to Karteek Addanki for supporting work,
and to Pascale Fung, Yongsheng Yang and Zhao-
jun Wu for sharing the maximum entropy Chinese
segmenter and C-ASSERT, the Chinese semantic
parser.
References
Karteek Addanki, Chi-kiu Lo, Markus Saers, and
Dekai Wu. LTG vs. ITG coverage of cross-
lingual verb frame alternations. In 16th An-
nual Conference of the European Association
for Machine Translation (EAMT-2012), Trento,
Italy, May 2012.
Alfred V. Aho and Jeffrey D. Ullman. The The-
ory of Parsing, Translation, and Compiling.
Prentice-Halll, Englewood Cliffs, New Jersey,
1972.
Satanjeev Banerjee and Alon Lavie. METEOR:
An automatic metric forMT evaluation with im-
proved correlation with human judgments. In
Workshop on Intrinsic and Extrinsic Evaluation
Measures for Machine Translation and/or Sum-
marization, Ann Arbor, Michigan, June 2005.
Chris Callison-Burch, Miles Osborne, and Philipp
Koehn. Re-evaluating the role of BLEU in ma-
chine translation research. In 11th Conference
of the European Chapter of the Association for
Computational Linguistics (EACL-2006), 2006.
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz, and Josh Schroeder.
(meta-) evaluation of machine translation. In
Second Workshop on Statistical Machine Trans-
lation (WMT-07), 2007.
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz, and Josh Schroeder.
Further meta-evaluation of machine transla-
tion. In Third Workshop on Statistical Machine
Translation (WMT-08), 2008.
Julio Castillo and Paula Estrella. Semantic tex-
tual similarity for MT evaluation. In 7th Work-
shop on Statistical Machine Translation (WMT
2012), 2012.
Michael Denkowski and Alon Lavie. METEOR
universal: Language specific translation eval-
uation for any target language. In 9th Work-
shop on Statistical Machine Translation (WMT
2014), 2014.
George Doddington. Automatic evaluation of
machine translation quality using n-gram co-
occurrence statistics. In The second interna-
tional conference on Human Language Technol-
ogy Research (HLT ?02), San Diego, California,
2002.
Jes?s Gim?nez and Llu?s M?rquez. Linguistic fea-
tures for automatic evaluation of heterogenous
MT systems. In Second Workshop on Statisti-
cal Machine Translation (WMT-07), pages 256?
264, Prague, Czech Republic, June 2007.
Jes?s Gim?nez and Llu?s M?rquez. A smorgas-
bord of features for automaticMT evaluation. In
Third Workshop on Statistical Machine Transla-
tion (WMT-08), Columbus, Ohio, June 2008.
Philipp Koehn and Christof Monz. Manual and
automatic evaluation of machine translation be-
tween european languages. InWorkshop on Sta-
tistical Machine Translation (WMT-06), 2006.
Gregor Leusch and Hermann Ney. Bleusp, invwer,
cder: Three improved mt evaluation measures.
In NIST Metrics for Machine Translation Chal-
lenge (MetricsMATR), at Eighth Conference of
the Association for Machine Translation in the
Americas (AMTA 2008), Waikiki, Hawaii, Oct
2008.
Gregor Leusch, Nicola Ueffing, and Hermann
Ney. A novel string-to-string distance measure
with applications to machine translation evalu-
ation. In Machine Translation Summit IX (MT
Summit IX), New Orleans, Sep 2003.
Gregor Leusch, Nicola Ueffing, and Hermann
Ney. CDer: Efficient MT evaluation using
block movements. In 11th Conference of the
European Chapter of the Association for Com-
putational Linguistics (EACL-2006), 2006.
31
Ding Liu and Daniel Gildea. Syntactic features for
evaluation of machine translation. InWorkshop
on Intrinsic and Extrinsic Evaluation Measures
for Machine Translation and/or Summarization,
Ann Arbor, Michigan, June 2005.
Chi-kiu Lo and Dekai Wu. MEANT: An inexpen-
sive, high-accuracy, semi-automatic metric for
evaluating translation utility based on seman-
tic roles. In 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human
Language Technologies (ACL HLT 2011), 2011.
Chi-kiu Lo and Dekai Wu. SMT vs. AI redux:
How semantic frames evaluate MT more ac-
curately. In Twenty-second International Joint
Conference on Artificial Intelligence (IJCAI-
11), 2011.
Chi-kiu Lo andDekaiWu. Unsupervised vs. super-
vised weight estimation for semantic MT evalu-
ation metrics. In Sixth Workshop on Syntax, Se-
mantics and Structure in Statistical Translation
(SSST-6), 2012.
Chi-kiu Lo and Dekai Wu. Can informal genres
be better translated by tuning on automatic se-
mantic metrics? In 14th Machine Translation
Summit (MT Summit XIV), 2013.
Chi-kiu Lo and Dekai Wu. MEANT at WMT
2013: A tunable, accurate yet inexpensive se-
mantic frame based mt evaluation metric. In
8th Workshop on Statistical Machine Transla-
tion (WMT 2013), 2013.
Chi-kiu Lo, Anand Karthik Tumuluru, and Dekai
Wu. Fully automatic semantic MT evaluation.
In 7th Workshop on Statistical Machine Trans-
lation (WMT 2012), 2012.
Chi-kiu Lo, Karteek Addanki, Markus Saers, and
Dekai Wu. Improving machine translation by
training against an automatic semantic frame
based evaluation metric. In 51st Annual Meet-
ing of the Association for Computational Lin-
guistics (ACL 2013), 2013.
Chi-kiu Lo, Meriem Beloucif, and Dekai Wu. Im-
proving machine translation into Chinese by
tuning against Chinese MEANT. In Interna-
tional Workshop on Spoken Language Transla-
tion (IWSLT 2013), 2013.
Chi-kiu Lo, Meriem Beloucif, Markus Saers, and
Dekai Wu. XMEANT: Better semantic MT
evaluation without reference translations. In
52nd Annual Meeting of the Association for
Computational Linguistics (ACL 2014), 2014.
Matou?Mach??ek andOnd?ej Bojar. Results of the
WMT13 metrics shared task. In Eighth Work-
shop on Statistical Machine Translation (WMT
2013), Sofia, Bulgaria, August 2013.
I. Dan Melamed. Automatic construction of
clean broad-coverage translation lexicons. In
2nd Conference of the Association for Ma-
chine Translation in the Americas (AMTA-
1996), 1996.
Rada Mihalcea, Courtney Corley, and Carlo Strap-
parava. Corpus-based and knowledge-based
measures of text semantic similarity. In The
Twenty-first National Conference on Artificial
Intelligence (AAAI-06), volume 21, 2006.
Sonja Nie?en, Franz Josef Och, Gregor Leusch,
and Hermann Ney. A evaluation tool for ma-
chine translation: Fast evaluation for MT re-
search. In The Second International Conference
on Language Resources and Evaluation (LREC
2000), 2000.
Karolina Owczarzak, Josef van Genabith, and
Andy Way. Dependency-based automatic eval-
uation for machine translation. In Syntax
and Structure in Statistical Translation (SSST),
2007.
Karolina Owczarzak, Josef van Genabith, and
Andy Way. Evaluating machine translation
with LFG dependencies. Machine Translation,
21:95?119, 2007.
Kishore Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. BLEU: a method for automatic
evaluation of machine translation. In 40th An-
nual Meeting of the Association for Compu-
tational Linguistics (ACL-02), pages 311?318,
Philadelphia, Pennsylvania, July 2002.
Sameer Pradhan, Wayne Ward, Kadri Hacioglu,
James H. Martin, and Dan Jurafsky. Shallow se-
mantic parsing using support vector machines.
In Human Language Technology Conference
of the North American Chapter of the Asso-
ciation for Computational Linguistics (HLT-
NAACL 2004), 2004.
Miguel Rios, Wilker Aziz, and Lucia Specia.
TINE: A metric to assess MT adequacy. In
Sixth Workshop on Statistical Machine Transla-
tion (WMT 2011), 2011.
32
Markus Saers and Dekai Wu. Improving phrase-
based translation via word alignments from
stochastic inversion transduction grammars. In
Third Workshop on Syntax and Structure in
Statistical Translation (SSST-3), pages 28?36,
Boulder, Colorado, June 2009.
Markus Saers, Joakim Nivre, and Dekai Wu.
Learning stochastic bracketing inversion trans-
duction grammars with a cubic time biparsing
algorithm. In 11th International Conference on
Parsing Technologies (IWPT?09), pages 29?32,
Paris, France, October 2009.
Matthew Snover, Bonnie Dorr, Richard Schwartz,
Linnea Micciulla, and John Makhoul. A study
of translation edit rate with targeted human an-
notation. In 7th Biennial Conference Asso-
ciation for Machine Translation in the Ameri-
cas (AMTA 2006), pages 223?231, Cambridge,
Massachusetts, August 2006.
Anand Karthik Tumuluru, Chi-kiu Lo, and Dekai
Wu. Accuracy and robustness in measuring the
lexical similarity of semantic role fillers for au-
tomatic semantic MT evaluation. In 26th Pa-
cific Asia Conference on Language, Informa-
tion, and Computation (PACLIC 26), 2012.
Mengqiu Wang and Christopher D. Manning.
SPEDE: Probabilistic edit distance metrics for
MT evaluation. In 7th Workshop on Statistical
Machine Translation (WMT 2012), 2012.
DekaiWu. An algorithm for simultaneously brack-
eting parallel texts by aligning words. In 33rd
Annual Meeting of the Association for Compu-
tational Linguistics (ACL 95), pages 244?251,
Cambridge, Massachusetts, June 1995.
Dekai Wu. Trainable coarse bilingual grammars
for parallel text bracketing. In Third Annual
Workshop on Very Large Corpora (WVLC-3),
pages 69?81, Cambridge, Massachusetts, June
1995.
Dekai Wu. Stochastic inversion transduction
grammars and bilingual parsing of parallel cor-
pora. Computational Linguistics, 23(3):377?
403, 1997.
Richard Zens and Hermann Ney. A compara-
tive study on reordering constraints in statisti-
cal machine translation. In 41st Annual Meeting
of the Association for Computational Linguis-
tics (ACL-2003), pages 144?151, Stroudsburg,
Pennsylvania, 2003.
33
