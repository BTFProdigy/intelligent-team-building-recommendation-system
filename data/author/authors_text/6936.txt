An Extensible Framework for Efficient Document Management Using
RDF and OWL
Erica Meena
Laboratory LORIA
Vandoeuvre-les-Nancy
France
meena@loria.fr
Ashwani Kumar
M.I.T
Cambridge, MA
USA
ashwani@mit.edu
Laurent Romary
Laboratory LORIA
Vandoeuvre-les-Nancy
France
romary@loria.fr
Abstract
In this paper, we describe an integrated approach to-
wards dealing with various semantic and structural is-
sues associated with document management. We
provide motivations for using XML, RDF and OWL in
building a seamless architecture to serve not only as a
document exchange service but also to enable higher
level services such as annotations, metadata access and
querying. The key idea is to manifest differential treat-
ments for the actual document structure, semantic con-
tent of the document and ontological document
organization. The deployment of this architecture in the
PROTEUS project
1
 provides an industrial setting for
evaluation and further specification.
1. Introduction
Digital documents are ubiquitously used to encode, pre-
serve as well as exchange useful information in order to
accomplish information sharing across the community.
As the growth in volumes of digital data is exponential,
it is necessary to adopt a principled way of managing
these documents. Besides, due to the distributed nature
of information, it is also imperative to take into account
the geographical and enterprise-level barriers for uni-
form data access and retrieval.
 The ITEA (Information Technology for European Ad-
vancement) project, Proteus
2
 has similar objectives.
Proteus is a collaborative initiative of French, German
and Belgium companies, universities and research in-
stitutes aimed at developing a European generic soft-
ware platform usable for implementation of web-based
e-maintenance centers. It implements a generic archi-
tecture for integrated document management using ena-
                                                           
1
 This material is based upon work supported by the ITEA (Information Tech-
nology for European Advancement) programme under Grant 01011 (2002).
2
 http://www.proteus-iteaproject.com/
bling technologies such as XML, RDF and OWL. Most
of the existing document management systems ([1], [2])
limit themselves in the scope of application or document
formats or simply neglect any structure-based analysis.
However, considering our requirements, it is obvious
that only a multi-layered functional architecture can
cover various issues related to distributed document
management such as localized vs global structural con-
straints, conceptual definition of documents, reasoning-
based discovery etc.
Indeed, evolving technologies such as XML (eXtensible
Markup Language), RDF (Resource Description
Framework) and OWL (Web Ontology Language) pro-
vide us with rich set of application frameworks that if
applied intelligently, can help a great deal in solving
these problems. XML ([3]) is primarily designed for
low-level structural descriptions. It provides a tree of
structured nodes, which can be efficiently used to de-
scribe documents and check their models using DTDs
(Document Type Definitions) or XML Schemas. Be-
sides, XML enables easy human readability as well as
efficient machine interpretability. However, there are
issues if we only deal with the structural aspect. If one
wants to pick some semantic information from a docu-
ment, there is no straightforward way other than to con-
strain it by an schema or make an application hand-
programmed to recognize certain document-specific
semantics. Furthermore, if the schema changes over
time, it could typically introduce new intermediate ele-
ments. This might have the consequences of invalidat-
ing certain queries and creating incoherencies in the
semantic data-model of the document.
RDF (Resource Description Framework) and OWL
(Web Ontology Language) build upon the XML syntax
to describe the actual semantics of a document and pro-
vide useful reasoning and inference mechanisms. RDF
([4]) specifies graphs of nodes, which are connected by
directed arcs representing relational predicates such as
URIs (Uniform Resource Identifiers) and encode the
conceptual model of the real world. Unlike XML, an
RDF schema is a simple vocabulary language. The
parse of the semantic graph results in a set of triples,
which mimic predicate-argument conceptual structures.
OWL can be used on top of these semantic structures to
do logical reasoning and discover relations that are not
explicit and obvious.
In the following sections we discuss how we use these
technologies to enable a generic document management
system. Firstly, in Section 2 we describe the document
management and the Proteus architecture followed by
discussion on Annotations in Section 3. Section 4 pro-
vides brief account of the model theoretic access
mechanisms enabled by OWL followed by description
of data categories in Section 5.
2. Document Management Architecture
Without differentiating at the level of content, layout
and formats, we treat documents as information re-
sources. These information resources can potentially be
distributed across various document repositories called
e-Doc servers. Figure 2.1 demonstrates a simplified
distributed document management system. The archi-
tecture shows how three different document repositories
could co-exist functionally along with the Annotea en-
abled annotation framework ([5]). These servers imple-
ment procedural mechanisms for query access and
retrieval of documents. Besides, these documents can be
annotated and the annotations reside on an independent
server known as the annotation server, which also
serves as a document server. Principally, annotations
can be viewed as information resources, which are de-
scribed in RDF.
Server 1
Server 2 Server 3
Annotea RDF
TEI/XML Doc
TEI/XML Doc
TEI/XML Doc
Header
Body
Header
Body
Header
Body
Figure 2.1: Simplified view of the distributed document
server architecture
The e-Doc server consists of several functional layers
that inter-communicate and holistically, serve the cu-
mulative purpose of document management. These lay-
ers though distinct at the level of data flow and
individual processing of information, afford function-
alities that are exploited by the e-Doc server.
Figure 2.2 shows various such layers of the e-Doc
server. On the foundation level, it is assumed that every
document on the e-Doc server adheres to a single syntax
i.e. XML, which represents the top most layer in the
architecture. The second layer depicts the access points
that are broadly categorized along various dimensions
such as metadata, conceptual/ontology system and ter-
minology. A detailed description of the access points
will be carried out in the Section 4. The e-Doc server is
assumed to be flexible enough to handle all possible
ontology formats/standards whether it is a native XML
document or a text or a picture/video data coming from
some streaming applications. This forms the third im-
portant layer of the e-Doc server. The bottom layer rep-
resents Annotations [6], which adheres to the RDF [4]
syntax. This layer forms an integral part of the e-Doc
server as it enables annotation capability and RDF-
describable semantics to the actively retrieved document
or existing documents in the server [7]. Besides, RDF
also provides the opportunity to utilize annotations as
access points for the documents.
XML Syntax
Meta-data
(1 doc)
Native XML
document
Conceptual
system
(n ontologies)
Misc. textual
format
Picture/
Videos
Application
Data (XML)
Terminology
Document
server
Access
points
Document
s
RDF
View
Comment
Query
Retrieve
User
Annotations
Figure 2.2: General Organization of the e-Doc Server
As can be seen from the Figure 2.2, a user interacts with
the server through a client interface by launching his
queries. The architecture provides the user ample flexi-
bility in utilizing different levels of descriptions for re-
trieving documents by providing variety of access
points. In the following sections, we describe each of
these access layers in more detail.
3. Annotations: Specified as RDF Model
Annotations form the most abstract layer within the e-
Doc architecture. They can be broadly defined as com-
ments, notes, explanations, or other types of external
remarks that can be attached to either a document or a
sub portion of a document. As annotations are consid-
ered external, it is possible to annotate a document as a
whole or in part without actually editing its content or
structure. Conceptually, annotations can be considered
as metadata, as they give additional information about
an existing piece of data. Annotations can have many
distinguishing properties, which can be broadly classi-
fied as:-
? Physical location:- An annotation can be stored
locally or on one or more annotation servers;
? Scope:- An annotation can be associated with a
document as a whole or to a sub-portion of a
document.
? Annotation type:- Annotations can have vari-
ous functional types such as, ?Comment?,
?Remark?, ?Query? e.t.c.?
Due to this abstract nature and multiplicity of functional
types, a formal treatment of annotations is often un-
wieldy. Therefore, it is desired to have a semantically
driven structural representation for annotations, which
we describe below.
Annotation Semantics
Annotations are stored in one or multiple annotation
servers. These servers endorse exchange protocols as
specified by Annotea [5]. Essentially, the Annotation
Server can be regarded as a general purpose RDF store,
with additional mechanisms for optimized queries and
access. This RDF store is built on top of a general SQL
store. Annotations are stored in a generic RDF database
accessible through an Apache HTTP server (see Figure
3.1). All communication between a client and an anno-
tation server uses the standard HTTP methods such as
POST or GET.
RDF
database
SQL store
Query
interface
         Client Annotation Serverhttp
POST
http GET
Figure 3.1: Access to the Annotation server
Annotations have metadata associated with them, which
is modeled according to an RDF schema and encode
information such as date of creation of the annotation,
name of the author, the annotation type (e.g. comment,
query, correction) the URI [8] of the annotated docu-
ment, and an Xpointer [9] that specifies what part of the
document was annotated, and the URI to the body of the
annotation which is assumed to be an XHTML [10]
document (Figure 3.2).
<rdf:RDF
xmlns:NS0='http://www.w3.org/2000/10
/annotation-ns#'
?>
<dc:creator>Ashwani</dc:creator>
<rdf:type
rdf:resource='http://www.w3.org/2000
/10/annotation-ns#Annotation'/>
<NS1:origin rdf:nodeID='A0'/>
<NS0:created>2004-05-
24T01:11Z</NS0:created>
<NS0:annotates
rdf:resource='http://docB4.teiSpec.o
rg'/>    <rdf:type
rdf:resource='http://www.w3.org/2000
/10/annotationType#Comment'/>
<NS0:body rdf:resource='Please re-
view this document.'/>
<dc:title>review</dc:title>
<dc:date>2004-05-24T01:11Z</dc:date>
</rdf:Description>  <rdf:Description
rdf:nodeID='A1'>
?.
</rdf:Description>
</rdf:RDF>
Figure 3.2: An abridged Annotation in RDF
Xpointers are used to point to the Annotated portions
within the document, while Xlinks [11] are used to
setup a link between the Document and it's annotation.
Annotation Operations
The user makes a selection of the text to be annotated
and provides the annotation along with other details
such as author name, date of creation, type of annota-
tion, URI of the annotated document etc. The annota-
tions are published using standard HTTP POST method.
To do this the client generates an RDF description of the
annotation that includes the metadata and the body and
sends it to the server. The annotation server receives the
data and assigns a URI to the annotation i.e. the body,
while metadata is identified by the URI of the Docu-
ment.
For annotation retrieval, the client queries the annota-
tion server via the HTTP GET method, requesting the
annotation metadata by means of the document's URI.
The annotation server replies with an RDF-specified list
of the annotation metadata. For each list of annotations
that the client receives, it parses the metadata of each
annotation, resolves the Xpointer of the annotation, and
if successful, highlights the annotated text. If the user
clicks on the highlighted text, the browser uses an
HTTP GET method to fetch the body of the annotation
from the URI specified in the metadata.
The following are the broad categories of the annotation
functions implemented by the annotation server:
? Annotate a document as a whole.
? Annotate a portion of a document.
? Query to access all the annotations for a par-
ticular document.
? Query to access type specific or any of the
metadata property specific annotations, which
serve as query parameters for all the annotated
documents.
4. Model Based Access: Using OWL
As described in the previous section, the RDF layer
provides an enhanced mechanism for querying and ac-
cessing a document. However, to enable full-fledged
management of documents, it is imperative to incorpo-
rate some reasoning-based abstract semantics such as
OWL (Web Ontology Language) over a cluster of
documents. OWL provides formal mechanisms for de-
scribing ontology of documents. By doing so, the ar-
chitecture can provide flexible access points as well as
logical inference mechanisms, which are necessary
while performing metadata queries.
Access points play an important role by providing flexi-
bility and intuitiveness in access mechanisms to the
user. Figure 4.1 depicts a very basic characterization of
the access points. As it is illustrated in the figure, a spe-
cific access point is needed to direct a query to attain
certain desired result set. Within the Proteus framework,
the e-doc architecture provides a model driven specifi-
cation of access points such as metadata-based, onto-
logical, or terminological model. The model driven
approach has strong significance in the sense that every
access point is associated by certain abstract informa-
tion structure so that it provides transparency to the que-
ries, which remain independent from actual
implementation and data formats (e.g. XML DTD).
Even though these models are independent, they are
flexible enough to interact among themselves. For ex-
ample, results of queries on one model can act as a ref-
erence for another model. The references may be
transformed into document excerpts by requests made
synchronously at the query stage or asynchronously
when the user wants to visualize the information.
  
Data Source
Access point
Result
set
Data server
1
Data server
i
Data server
n
Search primitives
?
? Ontological
references
or
data excerpts
Query
Figure 4.1 Characterization of Access Points
Terminological Access:
Terminology can be defined as the description of spe-
cialized vocabulary of an application domain. As it
contains a nomenclature of technical terms, it is capable
of providing a conceptual view of the domain. Termi-
nology can be either monolingual or multilingual by
nature. Monolinguality specifies a one to one relation
between a term and a concept or a term to its equiva-
lences or a term to the related documents, while mul-
tilinguality specifies relation between term to certain
target terms or term to certain target documents.
 The Following is a simplified Proteus terminology ex-
ample:
<struct type=?TE?>[Terminological
Entry]
<feat type=?definition?
xml:lang=?fr?> Dispositif
permettant d'imprimer un
d?placement lin?aire ou angulaire ?
un ?l?ment mobile. </feat>
<struct type=?LS?>[Language Section]
<feat type=?language?>fr</feat>
<struct type=?TS?>[Term Section]
<feat type=?term?>v?rin</feat>
<struct type=?TCS?>[Term Component
Section]
<feat type=?partOfSpeech?>
noun
</feat>
?..
</struct>
Description of terminological model
A general terminological model contains a Termino-
logical entry section, a Language section and a Term
section.
 
Term Section
Language Section
Terminological Entry
/Entry 
identifier/ 
/Subject field/ 
/Definition/ 
/Explanation/ 
/Example/ 
/Note/ 
/Language/ 
/Note/ 
/Term/ 
/Term 
status/ 
Figure 4.2 Simplified terminological model
Figure 4.2 describes a simplified terminology model -
the terminological section contains entries such as iden-
tifier, subject field, definition, and explanations etc.,
where as the other sections such as the language and the
term sections contain details regarding the language
used and the term status respectively. This can also be
seen within the sample Proteus terminology described
above.
Terminological access is significant in cases where the
user is aware of the specific term and needs to make a
search within the related domain to access certain
documents of his interest. For example, an operator of a
firm might be willing to retrieve all the maintenance
documents related to the term ?Pump?. Thanks to the
terminological access point, the operator needs nothing
but just the term to launch his query and retrieve the
desired document. The above-mentioned scenario is
depicted in Figure 4.3
Terminological system
E-doc
                     Operator
Pump
List of available maintenance docs
Figure 4.3: A Sample Terminological Access
Terminological access plays a dual role. On one hand it
acts as a data source providing support for finding mono
or multilingual equivalences or linguistic descriptions.
On the other hand, it provides access for on-line docu-
ments. When seen as a data source, it can also provide
indexing support for manual indexing and can perform
semi-automated indexing:
? Graphic files (drawings, pictures, video,
scanned texts etc): manual indexing
? Text files: semi-automatic indexing; suggestion
of descriptors to be confirmed by a human ex-
pert
? Data, e.g. from monitoring: automatic indexing
with metadata.
Terminological model serves as a gateway to the Ontol-
ogy-based Conceptual model of the domain (Figure
4.4). Use of a technical term as a query parameter re-
lates to set of relevant concepts, which can further be
used to retrieve the desired set of documents.
Conceptual system
Terminological system
Turbine
E-doc
User
Suggestions
(concepts!)
Links
(hasDoc)
Figure 4.4 interaction of terminological model with
other models
Meta-Data Access
Metadata can be loosely defined as ?data about data?..
Specifically, metadata encodes certain attributive infor-
mation about the data, in our case documents, which can
be used to access data. Within this platform the meta-
model can be seen as a meta-tree of nodes in which
every node refers to certain precise set of information
descriptors. For example, Dublin Core descriptors such
as title, author, date, publisher, etc can potentially be
represented as nodes in the description trees.
Meta-model Description
This Meta-model is discussed keeping the specific
Dublin Core [12] model in mind. Meta model consists
of three basic components, a Resource, an Element, and
its value.
? Resource ? the object being described.
? Element ? a characteristic or property of the
Resource.
? Value ? the literal value corresponding to the
Element.
 Figure 4. 5 shows a simplified view of the Dublin core
reference model, within which the Element Qualifiers
are nothing but additional attributes that further specify
the relationship of the element to the resource. On the
other hand, the value qualifiers can be described as ad-
ditional attributes that further specify the relationship of
the value to the element.
Figure 4.5: Simplified view of the Dublin Core refer-
ence model.
For Example:
Element  = Creator
Component  = Firstname Value   =  Ashwani
Component  = Lastname Value   =  Kumar
Component  = Email Value   =  ashwani@kumar.com
Element = Contributor
Value = fn:Erica Meena; org:DSTC
Type  = Illustrator
Encoding = vCard
Resource = http://www.loria.fr/projets/proteus/RDU/NOTE-
PIR 20040304.html
Access of documents by means of metadata is a very
important as well as a practical usage, as the user can
directly retrieve a well defined piece of information,
under the condition that he knows a small number of
?facts? about the information: e.g. the authors name, the
date, the reference number or the date of a previous
maintenance. This corresponds to a typical situation
within the Proteus framework (see Figure 4.6). Meta-
data access, in other way, can be seen as an advanced
index functionality, which can update itself and grow
automatically in the same form as the amount of stored
information grows.
For example:
While sorting documents by date or type, the date, time,
source or author information can always be automati-
cally collected. However, in case of a new maintenance
document, advanced metadata can be collected by ask-
ing a human to enter it into the system.
E-doc
                        Operator
Dublin Core
Title: name of the client
Creator: name of the method agent
 Subject: Equipment ID
Description: type of equipment
Publisher: CEF CIGMA division
Contributor: division out of CIGMA
Date: date of the draw-up
Type: procedure, FMECA, ...
Format: .doc, .ppt, . xls (not useful)
Identifier: name of the site (location)
Source: former version
Language: French, English, German
Relation: related FMECA, procedure,
video, pictures, ...
Coverage: equipment location
Rights: public, confidential
1
2
3
4
5
6
7
8
9
1
0
1
1
2
1
3
1
4
1
5
List of available maintenance docs
Figure 4.6:  Document access via metadata
Metadata model can be seen as an enhanced search
mechanism. A sequence of access points i.e. terminol-
ogy followed by metadata, when launched can help in
refining the search along an attribute dimension.
Ontology Access
Ontology is a hierarchy of concepts or in other way a
platform for describing the concepts used within a spe-
cific domain, maintenance in our case. Its independence
with regard to specific model or format makes it inter-
operable. For example, one can have an ontology repre-
sented in a UML [13] class diagram whereas the same
ontology can be represented in an XML schema. As
already discussed, Ontology is complementary to termi-
nology in terms of attribution of concepts to terms.
Conceptually, it serves as an abstract structure, which
can be populated by the interested parties and thus, can
serve as a very important access point. An abridged
example of an abstract Proteus OWL [14] ontology ver-
sion can be seen in the figure 4.7
<?xml version="1.0"?>
<rdf:RDF
xmlns:owl="http://www.w3.org/2002/07/ow
l#"
xmlns:gmoloc="http://www.proteus.com/GM
O/locations#">
   <owl:Ontology rdf:about="">
   <rdf:comment>The Engineering compo-
nent of the PIR</rdf:comment>
</owl:Ontology>
  <owl:Class rdf:ID="Contract">
   <rdfs:subClassOf>
      <owl:Class rdf:ID="Document"/>
   </rdfs:subClassOf>
</owl:Class>
<owl:Class rdf:ID="Manager">
   <rdfs:subClassOf>
      <owl:Class rdf:ID="Actor"/>
   </rdfs:subClassOf>
</owl:Class>
  ??
</rdf:RDF>
                   
Figure 4.7: An example of Proteus ontology
Ontology model description
As per the requirements of the Proteus project, an ontol-
ogy model comprises of a three-tiered structure. The
three layers consist of General concepts (General
Maintenance Ontology), Application Profiles, and the
industrial contexts respectively. These layers are built
up keeping in mind the interoperability with other ex-
ternal applications. As can be seen from the Figure 4.8
below, the general concept layer has the highest
interoperability as it contains basic level concepts such
as Actors, Documents, Location, Equipments etc. The
second layer (Application Profiles) consists of concepts,
which are specific to a certain application, for instance
pertinent to a train manufacturing company, or an avia-
tion company. All the layers are bound to inherit con-
cepts, but not necessarily all from the first layer (general
concept), which in turn forms the parent layer of all
other layers. The third layer (Industrial contexts) con-
tains concepts very specific to an industry for instance,
car manufacturing companies such as Ford, GM etc.
Instances can be derived only from the last layer i.e. the
Industrial contexts layer.
The model is open for external sources i.e. ontology
from external sources can be merged within each layer,
for example, SUMO [15], which is a higher-level ontol-
ogy. It contains very general concepts, which can be
used directly within our ontology.
General concepts
(GMO)
Application profiles
Industrial contexts
High interoperability
Low interoperability
External
sources
Actors, Location, Documents,
Equipment
Car manufacturing company
FORD
               Example
          Instances
Ford Tech
Report
Figure 4.8: Proteus Ontology model
OWL-DL is used for specifying the ontological model
as it provides the following advantages:
? Basic support for describing classification hier-
archies and simple constraint features. e.g. mi-
gration path for thesauri and other taxonomies;
? Rich expressiveness;
? Computational completeness and decidability;
? Allows imports of OWL Lite simple descrip-
tion;
? Allows consistency checks across description
levels;
? Existence of optimized inference platforms.
E.g. Racer [16].
In a way, ontology access is a complementary approach
to the terminology access, as terminology structure de-
scribes the global concept behind a thematic domain,
but does not deliver a functional description of the do-
main. The ontology access exactly provides this func-
tional description (as is usually needed in the
maintenance domain). The concept remains global when
referring to a generic class of entities and gets specific
when describing a particular entity type. Apart from the
normal functionality of this access point, it can be very
important when combined with retrieval by natural lan-
guage and by visual elements (hierarchy structured sets
of pictures). In a way we can see ontology as an empty
structure with user-defined class relationships, which
can be filled with visual elements (photos, drawing,
scheme) and then the referring terms.
For example Figure 4.9 depicts visual search of docu-
ments via ontology. In order to avoid complexity, only
recommended terms are used to name the objects repre-
sented by the visual elements. Other terms can be left
apart pointing to plain concepts (without visual con-
cepts). The index of the metadata tool could be virtually
integrated into the index administrated by the terminol-
ogy tool. This enables a two-step-search, beginning with
a word and then finding the actually searched item not
by selecting a more specific term from the terminology
tool, but by looking for a picture of the searched item in
the functional concept. This index could also be virtu-
ally integrated into the index of the functional concept.
Thus the user could situate the search results provided
by the metadata tool within the functional structure of
the maintained equipment (instead of getting designa-
tion, ID-Number, description and meta data only).
 
 
C
e 
Conceptual system 
   E-Doc 
                      Operator 
dedicated to one single site 
 
Maintenance doc  
     (including 
needed tools), 
Parameters 
setting,  
User manual of 
this  
List of available 
maintenance docs, 
parameters setting, 
user manual 
portal 
Figure 4.9: Visual search of documents via ontology
5. Data Category Specification
The various models (terminology, annotations, etc.) and
functionalities (access primitives to an e-doc server)
have to be defined in such a way that a similar piece of
information (e.g. author, subject field, term, etc.) means
the same thing from one place to another. Such a se-
mantic definition of data categories (in the terminology
of ISO committee TC 37) acts in complementary to an
ontology such as the one we define in the Proteus sys-
tem since it is intended to be a general purpose layer of
descriptors that may be used in other environments than
that of a specific project. Therefore, we adopted a simi-
lar methodology as that of the efforts within the ISO TC
37 committee to deploy a data category registry of all
descriptors used in the project as reference semantic
units described in accordance to ISO standard 11179
(metadata registries). Such a registry plays a double
role:
? It provides unique entry point (of formal public
identifier) for any model that refers to it;
? It gives a precise description of the data cate-
gory by means of a definition and associated
documentation (examples, application notes,
etc.).
6. Conclusions
We have provided a brief account of how document
structure and inherent semantics can be captured and
processed efficiently by the emerging technologies such
as XML, RDF and OWL. By doing so, we have brought
innovations in correlating different levels of document
management with respect to various services afforded
by these technologies. The differential treatment of
structure, content and organization provides ample
flexibility and extensibility, which are the primary re-
quirements for such a system.
References
 [1]Lagoze C, Dienst - An Architecture for Distributed
Document Libraries, Communications of the ACM,
Vol. 38, No 4, April 1995. 12
[2]Satoshi Wakayama, Yasuki Ito, Toshihiko Fukuda
and Kanji Kato, Distributed Object-Based Applica-
tions for Document Management, Hitachi Review
Vol. 47 (1998), No.6
[3]Tim Bray, Jean Paoli, C. M. Sperberg-McQueen,
Extensible Markup Language (XML) 1.0., eds.W3C
Recommendation 10-February-1998.
[4]Swick Lassila, Resource Description Framework
(RDF) Model and Syntax Specification., World Wide
Web Consortium Recommendation, 1999.
http://www.w3.org/TR/REC-rdf-syntax/.
[5]Jos? Kahan, Marja-Riitta Koivunen, Eric Prud'Hom-
meaux, and Ralph R. Swick, Annotea: An Open RDF
Infrastructure for shared Web Annotations, in Proc.
of the WWW10 International Conference, Hong
Kong, May 2001.
[6]The W3C Collaborative Web Annotation Project ...
or how to have fun while building an RDF infra-
structure. http://www.w3.org/2000/Talks/www9-
annotations/Overview.html.
[7]N. F. Noy, M. Sintek, S. Decker, M. Crubezy, R. W.
Fergerson, & M. A. Musen. Creating Semantic Web
Contents with Protege-2000. IEEE Intelligent Sys-
tems 16(2):60-71, 2001.
[8]T. Berners-Lee, R. Fielding, and L. Masinter, Uni-
form Resource Identifiers (URI): Generic Syntax,
IETF Draft Standard August 1998 (RFC 2396).
[9]XML Pointer Language. http://www.w3.orgwtr/xptr/
[10]The Extensible HyperText Markup Language.
http://www.w3.org/TR/xhtml1/
[11]XML Linking Language.
http://www.w3.org/TR/xlink/
[12]Dublin Core Metadata Initiative. OCLC, Dublin
Ohio. http://purl.org/dc/ .
[13]Unified Modeling Language Home Page.
http://uml.org/ .
[14]Deborah L. McGuinness and Frank van Harmelen,
OWL Web Ontology Language Overview, W3C
Proposed Recommendation, 15 December 2003.
 http://www.w3.org/TR/owl-features/.
[15]Niles, I., and Pease, A.  2001.  Towards a Standard
Upper Ontology.  In Proceedings of the 2nd Interna-
tional Conference on Formal Ontology in Informa-
tion Systems (FOIS-2001), Chris Welty and Barry
Smith, eds, Ogunquit, Maine, October 17-19, 2001.
[16]V.Haarslev and R. Moller. Description of the
RACER system and its applications. In DL2001
Workshop on Description Logics, Stanford, CA,
2001.
 
	
	Standards going concrete: from LMF to Morphalou
Laurent Romary
Laboratoire Loria-INRIA
B.P. 239
54506 Vand?uvre Les Nancy
France
Laurent.Romary@loria.fr
Susanne Salmon-Alt
ATILF-CNRS
B.P. 30687
54063 Nancy
France
Susanne.Salmon-Alt@atilf.fr
Gil Francopoulo
INRIA-Syntax
B.P. 239
54506 Vandoeuvre Les Nancy,
France
Gil.francopoulo@wanadoo.fr
Abstract
Lexical resources are key components for
applications related to human language technology.
Various models of lexical resources have been
designed and implemented during the last twenty
years and the scientific community has now gained
enough experience to design a common standard at
an international level. This paper thus describes the
ongoing activity within ISO/TC 37/SC 4 on LMF
(Lexical Markup Framework) and shows how it
can be concretely implemented for the design of an
on-line morphological resource for French in the
Morphalou project.
1 Introduction
Lexical resources play a crucial role in most
applications related to human language technology.
They may be used by both human readers and
automatic processors for a wide range of activities
that require an even wider variety of lexical
structures. Some applications may demand broad
linguistic coverage, where the word is the entry
point and all the possible senses are attached to
them, whereas other applications could require a
concept-based organization of the lexical data,
from which the relevant words (or terms) may be
derived. Some applications barely need more then
a simple list of words, whereas other may require a
precise morpho-syntactic, syntactic and semantic
description of the various lexical entries.
Furthermore, the huge cost of creating and
maintaining a lexical resource in any of these
domains requires that they should not be designed
in isolation but that they may potentially be linked
with one another for mutual enrichment.
As a consequence, we believe that there is a
strong need for more widely accepted methods for
specifying lexical structures, so that the conditions
under which the corresponding databases can
exchange data are precisely defined. Moreover, it
seems that enough knowledge has been gathered
across the years to contemplate the idea that such
technical principles and methods could be the
source of an international standard that would
preserve the possibility of both describing various
types of formats and ensuring interoperability
among them.
This paper will present such a methodology as
currently under discussion in the context of ISO
committee TC 37/SC 4 in its on-going project
called LMF (Lexical Markup Framework, which
will become the future ISO 24613 standard). This
international context also provides us with a
unique opportunity to experiment with these most
recent proposals in the context of the concrete
necessity to deploy an open morphological
dictionary for French (the Morphalou project). We
will centre the discussion here on mapping the
modelling principles that have been agreed upon so
far in the LMF project with the actual requirements
associated with the design of a morphological
lexicon, hoping that it may lead to similar activities
on lexical modelling in the future.
2 Standards for lexical resources
Before describing the ongoing standardization
efforts within the LMF project, it is essential to get
an idea of the actual background available to us in
lexical representation at large and see how LMF
may build upon, or rather receive input from, other
past or ongoing standardization activities.
Lexical structures can classically be viewed
according to the way they organize the relation
between words and senses: either senses are
considered as subdivisions of the lexical entry (the
semasiological view of lexical data, which is the
one usually applied in print dictionaries) or on the
contrary, it is assumed that words (or ?terms?) are
described as ways of expressing a priori concepts,
(the onomasiological view).
The onomasiological view has formed the basis
for most previous standardization efforts since it is
at the focus of many applied contexts. This trend
started quite a while ago when the first standards
for thesaurus representation were issued in the
documentary field (ISO 2788 and ISO 5964).
Those standards basically organize lexical matter
as hierarchies of terms (e.g. broader-narrower
terms), with the possibility of adding some basic
lexical information (e.g. equivalences). More
recently, the terminological field has provided
more elaborate standards within ISO committee
TC 37, starting from the definition of an initial
SGML/XML-based  represen ta t ion  for
terminologies (ISO 12200), and progressing on to
the design of a flexible platform for specifying
terminological structures (ISO 16642). The main
problem with the onomasiological view is that
even if it is well suited for providing homogeneous
lexical descriptions within an application domain,
it is hardly extensible when broader linguistic
coverage is required.
In contrast, the semasiological view allows an
exhaustive survey of lexical content for a given
language. In particular, it provides the basis for any
classical editorial (or print) dictionary, but the wide
variety of possible dictionary formats seems to
have hampered the development of international
standards in this domain. The two main initiatives
that can be cited here are on the one hand the ISO
1951 standard dedicated solely to the
representation of dictionary entries, and on the
other hand, the seminal work done within the TEI
1
on print dictionaries, which, even though it has
already been applied to some large scale projects
such as the OED
2
, the Deutsches W?rterbuch
3
 or
the Anglo-Norman dictionary
4
, has never been
considered by publishers in particular as a real
international standard. As a consequence, many
relevant projects such as the TLFi
5
 ( Dendien &
Pierrel, 2003) have designed their own proprietary
structure for the description of their lexical
archives.
If one moves away from classical dictionaries
proper and considers lexical resources dedicated to
the domain of NLP, there are numerous projects
that have worked toward the definition of
standardized lexical structures in the domain of
NLP (Multext for basic morphological lexica;
Genelex, Simple, Isle/Mile for complex
multilingual entries; OLIF 1&2 for translation
lexica, etc.), but none of them has lead to a
standard that reflects a wide international
consensus and that is effectively maintained by an
authoritative body.
From a more theoretical point of view, it has
been shown that such lexical structures can be
modelled as feature structures (Ide et ali, 1995;
Veronis & Ide, 1992), leading to inheritance
properties within entries (Ide et ali, 2000), as
partially implemented in the TEI Print Dictionary
chapter (Ide &Veronis, 1995). It has also been
                                                       
1
 Text Encoding Initiative (http://www.tei-c.or)
2
 http://dictionary.oed.com/
3
 http://www.DWB.uni-trier.de
4
 http://www.mhra.org.uk/
5
 http://www.atilf.fr/_ns/produits/tlfi.htm
shown that, with respect to describing the micro-
structure of such lexica, at least three
configurations are possible: 2-layered, 3-layered
and 7-layered models. In the 2-layered approach,
following Ferdinand de Saussure (1974), a word is
described by a signifier/signified pair, corres-
ponding to a morphological/semantic description.
The syntactic behaviour of the word is then
systematically attached to the semantic description.
This is the approach that has been retained for
LMF. In the 3-layer approach (Antoni-Lay et ali,
1994), a word is described by three units: a
morphological, a syntactic and a semantic unit as
in Genelex or Eagles. It should be noted that due to
the fact that the syntactic unit is a mandatory
connection between morphology and semantics,
such a model is necessarily heavy and complex. In
the 7-layered approach (Mel?cuk et ali, 1995), a
word is described by various units in surface
phonology, deep phonology, surface morphology,
deep morphology, surface syntax, deep syntax and
semantics. This approach imposes a heavy burden
on the lexical description task.
Let us stress here the necessity of guaranteeing
that the methods used to describe onomasiological
and semasiological structures shall not be
completely different, so that it is possible (as
required by industrial applications in particular) to
combine various kinds of lexical resources, but
also to open the way for lexical architectures to
combine concept-based and word-based
descriptions as evidenced in the EDR dictionary
6
,
the Papillon project
7
, or IBM?s TransLexis
resource.
3 The Lexical Markup Framework project
The LMF proposal, as currently being developed
in ISO committee TC 37/SC 4, is conceived as a
generic platform for the specification of lexical
structures at any level of linguistic description. As
such, it does not provide one single model, but
rather a mechanism by which implementers com-
bine elementary lexical subsystems to design
models that can be both as close as possible to their
needs and comparable to any other lexical models
based on the same principles and, possibly, on the
same components.
The underlying data model for LMF follows the
general principles of the linguistic annotation
scheme design stated in Ide & Romary, 2003 and
implemented in the context of ISO standard 16642
for the representation of terminological data
(Romary, 2001). Those principles provide a
mechanism for combining a given structural
                                                       
6
 http://www2.crl.go.jp/kk/e416/EDR/index.html
7
 http://www.papillon-dictionary.org/
metamodel that informs the general organization of
a certain level of linguistic information (morpho-
logy, syntax, etc.) with elementary descriptors (so-
called data categories). Data categories reflect
basic linguistic concepts (e.g. /part of speech/,
/grammatical  number/, /paucal number/, etc.) and
allow for recording language-specific properties
independently of linguistic level specific models.
In order to share data categories within the
community, on-going work (in ISO/TC 37) is in
the process of deploying an on-line registry
8
 of
them, especially for use in conjunction with the
other standardization activities.
According to these principles, LMF consists of
the following elements:
? a core metamodel (i.e. the structural skeleton
shared by any linguistic description at the
lexical level);
? mechanisms for attaching lexical extensions
(see below) to the core metamodel in order to
build up more complex metamodels;
? mechanisms for selecting data categories used
for lexical description and for determining how
they relate to a metamodel;
? mechanisms for expressing any combination of
the core metamodel and data categories as
XML structures, i.e. by deciding to implement
a given data category (/gender/) as an XML
element rather than as an attribute and by
providing the corresponding vocabularies
(?gen?, ?gender?, ?genre?);
? methods for describing how to extend LMF to
analyze, design, and describe a variety of more
specific lexical resources.
As shown in Figure 1, the core metamodel of
LMF is organized as a purely hierarchical structure
built upon the following components:
The Lexical database component gathers up all
information related to a given lexicon;
The Global information component groups
together all the metadata  (e.g. version,
contributors, up-date, etc.) that can be globally
attached to the lexicon (see 4.4);
The Lexical entry component comprises the
elementary lexical unit in a lexical database. This
component can, of course, be iterated, but no
specific constraint is expressed as to  its level of
granularity in a lexical database (e.g. proper
treatment of homonyms), since this depends highly
on languages and local editorial practices;
The Form component groups together all the
general graphical or phonetic descriptions attached
to the lexical entry (reference orthographic form,
transliteration, hyphenation, pronunciation, etc.);
                                                       
8
 An experimental on-line data category registry is
accessible under http://syntax.loria.fr
Finally, the Sense component is the one that
actually organizes the lexical entry since it can be
both repeated and further subdivided into senses.
In a word-to-sense lexical structure, it is indeed
thought that this central way of organizing a lexical
entry should be part of the metamodel.
Lexical DB
1..1
Global Info
1..1
Lexical Entry
0..n
1..1
1..1
Form
1..1
Sense
0..n
1..1
0..n
1..1
Lexical
extensions
Lexical
extension
Lexical
extension
Figure 1: Core model and lexical extensions in LMF
In order to specify more complex models than
would be expressible with just the core metamodel,
LMF introduces the notion of lexical extensions.
Those extensions correspond to clusters of com-
ponents dedicated to the representation of a
specific type of lexical information (e.g. morpho-
logy, syntactic constructions, transfer patterns
(socalled interlingua), and theory dependant lexi-
cographical approaches such as Mel?cuk et al
1995 or V?ronis, 2000). Each lexical extension is
characterized by an anchor component, which is
either a component of the core metamodel or of
another lexical extension when more complex
combinations are being considered (e.g. descrip-
tion of morphological operations used to extend a
simple morphological lexical extension).
The future LMF standard as such should not
provide a specific list of data categories to be used
for lexical descriptions. This would by far be too
complex given, as we have seen, the potential
variety of applications. It is thus expected that
implementers will systematically refer to the
ISO/TC 37 data category registry to find the
adequate descriptive background for their own
purposes. Still, we can outline the basic types of
data categories that one could encounter in an LMF
based application, namely:
? data categories that may be considered as
rather specific to the domain of lexical
description: these are typically those attached
to the Form component (/pronunciation/,
/syllabification/, /stress pattern/ etc.) or to the
Sense component (e.g. /definition/, /example/,
/etymology/, etc.). Some of these categories
have already been partially described in the
?old? ISO 12620:1999 standard, but a more
precise list should be compiled as the work on
LMF is being completed;
? data categories that relate to a specific level of
linguistic description such as morphology, syn-
tax, etc. The strategy here is to avoid defining
ad hoc descriptors dedicated to lexical struc-
tures and to enforce coherence with other stan-
dardization activities by adopting those asso-
ciated with the development of related stan-
dards. For instance, data categories such as
/grammatical category/, /grammatical gender/
or /grammatical case/ should be shared
between POS tagging applications and corres-
ponding lexical descriptions;
? data categories corresponding to metadata des-
criptors used to document the production and
maintenance of a lexical database, a lexical
entry and probably, of any component in a
lexical structure (see 4.4).
To conclude this brief presentation of LMF,
which can only be considered to be a snapshot of
the ongoing discussions about it, it is important to
consider how it provides a whole standardization
spectrum for implementers who will want to apply
it for their own purposes. At a first level, they can
limit themselves to the core model, to standardized
lexical extensions and to the data categories that
are available in the DCR. Doing so, they will have
the certainty of being fully interoperable with any
other implementation that has adopted the same
scope. If necessary, it is possible for implementers
to define some proprietary data categories or
maybe their own lexical extensions, knowing that
the corresponding part of their lexical model will
probably require more work if they wants to
interchange data with other applications. Still, such
a strategy is probably the optimal one in the
current stage of LMF, since, for instance, we do
not know yet which lexical extensions will be
sufficiently consensual to be further adopted as
international standards. This is indeed the spirit in
which the Morphalou project has been established,
i.e. to design a simple morphological lexical
extension to the LMF core principles and see how
it could be validated when confronted with the real
development of a lexical resource. In the long run,
we do expect that some combinations of the core
metamodel and some standardized lexical
extensions may also be seen as possible future
standards when they match specific industrial
needs (e.g. transfer lexica ? la OLIF) or existing
practices (e.g. TEI Print Dictionary format).
4 An LMF-based model for a morphological
lexicon
4.1 Requirements for a morphological lexicon
Morphological dictionaries typically associate
inflected word forms (for example plural nouns or
past tense verb forms) with values for relevant
morphological features, such as gender and num-
ber for adjectives or person and tense for verbs. In
addition, there is often a link to one particular word
form, conventionally chosen as being the lemma.
Those dictionaries are basic resources in the field
of NLP (needed for any application based on
tagged and/or lemmatized input data) and in the
field of computer-assisted language acquisition.
Most existing morphological resources for NLP
(MulText, V?ronis 1999; LEFFF
9
, Cl?ment &
Sagot) occur as text files, whose lines display the
inflected word form, one or more morphological
tags (relative to a given tag-set) and the lemma.
This kind of representation, directly inspired by
one specific type of usage of such resources (i.e.
morphological tagging) takes the inflected form as
an entry point. At the same time, the
morphological point of view is an extensional one,
in the sense that the resource explicitly contains the
list of all inflected forms for one lemma.
Furthermore, the linguistic concepts underlying the
morphological description are not directly
transparent and accessible, since the tags are
generally synthetic tags for a set of values relative
to a set of relevant features. Finally, if any
metadata (such as the contributor or the last up-
date) are associated with such a resource, they are
often encoded in proprietary formats and there is
no possibility to parameterize their scope to
various description levels of the lexicon.
Starting from these observations, we tested LMF
as a formal framework for the design of a
morphological dictionary for French, based on
existing data originally compiled during the
digitization of a wide coverage French dictionary
(TFLi). From a theoretical point of view, the aim
of this experiment is to test the suitability of LMF
at a quite simple level of lexical description. On
the practical side, we wish to generate a resource
that is accessible on-line and that implements the
standardization proposals of ISO/TC 37/SC 4, and
that is application-independent, well documented,
extensible and provides the possibility to add
further lexical description levels, such as syntactic
and semantic information. Therefore, we have tried
to overcome the aforementioned shortcomings of
current morphological dictionaries by structuring
the data around lemmas rather than around
inflected forms, by proposing a data model that
combines the co-occurrence of extensional and
intensional morphological information (lists of
inflexions vs. reference to inflexion classes or
paradigms) and by paying special attention to the
issue of the metadata necessary to qualify the
identification of the source data (origin,
contributor, up-date, etc.) and the status of the data
(validated by an editorial committee, testified in a
corpus, etc.).
                                                       
9
 http://atoll.inria.fr/~lclement/lefff/
4.2 The lexical model of Morphalou
The underlying lexical model of the Morphalou
project is a direct application of the LMF prince-
ples with the sole addendum of a simple lexical
extension dedicated to the description of morpho-
logy. This extension can be directly linked to the
lexical entry component of the core metamodel. It
associates a single morphological description
(Morphology component) to each lexical entry.
This morphological description is made up of two
sub-components:
? a Paradigm  component that refers to or
possibly describes the inflexion rules that
govern the flexional behaviour of the entry;
? an Inflexion component that groups together
zero up to n  inflected forms related to the
lexical entry.
 
Lexical 
Entry 
Morphology 
1..1 
1..1 
Lexical Entry 
or gy 
0..1 
Paradigm 
1..1 
Inflexion 
0..n 
1..1 
Figure 2: Lexical extension for morphology
As stated in section 3, to build up a full model
for a concrete lexical database, one needs to
associate a selection of data categories anchored at
the different components of the metamodel (core
metamodel + morphological lexical extension).
To the Lexical entry component, we basically
associate to this component the data categories
/lemma/ and /grammatical category/. A /key form/
is used in order to uniquely identify the entry
within the lexical database. Possible orthographic
variants may be recorded as /spelling variant/?s.
Finally, depending on editorial choices, one could
also decide to attach /gender/ information here, for
example for nouns, in the case that gender
variation is not considered as inflexional variation,
as opposed to adjectives.
To the Inflexion component: beside /word form/,
which identifies the actual inflected form in the
component, it is necessary to associate the set of
morphological features to provide a unique
specification of the inflexion. The corresponding
data categories are complementary to the general
grammatical category of the entry: /gender/ and
/number/ for adjectives; /tense/, /person/, /number/
and /mood/ for verbs, etc. Appendix 1 provides a
complete list of the data categories we have
considered for the first version of the database;
? Paradigm component: here we essentially need
a /paradigm identifier/ to identify the inflexion
class to which the lexical entry belongs. In
order to integrate further data categories for the
description of the inflexion rules, we still need
to investigate linguistic practices for different
language families.
4.3 Implementing the model: basic examples
 Example 1 implements, in a generic XML
format (GMT, see Romary 2001), a simple lexical
entry and its morphological extension for the
French noun chat (?cat?). The data categories
associated with the lexical entry are /lemma/,
/grammatical category/ and /key form/,
respectively taking the values of chat, noun and
chat_1. The morphology component contains the
identification of the plural inflexion paradigm for
regular French nouns (/fr-s-plural/) and the
complete list of inflected word forms with
associated morphological features, i.e. /number/.
Example 1
<struct type=?lexical entry?>
<feat type=?lemma?>chat</feat>
<feat type=?grammatical category?>common
noun </feat>
<feat type=?key form?>chat_1</feat>
<struct type=?morphology?>
<struct type=?paradigm?>
<feat type=?paradigm identifier?>
fr-s-plural</feat>
</struct>
<struct type=?inflexion?>
<feat type=?word form?>chat</feat>
<feat type=?number?>singular</feat>
</struct>
<struct type=?inflexion?>
<feat type=?word form?>chats</feat>
<feat type=?number?>plural</feat>
</struct>
</struct>
</struct>
In the case that spelling variants exist such as
cheik vs. cheikh (Example 2), these are referred to
in the lexical entry component by means of the
data category /spelling variant/ and an associated
pointer to the /key form/ of the related lexical
entry. Additional mechanisms such as unification
may be envisaged in order to avoid duplication of
the lexical information that is independent from
this variation (syntactic or semantic information,
for example).
Example 2
<struct type=?lexical entry?>
<feat type=?lemma?>cheikh</feat>
<feat type=?spelling variant?
 target=?#cheik_noun?</feat>
<feat type=?grammatical category?>
common noun</feat>
<feat type=?key form?>cheikh_1</feat>
<struct type=?morphology?>
?
</struct>
</struct>
Example 3 to Example 5 ? afghan, ?afghani?,
used as masculine and feminine noun and as an
adjective ?  shows how data categories, here
/gender/, can be used in a flexible way. Depending
on editorial practices, the implementers may, for
example, chose to attach this feature to the lexical
entry for nouns, and to the inflexion component for
adjectives. They will thus consider masculine and
feminine forms of a noun as different lexical
entries (afghan_1 vs. afghane), while grouping
variations for adjectives into one single gender
(afghan_2).
Example 3
<struct type=?lexical entry?>
<feat type=?lemma?>afghan</feat>
<feat type=?grammatical category?>
common noun</feat>
<feat type=?gender?>masculine</feat>
<feat type=?key form?>afghan_1</feat>
<struct type=?morphology?>
?
<struct type=?inflexion?>
<feat type=?word form?>afghan</feat>
<feat type=?number?>singular</feat>
</struct>
<struct type=?inflexion?>
<feat type=?word form?>afghans</feat>
<feat type=?number?>plural</feat>
</struct>
</struct>
</struct>
Example 4
<struct type=?lexical entry?>
<feat type=?lemma?>afghane</feat>
<feat type=?grammatical category?>
common noun</feat>
<feat type=?gender?>feminine</feat>
<feat type=?key form?>afghan_2</feat>
<struct type=?morphology?>
?
<struct type=?inflexion?>
<feat type=?word form?>afghane</feat>
<feat type=?number?>singular</feat>
</struct>
<struct type=?inflexion?>
<feat type=?word form?>afghanes</feat>
<feat type=?number?>plural</feat>
</struct>
</struct>
</struct>
Example 5
<struct type=?lexical entry?>
<feat type=?lemma?>afghan</feat>
<feat type=?grammatical category?>
adjective</feat>
<feat type=?key form?>afghan_2</feat>
<struct type=?morphology?>
?
<struct type=?inflexion?>
<feat type=?word form?>afghan</feat>
?
</struct>
<struct type=?inflexion?>
<feat type=?word form?>afghans</feat>
?
</struct>
<struct type=?inflexion?>
<feat type=?word form?>afghane</feat>
?
</struct>
<struct type=?inflexion?>
<feat type=?word form?>afghanes</feat>
?
</struct>
</struct>
4.4 Integrating metadata descriptors
One important issue for the management, up-
dating and distribution of lexical databases is the
appropriate management of metadata, related either
to the identification of data sources or to the
characterization of the data.
Our proposal is based on several international
initiatives related to the definition of descriptors
for language data collections (cf. OLAC
10
,
IMDI
11
). We currently identify those descriptors
that may be relevant for lexical databases, such as
the language identifiers (ISO 16620) or the ?roles?
defined in OLAC (depositor, developer, research-
er, annotator, sponsor, etc.). Concerning data
characterization, existing standards (ISO 16620)
also contain an inventory of possible useful des-
criptors related to the updating process (origination
date, input date, modification date, approval date,
withdrawal date, etc.).
Additional information should be more speci-
fically related to the morphological extension: One
could for example wish to keep track of morpho-
syntactic tags (relative to a given tagset, such as
Multext) currently used to refer to certain in-
flexions (see Example 6). Other useful metadata
would be information about testimony and fre-
quency of inflected forms in corpora, completeness
of an inflexion list (relevant for defective verbs
such as pleuvoir (?to rain?) or indication of special
usages (diachronic, diatopic or diastratic variation).
Example 6
<struct type=?inflexion?>
<brack>
<feat type=?POS tag?>Nms__</feat>
<feat type=?tagset?>Multext</feat>
</brack>
<feat type=?word form?>chat</feat>
<feat type=?number?>singular</feat>
<struct/>
4.5 Morphalou?: current state
The basic model described in this paper (apart
from inflexion paradigms and metadata descrip-
tors, currently under definition) has been used to
build an electronic lexical database of inflected
forms for French
12
. It contains 539413 inflected
forms distributed over 68075 lemmas, converted
from data previously collected at the ATILF
laboratory. The whole database is encoded in
XML. Since we envisage on-line access and the
ability to up-date the data, we devoted particular
attention to the interfaces and to documentation.
The database is searchable through the web, via a
graphical interface or direct XPath queries. The
                                                       
10
 http://www.language-archives.org/
11
 http://www.mpi.nl/IMDI/
12
 http://loreley.loria.fr/morphalou/
graphical interface allows for lemmatization of a
given form and generation of all inflected forms
for a given lemma, whereas the XPath requests
allows for combining search criteria over any
combination of features and strings (for example,
all lexical entries for common nouns having an
inflected form containing the string aba). The next
steps are the development of a JAVA API and web
services to integrate search results directly into
NLP applications and the development of an
editorial line for efficient and coherent update of
the database. Preliminary updating experiments
based on freely accessible morphological databases
such as LEFFF and ABU
13
 are currently running
and reveal the most important problems to be
tackled (conversion of the input format, efficient
comparison of two XML files, linguistic validation
procedures and interfaces for submitted data,
fusion of lexical data).
5 References
Antoni-Lay MH., Francopoulo G., Zaysser L.
(1994). A generic model for reusable lexicons :
The Genelex project. Literary and Linguistic
Computing.
Dendien J., Pierrel J.-M. (2003). Le TLFi et le
logiciel Stella, au centre d'un ensemble de res-
sources informatis?es pour l'?tude du fran?ais.
Traitement Automatique des Langues, 44(2), 11-
37.
Ide N., V?ronis, J. (1995). Encoding dictionaries.
Computers and the Humanities, 29(2), 167-179.
Ide N., Kilgarriff A., Romary L. (2000). A Formal
Model of Dictionary Structure and Content.
Proceedings of Euralex 2000. Stuttgart, 113-126.
Ide N., Le Maitre J.,  V?ronis J. (1995). Outline of
a Model for Lexical Databases. Current Issues in
Computational Linguistics: In Honour of Don
Walker, Pisa, 283-320.
Ide N., Romary L. (2003).  Outline of the
International Standard Linguistic Annotation
Framework. ACL Workshop on Linguistic
Annotation: Getting the Model Right, Sapporo,
1-5.
ISO 12200:1999, Machine-readable terminology
interchange format (MARTIF) ? Negotiated
interchange.
ISO 12620:1999, Data categories.
ISO 16642:2003, Terminological markup frame-
work.
ISO 1951:1997, Lexicographical symbols and
typographical conventions for use in
terminography.
ISO 2788, Guidelines for the Establishment and
Development of Monolingual Thesauri.
ISO 5964, Guidelines for the Establishment and
Development of Multilingual Thesauri.
Mangeot-Lerebours M., S?rasset G., Lafourcade
M. (2003). Construction collaborative de base
                                                       
13
 http://abu.cnam.fr/DICO/mots-communs.html
donn?es lexicales multilingues : le projet
Papillon. Traitement Automatique des Langues,
44(2), 151-176.
Mel?cuk I., Clas A., Polgu?re A. (1995)
Introduction ? la lexicologie explicative et
combinatoire. Duculot, Bruxelles.
Romary L. (2001) Towards an Abstract
Representation of Terminological Data
Collections - the TMF model, TAMA 2001 ?
Terminology in Advanced Microcomputer
Applications, Antwerp.
Saussaure F. de (1974). Cours de linguistique
g?n?rale. Payot, 1974.
V?ronis J. (1999). Multext-Corpora. An annotated
corpus for five European languages [CD-ROM,
ELRA-ELDA].
V?ronis, J. (2000). Sense tagging: Don't look for
the meaning but for the use. Computational
Lexicography and Multimedia Dictionaries
(COMLEX'2000), Greece.
V?ronis J., Ide N. (1992). A feature-based model
for lexical databases, 14th International
Conference on Computational Linguistics
(COLING'92), Nantes (France), 588-594.
6 Acknowledgement
The work presented in this paper has received
support from the national RNTL/Outilex project,
the INRIA corporate action Syntax and the
Morphalou project (CPER Lorraine). Many thanks
to Monte George, Sue-Ellen Wright and Kornel
Bangha for their valuable comments.
7 Appendix : Data categories of the Inflexion
component of Morphalou (04/2004)
Component Data Category Conceptual Domain
/lemma/ String
/spelling variant/ String
/common noun/
/verb/
/adjective/
/adverb/
/interjection/
/onomatope/
Entry
/grammatical
category/
/function word/
/word form/ String
/indicative/
/conjunctive/
/conditional/
/past participle/
/present participle/
/mood/
/infinitive/
/present/
/imperfect/
/simple past/
/tense/
/future/
/first person/
/second person/
/person/
/third person/
/feminine/
/gender/
/masculine/
/singular/
Inflexion
/number/
/plural/
International Standard for a Linguistic Annotation Framework
Nancy Ide
Dept. of Computer Science
Vassar College
Poughkeepsie,
New York 12604-0520
 USA
ide@cs.vassar.edu
Laurent Romary
Equipe Langue et Dialogue
LORIA/INRIA
Vandoeuvre-l?s-Nancy
FRANCE
romary@loria.fr
Eric de la Clergerie
INRIA Rocquencourt, BP 105
78153 Le Chesnay cedex
FRANCE
Eric.De_La_Clergerie@
inria.fr
Abstract
This paper describes the outline of a linguistic
annotation framework under development by
ISO TC37 SC WG1-1. This international
standard will provide an architecture for the
creation, annotation, and manipulation of lin-
guistic resources and processing software. The
outline described here results from a meeting
of approximately 20 experts in the field, who
determined the principles and fundamental
structure of the framework. The goal is to
provide maximum flexibility for encoders and
annotators, while at the same time enabling
interchange and re-use of annotated linguistic
resources.
1 Introduction
Language resources are bodies of electronic language
data used to support research and applications in the
area of natural language processing. Typically, such
data are enhanced (annotated) with linguistic informa-
tion such as morpho-syntactic categories, syntactic or
discourse structure, co-reference information, etc.; or
two or more bodies may be aligned for correspondences
(e.g., parallel translations, speech signal and transcrip-
tion).
Over the past 15-20 years, increasingly large bodies
of language resources have been created and annotated
by the language engineering community. Certain fun-
damental representation principles have been widely
adopted, such as the use of stand-off annotation, use of
XML, etc., and several attempts to provide generalized
annotation mechanisms and formats have been devel-
oped (e.g., XCES, annotation graphs). However, it re-
mains the case that annotation formats often vary
considerably from resource to resource, often to satisfy
constraints imposed by particular processing software.
The language processing community has recognized
that commonality and interoperability are increasingly
imperative to enable sharing, merging, and comparison
of language resources.
To provide an infra-structure and framework for
language resource development and use, the Interna-
tional Organization for Standardization (ISO) has
formed a sub-committee (SC4) under Technical Com-
mittee 37 (TC37, Terminology and Other Language
Resources) devoted to Language Resource Manage-
ment. The objective of ISO/TC 37/SC 4 is to prepare
international standards and guidelines for effective lan-
guage resource management in applications in the mul-
tilingual information society. To this end, the committee
is developing principles and methods for creating, cod-
ing, processing and managing language resources, such
as written corpora, lexical corpora, speech corpora, dic-
tionary compiling and classification schemes. The focus
of the work is on data modeling, markup, data exchange
and the evaluation of language resources other than ter-
minologies (which have already been treated in ISO/TC
37). The worldwide use of ISO/TC 37/SC 4 standards
should improve information management within indus-
trial, technical and scientific environments, and increase
efficiency in computer-supported language communica-
tion.
At present, language professionals and standardiza-
tion experts are not sufficiently aware of the standardi-
zation efforts being undertaken by ISO/TC 37/SC 4.
Promoting awareness of future activities and rising
problems, therefore, is crucial for the success of the
committee, and will be required to ensure widespread
adoption of the standards it develops. An even more
critical factor for the success of the committee's work is
to involve, from the outset, as many and as broad a
range of potential users of the standards as possible.
Within ISO/TC 37/SC 4, a working group (WG1-1)
has been established to develop a Linguistic Annotation
Framework (LAF) that can serve as a basis for harmo-
nizing existing language resources as well as developing
new ones. In order to ensure that the framework is de-
veloped based on the input and consensus of the re-
search community, a group of experts
1
 was convened on
November 21-22, 2002, at Pont-?-Mousson, France, to
lay out the overall structure of the framework. .In this
paper, we outline the conclusions from this meeting, and
solicit the input of other members of the community to
inform its further development.
2 Background and rationale
The standardization of principles and methods for the
collection, processing and presentation of language re-
sources requires a distinct type of activity. Basic stan-
dards must be produced with wide-ranging applications
in view. In the area of language resources, these stan-
dards should provide various technical committees of
ISO, IEC and other standardizing bodies with the
groundwork for building more precise standards for
language resource management.
The need for harmonization of representation for-
mats for different kinds of linguistic information is criti-
cal, as resources and information are more and more
frequently merged, compared, or otherwise utilized in
common systems. This is perhaps most obvious for
processing multi-modal information, which must sup-
port the fusion of multimodal inputs and represent the
combined and integrated contributions of different types
of input (e.g., a spoken utterance combined with gesture
and facial expression), and enable multimodal output
(see, for example, Bunt and Romary, 2002). However,
language processing applications of any kind require the
integration of varieties of linguistic information, which,
in today?s environment, come from potentially diverse
sources. We can therefore expect use and integration of,
for example, syntactic, morphological, discourse, etc.
information for multiple languages, as well as informa-
tion structures like domain models and ontologies.
We are aware that standardization is a difficult busi-
ness, and that many members of the targeted communi-
ties are skeptical about imposing any sort of standards at
all. There are two major arguments against the idea of
standardization for language resources. First, the diver-
sity of theoretical approaches to, in particular, the an-
                                                           
1
 Participants: Nuria Bel (Universitat de Barcelona), David
Durand (Brown University), Henry Thompson (University of
Edinburgh), Koiti Hasida (AIST Tokyo), Eric De La Clergerie
(INRIA), Lionel Clement (INRIA), Laurent Romary (LORIA),
Nancy Ide (Vassar College), Kiyong Lee (Korea University),
Keith Suderman (Vassar College), Aswani Kumar (LORIA),
Chris Laprun (NIST), Thierry Declerck (DFKI), Jean Carletta
(University of Edinburgh), Michael Strube (European Media
Laboratory), Hamish Cunningham (University of Sheffield),
Tomaz Erjavec (Institute Jozef Stefan), Hennie Brugman
(Max-Planck-Institut f?r Psycholinguistik), Fabio Vitali (Uni-
versite di Bologna), Key-Sun Choi (Korterm), Jean-Michel
Borde (Digital Visual), Eric Kow (LORIA).
notation of various linguistic phenomena suggests that
standardization is at least impractical, if not impossible.
Second, it is feared that vast amounts of existing data
and processing software, which may have taken years of
effort and considerable funding to develop, will be ren-
dered obsolete by the acceptance of new standards by
the community. Recognizing the validity of both of
these concerns, WG1-1 does not seek to establish a sin-
gle, definitive annotation scheme or format. Rather, the
goal is to provide a framework for linguistic annotation
of language resources that can serve as a reference or
pivot for different annotation schemes, and which will
enable their merging and/or comparison. To this end,
the work of WG1-1 includes the following:
o analysis of the full range of annotation types and
existing schemes, to identify the fundamental
structural principles and content categories;
o instantiation of an abstract format capable of cap-
turing the structure and content of  linguistic anno-
tations, based on the analysis in (1);
o establishment of a mechanism for formal definition
of a set of reference content categories which can
be used ?off the shelf? or serve as a point of depar-
ture for precise definition of new or modified cate-
gories.
o provision of both a set of guidelines and principles
for developing new annotation schemes and con-
crete mechanisms for their implementation, for
those who wish to use them.
By situating all of the standards development
squarely in the framework of XML and related stan-
dards such as RDF, DAML+OIL, etc., we hope to en-
sure not only that the standards developed by the
committee provide for compatibility with established
and widely accepted web-based technologies, but also
that transduction from legacy formats into XML formats
conformant to the new standards is feasible.
3  General requirements for a linguistic
annotation framework
The following general requirements for a linguistic an-
notation framework were identified by the group of ex-
perts at Pont-?-Mousson:
Expressive adequacy
The framework must provide means to represent all
varieties of linguistic information (and possibly also
other types of information). This includes representing
the full range of information from the very general to
information at the finest level of granularity.
Media independence
The framework must handle all potential media types,
including text, audio, video, image, etc. and should, in
principle, provide common mechanisms for handling all
of them. The framework will rely on existing or devel-
oping standards for representing multi-media.
Semantic adequacy
o Representation structures must have a formal se-
mantics, including definitions of logical operations
o There must exist a centralized way of sharing de-
scriptors and information categories
Incrementality
o The framework must provide support for various
stages of input interpretation and output generation.
o The framework must provide for the representation
of partial/under-specified results and ambiguities,
alternatives, etc. and their merging and comparison.
Uniformity
Representations must utilize same ?building blocks? and
the same methods for combining them.
Openness
The framework must not dictate representations de-
pendent on a single linguistic theory.
Extensibility
The framework must provide ways to declare and inter-
change extensions to the centralized data category reg-
istry.
Human readability
Representations must be human readable, at least for
creation and editing.
Processability (explicitness)
Information in an annotation scheme must be ex-
plicit?that is, the burden of interpretation should not be
left to the processing software.
Consistency
Different mechanisms should not be used to indicate the
same type of information.
To fulfill these requirements, it is necessary to iden-
tify a consistent underlying data model for data and its
annotations. A data model is a formalized description of
the data objects (in terms of composition, attributes,
class membership, applicable procedures, etc.) and rela-
tions among them, independent of their instantiation in
any particular form. A data model capable of capturing
the structure and relations in diverse types of data and
annotations is a pre-requisite for developing a common
corpus-handling environment: it impacts the design of
annotation schema, encoding formats and data archi-
tectures, and tool architectures.
As a starting assumption, we can conceive of an an-
notation as a one- or two-way link between an annota-
tion object and a point (or a list/set of points) or span (or
a list/set of spans) within a base data set. Links may or
may not have a semantics--i.e., a type--associated with
them. Points and spans in the base data may themselves
be objects, or sets or lists of objects. We make several
observations concerning this assumption:
o the model assumes a fundamental linearity of ob-
jects in the base,
2
  e.g., as a time line (speech); a s e-
quence of characters, words, sentences, etc.; or
pixel data representing images;
o the granularity of the data representation and en-
coding is critical: it must be possible to uniquely
point to the smallest possible component (e.g.,
character, phonetic component, pitch signal, mor-
pheme, word, etc.);
o an annotation scheme must be mappable to the
structures defined for annotation objects in the
model;
o an encoding scheme must be able to capture the
object structure and relations expressed in the
model, including class membership and inheritance,
therefore requiring a sophisticated means to specify
linkage within and between documents;
o it is necessary to consider the logistics of identify-
ing spans by enclosing them in start and end tags
(thus enabling hierarchical grouping of objects in
the data itself), vs. explicit addressing of start and
end points, which is required for read-only data;
o it must be possible to represent objects and rela-
tions in some (fairly straightforward) form that pre-
vents information loss;
o ideally, it should be possible to represent the ob-
jects and relations in a variety of formats suitable to
different tools and applications.
ISO TC37/SC 4?s goal is to develop a framework for
the design and implementation of linguistic resource
formats and processes in order to facilitate the exchange
of information between language processing modules. A
well-defined representational framework for linguistic
information will also provide for the specification and
comparison of existing application-specific representa-
tions and the definition of new ones, while ensuring a
level of interoperability between them. The framework
should allow for variation in annotation schemes while
                                                           
2
 Note that this observation applies to the fundamental struc-
ture of stored data. Because the targets of a relation may be
either individual objects, or sets or lists of objects, information
with more than one dimension is accommodated.
at the same time enabling comparison and evaluation,
merging of different annotations, and development of
common tools for creating and using annotated data. For
this purpose we envisage a common ?pivot? format
based on a data model capable of capturing all types of
information in linguistic annotations, into and out of
which site-specific representation formats can be trans-
duced. This strategy is similar to that adopted in the
design of languages intended to be reusable across plat-
forms, such as Java. The pivot format must support the
communication among all modules in the system, and
be adequate for representing not only the end result of
interpretation, but also intermediate results.
4 Terms and definitions
The following terms and definitions are used in the dis-
cussion that follows:
Annotation: The process of adding linguistic informa-
tion to language data (?annotation of a corpus?) or the
linguistic information itself (?an annotation?), inde-
pendent of its representation. For example, one may
annotate a document for syntax using a LISP-like repre-
sentation, an XML representation, etc.
Representation: The format in which the annotation is
rendered, e.g. XML, LISP, etc. independent of its con-
tent. For example, a phrase structure syntactic annota-
tion and a dependency-based annotation may both be
represented using XML, even though the annotation
information itself is very different.
Types of Annotation: We distinguish two fundamental
types of annotation activity:
1. segmentation : delimits linguistic elements that
appear in the primary data. Including
o continuous segments (appear contiguously in
the primary data)
o super- and sub-segments, where groups of
segments will comprise the parts of a larger
segment (e.g., a contiguous word segments
typically comprise a sentence segment)
o discontinuous segments (linking continuous
segments)
o landmarks (e.g time stamps) that note a point in
the primary data
In current practice, segmental information may
or may not appear in the document containing the
primary data itself. Documents considered to be
read-only, for example, might be segmented by
specifying byte offsets into the primary document
where a given segment begins and ends.
2. linguistic annotation: provides linguistic informa-
tion about the segments in the primary data, e.g., a
morpho-syntactic annotation in which a part of
speech and lemma are associated with each seg-
ment in the data. Note that the identification of a
segment as a word, sentence, noun phrase, etc. also
constitutes linguistic annotation. In current practice,
when it is possible to do so, segmentation and
identification of the linguistic role or properties of
that segment are often combined (e.g., syntactic
bracketing, or delimiting each word in the docu-
ment with an XML tag that identifies the segment
as a word, sentence, etc.).
Stand-off annotation: Annotations layered over a
given primary document and instantiated in a document
separate from that containing the primary data. Stand-
off annotations refer to specific locations in the primary
data, by addressing byte offsets, elements, etc. to which
the annotation applies. Multiple stand-off annotation
documents for a given type of annotation can refer to
the same primary document (e.g., two different part of
speech annotations for a given text). There is no re-
quirement that a single XML-compliant document may
be created by merging stand-off annotation documents
with the primary data; that is, two annotation documents
may specify trees over the primary data that contain
overlapping hierarchies.
5 Design principles
The following general principles will guide the LAF
development:
o The data model and document form are distinct but
mappable to one another
o The data model is parsimonious, general, and for-
mally precise
o The data model is built around a clear separation of
structure and content
o There is an inventory of logical operations sup-
ported by the data model, which define its abstract
semantics
o The document form is largely under user control
o The mapping between the flexible document form
and data model is via a rigid dump-format
o The mapping from document form to the dump
format is documented in an XML Schema (or the
functional equivalent thereof) associated with the
document
o Mapping is operationalized either via schema-based
data-binding process o r via schema-derived
stylesheet mapping between the user document and
the dump-format document.
o It must be possible to isolate specific layers of an-
notation from other annotation layers or the primary
(base) data; i.e., it must be possible to create a for-
mat using stand-off annotation
o The dump format must be designed to enable
stream marshalling and unmarshalling
The overall architecture of LAF as dictated by these
principles is given in Figure 1. The left side of the dia-
gram represents the user-defined document form, and is
labeled ?human? to indicate that creation and editing, of
the resource is accomplished via human interaction with
this format. This format should, to the extent possible,
be human readable. We will support XML for these
formats (e.g., by providing style sheets, examples, etc.)
but not disallow other formats. The right side represents
the dump format, which is machine processable, and
may not be human readable as it is intended for use only
in processing. This format will be instantiated in XML.
Figure 1. Overall LAF architecture
6 Practice
The following set of practices will guide the implementa-
tion of the LAF:
o The data model is essentially a feature structure
graph with a moderate admixture of algebra (e.g.
disjunction, sets), grounded in n-dimensional regions
of primary data and literals.
o The dump format is isomorphic to the data model.
o Semantic coherence is provided by a registry of fea-
tures in an XML-compatible format (e.g., RDF),
which can be used directly in the user-defined for-
mats and is always used with the dump format.
o Resources will be available to support the design and
specification of document forms, for example:
- XML Schemas in several normal forms based on
type definitions and abstract elements that can be
exploited via type derivation and/or substitution
group;
- XPointer design-patterns with standoff seman-
tics;
- Schema annotations specifying mapping between
document form and data model;
- Meta-stylesheet for mapping from annotated
XML Schema to mapping stylesheets;
- Data-binding stylesheets with language-specific
bindings (e.g. Java).
o Users may define their own data categories or estab-
lish variants of categories in the registry. In such
cases, the newly defined data categories will be for-
malized using the same format as definitions avail-
able in the registry, and will be associated with the
dump format.
o The responsibility of converting to the dump format
is on the producer of the resource.
o The producer is responsible for documenting the
mapping from the user format to the data model.
Dump format
(some instantiation of
data model)
MAPPING SPEC
(e.g. W3C
XML schema)
User annotation
form
Meta-stylesheet
stylesheet
Intended for archive
Accompanied by
mapping to data
model
Pivot format
HUMAN MACHINE
o The ISO working group will provide test suites and
examples following these guidelines:
- The example format should illustrate use of data
model/mapping
- The examples will show both the left (human-
readable) and right (machine processable) side
formats
- Examples will be provided that use existing
schemes
7 Discussion
The framework outlined in the previous section provides
for the use of any annotation format consistent with the
feature structure-based data model that will be used to
define the pivot format. This suggests a future scenario in
which annotators may create and edit annotations in a
proprietary format, transduce the annotations using avail-
able tools to the pivot format for interchange and/or proc-
essing, and if desired, transduce the pivot form of the
annotations (and/or additional annotation introduced by
processing) back into the proprietary format. We antici-
pate the future development of annotation tools that pro-
vide a user-oriented interface for specifying annotation
information, and which then generate annotations in the
pivot format directly. Thus the pivot format is intended to
function in the same way as, for example, Java byte code
functions for programmers, as a universal ?machine lan-
guage? that is interpreted by processing software into an
internal representation suited to its particular require-
ments. As with Java byte code, users need never see or
manipulate the pivot format; it is solely for machine con-
sumption.
Information units or data categories provide the se-
mantics of an annotation. Data categories are the most
theory and application-specific part of an annotation
scheme. Therefore, LAF includes a Data Category Regis-
try to provide a means to formally define data categories
for reference and use in annotation. To make them maxi-
mally interoperable and consistent with existing stan-
dards, RDF schemas can be used to formalize the
properties and relations associated with each data cate-
gory. The RDF schema ensures that each instantiation of
the described objects is recognized as a sub-class of more
general classes and inherits the appropriate properties.
Annotations will reference the data categories via a URL
identifying their instantiations in the Data Category Reg-
istry itself. The class and sub-class mechanisms provided
in RDFS and its extensions in OWL will also enable
creation of an ontology of annotation classes and types.
A formally defined set of categories will have several
functions: (1) it will provide a precise semantics for an-
notation categories that can be either used ?off the shelf?
by annotators or modified to serve specific needs; (2) it
will provide a set of reference categories onto which
scheme-specific names can be mapped; and (3) it will
provide a point of departure for definition of variant or
more precise categories. Thus the overall goal of the Data
Category Registry is not to impose a specific set of cate-
gories, but rather to ensure that the semantics of data
categories included in annotations (whether they exist in
the Registry or not) are well-defined and understood.
The data model that will define the pivot format must
be capable of representing all of the information con-
tained in diverse annotation types. The model we assume
is a feature structure graph for annotation information,
capable of referencing n-dimensional regions of primary
data as well as other annotations. The choice of this
model is indicated by its almost universal use in defining
general-purpose annotation formats, including the Ge-
neric Modeling Tool (GMT) (Ide & Romary, 2001, 2002)
and Annotation Graphs (Bird & Liberman, 2001). The
XML-based GMT could serve as a starting point for de-
fining the pivot format; its applicability to diverse anno-
tation types, including terminology, dictionaries and other
lexical data (Ide, et al, 2000), morphological annotation
(Ide & Romary, 2001a; 2003) and syntactic annotation
(Ide & Romary, 2001b) demonstrates its generality. As
specified by the LAF architecture, the GMT implements a
feature structure graph, and exploits the hierarchical
structure of XML elements and XML?s powerful inter-
and intra-document pointing and linkage mechanisms for
referencing both ?raw? and XML-tagged primary data
and its annotations.
The provision of development resources, including
schemas, design patterns, and stylesheets, will enable
annotators and software developers to immediately adapt
to LAF. Example mappings, e.g., for XCES-encoded an-
notations, will also be provided.
8 Conclusion
In this paper we describe the Linguistic Annotation
Framework under development by ISO TC37/SC 4 WG1-
1, as defined by a group of experts convened at a work-
shop in Pont-?-Mousson, France, in late 2002. Its design
is intended to allow for, on the one hand, maximum flexi-
bility for annotators, and. on the other, processing effi-
ciency and reusability. This is accomplished by
separating user annotation formats from the ex-
change/processing format. This separation also ensures
that pre-existing annotations are compatible with LAF.
ISO TC37/SC4 is just beginning its work, and will use
the general framework discussed in the preceding sections
as its starting point. However, the work of the committee
will not be successful unless it is accepted by the lan-
guage processing community. To ensure widespread ac-
ceptance, it is critical to involve as many representatives
of the community in the development of the standards as
possible, in order to ensure that all needs are addressed.
This paper serves as a call for participation to the lan-
guage processing community; those interested should
contact the TC 37/SC 4 chairman (Laurent Romary:
romary@loria.fr). For general information, consult the
ISO TC37/SC4 website (http://www.tc37sc4.org).
References
Bird, S. & Liberman, M. (2001). A formal framework for
linguistic annotation. Speech Communication, 33:1-2,
23-60.
Bunt. H. & Romary, L. (2002). Towards Multimodal
Content Representation.  Proceedings of the Workshop
on International Standards for Terminology and Lan-
guage Resource Management, Las Palmas.
Ide, N. & Romary, L. (2001a). Standards for Language
Resources, IRCS Workshop on Linguistic Databases,
Philadelphia, 141-49.
Ide, N. & Romary, L. (2001b). A Common Framework
for Syntactic Annotation. Proceedings of ACL'2001,
Toulouse, 298-305.
Ide, N., Kilgarriff, A., & Romary, L. (2000). A Formal
Model of Dictionary Structure and Content. Proceed-
ings of Euralex 2000, Stuttgart, 113-126.
Ide, N. & Romary, L. (2003). Encoding Syntactic Anno-
tation. In Abeill?, A. (ed.). Treebanks: Building and
Using Syntactically Annotated Corpora. Dordrecht:
Kluwer Academic Publishers (in press).
Outline of the International Standard Linguistic Annotation
Framework
Nancy Ide
Dept. of Computer Science
Vassar College
Poughkeepsie,
New York 12604-0520
 USA
ide@cs.vassar.edu
Laurent Romary
Equipe Langue et Dialogue
LORIA/INRIA
Vandoeuvre-l?s-Nancy
FRANCE
romary@loria.fr
Abstract
This paper describes the outline of a lin-
guistic annotation framework under de-
velopment by ISO TC37 SC WG1-1. This
international standard provides an archi-
tecture for the creation, annotation, and
manipulation of linguistic resources and
processing software. The goal is to pro-
vide maximum flexibility for encoders
and annotators, while at the same time
enabling interchange and re-use of anno-
tated linguistic resources. We describe
here the outline of the standard for the
purposes of enabling annotators to begin
to explore how their schemes may map
into the framework.
1 Introduction
Over the past 15-20 years, increasingly large bod-
ies of language resources have been created and
annotated by the language engineering community.
Certain fundamental representation principles have
been widely adopted, such as the use of stand-off
annotation, use of XML, etc., and several attempts
to provide generalized annotation mechanisms and
formats have been developed (e.g., XCES, annota-
tion graphs). However, it remains the case that an-
notation formats often vary considerably from
resource to resource, often to satisfy constraints
imposed by particular processing software. The
language processing community has recognized
that commonality and interoperability are increas-
ingly imperative to enable sharing, merging, and
comparison of language resources.
To provide an infra-structure and framework for
language resource development and use, the Inter-
national Organization for Standardization (ISO)
has formed a sub-committee (SC4) under Techni-
cal Committee 37 (TC37, Terminology and Other
Language Resources) devoted to Language Re-
source Management. The objective of ISO/TC
37/SC 4 is to prepare international standards and
guidelines for effective language resource man-
agement in applications in the multilingual infor-
mation society. To this end, the committee is
developing principles and methods for creating,
coding, processing and managing language re-
sources, such as written corpora, lexical corpora,
speech corpora, and dictionary compiling and clas-
sification schemes. The focus of the work is on
data modeling, markup, data exchange and the
evaluation of language resources other than termi-
nologies (which have already been treated in
ISO/TC 37). The worldwide use of ISO/TC 37/SC
4 standards should improve information manage-
ment within industrial, technical and scientific en-
vironments, and increase efficiency in computer-
supported language communication.
Within ISO/TC 37/SC 4, a working group (WG1-
1) has been established to develop a Linguistic An-
notation Framework (LAF) that can serve as a ba-
sis for harmonizing existing language resources as
well as developing new ones. The overall design of
the architecture and the data model that it will in-
stantiate have been described in Ide et al, 2003. In
this paper we provide a description of the data
model and its instantiations in LAF, in order to
enable annotators to begin to explore how their
schemes will map into the framework.
2 Terms and definitions
The following terms and definitions are used in the
discussion that follows:
Annotation: The process of adding linguistic in-
formation to language data (?annotation of a cor-
pus?) or the linguistic information itself (?an
annotation?), independent of its representation. For
example, one may annotate a document for syntax
using a LISP-like representation, an XML repre-
sentation, etc.
Representation: The format in which the annota-
tion is rendered, e.g. XML, LISP, etc. independent
of its content. For example, a phrase structure syn-
tactic annotation and a dependency-based annota-
tion may both be represented using XML, even
though the annotation information itself is very
different.
Types of Annotation: We distinguish two funda-
mental types of annotation activity:
1. Segmentation: delimits linguistic elements that
appear in the primary data. Including
o  continuous segments (appear contiguously
in the primary data)
o  super- and sub-segments, where groups of
segments will comprise the parts of a
larger segment (e.g., a contiguous word
segments typically comprise a sentence
segment)
o  discontinuous segments (linked continuous
segments)
o  landmarks (e.g. time stamps) that note a
point in the primary data
In current practice, segmental information may
or may not appear in the document containing
the primary data itself. Documents considered
to be read-only, for example, might be seg-
mented by specifying byte offsets into the
primary document where a given segment be-
gins and ends.
2. Linguistic annotation: provides linguistic in-
formation about the segments in the primary
data, e.g., a morpho-syntactic annotation in
which a part of speech and lemma are associ-
ated with each segment in the data. Note that
the identification of a segment as a word, sen-
tence, noun phrase, etc. also constitutes lin-
guistic annotation. In current practice, when it
is possible to do so, segmentation and identifi-
cation of the linguistic role or properties of that
segment are often combined (e.g., syntactic
bracketing, or delimiting each word in the
document with an XML tag that identifies the
segment as a word, sentence, etc.).
Stand-off annotation: Annotations layered over a
given primary document and instantiated in a
document separate from that containing the pri-
mary data. Stand-off annotations refer to specific
locations in the primary data, by addressing byte
offsets, elements, etc. to which the annotation ap-
plies. Multiple stand-off annotation documents for
a given type of annotation can refer to the same
primary document (e.g., two different part of
speech annotations for a given text). There is no
requirement that a single XML-compliant docu-
ment may be created by merging stand-off annota-
tion documents with the primary data; that is, two
annotation documents may specify trees over the
primary data that contain overlapping hierarchies.
3 LAF overview
LAF development has proceeded by first identify-
ing an abstract data model that can formally de-
scribe linguistic annotations, distinct from any
particular representation (as defined in the previous
section). Development of this model has been dis-
cussed extensively within the language engineering
community and tested on a variety of annotation
types (see Ide and Romary, 2001a, 2001b, 2002).
The data model forms the core of the framework
by serving as the reference point for all annotation
representation schemes.
The overall design of LAF is illustrated in Figure
1. The fundamental principle is that the user con-
trols the representation format for linguistic anno-
tations, which is mappable to the data model. This
mapping is accomplished via a rigid ?dump? for-
mat, isomorphic to the data model and intended
primarily for machine rather than human use.
Figure 1. Overall LAF architecture
4 Dump format specification
The data model is built around a clear separation of
the structure of annotations and their content, that
is, the linguistic information the annotation pro-
vides. The model therefore combines a structural
meta-model, that is, an abstract structure shared by
all documents of a given type (e.g. syntactic anno-
tation), and a set of data categories associated with
the various components of the structural meta-
model.
The structural component of the data model is a
feature structure graph capable of referencing n-
dimensional regions of primary data as well as
other annotations. The choice of this model is indi-
cated by its almost universal use in defining gen-
eral-purpose annotation formats, including the
Generic Modeling Tool (GMT) (Ide and Romary,
2001, 2002) and Annotation Graphs (Bird and
Liberman, 2001). A small inventory of logical op-
erations over annotation structures is specified,
which define the model?s abstract semantics. These
operations allow for expressing the following rela-
tions among annotation fragments:
? Parallelism: two or more annotations refer to
the same data object;
? Alternatives: two or more annotations com-
prise a set of mutually exclusive alternatives
(e.g., two possible part-of-speech assignments,
before disambiguation);
? Aggregation: two or more annotations com-
prise a list (ordered) or set (unordered) that
should be taken as a unit.
The feature structure graph is a graph of elemen-
tary structural nodes to which one or more data
category/value pairs are attached, providing the
semantics of the annotation. LAF does not provide
definitions for data categories. Rather, to ensure
semantic coherence we specify a mechanism for
the formal definition of categories and relations,
and provide a Data Category Registry of pre-
defined categories that can be used directly in an-
notations. Alternatively, users may define their
own data categories or establish variants of catego-
ries in the registry; in such cases, the newly defined
data categories will be formalized using the same
format as definitions available in the registry.
5 Implementation
5.1 Dump format
The dump format is instantiated in XML. Struc-
tural nodes are represented as XML elements. The
XML-based GMT will serve as a starting point for
defining the dump format. Its applicability to di-
verse annotation types, including terminology, dic-
tionaries and other lexical data (Ide, et al, 2000),
morphological annotation (Ide and Romary, 2002)
and syntactic annotation (Ide and Romary, 2001b,
2003) demonstrates its generality.
As specified by the LAF architecture, the GMT
implements a feature structure graph. Structural
nodes in the graph are represented with the XML
element <struct>. <brack> and <alt> elements
User-defined
representation
format
User-defined
representation
format
User-definedrepresentationformat DUMPFORMAT DATAMODELMappingspecifica-
tion
are provided as grouping tags to handle aggrega-
tion (grouping) and alternatives (disjunction), as
described above. A <feat> element is used to ex-
press category/value pairs. All of these elements
are recursively nestable. Therefore, hierarchical
relations among annotations and annotation com-
ponents can be expressed via XML syntax via ele-
ment nesting. Other relations, including those
among discontiguous elements, rely on XML?s
powerful inter- and intra-document pointing and
linkage mechanisms. Because all annotations are
stand-off (i.e., in documents separate from the pri-
mary data and other annotations), the same mecha-
nisms are used to associate annotations with both
?raw? and XML-tagged primary data and with
other annotations.
The final XML implementation of the dump format
may differ slightly from the GMT, in particular
where processing concerns (e.g. ease of processing
elements vs. attributes vs. content) and conciseness
are applied. However, in its general form the above
are sufficient to express the information required in
LAF. For examples of morphological and syntactic
annotation in GMT format, see Ide and Romary,
2001a; 2003; and Ide and Romary, 2001b.
5.2 Data Categories
To make them maximally interoperable and con-
sistent with existing standards, RDF schemas can
be used to formalize the properties and relations
associated with data categories. Instances of the
categories themselves will be represented in RDF.
The RDF schema ensures that each instantiation of
the described objects is recognized as a sub-class
of more general classes and inherits the appropriate
properties. Annotations will reference the data
categories via a URL identifying their instantia-
tions in the Data Category Registry itself. The class
and sub-class mechanisms provided in RDFS and
its extensions in OWL will also enable creation of
an ontology of annotation classes and types.
For example, the syntactic feature defined in the
ISLE/MILE format for lexical entries (Calzolari, et
al. 2003) can be represented in RDF as follows
1
:
                                                       
1
 For brevity, this representation does not include the full i n-
formation necessary for the RDF representation.
<rdf:RDF>
<Phrase rdf:ID="Vauxhave">
   <hasSynFeature>
     <SynFeature>
        <hasSynFeatureName rdf:value="aux"/>
        <hasSynFeatureValue rdf:value="have"/>
   </SynFeature>
</hasSynFeature></Phrase>
</rdf:RDF>
Once declared in the Data Category registry, an-
notations or lexicons can reference this object di-
rectly, for example:
<Self rdf:ID="eat1Self">
  <headedBy
   rdf:resource="http://www.DCR /Vauxhave"/>
</Self>
For a full example of the use of RDF-instantiated
data categories, see Ide, et al, in this volume.
Note that RDF descriptions function much like
class definitions in an object-oriented program-
ming language: they provide, effectively, templates
that describe how objects may be instantiated, but
do not constitute the objects themselves. Thus, in a
document containing an actual annotation, several
objects with the same type may be instantiated,
each with a different value. The RDF schema en-
sures that each instantiation is recognized as a sub-
class of more general classes and inherits the ap-
propriate properties.
A formally defined set of categories will have sev-
eral functions: (1) it will provide a precise seman-
tics for annotation categories that can be either
used ?off the shelf? by annotators or modified to
serve specific needs; (2) it will provide a set of ref-
erence categories onto which scheme-specific
names can be mapped; and (3) it will provide a
point of departure for definition of variant or more
precise categories. Thus the overall goal of the
Data Category Registry is not to impose a specific
set of categories, but rather to ensure that the se-
mantics of data categories included in annotations
(whether they exist in the Registry or not) are well-
defined and understood.
6 Conclusion
In this paper we describe the Linguistic Annotation
Framework under development by ISO TC37/SC 4
WG1-1. Its design is intended to allow for, on the
one hand, maximum flexibility for annotators, and.
on the other, processing efficiency and reusability.
This is accomplished by separating user annotation
formats from the exchange/processing format. This
separation ensures that pre-existing annotations are
compatible with LAF, and that users have the free-
dom to design specific schemes to meet their
needs, while still conforming to LAF requirements.
LAF provides for the use of any annotation format
consistent with the feature structure-based data
model that will be used to define the pivot format.
This suggests a future scenario in which annotators
may create and edit annotations in a proprietary
format, transduce the annotations using available
tools to the pivot format for interchange and/or
processing, and if desired, transduce the pivot form
of the annotations (and/or additional annotation
introduced by processing) back into the proprietary
format. We anticipate the future development of
annotation tools that provide a user-oriented inter-
face for specifying annotation information, and
which then generate annotations in the pivot format
directly. Thus the pivot format is intended to func-
tion in the same way as, for example, Java byte
code functions for programmers, as a universal
?machine language? that is interpreted by process-
ing software into an internal representation suited
to its particular requirements. As with Java byte
code, users need never see or manipulate the pivot
format; it is solely for machine consumption.
Part of the work of SC4 WG1-1 is to provide de-
velopment resources, including schemas, design
patterns, and stylesheets, which will enable anno-
tators and software developers to immediately
adapt to LAF. Example mappings, e.g., for XCES-
encoded annotations, will also be provided. In this
way, we hope to realize the goal of harmonized and
reusable resources in the near future.
References
Bird, S. and Liberman, M. (2001). A formal
framework for linguistic annotation. Speech Com-
munication, 33:1-2, 23-60.
Bunt. H. and Romary, L. (2002). Towards Multi-
modal Content Representation.  Proceedings of the
Workshop on International Standards for Termi-
nology and Language Resource Management, Las
Palmas.
Calzolari, N., Bertagna, F., Lenci, A., Monachini,
M., 2003. Standards and best Practice for Multi-
lingual Computational Lexicons and MILE (Multi-
lingual ISLE Lexical Entry), ISLE Computational
Lexicon Working Group deliverables D2.2 ? D3.2,
Pisa.
Ide, N. and Romary, L. (2001a). Standards for
Language Resources, IRCS Workshop on Linguis-
tic Databases, Philadelphia, 141-49.
Ide, N. and Romary, L. (2001b). A Common
Framework for Syntactic Annotation. Proceedings
of ACL'2001, Toulouse, 298-305.
Ide, N. and Romary, L. (2002). Standards for Lan-
guage Resources. Proceedings of the Third Lan-
guage Resources and Evaluation Conference
(LREC), Las Palmas, Canary Islands, Spain, 839-
44.
Ide, N. and Romary, L. (2003). Encoding Syntactic
Annotation. In Abeill?, A. (ed.). Treebanks:
Building and Using Syntactically Annotated Cor-
pora. Dordrecht: Kluwer Academic Publishers (in
press).
Ide, N., Kilgarriff, A., and Romary, L. (2000). A
Formal Model of Dictionary Structure and Content.
Proceedings of Euralex 2000, Stuttgart, 113-126.
Ide, N., Lenci, A., And Calzolari, N. (2003). RDF
Instantiation of ISLE/MILE Lexical Entries. This
volume.
Ide, N., Romary, L, and De la Clergerie, E. (2003).
International Standard for a Linguistic Annotation
Framework. Proceedings of NAACL?03 Workshop
on Software Engineering and Architecture of Lan-
guage Technology Systems (to appear).
A Common Framework for Syntactic Annotation
Nancy Ide
Department of Computer Science
Vassar College
Poughkeepsie, NY 12604-0520 USA
ide@cs.vassar.edu
Laurent Romary
LORIA/CNRS
Campus Scientifique, B.P. 239
54506 Vandoeuvre-l?s-Nancy, FRANCE
romary@loria.fr
Abstract
It is widely recognized that the
proliferation of annotation schemes
runs counter to the need to re-use
language resources, and that standards
for linguistic annotation are becoming
increasingly mandatory. To answer this
need, we have developed a
representation framework comprised of
an abstract model for a variety of
different annotation types (e.g.,
morpho-syntactic tagging, syntactic
annotation, co-reference annotation,
etc.), which can be instantiated in
different ways depending on the
annotator?s approach and goals. In this
paper we provide an overview of our
representation framework and
demonstrate its applicability to
syntactic annotation. We show how the
framework can contribute to
comparative evaluation and merging of
parser output and diverse syntactic
annotation schemes.
1 Introduction
It is widely recognized that the proliferation of
annotation schemes runs counter to the need to
re-use language resources, and that standards for
linguistic annotation are becoming increasingly
mandatory. In particular, there is a need for a
general framework for linguistic annotation that
is flexible and extensible enough to
accommodate different annotation types and
different theoretical and practical approaches,
while at the same time enabling their
representation in a ?pivot? format that can serve
as the basis for comparative evaluation of parser
output, such as PARSEVAL  (Harrison, et al,
1991), as well as the development of reusable
editing and processing tools.
To answer this need, we have developed a
representation framework comprised of an
abstract model for a variety of different
annotation types (e.g., morpho-syntactic
tagging, syntactic annotation, co-reference
annotation, etc.), which can be instantiated in
different ways depending on the annotator?s
approach and goals. We have implemented both
the abstract model and various instantiations
using XML schemas (Thompson, et al, 2000),
the Resource Definition Framework (RDF)
(Lassila and Swick, 2000) and RDF schemas
(Brickley and Guha, 2000), which enable
description and definition of abstract data
models together with means to interpret, via the
model, information encoded according to
different conventions. The results have been
incorporated into XCES (Ide, et al, 2000a), part
of the EAGLES Guidelines developed by the
Expert Advisory Group on Language
Engineering Standards (EAGLES)1. The XCES
provides a ready-made, standard encoding
format together with a data architecture
designed specifically for linguistically annotated
corpora.
In this paper we provide an overview of our
representation framework and demonstrate its
applicability to syntactic annotation. The
framework has been applied to the
representation of terminology (Terminological
Markup Framework2, ISO project n.16642) and
computational lexicons (Ide, et al, 2000b), thus
demonstrating its general applicability for a
variety of linguistic annotation types. We also
show how the  framework can contribute to
                                                           
1
 http://www.ilc.pi.cnr.it/EAGLES/home.html
2
 http://www.loria.fr/projects/TMF
comparison and merging of diverse syntactic
annotation schemes.
2 Current Practice
At the highest level of abstraction, syntactic
annotation schemes represent the following
kinds of information:
?  Category  in format ion : labeling of
components based on syntactic category
(e.g., noun phrase, prepositional phrase),
syntactic role (subject, object), etc.;
? Dependency information: relations among
components, including constituency
relations, grammatical role relations, etc.
For example, the annotation in Figure 1, drawn
from the Penn Treebank II3 (hereafter, PTB),
uses LISP-like list structures to specify
constituency relations and provide syntactic
category labels for constituents. Some
grammatical roles (subject, object, etc.) are
implicit in the structure of the encoding: for
instance, the nesting of the NP ?the front room?
implies that the NP is the object of the
prepositional phrase, whereas the position of the
NP ?him? following and at the same level as the
VP node implies that this NP is the grammatical
object. Additional processing (or human
intervention) is required to render these relations
explicit. Note that the PTB encoding provides
some explicit information about grammatical
role, in that ?subject? is explicitly labeled
(although its relation to the verb remains
implicit in the structure), but most relations
(e.g., ?object?) are left implicit. Relations
among non-contiguous elements demand a
special numbering mechanism to enable cross-
reference, as in the specification of the NP-SBJ
of the embedded sentence by reference to the
earlier NP-SBJ-1 node.
Although they differ in the labels and in
some cases the function of various nodes in the
tree, most annotation schemes provide a similar
constituency-based representation of relations
among syntactic components (see Abeille,
forthcoming, for a comprehensive survey of
syntactic annotation schemes). In contrast,
dependency  schemes (e.g., Sleator and
Temperley, 1993; Tapanainen and Jarvinen,
1997; Carroll, et al, forthcoming) do not
                                                           
3
 http://www.cis.upenn.edu/treebank
provide a constituency analysis4 but rather
specify grammatical relations among elements
explicitly; for example, the sentence ?Paul
intends to leave IBM? could be represented as
shown in Figure 2, where the predicate is the
relation type, the first argument is the head, the
second the dependent, and additional arguments
may provide category-specific information (e.g.,
introducer for prepositional phrases, etc.).
((S (NP-SBJ-1 Jones)
(VP followed)
(NP him)
(PP-DIR into
  (NP the front room))
,
(S-ADV (NP-SBJ *-1)
  (VP closing
  (NP the door)
(PP behind
(NP him)))))
.))
Figure 1. PTB annotation of ?Jones followed him
into the front room, closing the door behind
him.?
subj(intend,Paul,_)
xcomp(intend,leave,to)
subj(leave,Paul)
dobj(leave,IBM,_)
Figure 2. Dependency annotation according to
Carroll, Minnen, and Briscoe (forthcoming)
3 A Model for Syntactic Annotation
The goal in the XCES is to provide a framework
for annotation that is theory and tagset
independent. We accomplish this by treating the
description of any specific syntactic annotation
scheme as a process involving several
knowledge sources that interact at various
levels. The process allows one to specify, on the
one hand, the informational properties of the
scheme (i.e., its capacity to represent a given
piece of information), and, on the other, the way
the scheme can be instantiated (e.g., as an XML
document). Figure 3 shows the overall
architecture of the XCES framework for
syntactic annotation.
                                                           
4
  So-called ?hybrid systems?  (e.g., Basili, et al, 199;
Grefenstette, 1999) combine constituency analysis and
functional dependencies, usually producing a shallow
constituent parse that brackets major phrase types and
identifying the dependencies between heads of
constituents.
Figure 3. Overall architecture of the XCES annotation framework
Two knowledge sources are used define the
abstract model:
Data Category Registry: Within the framework
of the XCES we are establishing an inventory of
data categories for syntactic annotation, initially
based on the EAGLES Recommendations for
Syntactic Annotation of Corpora (Leech et al,
1996). Data categories are defined using RDF
descriptions that formalize the properties
associated with each. The categories are
organized in a hierarchy, from general to
specific. For example, a general dependent
relation may be defined, which may have one of
the possible values argument or modifier;
argument in turn may have the possible values
subject, object, or complement; etc.5 Note that
RDF descriptions function much like class
definitions in an object-oriented programming
language: they provide, effectively, templates
that describe how objects may be instantiated,
but do not constitute the objects themselves.
Thus, in a document containing an actual
annotation, several objects with the type
argument  may be instantiated, each with a
different value. The RDF schema ensures that
each instantiation of argument is recognized as a
sub-class of dependent and inherits the
appropriate properties.
Structural Skeleton: a domain-dependent
abstract structural framework for syntactic
                                                           
5
 Cf. the hierarchy in Figure 1.1, Caroll, Minnen, and
Briscoe (forthcoming).
General Markup Language
XSLT Script
Dialect
Specification
DATA
CATEGORY
REGISTRY
Virtual
AML
Concrete
AML
Data
Category
Specification
STRUCTURAL
SKELETON
Abstract
XML
encoding
Concrete
XML
encoding
Non-XML
Encoding
Universal Resources
Project Specific Resources
annotations, capable of fully capturing all the
information in a specific annotation scheme. The
structural skeleton for syntactic annotations is
described below in section 12.1.
Two other knowledge sources are used to
define a project-specific format for the
annotation scheme, in terms of its expressive
power and its instantiation in XML:
Data Category Specification (DCS): describes
the set of data categories that can be used within
a given annotation scheme, again using RDF
schema. The DCS defines constraints on each
category, including restrictions on the values
they can take (e.g., "text with markup"; a
"picklist" for grammatical gender, or any of the
data types defined for XML), restrictions on
where a particular data category can appear
(level in the structural hierarchy). The DCS may
include a subset of categories from the DCR
together with application-specific categories
additionally defined in the DCS.  The DCS also
indicates a level of granularity based on the
DCR hierarchy.
Dialect specification: defines, using XML
schemas, XSLT scripts, and XSL style sheets,
the project-specific XML format for syntactic
annotations. The specifications may include:
?  Data category instantiation styles:  Data
categories may be realized in a project-
specific scheme in any of a variety of
formats. For example, if there exists a data
category NounPhrase, this may be realized
as an <NounPhrase> element (possibly
containing additional elements), a typed
element (e.g. <cat type=NounPhrase>), tag
content (e.g., <cat>NounPhrase</cat>), etc.
?  Data category vocabulary styles: Project-
specific formats can utilize names different
from those in the Data Category Registry;
for instance, a DCR specification for
NounPhrase can be expressed as ?NP? or
?SN? (? syntagme nominal?) in the project-
specific format, if desired.
?  Expansion structures: A project-specific
format may alter the structure of the
annotation as expressed using the structural
skeleton. For example, it may be desirable
for processing or other reasons to create two
sub-nodes under a given <struct> node, one
to group features and one to group relations.
The combination of the structural skeleton
and the DCS defines a virtual annotation
markup language (AML). Any information
structure that corresponds to a virtual AML has
a canonical expression as an XML document;
therefore, the inter-operability of different
AMLs is dependent only on their compatibility
at the virtual level. As such, virtual AML is the
hub of the annotation framework: it defines a
lingua franca for syntactic annotations that can
be used to compare and merge annotations, as
well as enable design of generic tools for
visualization, editing, extraction, etc.
The combination of a virtual AML with the
Dialect Specification provides the information
necessary to automatically generate a concrete
AML representation of the annotation scheme,
which conforms to the project-specific format
provided in the Dialect Specification. XSLT
filters translate between the representations of
the annotation in concrete and virtual AML, as
well as between non-XML formats (such as the
LISP-like PTB notation) and concrete AML.6
2.1 The Structural Skeleton
For syntactic annotation, we can identify a
general, underlying model that informs current
practice: specification of constituency relations
(with some set of application-specific names and
properties) among syntactic or grammatical
components (also with a set of application-
specific names and properties), whether this is
modeled with a tree structure or the relations are
given explicitly.
Because of the common use of trees in
syntactic annotation, together with the natural
tree-structure of markup in XML documents, we
provide a structural skeleton for syntactic
markup following this model. The most
important element in the skeleton is the
<struct> element, which represents a node
(level) in the syntax tree. <struct> elements may
be recursively nested at any level to reflect the
structure of the corresponding tree. The <struct>
element has the following attributes:
                                                           
6
  Strictly speaking, an application-specific format could be
translated directly into the virtual AML, eliminating the
need for the intermediary concrete AML format. However,
especially for existing formats, it is typically more
straightforward to perform the two-step process.
?  type : specifies the node label (e.g., ?S?,
?NP?, etc.) or points to an object in another
document that provides the value. This
allows specifying complex data items as
annotations. It also enables generating a
single instantiation of an annotation value in
a separate document that can be referenced as
needed.
?  xlink : points to the data to which the
annotation applies. In the XCES, we
recommend the use of s t a n d - o f f
a n n o t a t i o n ? i .e., annotation that is
maintained in a document separate from the
primary (annotated) data.7 The xlink attribute
uses the XML Path Language (XPath) (Clark
& DeRose, 1999) to specify the location of
the relevant data in the primary document.
? ref : refers to a node defined elsewhere, used
instead of xlink.
? rel?: specifies a type of relation (e.g., ?subj?)
?  head : specifies the node corresponding to
the head of the relation
? dependent : specifies the node corresponding
to the dependent of the relation
? introducer : specifies the node corresponding
to an introducing word or phrase
? initial : gives a thematic or semantic role of a
component, e.g., ?subj? for the object of a
by-phrase in a passive sentence.
The hierarchy of <struct> elements
corresponds to the nodes in a phrase structure
analysis; each <struct> element is typed
accordingly. The grammar underlying the
annotation  therefore specifies constraints on
embedding that can be instantiated in an XML
schema, which can then be used to prevent or
detect tree structures that do not conform to the
grammar. Conversely, the grammar rules
implicit in annotated treebanks, which are
typically not annotated according to a formal
grammar, can be easily extracted from the
abstract structural encoding.
The skeleton also includes a <feat> (feature)
element, which can be used to provide
additional information (e.g., gender, number)
that is attached to the node in the tree
represented by the enclosing <struct> element.
Like <struct>, this element can be recursively
nested or can point to a description in another
                                                           
7
 The stand-off scheme also provides means to represent
ambiguities, since there can be multiple links between data
and alternative annotations.
document, thereby providing means to associate
information at any level of detail or complexity
to the annotated structure.
Figure 4 shows the annotation from the PTB
(Figure 1) rendered in the abstract XML format.
Note that in this example, relations are encoded
only when they appear explicitly in the original
annotation (therefore, heads of relations default
to ?unknown?.)  An XSLT script could be used
to create a second XML document that includes
the relations implicit in the embedding (e.g., the
first embedded <struct> with category NP has
relation ?subject?, the first VP is the head, etc.).
A strict dependency annotation encoded in the
abstract format uses a flat hierarchy and
specifies all relations explicitly with the rel
attribute, as shown in Figure 5.8
4 Using the XCES Scheme
The Virtual AML provides a pivot format that
enables comparison of annotations in different
formats ? including not only different
constituency-based annotations, but also
constituency-based and dependency annotations.
For example, the PTB annotation corresponding
to the dependency annotation in Figure 2 is
shown in Figure 6. Figure 7 gives the
corresponding encoding in the XCES abstract
scheme. It is relatively trivial with an XSLT
script to extract the information in the
dependency annotation  (Figure 5) from the PTB
encoding (Figure 7) to produce a nearly identical
dependency encoding. The script would use
rules to make relations that are implicit in the
structure of the P T B encoding explicit (for
example, the ?xcomp? relation  that is implicit in
the embedding of the ?S? phrase).
The ability to generate a common
representation for different annotations
overcomes several obstacles that have hindered
evaluation exercises in the past. For instance, the
evaluation technique used in the PARSEVAL
exercise is applicable to phrase structure
analyses only, and cannot be applied to
dependency-style analyses or ?lexical? parsing
frameworks such as finite-state constraint
parsers. As the example above shows, this
                                                           
8
 For the sake of readability, this encoding assumes that the
sentence ?Paul intends to leave IBM? is marked up as
<s1><w1>Paul</w1><w2>intends</w2><w3>to</w3><w
4>leave</w4><w5>IBM</w5></s1>.
problem can be addressed using the XCES
framework.
It has also been noted that that the PARSEVAL
bracket-precision measure penalizes parsers that
return more structure than exists in the relatively
?flat? treebank structures, even if they are
correct (Srinivas, et al, 1995). XSLT scripts can
extract the appropriate information for
comparison purposes while retaining links to
additional parts of the annotation in the original
document, thus eliminating the need to ?dumb
down? parser output in order to participate in the
evaluation exercise. Similarly, information lost
in the transduction from phrase structure to a
dependency-based analysis (as in the example
above), which, as Atwell (1996) points out, may
eliminate grammatical information potentially
required for later processing, can also be
retained.
((S (NP-SBJ-1 Paul)
(VP intends)
(S (NP-SBJ *-1)
(VP  to
                (VP  leave
        (NP IBM))))
.))
Figure 6. PTB annotation of "Paul intends to
leave IBM.
<struct id="s0" type="S">
 <struct id="s1" type="NP"
          xlink:href="xptr(substring(/p/s[1]/text(),1,5))"
          rel ="SBJ"/>
 <struct id="s2" type="VP"
          xlink:href="xptr(substring(/p/s[1]/text(),7,8))"/>
 <struct id="s3" type="NP"
          xlink:href="xptr(substring(/p/s[1]/text(),16,3))"/>
 <struct id="s4" type="PP"
          xlink:href="xptr(substring(/p/s[1]/text(),20,4))"
          rel="DIR">
  <struct id="s5" type="NP"
           xlink:href="xptr(substring(/p/s[1]/text(),25,14))"/>
 </struct>
 <struct id="s6" type="S" rel="ADV">
     <struct id="s7" ref="s1" type="NP" rel="SBJ"/>
     <struct id="s8" type="VP"
             xlink:href="xptr(substring(/p/s[1]/text(),41,7))">
         <struct id="s9" type="NP"
                  xlink:href="xptr(substring(/p/s[1]/text(),49,8))"/>
         <struct id="s10" type="PP" rel="DIR"
                  xlink:href="xptr(substring(/p/s[1]/text(),57,6))">
             <struct id="s11" type="NP"
                      xlink:href="xptr(substring(/p/s[1]/text(),64,3))"/>
            </struct>
     </struct>
   </struct>
</struct>
Figure 4. The PTB example encoded according to the structural skeleton
<struct rel="subj"  head="w2" dependent="w1"/>
<struct rel="xcomp" head="w2" dependent="w4"  introducer="w3"/>
<struct rel="subj"  head="w4" dependent="w1"/>
<struct rel="dobj"  head="w4" dependent="w5"/>
Figure 5. Abstract XML encoding for the  dependency annotation in Figure 2.
<struct id="s0" type="S?>
        <struct id="s1" type="NP? target="w1?
                rel="SBJ" head="s2"/>
        <struct id="s2" type="VP? target="w2"/>
        <struct id="s3" type="S?>
                <struct id="s4" ref="s1"
                        rel="SBJ" head="s6"/>
                <struct id="s5" type="VP? target="w3">
                        <struct id="s6" type="VP? target="w4">
                                <struct id=?s7? type="NP? target="w5"/>
                        </struct>
                </struct>
        </struct>
</struct>
Figure 4 : PTB encoding of "Paul intends to leave IBM."
5 Discussion
Despite its seeming complexity, the XCES
framework is designed to reduce overhead for
annotators and users. Part of the work of the
XCES is to provide XML support (e.g.,
development of XSLT scripts, XML schemas,
etc.) for use by the research community, thus
eliminating the need for XML expertise at
each development site. Because XML-
encoded annotated corpora are increasingly
used for interchange between processing and
analytic tools, we are developing XSLT
scripts for mapping, and extraction of
annotated data, import/export of (partially)
annotated material, and integration of results
of external tools into existing annotated data
in XML. Tools for editing annotations in the
abstract format, which automatically generate
virtual AML from Data Category and Dialect
Specifications, are already under development
in the context of work on the Terminological
Markup Language, and a tool for
automatically generating RDF specifications
for user-specified data categories has already
been developed in the SALT project.9 Several
freely distributed interpreters for XSLT have
also been developed (e.g., xt10, Xalan11). In
practice, annotators and users of annotated
corpora will rarely see XML and RDF
instantiations of annotated data; rather, they
will access the data via interfaces that
automatically generate, interpret, and display
the data in easy-to-read formats.
                                                           
9
  http://www.loria.fr/projets/SALT
10
 Clark, J., 1999. XT Version 1991105.
http://www.jclark.com/xml/xt.html
11
 http://www.apache.org
The abstract model that captures the
fundamental properties of syntactic annotation
schemes provides a conceptual tool for
assessing the coherence and consistency of
existing schemes and those being developed.
The model enforces clear distinctions between
implicit and explicit information (e.g.,
functional relations implied by structural
relations in constituent analyses), and phrasal
and functional relations. It is alarmingly
common for annotation schemes to represent
these different kinds of information in the
same way, rendering their distinction
computationally intractable (even if they are
perfectly understandable by the informed
human reader).  Hand-developed annotation
schemes used in treebanks are often described
informally in guidebooks for annotators,
leaving considerable room for variation; for
example, Charniak (1996) notes that the PTB
implicitly contains more than 10,000 context-
free rules, most of which are used only once.
Comparison and transduction of schemes
becomes virtually impossible under such
circumstances. While requiring that annotators
make relations explicit and consider the
mapping to the XCES abstract format
increases overhead, we feel that the exercise
will help avoid such problems and can only
lead to greater coherence, consistency, and
inter-operability among annotation schemes.
The most important contribution to inter-
operability of annotation schemes is the Data
Category Registry. By mapping site-specific
categories onto definitions in the Registry,
equivalences (and non-equivalences) are made
explicit. Again, the provision of a ?standard?
set of categories, together with the
requirement that scheme-specific categories
are mapped to them where possible, will
contribute to greater consistency and
commonality among annotation schemes.
6 Conclusion
The XCES framework for linguistic
annotation is built around some relatively
straightforward ideas: separation of
information conveyed by means of structure
and information conveyed directly by
specification of content categories;
development of an abstract format that puts a
layer of abstraction between site-specific
annotat ion schemes and standard
specifications; and creation of a Data
Category Registry to provide a reference set
of annotation categories. The emergence of
XML and related standards such as RDF
provides the enabling technology. We are,
therefore, at a point where the creation and
use of annotated data and concerns about the
way it is represented can be treated
separately?that is, researchers can focus on
the question of what to encode, independent of
the question of how  to encode it. The end
result should be greater coherence,
consistency, and ease of use and access for
annotated data.
References
Anne Abeill? (ed.), forthcoming.  Treebanks:
Building and Using Syntactically Annotated
Corpora, Kluwer Academic Publishers.
Eric Atwell, 1996. Comparative evaluation of
grammatical annotation models. In R. Sutcliffe,
H. Koch, A. McElligott (eds.), Industrial
Parsing of Software Manuals, 25-46. Rodopi.
Paul Biron and Ashok Malhotra, 2000. XML
Schema Part 2: Datatypes. W3C Candidate
Recommendation.
http://www.w3.org/TR/xmlschema-2/.
Tim Bray, Jean Paoli and C. Michael Sperberg-
McQueen (eds.), 1998. Extensible Markup
Language (XML).
Dan Brickley and R.V. Guha, 2000. Resource
Description Framework (RDF) Schema
Specification 1.0. http://www.w3.org/TR/rdf-
schema/.
John Carroll, Guido Minnen, and Ted Briscoe,
forthcoming. Parser Evaluation Using a
Grammatical Relation Annotation Scheme. In
Anne Abeill? (ed.)  Treebanks: Building and
Using Syntactically Annotated Corpora, Kluwer
Academic Publishers.
Eugene Charniak, 1996. Tree-bank grammars.
Proceedings of the 13th National Conference on
Artificial Intelligence, AAAI?96, 1031-36.
James Clark (ed.), 1999. XSL Transformations
(XSLT). http://www.w3.org/TR/xslt.
James Clark and Steven DeRose, 1999. XML Path
Language. http://www.w3.org/TR/xpath.
Philip Harrison, Steven Abney, Ezra Black, Dan
Flickinger, Claudia Gdaniec, Ralph Grishman,
Don Hindle, Bob Ingria, Mitch Marcus,
Beatrice Santorini, and Tomek Strzalkowski,
1991. Evaluating syntax performance of
parser/grammars of English. Proceedings of the
Workshop on Evaluating Natural Language
Processing Systems, 71-77.
Nancy Ide,  Patrice Bonhomme, and Laurent
Romary, 2000. XCES: An XML-based Standard
for Linguistic Corpora. Proceedings of the
Second Language Resources and Evaluation
Conference (LREC), 825-30.
Nancy Ide, Adam Kilgarriff, and Laurent Romary,
2000. A Formal Model of Dictionary Structure
and Content. In Proceedings of EURALEX?00,
113-126.
Ora Lassila and Ralph Swick, 1999. Resource
Description framework (RDF) Model and
Syntax. http://www.w3.org/TR/REC-rdf-syntax.
Geoffrey Leech, R. Barnett, and P. Kahrel, 1996.
EAGLES Recommendations for the Syntactic
Annotation of Corpora.
Daniel Sleator and Davy Temperley, 1993. Parsing
English with a link grammar. T h i r d
International Workshop on Parsing
Technologies.
Bangalore Srinivas, Christy Doran, Beth-Ann
Hockey and Avarind Joshi,  1996. An approach
to robust partial parsing and evaluation metrics.
Proceedings of the ESSLI?96 Workshop on
Robust Parsing,  70-82.
Pasi Tapanainen and Timo J?rvinen. 1997.  A non-
projective dependency parser. Proceedings of
the 5th Conference on Applied Natural
Language Processing (ANLP?97), 64-71.
Henry Thompson, David Beech, Murray Maloney,
and Noah Mendelsohn, 2000. XML Schema
Part 1: Structures.
 http://www.w3.org/TR/xmlschema-1/.
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 11?15,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Collaborative Machine Translation Service for Scientific texts
Patrik Lambert
University of Le Mans
patrik.lambert@lium.univ-lemans.fr
Jean Senellart
Systran SA
senellart@systran.fr
Laurent Romary
Humboldt Universita?t Berlin /
INRIA Saclay - Ile de France
laurent.romary@inria.fr
Holger Schwenk
University of Le Mans
holger.schwenk@lium.univ-lemans.fr
Florian Zipser
Humboldt Universita?t Berlin
f.zipser@gmx.de
Patrice Lopez
Humboldt Universita?t Berlin /
INRIA Saclay - Ile de France
patrice.lopez@inria.fr
Fre?de?ric Blain
Systran SA /
University of Le Mans
frederic.blain@
lium.univ-lemans.fr
Abstract
French researchers are required to fre-
quently translate into French the descrip-
tion of their work published in English. At
the same time, the need for French people
to access articles in English, or to interna-
tional researchers to access theses or pa-
pers in French, is incorrectly resolved via
the use of generic translation tools. We
propose the demonstration of an end-to-end
tool integrated in the HAL open archive for
enabling efficient translation for scientific
texts. This tool can give translation sugges-
tions adapted to the scientific domain, im-
proving by more than 10 points the BLEU
score of a generic system. It also provides
a post-edition service which captures user
post-editing data that can be used to incre-
mentally improve the translations engines.
Thus it is helpful for users which need to
translate or to access scientific texts.
1 Introduction
Due to the globalisation of research, the English
language is today the universal language of sci-
entific communication. In France, regulations re-
quire the use of the French language in progress
reports, academic dissertations, manuscripts, and
French is the official educational language of the
country. This situation forces researchers to fre-
quently translate their own articles, lectures, pre-
sentations, reports, and abstracts between English
and French. In addition, students and the general
public are also challenged by language, when it
comes to find published articles in English or to
understand these articles. Finally, international
scientists not even consider to look for French
publications (for instance PhD theses) because
they are not available in their native languages.
This problem, incorrectly resolved through the
use of generic translation tools, actually reveals
an interesting generic problem where a commu-
nity of specialists are regularly performing trans-
lations tasks on a very limited domain. At the
same time, other communities of users seek trans-
lations for the same type of documents. Without
appropriate tools, the expertise and time spent for
translation activity by the first community is lost
and do not benefit to translation requests of the
other communities.
We propose the demonstration of an end-to-end
tool for enabling efficient translation for scientific
texts. This system, developed for the COSMAT
ANR project,1 is closely integrated into the HAL
open archive,2 a multidisciplinary open-access
archive which was created in 2006 to archive pub-
lications from all the French scientific commu-
nity. The tool deals with handling of source doc-
ument format, generally a pdf file, specialised
translation of the content, and user-friendly user-
interface allowing to post-edit the output. Behind
1http://www.cosmat.fr/
2http://hal.archives-ouvertes.fr/?langue=en
11
the scene, the post-editing tool captures user post-
editing data which are used to incrementally im-
prove the translations engines. The only equip-
ment required by this demonstration is a computer
with an Internet browser installed and an Internet
connection.
In this paper, we first describe the complete
work-flow from data acquisition to final post-
editing. Then we focus on the text extraction pro-
cedure. In Section 4, we give details about the
translation system. Then in section 5, we present
the translation and post-editing interface. We fi-
nally give some concluding remarks.
The system will be demonstrated at EACL in
his tight integration with the HAL paper deposit
system. If the organizers agree, we would like to
offer the use of our system during the EACL con-
ference. It would automatically translate all the
abstracts of the accepted papers and also offers
the possibility to correct the outputs. This result-
ing data would be made freely available.
2 Complete Processing Work-flow
The entry point for the system are ?ready to pub-
lish? scientific papers. The goal of our system
was to extract content keeping as many meta-
information as possible from the document, to
translate the content, to allow the user to perform
post-editing, and to render the result in a format as
close as possible to the source format. To train our
system, we collected from the HAL archive more
than 40 000 documents in physics and computer
science, including articles, PhD theses or research
reports (see Section 4). This material was used to
train the translation engines and to extract domain
bilingual terminology.
The user scenario is the following:
? A user uploads an article in PDF format3 on
the system.
? The document is processed by the open-
source Grobid tool (see section 3) to extract
3The commonly used publishing format is PDF files
while authoring format is principally a mix of Microsoft
Word file and LaTeX documents using a variety of styles.
The originality of our approach is to work on the PDF file
and not on these source formats. The rationale being that 1/
the source format is almost never available, 2/ even if we had
access to the source format, we would need to implement a
filter specific to each individual template required by such or
such conference for a good quality content extraction
the content. The extracted paper is structured
in the TEI format where title, authors, refer-
ences, footnotes, figure captions are identi-
fied with a very high accuracy.
? An entity recognition process is performed
for markup of domain entities such as:
chemical compounds for chemical papers,
mathematical formulas, pseudo-code and ob-
ject references in computer science papers,
but also miscellaneous acronyms commonly
used in scientific communication.
? Specialised terminology is then recognised
using the Termsciences4 reference termi-
nology database, completed with terminol-
ogy automatically extracted from the train-
ing corpus. The actual translation of the pa-
per is performed using adapted translation as
described in Section 4.
? The translation process generates a bilingual
TEI format preserving the source structure
and integrating the entity annotation, multi-
ple terminology choices when available, and
the token alignment between source and tar-
get sentences.
? The translation is proposed to the user for
post-editing through a rich interactive inter-
face described in Section 5.
? The final version of the document is then
archived in TEI format and available for dis-
play in HTML using dedicated XSLT style
sheets.
3 The Grobid System
Based on state-of-the-art machine learning tech-
niques, Grobid (Lopez, 2009) performs reliable
bibliographic data extraction from scholar articles
combined with multi-level term extraction. These
two types of extraction present synergies and cor-
respond to complementary descriptions of an arti-
cle.
This tool parses and converts scientific arti-
cles in PDF format into a structured TEI docu-
ment5 compliant with the good practices devel-
oped within the European PEER project (Bretel et
al., 2010). Grobid is trained on a set of annotated
4http://www.termsciences.fr
5http://www.tei-c.org
12
scientific article and can be re-trained to fit tem-
plates used for a specific conference or to extract
additional fields.
4 Translation of Scientific Texts
The translation system used is a Hybrid Machine
Translation (HMT) system from French to En-
glish and from English to French, adapted to
translate scientific texts in several domains (so
far physics and computer science). This sys-
tem is composed of a statistical engine, cou-
pled with rule-based modules to translate spe-
cial parts of the text such as mathematical for-
mulas, chemical compounds, pseudo-code, and
enriched with domain bilingual terminology (see
Section 2). Large amounts of monolingual and
parallel data are available to train a SMT system
between French and English, but not in the scien-
tific domain. In order to improve the performance
of our translation system in this task, we extracted
in-domain monolingual and parallel data from the
HAL archive. All the PDF files deposited in HAL
in computer science and physics were made avail-
able to us. These files were then converted to
plain text using the Grobid tool, as described in
the previous section. We extracted text from all
the documents from HAL that were made avail-
able to us to train our language model. We built
a small parallel corpus from the abstracts of the
PhD theses from French universities, which must
include both an abstract in French and in English.
Table 1 presents statistics of these in-domain data.
The data extracted from HAL were used to
adapt a generic system to the scientific litera-
ture domain. The generic system was mostly
trained on data provided for the shared task of
Sixth Workshop on Statistical Machine Transla-
tion6 (WMT 2011), described in Table 2.
Table 3 presents results showing, in the
English?French direction, the impact on the sta-
tistical engine of introducing the resources ex-
tracted from HAL, as well as the impact of do-
main adaptation techniques. The baseline statis-
tical engine is a standard PBSMT system based
on Moses (Koehn et al 2007) and the SRILM
tookit (Stolcke, 2002). Is was trained and tuned
only on WMT11 data (out-of-domain). Incorpo-
rating the HAL data into the language model and
tuning the system on the HAL development set,
6http://www.statmt.org/wmt11/translation-task.html
Set Domain Lg Sent. Words Vocab.
Parallel data
Train cs+phys En 55.9 k 1.41 M 43.3 k
Fr 55.9 k 1.63 M 47.9 k
Dev cs En 1100 25.8 k 4.6 k
Fr 1100 28.7 k 5.1 k
phys En 1000 26.1 k 5.1 k
Fr 1000 29.1 k 5.6 k
Test cs En 1100 26.1 k 4.6 k
Fr 1100 29.2 k 5.2 k
phys En 1000 25.9 k 5.1 k
Fr 1000 28.8 k 5.5 k
Monolingual data
Train cs En 2.5 M 54 M 457 k
Fr 761 k 19 M 274 k
phys En 2.1 M 50 M 646 k
Fr 662 k 17 M 292 k
Table 1: Statistics for the parallel training, develop-
ment, and test data sets extracted from thesis abstracts
contained in HAL, as well as monolingual data ex-
tracted from all documents in HAL, in computer sci-
ence (cs) and physics (phys). The following statistics
are given for the English (En) and French (Fr) sides
(Lg) of the corpus: the number of sentences, the num-
ber of running words (after tokenisation) and the num-
ber of words in the vocabulary (M and k stand for mil-
lions and thousands, respectively).
yielded a gain of more than 7 BLEU points, in
both domains (computer science and physics). In-
cluding the theses abstracts in the parallel training
corpus, a further gain of 2.3 BLEU points is ob-
served for computer science, and 3.1 points for
physics. The last experiment performed aims at
increasing the amount of in-domain parallel texts
by translating automatically in-domain monolin-
gual data, as suggested by Schwenk (2008). The
synthesised bitext does not bring new words into
the system, but increases the probability of in-
domain bilingual phrases. By adding a synthetic
bitext of 12 million words to the parallel training
data, we observed a gain of 0.5 BLEU point for
computer science, and 0.7 points for physics.
Although not shown here, similar results were
obtained in the French?English direction. The
French?English system is actually slightly bet-
ter than the English?French one as it is an easier
translation direction.
13
Translation Model Language Model Tuning Domain CS PHYS
words (M) Bleu words (M) Bleu
wmt11 wmt11 wmt11 371 27.3 371 27.1
wmt11 wmt11+hal hal 371 36.0 371 36.2
wmt11+hal wmt11+hal hal 287 38.3 287 39.3
wmt11+hal+adapted wmt11+hal hal 299 38.8 307 40.0
Table 3: Results (BLEU score) for the English?French systems. The type of parallel data used to train the
translation model or language model are indicated, as well as the set (in-domain or out-of-domain) used to tune
the models. Finally, the number of words in the parallel corpus and the BLEU score on the in-domain test set are
indicated for each domain: computer science and physics.
Figure 1: Translation and post-editing interface.
Corpus English French
Bitexts:
Europarl 50.5M 54.4M
News Commentary 2.9M 3.3M
Crawled (109 bitexts) 667M 794M
Development data:
newstest2009 65k 73k
newstest2010 62k 71k
Monolingual data:
LDC Gigaword 4.1G 920M
Crawled news 2.6G 612M
Table 2: Out-of-domain development and training data
used (number of words after tokenisation).
5 Post-editing Interface
The collaborative aspect of the demonstrated ma-
chine translation service is based on a post-editing
tool, whose interface is shown in Figure 1. This
tool provides the following features:
? WYSIWYG display of the source and target
texts (Zones 1+2)
? Alignment at the sentence level (Zone 3)
? Zone to review the translation with align-
ment of source and target terms (Zone 4) and
terminology reference (Zone 5)
? Alternative translations (Zone 6)
The tool allows the user to perform sentence
level post-editing and records details of post-
editing activity, such as keystrokes, terminology
selection, actual edits and time log for the com-
plete action.
6 Conclusions and Perspectives
We proposed the demonstration of an end-to-end
tool integrated into the HAL archive and enabling
14
efficient translation for scientific texts. This tool
consists of a high-accuracy PDF extractor, a hy-
brid machine translation engine adapted to the sci-
entific domain and a post-edition tool. Thanks to
in-domain data collected from HAL, the statisti-
cal engine was improved by more than 10 BLEU
points with respect to a generic system trained on
WMT11 data.
Our system was deployed for a physic confer-
ence organised in Paris in Sept 2011. All accepted
abstracts were translated into author?s native lan-
guages (around 70% of them) and proposed for
post-editing. The experience was promoted by
the organisation committee and 50 scientists vol-
unteered (34 finally performed their post-editing).
The same experience will be proposed for authors
of the LREC conference. We would like to offer
a complete demonstration of the system at EACL.
The goal of these experiences is to collect and dis-
tribute detailed ?post-editing? data for enabling
research on this activity.
Acknowledgements
This work has been partially funded by the French
Government under the project COSMAT (ANR
ANR-09-CORD-004).
References
Foudil Bretel, Patrice Lopez, Maud Medves, Alain
Monteil, and Laurent Romary. 2010. Back to
meaning ? information structuring in the PEER
project. In TEI Conference, Zadar, Croatie.
Philipp Koehn, Hieu Hoang, Alexandra Birch,
Chris Callison-Burch, Marcello Federico, Nicola
Bertoldi, Brooke Cowan, Wade Shen, Christine
Moran, Richard Zens, Chris Dyer, Ondrej Bojar,
Alexandra Constantin, and Evan Herbst. 2007.
Moses: Open source toolkit for statistical ma-
chine translation. In Proc. of the 45th Annual
Meeting of the Association for Computational Lin-
guistics (Demo and Poster Sessions), pages 177?
180, Prague, Czech Republic, June. Association for
Computational Linguistics.
Patrice Lopez. 2009. GROBID: Combining auto-
matic bibliographic data recognition and term ex-
traction for scholarship publications. In Proceed-
ings of ECDL 2009, 13th European Conference on
Digital Library, Corfu, Greece.
Holger Schwenk. 2008. Investigations on large-scale
lightly-supervised training for statistical machine
translation. In IWSLT, pages 182?189.
A. Stolcke. 2002. SRILM: an extensible language
modeling toolkit. In Proc. of the Int. Conf. on Spo-
ken Language Processing, pages 901?904, Denver,
CO.
15
Book Review
Natural Language Processing for Historical Texts
Michael Piotrowski
(Leibniz Institute of European History)
Morgan & Claypool (Synthesis Lectures on Human Language Technologies, edited by
Graeme Hirst, volume 17), 2012, ix+157 pp; paperbound, ISBN 978-1608459469
Reviewed by
Laurent Romary
Inria & Humboldt University, Berlin
The publication of a scholarly book is always the conjunction of an author?s desire (or
need) to disseminate their experience and knowledge and the interest or expectations
of a potential community of readers to gain benefit from the publication itself. Michael
Piotrowski has indeed managed to optimize this relation by bringing to the public a
compendium of information that I think has been heavily awaited by many scholars
having to deal with corpora of historical texts. The book covers most topics related to the
acquisition, encoding, and annotation of historical textual data, seen from the point of
view of their linguistic content. As such, it does not address issues related, for instance,
to scholarly editions of these texts, but conveys a wealth of information on the various
aspects where recent developments in language technology may help digital humanities
projects to be aware of the current state of the art in the field.
? 2014 Association for Computational Linguistics
Still, the book is not an encyclopedic description of such technologies. It is based
on the experience acquired by the author within the corpus development projects he
has been involved in, and reflects in particular the specific topics on which he has made
more in-depth explorations. It is thus written more as a series of returns on experience
than a systematic resource to which one would want to return after its initial reading.
The book is organized as a series of nine short chapters.
In the first two (very short) chapters, the author presents the general scope of
the book and provides an overview of the reasons why natural language processing
(NLP) has such an entrenched position in digital humanities at large and the study of
historical text in particular. Citing several prominent projects and corpus initiatives that
have taken place in the last few decades, Piotrowski defends the thesis, which I share,
that a deep understanding of textual documents requires some basic knowledge of
language processing methods and techniques. Chapter 2 in particular (?NLP and Digital
Humanities?) could be read as an autonomous position paper, which, independently of
the following chapters, presents the current landscape of infrastructural initiatives and
scholarly projects that shape this convergence between the two fields.
Chapter 3 (?Spelling in Historical Texts,? pp. 11?23) describes the various issues re-
lated to spelling variations in historical text. It shows how difficult it may be to deal with
both diachronic (e.g., in comparison to modern standardized spellings) and synchronic
(degree of stabilization of historical spellings) variations, especially in the context of the
uncertainty brought about by the transcription process itself. This is particularly true
for historical manuscripts and Piotrowski goes deeply into this, showing some concrete
doi:10.1162/COLI r 00180
Computational Linguistics Volume 40, Number 1
examples of the kind of hurdles that a scholar may fall into. This is the kind of short
introduction I would recommend for anyone, in particular students, wanting to gain a
first understanding in the domain of historical spelling.
Chapter 4 is the longest chapter in the book (?Acquiring Historical Texts,? pp. 25?
52) and covers various aspects of the digitization workflow that needs to be set up to
create a corpus of historical texts. The chapter is quite difficult to read as a single unit
because of its intrinsic heterogeneity. Indeed, it covers quite a wide range of topics:
presentation of existing digitization projects worldwide, technical issues related to
scanning, comparison of various optical character recognition systems for various types
of scripts, the potential role of lexical resources, crowdsourcing for optical character
recognition (OCR) post-processing, and manual or semi-automatic keying. Getting an
overview of the various topics is even more difficult because of the way the author has
followed his own personal experience, and alternates between general considerations
and in-depth presentations of concrete results. Pages 34?40, for instance, is one single
subsection on the comparison of OCR outputs that goes into so much detail that it
breaks the continuity of the argument, although in itself this subsection could be really
interesting for a specialized reader. This chapter illustrates the point that the content of
this book would benefit from being published in a more modern and open setting.
Data representation aspects are covered in Chapter 5 (?Text Encoding and
Annotation Schemes,? pp. 53?68), which tackles two specific issues, namely, character
and document encoding. On these two, the author presents what could be considered
best practices. For character encoding, the book rightly focuses on the advantages
that the move towards ISO 10646/Unicode has brought to the community. The
corresponding sub-section actually covers three different aspects: It first makes an
extensive presentation of the history of character encoding standards (from ASCII/ISO
646 to Unicode/ISO 10646), it provides insights into the current coverage and encoding
principles (e.g., UTF-8 vs. UTF-16) of ISO 10646, and finally, it focuses on the specific
difficulties occurring in historical texts both from the point of view of legacy ASCII-
based transcription languages and the management of characters that are not present
in Unicode. Although well documented, these three topics should have been more
clearly separated so that readers interested in one or the other could directly refer to
it. This is a typical case where, given the great expertise of the author on the subject,
I can imagine the corresponding texts being published on-line as separate entries in a
blog. The second half of the chapter focuses on the role of the Text Encoding Initiative
(TEI) guidelines for the transcription and encoding of historical text. It covers the
various representation levels that may be concerned (metadata, text structure, surface
annotation) and insists on the current difficulty of linking current NLP tools to TEI
encoded documents. Although this is indeed still an issue in general, it might have
been interesting to refer to standards (ISO 24611? MAF) and initiatives (Textgrid core
encoding at the token level; the TXM platform for text mining) that have started to
provide concrete sustainable answers to the issue.
The following chapter (?Handling Spelling Variations,? pp. 69?84), provides a series
of short studies describing possible methods for dealing with OCR errors or spelling
variations as described in Chapter 3. Independent of the fact that I find it strange to
see the two chapters set quite far from one another, Chapter 6 distinguishes itself by its
profound heterogeneity. Whereas several sections do have the most appropriate level
of detail and topicality for historical texts (in particular those on canonicalization),
some sections seem to be completely off topic (Section 6.2, ?Edit Distance,? describes
what I would consider as background knowledge for such a book). It is all the more
disappointing that the author shows here a very high level of expertise and, as in the
232
Book Review
case of Chapter 3, I would strongly recommend the reading of the relevant sections to
newcomers in the field.
In contrast with the previous chapter, Chapter 7 (?NLP Tools for Historical
Languages,? pp. 85?100) is more coherent and focused. It mainly addresses the morpho-
syntactic analysis of historical text and presents, through concrete deployment scenar-
ios, possible methods to constrain the appropriate parsers, in a context where hardly any
existing tools can be simply re-used. The chapter is very well documented and refers
to most of the relevant initiatives in the domain of morphology for historical text, at
least on the European scene. This focus may also be misleading because recent work on
named entity recognition on historical texts are not at all mentioned and are probably,
to my view, one of the most promising direction for enhanced digital scholarship.
The last chapter (?Historical Corpora,? pp. 101?116) is a compendium, sorted by
language, of the major historical corpora available worldwide. It shows the dynamic
that currently exists in the community and is an essential background resource to both
understanding who is active in maintaining historical corpora and discerning the most
relevant resources. The chapter as a whole provides an interesting ?historical? per-
spective on the progress made by most text-based projects in using the TEI guidelines
as their reference standard. It seems quite difficult now to imagine an initiative which
would not take TEI for granted, and would not build inside the TEI framework. On
another issue, namely, copyright, Piotrowski also provides an interesting analysis on
the difficulty of re-using old editions which have been recently re-edited on paper, and
thus fall into some publisher?s copyright restrictions. The conclusion could have been a
little tougher here though, and probably should have recommended putting a hold on
any paper publication of historical sources by a private publisher unless it is guaranteed
that the electronic material can be used freely, under an appropriate open license.
As a whole, the book leaves the reader with a mixed feeling of enthusiasm and
disappointment. Enthusiasm, because the content is so rich that it should serve as back-
ground reference (and indeed be quoted) for any further work on the creation, manage-
ment, and curation of historical corpora. Still, I cannot help thinking that the editorial
setting as a book is not the most appropriate setting for such content. The variety of
topics that are addressed as well as the heterogeneous level of detail provided through
the different chapters would benefit from a more fragmented treatment. Indeed, this
would be the perfect content for a series of blog entries (for instance, in a scholarly
blog such as those on the hypotheses.org platform) which in turn would allow an
interested reader to discover exactly the topics they want information about and cite
the corresponding entries. With the bibliography in Zotero and relevant pointers to
the corresponding on-line corpora or tools, I could imagine the resulting content soon
becoming one of the most cited on-line resources. I am sure the author would gain more
visibility in doing so than having the material hidden on a library shelf or behind a
paywall. Not knowing the exact copyright transfer agreement associated with the book,
I cannot judge if it is too late for the author to think in these terms, but this could be a
lesson for scholars who are now planning to write such an introductory publication. Is
the book still the best medium?
This book review was edited by Pierre Isabelle.
Laurent Romary is director of research at Inria, France, and guest scientist at the Humboldt Uni-
versity in Berlin. He has been involved for many years in language resource modeling activities
and in particular in standardization initiatives in the TEI consortium and ISO committee TC
37/SC 4 (language resource management). He is the director of the European DARIAH digital
infrastructure in the humanities. e-mail: laurent.romary@inria.fr.
233

Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 248?251,
Uppsala, Sweden, 15-16 July 2010.
c?2010 Association for Computational Linguistics
HUMB: Automatic Key Term Extraction from Scientific Articles
in GROBID
Patrice Lopez
INRIA
Berlin, Germany
patrice lopez@hotmail.com
Laurent Romary
INRIA & HUB-IDSL
Berlin, Germany
laurent.romary@inria.fr
Abstract
The Semeval task 5 was an opportunity
for experimenting with the key term ex-
traction module of GROBID, a system for
extracting and generating bibliographical
information from technical and scientific
documents. The tool first uses GROBID?s
facilities for analyzing the structure of sci-
entific articles, resulting in a first set of
structural features. A second set of fea-
tures captures content properties based on
phraseness, informativeness and keyword-
ness measures. Two knowledge bases,
GRISP and Wikipedia, are then exploited
for producing a last set of lexical/semantic
features. Bagged decision trees appeared
to be the most efficient machine learning
algorithm for generating a list of ranked
key term candidates. Finally a post rank-
ing was realized based on statistics of co-
usage of keywords in HAL, a large Open
Access publication repository.
1 Introduction
Key terms (or keyphrases or keywords) are meta-
data providing general information about the con-
tent of a document. Their selection by authors
or readers is, to a large extent, subjective which
makes automatic extraction difficult. This is, how-
ever, a valuable exercise, because such key terms
constitute good topic descriptions of documents
which can be used in particular for information
retrieval, automatic document clustering and clas-
sification. Used as subject headings, better key-
words can lead to higher retrieval rates of an arti-
cle in a digital library.
We view automatic key term extraction as a sub-
task of the general problem of extraction of tech-
nical terms which is crucial in technical and scien-
tific documents (Ahmad and Collingham, 1996).
Among the extracted terms for a given scientific
document in a given collection, which key terms
best characterize this document?
This article describes the system realized for
the Semeval 2010 task 5, based on GROBID?s
(GeneRation Of BIbilographic Data) module ded-
icated to key term extraction. GROBID is a tool
for analyzing technical and scientific documents,
focusing on automatic bibliographical data extrac-
tion (header, citations, etc.) (Lopez, 2009) and
structure recognition (section titles, figures, etc).
As the space for the system description is very
limited, this presentation focuses on key aspects.
We present first an overview of our approach, then
our selection of features (section 3), the different
tested machine learning models (section 4) and the
final post-ranking (section 5). We briefly describe
our unsuccessful experiments (section 6) and we
conclude by discussing future works.
2 Bases
Principle As most of the successful works for
keyphrase extraction, our approach relies on Ma-
chine Learning (ML). The following steps are ap-
plied to each document to be processed:
1. Analysis of the structure of the article.
2. Selection of candidate terms.
3. Calculation of features.
4. Application of a ML model for evaluating
each candidate term independently.
5. Final re-ranking for capturing relationships
between the term candidates.
For creating the ML model, steps 1-3 are applied
to the articles of the training set. We view steps 1
and 5 as our main novel contributions. The struc-
ture analysis permits the usage of reliable features
in relation to the logical composition of the arti-
cle to be processed. The final re-ranking exploits
248
general relationships between the set of candidates
which cannot be captured by the ML models.
Candidate term selection In the following,
word should be understood as similar to token in
the sense of MAF
1
. Step 2 has been implemented
in a standard manner, as follows:
1. Extract all n-grams up to 5 words,
2. Remove all candidate n-grams starting or
ending with a stop word,
3. Filter from these candidates terms having
mathematical symbols,
4. Normalize each candidate by lowercasing
and by stemming using the Porter stemmer.
Training data The task?s collection consists of
articles from the ACM (Association for Computa-
tional Machinery) in four narrow domains (C.2.4
Distributed Systems, H.3.3 Information Search
and Retrieval, I.2.6 Learning and J.4 Social and
Behavioral Sciences). As training data, we used
this task?s training resources (144 articles from
ACM) and the National University of Singapore
(NUS) corpus
2
(156 ACM articles from all com-
puting domains). Adding the additional NUS
training data improved our final results (+7.4%
for the F-score at top 15, i.e. from 25.6 to 27.5).
3 Features
3.1 Structural features
One of the goals of GROBID is to realize reli-
able conversions of technical and scientific docu-
ments in PDF to fully compliant TEI
3
documents.
This conversion implies first the recognition of
the different sections of the document, then the
extraction of all header metadata and references.
The analysis is realized in GROBID with Condi-
tional Random Fields (CRF) (Peng and McCal-
lum, 2004) exploiting a large amount of training
data. We added to this training set a few ACM doc-
uments manually annotated and obtained a very
high performance for field recognitions, between
97% (section titles, reference titles) and 99% (ti-
tle, abstract) accuracy for the task?s collection.
Authors commonly introduce the main concepts
of a written communication in the header (title,
abstract, table of contents), the introduction, the
1
Morpho-syntactic Annotation Framework, see
http://pauillac.inria.fr/ clerger/MAF/
2
http://wing.comp.nus.edu.sg/downloads/keyphraseCorpus
3
Text Encoding Initiative (TEI), http://www.tei-c.org.
section titles, the conclusion and the reference list.
Similarly human readers/annotators typically fo-
cus their attention on the same document parts.
We introduced thus the following 6 binary fea-
tures characterizing the position of a term with re-
spect to the document structure for each candidate:
present in the title, in the abstract, in the introduc-
tion, in at least one section titles, in the conclusion,
in at least one reference or book title.
In addition, we used the following standard fea-
ture: the position of the first occurrence, calcu-
lated as the number of words which precede the
first occurrence of the term divided by the num-
ber of words in the document, similarly as, for in-
stance, (Witten et al, 1999).
3.2 Content features
The second set of features used in this work tries
to captures distributional properties of a term rel-
atively to the overall textual content of the docu-
ment where the term appears or the collection.
Phraseness The phraseness measures the lexical
cohesion of a sequence of words in a given docu-
ment, i.e. the degree to which it can be consid-
ered as a phrase. This measure is classically used
for term extraction and can rely on different tech-
niques, usually evaluating the ability of a sequence
of words to appear as a stable phrase more often
than just by chance. We applied here the Gen-
eralized Dice Coeficient (GDC) as introduced by
(Park et al, 2002), applicable to any arbitrary n-
gram of words (n ? 2). For a given term T , | T |
being the number of words in T , freq(T ) the fre-
quency of occurrence of T and freq(w
i
) the fre-
quency of occurrence of the word w
i
, we have:
GDC(T ) =
| T | log
10
(freq(T ))freq(T )
?
w
i
?T
freq(w
i
)
We used a default value for a single word, because,
in this case, the association measure is not mean-
ingful as it depends only on the frequency.
Informativeness The informativeness of a term
is the degree to which the term is representative of
a document given a collection of documents. Once
again many measures can be relevant, and we opt
for the standard TF-IDF value which is used in
most of the keyphrase extraction systems, see for
instance (Witten et al, 1999) or (Medelyan and
249
Witten, 2008). The TF-IDF score for a Term T in
document D is given by:
TF-IDF(T,D) =
freq(T,D)
| D |
??log
2
count(T )
N
where | D | is the number of words in D,
count(T ) is the number of occurrence of the term
T in the global corpus, and N is the number of doc-
uments in the corpus.
Keywordness Introduced by (Witten et al,
1999), the keywordness reflects the degree to
which a term is selected as a keyword. In prac-
tice, it is simply the frequency of the keyword in
the global corpus. The efficiency of this feature
depends, however, on the amount of training data
available and the variety of technical domains con-
sidered. As the training set of documents for this
task is relatively large and narrow in term of tech-
nical domains, this feature was relevant.
3.3 Lexical/Semantic features
GRISP is a large scale terminological database
for technical and scientific domains resulting from
the fusion of terminological resources (MeSH, the
Gene Ontology, etc.), linguistic resources (part of
WordNet) and part of Wikipedia. It has been cre-
ated for improving patent retrieval and classifica-
tion (Lopez and Romary, 2010). The assumption
is that a phrase which has been identified as con-
trolled term in these resources tend to be a more
important keyphrase. A binary feature is used to
indicate if the term is part of GRISP or not.
We use Wikipedia similarly as the Wikipedia
keyphraseness in Maui (Medelyan, 2009). The
Wikipedia keyphraseness of a term T is the prob-
ability of an appearance of T in a document being
an anchor (Medelyan, 2009). We use Wikipedia
Miner
4
for obtaining this value.
Finally we introduced an additional feature
commonly used in keyword extraction, the length
of the term candidate, i.e. its number of words.
4 Machine learning model
We experimented different ML models: Decision
tree (C4.5), Multi-Layer perceptron (MLP) and
Support Vector Machine (SVM). In addition, we
combined these models with boosting and bagging
techniques. We used WEKA (Witten and Frank,
2005) for all our experiments, except for SVM
4
http://wikipedia-miner.sourceforge.net
where LIBSVM (Chang and Lin, 2001) was used.
We failed to obtain reasonable results with SVM.
Our hypothesis is that SVM is sensitive to the very
large number of negative examples compared to
the positive ones and additional techniques should
be used for balancing the training data. Results
for decision tree and MLP were similar but the lat-
ter is approx. 57 times more time-consuming for
training. Bagged decision tree appeared to per-
form constantly better than boosting (+8,4% for
F-score). The selected model for the final run was,
therefore, bagged decision tree, similarly as, for
instance, in (Medelyan, 2009).
5 Post-ranking
Post-ranking uses the selected candidates as a
whole for improving the results, while in the pre-
vious step, each candidate was selected indepen-
dently from the other. If we have a ranked list of
term T
1?N
, each having a score s(T
i
), the new
score s
?
for the term T
i
is obtained as follow:
s
?
(T
i
) = s(T
i
) + ?
?1
?
j 6=i
P (T
j
|T
i
)s(T
j
)
where ? is a constant in [0 ? 1] for control-
ling the re-ranking factor. ? has been set ex-
perimentally to 0.8. P (T
j
|T
i
) is the probability
that the keyword T
j
is chosen by the author when
the keyword T
i
has been selected. For obtain-
ing these probabilities, we use statistics for the
HAL
5
research archive. HAL contains approx.
139,000 full texts articles described by a rich set of
metadata, often including author?s keywords. We
use the keywords appearing in English and in the
Computer Science domain (a subset of 29,000 ar-
ticles), corresponding to a total of 16,412 different
keywords. No smoothing was used. The usage of
open publication repository as a research resource
is in its infancy and very promising.
6 Results
Our system was ranked first of the competition
among 19 participants. Table 1 presents our offi-
cial results (Precision, Recall, F-score) for com-
bined keywords and reader keywords, together
with the scores of the systems ranked second
(WINGNUS and KX FBK).
5
HAL (Hyper Article en Ligne) is the French Institutional
repository for research publications: http://hal.archives-
ouvertes.fr/index.php?langue=en
250
Set System top 5 top 10 top 15
Comb. HUMB P:39.0 R:13.3 F:19.8 F:32.0 R:21.8 F:25.9 P:27.2 R:27.8 F:27.5
WINGNUS P:40.2 R:13.7 F:20.5 P:30.5 R:20.8 F:24.7 P:24.9 R:25.5 F:25.2
Reader HUMB P:30.4 R:12.6 F:17.8 P:24.8 R:20.6 F:22.5 P:21.2 R:26.4 F:23.5
KX FBK P:29.2 R:12.1 F:17.1 P:23.2 R:19.3 F:21.1 P:20.3 R:25.3 F:22.6
Table 1: Performance of our system (HUMB) and of the systems ranked second.
7 What did not work
The previously described features were selected
because they all had a positive impact on the ex-
traction accuracy based on our experiments on the
task?s collection. The following intuitively perti-
nent ideas appeared, however, to deteriorate or to
be neutral for the results.
Noun phrase filtering We applied a filtering of
noun phrases based on a POS tagging and extrac-
tion of all possible NP based on typical patterns.
This filtering lowered both the recall and the pre-
cision (?7.6% for F-score at top 15).
Term variants We tried to apply a post-ranking
by conflating term variants using FASTR
6
, result-
ing in a disappointing ?11.5% for the F-score.
Global keywordness We evaluated the key-
wordness using also the overall HAL keyword fre-
quencies rather than only the training corpus. It
had no impact on the results.
Language Model deviation We experimented
the usage of HMM deviation using LingPipe
7
as
alternative informativeness measure, resulting in
?3.7% for the F-score at top 15.
Wikipedia term Relatedness Using Wikipedia
Miner, we tried to apply as post-ranking a boosting
of related terms, but saw no impact on the results.
8 Future work
We think that automatic key term extraction can
be highly valuable for assisting self-archiving of
research papers by authors in scholarly reposito-
ries such as arXiv or HAL. We plan to experiment
keyword suggestions in HAL based on the present
system. Many archived research papers are cur-
rently not associated with any keyword.
We also plan to adapt our module to a large col-
lection of approx. 2.6 million patent documents in
6
http://perso.limsi.fr/jacquemi/FASTR
7
http://alias-i.com/lingpipe
the context of CLEF IP 2010. This will be the op-
portunity to evaluate the relevance of the extracted
key terms for large scale topic-based IR.
References
K. Ahmad and S. Collingham. 1996. Pointer project
final report. Technical report, University of Surrey.
http://www.computing.surrey.ac.uk/ai/pointer/report.
C.-C. Chang and C.-J. Lin. 2001. Libsvm: a library
for support vector machines. Technical report.
P. Lopez and L. Romary. 2010. GRISP: A Massive
Multilingual Terminological Database for Scientific
and Technical Domains. In Seventh international
conference on Language Resources and Evaluation
(LREC), Valletta, Malta.
P. Lopez. 2009. GROBID: Combining Automatic
Bibliographic Data Recognition and Term Extrac-
tion for Scholarship Publications. In Proceedings of
ECDL 2009, 13th European Conference on Digital
Library, Corfu, Greece.
O. Medelyan and I.H. Witten. 2008. Domain-
independent automatic keyphrase indexing with
small training sets. Journal of the American
Society for Information Science and Technology,
59(7):1026?1040.
O. Medelyan. 2009. Human-competitive automatic
topic indexing. Ph.D. thesis.
Y. Park, R.J. Byrd, and B.K. Boguraev. 2002. Auto-
matic glossary extraction: beyond terminology iden-
tification. In Proceedings of the 19th international
conference on Computational linguistics-Volume 1,
pages 1?7. Association for Computational Linguis-
tics.
F. Peng and A. McCallum. 2004. Accurate infor-
mation extraction from research papers using con-
ditional random fields. In Proceedings of HLT-
NAACL, Boston, USA.
I.H. Witten and E. Frank. 2005. Data Mining: Practi-
cal machine learning tools and techniques. Morgan
Kaufmann, San Francisco, 2nd edition edition.
I.H. Witten, G.W. Paynter, E. Frank, C. Gutwin, and
C.G. Nevill-Manning. 1999. KEA: Practical auto-
matic keyphrase extraction. In Proceedings of the
fourth ACM conference on Digital libraries, page
255. ACM.
251
