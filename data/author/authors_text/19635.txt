Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 741?752, Dublin, Ireland, August 23-29 2014.
An Entity-Centric Coreference Resolution System for Person Entities with
Rich Linguistic Information
Marcos Garcia and Pablo Gamallo
Centro Singular de Investigaci?on en Tecnolox??as da Informaci?on (CiTIUS)
University of Santiago de Compostela
{ marcos.garcia.gonzalez, pablo.gamallo}@usc.es
Abstract
This paper presents a first version of LinkPeople, an entity-centric system for coreference reso-
lution of person entities. The approach combines (i) a multi-pass architecture which takes advan-
tage of entity features at document-level with (ii) a set of linguistically-motivated constraints and
rules which allows the system to restrict the candidates of a given mention. The paper includes
evaluations and error analysis of LinkPeople in 3 different languages, achieving promising results
(more than 81% F1 in different metrics). Both the system and the corpora are freely distributed.
1 Introduction
Coreference Resolution (CR) is a crucial task for several Natural Language Processing (NLP) applica-
tions such as Text Summarization, Machine Translation or Information Extraction (IE).
Specially for IE, person entities are those which more effort have deserved from different perspectives.
Evaluations such as the Knowledge Base Population (KBP) Slot Filling Task (in the Text Analysis Con-
ference)
1
and the Person Attribute Extraction (in the Web People Search Evaluation Campaign, WePS)
2
,
tasks such as Personal Name Matching (Cohen et al., 2003), or different works on Relation Extraction of
person entities (Mann, 2002; Garcia and Gamallo, 2013) are some examples of their importance.
Recently, entity-centric models for coreference resolution, which use features from all the mentions
of an entity, have shown better performance than pair-mention systems, which carry out coreference
resolution on single pairs of mentions (Lee et al., 2013).
3
Furthermore, the use of linguistic information
such as syntax or semantic knowledge has proved to be essential for high-precision CR (Ng and Cardie,
2002; Ponzetto and Strube, 2006; Uryupina, 2007).
This paper presents the first version of LinkPeople, an open-source system for CR of person entities.
LinkPeople is inspired by the Stanford Deterministic Coreference Resolution System (Raghunathan et
al., 2010; Lee et al., 2013), using a multi-pass architecture which applies a battery of modules sorted
from high-precision to high-recall.
Moreover, the system presented in this paper adds new sieves based on linguistic knowledge, for both
cataphoric and anaphoric mentions: It includes a high-precision module which finds cataphoric mentions
of Noun Phrases (NP) and personal and elliptical pronouns. The inclusion of this module is based on the
claim that definite NPs are not primarily anaphoric (Vieira and Poesio, 2000). In addition, LinkPeople
applies a set of syntactic constraints on the pronominal CR module, increasing its precision by blocking
links which do not satisfy the constraints (Mitkov, 1998; Palomar et al., 2001; Chaves and Rino, 2007).
The system was evaluated in three languages (Portuguese, Spanish and Galician) with promising re-
sults (F1 ? 83%, with BLANC score). Both LinkPeople and the corpora are freely distributed.
4
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1
http://www.nist.gov/tac/data/index.html
2
http://nlp.uned.es/weps/weps-3
3
In this paper, a mention is every instance of reference to a person. An entity is the group of all the mentions referring to the
same person in the text (Recasens and Mart??, 2010).
4
http://gramatica.usc.es/
?
marcos/coling14.tar.bz2
741
Apart from this Introduction, Section 2 contains some related work. The architecture of the system is
presented in Section 3 while its evaluation is shown in Section 4. Finally, the results of an error analysis
are presented in Section 5, and some conclusions and further work are pointed out in Section 6.
2 Related Work
Coreference (and anaphora) resolution is one of the older topics in NLP, so it has been the subject of
many works. Two main distinctions can be stated in coreference resolution systems: (i) mention-pair vs
entity-centric approaches and (ii) machine learning-based vs rule-based models.
On the one hand, mention-pair systems classify two mentions in a text as coreferent or not, by us-
ing a feature vector obtained from this pair of mentions. On the other hand, entity-centric approaches
determine if a mention (or a partial entity) belongs to another partial entity, using features from other
mentions of the same (partial) entities.
5
Machine learning classifiers for CR often use annotated corpora for training supervised models. Su-
pervised models rely on these data in order to learn preferences and constraints (McCarthy and Lehnert,
1995; Soon et al., 2001; Ng and Cardie, 2002; Sapena et al., 2013), while unsupervised models apply
clustering approaches to the coreference resolution problem (Haghighi and Klein, 2007; Ng, 2008).
Rule-based strategies make use of sets of rules and heuristics for finding the best element to link each
mention to (Lappin and Leass, 1994; Baldwin, 1997; Mitkov, 1998; Bontcheva et al., 2002; Raghunathan
et al., 2010; Lee et al., 2013). This last system is based on a multi-pass approach which first solves the
easy links, then increasing the recall with more rules. Stoyanov and Eisner (2012) presented EasyFirst,
which uses annotated corpora in order to know whether coreference links are easy or hard.
Concerning the languages LinkPeople deals with, some studies addressed pronominal CR in Por-
tuguese (Paraboni, 1997; Chaves and Rino, 2007; Cuevas and Paraboni, 2008). Coelho and Carvalho
(2005) adapted the Lappin and Leass (1994) algorithm for this language, while de Souza et al. (2008)
presented a supervised approach for solving the coreference between NPs.
For Spanish, Palomar et al. (2001) presented a set of constraints and preferences for pronominal
anaphora resolution. Recasens and Hovy (2009) analyzed the impact of several features for CR, then
implemented in Recasens and Hovy (2010). The availability of a large coreference annotated corpus
for Spanish (Recasens and Mart??, 2010) also allowed other supervised systems being adapted for this
language (Recasens et al., 2010).
To the best of our knowledge, there are no specific systems for coreference or anaphora resolution for
Galician language.
Other related areas such as the above mentioned personal name matching perform coreference resolu-
tion of personal names by linking variants referring to the same person (Cohen et al., 2003).
The system presented in this paper uses a similar approach than Lee et al. (2013), adapting ?and
adding? some modules for person entities, and enriching others with linguistic-based heuristics such as
cataphoric analysis and syntactic constraints.
3 Architecture of LinkPeople
LinkPeople is based on two main principles: (i) an entity-centric approach and (ii) a multi-pass architec-
ture. On the one hand, the entity-centric approach allows the system to use all the features of an entity
when a mention is evaluated. On the other hand, the multi-pass model dynamically enriches an entity
(with new features) in every iteration. Thus, latter passes take advantage of the information provided by
the previous coreference resolution modules.
Figure 1 shows a text with coreference annotation of person entities. It will be used to show how the
system works. The input of LinkPeople needs to be pre-processed by NLP tools which provide PoS-tags,
Named Entity Recognition (NER) and dependency analysis. In our experiments, FreeLing (Padr?o and
Stanilovsky, 2012; Garcia and Gamallo, 2010) was used for tokenizing, lemmatizing and PoS-tagging.
NER labeling for Spanish and Portuguese was also added by FreeLing (Carreras et al., 2003; Gamallo
5
Partial entities are sets of mentions of the same entity.
742
Who was
1
[the singer of the Beatles]
1
.
2
[The musician John Winston Ono Lennon]
1
was one of the founders of the Beatles. With
3
[Paul McCartney]
2
,
4
[he]
1
formed a
songwriting partnership.
5
[Lennon]
1
was born at Liverpool Hospital to
6
[Julia]
3
and
7
[Alfred Lennon]
4
.
8/9
[
10
[His]
1
parents]
3/4
named
11
[him]
1 12
[John Winston Lennon]
1
.
13
[Lennon]
1
revealed a rebellious nature and acerbic wit.
14
[The musician]
1
was
murdered in 1980.
Figure 1: Example of a text with coreference annotation of person entities. Mentions appear inside
brackets. Numbers on the left are mention ids, while entity ids appear in the right side.
IdentificationofMentions
Nominal Coreference:
StringMatchNP_CataphoraPN_StMatchPN_InclusionPN_TokensHeadMatchOrphan_NP
Pronominal Coreference:
Pro_CataphoraPronominalPivot_Ent
Output
Input
Figure 2: Architecture of the system.
and Garcia, 2011), while the named entities in Galician were classified by the system presented in Garcia
et al. (2012). Finally, dependency information for the three languages was added by DepPattern (Gamallo
and Gonz?alez L?opez, 2011).
3.1 Coreference Resolution Modules
Figure 2 summarizes the architecture of the system, which starts by identifying the mentions. Then, a
battery of nominal and pronominal CR modules is applied. Modules with high-precision are applied first,
while other modules increase recall by taking advantage of the previously extracted features.
In the first stage, a specific pass identifies the mentions referring to a person entity, using the informa-
tion provided by the PoS-tagger and the NER as well as applying basic approaches for NP and elliptical
pronoun identification: First, personal names (and noun phrases including personal names) are identi-
fied. Then, it seeks for definite NPs whose head may refer to a person (e.g., ?the singer?). Finally, this
module selects singular possessives and applies basic rules for identifying relative, personal and elliptical
pronouns (in sentence-initial position, after adverbial phrases and after preposition phrases) (Ferr?andez
and Peral, 2000). At this step, each mention belongs to a different entity. Each entity contains the gender,
number, head of a noun phrase, head of a Proper Noun (PN) and full proper noun as features. Once the
mentions are identified, the coreference resolution modules are sequentially executed.
In order to perform CR, each module applies the following strategy (except for some exceptional rules,
explained below): mentions are traversed from the beginning of the text and each one is selected if (i) it
is not the first mention of the text and (ii) it is the first mention of its entity. Once a mention is selected, it
looks backwards for candidates in order to find an appropriate antecedent (in the experiments, using the
whole text). If an antecedent is found, mentions are merged together in the same entity. Then, the next
selected mention is evaluated.
Besides the identification of mentions, current version of LinkPeople contains the following modules:
StringMatch (StM): this pass performs strict matching of the whole string of both mentions (the
selected one and the candidate). In the example (Figure 1), mentions 13 and 5 are linked in this step.
NP Cataphora (NP C): this module verifies if the first mention ?in the first paragraph? is an NP
without a personal name. If so, it is considered a cataphoric mention, and the system checks if the next
sentence contains a personal name as a Subject. In this case, these mentions are linked if they agree in
743
gender and number. Mentions 1 and 2 in the example meet these requirements, so they merge. Note that,
at the end of this pass, this entity has as NP heads the words ?singer? and ?musician?, and ?John Winston
Ono Lennon? as the PN. This module also matches fixed synonym structures through dependency paths,
such as ?Person
A
, also known as Person
B
?.
PN StMatch (PN St): in this stage, the system looks for mentions which share the whole PN, even if
their heads are different (or if one of them does not have head). ?The musician John Lennon? and ?John
Lennon? (not in Figure 1) would be an example.
PN Inclusion (PN I): here, the system verifies if the full PN of the selected mention (in the entity)
includes the proper noun of the candidate mention (also in the entity), or vice-versa. In the example,
mention 5 is linked to mention 2 in this step. Note that mention 7 is not linked to mention 5, because the
full PN of mention 5 is now ?John Winston Ono Lennon?, not compatible with ?Alfred Lennon?. Also,
mention 13 is not selected here because it is not the first mention of its entity.
PN Tokens (PN T): this module splits the full PN of a partial entity in their tokens, and verifies if the
full PN of the candidate contains all the tokens in the same order, or vice-versa (except for some stop
words, such as ?Sr.?, ?Jr.?, etc.). As the pair ?John Winston Ono Lennon? - ?John Winston Lennon? are
compatible, mentions 12 and 5 are merged.
HeadMatch (HM): in this step, the system checks if the selected mention and the candidate one share
the heads (or the heads of their entities). In Figure 1, mention 14 is linked to mention 13.
Orphan NP (Orph): the last module of nominal CR applies a pronominal-based rule to orphan noun
phrases. Here, a definite NP is marked as orphan if it is still a singleton and it does not contain a personal
name. Thus, an orphan NP is linked to the previous PN with gender and number agreement. In the
example, the mentions 8/9 are linked to 7 and 6.
Pro Cataphora (Pro C): similar to NP Cataphora, this module verifies if a text starts with a personal
(or elliptical) pronoun. If so, it looks in the following sentence if there are a compatible PN.
Pronominal (PRO): this is the standard module for pronominal CR. For each selected pronoun, it ver-
ifies if the candidate nominal mentions satisfy the syntactic (and morpho-syntactic) constraints (inspired
by Palomar et al. (2001)). They include a set of constraints for each type of pronoun, which remove a
candidate if any of the constraints is violated. Some of them are: an object pronoun (direct or indirect)
cannot corefer with its subject (mention 11 vs mentions 8/9); a personal pronoun does not corefer with a
mention inside a prepositional phrase (mention 4 vs mention 3), a possessive cannot corefer with the NP
it belongs to (mention 10 vs mentions 8/9) or a pronoun prefers a subject NP as its antecedent (mentions
10 and 11 vs mentions 6 and 7). This way, in Figure 1 the pronominal mention 4 is linked to mention
2, and mentions 10 and 11 to mention 5. This module only looks in the same and previous sentence for
candidates.
Pivot Ent: this last module is only applied if there are orphan pronouns (not linked to any proper
noun/noun phrase) at this step. First, it verifies if the text has a pivot entity, which is the more frequent
personal name in a text whose frequency is at least 33% higher than the second person with more oc-
currences. Then, if there is a pivot entity, all the orphan pronouns are linked to its mention. If not, each
orphan pronoun is linked to the previous PN/NP (with no constraint).
4 Evaluation
LinkPeople was tested on three different corpora (for Portuguese, Galician, and Spanish) with corefer-
ence annotation of person entities (Garcia and Gamallo, 2014). The annotation follows the SemEval-
2010 guidelines. The corpus for Portuguese has about 51k tokens and ? 4,000 mentions. The Galician
one, 42k tokens and ? 3,500 mentions. The Spanish corpus has over 46k tokens, and ? 4,500 mentions.
Some of the annotation (gender, number and syntactic labeling) was not manually revised, so it may
contain errors (regular setting). The tests were carried out using a gold mention evaluation (i. e., using
744
as input the corpora with the mentions already identified). Moreover, no external resources (gender
dictionaries of proper nouns, WordNet, etc.) were used (closed setting).
In order to compare the results of LinkPeople, four well-known baselines were also evaluated: (i)
Singletons (Stons), where every mention belongs to a different entity. (ii) All in One (AOne), where
all the mentions belong to the same entity; (iii) HeadMatch (HMb), which clusters in the same entity
mentions sharing the head and classify each pronoun as a singleton, and (iv) HeadMatch Pro (HMP),
same as the previous one, but linking each pronoun to the previous nominal mention with gender and
number agreement.
6
Five different metrics were taken into account: MUC (Vilain et al., 1995), B
3
(Bagga and Baldwin,
1998), CEAF
entity
(Luo, 2005), BLANC (Recasens and Hovy, 2011) and ConLL (Pradhan et al., 2011).
They were computed with the scorers used in SemEval-2010
7
(for BLANC) and ConLL 2011
8
(for the
other metrics).
Table 1 contains the results of the four baselines and of LinkPeople in the three corpora. The first block
of each language includes the results of the baseline models. The central rows show the results of the
different modules of LinkPeople (see Figure 2), added incrementally. The first nine rows (StM > PRO)
include two default rules in order to classify mentions not covered by the active modules: (i) nominal
mentions not analyzed are singletons and (ii) pronouns are linked to the previous nominal mention with
gender agreement (except for those pronouns covered by PRO in this model). Furthermore, PRO systems
do not restrict the number of previous sentences while looking for antecedents.
The last model (LinkP, the result of all the modules included in LinkPeople) does include a distance
restriction in the Pronominal pass (see Section 3.1), so it combines Pronominal with Pivot Ent modules.
As expected, Singletons and HeadMatch baselines produce poor results in most languages and metrics
(Singletons values in MUC are null because this metric do not reward correctly identified singletons).
However, All In One models achieved reasonable results in some scenarios (MUC and B
3
). The differ-
ences between these values and those from SemEval-2010 are due to the existence (in this work) of just
one type of entity. Journalistic and encyclopedic texts are often focused on just one or two persons, (i.e.,
there is a much lower number of entities in each text), so the precision is higher in All In One and lower
in Singletons.
As Recasens and Hovy (2010) shown, HeadMatch Pro baselines obtain good results in the three lan-
guages and with every metric (? 60% and 67% in F1 BLANC and CoNLL, respectively).
Concerning the different passes of LinkPeople, the performance of the first matching modules depends
on the distribution and structure of PNs and NPs in the corpora. In this respect, PN StMatch works well
in all the contexts. However, PN Inclusion stands out in the Nominal modules, increasing in more than
5% (BLANC and CoNLL) the performance of the previous model. This is due to the high increase in
recall together with the high-precision of this module.
It is worth noting that the addition of some modules seems to improve not only recall, but also preci-
sion. This is due to the execution of the two default rules: as the system uses more modules, the amount
of (partial)entity mergings (usually) grows. Thus, the precision increases because the new mergings
restrict incorrect links performed by the two default rules in the previous models.
HeadMatch module is the first one that deals with mentions without PN (except for the rules applied
in NP Cataphora, with low recall). Due to the knowledge provided by previous modules, it also benefits
all the models and languages.
The performance of Orphan NP and Pro Cataphora also depends on the corpora and on the evaluation
metric. The latter involves a 0.2% loss in Spanish with the BLANC score (but increases in 1.1% using
CoNLL). However, Orphan NP allows the system to not classify as singletons some mentions, which in
turn helps to increase the performance of Pronominal modules. Similarly, Pro Cataphora prevents the
next sieve from selecting pronominal mentions that are cataphoric.
6
Due to language differences and format issues, other coreference resolution systems could not be used for comparison
(Raghunathan et al., 2010; Sapena et al., 2013).
7
http://www.lsi.upc.edu/
?
esapena/downloads/scorer-v1.04.zip
8
http://conll.cemantix.org/download/reference-coreference-scorers.v7.tar.gz
745
Lang Model
MUC B
3
CEAF
e
BLANC CoNLL
R P F1 R P F1 R P F1 R P F1 F1
Port.
Stons - - - 15.0 100 26.1 65.3 10.9 18.7 50.0 29.0 36.7 14.9
AOne 93.8 85.5 89.4 94.8 47.5 63.3 11.9 78.1 20.7 50.0 21.0 29.1 57.8
HMb 26.5 93.9 41.3 22.2 97.9 36.2 72.3 16.1 26.4 53.6 78.5 44.2 34.6
HMP 76.0 91.2 82.9 46.0 85.8 59.9 76.7 49.2 59.9 68.5 80.0 68.1 67.6
StM 69.8 91.5 79.2 38.8 88.7 54.0 78.1 40.5 53.3 64.7 79.2 62.9 62.2
NP C 70.4 91.4 79.6 39.2 88.5 54.3 78.3 41.5 54.3 64.7 79.2 62.9 62.7
PN St 72.8 91.9 81.3 40.9 88.3 55.9 79.3 44.7 57.2 65.0 79.2 63.4 64.8
PN I 77.1 92.5 84.1 50.5 87.5 64.0 81.9 52.7 64.1 71.1 81.0 71.2 70.8
PN T 77.3 92.5 84.2 50.8 87.5 64.3 82.0 53.0 64.4 71.1 81.0 71.3 71.0
HM 79.7 92.3 85.6 53.6 85.5 65.9 81.3 58.3 67.9 71.5 80.7 71.7 73.1
Orph 83.4 91.8 87.4 58.1 82.7 68.3 81.4 70.2 75.4 71.6 80.3 71.9 77.0
ProC 83.4 91.8 87.4 58.1 82.7 68.3 81.4 70.3 75.5 71.6 80.3 72.0 77.0
PRO 81.8 91.7 86.4 59.1 83.9 69.3 82.7 66.5 73.7 76.0 83.7 76.7 76.5
LinkP 82.7 92,7 87.4 65.8 84.5 74.0 84.4 67.9 75.2 83.6 85.4 84.2 78.9
Gal.
Stons - - - 14.6 100 25.4 71.7 11.0 19.1 50.0 28.4 36.3 14.8
AOne 96.6 86.0 91.0 97.1 53.9 69.3 9.0 82.7 16.2 50.0 21.6 30.1 58.8
HMb 21.1 90.5 34.2 20.2 97.5 33.5 74.1 14.3 24.0 51.3 74.7 39.1 30.6
HMP 81.9 89.8 85.7 44.1 83.6 57.7 70.0 53.5 60.6 61.3 76.5 57.9 68.0
StM 77.1 90.6 83.3 36.5 86.7 51.4 75.1 45.5 56.6 58.9 76.9 53.7 63.8
NP C 77.6 90.7 83.6 37.2 86.7 52.1 75.2 46.2 57.3 59.2 77.0 54.3 64.3
PN St 79.0 90.9 84.6 39.1 86.2 53.8 75.6 48.8 59.3 59.7 77.0 55.1 65.9
PN I 83.1 91.5 87.1 46.7 85.3 60.4 76.7 57.8 66.0 62.5 77.5 59.5 71.1
PN T 83.3 91.5 87.2 48.2 85.3 61.6 76.9 58.6 66.5 63.2 77.9 60.5 71.8
HM 84.6 91.6 87.9 49.8 84.4 62.6 76.8 62.0 68.6 63.4 77.5 60.8 73.1
Orph 84.7 91.3 87.9 49.9 83.9 62.6 76.8 63.2 69.4 63.3 77.3 60.8 73.3
ProC 84.7 91.3 87.9 49.1 83.9 62.6 76.8 63.2 69.4 63.3 77.3 60.8 73.3
PRO 86.9 92.5 89.6 60.7 86.8 71.4 82.8 72.2 77.1 73.6 82.0 73.9 79.4
LinkP 89.0 94.6 91.7 72.9 88.4 79.9 87.6 76.6 81.7 82.7 85.8 83.4 84.4
Spa.
Stons - - - 10.9 100 19.7 69.5 8.7 15.4 50.0 29.4 37.0 11.7
AOne 91.7 88.4 90.0 92.6 51.3 66.0 6.4 83.0 11.9 50.0 20.6 29.2 55.9
HMb 20.7 94.2 34.0 15.4 98.0 26.6 75.4 11.9 20.6 51.3 74.6 39.9 27.0
HMP 78.2 90.7 84.0 35.3 81.2 49.2 72.9 51.5 60.4 59.3 74.7 55.5 64.5
StM 73.9 90.7 81.4 30.1 83.7 44.3 73.9 41.6 53.3 58.6 75.6 54.1 59.7
NP C 74.1 90.7 81.5 30.2 83.7 44.4 73.9 42.0 53.6 58.6 75.6 54.1 59.8
PN St 75.4 91.0 82.5 31.2 83.1 45.4 73.8 44.1 55.2 58.6 75.4 54.3 61.0
PN I 78.8 91.7 84.8 39.3 82.2 53.1 75.9 52.8 62.3 62.0 76.7 59.6 66.7
PN T 79.0 91.7 84.9 40.0 82.1 53.8 76.0 53.3 62.7 62.6 76.3 60.5 67.1
HM 80.5 92.0 85.9 41.7 80.9 55.1 75.6 57.3 65.2 63.1 75.0 61.4 68.7
Orph 81.1 91.9 86.1 42.3 80.5 55.5 75.4 59.8 66.7 63.2 75.0 61.6 69.4
ProC 82.3 91.9 86.8 43.2 79.6 56.0 74.6 64.1 68.9 63.0 74.7 61.4 70.6
PRO 82.6 92.4 87.2 46.0 80.8 58.7 77.5 65.8 71.2 66.8 77.9 66.2 72.4
LinkP 84.1 94.1 88.8 62.9 84.8 72.2 83.4 71.0 76.7 81.7 84.9 82.6 79.2
Table 1: Results of LinkPeople compared to the baselines in Portuguese (Port.), Galician (Gal.) and
Spanish (Spa.). LinkP contains the results of the execution of the whole system.
The standard pronominal resolution module also increases the accuracy of all the systems (with the
only exception in Portuguese language with the CoNLL score, which also had a high increase with the
Orphan NP module).
Finally, one of the main contributions to the performance of LinkPeople is the combination of the
Pronominal module with the Pivot Ent one. This combination reduces the scope of the Pronominal mod-
ule, thus strengthening the impact of syntactic constraints. Furthermore, Pivot Ent looks for a prominent
person entity in each text, and links the orphan pronouns to this entity. In the three languages, the
improvement is noticeably better with the BLANC score.
Last row of each language shows the current results of LinkPeople in the three corpora, with macro-
average values of ? 83% and ? 81% with BLANC and CoNLL scores, respectively.
5 Error Analysis
In order to determine the major classes of errors produced by the system, 150 errors (50 for each lan-
guage) were randomly selected from the output of LinkPeople. Each error was analyzed in order to find
746
its source, and was classified according to its typology. This section shows the different error typologies
together with some examples, sorted by their frequency in the corpora (first percentage in parenthesis
is the average frequency, while the other three correspond to Portuguese, Galician and Spanish values,
respectively).
9
They are real examples of incorrectly analyzed mentions (or pairs of mentions belonging
to the same entity), with some simplifications due to space reasons:
5.1 Missing links between Noun Phrases and/or Proper Nouns (46%: 58% / 32% / 48%)
This category includes some error typologies that differ in the type of knowledge and analysis required
by the system in order to accurately link two mentions:
Synonym heads (35.3%: 48% / 32% / 26%): The most frequent type of missing links was produced
by mentions of the same entity whose heads are synonyms:
Mention A: ?El joven? (the young)
Mention B: ?el muchacho? (the boy)
External (real-world) knowledge (6%: 0% / 0% / 18%): This class includes mentions of the same
entity which do not share the lexical features, usually because they refer to well-known entities in the
real world:
Mention A: ?la presidenta? (the president)
Mention B: ?Cristina Kirchner?
Here, the noun phrase ?the president? is used to refer ?Cristina Kirchner?, but the mentions are not
linked because the system does not take advantage of resources that define Cristina Kirchner as a presi-
dent.
Semantic knowledge (2.7%: 4% / 0 % / 4%): Lack of other type of semantic knowledge, such as
hyponym-hypernym pairs, also involves missing links like the following:
Mention A: ?o escoc?es? (the scotish)
Mention B: ?o brit?anico? (the british)
Head modifiers (1.3%: 4% / 0 % / 0%): Internal modifiers of some heads may also produce missing
links, as in the following example, where a mention does not contain the modifier adjunto (vice):
Mention A: ?o ministro (the minister)
Mention B: ?o ministro-adjunto? (the vice-minister)
Spelling differences (0.7%: 2% / 0% / 0%): Some personal names are spelled differently in the same
text:
Mention A: ?Andr?e Villas-Boas?
Mention B: ?Andr?e Villas Boas?
5.2 Errors due to incorrect predicted (syntactic and morpho-syntactic) analysis (15.3%: 2% /
22% / 22%)
Since the corpora do not have PoS-tagging and dependency labels fully revised, some of these errors
involve missing and spurious links between mentions.
Errors in syntactic constraints (10.7%: 0% / 16% / 16%): Direct and indirect object pronouns
incorrectly labeled are not covered by some of the syntactic constraints, thus involving an incorrect link
between a pronoun and its subject noun phrase.
9
The results of 0% in some languages and categories do not mean that these languages cannot have those error typologies,
but they did not appear due to the small number of errors evaluated.
747
Incorrect gender (2.7%: 2% / 4% / 2%): The gender of some nouns and adjectives also can be
wrongly labeled, so other mentions may be incorrectly linked, or involve a missing link. For instance,
the word atleta (sportsperson, which can be both masculine or feminine), labeled as masculine blocked
a link to the feminine pronoun ela (she) in Galician.
Incorrect head (2%: 0% / 2% / 4%): Errors in PoS-tagging (usually between nouns and adjectives)
also produce wrong dependency analysis, which in turn involve incorrect extractions of the NP heads:
Mention: ?el jugador alem?an? (the german player)
Extracted Head: *alem?an (*german, instead of jugador/player)
5.3 Missing links due to long distance pronominal anaphora (11.3%: 14% / 18% / 2%)
This kind of errors arises when the distance between a pronoun and its nominal antecedent is outside
the scope of a rule (in our case, between two and four sentences, depending on the module), and the
antecedent is not the pivot entity.
5.4 Errors due to quoted speech coreference (10%: 10% / 14% / 6%)
Another category of errors includes mentions inside quoted speech. These mentions can refer to the
speaker (first person) or to a third person in the quoted speech:
First person (4.7%: 6% / 6% / 2%): The 1
st
person of the quoted speech should be linked to the
speaker instead of to a previous entity (note that the elliptical pronoun might also be a 3
rd
person pro-
noun):
?Si ?
1
st
tuviera que redactar [...]?, resumi?o Lezcano
Speaker
.
?If [I
1
st
] had to write [...]?, Lezcano
Speaker
summarized.
Third person (5.3%: 4% / 8% / 4%): 3
rd
persons of a quoted speech should not be linked to the
speaker:
Gustavo
Speaker
: ?Cuando yo
1
st
me fui, ?el
3
rd
dej?o Boca.?
Gustavo
Speaker
: ?When I
1
st
quit, he
3
rd
left Boca.?
5.5 Spurious links in plural mentions (5.3%: 4% / 4% / 8%)
Coreference of plural mentions was performed through basic links to the previous entities, producing
incorrect classifications. Also, some plural mentions include entities with different genders (e.g., amigos
?friends? may refer to feminine and masculine entities, but the grammatical gender of the word is
masculine in the three analyzed languages):
1
[Hulk]
1
,
2
[Moutinho]
2
e
3
[
?
Alvaro Pereira]
3
na lista de compra de
4
[Villas-Boas]
4
[...].
5/6/7
[O
trio do F.C. Porto]
2/3/*4
[...].
1
[Hulk]
1
,
2
[Moutinho]
2
and
3
[
?
Alvaro Pereira]
3
in the shopping list of
4
[Villas-Boas]
4
[...].
5/6/7
[The F.C. Porto trio]
2/3/*4
[...].
In this example, the plural mention (O trio do F.C. Porto) is linked to the previous nominal mentions
with gender agreement, so an incorrect link between mentions 7 and 4 is done.
5.6 Errors due to incorrect gender agreement (4.7%: 4% / 4% / 6%)
Some nominal phrases referring to the same entity may have different gender, thus causing wrong links:
Mention A: la v??ctima (the victim: feminine)
Mention B: el muchacho (the boy: masculine)
748
5.7 Errors produced by constraints and Pivot Ent modules (4.6%: 6% / 0% / 8%)
The syntactic constraints, although precise, may restrict some correct links. This can involve (i) an
incorrect discourse analysis or (ii) the application of Pivot Ent, linking the mention to the most frequent
entity, which might be incorrect:
1
[El escritor]
1
tuvo que visitar a
2
[Mart??n]
2
en el hotel. Seg?un
3
?
*1
dijo [...]
1
[The writer]
1
had to visit
2
[Mart??n]
2
in the hotel. As
3
[he]
*1
said [...]
Here, the elliptical subject of dijo (said) is Mart??n, but the link is blocked due to a syntactic constraint:
the antecedent of the (subject) elliptical pronoun should be a subject. Thus, the system incorrectly links
mention 3 to mention 1.
5.8 Spurious links between Noun Phrases sharing the same head (1.3%: 0% / 4% / 0%)
In the same text, different entities can share their heads in some mentions, which may involve errors
in coreference links, depending on their position and on their features. Thus, the NP ?the president?
may be linked to two different persons like ?the president of the Academia? and ?the president of the
Government?.
5.9 Spurious links produced by errors in previous modules (0.7%: 0% / 2% / 0%)
First modules also produce some incorrect clusters which involve errors in further modules. For in-
stance, in the Galician corpus, NP Cataphora incorrectly linked the noun phrase o alcalde (the mayor)
to the proper noun ?Dorribo?. Then, HeadMatch merged ?Dorribo? with o alcalde Orozco, creating an
incorrect entity that contains two different persons (Dorribo and Orozco).
5.10 Errors due to fixed language structures (0.7%: 2% / 0% / 0%)
Other minor errors include some fixed structures such as the following cataphoric possessive:
Por
1
[sua]
1
parte,
2
[Cristina]
*2
[...]
For
1
[her]
1
part,
2
[Cristina]
*2
[...]
The results of the error analysis bring interesting information to further work. Thus, including some
kind of semantic knowledge (synonyms), improving pronominal coreference resolution and implement-
ing specific rules for quoted speech might solve many of the most frequent errors made by LinkPeople.
6 Conclusions and Further Work
This paper presents the first version of LinkPeople, an open-source entity-centric approach for corefer-
ence resolution of person entities which applies a battery of deterministic modules enriched with precise
linguistic information.
The system was evaluated in three different languages (Portuguese, Galician and Spanish), clearly
surpassing some powerful baselines and achieving promising results.
The addition of rules focused on cataphoric coreference as well as pronominal constraints based on
syntactic and discourse restrictions increases the performance of similar approaches with lack of this
kind of knowledge.
Current work explores better nominal (Elsner and Charniak, 2010) and pronominal constraints and
dedicated handling of plural mentions. In further work, the implementation of an inheritance constraint
is planned, which could prevent the merging of partial entities if their mentions were blocked by previous
modules. Moreover, the extension of the system for solving the coreference of other types of entities is
also planned.
Acknowledgments
This work has been supported by the HPCPLN project ? Ref: EM13/041 (Galician Government) and by
the Celtic ? Ref: 2012-CE138 and Plastic ? Ref: 2013-CE298 projects (Feder-Interconnecta).
749
References
Amit Bagga and Breck Baldwin. 1998. Algorithms for scoring coreference chains. In Proceedings of the Work-
shop on Linguistic Coreference at the International Language Resources and Evaluation Conference (LREC
1998), volume 1, pages 563?566.
Breck Baldwin. 1997. CogNIAC: high precision coreference with limited knowledge and linguistic resources. In
Proceedings of a Workshop on Operational Factors in Practical, Robust Anaphora Resolution for Unrestricted
Texts, pages 38?45. Association for Computational Linguistics.
Kalina Bontcheva, Marin Dimitrov, Diana Maynard, Valentin Tablan, and Hamish Cunningham. 2002. Shallow
Methods for Named Entity Coreference Resolution. In Proceedings of the Workshop on Cha?nes de r?ef?erences
et r?esolveurs d?anaphores at Traitement Automatique des Langues Naturelles (TALN 2002).
Xavier Carreras, Llu??s M?arquez, and Llu??s Padr?o. 2003. A simple named entity extractor using AdaBoost. In
Proceedings of the 7th Conference on Computational Natural Language Learning (CoNLL 2003): Shared Task,
volume 4, pages 152?155. Association for Computational Linguistics.
Amanda Rocha Chaves and Lucia Helena Machado Rino. 2007. A resoluc??ao de pronomes anaf?oricos do portugu?es
com base em heur??sticas que apontam o antecedente. In Proceedings of VI Congresso de P?os-Graduac??ao da
UFSCar, volume 2, pages 1272?1273, S?ao Carlos, S?ao Paulo.
Thiago Thomes Coelho and Ariadne Maria Brito Rizzoni Carvalho. 2005. Uma adaptac??ao do algoritmo de Lappin
e Leass para resoluc??ao de an?aforas em portugu?es. In III Workshop em Tecnologia da Informac??ao e da Linguagem
Humana?TIL. Proceedings of XXV Congresso da SBC, pages 2069?2078.
William W. Cohen, Pradeep Ravikumar, and Stepehn G. Fienberg. 2003. A Comparison of String Distance Metrics
for Name-Matching Tasks. In Proceedings of the IJCAI 2003 Workshop on Information Integration on the Web,
pages 73?78.
Ramon R?e Moya Cuevas and Invandr?e Paraboni. 2008. A machine learning approach to portuguese pronoun
resolution. In Advances in Artificial Intelligence (IBERAMIA 2008), pages 262?271. Springer-Verlag.
Jos?e Guilherme C. de Souza, Patr??cia Gonc?alves, and Renata Vieira. 2008. Learning Coreference Resolution for
Portuguese Texts. In Computational Processing of the Portuguese Language (PROPOR 2008), pages 153?162.
Springer-Verlag.
Micha Elsner and Eugene Charniak. 2010. The same-head heuristic for coreference. In Proceedings of the 48th
Association for Computational Linguistics Conference Short Papers (ACL 2010), pages 33?37. Association for
Computational Linguistics.
Antonio Ferr?andez and Jes?us Peral. 2000. A computational approach to zero-pronouns in Spanish. In Proceed-
ings of the 38th Annual Meeting on Association for Computational Linguistics (ACL 2000), pages 166?172.
Association for Computational Linguistics.
Pablo Gamallo and Marcos Garcia. 2011. A resource-based method for named entity extraction and classification.
In Progress in Artificial Intelligence (LNCS/LNAI), volume 7026/2011, pages 610?623, Berlin. Springer-Verlag.
Pablo Gamallo and Isaac Gonz?alez L?opez. 2011. A Grammatical Formalism Based on Patterns of Part-of-Speech
Tags. International Journal of Corpus Linguistics, 16(1):45?71.
Marcos Garcia and Pablo Gamallo. 2010. An?alise Morfossint?actica para Portugu?es Europeu e Galego: Problemas,
Soluc??oes e Avaliac??ao. Linguam?atica, 2(2):59?67.
Marcos Garcia and Pablo Gamallo. 2013. Exploring the Effectiveness of Linguistic Knowledge
for Biographical Relation Extraction. Natural Language Engineering. Available on CJO 2013
doi:10.1017/S1351324913000314.
Marcos Garcia and Pablo Gamallo. 2014. Multilingual corpora with coreferential annotation of person entities.
In Proceedings of the 9th edition of the Language Resources and Evaluation Conference (LREC 2014), pages
3229?3233. European Language and Resources Association.
Marcos Garcia, Iria Gayo, and Isaac Gonz?alez L?opez. 2012. Identificac??ao e Classificac??ao de Entidades Men-
cionadas em Galego. Estudos de Ling?u??stica Galega, 4:13?25.
Aria Haghighi and Dan Klein. 2007. Unsupervised coreference resolution in a nonparametric bayesian model. In
Proceedings of the 45th Annual Meeting on Association for Computational Linguistics (ACL 2007), volume 45,
pages 848?855. Association for Computational Linguistics.
750
Shalom Lappin and Herbert J. Leass. 1994. An algorithm for pronominal anaphora resolution. Computational
linguistics, 20(4):535?561.
Heeyoung Lee, Angel Chang, Yves Peirsman, Nathanael Chambers, Mihai Surdeanu, and Dan Jurafsky. 2013.
Deterministic coreference resolution based on entity-centric, precision-ranked rules. Computational Linguistics,
39(4):885?916.
Xiaoqiang Luo. 2005. On Coreference Resolution Performance Metrics. In Proceedings of the conference on
Human Language Technology and Empirical Methods in Natural Language Processing (HLT/EMNLP 2005),
pages 25?32. Association for Computational Linguistics.
Gideon S. Mann. 2002. Fine-grained proper noun ontologies for question answering. In Proceedings of the 2002
workshop on Building and using semantic networks, volume 11. Association for Computational Linguistics.
Joseph McCarthy and Wendy G. Lehnert. 1995. Using decision trees for coreference resolution. In Proceedings
of the 14th International Conference on Artificial Intelligence, pages 1050?1055.
Ruslan Mitkov. 1998. Robust pronoun resolution with limited knowledge. In Proceedings of the 36th Annual
Meeting of the Association for Computational Linguistics and 17th International Conference on Computational
Linguistics (ACL/COLING 1998), volume 2, pages 869?875. Association for Computational Linguistics.
Vincent Ng and Claire Cardie. 2002. Improving machine learning approaches to coreference resolution. In
Proceedings of the 40th Annual Meeting on Association for Computational Linguistics (ACL 2002), pages 104?
111. Association for Computational Linguistics.
Vincent Ng. 2008. Unsupervised models for coreference resolution. In Proceedings of the Conference on Empir-
ical Methods in Natural Language Processing (EMNLP 2008), pages 640?649. Association for Computational
Linguistics.
Llu??s Padr?o and Evgeny Stanilovsky. 2012. FreeLing 3.0: Towards Wider Multilinguality. In Proceedings of
the Language Resources and Evaluation Conference (LREC 2012), Turkey. European Language and Resources
Association.
Manuel Palomar, Antonio Ferr?andez, Lidia Moreno, Patricio Mart??nez-Barco, Jes?us Peral, Maximiliano Saiz-
Noeda, and Rafael Mu?noz. 2001. An algorithm for anaphora resolution in Spanish texts. Computational
Linguistics, 27(4):545?567.
Ivandr?e Paraboni. 1997. Uma arquitetura para a resoluc??ao de refer?encias pronominais possessivas no processa-
mento de textos em l??ngua portuguesa. Master?s thesis, Pontif??cia Universidade Cat?olica do Rio Grande do Sul,
Porto Alegre.
Simone Paolo Ponzetto and Michael Strube. 2006. Exploiting semantic role labeling, WordNet and Wikipedia
for coreference resolution. In Proceedings of the main conference on Human Language Technology Conference
of the North American Chapter of the Association of Computational Linguistics (HLT/NAACL 2006), pages
192?199. Association for Computational Linguistics.
Sameer Pradhan, Lance Ramshaw, Mitchell Marcus, Martha Palmer, Ralph Weischedel, and Nianwen Xue. 2011.
CoNLL-2011 Shared Task: Modeling Unrestricted Coreference in OntoNotes. In Proceedings of the 15th Con-
ference on Computational Natural Language Learning (CoNLL 2011): Shared Task, pages 1?27. Association
for Computational Linguistics.
Kathik Raghunathan, Heeyoung Lee, Sudarshan Rangarajan, Nathanael Chambers, Mihai Surdeanu, Dan Jurafsky,
and Christopher Manning. 2010. A multi-pass sieve for coreference resolution. In Proceedings of the 2010
Conference on Empirical Methods in Natural Language Processing (EMNLP 2010), pages 492?501. Associa-
tion for Computational Linguistics.
Marta Recasens and Eduard Hovy. 2009. A deeper look into features for coreference resolution. In Anaphora
Processing and Applications, pages 29?42. Springer-Verlag.
Marta Recasens and Eduard Hovy. 2010. Coreference resolution across corpora: Languages, coding schemes, and
preprocessing information. In Proceedings of the 48th Annual Meeting of the Association for Computational
Linguistics (ACL 2010), pages 1423?1432. Association for Computational Linguistics.
Marta Recasens and Eduard Hovy. 2011. BLANC: Implementing the Rand Index for Coreference Evaluation.
Natural Language Engineering, 17(4):485?510.
Marta Recasens and M. Ant`onia Mart??. 2010. AnCora-CO: Coreferentially annotated corpora for Spanish and
Catalan. Language Resources and Evaluation, 44.4:315?345.
751
Marta Recasens, Llu??s M`arquez, Emili Sapena, M. Ant`onia Mart??, Mariona Taul?e, V?eronique Hoste, Massimo
Poesio, and Yannick Versley. 2010. SemEval-2010 Task 1: Coreference resolution in multiple languages. In
Proceedings of the 5th International Workshop on Semantic Evaluation (SemEval?10), pages 1?8. Association
for Computational Linguistics.
Emili Sapena, Llu??s Padr?o, and Jordi Turmo. 2013. A Constraint-Based Hypergraph Partitioning Approach to
Coreference Resolution. Computational Linguistics, 39(4).
Wee Meng Soon, Hwee Tou Ng, and Daniel Chung Yong Lim. 2001. A machine learning approach to coreference
resolution of noun phrases. Computational linguistics, 27(4):521?544.
Veselin Stoyanov and Jason Eisner. 2012. Easy-first coreference resolution. In Proceedings of the International
Conference on Computational Linguistics (COLING 2012), pages 2519?2534.
Olga Uryupina. 2007. Knowledge acquisition for coreference resolution. Ph.D. thesis, Universit?at des Saarlandes.
Renata Vieira and Massimo Poesio. 2000. An empirically based system for processing definite descriptions.
Computational Linguistics, 26(4):539?593.
Marc Vilain, John Burger, John Aberdeen, Dennis Connolly, and Lynette Hirschman. 1995. A model-theoretic
coreference scoring scheme. In Proceedings of Message Understanding Conference 6 (MUC-6), pages 45?52.
Association for Computational Linguistics.
752
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 171?175,
Dublin, Ireland, August 23-24, 2014.
Citius: A Naive-Bayes Strategy for Sentiment Analysis on English Tweets
?
Pablo Gamallo
CITIUS
Univ. de Santiago de Compostela
pablo.gamallo@usc.es
Marcos Garcia
Cilenis Language Technology, S.L.
marcos.garcia@cilenis.com
Abstract
This article describes a strategy based on a
naive-bayes classifier for detecting the po-
larity of English tweets. The experiments
have shown that the best performance is
achieved by using a binary classifier be-
tween just two sharp polarity categories:
positive and negative. In addition, in or-
der to detect tweets with and without po-
larity, the system makes use of a very basic
rule that searchs for polarity words within
the analysed tweets/texts. When the clas-
sifier is provided with a polarity lexicon
and multiwords it achieves 63% F-score.
1 Introduction
Sentiment Analysis consists in finding the opin-
ion (e.g. positive, negative, or neutral) from text
documents such as movie reviews or product re-
views. Opinions about movies, products, etc. can
be found in web blogs, social networks, discus-
sion forums, and so on. Companies can improve
their products and services on the basis of the re-
views and comments of their costumers. Recently,
many works have stressed the microblogging ser-
vice Twitter. As Twitter can be seen as a large
source of short texts (tweets) containing user opin-
ions, most of these works make sentiment analysis
by identifying user attitudes and opinions toward
a particular topic or product (Go et al., 2009). The
task of making sentiment analysis from tweets is a
hard challenge. On the one hand, as in any senti-
ment analysis framework, we have to deal with hu-
man subjectivity. Even humans often disagree on
?
This work has been supported by the projects: HPC-
PLN: Ref:EM13/041 (Program Emergentes, Xunta de Gali-
cia), Celtic: Ref:2012-CE138 and Plastic: Ref:2013-CE298
(Program Feder-Innterconecta)
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
the categorization of the positive or negative sen-
timent that is supposed to be expressed on a given
text (Villena-Rom?an et al., 2013). On the other
hand, tweets are too short text to be linguistically
analyzed, and it makes the task of finding relevant
information (e.g. opinions) much harder.
The SemEval-2014 task ?Sentiment Analysis
in Twitter? is an evaluation competition that in-
cludes a specific task directly related to sentiment
analyisis. In particular, subtask B, called ?Mes-
sage Polarity Classification?, consists in classify-
ing whether a given message is of positive, neg-
ative, or neutral sentiment. For messages con-
veying both a positive and negative sentiment, the
stronger sentiment should be chosen. The results
of our system in this task are situated in the aver-
age out of 51 evaluated systems.
In this article, we describe the learning strate-
gies we developed so as to perform this task, all of
them based on bayesian classification.
2 Naive Bayes Classifier
Most of the algorithms for sentiment analysis
are based on a classifier trained using a collec-
tion of annotated text data. Before training, data
is preprocessed so as to extract the main fea-
tures. Some classification methods have been pro-
posed: Naive Bayes, Support Vector Machines, K-
Nearest Neighbors, etc. However, and according
to (Go et al., 2009), it is not clear which of these
classification strategies is the more appropriate to
perform sentiment analysis.
We decided to use a classification strategy based
on Naive Bayes (NB) because it is a simple and
intuitive method whose performance is similar to
other approaches. NB combines efficiency (opti-
mal time performance) with reasonable accuracy.
The main theoretical drawback of NB methods is
that it assumes conditional independence among
the linguistic features. If the main features are the
tokens extracted from texts, it is evident that they
171
cannot be considered as independent, since words
co-occuring in a text are somehow linked by dif-
ferent types of syntactic and semantic dependen-
cies. However, even if NB produces an oversim-
plified model, its classification decisions are sur-
prinsingly accurate (Manning et al., 2008).
2.1 Strategy
Two different naive bayes classifiers have been
built, according to two different strategies:
Baseline This is a naive bayes classifier that
learns from the original training corpus how
to classify the three categories found in the
corpus: Positive, Negative, and Neutral. So,
no modification has been introduced in the
training corpus.
Binary The second classifier was trained on a
simplified training corpus and makes use of
a polarity lexicon. The corpus was simpli-
fied since only positive and negative tweets
were considered. Neutral tweets were not
taken into account. As a result, a basic bi-
nary (or boolean) classifier which only iden-
tifies both Positive and Negative tweets was
trained. In order to detect tweets without po-
larity (or Neutral), the following basic rule is
used: if the tweet contains at least one word
that is also found in the polarity lexicon, then
the tweet has some degree of polarity. Othe-
wise, the tweet has no polarity at all and is
classified as Neutral. The binary classifier
is actually suited to specify the basic polar-
ity between positive and negative, reaching a
precision of more than 80% in a corpus with
just these two categories.
3 Preprocessing
As we will describe in the next section, the main
features of the model are lemmas extracted using
lemmatization. Given that the language of mi-
croblogging requires a special treatment, we pro-
pose a pre-processing task to correct and normal-
ize the tweets before lemmatizing them.
The main preprocessing tasks we considered are
the following:
? removing urls, references to usernames, and
hashtags
? reduction of replicated characters (e.g.
looooveeee? love)
? identifying emoticons and interjections and
replacing them with polarity or sentiment ex-
pressions (e.g. :-)? good)
4 Features
The features considered by the classifier are lem-
mas, multiwords, polarity lexicons, and valence
shifters.
4.1 Lemmas (UL)
To characterise the main features underlying the
classifier, we make use of unigrams of lemmas in-
stead of tokens to minimize the problems derived
from the sparse distribution of words. Moreover,
only lemmas belonging to lexical categories are
selected as features, namely nouns, verbs, adjec-
tives, and adverbs. So, grammatical words, such
as determiners, conjunctions, and prepositions are
removed from the model.
To configure the feature representation, the fre-
quency of each selected lemma in a tweet is stored.
4.2 Multiwords (MW)
There is no agreement on which is the best option
for sentiment analysis (unigrams, bigrams, ...). In
(Pak and Paroubek, 2010), the best performance
is achieved with bigrams, while (Go et al., 2009)
show that the better results are reached with uni-
grams. An alternative option is to make use of a
selected set of n-grams (or multiwords) identified
by means of regular patterns of PoS tags. Multi-
word expressions identified by means of PoS tags
patterns can be conceived as linguistically moti-
vated terms, since most of them are pairs of words
linked by syntactic dependencies.
So, in addition to unigrams of lemmas, we also
consider multiwords extracted by an algorithm
based on patterns of PoS tags. In particular, we
used the following set of patterns:
? NOUN-ADJ
? NOUN-NOUN
? ADJ-NOUN
? NOUN-PRP-NOUN
? VERB-NOUN
? VERB-PRP-NOUN
The instances of bigrams and trigrams extracted
with these patterns ared added to the unigrams
172
to build the language model. Multiword extrac-
tion was performed using our tool GaleXtra
1
, re-
leased under GPL license and described in (Mario
Barcala and Eva Dom??nguez and Pablo Gamallo
and Marisol L?opez and Eduardo Moscoso and
Guillermo Rojo and Paula Santalla and Susana
Sotelo, 2007).
4.3 Polarity Lexicon (LEX)
We have built a polarity lexicon with both Positive
and Negative entries from different sources:
? AFINN-111
2
contains 2, 477 word forms,
which were lemmatized and converted into
1, 520, positive and negative lemmas.
? Hedonometer
3
contains about 10, 000 fre-
quent words extracted from tweets which
were classified as expressing some degree of
hapiness (Dodds et al., 2011). We selected
the 300 most positive lemmas from the initial
list.
? Hu&Liu list (Liu et al., 2005) contains over
6, 800 words out of which 5 positive and neg-
ative lemmas were selected 5, 695.
? Sentiwordnet-3.0 (Baccianella et al., 2010)
contains more than 100, 000 entries. We se-
lected a subset of 6, 600 positive and negative
lemmas with the highest polarity values.
? Finally, we have built a polarity lexicon with
10, 850 entries by merging the previous ones.
The final polarity lexicon is used in two differ-
ent ways: on the one hand, it is used to identify
neutral tweets, since a tweet is considered as being
neutral if it does not contain any lemma appearing
in the polarity lexicon. On the other hand, we have
built artificial tweets as follows: each entry of the
lexicon is converted into an artificial tweet with
just one lemma inheriting the polarity (positive or
negative) from the lexicon. The frequency of the
word in each new tweet is the average frequency
of lemmas in the training corpus. These artificial
tweets will be taken into account for training the
classifiers.
1
http://gramatica.usc.es/\
?
gamallo/
gale-extra/index.htm
2
http://arxiv.org/abs/1103.2903
3
http://www.hedonometer.org/
4.4 Valence Shifters (VS)
We take into account negative words that can shift
the polarity of specific lemmas in a tweet. In
the presented work, we will make use of only
those valence shifters that reverse the sentiment of
words, namely negations. The strategy to identify
the scope of negations relies on the PoS tags of the
negative word as well as of those words appearing
to its right in the sequence. The algorithm is as
follows:
Whenever a negative word is found, its PoS tag
is considered and, according to its syntactic prop-
erties, we search for a polarity word (noun, verb,
or adjective) within a window of 2 words after the
negation. If a polarity word is found and is syntac-
tically linked to the negative word, then its polarity
is reversed. For instance, if the negation word is
the adverb ?not?, the system only reverses the po-
larity of verbs or adjectives appearing to its right.
Nouns are not syntactically linked to this adverb.
By contrast, if the negation is the determiner ?no?
or ?none?, only the polarity of nouns can be re-
versed. Our strategy to deal with negation scope
is not so basic as those described in (Yang, 2008)
and (Anta et al., 2013), which are just based on
a rigid window after the negation word: 1 and 3
words, respectively.
5 Experiments and Evaluation
5.1 Training corpus
In our preliminary experiments we have used the
training dataset of tweets provided by SemEval-
2014 organization (tweeti-b.dist.tsv). This set
contains 6, 408 tweets, which were tagged with
the following polarity values: Positive, Nega-
tive, Neutral, Objective, and Neutral-or-Objective.
In order to fill the requirements of the task, we
transformed Neutral, Objective, and Natural-or-
Objective into a single tag: Neutral. In addi-
tion, we also used a selection of annotated tweets
(namely 5, 050 positive and negative ones), which
were compiled from an external source (Narr et al.,
2012). Using the terminology provided by the or-
ganizers of SemEval-2014, we call ?constrained?
the systems trained with only the dataset provided
by the organization and ?unconstrained? the sys-
tems trained with both datasets.
5.2 Evaluated classifiers
We have implemented and evaluated several clas-
sifiers by making use of the two strategies de-
173
scribed in section 2, combined with the features
defined in 4. We also distinguished those clas-
sifiers trained with only tweeti-b.dist.tsv (con-
strained systems) from those trained with both in-
ternal and external datasets (unconstrained). As a
result, we implemented the following classifiers:
CONSTRAINED-BASELINE: This system
was implemented on the basis of the ?Base-
line? strategy and the following two features:
unigrams of lemmas (UL) and valence
shifters (VS).
CONSTRAINED-BASELINE-LEX: This sys-
tem was implemented on the basis of the
?Baseline? strategy and the following three
features: unigrams of lemmas (UL), polarity
lexicon (LEX), and valence shifters (VS).
CONSTRAINED-BINARY-LEX: This system
was implemented on the basis of the ?Base-
line? strategy and the following three fea-
tures: unigrams of lemmas (UL), polarity
lexicon (LEX), and valence shifters (VS).
CONSTRAINED-BINARY-LEX-MW: This
system was implemented on the basis of the
?Binary? strategy and the following features:
unigrams of lemmas (UL), multiwords
(MW), polarity lexicon (LEX), and valence
shifters (VS).
UNCONSTRAINED-BINARY-LEX: This sys-
tem was implemented on the basis of the
?Binary? strategy and the following features:
unigrams of lemmas (UL), polarity lexicon
(LEX), and valence shifters (VS).
UNCONSTRAINED-BINARY-LEX-MW:
This system was implemented on the basis of
the ?Binary? strategy and the following fea-
tures: unigrams of lemmas (UL), multiwords
(MW), polarity lexicon (LEX), and valence
shifters (VS).
All the classifers have been implemented with
Perl language. They rely on the naive-bayes algo-
rithm and incorporate the preprocessing tasks de-
fined in section 3.
5.3 Evaluation
To evaluate the classification performance of these
classifiers, we used as test corpus another dataset
provided by the organization: tweeti-b.devel.tsv.
The results are shown in table 1, where the names
of the evaluated systems are in the first column and
F-Score in the second one.
System F-score
CONSTR-BASE .49
CONSTR-BASE-LEX .56
CONSTR-BIN-LEX .57
CONSTR-BIN-LEX-MW .61
UNCONSTR-BIN-LEX .58
UNCONSTR-BIN-LEX-MW .63
Table 1: Results of our six systems
.
The results show that there is an improve-
ment in performance when the classifiers are im-
plemented with the Binary strategy, when they
use a polarity lexicon, and when multiwords are
considered as features. The two systems sub-
mmited to Semeval competition were those ob-
tained the best scores: CONSTR-BIN-LEX-MW
and UNCONSTR-BIN-LEX-MW. The scores ob-
tained by these two systems in the competition
are very similar to those obtained in the experi-
ments depicted in Table 1. More precisely, in the
Tweets2014 test corpus, the constrained system
reached 0.62 F-score while the unconstrained ver-
sion achieved 0.63. Our best system was ranked
as 26th from 53 systems. A Spanish version of
this system (Gamallo et al., 2013) also participated
in the TASS-2013 competition (Villena-Rom?an et
al., 2013), where it was ranked as the 3th best sys-
tem out of 13 participants.
6 Conclusions
We have presented a family of naive-bayes classi-
fiers for detecting the polarity of English tweets.
The experiments have shown that the best per-
formance is achieved by using a binary classi-
fier trained to detect just two categories: posi-
tive and negative. In order to detect tweets with
and without polarity we used a very basic strat-
egy based on searching for polarity lemmas within
the text/tweet. If the tweet does not contain at
least one lemma also found in an external polarity
lexicon, then the tweet has not any polarity and,
thereby, is tagged with the Neutral value. The use
of both a polarity lexicon and multiwords also im-
proves the results in a significant way. Our sys-
tem is being used by Cilenis S.L, a company spe-
cialised in natural language technology, and being
applied to four languages: English, Spanish, Por-
tuguese, and Galician.
174
References
Antonio Fern?andez Anta, Luis N?u?nez Chiroque,
Philippe Morere, and Agust??n Santos. 2013. Sen-
timent Analysis and Topic Detection of Spanish
Tweets: A Comparative Study of NLP Techniques.
Procesamiento del Lenguaje Natural, 50:45?52.
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2010. SentiWordNet 3.0: An Enhanced Lex-
ical Resource for Sentiment Analysis and Opinion
Mining. In Human Language Technology Confer-
ence - North American chapter of the Association
for Computational Linguistics, pages 2200?2204.
Peter Sheridan Dodds, Kameron Decker Harris, Is-
abel M. Kloumann, Catherine A. Bliss, and Christo-
pher M. Danforth. 2011. Temporal patterns of
happiness and information in a global social net-
work: Hedonometrics and Twitter. PLoS ONE,
6(12):e26752.
Pablo Gamallo, Marcos Garcia, and Santiago
Fern?andez-Lanza. 2013. TASS: A Naive-Bayes
strategy for sentiment analysis on Spanish tweets.
In Workshop on Sentiment Analysis at SEPLN
(TASS2013), pages 126?132, Madrid, Spain.
Alec Go, Richa Bhayani, and Lei Huang. 2009. Twit-
ter sentiment classification using distant supervision.
In CS224N Technical report. Standford.
Bing Liu, Minqing Hu, and Junsheng Cheng. 2005.
Opinion observer: Analyzing and comparing opin-
ions on the web. In 14th International World Wide
Web conference (WWW-2005), pages 342?351, New
York, NY, USA.
Chris Manning, Prabhakar Raghadvan, and Hinrich
Sch?utze. 2008. Introduction to Information Re-
trieval. Cambridge University Press, Cambridge,
MA, USA.
Mario Barcala and Eva Dom??nguez and Pablo Gamallo
and Marisol L?opez and Eduardo Moscoso and
Guillermo Rojo and Paula Santalla and Susana
Sotelo. 2007. A Corpus and Lexical Resources for
Multi-word Terminology Extraction in the Field of
Economy. In 3rd Language & Technology Confer-
ence (LeTC?2007), pages 355?359, Poznan, Poland.
Sascha Narr, Michael Hulfenhaus, and Sahin Albayrak.
2012. Language-Independent Twitter Sentiment
Analysis. In Knowledge Discovery and Machine
Learning (KDML), LWA, pages 12?14, Dortmund,
Germany.
Alexander Pak and Patrick Paroubek. 2010. Twitter as
a Corpus for Sentiment Analysis and Opinion Min-
ing. In LREC-2010, Valletta, Malta.
Julio Villena-Rom?an, Sara Lana, Eugeinio Mart??nez-
C?amara, and Juan Carlos Gonz?alez-Crist?obal. 2013.
TASS - Workshop on Sentiment Analysis at SEPLN.
Procesamiento del Lenguaje Natural, 50:37?44.
Kiduk Yang. 2008. WIDIT in TREC 2008 blog
track: Leveraging Multiple Sources of Opinion Ev-
idence. In The Seventeenth Text Retrieval Confer-
ence (TREC-2008), Gaithersburg, Maryland, USA.
175
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 10?18,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Dependency-Based Open Information Extraction
Pablo Gamallo and Marcos Garcia
Centro de Investigac?a?o sobre Tecnologias da Informac?a?o (CITIUS)
Universidade de Santiago de Compostela
pablo.gamallo@usc.es marcos.garcia.gonzalez@usc.es
Santiago Ferna?ndez-Lanza
Escola Superior de Enxen?er??a Informa?tica
Universidade de Vigo
sflanzal@uvigo
Abstract
Building shallow semantic representations
from text corpora is the first step to perform
more complex tasks such as text entailment,
enrichment of knowledge bases, or ques-
tion answering. Open Information Extrac-
tion (OIE) is a recent unsupervised strategy
to extract billions of basic assertions from
massive corpora, which can be considered
as being a shallow semantic representation
of those corpora. In this paper, we propose
a new multilingual OIE system based on ro-
bust and fast rule-based dependency pars-
ing. It permits to extract more precise as-
sertions (verb-based triples) from text than
state of the art OIE systems, keeping a cru-
cial property of those systems: scaling to
Web-size document collections.
1 Introduction
There is an increasing interest in capturing shal-
low semantic representations from large amounts
of text, with the aim of elaborating more com-
plex semantic tasks involved in text understand-
ing, such as textual entailment, filling knowledge
gaps in text, or integration of text information
into background knowledge bases. Two recent
approaches to text understanding are interested in
shallow semantics: Machine Reading (Etzioni et
al., 2006) and Learning by Reading (Barker et al,
2007). Both approaches aim at understanding text
by starting with a very basic representation of the
facts conveyed by the input text. In addition, they
rely on unsupervised strategies. There are, how-
ever, two significant differences between Machine
Reading and Learning by Reading:
The first difference concerns the basic repre-
sentation required at the beginning of the under-
standing process. While Machine Reading is fo-
cused on fixed structures (triples), constituted by
a relation (a verb or verb phrase) and two argu-
ments, in Learning by Reading the text is rep-
resented by means of more flexible predicate-
argument structures (n-tuples) derived from syn-
tactic dependency trees. In Learning by Reading,
on the one hand, relations with more than two ar-
guments are also extracted, and on the other, rela-
tions are not restricted to verb phrases but to what-
ever relation expressed by a dependency based
triple, (head, relation, modifier), also called Ba-
sic Element (Hovy et al, 2005). The second dif-
ference is related to the notion of text domain.
Whereas Machine Reading works on open rela-
tions and unrestricted topics and domains, Learn-
ing by Reading prefers being focused on domain-
specific texts in order to build a semantic model
of a particular topic.
One of the major contributions of Machine
Reading is the development of an extraction
paradigm, called Open Information Extraction
(OIE), which aims at extracting a large set of verb-
based triples (or assertions) from unrestricted text.
An OIE system reads in sentences and rapidly ex-
tracts one or more textual assertions, consisting
in a verb relation and two arguments, which try
to capture the main relationships in each sentence
(Banko et al, 2007). Unlike most relation ex-
traction methods which are focused on a prede-
fined set of target relations, OIE is not limited to
a small set of target relations known in advance,
but extracts all types of (verbal) binary relations
found in the text. The OIE system with best per-
formance, called ReVerb (Etzioni et al, 2011),
is a logistic regression classifier that takes as in-
put PoS-tagged and NP-chunked sentences. So,
10
it only requires shallow syntactic features to gen-
erate semantic relations, guaranteeing robustness
and scalability with the size of the corpus. One of
the main critics within the OIE paradigm against
dependency based methods, such as Learning by
Reading, concerns the computational cost asso-
ciated with rich syntactic features. Dependency
parsing could improve precision and recall over
shallow syntactic features, but at the cost of ex-
traction speed (Etzioni et al, 2011). In order to
operate at the Web scale, OIE systems needs to be
very fast and efficient.
In this paper, we describe an OIE method to
generate verb-based triples by taking into account
the positive properties of the two traditions: con-
sidering Machine Reading requirements, our sys-
tem is efficient and fast guaranteeing scalability as
the corpus grows. And considering ideas behind
Learning by Reading, we use a dependency parser
in order to obtain fine-grained information (e.g.,
internal heads and dependents) on the arguments
and relations extracted from the text. In addition,
we make extraction multilingual. More precisely,
our system has the following properties:
? Unsupervised extraction of triples repre-
sented at different levels of granularity: sur-
face forms and dependency level.
? Multilingual extraction (English, Spanish,
Portuguese, and Galician) by making use of
a multilingual rule-based parser, called Dep-
Pattern (Gamallo and Gonza?lez, 2011).
Our claim is that it is possible to perform
Open Information Extraction by making use of
very conventional tools, namely rule-based de-
pendency analysis and simple post-processing ex-
traction rules. In addition, we also show that we
can deal with knowledge-rich syntactic informa-
tion while remaining scalable.
This article is organized as follows. Section 2
introduces previous work on OIE: in particular it
describes three of the best known OIE systems up
to date. Next, in Section 3, the proposed method
is described in detail. Then, some experiments are
performed in Section 4, where our OIE system is
compared against ReVerb. In 5, we sketch some
applications that use the output of our OIE sys-
tem, and finally, conclusions and current work are
addressed in 6.
2 Open Information Extraction Systems
An OIE system extracts a large number of triples
(Arg1, Rel, Arg2) for any binary relation found in
the text. For instance, given the sentence ?Vigo
is the largest city in Galicia and is located in the
northwest of Spain?, an OIE system should ex-
tract two triples: (Vigo, is the largest city in, Galicia)
and (Vigo, is located in, northwest of Spain). Up to
now, OIE is focused only on verb-based relations.
Several OIE systems have been proposed, all of
them are based on an extractor learned from la-
belled sentences. Some of these systems are:
? TextRunner (Banko et al, 2008): the ex-
tractor is a second order linear-chain CRF
trained on samples of triples generated from
the Penn Treebank. The input of TextRunner
are PoS-tagged and NP-chunked sentences,
both processes performed with OpenNLP
tools.
? WOE (Wu and Weld, 2010): the extractor
was learned by identifying the shortest de-
pendency paths between two noun phrases,
using training examples of Wikipedia. The
main drawback is that extraction is 30 times
slower than TextRunner.
? ReVerb (Etzioni et al, 2011; Fader et al,
2011): the extractor is a logistic regression
classifier trained with shallow syntactic fea-
tures, which also incorporates lexical con-
straints to filter out over-specified relation
phrases. It takes as input the same features
as TextRunner, i.e., PoS-tagged and NP-
chunked sentences analyzed with OpenNLP
tools. It is considered to be the best OIE
system up to now. Its performance is 30%
higher than WOE and more than twice that
of TextRunner.
One of the most discussed problems of OIE
systems is that about 90% of the extracted triples
are not concrete facts (Banko et al, 2007) ex-
pressing valid information about one or two
named entities, e.g. ?Obama was born in Hon-
olulu?. However, the vast amount of high con-
fident relational triples extracted by OIE systems
are a very useful startpoint for further NLP tasks
and applications, such as common sense knowl-
edge acquisition (Lin et al, 2010), and extrac-
tion of domain-specific relations (Soderland et al,
11
2010). The objective of OIE systems is not to ex-
tract concrete facts, but to transform unstructured
texts into structured information, closer to ontol-
ogy formats.
Nevertheless, some linguistics problems arise.
OIE systems were trained to identify only verb
clauses within the sentences and, therefore, to
extract just binary verb-based relations from the
clause structure. It follows that they cannot be
easily adapted to learn other non-clausal relations
also found in the text. Let us take the following
sentence: ?The soccer player of FC Barcelona,
Lionel Messi, won the Fifa World Player of the
Year award?. In addition to the main verb-based
relationship:
(Lionel Messi, won, the Fifa Worlds Player
of the Year award)
which could be extracted by the OIE systems in-
troduced above, it should also be important to ex-
tract other non-verbal relations found within the
noun phrases:
(Messi, is, a soccer player of FC Barcelona)
(Fifa World Player of the Year, is, an award)
However, the cited systems were not trained to
learn such a basic relations.
Besides, the OIE systems are not adapted to
process clauses denoting events with many argu-
ments. Take the sentence: ?The first commercial
airline flight was from St. Petersburg to Tampa in
1914?. We should extract, at least, two or three
different relational triples from the verb clause
contained in this sentence, for instance:
(the first commercial airline flight, was from, St. Pe-
tersburg)
(the first commercial airline flight, was to, Tampa)
(the first commercial airline flight, was in, 1914)
Yet, current OIE systems are not able to perform
this multiple extraction. Even if the cited OIE
systems can identify several clauses per sentence,
they were trained to only extract one triple per
clause.
In the following, we will describe a
dependency-based OIE system that overcomes
these linguistic limitations.
3 A Dependency-Based Method for
Open Information Extraction
The proposed extraction method consists of three
steps organized as a chain of commands in a
pipeline:
Dependency parsing Each sentence of the input
text is analyzed using the dependency-based
parser DepPattern, a multilingual tool avail-
able under GPL license1.
Clause constituents For each parsed sentence,
we discover the verb clauses it contains and,
then, for each clause, we identify the verb
participants, including their functions: sub-
ject, direct object, attribute, and preposi-
tional complements.
Extraction rules A set of rules is applied on the
clause constituents in order to extract the tar-
get triples.
These three steps are described in detail below.
3.1 Dependency Parsing
To parse text, we use an open-source suite of mul-
tilingual syntactic analysis, DepPattern (Gamallo
and Gonza?lez, 2011). The suite includes basic
grammars for five languages as well as a compiler
to build parsers in Perl. A parser takes as input the
output of a PoS-tagger, either, FreeLing (Carreras
et al, 2004) or Tree-Tagger2. The whole process
is robust and fast. It takes 2600 words per second
on a Linux platform with 2.4GHz CPU and 2G
memory. The basic grammars of DepPattern con-
tain rules for many types of linguistic phenomena,
from noun modification to more complex struc-
tures such as apposition or coordination. However
their coverage is still not very high. We added
several rules to the DepPattern grammars in En-
glish, Spanish, Portuguese, and Galician, in order
to improve the coverage of our OIE system.
The output of a DepPattern parser consists
of sentences represented as binary dependencies
from the head lemma to the dependent lemma:
rel(head, dep). Consider the sentence ?The coach
of Benfica has held a press conference in Lisbon?.
1htpp://gramatica.usc.es/pln/tools/
deppattern.htm
2http://www.ims.uni-stuttgart.
de/projekte/corplex/TreeTagger/
DecisionTreeTagger.html
12
(have hold)vp
(the coach of benfica)np
subj
(a press conference)np
dobj
(in lisbon)pp
vprep
Figure 1: Constituency tree with function information
The DepPattern dependencies are the following:
spec(coach-2, the-1)
nprep(coach-2, of-3)
term(of-3, benfica-4)
aux(hold-6, have-5)
subj(hold-6, coach-2)
dobj(hold-6, conference-9)
spec(conference-9, a-7)
modif(conference-9, press-8)
vprep(hold-6, in-10)
term(in-10, lisbon-11)
The directed graph formed by these dependencies
will be the input of the following step.
3.2 Clause Constituents
In the second step, we identify the clauses of each
sentence, and, for each clause, we retain the par-
ticipants and their functions with regard to the
verb of the clause. A sentence can contain several
clauses, in particular, we identify the main clause,
relative clauses, and that-clauses.
In our example, there is just one clause consti-
tuted by a verb phrase (?have hold?) and three
participants: the subject ?the coach of benfica?,
the direct object ?a press conference?, and a
prepositional phrase ?in lisbon?. So, the objec-
tive here is to transform the dependency path built
in the first step into a partial constituency tree,
where only the constituents of the clause are se-
lected. The process of constructing the clause
constituents and the verb phrase is as follows.
Given a verb dependency (namely subj, dobj,
vprep, or attrib), we select the dependent lemma
of the clause verb and then we list all dependent
lemmas linked to the target lemma (as a head)
through the syntactic dependency path. It results
in the construction of the main phrases of the
clause, including information about the head of
the phrase. We show below the three constituents
identified from our example, where the directed
arrows stand for the internal dependencies used
for their identification (the head of each phrase is
in bold):
(a press conference)np
spec
modif
(the coach of benfica)np
spec nprep term
(in lisbon)pp
term
The verb phrase is also built in a similar way.
It contains all dependent lemmas of the verb that
are not part of the clause constituents identified
before:
(have hold)vp
aux
The three clause constituents are also provided
with information about their function with regard
to the clause verb, as Figure 1 shows. The func-
tion of a constituent inherits the name of the de-
pendent relation linking the clause verb to the
head of the constituent. For instance, the function
of (the coach of benfica)np is the name of the de-
pendent relation in subj(hold-6, coach-2), that is
subj. The clause constituents as well as the verb
phrase of each clause are the input of the extrac-
tion rules.
3.3 Extraction Rules
The third and last process consists of a small set
of simple extraction rules that are applied on the
clauses identified in the previous step. The out-
put of an extraction rule is a triple whose inter-
nal word tokens are provided with some linguistic
information: lemma, PoS tag, head of the con-
stituent, etc.
The simplest rule is applied on a clause just
containing a subject and a direct object. In such
a case, the two constituents are the arguments of
the triple, while the verb phrase is the relation.
13
In our previous example, the clause contains
three arguments: a subject (?the coach of ben-
fica?), a direct object ( ?a press conference?),
and a prepositional complement (?in Lisbon?).
In this case, our strategy is similar to that of
ReVerb system, namely to consider the relation
as the verb phrase followed by a noun phrase
and ending in a preposition. For this purpose,
we have defined an extraction rule that builds
the relation of the triple using the verb phrase,
the direct object, and the head preposition of
the prepositional phrase: ?have hold a press
conference in?. The two arguments are: ?the
coach of benfica? and ?Lisbon?. The triple
generated by our rule is represented as follows:
ARG1: the DT coach N-H of PRP benfica N
REL: have V hold V-H a DT press N confer-
ence N-H in PRP
ARG2: Lisbon N-H
which contains lemmas, PoS tags (DT, N,
PRP,...), as well as the heads (tag ?H?) of the
main constituents. In addition to this syntax-
based representation, the extraction rule also
gives us a surface form of the triple with just
tokens:
(the coach of Benfica, has hold a press conference in,
Lisbon)
Table 1 shows the main rules we defined to ex-
tract triples from patterns of clause arguments.
The order of arguments within a pattern is not
relevant. The argument ?vprep? stands for a
prepositional complement of the verb, which
consists of a preposition and a nominal phrase
(np). The third row represents the extraction rule
used in our previous example. All rules in Table
1 are applied at different clause levels: main
clauses, relative clauses and that-clauses.
As in the case of all current OIE systems,
our small set of rules only considers verb-based
clause triples and only extract one triple per
clause. We took this decision in order to make a
fair comparison when evaluating the performance
of our system against ReVerb (in the next section).
However, nothing prevents us from writing ex-
traction rules to generate several triples from one
clause with many arguments, or to extract triples
from other patterns of constituents, for instance:
patterns triples
subj-vp-dobj Arg1 = subj
Rel= vp
Arg2 = dobj
subj-vp-vprep Arg1 = subj
Rel= vp+prep (prep from vprep)
Arg2 = np (from vprep)
subj-vp-dobj-vprep Arg1 = subj
Rel= vp+dobj+prep
Arg2 = np (from vprep)
subj-vp-attr Arg1 = subj
Rel= vp
Arg2 = attr
subj-vp-attr-vprep Arg1 = subj
Rel= vp+attr+prep (from vprep)
Arg2 = np (from vprep)
Table 1: Pattern based rules to generate final triples
vp-pp-pp, noun-prep-noun, noun-noun, adj-noun,
or verb-adverb..
Finally, let us note that current OIE systems,
such as ReVerb, produces triples only in tex-
tual, surface form. Substantial postprocessing is
needed to derive relevant linguistic information
from the tuples. By contrast, in addition to surface
form triples, we also provide syntax-based infor-
mation, PoS tags, lemmas, and heads. If more
information is required, it can be easily obtained
from the dependency analysis.
4 Experiments
4.1 Wikipedia Extraction
The system proposed in this paper, hereafter
DepOE, was used to extract triples from the
Wikipedia in four languages: Portuguese, Span-
ish, Galician, and English.3 Before applying the
extractor, the xml files containing the Wikipedia
were properly converted into plaintext. The num-
ber of both sentences and extracted triples are
shown in Table 2. We used PoS-tagged text with
Tree-Tagger as input of DepPattern for the En-
glish extraction, and FreeLing for the other three
languages. Note that, unlike OIE systems de-
scribed in previous work, DepOE can be consid-
ered as being a multilingual OIE system.4
3Wikipedia dump files were downloaded at http://
download.wikipedia.org on September 2010.
4DepOE is an open source system freely available,
under GPL license, at http://gramatica.usc.es/
?
gamallo/prototypes.htm.
14
Wikipedia version sentences triples
English 78, 826, 696 47, 284, 799
Spanish 21, 208, 089 6, 527, 195
Portuguese 11, 714, 672 3, 738, 922
Galician 1, 461, 705 480, 138
Table 2: Number of sentences and triples from four
Wikipedias
It is worth mentioning that the number of ex-
tracted triples is lower than that obtained with Re-
Verb, which reaches 63, 846, 865 triples (without
considering a threshold for confidence scores).
This is due to the fact that the DepPattern gram-
mars are not complete and, then, they do not per-
form deep analysis, just partial parsing. In par-
ticular, they do not consider all types of coordi-
nation and do not deal with significant linguistic
clausal phenomena such as interrogative, condi-
tional, causal, or adversative clauses. Preliminary
evaluations of the four parsers showed that they
behave in a similar way, yet Portuguese and Gali-
cian parsers achieve the best performance, about
70% f-score.
In this paper, we do not report experimental
evaluation of the OIE system for languages other
than English.
4.2 Evaluation
We compare Dep-OE to ReVerb5, regarding the
quantity and quality of extracted triples just in En-
glish, since ReVerb only can be applied on this
language. Each system is given a set of sentences
as input, and returns a set of triples as output. A
test set of 200 sentences was created by randomly
selecting sentences from the English Wikipedia.
Each test sentence was independently examined
by two judges in order to, on the one hand, iden-
tify the triples actually contained in the sentence,
and on the other, evaluate each extraction as cor-
rect or incorrect. Incoherent and uninformative
extractions were considered as incorrect. Given
the sentence ?The relationship between the Tal-
iban and Bin Laden was close?, an example of in-
coherent extraction is:
(Bin Laden, was, close)
Uninformative extractions occur when critical
information is omitted, for instance, when one of
5http://reverb.cs.washington.edu/
the arguments is truncated. Given the sentence
?FBI examined the relationship between Bin
Laden and the Taliban?, an OIE system could
return a truncated triple:
(FBI, examined the relationship between, Bin Landen)
We follow similar criteria to those defined in
previous OIE evaluations (Etzioni et al, 2011).
Concerning the decisions taken by the judges
on the extractions made by the systems, the judges
reached a very high agreement, 93%, with an
agreement score of ? = 0.83. They also reached
a high agreement, 86%, with regard to the num-
ber of triples (gold standard) found in the test sen-
tences.
The precision of a system is the number of ex-
tractions returned as correct by the system divided
by the number of returned extractions. Recall is
the number of extractions returned as correct by
the system divided by the number of triples iden-
tified by the judges (i.e., the size of the gold stan-
dard). Moreover, to compare our rule-based sys-
tem DepOE to ReVerb, we had to select a par-
ticular threshold restricting the extractions made
by ReVerb. Let us note that this extractor is a lo-
gistic regression classifier that assign confidence
scores to its extractions. We computed precision
and recall for many threshold and selected that
giving rise to the best f-score. Such a threshold
was 0.15. So, we compare DepOE to the results
given by ReVerb for those extractions whose con-
fidence score is higher than 0.15.
As it was done in previous OIE evaluations, the
judges evaluated two different aspects of the ex-
traction:
? how well the system identify correct relation
phrases,
? the full extraction task, i.e., whether the sys-
tem identifies correct triples (both the rela-
tion and its arguments).
Figures 2 and 3 represent the score average ob-
tained by the two judges. They show that DepOE
system is more precise than ReVerb. This is clear
in the full extraction task, where DepOE achieves
68% precision while ReVerb reaches 52%. By
contrast, as it was expected, DepOE has lower
recall because of the low coverage of the gram-
mars it depends on. Regarding f-score, DepOE
15
ReVerb (<= 0.15) DepOE
0
10
20
30
40
50
60
70
prec
recall
f-score
Figure 2: Evaluation of the extraction of triples (both
relation and its arguments) performed by DepOE and
ReVerb (with a confidence score >= 0.15).
ReVerb (<= 0.15) DepOE
0
10
20
30
40
50
60
70
80
90
prec
recall
f-score
Figure 3: Evaluation of the relation extraction per-
formed by DepOE and ReVerb (with a confidence
score >= 0.15).
performs better than ReVerb in the full extraction
task, but when only relations are considered, Re-
Verb achieves the highest score.
We found that most of the incorrect extractions
returned by the two systems where cases where
the relation phrase was correctly identified, but
not one of the arguments. However, there are sig-
nificant differences between the two systems con-
cerning the type of problems arising in argument
identification.
The most common errors of ReVerb are both:
incorrect identification of the first argument (arg1)
and extraction of only a truncated part of the sec-
ond argument (arg2), as in the case of coordinat-
ing conjunctions. These two problems are crucial
for ReVerb since more than 60% of incorrect ex-
tractions were cases with incorrect arguments and
correct relations. DepOE has more precise extrac-
tions of the two arguments, in particular of arg1,
since the parser is able to correctly identify the
subject. Nevertheless, it also produces many trun-
cated arg2. Let us see an example. Given the sen-
tence ?Cities and towns in Romania can have the
status either of municipiu or oras?, ReVerb was
not able to identify the correct arg1 and returned
a truncated arg2:
(Romania, can have, the status)
DepOE correctly identified the subject (arg1)
but also failed to return the correct arg2:
(Cities and towns in Romania, can have, the status)
In general, when DepOE fails to correctly identify
an argument, it is often trivial to find the reason
of the problem. In the example above, arg2 was
truncated because the English grammar has not
any specific rule linking the particle ?either? to
a coordinate expression. So, the improvement
of DepOE depends on improving the grammars
it is based on. Besides the low coverage of the
grammar, there are other sources of problems
concerning the correct identification of argu-
ments. In particular, it is worth mentioning that
the English version of DepOE is not provided
with an efficient Named Entity Recognition
system. This makes it difficult to correctly iden-
tify multiword arguments with Named Entities,
quantities, measures, and dates. Such a problem
was partially solved by the use of FreeLing in
the Portuguese, Spanish, and Galician DepOE
versions.
4.3 Extraction Speed
To test the system?s speed, we ran each extrac-
tor on the 100, 000 first lines of the English
Wikipedia using a Linux platform with 2.4GHz
CPU and 2GB memory. The processing time of
ReVerb was 4 minutes while that of DepOE was 5
minutes and 19 seconds. In this platform, ReVerb
is able to process 2, 500 words per second, and
DepOE 1, 650. Concerning the use of RAM, Re-
Verb requires the 27% memory of the computer,
while DepOE only needs 0.1%.
5 Applications
The extracted triples can be used for several NLP
applications. The first application we are devel-
oping is a multilingual search engine over the
triples extracted from the Wikipedia. All triples
are indexed with Apache Solr6, which enables it
to rapidly answer queries regarding the extracted
information, as in the query form of ReVerb7.
Another application is to use the extracted
triples to discover commonsense knowledge of
6http://lucene.apache.org/solr/
7http://textrunner.cs.washington.edu/reverb demo.pl
16
team play game
team win championship
team win medal
team win game
team play match
organism have DNA
organism use energy
organism recycle detritus
organism respond to selection
organism modify environment
Table 3: Some of the most frequent basic propositions
containing the words ?team? and ?organism?, discov-
ered by our system from Wikipedia.
specific domains. One of the goals of Learning by
Reading is to enable a computer to acquire basic
knowledge of different domains in order to im-
prove question answering systems (Hovy et al,
2011). We assume that the head expressions of
the most frequent triples extracted from a spe-
cific domain represent basic propositions (com-
mon knowledge) of that domain.
To check this assumption, we built two domain-
specific corpora from Wikipedia: a corpus consti-
tuted by articles about sports, and another corpus
with articles about Biology. Then, we extracted
the triples from those corpora and, for each triple,
we selected just the head words of its three ele-
ments: namely the main verb (and preposition if
any) of the relation and the head nouns of the two
arguments. It resulted in a list of basic proposi-
tions of a specific domain. Table 3 shows some of
the propositions acquired following this method.
They are some of the most frequent propositions
containing two specific words, ?team? and ?or-
ganism?, in the subject position (arg1) of the
triples. The propositions with ?team? were ex-
tracted from the corpus about sports, while those
with ?organism? were acquired from the corpus
of Biology.
6 Conclusions and Current Work
We have described a multilingual Open Infor-
mation Extraction method to extract verb-based
triples from massive corpora. The method
achieves better precision than state of the art sys-
tems, since it is based on deep syntactic informa-
tion, namely dependency trees. In addition, given
that dependency analysis is performed by fast, ro-
bust, and multilingual parsers, the method is scal-
able and applied to texts in several languages: we
made experiments in English, Portuguese, Span-
ish, and Galician.
Our work shows that it is possible to perform
Open Information Extraction by making use of
knowledge-rich tools, namely rule-based depen-
dency parsing and pattern-based extraction rules,
while remaining scalable.
Even if in the experiments reported here we did
not deal with relationships that are not binary, the
use of deep syntactic information makes it easy to
build n-ary relations from such cases, for instance
complex events with internal (subject and object)
and external (time and location) arguments: ?The
treaty was signed by Portugal in 2003 in Lisbon?.
Furthermore, the use of deep syntactic informa-
tion will also be useful to find important relation-
ships that are not expressed by verbs. For in-
stance, from the noun phrase ?Nobel Prize?, we
should extract the basic proposition: (Nobel, is a,
prize).
In current work, we are working on synonymy
resolution for two different cases found in the ex-
tracted triples: first, the case of multiple proper
names for the same named entity and, second,
the multiple ways a relationship can be expressed.
Concerning the latter case, to solve relationship
synonymy, we are making use of classic methods
for relation extraction. Given a predefined set of
target relations, a set of lexico-syntactic patterns
is learned and used to identify those triples ex-
pressing the same relationship. This way, tradi-
tional closed information extraction could be per-
ceived as a specific task aimed at normalizing and
semantically organizing the results of open infor-
mation extraction.
Acknowledgments
This work has been supported by the MICINN,
within the projects with reference FFI2010-14986
and FFI2009-08828, as well as by Diputacio?n de
Ourense (INOU11A-04).
References
Michele Banko, Michael J Cafarella, Stephen Soder-
land, Matt Broadhead, and Oren Etzioni. 2007.
Open information extraction from the web. In Inter-
national Joint Conference on Artificial Intelligence.
Michele Banko, , and Oren Etzioni. 2008. The trade-
offs between open and traditional relation extrac-
17
tion. In Annual Meeting of the Association for Com-
putational Linguistics.
K. Barker, B. Agashe, S. Chaw, J. Fan, N. Friedland,
M. Glass, J. Hobbs, E. Hovy, D. Israel, D.S. Kim,
et al 2007. Learning by reading: A prototype sys-
tem, performance baseline and lessons learned. In
Proceeding of Twenty-Second National Conference
of Artificial Intelligence (AAAI 2007).
X. Carreras, I. Chao, L. Padro?, and M. Padro?. 2004.
An Open-Source Suite of Language Analyzers.
In 4th International Conference on Language Re-
sources and Evaluation (LREC?04), Lisbon, Portu-
gal.
Oren Etzioni, Michele Banko, and Michael J. Ca-
farella. 2006. Machine reading. In AAAI Confer-
ence on Artificial Intelligence.
Oren Etzioni, Anthony Fader, Janara Christensen,
Stephen Soderland, and Mausam. 2011. Open
information extraction: the second generation. In
International Joint Conference on Artificial Intelli-
gence.
Anthony Fader, Stephen Soderland, and Oren Etzioni.
2011. Identifying relations for open information ex-
traction. In Conference on Empirical Methods in
Natural Language Processing.
Pablo Gamallo and Isaac Gonza?lez. 2011. A gram-
matical formalism based on patterns of part-of-
speech tags. International Journal of Corpus Lin-
guistics, 16(1):45?71.
Eduard Hovy, Chin yew Lin, and Liang Zhou. 2005.
A BE-based Multi-document Summarizer with Sen-
tence Compression. In Proceedings of Multilin-
gual Summarization Evaluation (ACL workshop).
Ann Arbor, MI.
Dirk Hovy, Chunliang Zhang, Eduard Hovy, and
Anselmo Pe nas. 2011. Unsupervised discovery of
domain-specific knowledge from text. In Proceed-
ings of 49th Annual Meeting of the Association for
Computational Linguistics, Portland, Oregon, USA.
Thomas Lin, Mausman, and Oren Etzioni. 2010.
Identifying functional relations in web text. In Con-
ference on Empirical Methods in Natural Language
Processing.
Stephen Soderland, Brendan Roof, Bo Qin, Shi Xu,
Mausam, and Oren Etzioni. 2010. Adapting open
information extraction to domain-specific relations.
AI Magazine, 31(3):93?102.
Fei Wu and Daniel S. Weld. 2010. Open information
extraction using wikipedia. In Annual Meeting of
the Association for Computational Linguistics.
18
