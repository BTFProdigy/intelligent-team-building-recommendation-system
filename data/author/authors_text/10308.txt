Extracting Formal Specifications from Natural
Language Regulatory Documents
Nikhil Dinesh, Aravind Joshi, and Insup Lee
Department of Computer Science, Univeristy of Pennsylvania,
Philadelphia, PA - 19104 USA
nikhild,joshi,lee@cis. upenn.edu
Bonnie Webber
University of Edinburgh, Edinburgh, EH8 9LW Scotland
bonnie@inf.ed. ac. uk
Abstract
Formal verification techniques provide a way to determine whether regulatory doc-
uments are consistent and whether implementations conform to them. To apply
these techniques a formal description of the regulation needs to be extracted. We
present a framework, under which NLP techniques can be brought to bear, to aid
a requirements engineer in extracting the formal description.
1 Introduction
Regulatory documents, which include the vast bodies of legislation, operating
procedures and organizational policy, are meant to be accessible to the people
affected by them. Hence, they have to be in natural language (NL). On
the other hand, regulations are expected to be consistent, and the governed
entities/events are expected to conform to the regulation.
For example, the Food and Drug Administration?s Code of Federal Reg-
ulations (FDA CFR) governs the bloodbanks in America. 1 The bloodbanks
perform safety-critical functions like the testing of blood for communicable
disease agents (like HIV). It is highly desirable to determine whether (a) the
CFR is consistent, and (b) a bloodbank?s implementation of such a function
conforms to the CFR.
? This research was supported in part by NSF CCF-0429948 and ARO 911NF-05-1-0158
1 http://www.gpoaccess.gov/cfr/index.html
The problem of creating descriptions of regulation which can be checked
for consistency has been explored by several authors [1,8], but the challenge of
checking an implementation for conformance has not been addressed, and this
is the main goal of our work. The conformance guarantees can be obtained
if formal descriptions of regulation and implementations are available, and if
verification techniques [4] can be applied to these descriptions. But extracting
a formal description of regulation is expensive, as regulatory bases like the
CFR are large (about a million words) and complex.
Formal descriptions of regulation are usually extracted by an individual
who has a background in logic, e.g., [1,8]. We will call this individual the
requirements engineer. In this paper, we describe a framework to assist a
requirements engineer in extracting a formal description of regulation for use
in conformance checking.
An overview of the framework, the theoretical background and the various
constraints that apply is given in Section 2. This lets us determine the nature
of the description that needs to be extracted from the regulation. We then
turn to the question of how these descriptions might be composed. In Section
3, we attempt to map the denotations of sentences assigned by Kratzer [12]
to a form that can be used for the task at hand. Some difficulties arise in
this mapping, mainly because notions of obligation (that which is required)
and permission (that which is allowed) are not captured in the denotations.
We argue that an account of these notions is essential to the task at hand.
Section 4 describes a semantic representation, and composition procedure to
assist the requirements engineer in extracting the required description. By
treating obligations and permissions as different dimensions of the description
computed, the difficulties encountered in Section 3 are addressed.
The approach is motivated by our case study of the FDA CFR, and we
use (1) and (2) as examples through the course of this paper. 2 (1) conveys
an obligation to perform a test for HIV and Hepatitis B, and (2) conveys a
permission not to test source plasma (a blood component) for Hepatitis B.
(1) Except as specified in (2), you must test each donation of human blood or blood
component, for evidence of infection due to the Human immunodeficiency virus,
and the Hepatitis B virus.
(2) You are not required to test donations of Source Plasma for evidence of infection
due to the Hepatitis B virus.
2 A Framework
To determine whether an implementation (bloodbank) conforms to the regu-
lation (CFR), we extract specifications in the Computation Tree Logic (CTL)
from the CFR. Then, given a description of a bloodbank?s procedure (as a
finite transition system, or model) there is an efficient search procedure to
2 (1) and (2) are modified versions of sentences that appear in the FDA CFR 610.40. The
actual sentences are very long, and the modifications are made in the interests of space.
determine if the model conforms to the CTL specification [3]. This is known
as temporal model checking [2,13]. The problem of conformance checking is
thus split into three steps:
(1) Extract CTL specifications from the regulation - This is done by a
requirements engineer, and our goal is to assist her. We use CTL as the
specification language, because it allows for efficient model checking [3].
(2) Obtain a model of an implementation - We assume the availability of
models. There are tools that aid in extracting models from software [5], and
in creating models if they cannot be extracted directly [11].
(3) Apply model checking to determine if the model conforms to the CTL
specification.
Formally, a model can be defined as follows:
Definition 2.1 A model M is the five-tuple (S, I, ?, pi,?), where:
(a) S is a set of states, I ? S is a non-empty set of initial states,
(b) ? ? S ? S is a total transition relation (that is, ?s ? S : [?t ? S : (s, t) ? ?]),
(c) pi is a set of propositions (with power set 2pi), and
(d) ? : S ? 2pi is a function from states to sets of propositions. ?(s) for s ? S can be
thought of as the propositions true at s.
Figure 1(a) and 1(b) show models of two different bloodbanks. The left-
most state is the initial state. Each state is labeled with ?(s). The propo-
sitions have the following interpretation: d? is true (d? ? ?(s)) iff a donation
of blood or blood component is being processed, sp? is true iff a donation of
source plasma is being processed, thiv? is true iff a test for HIV has been per-
formed, and thepb? is true iff a test for Hepatitis B has been performed. The
use of the propositions deo (denoting deontic accessibility) and app1 (denoting
the application of a permission) is explained in later sections.
deo, d?,
sp?,
thiv?,
thepb?
deo, d?,
thiv?,
thepb?
(a) A model of a bloodbank
which tests all donations
deo, d?,
sp?,
thiv?,
app1
deo, d?,
thiv? ,
thepb?
(b) A model of a bloodbank
which does not test dona-
tions of source plasma for
Hepatitis B
Fig. 1. Two models of bloodbanks
Definition 2.2 Given a finite set of propositions pi, CTL formulas are defined induc-
tively as follows:
(a) p ? pi is a formula,
(b) Boolean combinations and negations of formulas are formulas,
(c) if ?, and ? are formulas, then AG(?) (on all paths, globally ?), AX(?) (on all
paths, at the next state ?), and ?AU?) (on all paths, ? until ?) are formulas.
The only temporal operator in CTL that we use is AG (for reasons that
we describe below), and hence rather than define the interpretation formally,
we will give some examples. Let M1 be the model in Figure 1(a), and M2 be
the model in Figure 1(b). The CTL specification AG(deo ? (d? ? thiv?)) holds
of both models, since on all paths (from the initial state, the leftmost one in
Figures 1(a), and 1(b)), globally, in all deontically accessible states deo, if a
donation of blood or blood component is being processed d?, it is tested for HIV
thiv?. Hence, we write M1  AG(deo ? (d? ? thiv?)), and M2  AG(deo ? (d? ?
thiv?)). Also, M1  AG(deo ? (sp? ? thepb?)). But, M2 6 AG(deo ? (sp? ? thepb?))
(since there is a state s with sp? ? ?(s), and thepb? 6? ?(s)).
2.1 Approaches to extracting specifications
The central problem we face is that CTL and other temporal logics that lend
themselves to model checking are not expressive enough for a compositional
semantic procedure to be defined for natural language. One reason is that
CTL, like propositional logic, cannot express relations between entities.
There are several routes one might take to address this problem, i.e., design
more expressive logics that allow for tractable model checking, focus on a
subset of NL from which an automatic translation is guaranteed, or make
the procedure machine-assisted. While the design of more expressive logics
makes the composition of specifications easier, using them for model checking
needs the creation of more expressive models (which requires more effort).
As a result, there is a trade-off between amount of effort spent in obtaining
models, and that in obtaining the specifications. Our decision to work with
less expressive models is motivated by the extensive tool support available
for creating and extracting such models [5,11]. Further, subsets of NL for
which automatic translation is guaranteed, such as the one derived by Holt
and Klein [10], assume (among other things) that references are resolved and
hence cannot be directly applied to regulatory documents. We are thus left
with the choice of making the procedure machine-assisted.
There have been two kinds of machine-assisted approaches to extracting
temporal logic specifications: (a) composing the semantics in a general seman-
tic framework which is then mapped to temporal logic [7], and (b) attempting
to compose the semantics in the temporal logic directly [6]. In the latter ap-
proach, a human specifies denotations for a portion of the sentence, and the
rest of the composition happens automatically. We attempt to compose the
semantics in a temporal logic directly like [6], as it lends itself to defining
semantic representations with which a requirements engineer can interact in
well-defined ways.
2.2 Constraints on the CTL specifications
We apply two constraints to the CTL specifications:
(i) The specifications extracted should hold of all and only the valid mod-
els. There may be several implementations that aim to conform to a single
base of regulation. Given (1) and (2), the models in Figures 1(a) and 1(b) are
both valid. This is an important difference from the NL sentences considered
in previous approaches, which were elicited from appropriate users by pre-
senting them with a single model. For example, Holt and Klein [10] obtained
specifications by asking users to describe a particular timing diagram.
(ii) To account for the variation between models, all temporal information
about the governed entities/events is modelled through propositions. The only
use of the temporal operators in CTL is to obtain a quantification over paths
and states. A mapping will need to be performed so that the propositions
used in the specifications can be evaluated at a states in different models, and
the critical assumption is that this mapping will be very easy to specify.
3 From Sets of Worlds to Sets of Models
Several approaches in formal semantics take sentences to denote sets of worlds.
For normative statements, we assume (following Kratzer [12]) that worlds are
connected by an accessibility relation. Consider (1) in Section 1 which among
other things requires a test for Hepatitis B if no exceptions apply. A denotation
of this requirement is given in (3), and is the set of worlds w0, such that for
every deontically accessible world w, for every entity x such that x is a donation
in that world d?(x,w), if no exception holds of that donation ?e?(x,w), a test
for Hepatitis B is carried out for that donation thepb?(x,w). We will assume
that negation has the highest precedence. Therefore ?a ? b ? (?a) ? b, and
brackets are used to resolve other ambiguities.
(3) ?w0.?w : (w ? deo(w0)? (?x : (d?(x,w) ? (?e?(x,w)? thepb?(x,w)))))
A difference between worlds in Kratzer?s denotations and states in a model
is that: in a state there is no notion of entities and relations between them.
All that is available at a state s is the set of propositions which are true at
that state ?(s). To map (3) to a form that is useful for checking conformance,
we need two assumptions.
First, we assume that regulation denotes the set of models that conform to
it. Intuitively speaking, w0 in (3) can be thought of as a model in its entirety,
and w ? deo(w0) correspond to special states in the model. A universal quan-
tification over accessible worlds can be replaced with the CTL AG operator.
We then obtain the denotation in (4), read as : on every path in M, if a state is
deontically accessible, for each donation x at that state, if no exception holds,
a test is carried out. In a model, only special states (like when the bloodbank
has finished processing all the donations it has received) need to conform to
the regulation, and deo can be thought of as marking those states.
(4) ?M. M  AG(deo? (?x : (d?(x)? (?e?(x)? thepb?(x)))))
(4) is still not in CTL because of the universal quantification over enti-
ties x at a state. The universal quantifier can be eliminated by assuming a
serial processing model. This has the effect that at the deontically accessible
states, exactly one donation is under consideration (e.g. the models in Fig-
ures 1(a) and 1(b)). In the sections of the CFR that we examined, a universal
quantification over entities is absolutely essential when these entities corre-
spond to inputs of an implemenation. This assumption lets us tie the inputs
to states, and use the quantification over states to achieve the quantification
over entities. Thus (4) can be reduced to (5).
(5) ?M. M  AG(deo? (d? ? (?e? ? thepb?)))
A problem that is encountered in taking this approach is that there is no
distinction between obligations, and permissions (both of which stem from the
Hohfeldian legal conceptions of right, duty, privilege, and no right [9]). While
this did not cause a problem for the obligation in (1), if one were to follow the
same procedure for the permission in (2), we would get the denotation in (6).
(6) ?M. M  ?(AG(deo ? (sp? ? thepb?)))
A model satisfies (6) only if there is some path in which there is a state that
is deontically accessible, and if a donation of source plasma is being processed
it is not tested. This is too strong a requirement, because an organization may
choose not to do what it is permitted to do. The model in Figure 1(a) is a
valid model, which would be declared invalid if (6) were required of it.
Another problem is that it is not clear how one would use (6) in interpreting
the exemption e? in (5). A reasonable candidate is e? ? deo ? (sp? ? ?thepb?).
But this is not the exemption because it is true in every deontically accessible
state in which a donation of source plasma is not being processed. Consider a
state s at which sp? = false (sp? 6? ?(s)). At s, e? ? (deo ? (false ? ?thepb?)) ?
(deo? true) ? true. The specification in (5), at s is: AG(deo? (?e? ?thepb?)) ?
AG(deo ? (?true ? ?thepb?)) ? AG(deo ? true) ? AG(true) ? true . Therefore, a
model that doesn?t test any donation for Hepatitis B would conform to (5).
We now turn to the task of addressing these problems by revising how the
specifications are composed.
4 Extracting the specifications
To aid the requirements engineer in extracting the specifications, the idea
is to present her with intermediate semantic representations of the sentence
with which she interacts. The intermediate representations that we use fall
into the category of abstract syntax trees (ASTs). ASTs are generally used as
intermediate representations in compiling code in a high-level programming
language to machine dependant code. The internal nodes in ASTs are oper-
ators (predicates/meta-predicates), the subtrees they dominate are operands
(arguments), and leaf nodes correspond to variables or constants (the require-
ments engineer specifies the denotation of the leaves). An AST encodes the
resolution of scope ambiguities, i.e., if p1 dominates p2 in the AST, then p1
outscopes p2.
Section 4.1 describes some phenomena in natural language that can be
used in the construction of the ASTs, and how these ASTs can be interpreted.
In Section 4.2, we describe how the ASTs and their interpretation for (1) and
(2) (in Figures 3 and 4) address the problems described in Section 3. 3
4.1 Abstract Syntax Trees (ASTs) and their interpretation
To capture the distinction between obligations and permissions, the denotation
of each node N in an AST is given by the 3-tuple: [[N ]] =
0
@
?N
ON
PN
1
A , where ON
is a set of propositional logic formulas which correspond to the obligations
that have been satisified, and PN is a set of propositional logic formulas that
correspond to the permissions that have been taken, and ?N is a propositional
logic formula which can be thought of as indicating whether N is true at a
state. The set of obligations O obtained from the policy base is the union
of the obligations obtained at the root of the AST for each sentence. The
denotation of the policy base is then given by: ?M. M  AG
0
@deo ?
^
??O
?
1
A . We
now identify various linguistic constructions that can be used to obtain ASTs.
Copy
p i z T l1, l2....ln
p
T :
[
z ? l1, i ? 1
]
... T :
[
z ? ln, i ? n
]
Fig. 2. Semantics of the Copy meta-predicate
Copy
and i z each
x is a donation of hu-
man blood or blood com-
ponent ?(1). i .1
except as
specified in (2) -
?(1). i .2
must
you, test x , for evi-
dence of infection due
to z - ?(1). i .3
the Human immuno-
deficiency vius
the Hepatitis B virus
and
?
?
?(1).1.1 ? (??(1).1.2 ? ?(1).1.3)
{?(1).1.1 ? (??(1).1.2 ? ?(1).1.3)}
{}
?
?
?
?
?(1).1.1
{}
{}
?
?
?
?
??(1).1.2 ? ?(1).1.3
{??(1).1.2 ? ?(1).1.3}
{}
?
?
?
?
?(1).1.2
{}
{}
?
?
?
?
?(1).1.3
{?(1).1.3}
{}
?
?
?
?
?(1).1.3
{}
{}
?
?
...
Fig. 3. AST and its interpretation for (1)
Distributive readings and the Copy meta-predicate: (1) is ambigu-
ous between a collective reading (where there is a single test for both the
3 We assume that obligation and permission denoting categories, e.g. must, do not occur
in contexts like antecedent clauses of subordinating conjunctions (like if), and restrictors of
determiners. Handling these cases requires an extension to CTL which is beyond the scope
of this paper.
every
x is a donation of Source
Plasma - ?2.1
not
are required
you to test x for evidence of
infection due to the Hepatitis B
virus z of this section - ?2.2
?
?
?(2).1 ? app1 ? ??(2).2
{}
{?(2).1 ? app1 ? ??(2).2}
?
?
?
?
?(2).1
{}
{}
?
?
?
?
app1 ? ??(2).2
{}
{app1 ? ??(2).2}
?
?
?
?
?(2).2
{?(2).2}
{}
?
?
?
?
?(2).2
{}
{}
?
?
Fig. 4. AST and its interpretation for (2)
diseases), and a distributive reading (where there are separate tests for each
disease). However, (2) gives an exemption to a test for one of the diseases, and
this suggests that a distributive reading may be more appropriate in the spec-
ifications extracted, and that the distributivity has scope over the exception.
Hence Copy dominates except in Figure 3.
The interpretation of the Copy meta-predicate is given in Figure 2. It is
called a meta-predicate because it is a function from an AST to another AST,
by simple variable substitution. For the AST for (1) shown in Figure 3, this
results in an AST rooted with and with subtrees corresponding to each of the
tests. The interpretation of and in this context is given by:
and
0
@
?1A
O1A
P1A
1
A ...
0
@
?nA
OnA
PnA
1
A =
0
@
?ni=1?iA
?ni=1OiA
?ni=1PiA
1
A
The RHS of the equation corresponds to the denotation of the node labeled
and in the AST (shaded in gray in Figure 3).
Universally Quantified NPs correponding to inputs: As mentioned
in Section 3, the universal quantification over inputs (donations) is achieved
by associating states with unique inputs. The interpretation of the determiner
each is designed with idea that the obligations will be evaluated at each state.
each
0
@
?A
{}
{}
1
A
0
@
?B
OB
PB
1
A =
0
@
?A ? ?B
{?A ? ?BO.j |?BO.j ? OB}
{?A ? ?BP.j |?BP.j ? PB}
1
A
The interpretation of the determiner no is similar to that of each/every,
except that a negation needs to be applied to the nuclear scope. We discuss
the interpretation of negation in what follows.
Conditional and Exceptive constructions: There are several predi-
cates that denote conditions and exceptions. For example, the subordinat-
ing conjunctions if , unless, and except as, coordinating conjunctions like
except that or but. The interpretation of if is the same as that for every. The
interpretation of predicates like except as, and unless are similar, the only
difference being that ??A is used instead of ?A in the RHS.
Modals and Negation: The semantics of modals and negation are given
below:
must
0
@
?A
{}
{}
1
A =
0
@
?A
{?A}
{}
1
A may
0
@
?A
{}
{}
1
A =
0
@
appi ? ?A
{}
{appi ? ?A}
1
A
not
0
@
?A
OA
PA
1
A =
0
@
??A
{??AP.j |?AP.j ? PA}
{appj ? ??AO.j |?AO.j ? OA}
1
A , where ??A =
?
appj ? ??A ?A ? ?AO.j ? OA
??A otherwise
must(A) results in the interpretation that ?A is an obligation. may(A)
results in the interpretation that appi ? ?A is a permission, where appi is
a variable introduced which the implementation must set to true when the
permission is applied (we discuss its use in Section 4.2). And intuitively, the
interpretation of negation captures the idea that may(?A) ? not(must(A)).
4.2 Discussion
There are two obligations obtained at the root of the AST for (1): ?(1).1.1 ?
(??(1).1.2 ? ?(1).1.3) ? d? ? (?e?1 ? thiv?) and ?(1).2.1 ? (??(1).2.2 ? ?(1).2.3) ?
d? ? (?e?2 ? thepb?) , where d? is true iff the donation is one of blood or blood
component, e?1 and e?2 are the exceptions to the required test for each disease,
and thiv? and thepb? are true iff tests for HIV and Hepatitis B respectively
have been performed. The computation of the second obligation is not shown
in Figure 3, and is obtained from the second child of and (in the AST shaded
in gray). Note that the individual propositions like d? need to be specified by
the requirements engineer at the leaf nodes of the AST.
Figure 4 shows the AST and its interpretation for (2). The permission
obtained at the root node is : ?(2).1?app1???(2).2 ? sp??app1??thepb? where sp?
is true iff a donation of source plasma is being processed, and thepb? is true iff
a test for the Hepatitis B virus has been carried out.
The use of the app1 proposition is as follows. It is possible for the regula-
tion to cancel the permission given in (2), but there may be several cases in
which permission not to test a donation of source plasma for Hepatitis B is
given. Suppose the case under consideration is one where the permission in
(2) is cancelled, but the organization doesn?t test a donation of source plasma
for Hepatitis B because a different permission can be applied. Since the per-
mission being applied sets thepb? to false, and sp? is true, the only way for the
implementation to indicate that the permission in (2) is not being applied is
by setting app1 to false. Setting e?1 ? false, and e?2 ? sp? ? app1 ? ?thepb?:
?O.1 ? d? ? (?false? thiv?), and ?O.2 ? d? ? (?(sp? ? app1 ? ?thepb?)? thepb?)
Considering just these obligations, the denotation of the regulatory doc-
ument would be: ?M. M  AG(deo ? (?O.1 ? ?O.2)) . Therefore, a bloodbank
could decide not to test a donation of source plasma for Hepatitis B, but they
would always have to test a donation for HIV.
5 Conclusions and Future Work
We have described a framework to assist a requirements engineer in extracting
CTL specifications from regulatory documents. An account of obligations and
permissions turns out to be essential in composing the specifications. The
composition procedure (defined in Section 4) was applied to a large part of
the FDA CFR 610.40. While it does seem to scale well, providing tool support
to extract and interact with the ASTs is vital. To this end, we plan to conduct
a small scale annotation of ASTs which will let us determine the accuracy with
which these representations can be computed. On the user interface side, we
are working on ways of presenting the ASTs to the requirements engineer.
References
[1] Breuker, J. and N. den Haan, Separating world and regulation knowledge: where is the
logic?, in: M. Sergot, editor, Proceedings of the third international conference on AI
and Law (1991), pp. 41?51.
[2] Clarke, E. M. and E. A. Emerson, Synthesis of synchronization skeletons for branching
time temporal logic, in: Logic of Programs: Workshop, 1981.
[3] Clarke, E. M., E. A. Emerson and A. P. Sistla, Automatic verification of finite-
state concurrent systems using temporal logic specifications, ACM Transactions on
Programming Languages and Systems 8 (1986), pp. 244?263.
[4] Clarke, E. M. and J. M. Wing, Formal methods: State of the art and future directions,
ACM Computing Surveys 28 (1996), pp. 626?643.
[5] Corbett, J. C., M. B. Dwyer, J. Hatcliff, S. Laubach, C. S. Pasareanu, Robby and
H. Zheng, Bandera: Extracting finite-state models from java source code, in: Proceedings
of the International Conference on Software Engineering (ICSE), 2000.
[6] Fantechi, A., S. Gnesi, G. Ristori, M. Carenini, M. Marino and M. Moreschini, Assisting
requirements formalization by means of natural language translation, Formal Methods
in System Design 4 (1994), pp. 243?263.
[7] Fuchs, N. and R. Schwitter, Attempto controlled english (ace), in: First International
Workshop on Controlled Language Applications, 1996.
[8] Glasse, E., T. V. Engers and A. Jacobs, Power: An integrated method for legislation and
regulations from their design to their use in e-government services and law enforcement,
in: M.-F. Moens, editor, Digitale Wetgeving, Digital Legislation, Die Keure Brugge, 2003
pp. 175?204, iSBN 90 5958 039 7.
[9] Hohfeld, W. N., Fundamental legal conceptions as applied in judicial reasoning, Yale
Law Journal 23 (1913), pp. 16?59.
[10] Holt, A. and E. Klein, A semantically-derived subset of English for hardware
verification, in: 37th Annual Meeting of the ACL, 1999.
[11] Holzmann, G., The Spin model checker, IEEE Trans. on Software Engineering 23
(1997), pp. 279?295.
[12] Kratzer, A., The notational category of modality, in: H.-J. Eikmeyer and H. Rieser,
editors, Words, Worlds, and Contexts. New approaches to Word Semantics, deGruyter,
Berlin, 1981 .
[13] Queille, J. P. and J. Sifakis, Specification and verification of concurrent systems in
CAESAR, in: Proceeding of the Fifth ISP, 1981.
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1202?1212,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Computing Logical Form on Regulatory Texts?
Nikhil Dinesh
Artificial Intelligence Center
SRI International
Menlo Park, CA - 94025
dinesh@ai.sri.com
Aravind Joshi and Insup Lee
Department of Computer Science
University of Pennsylvania
Philadelphia, PA - 19104
{joshi,lee}@seas.upenn.edu
Abstract
The computation of logical form has been pro-
posed as an intermediate step in the translation
of sentences to logic. Logical form encodes
the resolution of scope ambiguities. In this
paper, we describe experiments on a modest-
sized corpus of regulation annotated with a
novel variant of logical form, called abstract
syntax trees (ASTs). The main step in com-
puting ASTs is to order scope-taking opera-
tors. A learning model for ranking is adapted
for this ordering. We design features by study-
ing the problem of comparing the scope of one
operator to another. The scope comparisons
are used to compute ASTs, with an F-score of
90.6% on the set of ordering decisons.
1 Introduction
May (1985) argued for a level of logical form as a
prelude to translating sentences to logic. Just as a
parse tree determines the constituent structure of a
sentence, a logical form of a sentence represents one
way of resolving scope ambiguities. The level of
logical form is an appealing layer of modularity; it
allows us to take a step beyond parsing in studying
scope phenomenon, and yet, avoid the open problem
of fully translating sentences to logic.
Data-driven analyses of scope have been of in-
terest in psycholinguistics (Kurtzman and MacDon-
ald, 1993) and more recently in NLP (Srinivasan
and Yates, 2009). The focus has typically been
?This research was supported in part by ONR MURI
N00014-07-1-0907, NSF CNS-1035715, NSF IIS 07-05671,
and SRI International.
on predicting the preferred scopal ordering of sen-
tences with two quantifying determiners, for exam-
ple, in the sentence ?every kid climbed a tree?. In
the related problem of translating database queries
to logic, Zettlemoyer and Collins (2009) and Wong
and Mooney (2007) consider the scope of adjectives
in addition to determiners, for example the scope of
?cheapest? in the noun phrase ?the cheapest flights
from Boston to New York?. To our knowledge, em-
pirical studies of scope have been restricted to phe-
nomenon between and within noun phrases.
In this paper, we describe experiments on a novel
annotation of scope phenomenon in regulatory texts
? Section 610 of the Food and Drug Administra-
tion?s Code of Federal Regulations1 (FDA CFR).
Determiners, modals, negation, and verb phrase
modifiers are the main scope-taking operators. We
have annotated 195 sentences with a variant of log-
ical form, called abstract syntax trees (ASTs). Our
focus is on the problem of computing the AST, given
a (variant of a) parse tree of a sentence.
The long term goal of this work is to assist in the
translation of regulation to logic, for the application
of conformance checking. The problem is to for-
mally determine whether an organization conforms
to regulation, by checking the organization?s records
using the logical translation of regulation. Confor-
mance checking has been of interest in a variety of
regulatory contexts, and examples include privacy
policy (Barth et al, 2006; Jones and Sergot, 1992;
Anderson, 1996) and business contracts (Governa-
tori et al, 2006; Grosof et al, 1999).
We now discuss some problems that arise in defin-
1http://www.gpoaccess.gov/cfr/index.html
1202
ing logical form and the assumptions that we make
to circumvent these problems.
1.1 Problems and Assumptions
A key assumption of logical form is that the trans-
lation from language to logic is syntax-based. As
a result, the logic needs to be expressive enough to
accomodate a syntactic translation. There is no con-
sensus logic for constructs, such as, plurals, purpose
clauses, and certain modals. This leads to the fol-
lowing problem in defining logical form.
How do we define the logical form of a sentence,
without defining the logic? We adopt a specific for-
malism that accomodates a subset of the constructs
found in regulation. We generalize from the formal-
ized constructs to other constructs. Some of these
generalizations may need revision in the future.
We assume that sentences in regulation are trans-
lated to statements in logic of the form:
(id) ?(x1, ..., xn) 7? ?(x1, ..., xn)
where, ?id? is an identifier, ? is the precondition,
? is the postcondition, and x1, ..., xn are free vari-
ables. The distinction between pre and postcondi-
tions has been adopted by most logics for regula-
tion, to accomodate exceptions to laws (Sergot et al,
1986; Makinson and van der Torre, 2000; Governa-
tori et al, 2006). The pre and postconditions are
expressed in a modal logic that we designed in prior
work (Dinesh et al, 2011). In describing the logi-
cal form, we will sketch how the logical form can
be mapped to logic. But, we do not assume that the
reader has a detailed understanding of the logic.
Given the assumptions about the logic, our goal
is to transform a regulatory sentence into a structure
that lets us determine: (I) the constituents of a sen-
tence that contribute to the pre/postcondition, and
(II) the scope of operators in the pre/postcondition.
The structures that we use are called abstract syn-
tax trees (ASTs), which can be understood as a re-
stricted kind of logical form for regulatory texts.
1.2 Contributions and Outline
In this paper, we focus on the problem of computing
the AST given a (kind of) parse tree for a sentence.
The main step is is to order or rank scope-taking
operators. A learning model for ranking is adapted
for this ordering. We design features by studying the
problem of comparing the scope of one operator to
another. The pairwise scope comparisons are then
used to compute ASTs, with an F-score of 90.6% on
the set of ordering decisons.
The rest of this paper is organized as follows. We
define ASTs using an example in Section 2, and
setup the learning problem in Section 3. We then de-
scribe the corpus using statistics about operators in
Section 4. In Section 5, we describe experiments on
comparing the scope of an operator to another. We
use the pairwise scope comparisons, in Section 6 to
comput the AST. We discuss related work in Sec-
tion 7 and conclude in Section 8.
2 Abstract Syntax Trees
We describe abstract syntax trees (ASTs) using an
example from CFR Section 610.11:
(1) A general safety test for the detection of extra-
neous toxic contaminants shall be performed on
biological products intended for administration
to humans.
We discuss the translation in logic and the AST
for the fragment of (1) that appears in black. In or-
der to keep figures to a manageable size, we restrict
attention to fragments of sentences, by graying out
portions. The term AST is borrowed from compil-
ers (Aho et al, 1986), where it is used as an interme-
diate step in the semantic interpretation of programs.
Translation in Logic: The sentence (1) is formally
expressed as:
(1) bio prod(x) 7? Om(x)(?y : test(y) ? ?(x, y))
where, ?(x, y) = gensaf(y)?ag(y,m(x))?ob(y, x)
The predicates and function symbols are read as
follows. bio prod(x) - ?x is a biological product?.
m(x) denotes the manufacturer of x. The modal op-
erator O stands for ?obligation?. test(y) - ?y is a
test (event)?. gensaf(y) - ?y is a general safety pro-
cedure?. ag(y,m(x)) - ?the agent of y ism(x)?, and
ob(y, x) - ?the object of the event y is x?. The for-
malized version of the law is read as follows: ?If x
is a biological product, then the manufacturer m(x)
is required/obligated to perform a general safety test
y which has x as its object?. We refer the reader
to (Dinesh et al, 2011) for details on the logic.
1203
The distinction between pre and postconditions
is a non-trivial assumption. As with all logic-
programming formalisms, only free variables are
?shared? between pre and postconditons. This im-
plies that all existential quantification, modals, and
negation appear within the pre or postcondition. In
the example above, the existential quantifier (?y)
and the modal (O) appear within the postcondition.
Abstract Syntax Tree: The AST for (1) is shown in
Figure 1. The main nodes of interest are the inter-
nal nodes labeled ? . An internal node with n + 1
children corresponds to an n-ary operator. The first
child of the internal node is the operator. Opera-
tors are labeled with a part-of-speech tag, for exam-
ple, ?D? for determiner, ?M? for modal, and ?O? for
other. The remaining n children are its arguments.
We use the term nuclear scope to refer to the last
(nth) argument of the operator, and the term restric-
tor to refer to any other argument. We borrow these
terms from the literature on quantifier scope for de-
terminers (Heim and Kratzer, 1998, Chapter 7).
For example, the phrase ?general safety test? is in
the restrictor of the operator A, and the variable y
is in its nuclear scope. The modal shall is a unary
operator, and doesn?t have a restrictor. Non-unary
operators bind the variable displayed on the internal
node. The variable y is bound by the operator A.
Implicit operators are inserted when there is no
overt word or phrase. In Figure 1, the implicit oper-
ators are underlined. The generic noun phrase ?bi-
ological products? is associated with the implicit de-
terminer all. Similarly, we use the implicit operator
Post to mark the position of the postcondition.
?x
D
all
R
bio. prod.
?
O
Post
?
M
shall
?
M
be
?y
D
A
R
gen. saf. test
.
y performed on x
Figure 1: Example of an abstract syntax tree (AST).
We conclude this section with some notation for
describing ASTs. Given an AST for a sentences, we
say that an operator oi scopes over oj, denoted oi 
oj, if oj appears in the nuclear scope of oi. For ex-
ample, in Figure 1, we have all  Post, all  shall,
all  A, Post  A, and shall  A. In addition, we
say that the restrictor of oi scopes over oj, denoted
R(oi)  oj, if oj appears in the restrictor of oi. Such
configurations occur with PP-modification of NPs,
and we discuss examples in later sections.
3 Computing ASTs ? Overview
In this section, we give an overview of our approach
to computing ASTs. We will assume as given a Pro-
cessed Parse Tree (PPT) of a sentence, with the op-
erators and their restrictors identified. An example
is discussed in Section 3.1. Given such a PPT, the
AST is computed in two steps: (1) finding the preter-
minal at which an operator takes scope, and (2) or-
dering the operators associated with a preterminal.
We describe the second step in Section 3.2, and then
briefly outline the first step in Section 3.3. The steps
are described in reverse order, because in most cases,
the operators associated with a preterminal are deter-
mined directly by syntactic attachment.
3.1 Processed Parse Trees
We compute ASTs from processed parse trees
(PPTs) of sentences. Figure 2 gives the PPT cor-
responding to the AST in Figure 1.
.
?
P
Post
?y
D
A
R
gen. saf. test
?
M
shall
?
M
be
.
performed on
?x
D
IMP
R
bio. prod.
Figure 2: Processed parse tree (PPT) for (1).
A PPT provides the set of operators in a sen-
tence, associated with their restrictors. For exam-
ple, the determiner ?a? has the restrictor general
safety test. The phrase biological products has no
explicit determiner associated with it, and the cor-
responding operator in the PPT is labeled ?IMP?
for implicit. In addition, the postcondition marker
?Post? is also identified. Except for the postcon-
1204
dition marker, annotator-specified implicit operators
are not given in the PPT.
There are two main types of nodes in the PPT ?
operators and preterminals. The nodes labeled with
the symbol ?, e.g., ? and ?x , correspond to op-
erators. The root of the PPT and the restrictors of
the operators, are the preterminals. Based on this
example, it may seem that a sentence just has a list
of operators. While this is true of example (1), em-
bedded operators arise, for example, in the context
of PP-modification of NPs and relative clauses. We
will discuss an example in Section 3.3.
In this work, the PPTs are obtained by removing
all scope decisions from the AST. To a first approxi-
mation, we start by removing all operators from the
AST, and then, replace the corresponding variables
by the operators. Implicit unary operators (such as
the postcondition marker) are placed at the start of
the preterminal.
It is worthwhile to consider whether it is rea-
sonable to assume PPTs as given. We believe that
this assumption is (slightly) stronger than assuming
perfect parse trees. Although the PPT leaves cer-
tain chunks of the sentence unprocessed, in most
cases, the unprocessed chunks correspond to base
NPs. The main additional piece of information is the
existence of a postcondition marker for each main
clause of a sentence. We believe that computation
of PPTs is better seen as a problem of syntax rather
than scope, and we set it aside to future work. Our
focus here is on converting a PPT to an AST.
3.2 Ordering Operators
The problem of learning to order a set of items is
not new. Cohen et al (1998) give a learning theo-
retic perspective, and Liu (2009) surveys informa-
tion retrieval applications. The approach that we use
can be seen as a probabilistic version of the boosting
approach developed by Cohen et al (1998). We ex-
plain the step of ordering operators, by revisiting the
example of the general safety test, from Section 2.
Given the PPT in Figure 2, we compute the AST
in Figure 1 by ordering or ranking the operators. For
example, we need to determine that the implicit de-
terminer associated with biological products is uni-
versal, and hence, we have IMP  Post. However,
the determiner ?A? associated with general safety
test is existential, and hence, we have Post  A.
We now develop some notation to describe the
scopal ordering of operators. A PPT ? is viewed
as a set of preterminal nodes, and we will write ?
(a) p ? ? to denote that p occurs in ? , and (b)
|? | to denote the number of preterminals in ? . A
preterminal p is viewed as an ordered set of oper-
ators p = (o1, ..., o|p|). For example, in Figure 2,
the root preterminal p has |p| = 5, and the operators
o1 = Post, o2 = A, o3 = shall, and so on.
An AST ? contains a ranking of operators asso-
ciated with each preterminal, denoted r?(p). The
ranks of operators are denoted by subscripts. Let
p = (o1, ..., o5) be the root preterminal of the PPT
in Figure 2. The ranking associated with the AST in
Figure 1 is given by r?(p) = (o12, o25, o33, o44, o51). For
example, o25 = A denotes that the determiner ?A? ap-
pears second in the surface order (Figure 2) and fifth
or lowest in the scope order (Figure 1). Similarly,
o51 = IMP denotes that the implicit determiner ap-
pears fifth or last in the surface order (Figure 2) and
first or highest in the scope order (Figure 1). Note
that the subscript suffices to identify the position of
an operator in the AST.
Model: We now describe the learning model for or-
dering operators. Given a PPT ? , let A(?) be the set
of all possible ASTs. Our goal is to find the AST
which has the highest probability given the PPT:
?? = arg max
??A(?)
P (?|?)
The conditional probability of an AST is defined as:
P (?|?) =
?
p??
P (r?(p)|?)
P (r?(p)|?) =
|p|?1?
i=1
|p|?
j=i+1
P (oi  oj|?)
In other words, P (?|?) is modeled as the product
of the probabilities of the ranking of each pretermi-
nal, which is in turn expressed as the product of the
probabilities of the pairwise ordering decisions. The
model falls under the class of pairwise ranking ap-
proaches (Liu, 2009). We will consider the problem
of estimating the probabilities in Section 5, and the
problem of searching for the best AST in Section 6.
1205
.?
P
Post
?x3
D
IMP
R
samp. of
?x1
D
any
R
.
lot of
?x2
D
a
R
lic. prod.
?
M
may
?
M
be
.
...
Figure 3: PPT for (2)
?x1
D
any
R
?x2
D
a
R
lic. prod.
.
lot of x2
?
P
Post
?
M
may
?
M
be
?x3
D
some
R
samp. of x1
.
...
Figure 4: AST for (2)
3.3 Finding the Scope Preterminal
In the example that we discussed in the previous sec-
tion, there were no embedded operators, i.e., an op-
erator or its variable located in the restrictor of an-
other. An embedded operator can either ? (a) take
scope within the restrictor of the embedding oper-
ator, or (b) outscope the embedding operator. To
account for the second case, we need to determine
whether it is appropriate to lift an embedded opera-
tor to a higher preterminal than the one to which it
is associated syntactically.
We discuss an example of inverse linking (Larson,
1985) to illustrate the problem. Consider the follow-
ing sentence:
(2) Samples of any lot of a licensed product, except
for radioactive biological products, together with
the protocols showing results of applicable tests,
may at any time be required to be sent to the Di-
rector, Center for Biologics Evaluation and Re-
search.
The PPT and AST for (2) are shown in Figures 3
and 4 respectively. Consider the noun phrase ?IMP
samples of any lot of a licensed product? in the
.
?
P
Post
?x1
D
any
R
.
lot of
?x2
D
a
R
lic. prod.
?x3
D
IMP
R
samp. of x1
?
M
may
?
M
be
.
...
Figure 5: Second PPT for (2), obtained from the PPT in
Figure 3, by raising any to the root preterminal.
PPT. The implicit determiner IMP in the PPT is in-
terpreted as the existential determiner some in the
AST. The three operators are related as follows in
the AST: any  some and R(any)  a, i.e., any
outscopes the implicit determiner, and a appears in
the restrictor of any. Observe that the variables x1
and x2 , which are associated with any and a, ap-
pear in the restrictors of some and any respectively.
As a result, in the PPT, in Figure 3, any and a appear
in the restrictor of IMP and any. The PPT provides
a standard parse of PP-modification of NPs.
The important feature of this example is that
the determiner ?any? is syntactically embedded in
the restrictor of IMP in the PPT (Figure 4), but it
outscopes the implicit determiner in the AST (Fig-
ure 3). As a result, the PPT in Figure 3 cannot be
converted to the AST in Figure 4 simply by ranking
sibling operators (as we did in the previous section).
To handle such cases, we convert the PPT in Fig-
ure 3 to a second PPT (shown in Figure 5). The only
allowed operation during this conversion is to raise
an embedded operator to a higher preterminal. The
PPT in Figure 5 is obtained by raising any to the
root preterminal, making it a sibling of the implicit
determiner IMP in the PPT in Figure 5. This second
PPT can be converted to the AST by reordering sib-
ling operators. The learning model used for this step
is similar to the one used to order operators, and in
the interests of space, we omit the details.
4 Brief Overview of the Corpus
We have annotated 195 sentences from the FDA
CFR Section 610 with ASTs. The operators are di-
vided into the following types ? determiners (e.g.,
1206
every, a, at least), modal auxiliaries (e.g., must,
be), VP modifiers (e.g., if, for, after), negation and
coordinating conjunctions (e.g., and, but, or). The
majority of the corpus was annotated by a single an-
notator. However, to estimate inter-annotator agree-
ment, a set of 32 sentences was annotated by a
second annotator. In this section, we restrict our-
selves to presenting statistics that highlight part of
the guidelines and motivate the features that we use
to order operators. An example-based justification
of guidelines, and a discussion of inter-annotator
agreement can be found in (Dinesh, 2010).
De Re vs De Dicto: We narrow our focus to one part
of the annotation, the de re vs de dicto distinction.
Informally, operators with de re scope occur in the
precondition of the logical translation of a sentence,
while those with de dicto scope occur in the post-
condition. This distinction is of key importance in
the application of conformance checking, as it helps
determine the facts that need to be provided by an
organization (de re), and the actions that an organi-
zation is required to take (de dicto).
For simplicity, we further restrict attention to op-
erators that are siblings of the postcondition in the
AST, and ignore the operators embedded in preposi-
tional phrases and clauses, for example. A (main
clause) operator o is said to have de re scope iff
it outscopes the postcondition marker (o  Post).
Otherwise, the operator is said to have de dicto scope
(Post  o). In the example of the general safety test
from Section 2, the implicit determiner associated
with ?biological products? has de re scope, while all
other operators in the sentence have de dicto scope.
Operator Number of De Re Scope
Type Instances Percentage
Determiner 277 59.9%
Modal Aux 268 0%
VP Modifier 132 68.2%
CC 36 22.2%
Neg 33 0%
Other 74 17.6%
Table 1: De Re scope distribution. An operator has de re
scope iff it outscopes the postcondition marker.
Table 1 shows the percentage of each type of op-
erator that has de re scope. Modal auxiliaries and
negation are umambigous to this distinction, and al-
ways have de dicto scope. Note that a type of opera-
tor with 50% occuring de re is ambiguous, while 0%
or 100% are unambiguous. Thus, from Table 1, we
can conclude that determiners, and VP modifiers are
the most ambiguous types. And, more features are
needed to disambiguate them.
Determiner Number of De Re Scope
Type Instances Percentage
Universal 74 100%
Existential 12 0%
Ambiguous 50 28%
Deictic 127 53.5%
Other 14 35.7%
Table 2: De Re scope distribution for determiners.
Determiners: We divide the determiners into the
following subtypes: universal/generic (e.g., every,
all), existential (some), ambiguous (e.g., a, an), de-
ictic (e.g., the, those), and other (e.g., at least, at
most). The guidelines for annotation were as fol-
lows ? (a) universal determiners have de re scope,
(b) existential determiners have de dicto scope, and
(c) for other determiners, the annotator needs to de-
cide whether a particular use is interpreted existen-
tially or universally. Table 2 shows de re scope dis-
tribution for each of these subtypes. As expected,
universal and existential determiners are unambigu-
ous, while ambiguous and deictic determiners show
more variety. For example, the deictic determiner
the can refer to a specific entity (?the FDA?) or have
a universal interpretation (?the products?).
Thus, to disambiguate between de re and de dicto
interpretations for determiners, we need two types of
features ? (1) Features to predict whether ambiguous
and deictic determiners are universal or not, and (2)
Features to determine the type of implicit determin-
ers. In Table 2, we assume that the type of implicit
determiners are given. This assumption is unreal-
istic. Rather, we need to predict the type of such
determiners, during the computation of the AST.
VP Modifier Number of De Re Scope
Type Instances Percentage
Temporal and Conditional 73 100%
Purpose 8 0%
References to Laws 33 0.9%
Other 29 65.5%
Table 3: De Re scope distribution for VP modifiers.
1207
VP Modifiers: We divide the VP modifiers into the
following subtypes: temporal and conditional (e.g.,
after, if), purpose (for), references to laws (which
are a special type of modifier in the legal domain,
e.g., ?as specified in paragraph (c)?), and other (e.g.,
regardless, notwithstanding). Table 3 shows the
percentage of each subtype of modifier that has de
re scope. Following the guidelines for annotation,
the temporal and conditional modifiers are always de
re, the purpose modifiers and modifiers conveying
references to laws are always de dicto.
5 Comparing the Scope of Operators
We now consider a subproblem in computing the
AST ? comparing the scope of pairs of operators.
In Section 6, we will use the classifiers that perform
comparisons, to compute the AST. All experiments
in this section use the MAXENT implentation from
the MALLET toolkit (McCallum, 2002). We begin
by revisiting de re-de dicto distinction from Sec-
tion 4. Then, we generalize to other comparisons.
De Re vs De Dicto: The (binary) classification
problem is as follows. Our observations are triples
x = (o, o?, ?) are such that there is a preterminal
p ? ? , {o, o?} ? p, and o? = Post. In other words,
we are considering operators (o) that are siblings of
the postcondition marker (o?). An observation has
the label 1 if o  o? (de re scope), and a label of 0
otherwise (de dicto scope).
Features: We use the following (classes of) fea-
tures for an observation x = (o, o?, ?):
? TYPE - The type and subtype of the operator.
We use the subtypes from Section 4 only for
explicit operators.
? PRE-VERB - Tracks whether o and o? appear
before or after the main verb of the sentence.
? PRE-VERB + PERF - Conjunction of the previ-
ous feature with whether the main verb is per-
form. The verb perform is frequent in the CFR,
and its subject is typically given de dicto scope,
as it is the main predicate of the sentence.
? POS - The part-of-speech of the head word. For
example, for the noun phrase biological prod-
ucts, the head word is products, and the POS is
NNS (plural common noun). And, this POS tag
may indicate a generic/universal interpretation.
Count MAJORITY TYPE ALL
All 823 66.2% 84.1% 89.2%
No MD 522 53.2% 74.9% 83.7%
DT 277 59.9% 62.9% 81.2%
Imp. DT 100 69% 76%
Table 4: De Re vs De Dicto classification. Average accu-
racies over 10-fold cross-validation. The rows describe
the subset of observations considered, and the columns
describe the subset of features used.
Experiments: We evaluate the features by perform-
ing 10-fold cross-validation. The results are summa-
rized in Table 4. The rows describe the subset of ob-
servations used. ?All? includes all observations, ?No
MD? excludes the modal auxiliaries, ?DT? includes
only the determiners, and ?Imp. DT? includes only
implicit determiners. The columns describe the fea-
tures used. MAJORITY is the majority baseline, i.e.,
the accuracy obtained by predicting the most fre-
quent class or the majority class. The majority class
is de dicto when all operators are considered (the
first row), and de re in all other rows. The TYPE
column gives the accuracy when only the type and
subtypes are used as features. This column does not
apply to implicit determiners, as the subtype infor-
mation is unavailable. And, finally, the ALL column
gives the accuracy when all features are used.
From Table 4, we can conclude that the TYPE fea-
ture is useful in making the de re-de dicto distinc-
tion, and further gains are obtained by using ALL
features. The most dramatic improvement is for de-
terminers, and indeed, our features were designed
for this case. However, the performance gains are
not very high for implicit determiners, and further
investigation is needed.
Next, we apply the features to more general oper-
ator comparisons. The first row of Table 5 considers
observations x = (o, o?, ?), where o and o? are sib-
lings, and predicts whether o  o?. The second row
considers observations where o? is embedded syn-
tactically within o, and predicts whether R(o)  o?.
In other words, the problem is to determine whether
a syntactically embedded operator remains scopally
embedded, or whether it has inverse scope (see Sec-
tion 3.3).
1208
Count MAJORITY TYPE ALL
Siblings 2793 76.1% 83.3% 87.5%
Embedded 5081 95% 95.3% 96.4%
Table 5: Ordering siblings and embedded operators.
Average accuracies over 10-fold cross-validation. The
columns describe the subset of features used.
6 From Operator Comparisons to ASTs
We now consider the problem of computing the
AST given the classifiers for comparing operators.
Section 6.1 describes the algorithms used. In Sec-
tion 6.2, we develop metrics to evaluate the com-
putation of ASTs. We conclude, in Section 6.3, by
evaluating different algorithms using the metrics.
6.1 Algorithms
We begin by discussing the intractability of the prob-
lem of ranking or ordering operators. Then, we
sketch the search heuristics used.
Intractability: The decision version of the rank-
ing problem is NP-complete. A similar result is es-
tablished by Cohen et al (1998) in the context of a
boosting approach to ranking.
Theorem 1. The following problem is NP-complete:
Input: A PPT ? , a preterminal p ? ? , probabilities
P (oi  oj|?), and c ? [0, 1]
Output: Yes, if there is an ordering r such that
P (r(p)|?) ? c
The proof is by reduction from ACYCLIC SUB-
GRAPH (Karp, 1972) ? finding a subgraph which is
acyclic and has at least k edges.
Heuristics: To order operators, we use a beam
search procedure. Each search state consists preter-
minal, in which the first i ranks have been assigned
to operators. We then search over next states by as-
signing the rank i+1 to one of the remaining opera-
tors. We used a beam size of 104 in our experiments.
In most cases, the number of operators per preter-
minal is less than 7. As a result, the total number
of possible orderings is typically less than 7!, and a
beam size of 104 is sufficient to compute an exact or-
dering. In other words, due to the size restrictions, in
most cases, beam search is equivalent to exact (ex-
haustive) search.
To handle embedded operators, we use a simple
greedy heuristic. We enumerate the operators in the
initial PPT, corresponding to an in-order traversal.
For each operator, we attach it to the most likely an-
cestor, given the attachment decisions for the previ-
ous operators. This heuristic is optimal for the case
where the depth of embedding is at most 1, which is
the common case.
6.2 Metrics
In this section, we describe metrics used to evaluate
the computation of ASTs. Let ? be the initial PPT,
? the correct AST, and ?? the computed AST. We
define accuracy at various levels.
The simplest metric is to define accuracy at the
level of ASTs, i.e., by computing the fraction of cases
for which ? = ??. However, this metric is harsh,
in the sense that it does not give algorithms partial
credit for getting a portion of the AST correct.
The next possible metric is to define accuracy at
the level of preterminals. Let p be a preterminal.
Note that ? , ? and ?? share the same set of preter-
minals, but may associate different operators with
them. We say that p is correct in ??, if it is asso-
ciated with the same set of operators as in ?, and
for all {o, o?} ? p, we have o  o? w.r.t. ?? iff
o  o? w.r.t. ?. In other words, the preterminals
are identical, both in terms of the set of operators
and the ordering between pairs of operators. While
preterminal-level accuracy gives partial credit, it is
still a little harsh, in the sense that an algorithm
which makes one ordering mistake at a preterminal
is penalized the same as an algorithm which makes
multiple mistakes.
Finally, we consider metrics to define accuracy at
the level of pairs of operators. Let p be a preter-
minal. The set Pairs(p, ?) consists of pairs of op-
erators (o, o?) such that o and o? are both associ-
ated with p in ?, and o = o? or o  o?. The set
Pairs(p, ??) is defined similarly using ?? instead of
?. Given the sets Pairs(p, ?) and Pairs(p, ??), pre-
cision, recall, and f-score are defined in the usual
way. We leave the details to the reader.
6.3 Results
We evaluate the following algorithms:
1. No Embedding ? The AST is computed purely
by reordering operators within a preterminal in
the PPT.
1209
(a) SURFACE ? No reordering is performed,
i.e., the order of operators in the AST re-
spects the surface order
(b) TYPE ? Using only type and subtype in-
formation for the operators
(c) ALL ? Using all the features described in
Section 5
2. ALL+ ? The initial PPT is transformed into a
second PPT before reordering (as described in
Section 3.3). All features are used.
Prec. Rec. F p ?
SURF. 86.9% 82.7% 84.6% 81% 4.2%
TYPE 90.4% 86% 88.1% 83.6% 24.7%
ALL 92% 87.6% 89.8% 85.1% 33.5%
ALL+ 91.9% 89.4% 90.6% 85.9% 36.2%
Table 6: Performance of the algorithms in computing the
ASTs. Averaged over 10-fold cross-validation. 195 ASTs
in total, an average of 8.6 preterminals per AST, and 1.8
operators per preterminal.
Table 6 summarizes the performance of the al-
gorithms, under the various metrics. The accura-
cies are averaged over 10-fold cross-validation. A
total of 195 ASTs are used. The average number
of preterminals per AST is 8.6, with an average of
1.8 operators per preterminal. The best number un-
der each metric is shown in bold-face. By adding
features, we improve the precision from 86.9% to
90.4% to 92% in moving from SURFACE to TYPE
to ALL. By handling embedded operators, we im-
prove the recall from 87.6% to 89.4% in moving
from ALL to ALL+. As we saw in Section 5, in 95%
of the cases, the embedded operators respects syn-
tactic scope, and as a result, we obtain only modest
gains from handling embedded operators.
The reader may feel that the F-score of 90.6% is
quite high given the size of our training data. This
score is inflated by inclusion of reflexive pairs, of
the form (o, o). Such pairs are included for the
following (technical) reasons. The algorithm that
handles embedded operators (ALL+) usually raises
them from a single operator node (as in Figure 3) to
a multi-operator node (as in Figure 5). If it makes
an incorrect decision to raise an operator it takes a
precision hit, at the multi-operator node (because
it has some false positives). By contrast, an algo-
rithm loses precision for failing to correctly raise,
only when we encounter the single operator node.
For these reasons, it is better to consider the rela-
tive improvement in F-score over the baseline. The
relative improvement of ALL+ over SURFACE in
terms of F-score is 36.6%. We believe that the
preterminal-level accuracy is more indicative in an
absolute sense. Furthermore, when we restrict atten-
tion to those preterminals with two or more opera-
tors in the PPT, the accuracy of ALL+ is 69.4%.
7 Related Work
ASTs can be seen as a middle ground between two
lines of research in translating sentences to logic.
At one end of the spectrum, we have methods
that achieve good accuracy on restricted texts. The
two main corpora that have been considered are the
GEOQUERY corpus (Thompson et al, 1997) and
the ATIS-3 corpus (Dahl et al, 1994). The GEO-
QUERY corpus consists of queries to a geographical
database. The queries were collected from students
participating in a study and the average sentence
length is 8 words. The ATIS corpus is collected
from subjects? interaction with a database of flight
information, using spoken natural language. The ut-
terances have be transcribed, and the average sen-
tence length is 10 words (Berant et al, 2007). Algo-
rithms, which achieve good accuracy, have been de-
veloped to compute the logical translation for these
queries (Zettlemoyer and Collins, 2005; Wong and
Mooney, 2007; Zettlemoyer and Collins, 2009). The
annotated sentences in the FDA CFR Section 610.40
are longer (about 30 words on average), and contain
modalities which are not present in these corpora.
At the other end of the spectrum, Bos et al (Bos et
al., 2004) have developed a broad-coverage parser to
translate sentences to a logic based on discourse rep-
resentation theory. Here, there is no direct method to
evaluate the correctness of the translation. However,
indirect evaluations are possible, for example, by
studying improvement in textual entailment tasks.
To summarize, there are techniques that either
produce an accurate translation for sentences in a
limited domain, or produce some translation for sen-
tences in a broader range of texts. ASTs offer a mid-
dle ground in two ways. First, we focus on regula-
1210
tory texts which are less restricted than the database
queries in the GEOQUERY and ATIS corpora, but do
not exhibit anaphoric phenomenon found in genres,
such as, newspaper text. In (Dinesh et al, 2007), we
discuss lexical statistics that show significant differ-
ences in the distribution of anaphoric items in the
CFR and Wall Street Journal (WSJ) corpora. For ex-
ample, the frequency of pronouns and anaphoric dis-
course connectives is significantly lower in the CFR
than in the WSJ. Instead, the CFR has an idiosyn-
cratic mechanism for referring to sentences, using
phrases such as ?except as specified in paragraph
(c) and (d)?. A question of interest is whether the
GEOQUERY and ATIS corpora show similar pecu-
liarities in terms of anaphora. The second difference
between our approach and others is that we do not
attempt to translate all the way to logic. The level of
logical form lets us obtain a direct evaluation, while
leaving open the design of parts of the logic.
8 Conclusions
We described experiments on a modest-sized cor-
pus of regulatory sentences, annotated with a novel
variant of logical form, called abstract syntax trees
(ASTs). An example from the corpus was presented
in Section 2 and some statistics, describing the cor-
pus, were discussed in Section 4. In Sections 3, 5,
and 6, we developed and tested algorithms to con-
vert a processed parse tree (PPT) to an AST. The
main step in this conversion was to rank or order the
operators at a preterminal. We presented a proba-
bilistic model for ranking, investigated the design of
features, and developed search heuristics. The best
algorithm, which uses all features and handles em-
bedded operators, achieves an F-score of 90.6%.
An important direction for further inquiry is in the
design of better features. Various types of features
have been proposed for the scopal ordering of deter-
miners. Examples include syntactic features (Ioup,
1975; Reinhart, 1983), such as position and voice,
semantic features (Grimshaw, 1990; Jackendoff,
1972), such as thematic roles. More recently, Srini-
vasan and Yates (2009) showed how pragmatic in-
formation, for example ?there are more people than
cities?, can be leveraged for scope disambiguation.
We experimented with lexico-syntactic features in
this work, and leave an investigation of semantic and
pragmatic features to future work.
Acknowledgements
We thank Claire Cardie, Steve Kimbrough, Annie
Louis, Fernando Pereira, Emily Pitler, Oleg Sokol-
sky, and the anonymous reviewers for helpful com-
ments on earlier versions of this paper.
References
A. Aho, R. Sethi, and J. Ullman. 1986. Compilers: Prin-
ciples, Techniques, and Tools. Addison-Wessley.
R. J. Anderson. 1996. A security policy model for clin-
cial information systems. In Proceedings of the IEEE
Symposium on Security and Privacy.
A. Barth, A. Dutta, J. C. Mitchell, and H. Nissenbaum.
2006. Privacy and contextual integrity: Framework
and applications. In Proceedings IEEE Symposium on
Security and Privacy.
J. Berant, Y. Gross, M. Mussel, B. Sandbank, E. Ruppin,
and S. Edelman. 2007. Boosting unsupervised gram-
mar induction by splitting complex sentences on func-
tion words. In Proceedings of the Boston University
Conference on Language Development.
J. Bos, S. Clark, M. Steedman, J. R. Curran, and J. Hock-
enmaier. 2004. Wide-coverage semantic representa-
tions from a CCG parser. In Proceedings of COLING.
W. W. Cohen, R. E. Schapire, and Y. Singer. 1998.
Learning to order things. Journal of Artificial Intel-
ligence Research, 10:243?270.
D. Dahl, M. Bates, M. Brown, W. Fisher, K. Hunicke-
Smith, D. Pallett, C. Pao, A. Rudnicky, and
E. Shriberg. 1994. Expanding the scope of the ATIS
task: the ATIS-3 corpus. In Proceedings of the ARPA
HLT Workshop.
N. Dinesh, A. Joshi, I. Lee, and O. Sokolsky. 2007.
Logic-based regulatory conformance checking. In
Proceedings of the 14th Monterey Workshop.
N. Dinesh, A. Joshi, I. Lee, and O. Sokolsky. 2011. Per-
mission to speak: A logic for access control and con-
formance. Journal of Logic and Algebraic Program-
ming, 80(1):50?74.
N. Dinesh. 2010. Regulatory Conformance Checking:
Logic and Logical Form. Ph.D. thesis, University of
Pennsylvania.
G. Governatori, Z. Milosevic, and S. Sadiq. 2006. Com-
pliance checking between business processes and busi-
ness contracts. In 10th International Enterprise Dis-
tributed Object Computing Conference (EDOC).
J. Grimshaw. 1990. Argument Structure. MIT Press.
1211
B. Grosof, Y. Labrou, and H. Y. Chan. 1999. A declar-
ative approache to business rules in contracts: Cour-
teous logic programs in xml. In ACM Conference on
Electronic Commerce.
Irene Heim and Angelika Kratzer. 1998. Semantics in
Generative Grammar. Blackwell.
G. Ioup. 1975. Some universals for quantifier scope.
Syntax and Semantics, 4:37?58.
R. Jackendoff. 1972. Semantic Interpretation in Genera-
tive Grammar. MIT Press.
A. J. I. Jones and M. J. Sergot. 1992. Formal spec-
ification of security requirements using the theory
of normative positions. In European Symposium on
Reasearch in Computer Security (ESORICS).
R. M. Karp. 1972. Reducibility among combinatorial
problems. In R. E. Miller and J. W. Thatcher, editors,
Complexity of Computer Computations, pages 85?103.
Plenum Press.
H. S. Kurtzman and M. C. MacDonald. 1993. Resolution
of quantifier scope ambiguities. Cognition, 48:243?
279.
R. K. Larson. 1985. Quantifying to np. Manuscript,
MIT.
T. Liu. 2009. Learning to rank for information retrieval.
Foundations and Trends in Information Retrieval, 3(3).
D. Makinson and L. van der Torre. 2000. Input/output
logics. Journal of Philosophical Logic, 29:383?408.
R. May. 1985. Logical Form: Its structure and deriva-
tion. MIT Press.
A. McCallum. 2002. MALLET: A machine learning for
language toolkit. http://mallet.cs.umass.edu.
T. Reinhart. 1983. Anaphora and Semantic Interpreta-
tion. Croom Helm.
M.J. Sergot, F.Sadri, R.A. Kowalski, F.Kriwaczek,
P.Hammond, and H.T. Cory. 1986. The british na-
tionality act as a logic program. Communications of
the ACM, 29(5):370?86.
P. Srinivasan and A. Yates. 2009. Quantifier scope
disambiguation using extracted pragmatic knowledge:
Preliminary results. In Proceedings of the Conference
on Empirical Methods in Natural Language Process-
ing (EMNLP).
C. A. Thompson, R. J. Mooney, and L. R. Tang. 1997.
Learning to parse natural language database queries
into logical form. In Proceedings of the Workshop on
Automata Induction, Grammatical Inference and Lan-
guage Acquisition.
Y. W. Wong and R. J. Mooney. 2007. Learning syn-
chronous grammars for semantic parsing with lambda
calculus. In Proceedings of the Annual Meeting of the
Association for Computational Linguistics (ACL).
L. S. Zettlemoyer and M. Collins. 2005. Learning to
map sentences to logical form: Structured classifica-
tion with probabilistic categorial grammars. In Pro-
ceedings of UAI.
L. S. Zettlemoyer and M. Collins. 2009. Learning
context-dependent mappings from sentences to logi-
cal form. In Proceedings of the Annual Meeting of the
Association for Computational Linguistics (ACL).
1212
