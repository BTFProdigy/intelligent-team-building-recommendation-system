Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 41?48,
New York, June 2006. c?2006 Association for Computational Linguistics
Learning to recognize features of valid textual entailments
Bill MacCartney, Trond Grenager, Marie-Catherine de Marneffe,
Daniel Cer, and Christopher D. Manning
Computer Science Department
Stanford University
Stanford, CA 94305
{wcmac, grenager, mcdm, cerd, manning}@cs.stanford.edu
Abstract
This paper advocates a new architecture for tex-
tual inference in which finding a good alignment is
separated from evaluating entailment. Current ap-
proaches to semantic inference in question answer-
ing and textual entailment have approximated the
entailment problem as that of computing the best
alignment of the hypothesis to the text, using a lo-
cally decomposable matching score. We argue that
there are significant weaknesses in this approach,
including flawed assumptions of monotonicity and
locality. Instead we propose a pipelined approach
where alignment is followed by a classification
step, in which we extract features representing
high-level characteristics of the entailment prob-
lem, and pass the resulting feature vector to a statis-
tical classifier trained on development data. We re-
port results on data from the 2005 Pascal RTE Chal-
lenge which surpass previously reported results for
alignment-based systems.
1 Introduction
During the last five years there has been a surge in
work which aims to provide robust textual inference
in arbitrary domains about which the system has no
expertise. The best-known such work has occurred
within the field of question answering (Pasca and
Harabagiu, 2001; Moldovan et al, 2003); more re-
cently, such work has continued with greater focus
in addressing the PASCAL Recognizing Textual En-
tailment (RTE) Challenge (Dagan et al, 2005) and
within the U.S. Government AQUAINT program.
Substantive progress on this task is key to many
text and natural language applications. If one could
tell that Protestors chanted slogans opposing a free
trade agreement was a match for people demonstrat-
ing against free trade, then one could offer a form of
semantic search not available with current keyword-
based search. Even greater benefits would flow to
richer and more semantically complex NLP tasks.
Because full, accurate, open-domain natural lan-
guage understanding lies far beyond current capa-
bilities, nearly all efforts in this area have sought
to extract the maximum mileage from quite lim-
ited semantic representations. Some have used sim-
ple measures of semantic overlap, but the more in-
teresting work has largely converged on a graph-
alignment approach, operating on semantic graphs
derived from syntactic dependency parses, and using
a locally-decomposable alignment score as a proxy
for strength of entailment. (Below, we argue that
even approaches relying on weighted abduction may
be seen in this light.) In this paper, we highlight the
fundamental semantic limitations of this type of ap-
proach, and advocate a multi-stage architecture that
addresses these limitations. The three key limita-
tions are an assumption of monotonicity, an assump-
tion of locality, and a confounding of alignment and
evaluation of entailment.
We focus on the PASCAL RTE data, examples
from which are shown in table 1. This data set con-
tains pairs consisting of a short text followed by a
one-sentence hypothesis. The goal is to say whether
the hypothesis follows from the text and general
background knowledge, according to the intuitions
of an intelligent human reader. That is, the standard
is not whether the hypothesis is logically entailed,
but whether it can reasonably be inferred.
2 Approaching a robust semantics
In this section we try to give a unifying overview
to current work on robust textual inference, to
present fundamental limitations of current meth-
ods, and then to outline our approach to resolving
them. Nearly all current textual inference systems
use a single-stage matching/proof process, and differ
41
ID Text Hypothesis Entailed
59 Two Turkish engineers and an Afghan translator kidnapped
in December were freed Friday.
translator kidnapped in Iraq no
98 Sharon warns Arafat could be targeted for assassination. prime minister targeted for assassination no
152 Twenty-five of the dead were members of the law enforce-
ment agencies and the rest of the 67 were civilians.
25 of the dead were civilians. no
231 The memorandum noted the United Nations estimated that
2.5 million to 3.5 million people died of AIDS last year.
Over 2 million people died of AIDS last
year.
yes
971 Mitsubishi Motors Corp.?s new vehicle sales in the US fell
46 percent in June.
Mitsubishi sales rose 46 percent. no
1806 Vanunu, 49, was abducted by Israeli agents and convicted
of treason in 1986 after discussing his work as a mid-level
Dimona technician with Britain?s Sunday Times newspaper.
Vanunu?s disclosures in 1968 led experts
to conclude that Israel has a stockpile of
nuclear warheads.
no
2081 The main race track in Qatar is located in Shahaniya, on the
Dukhan Road.
Qatar is located in Shahaniya. no
Table 1: Illustrative examples from the PASCAL RTE data set, available at http://www.pascal-network.org/Challenges/RTE.
Though most problems shown have answer no, the data set is actually balanced between yes and no.
mainly in the sophistication of the matching stage.
The simplest approach is to base the entailment pre-
diction on the degree of semantic overlap between
the text and hypothesis using models based on bags
of words, bags of n-grams, TF-IDF scores, or some-
thing similar (Jijkoun and de Rijke, 2005). Such
models have serious limitations: semantic overlap is
typically a symmetric relation, whereas entailment
is clearly not, and, because overlap models do not
account for syntactic or semantic structure, they are
easily fooled by examples like ID 2081.
A more structured approach is to formulate the
entailment prediction as a graph matching problem
(Haghighi et al, 2005; de Salvo Braz et al, 2005).
In this formulation, sentences are represented as nor-
malized syntactic dependency graphs (like the one
shown in figure 1) and entailment is approximated
with an alignment between the graph representing
the hypothesis and a portion of the corresponding
graph(s) representing the text. Each possible align-
ment of the graphs has an associated score, and the
score of the best alignment is used as an approxi-
mation to the strength of the entailment: a better-
aligned hypothesis is assumed to be more likely to
be entailed. To enable incremental search, align-
ment scores are usually factored as a combination
of local terms, corresponding to the nodes and edges
of the two graphs. Unfortunately, even with factored
scores the problem of finding the best alignment of
two graphs is NP-complete, so exact computation is
intractable. Authors have proposed a variety of ap-
proximate search techniques. Haghighi et al (2005)
divide the search into two steps: in the first step they
consider node scores only, which relaxes the prob-
lem to a weighted bipartite graph matching that can
be solved in polynomial time, and in the second step
they add the edges scores and hillclimb the align-
ment via an approximate local search.
A third approach, exemplified by Moldovan et al
(2003) and Raina et al (2005), is to translate de-
pendency parses into neo-Davidsonian-style quasi-
logical forms, and to perform weighted abductive
theorem proving in the tradition of (Hobbs et al,
1988). Unless supplemented with a knowledge
base, this approach is actually isomorphic to the
graph matching approach. For example, the graph
in figure 1 might generate the quasi-LF rose(e1),
nsubj(e1, x1), sales(x1), nn(x1, x2), Mitsubishi(x2),
dobj(e1, x3), percent(x3), num(x3, x4), 46(x4).
There is a term corresponding to each node and arc,
and the resolution steps at the core of weighted ab-
duction theorem proving consider matching an indi-
vidual node of the hypothesis (e.g. rose(e1)) with
something from the text (e.g. fell(e1)), just as in
the graph-matching approach. The two models be-
come distinct when there is a good supply of addi-
tional linguistic and world knowledge axioms?as in
Moldovan et al (2003) but not Raina et al (2005).
Then the theorem prover may generate intermedi-
ate forms in the proof, but, nevertheless, individ-
ual terms are resolved locally without reference to
global context.
Finally, a few efforts (Akhmatova, 2005; Fowler
et al, 2005; Bos and Markert, 2005) have tried to
42
translate sentences into formulas of first-order logic,
in order to test logical entailment with a theorem
prover. While in principle this approach does not
suffer from the limitations we describe below, in
practice it has not borne much fruit. Because few
problem sentences can be accurately translated to
logical form, and because logical entailment is a
strict standard, recall tends to be poor.
The simple graph matching formulation of the
problem belies three important issues. First, the
above systems assume a form of upward monotonic-
ity: if a good match is found with a part of the text,
other material in the text is assumed not to affect
the validity of the match. But many situations lack
this upward monotone character. Consider variants
on ID 98. Suppose the hypothesis were Arafat tar-
geted for assassination. This would allow a perfect
graph match or zero-cost weighted abductive proof,
because the hypothesis is a subgraph of the text.
However, this would be incorrect because it ignores
the modal operator could. Information that changes
the validity of a proof can also exist outside a match-
ing clause. Consider the alternate text Sharon denies
Arafat is targeted for assassination.1
The second issue is the assumption of locality.
Locality is needed to allow practical search, but
many entailment decisions rely on global features of
the alignment, and thus do not naturally factor by
nodes and edges. To take just one example, drop-
ping a restrictive modifier preserves entailment in a
positive context, but not in a negative one. For exam-
ple, Dogs barked loudly entails Dogs barked, but No
dogs barked loudly does not entail No dogs barked.
These more global phenomena cannot be modeled
with a factored alignment score.
The last issue arising in the graph matching ap-
proaches is the inherent confounding of alignment
and entailment determination. The way to show that
one graph element does not follow from another is
to make the cost of aligning them high. However,
since we are embedded in a search for the lowest
cost alignment, this will just cause the system to
choose an alternate alignment rather than recogniz-
ing a non-entailment. In ID 152, we would like the
hypothesis to align with the first part of the text, to
1This is the same problem labeled and addressed as context
in Tatu and Moldovan (2005).
be able to prove that civilians are not members of
law enforcement agencies and conclude that the hy-
pothesis does not follow from the text. But a graph-
matching system will to try to get non-entailment
by making the matching cost between civilians and
members of law enforcement agencies be very high.
However, the likely result of that is that the final part
of the hypothesis will align with were civilians at
the end of the text, assuming that we allow an align-
ment with ?loose? arc correspondence.2 Under this
candidate alignment, the lexical alignments are per-
fect, and the only imperfect alignment is the subject
arc of were is mismatched in the two. A robust in-
ference guesser will still likely conclude that there is
entailment.
We propose that all three problems can be re-
solved in a two-stage architecture, where the align-
ment phase is followed by a separate phase of en-
tailment determination. Although developed inde-
pendently, the same division between alignment and
classification has also been proposed by Marsi and
Krahmer (2005), whose textual system is developed
and evaluated on parallel translations into Dutch.
Their classification phase features an output space
of five semantic relations, and performs well at dis-
tinguishing entailing sentence pairs.
Finding aligned content can be done by any search
procedure. Compared to previous work, we empha-
size structural alignment, and seek to ignore issues
like polarity and quantity, which can be left to a
subsequent entailment decision. For example, the
scoring function is designed to encourage antonym
matches, and ignore the negation of verb predicates.
The ideas clearly generalize to evaluating several
alignments, but we have so far worked with just
the one-best alignment. Given a good alignment,
the determination of entailment reduces to a simple
classification decision. The classifier is built over
features designed to recognize patterns of valid and
invalid inference. Weights for the features can be
hand-set or chosen to minimize a relevant loss func-
tion on training data using standard techniques from
machine learning. Because we already have a com-
plete alignment, the classifier?s decision can be con-
2Robust systems need to allow matches with imperfect arc
correspondence. For instance, given Bill went to Lyons to study
French farming practices, we would like to be able to conclude
that Bill studied French farming despite the structural mismatch.
43
ditioned on arbitrary global features of the aligned
graphs, and it can detect failures of monotonicity.
3 System
Our system has three stages: linguistic analysis,
alignment, and entailment determination.
3.1 Linguistic analysis
Our goal in this stage is to compute linguistic rep-
resentations of the text and hypothesis that contain
as much information as possible about their seman-
tic content. We use typed dependency graphs, which
contain a node for each word and labeled edges rep-
resenting the grammatical relations between words.
Figure 1 gives the typed dependency graph for ID
971. This representation contains much of the infor-
mation about words and relations between them, and
is relatively easy to compute from a syntactic parse.
However many semantic phenomena are not repre-
sented properly; particularly egregious is the inabil-
ity to represent quantification and modality.
We parse input sentences to phrase structure
trees using the Stanford parser (Klein and Manning,
2003), a statistical syntactic parser trained on the
Penn TreeBank. To ensure correct parsing, we pre-
process the sentences to collapse named entities into
new dedicated tokens. Named entities are identi-
fied by a CRF-based NER system, similar to that
described in (McCallum and Li, 2003). After pars-
ing, contiguous collocations which appear in Word-
Net (Fellbaum, 1998) are identified and grouped.
We convert the phrase structure trees to typed de-
pendency graphs using a set of deterministic hand-
coded rules (de Marneffe et al, 2006). In these rules,
heads of constituents are first identified using a mod-
ified version of the Collins head rules that favor se-
mantic heads (such as lexical verbs rather than aux-
iliaries), and dependents of heads are typed using
tregex patterns (Levy and Andrew, 2006), an exten-
sion of the tgrep pattern language. The nodes in the
final graph are then annotated with their associated
word, part-of-speech (given by the parser), lemma
(given by a finite-state transducer described by Min-
nen et al (2001)) and named-entity tag.
3.2 Alignment
The purpose of the second phase is to find a good
partial alignment between the typed dependency
graphs representing the hypothesis and the text. An
alignment consists of a mapping from each node
(word) in the hypothesis graph to a single node in
the text graph, or to null.3 Figure 1 gives the align-
ment for ID 971.
The space of alignments is large: there are
O((m + 1)n) possible alignments for a hypothesis
graph with n nodes and a text graph with m nodes.
We define a measure of alignment quality, and a
procedure for identifying high scoring alignments.
We choose a locally decomposable scoring function,
such that the score of an alignment is the sum of
the local node and edge alignment scores. Unfor-
tunately, there is no polynomial time algorithm for
finding the exact best alignment. Instead we use an
incremental beam search, combined with a node or-
dering heuristic, to do approximate global search in
the space of possible alignments. We have exper-
imented with several alternative search techniques,
and found that the solution quality is not very sensi-
tive to the specific search procedure used.
Our scoring measure is designed to favor align-
ments which align semantically similar subgraphs,
irrespective of polarity. For this reason, nodes re-
ceive high alignment scores when the words they
represent are semantically similar. Synonyms and
antonyms receive the highest score, and unrelated
words receive the lowest. Our hand-crafted scor-
ing metric takes into account the word, the lemma,
and the part of speech, and searches for word relat-
edness using a range of external resources, includ-
ing WordNet, precomputed latent semantic analysis
matrices, and special-purpose gazettes. Alignment
scores also incorporate local edge scores, which are
based on the shape of the paths between nodes in
the text graph which correspond to adjacent nodes
in the hypothesis graph. Preserved edges receive the
highest score, and longer paths receive lower scores.
3.3 Entailment determination
In the final stage of processing, we make a deci-
sion about whether or not the hypothesis is entailed
by the text, conditioned on the typed dependency
graphs, as well as the best alignment between them.
3The limitations of using one-to-one alignments are miti-
gated by the fact that many multiword expressions (e.g. named
entities, noun compounds, multiword prepositions) have been
collapsed into single nodes during linguistic analysis.
44
rose
sales
Mitsubishi
percent
46
nsubj dobj
nn num
Alignment
rose ? fell
sales ? sales
Mitsubishi ? Mitsubishi Motors Corp.
percent ? percent
46 ? 46
Alignment score: ?0.8962
Features
Antonyms aligned in pos/pos context ?
Structure: main predicate good match +
Number: quantity match +
Date: text date deleted in hypothesis ?
Alignment: good score +
Entailment score: ?5.4262
Figure 1: Problem representation for ID 971: typed dependency graph (hypothesis only), alignment, and entailment features.
Because we have a data set of examples that are la-
beled for entailment, we can use techniques from su-
pervised machine learning to learn a classifier. We
adopt the standard approach of defining a featural
representation of the problem and then learning a
linear decision boundary in the feature space. We
focus here on the learning methodology; the next
section covers the definition of the set of features.
Defined in this way, one can apply any statistical
learning algorithm to this classification task, such
as support vector machines, logistic regression, or
naive Bayes. We used a logistic regression classifier
with a Gaussian prior parameter for regularization.
We also compare our learning results with those
achieved by hand-setting the weight parameters for
the classifier, effectively incorporating strong prior
(human) knowledge into the choice of weights.
An advantage to the use of statistical classifiers
is that they can be configured to output a proba-
bility distribution over possible answers rather than
just the most likely answer. This allows us to get
confidence estimates for computing a confidence
weighted score (see section 5). A major concern in
applying machine learning techniques to this clas-
sification problem is the relatively small size of the
training set, which can lead to overfitting problems.
We address this by keeping the feature dimensional-
ity small, and using high regularization penalties in
training.
4 Feature representation
In the entailment determination phase, the entail-
ment problem is reduced to a representation as a
vector of 28 features, over which the statistical
classifier described above operates. These features
try to capture salient patterns of entailment and
non-entailment, with particular attention to contexts
which reverse or block monotonicity, such as nega-
tions and quantifiers. This section describes the most
important groups of features.
Polarity features. These features capture the pres-
ence (or absence) of linguistic markers of negative
polarity contexts in both the text and the hypothesis,
such as simple negation (not), downward-monotone
quantifiers (no, few), restricting prepositions (with-
out, except) and superlatives (tallest).
Adjunct features. These indicate the dropping or
adding of syntactic adjuncts when moving from the
text to the hypothesis. For the common case of
restrictive adjuncts, dropping an adjunct preserves
truth (Dogs barked loudly |= Dogs barked), while
adding an adjunct does not (Dogs barked 6|= Dogs
barked today). However, in negative-polarity con-
texts (such as No dogs barked), this heuristic is
reversed: adjuncts can safely be added, but not
dropped. For example, in ID 59, the hypothesis
aligns well with the text, but the addition of in Iraq
indicates non-entailment.
We identify the ?root nodes? of the problem: the
root node of the hypothesis graph and the corre-
sponding aligned node in the text graph. Using de-
pendency information, we identify whether adjuncts
have been added or dropped. We then determine
the polarity (negative context, positive context or
restrictor of a universal quantifier) of the two root
nodes to generate features accordingly.
Antonymy features. Entailment problems might
involve antonymy, as in ID 971. We check whether
an aligned pairs of text/hypothesis words appear to
be antonymous by consulting a pre-computed list
of about 40,000 antonymous and other contrasting
pairs derived from WordNet. For each antonymous
pair, we generate one of three boolean features, in-
dicating whether (i) the words appear in contexts of
matching polarity, (ii) only the text word appears in
a negative-polarity context, or (iii) only the hypoth-
esis word does.
45
Modality features. Modality features capture
simple patterns of modal reasoning, as in ID 98,
which illustrates the heuristic that possibility does
not entail actuality. According to the occurrence
(or not) of predefined modality markers, such as
must or maybe, we map the text and the hypoth-
esis to one of six modalities: possible, not possi-
ble, actual, not actual, necessary, and not necessary.
The text/hypothesis modality pair is then mapped
into one of the following entailment judgments: yes,
weak yes, don?t know, weak no, or no. For example:
(not possible |= not actual)? ? yes
(possible |= necessary)? ? weak no
Factivity features. The context in which a verb
phrase is embedded may carry semantic presuppo-
sitions giving rise to (non-)entailments such as The
gangster tried to escape 6|= The gangster escaped.
This pattern of entailment, like others, can be re-
versed by negative polarity markers (The gangster
managed to escape |= The gangster escaped while
The gangster didn?t manage to escape 6|= The gang-
ster escaped). To capture these phenomena, we
compiled small lists of ?factive? and non-factive
verbs, clustered according to the kinds of entail-
ments they create. We then determine to which class
the parent of the text aligned with the hypothesis
root belongs to. If the parent is not in the list, we
only check whether the embedding text is an affir-
mative context or a negative one.
Quantifier features. These features are designed
to capture entailment relations among simple sen-
tences involving quantification, such as Every com-
pany must report |= A company must report (or
The company, or IBM). No attempt is made to han-
dle multiple quantifiers or scope ambiguities. Each
quantifier found in an aligned pair of text/hypothesis
words is mapped into one of five quantifier cate-
gories: no, some, many, most, and all. The no
category is set apart, while an ordering over the
other four categories is defined. The some category
also includes definite and indefinite determiners and
small cardinal numbers. A crude attempt is made to
handle negation by interchanging no and all in the
presence of negation. Features are generated given
the categories of both hypothesis and text.
Number, date, and time features. These are de-
signed to recognize (mis-)matches between num-
bers, dates, and times, as in IDs 1806 and 231. We
do some normalization (e.g. of date representations)
and have a limited ability to do fuzzy matching. In
ID 1806, the mismatched years are correctly iden-
tified. Unfortunately, in ID 231 the significance of
over is not grasped and a mismatch is reported.
Alignment features. Our feature representation
includes three real-valued features intended to rep-
resent the quality of the alignment: score is the
raw score returned from the alignment phase, while
goodscore and badscore try to capture whether the
alignment score is ?good? or ?bad? by computing
the sigmoid function of the distance between the
alignment score and hard-coded ?good? and ?bad?
reference values.
5 Evaluation
We present results based on the First PASCAL RTE
Challenge, which used a development set contain-
ing 567 pairs and a test set containing 800 pairs.
The data sets are balanced to contain equal num-
bers of yes and no answers. The RTE Challenge
recommended two evaluation metrics: raw accuracy
and confidence weighted score (CWS). The CWS is
computed as follows: for each positive integer k up
to the size of the test set, we compute accuracy over
the k most confident predictions. The CWS is then
the average, over k, of these partial accuracies. Like
raw accuracy, it lies in the interval [0, 1], but it will
exceed raw accuracy to the degree that predictions
are well-calibrated.
Several characteristics of the RTE problems
should be emphasized. Examples are derived from a
broad variety of sources, including newswire; there-
fore systems must be domain-independent. The in-
ferences required are, from a human perspective,
fairly superficial: no long chains of reasoning are
involved. However, there are ?trick? questions ex-
pressly designed to foil simplistic techniques. The
definition of entailment is informal and approx-
imate: whether a competent speaker with basic
knowledge of the world would typically infer the hy-
pothesis from the text. Entailments will certainly de-
pend on linguistic knowledge, and may also depend
on world knowledge; however, the scope of required
46
Algorithm RTE1 Dev Set RTE1 Test Set
Acc CWS Acc CWS
Random 50.0% 50.0% 50.0% 50.0%
Jijkoun et al 05 61.0% 64.9% 55.3% 55.9%
Raina et al 05 57.8% 66.1% 55.5% 63.8%
Haghighi et al 05 ? ? 56.8% 61.4%
Bos & Markert 05 ? ? 57.7% 63.2%
Alignment only 58.7% 59.1% 54.5% 59.7%
Hand-tuned 60.3% 65.3% 59.1% 65.0%
Learning 61.2% 74.4% 59.1% 63.9%
Table 2: Performance on the RTE development and test sets.
CWS stands for confidence weighted score (see text).
world knowledge is left unspecified.4
Despite the informality of the problem definition,
human judges exhibit very good agreement on the
RTE task, with agreement rate of 91?96% (Dagan
et al, 2005). In principle, then, the upper bound
for machine performance is quite high. In practice,
however, the RTE task is exceedingly difficult for
computers. Participants in the first PASCAL RTE
workshop reported accuracy from 49% to 59%, and
CWS from 50.0% to 69.0% (Dagan et al, 2005).
Table 2 shows results for a range of systems and
testing conditions. We report accuracy and CWS on
each RTE data set. The baseline for all experiments
is random guessing, which always attains 50% accu-
racy. We show comparable results from recent sys-
tems based on lexical similarity (Jijkoun and de Ri-
jke, 2005), graph alignment (Haghighi et al, 2005),
weighted abduction (Raina et al, 2005), and a mixed
system including theorem proving (Bos and Mark-
ert, 2005).
We then show results for our system under several
different training regimes. The row labeled ?align-
ment only? describes experiments in which all fea-
tures except the alignment score are turned off. We
predict entailment just in case the alignment score
exceeds a threshold which is optimized on devel-
opment data. ?Hand-tuning? describes experiments
in which all features are on, but no training oc-
curs; rather, weights are set by hand, according to
human intuition. Finally, ?learning? describes ex-
periments in which all features are on, and feature
weights are trained on the development data. The
4Each RTE problem is also tagged as belonging to one of
seven tasks. Previous work (Raina et al, 2005) has shown that
conditioning on task can significantly improve accuracy. In this
work, however, we ignore the task variable, and none of the
results shown in table 2 reflect optimization by task.
figures reported for development data performance
therefore reflect overfitting; while such results are
not a fair measure of overall performance, they can
help us assess the adequacy of our feature set: if
our features have failed to capture relevant aspects
of the problem, we should expect poor performance
even when overfitting. It is therefore encouraging
to see CWS above 70%. Finally, the figures re-
ported for test data performance are the fairest ba-
sis for comparison. These are significantly better
than our results for alignment only (Fisher?s exact
test, p < 0.05), indicating that we gain real value
from our features. However, the gain over compara-
ble results from other teams is not significant at the
p < 0.05 level.
A curious observation is that the results for hand-
tuned weights are as good or better than results for
learned weights. A possible explanation runs as fol-
lows. Most of the features represent high-level pat-
terns which arise only occasionally. Because the
training data contains only a few hundred exam-
ples, many features are active in just a handful of
instances; their learned weights are therefore quite
noisy. Indeed, a feature which is expected to fa-
vor entailment may even wind up with a negative
weight: the modal feature weak yes is an example.
As shown in table 3, the learned weight for this fea-
ture was strongly negative ? but this resulted from
a single training example in which the feature was
active but the hypothesis was not entailed. In such
cases, we shouldn?t expect good generalization to
test data, and human intuition about the ?value? of
specific features may be more reliable.
Table 3 shows the values learned for selected fea-
ture weights. As expected, the features added ad-
junct in all context, modal yes, and text is factive
were all found to be strong indicators of entailment,
while date insert, date modifier insert, widening
from text to hyp all indicate lack of entailment. Inter-
estingly, text has neg marker and text & hyp diff po-
larity were also found to disfavor entailment; while
this outcome is sensible, it was not anticipated or
designed.
6 Conclusion
The best current approaches to the problem of tex-
tual inference work by aligning semantic graphs,
47
Feature class & condition weight
Adjunct added adjunct in all context 1.40
Date date mismatch 1.30
Alignment good score 1.10
Modal yes 0.70
Modal no 0.51
Factive text is factive 0.46
. . . . . . . . .
Polarity text & hyp same polarity ?0.45
Modal don?t know ?0.59
Quantifier widening from text to hyp ?0.66
Polarity text has neg marker ?0.66
Polarity text & hyp diff polarity ?0.72
Alignment bad score ?1.53
Date date modifier insert ?1.57
Modal weak yes ?1.92
Date date insert ?2.63
Table 3: Learned weights for selected features. Positive weights
favor entailment. Weights near 0 are omitted. Based on training
on the PASCAL RTE development set.
using a locally-decomposable alignment score as a
proxy for strength of entailment. We have argued
that such models suffer from three crucial limita-
tions: an assumption of monotonicity, an assump-
tion of locality, and a confounding of alignment and
entailment determination.
We have described a system which extends
alignment-based systems while attempting to ad-
dress these limitations. After finding the best align-
ment between text and hypothesis, we extract high-
level semantic features of the entailment problem,
and input these features to a statistical classifier to
make an entailment decision. Using this multi-stage
architecture, we report results on the PASCAL RTE
data which surpass previously-reported results for
alignment-based systems.
We see the present work as a first step in a promis-
ing direction. Much work remains in improving the
entailment features, many of which may be seen as
rough approximations to a formal monotonicity cal-
culus. In future, we aim to combine more precise
modeling of monotonicity effects with better mod-
eling of paraphrase equivalence.
Acknowledgements
We thank Anna Rafferty, Josh Ainslie, and partic-
ularly Roger Grosse for contributions to the ideas
and system reported here. This work was supported
in part by the Advanced Research and Development
Activity (ARDA)?s Advanced Question Answering
for Intelligence (AQUAINT) Program.
References
E. Akhmatova. 2005. Textual entailment resolution via atomic
propositions. In Proceedings of the PASCAL Challenges
Workshop on Recognising Textual Entailment, 2005.
J. Bos and K. Markert. 2005. Recognising textual entailment
with logical inference. In EMNLP-05.
I. Dagan, O. Glickman, and B. Magnini. 2005. The PASCAL
recognising textual entailment challenge. In Proceedings of
the PASCAL Challenges Workshop on Recognising Textual
Entailment.
Marie-Catherine de Marneffe, Bill MacCartney, and Christo-
pher D. Manning. 2006. Generating typed dependency
parses from phrase structure parses. In LREC 2006.
R. de Salvo Braz, R. Girju, V. Punyakanok, D. Roth, and
M. Sammons. 2005. An inference model for semantic entail-
ment and question-answering. In Proceedings of the Twenti-
eth National Conference on Artificial Intelligence (AAAI).
C. Fellbaum. 1998. WordNet: an electronic lexical database.
MIT Press.
A. Fowler, B. Hauser, D. Hodges, I. Niles, A. Novischi, and
J. Stephan. 2005. Applying COGEX to recognize textual
entailment. In Proceedings of the PASCAL Challenges Work-
shop on Recognising Textual Entailment.
A. Haghighi, A. Ng, and C. D. Manning. 2005. Robust textual
inference via graph matching. In EMNLP-05.
J. R. Hobbs, M. Stickel, P. Martin, and D. D. Edwards. 1988.
Interpretation as abduction. In 26th Annual Meeting of the
Association for Computational Linguistics: Proceedings of
the Conference, pages 95?103, Buffalo, New York.
V. Jijkoun and M. de Rijke. 2005. Recognizing textual entail-
ment using lexical similarity. In Proceedings of the PAS-
CAL Challenge Workshop on Recognising Textual Entail-
ment, 2005, pages 73?76.
D. Klein and C. D. Manning. 2003. Accurate unlexicalized
parsing. In Proceedings of the 41st Meeting of the Associa-
tion of Computational Linguistics.
Roger Levy and Galen Andrew. 2006. Tregex and Tsurgeon:
tools for querying and manipulating tree data structures. In
LREC 2006.
E. Marsi and E. Krahmer. 2005. Classification of semantic re-
lations by humans and machines. In Proceedings of the ACL
2005 Workshop on Empirical Modeling of Semantic Equiva-
lence and Entailment.
A. McCallum and W. Li. 2003. Early results for named entity
recognition with conditional random fields, feature induction
and web-enhanced lexicons. In Proceedings of CoNLL 2003.
G. Minnen, J. Carroll, and D. Pearce. 2001. Applied morpho-
logical processing in English. In Natural Language Engi-
neering, volume 7(3), pages 207?233.
D. Moldovan, C. Clark, S. Harabagiu, and S. Maiorano. 2003.
COGEX: A logic prover for question answering. In NAACL-
03.
M. Pasca and S. Harabagiu. 2001. High performance ques-
tion/answering. In SIGIR-01, pages 366?374.
R. Raina, A .Ng, and C. D. Manning. 2005. Robust textual
inference via learning and abductive reasoning. In Proceed-
ings of the Twentieth National Conference on Artificial Intel-
ligence (AAAI).
M. Tatu and D. Moldovan. 2005. A semantic approach to rec-
ognizing textual entailment. In HLT/EMNLP 2005, pages
371?378.
48
Proceedings of the 43rd Annual Meeting of the ACL, pages 363?370,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Incorporating Non-local Information into Information
Extraction Systems by Gibbs Sampling
Jenny Rose Finkel, Trond Grenager, and Christopher Manning
Computer Science Department
Stanford University
Stanford, CA 94305
{jrfinkel, grenager, mannning}@cs.stanford.edu
Abstract
Most current statistical natural language process-
ing models use only local features so as to permit
dynamic programming in inference, but this makes
them unable to fully account for the long distance
structure that is prevalent in language use. We
show how to solve this dilemma with Gibbs sam-
pling, a simple Monte Carlo method used to per-
form approximate inference in factored probabilis-
tic models. By using simulated annealing in place
of Viterbi decoding in sequence models such as
HMMs, CMMs, and CRFs, it is possible to incorpo-
rate non-local structure while preserving tractable
inference. We use this technique to augment an
existing CRF-based information extraction system
with long-distance dependency models, enforcing
label consistency and extraction template consis-
tency constraints. This technique results in an error
reduction of up to 9% over state-of-the-art systems
on two established information extraction tasks.
1 Introduction
Most statistical models currently used in natural lan-
guage processing represent only local structure. Al-
though this constraint is critical in enabling tractable
model inference, it is a key limitation in many tasks,
since natural language contains a great deal of non-
local structure. A general method for solving this
problem is to relax the requirement of exact infer-
ence, substituting approximate inference algorithms
instead, thereby permitting tractable inference in
models with non-local structure. One such algo-
rithm is Gibbs sampling, a simple Monte Carlo algo-
rithm that is appropriate for inference in any factored
probabilistic model, including sequence models and
probabilistic context free grammars (Geman and Ge-
man, 1984). Although Gibbs sampling is widely
used elsewhere, there has been extremely little use
of it in natural language processing.1 Here, we use
it to add non-local dependencies to sequence models
for information extraction.
Statistical hidden state sequence models, such
as Hidden Markov Models (HMMs) (Leek, 1997;
Freitag and McCallum, 1999), Conditional Markov
Models (CMMs) (Borthwick, 1999), and Condi-
tional Random Fields (CRFs) (Lafferty et al, 2001)
are a prominent recent approach to information ex-
traction tasks. These models all encode the Markov
property: decisions about the state at a particular po-
sition in the sequence can depend only on a small lo-
cal window. It is this property which allows tractable
computation: the Viterbi, Forward Backward, and
Clique Calibration algorithms all become intractable
without it.
However, information extraction tasks can benefit
from modeling non-local structure. As an example,
several authors (see Section 8) mention the value of
enforcing label consistency in named entity recogni-
tion (NER) tasks. In the example given in Figure 1,
the second occurrence of the token Tanjug is mis-
labeled by our CRF-based statistical NER system,
because by looking only at local evidence it is un-
clear whether it is a person or organization. The first
occurrence of Tanjug provides ample evidence that
it is an organization, however, and by enforcing la-
bel consistency the system should be able to get it
right. We show how to incorporate constraints of
this form into a CRF model by using Gibbs sam-
pling instead of the Viterbi algorithm as our infer-
ence procedure, and demonstrate that this technique
yields significant improvements on two established
IE tasks.
1Prior uses in NLP of which we are aware include: Kim et
al. (1995), Della Pietra et al (1997) and Abney (1997).
363
the news agency Tanjug reported . . . airport , Tanjug said .
Figure 1: An example of the label consistency problem excerpted from a document in the CoNLL 2003 English dataset.
2 Gibbs Sampling for Inference in
Sequence Models
In hidden state sequence models such as HMMs,
CMMs, and CRFs, it is standard to use the Viterbi
algorithm, a dynamic programming algorithm, to in-
fer the most likely hidden state sequence given the
input and the model (see, e.g., Rabiner (1989)). Al-
though this is the only tractable method for exact
computation, there are other methods for comput-
ing an approximate solution. Monte Carlo methods
are a simple and effective class of methods for ap-
proximate inference based on sampling. Imagine
we have a hidden state sequence model which de-
fines a probability distribution over state sequences
conditioned on any given input. With such a model
M we should be able to compute the conditional
probability PM (s|o) of any state sequence s =
{s0, . . . , sN} given some observed input sequence
o = {o0, . . . , oN}. One can then sample se-
quences from the conditional distribution defined by
the model. These samples are likely to be in high
probability areas, increasing our chances of finding
the maximum. The challenge is how to sample se-
quences efficiently from the conditional distribution
defined by the model.
Gibbs sampling provides a clever solution (Ge-
man and Geman, 1984). Gibbs sampling defines a
Markov chain in the space of possible variable as-
signments (in this case, hidden state sequences) such
that the stationary distribution of the Markov chain
is the joint distribution over the variables. Thus it
is called a Markov Chain Monte Carlo (MCMC)
method; see Andrieu et al (2003) for a good MCMC
tutorial. In practical terms, this means that we
can walk the Markov chain, occasionally outputting
samples, and that these samples are guaranteed to
be drawn from the target distribution. Furthermore,
the chain is defined in very simple terms: from each
state sequence we can only transition to a state se-
quence obtained by changing the state at any one
position i, and the distribution over these possible
transitions is just
PG(s(t)|s(t?1)) = PM (s(t)i |s
(t?1)
?i ,o). (1)
where s?i is all states except si. In other words, the
transition probability of the Markov chain is the con-
ditional distribution of the label at the position given
the rest of the sequence. This quantity is easy to
compute in any Markov sequence model, including
HMMs, CMMs, and CRFs. One easy way to walk
the Markov chain is to loop through the positions i
from 1 to N , and for each one, to resample the hid-
den state at that position from the distribution given
in Equation 1. By outputting complete sequences
at regular intervals (such as after resampling all N
positions), we can sample sequences from the con-
ditional distribution defined by the model.
This is still a gravely inefficient process, how-
ever. Random sampling may be a good way to es-
timate the shape of a probability distribution, but it
is not an efficient way to do what we want: find
the maximum. However, we cannot just transi-
tion greedily to higher probability sequences at each
step, because the space is extremely non-convex. We
can, however, borrow a technique from the study
of non-convex optimization and use simulated an-
nealing (Kirkpatrick et al, 1983). Geman and Ge-
man (1984) show that it is easy to modify a Gibbs
Markov chain to do annealing; at time t we replace
the distribution in (1) with
PA(s(t)|s(t?1)) =
PM (s(t)i |s
(t?1)
?i ,o)1/ct
?
j PM (s
(t)
j |s
(t?1)
?j ,o)1/ct
(2)
where c = {c0, . . . , cT } defines a cooling schedule.
At each step, we raise each value in the conditional
distribution to an exponent and renormalize before
sampling from it. Note that when c = 1 the distri-
bution is unchanged, and as c ? 0 the distribution
364
Inference CoNLL Seminars
Viterbi 85.51 91.85
Gibbs 85.54 91.85
Sampling 85.51 91.85
85.49 91.85
85.51 91.85
85.51 91.85
85.51 91.85
85.51 91.85
85.51 91.85
85.51 91.86
Mean 85.51 91.85
Std. Dev. 0.01 0.004
Table 1: An illustration of the effectiveness of Gibbs sampling,
compared to Viterbi inference, for the two tasks addressed in
this paper: the CoNLL named entity recognition task, and the
CMU Seminar Announcements information extraction task. We
show 10 runs of Gibbs sampling in the same CRF model that
was used for Viterbi. For each run the sampler was initialized
to a random sequence, and used a linear annealing schedule that
sampled the complete sequence 1000 times. CoNLL perfor-
mance is measured as per-entity F1, and CMU Seminar An-
nouncements performance is measured as per-token F1.
becomes sharper, and when c = 0 the distribution
places all of its mass on the maximal outcome, hav-
ing the effect that the Markov chain always climbs
uphill. Thus if we gradually decrease c from 1 to
0, the Markov chain increasingly tends to go up-
hill. This annealing technique has been shown to
be an effective technique for stochastic optimization
(Laarhoven and Arts, 1987).
To verify the effectiveness of Gibbs sampling and
simulated annealing as an inference technique for
hidden state sequence models, we compare Gibbs
and Viterbi inference methods for a basic CRF, with-
out the addition of any non-local model. The results,
given in Table 1, show that if the Gibbs sampler is
run long enough, its accuracy is the same as a Viterbi
decoder.
3 A Conditional Random Field Model
Our basic CRF model follows that of Lafferty et al
(2001). We choose a CRF because it represents the
state of the art in sequence modeling, allowing both
discriminative training and the bi-directional flow of
probabilistic information across the sequence. A
CRF is a conditional sequence model which rep-
resents the probability of a hidden state sequence
given some observations. In order to facilitate ob-
taining the conditional probabilities we need for
Gibbs sampling, we generalize the CRF model in a
Feature NER TF
Current Word X X
Previous Word X X
Next Word X X
Current Word Character n-gram all length ? 6
Current POS Tag X
Surrounding POS Tag Sequence X
Current Word Shape X X
Surrounding Word Shape Sequence X X
Presence of Word in Left Window size 4 size 9
Presence of Word in Right Window size 4 size 9
Table 2: Features used by the CRF for the two tasks: named
entity recognition (NER) and template filling (TF).
way that is consistent with the Markov Network lit-
erature (see Cowell et al (1999)): we create a linear
chain of cliques, where each clique, c, represents the
probabilistic relationship between an adjacent pair
of states2 using a clique potential ?c, which is just
a table containing a value for each possible state as-
signment. The table is not a true probability distribu-
tion, as it only accounts for local interactions within
the clique. The clique potentials themselves are de-
fined in terms of exponential models conditioned on
features of the observation sequence, and must be
instantiated for each new observation sequence. The
sequence of potentials in the clique chain then de-
fines the probability of a state sequence (given the
observation sequence) as
PCRF(s|o) ?
N
?
i=1
?i(si?1, si) (3)
where ?i(si?1, si) is the element of the clique po-
tential at position i corresponding to states si?1 and
si.3
Although a full treatment of CRF training is be-
yond the scope of this paper (our technique assumes
the model is already trained), we list the features
used by our CRF for the two tasks we address in
Table 2. During training, we regularized our expo-
nential models with a quadratic prior and used the
quasi-Newton method for parameter optimization.
As is customary, we used the Viterbi algorithm to
infer the most likely state sequence in a CRF.
2CRFs with larger cliques are also possible, in which case
the potentials represent the relationship between a subsequence
of k adjacent states, and contain |S|k elements.
3To handle the start condition properly, imagine also that we
define a distinguished start state s0.
365
The clique potentials of the CRF, instantiated for
some observation sequence, can be used to easily
compute the conditional distribution over states at
a position given in Equation 1. Recall that at posi-
tion i we want to condition on the states in the rest
of the sequence. The state at this position can be
influenced by any other state that it shares a clique
with; in particular, when the clique size is 2, there
are 2 such cliques. In this case the Markov blanket
of the state (the minimal set of states that renders
a state conditionally independent of all other states)
consists of the two neighboring states and the obser-
vation sequence, all of which are observed. The con-
ditional distribution at position i can then be com-
puted simply as
PCRF(si|s?i,o) ? ?i(si?1, si)?i+1(si, si+1) (4)
where the factor tables F in the clique chain are al-
ready conditioned on the observation sequence.
4 Datasets and Evaluation
We test the effectiveness of our technique on two es-
tablished datasets: the CoNLL 2003 English named
entity recognition dataset, and the CMU Seminar
Announcements information extraction dataset.
4.1 The CoNLL NER Task
This dataset was created for the shared task of the
Seventh Conference on Computational Natural Lan-
guage Learning (CoNLL),4 which concerned named
entity recognition. The English data is a collection
of Reuters newswire articles annotated with four en-
tity types: person (PER), location (LOC), organi-
zation (ORG), and miscellaneous (MISC). The data
is separated into a training set, a development set
(testa), and a test set (testb). The training set con-
tains 945 documents, and approximately 203,000 to-
kens. The development set has 216 documents and
approximately 51,000 tokens, and the test set has
231 documents and approximately 46,000 tokens.
We evaluate performance on this task in the man-
ner dictated by the competition so that results can be
properly compared. Precision and recall are evalu-
ated on a per-entity basis (and combined into an F1
score). There is no partial credit; an incorrect entity
4Available at http://cnts.uia.ac.be/conll2003/ner/.
boundary is penalized as both a false positive and as
a false negative.
4.2 The CMU Seminar Announcements Task
This dataset was developed as part of Dayne Fre-
itag?s dissertation research Freitag (1998).5 It con-
sists of 485 emails containing seminar announce-
ments at Carnegie Mellon University. It is annotated
for four fields: speaker, location, start time, and end
time. Sutton and McCallum (2004) used 5-fold cross
validation when evaluating on this dataset, so we ob-
tained and used their data splits, so that results can
be properly compared. Because the entire dataset is
used for testing, there is no development set. We
also used their evaluation metric, which is slightly
different from the method for CoNLL data. Instead
of evaluating precision and recall on a per-entity ba-
sis, they are evaluated on a per-token basis. Then, to
calculate the overall F1 score, the F1 scores for each
class are averaged.
5 Models of Non-local Structure
Our models of non-local structure are themselves
just sequence models, defining a probability distri-
bution over all possible state sequences. It is pos-
sible to flexibly model various forms of constraints
in a way that is sensitive to the linguistic structure
of the data (e.g., one can go beyond imposing just
exact identity conditions). One could imagine many
ways of defining such models; for simplicity we use
the form
PM (s|o) ?
?
???
?#(?,s,o)? (5)
where the product is over a set of violation types ?,
and for each violation type ? we specify a penalty
parameter ??. The exponent #(?, s,o) is the count
of the number of times that the violation ? occurs
in the state sequence s with respect to the observa-
tion sequence o. This has the effect of assigning
sequences with more violations a lower probabil-
ity. The particular violation types are defined specif-
ically for each task, and are described in the follow-
ing two sections.
This model, as defined above, is not normalized,
and clearly it would be expensive to do so. This
5Available at http://nlp.shef.ac.uk/dot.kom/resources.html.
366
PER LOC ORG MISC
PER 3141 4 5 0
LOC 6436 188 3
ORG 2975 0
MISC 2030
Table 3: Counts of the number of times multiple occurrences of
a token sequence is labeled as different entity types in the same
document. Taken from the CoNLL training set.
PER LOC ORG MISC
PER 1941 5 2 3
LOC 0 167 6 63
ORG 22 328 819 191
MISC 14 224 7 365
Table 4: Counts of the number of times an entity sequence is
labeled differently from an occurrence of a subsequence of it
elsewhere in the document. Rows correspond to sequences, and
columns to subsequences. Taken from the CoNLL training set.
doesn?t matter, however, because we only use the
model for Gibbs sampling, and so only need to com-
pute the conditional distribution at a single position
i (as defined in Equation 1). One (inefficient) way
to compute this quantity is to enumerate all possi-
ble sequences differing only at position i, compute
the score assigned to each by the model, and renor-
malize. Although it seems expensive, this compu-
tation can be made very efficient with a straightfor-
ward memoization technique: at all times we main-
tain data structures representing the relationship be-
tween entity labels and token sequences, from which
we can quickly compute counts of different types of
violations.
5.1 CoNLL Consistency Model
Label consistency structure derives from the fact that
within a particular document, different occurrences
of a particular token sequence are unlikely to be la-
beled as different entity types. Although any one
occurrence may be ambiguous, it is unlikely that all
instances are unclear when taken together.
The CoNLL training data empirically supports the
strength of the label consistency constraint. Table 3
shows the counts of entity labels for each pair of
identical token sequences within a document, where
both are labeled as an entity. Note that inconsis-
tent labelings are very rare.6 In addition, we also
6A notable exception is the labeling of the same text as both
organization and location within the same document. This is a
consequence of the large portion of sports news in the CoNLL
want to model subsequence constraints: having seen
Geoff Woods earlier in a document as a person is
a good indicator that a subsequent occurrence of
Woods should also be labeled as a person. How-
ever, if we examine all cases of the labelings of
other occurrences of subsequences of a labeled en-
tity, we find that the consistency constraint does not
hold nearly so strictly in this case. As an exam-
ple, one document contains references to both The
China Daily, a newspaper, and China, the country.
Counts of subsequence labelings within a document
are listed in Table 4. Note that there are many off-
diagonal entries: the China Daily case is the most
common, occurring 328 times in the dataset.
The penalties used in the long distance constraint
model for CoNLL are the Empirical Bayes estimates
taken directly from the data (Tables 3 and 4), except
that we change counts of 0 to be 1, so that the dis-
tribution remains positive. So the estimate of a PER
also being an ORG is 53151 ; there were 5 instance of
an entity being labeled as both, PER appeared 3150
times in the data, and we add 1 to this for smoothing,
because PER-MISC never occured. However, when
we have a phrase labeled differently in two differ-
ent places, continuing with the PER-ORG example,
it is unclear if we should penalize it as PER that is
also an ORG or an ORG that is also a PER. To deal
with this, we multiply the square roots of each esti-
mate together to form the penalty term. The penalty
term is then multiplied in a number of times equal
to the length of the offending entity; this is meant to
?encourage? the entity to shrink.7 For example, say
we have a document with three entities, Rotor Vol-
gograd twice, once labeled as PER and once as ORG,
and Rotor, labeled as an ORG. The likelihood of a
PER also being an ORG is 53151 , and of an ORG also
being a PER is 53169 , so the penalty for this violation
is (
?
5
3151 ?
?
5
3151 )2. The likelihood of a ORG be-
ing a subphrase of a PER is 2842 . So the total penalty
would be 53151 ? 53169 ? 2842 .
dataset, so that city names are often also team names.
7While there is no theoretical justification for this, we found
it to work well in practice.
367
5.2 CMU Seminar Announcements
Consistency Model
Due to the lack of a development set, our consis-
tency model for the CMU Seminar Announcements
is much simpler than the CoNLL model, the num-
bers where selected due to our intuitions, and we did
not spend much time hand optimizing the model.
Specifically, we had three constraints. The first is
that all entities labeled as start time are normal-
ized, and are penalized if they are inconsistent. The
second is a corresponding constraint for end times.
The last constraint attempts to consistently label the
speakers. If a phrase is labeled as a speaker, we as-
sume that the last word is the speaker?s last name,
and we penalize for each occurrance of that word
which is not also labeled speaker. For the start and
end times the penalty is multiplied in based on how
many words are in the entity. For the speaker, the
penalty is only multiplied in once. We used a hand
selected penalty of exp?4.0.
6 Combining Sequence Models
In the previous section we defined two models of
non-local structure. Now we would like to incor-
porate them into the local model (in our case, the
trained CRF), and use Gibbs sampling to find the
most likely state sequence. Because both the trained
CRF and the non-local models are themselves se-
quence models, we simply combine the two mod-
els into a factored sequence model of the following
form
PF (s|o) ? PM (s|o)PL(s|o) (6)
where M is the local CRF model, L is the new non-
local model, and F is the factored model.8 In this
form, the probability again looks difficult to com-
pute (because of the normalizing factor, a sum over
all hidden state sequences of length N ). However,
since we are only using the model for Gibbs sam-
pling, we never need to compute the distribution ex-
plicitly. Instead, we need only the conditional prob-
ability of each position in the sequence, which can
be computed as
PF (si|s?i,o) ? PM (si|s?i,o)PL(si|s?i,o). (7)
8This model double-generates the state sequence condi-
tioned on the observations. In practice we don?t find this to
be a problem.
CoNLL
Approach LOC ORG MISC PER ALL
B&M LT-RMN ? ? ? ? 80.09
B&M GLT-RMN ? ? ? ? 82.30
Local+Viterbi 88.16 80.83 78.51 90.36 85.51
NonLoc+Gibbs 88.51 81.72 80.43 92.29 86.86
Table 5: F1 scores of the local CRF and non-local models on the
CoNLL 2003 named entity recognition dataset. We also provide
the results from Bunescu and Mooney (2004) for comparison.
CMU Seminar Announcements
Approach STIME ETIME SPEAK LOC ALL
S&M CRF 97.5 97.5 88.3 77.3 90.2
S&M Skip-CRF 96.7 97.2 88.1 80.4 90.6
Local+Viterbi 96.67 97.36 83.39 89.98 91.85
NonLoc+Gibbs 97.11 97.89 84.16 90.00 92.29
Table 6: F1 scores of the local CRF and non-local models on
the CMU Seminar Announcements dataset. We also provide
the results from Sutton and McCallum (2004) for comparison.
At inference time, we then sample from the Markov
chain defined by this transition probability.
7 Results and Discussion
In our experiments we compare the impact of adding
the non-local models with Gibbs sampling to our
baseline CRF implementation. In the CoNLL named
entity recognition task, the non-local models in-
crease the F1 accuracy by about 1.3%. Although
such gains may appear modest, note that they are
achieved relative to a near state-of-the-art NER sys-
tem: the winner of the CoNLL English task reported
an F1 score of 88.76. In contrast, the increases pub-
lished by Bunescu and Mooney (2004) are relative
to a baseline system which scores only 80.9% on
the same task. Our performance is similar on the
CMU Seminar Announcements dataset. We show
the per-field F1 results that were reported by Sutton
and McCallum (2004) for comparison, and note that
we are again achieving gains against a more compet-
itive baseline system.
For all experiments involving Gibbs sampling, we
used a linear cooling schedule. For the CoNLL
dataset we collected 200 samples per trial, and for
the CMU Seminar Announcements we collected 100
samples. We report the average of all trials, and in all
cases we outperform the baseline with greater than
95% confidence, using the standard t-test. The trials
had low standard deviations - 0.083% and 0.007% -
and high minimun F-scores - 86.72%, and 92.28%
368
- for the CoNLL and CMU Seminar Announce-
ments respectively, demonstrating the stability of
our method.
The biggest drawback to our model is the com-
putational cost. Taking 100 samples dramatically
increases test time. Averaged over 3 runs on both
Viterbi and Gibbs, CoNLL testing time increased
from 55 to 1738 seconds, and CMU Seminar An-
nouncements testing time increases from 189 to
6436 seconds.
8 Related Work
Several authors have successfully incorporated a
label consistency constraint into probabilistic se-
quence model named entity recognition systems.
Mikheev et al (1999) and Finkel et al (2004) in-
corporate label consistency information by using ad-
hoc multi-stage labeling procedures that are effec-
tive but special-purpose. Malouf (2002) and Curran
and Clark (2003) condition the label of a token at
a particular position on the label of the most recent
previous instance of that same token in a prior sen-
tence of the same document. Note that this violates
the Markov property, but is achieved by slightly re-
laxing the requirement of exact inference. Instead
of finding the maximum likelihood sequence over
the entire document, they classify one sentence at a
time, allowing them to condition on the maximum
likelihood sequence of previous sentences. This ap-
proach is quite effective for enforcing label consis-
tency in many NLP tasks, however, it permits a for-
ward flow of information only, which is not suffi-
cient for all cases of interest. Chieu and Ng (2002)
propose a solution to this problem: for each to-
ken, they define additional features taken from other
occurrences of the same token in the document.
This approach has the added advantage of allowing
the training procedure to automatically learn good
weightings for these ?global? features relative to the
local ones. However, this approach cannot easily
be extended to incorporate other types of non-local
structure.
The most relevant prior works are Bunescu and
Mooney (2004), who use a Relational Markov Net-
work (RMN) (Taskar et al, 2002) to explicitly mod-
els long-distance dependencies, and Sutton and Mc-
Callum (2004), who introduce skip-chain CRFs,
which maintain the underlying CRF sequence model
(which (Bunescu and Mooney, 2004) lack) while
adding skip edges between distant nodes. Unfortu-
nately, in the RMN model, the dependencies must
be defined in the model structure before doing any
inference, and so the authors use crude heuristic
part-of-speech patterns, and then add dependencies
between these text spans using clique templates.
This generates a extremely large number of over-
lapping candidate entities, which then necessitates
additional templates to enforce the constraint that
text subsequences cannot both be different entities,
something that is more naturally modeled by a CRF.
Another disadvantage of this approach is that it uses
loopy belief propagation and a voted perceptron for
approximate learning and inference ? ill-founded
and inherently unstable algorithms which are noted
by the authors to have caused convergence prob-
lems. In the skip-chain CRFs model, the decision
of which nodes to connect is also made heuristi-
cally, and because the authors focus on named entity
recognition, they chose to connect all pairs of identi-
cal capitalized words. They also utilize loopy belief
propagation for approximate learning and inference.
While the technique we propose is similar math-
ematically and in spirit to the above approaches, it
differs in some important ways. Our model is im-
plemented by adding additional constraints into the
model at inference time, and does not require the
preprocessing step necessary in the two previously
mentioned works. This allows for a broader class of
long-distance dependencies, because we do not need
to make any initial assumptions about which nodes
should be connected, and is helpful when you wish
to model relationships between nodes which are the
same class, but may not be similar in any other way.
For instance, in the CMU Seminar Announcements
dataset, we can normalize all entities labeled as a
start time and penalize the model if multiple, non-
consistent times are labeled. This type of constraint
cannot be modeled in an RMN or a skip-CRF, be-
cause it requires the knowledge that both entities are
given the same class label.
We also allow dependencies between multi-word
phrases, and not just single words. Additionally,
our model can be applied on top of a pre-existing
trained sequence model. As such, our method does
not require complex training procedures, and can
369
instead leverage all of the established methods for
training high accuracy sequence models. It can in-
deed be used in conjunction with any statistical hid-
den state sequence model: HMMs, CMMs, CRFs, or
even heuristic models. Third, our technique employs
Gibbs sampling for approximate inference, a simple
and probabilistically well-founded algorithm. As a
consequence of these differences, our approach is
easier to understand, implement, and adapt to new
applications.
9 Conclusions
We have shown that a constraint model can be effec-
tively combined with an existing sequence model in
a factored architecture to successfully impose var-
ious sorts of long distance constraints. Our model
generalizes naturally to other statistical models and
other tasks. In particular, it could in the future
be applied to statistical parsing. Statistical context
free grammars provide another example of statistical
models which are restricted to limiting local struc-
ture, and which could benefit from modeling non-
local structure.
Acknowledgements
This work was supported in part by the Advanced
Researchand Development Activity (ARDA)?s
Advanced Question Answeringfor Intelligence
(AQUAINT) Program. Additionally, we would like
to that our reviewers for their helpful comments.
References
S. Abney. 1997. Stochastic attribute-value grammars. Compu-
tational Linguistics, 23:597?618.
C. Andrieu, N. de Freitas, A. Doucet, and M. I. Jordan. 2003.
An introduction to MCMC for machine learning. Machine
Learning, 50:5?43.
A. Borthwick. 1999. A Maximum Entropy Approach to Named
Entity Recognition. Ph.D. thesis, New York University.
R. Bunescu and R. J. Mooney. 2004. Collective information
extraction with relational Markov networks. In Proceedings
of the 42nd ACL, pages 439?446.
H. L. Chieu and H. T. Ng. 2002. Named entity recognition:
a maximum entropy approach using global information. In
Proceedings of the 19th Coling, pages 190?196.
R. G. Cowell, A. Philip Dawid, S. L. Lauritzen, and D. J.
Spiegelhalter. 1999. Probabilistic Networks and Expert Sys-
tems. Springer-Verlag, New York.
J. R. Curran and S. Clark. 2003. Language independent NER
using a maximum entropy tagger. In Proceedings of the 7th
CoNLL, pages 164?167.
S. Della Pietra, V. Della Pietra, and J. Lafferty. 1997. Induc-
ing features of random fields. IEEE Transactions on Pattern
Analysis and Machine Intelligence, 19:380?393.
J. Finkel, S. Dingare, H. Nguyen, M. Nissim, and C. D. Man-
ning. 2004. Exploiting context for biomedical entity recog-
nition: from syntax to the web. In Joint Workshop on Natural
Language Processing in Biomedicine and Its Applications at
Coling 2004.
D. Freitag and A. McCallum. 1999. Information extraction
with HMMs and shrinkage. In Proceedings of the AAAI-99
Workshop on Machine Learning for Information Extraction.
D. Freitag. 1998. Machine learning for information extraction
in informal domains. Ph.D. thesis, Carnegie Mellon Univer-
sity.
S. Geman and D. Geman. 1984. Stochastic relaxation, Gibbs
distributions, and the Bayesian restoration of images. IEEE
Transitions on Pattern Analysis and Machine Intelligence,
6:721?741.
M. Kim, Y. S. Han, and K. Choi. 1995. Collocation map
for overcoming data sparseness. In Proceedings of the 7th
EACL, pages 53?59.
S. Kirkpatrick, C. D. Gelatt, and M. P. Vecchi. 1983. Optimiza-
tion by simulated annealing. Science, 220:671?680.
P. J. Van Laarhoven and E. H. L. Arts. 1987. Simulated Anneal-
ing: Theory and Applications. Reidel Publishers.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional
Random Fields: Probabilistic models for segmenting and
labeling sequence data. In Proceedings of the 18th ICML,
pages 282?289. Morgan Kaufmann, San Francisco, CA.
T. R. Leek. 1997. Information extraction using hidden Markov
models. Master?s thesis, U.C. San Diego.
R. Malouf. 2002. Markov models for language-independent
named entity recognition. In Proceedings of the 6th CoNLL,
pages 187?190.
A. Mikheev, M. Moens, and C. Grover. 1999. Named entity
recognition without gazetteers. In Proceedings of the 9th
EACL, pages 1?8.
L. R. Rabiner. 1989. A tutorial on Hidden Markov Models and
selected applications in speech recognition. Proceedings of
the IEEE, 77(2):257?286.
C. Sutton and A. McCallum. 2004. Collective segmentation
and labeling of distant entities in information extraction. In
ICML Workshop on Statistical Relational Learning and Its
connections to Other Fields.
B. Taskar, P. Abbeel, and D. Koller. 2002. Discriminative
probabilistic models for relational data. In Proceedings of
the 18th Conference on Uncertianty in Artificial Intelligence
(UAI-02), pages 485?494, Edmonton, Canada.
370
Proceedings of the 43rd Annual Meeting of the ACL, pages 371?378,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Unsupervised Learning of Field Segmentation Models
for Information Extraction
Trond Grenager
Computer Science Department
Stanford University
Stanford, CA 94305
grenager@cs.stanford.edu
Dan Klein
Computer Science Division
U.C. Berkeley
Berkeley, CA 94709
klein@cs.berkeley.edu
Christopher D. Manning
Computer Science Department
Stanford University
Stanford, CA 94305
manning@cs.stanford.edu
Abstract
The applicability of many current information ex-
traction techniques is severely limited by the need
for supervised training data. We demonstrate that
for certain field structured extraction tasks, such
as classified advertisements and bibliographic ci-
tations, small amounts of prior knowledge can be
used to learn effective models in a primarily unsu-
pervised fashion. Although hidden Markov models
(HMMs) provide a suitable generative model for
field structured text, general unsupervised HMM
learning fails to learn useful structure in either of
our domains. However, one can dramatically im-
prove the quality of the learned structure by ex-
ploiting simple prior knowledge of the desired so-
lutions. In both domains, we found that unsuper-
vised methods can attain accuracies with 400 un-
labeled examples comparable to those attained by
supervised methods on 50 labeled examples, and
that semi-supervised methods can make good use
of small amounts of labeled data.
1 Introduction
Information extraction is potentially one of the most
useful applications enabled by current natural lan-
guage processing technology. However, unlike gen-
eral tools like parsers or taggers, which generalize
reasonably beyond their training domains, extraction
systems must be entirely retrained for each appli-
cation. As an example, consider the task of turn-
ing a set of diverse classified advertisements into a
queryable database; each type of ad would require
tailored training data for a supervised system. Ap-
proaches which required little or no training data
would therefore provide substantial resource savings
and extend the practicality of extraction systems.
The term information extraction was introduced
in the MUC evaluations for the task of finding short
pieces of relevant information within a broader text
that is mainly irrelevant, and returning it in a struc-
tured form. For such ?nugget extraction? tasks, the
use of unsupervised learning methods is difficult and
unlikely to be fully successful, in part because the
nuggets of interest are determined only extrinsically
by the needs of the user or task. However, the term
information extraction was in time generalized to a
related task that we distinguish as field segmenta-
tion. In this task, a document is regarded as a se-
quence of pertinent fields, and the goal is to segment
the document into fields, and to label the fields. For
example, bibliographic citations, such as the one in
Figure 1(a), exhibit clear field structure, with fields
such as author, title, and date. Classified advertise-
ments, such as the one in Figure 1(b), also exhibit
field structure, if less rigidly: an ad consists of de-
scriptions of attributes of an item or offer, and a set
of ads for similar items share the same attributes. In
these cases, the fields present a salient, intrinsic form
of linguistic structure, and it is reasonable to hope
that field segmentation models could be learned in
an unsupervised fashion.
In this paper, we investigate unsupervised learn-
ing of field segmentation models in two domains:
bibliographic citations and classified advertisements
for apartment rentals. General, unconstrained induc-
tion of HMMs using the EM algorithm fails to detect
useful field structure in either domain. However, we
demonstrate that small amounts of prior knowledge
can be used to greatly improve the learned model. In
both domains, we found that unsupervised methods
can attain accuracies with 400 unlabeled examples
comparable to those attained by supervised methods
on 50 labeled examples, and that semi-supervised
methods can make good use of small amounts of la-
beled data.
371
(a) AUTH
Pearl
AUTH
,
AUTH
J.
DATE
(
DATE
1988
DATE
)
DATE
.
TTL
Probabilistic
TTL
Reasoning
TTL
in
TTL
Intelligent
TTL
Systems
TTL
:
TTL
Networks
TTL
of
TTL
Plausible
TTL
Inference
TTL
.
PUBL
Morgan
PUBL
Kaufmann
PUBL
.
(b) SIZE
Spacious
SIZE
1
SIZE
Bedroom
SIZE
apt
SIZE
.
FEAT
newly
FEAT
remodeled
FEAT
,
FEAT
gated
FEAT
,
FEAT
new
FEAT
appliance
FEAT
,
FEAT
new
FEAT
carpet
FEAT
,
NBRHD
near
NBRHD
public
NBRHD
transportion
NBRHD
,
NBRHD
close
NBRHD
to
NBRHD
580
NBRHD
freeway
NBRHD
,
RENT
$
RENT
500.00
RENT
Deposit
CONTACT
(510)655-0106
(c) RB
No
,
,
PRP
it
VBD
was
RB
n?t
NNP
Black
NNP
Monday
.
.
Figure 1: Examples of three domains for HMM learning: the bibliographic citation fields in (a) and classified advertisements for
apartment rentals shown in (b) exhibit field structure. Contrast these to part-of-speech tagging in (c) which does not.
2 Hidden Markov Models
Hidden Markov models (HMMs) are commonly
used to represent a wide range of linguistic phe-
nomena in text, including morphology, parts-of-
speech (POS), named entity mentions, and even
topic changes in discourse. An HMM consists of
a set of states S, a set of observations (in our case
words or tokens) W , a transition model specify-
ing P(st|st?1), the probability of transitioning from
state st?1 to state st, and an emission model specify-
ing P(w|s) the probability of emitting word w while
in state s. For a good tutorial on general HMM tech-
niques, see Rabiner (1989).
For all of the unsupervised learning experiments
we fit an HMM with the same number of hidden
states as gold labels to an unannotated training set
using EM.1 To compute hidden state expectations
efficiently, we use the Forward-Backward algorithm
in the standard way. Emission models are initialized
to almost-uniform probability distributions, where
a small amount of noise is added to break initial
symmetry. Transition model initialization varies by
experiment. We run the EM algorithm to conver-
gence. Finally, we use the Viterbi algorithm with
the learned parameters to label the test data.
All baselines and experiments use the same tok-
enization, normalization, and smoothing techniques,
which were not extensively investigated. Tokeniza-
tion was performed in the style of the Penn Tree-
bank, and tokens were normalized in various ways:
numbers, dates, phone numbers, URLs, and email
1EM is a greedy hill-climbing algorithm designed for this
purpose, but it is not the only option; one could also use coordi-
nate ascent methods or sampling methods.
addresses were collapsed to dedicated tokens, and
all remaining tokens were converted to lowercase.
Unless otherwise noted, the emission models use
simple add-? smoothing, where ? was 0.001 for su-
pervised techniques, and 0.2 for unsupervised tech-
niques.
3 Datasets and Evaluation
The bibliographic citations data is described in
McCallum et al (1999), and is distributed at
http://www.cs.umass.edu/~mccallum/. It consists of
500 hand-annotated citations, each taken from the
reference section of a different computer science re-
search paper. The citations are annotated with 13
fields, including author, title, date, journal, and so
on. The average citation has 35 tokens in 5.5 fields.
We split this data, using its natural order, into a 300-
document training set, a 100-document development
set, and a 100-document test set.
The classified advertisements data set is
novel, and consists of 8,767 classified ad-
vertisements for apartment rentals in the San
Francisco Bay Area downloaded in June 2004
from the Craigslist website. It is distributed at
http://www.stanford.edu/~grenager/. 302 of the
ads have been labeled with 12 fields, including
size, rent, neighborhood, features, and so on.
The average ad has 119 tokens in 8.7 fields. The
annotated data is divided into a 102-document
training set, a 100-document development set,
and a 100-document test set. The remaining 8465
documents form an unannotated training set.
In both cases, all system development and param-
eter tuning was performed on the development set,
372
size
rent
features
restrictions
neighborhood
utilities
available
contact
photos
roomates
other
address
author
title
editorjournal
booktitle
volume
pages
publisher
location
tech
institution
date
DT
JJ
NN
NNS
NNP
PRP
CC
MD
VBD
VB
TO
IN
(a) (b) (c)
Figure 2: Matrix representations of the target transition structure in two field structured domains: (a) classified advertisements
(b) bibliographic citations. Columns and rows are indexed by the same sequence of fields. Also shown is (c) a submatrix of the
transition structure for a part-of-speech tagging task. In all cases the column labels are the same as the row labels.
and the test set was only used once, for running fi-
nal experiments. Supervised learning experiments
train on documents selected randomly from the an-
notated training set and test on the complete test set.
Unsupervised learning experiments also test on the
complete test set, but create a training set by first
adding documents from the test set (without anno-
tation), then adding documents from the annotated
training set (without annotation), and finally adding
documents from the unannotated training set. Thus
if an unsupervised training set is larger than the test
set, it fully contains the test set.
To evaluate our models, we first learn a set of
model parameters, and then use the parameterized
model to label the sequence of tokens in the test data
with the model?s hidden states. We then compare
the similarity of the guessed sequence to the human-
annotated sequence of gold labels, and compute ac-
curacy on a per-token basis.2 In evaluation of su-
pervised methods, the model states and gold labels
are the same. For models learned in a fully unsuper-
vised fashion, we map each model state in a greedy
fashion to the gold label to which it most often cor-
responds in the gold data. There is a worry with
this kind of greedy mapping: it increasingly inflates
the results as the number of hidden states grows. To
keep the accuracies meaningful, all of our models
have exactly the same number of hidden states as
gold labels, and so the comparison is valid.
2This evaluation method is used by McCallum et al (1999)
but otherwise is not very standard. Compared to other evalu-
ation methods for information extraction systems, it leads to a
lower penalty for boundary errors, and allows long fields also
contribute more to accuracy than short ones.
4 Unsupervised Learning
Consider the general problem of learning an HMM
from an unlabeled data set. Even abstracting away
from concrete search methods and objective func-
tions, the diversity and simultaneity of linguistic
structure is already worrying; in Figure 1 compare
the field structure in (a) and (b) to the parts-of-
speech in (c). If strong sequential correlations exist
at multiple scales, any fixed search procedure will
detect and model at most one of these levels of struc-
ture, not necessarily the level desired at the moment.
Worse, as experience with part-of-speech and gram-
mar learning has shown, induction systems are quite
capable of producing some uninterpretable mix of
various levels and kinds of structure.
Therefore, if one is to preferentially learn one
kind of inherent structure over another, there must
be some way of constraining the process. We could
hope that field structure is the strongest effect in
classified ads, while parts-of-speech is the strongest
effect in newswire articles (or whatever we would
try to learn parts-of-speech from). However, it is
hard to imagine how one could bleach the local
grammatical correlations and long-distance topical
correlations from our classified ads; they are still
English text with part-of-speech patterns. One ap-
proach is to vary the objective function so that the
search prefers models which detect the structures
which we have in mind. This is the primary way
supervised methods work, with the loss function rel-
ativized to training label patterns. However, for un-
supervised learning, the primary candidate for an
objective function is the data likelihood, and we
don?t have another suggestion here. Another ap-
proach is to inject some prior knowledge into the
373
search procedure by carefully choosing the starting
point; indeed smart initialization has been critical
to success in many previous unsupervised learning
experiments. The central idea of this paper is that
we can instead restrict the entire search domain by
constraining the model class to reflect the desired
structure in the data, thereby directing the search to-
ward models of interest. We do this in several ways,
which are described in the following sections.
4.1 Baselines
To situate our results, we provide three different
baselines (see Table 1). First is the most-frequent-
field accuracy, achieved by labeling all tokens with
the same single label which is then mapped to the
most frequent field. This gives an accuracy of 46.4%
on the advertisements data and 27.9% on the cita-
tions data. The second baseline method is to pre-
segment the unlabeled data using a crude heuristic
based on punctuation, and then to cluster the result-
ing segments using a simple Na??ve Bayes mixture
model with the Expectation-Maximization (EM) al-
gorithm. This approach achieves an accuracy of
62.4% on the advertisements data, and 46.5% on the
citations data.
As a final baseline, we trained a supervised first-
order HMM from the annotated training data using
maximum likelihood estimation. With 100 training
examples, supervised models achieve an accuracy of
74.4% on the advertisements data, and 72.5% on the
citations data. With 300 examples, supervised meth-
ods achieve accuracies of 80.4 on the citations data.
The learning curves of the supervised training ex-
periments for different amounts of training data are
shown in Figure 4. Note that other authors have
achieved much higher accuracy on the the citation
dataset using HMMs trained with supervision: Mc-
Callum et al (1999) report accuracies as high as
92.9% by using more complex models and millions
of words of BibTeX training data.
4.2 Unconstrained HMM Learning
From the supervised baseline above we know that
there is some first-order HMM over |S| states which
captures the field structure we?re interested in, and
we would like to find such a model without super-
vision. As a first attempt, we try fitting an uncon-
strained HMM, where the transition function is ini-
1
2
3
4
5
6
7
8
9
10
11
12
(a) Classified Advertisements
1
2
3
4
5
6
7
8
9
10
11
12
(b) Citations
Figure 3: Matrix representations of typical transition models
learned by initializing the transition model uniformly.
tialized randomly, to the unannotated training data.
Not surprisingly, the unconstrained approach leads
to predictions which poorly align with the desired
field segmentation: with 400 unannotated training
documents, the accuracy is just 48.8% for the ad-
vertisements and 49.7% for the citations: better than
the single state baseline but far from the supervised
baseline. To illustrate what is (and isn?t) being
learned, compare typical transition models learned
by this method, shown in Figure 3, to the maximum-
likelihood transition models for the target annota-
tions, shown in Figure 2. Clearly, they aren?t any-
thing like the target models: the learned classified
advertisements matrix has some but not all of the
desired diagonal structure, and the learned citations
matrix has almost no mass on the diagonal, and ap-
pears to be modeling smaller scale structure.
4.3 Diagonal Transition Models
To adjust our procedure to learn larger-scale pat-
terns, we can constrain the parametric form of the
transition model to be
P(st|st?1) =
?
?
?
? + (1??)|S| if st = st?1
(1??)
|S| otherwise
where |S| is the number of states, and ? is a global
free parameter specifying the self-loop probability:
374
(a) Classified advertisements
(b) Bibliographic citations
Figure 4: Learning curves for supervised learning and unsuper-
vised learning with a diagonal transition matrix on (a) classified
advertisements, and (b) bibliographic citations. Results are av-
eraged over 50 runs.
the probability of a state transitioning to itself. (Note
that the expected mean field length for transition
functions of this form is 11?? .) This constraint pro-
vides a notable performance improvement: with 400
unannotated training documents the accuracy jumps
from 48.8% to 70.0% for advertisements and from
49.7% to 66.3% for citations. The complete learning
curves for models of this form are shown in Figure 4.
We have tested training on more unannotated data;
the slope of the learning curve is leveling out, but
by training on 8000 unannotated ads, accuracy im-
proves significantly to 72.4%. On the citations task,
an accuracy of approximately 66% can be achieved
either using supervised training on 50 annotated ci-
tations, or unsupervised training using 400 unanno-
tated citations. 3
Although ? can easily be reestimated with EM
(even on a per-field basis), doing so does not yield
3We also tested training on 5000 additional unannotated ci-
tations collected from papers found on the Internet. Unfortu-
nately the addition of this data didn?t help accuracy. This prob-
ably results from the fact that the datasets were collected from
different sources, at different times.
Figure 5: Unsupervised accuracy as a function of the expected
mean field length 11?? for the classified advertisements dataset.
Each model was trained with 500 documents and tested on the
development set. Results are averaged over 50 runs.
better models.4 On the other hand, model accuracy
is not very sensitive to the exact choice of ?, as
shown in Figure 5 for the classified advertisements
task (the result for the citations task has a similar
shape). For the remaining experiments on the adver-
tisements data, we use ? = 0.9, and for those on the
citations data, we use ? = 0.5.
4.4 Hierarchical Mixture Emission Models
Consider the highest-probability state emissions
learned by the diagonal model, shown in Figure 6(a).
In addition to its characteristic content words, each
state also emits punctuation and English function
words devoid of content. In fact, state 3 seems to
have specialized entirely in generating such tokens.
This can become a problem when labeling decisions
are made on the basis of the function words rather
than the content words. It seems possible, then,
that removing function words from the field-specific
emission models could yield an improvement in la-
beling accuracy.
One way to incorporate this knowledge into the
model is to delete stopwords, which, while perhaps
not elegant, has proven quite effective in the past.
A better founded way of making certain words un-
available to the model is to emit those words from
all states with equal probability. This can be accom-
plished with the following simple hierarchical mix-
ture emission model
Ph(w|s) = ?Pc(w) + (1 ? ?)P(w|s)
where Pc is the common word distribution, and ? is
4While it may be surprising that disallowing reestimation of
the transition function is helpful here, the same has been ob-
served in acoustic modeling (Rabiner and Juang, 1993).
375
State 10 Most Common Words
1 . $ no ! month deposit , pets rent avail-
able
2 , . room and with in large living kitchen
-
3 . a the is and for this to , in
4 [NUM1] [NUM0] , bedroom bath / - .
car garage
5 , . and a in - quiet with unit building
6 - . [TIME] [PHONE] [DAY] call
[NUM8] at
(a)
State 10 Most Common Words
1 [NUM2] bedroom [NUM1] bath bed-
rooms large sq car ft garage
2 $ no month deposit pets lease rent avail-
able year security
3 kitchen room new , with living large
floors hardwood fireplace
4 [PHONE] call please at or for [TIME] to
[DAY] contact
5 san street at ave st # [NUM:DDD] fran-
cisco ca [NUM:DDDD]
6 of the yard with unit private back a
building floor
Comm. *CR* . , and - the in a / is with : of for
to
(b)
Figure 6: Selected state emissions from a typical model learned
from unsupervised data using the constrained transition func-
tion: (a) with a flat emission model, and (b) with a hierarchical
emission model.
a new global free parameter. In such a model, before
a state emits a token it flips a coin, and with probabil-
ity ? it allows the common word distribution to gen-
erate the token, and with probability (1??) it gener-
ates the token from its state-specific emission model
(see Vaithyanathan and Dom (2000) and Toutanova
et al (2001) for more on such models). We tuned
? on the development set and found that a range of
values work equally well. We used a value of 0.5 in
the following experiments.
We ran two experiments on the advertisements
data, both using the fixed transition model described
in Section 4.3 and the hierarchical emission model.
First, we initialized the emission model of Pc to a
general-purpose list of stopwords, and did not rees-
timate it. This improved the average accuracy from
70.0% to 70.9%. Second, we learned the emission
model of Pc using EM reestimation. Although this
method did not yield a significant improvement in
accuracy, it learns sensible common words: Fig-
ure 6(b) shows a typical emission model learned
with this technique. Unfortunately, this technique
does not yield improvements on the citations data.
4.5 Boundary Models
Another source of error concerns field boundaries.
In many cases, fields are more or less correct, but the
boundaries are off by a few tokens, even when punc-
tuation or syntax make it clear to a human reader
where the exact boundary should be. One way to ad-
dress this is to model the fact that in this data fields
often end with one of a small set of boundary tokens,
such as punctuation and new lines, which are shared
across states.
To accomplish this, we enriched the Markov pro-
cess so that each field s is now modeled by two
states, a non-final s? ? S? and a final s+ ? S+.
The transition model for final states is the same as
before, but the transition model for non-final states
has two new global free parameters: ?, the probabil-
ity of staying within the field, and ?, the probability
of transitioning to the final state given that we are
staying in the field. The transition function for non-
final states is then
P(s?|s?) =
?
?
?
?
?
?
?
?
?
?
?
(1 ? ?)(? + (1??)|S?| ) if s? = s?
?(?+ (1??)|S?| ) if s? = s+
(1??)
|S?| if s
? ? S?\s?
0 otherwise.
Note that it can bypass the final state, and transi-
tion directly to other non-final states with probabil-
ity (1 ? ?), which models the fact that not all field
occurrences end with a boundary token. The transi-
tion function for non-final states is then
P(s?|s+) =
?
?
?
?
?
?
?
? + (1??)|S?| if s? = s?
(1??)
|S?| if s
? ? S?\s?
0 otherwise.
Note that this has the form of the standard diagonal
function. The reason for the self-loop from the fi-
nal state back to the non-final state is to allow for
field internal punctuation. We tuned the free param-
eters on the development set, and found that ? = 0.5
and ? = 0.995 work well for the advertisements do-
main, and ? = 0.3 and ? = 0.9 work well for the
citations domain. In all cases it works well to set
? = 1 ? ?. Emissions from non-final states are as
376
Ads Citations
Baseline 46.4 27.9
Segment and cluster 62.4 46.5
Supervised 74.4 72.5
Unsup. (learned trans) 48.8 49.7
Unsup. (diagonal trans) 70.0 66.3
+ Hierarchical (learned) 70.1 39.1
+ Hierarchical (given) 70.9 62.1
+ Boundary (learned) 70.4 64.3
+ Boundary (given) 71.9 68.2
+ Hier. + Bnd. (learned) 71.0 ?
+ Hier. + Bnd. (given) 72.7 ?
Table 1: Summary of results. For each experiment, we report
percentage accuracy on the test set. Supervised experiments
use 100 training documents, and unsupervised experiments use
400 training documents. Because unsupervised techniques are
stochastic, those results are averaged over 50 runs, and differ-
ences greater than 1.0% are significant at p=0.05% or better ac-
cording to the t-test. The last 6 rows are not cumulative.
before (hierarchical or not depending on the experi-
ment), while all final states share a boundary emis-
sion model. Note that the boundary emissions are
not smoothed like the field emissions.
We tested both supplying the boundary token dis-
tributions and learning them with reestimation dur-
ing EM. In experiments on the advertisements data
we found that learning the boundary emission model
gives an insignificant raise from 70.0% to 70.4%,
while specifying the list of allowed boundary tokens
gives a significant increase to 71.9%. When com-
bined with the given hierarchical emission model
from the previous section, accuracy rises to 72.7%,
our best unsupervised result on the advertisements
data with 400 training examples. In experiments on
the citations data we found that learning boundary
emission model hurts accuracy, but that given the set
of boundary tokens it boosts accuracy significantly:
increasing it from 66.3% to 68.2%.
5 Semi-supervised Learning
So far, we have largely focused on incorporating
prior knowledge in rather general and implicit ways.
As a final experiment we tested the effect of adding
a small amount of supervision: augmenting the large
amount of unannotated data we use for unsuper-
vised learning with a small amount of annotated
data. There are many possible techniques for semi-
supervised learning; we tested a particularly simple
one. We treat the annotated labels as observed vari-
ables, and when computing sufficient statistics in the
M-step of EM we add the observed counts from the
Figure 7: Learning curves for semi-supervised learning on the
citations task. A separate curve is drawn for each number of
annotated documents. All results are averaged over 50 runs.
annotated documents to the expected counts com-
puted in the E-step. We estimate the transition
function using maximum likelihood from the an-
notated documents only, and do not reestimate it.
Semi-supervised results for the citations domain are
shown in Figure 7. Adding 5 annotated citations
yields no improvement in performance, but adding
20 annotated citations to 300 unannotated citations
boosts performance greatly from 65.2% to 71.3%.
We also tested the utility of this approach in the clas-
sified advertisement domain, and found that it did
not improve accuracy. We believe that this is be-
cause the transition information provided by the su-
pervised data is very useful for the citations data,
which has regular transition structure, but is not as
useful for the advertisements data, which does not.
6 Previous Work
A good amount of prior research can be cast as
supervised learning of field segmentation models,
using various model families and applied to var-
ious domains. McCallum et al (1999) were the
first to compare a number of supervised methods
for learning HMMs for parsing bibliographic cita-
tions. The authors explicitly claim that the domain
would be suitable for unsupervised learning, but
they do not present experimental results. McCallum
et al (2000) applied supervised learning of Maxi-
mum Entropy Markov Models (MEMMs) to the do-
main of parsing Frequently Asked Question (FAQ)
lists into their component field structure. More re-
cently, Peng and McCallum (2004) applied super-
vised learning of Conditional Random Field (CRF)
sequence models to the problem of parsing the head-
377
ers of research papers.
There has also been some previous work on un-
supervised learning of field segmentation models in
particular domains. Pasula et al (2002) performs
limited unsupervised segmentation of bibliographic
citations as a small part of a larger probabilistic
model of identity uncertainty. However, their sys-
tem does not explicitly learn a field segmentation
model for the citations, and encodes a large amount
of hand-supplied information about name forms, ab-
breviation schemes, and so on. More recently, Barzi-
lay and Lee (2004) defined content models, which
can be viewed as field segmentation models occur-
ring at the level of discourse. They perform un-
supervised learning of these models from sets of
news articles which describe similar events. The
fields in that case are the topics discussed in those
articles. They consider a very different set of ap-
plications from the present work, and show that
the learned topic models improve performance on
two discourse-related tasks: information ordering
and extractive document summarization. Most im-
portantly, their learning method differs significantly
from ours; they use a complex and special purpose
algorithm, which is difficult to adapt, while we see
our contribution to be a demonstration of the inter-
play between model family and learned structure.
Because the structure of the HMMs they learn is
similar to ours it seems that their system could ben-
efit from the techniques of this paper. Finally, Blei
and Moreno (2001) use an HMM augmented by an
aspect model to automatically segment documents,
similar in goal to the system of Hearst (1997), but
using techniques more similar to the present work.
7 Conclusions
In this work, we have examined the task of learn-
ing field segmentation models using unsupervised
learning. In two different domains, classified ad-
vertisements and bibliographic citations, we showed
that by constraining the model class we were able
to restrict the search space of EM to models of in-
terest. We used unsupervised learning methods with
400 documents to yield field segmentation models
of a similar quality to those learned using supervised
learning with 50 documents. We demonstrated that
further refinements of the model structure, including
hierarchical mixture emission models and boundary
models, produce additional increases in accuracy.
Finally, we also showed that semi-supervised meth-
ods with a modest amount of labeled data can some-
times be effectively used to get similar good results,
depending on the nature of the problem.
While there are enough resources for the citation
task that much better numbers than ours can be and
have been obtained (with more knowledge and re-
source intensive methods), in domains like classi-
fied ads for lost pets or used bicycles unsupervised
learning may be the only practical option. In these
cases, we find it heartening that the present systems
do as well as they do, even without field-specific
prior knowledge.
8 Acknowledgements
We would like to thank the reviewers for their con-
sideration and insightful comments.
References
R. Barzilay and L. Lee. 2004. Catching the drift: Probabilistic
content models, with applications to generation and summa-
rization. In Proceedings of HLT-NAACL 2004, pages 113?
120.
D. Blei and P. Moreno. 2001. Topic segmentation with an aspect
hidden Markov model. In Proceedings of the 24th SIGIR,
pages 343?348.
M. A. Hearst. 1997. TextTiling: Segmenting text into multi-
paragraph subtopic passages. Computational Linguistics,
23(1):33?64.
A. McCallum, K. Nigam, J. Rennie, and K. Seymore. 1999.
A machine learning approach to building domain-specific
search engines. In IJCAI-1999.
A. McCallum, D. Freitag, and F. Pereira. 2000. Maximum
entropy Markov models for information extraction and seg-
mentation. In Proceedings of the 17th ICML, pages 591?598.
Morgan Kaufmann, San Francisco, CA.
H. Pasula, B. Marthi, B. Milch, S. Russell, and I. Shpitser. 2002.
Identity uncertainty and citation matching. In Proceedings of
NIPS 2002.
F. Peng and A. McCallum. 2004. Accurate information extrac-
tion from research papers using Conditional Random Fields.
In Proceedings of HLT-NAACL 2004.
L. R. Rabiner and B.-H. Juang. 1993. Fundamentals of Speech
Recognition. Prentice Hall.
L. R. Rabiner. 1989. A tutorial on Hidden Markov Models and
selected applications in speech recognition. Proceedings of
the IEEE, 77(2):257?286.
K. Toutanova, F. Chen, K. Popat, and T. Hofmann. 2001. Text
classification in a hierarchical mixture model for small train-
ing sets. In CIKM ?01: Proceedings of the tenth interna-
tional conference on Information and knowledge manage-
ment, pages 105?113. ACM Press.
S. Vaithyanathan and B. Dom. 2000. Model-based hierarchical
clustering. In UAI-2000.
378
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 272?279,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
The Infinite Tree
Jenny Rose Finkel, Trond Grenager, and Christopher D. Manning
Computer Science Department, Stanford University
Stanford, CA 94305
{jrfinkel, grenager, manning}@cs.stanford.edu
Abstract
Historically, unsupervised learning tech-
niques have lacked a principled technique
for selecting the number of unseen compo-
nents. Research into non-parametric priors,
such as the Dirichlet process, has enabled in-
stead the use of infinite models, in which the
number of hidden categories is not fixed, but
can grow with the amount of training data.
Here we develop the infinite tree, a new infi-
nite model capable of representing recursive
branching structure over an arbitrarily large
set of hidden categories. Specifically, we
develop three infinite tree models, each of
which enforces different independence as-
sumptions, and for each model we define a
simple direct assignment sampling inference
procedure. We demonstrate the utility of
our models by doing unsupervised learning
of part-of-speech tags from treebank depen-
dency skeleton structure, achieving an accu-
racy of 75.34%, and by doing unsupervised
splitting of part-of-speech tags, which in-
creases the accuracy of a generative depen-
dency parser from 85.11% to 87.35%.
1 Introduction
Model-based unsupervised learning techniques have
historically lacked good methods for choosing the
number of unseen components. For example, k-
means or EM clustering require advance specifica-
tion of the number of mixture components. But
the introduction of nonparametric priors such as the
Dirichlet process (Ferguson, 1973) enabled develop-
ment of infinite mixture models, in which the num-
ber of hidden components is not fixed, but emerges
naturally from the training data (Antoniak, 1974).
Teh et al (2006) proposed the hierarchical Dirich-
let process (HDP) as a way of applying the Dirichlet
process (DP) to more complex model forms, so as to
allow multiple, group-specific, infinite mixture mod-
els to share their mixture components. The closely
related infinite hidden Markov model is an HMM
in which the transitions are modeled using an HDP,
enabling unsupervised learning of sequence models
when the number of hidden states is unknown (Beal
et al, 2002; Teh et al, 2006).
We extend this work by introducing the infinite
tree model, which represents recursive branching
structure over a potentially infinite set of hidden
states. Such models are appropriate for the syntactic
dependency structure of natural language. The hid-
den states represent word categories (?tags?), the ob-
servations they generate represent the words them-
selves, and the tree structure represents syntactic de-
pendencies between pairs of tags.
To validate the model, we test unsupervised learn-
ing of tags conditioned on a given dependency tree
structure. This is useful, because coarse-grained
syntactic categories, such as those used in the Penn
Treebank (PTB), make insufficient distinctions to be
the basis of accurate syntactic parsing (Charniak,
1996). Hence, state-of-the-art parsers either supple-
ment the part-of-speech (POS) tags with the lexical
forms themselves (Collins, 2003; Charniak, 2000),
manually split the tagset into a finer-grained one
(Klein and Manning, 2003a), or learn finer grained
tag distinctions using a heuristic learning procedure
(Petrov et al, 2006). We demonstrate that the tags
learned with our model are correlated with the PTB
POS tags, and furthermore that they improve the ac-
curacy of an automatic parser when used in training.
2 Finite Trees
We begin by presenting three finite tree models, each
with different independence assumptions.
272
C? pik
H ?k
z1
z2 z3
x1 x2 x3
Figure 1: A graphical representation of the finite
Bayesian tree model with independent children. The
plate (rectangle) indicates that there is one copy of
the model parameter variables for each state k ? C .
2.1 Independent Children
In the first model, children are generated indepen-
dently of each other, conditioned on the parent. Let
t denote both the tree and its root node, c(t) the list
of children of t, ci(t) the ith child of t, and p(t) the
parent of t. Each tree t has a hidden state zt (in a syn-
tax tree, the tag) and an observation xt (the word).1
The probability of a tree is given by the recursive
definition:2
Ptr(t) = P(xt|zt)
?
t??c(t)
P(zt? |zt)Ptr(t?)
To make the model Bayesian, we must define ran-
dom variables to represent each of the model?s pa-
rameters, and specify prior distributions for them.
Let each of the hidden state variables have C possi-
ble values which we will index with k. Each state k
has a distinct distribution over observations, param-
eterized by ?k, which is distributed according to a
prior distribution over the parameters H:
?k|H ? H
We generate each observation xt from some distri-
bution F (?zt) parameterized by ?zt specific to its
corresponding hidden state zt. If F (?k)s are multi-
nomials, then a natural choice for H would be a
Dirichlet distribution.3
The hidden state zt? of each child is distributed
according to a multinomial distribution pizt specific
to the hidden state zt of the parent:
xt|zt ? F (?zt)
zt? |zt ? Multinomial(pizt)
1To model length, every child list ends with a distinguished
stop node, which has as its state a distinguished stop state.
2We also define a distinguished node t0, which generates the
root of the entire tree, and P (xt0 |zt0) = 1.
3A Dirichlet distribution is a distribution over the possible
parameters of a multinomial distributions, and is distinct from
the Dirichlet process.
Each multinomial over children pik is distributed ac-
cording to a Dirichlet distribution with parameter ?:
pik|? ? Dirichlet(?, . . . , ?)
This model is presented graphically in Figure 1.
2.2 Simultaneous Children
The independent child model adopts strong indepen-
dence assumptions, and we may instead want mod-
els in which the children are conditioned on more
than just the parent?s state. Our second model thus
generates the states of all of the children c(t) simul-
taneously:
Ptr(t) = P(xt|zt)P((zt?)t??c(t)|zt)
?
t??c(t)
Ptr(t?)
where (zt?)t??c(t) indicates the list of tags of the chil-
dren of t. To parameterize this model, we replace the
multinomial distribution pik over states with a multi-
nomial distribution ?k over lists of states.4
2.3 Markov Children
The very large domain size of the child lists in the
simultaneous child model may cause problems of
sparse estimation. Another alternative is to use a
first-order Markov process to generate children, in
which each child?s state is conditioned on the previ-
ous child?s state:
Ptr(t) = P(xt|zt)
?|c(t)|
i=1
P(zci(t)|zci?1(t), zt)Ptr(t?)
For this model, we augment all child lists with a dis-
tinguished start node, c0(t), which has as its state
a distinguished start state, allowing us to capture
the unique behavior of the first (observed) child. To
parameterize this model, note that we will need to
define C(C + 1) multinomials, one for each parent
state and preceding child state (or a distinguished
start state).
3 To Infinity, and Beyond . . .
This section reviews needed background material
for our approach to making our tree models infinite.
3.1 The Dirichlet Process
Suppose we model a document as a bag of words
produced by a mixture model, where the mixture
components might be topics such as business, pol-
itics, sports, etc. Using this model we can generate a
4This requires stipulating a maximum list length.
273
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
P(xi = "game")
P(xi = "profit")
Figure 2: Plot of the density function of a Dirich-
let distribution H (the surface) as well as a draw
G (the vertical lines, or sticks) from a Dirichlet
process DP(?0,H) which has H as a base mea-
sure. Both distributions are defined over a sim-
plex in which each point corresponds to a particular
multinomial distribution over three possible words:
?profit?, ?game?, and ?election?. The placement of
the sticks is drawn from the distribution H , and is
independent of their lengths, which is drawn from a
stick-breaking process with parameter ?0.
document by first generating a distribution over top-
ics pi, and then for each position i in the document,
generating a topic zi from pi, and then a word xi
from the topic specific distribution ?zi . The word
distributions ?k for each topic k are drawn from a
base distribution H . In Section 2, we sample C
multinomials ?k from H . In the infinite mixture
model we sample an infinite number of multinomi-
als from H , using the Dirichlet process.
Formally, given a base distribution H and a con-
centration parameter ?0 (loosely speaking, this con-
trols the relative sizes of the topics), a Dirichlet pro-
cess DP(?0,H) is the distribution of a discrete ran-
dom probability measure G over the same (possibly
continuous) space that H is defined over; thus it is a
measure over measures. In Figure 2, the sticks (ver-
tical lines) show a draw G from a Dirichlet process
where the base measure H is a Dirichlet distribution
over 3 words. A draw comprises of an infinite num-
ber of sticks, and each corresponding topic.
We factor G into two coindexed distributions: pi,
a distribution over the integers, where the integer
represents the index of a particular topic (i.e., the
height of the sticks in the figure represent the proba-
bility of the topic indexed by that stick) and ?, rep-
resenting the word distribution of each of the top-
N
?
?0 H
pi ?k
zi
xi
pi|?0 ? GEM(?0)
?k|H ? H
zi|pi ? pi
xi|zi,? ? F (?zi) N
?
? ?0
? H
pij ?k
zji
xji
(a) (b)
Figure 3: A graphical representation of a simple
Dirichlet process mixture model (left) and a hierar-
chical Dirichlet process model (right). Note that we
show the stick-breaking representations of the mod-
els, in which we have factored G ? DP(?0,H) into
two sets of variables: pi and ?.
ics (i.e., the location of the sticks in the figure). To
generate pi we first generate an infinite sequence of
variables pi? = (pi?k)?k=1, each of which is distributed
according to the Beta distribution:
pi?k|?0 ? Beta(1, ?0)
Then pi = (pik)?k=1 is defined as:
pik = pi?k
?k?1
i=1
(1? pi?i)
Following Pitman (2002) we refer to this process as
pi ? GEM(?0). It should be noted that
??
k=1 pik =
1,5 and P (i) = pii. Then, according to the DP,
P (?i) = pii. The complete model, is shown graphi-
cally in Figure 3(a).
To build intuition, we walk through the process of
generating from the infinite mixture model for the
document example, where xi is the word at posi-
tion i, and zi is its topic. F is a multinomial dis-
tribution parameterized by ?, and H is a Dirichlet
distribution. Instead of generating all of the infinite
mixture components (pik)?k=1 at once, we can build
them up incrementally. If there are K known top-
ics, we represent only the known elements (pik)Kk=1
and represent the remaining probability mass piu =
5This is called the stick-breaking construction: we start with
a stick of unit length, representing the entire probability mass,
and successively break bits off the end of the stick, where the
proportional amount broken off is represented by pi?k and the
absolute amount is represented by pik.
274
?1 ?2 ?3 ?4 ?5 ?6 ?7 . . .
? :
pij :
. . .
Figure 4: A graphical representation of pij , a broken
stick, which is distributed according to a DP with a
broken stick ? as a base measure. Each ?k corre-
sponds to a ?k.
1 ? (?Kk=1 pik). Initially we have piu = 1 and
? = ().
For the ith position in the document, we first draw
a topic zi ? pi. If zi 6= u, then we find the coin-
dexed topic ?zi . If zi = u, the unseen topic, we
make a draw b ? Beta(1, ?0) and set piK+1 = bpiu
and pinewu = (1 ? b)piu. Then we draw a parame-
ter ?K+1 ? H for the new topic, resulting in pi =
(pi1, . . . , piK+1, pinewu ) and ? = (?1, . . . , ?K+1). A
word is then drawn from this topic and emitted by
the document.
3.2 The Hierarchical Dirichlet Process
Let?s generalize our previous example to a corpus
of documents. As before, we have a set of shared
topics, but now each document has its own charac-
teristic distribution over these topics. We represent
topic distributions both locally (for each document)
and globally (across all documents) by use of a hier-
archical Dirichlet process (HDP), which has a local
DP for each document, in which the base measure is
itself a draw from another, global, DP.
The complete HDP model is represented graphi-
cally in Figure 3(b). Like the DP, it has global bro-
ken stick ? = (?k)?k=1 and topic specific word dis-
tribution parameters ? = (?k)?k=1, which are coin-
dexed. It differs from the DP in that it also has lo-
cal broken sticks pij for each group j (in our case
documents). While the global stick ? ? GEM(?)
is generated as before, the local sticks pij are dis-
tributed according to a DP with base measure ?:
pij ? DP(?0,?).
We illustrate this generation process in Figure 4.
The upper unit line represents ?, where the size of
segment k represents the value of element ?k, and
the lower unit line represents pij ? DP(?0,?) for a
particular group j. Each element of the lower stick
was sampled from a particular element of the upper
stick, and elements of the upper stick may be sam-
pled multiple times or not at all; on average, larger
elements will be sampled more often. Each element
?k, as well as all elements of pij that were sampled
from it, corresponds to a particular ?k. Critically,
several distinct pij can be sampled from the same
?k and hence share ?k; this is how components are
shared among groups.
For concreteness, we show how to generate a cor-
pus of documents from the HDP, generating one
document at a time, and incrementally construct-
ing our infinite objects. Initially we have ?u = 1,
? = (), and piju = 1 for all j. We start with the
first position of the first document and draw a local
topic y11 ? pi1, which will return u with probabil-
ity 1. Because y11 = u we must make a draw from
the base measure, ?, which, because this is the first
document, will also return u with probability 1. We
must now break ?u into ?1 and ?newu , and break pi1u
into pi11 and pinew1u in the same manner presented for
the DP. Since pi11 now corresponds to global topic
1, we sample the word x11 ? Multinomial(?1). To
sample each subsequent word i, we first sample the
local topic y1i ? pi1. If y1i 6= u, and pi1y1i corre-
sponds to ?k in the global stick, then we sample the
word x1i ? Multinomial(?k). Once the first docu-
ment has been sampled, subsequent documents are
sampled in a similar manner; initially piju = 1 for
document j, while ? continues to grow as more doc-
uments are sampled.
4 Infinite Trees
We now use the techniques from Section 3 to create
infinite versions of each tree model from Section 2.
4.1 Independent Children
The changes required to make the Bayesian inde-
pendent children model infinite don?t affect its ba-
sic structure, as can be witnessed by comparing the
graphical depiction of the infinite model in Figure 5
with that of the finite model in Figure 1. The in-
stance variables zt and xt are parameterized as be-
fore. The primary change is that the number of
copies of the state plate is infinite, as are the number
of variables pik and ?k.
Note also that each distribution over possible
child states pik must also be infinite, since the num-
ber of possible child states is potentially infinite. We
achieve this by representing each of the pik variables
as a broken stick, and adopt the same approach of
275
?|? ? GEM(?)
pik|?0,? ? DP(?0,?)
?k|H ? H
?
? ?
?0 pik
H ?k
z1
z2 z3
x1 x2 x3
Figure 5: A graphical representation of the infinite
independent child model.
sampling each pik from a DP with base measure ?.
For the dependency tree application, ?k is a vector
representing the parameters of a multinomial over
words, and H is a Dirichlet distribution.
The infinite hidden Markov model (iHMM) or
HDP-HMM (Beal et al, 2002; Teh et al, 2006) is
a model of sequence data with transitions modeled
by an HDP.6 The iHMM can be viewed as a special
case of this model, where each state (except the stop
state) produces exactly one child.
4.2 Simultaneous Children
The key problem in the definition of the simulta-
neous children model is that of defining a distribu-
tion over the lists of children produced by each state,
since each child in the list has as its domain the posi-
tive integers, representing the infinite set of possible
states. Our solution is to construct a distribution Lk
over lists of states from the distribution over individ-
ual states pik. The obvious approach is to sample the
states at each position i.i.d.:
P((zt?)t??c(t)|pi) =
?
t??c(t)
P(zt? |pi) =
?
t??c(t)
pizt?
However, we want our model to be able to rep-
resent the fact that some child lists, ct, are more
or less probable than the product of the individual
child probabilities would indicate. To address this,
we can sample a state-conditional distribution over
child lists ?k from a DP with Lk as a base measure.
6The original iHMM paper (Beal et al, 2002) predates, and
was the motivation for, the work presented in Teh et al (2006),
and is the origin of the term hierarchical Dirichlet process.
However, they used the term to mean something slightly differ-
ent than the HDP presented in Teh et al (2006), and presented a
sampling scheme for inference that was a heuristic approxima-
tion of a Gibbs sampler.
Thus, we augment the basic model given in the pre-
vious section with the variables ? , Lk, and ?k:
Lk|pik ? Deterministic, as described above
?k|?, Lk ? DP(?, Lk)
ct|?k ? ?k
An important consequence of defining Lk locally
(instead of globally, using ? instead of the piks) is
that the model captures not only what sequences of
children a state prefers, but also the individual chil-
dren that state prefers; if a state gives high proba-
bility to some particular sequence of children, then
it is likely to also give high probability to other se-
quences containing those same states, or a subset
thereof.
4.3 Markov Children
In the Markov children model, more copies of the
variable pi are needed, because each child state must
be conditioned both on the parent state and on the
state of the preceding child. We use a new set of
variables piki, where pi is determined by the par-
ent state k and the state of the preceding sibling i.
Each of the piki is distributed as pik was in the basic
model: piki ? DP(?0,?).
5 Inference
Our goal in inference is to draw a sample from the
posterior over assignments of states to observations.
We present an inference procedure for the infinite
tree that is based on Gibbs sampling in the direct
assignment representation, so named because we di-
rectly assign global state indices to observations.7
Before we present the procedure, we define a few
count variables. Recall from Figure 4 that each state
k has a local stick pik, each element of which cor-
responds to an element of ?. In our sampling pro-
cedure, we only keep elements of pik and ? which
correspond to states observed in the data. We define
the variable mjk to be the number of elements of the
finite observed portion of pik which correspond to ?j
and njk to be the number of observations with state
k whose parent?s state is j.
We also need a few model-specific counts. For the
simultaneous children model we need njz, which is
7We adapt one of the sampling schemes mentioned by Teh
et al (2006) for use in the iHMM. This paper suggests two
sampling schemes for inference, but does not explicitly present
them. Upon discussion with one of the authors (Y. W. Teh,
2006, p.c.), it became clear that inference using the augmented
representation is much more complicated than initially thought.
276
the number of times the state sequence z occurred
as the children of state j. For the Markov chil-
dren model we need the count variable n?jik which
is the number of observations for a node with state
k whose parent?s state is j and whose previous sib-
ling?s state is i. In all cases we represent marginal
counts using dot-notation, e.g., n?k is the total num-
ber of nodes with state k, regardless of parent.
Our procedure alternates between three distinct
sampling stages: (1) sampling the state assignments
z, (2) sampling the counts mjk, and (3) sampling
the global stick ?. The only modification of the pro-
cedure that is required for the different tree mod-
els is the method for computing the probability
of the child state sequence given the parent state
P((zt?)t??c(t)|zt), defined separately for each model.
Sampling z. In this stage we sample a state for
each tree node. The probability of node t being as-
signed state k is given by:
P(zt = k|z?t,?) ? P(zt = k, (zt?)t??s(t)|zp(t))
? P((zt?)t??c(t)|zt = k) ? f?xtk (xt)
where s(t) denotes the set of siblings of t, f?xtk (xt)
denotes the posterior probability of observation xt
given all other observations assigned to state k, and
z?t denotes all state assignments except zt. In other
words, the probability is proportional to the product
of three terms: the probability of the states of t and
its siblings given its parent zp(t), the probability of
the states of the children c(t) given zt, and the pos-
terior probability of observation xt given zt. Note
that if we sample zt to be a previously unseen state,
we will need to extend ? as discussed in Section 3.2.
Now we give the equations for P((zt?)t??c(t)|zt)
for each of the models. In the independent child
model the probability of generating each child is:
Pind(zci(t) = k|zt = j) =
njk + ?0?k
nj? + ?0
Pind((zt?)t??c(t)|zt = j) =
?
t??c(t)
Pind(zt? |zt = j)
For the simultaneous child model, the probability of
generating a sequence of children, z, takes into ac-
count how many times that sequence has been gen-
erated, along with the likelihood of regenerating it:
Psim((zt?)t??c(t) = z|zt = j) =
njz + ?Pind(z|zt = j)
nj? + ?
Recall that ? denotes the concentration parameter
for the sequence generating DP. Lastly, we have the
DT NN IN DT NN VBD PRP$ NN TO VB NN EOS
The man in the corner taught his dachshund to play golf EOS
Figure 6: An example of a syntactic dependency tree
where the dependencies are between tags (hidden
states), and each tag generates a word (observation).
Markov child model:
Pm(zci(t) = k|zci?1(t) = i, zt = j) =
n?jik + ?0?k
n?ji? + ?0
Pm((zt?)t??c(t)|zt) =
?|c(t)|
i=1
Pm(zci(t)|zci?1(t), zt)
Finally, we give the posterior probability of an ob-
servation, given that F (?k) is Multinomial(?k), and
that H is Dirichlet(?, . . . , ?). Let N be the vocab-
ulary size and n?k be the number of observations x
with state k. Then:
f?xtk (xt) =
n?xtk + ?
n??k + N?
Sampling m. We use the following procedure,
which slightly modifies one from (Y. W. Teh, 2006,
p.c.), to sample each mjk:
SAMPLEM(j, k)
1 if njk = 0
2 then mjk = 0
3 else mjk = 1
4 for i? 2 to njk
5 do if rand() < ?0?0+i?1
6 then mjk = mjk + 1
7 return mjk
Sampling ?. Lastly, we sample ? using the Di-
richlet distribution:
(?1, . . . , ?K , ?u) ? Dirichlet(m?1, . . . ,m?K , ?0)
6 Experiments
We demonstrate infinite tree models on two dis-
tinct syntax learning tasks: unsupervised POS learn-
ing conditioned on untagged dependency trees and
learning a split of an existing tagset, which improves
the accuracy of an automatic syntactic parser.
For both tasks, we use a simple modification of
the basic model structure, to allow the trees to gen-
erate dependents on the left and the right with dif-
ferent distributions ? as is useful in modeling natu-
ral language. The modification of the independent
child tree is trivial: we have two copies of each of
277
the variables pik, one each for the left and the right.
Generation of dependents on the right is completely
independent of that for the left. The modifications of
the other models are similar, but now there are sepa-
rate sets of pik variables for the Markov child model,
and separate Lk and ?k variables for the simultane-
ous child model, for each of the left and right.
For both experiments, we used dependency trees
extracted from the Penn Treebank (Marcus et al,
1993) using the head rules and dependency extrac-
tor from Yamada and Matsumoto (2003). As is stan-
dard, we used WSJ sections 2?21 for training, sec-
tion 22 for development, and section 23 for testing.
6.1 Unsupervised POS Learning
In the first experiment, we do unsupervised part-of-
speech learning conditioned on dependency trees.
To be clear, the input to our algorithm is the de-
pendency structure skeleton of the corpus, but not
the POS tags, and the output is a labeling of each
of the words in the tree for word class. Since the
model knows nothing about the POS annotation, the
new classes have arbitrary integer names, and are
not guaranteed to correlate with the POS tag def-
initions. We found that the choice of ?0 and ?
(the concentration parameters) did not affect the out-
put much, while the value of ? (the parameter for
the base Dirichlet distribution) made a much larger
difference. For all reported experiments, we set
?0 = ? = 10 and varied ?.
We use several metrics to evaluate the word
classes. First, we use the standard approach of
greedily assigning each of the learned classes to the
POS tag with which it has the greatest overlap, and
then computing tagging accuracy (Smith and Eisner,
2005; Haghighi and Klein, 2006).8 Additionally, we
compute the mutual information of the learned clus-
ters with the gold tags, and we compute the cluster
F-score (Ghosh, 2003). See Table 1 for results of
the different models, parameter settings, and met-
rics. Given the variance in the number of classes
learned it is a little difficult to interpret these results,
but it is clear that the Markov child model is the
best; it achieves superior performance to the inde-
pendent child model on all metrics, while learning
fewer word classes. The poor performance of the
simultaneous model warrants further investigation,
but we observed that the distributions learned by that
8The advantage of this metric is that it?s comprehensible.
The disadvantage is that it?s easy to inflate by adding classes.
Model ? # Classes Acc. MI F1
Indep. 0.01 943 67.89 2.00 48.29
0.001 1744 73.61 2.23 40.80
0.0001 2437 74.64 2.27 39.47
Simul. 0.01 183 21.36 0.31 21.57
0.001 430 15.77 0.09 13.80
0.0001 549 16.68 0.12 14.29
Markov 0.01 613 68.53 2.12 49.82
0.001 894 75.34 2.31 48.73
Table 1: Results of part unsupervised POS tagging
on the different models, using a greedy accuracy
measure.
model are far more spiked, potentially due to double
counting of tags, since the sequence probabilities are
already based on the local probabilities.
For comparison, Haghighi and Klein (2006) re-
port an unsupervised baseline of 41.3%, and a best
result of 80.5% from using hand-labeled prototypes
and distributional similarity. However, they train on
less data, and learn fewer word classes.
6.2 Unsupervised POS Splitting
In the second experiment we use the infinite tree
models to learn a refinement of the PTB tags. We
initialize the set of hidden states to the set of PTB
tags, and then, during inference, constrain the sam-
pling distribution over hidden state zt at each node t
to include only states that are a refinement of the an-
notated PTB tag at that position. The output of this
training procedure is a new annotation of the words
in the PTB with the learned tags. We then compare
the performance of a generative dependency parser
trained on the new refined tags with one trained on
the base PTB tag set. We use the generative de-
pendency parser distributed with the Stanford fac-
tored parser (Klein and Manning, 2003b) for the
comparison, since it performs simultaneous tagging
and parsing during testing. In this experiment, un-
labeled, directed, dependency parsing accuracy for
the best model increased from 85.11% to 87.35%, a
15% error reduction. See Table 2 for the full results
over all models and parameter settings.
7 Related Work
The HDP-PCFG (Liang et al, 2007), developed at
the same time as this work, aims to learn state splits
for a binary-branching PCFG. It is similar to our
simultaneous child model, but with several impor-
tant distinctions. As discussed in Section 4.2, in our
model each state has a DP over sequences, with a
base distribution that is defined over the local child
278
Model ? Accuracy
Baseline ? 85.11
Independent 0.01 86.18
0.001 85.88
Markov 0.01 87.15
0.001 87.35
Table 2: Results of untyped, directed dependency
parsing, where the POS tags in the training data have
been split according to the various models. At test
time, the POS tagging and parsing are done simulta-
neously by the parser.
state probabilities. In contrast, Liang et al (2007)
define a global DP over sequences, with the base
measure defined over the global state probabilities,
?; locally, each state has an HDP, with this global
DP as the base measure. We believe our choice to
be more linguistically sensible: in our model, for a
particular state, dependent sequences which are sim-
ilar to one another increase one another?s likelihood.
Additionally, their modeling decision made it diffi-
cult to define a Gibbs sampler, and instead they use
variational inference. Earlier, Johnson et al (2007)
presented adaptor grammars, which is a very simi-
lar model to the HDP-PCFG. However they did not
confine themselves to a binary branching structure
and presented a more general framework for defin-
ing the process for splitting the states.
8 Discussion and Future Work
We have presented a set of novel infinite tree models
and associated inference algorithms, which are suit-
able for representing syntactic dependency structure.
Because the models represent a potentially infinite
number of hidden states, they permit unsupervised
learning algorithms which naturally select a num-
ber of word classes, or tags, based on qualities of
the data. Although they require substantial techni-
cal background to develop, the learning algorithms
based on the models are actually simple in form, re-
quiring only the maintenance of counts, and the con-
struction of sampling distributions based on these
counts. Our experimental results are preliminary but
promising: they demonstrate that the model is capa-
ble of capturing important syntactic structure.
Much remains to be done in applying infinite
models to language structure, and an interesting ex-
tension would be to develop inference algorithms
that permit completely unsupervised learning of de-
pendency structure.
Acknowledgments
Many thanks to Yeh Whye Teh for several enlight-
ening conversations, and to the following mem-
bers (and honorary member) of the Stanford NLP
group for comments on an earlier draft: Thad
Hughes, David Hall, Surabhi Gupta, Ani Nenkova,
Sebastian Riedel. This work was supported by a
Scottish Enterprise Edinburgh-Stanford Link grant
(R37588), as part of the EASIE project, and by
the Advanced Research and Development Activity
(ARDA)?s Advanced Question Answering for Intel-
ligence (AQUAINT) Phase II Program.
References
C. E. Antoniak. 1974. Mixtures of Dirichlet processes with ap-
plications to Bayesian nonparametrics. Annals of Statistics,
2:1152?1174.
M.J. Beal, Z. Ghahramani, and C.E. Rasmussen. 2002. The
infinite hidden Markov model. In Advances in Neural Infor-
mation Processing Systems, pages 577?584.
E. Charniak. 1996. Tree-bank grammars. In AAAI 1996, pages
1031?1036.
E. Charniak. 2000. A maximum-entropy-inspired parser. In
HLT-NAACL 2000, pages 132?139.
M. Collins. 2003. Head-driven statistical models for natural lan-
guage parsing. Computational Linguistics, 29(4):589?637.
T. S. Ferguson. 1973. A Bayesian analysis of some nonpara-
metric problems. Annals of Statistics, 1:209?230.
J. Ghosh. 2003. Scalable clustering methods for data mining. In
N. Ye, editor, Handbook of Data Mining, chapter 10, pages
247?277. Lawrence Erlbaum Assoc.
A. Haghighi and D. Klein. 2006. Prototype-driven learning for
sequence models. In HLT-NAACL 2006.
M. Johnson, T. Griffiths, and S. Goldwater. 2007. Adaptor
grammars: A framework for specifying compositional non-
parametric Bayesian models. In NIPS 2007.
D. Klein and C. D. Manning. 2003a. Accurate unlexicalized
parsing. In ACL 2003.
D. Klein and C. D. Manning. 2003b. Factored A* search for
models over sequences and trees. In IJCAI 2003.
P. Liang, S. Petrov, D. Klein, and M. Jordan. 2007. Nonpara-
metric PCFGs using Dirichlet processes. In EMNLP 2007.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz. 1993.
Building a large annotated corpus of English: The Penn
Treebank. Computational Linguistics, 19(2):313?330.
S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006. Learning
accurate, compact, and interpretable tree annotation. In ACL
44/COLING 21, pages 433?440.
J. Pitman. 2002. Poisson-Dirichlet and GEM invariant distribu-
tions for split-and-merge transformations of an interval par-
tition. Combinatorics, Probability and Computing, 11:501?
514.
N. A. Smith and J. Eisner. 2005. Contrastive estimation: Train-
ing log-linear models on unlabeled data. In ACL 2005.
Y. W. Teh, M.I. Jordan, M. J. Beal, and D.M. Blei. 2006. Hier-
archical Dirichlet processes. Journal of the American Statis-
tical Association, 101:1566?1581.
H. Yamada and Y. Matsumoto. 2003. Statistical dependency
analysis with support vector machines. In Proceedings of
IWPT, pages 195?206.
279
Verb Sense and Subcategorization:
Using Joint Inference to Improve Performance on Complementary Tasks
Galen Andrew, Trond Grenager, and Christopher Manning
Computer Science Department
Stanford University
Stanford, CA 94305-9040
{pupochik, grenager, manning}@cs.stanford.edu
Abstract
We propose a general model for joint inference in corre-
lated natural language processing tasks when fully anno-
tated training data is not available, and apply this model
to the dual tasks of word sense disambiguation and verb
subcategorization frame determination. The model uses
the EM algorithm to simultaneously complete partially
annotated training sets and learn a generative probabilis-
tic model over multiple annotations. When applied to the
word sense and verb subcategorization frame determina-
tion tasks, the model learns sharp joint probability dis-
tributions which correspond to linguistic intuitions about
the correlations of the variables. Use of the joint model
leads to error reductions over competitive independent
models on these tasks.
1 Introduction
Natural language processing research has tradition-
ally been divided into a number of separate tasks,
each of which is believed to be an important sub-
task of the larger language comprehension or gener-
ation problem. These tasks are usually addressed
separately, with systems designed to solve a sin-
gle problem. However, many of these tasks are not
truly independent; if solutions to one were known
they would facilitate finding solutions to the others.
For some sets of these problems, one would like to
be able to do joint inference, where information of
one kind can influence decisions about information
of another kind and vice versa. For instance, in-
formation about named entities can usefully inform
the decisions of a part-of-speech tagger, but equally,
part-of-speech information can help a named entity
recognizer. If one had a large corpus annotated with
all the information types of interest, one could es-
timate a joint distribution over all of the variables
simply by counting. However, it is more often the
case that one lacks any jointly annotated corpus,
or at least one that is sufficiently large, given that
the joint distribution is necessarily sparser than the
marginal distributions. It would therefore be useful
to be able to build a model for this joint inference
task using only partially supervised data. In this
System Name Accuracy
kunlp 57.6
jhu-english-JHU-final 56.6
SMUls 56.3
LIA-Sinequa-Lexsample 53.5
manning-cs224n 52.3
Table 1: Performance of the top 5 Senseval-2 word sense
disambiguation systems when considering accuracy only
on the 29 verbs. Systems not guessing on all instances
have been omitted.
paper we examine these problems in the context of
joint inference over verb senses and their subcate-
gorization frames (SCFs).
1.1 Verb Sense and Subcategorization
Of the syntactic categories tested in the Senseval
word sense disambiguation (WSD) competitions,
verbs have proven empirically to be the most dif-
ficult. In Senseval-1, Kilgarriff and Rosenzweig
(2000) found a 10-point difference between the
best systems? performance on verbs compared with
other parts-of-speech. In Senseval-2, Yarowsky and
Florian (2002) also found that while accuracies of
around 73% were possible for adjectives and nouns,
even the most competitive systems have accuracies
of around 57% when tested on verbs (see Table 1).
A likely explanation for this discrepancy is that dif-
ferent senses of common verbs can occur in sim-
ilar lexical contexts, thereby decreasing the effec-
tiveness of ?bag-of-words? models.
Verbs also pose serious challenges in a very dif-
ferent task: syntactic parsing. Verb phrases are syn-
tactically complex and frought with pitfalls for auto-
mated parsers, such as prepositional phrase attach-
ment ambiguities. These challenges may be par-
tially mitigated by the fact that particular verbs often
have strong preferences for particular SCFs. Unfor-
tunately, it is not the case that each verb consistently
takes the same SCF. More often, a verb has several
preferred SCFs, with rarer forms also occurring, for
example, in idioms. Jurafsky (1998) proposes us-
? NP PP NPPP VPto VPing
2:30:00 4 1 0 0 20 33
2:30:01 1 7 0 4 0 0
2:42:04 12 0 3 0 0 1
Table 2: The learned joint distribution over the senses
and subcategorizations of the verb begin (in percent
probability). Low probability senses and subcategoriza-
tions have been omitted.
ing a probabilistic framework to represent subcate-
gorization preferences, where each lexical item has
a corresponding distribution over the possible sets
of arguments. Modeling these distributions may be
useful: Collins (2003) has shown that verb subcate-
gorization information can be used to improve syn-
tactic parsing performance.
It has also been recognized that a much more ac-
curate prediction of verb subcategorization prefer-
ence can be made if conditioned on the sense of
the verb. Roland and Jurafsky (2002) conclude that
for a given lexical token in English, verb sense is
the best determiner of SCF, far outweighing either
genre or dialect. Demonstrating the utility of this,
Korhonen and Preiss (2003) achieve significant im-
provement at a verb subcategorization acquisition
task by conditioning on the verb sense as predicted
by a statistical word sense disambiguation system.
Conversely, if different senses have distinct subcat-
egorization preferences, it is reasonable to expect
that information about the way a verb subcatego-
rizes in a particular case may be of significant util-
ity in determining the verb?s sense. As an example,
Yarowsky (2000) makes use of rich syntactic fea-
tures to improve the performance of a supervised
WSD system.
As an illustration of this correlation, Table 2
shows a learned joint distribution over sense and
SCF for the common verb begin.1 Its common
senses, taken from WordNet, are as follows: sense
2:30:00, to initiate an action or activity, (?begin
working?), sense 2:30:01, to set in motion or cause
to start, (?to begin a war?), and sense 2:42:04, to
have a beginning, (?the day began?). The SCFs
shown here are a subset of the complete set of SCFs,
described in Table 3. Note that the sense and SCF
variables are highly correlated for this verb. Sense
2:30:00 occurs almost entirely with verb phrase ar-
guments, sense 2:30:01 occurs almost entirely as a
transitive verb, and sense 2:42:04 occurs as an in-
transitive verb (no arguments following the verb).
It should be evident that the strong correlation be-
1We cannot show an empirical joint distribution because of
the lack of a sufficiently large jointly annotated corpus, as dis-
cussed below.
tween these two variables can be exploited to in-
crease performance in the tasks of predicting their
values in either direction, even when the evidence is
weak or uncertain.
1.2 Learning a Joint Model
Performing joint inference requires learning a joint
distribution over sense and SCF for each verb. In
order to estimate the joint distribution directly from
data we would need a large corpus that is annotated
for both verb sense and SCF. Unfortunately, no such
corpus of adequate size exists.2 Instead, there are
some corpora such as SemCor and Senseval-2 la-
beled for sense, and others that are parsed and from
which it is possible to compute verb SCFs determin-
istically. In the current work we use two corpora to
learn a joint model: Senseval-2, labeled for sense
but not syntax, and the Penn Treebank, labeled for
syntax but not sense. We do so by treating the two
data sets as a single one with incompletely labeled
instances. This partially labeled data set then yields
a semi-supervised learning problem, suitable for the
Expectation-Maximization (EM) algorithm (Demp-
ster et al, 1977).
2 Tasks and Data Sets
We evaluate our system on both the WSD task and
the verb SCF determination task. We describe each
task in turn.
2.1 Word Sense Disambiguation
We used as our sense-annotated corpus the data
sets from the English lexical sample portion of the
Senseval-2 word sense disambiguation competition
(Kilgarriff and Rosenzweig, 2000). This data set
contains multiple instances of 73 different English
word types, divided into training and testing exam-
ples. Each word type is marked for part of speech,
so that the sense disambiguation task does not need
to distinguish between senses that have different
parts of speech. We selected from this data set al
29 words that were marked as verbs.
Each example consists of a marked occurrence of
the target word in approximately 100 words of sur-
rounding context. The correct sense of the word,
marked by human annotators, is also given. Each
instance is labeled with a sense corresponding to a
synset from WordNet (Miller, 1995). The number
of senses per word varies enormously: some words
have more than 30 senses, while others have five
2A portion of the Brown corpus has been used both in the
construction of the SemCor word sense database and in the con-
struction of the Penn Treebank, but coverage is very low, espe-
cially for sense markings, and the individual sentences have not
to our knowledge been explicitly aligned.
or fewer. These ?fine-grained? senses are also par-
titioned into a smaller number of ?coarse-grained?
senses, and systems are evaluated according to both
metrics. The number of training and testing exam-
ples per word varies from tens to nearly a thousand.
We used the same train/test division as in Senseval-
2, so that our reported accuracy numbers are directly
comparable with those of other Senseval-2 submis-
sions, as given in Table 1.
2.2 Verb Subcategorization
We use as our SCF-annotated corpus sentences
drawn from the Wall Street Journal section of the
Penn Treebank. For each target verb we select sen-
tences containing a form of the verb (tagged as a
verb) with length less than 40 words. We select
training examples from sections 2 through 21, and
test examples from all other sections.3
There are many conceivable ways to partition
the set of possible verb argument combinations into
SCFs. One possible approach would be to use as the
SCF representation the raw sequence of constituents
occurring in the verb phrase. This is certainly an
unbiased representation, but as there are many thou-
sands of rewrites for VP in the Penn Treebank, data
sparsity would present a significant problem. In ad-
dition, many of the variants do not contain useful
information for our task: for example, we wouldn?t
expect to get much value from knowing about the
presence or absence of an adverb in the phrase. In-
stead, we chose to use a small number of linguis-
tically motivated SCFs which form a partition over
the large space of possible verb arguments.
We chose as a starting point the SCF partition
specified in Roland (2001). These SCFs are defined
declaratively using a set of tgrep expressions that
match appropriate verb phrases.4 We made signifi-
cant modifications to the set of SCFs, and also sim-
plified the tgrep expressions used to match them.
One difference from Roland?s SCF set is that we
analyze verb particles as arguments, so that several
SCFs differ only in the existence of a particle. This
is motivated by the fact that the particle is a syntactic
feature that provides strong evidence about the verb
sense. One might argue that the presence of a par-
ticle should be considered a lexical feature modeled
independently from the SCF, but the distinction is
blurry, and we have instead combined the variables
in favor of model simplicity. A second difference is
3Sections 2 through 21 of the WSJ are typically used for
training PCFG parsers, and section 23 is typically used for test-
ing. Because of sparse data we drew our test examples from all
non-training sections.
4tgrep is a tree node matching program written by Richard
Pito, distributed with the Penn Treebank.
Subcat Description
? No arguments
NP Transitive
PP Prepositional phrase
NP PP Trans. with prep. phrase
VPing Gerundive verb phrase
NP VPing Perceptual complement
VPto Intrans. w/ infinitival VP
NP VPto Trans. w/ infinitival VP
S for to Intrans. w/ for PP and infin. VP
NP SBAR Trans. w/ finite clause
NP NP Ditransitive
PRT Particle and no args.
NP PRT Transitive w/ particle
PP PRT Intrans. w/ PP and particle
VP PRT Intrans. w/ VP and particle
SBAR PRT Intrans. w/ fin. clause and part.
Other None of the above
Table 3: The 17 subcategorization frames we use.
that unlike Roland, we do not put passive verb con-
structions in a separate ?passive? SCF, but instead
we undo the passivization and put them in the un-
derlying category. Although one might expect that
passivization itself is a weak indicator of sense, we
believe that the underlying SCF is more useful. Our
final set of SCFs is shown in Table 3.
Given a sentence annotated with a syntactic
parse, the SCF of the target verb can be computed by
attempting to match each of the SCF-specific tgrep
expressions with the verb phrase containing the tar-
get verb. Unlike those given by Roland, our tgrep
expressions are not designed to be mutually exclu-
sive; instead we determine verb SCF by attempting
matches in a prescribed sequence, using ?if-then-
else? logic.
3 Model Structure and Inference
Our generative probabilistic model can be thought
of as having three primary components: the sense
model, relating the verb sense to the surrounding
context, the subcategorization model, relating the
verb subcategorization to the sentence, and the joint
model, relating the sense and SCF of the verb to
each other. More formally, the model is a factored
representation of a joint distribution over these vari-
ables and the data: the verb sense (V ), the verb SCF
(C), the unordered context ?bag-of-words? (W ),
and the sentence as an ordered sequence of words
(S). The joint distribution P(V, C, W, S) is then
factored as
P(V )P(C|V )P(S|C)
?
i
P(Wi |V )
W S
V C
n
Figure 1: A graphical representation of the combined
sense and subcategorization probabilistic model. Note
that the box defines a plate, indicating that the model
contains n copies of this variable.
where Wi is the word type occurring in each po-
sition of the context (including the target sentence
itself). The first two terms together define a joint
distribution over verb sense (V ) and SCF (C), the
third term defines the subcategorization model, and
the last term defines the sense model. A graphical
model representation is shown in Figure 1.
The model assumes the following generative pro-
cess for a data instance of a particular verb. First we
generate the sense of the target verb. Conditioned
on the sense, we generate the SCF of the verb. (Note
that the decision to generate sense and then SCF
is arbitrary and forced by the desire to factor the
model; we discuss reversing the order below.) Then,
conditioned on the sense of the verb, we generate
an unordered collection of context words. (For the
Senseval-2 corpus, this collection includes not only
the words in the sentence in which the verb occurs,
but also the words in surrounding sentences.) Fi-
nally, conditioned on the SCF of the verb, we gen-
erate the immediate sentence containing the verb as
an ordered sequence of words.
An apparent weakness of this model is that it
double-generates the context words from the en-
closing sentence: they are generated once by the
sense model, as an unordered collection of words,
and once by the subcategorization model, as an or-
dered sequence of words. The model is thus defi-
cient in that it assigns a large portion of its probabil-
ity mass to impossible cases: those instances which
have words in the context which do not match those
in the sentence. However because the sentences are
always observed, we only consider instances in the
set of consistent cases, so the deficiency should be
irrelevant for the purpose of reasoning about sense
and SCF.
We discuss each of the model components in turn.
3.1 Verb Sense Model
The verb sense component of our model is an or-
dinary multinomial Naive Bayes ?bag-of-words?
model: P(V )
?
i P(Wi |V ). We learn the marginal
over verb sense with maximum likelihood estima-
tion (MLE) from the sense annotated data. We learn
the sense-conditional word model using smoothed
MLE from the sense annotated data, and to smooth
we use Bayesian smoothing with a Dirichlet prior.
The free smoothing parameter is determined empir-
ically, once for all words in the data set. In the inde-
pendent sense model, to infer the most likely sense
given a context of words P(S|W), we just find the V
that maximizes P(V )
?
i P(Wi |V ). Inference in thejoint model over sense and SCF is more complex,
and is described below.
In order to make our system competitive with
leading WSD systems we made an important modi-
fication to this basic model: we added relative posi-
tion feature weighting. It is known that words closer
to the target word are more predictive of sense, so it
is reasonable to weight them more highly. We de-
fine a set of ?buckets?, or partition over the position
of the context word relative to the target verb, and
we weight each context word feature with a weight
given by its bucket, both when estimating model pa-
rameters at train time and when performing infer-
ence at test time. We use the following 8 relative
position buckets: (??,?6], [?5,?3], ?2, ?1, 1,
2, [3, 5], and [6,?). The bucket weights are found
empirically using a simple optimization procedure
on k-fold training set accuracy. In ablation tests on
this system we found that the use of relative posi-
tion feature weighting, when combined with corre-
sponding evidence attenuation (see Section 3.3) in-
creased the accuracy of the standalone verb sense
disambiguation model from 46.2% to 54.0%.
3.2 Verb Subcategorization Model
The verb SCF component of our model P(S|C)
represents the probability of particular sentences
given each possible SCF. Because there are in-
finitely many possible sentences, a multinomial rep-
resentation is infeasible, and we instead chose to
encode the distribution using a set of probabilistic
context free grammars (PCFGs). A PCFG is created
for each possible SCF: each PCFG yields only parse
trees in which the distinguished verb subcategorizes
in the specified manner (but other verbs can parse
freely). Given a SCF-specific PCFG, we can deter-
mine the probability of the sentence using the inside
algorithm, which sums the probabilities of all pos-
sible trees in the grammar producing the sentence.
To do this, we modified the exact PCFG parser of
Klein and Manning (2003). In the independent SCF
model, to infer the most likely SCF given a sen-
tence P(C|S), we just find the C that maximizes
P(S|C)P(C). (For the independent model, the SCF
prior is estimated using MLE from the training ex-
amples.) Inference in the joint model over sense and
SCF is more complex, and is described below.
Learning this model, SCF-specific PCFGs, from
our SCF-annotated training data, requires some
care. Commonly PCFGs are learned using MLE
of rewrite rule probabilities from large sets of tree-
annotated sentences. Thus to learn SCF-specific
PCFGs, it seems that we should select a set of an-
notated sentences containing the target verb, deter-
mine the SCF of the target verb in each sentence,
create a separate corpus for each SCF of the target
verb, and then learn SCF-specific grammars from
the SCF-specific corpora. If we are careful to dis-
tinguish rules which dominate the target verb from
those which do not, then the grammar will be con-
strained to generate trees in which the target verb
subcategorizes in the specified manner, and other
verbs can occur in general tree structures. The prob-
lem with this approach is that in order to create a
broad-coverage grammar (which we will need in or-
der for it to generalize accurately to unseen test in-
stances) we will need a very large number of sen-
tences in which the target verb occurs, and we do
not have enough data for this approach.
Because we want to maximize the use of the
available data, we must instead make use of every
verb occurrence when learning SCF-specific rewrite
rules. We can accomplish this by making a copy
of each sentence for each verb occurrence (not just
the target verb), determining the SCF of the distin-
guished verb in each sentence, partitioning the sen-
tence copies by distinguished verb SCF, and learn-
ing SCF-specific grammars using MLE. Finally, we
change the lexicon by forcing the distinguished verb
tag to rewrite to only our target verb. The method
we actually use is functionally equivalent to this lat-
ter approach, but altered for efficiency. Instead of
making copies of sentences with multiple verbs, we
use a dense representation. We determine the SCF
of each verb in the sentence, and then annotate the
verb and all nonterminal categories occurring above
the verb in the tree, up to the root, with the SCF
of the verb. Note that some nonterminals will then
have multiple annotations. Then to learn a SCF-
specific PCFG, we count rules that have the speci-
fied SCF annotation as rules which can dominate the
distinguished verb, and then count all rules (includ-
ing the SCF-specific ones) as general rules which
cannot dominate the distinguished verb.
3.3 The Joint Model
Given a fully annotated dataset, it is trivial to learn
the parameters of the joint distribution over verb
sense and SCF P(V, C) using MLE. However, be-
cause we do not have access to such a dataset, we
instead use the EM algorithm to ?complete? the
missing annotations with expectations, or soft as-
signments, over the values of the missing variable
(we present the EM algorithm in detail in the next
section). Given this ?completed? data, it is again
trivial to learn the parameters of the joint proba-
bility model using smoothed MLE. We use simple
Laplace add-one smoothing to smooth the distribu-
tion.
However, a small complication arises from the
fact that the marginal distributions over senses and
SCFs for a particular verb may differ between the
two halves of our data set. They are, after all, wholly
different corpora, assembled by different people for
different purposes. For this reason, when testing
the system on the sense corpus we?d like to use a
sense marginal distribution trained from the sense
corpus, and when testing the system on the SCF
corpus we?d like to use a SCF marginal distribu-
tion trained from the SCF corpus. To address this,
recall from above that the factoring we choose for
the joint distribution is arbitrary. When performing
sense inference we use the model Pv(V )P j(C|V )
where P j(C|V ) was learned from the complete data,
and Pv(V ) was learned from the sense-marked ex-
amples only. When performing SCF inference we
use the equivalent factoring Pc(C)P j(V |C), where
P j(V |C) was learned from the complete data, and
Pc(C) was learned from the SCF-annotated exam-
ples only.
We made one additional modification to this joint
model to improve performance. When performing
inference in the model, we found it useful to dif-
ferentially weight different probability terms. The
most obvious need for this comes from the fact
that the sense-conditional word model employs rel-
ative position feature weighting, which can change
the relatively magnitude of the probabilities in this
term. In particular, by using feature weights greater
than 1.0 during inference we overestimate the ac-
tual amount of evidence. Even without the feature
weighting, however, the word model can still over-
estimate the actual evidence given that it encodes an
incorrect independence assumption between word
features (of course word occurrence in text is ac-
tually very highly correlated). The PCFG model
also suffers from a less severe instance of the same
problem: human languages are of course not con-
text free, and there is in fact correlation between
supposedly independent tree structures in different
parts of the tree. To remedy this evidence over-
confidence, it is helpful to attenuate or downweight
the evidence terms accordingly. More generally, we
place weights on each of the probability terms used
in inference calculations, yielding models of the fol-
lowing form:
P(V )?(v)P(C|V )?(c)P(S|C)?(s)[
?
i
P(Wi |V )]?(w)
These ?(?) weights are free parameters, and we
find them by simple optimization on k-fold accu-
racy. In ablation tests on this system, we found
that term weighting (particularly evidence attenua-
tion) increased the accuracy of the standalone sense
model from 51.9% to 54.0% at the fine-grained verb
sense disambiguation task.
We now describe the precise EM algorithm used.
Prior to running EM we first learn the independent
sense and SCF model parameters from their respec-
tive datasets. We also initialize the joint sense and
SCF distribution to the uniform distribution. Then
we iterate over the following steps:
? E-step: Using the current model parameters,
for each datum in the sense-annotated corpus,
compute expectations over the possible SCFs,
and for each datum in the SCF-annotated cor-
pus, compute expectations over the possible
senses.
? M-step: use the completed data to reestimate
the joint distribution over sense and SCF.
We run EM to convergence, which for our dataset
occurs within 6 iterations. Additional iterations do
not change the accuracy of our model. Early stop-
ping of EM after 3 iterations was found to hurt k-
fold sense accuracy by 0.1% and SCF accuracy by
0.2%. Early stopping of EM after only 1 iteration
was found to hurt k-fold sense accuracy by a total of
0.2% and SCF accuracy by 0.4%. These may seem
like small differences, but significant relative to the
advantages given by the joint model (see below).
In the E-step of EM, it is necessary to do infer-
ence over the joint model, computing posterior ex-
pectations of unknown variables conditioned on ev-
idence variables. During the testing phase, it is also
necessary to do inference, computing maximum a
posteriori (MAP) values of unknown variables con-
ditioned on evidence variables. In all cases we do
exact Bayesian network inference, which involves
conditioning on evidence variables, summing over
extraneous variables, and then either maximizing
over the resulting factors of query variables, or nor-
malizing them to obtain distributions of query vari-
ables. At test time, when querying about the MAP
sense (or SCF) of an instance, we chose to max-
imize over the marginal distribution, rather than
maximize over the joint sense and SCF distribution.
We found empirically that this gave us higher accu-
racy at the individual tasks. If instead we were do-
ing joint prediction, we would expect high accuracy
to result from maximizing over the joint.
4 Results and Discussion
In Figures 2, 3 and 4 we compare the perfor-
mance of the independent and joint models on the
verb sense disambiguation and verb SCF determina-
tion problems, evaluated using both 10-fold cross-
validation accuracy and test set accuracy. In Figure
2, we report the performance of a system resulting
from doing optimization of free parameters (such as
feature and term weights) on a per-verb basis. We
also provide a baseline computed by guessing the
most likely class.
Although the parameter optimization of Figure
2 was performed with respect to 10-fold cross-
validation on the training sets, its lower perfor-
mance on the test sets suggests that it suffers from
overfitting. To test this hypothesis we also trained
and tested on the test sets a version of the system
with corpus-wide free parameter optimization, and
the results of this test are shown in Figure 3. The
lower gap between the training set cross-validation
and test set performance on the WSD task confirms
our overfitting hypothesis. However, note that the
gap between training set cross-validation and test
set performance on the SCF determination task per-
sists (although it is diminished slightly). We believe
that this results from the fact that there is significant
data drift between the training sections of the WSJ
in the Penn Treebank (sections 2 through 21) and all
other sections.
Using corpus-wide optimization, the joint model
improves sense disambiguation accuracy by 1.9%
over the independent model, bringing our system
to 55.9% accuracy on the test set, performance that
is comparable with that of the state of the art sys-
tems on verbs given in Table 1. The joint model re-
duces sense disambiguation error by 4.1%. On the
verb SCF determination task, the joint model yields
a 2.1% improvement in accuracy over the indepen-
dent model, reducing total error by 5.1%.
We also report results of the independent and
joint systems on each verb individually in Table 4
Not surprisingly, making use of the joint distribution
was much more helpful for some verbs than others.
40.5
38.8
59.9
70.8
60.2
72.8
54.7
59.7
55.6
61.4
35
40
45
50
55
60
65
70
75
80
Sense Subcat
Te
st
 
Ac
cu
ra
cy
Baseline
Individual
10-fold
Joint 10-fold
Individual
test
Joint test
Figure 2: Chart comparing results of independent and
joint systems on the verb sense and SCF tasks, evaluated
with 10-fold cross-validation on the training sets and on
the test sets. The baseline shown is guessing most likely
class. These systems used per-verb optimization of free
parameters.
40.5
38.8
52.4
68.5
54.7
69.8
54.0
59.3
55.9
61.4
35
40
45
50
55
60
65
70
75
80
Sense Subcat
Te
st
 
Ac
cu
ra
cy
Baseline
Individual
10-fold
Joint 10-fold
Individual
test
Joint test
Figure 3: Chart comparing results of independent and
joint systems on the verb sense and SCF tasks. These
systems used corpus-wide optimization of free parame-
ters.
40.5
38.8
45.0
68.5
49.2
69.1
46.2
59.3
50.7
59.6
35
40
45
50
55
60
65
70
75
80
Sense Subcat
Te
st
 
Ac
cu
ra
cy
Baseline
Individual
10-fold
Joint 10-fold
Individual
test
Joint test
Figure 4: Chart comparing results of independent and
joint systems on the verb sense and SCF tasks. This sys-
tem has no relative position word feature weighting and
no term weighting.
Indep Joint Indep Joint
Verb Sense Sense Subcat Subcat
begin 76.8 84.3 57.0 63.3
call 39.4 42.4 44.9 49.0
carry 45.5 40.9 63.3 70.0
collaborate 90.0 90.0 100.0 100.0
develop 42.0 39.1 69.7 69.7
draw 29.3 26.8 72.7 63.6
dress 59.3 59.3 NA NA
drift 43.8 40.6 50.0 50.0
drive 45.2 52.4 54.5 54.5
face 81.7 80.6 82.4 82.4
ferret 100.0 100.0 NA NA
find 23.5 29.4 61.1 64.8
keep 46.3 58.2 52.1 53.5
leave 47.0 54.5 36.4 40.0
live 62.7 65.7 85.7 85.7
match 57.1 54.8 58.3 66.7
play 42.4 45.5 66.7 61.9
pull 28.3 26.7 44.4 55.6
replace 57.8 62.2 56.0 60.0
see 40.6 39.1 53.6 55.1
serve 60.8 52.9 72.0 72.0
strike 37.0 27.8 50.0 50.0
train 55.6 55.6 40.0 40.0
treat 52.3 54.5 69.2 76.9
turn 29.9 29.9 46.3 50.0
use 65.8 68.4 69.7 68.8
wander 78.0 80.0 NA NA
wash 50.0 41.7 0.0 0.0
work 41.7 43.3 67.9 66.1
Table 4: Comparison of the performance of the indepen-
dent and joint inference models on the verb sense and
SCF tasks,evaluated on the Senseval-2 test set, for each
of the 29 verbs in the study. These results were obtained
with no per-verb parameter optimization. Note the great
variation in problem difficulty and joint model perfor-
mance across verbs.
For example, on the verbs begin, drive, find, keep,
leave, and work, the joint model gives a greater than
5% accuracy boost on the WSD task. In contrast, for
some other verbs, the joint model showed a slight
decrease in accuracy on the test set relative to the
independent model.
We present a few representative examples where
the joint model makes better decisions than the in-
dividual model. In the sentence
. . . prices began weakening last month after
Campeau hit a cash crunch.
the sense model (based on bag-of-words evidence)
believes that the sense 2:42:04 is most likely (see
Table 2 for senses and joint distribution). How-
ever, the SCF model gives high weight to the frames
VPto and VPing, which when combined with the
joint distribution, give much more probability to
the sense 2:30:00. The joint model thus correctly
chooses sense 2:30:00. In the sentence
. . . before beginning a depressing eight-year
slide that continued through last year.
the sense model again believes that the sense
2:42:04 is most likely. However, the SCF model
correctly gives high weight to the NP frame, which
when combined with the joint distribution, gives
much more probability to the sense 2:30:01. The
joint model thus correctly chooses sense 2:30:01.
Given the amount of information contained in the
joint distribution it is surprising that the joint model
doesn?t yield a greater advantage over the indepen-
dent models. It seems to be the case that the word
sense model is able to capture much of the SCF in-
formation by itself, without using an explicit syn-
tactic model. This results from the relative posi-
tion weighting, since many of our SCFs correlate
highly with the presence of small sets of words in
particular positions (for instance, the infinitival ?to?,
prepositions, and pronouns). We tested this hypoth-
esis by examining how the addition of SCF informa-
tion affected performance of a weaker sense model,
obtained by removing feature and term weighting.
The results are shown in Figure 4. Indeed, when us-
ing this weaker word sense model, the joint model
yields a much larger 4.5% improvement in WSD ac-
curacy.
5 Future Work
We can imagine several modifications to the ba-
sic system that might improve performance. Most
importantly, more specific use could be made of
SCF information besides modeling its joint distribu-
tion with sense, for example conditioning on head-
words of (perceived) arguments, especially parti-
cles and prepositions. Second, although we made
some attempt at extracting the ?underlying? SCF of
verbs by analyzing passive constructions separately,
similar analysis of other types of movement such
as relative clauses may also be useful. Third, we
could hope to get some improvement from changing
our model structure to address the issue of double-
generation of words discussed in section 3. One way
this could be done would be to use a parser only
to estimate the probability of the sequence of word
tags (i.e., parts of speech) in the sentence, then to
use a sense-specific lexicon to estimate the proba-
bility of finding the words under the tags.
Although we chose WSD and SCF determination
as a test case, the approach of this paper is appli-
cable to other pairs of tasks. It may also be pos-
sible to improve parsing accuracy on verb phrases
or other phrases, by simultaneously resolving word
sense ambiguities, as attempted unsuccessfully by
Bikel (2000). This work is intended to introduce
a general methodology for combining disjoint NLP
tasks that is of use outside of these specific tasks.
6 Acknowledgements
This paper is based on work supported in part by
the Advanced Research and Development Activity
(ARDA)?s Advanced Question Answering for Intel-
ligence (AQUAINT) Program, and by the National
Science Foundation under Grant No. IIS-0085896,
as part of the Knowledge Discovery and Dissemina-
tion program. We additionally thank the reviewers
for their insightful comments.
References
Daniel M. Bikel. 2000. A statistical model for parsing
and word-sense disambiguation. Joint SIGDAT Con-
ference on Empirical Methods in Natural Language
Processing and Very Large Corpora.
Michael Collins. 2003. Head-driven statistical models
for natural language parsing. To appear in Computa-
tional Linguistics.
A.P. Dempster, N.M. Laird, and D.B. Rubin. 1977. Max-
imum likelihood from incomplete data via the em al-
gorithm. Journal of the Royal Statistical Society, Se-
ries B (Methodological), 39(1):1?38.
Daniel Jurafsky. 1998. A probabilistic model of lexical
and syntactic access and disambiguation. Cognitive
Science, 20(2):139?194.
Adam Kilgarriff and Joseph Rosenzweig. 2000. Frame-
work and results for english senseval. Computers and
the Humanities, 34(1-2):15?48.
Dan Klein and Christopher Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of the 41st An-
nual Meeting of the Association for Computational
Linguistics.
Anna Korhonen and Judita Preiss. 2003. Improving
subcategorization acquisition using word sense disam-
biguation. In Proceedings of the 41st Annual Meeting
of the Association for Computational Linguistics, Sap-
poro, Japan, pages 48?55.
G.A. Miller. 1995. Wordnet: A lexical database for en-
glish. Communications of the ACM, 38(11):39?41.
Douglas Roland and Daniel Jurafsky. 2002. Verb sense
and verb subcategorization probabilities. In Paola
Merlo and Suzanne Stevenson, editors, The Lexical
Basis of Sentence Processing, chapter 16. John Ben-
jamins, Amsterdam.
Douglas Roland. 2001. Verb Sense and Verb Subcate-
gorization Probabilities. Ph.D. thesis, University of
Colorado.
David Yarowsky and Radu Florian. 2002. Evaluating
sense disambiguation across diverse parameter spaces.
Natural Language Engineering, 8(4):293?310.
David Yarowsky. 2000. Hierarchical decision lists for
word sense disambiguation. Computers and the Hu-
manities, 34(1-2).
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 1?8,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Unsupervised Discovery of a Statistical Verb Lexicon
Trond Grenager and Christopher D. Manning
Computer Science Department
Stanford University
Stanford, CA 94305
{grenager, manning}@cs.stanford.edu
Abstract
This paper demonstrates how unsupervised tech-
niques can be used to learn models of deep linguis-
tic structure. Determining the semantic roles of a
verb?s dependents is an important step in natural
language understanding. We present a method for
learning models of verb argument patterns directly
from unannotated text. The learned models are sim-
ilar to existing verb lexicons such as VerbNet and
PropBank, but additionally include statistics about
the linkings used by each verb. The method is
based on a structured probabilistic model of the do-
main, and unsupervised learning is performed with
the EM algorithm. The learned models can also
be used discriminatively as semantic role labelers,
and when evaluated relative to the PropBank anno-
tation, the best learned model reduces 28% of the
error between an informed baseline and an oracle
upper bound.
1 Introduction
An important source of ambiguity that must be
resolved by any natural language understanding
system is the mapping between syntactic depen-
dents of a predicate and the semantic roles1 that
they each express. The ambiguity stems from the
fact that each predicate can allow several alternate
mappings, or linkings,2 between its semantic roles
and their syntactic realization. For example, the
verb increase can be used in two ways:
(1) The Fed increased interest rates.
(2) Interest rates increased yesterday.
The instances have apparently similar surface syn-
tax: they both have a subject and a noun phrase
directly following the verb. However, while the
subject of increase expresses the agent role in the
first, it instead expresses the patient role in the sec-
ond. Pairs of linkings such as this allowed by a
single predicate are often called diathesis alterna-
tions (Levin, 1993).
The current state-of-the-art approach to resolv-
ing this ambiguity is to use discriminative classi-
fiers, trained on hand-tagged data, to classify the
1Also called thematic roles, theta roles, or deep cases.
2Sometimes called frames.
semantic role of each dependent (Gildea and Juraf-
sky, 2002; Pradhan et al, 2005; Punyakanok et al,
2005). A drawback of this approach is that even
a relatively large training corpus exhibits consid-
erable sparsity of evidence. The two main hand-
tagged corpora are PropBank (Palmer et al, 2003)
and FrameNet (Baker et al, 1998), the former of
which currently has broader coverage. However,
even PropBank, which is based on the 1M word
WSJ section of the Penn Treebank, is insufficient
in quantity and genre to exhibit many things. A
perfectly common verb like flap occurs only twice,
across all morphological forms. The first example
is an adjectival use (flapping wings), and the sec-
ond is a rare intransitive use with an agent argu-
ment and a path (ducks flapping over Washington).
From this data, one cannot learn the basic alterna-
tion pattern for flap: the bird flapped its wings vs.
the wings flapped.
We propose to address the challenge of data
sparsity by learning models of verb behavior di-
rectly from raw unannotated text, of which there
is plenty. This has the added advantage of be-
ing easily extendible to novel text genres and lan-
guages, and the possibility of shedding light on
the question of human language acquisition. The
models learned by our unsupervised approach pro-
vide a new broad-coverage lexical resource which
gives statistics about verb behavior, information
that may prove useful in other language process-
ing tasks, such as parsing. Moreover, they may be
used discriminatively to label novel verb instances
for semantic role. Thus we evaluate them both in
terms of the verb alternations that they learn and
their accuracy as semantic role labelers.
This work bears some similarity to the sub-
stantial literature on automatic subcategorization
frame acquisition (see, e.g., Manning (1993),
Briscoe and Carroll (1997), and Korhonen
(2002)). However, that research is focused on ac-
quiring verbs? syntactic behavior, and we are fo-
cused on the acquisition of verbs? linking behav-
ior. More relevant is the work of McCarthy and
1
Relation Description
subj NP preceding verb
np#n NP in the nth position following verb
np NP that is not the subject and
not immediately following verb
cl#n Complement clause
in the nth position following verb
cl Complement clause
not immediately following verb
xcl#n Complement clause without subject
in the nth position following verb
xcl Complement clause without subject
not immediately following verb
acomp#n Adjectival complement
in the nth position following verb
acomp Adjectival complement
not immediately following verb
prep x Prepositional modifier
with preposition x
advmod Adverbial modifier
advcl Adverbial clause
Table 1: The set of syntactic relations we use, where n ?
{1, 2, 3} and x is a preposition.
Korhonen (1998), which used a statistical model
to identify verb alternations, relying on an existing
taxonomy of possible alternations, as well as La-
pata (1999), which searched a large corpus to find
evidence of two particular verb alternations. There
has also been some work on both clustering and
supervised classification of verbs based on their
alternation behavior (Stevenson and Merlo, 1999;
Schulte im Walde, 2000; Merlo and Stevenson,
2001). Finally, Swier and Stevenson (2004) per-
form unsupervised semantic role labeling by using
hand-crafted verb lexicons to replace supervised
semantic role training data. However, we believe
this is the first system to simultaneously discover
verb roles and verb linking patterns from unsuper-
vised data using a unified probabilistic model.
2 Learning Setting
Our goal is to learn a model which relates a verb,
its semantic roles, and their possible syntactic re-
alizations. As is the case with most semantic role
labeling research, we do not attempt to model the
syntax itself, and instead assume the existence of a
syntactic parse of the sentence. The parse may be
from a human annotator, where available, or from
an automatic parser. We can easily run our system
on completely unannotated text by first running
an automatic tokenizer, part-of-speech tagger, and
parser to turn the text into tokenized, tagged sen-
tences with associated parse trees.
In order to keep the model simple, and indepen-
dent of any particular choice of syntactic represen-
tation, we use an abstract representation of syn-
Sentence: A deeper market plunge today could
give them their first test.
Verb: give
Syntactic Semantic Head
Relation Role Word
subj ARG0 plunge/NN
np ARGM today/NN
np#1 ARG2 they/PRP
np#2 ARG1 test/NN
v = give
` = {ARG0 ? subj, ARG1 ? np#2
ARG2 ? np#1}
o = [(ARG0, subj), (ARGM, ?),
(ARG2, np#1), (ARG1, np#2)]
(g1, r1, w1) = (subj,ARG0, plunge/NN)
(g2, r2, w2) = (np,ARG0, today/NN)
(g3, r3, w3) = (np#1, ARG2, they/PRP )
(g4, r4, w4) = (np#2, ARG1, test/NN)
Figure 1: An example sentence taken from the Penn Treebank
(wsj 2417), the verb instance extracted from it, and the values
of the model variables for this instance. The semantic roles
listed are taken from the PropBank annotation, but are not
observed in the unsupervised training method.
tax. We define a small set of syntactic relations,
listed in Table 1, each of which describes a possi-
ble syntactic relationship between the verb and a
dependent. Our goal was to choose a set that pro-
vides sufficient syntactic information for the se-
mantic role decision, while remaining accurately
computable from any reasonable parse tree using
simple deterministic rules. Our set does not in-
clude the relations direct object or indirect object,
since this distinction can not be made determin-
istically on the basis of syntactic structure alone;
instead, we opted to number the noun phrase (np),
complement clause (cl, xcl), and adjectival com-
plements (acomp) appearing in an unbroken se-
quence directly after the verb, since this is suffi-
cient to capture the necessary syntactic informa-
tion. The syntactic relations used in our experi-
ments are computed from the typed dependencies
returned by the Stanford Parser (Klein and Man-
ning, 2003).
We also must choose a representation for se-
mantic roles. We allow each verb a small fixed
number of roles, in the manner similar to Prop-
Bank?s ARG0 . . . ARG5. We also designate a
single adjunct role which is shared by all verbs,
similar to PropBank?s ARGM role. We say ?sim-
ilar? because our system never observes the Prop-
Bank roles (or any human annotated semantic
roles) and so cannot possibly use the same names.
Our system assigns arbitrary integer names to the
roles it discovers, just as clustering systems give
2
1 ? j ? M
v
`
o
gj rj wj
Figure 2: A graphical representation of the verb linking
model, with example values for each variable. The rectangle
is a plate, indicating that the model contains multiple copies
of the variables shown within it: in this case, one for each
dependent j. Variables observed during learning are shaded.
arbitrary names to the clusters they discover.3
Given these definitions, we convert our parsed
corpora into a simple format: a set of verb in-
stances, each of which represents an occurrence
of a verb in a sentence. A verb instance consists of
the base form (lemma) of the observed verb, and
for each dependent of the verb, the dependent?s
syntactic relation and head word (represented as
the base form with part of speech information). An
example Penn Treebank sentence, and the verb in-
stances extracted from it, are given in Figure 1.
3 Probabilistic Model
Our learning method is based on a structured prob-
abilistic model of the domain. A graphical repre-
sentation of the model is shown in Figure 2. The
model encodes a joint probability distribution over
the elements of a single verb instance, including
the verb type, the particular linking, and for each
dependent of the verb, its syntactic relation to the
verb, semantic role, and head word.
We begin by describing the generative process
to which our model corresponds, using as our run-
ning example the instance of the verb give shown
in Figure 1. We begin by generating the verb
lemma v, in this case give. Conditioned on the
3In practice, while our system is not guaranteed to choose
role names that are consistent with PropBank, it often does
anyway, which is a consequence of the constrained form of
the linking model.
choice of verb give, we next generate a linking
`, which defines both the set of core semantic
roles to be expressed, as well as the syntactic re-
lations that express them. In our example, we
sample the ditransitive linking ` = {ARG0 ?
subj,ARG1 ? np#2, ARG2 ? np#1}. Con-
ditioned on this choice of linking, we next gen-
erate an ordered linking o, giving a final position
in the dependent list for each role and relation in
the linking `, while also optionally inserting one
or more adjunct roles. In our example, we gener-
ate the vector o = [(ARG0, subj), (ARGM, ?),
(ARG2, np#1), (ARG1, np#2)]. In doing so
we?ve specified positions for ARG0, ARG1, and
ARG2 and added one adjunct role ARGM in the
second position. Note that the length of the or-
dered linking o is equal to the total number of de-
pendents M of the verb instance. Now we iterate
through each of the dependents 1 ? j ? M , gen-
erating each in turn. For the core arguments, the
semantic role rj and syntactic relation gj are com-
pletely determined by the ordered linking o, so it
remains only to sample the syntactic relation for
the adjunct role: here we sample g2 = np. We
finish by sampling the head word of each depen-
dent, conditioned on the semantic role of that de-
pendent. In this example, we generate the head
words w1 = plunge/NN , w2 = today/NN ,
w3 = they/NN , and w4 = test/NN .
Before defining the model more formally, we
pause to justify some of the choices made in de-
signing the model. First, we chose to distinguish
between a verb?s core arguments and its adjuncts.
While core arguments must be associated with a
semantic role that is verb specific (such as the pa-
tient role of increase: the rates in our example),
adjuncts are generated by a role that is verb inde-
pendent (such as the time of a generic event: last
month in our example). Linkings include map-
pings only for the core semantic roles, resulting in
a small, focused set of possible linkings for each
verb. A consequence of this choice is that we in-
troduce uncertainty between the choice of linking
and its realization in the dependent list, which we
represent with ordered linking variable o.4
We now present the model formally as a fac-
tored joint probability distribution. We factor the
joint probability distribution into a product of the
4An alternative modeling choice would have been to add a
state variable to each dependent, indicating which of the roles
in the linking have been ?used up? by previous dependents.
3
probabilities of each instance:
P(D) =
N
?
i=1
P(vi, `i, oi,gi, ri,wi)
where we assume there are N instances, and we
have used the vector notation g to indicate the vec-
tor of variables gj for all values of j (and similarly
for r and w). We then factor the probability of
each instance using the independencies shown in
Figure 2 as follows:
P(v, `, o,g, r,w) =
P(v)P(`|v)P(o|`)
M
?
j=1
P(gj |o)P(rj |o)P(wj |rj)
where we have assumed that there are M depen-
dents of this instance. The verb v is always ob-
served in our data, so we don?t need to define
P(v). The probability of generating the linking
given the verb P(`|v) is a multinomial over pos-
sible linkings.5 Next, the probability of a partic-
ular ordering of the linking P(o|`) is determined
only by the number of adjunct dependents that are
added to o. One pays a constant penalty for each
adjunct that is added to the dependent list, but oth-
erwise all orderings of the roles are equally likely.
Formally, the ordering o is distributed according
to the geometric distribution of the difference be-
tween its length and the length of `, with constant
parameter ?.6 Next, P(gj |o) and P(rj|o) are com-
pletely deterministic for core roles: the syntactic
relation and semantic role for position j are speci-
fied in the ordering o. For adjunct roles, we gener-
ate gj from a multinomial over syntactic relations.
Finally, the word given the role P(wj |rj) is dis-
tributed as a multinomial over words.
To allow for labeling elements of verb instances
(verb types, syntactic relations, and head words) at
test time that were unobserved in the training set,
we must smooth our learned distributions. We use
Bayesian smoothing: all of the learned distribu-
tions are multinomials, so we add psuedocounts, a
generalization of the well-known add-one smooth-
ing technique. Formally, this corresponds to a
Bayesian model in which the parameters of these
multinomial distributions are themselves random
5The way in which we estimate this multinomial from
data is more complex, and is described in the next section.
6While this may seem simplistic, recall that all of the im-
portant ordering information is captured by the syntactic re-
lations.
Role Linking Operations
ARG0 Add ARG0 to subj
ARG1 No operation
Add ARG1 to np#1
Add ARG1 to cl#1
Add ARG1 to xcl#1
Add ARG1 to acomp#1
Add ARG1 to subj, replacing ARG0
ARG2 No operation
Add ARG2 to prep x, ?x
Add ARG2 to np#1, shifting ARG1 to np#2
Add ARG2 to np#1, shifting ARG1 to prep with
ARG3 No operation
Add ARG3 to prep x, ?x
Add ARG3 to cl#n, 1 < n < 3
ARG4 No operation
Add ARG4 to prep x, ?x
Table 2: The set of linking construction operations. To con-
struct a linking, select one operation from each list.
variables, distributed according to a Dirichlet dis-
tribution.7
3.1 Linking Model
The most straightforward choice of a distribution
for P(`|v) would be a multinomial over all pos-
sible linkings. There are two problems with this
simple implementation, both stemming from the
fact that the space of possible linkings is large
(there are O(|G+1||R|), where G is the set of syn-
tactic relations and R is the set of semantic roles).
First, most learning algorithms become intractable
when they are required to represent uncertainty
over such a large space. Second, the large space
of linkings yields a large space of possible mod-
els, making learning more difficult.
As a consequence, we have two objectives when
designing P(`|v): (1) constrain the set of linkings
for each verb to a set of tractable size which are
linguistically plausible, and (2) facilitate the con-
struction of a structured prior distribution over this
set, which gives higher weight to linkings that are
known to be more common. Our solution is to
model the derivation of each linking as a sequence
of construction operations, an idea which is sim-
ilar in spirit to that used by Eisner (2001). Each
operation adds a new role to the linking, possibly
replacing or displacing one of the existing roles.
The complete list of linking operations is given in
Table 2. To build a linking we select one opera-
tion from each list; the presence of a no-operation
for each role means that a linking doesn?t have to
include all roles. Note that this linking derivation
process is not shown in Figure 2, since it is possi-
7For a more detailed presentation of Bayesian methods,
see Gelman et al (2003).
4
ble to compile the resulting distribution over link-
ings into the simpler multinomial P(`|v).
More formally, we factor P(`|v) as follows,
where c is the vector of construction operations
used to build `:
P(`|v) =
?
c
P(`|c)P(c|v)
=
?
c
|R|
?
i=1
P(ci|v)
Note that in the second step we drop the term
P(`|c) since it is always 1 (a sequence of opera-
tions leads deterministically to a linking).
Given this derivation process, it is easy to cre-
ated a structured prior: we just place pseudocounts
on the operations that are likely a priori across
all verbs. We place high pseudocounts on the
no-operations (which preserve simple intransitive
and transitive structure) and low pseudocounts on
all the rest. Note that the use of this structured
prior has another desired side effect: it breaks the
symmetry of the role names (because some link-
ings more likely than others) which encourages the
model to adhere to canonical role naming conven-
tions, at least for commonly occurring roles like
ARG0 and ARG1.
The design of the linking model does incorpo-
rate prior knowledge about the structure of verb
linkings and diathesis alternations. Indeed, the
linking model provides a weak form of Univer-
sal Grammar, encoding the kinds of linking pat-
terns that are known to occur in human languages.
While not fully developed as a model of cross-
linguistic verb argument realization, the model is
not very English specific. It provides a not-very-
constrained theory of alternations that captures
common cross-linguistic patterns. Finally, though
we do encode knowledge in the form of the model
structure and associated prior distributions, note
that we do not provide any verb-specific knowl-
edge; this is left to the learning algorithm.
4 Learning
Our goal in learning is to find parameter settings of
our model which are likely given the data. Using
? to represent the vector of all model parameters,
if our data were fully observed, we could express
our learning problem as
?? = argmax
?
P(?|D) = argmax
?
N
?
i=1
P(di; ?)
= argmax
?
N
?
i=1
P(vi, `i, oi,gi, ri,wi; ?)
Because of the factorization of the joint distri-
bution, this learning task would be trivial, com-
putable in closed form from relative frequency
counts. Unfortunately, in our training set the vari-
ables `, o and r are hidden (not observed), leaving
us with a much harder optimization problem:
?? = argmax
?
N
?
i=0
P(vi,gi,wi; ?)
= argmax
?
N
?
i=0
?
`i,oi,ri
P(vi, `i, oi,gi, ri,wi; ?)
In other words, we want model parameters which
maximize the expected likelihood of the observed
data, where the expectation is taken over the
hidden variables for each instance. Although
it is intractable to find exact solutions to opti-
mization problems of this form, the Expectation-
Maximization (EM) algorithm is a greedy search
procedure over the parameter space which is guar-
anteed to increase the expected likelihood, and
thus find a local maximum of the function.
While the M-step is clearly trivial, the E-step
at first looks more complex: there are three hid-
den variables for each instance, `, o, and r, each of
which can take an exponential number of values.
Note however, that conditioned on the observed
set of syntactic relations g, the variables ` and o
are completely determined by a choice of roles r
for each dependent. So to represent uncertainty
over these variables, we need only to represent a
distribution over possible role vectors r. Though
in the worst case the set of possible role vectors is
still exponential, we only need role vectors that are
consistent with both the observed list of syntactic
relations and a linking that can be generated by
the construction operations. Empirically the num-
ber of linkings is small (less than 50) for each of
the observed instances in our data sets.
Then for each instance we construct a condi-
tional probability distribution over this set, which
5
is computable in terms of the model parameters:
P(r, `r, or, |v,g,w) ?
P(`r|v)P(or|`r)
M
?
j=1
P(gj |or)P(rj |or)P(wj |rj)
We have denoted as `r and or the values of ` and
o that are determined by each choice of r.
To make EM work, there are a few additional
subtleties. First, because EM is a hill-climbing al-
gorithm, we must initialize it to a point in parame-
ter space with slope (and without symmetries). We
do so by adding a small amount of noise: for each
dependent of each verb, we add a fractional count
of 10?6 to the word distribution of a semantic role
selected at random. Second, we must choose when
to stop EM: we run until the relative change in data
log likelihood is less than 10?4.
A separate but important question is how well
EM works for finding ?good? models in the space
of possible parameter settings. ?Good? models are
ones which list linkings for each verb that corre-
spond to linguists? judgments about verb linking
behavior. Recall that EM is guaranteed only to
find a local maximum of the data likelihood func-
tion. There are two reasons why a particular maxi-
mum might not be a ?good? model. First, because
it is a greedy procedure, EM might get stuck in lo-
cal maxima, and be unable to find other points in
the space that have much higher data likelihood.
We take the traditional approach to this problem,
which is to use random restarts; however empir-
ically there is very little variance over runs. A
deeper problem is that data likelihood may not cor-
respond well to a linguist?s assessment of model
quality. As evidence that this is not the case, we
have observed a strong correlation between data
log likelihood and labeling accuracy.
5 Datasets and Evaluation
We train our models with verb instances ex-
tracted from three parsed corpora: (1) the Wall
Street Journal section of the Penn Treebank (PTB),
which was parsed by human annotators (Marcus et
al., 1993), (2) the Brown Laboratory for Linguis-
tic Information Processing corpus of Wall Street
Journal text (BLLIP), which was parsed automat-
ically by the Charniak parser (Charniak, 2000),
and (3) the Gigaword corpus of raw newswire text
(GW), which we parsed ourselves with the Stan-
ford parser. In all cases, when training a model,
Coarse Roles Core Roles
Sec. 23 P R F1 P R F1
ID Only .957 .802 .873 .944 .843 .891
CL Only
Baseline .856 .856 .856 .975 .820 .886
PTB Tr. .889 .889 .889 .928 .898 .911
1000 Tr. .897 .897 .897 .947 .898 .920
ID+CL
Baseline .819 .686 .747 .920 .691 .789
PTB Tr. .851 .712 .776 .876 .757 .812
1000 Tr. .859 .719 .783 .894 .757 .820
Sec. 24 P R F1 P R F1
ID Only .954 .788 .863 .941 .825 .879
CL Only
Baseline .844 .844 .844 .980 .810 .882
PTB Tr. .893 .893 .893 .940 .903 .920
1000 Tr. .899 .899 .899 .956 .898 .925
ID+CL
Baseline .804 .665 .729 .922 .668 .775
PTB Tr. .852 .704 .771 .885 .745 .809
1000 Tr. .858 .709 .776 .900 .741 .813
Table 3: Summary of results on labeling verb instances
in PropBank Section 23 and Section 24 for semantic role.
Learned results are averaged over 5 runs.
we specify a set of target verb types (e.g., the ones
in the test set), and build a training set by adding a
fixed number of instances of each verb type from
the PTB, BLLIP, and GW data sets, in that order.
For the semantic role labeling evaluation, we
use our system to label the dependents of unseen
verb instances for semantic role. We use the sen-
tences in PTB section 23 for testing, and PTB sec-
tion 24 for development. The development set
consists of 2507 verb instances and 833 different
verb types, and the test set consists of 4269 verb
instances and 1099 different verb types. Free pa-
rameters were tuned on the development set, and
the test set was only used for final experiments.
Because we do not observe the gold standard
semantic roles at training time, we must choose
an alignment between the guessed labels and the
gold labels. We do so optimistically, by choos-
ing the gold label for each guessed label which
maximizes the number of correct guesses. This is
a well known approach to evaluation in unsuper-
vised learning: when it is used to compute accu-
racy, the resulting metric is sometimes called clus-
ter purity. While this amounts to ?peeking? at the
answers before evaluation, the amount of human
knowledge that is given to the system is small: it
corresponds to the effort required to hand assign a
?name? to each label that the system proposes.
As is customary, we divide the problem into
two subtasks: identification (ID) and classifica-
tion (CL). In the identification task, we identify
the set of constituents which fill some role for a
6
    
    
    
    
    
    
    
    	
    

                     
               


Proceedings of the Workshop on Textual Entailment and Paraphrasing, pages 165?170,
Prague, June 2007. c?2007 Association for Computational Linguistics
Learning Alignments and Leveraging Natural Logic
Nathanael Chambers, Daniel Cer, Trond Grenager, David Hall, Chloe Kiddon
Bill MacCartney, Marie-Catherine de Marneffe, Daniel Ramage
Eric Yeh, Christopher D. Manning
Computer Science Department
Stanford University
Stanford, CA 94305
{natec,dcer,grenager,dlwh,loeki,wcmac,mcdm,dramage,yeh1,manning}@stanford.edu
Abstract
We describe an approach to textual infer-
ence that improves alignments at both the
typed dependency level and at a deeper se-
mantic level. We present a machine learning
approach to alignment scoring, a stochas-
tic search procedure, and a new tool that
finds deeper semantic alignments, allowing
rapid development of semantic features over
the aligned graphs. Further, we describe a
complementary semantic component based
on natural logic, which shows an added gain
of 3.13% accuracy on the RTE3 test set.
1 Introduction
Among the many approaches to textual inference,
alignment of dependency graphs has shown utility
in determining entailment without the use of deep
understanding. However, discovering alignments
requires a scoring function that accurately scores
alignment and a search procedure capable of approx-
imating the optimal mapping within a large search
space. We address the former requirement through
a machine learning approach for acquiring lexical
feature weights, and we address the latter with an
approximate stochastic approach to search.
Unfortunately, the most accurate aligner can-
not capture deeper semantic relations between two
pieces of text. For this, we have developed a tool,
Semgrex, that allows the rapid development of de-
pendency rules to find specific entailments, such as
familial or locative relations, a common occurence
in textual entailment data. Instead of writing code by
hand to capture patterns in the dependency graphs,
we develop a separate rule-base that operates over
aligned dependency graphs. Further, we describe a
separate natural logic component that complements
our textual inference system, making local entail-
ment decisions based on monotonic assumptions.
The next section gives a brief overview of the sys-
tem architecture, followed by our proposal for im-
proving alignment scoring and search. New coref-
erence features and the Semgrex tool are then de-
scribed, followed by a description of natural logic.
2 System Overview
Our system is a three stage architecture that con-
ducts linguistic analysis, builds an alignment be-
tween dependency graphs of the text and hypothesis,
and performs inference to determine entailment.
Linguistic analysis identifies semantic entities, re-
lationships, and structure within the given text and
hypothesis. Typed dependency graphs are passed
to the aligner, as well as lexical features such as
named entities, synonymity, part of speech, etc. The
alignment stage then performs dependency graph
alignment between the hypothesis and text graphs,
searching the space of possible alignments for the
highest scoring alignment. Improvements to the
scorer, search algorithm, and automatically learned
weights are described in the next section.
The final inference stage determines if the hy-
pothesis is entailed by the text. We construct a set
of features from the previous stages ranging from
antonyms and polarity to graph structure and seman-
tic relations. Each feature is weighted according to a
set of hand-crafted or machine-learned weights over
165
the development dataset. We do not describe the fea-
tures here; the reader is referred to de Marneffe et al
(2006a) for more details. A novel component that
leverages natural logic is also used to make the final
entailment decisions, described in section 6.
3 Alignment Model
We examine three tasks undertaken to improve the
alignment phase: (1) the construction of manu-
ally aligned data which enables automatic learning
of alignment models, and effectively decouples the
alignment and inference development efforts, (2) the
development of new search procedures for finding
high-quality alignments, and (3) the use of machine
learning techniques to automatically learn the pa-
rameters of alignment scoring models.
3.1 Manual Alignment Annotation
While work such as Raina et al (2005) has tried
to learn feature alignment weights by credit assign-
ment backward from whether an item is answered
correctly, this can be very difficult, and here we fol-
low Hickl et al (2006) in using supervised gold-
standard alignments, which help us to evaluate and
improve alignment and inference independently.
We built a web-based tool that allows annotators
to mark semantic relationships between text and hy-
pothesis words. A table with the hypothesis words
on one axis and the text on the other allows re-
lationships to be marked in the corresponding ta-
ble cell with one of four options. These relation-
ships include text to hypothesis entailment, hypothe-
sis to text entailment, synonymy, and antonymy. Ex-
amples of entailment (from the RTE 2005 dataset)
include pairs such as drinking/consumption, coro-
navirus/virus, and Royal Navy/British. By distin-
guishing between these different types of align-
ments, we can capture some limited semantics in the
alignment process, but full exploitation of this infor-
mation is left to future work.
We annotated the complete RTE2 dev and
RTE3 dev datasets, for a total of 1600 aligned
text/hypothesis pairs (the data is available at
http://nlp.stanford.edu/projects/rte/).
3.2 Improving Alignment Search
In order to find ?good? alignments, we define both a
formal model for scoring the quality of a proposed
alignment and a search procedure over the alignment
space. Our goal is to build a model that maximizes
the total alignment score of the full datasetD, which
we take to be the sum of the alignment scores for all
individual text/hypothesis pairs (t, h).
Each of the text and hypothesis is a semantic de-
pendency graph; n(h) is the set of nodes (words)
and e(h) is the set of edges (grammatical relations)
in a hypothesis h. An alignment a : n(h) 7? n(t) ?
{null} maps each hypothesis word to a text word
or to a null symbol, much like an IBM-style ma-
chine translation model. We assume that the align-
ment score s(t, h, a) is the sum of two terms, the first
scoring aligned word pairs and the second the match
between an edge between two words in the hypoth-
esis graph and the corresponding path between the
words in the text graph. Each of these is a sum, over
the scoring function for individual word pairs sw and
the scoring function for edge path pairs se:
s(t, h, a) =
?
hi?n(h)
sw(hi, a(hi))
+
?
(hi,hj)?e(h)
se((hi, hj), (a(hi), a(hj)))
The space of alignments for a hypothesis with m
words and a text with n words contains (n + 1)m
possible alignments, making exhaustive search in-
tractable. However, since the bulk of the alignment
score depends on local factors, we have explored
several search strategies and found that stochastic
local search produces better quality solutions.
Stochastic search is inspired by Gibbs sampling
and operates on a complete state formulation of the
search problem. We initialize the algorithm with the
complete alignment that maximizes the greedy word
pair scores. Then, in each step of the search, we
seek to randomly replace an alignment for a single
hypothesis word hi. For each possible text word tj
(including null), we compute the alignment score if
we were to align hi with tj . Treating these scores as
log probabilities, we create a normalized distribution
from which we sample one alignment. This Gibbs
sampler is guaranteed to give us samples from the
posterior distribution over alignments defined im-
plicitly by the scoring function. As we wish to find a
maximum of the function, we use simulated anneal-
ing by including a temperature parameter to smooth
166
the sampling distribution as a function of time. This
allows us to initially explore the space widely, but
later to converge to a local maximum which is hope-
fully the global maximum.
3.3 Learning Alignment Models
Last year, we manually defined the alignment scor-
ing function (de Marneffe et al, 2006a). However,
the existence of the gold standard alignments de-
scribed in section 3.1 enables the automatic learning
of a scoring function. For both the word and edge
scorers, we choose a linear model where the score is
the dot product of a feature and a weight vector:
sw(hi, tj) = ?w ? f(hi, tj), and
se((hi, hj), (tk, t`)) = ?e ? f((hi, hj), (tk, t`)).
Recent results in machine learning show the ef-
fectiveness of online learning algorithms for struc-
ture prediction tasks. Online algorithms update their
model at each iteration step over the training set. For
each datum, they use the current weight vector to
make a prediction which is compared to the correct
label. The weight vector is updated as a function
of the difference. We compared two different up-
date rules: the perceptron update and the MIRA up-
date. In the perceptron update, for an incorrect pre-
diction, the weight vector is modified by adding a
multiple of the difference between the feature vector
of the correct label and the feature vector of the pre-
dicted label. We use the adaptation of this algorithm
to structure prediction, first proposed by (Collins,
2002). TheMIRA update is a proposed improvement
that attempts to make the minimal modification to
the weight vector such that the score of the incorrect
prediction for the example is lower than the score of
the correct label (Crammer and Singer, 2001).
We compare the performance of the perceptron
and MIRA algorithms on 10-fold cross-validation
on the RTE2 dev dataset. Both algorithms improve
with each pass over the dataset. Most improve-
ment is within the first five passes. Table 1 shows
runs for both algorithms over 10 passes through the
dataset. MIRA consistently outperforms perceptron
learning. Moreover, scoring alignments based on the
learned weights marginally outperforms our hand-
constructed scoring function by 1.7% absolute.
A puzzling problem is that our overall per-
formance decreased 0.87% with the addition of
Perfectly aligned
Individual words Text/hypothesis pairs
Perceptron 4675 271
MIRA 4775 283
Table 1: Perceptron and MIRA results on 10-fold cross-
validation on RTE2 dev for 10 passes.
RTE3 dev alignment data. We believe this is due
to a larger proportion of ?irrelevant? and ?relation?
pairs. Irrelevant pairs are those where the text and
hypothesis are completely unrelated. Relation pairs
are those where the correct entailment judgment re-
lies on the extraction of relations such as X works
for Y, X is located in Y, or X is the wife of Y. Both
of these categories do not rely on alignments for en-
tailment decisions, and hence introduce noise.
4 Coreference
In RTE3, 135 pairs in RTE3 dev and 117 in
RTE3 test have lengths classified as ?long,? with
642 personal pronouns identified in RTE3 dev and
504 in RTE3 test. These numbers suggest that re-
solving pronomial anaphora plays an important role
in making good entailment decisions. For exam-
ple, identifying the first ?he? as referring to ?Yunus?
in this pair from RTE3 dev can help alignment and
other system features.
P: Yunus, who shared the 1.4 million prize Friday with the
Grameen Bank that he founded 30 years ago, pioneered the con-
cept of ?microcredit.?
H: Yunus founded the Grameen Bank 30 years ago.
Indeed, 52 of the first 200 pairs from RTE3 dev
were deemed by a human evaluator to rely on ref-
erence information. We used the OpenNLP1 pack-
age?s maximum-entropy coreference utility to per-
form resolution on parse trees and named-entity data
from our system. Found relations are stored and
used by the alignment stage for word similarity.
We evaluated our system with and without coref-
erence over RTE3 dev and RTE3 test. Results are
shown in Table 3. The presence of reference infor-
mation helped, approaching significance on the de-
velopment set (p < 0.1, McNemar?s test, 2-tailed),
but not on the test set. Examination of alignments
and features between the two runs shows that the
alignments do not differ significantly, but associated
1http://opennlp.sourceforge.net/
167
weights do, thus affecting entailment threshold tun-
ing. We believe coreference needs to be integrated
into all the featurizers and lexical resources, rather
than only with word matching, in order to make fur-
ther gains.
5 Semgrex Language
A core part of an entailment system is the ability to
find semantically equivalent patterns in text. Pre-
viously, we wrote tedious graph traversal code by
hand for each desired pattern. As a remedy, we
wrote Semgrex, a pattern language for dependency
graphs. We use Semgrex atop the typed dependen-
cies from the Stanford Parser (de Marneffe et al,
2006b), as aligned in the alignment phase, to iden-
tify both semantic patterns in a single text and over
two aligned pieces of text. The syntax of the lan-
guage was modeled after tgrep/Tregex, query lan-
guages used to find syntactic patterns in trees (Levy
and Andrew, 2006). This speeds up the process of
graph search and reduces errors that occur in com-
plicated traversal code.
5.1 Semgrex Features
Rather than providing regular expression match-
ing of atomic tree labels, as in most tree pattern
languages, Semgrex represents nodes as a (non-
recursive) attribute-value matrix. It then uses regular
expressions for subsets of attribute values. For ex-
ample, {word:run;tag:/?NN/} refers to any
node that has a value run for the attribute word and
a tag that starts with NN, while {} refers to any node
in the graph.
However, the most important part of Semgrex is
that it allows you to specify relations between nodes.
For example, {} <nsubj {} finds all the depen-
dents of nsubj relations. Logical connectives can
be used to form more complex patterns and node
naming can help retrieve matched nodes from the
patterns. Four base relations, shown in figure 1, al-
low you to specify the type of relation between two
nodes, in addition to an alignment relation (@) be-
tween two graphs.
5.2 Entailment Patterns
A particularly useful application of Semgrex is to
create relation entailment patterns. In particular, the
IE subtask of RTE has many characteristics that are
Semgrex Relations
Symbol #Description
{A} >reln {B} A is the governor of a reln relation
with B
{A} <reln {B} A is the dependent of a reln relation
with B
{A} >>reln {B} A dominates a node that is the
governor of a reln relation with B
{A} <<reln {B} A is the dependent of a node that is
dominated by B
{A} @ {B} A aligns to B
Figure 1: Semgrex relations between nodes.
not well suited to the core alignment features of our
system. We began integrating Semgrex into our sys-
tem by creating semantic alignment rules for these
IE tasks.
T: Bill Clinton?s wife Hillary was in Wichita today, continuing
her campaign.
H: Bill Clinton is married to Hillary. (TRUE)
Pattern:
({}=1
<nsubjpass ({word:married} >pp to {}=2))
@ ({} >poss ({lemma:/wife/} >appos {}=3))
This is a simplified version of a pattern that looks
for marriage relations. If it matches, additional pro-
grammatic checks ensure that the nodes labeled 2
and 3 are either aligned or coreferent. If they are,
then we add a MATCH feature, otherwise we add a
MISMATCH. Patterns included other familial rela-
tions and employer-employee relations. These pat-
terns serve both as a necessary component of an IE
entailment system and as a test drive of Semgrex.
5.3 Range of Application
Our rules for marriage relations correctly matched
six examples in the RTE3 development set and one
in the test set. Due to our system?s weaker per-
formance on the IE subtask of the data, we ana-
lyzed 200 examples in the development set for Sem-
grex applicability. We identified several relational
classes, including the following:
? Work: works for, holds the position of
? Location: lives in, is located in
? Relative: wife/husband of, are relatives
? Membership: is an employee of, is part of
? Business: is a partner of, owns
? Base: is based in, headquarters in
These relations make up at least 7% of the data, sug-
gesting utility from capturing other relations.
168
6 Natural Logic
We developed a computational model of natural
logic, the NatLog system, as another inference en-
gine for our RTE system. NatLog complements our
core broad-coverage system by trading lower recall
for higher precision, similar to (Bos and Markert,
2006). Natural logic avoids difficulties with translat-
ing natural language into first-order logic (FOL) by
forgoing logical notation and model theory in favor
of natural language. Proofs are expressed as incre-
mental edits to natural language expressions. Edits
represent conceptual contractions and expansions,
with truth preservation specified natural logic. For
further details, we refer the reader to (Sa?nchez Va-
lencia, 1995).
We define an entailment relation v between
nouns (hammer v tool), adjectives (deafening v
loud), verbs (sprint v run), modifiers, connectives
and quantifiers. In ordinary (upward-monotone)
contexts, the entailment relation between compound
expressions mirrors the entailment relations be-
tween their parts. Thus tango in Paris v dance
in France, since tango v dance and in Paris v in
France. However, many linguistic constructions cre-
ate downward-monotone contexts, including nega-
tion (didn?t sing v didn?t yodel), restrictive quanti-
fiers (few beetles v few insects) and many others.
NatLog uses a three-stage architecture, compris-
ing linguistic pre-processing, alignment, and entail-
ment classification. In pre-processing, we define a
list of expressions that affect monotonicity, and de-
fine Tregex patterns that recognize each occurrence
and its scope. This monotonicity marking can cor-
rectly account for multiple monotonicity inversions,
as in no soldier without a uniform, and marks each
token span with its final effective monotonicity.
In the second stage, word alignments from our
RTE system are represented as a sequence of atomic
edits over token spans, as entailment relations
are described across incremental edits in NatLog.
Aligned pairs generate substitution edits, unaligned
premise words yield deletion edits, and unaligned
hypothesis words yield insertion edits. Where pos-
sible, contiguous sequences of word-level edits are
collected into span edits.
In the final stage, we use a decision-tree classi-
fier to predict the elementary entailment relation (ta-
relation symbol in terms of v RTE
equivalent p = h p v h, h v p yes
forward p < h p v h, h 6v p yes
reverse p = h h v p, p 6v h no
independent p # h p 6v h, h 6v p no
exclusive p | h p v ?h, h v ?p no
Table 2: NatLog?s five elementary entailment relations. The last
column indicates correspondences to RTE answers.
ble 2) for each atomic edit. Edit features include
the type, effective monotonicity at affected tokens,
and their lexical features, including syntactic cate-
gory, lemma similarity, and WordNet-derived mea-
sures of synonymy, hyponymy, and antonymy. The
classifier was trained on a set of 69 problems de-
signed to exercise the feature space, learning heuris-
tics such as deletion in an upward-monotone context
yields<, substitution of a hypernym in a downward-
monotone context yields =, and substitution of an
antonym yields |.
To produce a top-level entailment judgment, the
atomic entailment predictions associated with each
edit are composed in a fairly obvious way. If r is any
entailment relation, then (= ? r) ? r, but (# ? r) ?
#. < and= are transitive, but (< ? =) ? #, and so
on.
We do not expect NatLog to be a general-purpose
solution for RTE problems. Many problems depend
on types of inference that it does not address, such
as paraphrase or relation extraction. Most pairs have
large edit distances, and more atomic edits means
a greater chance of errors propagating to the final
output: given the entailment composition rules, the
system can answer yes only if all atomic-level pre-
dictions are either< or =. Instead, we hope to make
reliable predictions on a subset of the RTE problems.
Table 3 shows NatLog performance on RTE3. It
makes positive predictions on few problems (18%
on development set, 24% on test), but achieves good
precision relative to our RTE system (76% and 68%,
respectively). For comparison, the FOL-based sys-
tem reported in (Bos and Markert, 2006) attained a
precision of 76% on RTE2, but made a positive pre-
diction in only 4% of cases. This high precision sug-
gests that superior performance can be achieved by
hybridizing NatLog with our core RTE system.
The reader is referred to (MacCartney and Man-
169
ID Premise(s) Hypothesis Answer
518 The French railway company SNCF is cooperating in
the project.
The French railway company is called SNCF. yes
601 NUCOR has pioneered a giant mini-mill in which steel
is poured into continuous casting machines.
Nucor has pioneered the first mini-mill. no
Table 4: Illustrative examples from the RTE3 test suite
RTE3 Development Set (800 problems)
System % yes precision recall accuracy
Core +coref 50.25 68.66 66.99 67.25
Core -coref 49.88 66.42 64.32 64.88
NatLog 18.00 76.39 26.70 58.00
Hybrid, bal. 50.00 69.75 67.72 68.25
Hybrid, opt. 55.13 69.16 74.03 69.63
RTE3 Test Set (800 problems)
System % yes precision recall accuracy
Core +coref 50.00 61.75 60.24 60.50
Core -coref 50.00 60.25 58.78 59.00
NatLog 23.88 68.06 31.71 57.38
Hybrid, bal. 50.00 64.50 62.93 63.25
Hybrid, opt. 54.13 63.74 67.32 63.62
Table 3: Performance on the RTE3 development and test sets.
% yes indicates the proportion of yes predictions made by the
system. Precision and recall are shown for the yes label.
ning, 2007) for more details on NatLog.
7 System Results
Our core systemmakes yes/no predictions by thresh-
olding a real-valued inference score. To construct
a hybrid system, we adjust the inference score by
+x if NatLog predicts yes, ?x otherwise. x is cho-
sen by optimizing development set accuracy when
adjusting the threshold to generate balanced predic-
tions (equal numbers of yes and no). As another
experiment, we fix x at this value and adjust the
threshold to optimize development set accuracy, re-
sulting in an excess of yes predictions. Results for
these two cases are shown in Table 3. Parameter
values tuned on development data yielded the best
performance. The optimized hybrid system attained
an absolute accuracy gain of 3.12% over our RTE
system, corresponding to an extra 25 problems an-
swered correctly. This result is statistically signifi-
cant (p < 0.01, McNemar?s test, 2-tailed).
The gain cannot be fully attributed to NatLog?s
success in handling the kind of inferences about
monotonicity which are the staple of natural logic.
Indeed, such inferences are quite rare in the RTE
data. Rather, NatLog seems to have gained primarily
by being more precise. In some cases, this precision
works against it: NatLog answers no to problem 518
(table 4) because it cannot account for the insertion
of called. On the other hand, it correctly rejects the
hypothesis in problem 601 because it cannot account
for the insertion of first, whereas the less-precise
core system was happy to allow it.
Acknowledgements
This material is based upon work supported in
part by the Disruptive Technology Office (DTO)?s
AQUAINT Phase III Program.
References
Johan Bos and Katja Markert. 2006. When logical inference
helps determining textual entailment (and when it doesn?t).
In Proceedings of the Second PASCAL RTE Challenge.
Michael Collins. 2002. Discriminative training methods for
hidden markov models: Theory and experiments with per-
ceptron algorithms. In Proceedings of EMNLP-2002.
Koby Crammer and Yoram Singer. 2001. Ultraconservative
online algorithms for multiclass problems. In Proceedings
of COLT-2001.
Marie-Catherine de Marneffe, Bill MacCartney, Trond Grena-
ger, Daniel Cer, Anna Rafferty, and Christopher D. Manning.
2006a. Learning to distinguish valid textual entailments. In
Second Pascal RTE Challenge Workshop.
Marie-Catherine de Marneffe, Bill MacCartney, and Christo-
pher D. Manning. 2006b. Generating typed dependency
parses from phrase structure parses. In 5th Int. Conference
on Language Resources and Evaluation (LREC 2006).
Andrew Hickl, John Williams, Jeremy Bensley, Kirk Roberts,
Bryan Rink, and Ying Shi. 2006. Recognizing textual entail-
ment with LCC?s GROUNDHOG system. In Proceedings of
the Second PASCAL RTE Challenge.
Roger Levy and Galen Andrew. 2006. Tregex and Tsurgeon:
tools for querying and manipulating tree data structures. In
Proceedings of the Fifth International Conference on Lan-
guage Resources and Evaluation.
Bill MacCartney and Christopher D. Manning. 2007. Natu-
ral logic for textual inference. In ACL Workshop on Textual
Entailment and Paraphrasing.
Rajat Raina, Andrew Y. Ng, and Christopher D. Manning. 2005.
Robust textual inference via learning and abductive reason-
ing. In AAAI 2005, pages 1099?1105.
Victor Sa?nchez Valencia. 1995. Parsing-driven inference: Nat-
ural logic. Linguistic Analysis, 25:258?285.
170
