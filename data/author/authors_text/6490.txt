A Divide-and-Conquer Strategy for Shallow Parsing of German 
Free Texts 
Gi in ter  Neumann*  Chr i s t ian  Braun t Jakub  P i skorsk i  ~ 
Abst ract  
We present a divide-and-conquer st ategy based on 
finite state technology for shallow parsing of real- 
world German texts. In a first phase only the topo- 
logical structure of a sentence (i.e., verb groups, 
subclauses) are determined. In a second phase the 
phrasal grammars are applied to the contents of the 
different fields of the main and sub-clauses. Shallow 
parsing is supported by suitably configured prepro- 
cessing, including: morphological and on-line com- 
pound analysis, efficient POS-filtering, and named 
entity recognition. The whole approach proved to 
be very useful for processing of free word order lan- 
guages like German. Especially for the divide-and- 
conquer parsing strategy we obtained an f-measure 
of 87.14% on unseen data. 
1 In t roduct ion  
Current information extraction (IE) systems are 
quite successful in efficient processing of large free 
text collections due to the fact that they can provide 
a partial understanding of specific types of text with 
a certain degree of partial accuracy using fast and ro- 
bust language processing strategies (basically finite 
state technology). They have been "made sensitive" 
to certain key pieces of information and thereby pro- 
vide an easy means to skip text without deep anal- 
ysis. The majority of existing IE systems are ap- 
plied to English text, but there are now a number of 
systems which process other languages as well (e.g., 
German (Neumann et al, 1997), Italian (Ciravegna 
et al, 1999) or Japanese (Sekine and Nobata, 1998)). 
The majority of current systems perform a partial 
parsing approach using only very few general syntac- 
tic knowledge for the identification of nominal and 
prepositional phrases and verb groups. The combi- 
nation of such units is then performed by means of 
domain-specific templates. Usually, these templates 
* DFKI GmbH, Stuhlsatzenhausweg 3, 66123 Saarbriicken, 
Germany, neumann@dfki, de 
t DFKI GmbH, Stuhlsatzenhausweg 3, 66123 Saarbriicken, 
Germany, cbratm@dfki, de 
DFKI GmbH, Stuhlsatzenhausweg 3, 66123 Saarbriicken, 
Germany, piskorsk@dfki, de 
are triggered by domain-specific predicates attached 
only to a relevant subset of verbs which express 
domain-specific selectional restrictions for possible 
argument fillers. 
In most of the well-known shallow text process- 
ing systems (cf. (Sundheim, 1995) and (SAIC, 
1998)) cascaded chunk parsers are used which per- 
form clause recognition after fragment recognition 
following a bottom-up style as described in (Abne), 
1996). We have also developed a similar bottom- 
up strategy for the processing of German texts, cf. 
(Neumann et al, 1997). However, the main prob- 
lem we experienced using the bottom-up strategy 
was insufficient robustness: because the parser de- 
pends on the lower phrasal recognizers, its perfor- 
mance is heavily influenced by their respective per- 
formance. As a consequence, the parser frequently 
wasn't able to process tructurally simple sentences, 
because they contained, for example, highly complex 
nominal phrases, as in the following example: 
"\[Die vom Bundesgerichtshof und den 
Wettbewerbshfitern als Verstofi gegen 
das Kartellverbot gegeiflelte zentrale TV- 
Vermarktung\] ist g~ngige Praxis." 
Central television raarketing, censured by the 
German Federal High Court and the guards 
against unfair competition as an infringement 
of anti-cartel legislation, is common practice. 
During free text processing it might be not possible 
(or even desirable) to recognize such a phrase com- 
pletely. However, if we assume that domain-specific 
templates are associated with certain verbs or verb 
groups which trigger template filling, then it will be 
very difficult to find the appropriate fillers without 
knowing the correct clause structure. Furthermore 
in a sole bottom-up approach some ambiguities - for 
example relative pronouns - can't be resolved with- 
out introducing much underspecification into the in- 
termediate structures. 
Therefore we propose the following divide-and- 
conquer parsing strategy: In a first phase only 
the verb groups and the topological structure of 
a sentence according to the linguistic field the- 
239 
"\[CooraS \[sse,,* Diese Angaben konnte der Bundes- 
grenzschutz aber nicht best~itigen\], [ssent Kinkel 
sprach von Horrorzahlen, \[relct denen er keinen 
Glauben schenke\]\]\]." 
This information couldn't be verified by the Border 
Police, Kinkel spoke of horrible figures that he didn't 
believe. 
Figure 1: An example of a topological structure. 
PREPROCESSOR 
-TokeNlatl~ 
, v .  
! 
DC-PARSER 
TOPOt.OGICAL P ~  
v , ,~ ,~,~ .. 
FRAGMENT RECOGNIZER 
Underspeclfied dependency trees 
Figure 2: Overview of the system's architecture. 
ory (cf. (Engel, 1988)) are determined omain- 
independently. In a second phase, general (as well 
as domain-specific) phrasal grammars (nominal and 
prepositional phrases) are applied to the contents of 
the different fields of the main and sub-clauses ( ee 
fig. 1) 
This approach offers several advantages: 
? improved robustness, because parsing of the 
sentence topology is based only on simple in- 
dicators like verbgroups and conjunctions and 
their interplay, 
? the resolution of some ambiguities, including 
relative pronouns vs. determiner, sub junction 
vs. preposition and sentence coordination vs. 
NP coordination, and 
? a high degree of modularity (easy integration of 
domain-dependent subcomponents). 
The shallow divide-and-conquer parser (DC- 
PARSER) is supported by means of powerful mor- 
phological processing (including on-line compound 
analysis), efficient POS-filtering and named entity 
recognition. Thus the architecture of the complete 
shallow text processing approach consists basically 
of two main components: the preprocessor and the 
DC-PARSER itself (see fig. 2). 
2 Preprocessor  
The DC-PARSER relies on a suitably configured pre- 
processing strategy in order to achieve the desired 
simplicity and performance. It consists of the fol- 
lowing main steps: 
Tokenization The tokenizer maps sequences of 
consecutive characters into larger units called tokens 
and identifies their types. Currently we use more 
than 50 domain-independent token classes including 
generic classes for semantically ambiguous tokens 
(e.g., "10:15" could be a time expression or volley- 
ball result, hence we classify this token as number- 
dot compound) and complex classes like abbrevia- 
tions or complex compounds (e.g., "AT&T-Chief"). 
It proved that such variety of token classes impli- 
fies the processing of subsequent submodules signif- 
icantly. 
Morphology Each token identified as a potential 
wordform is submitted to the morphological nalysis 
including on-line recognition of compounds (which is 
crucial since compounding is a very productive pro- 
cess of the German language) and hyphen coordina- 
tion (e.g., in "An- und Verkauf" (purchase and sale) 
"An-" is resolved to "Ankauf" (purchase)). Each 
token recognized as a valid word form is associated 
with the list of its possible readings, characterized 
by stem, inflection information and part of speech 
category. 
POS-fi ltering Since a high amount of German 
word forms is ambiguous, especially word forms with 
a verb reading 1 and due to the fact that the qual- 
ity of the results of the DC-PARSER relies essen- 
tially on the proper recognition of verb groups, ef- 
ficient disambiguation strategies are needed. Using 
case-sensitive rules is straightforward since generally 
only nouns (and proper names) are written in stan- 
dard German with a capitalized initial letter (e.g., 
"das Unternehmen" - the enterprise vs. "wir un- 
ternehmen" - we undertake). However for disam- 
biguation of word forms appearing at the beginning 
of the sentence local contextual filtering rules are 
applied. For instance, the rule which forbids the 
verb written with a capitalized initial letter to be 
followed by a finite verb would filter out the verb 
reading of the word "unternehmen" i  the sentence 
130% of the wordforms in the test corpus 
"Wirtschaftswoche" (business news journal) ,  which have a 
verb reading, turned to have at least one other non-verb 
reading. 
240 
"Unternehmen sind an Gewinnmaximierung intere- 
siert." (Enterprises are interested in maximizing 
their profits). A major subclass of ambiguous word- 
forms are those which have an adjective or attribu- 
tivly used participle reading beside the verb reading. 
For instance, in the sentence "Sie bekannten, die 
bekannten Bilder gestohlen zu haben." (They con- 
fessed they have stolen the famous paintings.) the 
wordform "bekannten" is firstly used as a verb (con- 
fessed) and secondly as an adjective (famous). Since 
adjectives and attributively used participles are in 
most cases part of a nominal phrase a convenient 
rule would reject the verb reading if the previous 
word form is a determiner or the next word form is 
a noun. It is important o notice that such rules 
are based on some regularities, but they may yield 
false results, like for instance the rule for filtering 
out the verb reading of some word forms extremely 
rarely used as verbs (e.g., "recht" - right, to rake 
(3rd person,sg)). All rules are compiled into a sin- 
gle finite-state transducer according to the approach 
described in (Roche and Schabes, 1995). 2 
Named ent i ty  f inder  Named entities such as or- 
ganizations, persons, locations and time expressions 
are identified using finite-state grammars. Since 
some named entities (e.g. company names) may ap- 
pear in the text either with or without a designator, 
we use a dynamic lexicon to store recognized named 
entities without their designators (e.g., "Braun AG" 
vs. "Braun") in order to identify subsequent occur- 
rences correctly. However a named entity, consisting 
solely of one word, may be also a valid word form 
(e.g., "Braun" - brown). Hence we classify such 
words as candidates for named entities since gen- 
erally such ambiguities cannot be resolved at this 
level. Recognition of named entities could be post- 
poned and integrated into the fragment recognizer, 
but performing this task at this stage of processing 
seems to be more appropriate. Firstly because the 
results of POS-filtering could be partially verified 
and improved and secondly the amount of the word 
forms to be processed by subsequent modules could 
be considerably reduced. For instance the verb read- 
ing of the word form "achten" (watch vs. eight) in 
the time expression "am achten Oktober 1995" (at 
the eight of the October 1995) could be filtered out 
if not done yet. 
3 A Sha l low D iv ide -and-Conquer  
S t ra tegy  
The DC-PARSER consists of two major domain- 
independent modules based on finite state technol- 
2The manually constructed rules proved to be a useful 
means for disambiguation, however not sufficient enough to 
filter out all unplausible readings. Hence supplementary rules 
determined by Brill's tagger were used in order to achieve 
broader coverage. 
ogy: 1) construction of the topological sentence 
structure, and 2) application of phrasal grammars 
on each determined subclause (see also fig. 3). In 
this paper we will concentrate on the first step, be- 
cause it is the more novel part of the DC-PARSER, 
and will only briefly describe the second step in sec- 
tion 3.2. 
3.1 Topo log ica l  s t ruc ture  
The DC-PARSER applies cascades of finite-state 
grammars to the stream of tokens and named en- 
titles delivered by the preprocessor in order to de- 
termine the topological structure of the sentence ac- 
cording to the linguistic field theory (Engel, 1988). 3
Based on the fact that in German a verb group 
(like "h~tte fiberredet werden mfissen" - -  *have con- 
vinced been should meaning should have been con- 
vinced) can be split into a left and a right verb part 
("h?tte" and "fiberredet werden miissen") these 
parts (abbreviated as LVP and RVP) are used for the 
segmentation of a main sentence into several parts: 
the front field (VF), the left verb part, middle field 
(MF), right verb part, and rest field (RF). Subclauses 
can also be expressed in that way such that the left" 
verb part is either empty or occupied by a relative 
pronoun or a sub junction element, and the complete 
verb group is placed in the right verb part, cf. figure 
3. Note that each separated field can be arbitrarily 
complex with very few restrictions on the ordering 
of the phrases inside a field. 
Recognition of the topological structure of a sen- 
tence can be described by the following four phases 
realized as cascade of finite state grammars (see also 
fig. 2; fig. 4 shows the different steps in action). 4
Initially, the stream of tokens and named entities is 
separated into a list of sentences based on punctua- 
tion signs. 5 
Verb  groups  A verb grammar ecognizes all sin- 
gle occurrences of verbforms (in most cases corre- 
sponding to LVP) and all closed verbgroups (i.e., se- 
quences of verbforms, corresponding to RVP). The 
parts of discontinuous verb groups (e.g., separated 
LvP and RVP or separated verbs and verb-prefixes) 
cannot be put together at that step of processing 
because one needs contextual information which will 
only be available in the next steps. The major prob- 
lem at this phase is not a structural one but the 
3Details concerning the implementation f the topological 
parsing strategy can be found in (Braun, 1999). Details con- 
cerning the representation a d compilation of the used finite 
state machinery can be found in (Neumann et al, 1997) 
4In this paper we can give only a brief overview of the 
current coverage of the individual steps. An exhaustive de- 
scription of the covered phenomena can be found in (Braun, 
1999). 
5 Performing this step after preprocessing has the advan- 
tage that the tokenizer and named entity finder already have 
determined abbreviation signs, so that this sort of disam- 
biguation is resolved. 
241 
'$KERH~TE$ 
SENT~ERI  " . "  
VERBI IMODUERB$ 
FORrl~ "lK~s~e uer-k~ut'en" 
flORPHO-INFOt rRCR: ~toPphix-FUek~or$\] 
GE}I~: IRK 
EHSE .. :FI~'S 
FORM: "ve~kau~em" 
P~RPHO-IHFO= R(~R= ~rZ%~'l~iX-i:V~kto~* 
FIHIT| =INF 
G~HUS: ~ 
TI~SE = :PRES 
LST~= "~=ue"  j 
STEM: "w JeSS" 
XSU~J-CL/~ 
CONTENT~ 'ZSPANNSAT L~ 
I1FI "~REL-CL$ 
CONTKHT| Z.SF'FIHHSAT ~
L ~,  (-~,') J h 
='/\[z: / 
\[~: m~-phlx-FVel~to~* J ,?RB: *VERB* 
F01~I.. ? leb%" 
~INFO."  rp~,Rl :l~to~phix-FVek*Oe:l: 
L~ lHI Ts T GEHUSt IR~ ENSIEI IPRIE 
STEM: =leb  ~ 
;~EL-P1RI~4: (" die') 
"RB, \[*~* 1 FORM: "er l i t~en ha%~?e" 
EHSE~ IPERF 
LsTErh "er le id"  J 
) 
Figure 3: The result of the DC-PARSER for the sentence "Weil die Siemens GmbH, die vom Export lebt, 
Verluste erlitten hat, musste sie Aktien verkaufen." (Because the Siemens GmbH which strongly depends on 
exports suffered from losses they had to sell some of the shares.) abbreviated where convenient. It shows 
the separation of a sentence into the front field (vF), the verb group (VERB), and the middle field (MF). The 
elements of different fields have been computed by means of fragment recognition which takes place after 
the (possibly recursive) topological structure has been computed. Note that the front field consists only of 
one but complex subclause which itself has an internal field structure. 
Well die Siemens GmbH, die vom Export lebt, Verluste erlitt, musste sie Aktien verkaufen. 
Well die Siemens GmbH, die ...\[Verb-Fin\], Verl. \[Verb-Fin\], \[Modv-Fin\] sie Akt. \[FV-Inf\]. 
Weil die Siemens GmbH \[ReI-CI\], Verluste \[Verb-Fin\], \[Modv-Fin\] sie Aktien \[FV-Inf\]. 
\[Subconj-CL\], \[Modv-Fin\] sie Aktien \[FV-Inf\]. 
\[Subconj-CL\], \[Modv-Fin\] sie Aktien \[FV-Inf\]. 
\[clause\] 
Figure 4: The different steps of the DC-PARSER for the sentence of figure 3. 
massive morphosyntactic ambiguity of verbs (for ex- 
ample, most plural verb forms can also be non-finite 
or imperative forms). This kind of ambiguity can- 
not be resolved without taking into account a wider 
context. Therefore these verb forms are assigned dis- 
junctive types, similar to the underspecified chunk 
categories proposed by (Federici et al, 1996). These 
types, like for example Fin-Inf-PP or Fin-PP, re- 
flect the different readings of the verbform and en- 
able following modules to use these verb fonns ac- 
cording to the wider context, thereby removing the 
ambiguity. In addition to a type each recognized 
verb form is assigned a set of features which rep- 
resent various properties of the form like tense and 
mode information. (cf. figure 5). 
Base clauses (BC)  are subclauses of type sub- 
junctive and subordinate. Although they are embed- 
ded into a larger structure they can independently 
9~L9 242
I Type VG-f inal 1 Subtype Mod-Perf-Ak Modal-stem kSnn Stem lob Form nicht gelobt haben kann Neg T Agr ... 
Figure 5: The structure of the verb fragment "nicht 
gelobt haben kann" - *not praised have could-been 
meaning could not have been praised 
and simply be recognized on the basis of commas, 
initial elements (like complementizer, interrogative 
or relative item - see also fig. 4, where SUBCONJ- 
CL and REL-CL are tags for subclauses) and verb 
fragments. The different ypes of subclauses are de- 
scribed very compactly as finite state expressions. 
Figure 6 shows a (simplified) BC-structure in fea- 
ture matrix notation. 
"Type Subj-Cl 
Subj wenn 
-Type Spannsatz 
Verb J 
"Type stelltenJ Verb .For m 
MF die Arbeitgeber Forderungen) 
Cont \['Type Iohfi\[l 1~ \] \[Type   mp e-Io: I II \[Verb rType Ve.b 1!!I L~.o. rm .u 'eb"enJl|| 
L MF (als Gegenleistung / / /  neue Stellen) j j /  
Figure 6: Simplified feature matrix of the base clause 
"... ,  wenn die Arbeitgeber Forderungen steUten, 
ohne als Gegenleistung neue Stellen zu schaffen." ... 
if the employers make new demands, without compensat- 
ing by creating new jobs. 
Clause combination It is very often the case that 
base clauses are recursively embedded as in the fol- 
lowing example: 
. . .  well der Hund den Braten gefressen 
hatte, den die Frau, nachdem sie ihn zu- 
bereitet hatte, auf die Fensterbank gestellt 
hatte. 
Because the dog ate the beef which was put on 
the window sill after it had been prepared by 
the woman. 
Two sorts of recursion can be distinguished: 1) 
middle field (MF) recursion, where the embedded 
base clause is framed by the left and right verb parts 
of the embedding sentence, and 2) the rest field (RF) 
recursion, where the embedded clause follows the 
right verb part of the embedding sentence. In order 
to express and handle this sort of recursion using 
a finite state approach, both recursions are treated 
as iterations uch that they destructively substitute 
recognized embedded base clauses with their type. 
Hence, the complexity of the recognized structure 
of the sentence is reduced successively. However, 
because subclauses of MF-recursion may have their 
own embedded RF-recursion the CLAUSE COMBINA- 
TION (CC) is used for bundling subsequent base 
clauses before they would be combined with sub- 
clauses identified by the outer MF-recursion. The 
BC and CC module are called until no more base 
clauses can be reduced. If the CC module would not 
be used, then the following incorrect segmentation 
could not be avoided: 
. . .  *\[daft das Gliick \[, das Jochen 
Kroehne ernpfunden haben sollte Rel-C1\] 
\[, als ihm jiingst sein Groflaktion/ir die 
Ubertragungsrechte bescherte Sub j -e l f ,  
nicht mehr so recht erwKrmt Sub j-C1\] 
In the correct reading the second subclause "... als 
ihm jiingst sein .. ." is embedded into the first one 
".. .  das Jochen Kroehne .. .".  
Ma in  c lauses (MC)  Finally the MC module 
builds the complete topological structure of the in- 
put sentence on the basis of the recognized (remain- 
ing) verb groups and base clauses, as well as on the 
word form information ot yet consumed. The latter 
includes basically punctuations and coordinations. 
The following figure schematically describes the cur- 
rent coverage of the implemented MC-module (see 
figure 1 for an example structure): 
CSent 
SSent 
CoordS 
AsyndSent 
CmpCSent 
AsyndCond 
:: . . . .  LVP .. .  \[RVP\] . . .  
::= LVP ...\[RVP\] . . .  
::= CSent ( , CSent)* Coord CSent \] 
::= CSent (, SSent)* Coord SSent 
::= CSent , CSent 
::= CSent , SSent I CSent , CSent 
::= SSent , SSent 
3.2 Phrase recognition 
After the topological structure of a sentence has been 
identified, each substring is passed to the FRAG- 
MENT RECOGNIZER in order to determine the in- 
ternal phrasal structure. Note that processing of 
a substring might still be partial in the sense that 
no complete structure need be found (e.g., if we 
cannot combine sequences of phrases to one larger 
unit). The FRAGMENT RECOGNIZER uses finite state 
grammars in order to extract nominal and preposi- 
tional phrases, where the named entities recognized 
by the preprocessor are integrated into appropriate 
places (unplausibte phrases are rejected by agree- 
ment checking; see (Neumann et al, 1997) for more 
243 
details)). The phrasal recognizer currently only con- 
siders processing of simple, non-recursive structures 
(see fig. 3; here, *NP* and *PP* are used for de- 
noting phrasal types). Note that because of the 
high degree of modularity of our shallow parsing 
architecture, it is very easy to exchange the cur- 
rently domain-independent fragment recognizer with 
a domain-specific one, without effecting the domain- 
independent DC-PARSER. 
The final output of the parser for a sentence is an 
underspecified dependence structure UDS. An UDS 
is a flat dependency-based structure of a sentence, 
where only upper bounds for attachment and scop- 
ing of modifiers are expressed. This is achieved by 
collecting all NPs and PPs of a clause into sepa- 
rate sets as long as they are not part of some sub- 
clauses. This means that although the exact attach- 
ment point of each individual PP is not known it 
is guaranteed that a PP can only be attached to 
phrases which are dominated by the main verb of the 
sentence (which is the root node of the clause's tree). 
However, the exact point of attachment is a matter 
of domain-specific knowledge and hence should be 
defined as part of the domain knowledge of an ap- 
plication. 
4 Evaluat ion 
Due to the limited space, we concentrate on the 
evaluation of the topological structure. An eval- 
uation of the other components (based on a sub- 
set of 20.000 tokens of the mentioned corpus from 
the "Wirtschaftswoche", see below) yields: From 
the 93,89% of the tokens which were identified by 
the morphological component as valid word forms, 
95,23% got a unique POS-assignment with an ac- 
curacy of 97,9%. An initial evaluation on the same 
subset yielded a precision of 95.77% and a recall of 
85% (90.1% F-measure) for our current named en- 
tity finder. Evaluation of the compound analysis 
of nouns, i.e. how often a morphosyntactical cor- 
rect segmentation was found yield: Based on the 
20.000 tokens, 1427 compounds are found, where 
1417 have the correct segmentation (0.9929% preci- 
sion). On a smaller subset of 1000 tokens containing 
102 compounds, 101 correct segmentations where 
found (0.9901% recall), which is a quite promising 
result. An evaluation of simple NPs yielded a recall 
of 0.7611% and precision of 0.9194%. The low recall 
was mainly because of unknown words. 
During the 2nd and 5th of July 1999 a test cor- 
pus of 43 messages from different press releases (viz. 
DEUTSCHE PREESSEAGENTUR (dpa), ASSOCIATED 
PRESS (ap) and REUTERS) and different domains 
(equal distribution of politics, business, sensations) 
was collected. 6 The corpus contains 400 sentences 
6This data collection and evaluation was carried out by 
(Braun, 1999). 
with a total of 6306 words. Note that it also was 
created after the DC-PARSER and all grammars were 
finally implemented. Table 1 shows the result of 
the evaluations (the F-measure was computed with 
/3=1). We used the correctness criteria as defined in 
figure 7. 
The evaluation of each component was measured 
on the basis of the result of all previous components. 
For the BC and MC module we also measured the 
performance by manually correcting the errors of the 
previous components (denoted as "isolated evalua- 
tion"). In most cases the difference between the pre- 
cision and recall values is quite small, meaning that 
the modules keep a good balance between coverage 
and correctness. Only in the case of the MC-module 
the difference is about 5%. However, the result for 
the isolated evaluation of the MC-module suggests 
that this is mainly due to errors caused by previous 
components. 
A more detailed analysis showed that the major- 
ity of errors were caused by mistakes in the prepro- 
cessing phase. For example ten errors were caused 
by an ambiguity between different verb stems (only 
the first reading is chosen) and ten errors because 
of wrong POS-filtering. Seven errors were caused by 
unknown verb forms, and in eight cases the parser 
failed because it could not properly handle the ambi- 
guities of some word forms being either a separated 
verb prefix or adverb. 
The evaluation has been performed with the 
Lisp-based version of SMES (cf. (Neumann et al, 
1997)) by replacing the original bidirectional shal- 
low buttom-up arsing module with the DC-PARSER. 
The average run-time per sentence (average length 
26 words) is 0.57 sec. A C++-version is nearly 
finished missing only the re-implementation f the 
base and main clause recognition phases, cf. (Pisko- 
rski and Neumann, 2000). The run-time behavior 
is already encouraging: processing of a German text 
document (a collection of business news articles from 
the "Wirtschaftswoche") of 197118 tokens (1.26 MB) 
needs 45 seconds on a PentiumII, 266 MHz, 128 
RAM, which corresponds to 4380 tokens per second. 
Since this is an increase in speed-up by a factor > 20 
compared to the Lisp-version, we expect o be able 
to process 75-100 sentences per second. 
5 Re lated  Work 
To our knowledge, there are only very few other 
systems described which process free German texts. 
The new shallow text processor is a direct succes- 
sor of the one used in the SMES-system, an IE-core 
system for real world German text processing (Neu- 
mann et al, 1997). Here, a bidirectional verb-driven 
bottom-up arser was used, where the problems de- 
scribed in this paper concerning parsing of longer 
sentences were encountered. Another similar divide- 
9,a~ 244
Cr i ter ium Match ing of annotated  ata  and results Used by module 
Borders 
Type 
Partial 
Top 
Structl 
Struct2 
start  and end points 
s tar t  and end points, type 
s tar t  or end point, type 
s tar t  and end points, type 
for the largest tag 
see Top, plus test of substructures  
using Par t ia l  
see Top, plus test of substructures  
using Type 
verbforms, BC 
verbforms, BC, MC 
BC 
MC 
MC 
MC 
Figure 7: Correctness criteria used during evaluation. 
Verb-Modu le  
correctness Verb fragments Recall 
criterium total found correct in % 
Borders 897 894 883 98.43 
Type 897 894 880 98.10 
Base-C lause-Modu le  
correctness B C- Fragments Recall 
cmterium total found correct in% 
Type 130 129 121 93.08 
Par t ia l  130 129 125 96.15 
Precision F-measure 
i n% in% 
98.77 98.59 
98.43 98.26 
Precision F-measure 
in % in % 
93180 93.43 
96.89 96.51 
Base-C lause-Modu le  (isolated evaluation) 
correctness 
criterium 
Type 
Partial 
Base-Clauses Recall 
total found correct in % 
130 131 123 94.61 
130 131 127 97.69 
Ma in -C lause-Modu le  
correctness Main-Clauses Recall 
cmtemum total found correct in% 
Top 400 377 361 90.25 
St ruct l  400 377 361 90.25 
Struct2 400 377 356 89.00 
Precision F-measure 
in % in % 
93.89 94.24 
96.94 97.31 
Precision F-measure 
in % in % 
95.75 92.91 
95.75 92.91 
94.42 91.62 
Precision F-measure 
in % in ,% 
96.65 95.30 
96.65 95.30 
95.62 94.29 
Ma ln -C lause-Modu le  (isolated evaluation) 
correctness Main- Clauses Recall 
criterium total found correct in % 
Top 400 389 376 94.00 
St ruct l  400 389 376 94.00 
Struct2 400 389 372 93.00 
complete  analys is  
correctness all components Recall Precision F-measure 
criterium total \[ found \[ correct in% in% in% 
Struct2 400 \[ \ ]377  339 84.75 89.68 87.14 
Table 1: Results of the evaluation of the topological structure 
and-conquer approach using a chart-based parser 
for analysis of German text documents was pre- 
sented by (Wauschkuhn, 1996). Nevertheless, com- 
paring its performance with our approach seems to 
be rather difficult since he only measures for an un- 
annotated test corpus how often his parser finds at 
least one result (where he reports 85.7% "coverage" 
of a test corpus of 72.000 sentences) disregarding to 
measure the accuracy of the parser. In this sense, 
our parser achieved a "coverage" of 94.25% (com- 
puting faund/total), ahnost certainly because we 
use more advanced lexical and phrasal components, 
245 
e.g., pos-filter, compound and named entity process- 
ing. (Peh and Ting, 1996) also describe a divide- 
and-conquer approach based on statistical methods, 
where the segmentation f the sentence is done by 
identifying so called link words (solely punctuations, 
conjunctions and prepositions) and disambiguating 
their specific role in the sentence. On an annotated 
test corpus of 600 English sentences they report an 
accuracy of 85.1% based on the correct recognition of 
part-of-speech, comma nd conjunction disambigua- 
tion, and exact noun phrase recognition. 
6 Conc lus ion and future work 
We have presented a divide-and-conquer st ategy 
for shallow analysis of German texts which is sup- 
ported by means of powerful morphological process- 
ing, efficient POS-filtering and named entity recog- 
nition. Especially for the divide-and-conquer pars- 
ing strategy we obtained an F-measure of 87.14% 
on unseen data. Our shallow parsing strategy has 
a high degree of modularity which allows the inte- 
gration of the domain-independent sentence recog- 
nition part with arbitrary domain-dependent sub- 
components (e.g., specific named entity finders and 
fragment recognizers). 
Considered from an application-oriented point of 
view, our main experience is that even if we are only 
interested in some parts of a text (e.g., only in those 
linguistic entities which verbalize certain aspects of 
a domain-concept) wehave to unfold the structural 
relationship between all elements of a large enough 
area (a paragraph or more) up to a certain level 
of depth in which the relevant information is em- 
bedded. Beside continuing the improvement of the 
whole approach we also started investigations to- 
wards the integration of deep processing into the 
DC-PARSER. The core idea is to call a deep parser 
only to the separated field elements which contain 
sequences of simple NPs and PPs (already deter- 
mined by the shallow parser). Thus seen the shallow 
parser is used as an efficient preprocessor for divid- 
ing a sentence into syntactically valid smaller units, 
where the deep parser's task would be to identify the 
exact constituent s ructure only on demand. 
Acknowledgments 
The research underlying this paper was supported 
by a research grant from the German Bundesmin- 
isterium fiir Bildung, Wissenschaft, Forschung 
und Technologie (BMBF) to the DFKI project 
PARADIME, FKZ ITW 9704. Many thanks to 
Thierry Declerck and Milena Valkova for their sup- 
port during the evaluation of the system. 
References 
S. Abney. 1996. Partial parsing via finite-state cas- 
cades. Proceedings ofthe ESSLLI 96 Robust Pars- 
ing Workshop. 
C. Braun. 1999. Flaches und robustes Parsen 
Deutscher Satzgeffige. Master's thesis, University 
of the Saarland. 
F. Ciravegna, A. Lavelli, N. Mana, L. Gilardoni, 
S. Mazza, M. Ferraro, J. Matiasek, W. Black, 
F. Rinaldi, and D. Mowatt. 1999. Facile: Clas- 
sifying texts integrating pattern matching and in- 
formation extraction. In Proceedings of IJCAI-99, 
Stockholm. 
Ulrich Engel. 1988. Deutsche Grammatik. Julius 
Groos Verlag, Heidelberg, 2., improved edition. 
S. Federici, S. Monyemagni, and V. Pirrelli. 1996. 
Shallow parsing and text chunking: A view on um 
derspecification in syntax. In Workshop on Robust 
Parsing, 8th ESSLLI, pages 35-44. 
G. Neumann, R. Backofen, J. Baur, M. Becker, 
and C. Braun. 1997. An information extraction 
core system for real world german text processing. 
In 5th International Conference of Applied Natu- 
ral Language, pages 208-215, Washington, USA, 
March. 
L. Peh and Christopher H. Ting. 1996. A divide- 
and-conquer strategy for parsing. In Proceedings 
of the ACL/SIGPARSE 5th International Work- 
shop on Parsing Technologies, pages 57-66. 
J. Piskorski and G. Neumann. 2000. An intelligent 
text extraction and navigation system. In 6th In- 
ternational Conference on Computer-Assisted In- 
formation Retrieval (RIAO-2000). Paris, April. 
18 pages. 
E. Roche and Y. Schabes. 1995. Deterministic part- 
of-speech tagging with finite state transducers. 
Computational Linguistics, 21(2):227-253. 
SAIC, editor. 1998. Seventh Message 
Understanding Conference (MUC- 7), 
http://www.muc.saic.com/. SAIC Information 
Extraction. 
S. Sekine and C. Nobata. 1998. An infor- 
mation extraction system and a customization 
tool. In Proceedings of Hitachi workshop-98, 
http://cs.nyu.edu/cs/projects/proteus/sekine/. 
B. Sundheim, editor. 1995. Sixth Message Un- 
derstanding Conference (MUC-6), Washington. 
Distributed by Morgan Kaufmann Publishers, 
Inc.,San Mateo, California. 
O. Wauschkuhn. 1996. Ein Werkzeug zur par- 
tiellen syntaktischen Analyse deutscher Textko- 
rpora. In Dafydd Gibbon, editor, Natural Lan- 
guage Processing and Speech Technology. Results 
of the Third KONVENS Conference, pages 356- 
368. Mouton de Gruyter, Berlin. 
246 
Modelling of a Gazetteer Look-up Component
Jakub Piskorski
DFKI GmbH
German Research Center for Artificial Intelligence
Stuhlsatzenhausweg 3, 66123 Saarbru?cken, Germany
Jakub.Piskorski@dfki.de
Abstract
This paper compares two storage mod-
els for gazetteers, nameley the stan-
dard one based on numbered indexing
automata associated with an auxiliary
storage device against a pure finite-state
model, the latter being superior in terms
of space and time complexity.1
1 Introduction
Gazetteers are dictionaries that include geograph-
ically related information on given places, names
of people, organizations, etc. Several data struc-
tures can be used to implement a gazetteer, e.g.
hash tables, tries and finite-state automata. The
latter require less memory than the alternative
techniques and guarantee efficient access to the
data (1).
In this paper, we compare two finite-state based
data structures for implementing a gazetteer look-
up component, one involving numbered automata
with multiple initial states combined with an ex-
ternal table (2) against the method focused on
converting the input data in such a way as to
model the gazetteer solely as a single finite-state
automaton without any auxiliary storage device
tailored to it. Further, we explore the impact of
transition jamming ? an equivalence transforma-
tion on finite-state devices (3) ? on the size of the
automata.
The paper is organized as follows. Section 2
introduces the basic definitions. Section 3 focuses
1This work is supported by German BMBF-funded
project COLLATE II under grant no. 01 IN C02.
on modeling the gazetteer component. Next, in
section 4 we report on empirical experiments and
finish off with conclusions in section 5.
2 Preliminaries
A deterministic finite-state automaton (DFSA) is
a quintuple M = (Q,?, ?, q0, F ), where Q is
a finite set of states, ? is the alphabet of M ,
? : Q ? ? ? Q is the transition function, q0
is the initial state and F ? Q is the set of final
states. The transition function can be extended to
?? : Q ? ?? ? Q by defining ?(q, ?) = q, and
?(q, wa) = ?(??(q, w), a) for a ? ?, w ? ??.
The language accepted by an automaton M is de-
fined as L(M) = {w ? ??|??(q0, w) ? F}.
In turn, the right language of a state q is de-
fined as L(q) = {w ? ??|??(q, w) ? F}.
A path in a DFSA M is a sequence of
triples ?(p0, a0, p1), . . . , (pk?1, ak?1, pk)?, where
(pi?1, ai?1, pi) ? Q???Q and ?(pi, ai) = pi+1
for 1 ? i < k. The string a0a1 . . . ak is the la-
bel of the path. The first and last state in a path
pi are denoted as f(pi) and l(pi) respectively. We
call a path pi a cycle if f(pi) = l(pi). Further, we
call a path pi sequential if all intermediate states
on pi are non-final and have exactly one incoming
and one outgoing transition. Among all DFSAs
recognizing the same language, the one with the
minimal number of states is called minimal.
Minimal acyclic DFSA (MADFSA) are the
most compact data structure for storing and effi-
ciently recognizing a finite set of words. They can
be built via application of the space-efficient in-
cremental algorithm for constructing a MADFSA
from a list of strings in nearly linear time (4). An-
161
other finite-state device we refer to is the so called
numbered minimal acyclic deterministic finite-
state automaton. Each state of such automata is
associated with an integer representing the cardi-
nality of its right language. An example is given
in Figure 1. Numbered automata can be used for
assigning each accepted word a unique numeric
key, i.e., they implement perfect hashing. An in-
dex I(w) of a word w can be computed as follows.
We start with an index I(w) equal to 1 and scan
the input w with the automaton. While traversing
the accepting path, in each state we increase the
index by the sum of all integers associated with
the target states of transitions lexicographically
preceding the transition used. Once the final state
has been reached I(w) contains the unique index
of w. Analogously, for a given index i the corre-
sponding word w such that I(w) = i can be com-
puted by deducing the path, which would lead to
the index i.2
    
  
  	 


 
	



 







Figure 1: Numbered MADFSA accepting {start,
art, card, stunt, calk}.
3 Modeling of a gazetteer
Raw gazetteers are usually represented by a
text file, where each line represents a sin-
gle entry and is in the following format:
keyword (attribute:value)+. For each
reading of an ambiguous keyword, a separate line
is introduced, e.g., for the word Washington the
following entries are introduced:
Washington | type:city | location:USA | subtype:cap_city
| full-name:Washington D.C. | variant:WASHINGTON
Washington | type:person | surname:Washington
| language:english | gender:m_f
Washington | type:region | variant:WASHINGTON
| location:USA | abbreviation: {W.A.,WA.}
2Instead of associating states with integers, each transi-
tion can be accompanied by the number of different routes
to any final state outgoing from the same state as the cur-
rent transition, whose label are lexicographically lower than
the current one. Consequently, computing I(w) for w would
consist solely of summing over the integers associated with
traversed transitions, whereas memory requirements rise to
30% (5; 2)
We differentiate between open-class and closed-
class attributes depending on their range of val-
ues, e.g., full-name is an open-class attribute,
whereas gender is a closed-class attribute. As
can be seen in the last reading for Washington at-
tribute may be assigned a list of values.
3.1 Standard Approach
The standard approach to implementing dictio-
naries presented in (5; 2) can be straightforwardly
adapted to model the architecture of a gazetteer.
The main idea is to encode the keywords and all
attribute values in a single numbered MADFSA.
In order to distinguish between keywords and dif-
ferent attribute values we extend the indexing au-
tomaton so that it has n+1 initial states, where n
is the number of attributes. The right language of
the first initial state corresponds to the set of the
keywords, whereas the right language of the i-th
initial state for i ? 1 corresponds to the range of
values appropriate for i-th attribute. The subau-
tomaton starting in each initial state implements
different perfect hashing function. Hence, the
aforesaid automaton constitutes a word-to-index
and index-to-word engine for keywords and at-
tribute values. Once we know the index of a given
keyword, we can access the indices of all associ-
ated attribute values in a row of an auxiliary table.
Consequently, these indices can be used to extract
the proper values from the indexing automaton.
In the case of multiple readings an intermediate
array for mapping the keyword indices to the ab-
solute position of the block containing all read-
ings is indispensable. The overall architecture is
sketched in figure 2. Through an introduction of
multiple initial states log2(card(i)) bits are suf-
ficient for representing the indices for values of
attribute i, where card(i) is the size of the corre-
sponding value set.
It is not necessarily convenient to store the
proper values of all attributes in the numbered au-
tomaton, e.g. numerical or alphanumerical data
could be stored directly in the attribute-value ma-
trix or elsewhere (cf. figure 2) if the range of
the values is bounded and integer representation
is more compact than anything else. Fortunately,
the vast majority (but definitely not all) of at-
tribute values in gazetteers deployed in NLP hap-
pens to be natural language expressions. There-
162
Figure 2: Compact storage model for a gazetteer look-up component.
fore, we can expect the major part of the entries
and attribute values to share suffixes, which leads
to a better compression rate. Prevalent bottle-
neck of the presented approach is a potentially
high redundancy of the information stored in the
attribute-value matrix. However, this problem can
be partially alleviated via automatic detection of
column dependency, which might expose sources
of information redundancy. Reccurring patterns
consisting of raw fragments could be indexed and
represented only once.
3.2 Pure Finite-State Representation
One of the common techniques for squeezing au-
tomata in the context of implementing dictionar-
ies is an appropriate coding of the input data.
Converting a list of strings into a MADFSA usu-
ally results in a good compression rate since many
words share prefixes and suffixes, which leads to
transition sharing. If strings are associated with
additional annotations representing certain cate-
gories, e.g., part-of-speech, inflection or stem in-
formation in a morphological lexicon, then an
adequate encoding is necessary in order to keep
the corresponding automaton small. A simple
solution is to reorder categories from the most
specific to the most general ones, so that stem
information would precede inflection and part-
of-speech tag. Alternatively, we could precom-
pute all possible annotation sequences and replace
them with some index. However, the major part
of a string that encodes the keyword and its tags
might be unique and could potentially blow up the
corresponding automaton enormously. Consider
again the entry for the morphological lexicon con-
sisting of an inflected word form and its tags,
e.g. striking:strike:v:a:p (v - verb, a -
present, p - participle). Obviously, the sequence
striking:strike is unique. Through the
exploitation of the word-specific information the
inflected form and its base form share one can
introduce patterns (6) describing how the lex-
eme can be reconstructed from the inflected word
form, e.g., 3+e - delete three terminal characters
and append an e (striking ? strik + e), which
would result in better suffix sharing, i.e., the suf-
fix 3+e:v:a:p is more frequently shared than
strike:v:a:p.
The main idea behind transforming a gazetteer
into a single automaton is to split each gazetteer
entry into a disjunction of subentries, each rep-
resenting some partial information. For each
open-class attribute-value pair present in the en-
try a single subentry is created, whereas closed-
class attribute-value pairs are merged into a single
subentry and rearranged in order to fulfill the first
most specific, last most general criterion. In our
example, the entry for the word Washington (city)
yields the following subentries:
Washington #1 NAM(subtype) VAL(cap_city) NAM(type) VAL(city)
Washington #1 NAM(variant) WASHINGTON
Washington #1 NAM(location) USA
Washington #1 NAM(full-name) Washington D.C.
where NAM maps attribute names to single uni-
vocal characters not appearing elsewhere in the
original gazetteer and VAL denotes a mapping
which converts the values of the closed-class at-
tributes into single characters which represent
these values. The string #1, where # is again a
unique symbol, denotes the reading index of the
entry (first reading). In case of list-valued open-
class attributes we can simply add an appropriate
subentry for each element in the list. Gazetteer re-
sources converted in this manner are subsequently
163
compiled into an MADFSA. In order to gain bet-
ter compression rate we utilized formation pat-
terns for a subset of attribute values appearing
in the gazetteer entries. These patterns resemble
the ones for encoding morphological information,
but they partially rely on other information. For
instance, frequently, attribute values are just the
capitalized form of the corresponding keywords
as can be seen in our example. Such a pattern
can be represented by a single character. Further,
keywords and attribute values often share prefixes
or suffixes, e.g., Washington vs. Washington D.C.
Next, there are clearly several patterns for form-
ing acronyms from the full form, e.g., US can be
derived from United States, by concatenating all
capitals in the full name. Nevertheless, some part
of the attribute values can not be replaced by pat-
terns. Applying formation patterns to our sample
entry would result in:
Washington #1 NAM(subtype) VAL(cap_city) NAM(type) VAL(city)
Washington #1 NAM(variant) PAT(AllCapital)
Washington #1 NAM(location) USA
Washington #1 NAM(full-name) PAT(Identity) D.C.
where PAT maps pattern names to unique char-
acters. Some space savings may be obtained by
reversing the attribute values not covered by any
pattern since prefix compression might be eventu-
ally superior to suffix compression.
The outlined method of representing a
gazetteer is an elegant solution and exhibits
three major assets: (a) no external storage for
attribute values is needed, (b) the automaton
involved is not numbered which means less
space requirement and reduced searching time
in comparison to approach in 3.1, and (c) as a
consequence of the encoding strategy, there is
only one single final state in the automaton.3
From the other point of view, the information
stored in the gazetteers and the fashion in which
the automaton is built intuitively does not allow
for obtaining the same compression rates as in the
case of the automaton in 3.1. For instance, many
entries are multiword expressions, which increase
the size of the automaton by an introduction of
numerous sequential paths. In order to alleviate
this problem we applied transition jamming.
3The states having outgoing transitions labeled with the
unique symbols in the range of NAM are implicit final states.
The right languages of these states represent attribute-value
pairs attached to the gazetteer entries.
3.3 Transition Jamming
Transition jamming is an equivalence operation
on automata in which transitions on sequential
paths are transformed into a single transition la-
beled with the label of the whole path (3). In-
termediate states on the path are removed. The
jammed automaton still accepts the same lan-
guage. We have applied transition jamming in a
somewhat different way. Let pi be a sequential
path in the automaton and a = a0 . . . ak be the
label of pi. We remove all transitions of pi and
introduce a new transition from f(pi) to l(pi) la-
beled with a0 , i.e., ?(f(pi), a0) = l(pi) and
store the remaining character sequence a1 . . . ak
in a list of sequential path labels. Once all such
labels are collected, we introduce a new initial
state in the automaton and consecutively starting
from this state we add all these labels to the min-
imized automaton while maintaining its property
of being minimal (4). The subautomaton start-
ing from the new initial state implements a per-
fect hashing function. Finally, the new ?jammed?
transitions are associated with the corresponding
indices in order to reconstruct the full label on
demand. There are several ways of selecting se-
quential paths for jamming. Maximum-length se-
quential paths constitute the first choice. Jam-
ming paths of bounded length might yield better
or at least different results. For instance, a se-
quential path whose label is a long fragment of a
multiword expression could be decomposed into
subpaths that either do not include whitespaces or
consist solely of whitespaces. In turn, we could
jam only the subpaths of the first type.
Storing sequential path labels in a new branch
of the automaton obviously leads to the intro-
duction of new sequential paths. Therefore, we
have investigated the impact of repetitive transi-
tion jamming on the size of the automaton. In
each phase of repetitive jamming, a new initial
state is introduced from which the labels of the
jammed paths identified in this phase are stored.
4 Experiments
4.1 Data
We have selected following gazetteers for the
evaluation purposes: (a) UK-Postal - city names
in the UK associated with county and postal code
164
Gazetteer size #entries #attributes #open-class average formation pattern
name attributes entry length applicability
LT?World 4,154 96837 19 14 40 99,1%
PL?NE 2,809 51631 8 3 52 96,3%
Mixed 6,957 148468 27 17 44 97,8%
GeoNames I 13,590 80001 17 6 166 89,2%
GeonNames II 33,500 20001 17 6 164 92,0%
Table 1: Parameters of test gazetteers.
Gazetteer Standard Pure-FSA Standard Pure-FSA
& Jamming & Jamming
|Q| |?| |Q| |?| |Q| |?| |Q| |?|
UK-Postal 28596 53041 101145 132008 15008 (15251) 40828 (40903) 32072 (32146) 67831 (67248)
LT-World 191767 266465 259666 341015 86613 (67891) 172583 (152571) 110409 (81479) 207950 (178396)
PL-NE 37935 70773 60119 97035 21106 (19979) 55839 (54639) 27919 (26274) 67435 (65722)
Mixed 206802 295416 299540 399286 94440 (75755) 194815 (174817) 125362 (96038) 242512 (212265)
GeoNames I 280550 410609 803390 1110668 104857 (107631) 258680 (254130) 231887 (226335) 603320 (595122)
GeoNames II 491744 784001 1655790 2396984 198630 (204188) 514595 (517081) 474572 (469678) 1322058 (1311564)
Table 2: Size of the four types of automata.
information, (b) LT?World - a gazetteer of key
players and events in the language technology
community, (c) PL-NE - a gazetteer of MUC-type
Polish named entities, (d) Mixed - a combination
of (b) and (c), (e) GeoNames - an excerpt of the
huge gazetteer of geographic names information
covering geopolitical areas, including name vari-
ants, administrative divisions, different codes, etc.
Table 1 gives an overview of our test data.4
4.2 Evaluation
Several experiments with different set-ups were
conducted. Firstly, we compared the standard
with the pure-FSA approach. Next, we repeated
the experiments enhanced by integration of sin-
gle transition jamming. The results are given in
table 2. The numbers in the columns concern-
ing transition jamming correspond to jamming of
maximum-length sequential paths and jamming
of whitespace-free paths (in brackets).
The increase in physical storage in the case of
numbered automata has been reported to be in
range of 30-40% (state numbering) and 60-70%
(transition numbering) (1). Note at this point that
automata are usually stored as a sequence of tran-
sitions, where states are represented only implic-
itly (7). Considering additionally the space re-
quirement for the auxiliary table in the standard
approach for storing the indices for open-class at-
tribute values, it turns out, that this number os-
cillates around m ? n ? log256n bytes, where m
is the number of open-class attributes and n is
4The last column gives the ratio of open-class attribute
values for which formation patterns can be applied to the to-
tal number of open-class attribute values in a given gazetteer.
the number of entries in the gazetteer. Summing
up these observations and taking a look at the ta-
ble 2, we conclude without naming absolute size
of the physical storage required that the pure-FSA
approach turns out to be the superior when ap-
plied to our test gazetteers. However, some re-
sults, in particular for the Geo-Names, where |?|
is about three time as big as in the automaton
in the standard approach, indicate some pitfalls.
Mainly due to the fact that some open-class at-
tributes in GeoNames are alphanumeric strings
which do not compress well with the rest. Sec-
ondly, some investigation reveal the necessity of
additional formation patterns, which could work
better with this particular gazetteer. Finally, the
GeoNames gazetteer exhibits highly multilingual
character, i.e., the size of the alphabet is larger.
As expected, transition jamming works better
with the Pure-FSA approach, i.e., it reduces the
size of |?| by a factor of 1.35 to 1.9, whereas
in the other case the gain is less significant.
Transition jamming constrained to witespace-free
paths yielded better compression rates, in partic-
ular for gazetteers without numerical data (see ta-
ble 2). Obviously, transition jamming is penal-
ized through the introduction of state numbering
in some part of the automaton and indexing cer-
tain edges, but the overall size of the automaton
is still smaller than the original one. In the case
of the LT-World gazetteer, there were circa 20000
sequential paths in the automaton. Consequently,
we removed circa 134 000 transitions.
Next, we studied the profitability of repeti-
tive transition jamming. Figure 3 presents two
165
Figure 3: Impact of repetitive transition jamming on the size of states and transitions (Standard-B and
Pure-FSA-B stands for repetitive jamming on whitespace-free paths).
diagrams which depict how this operation im-
pacts the size of the automaton for the LT-World
gazetteer. As can be observed, a more than 2-
stage repetitive jamming does not significantly
improve the compression rate. Interestingly, we
can observe in the left diagram that for both ap-
proaches the repetitive jamming of maximum-
length sequential paths leads (after stage 3) to
a greater reduction of |Q| than jamming of
whitespace-free paths. The corresponding num-
bers for other gazetteers with respect to repetitive
jamming were of similar nature. Reversing labels
of sequential paths and reversing open-class at-
tribute values not covered by any formation pat-
tern results in insignificant difference (1-2%) in
the size of the automata.
5 Conclusions and Future Work
In the context of modeling a compact data
structure for implementing a gazetteer empiri-
cal experiments reveal that a pure-FSA approach,
in which all data is converted into a single
MADFSA, turns out to outperform the standard
approach based on an indexing numbered au-
tomaton and an auxiliary table. At least in the
case of data we are dealing with benefits are ob-
servable, since major part of the attribute values
are contemporary word forms. A further inves-
tigation revealed that transition jamming reduces
the size of the automata significantly. However,
for storing gazetteers containing large number of
(alpha)numerical data the standard approach or
other techniques might be a better choice. There-
fore, the evaluation results are only meant to con-
stitute a handy guideline for selecting a solution.
There are number of interesting issues that can
be researched in the future, e.g. investigation of
jamming paths of bounded length or deployment
of finite-state transducers for handling the same
task.
References
Ciura, M.G., Deorowicz, S.: How to squeeze a lex-
icon. Software - Practice and Experience 31(11)
(2001) 1077?1090
Kowaltowski, T., Lucchesi, C.L.: Applications of
Finite Automata Representing Large Vocabularies.
TR DCC-01/92, University of Campinas, Brazil
(1992)
Beijer, N.D., Watson, B., Kourie, D.: Stretching and
Jamming of automata. In: Proceedings of SAICSIT
2003, RSA (2003) 198?207
Daciuk, J., Mihov, S., Watson, B., Watson, R.: In-
cremental Construction of Minimal Acyclic Finite
State Automata. Computational Linguistics 26(1)
(2000) 3?16
Gran?a, J., Barcala, F.M., Alonso, M.A.: Compilation
methods of minimal acyclic automata for large dic-
tionaries. LNCS - Implementation and Application
of Automata 2494 (2002) 135?148
Kowaltowski, T., Lucchesi, C.L., Stolfi, J.: Finite Au-
tomata and Efficient Lexicon Implementation. TR
IC-98-02, University of Campinas, Brazil (1998)
Daciuk, J.: Experiments with automata compres-
sion. In Yu, S., Paun, A., eds.: Proceedings
of CIAA 2000, London, Ontario, Canada, LNCS
2088, Springer (2000) 113?119
166
An Integrated Architecture for Shallow and Deep Processing
Berthold Crysmann, Anette Frank, Bernd Kiefer, Stefan Mu?ller,
Gu?nter Neumann, Jakub Piskorski, Ulrich Scha?fer, Melanie Siegel, Hans Uszkoreit,
Feiyu Xu, Markus Becker and Hans-Ulrich Krieger
DFKI GmbH
Stuhlsatzenhausweg 3
Saarbru?cken, Germany
whiteboard@dfki.de
Abstract
We present an architecture for the integra-
tion of shallow and deep NLP components
which is aimed at flexible combination
of different language technologies for a
range of practical current and future appli-
cations. In particular, we describe the inte-
gration of a high-level HPSG parsing sys-
tem with different high-performance shal-
low components, ranging from named en-
tity recognition to chunk parsing and shal-
low clause recognition. The NLP com-
ponents enrich a representation of natu-
ral language text with layers of new XML
meta-information using a single shared
data structure, called the text chart. We de-
scribe details of the integration methods,
and show how information extraction and
language checking applications for real-
world German text benefit from a deep
grammatical analysis.
1 Introduction
Over the last ten years or so, the trend in application-
oriented natural language processing (e.g., in the
area of term, information, and answer extraction)
has been to argue that for many purposes, shallow
natural language processing (SNLP) of texts can
provide sufficient information for highly accurate
and useful tasks to be carried out. Since the emer-
gence of shallow techniques and the proof of their
utility, the focus has been to exploit these technolo-
gies to the maximum, often ignoring certain com-
plex issues, e.g. those which are typically well han-
dled by deep NLP systems. Up to now, deep natural
language processing (DNLP) has not played a sig-
nificant role in the area of industrial NLP applica-
tions, since this technology often suffers from insuf-
ficient robustness and throughput, when confronted
with large quantities of unrestricted text.
Current information extractions (IE) systems
therefore do not attempt an exhaustive DNLP analy-
sis of all aspects of a text, but rather try to analyse or
?understand? only those text passages that contain
relevant information, thereby warranting speed and
robustness wrt. unrestricted NL text. What exactly
counts as relevant is explicitly defined by means
of highly detailed domain-specific lexical entries
and/or rules, which perform the required mappings
from NL utterances to corresponding domain knowl-
edge. However, this ?fine-tuning? wrt. a particular
application appears to be the major obstacle when
adapting a given shallow IE system to another do-
main or when dealing with the extraction of com-
plex ?scenario-based? relational structures. In fact,
(Appelt and Israel, 1997) have shown that the cur-
rent IE technology seems to have an upper perfor-
mance level of less than 60% in such cases. It seems
reasonable to assume that if a more accurate analy-
sis of structural linguistic relationships could be pro-
vided (e.g., grammatical functions, referential rela-
tionships), this barrier might be overcome. Actually,
the growing market needs in the wide area of intel-
ligent information management systems seem to re-
quest such a break-through.
In this paper we will argue that the quality of cur-
                Computational Linguistics (ACL), Philadelphia, July 2002, pp. 441-448.
                         Proceedings of the 40th Annual Meeting of the Association for
rent SNLP-based applications can be improved by
integrating DNLP on demand in a focussed manner,
and we will present a system that combines the fine-
grained anaysis provided by HPSG parsing with a
high-performance SNLP system into a generic and
flexible NLP architecture.
1.1 Integration Scenarios
Owing to the fact that deep and shallow technologies
are complementary in nature, integration is a non-
trivial task: while SNLP shows its strength in the
areas of efficiency and robustness, these aspects are
problematic for DNLP systems. On the other hand,
DNLP can deliver highly precise and fine-grained
linguistic analyses. The challenge for integration is
to combine these two paradigms according to their
virtues.
Probably the most straightforward way to inte-
grate the two is an architecture in which shallow and
deep components run in parallel, using the results of
DNLP, whenever available. While this kind of ap-
proach is certainly feasible for a real-time applica-
tion such as Verbmobil, it is not ideal for processing
large quantities of text: due to the difference in pro-
cessing speed, shallow and deep NLP soon run out
of sync. To compensate, one can imagine two possi-
ble remedies: either to optimize for precision, or for
speed. The drawback of the former strategy is that
the overall speed will equal the speed of the slow-
est component, whereas in case of the latter, DNLP
will almost always time out, such that overall preci-
sion will hardly be distinguishable from a shallow-
only system. What is thus called for is an integrated,
flexible architecture where components can play at
their strengths. Partial analyses from SNLP can be
used to identify relevant candidates for the focussed
use of DNLP, based on task or domain-specific crite-
ria. Furthermore, such an integrated approach opens
up the possibility to address the issue of robustness
by using shallow analyses (e.g., term recognition)
to increase the coverage of the deep parser, thereby
avoiding a duplication of efforts. Likewise, integra-
tion at the phrasal level can be used to guide the
deep parser towards the most likely syntactic anal-
ysis, leading, as it is hoped, to a considerable speed-
up.
shallow
NLP
components
NLP
deep
components internal repr.
layer
multi
chart
annot.
XML
external repr.
generic OOP
component
interface
WHAM
application
specification
input and
result
Figure 1: The WHITEBOARD architecture.
2 Architecture
The WHITEBOARD architecture defines a platform
that integrates the different NLP components by en-
riching an input document through XML annota-
tions. XML is used as a uniform way of represent-
ing and keeping all results of the various processing
components and to support a transparent software
infrastructure for LT-based applications. It is known
that interesting linguistic information ?especially
when considering DNLP? cannot efficiently be
represented within the basic XML markup frame-
work (?typed parentheses structure?), e.g., linguistic
phenomena like coreferences, ambiguous readings,
and discontinuous constituents. The WHITEBOARD
architecture employs a distributed multi-level repre-
sentation of different annotations. Instead of trans-
lating all complex structures into one XML docu-
ment, they are stored in different annotation layers
(possibly non-XML, e.g. feature structures). Hyper-
links and ?span? information together support effi-
cient access between layers. Linguistic information
of common interest (e.g. constituent structure ex-
tracted from HPSG feature structures) is available in
XML format with hyperlinks to full feature struc-
ture representations externally stored in correspond-
ing data files.
Fig. 1 gives an overview of the architecture of
the WHITEBOARD Annotation Machine (WHAM).
Applications feed the WHAM with input texts and
a specification describing the components and con-
figuration options requested. The core WHAM en-
gine has an XML markup storage (external ?offline?
representation), and an internal ?online? multi-level
annotation chart (index-sequential access). Follow-
ing the trichotomy of NLP data representation mod-
els in (Cunningham et al, 1997), the XML markup
contains additive information, while the multi-level
chart contains positional and abstraction-based in-
formation, e.g., feature structures representing NLP
entities in a uniform, linguistically motivated form.
Applications and the integrated components ac-
cess the WHAM results through an object-oriented
programming (OOP) interface which is designed
as general as possible in order to abstract from
component-specific details (but preserving shallow
and deep paradigms). The interfaces of the actu-
ally integrated components form subclasses of the
generic interface. New components can be inte-
grated by implementing this interface and specifying
DTDs and/or transformation rules for the chart.
The OOP interface consists of iterators that walk
through the different annotation levels (e.g., token
spans, sentences), reference and seek operators that
allow to switch to corresponding annotations on a
different level (e.g., give all tokens of the current
sentence, or move to next named entity starting
from a given token position), and accessor meth-
ods that return the linguistic information contained
in the chart. Similarily, general methods support
navigating the type system and feature structures of
the DNLP components. The resulting output of the
WHAM can be accessed via the OOP interface or as
XML markup.
The WHAM interface operations are not only
used to implement NLP component-based applica-
tions, but also for the integration of deep and shallow
processing components itself.
2.1 Components
2.1.1 Shallow NL component
Shallow analysis is performed by SPPC, a rule-
based system which consists of a cascade of
weighted finite?state components responsible for
performing subsequent steps of the linguistic anal-
ysis, including: fine-grained tokenization, lexico-
morphological analysis, part-of-speech filtering,
named entity (NE) recognition, sentence bound-
ary detection, chunk and subclause recognition,
see (Piskorski and Neumann, 2000; Neumann and
Piskorski, 2002) for details. SPPC is capable of pro-
cessing vast amounts of textual data robustly and ef-
ficiently (ca. 30,000 words per second in standard
PC environment). We will briefly describe the SPPC
components which are currently integrated with the
deep components.
Each token identified by a tokenizer as a poten-
tial word form is morphologically analyzed. For
each token, its lexical information (list of valid read-
ings including stem, part-of-speech and inflection
information) is computed using a fullform lexicon
of about 700,000 entries that has been compiled out
from a stem lexicon of about 120,000 lemmas. Af-
ter morphological processing, POS disambiguation
rules are applied which compute a preferred read-
ing for each token, while the deep components can
back off to all readings. NE recognition is based on
simple pattern matching techniques. Proper names
(organizations, persons, locations), temporal expres-
sions and quantities can be recognized with an av-
erage precision of almost 96% and recall of 85%.
Furthermore, a NE?specific reference resolution is
performed through the use of a dynamic lexicon
which stores abbreviated variants of previously rec-
ognized named entities. Finally, the system splits
the text into sentences by applying only few, but
highly accurate contextual rules for filtering implau-
sible punctuation signs. These rules benefit directly
from NE recognition which already performs re-
stricted punctuation disambiguation.
2.1.2 Deep NL component
The HPSG Grammar is based on a large?scale
grammar for German (Mu?ller, 1999), which was
further developed in the VERBMOBIL project for
translation of spoken language (Mu?ller and Kasper,
2000). After VERBMOBIL the grammar was adapted
to the requirements of the LKB/PET system (Copes-
take, 1999), and to written text, i.e., extended with
constructions like free relative clauses that were ir-
relevant in the VERBMOBIL scenario.
The grammar consists of a rich hierarchy of
5,069 lexical and phrasal types. The core grammar
contains 23 rule schemata, 7 special verb move-
ment rules, and 17 domain specific rules. All rule
schemata are unary or binary branching. The lexicon
contains 38,549 stem entries, from which more than
70% were semi-automatically acquired from the an-
notated NEGRA corpus (Brants et al, 1999).
The grammar parses full sentences, but also other
kinds of maximal projections. In cases where no full
analysis of the input can be provided, analyses of
fragments are handed over to subsequent modules.
Such fragments consist of maximal projections or
single words.
The HPSG analysis system currently integrated
in the WHITEBOARD system is PET (Callmeier,
2000). Initially, PET was built to experiment
with different techniques and strategies to process
unification-based grammars. The resulting sys-
tem provides efficient implementations of the best
known techniques for unification and parsing.
As an experimental system, the original design
lacked open interfaces for flexible integration with
external components. For instance, in the beginning
of the WHITEBOARD project the system only ac-
cepted fullform lexica and string input. In collabora-
tion with Ulrich Callmeier the system was extended.
Instead of single word input, input items can now
be complex, overlapping and ambiguous, i.e. essen-
tially word graphs. We added dynamic creation of
atomic type symbols, e.g., to be able to add arbitrary
symbols to feature structures. With these enhance-
ments, it is possible to build flexible interfaces to
external components like morphology, tokenization,
named entity recognition, etc.
3 Integration
Morphology and POS The coupling between the
morphology delivered by SPPC and the input needed
for the German HPSG was easily established. The
morphological classes of German are mapped onto
HPSG types which expand to small feature struc-
tures representing the morphological information in
a compact way. A mapping to the output of SPPC
was automatically created by identifying the corre-
sponding output classes.
Currently, POS tagging is used in two ways. First,
lexicon entries that are marked as preferred by the
shallow component are assigned higher priority than
the rest. Thus, the probability of finding the cor-
rect reading early should increase without excluding
any reading. Second, if for an input item no entry is
found in the HPSG lexicon, we automatically create
a default entry, based on the part?of?speech of the
preferred reading. This increases robustness, while
avoiding increase in ambiguity.
Named Entity Recognition Writing HPSG gram-
mars for the whole range of NE expressions etc. is
a tedious and not very promising task. They typi-
cally vary across text sorts and domains, and would
require modularized subgrammars that can be easily
exchanged without interfering with the general core.
This can only be realized by using a type interface
where a class of named entities is encoded by a gen-
eral HPSG type which expands to a feature structure
used in parsing. We exploit such a type interface for
coupling shallow and deep processing. The classes
of named entities delivered by shallow processing
are mapped to HPSG types. However, some fine-
tuning is required whenever deep and shallow pro-
cessing differ in the amount of input material they
assign to a named entity.
An alternative strategy is used for complex syn-
tactic phrases containing NEs, e.g., PPs describ-
ing time spans etc. It is based on ideas from
Explanation?based Learning (EBL, see (Tadepalli
and Natarajan, 1996)) for natural language analy-
sis, where analysis trees are retrieved on the basis
of the surface string. In our case, the part-of-speech
sequence of NEs recognised by shallow analysis is
used to retrieve pre-built feature structures. These
structures are produced by extracting NEs from a
corpus and processing them directly by the deep
component. If a correct analysis is delivered, the
lexical parts of the analysis, which are specific for
the input item, are deleted. We obtain a sceletal
analysis which is underspecified with respect to the
concrete input items. The part-of-speech sequence
of the original input forms the access key for this
structure. In the application phase, the underspeci-
fied feature structure is retrieved and the empty slots
for the input items are filled on the basis of the con-
crete input.
The advantage of this approach lies in the more
elaborate semantics of the resulting feature struc-
tures for DNLP, while avoiding the necessity of
adding each and every single name to the HPSG lex-
icon. Instead, good coverage and high precision can
be achieved using prototypical entries.
Lexical Semantics When first applying the origi-
nal VERBMOBIL HPSG grammar to business news
articles, the result was that 78.49% of the miss-
ing lexical items were nouns (ignoring NEs). In
the integrated system, unknown nouns and NEs can
be recognized by SPPC, which determines morpho-
syntactic information. It is essential for the deep sys-
tem to associate nouns with their semantic sorts both
for semantics construction, and for providing se-
mantically based selectional restrictions to help con-
straining the search space during deep parsing. Ger-
maNet (Hamp and Feldweg, 1997) is a large lexical
database, where words are associated with POS in-
formation and semantic sorts, which are organized in
a fine-grained hierarchy. The HPSG lexicon, on the
other hand, is comparatively small and has a more
coarse-grained semantic classification.
To provide the missing sort information when re-
covering unknown noun entries via SPPC, a map-
ping from the GermaNet semantic classification to
the HPSG semantic classification (Siegel et al,
2001) is applied which has been automatically ac-
quired. The training material for this learning pro-
cess are those words that are both annotated with se-
mantic sorts in the HPSG lexicon and with synsets
of GermaNet. The learning algorithm computes a
mapping relevance measure for associating seman-
tic concepts in GermaNet with semantic sorts in the
HPSG lexicon. For evaluation, we examined a cor-
pus of 4664 nouns extracted from business news
that were not contained in the HPSG lexicon. 2312
of these were known in GermaNet, where they are
assigned 2811 senses. With the learned mapping,
the GermaNet senses were automatically mapped to
HPSG semantic sorts. The evaluation of the map-
ping accuracy yields promising results: In 76.52%
of the cases the computed sort with the highest rel-
evance probability was correct. In the remaining
20.70% of the cases, the correct sort was among the
first three sorts.
3.1 Integration on Phrasal Level
In the previous paragraphs we described strategies
for integration of shallow and deep processing where
the focus is on improving DNLP in the domain of
lexical and sub-phrasal coverage.
We can conceive of more advanced strategies for
the integration of shallow and deep analysis at the
length cover- complete LP LR 0CB   2CB
age match
  40 100 80.4 93.4 92.9 92.1 98.9
 40 99.8 78.6 92.4 92.2 90.7 98.5
Training: 16,000 NEGRA sentences
Testing: 1,058 NEGRA sentences
Figure 2: Stochastic topological parsing: results
level of phrasal syntax by guiding the deep syntac-
tic parser towards a partial pre-partitioning of com-
plex sentences provided by shallow analysis sys-
tems. This strategy can reduce the search space, and
enhance parsing efficiency of DNLP.
Stochastic Topological Parsing The traditional
syntactic model of topological fields divides basic
clauses into distinct fields: so-called pre-, middle-
and post-fields, delimited by verbal or senten-
tial markers. This topological model of German
clause structure is underspecified or partial as to
non-sentential constituent boundaries, but provides
a linguistically well-motivated, and theory-neutral
macrostructure for complex sentences. Due to its
linguistic underpinning the topological model pro-
vides a pre-partitioning of complex sentences that is
(i) highly compatible with deep syntactic structures
and (ii) maximally effective to increase parsing ef-
ficiency. At the same time (iii) partiality regarding
the constituency of non-sentential material ensures
the important aspects of robustness, coverage, and
processing efficiency.
In (Becker and Frank, 2002) we present a corpus-
driven stochastic topological parser for German,
based on a topological restructuring of the NEGRA
corpus (Brants et al, 1999). For topological tree-
bank conversion we build on methods and results
in (Frank, 2001). The stochastic topological parser
follows the probabilistic model of non-lexicalised
PCFGs (Charniak, 1996). Due to abstraction from
constituency decisions at the sub-sentential level,
and the essentially POS-driven nature of topologi-
cal structure, this rather simple probabilistic model
yields surprisingly high figures of accuracy and cov-
erage (see Fig.2 and (Becker and Frank, 2002) for
more detail), while context-free parsing guarantees
efficient processing.
The next step is to elaborate a (partial) map-
ping of shallow topological and deep syntactic struc-
tures that is maximally effective for preference-gui-
Topological Structure:
CL-V2
VF-TOPIC LK-FIN MF RK-t
NN VVFIN ADV NN PREP NN VVFIN
[ 	 [ 
	 Peter] [ 
 i?t] [ 
 gerne Wu?rstchen mit Kartoffelsalat] [ Integrating Information Extraction and Automatic Hyperlinking 
 
Stephan Busemann, Witold'UR G \ VNL Hans-Ulrich Krieger,  
Jakub Piskorski, Ulrich Sch?fer, Hans Uszkoreit, Feiyu Xu 
German Research Center for Artificial Intelligence (DFKI GmbH) 
Stuhlsatzenhausweg 3, D-66123 Saarbr?cken, Germany 
sprout@dfki.de 
 
 
Abstract 
This paper presents a novel information sys-
tem integrating advanced information extrac-
tion technology and automatic hyper-linking. 
Extracted entities are mapped into a domain 
ontology that relates concepts to a selection of 
hyperlinks. For information extraction, we use 
SProUT, a generic platform for the develop-
ment and use of multilingual text processing 
components. By combining finite-state and 
unification-based formalisms, the grammar 
formalism used in SProUT offers both pro-
cessing efficiency and a high degree of decal-
rativeness. The ExtraLink demo system show-
cases the extraction of relevant concepts from 
German texts in the tourism domain, offering 
the direct connection to associated web docu-
ments on demand.   
1 Introduction 
The utilization of language technology for the 
creation of hyperlinks has a long history (e.g., 
Allen et al, 1993). Information extraction (IE) is a 
technology that can be applied to identifying both 
sources and targets of new hyperlinks. IE systems 
are becoming commercially viable in supporting 
diverse information discovery and management 
tasks. Similarly, automatic hyperlinking is a matu-
ring technology designed to interrelate pieces of 
information, using ontologies to define the rela-
tionships. With ExtraLink, we present a novel 
information system that integrates both technolo-
gies in order to reach at an improved level of 
informativeness and comfort. Extraction and link 
generation occur completely in the background. 
Entities identified by the IE system are mapped 
into a domain ontology that relates concepts to a 
structured selection of predefined hyperlinks, 
which can be directly visualized on demand using 
a standard web browser. This way, the user can, 
while reading a text, immediately link up textual 
information to the Internet or to any other docu-
ment base without accessing a search engine.  
The quality of the link targets is much higher 
than with standard search engines since, first of all, 
only domain-specific interpretations are sought, 
and second, the ontology provides additional 
structure, including related information. 
ExtraLink uses as its IE system SProUT, a gene-
ric multilingual shallow analysis platform, which 
currently provides linguistic processing resources 
for English, German, Italian, French, Spanish, 
Czech, Polish, Japanese, and Chinese (Becker et 
al., 2002). SProUT is used for tokenization, mor-
phological analysis, and named entity recognition 
in free texts. In Section 2 to 4, we describe innova-
tive features of SProUT. Section 5 gives details 
about the ExtraLink demonstrator. 
2 Integrating Typed Feature Structures 
and Finite State Machines 
The main motivation for developing SProUT 
comes from the need to have a system that (i) 
allows a flexible integration of different processing 
modules and (ii) to find a good trade-off between 
processing efficiency and linguistic expressive-
ness. On the one hand, very efficient finite state 
devices have been successfully applied to real-
world applications. On the other hand, unification-
based grammars (UBGs) are designed to capture 
fine-grained syntactic and semantic constraints, 
resulting in better descriptions of natural language 
phenomena. In contrast to finite state devices, 
unification-based grammars are also assumed to be 
more transparent and more easily modifiable. 
SProUT?s mission is to take the best from these 
two worlds, having a finite state machine that 
operates on typed feature structures (TFSs). I.e., 
transduction rules in SProUT do not rely on simple 
atomic symbols, but instead on TFSs, where the 
left-hand side of a rule is a regular expression over 
TFSs, representing the recognition pattern, and the 
right-hand side is a sequence of TFSs, specifying 
the output structure. Consequently, equality of 
atomic symbols is replaced by unifiability of TFSs 
and the output is constructed using TFS unification 
w.r.t. a type hierarchy. Such rules not only recog-
nize and classify patterns, but also extract frag-
ments embedded in the patterns and fill output 
templates with them. 
Standard finite state techniques such as minimi-
zation and determinization are no longer applicable 
here, due to the fact that edges in our automata are 
annotated by TFSs, instead of atomic symbols. 
However, not every outgoing edge in such an 
automaton must be analyzed, since TFS annota-
tions can be arranged under subsumption, and the 
failure of a general edge automatically causes the 
failure of several, more specialized edges, without 
applying the unifiability test. Such information can 
in fact be precompiled. This and other optimization 
techniques are described in (Krieger and Piskorski, 
2003). 
When compared to symbol-based finite state 
approaches, our method leads to smaller grammars 
and automata, which usually better approximate a 
given language.  
3 XTDL ? The Formalism in SProUT 
XTDL combines two well-known frameworks, 
viz., typed feature structures and regular ex-
pressions. XTDL is defined on top of TDL, a defi-
nition language for TFSs (Krieger and Sch?fer, 
1994) that is used as a descriptive device in several 
grammar systems (LKB, PAGE, PET).  
Apart from the integration into the rule 
definitions, we also employ TDL in SProUT for 
the establishment of a type hierarchy of linguistic 
entities. In the example definition below, the 
morph type inherits from sign and introduces three 
more morphologically motivated attributes with 
the corresponding typed values: 
morph := sign & [ POS  atom, STEM atom, INFL infl ]. 
A rule in XTDL is straightforwardly defined as 
a recognition pattern on the left-hand side, written 
as a regular expression, and an output description 
on the right-hand side. A named label serves as a 
handle to the rule. Regular expressions over TFSs 
describe sequential successions of linguistic signs. 
We provide a couple of standard operators. Con-
catenation is expressed by consecutive items. Dis-
junction, Kleene star, Kleene plus, and optionality 
are represented by the operators |, *, +, and ?, resp. 
{n} after an expression denotes an n-fold repetition. 
{m,n} repeats at least m times and at most n times. 
The XTDL grammar rule below may illustrate 
the syntax. It describes a sequence of morphologi-
cally analyzed tokens (of type morph). The first 
TFS matches one or zero items (?) with part-of-
speech Determiner. Then, zero or more Adjective 
items are matched (*). Finally, one or two Noun 
items ({1,2}) are consumed. The use of a variable 
(e.g., #1) in different places establishes a 
coreference between features. This example enfor-
ces agreement in case, number, and gender for the 
matched items. Eventually, the description on the 
RHS creates a feature structure of type phrase, 
where the category is coreferent with the category 
Noun of the right-most token(s), and the agreement 
features corefer to features of the morph tokens. 
 np :> 
   (morph & [ POS  Determiner, 
 INFL  [CASE #1, NUM #2, GEN #3 ]] )?  
   (morph & [ POS  Adjective, 
 INFL  [CASE #1, NUM #2, GEN #3 ]] )*  
   (morph & [ POS  Noun & #4, 
 INFL  [CASE #1, NUM #2, GEN #3 ]] ){1,2} 
 -> phrase & [CAT #4, 
 AGR agr & [CASE #1, NUM #2, GEN #3 ]]. 
 
The choice of TDL has a couple of advantages. 
TFSs as such provide a rich descriptive language 
over linguistic structures and allow for a fine-
grained inspection of input items. They represent a 
generalization over pure atomic symbols. Unifia-
bility as a test criterion in a transition is a generali-
zation over symbol equality. Coreferences in 
feature structures express structural identity. Their 
properties are exploited in two ways. They provide 
a stronger expressiveness, since they create 
dynamic value assignments on the automaton 
transitions and thus exceed the strict locality of 
constraints in an atomic symbol approach. Further-
more, coreferences serve as a means of information 
transport into the output description on the RHS of 
the rule. Finally, the choice of feature structures as 
primary citizens of the information domain makes 
composition of modules very simple, since input 
and output are all of the same abstract data type.  
Functional (in contrast to regular) operators are 
a door to the outside world of SProUT.  They 
either serve as predicates, helping to locate 
complex tests that might cancel a rule application, 
or they construct new material, involving pieces of 
information from the LHS of a rule.  The sketch of 
a rule below transfers numerals into their 
corresponding digits using the functional operator 
normalize() that is defined externally. For instance, 
"one" is mapped onto "1", "two" onto "2", etc. 
 
  ?  numeral & [ SURFACE #surf, ... ] .?  -> 
  digit & [ ID #id, ... ],  where #id = normalize(#surf). 
4 The SProUT System  
The core of SProUT comprises of the following 
components: (i) a finite-state machine toolkit for 
building, combining, and optimizing finite-state 
devices; (ii) a flexible XML-based regular com-
piler for converting regular patterns into their cor-
responding compressed finite-state representation 
(Piskorski et al, 2002); (iii) a JTFS package which 
provides standard operations for constructing and 
manipulating TFSs; and (iv) an XTDL grammar 
interpreter. 
Currently, SProUT offers three online compo-
nents: a tokenizer, a gazetteer, and a morphological 
analyzer. The tokenizer maps character sequences 
to tokens and performs fine-grained token classifi-
cation. The gazetteer recognizes named entities 
based on static named entity lexica.  
The morphology unit provides lexical resources 
for English, German (equipped with online shallow 
compound recognition), French, Italian, and 
Spanish, which were compiled from the full form 
lexica of MMorph (Petitpierre and Russell, 1995). 
Considering Slavic languages, a component for 
Czech presented in (Haji?, 2001), and Morfeusz 
(Przepi?rkowski and Wolinski, 2003) for Polish. 
For Asian languages, we integrated Chasen 
(Asahara and Matsumoto, 2000) for Japanese and 
Shanxi (Liu, 2000) for Chinese.  
The XTDL-based grammar engineering plat-
form has been used to define grammars for 
English, German, French, Spanish, Chinese and 
Japanese allowing for named entity recognition 
and extraction. To guarantee a comparable 
coverage, and to ease evaluation, an extension of 
the MUC-7 standard for entities has been adopted.   
 
ne-person := enamex & [ TITLE list-of-strings, 
                          GIVEN_NAME list-of-strings, 
                          SURNAME list-of-strings, 
                                  P-POSITION list-of-strings, 
                          NAME-SUFFIX string, 
                                    DESCRIPTOR string ]. 
 
Given the expressiveness of XTDL expressions, 
MUC-7/MET-2 named entity types can be 
enhanced with more complex internal structures. 
For instance, a person name ne-person is defined 
as a subtype of enamex with the above structure. 
The named entity grammars can handle types 
such as person, location, organization, time point, 
time span (instead of date and time defined by 
MUC), percentage, and currency.  
The core system together with the grammars 
forms a basis for developing applications. SProUT 
is being used by several sites in both research and 
industrial contexts. 
A component for resolving coreferent named 
entities disambiguates and classifies incomplete 
named entities via dynamic lexicon search, e.g., 
Microsoft is coreferent with Microsoft corporation 
and is thus correctly classified as an organization. 
5 ExtraLink: Integrating Information 
Extraction and Automatic Hyperlinking  
A methodology for automatically enriching web 
documents with typed hyperlinks has been develo-
ped and applied to several domains, among them 
the domain of tourism information. A core compo-
nent is a domain ontology describing tourist sites 
in terms of sights, accommodations, restaurants, 
cultural events, etc. The ontology was specialized 
for major European tourism sites and regions (see 
Figure 1). It is associated with a large selection of  
 
 
 
Figure 1: Link Target Page (excerpt). The instance the 
web document is associated to (Isle of Capri) is shown 
on the left, together with neighboring concepts in the 
ontology, which the user can navigate through. 
 
link targets gathered, intellectually selected and 
continuously verified. Although language techno-
logy could also be employed to prime target 
selection, for most applications quality require-
ments demand the expertise of a domain specialist. 
In the case of the tourism domain, the selection 
was performed by a travel business professional. 
The system is equipped with an XML interface and 
accessible as a server. 
The ExtraLink GUI marks the relevant entities 
(usually locations) identified by SProUT (see 
second window on the left in Figure 2). Clicking 
on a marked expression causes a query related to 
the entity being shipped to the server. Coreferent 
concepts are handled as expanded queries. The 
server returns a set of links structured according to 
the ontology, which is presented in the ExtraLink 
GUI (Figure 2). The user can choose to visualize 
any link target in a new browser window that also 
shows the respective subsection of the ontology in 
an indented tree notation (see Figure 1).  
 
 
 
Figure 2: ExtraLink GUI. The links in the right-hand 
window are generated after clicking on the marked 
named entity for Lisbon (marked in dark). The bottom 
left window shows the SProUT result for ?Lissabon?. 
 
The ExtraLink demonstrator has been imple-
mented in Java and C++, and runs under both MS 
Windows and Linux. It is operational for German, 
but it can easily be extended to other languages 
covered by SProUT. This involves the adaptation 
of the mapping into the ontology and a multi-
lingual presentation of the ontology in the link 
target page. 
Acknowledgements 
Work on ExtraLink has been partially funded 
through grants by the German Ministry for 
Education, Science, Research and Technology 
(BMBF) to the project Whiteboard (contract 01 IW 
002), by the EC to the project Airforce (contract 
IST-12179), and by the state of the Saarland to the 
project SATOURN. We are indebted to Tim vor 
der Br?ck, Thierry Declerck, Adrian Raschip, and 
Christian Woldsen for their contributions to 
developing ExtraLink. 
References 
J. Allen, J. Davis, D. Krafft, D. Rus, and D. Subrama-
nian. Information agents for building hyperlinks. J. 
Mayfield and C. Nicholas: Proceedings of the Work-
shop on Intelligent Hypertext, 1993. 
M. Asahara and Y. Matsumoto. Extended models and 
tools for high-performance part-of-speech tagger. 
Proceedings of  COLING, 21-27, 2000. 
0 %HFNHU : 'UR G \ VNL +-U. Krieger, J. 
Piskorski, U. Sch?fer, F. Xu. SProUT?Shallow Pro-
cessing with Typed Feature Structures and Unifica-
tion. In Proceedings of  ICON, 2002. 
J. +DML? Disambiguation of rich inflection?compu-
tational morphology of Czech. Prague Karolinum, 
Charles University Press, 2001. 
H.-U. Krieger and U. Sch?fer. TDL?A Type Description 
Language for Constraint-Based Grammars. Procee-
dings of COLING, 893-899, 1994. 
H.-U. Krieger and J. Piskorski. Speed-up methods for 
complex annotated finite state grammars. DFKI 
Report, 2003. 
K. Liu. Research of automatic Chinese word segmen-
tation. Proceedings of ILT&CIP, 2001. 
D. Petitpierre and G. Russell. MMORPH?the Multext 
morphology program. Multext deliverable report 
2.3.1. ISSCO, University of Geneva, 1995. 
J. PiskRUVNL:'UR G \ VNL );X DQG2 6FKHUIA 
flexible XML-based regular compiler for creation 
and converting linguistic resources. Proceedings of 
LREC 2002, Las Palmas, Spain, 2002. 
A. Przepi?rkowski and M. Wolinski. The Unbearable 
Lightness of Tagging: A Case Study in Morphosyn-
tactic Tagging of Polish. Proceedings of the Work-
shop on Linguistically Interpreted Corpora, 2003. 
 
Coling 2008: Companion volume ? Posters and Demonstrations, pages 145?148
Manchester, August 2008
Online-Monitoring of Security-Related Events
Martin Atkinson, Jakub Piskorski, Bruno Pouliquen
Ralf Steinberger, Hristo Tanev, Vanni Zavarella
Joint Research Centre of the European Commission
Institute for the Protection and Security of the Citizen
Via Fermi 2749, 21027 Ispra (VA), Italy
firstname.lastname@jrc.it
Abstract
This paper presents a fully operational
real-time event extraction system which is
capable of accurately and efficiently ex-
tracting violent and natural disaster events
from vast amount of online news articles
per day in different languages. Due to the
requirement that the system must be mul-
tilingual and easily extendable, it is based
on a shallow linguistic analysis. The event
extraction results can be viewed on a pub-
licly accessible website.
1 Introduction
Gathering information about violent and natural
disaster events from online news is of paramount
importance to better understand conflicts and to
develop global monitoring systems for the auto-
matic detection of precursors for threats in the
fields of conflict and health. This paper reports
on a fully operational live event extraction system
to detect information on violent events and natural
disasters in large multilingual collections of online
news articles collected by the news aggregation
system Europe Media Monitor (Best et al, 2005),
http://press.jrc.it/overview.html.
Although a considerable amount of work on the
automatic extraction of events has been reported,
it still appears to be a lesser studied area in com-
parison to the somewhat easier tasks of named-
entity and relation extraction. Two comprehensive
examples of the current functionality and capabil-
ities of event extraction technology dealing with
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
the identification of disease outbreaks and con-
flict incidents are given in (Grishman et al, 2002)
and (King and Lowe, 2003) respectively. The most
recent trends and developments in this area are re-
ported in (Ashish et al, 2006)
In order to be capable of processing vast
amounts of textual data in real time (as in the case
of EMM)we follow a linguistically lightweight ap-
proach and exploit clustered news at various pro-
cessing stages (pattern learning, information fu-
sion, geo-tagging, etc.). Consequently, only a tiny
fraction of each text is analysed. In a nutshell, our
system deploys simple 1 and 2-slot extraction pat-
terns to identify event-relevant entities. These pat-
terns are semi-automatically acquired in a boot-
strapping manner by using clustered news data.
Next, information about events scattered over dif-
ferent documents is integrated by applying voting
heuristics. The results of the core event extraction
system are integrated into a real-world global mon-
itoring system. Although we mainly cover the se-
curity domain, the techniques deployed in our sys-
tem can be applied to other domains, such as for
instance tracking business-related events for risk
assessment.
In the remaining part of this paper we give a
brief overview of the real-time event extraction
processing chain and describe the particularities of
selected subcomponents. Finally, the online appli-
cation is presented.
2 Real-time Event Extraction Process
The real-time event extraction processing chain is
depicted in Figure 1. First, news articles are gath-
ered by dedicated software for electronic media
monitoring, namely the EMM system (Best et al,
2005). EMM receives an average of 50,000 news
articles per day from about 1,500 news sources in
145
over 40 languages, and regularly checks for up-
dates of news. Secondly, the input data is grouped
into news clusters ideally including documents
on one topic or event. Then, clusters describing
security-related events are selected using keyword-
based heuristics. For each such cluster, the system
tries to detect and extract only the main event by
analysing all documents in the cluster.
EMM
News
Clustering / 
Geo Tag
Text Pre-
Processing 
Pattern 
Matching
Information 
Aggregation 
Events
NEXUS
Figure 1: Real-time processing chain.
Next, each cluster is processed by our core event
extraction engine. For each detected violent event,
it produces a frame, whose main slots are: date and
location, number of killed, injured or kidnapped
people, actors, type of event, weapons used, etc.
In an initial step, each document in the cluster
is linguistically pre-processed in order to produce
a more abstract representation of the texts. This
encompasses: fine-grained tokenisation, sentence
splitting, matching of known named entities, la-
belling of key terms and phrases like action words
(e.g. kill, shoot) and person groups.
Once texts are grouped into clusters and lin-
guistically pre-processed, the pattern engine ap-
plies a cascade of extraction grammars (consisting
of 1 and 2-slot extraction patterns) on each docu-
ment within a cluster. For creating extraction pat-
terns, we apply a blend of machine learning and
knowledge-based techniques. The extraction pat-
terns are matched against the first sentence and the
title of each article from the cluster. By processing
only the top sentence and the title, the system is
more likely to capture facts about the most impor-
tant event in the cluster. Even if we fail to detect
a single piece of information in one document in a
cluster, the same information is likely to be found
in another document of the cluster, where it may
be expressed in a different way.
Finally, since information about events is scat-
tered over different articles, the last step con-
sists of cross-document cluster-level information
fusion, i.e., we aggregate and validate information
extracted locally from each single article in the
same cluster. For this purpose, simple voting-like
heuristics are deployed.
Every ten minutes, EMM clusters the articles
found during the last four hours. The event extrac-
tion engine analyses each of these clusters. The
event information is thus always up-to-date. The
output of the event extraction engine constitutes
the input for a global monitoring system.
3 Geo-tagging Clusters
Challenges for geo-tagging clusters are that place
names can be homographic with person names and
with other place names. We solve the former am-
biguity by first identifying person names found
in our automatically populated database of known
people and organisations. For the latter ambiguity,
we adopted a cluster-centric approach by weight-
ing all place names found in a cluster and by select-
ing the one with the highest score. For each cluster,
we thus first establish all possible candidate loca-
tions by looking up in the texts all place, province,
region and country names found in a multilingual
gazetteer (including name variants). The weights
of the locations are then based on the place name
significance (e.g., a capital city scores higher than
a village) and on the place name hierarchy (i.e. if
the province or region to which the place belongs
are also mentioned in the text, it scores higher).
4 Pattern Acquisition
For pattern acquisition, we deploy a weakly super-
vised bootstrapping algorithm (Tanev and Oezden-
Wennerberg, 2008) similar in spirit to the one de-
scribed in (Yangarber, 2003), which involves some
manual validation. Contrary to other approaches,
the learning phase exploits the knowledge to which
cluster the news items belong. Intuitively, this
guarantees better precision of the learned patterns.
In particular, for each event-specific semantic role
(e.g. killed), a separate cycle of learning iterations
is executed (usually up to three) in order to learn
1-slot extraction patterns. Each cluster includes ar-
ticles from different sources about the same news
story. Therefore, we assume that each entity ap-
pears in the same semantic role (actor, victim, in-
jured) in the context of one cluster. An auto-
matic procedure for syntactic expansion comple-
ments the learning. This procedure accepts a man-
ually provided list of words which have identical
(or similar) syntactic usage patterns (e.g. killed,
assassinated, murdered, etc.). It then generates
new patterns from the old ones by substituting for
each other the words in the list. After 1-slot pat-
terns are acquired, some of them are used to man-
ually create 2-slot patterns like X shot Y.
146
5 Pattern matching engine
In order to guarantee that massive amounts of tex-
tual data can be processed in real time, we have
developed ExPRESS (Piskorski, 2007), an effi-
cient extraction pattern engine, which is capable of
matching thousands of patterns against MB-sized
texts within seconds. The pattern specification lan-
guage is a blend of two previously introduced IE-
oriented grammar formalisms, namely JAPE used
in GATE (Cunningham et al, 2000) and XTDL,
used in SPROUT (Dro?zd?zy?nski et al, 2004).
A single pattern is a regular expression over flat
feature structures (FS), i.e., non-recursive typed
feature structures without structure sharing, where
features are string-valued and ? unlike in XTDL
types ? are not organised in a hierarchy. Each such
regular expression is associated with a list of FSs
which constitute the output specification. Like in
XTDL, we deploy variables and functional oper-
ators for forming slot values and for establishing
contact with the ?outer world?. Further, we adapted
JAPEs feature of associating patterns with mul-
tiple actions, i.e., producing multiple annotations
(possibly nested). An empirical comparison of the
run-time behaviour of the new formalism against
the other 2 revealed that significant speed-ups can
be achieved (at least 30 times faster). ExPRESS
comes with a pool of highly efficient core linguis-
tic processing resources (Piskorski, 2008).
6 Information Aggregation
Once single pieces of information are extracted by
the pattern engine, they are merged into event de-
scriptions by applying an information aggregation
algorithm. This algorithm assumes that each clus-
ter reports at most one main event of interest. It
takes as input the text entities extracted from one
news cluster with their semantic roles and consid-
ers the sentences from which these entities are ex-
tracted. If one and the same entity has two roles as-
signed, a preference is given to the role assigned by
the most reliable group of patterns (e.g., 2-slot pat-
terns are more reliable). Another ambiguity which
has to be resolved arises from the contradictory in-
formation which news sources give about the num-
ber of victims. We use an ad-hoc heuristic for
computing the most probable estimation for these
numbers, i.e., firstly the largest group of numbers
which are close to each other is selected and sec-
ondly the number closest to the average in that
group is chosen. After this estimation is com-
puted, the system discards from each news clus-
ter all the articles whose reported victim numbers
significantly differ from the estimated numbers for
the whole cluster. Additionally, some victim arith-
metic is applied, i.e., a small taxonomy of person
classes is used to sum victim numbers (e.g., gun-
men and terrorists belong to the same class ofNon-
GovernmentalArmedGroup).
7 Event Classification
After the single pieces of information are assem-
bled into the event description, an event classifica-
tion is performed. Some of the most used event
classes are Terrorist Attack, Bombing, Shooting,
Air Attack, etc. The classification algorithm uses
a blend of keyword matching and domain spe-
cific rules. As an example, consider the following
domain-specific rule: if the event description in-
cludes named entities, which are assigned the se-
mantic role kidnapped, as well as entities which
are assigned the semantic role released, then the
type of the event is Hostage Release, rather than
Kidnapping. If the event refers to kidnapped peo-
ple and at the same time the news articles contain
words like video or videotape, then the event type
is Hostage Video Release. The second rule has a
higher priority, therefore it impedes the Hostage
Release rule to fire erroneously, when the release
of a hostage video is reported.
8 Monitoring Events
The core event extraction engine for English is
fully operational since December 2007. There are
two online applications running on top of it which
allow monitoring events. The first one is a dedi-
cated webpage using the Google Maps JavaScript
API (see Figure 2). It is publicly accessible at:
http://press.jrc.it/geo?type=event
&format=html&language=en and provides
an instant overview of what is occurring where in
the world. A small problem with this application
is that it overlays and hides events that are close to
each other.
The second application shows the same events
using the Google Earth client application. The
geo-located data is transmitted via the Keyhole
Markup Language (KML) format
1
supported di-
rectly by Google Earth.
2
The application is re-
1
http://code.google.com/apis/kml/documentation/
2
In order to run it, start Google Earth with KML:
http://press.jrc.it/geo?type=event&format=kml&language=en
147
Figure 2: Event visualisation with Google Maps
stricted to displaying at most half the globe, but
it allows expanding overlaid events.
Since it is important for stakeholders to be
quickly and efficiently informed about the type and
gravity of the event, various icons are used to rep-
resent the type or group of events visually (see Fig-
ure 3). We use general forms of icons for violent
events and specific forms of icons for natural and
man-made disasters. For violent events, the gen-
eral form represents the major consequence of the
event, except for kidnappings, where specific icons
are used. Independently of the type of event, all
icons are sized according to the damage caused,
i.e. it is dependent on the number of victims in-
volved in the event. Also, to highlight the events
with a more significant damage, a border is drawn
around the icon to indicate that a threshold of peo-
ple involved has been passed.
The online demo is available for English, Italian
and French. We are currently working on adapt-
ing the event extraction engine to other languages,
including Russian, Spanish, Polish, German and
Arabic. A more thorough description of the sys-
tem can be found in (Tanev et al, 2008; Piskorski
et al, 2008).
References
Ashish, N., D. Appelt, D. Freitag, and D. Zelenko. 2006.
Proceedings of the workshop on Event Extraction and Syn-
thesis, held in conjunction with the AAAI 2006 conference.
Menlo Park, California, USA.
Best, C., E. van der Goot, K. Blackler, T. Garcia, and
D. Horby. 2005. Europe Media Monitor. Technical Re-
port EUR 22173 EN, European Commission.
Cunningham, H., D. Maynard, and V. Tablan. 2000. JAPE: a
Java Annotation Patterns Engine (Second Edition). Tech-
nical Report, CS?00?10, University of Sheffield, Depart-
ment of Computer Science.
?
Kidnap
K
A
Arrest
R
Release
V
Video
V
Man
Made
?Violent EventUndefined Violent EventKilled Violent EventInjured Violent EventKindnapped Violent EventArrest Hostage Release VideoRelease Violent EventNo Consequneces
Man Made
Disaster
Man Made 
Fire
Man Made
Explosion
ND
!
Natural
Dister
Volcanic 
Eruption
Tsunami Earthquake Landslide
?
Avalanche Tropical
Storm
Lightning
Strike
Storm
Snow
Storm
Flood Wild Fire
Heatwave
Key to Symbols
Consequence Significance (number of people involved)
No Circle  = up to 10 Red Circle = More than 100Yellow Circle= between 10 and 100
Humanitarian
Crisis
Trial
Unclassified
Figure 3: Key to event type icons and magnitude
indicators
Dro?zd?zy?nski, W., H.-U. Krieger, J. Piskorski, U. Sch?afer,
and F. Xu. 2004. Shallow Processing with Unification
and Typed Feature Structures ? Foundations and Appli-
cations. K?unstliche Intelligenz, 2004(1):17?23.
Grishman, R., S. Huttunen, and R. Yangarber. 2002. Real-
time Event Extraction for Infectious Disease Outbreaks.
Proceedings of the Human Language Technology Confer-
ence (HLT) 2002.
King, G. and W. Lowe. 2003. An Automated Information
Extraction Tool for International Conflict Data with Per-
formance as Good as Human Coders: A Rare Events Eval-
uation Design. International Organization, 57:617?642.
Piskorski, J., H. Tanev, M. Atkinson, and E. Van der Goot.
2008. Cluster-centric Approach to News Event Extraction.
In Proceedings of MISSI 2008, Wroclaw, Poland.
Piskorski, J. 2007. ExPRESS Extraction Pattern Recogni-
tion Engine and Specification Suite. In Proceedings of the
International Workshop Finite-State Methods and Natu-
ral language Processing 2007 (FSMNLP?2007), Potsdam,
Germany.
Piskorski, J. 2008. CORLEONE ? Core Linguistic Entity
Online Extraction. Technical report 23393 EN, Joint Re-
search Centre of the European Commission, Ispra, Italy.
Tanev, H. and P. Oezden-Wennerberg. 2008. Learning to
Populate an Ontology of Violent Events (in print). In
Fogelman-Soulie, F. and Perrotta, D. and Piskorski, J. and
Steinberger, R., editor, NATO Security through Science Se-
ries: Information and Communication Security. IOS Press.
Tanev, H., J. Piskorski, and M. Atkinson. 2008. Real-
Time News Event Extraction for Global Crisis Monitor-
ing. In Proceedings of the 13
th
International Conference
on Applications of Natural Language to Information Sys-
tems (NLDB 2008, Lecture Notes in Computer Science Vol.
5039), pages 207?218. Springer-Verlag Berlin Heidelberg.
Yangarber, R. 2003. Counter-Training in Discovery of Se-
mantic Patterns. In Proceedings of the 41
st
Annual Meet-
ing of the ACL.
148
Balto-Slavonic Natural Language Processing 2007, June 29, 2007, pages 27?34,
Prague, June 2007. c?2007 Association for Computational Linguistics
Lemmatization of Polish Person Names
Jakub Piskorski
European Commission
Joint Research Centre
Via Fermi 1
21020 Ispra, Italy
Jakub.Piskorski@jrc.it
Marcin Sydow
Polish-Japanese Institute
of Information Technology
Koszykowa 86
02-008 Warsaw, Poland
msyd@pjwstk.edu.pl
Anna Kups?c?
Universit? Paris3/LLF, PAS ICS
Case Postale 7031
2, place Jussieu
75251 Paris Cedex 05
akupsc@univ-paris3.fr
Abstract
The paper presents two techniques for
lemmatization of Polish person names. First,
we apply a rule-based approach which re-
lies on linguistic information and heuris-
tics. Then, we investigate an alterna-
tive knowledge-poor method which employs
string distance measures. We provide an
evaluation of the adopted techniques using
a set of newspaper texts.
1 Introduction
Proper names constitute a significant part of natural
language texts (estimated to about 10% in newspa-
per articles) and are important for NLP applications,
such as Information Extraction, which rely on au-
tomatic text understanding.1 In particular, corefer-
ence resolution (e.g., identifying several name vari-
ants as referring to the same entity) plays a crucial
role in such systems. Although automatic recogni-
tion of proper names in English, French and other
major languages has been in the research focus for
over a decade now, cf. (Bikel et al, 1997), (Borth-
wick, 1999), (Li et al, 2003), only a few efforts have
been reported for Slavic languages, cf. (Cunning-
ham et al, 2003) (Russian and Bulgarian), (Pisko-
rski, 2005) (Polish). Rich inflection and a more re-
laxed word order make recognition of proper names
in Slavic more difficult than for other languages.
Moreover, inflection of proper names is usually
1The research presented in this paper was partially founded
by the Ministry of Education and Science (Poland), grant num-
ber 3T11C00727.
quite different from common nouns, which compli-
cates the lemmatization process necessary for cor-
rect coreference resolution. In this paper, we focus
on lemmatization of Polish person names, the most
idiosyncratic class of proper names in this language.
First, we report results of a rule-based symbolic ap-
proach. We apply different heuristics, mostly based
on the internal (morphological and syntactic) struc-
ture of proper names but also on the surrounding
context. Sometimes, however, the required infor-
mation is not available, even if the entire docu-
ment is considered, and lemmatization cannot be
performed. Therefore, we experimented with var-
ious knowledge-poor methods, namely string dis-
tance metrics, in order to test their usefulness for
lemmatization of Polish person names as an alterna-
tive technique, especially for cases where document-
level heuristics are insufficient.
Lemmatization of proper names in Slavic has not
attracted much attention so far but some work has
been done for Slovene: (Erjavec et al, 2004) present
a machine-learning approach to lemmatization of
unknown single-token words, whereas (Pouliquen et
al., 2005) report on a shallow approach to find base
forms.
The organization of the paper is as follows. First,
we present a description of phenomena which make
lemmatization of Polish person names a difficult
task. Next, a rule-based approach and its evaluation
are presented. Then, various string distance metrics
are introduced, followed by the results of experi-
ments on newspaper texts. The final section presents
conclusions and perspectives for future work.
27
case male name female name
nom Kazimierz Polak Kazimiera Polak
gen Kazimierza Polaka Kazimiery Polak
dat Kazimierzowi Polakowi Kazimierze Polak
acc Kazimierza Polaka Kazimiere? Polak
ins Kazimierzem Polakiem Kazimiera? Polak
loc Kazimierzu Polaku Kazimierze Polak
voc Kazimierzu Polaku Kazimiero Polak
Table 1: Declension of Polish male vs. female names
2 Declension Patterns of Polish Person
Names
Polish is a West Slavic language with rich nomi-
nal inflection: nouns and adjectives are inflected for
case, number and gender. There are 7 cases, 2 num-
bers and traditionally 3 genders are distinguished:
masculine, feminine and neuter. Just like common
nouns, Polish person names undergo declension but
their inflectional patterns are more complicated. A
typical Polish name consists of a first name and a
last name; unlike in Russian or Bulgarian, there are
no patronymics. Additionally, titles (e.g., dr ?Phd?,
inz?. ?engineer?, prof. ?professor?) or honorific forms
(pan ?Mr.? or pani ?Mrs./Miss?) are often used. In
general, both the first and the last name can be in-
flected, e.g., Jan Kowalski (nominative) vs. Jana
Kowalskiego (genitive/accusative). If the surname
is also a regular word form, things get more compli-
cated. Whether the last name can be inflected in such
cases depends on several factors, e.g., on the gen-
der of the first name, a category (part-of-speech) and
gender of the (common) word used as a surname.
For instance, if the surname is a masculine noun, it
is inflected only if the first name is also masculine.
This is illustrated in Table 1 with declension of the
male name Kazimierz Polak ?Casimir Pole? and its
variant with the female first name Kazimiera.
If the surname is an adjective (e.g., Niski ?Short?),
it is inflected (according to the adjectival paradigm)
and agrees in gender with the first name, i.e., male
and female last name forms are different (e.g., Niski
?Short? (masc.) vs. Niska ?Short? (fem.)). The de-
clension of foreign surnames may strongly depend
on their origin, and in particular on the pronuncia-
tion. For example, the name Wilde is pronounced
differently in English and German, which impacts
its declension in Polish. If it?s of English origin, a
nominal declension is applied, i.e., Wilde?a (gen.),
case sg pl sg pl
nom go?a?b go?e?bie Go?a?b Go?a?bowie
gen go?e?bia go?e?bi Go?a?ba Go?a?b?w
dat go?e?biowi go?e?biom Go?a?bowi Go?a?bom
acc go?e?bia go?e?bie Go?a?ba Go?a?b?w
ins go?e?biem go?e?biami Go?a?bem Go?a?bami
loc go?e?bia go?e?bie Go?a?biu Go?a?bach
voc go?e?biu go?e?bie Go?a?b Go?a?bowie
Table 2: Common noun vs. person name inflection
whereas if it comes from German, an adjective-like
declension is adopted: Wildego (gen.).
Declension of surnames which are also common
nouns can be different from the declension of com-
mon nouns.2 In Table 2, we present a comparison
of the common noun go?a?b ?dove? in singular and
plural with the corresponding forms used for the
surname. A comprehensive overview of this rather
intriguing declension paradigm of Polish names is
given in (Grzenia, 1998).
Finally, first name forms present problems as
well. Foreign masculine first names, whose pro-
nounced version ends in a consonant or whose writ-
ten version ends in -a, -o, -y or -i do in general
get inflected (e.g., Jacques (nom.) vs. Jacques?a
(gen./acc.)), whereas names whose pronounced ver-
sion ends in a vowel and are stressed on the last syl-
lable (e.g., Fran?ois) usually do not change form.
For female first names created from a male first
name, e.g., J?zef (masc.) vs. J?zefa (fem.), there is
a frequent homonymy between the nominative form
of the female name and the genitive/accusative form
of the corresponding male form, e.g., J?zefa is no-
minative of J?zefa (fem.) and genitive/accusative of
J?zef (masc.).
3 Rule-Based Approach to Person Name
Lemmatization
3.1 Experiment
Our rule-based approach to person name lemmatiza-
tion exploits existing resources (a dictionary of first
names and contextual triggers) and relies on con-
textual information (heuristics). It has been imple-
mented using SProUT, a shallow processing plat-
form, integrated with a Polish morphological anal-
2The declension of such surnames depends on the local tra-
dition and sometimes can be identical with the pattern used for
common nouns.
28
yser (Piskorski et al, 2004). For first names, all in-
flected forms of the most frequent Polish first names
are stored in a database so a simple gazetteer look-up
associates names with the corresponding base form.
We also used a list of ca 30 000 foreign first names
(nominative forms). For last names, we applied sev-
eral heuristic rules in order to recognize and produce
their base forms. First, we identify most common
types of Polish surnames, e.g., capitalized words
ending in -skiego, -skim, -skiemu or -icza, -iczem, -
iczu (typical last name suffixes), and convert them to
the corresponding base forms (i.e., words ending in
-ski and -icz, respectively). In this way, a significant
number of names can be lemmatized in a brute-force
manner.
For all remaining surnames, more sophisticated
rules have to be applied. As discussed in sec. 2,
these rules have to take into account several pieces
of information such as part-of-speech and gender
of the (common) word which serves as a surname,
but also gender of the first name. The major prob-
lem we encountered while applying these rules is
that the information necessary to trigger the appro-
priate rule is often missing. For example, in sen-
tence (1), inferring gender of the surname/first name
could involve a subcategorization frame for the verb
powiadomic? ?inform?, which requires an accusative
NP argument. In this way we might possibly predict
that the base form of Putina is Putin, as -a is the typi-
cal accusative ending of masculine names. Since the
subcategorization lexicon is not available, such in-
stances are either not covered or different heuristics
are employed for guessing the base form.
(1) Powiadomiono
informed
wczoraj
yesterday
wieczorem
evening
V. Putina
V. Putinacc
o
about
ataku.
attack
?Yesterday evening they informed V. Putin about the at-
tack.?
Additionally, grammar rules may produce vari-
ants of recognized full person names. For exam-
ple, for the full name CEO dr Jan Kowalski the fol-
lowing variants can be produced: Kowalski, CEO
Kowalski, dr Kowalski, etc. As the grammar rules
always return the longest match, a shorter form may
not be recognized. The produced variants are there-
fore used in the second pass through the text in order
to identify ?incomplete? forms. As no morphological
generation is involved, only base forms can be iden-
tified in this way. The system evaluation indicates
that 23.8% of the recognized names were identified
by this partial coreference resolution mechanism.
An analysis of incorrectly recognized named en-
tities (NEs) revealed that major problems concerned
(a) classical ambiguities, such as a proper name
vs. a common word, and (b) person vs. organiza-
tion name, caused by a specific word order and a
structural ambiguity of phrases containing NEs. Let
us consider the following examples to illustrate the
problems.
(2) Dane
Datanom
Federalnego
federalgen
Urze?du
officegen
Statystycznego
statisticalgen
?Data of the federal office for statistics?
(3) prezes
presidentnom
Della
Dellgen
?president of Dell?
(4) kanclerz
chancellornom
Austriak?w
Austriansgen
?chancellor of the Austrians?
(5) ... powiedzia?
said
prezes
presidentnom
sp??ki
companygen
Kruk
Kruknom
?. . . said the president of Kruk company / Kruk, the pre-
sident of the company?
The text fragment Dane Federalnego in (2) is rec-
ognized by the grammar as a person name since
Dane is a gazetteer entry for a foreign (English) first
name. Consequently, Federalnego Urze?du Statys-
tycznego could not be recognized as an organization
name. Potentially, heuristics solving such NE over-
lapping collisions could improve the precision. Sim-
ilar techniques have been applied to other languages.
In (3) and (4) the names Della ?of Dell? and Austri-
ak?w ?of Austrians? were erroneously recognized as
surnames. The rule matching a token representing
a title followed by a capitalized word, adopted for
English person names, is less reliable for Polish due
to declension of proper names and lack of prepo-
sitions in genitive constructions. One solution to
this problem would involve matchingDella and Aus-
triak?w with their base forms (Dell and Austriacy,
resp.), which might appear in the immediate con-
text. In this way, the name type could be validated.
However, a corpus inspection revealed that quite fre-
quently no base form appears in the same document.
The last example, (5), illustrates another problem,
which is even harder to solve. The phrase prezes
29
sp??ki Kruk is structurally ambiguous, i.e., it can
be bracketed as [prezes [sp??ki Kruk]] or [[prezes
sp??ki] Kruk]. Consequently, the name Kruk might
either refer to a company name (?. . . said the pre-
sident of the Kruk company?) or to a person name
(?. . . said Kruk, the president of the company?). In-
ferring the proper interpretation might not be possi-
ble even if we consider the subcategorization frame
of the verb powiedziec? ?to say?.
3.2 Evaluation
For evaluation of recognition and lemmatization of
person names, a set of 30 articles on various top-
ics (politics, finance, sports, culture and science) has
been randomly chosen from Rzeczpospolita (Weiss,
2007), a leading Polish newspaper. The total num-
ber of person name occurrences in this document set
amounts to 858. Evaluation of recognition?s preci-
sion and recall yielded 88.6% and 82.6%, respec-
tively. Precision of lemmatization of first names
and surnames achieved 92.2% and 75.6%, respec-
tively. For 12.4% of the recognized person names
more than one output structure was returned. For in-
stance, in case of the person name Marka Belki, the
first name Marka is interpreted by the gazetteer ei-
ther as an accusative form of the male name Marek
or as a nominative form of a foreign female name
Marka. In fact, 10% of the Polish first-name forms
in our gazetteer are ambiguous with respect to gen-
der. As for the last name Belki, it is a genitive form
of the common Polish noun belka ?beam?, so the
base form can be obtained directly. Nevertheless,
as inflection of proper names differs from that of
common nouns, various combinations of the regular
noun Belka and the special proper name form Belki
are possible, which increases ambiguity of the iden-
tified form. All possible lemmatizations are as fol-
lows:
(6) Marek Belka (masc.),
Marka Belka (fem.),
Marek Belki (masc.),
Marka Belki (fem.)
A good heuristics to reduce such ambiguous
lemmatizations is to prioritize rules which refer to
morphological information over those which rely
solely on orthography and/or token types.
4 Application of String Distance Metrics
for Lemmatization
Since knowledge-based lemmatization of Polish
NEs is extremely hard, we also explored a possibil-
ity of using string distance metrics for matching in-
flected person names with their base forms (and their
variants) in a collection of document, rather than
within a single document. The rest of this section de-
scribes our experiments in using different string dis-
tance metrics for this task, inspired by the work pre-
sented in (Cohen et al, 2003) and (Christen, 2006).
The problem can be formally defined as follows.
Let A, B and C be three sets of strings over some
alphabet ?, with B ? C. Further, let f : A ? B
be a function representing a mapping of inflected
forms (A) into their corresponding base forms (B).
Given,A andC (the search space), the task is to con-
struct an approximation of f , namely f? : A ? C.
If f?(a) = f(a) for a ? A, we say that f? returns
the correct answer for a; otherwise, f? is said to re-
turn an incorrect answer. For another task, a multi-
result experiment, we construct an approximation
f? : A ? ?C , where f? returns the correct answer
for a if f(a) ? f?(a).
4.1 String distance metrics
In our experiments, we have explored mainly
character-level string metrics3 applied by the
database community for record linkage.
Our point of departure is the well-known Lev-
enshtein edit distance metric specified as the min-
imum number of character-level operations (inser-
tion, deletion or substitution) required for trans-
forming one string into another (Levenshtein, 1965)
and bag distance metric (Bartolini et al, 2002)
which is a time-efficient approximation of the Lev-
enshtein metric. Next, we have tested the Smith-
Waterman (Smith and Waterman, 1981) metric,
which is an extension of Levenshtein metric and al-
low a variable cost adjustment to edit operations and
an alphabet mapping to costs.
Another group of string metrics we explored is
based on a comparison of character-level n-grams in
two strings. The q-gram metric (Ukkonen, 1992) is
3Distance (similarity) metrics map a pair of strings s and t
to a real number r, where a smaller (larger) value of r indicates
greater (lower) similarity.
30
computed by counting the number of q-grams con-
tained in both strings. An extension to q-grams is
to add positional information, and to match only
common q-grams that occur at a specified distance
from each other (positional q-grams) (Gravano et
al., 2001). Finally, the skip-gram metric (Keskustalo
et al, 2003) is based on the idea that in addition
to forming bigrams of adjacent characters, bigrams
that skip characters are considered as well. Gram
classes are defined that specify what kind of skip-
grams are created, e.g. {0, 1} class means that regu-
lar bigrams (0 characters skipped) and bigrams that
skip one character are formed. We have explored
{0, 1}, {0, 2} and {0, 1, 2} gram classes.
Taking into account the Polish declension
paradigm, we also added a basic metric based on the
longest common prefix, calculated as follows:
CP ?(s, t) = ((|lcp(s, t)|+ ?)
?/(|s| ? |t|),
where lcp(s, t) denotes the longest common prefix
for s and t. The symbol ? is a parameter for favoring
certain suffix pairs in s (t). We have experimented
with two variants: CP ?? with ? = 0 and CP ?? ,
where ? is set to 1 if s ends in: o, y, a?, e?, and t ends
in an a, or 0 otherwise. The latter setting results
from empirical study of the data and the declension
paradigm.
For coping with multi-token strings, we tested
a similar metric called longest common substrings
(LCS) (Christen, 2006), which recursively finds and
removes the longest common substring in the two
strings compared, up to a specified minimum length.
Its value is calculated as the ratio of the sum of all
found longest common substrings to the length of
the longer string. We extended LCS by additional
weighting the lengths of the longest common sub-
strings. The main idea is to penalize the longest
common substrings which do not match the begin-
ning of a token in at least one of the compared
strings. In such cases, the weight for lcs(s, t) (the
longest common substring for s and t) is computed
as follows. Let ? denote the maximum number of
non-whitespace characters which precede the first
occurrence of lcs(s, t) in s or t. Then, lcs(s, t) is
assigned the weight:
wlcs(s,t) =
|lcs(s, t)|+ ??max(?, p)
|lcs(s, t)|+ ?
where p has been experimentally set to 4. We refer
to the ?weighted? variant of LCS as WLCS.
Good results for name-matching tasks (Cohen et
al., 2003) have been reported using the Jaro metric
and its variant, the Jaro-Winkler (JW ) metric (Win-
kler, 1999). These metrics are based on the num-
ber and order of common characters in two com-
pared strings. We have extended the Jaro-Winkler
metric to improve the comparison of multi-token
strings. We call this modification JWM and it can
be briefly characterized as follows. Let J(s, t) de-
note the value of the Jaro metric for s and t. Then,
let s = s? . . . sK and t = t? . . . tL, where si (ti) rep-
resent i-th token of s and t respectively, and assume,
without loss of generality, L ? K. JWM(s, t) is
defined as:
JWM(s, t) = J(s, t)+? ?boostp(s, t)?(??J(s, t))
where ? denotes the common prefix adjustment fac-
tor and boostp is calculated as follows:
boostp(s, t) =
?
L
?
?
i=?
L?? min(|lcp(si, ti)|, p)+
min(|lcp(sL, tL..tK)|, p)
L
The main idea behind JWM is to boost the Jaro
similarity for strings with the highest number of
agreeing initial characters in the corresponding to-
kens in the compared strings.
Finally, for multi-token strings, we tested a recur-
sive matching pattern, known also as Monge-Elkan
distance (Monge and Elkan, 1996). The intuition be-
hind this measure is the assumption that a token in
s (strings are treated as sequences of tokens) corre-
sponds to a token in t which has the highest num-
ber of agreeing characters. The similarity between
s and t is the mean of these maximum scores. Two
further metrics for multi-token strings were investi-
gated, namely Sorted-Tokens and Permuted-Tokens.
The first one is computed in two steps: (a) first, to-
kens forming a full string are sorted alphabetically,
and then (b) an arbitrary metric is applied to com-
pute the similarity for the ?sorted? strings. The latter
compares all possible permutations of tokens form-
ing the full strings and returns the calculated maxi-
mal similarity value.
A detailed description of string metrics used here
is given in (Christen, 2006) and in (Piskorski et al,
2007).
31
4.2 Test Data
For the experiments on coreference of person names,
we used two resources: (a) a lexicon of the most
frequent Polish first names (PL-F(IRST)-NAMES)
consisting of pairs of an inflected form and the cor-
responding base form, and (b) an analogous lexicon
of inflected full person names (first name + surname)
(PL-FULL-NAMES).4 The latter resource was cre-
ated semi-automatically as follows. We have auto-
matically extracted a list of 22485 full person-name
candidates from a corpus of 15724 on-line news ar-
ticles from Rzeczpospolita by using PL-F-NAMES
lexicon and an additional list of 30000 uninflected
foreign first names. Subsequently, we have ran-
domly selected a subset of about 1900 entries (in-
flected forms) from this list.
In basic experiments, we simply used the base
forms as the search space. Moreover, we produced
variants of PL-F-NAMES and PL-FULL-NAMES
by adding to the search space base forms of for-
eign first names and a complete list of full names ex-
tracted from the Rzeczpospolita corpus, respectively.
Table 3 gives an overview of our test datasets.
Dataset #inflected #base search space
PL-F-NAMES 5941 1457 1457
PL-F-NAMES-2 5941 1457 25490
PL-FULL-NAMES 1900 1219 1219
PL-FULL-NAMES-2 1900 1219 2351
PL-FULL-NAMES-3 1900 1219 20000
Table 3: Dataset used for the experiments
4.3 Evaluation Metrics
Since for a given string more than one answer can be
returned, we measured the accuracy in three ways.
First, we calculated the accuracy on the assumption
that a multi-result answer is incorrect and we defined
all-answer accuracy (AA) measure which penalizes
multi-result answers. Second, we measured the ac-
curacy of single-result answers (single-result accu-
racy (SR)) disregarding the multi-result answers.
Finally, we used a weaker measure which treats a
multi-result answer as correct if one of the results in
the answer set is correct (relaxed-all-answer accu-
racy (RAA)).
4Inflected forms which are identical to their corresponding
base form were excluded from the experiments since finding an
answer for such cases is straightforward.
Let s denote the number of strings for which a sin-
gle result (base form) was returned. Analogously,
m is the number of strings for which more than
one result was returned. Let sc and mc denote, re-
spectively, the number of correct single-result an-
swers returned and the number of multi-result an-
swers containing at least one correct result. The ac-
curacy metrics are computed as: AA = sc/(s+m),
SR = sc/s, and RAA = (sc +mc)/(s+m).
4.4 Experiments
We started our experiments with the PL-F-NAME
dataset and applied all but the multi-token strings
distance metrics. The results of the accuracy eval-
uation are given in Table 4. The first three columns
give the accuracy figures, whereas the column la-
beled AV gives an average number of results re-
turned in the answer set.
Metrics AA SR RAA AV
Bag Distance 0.476 0.841 0.876 3.02
Levenshtein 0.708 0.971 0.976 2.08
Smith-Waterman 0.625 0.763 0.786 3.47
Jaro 0.775 0.820 0.826 2.06
Jaro-Winkler 0.820 0.831 0.831 2.03
q-grams 0.714 0.974 0.981 2.09
pos q-grams 0.721 0.976 0.982 2.09
skip grams 0.873 0.935 0.936 2.14
LCS 0.696 0.971 0.977 12.69
WLCS 0.731 0.983 0.986 2.97
CP ?? 0.829 0.843 0.844 2.11
CP ?? 0.947 0.956 0.955 2.18
Table 4: Results for PL-F-NAMES
Interestingly, the simple linguistically-aware
common prefix-based measure turned out to work
best in the AA category, which is the most relevant
one, whereas WLCS metrics is the most accurate in
case of single-result answers and the RAA category.
Thus, a combination of the two seems to be a rea-
sonable solution to further improve the performance
(i.e., if WLCS provides a single answer, return this
answer, otherwise return the answer ofCP ??). Next,
the time-efficient skip grams metrics performed sur-
prisingly well in the AA category. This result was
achieved with {0, 2} gram classes. Recall that about
10% of the inflected first name forms in Polish are
ambiguous, as they are either a male or a female per-
son name, see sec. 2.
Clearly, the AA accuracy figures in the experi-
ment run on the PL-F-NAME-2 (with a large search
space) was significantly worse. However, the SR
32
accuracy for some of the metrics is still acceptable.
The top ranking metrics with respect to SR and AA
accuracy are given in Table 5. Metrics which return
more than 5 answers on average were excluded from
this list. Also in the case of PL-F-NAME-2 the com-
bination of WLCS and CP ?? seems to be the best
choice.
Metrics SR AA
WLCS 0.893 0.469
CP ?? 0.879 0.855
pos 2-grams 0.876 0.426
skip grams 0.822 0.567
2-grams 0.810 0.398
LCS 0.768 0.340
CP ?? 0.668 0.600
JW 0.620 0.560
Table 5: Top results for PL-F-NAMES-2
Finally, we have made experiments for full per-
son names, each represented as two tokens. It is
important to note that the order of the first name
and the surname in some of the entities in our test
datasets is swapped. This inaccuracy is introduced
by full names where the surname may also function
as a first name. Nevertheless, the results of the ex-
periment on PL-FULL-NAMES given in Table 6 are
nearly optimal. JWM , WLCS, LCS, skip grams
and Smith-Waterman were among the ?best? metrics.
Internal Metrics AA SR RAA AV
Bag Distance 0.891 0.966 0.966 3.13
Smith-Waterman 0,965 0,980 0,975 3,5
Levenshtein 0.951 0.978 0.970 4.59
Jaro 0.957 0.970 0.964 3.54
JW 0.952 0.964 0.958 3.74
JWM 0.962 0.974 0.968 3.74
2-grams 0.957 0.988 0.987 3.915
pos 3-grams 0.941 0.974 0.966 4.32
skip-grams 0.973 0.991 0.990 5.14
LCS 0.971 0.992 0.990 5.7
WLCS 0.975 0.993 0.992 6.29
Table 6: Results for PL-FULL-NAMES
The Monge-Elkan, Sorted-Tokens and Permuted-
Tokens scored in general only slightly better than the
basic metrics. The best results oscillating around
0.97, 0.99, and 0.99 for the three accuracy metrics
were obtained using LCS, WLCS, JWM and CP ?
metrics as internal metrics. The highest score was
achieved by applying Sorted-Tokens with JWM with
0.976 in AA accuracy.
Further, in order to get a better picture, we have
compared the performance of the aforementioned
?recursive? metrics on PL-FULL-NAMES-2, which
has a larger search space. The most significant re-
sults for the AA accuracy are given in Table 7. The
JWM metric seems to be the best choice as an in-
ternal metric, whereas WLCS, CP ?? and Jaro per-
form slightly worse.
Internal M. Monge-Elkan Sorted-Tokens Permuted-Tokens
Bag Distance 0.868 0.745 0.745
Jaro 0.974 0.961 0.968
JWM 0.976 0.976 0.975
SmithWaterman 0.902 0.972 0.967
3-grams 0.848 0.930 0.911
pos 3-grams 0.855 0.928 0.913
skip-grams 0.951 0.967 0.961
LCS 0.941 0.960 0.951
WLCS 0.962 0.967 0.967
CP ?? 0.969 n.a. n.a.
CP ?? 0.974 n.a. n.a.
Table 7: AA accuracy for PL-FULL-NAMES-2
In our last experiment we selected the ?best?
metrics so far and tested them against PL-FULL-
NAMES-3 (largest search space). The top results for
non-recursive metrics are given in Table 8. JWM
and WLCS turned out to achieve the best scores.
Metrics AA SR RAA AV
Levenshtein 0.791 0.896 0.897 2.20
Smith-Waterman 0.869 0.892 0.889 2.35
JW 0.791 0.807 0.802 2.11
JWM 0.892 0.900 0.901 2.11
skip-grams 0.852 0.906 0.912 2.04
LCS 0.827 0.925 0.930 2.48
WLCS 0.876 0.955 0.958 2.47
Table 8: Results for PL-FULL-NAMES-3
The top scores achieved for the recursive metrics
on PL-FULL-NAMES-3 were somewhat better. In
particular, Monge-Elkan performed best with CP ??
(0.937 AA and 0.946 SR) and slightly worse re-
sults were obtained with JWM. Sorted-Tokens scored
best in AA and SR accuracy with JWM (0.904) and
WLCS (0.949), respectively. Finally, for Permuted-
Tokens the identical setting yielded the best results,
namely 0.912 and 0.948, respectively.
5 Conclusions and Perspectives
For Slavic languages, rich and idiosyncratic inflec-
tion of proper names presents a serious problem for
lemmatization. In this paper we investigated two
different techniques for finding base forms of per-
son names in Polish. The first one employs heuris-
33
tics and linguistic knowledge. This method does
not provide optimal results at the moment as nec-
essary tools and linguistic resources, e.g., a morpho-
logical generator or a subcategorization lexicon, are
still underdeveloped for Polish. Moreover, contex-
tual heuristics do not always find a solution as the
required information might not be present in a sin-
gle document. Therefore, we considered string dis-
tance metrics as an alternative approach. The results
of applying various measures indicate that for first
names, simple common prefix (CP?) metric obtains
the best results for all-answer accuracy, whereas
the weighted longest common substrings (WLCS)
measure provides the best score for the single-result
accuracy. Hence, a combination of these two metrics
seems the most appropriate knowledge-poor tech-
nique for lemmatizing Polish first names. As for full
names, our two modifications (WLCS and JWM )
of standard distance metrics and CP? obtain good re-
sults as internal metrics for recursive measures and
as stand-alone measures.
Although the results are encouraging, the pre-
sented work should not be considered a final solu-
tion. We plan to experiment with the best scoring
metrics (e.g., for AA and SR) in order to find opti-
mal figures. Additionally, we consider combining
the two techniques. For example, string distance
metrics can be used for validation of names found
in the context. We also envisage applying the same
methods to other types of proper names as well as to
lemmatization of specialized terminology.
References
I. Bartolini, P. Ciacca, and M. Patella. 2002. String matching
with metric trees using an approximate distance. In Proceed-
ings of SPIRE, LNCS 2476, Lisbon, Portugal.
D. Bikel, S. Miller, R. Schwartz, and R. Weischedel. 1997.
Nymble: A High-performance Learning Name-finder. In
Proceedings of ANLP-1997, Washington DC, USA.
A. Borthwick. 1999. AMaximumEntropy Approach to Named
Entity Recognition. PhD Thesis, Department of Computer
Science, New York University.
P. Christen. 2006. A Comparison of Personal Name Matching:
Techniques and Practical Issues. Technical report, TR-CS-
06-02, Computer Science Laboratory, The Australian Na-
tional University, Canberra, Australia.
W. Cohen, P. Ravikumar, and S. Fienberg. 2003. A compar-
ison of string metrics for matching names and records. In
Proceedings of the KDD2003.
H. Cunningham, E. Paskaleva, K. Bontcheva, and G. Angelova.
2003. Information extraction for Slavonic languages. In
Proceedings of the Workshop IESL, Borovets, Bulgaria.
T. Erjavec and S. D?eroski. 2004. Machine Learning of
Morphosyntactic Structure: Lemmatising Unknown Slovene
Words. In Journal of Applied Artificial Intelligence, 18(1),
pages 17-40.
L. Gravano, P. Ipeirotis, H. Jagadish, S. Koudas, N. Muthukrish-
nan, L. Pietarinen, and D. Srivastava. 2001. Using q-grams
in a DBMS for Approximate String Processing. IEEE Data
Engineering Bulletin, 24(4):28?34.
J. Grzenia. 1998. S?ownik nazw w?asnych ? ortografia,
wymowa, s?owotw?rstwo i odmiana. PWN.
H. Keskustalo, A. Pirkola, K. Visala, E. Leppanen, and
K. Jarvelin. 2003. Non-adjacent digrams improve matching
of cross-lingual spelling variants. In Proceedings of SPIRE,
LNCS 22857, Manaus, Brazil, pages 252?265.
V. Levenshtein. 1965. Binary Codes for Correcting Deletions,
Insertions, and Reversals. Doklady Akademii Nauk SSSR,
163(4):845?848.
W. Li, R. Yangarber, and R. Grishman. 2003. Bootstrapping
Learning of Semantic Classes from Positive and Negative
Examples. In Proceedings of the ICML-2003 Workshop on
The Continuum from Labeled to Unlabeled Data.
A. Monge and C. Elkan. 1996. The Field Matching Problem:
Algorithms and Applications. In Proceedings of Knowledge
Discovery and Data Mining 1996, pages 267?270.
J. Piskorski, P. Homola, M. Marciniak, A. Mykowiecka,
A. Przepi?rkowski, and M. Wolin?ski. 2004. Information
Extraction for Polish Using the Sprout Platform. Proceed-
ings of ISMIS 2004, Zakopane.
J. Piskorski. 2005. Named-entity Recognition for Polish with
SProUT. In Proceedings of IMTCI 2004, LNCS Vol 3490,
Warsaw, Poland.
J. Piskorski and M. Sydow. 2007. Usability of String Distance
Metrics for Name Matching Tasks in Polish. In progress.
B. Pouliquen, R. Steinberger, C. Ignat, I. Temnikova, A. Widi-
ger, W. Zaghouani and J. ?i?ka. 2005. Multilingual person
name recognition and transliteration. CORELA - Cognition,
Repr?sentation, Langage. Num?ros sp?ciaux, Le traitement
lexicographique des noms propres, ISSN 1638-5748.
T. Smith and M. Waterman. 1981. Identification of Common
Molecular Subsequences. Journal of Molecular Biology,
147:195?197.
E. Ukkonen. 1992. Approximate String Matching with q-
grams and Maximal Matches. Theoretical Computer Sci-
ence, 92(1):191?211.
D. Weiss. 2007. Korpus Rzeczpospolitej. Web document:
http://www.cs.put.poznan.pl/dweiss/rzeczpospolita
W. Winkler. 1999. The state of record linkage and current re-
search problems. Technical report, U.S. Bureau of the Cen-
sus, Washington, DC.
34
Proceedings of the 4th Biennial International Workshop on Balto-Slavic Natural Language Processing, pages 84?93,
Sofia, Bulgaria, 8-9 August 2013. c?2010 Association for Computational Linguistics
On Named Entity Recognition in Targeted Twitter Streams in Polish
Jakub Piskorski
Linguistic Engineering Group
Polish Academy of Sciences
Jakub.Piskorski@ipipan.waw.pl
Maud Ehrmann
Department of Computer Science
Sapienza University of Rome
ehrmann@di.uniroma1.it
Abstract
This paper reports on some experiments
aiming at tuning a rule-based NER sys-
tem designed for detecting names in Pol-
ish online news to the processing of tar-
geted Twitter streams. In particular, one
explores whether the performance of the
baseline NER system can be improved
through the incremental application of
knowledge-poor methods for name match-
ing and guessing. We study various set-
tings and combinations of the methods and
present evaluation results on five corpora
gathered from Twitter, centred around ma-
jor events and known individuals.
1 Introduction
Recently, Twitter emerged as an important so-
cial medium providing most up-to-date informa-
tion and comments on current events of any kind.
This results in an ever-growing interest of vari-
ous organizations in tools for real-time monitor-
ing of Twitter streams to collect their business-
specific information therefrom for analysis pur-
poses. Since monitoring the entire Twitter stream
appears to be unfeasible due to the high volume
of published tweets, one usually monitors targeted
Twitter streams, i.e., streams of tweets potentially
satisfying specific information needs.
Applications for monitoring Twitter streams
usually require named entity recognition (NER)
capacity. However, due to the nature of Twitter
messages, i.e., being short, noisy, written in an in-
formal style, lacking punctuation and capitaliza-
tion, containing misspellings, non-standard abbre-
viations, and non grammatically correct sentences,
the application of even basic NLP tools (trained on
formal texts) on tweets usually results in poor per-
formances. In the case of well-formed texts such
as online news, exploitation of contextual clues is
crucial to named entity identification and classifi-
cation (e.g., ?Mayor of ? in the left context of a cap-
italized token is a reliable pattern to classify it as
city name). Such external evidence is often miss-
ing in tweets, and entity names are frequently in-
complete, abbreviated or glued with other words.
Furthermore, deployment of supervised ML-based
techniques for NER from tweets is challenging
due to the dynamic nature of Twitter.
In this paper, we report on experiments aiming
at tuning a rule-based NER system, initially de-
signed for detecting names in Polish online news,
to the processing of targeted Twitter streams. In
particular, we explore whether the performance of
the baseline NER system can be improved through
the utilization of knowledge-poor methods (based
on string distance metrics) for name matching
and name guessing. In comparison to English,
Polish is a free-word order and highly inflective
language, with particularly complex declension
paradigm of proper names, which makes NER for
Polish a more difficult task.
The remaining part of the paper is structured
as follows. First, Section 2 provides information
on related work. Next, Section 3 describes the
baseline NER system and the knowledge-poor en-
hancements. Subsequently, Section 4 presents the
evaluation results. Finally, Section 5 gives a sum-
mary and an outlook as regards future research.
2 Related Work
The problem of NER has gained lot of attention in
the last two decades and a vast bulk of research
on development of NER from formal texts ex-
ists (Nadeau and Sekine, 2007). Although most of
the reported work focused on NER for major lan-
guages, efforts on NER for Polish have also been
reported. (Piskorski, 2005) describes a rule-based
NER system for Polish that covers the classical
named-entity types, i.e., persons, locations, orga-
nizations, as well as numeral and temporal expres-
84
sions. (Marcin?czuk and Piasecki, 2007) and (Mar-
cin?czuk and Piasecki, 2010) report on a memory-
based learning and Hidden Markov Model ap-
proach resp. to automatic extraction of informa-
tion on events in the reports of Polish Stockhold-
ers, which involves NER. Also in (Lubaszewski,
2007) and (Lubaszewski, 2009) some general-
purpose information extraction tools for Polish
are addressed. Efforts related to creation of a
dictionary of Warsaw urban proper names ori-
ented towards NER is reported in (Savary et al,
2009; Marciniak et al, 2009). (Gralin?ski et al,
2009) present NERT, another rule-based NER sys-
tem for Polish which covers similar types of NEs
as (Piskorski, 2005). Finally, some efforts on
CRF-based NER methods for Polish are reported
in (Waszczuk et al, 2010) and (Marcin?czuk and
Janicki, 2012).
While NER from formal texts has been well
studied, relatively little work on NER for Twit-
ter was reported. (Locke and Martin, 2009) pre-
sented a SVM-based classifier for classifying per-
sons, locations and organizations in Twitter. (Rit-
ter et al, 2011) described an approach to segmen-
tation and classification of a wider range of names
in tweets based on CRFs (using POS and shallow
parsing features) and Labeled LDA resp. (Liu et
al., 2011) proposed NER (segmentation and clas-
sification) approach for tweets, which combines
KNN and CRFs paradigms. The reported preci-
sion/recall figures are significantly lower than the
state-of-the-art results for NER from well-formed
texts and oscillate around 50-80%. Better results
were reported in case of extracting names from
targeted tweets (person names from tweets on
live sport events) (Choudhury and Breslin, 2011).
(Nebhi, 2012) presented a rule-based NER system
for detecting persons, organizations and locations
which exploits an external global knowledge base
on entities to disambiguate NE type. (Liu et al,
2012) proposed a factor graph-based approach to
jointly conducting NER and NEN (Named Entity
Normalization), which improves F-measure per-
formance of NER and accuracy of NEN when
run sequentially. An Expectation-Maximization
approach to NE disambiguation problem was re-
ported by (Davis et al, 2012). Finally, (Li et al,
2012) presented an unsupervised system for ex-
tracting (no classification) NEs in targeted Twitter
streams, which exploits knowledge gathered from
the web and exhibits comparable performance to
the supervised approaches mentioned earlier.
Most of the above mentioned work on NER in
tweets focused on English. To our best knowledge
no prior work on NER in tweets in Polish has been
reported, which makes our effort a pioneering con-
tribution in this specific field. Our work also con-
tributes to NER from targeted Twitter streams.
3 Named Entity Extraction from
Targeted Tweets in Polish
The objective of this work is to explore vari-
ous linguistically lightweight strategies to adapt
an existing news-oriented rule-based NER system
for Polish to the processing of tweets in targeted
Twitter streams. Starting from the adaptation of
a NER rule-based system to the processing of
tweets (Section 3.1), we incrementally refine the
approach with, first, the introduction of a string
similarity-based name matching step (Section 3.2)
and, second, the exploitation of corpus statistics
and knowledge-poor method for name guessing
(Section 3.3).
3.1 NER Grammar for Polish
The starting point of our explorations is an exist-
ing NER system for Polish, modeled as a cascade
of finite-state grammars using the EXPRESS for-
malism (Piskorski, 2007). Similarly to rule-based
approaches to NER for many other Indo-European
languages, the grammars consist of a set of extrac-
tion patterns for person, organization and location
names. The patterns exploit both internal (e.g.,
company designators) and external clues (e.g., ti-
tles and functions of a person, etc.) for name de-
tection and classification; a simple extraction pat-
tern for person names can be illustrated as follows:
PER :> ( ( gazetteer & [TYPE: "firstname",
SURFACE: #F] )
( gazetteer & [TYPE: "initial",
SURFACE: #I] ) ?
( surname-candidate & [SURFACE: #L] )
):name
-> name: person & [NAME: #FULL-NAME]
& #full_name := ConcWithBlanks(#F,#I,#L).
This rule first matches a sequence consisting of: a
first name (through a gazetteer look-up), an op-
tional initial (gazetteer look-up as well) and, fi-
nally, a sequence of characters considered as sur-
name candidate (e.g., capitalized tokens), which
was detected by a lower-level grammar1 and
is represented as a structure of type surname-
candidate. The right-hand side of the extraction
1Lower-level grammar extract small-scale structures
which might constitute parts of named entities.
85
pattern specifies the output structure of type per-
son with one attribute called NAME, whose value
is simply a concatenation of the values of the vari-
ables #F, #I and #L assigned to the surface forms
of the matched first name, initial and surname can-
didate respectively.
Overall the grammar contains 15 extraction pat-
terns for person names, 10 for location names,
and 10 for organization names. It relies on a
huge gazetteer of circa 294K entries, which is
an extended version of the gazetteer described
in (Savary and Piskorski, 2011) and includes, i.a.,
39K inflected forms of both Polish and foreign
first names, 86K inflected forms of surnames, 5K
of organisation names (only partially inflected),
10K of inflected location names (e.g., city names,
country names, rivers, etc.). No morphological an-
alyzer for Polish is used and only a tiny fraction of
the extraction patterns relies on morphological in-
formation (encoded in the gazetteer). In this orig-
inal grammar, the patterns are divided into sure-
fire patterns and less reliable patterns (whose pre-
cision is expected to be lower). The latter ones
are patterns that rely solely on gazetteer informa-
tion (simple look-up), which might have ambigu-
ous interpretation, e.g., patterns that only match
first names in text. When applied on conven-
tional online news, the performance of this orig-
inal NER grammar oscillates around 85% in terms
of F-measure.
In order to process tweets, we slightly modi-
fied this grammar, mostly by simplifying it. Since
mentions of entities in tweets frequently occur as
single tokens (e.g., external evidence as in clas-
sical news is often missing), we did not keep the
distinction between sure-fire and less-reliable pat-
terns. Furthermore, the original NER grammar
?included? a mechanism (encoded directly in pat-
tern specification) to lemmatize the recognized
names as well as to extract various attributes such
as titles (e.g., ?Pan? (Mr.)) and position (e.g.,
?Prezydent? (president)) for persons. As we are
mainly interested in the detection and classifica-
tion of NEs while processing tweets, these func-
tionalities were not needed and the grammar sim-
ply extracts names and their type. This ?reduced?
NER grammar constitutes the baseline approach,
and will be referred to as BASE in the remain-
ing part of the paper. It is worth mentioning that
we tested as well a version of the grammar with
lower-cased lexical resources, but due to poor re-
sults (mainly due to high ambiguity of lower-case
lexical entries) we did not conduct further explo-
rations in this direction.
3.2 String distance-based Name Matching
In tweets, names are often abbreviated (e.g., ?Parl.
Europ.? and ?PE? are abbreviations of ?Parla-
ment Europejski?), glued to other words (e.g.,
?prezydent Komorowski? is sometimes written as
?prezydentKomorowski?) and misspelled variants
are frequent (e.g., ?Donlad Tusk? is a frequent
misspelling of ?Donald Tusk?). The NER gram-
mar ?as is? would fail to recognize the particular
names in the aforementioned examples. There-
fore, in order to improve the recall of the ?tweet
grammar?, we perform a second run deploying
string distance metrics (in the entire targeted Twit-
ter stream) for matching new mentions of names
previously recognized by the NER grammar (see
Section 3.1). Furthermore, due to the highly in-
flective character of Polish, we also expect to cap-
ture with string distance metrics non-nominative
mentions of names (e.g., ?Rzeczpospolitej - geni-
tive/dative/locative form of ?Rzeczpospolita? - the
name of a Polish daily newspaper), which the NER
grammar might have failed to recognize.
Inspired by the work reported in (Piskorski et
al., 2009) we explored the performance of sev-
eral string distance metrics. First, we tested the
baseline Levenshtein edit distance metric given
by the minimum number of character-level oper-
ations (insertion, deletion, or substitution) needed
to transform one string into another (Levenshtein,
1965). Next, we used an extension thereof, namely
Smith-Waterman (SW) metric (Smith and Water-
man, 1981), which additionally allows for vari-
able cost adjustment to the cost of a gap and vari-
able cost of substitutions (mapping each pair of
symbols from alphabet to some cost). We used a
variant of this metric, where the Smith-Waterman
score is normalized using the Dice coefficient (the
average length of strings compared).
Subsequently, we explored variants of the Jaro
metric (Jaro, 1989; Winkler, 1999). It considers
the number and the order of the common char-
acters between the two strings being compared.
More precisely, given two strings s = a1 . . . aK
and t = b1 . . . bL, we say that ai in s is common
with t if there is a bj = ai in t such that i ? R ?
j ? i+R, where R = bmax(|s|, |t|)/2c? 1. Fur-
thermore, let s? = a?1 . . . a
?
K? be the characters in
86
s which are common with t (with preserved order
of appearance in s) and let t? = b?1 . . . b
?
L? be de-
fined analogously. A transposition for s? and t? is
defined as any position i such that a?i 6= b
?
i. Let us
denote the number of transpositions for s? and t?
as Ts?,t? . The Jaro similarity is then calculated as:
J(s, t) =
1
3
? (
|s?|
|s|
+
|t?|
|t|
+
|s?| ? bTs?,t?/2c
|s?|
)
A Winkler variant of Jaro metric boosts this
similarity for strings with agreeing initial charac-
ters and is calculated as:
JW (s, t) = J(s, t) + ? ? boostp(s, t) ? (1? J(s, t))
where ? denotes the common prefix adjustment
factor (default value is 0.1) and boostp(s, t) =
min(|lcp(s, t)|, p). Here lcp(s, t) denotes the
longest common prefix between s and t. Further, p
stands for the upper bound of |lcp(s, t)|2 , i.e., up
from a certain length of lcp(s, t) the ?boost value?
remains the same.
The q-gram metric (Ukkonen, 1992) is based
on the intuition that two strings are similar if
they share a large number of character-level q-
grams. We used a variant thereof, namely skip-
gram metric (Keskustalo et al, 2003), which ex-
hibited better performance than any other variant
of character-level q-grams based metrics. It is
based on the idea that in addition to forming bi-
grams of adjacent characters, bigrams that skip
characters are considered. Gram classes are de-
fined that specify what kind of skip-grams are cre-
ated, e.g. {0, 1} class means that normal bigrams
are formed, and bigrams that skip one character.
In particular, we tested {0, 1} and {0, 2} classes.
Due to the nature of Twitter we expected skip-
grams to be particularly useful in our experiments.
Considering the declension paradigm of Polish
we also considered the basic CommonPrefix met-
ric introduced in (Piskorski et al, 2009), which is
based on the longest common prefix. It is calcu-
lated as:
CP (s, t) = (|lcp(s, t)|)2/|s| ? |t|
Finally, we evaluated the performance of
longest common sub-strings distance metric,
which recursively finds and removes the longest
2Here p is set to 6.
common sub-string in the two strings compared.
Let lcs(s, t) denote the first longest common sub-
string for s and t and let s?p denote a string ob-
tained by removing from s the first occurrence of
p in s. The LCS metric is calculated as:
LCS(s, t) =
?
?
??
?
??
0 if |lcs(s, t)| ? 2
|lcs(s, t)|+ LCS(s?lcs(s,t), t?lcs(s,t))
otherwise
The string distance-based name matching de-
scribed in this section will be referred to as
MATCH-X, with X standing for the name of the
string distance metric being used.
3.3 Name Clustering
Since contextual clues for recognizing names in
formal texts are often missing in tweets, we ad-
ditionally developed a rudimentary name guesser
to boost the recall. Let us also observe that using
string distance metrics described in Section 3.2 to
match all not yet captured mentions of previously
recognized names might not be easy due the fact
that the process of creating abbreviations in Twit-
ter is very productive, e.g., ?Rzeczpospolita? ap-
pears abbreviated as ? Rzepa?, Rzp. or ?RP, which
are substantially different from the original name.
The main idea beyond the name guesser is based
on the following assumption: given a targeted
Twitter stream, if a capitalized word n-gram has
a couple of ?similar? word n-grams in the same
stream, most of which are not recognized as valid
word forms, then such a group of n-grams word
are most likely named mentions of the same entity
(e.g., person, organization or location, etc.). To be
more precise, the name guesser works as follows.
1. Compute S = {s1, s2, ....sk} - a set of word
uni- and bigrams (cluster seeds) in the Twit-
ter stream3, where frequency(si) ? ?4 and
character ? length(si) ? 3 for all si ? S.
2. Create an initial set of singleton ?name? clus-
ters: C = {C1, C2, . . . , Ck} with Ci = {si}.
3. Build clusters of simmilar n-grams
around the selected uni- and bigrams
3The vast majority of names annotated in our test corpus
are either word unigrams or bigrams (see Section 4.1.)
4? We explored various values of this parameter, which is
described in Section 4.2
87
using the string distance metric m: As-
sign each word n-gram w in the Twitter
stream to at most one cluster Cj with
j ? arg minx?{1,2,...,k} distm(sx, w)
5, and
distm(sj , w) ? maxDist, where maxDist
is a predefined constant.
4. Iteratively merge most-simmilar clusters in
C: If ?Cx, Cy ? C with DIST (Cx, Cy) ?
DIST (Ci, Cj) for i, j ? {1, . . . , |C|}6 and
DIST (Cx, Cy) ? maxDist then C = C \
{Cx, Cy} ? (Cx ? Cy).
5. Discard ?small? clusters:
C = {Cx ? C : |Cx| ? 3}
6. Discard clusters containing high number of
n-grams, whose parts are valid word forms,
but not proper names: C = {Cx ?
C : ?w?Cx
WordForm?(w)
|Cx|
? 0.3}, where
WordForm?(w) = 1 if all the words
constituting the word n-gram w are valid
word forms, but not proper names, and
WordForm?(w) = 0 otherwise, e.g.,
WordForm?(Jan Grzyb) = 0 since Grzyb
(eng. mushroom) can be interpreted as a
valid word form, which is not a proper name,
whereas Jan has only proper name interpre-
tation.
7. Use the n-grams in the remaining clusters
in C (each of them is considered to contain
named mentions of the same entity) to match
names in the Twitter stream through simple
lexicon look-up.
For computing similarity of n-grams and merg-
ing clusters we used the longest common sub-
strings (LCS) metric which performed on average
best (in terms of F-measure) in the context of name
matching (see Section 3.2 and 4). For checking
whether tokens constitute valid word forms we ex-
ploited PoliMorf (Wolin?ski et al, 2012), a freely
available morphological dictionary of Polish, con-
sisting of circa 6.7 million word forms, includ-
ing proper names. Proper names are distinguished
from other entries in the aforementioned resource.
The name guesser sketched above will be re-
ferred to as CLUSTERING. Instead of building the
5We denote the distance between two strings x and y mea-
sured with the string distance metric m as distm(x, y)
6DIST (Cx, Cy) = ?s?Cx?t?Cy
distm(s,t)
|Cx|?|Cy|
(average
distance between strings in the two clusters)
name clusters around n-grams, whose frequency
exceeds certain threshold, we also tested building
clusters around least frequent n-grams (i.e., whose
frequency is ? 3), which will be referred to as
CLUSTERING-INFRQ. The name guesser runs ei-
ther independently or on top of the NER grammar
described in Section 3.1 in order to detect ?new?
names in the unconsumed part of the tweet collec-
tion, i.e., names recognized by the grammar are
preserved. It is important to emphasize that the
clustering-based name guesser only detects names
without classifying them.
4 Experiments
4.1 Dataset
We have gathered tweet collections using Twit-
ter search API7 focusing on some major events in
2012/2013 and on famous individuals, namely: (a)
Boston marathon bombings, (b) general comments
on Donald Tusk, the prime minister of Poland,
(c) discussion on the public comments of Antoni
Macierewicz (a politician of the Law and Justice
opposition party in Poland) on the Polish presi-
dent crash in Smolen?sk (Russia) in 2010, (d) de-
bate on the controversial firing of the journalist
Cezary Gmyz from one of the major Polish news-
papers Rzeczpospolita and, (e) a collection of ran-
dom tweets in Polish. Each tweet collection was
extracted using simple queries, e.g., "zamach AND
(Boston OR Bostonie)" ("attack" AND "?Boston"?
either in nominative of locative form) for collect-
ing tweets on the Boston bombings. From each
collection a subset of randomly chosen tweets was
selected for evaluation purposes. We will refer
to the latter as the test corpus, whereas the entire
tweet collections will be referred to as the stream
corpus.
In the stream corpus, we computed for each
tweet: (a) the text-like fraction of its body, i.e., the
fraction of the body which contains text, and (b)
the lexical validity, i.e., the percentage of tokens in
the text-like part of the body of the tweet which are
valid word forms in Polish8. Figure 1 and 2 show
the histograms for text-like fraction and lexical va-
lidity of the tweets in each collection in the stream
corpus. We can observe that large portion of the
tweets contains significant text-like part, which is
7https://dev.twitter.com
8For computing lexical validity we used
PoliMorf (Wolin?ski et al, 2012), already mentioned in
the previous section.
88
also lexically valid. Interestingly, the random col-
lection exhibits lower lexical validity, which is due
to more colloquial language used in the tweets in
this collection.
  10 20 30 40 50 60 70 80 90 1000
5
10
15
20
25
30 Boston Tusk Macierewicz Gmyz Random
Text-like fraction
Percent
age of T
weets
Figure 1: Text-like fraction of the tweets in each
collection.
  10 20 30 40 50 60 70 80 90 1000
5
10
15
20
25
30
35 Boston Tusk Macierewicz Gmyz Random
Text-lik litftra
ceo-enr
lPekgwks
?eer?
Figure 2: Lexical validity of the tweets in each
collection.
We built the test corpus by randomly select-
ing tweets whose text-like fraction of the body
was ? 80%, additionally checking the language
and removing duplicates. These tweets were af-
terwards manually annotated with person, loca-
tion and organization names, according to the fol-
lowing guidelines: consideration of unigram en-
tities, non-inclusion of titles, functions and alike,
non-inclusion of spurious punctuation marks and
exclusion of names starting with ?@?, since their
recognition as names is trivial.
The test corpus statistics are provided in Ta-
ble 1. We provide in brackets the number of tweets
in the corresponding tweet collections in the en-
tire stream corpus. In this test corpus, 86,7% of
the annotated names are word unigrams, whereas
bigrams constitute 12,7% of the annotated names
and 3- and 4-grams account only for a tiny frac-
tion (0,6%); this is in line with the characteristics
of the Twitter language, which favours quick and
simple expressions. For each collection, we com-
puted the name diversity as the ratio between en-
tity occurrences and unique entities, as well as the
average number of entities per tweet9. Targeted
stream corpora show a medium name diversity
(except for Boston and Gmyz collections, centred
on a very specific location and person name resp.)
and a high rate of entity per tweet (around 2.2), in
contrast with random corpus which shows a high
name diversity (0.79) for a low average number of
entity per tweets. Reported to the limited number
of characters in tweets (140), the important signifi-
cant number of entity per tweet in targeted streams
accounts, on the one hand, for the usefulness of
working on targeted streams and, on the other, for
the importance of NER in tweets.
Corpus #tweets name #names #PER #LOC #ORG
diversity per
tweet
Boston 198 0.24 2.16 34 298 96
(2953)
Tusk 232 0.36 2.42 393 88 80
(1186)
Macierewicz 303 0.32 2.17 494 60 104
(931)
Gmyz 310 0.24 2.09 471 18 159
(672)
Random 286 0.79 0.36 59 19 27
(7806)
Table 1: Test corpus statistics.
4.2 Evaluation
In our experiments we evaluated the performance
of (i) the NER grammar (BASE), a combina-
tion thereof with (ii) different name matching
strategies (MATCH) and (iii) different variants of
the name guesser (CLUSTERING, CLUSTERING-
INFRQ) and, finally, (iv) the combinations of all
techniques. Within the MATCH configuration, we
experimented all string distance metrics presented
in 3.2 but since Jaro, Jaro-Winkler and Smith-
Waterman metrics performed on average worse
than the others, we did not consider them in
further experiments. We selected the best per-
forming metric, LCS 10, as the one used by the
name guesser (CLUSTERING) in subsequent exper-
iments. As a complement, we measured the per-
formance of the name guesser alone to compare
it with BASE. Furthermore, name matching and
9In the limit of our reference corpora, i.e. entities of type
person, location and organization.
10Skip-grams was the other metric which exhibited similar
performance
89
name guessing algorithms were using the tweet
collections in the stream corpus (as quasi ?Twitter
stream window?) in order to gather knowledge for
matching/guessing ?new? names in the test corpus.
We measured the performance of the different
configurations in terms of Precision (P), Recall
(R) and F-measure (F), according to two differ-
ent schemes: exact match, where entity types and
both boundaries should match perfectly, and fuzzy
match, which allows for one name boundary re-
turned by the system to be different from the ref-
erence, i.e., either too short or too long on the left
or on the right, but not on both. Furthermore, since
the clustering-based name guesser described in 3.3
does not classify names, for any settings with this
technique we only evaluated name detection per-
formance, i.e., no distinction between name types
was made. The overall summary of the results for
the entire pool of tweet collections, is presented in
Table 3.
In the context of the CLUSTERING algorithm we
explored various settings as regards the minimum
frequency of an n-gram to be considered as clus-
ter seed (? parameter - see Section 3.3). More
precisely, we tested values in the range of 1 to
30 for all corpora and system settings which in-
cluded CLUSTERING, and compared the resulting
P/R and F figures. An example of a curve with P/R
values (exact match) of BASE-CLUSTERING algo-
rithm applied on the ?Boston? corpus with vary-
ing values of ? is given in Figure 3. One can ob-
serve and hypothesize that the frequency threshold
does not impact much the performance. Suchlike
curves for other settings were of a similar nature.
Therefore we decided to set the ? to 1 in all set-
tings reported in Table 3.
4.3 Results analysis
The performance of the NER grammars is surpris-
ingly good, both in case of exact and fuzzy match
evaluation. Except for random corpus (which
shows rather low performance with 55% precision
and 39% recall), precision figures oscillate around
85-95%, whereas recall is somewhat worse (60-
75%), as was to be expected. The low recall for
?Gmyz? corpus is due to the non-matching of a fre-
quently occurring person name. Precision and re-
call figures for each entity type for BASE are given
in Table 2. In general, recognition of organization
names appears to be more difficult (lower recall),
especially in the random corpus.
  1 0 2 3 4 5 6 7 8 19 11 10 12 13 14 15 16 17 18 09 01 00 02 03 04 05 06 07 08 29
99B1
9B09B2
9B39B4
9B59B6
9B79B8
1 ostnTuTkM atncii
Texet-tlik frtlafco-cxnPllgwlsclnwx?e?cfc?lrln?-?gcfl?cc?
Figure 3: Precision and Recall figures for BASE-
CLUSTERING applied on ?Boston? corpus, with
different frequency thresholds of n-grams to be
considered cluster seeds.
Corpus PER ORG LOC
P R P R P R
Boston 31.6 35.3 87.9 30.2 94.3 71.8
Tusk 87.6 71.2 82.4 35.0 89.9 70.5
Gmyz 85.5 32.5 82.8 15.1 88.9 44.4
Macierewicz 93.6 80.2 71.2 35.6 83.7 60.0
Random 56.7 55.9 0 0 53.3 42.1
Table 2: Precision/recall figures for person, or-
ganization and location name recognition (exact
match) with BASE.
Extending BASE with MATCH yields some im-
provements in terms of recall (including random
corpus), whereas precision either oscillates around
the figures achieved by BASE, or deteriorates. In
case of ?Gmyz? corpus, we can observe significant
gain in both recall and precision through using the
name matching step. With regard to the other cor-
pora, the reason for not obtaining a significant gain
could be due to two reasons: (a) the n-grams iden-
tified as similar to the names recognized by BASE
are already covered by BASE with some patterns
(e.g., inflected forms of many entities are stored in
the gazetteer), or (b) using string distance metrics
in the MATCH step might not be the best method to
capture mentions of a recognized entity, as exem-
plified in Table 4, where the mentions of a news-
paper Rzeczpospolita (captured by BASE) may be
significantly different, e.g., in terms of the charac-
ter length.
Regarding the results for CLUSTERING-INFRQ,
running it alone, yielded poor results for all cor-
pora, only in case of the?Gmyz? corpus a gain
could be observed. CLUSTERING performed better
than CLUSTERING-INFRQ for all corpora.
Deploying BASE with CLUSTERING on top of
it results in up to 1.5-6% (exact match) and 4-
90
EXACT MATCH
Method Boston Tusk Gmyz Macierewicz AVERAGE
P R F P R F P R F P R F P R F
BASE 85.6 59.6 70.2 87.7 65.9 75.3 85.3 28.5 42.8 90.5 71.3 79.8 87.3 56.3 67.0
BASE-MATCH-LEV 80.8 62.9 70.7 87.4 66.5 75.5 90.9 63.6 74.8 90.2 72.3 80.3 87.3 66.3 75.3
BASE-MATCH-SW 70.9 62.1 66.3 76.6 67.5 71.8 78.0 59.1 68.0 89.4 73.1 80.4 78.7 65.5 71.6
BASE-MATCH-J 67.7 62.1 64.8 79.3 68.1 73.3 60.9 48.3 53.9 60.0 73.3 65.9 67.0 63.0 64.5
BASE-MATCH-JW 63.2 62.1 62.7 75.5 68.3 71.7 48.2 48.9 48.6 58.0 74.0 65.0 61.2 63.3 62.0
BASE-MATCH-SKIP(0,1) 80.9 62.1 70.3 87.6 66.5 75.6 91.3 63.0 74.5 90.3 72.2 80.2 87.5 66.0 75.2
BASE-MATCH-SKIP(0,2) 80.9 62.1 70.3 87.7 66.3 75.5 91.5 63.0 74.6 90.6 72.2 80.4 87.7 65.9 75.2
BASE-MATCH-CP 80.2 59.6 68.4 87.7 66.0 75.3 83.5 58.6 68.9 90.2 71.4 79.7 85.4 63.9 73.1
BASE-MATCH-LCS 80.7 63.6 71.1 86.8 67.0 75.7 82.3 59.0 68.7 90.2 72.9 80.7 85 65.6 74.1
CLUSTERING 66.2 10.0 17.4 60.6 33.2 42.9 61.3 36.0 45.3 52.9 33.4 41.0 60.3 28.2 36.7
CLUSTERING-INFRQ 37.5 1.4 2.7 27.3 1.1 2.1 60.7 31.5 41.5 54.8 28.6 37.6 45.1 15.7 21.0
BASE-CUSTERING 86.8 67.8 76.1 91.1 72.7 80.9 80.6 61.0 69.4 86.3 74.6 80.0 86.2 69.0 76.6
BASE-CLUSTERING-INFRQ 89.7 65.0 75.3 89.4 69.3 78.1 81.2 58.5 68.0 89.9 74.2 81.3 87.6 66.8 75.7
BASE-MATCH-CLUSTERING 87.6 75.9 81.4 90.2 73.8 81.2 74.1 62.8 68.0 86.1 76.3 80.9 84.5 72.2 77.9
BASE-MATCH-CLUSTERING-INFRQ 90.0 73.4 80.8 88.6 70.4 78.5 74.3 60.3 66.6 89.6 75.8 82.1 85.6 70.0 77.0
FUZZY MATCH
Method Boston Tusk Gmyz Macierewicz AVERAGE
P R F P R F P R F P R F P R F
BASE 86.6 60.3 71.1 92.2 69.3 79.1 88.0 29.5 44.2 95.0 74.8 83.7 90.5 58.5 69.5
BASE-MATCH-LEV 81.7 63.6 71.5 92.3 70.2 79.8 93.6 65.4 77.0 94.9 76.1 84.5 90.6 68.8 78.2
BASE-MATCH-SW 73.3 64.3 68.5 80.8 71.3 75.8 91.4 67.6 77.7 94.2 77.1 84.8 84.9 70.1 76.7
BASE-MATCH-J 70.5 64.7 67.5 85.5 73.4 79.0 86.2 68.4 76.2 63.4 77.5 69.8 76.4 71.0 73.1
BASE-MATCH-JW 65.8 64.7 65.3 81.9 74.0 77.7 68.2 69.1 68.7 61.4 78.4 68.9 69.3 71.6 70.2
BASE-MATCH-SKIP(0,1) 81.8 62.9 71.1 92.3 70.1 79.6 94.0 64.8 76.7 95.1 76.0 84.5 90.8 68.5 78.0
BASE-MATCH-SKIP(0,2) 81.8 62.9 71.1 92.2 69.7 79.4 94.2 64.8 76.8 95.0 75.7 84.3 90.8 68.3 77.9
BASE-MATCH-CP 81.1 60.3 69.2 92.2 69.3 79.1 93.8 65.9 77.4 95.0 75.2 84.0 90.5 67.7 77.4
BASE-MATCH-LCS 81.6 64.3 71.9 92.4 71.3 80.5 93.1 66.7 77.7 94.9 76.7 84.9 90.5 69.8 78.8
CLUSTERING 83.1 12.6 21.9 96.4 52.8 68.2 89.2 52.3 66.0 87.7 55.5 68.0 89.1 43.3 56.0
CLUSTERING-INFRQ 87.5 3.3 6.3 68.2 2.7 5.1 91.1 47.2 62.2 94.2 49.1 64.5 85.3 25.6 34.5
BASE-CLUSTERING 93.1 72.7 81.6 96.9 77.4 86.0 94.5 71.4 81.4 91.7 79.3 85.1 94.1 75.2 83.5
BASE-CLUSTERING-INFRQ 95.5 69.2 80.2 95.9 74.3 83.7 96.4 69.4 80.7 96.9 79.9 87.6 96.2 73.2 83.1
BASE-MATCH-CLUSTERING 93.3 80.8 86.6 96.5 79.0 86.9 92.9 78.7 85.2 91.8 81.3 86.2 93.6 80.0 86.2
BASE-MATCH-CLUSTERING-INFRQ 95.1 77.6 85.5 96.0 76.3 85.0 94.5 76.7 84.7 96.6 81.8 88.6 95.6 78.1 86.0
Table 3: Precision, Recall and F-measure figures for exact (top) and fuzzy match (bottom). The best
results are highlighted in bold.
CEZARY GMYZ zwolniony z "Rzeczpospolitej". To efekt spotkania z
Zarza?dem i Rada? Nadzorcza? wydawcy dziennika http://t.co/QspE3edh
@agawaa ...usi?ujesz czepic? sie szczeg??u, gdy istota sprawy jest taka:
Rzepa/Gmyz pitolili bez sensu.
Konflikt w Rzepie? Ta ca?a sytuacja na to wskazuje. Gmyz sie? nie wycofuje,
a Rzepa jak najbardziej.
@volanowski Nowa linia: Gmyz wyrzucony z Rzepy czyli PO we wszystkich
sprawach smolen?skich jest cacy i super. Ludzie na to nie p?jda.
@TomaszSkory Byc? moz?e "Rz" i Gmyz p?aca? teraz w?as?nie za "skr?ty
mys?lowe" swoich informator?w. Dlaczego RMF nie p?aci za "skr?ty" swoich?
Gmyz wylecia? z RP, a Ziemkiewicz straci? Subotnik? Nie lepiej by?o nieco
zejs?c? z 3.50 z?, czy chodzi o cos? zupe?nie innego?
Gmyz wyrzucony z "Rzeczpospolitej". "Dzisiaj zwolniono mnie dyscyp-
linarnie": Cezary Gmyz straci? prace? w "Rzeczp... http://t.co/ObZIxXML
Table 4: Examples of various ways of referring to
a newspaper Rzeczpospolita in tweets.
10% (fuzzy match) gain in F-measure compared
to BASE (mainly thanks to gain in recall), ex-
cept ?Gmyz? corpus, where the gain is higher.
The average gain over the four targeted corpora
against the best combination of BASE-MATCH in
F-measure is 1.3%. We observed comparable im-
provement for the random corpus. It turned out
CLUSTERING often contributes to the recognition
of names glued to other words and/or character se-
quences.
Combining BASE with MATCH-LCS and CLUS-
TERING/CLUSTERING-INFRQ yields further im-
provements against the other settings. In par-
ticular, the gain in F-measure of BASE-MATCH-
CLUSTERING against BASE, measured over the
four targeted corpora, is 10.9% and 16.7% for ex-
act and fuzzy match respectively (mainly due to
gain in recall).
Considering the nature of Twitter messages the
average F-measure score over the four targeted
corpora for BASE-MATCH-CLUSTERING, amount-
ing to 77.9% (exact match) and 86.2% (fuzzy
match) can be seen as a fairly good result. Al-
though the difference in some of the correspond-
ing scores for exact and fuzzy match appear sub-
stantial, it is worth mentioning that CLUSTERING
algorithm often guesses name candidates that are
either preceded or followed by some characters
not belonging to the name itself, which is pe-
nalized in exact-match evaluation. This problem
could be alleviated through deployment of heuris-
tics to trim such ?unwanted? characters. Another
source of false positives extracted by CLUSTER-
ING is the fact that this method might, beyond
person, organization and location types, recognize
any kind of NEs, which, even not very frequent, is
penalized since they are not present in our refer-
ence corpus.
In general, considering the shortness of names
in Twitter, the major type of errors in all settings
are either added or missed entities, but more rarely
overlapping problems. One of the main source of
errors is due to the fact that single-token names,
which are frequent in tweets, often exhibit type
91
ambiguity. Once badly recognized, these errors
are propagated over the next processing steps.
5 Conclusions and Outlook
In this paper we have reported on experiments on
tuning an existing finite-state based NER gram-
mar for processing formal texts to NER from
targeted Twitter streams in Polish through com-
bining it with knowledge-poor techniques for
string distance-based name matching and corpus
statistics-based name guessing. Surprisingly, the
NER grammar alone applied on the four test cor-
pora (including circa 2300 proper names) yielded
P, R, and F figures for exact (fuzzy) matching
proper names (including: person, organization and
locations) of 87.3% (90.5%), 56.3% (58.5) and
67% (69.5%) resp., which can be considered fairly
reasonable result, though some variations across
tweet collections could be observed (depending
on the topic and how people ?tweet? about).
The integration of the presented knowledge-poor
techniques for name matching/guessing resulted
in P, R and F figures for exact (fuzzy) match-
ing names of 84.5% (93.6%), 72.2% (80.0) and
77.9% (86.2%) resp. (setting with best F-measure
scores), which constitutes a substantial improve-
ment against the grammar-based approach. We
can observe that satisfactory-performing NER
from targeted Twitter streams in Polish can be
achieved in a relatively straightforward manner.
As future work to enhance our experiments, we
envisage to: (a) enlarge the pool of test corpora,
(b) carry out a more thorough error analysis, (c)
test a wider range of string distance metrics (Co-
hen et al, 2003), (d) study the applicability of the
particular NER grammar rules w.r.t. their useful-
ness in NER in targeted Twitter streams and (e),
compare our approach with an unsupervised ML-
approach, e.g. as in (Li et al, 2012).
Acknowledgments
The authors gratefully acknowledge the support of
the ERC Starting Grant MultiJEDI No. 259234
and the Polish National Science Centre grant N
N516 481940 ?Diversum?.
References
Smitashree Choudhury and John Breslin. 2011. Ex-
tracting Semantic Entities and Events from Sports
Tweets. In Proceedings of the 1st Workshop on
Making Sense of Microposts (#MSM2011), pages
22?32.
William W. Cohen, Pradeep Ravikumar, and
Stephen E. Fienberg. 2003. A Comparison of
String Distance Metrics for Name-matching Tasks.
In Proceedings of the IJCAI-2003 Workshop on
Information Integration on the Web (IIWeb-03),
pages 73?78.
Alexandre Davis, Adriano Veloso, Altigran S. da Silva,
Wagner Meira, Jr., and Alberto H. F. Laender.
2012. Named Entity Disambiguation in Streaming
Data. In Proceedings of the 50th Annual Meeting
of the Association for Computational Linguistics:
Long Papers - Volume 1, ACL ?12, pages 815?824,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Filip Gralin?ski, Krzysztof Jassem, Micha? Marcin?czuk,
and Pawe? Wawrzyniak. 2009. Named entity recog-
nition in machine anonymization. In Recent Ad-
vances in Intelligent Information Systems, pages
247?260, Warsaw. Exit.
Mathew Jaro. 1989. Advances in record linking
methodology as applied to the 1985 census of Tampa
Florida. Journal of the American Statistical Society,
84(406):414?420.
Heikki Keskustalo, Ari Pirkola, Kari Visala, Erkka
Lepp?nen, and Kalervo J?rvelin. 2003. Non-
adjacent Digrams Improve Matching of Cross-
lingual Spelling Variants. In Proceedings of SPIRE,
LNCS 22857, Manaus, Brazil, pages 252?265.
Vladimir Levenshtein. 1965. Binary Codes for Cor-
recting Deletions, Insertions, and Reversals. Dok-
lady Akademii Nauk SSSR, 163(4):845?848.
Chenliang Li, Jianshu Weng, Qi He, Yuxia Yao, An-
witaman Datta, Aixin Sun, and Bu-Sung Lee. 2012.
TwiNER: Named Entity Recognition in Targeted
Twitter Stream. In Proceedings of the 35th Inter-
national ACM SIGIR Conference on Research and
Development in Information Retrieval, SIGIR ?12,
pages 721?730, New York, NY, USA. ACM.
Xiaohua Liu, Shaodian Zhang, Furu Wei, and Ming
Zhou. 2011. Recognizing Named Entities in
Tweets. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguis-
tics: Human Language Technologies - Volume 1,
pages 359?367, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Xiaohua Liu, Ming Zhou, Furu Wei, Zhongyang Fu,
and Xiangyang Zhou. 2012. Joint Inference of
Named Entity Recognition and Normalization for
Tweets. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguis-
tics: Long Papers - Volume 1, ACL ?12, pages 526?
535, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
92
Brian Locke and James Martin. 2009. Named Entity
Recognition: Adapting to Microblogging. Senior
Thesis, University of Colorado.
Wies?aw Lubaszewski. 2007. Information extraction
tools for polish text. In Proc. of LTC?07, Poznan?,
Poland, Poznan?. Wydawnictwo Poznanskie.
Wies?aw Lubaszewski. 2009. S?owniki komputerowe i
automatyczna ekstrakcja informacji z tekstu. AGH
Uczelniane Wydawnictwa Naukowo-Dydaktyczne,
Krak?w.
Micha? Marcin?czuk and Maciej Janicki. 2012. Opti-
mizing CRF-Based Model for Proper Name Recog-
nition in Polish Texts. In A. Gelbukh, editor,
CICLing 2012, Part I, volume 7181 of Lecture
Notes in Computer Science (LNCS), pages 258?
?269. Springer, Heidelberg.
Micha? Marcin?czuk and Maciej Piasecki. 2007. Pat-
tern extraction for event recognition in the reports
of polish stockholders. In Proceedings of IMCSIT?
AAIA?07, Wis?a, Poland, pages 275?284.
Micha? Marcin?czuk and Maciej Piasecki. 2010.
Named Entity Recognition in the Domain of Pol-
ish Stock Exchange Reports. In Proceedings of In-
telligent Information Systems 2010, Siedlce, Poland,
pages 127?140.
Ma?gorzata Marciniak, Joanna Rabiega-Wis?niewska,
Agata Savary, Marcin Wolin?ski, and Celina Heliasz.
2009. Constructing an Electronic Dictionary of Pol-
ish Urban Proper Names. In Recent Advances in In-
telligent Information Systems. Exit.
David Nadeau and Satoshi Sekine. 2007. A Sur-
vey of Named Entity Recognition and Classification.
Lingvisticae Investigationes, 30(1):3?26.
Kamel Nebhi. 2012. Ontology-Based Information Ex-
traction from Twitter. In Proceedings of the COL-
ING 2012 IEEASM Workshop, Mumbai, India.
Jakub Piskorski, Karol Wieloch, and Marcin Sydow.
2009. On Knowledge-poor Methods for Person
Name Matching and Lemmatization for Highly
Inflectional Languages. Information Retrieval,
12(3):275?299.
Jakub Piskorski. 2005. Named-Entity Recognition for
Polish with SProUT. In LNCS Vol 3490: Proceed-
ings of IMTCI 2004, Warsaw, Poland.
Jakub Piskorski. 2007. ExPRESS ? Extraction Pat-
tern Recognition Engine and Specification Suite. In
Proceedings of FSMNLP 2007.
Alan Ritter, Sam Clark, Mausam, and Oren Etzioni.
2011. Named Entity Recognition in Tweets: An Ex-
perimental Study. In Proceedings of the 2011 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP 2011), pages 1524?1534, Ed-
inburgh, Scotland, UK. Association for Computa-
tional Linguistics.
Agata Savary and Jakub Piskorski. 2011. Language
Resources for Named Entity Annotation in the Na-
tional Corpus of Polish. Control and Cybernetics,
40(2):361?391.
Agata Savary, Joanna Rabiega-Wis?niewska, and
Marcin Wolin?ski. 2009. Inflection of Polish Multi-
Word Proper Names with Morfeusz and Multiflex.
LNAI, 5070.
T. Smith and M. Waterman. 1981. Identification
of Common Molecular Subsequences. Journal of
Molecular Biology, 147:195?197.
Esko Ukkonen. 1992. Approximate String Matching
with q-grams and Maximal Matches. Theoretical
Computer Science, 92(1):191?211.
Jakub Waszczuk, Katarzyna G?owin?ska, Agata Savary,
and Adam Przepi?rkowski. 2010. Tools and
Methodologies for Annotating Syntax and Named
Entities in the National Corpus of Polish. In Pro-
ceedings of the International Multiconference on
Computer Science and Information Technology (IM-
CSIT 2010): Computational Linguistics ? Applica-
tions (CLA?10), pages 531?539.
William Winkler. 1999. The State of Record Link-
age and Current Research Problems. Technical re-
port, Statistical Research Division, U.S. Bureau of
the Census, Washington, DC.
Marcin Wolin?ski, Marcin Mi?kowski, Maciej Ogrod-
niczuk, Adam Przepi?rkowski, and ?ukasz Sza-
lkiewicz. 2012. PoliMorf: A (not so) new open
morphological dictionary for Polish. In Proceedings
of the Eighth International Conference on Language
Resources and Evaluation, LREC 2012, pages 860?
864.
93
