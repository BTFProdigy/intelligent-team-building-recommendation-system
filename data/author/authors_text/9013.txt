Task-based dialog management using an agenda 
Wei Xu and Alexander I. Rudnicky 
School of Computer Science - Carnegie Mellon University 
5000 Forbes Ave - Pittsburgh, PA 15213 
{xw, air\] @cs. cmu. edu 
Abstract 
Dialog man tigement addresses two specific 
problems: (1) providing a coherent overall 
structure to interaction that extends beyond the 
single turn, (2) correctly managing mixed- 
initiative interaction. We propose a dialog 
management architecture based on the following 
elements: handlers that manage interaction 
focussed on tightly coupled sets of information, 
a product that reflects mutually agreed-upon 
information and an agenda that orders the topics 
relevant to task completion. 
1 Introduction 
Spoken language interaction can take many 
forms. Even fairly simple interaction can be very 
useful, for example in auto-attendant systems. 
For many other applications, however, more 
complex interactions eem necessary, either 
because users cannot always be expected to 
exactly specify what they want in a single 
utterance (e.g., obtaining schedule information) 
or because the task at hand requires ome degree 
of exploration of complex alternatives (e.g., 
travel planning). Additionally, unpredictable 
complexity is introduced through error or 
misunderstanding and the system needs to detect 
and deal with these cases. We are interested in 
managing interaction in the context of a goal- 
oriented task that extends oveg multiple tums. 
Dialog management i  the context of purposeful 
tasks must solve two problems: (1) Keep track 
of the overall interaction with a view to ensuring 
steady progress towards task completion. That 
is, the system must have some idea of how much 
of the task has been completed and more 
importantly some idea of what is yet to be done, 
so that it can participate in the setting of 
intermediate goals and generally shepherd the 
interaction towards a successful completion of 
the task at hand. (2) Robustly handle deviations 
from the nominal progression towards problem 
solution. Deviations are varied: the user may ask 
for something that is  not satisfiable (i. e., 
proposes a set of mutually-incompatible 
constraints), the user may misspeak (or, more 
likely, the system may misunderstand) a request 
and perhaps cause an unintended (and maybe 
unnoticed) deviation from the task. The user 
might also underspecify a request while the 
system requires that a single solution be chosen. 
Finally the user's conception of the task might 
deviate from the system's (and its developers) 
conception, requiring the system to alter the 
order in which it expects to perform the task. 
Ideally, a robust dialog management architecture 
can accommodate all of these circumstances 
within a single framework. 
We have been exploring dialog management 
issues in the context of the Communicator \[3\] 
task. The Communicator handles a complex 
travel task, consisting of air travel, hotels and 
car reservations. 
2 Model ing Dialog 
Existing approaches to dialog management are 
difficult to adapt o the current problem because 
they either impose a rigid structure on the 
interaction or because they are not capable of 
managing data structures beyond a certain level 
of complexity. Call-flow based systems (more 
generally, graph-based systems) handle the 
complexity of dialog management by explicitly 
enumerating all possible dialog states, as well as 
allowable transitions between states. This serves 
the purpose of partitioning the problem into a 
42 
finite set of states, with which can be associated 
topic-specific elements (such as grammar, 
prompts, help and interactions with other system 
components, e.g., database interaction). 
Transition between states is predicated on the 
occurrence of specific events, either the user's 
spoken inputs or through (e.g.) a change in back- 
end state. It is the nature of these systems that 
the graphs are often but not exclusively trees. 
Except for the simplest tasks, graph systems 
have several limitations: Unless the graph is 
carefully designed, users will find themselves 
unable to switch to a topic that is coded in a 
different sub-tree without going through the 
common par~e~t of the two. Often this is through 
the root node-of the dialog. Similarly it is not 
always possible to navigate an existing tree, in 
order, e.g., to correct information supplied in an 
earlier node. 
beforehand the exact type of trip an individual 
might take (though the building blocks of an 
itinerary are indeed known). The system benefits 
from being able to construct the itinerary 
dynamically; we denote these solution objects 
products. Users also expect to be able to 
manipulate and inspect the itinerary under 
construction. By contrast, frame systems do not 
afford the user the ability to manipulate the 
form, past supplying fillers for slots. The 
exception is the selection of an item from a 
solution set. We do not abandon the concept of a 
form altogether: an itinerary is actually a 
hierarchical composition of forms, where the 
forms in this case correspond to tightly-bound 
slots (e.g., those corresponding tothe constraints 
on a particular flight leg) and which can be 
treated as part of the -same topic. 
Frame-based systems provide an alternate, more 
flexible approach. Here the problem is cast as 
form filling: the form specifies all relevant 
information (slots) for an action. Dialog 
management consists of monitoring the form for 
completion, setting elements as these are 
specified by the user and using the presence of 
empty slots as a trigger for questions to the user. 
Form-filling does away with the need to specify 
a particular order in which slots need to be filled 
and allows for a more natural, unconstrained, 
form of input. While ideally suited for tasks that 
can be expressed in terms of filling a single 
form, form-filling can be combined with graph 
representations (typically ergodic) to support a 
set of (possibly) related activities, each of which 
can be cast into a form-filling format. 
Both graph and frame systems hare the property 
that the task usually has a fixed goal which is 
achieved by having the user specify information 
(fill slots) on successive turns. Using a filled out 
form the system performs ome action, such as 
information retrieval. While this capability 
encompasses a large number of useful 
applications it does not necessarily extend to 
more complex tasks, for example ones where the 
goal is to create a complex data object (e.g. \[1\]). 
We have been building a system that allows 
users to construct travel itineraries. This domain 
poses several problems: there is no "form" as 
such to fill out, since we do not know 
3 Task Structure and Scripts 
Intuitively (as well as evident from our 
empirical studies of human travel agents and 
clients) travel planning develops over time as a 
succession of episodes, each focused on a 
specific topic (such as a given flight leg, a hotel 
in a particular city, etc.). Users treat he task as a 
succession of topics, each of which ought to be 
discussed in full and closed, before moving on 
to the next topic. Topics can certainly be 
revisited, but doing so corresponds to an explicit 
conversational move on the part of the 
participants. 
Our first dialog manager took advantage of this 
task structure (\[3\]). By analogy to what we 
observed in the human-human data we refer to it 
as a script-based ialog manager. Script in this 
context simply refers to an explicit sequencing 
of task-related topics. Each topic is expressed as 
a form-filling task, with conventional free-order 
input allowed for form slots and a slot-state 
driven mixed-initiative interaction (i.e., ask the 
user about any empty slot). The topic-specific 
form is actually composed of two parts: 
constraint slots (typically corresponding to 
elements of a query) and a solution slot 
(containing the result of an executed query). 
43 
The control strategy is also actually more 
complex: slots are pre-ordered based on their 
(domain-derived) ability to constrain the 
solution; this ordering provides a default 
sequence in which the system selects elements to 
ask the user about. Control is predicated on the 
state of a slot (whether constraint or solution). 
The state can either be "empty", in which case 
the system should ask the user for a value, filled 
with a single value, in which case it is 
"complete", or filled with multiple values. The 
last case is cause to engage the user in a 
clarification sub-dialog whose goal is to reduce 
multiple values to a single value, either by 
selecting-ma item in the solution set or by 
restating a-constraint. Figure 1 shows the 
structure of the Flight Leg topic in the script- 
based system. 
Fligh, Leg 
"'?~??.o ~ 19e,,'tinatian 
~ Database lookup 
Available \]lights 
Figure 1 Task-based ialog control in a script-based 
system, as determined by the structure of a compound 
schema, with contributions from three simple schema. 
4 An Agenda-based Architecture 
While capable of efficiently handling routine 
travel arrangements, the script-based approach 
has a number of perceived limitations: the script 
is very closely identified with the product data 
structure. Specifically, we used a fixed product 
structure that served as a form to fill out. While 
the entire form does not need to be filled out to 
create a valid itinerary, it nevertheless et limits 
on what the user can construct. Instead we 
wanted a form structure that could be 
dynamically constructed over the course of a 
session, with contributions from both the user 
and the system. The script-based approach also 
seemed to make navigation over the product 
difficult. While we implemented a simple undo 
and correction mechanism that allowed the user 
to revisit preceding product elements, users had 
difficulty using it correctly. While some of the 
difficulty could be traced to inadequate support 
of orientation, the source was more likely the 
inability of the system to treat the product 
structure independent of the script. 
We sought to address-these problems by 
introducing two new data structures: an agenda 
to replace a fixed script and a dynamic product 
that can evolve over the course of a session. In 
the agenda-based system, the product is 
represented as a tree, which reflects the natural 
hierarchy, and order, of the information eeded 
to complete the task. A dynamic product is 
simply one that can be modified over the course 
of a session, for example by adding legs to a trip 
as these are requested by the user rather than 
working from a fixed form. Operationally, this 
means providing a set of operators over tree 
structures and making these available to the user 
and to the system. In our case, we  defined a 
library of sub-trees (say air travel legs or local 
arrangements) and a way to attach these to the 
product structure, triggered either by the setting 
of particular values in the existing tree or 
through explicit requests on the part of the user 
("and then I'd like to fly to Chicago"). 
Each node in the product ree corresponds to a 
handler, which encapsulates computation 
relevant to a single information item. All 
handlers have the same form: they specify a set 
of receptors corresponding to input nets, a 
transform to be applied to obtain a value and a 
specification of what the system might say to the 
user in relation to the information governed by 
the handler. Handlers correspond to the schema 
and compound schema of the script-based 
system (see Figure 1). 
The agenda is an ordered list of topics, 
represented by handlers that govern some single 
item or some collection of information. The 
agenda specifies the overall "plan" for carrying 
out a task. The system's priorities for action are 
captured by the agenda, an ordered list of 
handlers generated through traversal of the 
product structure. The handler on the top of the 
agenda has the highest priority and represents 
the focused topic. A handler can capture 
44 
relevant input from the user and can generate 
prompts to the user. A single handler deals only 
with a mini dialog centering on a particular 
piece of information (e.g. departure date). The 
agenda is a generalization f a stack. It indicates 
both the current focus of interaction (i.e., the 
top-most handler) as well as all undealt-with 
business, and captures the order in which such 
business should be dealt with. (The system's 
high-level goal is to ensure that all values in the 
current product ree have valid settings.) As all 
items in the agenda are potentially activatable 
through what the user speaks, the user has 
corresponding control over the topic in focus. 
The agenda .aLso contains generic handlers that 
sort to the bottom-of the agenda. These can be 
used to consume any inputs that are not caught 
by product-derived handlers (for example, 
requests for help). Figure 2 shows how a 
simplest handler looks like 
Handler User 
HandleOutput 
If !Valid 
NLG("query user_name") 
Return focused_waiting_answer 
Endif 
Return pass_control 
HandleInput 
If \[name I identification\] 
ReadProfile 
Validate 
Endif 
Return pass_control 
Figure 2 An example of handler 
The order of the agenda is generated from the 
left-to-right, depth-first traversal of the product 
tree. When a user input comes in, the system 
calls each handler per their order in the agenda 
and each handler will try to interpret he user 
input. We call the process of letting each handler 
to consume user input as input pass. When a 
handler captures a single piece of information, 
the information is marked as consumed. This 
guarantees that a single information item can be 
consumed by only one handler. After an input 
pass, if the user's input does not directly cause a 
particular handler to generate a question, then 
the system will go through an output pass during 
which each handler will have a chance to 
generate a prompt about itself (e.g. the handler 
for departure date can ask user for a departure 
date). 
AI: 
U2: 
A3: 
U4: 
A5: 
A6: 
U7: 
A8: 
U9: 
A10: 
Ul1: 
A12: 
U13: 
A14: 
A15: 
U16: 
A17: 
U18: 
AI9: 
Hello. You've reached the \[Communicator\]. 
Tell me your lull name 
hi i'd like to fly to seattle tuesday morning 
Traveling to Seattle on Tuesday, August 
eleventh in the morning. Your full name? 
\[John Doe\] 
Hello, \[John\], I'm glad to hear from you 
again. You can now ask me for help. 
OK. Let's see I have a United flight 
departing Pittsburgh at ten forty five a.m. 
arriving into Seattle at one fifty seven p.m. 
Is that OK? 
Yes 
Will you returnto Pittsburgh from Seattle? 
no i will go to Los Angeles 
Traveling to Los Angeles. on what date 
Sorry can we go back to talk about the 
previous leg again 
For the first leg, you are on a United flight 
1446 departing Pittsburgh at ten forty five 
A.M. tomorrow, arriving in Seattle at one 
fifty seven P.M. Do you want to take that? 
actually i want o leave on wednesday 
a flight on wednesday. 
I have a flight departing Pittsburgh at ten 
forty five a.m., arrives Seattle at one fifty 
seven p.m. Is that OK? 
Yes 
OK and on what day do you want to depart 
Seattle? 
next Monday 
Traveling on Monday, August sixteenth. 
Figure 3 An example dialog 
The framework can determine the next step from 
the return code of a handler, it can choose to 
continue the current pass, exit input pass and 
switch to output pass, exit current pass and wait 
for input from user, etc. During a pass, a handler 
can also declare itself as the focus through its 
return code. In this case, it will be promoted to 
the top of the agenda. In order to preserve the 
context of a specific topic, we use a method 
called sub-tree promotion. In this method, a 
handler is first promoted to the left-most node 
among its siblings. The system also handles the 
dependencies among the nodes of the product 
tree. A typical dependent relationship is between 
45 
The initial product ree (simplified) 
Travel 
User 
Flight 
Destl 
Date 1 
Time1 
Next 
User 
Travel 
Flightl 
Destl 
Date 1 
Time 1 
Next 
Flightl 
Travel Destl 
~ Date1 
i 
Destl rimel 
Datel User 
Time 1 travel 
Next Next 
initial A1 
Figure 4 
Next 
Flightl 
Destl 
Date 1 
Fimel 
User 
Fravel 
Product ree at utterance A 10 
Date2 Flightl Flightl 
Flight; Destl Destl 
Dest2 Destl Datel 
Time2 Timel Time1 
Next Date2 Date2 
Flightl Flight,~ Flight2 
Destl Dest2 Dest2 
Datel Time2 rime2 
rime 1 Next Next 
User User User 
travel Travel Travel 
A3 A6 A8 A10 A12 A15 
Date2 
Flight; 
Dest2 
rime2 
Flight~ 
Destl 
Datel 
rime 1 
Next 
User 
travel 
AI7 
Figure 5 The change of agenda long the session 
a parent node and a child node. Usually, the 
value of a parent node is dependent on its 
children. Each node maintains a list of its 
dependent nodes and it will notify its dependents 
about any changes of its value. The dependent 
node can then declare itself invalid and therefore 
a candidate topic for conversation. 
The dialog in figure 3, generated using the 
system, shows a number of features: the ability 
to absorb an implicit change of topic on the part 
of the user (A1-A3), adding to an existing 
itinerary (A8-A10) and handling an explicit 
topic shift (U11). Figure 2 and Figure 3 show 
how the product ree and agenda evolve over the 
course of the dialog 
5 System Implementation 
The Communicator is telephone-based and is 
implemented as a modular distributed system, 
running across NT and Linux platforms. 
Currently the task is captured in an 
approximately 2500-word language based on 
corpora derived from human-human, Wizard of 
Oz and human-computer interaction in this 
domain. Domain information is obtained from 
various sources on the Web. The system has 
information about 500 destinations worldwide, 
though with a majority of these are in the United 
States. To date, we have collected 
approximately 6000 calls, from over 360 
individuals. 
46 
6 Summary and Conclusions 
The agenda-based approach addresses the 
problem of dialog management in complex 
problem-solving tasks. It does so by treating the 
task at hand as one of cooperatively constructing 
a complex data structure, a product, and uses 
this structure to guide the task. The product 
consists of a tree of handlers, each handler 
encapsulates processing relevant o a particular 
schema. Handlers correspond to simple or 
compound schema, the latter acting essentially 
as multi-slOi=fofms. A handler encapsulates 
knowledge n~cessary for interacting about a 
specific information slot, including specification 
of user and system language and of interactions 
with domain agents. Handlers that deal with 
compound schema coordinate tightly bound 
schema and correspond to specific identifiable 
topics of conversation. We define tightly bound 
as those schema that users expect to discuss 
interchangeably, without explicit shifts in 
conversational focus. 
We believe that individual handlers can be 
authored independently of others at the same 
level of hierarchy; in turn we believe this will 
simplify the problem of developing dialog 
systems by managing the complexity of the 
process. 
The agenda contains all topics relevant o the 
current ask. The order of handlers on the agenda 
determines how user input will be will be 
attached to product nodes. Both the system and 
the user however have the ability to reorder 
items on the agenda, the system to foreground 
items that need to be discussed, the user to 
reflect heir current priorities within the task. 
factored out as independent pro~esses. 
We believe that the agenda mechanism can be 
adapted easily to less-complex domains that 
might currently be implemented as a standard 
form-based system (for example a movie 
schedule service). We do not know as yet how 
well the technique will succeed for domains of 
complexity comparable to travel planning but 
with different task structure. 
References 
\[1\] James F. Allen, Lenhart K. Schubert, George 
Ferguson, Peter Heeman, Chung Hee Hwang, 
Tsuneaki Kato, Marc Light, Nathaniel G. Martin, 
Bradford W. Miller, Massimo Poesio, and David 
R. Traum, "The TRAINS Project: A case study in 
building a conversational p anning agent" Journal 
of Experimental nd Theoretical AI, 7(I 995), 7-48. 
\[2\] Bansal, D. and Ravishankar, M. "New features for 
confidence annotation" In Proceedings of the 5th 
International Conference on Spoken Language 
Processing (ICSLP), December 1998, Sydney, 
Australia 
\[3\] Rudnicky, A., Thayer, E., Constantinides, P., 
Tchou, C., Shern, R., Lenzo, K., Xu W., Oh, A. 
"Creating natural dialogs in the Carnegie Mellon 
Communicator system" Proceedings of 
Eurospeech, 1999, Paper 014. 
\[4\] Ward, W. and Issar, S. "Recent improvements in 
the CMU spoken language understanding system" 
In Proceedings of the ARPA Human Language 
Technology Workshop, March 1994, 213-216. 
The mechanisms described in this paper do not 
cover all necessary aspects of dialog 
management but do provide an overall control 
architecture. For example, clarification 
processes, which involve possibly extended 
interaction with respect o the state of a value 
slot, fit into the confines of a single handler and 
are implemented as such. Ideally they could be 
47 
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 369?376,
Sydney, July 2006. c?2006 Association for Computational Linguistics
 
Extractive Summarization using Inter- and Intra- Event Relevance 
 
Wenjie Li, Mingli Wu and Qin Lu 
Department of Computing 
The Hong Kong Polytechnic University 
{cswjli,csmlwu,csluqin}@comp
.polyu.edu.hk 
Wei Xu and Chunfa Yuan 
Department of Computer Science and 
Technology, Tsinghua University 
{vivian00,cfyuan}@mail.ts
inghua.edu.cn 
 
 
 
Abstract 
Event-based summarization attempts to 
select and organize the sentences in a 
summary with respect to the events or 
the sub-events that the sentences de-
scribe. Each event has its own internal 
structure, and meanwhile often relates to 
other events semantically, temporally, 
spatially, causally or conditionally. In 
this paper, we define an event as one or 
more event terms along with the named 
entities associated, and present a novel 
approach to derive intra- and inter- event 
relevance using the information of inter-
nal association, semantic relatedness, 
distributional similarity and named en-
tity clustering. We then apply PageRank 
ranking algorithm to estimate the sig-
nificance of an event for inclusion in a 
summary from the event relevance de-
rived. Experiments on the DUC 2001 
test data shows that the relevance of the 
named entities involved in events 
achieves better result when their rele-
vance is derived from the event terms 
they associate. It also reveals that the 
topic-specific relevance from documents 
themselves outperforms the semantic 
relevance from a general purpose 
knowledge base like Word-Net. 
 
 
1. Introduction 
Extractive summarization selects sentences 
which contain the most salient concepts in 
documents. Two important issues with it are 
how the concepts are defined and what criteria 
should be used to judge the salience of the con-
cepts. Existing work has typically been based on 
techniques that extract key textual elements, 
such as keywords (also known as significant 
terms) as weighed by their tf*idf score, or con-
cepts (such as events or entities) with linguistic 
and/or statistical analysis. Then, sentences are 
selected according to either the important textual 
units they contain or certain types of inter-
sentence relations they hold.  
Event-based summarization which has e-
merged recently attempts to select and organize 
sentences in a summary with respect to events or 
sub-events that the sentences describe. With re-
gard to the concept of events, people do not 
have the same definition when introducing it in 
different domains. While traditional linguistics 
work on semantic theory of events and the se-
mantic structures of verbs, studies in 
information retrieval (IR) within topic detection 
and tracking framework look at events as 
narrowly defined topics which can be 
categorized or clustered as a set of related 
documents (TDT). IR events are broader (or to 
say complex) events in the sense that they may 
include happenings and their causes, 
consequences or even more extended effects. In 
the information extraction (IE) community, 
events are defined as the pre-specified and struc-
tured templates that relate an action to its 
participants, times, locations and other entities 
involved (MUC-7). IE defines what people call 
atomic events. Regardless of their distinct perspectives, peo-
ple all agree that events are collections of activi-
ties together with associated entities. To apply 
the concept of events in the context of text sum-
marization, we believe it is more appropriate to 
consider events at the sentence level, rather than 
at the document level. To avoid the complexity 
of deep semantic and syntactic processing, we 
complement the advantages of statistical 
techniques from the IR community and struc-
tured information provided by the IE community. 
369
 We propose to extract semi-structured events 
with shallow natural language processing (NLP) 
techniques and estimate their importance for 
inclusion in a summary with IR techniques. 
Though it is most likely that documents nar-
rate more than one similar or related event, most 
event-based summarization techniques reported 
so far explore the importance of the events inde-
pendently. Motivated by this observation, this 
paper addresses the task of event-relevance 
based summarization and explores what sorts of 
relevance make a contribution. To this end, we 
investigate intra-event relevance, that is action-
entity relevance, and inter-event relevance, that 
is event-event relevance. While intra-event rele-
vance is measured with frequencies of the asso-
ciated events and entities directly, inter-event 
relevance is derived indirectly from a general 
WordNet similarity utility, distributional simi-
larity in the documents to be summarized, 
named entity clustering and so on. Pagerank 
ranking algorithm is then applied to estimate the 
event importance for inclusion in a summary 
using the aforesaid relevance.  
The remainder of this paper is organized as 
follows. Section 2 introduces related work. Sec-
tions 3 introduces our proposed event-based 
summarization approaches which make use of 
intra- and inter- event relevance. Section 4 pre-
sents experiments and evaluates different ap-
proaches. Finally, Section 5 concludes the paper. 
2. Related Work 
Event-based summarization has been investi-
gated in recent research. It was first presented in 
(Daniel, Radev and Allison, 2003), who treated 
a news topic in multi-document summarization 
as a series of sub-events according to human 
understanding of the topic. They determined the 
degree of sentence relevance to each sub-event 
through human judgment and evaluated six ex-
tractive approaches. Their paper concluded that 
recognizing the sub-events that comprise a sin-
gle news event is essential for producing better 
summaries. However, it is difficult to automati-
cally break a news topic into sub-events.  
Later, atomic events were defined as the rela-
tionships between the important named entities 
(Filatova and Hatzivassiloglou, 2004), such as 
participants, locations and times (which are 
called relations) through the verbs or action 
nouns labeling the events themselves (which are 
called connectors). They evaluated sentences 
based on co-occurrence statistics of the named 
entity relations and the event connectors in-
volved. The proposed approach claimed to out-
perform conventional tf*idf approach. Appar-
ently, named entities are key elements in their 
model. However, the constraints defining events 
seemed quite stringent.  
The application of dependency parsing, 
anaphora and co-reference resolution in recog-
nizing events were presented involving NLP and 
IE techniques more or less (Yoshioka and Hara-
guchi, 2004), (Vanderwende, Banko and Mene-
zes, 2004) and (Leskovec, Grobelnik and Fral-
ing, 2004). Rather than pre-specifying events, 
these efforts extracted (verb)-(dependent rela-
tion)-(noun) triples as events and took the triples 
to form a graph merged by relations.  
As a matter of fact, events in documents are 
related in some ways. Judging whether the sen-
tences are salient or not and organizing them in 
a coherent summary can take advantage from 
event relevance. Unfortunately, this was ne-
glected in most previous work. Barzilay and La-
pata (2005) exploited the use of the distribu-
tional and referential information of discourse 
entities to improve summary coherence. While 
they captured text relatedness with entity transi-
tion sequences, i.e. entity-based summarization, 
we are particularly interested in relevance be-
tween events in event-based summarization. 
Extractive summarization requires ranking 
sentences with respect to their importance. 
Successfully used in Web-link analysis and 
more recently in text summarization, Google?s 
PageRank (Brin and Page, 1998) is one of the 
most popular ranking algorithms. It is a kind of 
graph-based ranking algorithm deciding on the 
importance of a node within a graph by taking 
into account the global information recursively 
computed from the entire graph, rather than re-
lying on only the local node-specific infor-
mation. A graph can be constructed by adding a 
node for each sentence, phrase or word. Edges 
between nodes are established using inter-
sentence similarity relations as a function of 
content overlap or grammatically relations be-
tween words or phrases.  
The application of PageRank in sentence ex-
traction was first reported in (Erkan and Radev, 
2004). The similarity between two sentence 
nodes according to their term vectors was used 
to generate links and define link strength. The 
same idea was followed and investigated exten-
370
 sively (Mihalcea, 2005). Yoshioka and Haragu-
chi (2004) went one step further toward event-
based summarization. Two sentences were 
linked if they shared similar events. When tested 
on TSC-3, the approach favoured longer sum-
maries. In contrast, the importance of the verbs 
and nouns constructing events was evaluated 
with PageRank as individual nodes aligned by 
their dependence relations (Vanderwende, 2004; 
Leskovec, 2004).  
Although we agree that the fabric of event 
constitutions constructed by their syntactic rela-
tions can help dig out the important events, we 
have two comments. First, not all verbs denote 
event happenings. Second, semantic similarity 
or relatedness between action words should be 
taken into account. 
3. Event-based Summarization 
3.1. Event Definition and Event Map 
Events can be broadly defined as ?Who did 
What to Whom When and Where?. Both lin-
guistic and empirical studies acknowledge that 
event arguments help characterize the effects of 
a verb?s event structure even though verbs or 
other words denoting event determine the se-
mantics of an event. In this paper, we choose 
verbs (such as ?elect?) and action nouns (such as 
?supervision?) as event terms that can character-
ize or partially characterize actions or incident 
occurrences. They roughly relate to ?did What?. 
One or more associated named entities are con-
sidered as what are denoted by linguists as event 
arguments. Four types of named entities are cur-
rently under the consideration. These are <Per-
son>, <Organization>, <Location> and <Date>. 
They convey the information of ?Who?, 
?Whom?, ?When? and ?Where?. A verb or an 
action noun is deemed as an event term only 
when it presents itself at least once between two 
named entities. 
Events are commonly related with one an-
other semantically, temporally, spatially, caus-
ally or conditionally, especially when the docu-
ments to be summarized are about the same or 
very similar topics. Therefore, all event terms 
and named entities involved can be explicitly 
connected or implicitly related and weave a 
document or a set of documents into an event 
fabric, i.e. an event graphical representation (see 
Figure 1). The nodes in the graph are of two 
types. Event terms (ET) are indicated by rectan-
gles and named entities (NE) are indicated by 
ellipses. They represent concepts rather than 
instances. Words in either their original form or 
morphological variations are represented with a 
single node in the graph regardless of how many 
times they appear in documents. We call this 
representation an event map, from which the 
most important concepts can be pick out in the 
summary. 
 
 
 
Figure 1 Sample sentences and their graphical representation 
 
 
The advantage of representing with separated 
action and entity nodes over simply combining 
them into one event or sentence node is to pro-
vide a convenient way for analyzing the rele-
vance among event terms and named entities 
either by their semantic or distributional similar-
ity. More importantly, this favors extraction of 
concepts and brings the conceptual compression 
available. 
We then integrate the strength of the connec-
tions between nodes into this graphical model in 
terms of the relevance defined from different 
perspectives. The relevance is indicated by 
),( ji nodenoder , where inode  and jnode  repre-
sent two nodes, and are either event terms ( iet ) 
or named entities ( jne ). Then, the significance 
of each node, indicated by )( inodew , is calcu-
<Organization> America Online </Organization> was to buy <Organization> 
Netscape </Organization> and forge a partnership with <Organization> Sun 
</Organization>, benefiting all three and giving technological independence 
from <Organization> Microsoft </Organization>. 
371
 lated with PageRank ranking algorithm. Sec-
tions 3.2 and 3.3 address the issues of deriving 
),( ji nodenoder  according to intra- or/and inter- 
event relevance and calculating )( inodew  in de-
tail. 
3.2 Intra- and Inter- Event Relevance 
We consider both intra-event and inter-event 
relevance for summarization. Intra-event rele-
vance measures how an action itself is associ-
ated with its associated arguments. It is indi-
cated as ),( NEETR  and ),( ETNER  in Table 1 
below. This is a kind of direct relevance as the 
connections between actions and arguments are 
established from the text surface directly. No 
inference or background knowledge is required. 
We consider that when the connection between 
an event term iet  and a named entity jne  is 
symmetry, then TNEETRETNER ),(),( = . Events 
are related as explained in Section 2. By means 
of inter-event relevance, we consider how an 
event term (or a named entity involved in an 
event) associate to another event term (or an-
other named entity involved in the same or dif-
ferent events) syntactically, semantically and 
distributionally. It is indicated by ),( ETETR or 
),( NENER in Table 1 and measures an indirect 
connection which is not explicit in the event 
map needing to be derived from the external 
resource or overall event distribution. 
 Event Term 
(ET) 
Named En-
tity (NE) 
Event Term (ET) ),( ETETR  ),( NEETR  
Named Entity (NE) ),( ETNER  ),( NENER
Table 1 Relevance Matrix 
The complete relevance matrix is: 
??
???
?=
),(),(
),(),(
NENERETNER
NEETRETETR
R  
The intra-event relevance ),( NEETR can be 
simply established by counting how many times 
iet  and jne  are associated, i.e.  
),(),( jijiDocument neetfreqneetr =  (E1) 
One way to measure the term relevance is to 
make use of a general language knowledge base, 
such as WordNet (Fellbaum 1998). Word-
Net::Similarity is a freely available software 
package that makes it possible to measure the 
semantic relatedness between a pair of concepts, 
or in our case event terms, based on WordNet 
(Pedersen, Patwardhan and Michelizzi, 2004). It 
supports three measures. The one we choose is 
the function lesk. 
),(),(),( jijijiWordNet etetlesketetsimilarityetetr ==
      (E2) 
Alternatively, term relevance can be meas-
ured according to their distributions in the speci-
fied documents. We believe that if two events 
are concerned with the same participants, occur 
at same location, or at the same time, these two 
events are interrelated with each other in some 
ways. This observation motivates us to try deriv-
ing event term relevance from the number of 
name entities they share. 
|)()(|),( jijiDocument etNEetNEetetr ?=  (E3) 
Where )( ietNE is the set of named entities iet  
associate. | | indicates the number of the ele-
ments in the set. The relevance of named entities 
can be derived in a similar way. 
|)()(|),( jijiDocument neETneETnener ?=  (E4) 
The relevance derived with (E3) and (E4) are 
indirect relevance. In previous work, a cluster-
ing algorithm, shown in Figure 2, has been pro-
posed (Xu et al 2006) to merge the named en-
tity that refer to the same person (such as 
Ranariddh, Prince Norodom Ranariddh and Presi-
dent Prince Norodom Ranariddh). It is used for 
co-reference resolution and aims at joining the 
same concept into a single node in the event 
map. The experimental result suggests that 
merging named entity improves performance in 
some extend but not evidently. When applying 
the same algorithm for clustering all four types 
of name entities in DUC data, we observe that 
the name entities in the same cluster do not al-
ways refer to the same objects, even when they 
are indeed related in some way. For example, 
?Mississippi? is a state in the southeast United 
States, while ?Mississippi River? is the second-
longest rever in the United States and flows 
through ?Mississippi?. 
Step1: Each name entity is represented by 
ikiii wwwne ...21= , where iw  is the ith 
word in it. The cluster it belongs to, in-
dicated by )( ineC , is initialled by 
ikii www ...21 itself.  
Step2: For each name entity  
           ikiii wwwne ...21=  
For each name entity 
372
 jljjj wwwne ...21= , if )( ineC  is a 
sub-string of )( jneC , then 
)()( ji neCneC = . 
Continue Step 2 until no change occurs. 
Figure 2 The algorithm proposed to merge the 
named entities 
Location Person Date Organization
Mississippi 
 
Professor Sir 
Richard 
Southwood 
first six 
months of 
last year 
Long Beach 
City Council 
Sir Richard 
Southwood 
San Jose City 
Council 
Mississippi 
River 
Richard 
Southwood 
last year 
City Council 
Table 2 Some results of the named entity 
merged 
It therefore provides a second way to measure 
named entity relevance based on the clusters 
found. It is actually a kind of measure of lexical 
similarity. 
??
?=
otherwise      ,0
cluster same in the are ,      ,1
),( jijiCluster
nene
nener
     (E5) 
In addition, the relevance of the named enti-
ties can be sometimes revealed by sentence con-
text. Take the following most frequently used 
sentence patterns as examples: 
 
Figure 3 The example patterns  
Considering that two neighbouring name enti-
ties in a sentence are usually relevant, the fol-
lowing window-based relevance is also experi-
mented with. 
??
?=
otherwise      ,0
size  windowspecified-pre a within are ,      1,
),(
ji
jiPattern
nene
nener
     (E6) 
3.3 Significance of Concepts 
The significance score, i.e. the weight 
)( inodew  of each inode , is then estimated recur-
sively with PageRank ranking algorithm which 
assigns the significance score to each node ac-
cording to the number of nodes connecting to it 
as well as the strength of their connections. The 
equation calculating )( inodew using PageRank 
of a certain inode  is shown as follows. 
)
),(
)(
...
),(
)(
...
),(
)(()1()(
1
1
ti
t
ji
j
i
i
nodenoder
nodew
nodenoder
nodew
nodenoder
nodewddnodew
+++
++?=
 (E7) 
In (E7), jnode ( tj ,...2,1= , ij ? ) are the 
nodes linking to inode . d is the factor used to 
avoid the limitation of loop in the map structure. 
It is set to 0.85 experimentally. The significance 
of each sentence to be included in the summary 
is then obtained from the significance of the 
events it contains. The sentences with higher 
significance are picked up into the summary as 
long as they are not exactly the same sentences. 
We are aware of the important roles of informa-
tion fusion and sentence compression in sum-
mary generation. However, the focus of this pa-
per is to evaluate event-based approaches in ex-
tracting the most important sentences. Concep-
tual extraction based on event relevance is our 
future direction. 
4. Experiments and Discussions 
To evaluate the event based summarization ap-
proaches proposed, we conduct a set of experi-
ments on 30 English document sets provide by 
the DUC 2001 multi-document summarization 
task. The documents are pre-processed with 
GATE to recognize the previously mentioned 
four types of name entities. On average, each set 
contains 10.3 documents, 602 sentences, 216 
event terms and 148.5 name entities. 
To evaluate the quality of the generated 
summaries, we choose an automatic summary 
evaluation metric ROUGE, which has been used 
in DUCs. ROUGE is a recall-based metric for 
fixed length summaries. It bases on N-gram co-
occurrence and compares the system generated 
summaries to human judges (Lin and Hovy, 
2003). For each DUC document set, the system 
creates a summary of 200 word length and pre-
sent three of the ROUGE metrics: ROUGE-1 
(unigram-based), ROUGE-2 (bigram-based), 
and ROUGE-W (based on longest common sub-
sequence weighed by the length) in the follow-
ing experiments and evaluations.  
We first evaluate the summaries generated 
based on ),( NEETR  itself. In the pre-evaluation 
experiments, we have observed that some fre-
<Person>, a-position-name of <Organization>, 
does something. 
<Person> and another <Person> do something. 
373
 quently occurring nouns, such as ?doctors? and 
?hospitals?, by themselves are not marked by 
general NE taggers. But they indicate persons, 
organizations or locations. We compare the 
ROUGE scores of adding frequent nouns or not 
to the set of named entities in Table 3. A noun is 
considered as a frequent noun when its fre-
quency is larger than 10. Roughly 5% improve-
ment is achieved when high frequent nouns are 
taken into the consideration. Hereafter, when we 
mention NE in latter experiments, the high fre-
quent nouns are included. 
),( NEETR  NE Without High 
Frequency Nouns 
NE With High 
Frequency Nouns
ROUGE-1 0.33320 0.34859 
ROUGE-2 0.06260 0.07157 
ROUGE-W 0.12965 0.13471 
Table 3 ROUGE scores using ),( NEETR  itself 
Table 4 below then presents the summariza-
tion results by using ),( ETETR  itself. It com-
pares two relevance derivation approaches, 
WordNetR  and DocumentR . The topic-specific rele-
vance derived from the documents to be summa-
rized outperforms the general purpose Word-Net 
relevance by about 4%. This result is reasonable 
as WordNet may introduce the word relatedness 
which is not necessary in the topic-specific 
documents. When we examine the relevance 
matrix from the event term pairs with the high-
est relevant, we find that the pairs, like ?abort? 
and ?confirm?, ?vote? and confirm?, do reflect 
semantics (antonymous) and associated (causal) 
relations to some degree.  
),( ETETR  Semantic Rele-
vance from 
Word-Net 
Topic-Specific 
Relevance from 
Documents 
ROUGE-1 0.32917 0.34178 
ROUGE-2 0.05737 0.06852 
ROUGE-W 0.11959 0.13262 
Table 4 ROUGE scores using ),( ETETR  itself 
Surprisingly, the best individual result is from 
document distributional similarity DocumentR  
),( NENE  in Table 5. Looking more closely, we 
conclude that compared to event terms, named 
entities are more representative of the docu-
ments in which they are included. In other words, 
event terms are more likely to be distributed 
around all the document sets, whereas named 
entities are more topic-specific and therefore 
cluster in a particular document set more. Ex-
amples of high related named entities in rele-
vance matrix are ?Andrew? and ?Florida?, 
?Louisiana? and ?Florida?. Although their rele-
vance is not as explicit as the same of event 
terms (their relevance is more contextual than 
semantic), we can still deduce that some events 
may happen in both Louisiana and Florida, or 
about Andrew in Florida. In addition, it also 
shows that the relevance we would have ex-
pected to be derived from patterns and clustering 
can also be discovered by ),( NENERDocument . 
The window size is set to 5 experimentally in 
window-based practice.  
),( NENER Relevance 
from 
Documents
Relevance 
from 
Clustering 
Relevance 
from Window-
based Context
ROUGE-1 0.35212 0.33561 0.34466 
ROUGE-2 0.07107 0.07286 0.07508 
ROUGE-W 0.13603 0.13109 0.13523 
Table 5 ROUGE scores using ),( NENER  itself 
Next, we evaluate the integration of 
),( NEETR , ),( ETETR  and ),( NENER . As 
DUC 2001 provides 4 different summary sizes 
for evaluation, it satisfies our desire to test the 
sensibility of the proposed event-based summa-
rization techniques to the length of summaries. 
While the previously presented results are 
evaluated on 200 word summaries, now we 
move to check the results in four different sizes, 
i.e. 50, 100, 200 and 400 words. The experi-
ments results show that the event-based ap-
proaches indeed prefer longer summaries. This 
is coincident with what we have hypothesized. 
For this set of experiments, we choose to inte-
grate the best method from each individual 
evaluation presented previously. It appears that 
using the named entities relevance which is de-
rived from the event terms gives the best 
ROUGE scores in almost all the summery sizes. 
Compared with the results provided in (Filatova 
and Hatzivassiloglou, 2004) whose average 
ROUGE-1 score is below 0.3 on the same data 
set, the significant improvement is revealed. Of 
course, we need to test on more data in the fu-
ture. 
),( NENER 50 100 200 400 
ROUGE-1 0.22383 0.28584 0.35212 0.41612
ROUGE-2 0.03376 0.05489 0.07107 0.10275
ROUGE-W 0.10203 0.11610 0.13603 0.13877
),( NEETR 50 100 200 400 
ROUGE-1 0.22224 0.27947 0.34859 0.41644
ROUGE-2 0.03310 0.05073 0.07157 0.10369
ROUGE-W 0.10229 0.11497 0.13471 0.13850
),( ETETR 50 100 200 400 
374
 ROUGE-1 0.20616 0.26923 0.34178 0.41201
ROUGE-2 0.02347 0.04575 0.06852 0.10263
ROUGE-W 0.09212 0.11081 0.13262 0.13742
),( NEETR + 
),( ETETR + 
),( NENER  
 
50 
 
100 
 
200 
 
400 
ROUGE-1 0.21311 0.27939 0.34630 0.41639
ROUGE-2 0.03068 0.05127 0.07057 0.10579
ROUGE-W 0.09532 0.11371 0.13416 0.13913
Table 6 ROUGE scores using complete R matrix 
and with different summary lengths 
As discussed in Section 3.2, the named enti-
ties in the same cluster may often be relevant but 
not always be co-referred. In the following last 
set of experiments, we evaluate the two ways to 
use the clustering results. One is to consider 
them as related as if they are in the same cluster 
and derive the NE-NE relevance with (E5). The 
other is to merge the entities in one cluster as 
one reprehensive named entity and then use it in 
ET-NE with (E1). The rationality of the former 
approach is validated. 
 Clustering is 
used to derive 
NE-NE 
Clustering is used to 
merge entities and 
then to derive ET-NE
ROUGE-1 0.34072 0.33006 
ROUGE-2 0.06727 0.06154 
ROUGE-W 0.13229 0.12845 
Table 7 ROUGE scores with regard to how to 
use the clustering information 
5. Conclusion 
In this paper, we propose to integrate event-
based approaches to extractive summarization. 
Both inter-event and intra-event relevance are 
investigated and PageRank algorithm is used to 
evaluate the significance of each concept (in-
cluding both event terms and named entities). 
The sentences containing more concepts and 
highest significance scores are chosen in the 
summary as long as they are not the same sen-
tences.  
To derive event relevance, we consider the 
associations at the syntactic, semantic and con-
textual levels. An important finding on the DUC 
2001 data set is that making use of named entity 
relevance derived from the event terms they as-
sociate with achieves the best result. The result 
of 0.35212 significantly outperforms the one 
reported in the closely related work whose aver-
age is below 0.3. We are interested in the issue 
of how to improve an event representation in 
order to build a more powerful event-based 
summarization system. This would be one of our 
future directions. We also want to see how con-
cepts rather than sentences are selected into the 
summary in order to develop a more flexible 
compression technique and to know what char-
acteristics of a document set is appropriate for 
applying event-based summarization techniques.  
 
Acknowledgements 
The work presented in this paper is supported 
partially by Research Grants Council on Hong 
Kong (reference number CERG PolyU5181/03E) 
and partially by National Natural Science Foun-
dation of China (reference number: NSFC 
60573186). 
 
References 
Chin-Yew Lin and Eduard Hovy. 2003. Automatic 
Evaluation of Summaries using N-gram Co-
occurrence Statistics. In Proceedings of HLT-
NAACL 2003, pp71-78. 
Christiane Fellbaum. 1998, WordNet: An Electronic 
Lexical Database. MIT Press. 
Elena Filatova and Vasileios Hatzivassiloglou. 2004. 
Event-based Extractive summarization. In Pro-
ceedings of ACL 2004 Workshop on Summariza-
tion, pp104-111.  
Gunes Erkan and Dragomir Radev. 2004. LexRank: 
Graph-based Centrality as Salience in Text Sum-
marization. Journal of Artificial Intelligence Re-
search. 
Jure Leskovec, Marko Grobelnik and Natasa Milic-
Frayling. 2004. Learning Sub-structures of Docu-
ment Semantic Graphs for Document Summariza-
tion. In LinkKDD 2004.  
Lucy Vanderwende, Michele Banko and Arul Mene-
zes. 2004. Event-Centric Summary Generation. In 
Working Notes of DUC 2004. 
Masaharu Yoshioka and Makoto Haraguchi. 2004. 
Multiple News Articles Summarization based on 
Event Reference Information. In Working Notes 
of NTCIR-4, Tokyo. 
MUC-7. http://www-nlpir.nist.gov/related_projects/ 
muc/proceeings/ muc_7_toc.html 
Naomi Daniel, Dragomir Radev and Timothy Allison. 
2003. Sub-event based Multi-document Summari-
zation. In Proceedings of the HLT-NAACL 2003 
Workshop on Text Summarization, pp9-16. 
375
 Page Lawrence, Brin Sergey, Motwani Rajeev and 
Winograd Terry. 1998. The PageRank Citation 
Ranking: Bring Order to the Web. Technical Re-
port, Stanford University. 
Rada Mihalcea. 2005. Language Independent Extrac-
tive Summarization. ACL 2005 poster. 
Regina Barzilay and Michael Elhadad. 2005. Model-
ling Local Coherence: An Entity-based Approach. 
In Proceedings of ACL, pp141-148. 
TDT. http://projects.ldc.upenn.edu/TDT. 
Ted Pedersen, Siddharth Patwardhan and Jason 
Michelizzi. 2004. WordNet::Similarity ? Measur-
ing the Relatedness of Concepts. In Proceedings of 
AAAI, pp25-29. 
Wei Xu, Wenjie Li, Mingli Wu, Wei Li and Chunfa 
Yuan. 2006. Deriving Event Relevance from the 
Ontology Constructed with Formal Concept 
Analysis, in Proceedings of CiCling?06, pp480-
489. 
 
376
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 423?431,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Who, What, When, Where, Why?  
Comparing Multiple Approaches to the Cross-Lingual 5W Task 
Kristen Parton*, Kathleen R. McKeown*, Bob Coyne*, Mona T. Diab*,  
Ralph Grishman?, Dilek Hakkani-T?r?, Mary Harper?, Heng Ji?, Wei Yun Ma*,  
Adam Meyers?, Sara Stolbach*, Ang Sun?, Gokhan Tur?, Wei Xu? and Sibel Yaman? 
 
*Columbia University 
New York, NY, USA 
{kristen, kathy, 
coyne, mdiab, ma, 
sara}@cs.columbia.edu 
 
?New York University 
New York, NY, USA 
{grishman, meyers, 
asun, xuwei} 
@cs.nyu.edu 
?International Computer 
Science Institute 
Berkeley, CA, USA 
{dilek, sibel} 
@icsi.berkeley.edu 
 
?Human Lang. Tech. Ctr. of 
Excellence, Johns Hopkins 
and U. of Maryland, 
College Park  
mharper@umd.edu 
?City University of  
New York 
New York, NY, USA 
hengji@cs.qc.cuny.edu 
 
 
?SRI International 
Palo Alto, CA, USA 
gokhan@speech.sri.com 
 
 
  
Abstract 
Cross-lingual tasks are especially difficult 
due to the compounding effect of errors in 
language processing and errors in machine 
translation (MT). In this paper, we present an 
error analysis of a new cross-lingual task: the 
5W task, a sentence-level understanding task 
which seeks to return the English 5W's (Who, 
What, When, Where and Why) corresponding 
to a Chinese sentence. We analyze systems 
that we developed, identifying specific prob-
lems in language processing and MT that 
cause errors. The best cross-lingual 5W sys-
tem was still 19% worse than the best mono-
lingual 5W system, which shows that MT 
significantly degrades sentence-level under-
standing. Neither source-language nor target-
language analysis was able to circumvent 
problems in MT, although each approach had 
advantages relative to the other. A detailed 
error analysis across multiple systems sug-
gests directions for future research on the 
problem. 
1 Introduction 
In our increasingly global world, it is ever more 
likely for a mono-lingual speaker to require in-
formation that is only available in a foreign lan-
guage document. Cross-lingual applications ad-
dress this need by presenting information in the 
speaker?s language even when it originally ap-
peared in some other language, using machine 
translation (MT) in the process. In this paper, we 
present an evaluation and error analysis of a 
cross-lingual application that we developed for a 
government-sponsored evaluation, the 5W task. 
The 5W task seeks to summarize the informa-
tion in a natural language sentence by distilling it 
into the answers to the 5W questions: Who, 
What, When, Where and Why. To solve this 
problem, a number of different problems in NLP 
must be addressed: predicate identification, ar-
gument extraction, attachment disambiguation, 
location and time expression recognition, and 
(partial) semantic role labeling. In this paper, we 
address the cross-lingual 5W task: given a 
source-language sentence, return the 5W?s trans-
lated (comprehensibly) into the target language. 
Success in this task requires a synergy of suc-
cessful MT and answer selection.  
The questions we address in this paper are: 
? How much does machine translation (MT) 
degrade the performance of cross-lingual 
5W systems, as compared to monolingual 
performance? 
? Is it better to do source-language analysis 
and then translate, or do target-language 
analysis on MT? 
? Which specific problems in language 
processing and/or MT cause errors in 5W 
answers?  
In this evaluation, we compare several differ-
ent approaches to the cross-lingual 5W task, two 
that work on the target language (English) and 
one that works in the source language (Chinese). 
423
A central question for many cross-lingual appli-
cations is whether to process in the source lan-
guage and then translate the result, or translate 
documents first and then process the translation. 
Depending on how errorful the translation is, 
results may be more accurate if models are de-
veloped for the source language. However, if 
there are more resources in the target language, 
then the translate-then-process approach may be 
more appropriate. We present a detailed analysis, 
both quantitative and qualitative, of how the ap-
proaches differ in performance.  
We also compare system performance on hu-
man translation (which we term reference trans-
lations) and MT of the same data in order to de-
termine how much MT degrades system per-
formance. Finally, we do an in-depth analysis of 
the errors in our 5W approaches, both on the 
NLP side and the MT side. Our results provide 
explanations for why different approaches suc-
ceed, along with indications of where future ef-
fort should be spent. 
2 Prior Work 
The cross-lingual 5W task is closely related to 
cross-lingual information retrieval and cross-
lingual question answering (Wang and Oard 
2006; Mitamura et al 2008). In these tasks, a 
system is presented a query or question in the 
target language and asked to return documents or 
answers from a corpus in the source language. 
Although MT may be used in solving this task, it 
is only used by the algorithms ? the final evalua-
tion is done in the source language. However, in 
many real-life situations, such as global business, 
international tourism, or intelligence work, users 
may not be able to read the source language. In 
these cases, users must rely on MT to understand 
the system response. (Parton et al 2008) exam-
ine the case of ?translingual? information re-
trieval, where evaluation is done on translated 
results in the target language. In cross-lingual 
information extraction (Sudo et al 2004) the 
evaluation is also done on MT, but the goal is to 
learn knowledge from a large corpus, rather than 
analyzing individual sentences.  
The 5W task is also closely related to Seman-
tic Role Labeling (SRL), which aims to effi-
ciently and effectively derive semantic informa-
tion from text. SRL identifies predicates and 
their arguments in a sentence, and assigns roles 
to each argument. For example, in the sentence 
?I baked a cake yesterday.?, the predicate 
?baked? has three arguments. ?I? is the subject of 
the predicate, ?a cake? is the object and ?yester-
day? is a temporal argument.  
Since the release of large data resources anno-
tated with relevant levels of semantic informa-
tion, such as the FrameNet (Baker et al, 1998) 
and PropBank corpora (Kingsbury and Palmer, 
2003), efficient approaches to SRL have been 
developed (Carreras and Marquez, 2005). Most 
approaches to the problem of SRL follow the 
Gildea and Jurafsky (2002) model. First, for a 
given predicate, the SRL system identifies its 
arguments' boundaries. Second, the Argument 
types are classified depending on an adopted 
lexical resource such as PropBank or FrameNet. 
Both steps are based on supervised learning over 
labeled gold standard data. A final step uses heu-
ristics to resolve inconsistencies when applying 
both steps simultaneously to the test data.  
Since many of the SRL resources are English, 
most of the SRL systems to date have been for 
English. There has been work in other languages 
such as German and Chinese (Erk 2006; Sun 
2004; Xue and Palmer 2005). The systems for 
the other languages follow the successful models 
devised for English, e.g. (Gildea and Palmer, 
2002; Chen and Rambow, 2003; Moschitti, 2004; 
Xue and Palmer, 2004; Haghighi et al, 2005). 
3 The Chinese-English 5W Task 
3.1 5W Task Description 
We participated in the 5W task as part of the 
DARPA GALE (Global Autonomous Language 
Exploitation) project. The goal is to identify the 
5W?s (Who, What, When, Where and Why) for a 
complete sentence. The motivation for the 5W 
task is that, as their origin in journalism suggests, 
the 5W?s cover the key information nuggets in a 
sentence. If a system can isolate these pieces of 
information successfully, then it can produce a 
pr?cis of the basic meaning of the sentence. Note 
that this task differs from QA tasks, where 
?Who? and ?What? usually refer to definition 
type questions. In this task, the 5W?s refer to se-
mantic roles within a sentence, as defined in Ta-
ble 1.  
In order to get al 5W?s for a sentence correct, 
a system must identify a top-level predicate, ex-
tract the correct arguments, and resolve attach-
ment ambiguity. In the case of multiple top-level 
predicates, any of the top-level predicates may be 
chosen. In the case of passive verbs, the Who is 
the agent (often expressed as a ?by clause?, or 
not stated), and the What should include the syn-
tactic subject.  
424
Answers are judged Correct1 if they identify a 
correct null argument or correctly extract an ar-
gument that is present in the sentence. Answers 
are not penalized for including extra text, such as 
prepositional phrases or subordinate clauses, 
unless the extra text includes text from another 
answer or text from another top-level predicate. 
In sentence 2a in Table 2, returning ?bought and 
cooked? for the What would be Incorrect. Simi-
larly, returning ?bought the fish at the market? 
for the What would also be Incorrect, since it 
contains the Where. Answers may also be judged 
Partial, meaning that only part of the answer was 
returned. For example, if the What contains the 
predicate but not the logical object, it is Partial.  
Since each sentence may have multiple correct 
sets of 5W?s, it is not straightforward to produce 
a gold-standard corpus for automatic evaluation. 
One would have to specify answers for each pos-
sible top-level predicate, as well as which parts 
of the sentence are optional and which are not 
allowed. This also makes creating training data 
for system development problematic. For exam-
ple, in Table 2, the sentence in 2a and 2b is the 
same, but there are two possible sets of correct 
answers. Since we could not rely on a gold-
standard corpus, we used manual annotation to 
judge our 5W system, described in section 5. 
3.2 The Cross-Lingual 5W Task 
In the cross-lingual 5W task, a system is given a 
sentence in the source language and asked to 
produce the 5W?s in the target language. In this 
task, both machine translation (MT) and 5W ex-
traction must succeed in order to produce correct 
answers. One motivation behind the cross-lingual 
5W task is MT evaluation. Unlike word- or 
phrase-overlap measures such as BLEU, the 5W 
evaluation takes into account ?concept? or ?nug-
get? translation. Of course, only the top-level 
predicate and arguments are evaluated, so it is 
not a complete evaluation. But it seeks to get at 
the understandability of the MT output, rather 
than just n-gram overlap. 
Translation exacerbates the problem of auto-
matically evaluating 5W systems. Since transla-
tion introduces paraphrase, rewording and sen-
tence restructuring, the 5W?s may change from 
one translation of a sentence to another transla-
tion of the same sentence. In some cases, roles 
may swap. For example, in Table 2, sentences 1a 
and 1b could be valid translations of the same 
                                                 
1
 The specific guidelines for determining correctness 
were formulated by BAE.  
Chinese sentence. They contain the same infor-
mation, but the 5W answers are different. Also, 
translations may produce answers that are textu-
ally similar to correct answers, but actually differ 
in meaning. These differences complicate proc-
essing in the source followed by translation. 
 
Example: On Tuesday, President Obama met with 
French President Sarkozy in Paris to discuss the 
economic crisis. 
W Definition Example  
answer 
WHO Logical subject of the 
top-level predicate in 
WHAT, or null. 
President 
Obama 
WHAT One of the top-level 
predicates in the sen-
tence, and the predi-
cate?s logical object. 
met with 
French Presi-
dent Sarkozy 
WHEN ARGM-TMP of the 
top-level predicate in 
WHAT, or null. 
On Tuesday 
WHERE ARGM-LOC of the 
top-level predicate in 
WHAT, or null. 
in Paris 
WHY ARGM-CAU of the 
top-level predicate in 
WHAT, or null. 
to discuss the 
economic crisis 
Table 1. Definition of the 5W task, and 5W answers 
from the example sentence above. 
4 5W System 
We developed a 5W combination system that 
was based on five other 5W systems. We se-
lected four of these different systems for evalua-
tion: the final combined system (which was our 
submission for the official evaluation), two sys-
tems that did analysis in the target-language 
(English), and one system that did analysis in the 
source language (Chinese). In this section, we 
describe the individual systems that we evalu-
ated, the combination strategy, the parsers that 
we tuned for the task, and the MT systems.  
 Sentence WHO WHAT 
1a Mary bought a cake 
from Peter. 
Mary bought a 
cake 
1b Peter sold Mary a 
cake. 
Peter sold Mary 
2a I bought the fish at 
the market yesterday 
and cooked it today. 
I bought the 
fish 
[WHEN: 
yesterday] 
2b I bought the fish at 
the market yesterday 
and cooked it today. 
I cooked it 
[WHEN: 
today] 
Table 2. Example 5W answers. 
425
4.1 Latent Annotation Parser 
For this work, we have re-implemented and en-
hanced the Berkeley parser (Petrov and Klein 
2007) in several ways: (1) developed a new 
method to handle rare words in English and Chi-
nese; (2) developed a new model of unknown 
Chinese words based on characters in the word; 
(3) increased robustness by adding adaptive 
modification of pruning thresholds and smooth-
ing of word emission probabilities. While the 
enhancements to the parser are important for ro-
bustness and accuracy, it is even more important 
to train grammars matched to the conditions of 
use. For example, parsing a Chinese sentence 
containing full-width punctuation with a parser 
trained on half-width punctuation reduces accu-
racy by over 9% absolute F. In English, parsing 
accuracy is seriously compromised by training a 
grammar with punctuation and case to process 
sentences without them.  
We developed grammars for English and Chi-
nese trained specifically for each genre by sub-
sampling from available treebanks (for English, 
WSJ, BN, Brown, Fisher, and Switchboard; for 
Chinese, CTB5) and transforming them for a 
particular genre (e.g., for informal speech, we 
replaced symbolic expressions with verbal forms 
and remove punctuation and case) and by utiliz-
ing a large amount of genre-matched self-labeled 
training parses. Given these genre-specific 
parses, we extracted chunks and POS tags by 
script. We also trained grammars with a subset of 
function tags annotated in the treebank that indi-
cate case role information (e.g., SBJ, OBJ, LOC, 
MNR) in order to produce function tags.   
4.2 Individual 5W Systems 
The English systems were developed for the 
monolingual 5W task and not modified to handle 
MT. They used hand-crafted rules on the output 
of the latent annotation parser to extract the 5Ws.  
English-function used the function tags from 
the parser to map parser constituents to the 5Ws. 
First the Who, When, Where and Why were ex-
tracted, and then the remaining pieces of the sen-
tence were returned as the What. The goal was to 
make sure to return a complete What answer and 
avoid missing the object. 
English-LF, on the other hand, used a system 
developed over a period of eight years (Meyers 
et al 2001) to map from the parser?s syntactic 
constituents into logical grammatical relations 
(GLARF), and then extracted the 5Ws from the 
logical form. As a back-up, it also extracted 
GLARF relations from another English-treebank 
trained parser, the Charniak parser (Charniak 
2001). After the parses were both converted to 
the 5Ws, they were then merged, favoring the 
system that: recognized the passive, filled more 
5W slots or produced shorter 5W slots (provid-
ing that the WHAT slot consisted of more than 
just the verb). A third back-up method extracted 
5Ws from part-of-speech tag patterns. Unlike 
English-function, English-LF explicitly tried to 
extract the shortest What possible, provided there 
was a verb and a possible object, in order to 
avoid multiple predicates or other 5W answers.  
Chinese-align uses the latent annotation 
parser (trained for Chinese) to parse the Chinese 
sentences. A dependency tree converter (Johans-
son and Nuges 2007) was applied to the constitu-
ent-based parse trees to obtain the dependency 
relations and determine top-level predicates. A 
set of hand-crafted dependency rules based on 
observation of Chinese OntoNotes were used to 
map from the Chinese function tags into Chinese 
5Ws.  Finally, Chinese-align used the alignments 
of three separate MT systems to translate the 
5Ws: a phrase-based system, a hierarchical 
phrase-based system, and a syntax augmented 
hierarchical phrase-based system. Chinese-align 
faced a number of problems in using the align-
ments, including the fact that the best MT did not 
always have the best alignment. Since the predi-
cate is essential, it tried to detect when verbs 
were deleted in MT, and back-off to a different 
MT system. It also used strategies for finding 
and correcting noisy alignments, and for filtering 
When/Where answers from Who and What.  
4.3 Hybrid System 
A merging algorithm was learned based on a de-
velopment test set. The algorithm selected all 
5W?s from a single system, rather than trying to 
merge W?s from different systems, since the 
predicates may vary across systems. For each 
document genre (described in section 5.4), we 
ranked the systems by performance on the devel-
opment data. We also experimented with a vari-
ety of features (for instance, does ?What? include 
a verb). The best-performing features were used 
in combination with the ranked list of priority 
systems to create a rule-based merger. 
4.4 MT Systems 
The MT Combination system used by both of the 
English 5W systems combined up to nine sepa-
rate MT systems. System weights for combina-
tion were optimized together with the language 
426
model score and word penalty for a combination 
of BLEU and TER (2*(1-BLEU) + TER). Res-
coring was applied after system combination us-
ing large language models and lexical trigger 
models. Of the nine systems, six were phrased-
based systems (one of these used chunk-level 
reordering of the Chinese, one used word sense 
disambiguation, and one used unsupervised Chi-
nese word segmentation), two were hierarchical 
phrase-based systems, one was a string-to-
dependency system, one was syntax-augmented, 
and one was a combination of two other systems. 
Bleu scores on the government supplied test set 
in December 2008 were 35.2 for formal text, 
29.2 for informal text, 33.2 for formal speech, 
and 27.6 for informal speech. More details may 
be found in (Matusov et al 2009). 
5 Methods 
5.1 5W Systems 
For the purposes of this evaluation2, we com-
pared the output of 4 systems: English-Function, 
English-LF, Chinese-align, and the combined 
system. Each English system was also run on 
reference translations of the Chinese sentence. 
So for each sentence in the evaluation corpus, 
there were 6 systems that each provided 5Ws. 
5.2 5W Answer Annotation 
For each 5W output, annotators were presented 
with the reference translation, the MT version, 
and the 5W answers. The 5W system names 
were hidden from the annotators. Annotators had 
to select ?Correct?, ?Partial? or ?Incorrect? for 
each W. For answers that were Partial or Incor-
rect, annotators had to further specify the source 
of the error based on several categories (de-
scribed in section 6). All three annotators were 
native English speakers who were not system 
developers for any of the 5W systems that were 
being evaluated (to avoid biased grading, or as-
signing more blame to the MT system). None of 
the annotators knew Chinese, so all of the judg-
ments were based on the reference translations. 
After one round of annotation, we measured 
inter-annotator agreement on the Correct, Partial, 
or Incorrect judgment only. The kappa value was 
0.42, which was lower than we expected. An-
other surprise was that the agreement was lower 
                                                 
2
 Note that an official evaluation was also performed by 
DARPA and BAE. This evaluation provides more fine-
grained detail on error types and gives results for the differ-
ent approaches. 
for When, Where and Why (?=0.31) than for 
Who or What (?=0.48). We found that, in cases 
where a system would get both Who and What 
wrong, it was often ambiguous how the remain-
ing W?s should be graded. Consider the sentence: 
?He went to the store yesterday and cooked lasa-
gna today.? A system might return erroneous 
Who and What answers, and return Where as ?to 
the store? and When as ?today.? Since Where 
and When apply to different predicates, they 
cannot both be correct. In order to be consistent, 
if a system returned erroneous Who and What 
answers, we decided to mark the When, Where 
and Why answers Incorrect by default. We added 
clarifications to the guidelines and discussed ar-
eas of confusion, and then the annotators re-
viewed and updated their judgments.  
After this round of annotating, ?=0.83 on the 
Correct, Partial, Incorrect judgments. The re-
maining disagreements were genuinely ambigu-
ous cases, where a sentence could be interpreted 
multiple ways, or the MT could be understood in 
various ways. There was higher agreement on 
5W?s answers from the reference text compared 
to MT text, since MT is inherently harder to 
judge and some annotators were more flexible 
than others in grading garbled MT. 
5.3 5W Error Annotation 
In addition to judging the system answers by the 
task guidelines, annotators were asked to provide 
reason(s) an answer was wrong by selecting from 
a list of predefined errors. Annotators were asked 
to use their best judgment to ?assign blame? to 
the 5W system, the MT, or both. There were six 
types of system errors and four types of MT er-
rors, and the annotator could select any number 
of errors. (Errors are described further in section 
6.) For instance, if the translation was correct, 
but the 5W system still failed, the blame would 
be assigned to the system. If the 5W system 
picked an incorrectly translated argument (e.g., 
?baked a moon? instead of ?baked a cake?), then 
the error would be assigned to the MT system. 
Annotators could also assign blame to both sys-
tems, to indicate that they both made mistakes.  
Since this annotation task was a 10-way selec-
tion, with multiple selections possible, there were 
some disagreements. However, if categorized 
broadly into 5W System errors only, MT errors 
only, and both 5W System and MT errors, then 
the annotators had a substantial level of agree-
ment (?=0.75 for error type, on sentences where 
both annotators indicated an error).  
427
5.4 5 W Corpus 
The full evaluation corpus is 350 documents, 
roughly evenly divided between four genres: 
formal text (newswire), informal text (blogs and 
newsgroups), formal speech (broadcast news) 
and informal speech (broadcast conversation). 
For this analysis, we randomly sampled docu-
ments to judge from each of the genres. There 
were 50 documents (249 sentences) that were 
judged by a single annotator. A subset of that set, 
with 22 documents and 103 sentences, was 
judged by two annotators. In comparing the re-
sults from one annotator to the results from both 
annotators, we found substantial agreement. 
Therefore, we present results from the single an-
notator so we can do a more in-depth analysis. 
Since each sentence had 5W?s, and there were 6 
systems that were compared, there were 7,500 
single-annotator judgments over 249 sentences. 
6 Results 
Figure 1 shows the cross-lingual performance 
(on MT) of all the systems for each 5W. The best 
monolingual performance (on human transla-
tions) is shown as a dashed line (% Correct 
only). If a system returned Incorrect answers for 
Who and What, then the other answers were 
marked Incorrect (as explained in section 5.2). 
For the last 3W?s, the majority of errors were due 
to this (details in Figure 1), so our error analysis 
focuses on the Who and What questions. 
6.1 Monolingual 5W Performance 
To establish a monolingual baseline, the Eng-
lish 5W system was run on reference (human) 
translations of the Chinese text. For each partial 
or incorrect answer, annotators could select one 
or more of these reasons: 
? Wrong predicate or multiple predicates. 
? Answer contained another 5W answer. 
? Passive handled wrong (WHO/WHAT). 
? Answer missed. 
? Argument attached to wrong predicate. 
Figure 1 shows the performance of the best 
monolingual system for each 5W as a dashed 
line. The What question was the hardest, since it 
requires two pieces of information (the predicate 
and object). The When, Where and Why ques-
tions were easier, since they were null most of 
the time. (In English OntoNotes 2.0, 38% of sen-
tences have a When, 15% of sentences have a 
Where, and only 2.6% of sentences have a Why.) 
The most common monolingual system error on 
these three questions was a missed answer, ac-
counting for all of the Where errors, all but one 
Why error and 71% of the When errors. The re-
maining When errors usually occurred when the 
system assumed the wrong sense for adverbs 
(such as ?then? or ?just?). 
 Missing Other 
5W 
Wrong/Multiple 
Predicates 
Wrong 
REF-func 37 29 22 7 
REF-LF 54 20 17 13 
MT-func 18 18 18 8 
MT-LF 26 19 10 11 
Chinese 23 17 14 8 
Hybrid 13 17 15 12 
Table 3. Percentages of Who/What errors attributed to 
each system error type. 
The top half of Table 3 shows the reasons at-
tributed to the Who/What errors for the reference 
corpus. Since English-LF preferred shorter an-
swers, it frequently missed answers or parts of 
Figure 1. System performance on each 5W. ?Partial? indicates that part of the answer was missing. Dashed lines 
show the performance of the best monolingual system (% Correct on human translations). For the last 3W?s, the 
percent of answers that were Incorrect ?by default? were: 30%, 24%, 27% and 22%, respectively, and 8% for the 
best monolingual system 
60 60 56 66
36 40 38 42
56 59 59 64 63
70 66 73 68 75 71 78
19201914
0
10
20
30
40
50
60
70
80
90
100
En
g-
fu
nc
En
g-
LF
Ch
in
es
e
Hy
br
id
En
g-
fu
nc
En
g-
LF
Ch
in
es
e
Hy
br
id
En
g-
fu
nc
En
g-
LF
Ch
in
es
e
Hy
br
id
En
g-
fu
nc
En
g-
LF
Ch
in
es
e
Hy
br
id
En
g-
fu
nc
En
g-
LF
Ch
in
es
e
Hy
br
id
WHO WHAT WHEN WHERE WHY
Partia l
Correct
90
75 81
83 90
Best 
mono-
lingual
428
answers. English-LF also had more Partial an-
swers on the What question: 66% Correct and 
12% Partial, versus 75% Correct and 1% Partial 
for English-function. On the other hand, English-
function was more likely to return answers that 
contained incorrect extra information, such as 
another 5W or a second predicate. 
6.2 Effect of MT on 5W Performance 
The cross-lingual 5W task requires that systems 
return intelligible responses that are semantically 
equivalent to the source sentence (or, in the case 
of this evaluation, equivalent to the reference).  
As can be seen in Figure 1, MT degrades the 
performance of the 5W systems significantly, for 
all question types, and for all systems. Averaged 
over all questions, the best monolingual system 
does 19% better than the best cross-lingual sys-
tem. Surprisingly, even though English-function 
outperformed English-LF on the reference data, 
English-LF does consistently better on MT. This 
is likely due to its use of multiple back-off meth-
ods when the parser failed.  
6.3 Source-Language vs. Target-Language 
The Chinese system did slightly worse than ei-
ther English system overall, but in the formal 
text genre, it outperformed both English systems.  
Although the accuracies for the Chinese and 
English systems are similar, the answers vary a 
lot. Nearly half (48%) of the answers can be an-
swered correctly by both the English system and 
the Chinese system. But 22% of the time, the 
English system returned the correct answer when 
the Chinese system did not. Conversely, 10% of 
the answers were returned correctly by the Chi-
nese system and not the English systems. The 
hybrid system described in section 4.2 attempts 
to exploit these complementary advantages. 
After running the hybrid system, 61% of the 
answers were from English-LF, 25% from Eng-
lish-function, 7% from Chinese-align, and the 
remaining 7% were from the other Chinese 
methods (not evaluated here). The hybrid did 
better than its parent systems on all 5Ws, and the 
numbers above indicate that further improvement 
is possible with a better combination strategy.  
6.4 Cross-Lingual 5W Error Analysis 
For each Partial or Incorrect answer, annotators 
were asked to select system errors, translation 
errors, or both. (Further analysis is necessary to 
distinguish between ASR errors and MT errors.) 
The translation errors considered were: 
? Word/phrase deleted. 
? Word/phrase mistranslated. 
? Word order mixed up. 
? MT unreadable. 
Table 4 shows the translation reasons attrib-
uted to the Who/What errors. For all systems, the 
errors were almost evenly divided between sys-
tem-only, MT-only and both, although the Chi-
nese system had a higher percentage of system-
only errors. The hybrid system was able to over-
come many system errors (for example, in Table 
2, only 13% of the errors are due to missing an-
swers), but still suffered from MT errors. 
Table 4. Percentages of Who/What errors by each 
system attributed to each translation error type. 
Mistranslation was the biggest translation 
problem for all the systems. Consider the first 
example in Figure 3. Both English systems cor-
rectly extracted the Who and the When, but for 
Mistrans-
lation 
Deletion Word 
Order 
Unreadable 
MT-func 34 18 24 18 
MT-LF 29 22 21 14 
Chinese 32 17 9 13 
Hybrid 35 19 27 18 
MT: After several rounds of reminded, I was a little bit 
Ref: After several hints, it began to come back to me. 
 ??????,?????????? 
MT: The Guizhou province, within a certain bank robber, under the watchful eyes of a weak woman, and, with a 
knife stabbed the woman. 
Ref: I saw that in a bank in Guizhou Province, robbers seized a vulnerable young woman in front of a group of 
onlookers and stabbed the woman with a knife. 
 ?????????,?????????,???????,??,???????? 
MT: Woke up after it was discovered that the property is not more than eleven people do not even said that the 
memory of the receipt of the country into the country. 
Ref: Well, after waking up, he found everything was completely changed. Apart from having additional eleven 
grandchildren, even the motherland as he recalled has changed from a socialist country to a capitalist country. 
 ?????????????,?????????,????????????????????????? 
Figure 3 Example sentences that presented problems for the 5W systems. 
 
429
What they returned ?was a little bit.? This is the 
correct predicate for the sentence, but it does not 
match the meaning of the reference. The Chinese 
5W system was able to select a better translation, 
and instead returned ?remember a little bit.? 
Garbled word order was chosen for 21-24% of 
the target-language system Who/What errors, but 
only 9% of the source-language system 
Who/What errors. The source-language word 
order problems tended to be local, within-phrase 
errors (e.g., ?the dispute over frozen funds? was 
translated as ?the freezing of disputes?). The tar-
get-language system word order problems were 
often long-distance problems. For example, the 
second sentence in Figure 3 has many phrases in 
common with the reference translation, but the 
overall sentence makes no sense. The watchful 
eyes actually belong to a ?group of onlookers? 
(deleted). Ideally, the robber would have 
?stabbed the woman? ?with a knife,? rather than 
vice versa. Long-distance phrase movement is a 
common problem in Chinese-English MT, and 
many MT systems try to handle it (e.g., Wang et 
al. 2007). By doing analysis in the source lan-
guage, the Chinese 5W system is often able to 
avoid this problem ? for example, it successfully 
returned ?robbers? ?grabbed a weak woman? for 
the Who/What of this sentence. 
Although we expected that the Chinese system 
would have fewer problems with MT deletion, 
since it could choose from three different MT 
versions, MT deletion was a problem for all sys-
tems. In looking more closely at the deletions, 
we noticed that over half of deletions were verbs 
that were completely missing from the translated 
sentence. Since MT systems are tuned for word-
based overlap measures (such as BLEU), verb 
deletion is penalized equally as, for example, 
determiner deletion. Intuitively, a verb deletion 
destroys the central meaning of a sentence, while 
a determiner is rarely necessary for comprehen-
sion. Other kinds of deletions included noun 
phrases, pronouns, named entities, negations and 
longer connecting phrases.  
Deletion also affected When and Where. De-
leting particles such as ?in? and ?when? that in-
dicate a location or temporal argument caused 
the English systems to miss the argument. Word 
order problems in MT also caused attachment 
ambiguity in When and Where. 
The ?unreadable? category was an option of 
last resort for very difficult MT sentences. The 
third sentence in Figure 3 is an example where 
ASR and MT errors compounded to create an 
unparseable sentence.  
7 Conclusions 
In our evaluation of various 5W systems, we dis-
covered several characteristics of the task. The 
What answer was the hardest for all systems, 
since it is difficult to include enough information 
to cover the top-level predicate and object, with-
out getting penalized for including too much. 
The challenge in the When, Where and Why 
questions is due to sparsity ? these responses 
occur in much fewer sentences than Who and 
What, so systems most often missed these an-
swers. Since this was a new task, this first 
evaluation showed clear issues on the language 
analysis side that can be improved in the future. 
The best cross-lingual 5W system was still 
19% worse than the best monolingual 5W sys-
tem, which shows that MT significantly degrades 
sentence-level understanding. A serious problem 
in MT for systems was deletion. Chinese con-
stituents that were never translated caused seri-
ous problems, even when individual systems had 
strategies to recover. When the verb was deleted, 
no top level predicate could be found and then all 
5Ws were wrong.  
One of our main research questions was 
whether to extract or translate first. We hypothe-
sized that doing source-language analysis would 
be more accurate, given the noise in Chinese 
MT, but the systems performed about the same. 
This is probably because the English tools (logi-
cal form extraction and parser) were more ma-
ture and accurate than the Chinese tools.  
Although neither source-language nor target-
language analysis was able to circumvent prob-
lems in MT, each approach had advantages rela-
tive to the other, since they did well on different 
sets of sentences. For example, Chinese-align 
had fewer problems with word order, and most 
of those were due to local word-order problems.  
Since the source-language and target-language 
systems made different kinds of mistakes, we 
were able to build a hybrid system that used the 
relative advantages of each system to outperform 
all systems. The different types of mistakes made 
by each system suggest features that can be used 
to improve the combination system in the future. 
Acknowledgments 
This work was supported in part by the Defense 
Advanced Research Projects Agency (DARPA) 
under contract number HR0011-06-C-0023. Any 
opinions, findings and conclusions or recom-
mendations expressed in this material are the 
authors' and do not necessarily reflect those of 
the sponsors. 
430
References  
Collin F. Baker, Charles J. Fillmore, and John B. 
Lowe. 1998. The Berkeley FrameNet project. In 
COLING-ACL '98: Proceedings of the Conference, 
held at the University of Montr?al, pages 86?90. 
Xavier Carreras and Llu?s M?rquez. 2005. Introduc-
tion to the CoNLL-2005 shared task: Semantic role 
labeling. In Proceedings of the Ninth Conference 
on Computational Natural Language Learning 
(CoNLL-2005), pages 152?164. 
Eugene Charniak. 2001. Immediate-head parsing for 
language models. In Proceedings of the 39th An-
nual Meeting on Association For Computational 
Linguistics (Toulouse, France, July 06 - 11, 2001).   
John Chen and Owen Rambow. 2003. Use of deep 
linguistic features for the recognition and labeling 
of semantic arguments. In Proceedings of the 2003 
Conference on Empirical Methods in Natural Lan-
guage Processing, Sapporo, Japan. 
Katrin Erk and Sebastian Pado. 2006. Shalmaneser ? 
a toolchain for shallow semantic parsing. Proceed-
ings of LREC. 
Daniel Gildea and Daniel Jurafsky. 2002. Automatic 
labeling of semantic roles. Computational Linguis-
tics, 28(3):245?288. 
Daniel Gildea and Martha Palmer. 2002. The neces-
sity of parsing for predicate argument recognition. 
In Proceedings of the 40th Annual Conference of 
the Association for Computational Linguistics 
(ACL-02), Philadelphia, PA, USA. 
Mary Harper and Zhongqiang Huang. 2009. Chinese 
Statistical Parsing, chapter to appear. 
Aria Haghighi, Kristina Toutanova, and Christopher 
Manning. 2005. A joint model for semantic role la-
beling. In Proceedings of the Ninth Conference on 
Computational Natural Language Learning 
(CoNLL-2005), pages 173?176.  
Paul Kingsbury and Martha Palmer. 2003. Propbank: 
the next level of treebank. In Proceedings of Tree-
banks and Lexical Theories. 
Evgeny Matusov, Gregor Leusch, & Hermann Ney: 
Learning to combine machine translation systems.  
In: Cyril Goutte, Nicola Cancedda, Marc Dymet-
man, & George Foster (eds.) Learning machine 
translation. (Cambridge, Mass.: The MIT Press, 
2009); pp.257-276. 
Adam Meyers, Ralph Grishman, Michiko Kosaka and 
Shubin Zhao.  2001. Covering Treebanks with 
GLARF. In Proceedings of the ACL 2001 Work-
shop on Sharing Tools and Resources. Annual 
Meeting of the ACL. Association for Computa-
tional Linguistics, Morristown, NJ, 51-58. 
Teruko Mitamura, Eric Nyberg, Hideki Shima, 
Tsuneaki Kato, Tatsunori Mori, Chin-Yew Lin, 
Ruihua Song, Chuan-Jie Lin, Tetsuya Sakai, 
Donghong Ji, and Noriko Kando. 2008. Overview 
of the NTCIR-7 ACLIA Tasks: Advanced Cross-
Lingual Information Access. In Proceedings of the 
Seventh NTCIR Workshop Meeting. 
Alessandro Moschitti, Silvia Quarteroni, Roberto 
Basili, and Suresh Manandhar. 2007. Exploiting 
syntactic and shallow semantic kernels for question 
answer classification. In Proceedings of the 45th 
Annual Meeting of the Association of Computa-
tional Linguistics, pages 776?783.  
Kristen Parton, Kathleen R. McKeown, James Allan, 
and Enrique Henestroza. Simultaneous multilingual 
search for translingual information retrieval. In 
Proceedings of ACM 17th Conference on Informa-
tion and Knowledge Management (CIKM), Napa 
Valley, CA, 2008. 
Slav Petrov and Dan Klein. 2007. Improved Inference 
for Unlexicalized Parsing. North American Chapter 
of the Association for Computational Linguistics 
(HLT-NAACL 2007). 
Sudo, K., Sekine, S., and Grishman, R. 2004. Cross-
lingual information extraction system evaluation. 
In Proceedings of the 20th international Confer-
ence on Computational Linguistics. 
Honglin Sun and Daniel Jurafsky. 2004. Shallow Se-
mantic Parsing of Chinese. In Proceedings of 
NAACL-HLT. 
Cynthia A. Thompson, Roger Levy, and Christopher 
Manning. 2003. A generative model for semantic 
role labeling. In 14th European Conference on Ma-
chine Learning. 
Nianwen Xue and Martha Palmer. 2004. Calibrating 
features for semantic role labeling. In Dekang Lin 
and Dekai Wu, editors, Proceedings of EMNLP 
2004, pages 88?94, Barcelona, Spain, July. Asso-
ciation for Computational Linguistics. 
Xue, Nianwen and Martha Palmer. 2005. Automatic 
semantic role labeling for Chinese verbs. InPro-
ceedings of the Nineteenth International Joint Con-
ference on Artificial Intelligence, pages 1160-1165.  
Chao Wang, Michael Collins, and Philipp Koehn. 
2007. Chinese Syntactic Reordering for Statistical 
Machine Translation. Proceedings of the 2007 Joint 
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), 737-745. 
Jianqiang Wang and Douglas W. Oard, 2006. "Com-
bining Bidirectional Translation and Synonymy for 
Cross-Language Information Retrieval," in 29th 
Annual International ACM SIGIR Conference on 
Research and Development in Information Re-
trieval, pp. 202-209. 
431
Proceedings of the NAACL HLT Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 146?154,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Automatic Recognition of Logical Relations for English, Chinese and
Japanese in the GLARF Framework
Adam Meyers?, Michiko Kosaka?, Nianwen Xue?, Heng Ji?, Ang Sun?, Shasha Liao? and Wei Xu?
? New York University, ?Monmouth University, ?Brandeis University, ? City University of New York
Abstract
We present GLARF, a framework for repre-
senting three linguistic levels and systems for
generating this representation. We focus on a
logical level, like LFG?s F-structure, but com-
patible with Penn Treebanks. While less fine-
grained than typical semantic role labeling ap-
proaches, our logical structure has several ad-
vantages: (1) it includes all words in all sen-
tences, regardless of part of speech or seman-
tic domain; and (2) it is easier to produce ac-
curately. Our systems achieve 90% for En-
glish/Japanese News and 74.5% for Chinese
News ? these F-scores are nearly the same as
those achieved for treebank-based parsing.
1 Introduction
For decades, computational linguists have paired a
surface syntactic analysis with an analysis represent-
ing something ?deeper?. The work of Harris (1968),
Chomsky (1957) and many others showed that one
could use these deeper analyses to regularize differ-
ences between ways of expressing the same idea.
For statistical methods, these regularizations, in ef-
fect, reduce the number of significant differences be-
tween observable patterns in data and raise the fre-
quency of each difference. Patterns are thus easier
to learn from training data and easier to recognize in
test data, thus somewhat compensating for the spare-
ness of data. In addition, deeper analyses are often
considered semantic in nature because conceptually,
two expressions that share the same regularized form
also share some aspects of meaning. The specific de-
tails of this ?deep? analysis have varied quite a bit,
perhaps more than surface syntax.
In the 1970s and 1980s, Lexical Function Gram-
mar?s (LFG) way of dividing C-structure (surface)
and F-structure (deep) led to parsers such as (Hobbs
and Grishman, 1976) which produced these two lev-
els, typically in two stages. However, enthusiasm
for these two-stage parsers was eclipsed by the ad-
vent of one stage parsers with much higher accu-
racy (about 90% vs about 60%), the now-popular
treebank-based parsers including (Charniak, 2001;
Collins, 1999) and many others. Currently, many
different ?deeper? levels are being manually anno-
tated and automatically transduced, typically using
surface parsing and other processors as input. One
of the most popular, semantic role labels (annota-
tion and transducers based on the annotation) char-
acterize relations anchored by select predicate types
like verbs (Palmer et al, 2005), nouns (Meyers et
al., 2004a), discourse connectives (Miltsakaki et al,
2004) or those predicates that are part of particular
semantic frames (Baker et al, 1998). The CONLL
tasks for 2008 and 2009 (Surdeanu et al, 2008;
Hajic? et al, 2009) has focused on unifying many of
these individual efforts to produce a logical structure
for multiple parts of speech and multiple languages.
Like the CONLL shared task, we link surface lev-
els to logical levels for multiple languages. How-
ever, there are several differences: (1) The logical
structures produced automatically by our system can
be expected to be more accurate than the compara-
ble CONLL systems because our task involves pre-
dicting semantic roles with less fine-grained distinc-
tions. Our English and Japanese results were higher
than the CONLL 2009 SRL systems. Our English F-
scores range from 76.3% (spoken) to 89.9% (News):
146
the best CONLL 2009 English scores were 73.31%
(Brown) and 85.63% (WSJ). Our Japanese system
scored 90.6%: the best CONLL 2009 Japanese score
was 78.35%. Our Chinese system 74.5%, 4 points
lower than the best CONLL 2009 system (78.6%),
probably due to our system?s failings, rather than the
complexity of the task; (2) Each of the languages
in our system uses the same linguistic framework,
using the same types of relations, same analyses of
comparable constructions, etc. In one case, this re-
quired a conversion from a different framework to
our own. In contrast, the 2009 CONLL task puts
several different frameworks into one compatible in-
put format. (3) The logical structures produced by
our system typically connect all the words in the sen-
tence. While this is true for some of the CONLL
2009 languages, e.g., Czech, it is not true about
all the languages. In particular, the CONLL 2009
English and Chinese logical structures only include
noun and verb predicates.
In this paper, we will describe the GLARF frame-
work (Grammatical and Logical Representation
Framework) and a system for producing GLARF
output (Meyers et al, 2001; Meyers, 2008). GLARF
provides a logical structure for English, Chinese and
Japanese with an F-score that is within a few per-
centage points of the best parsing results for that
language. Like LFG?s (LFG) F-structure, our log-
ical structure is less fine-grained than many of the
popular semantic role labeling schemes, but also has
two main advantages over these schemes: it is more
reliable and it is more comprehensive in the sense
that it covers all parts of speech and the resulting
logical structure is a connected graph. Our approach
has proved adequate for three genetically unrelated
natural languages: English, Chinese and Japanese.
It is thus a good candidate for additional languages
with accurate parsers.
2 The GLARF framework
Our system creates a multi-tiered representation in
the GLARF framework, combining the theory un-
derlying the Penn Treebank for English (Marcus et
al., 1994) and Chinese (Xue et al, 2005) (Chom-
skian linguistics of the 1970s and 1980s) with: (2)
Relational Grammar?s graph-based way of repre-
senting ?levels? as sequences of relations; (2) Fea-
ture structures in the style of Head-Driven Phrase
Structure Grammar; and (3) The Z. Harris style goal
of attempting to regularize multiple ways of saying
the same thing into a single representation. Our
approach differs from LFG F-structure in several
ways: we have more than two levels; we have a
different set of relational labels; and finally, our ap-
proach is designed to be compatible with the Penn
Treebank framework and therefore, Penn-Treebank-
based parsers. In addition, the expansion of our the-
ory is governed more by available resources than by
the underlying theory. As our main goal is to use
our system to regularize data, we freely incorporate
any analysis that fits this goal. Over time, we have
found ways of incorporating Named Entities, Prop-
Bank, NomBank and the Penn Discourse Treebank.
Our agenda also includes incorporating the results of
other research efforts (Pustejovsky et al, 2005).
For each sentence, we generate a feature structure
(FS) representing our most complete analysis. We
distill a subset of this information into a dependency
structure governed by theoretical assumptions, e.g.,
about identifying functors of phrases. Each GLARF
dependency is between a functor and an argument,
where the functor is the head of a phrase, conjunc-
tion, complementizer, or other function word. We
have built applications that use each of these two
representations, e.g., the dependency representation
is used in (Shinyama, 2007) and the FS represen-
tation is used in (K. Parton and K. R. McKeown
and R. Coyne and M. Diab and R. Grishman and
D. Hakkani-Tu?r and M. Harper and H. Ji and W. Y.
Ma and A. Meyers and S. Stolbach and A. Sun and
G. Tu?r and W. Xu and S. Yarman, 2009).
In the dependency representation, each sentence
is a set of 23 tuples, each 23-tuple characterizing up
to three relations between two words: (1) a SUR-
FACE relation, the relation between a functor and an
argument in the parse of a sentence; (2) a LOGIC1
relation which regularizes for lexical and syntac-
tic phenomena like passive, relative clauses, deleted
subjects; and (3) a LOGIC2 relation corresponding
to relations in PropBank, NomBank, and the Penn
Discourse Treebank (PDTB). While the full output
has all this information, we will limit this paper to
a discussion of the LOGIC1 relations. Figure 1 is
a 5 tuple subset of the 23 tuple GLARF analysis of
the sentence Who was eaten by Grendel? (The full
147
L1 Surf L2 Func Arg
NIL SENT NIL Who was
PRD PRD NIL was eaten
COMP COMP ARG0 eaten by
OBJ NIL ARG1 eaten Who
NIL OBJ NIL by Grendel
SBJ NIL NIL eaten Grendel
Figure 1: 5-tuples: Who was eaten by Grendel
Who
eaten
was
by
PRD
S?OBJ
L?OBJ
ARG1
COMP
ARG0
S?SENT
L?SBJ
Grendel
Figure 2: Graph of Who was eaten by Grendel
23 tuples include unique ids and fine-grained lin-
guistic features). The fields listed are: logic1 label
(L1), surface label (Surf), logic2 label (L2), func-
tor (Func) and argument (Arg). NIL indicates that
there is no relation of that type. Figure 2 repre-
sents this as a graph. For edges with two labels,
the ARG0 or ARG1 label indicates a LOGIC2 re-
lation. Edges with an L- prefix are LOGIC1 la-
bels (the edges are curved); edges with S-prefixes
are SURFACE relations (the edges are dashed); and
other (thick) edges bear unprefixed labels represent-
ing combined SURFACE/LOGIC1 relations. Delet-
ing the dashed edges yields a LOGIC1 representa-
tion; deleting the curved edges yields a SURFACE
representation; and a LOGIC2 consists of the edges
labeled ARGO and ARG1 relations, plus the sur-
face subtrees rooted where the LOGIC2 edges ter-
minate. Taken together, a sentence?s SURFACE re-
lations form a tree; the LOGIC1 relations form a
directed acyclic graph; and the LOGIC2 relations
form directed graphs with some cycles and, due to
PDTB relations, may connect sentences to previous
ones, e.g., adverbs like however, take the previous
sentence as one of their arguments.
LOGIC1 relations (based on Relational Gram-
mar) regularize across grammatical and lexical al-
ternations. For example, subcategorized verbal ar-
guments include: SBJect, OBJect and IND-OBJ (in-
direct Object), COMPlement, PRT (Particle), PRD
(predicative complement). Other verbal modifiers
include AUXilliary, PARENthetical, ADVerbial. In
contrast, FrameNet and PropBank make finer dis-
tinctions. Both PP arguments of consulted in John
consulted with Mary about the project bear COMP
relations with the verb in GLARF, but would have
distinct labels in both PropBank and FrameNet.
Thus Semantic Role Labeling (SRL) should be more
difficult than recognizing LOGIC1 relations.
Beginning with Penn Treebank II, Penn Treebank
annotation includes Function tags, hyphenated addi-
tions to phrasal categories which indicate their func-
tion. There are several types of function tags:
? Argument Tags such as SBJ, OBJ, IO (IND-
OBJ), CLR (COMP) and PRD?These are lim-
ited to verbal relations and not all are used in
all treebanks. For example, OBJ and IO are
used in the Chinese, but not the English tree-
bank. These labels can often be directly trans-
lated into GLARF LOGIC1 relations.
? Adjunct Tags such as ADV, TMP, DIR, LOC,
MNR, PRP?These tags often translate into a
single LOGIC1 tag (ADV). However, some of
these also correspond to LOGIC1 arguments.
In particular, some DIR and MNR tags are re-
alized as LOGIC1 COMP relations (based on
dictionary entries). The fine grained seman-
tic distinctions are maintained in other features
that are part of the GLARF description.
In addition, GLARF treats Penn?s PRN phrasal
category as a relation rather than a phrasal category.
For example, given a sentence like, Banana ketchup,
the agency claims, is very nutritious, the phrase
the agency claims is analyzed as an S(entence) in
GLARF bearing a (surface) PAREN relation to the
main clause. Furthermore, the whole sentence is a
COMP of the verb claims. Since PAREN is a SUR-
FACE relation, not a LOGIC1 relation, there is no
LOGIC1 cycle as shown by the set of 5-tuples in
Figure 3? a cycle only exists if you include both
SURFACE and LOGIC1 relations in a single graph.
Another important feature of the GLARF frame-
work is transparency, a term originating from N.
148
L1 Surf L2 Func Arg
NIL SBJ ARG1 is ketchup
PRD PRD ARG2 is nutritious
SBJ NIL NIL nutritious Ketchup
ADV ADV NIL nutritious very
N-POS N-POS NIL ketchup Banana
NIL PAREN NIL is claims
SBJ SBJ ARG0 claims agency
Q-POS Q-POS NIL agency the
COMP NIL ARG1 claims is
Figure 3: 5-tuples: Banana Ketchup, the agency claims,
is very nutritious
L1 Surf L2 Func Arg
SBJ SBJ ARG0 ate and
OBJ OBJ ARG1 ate box
CONJ CONJ NIL and John
CONJ CONJ NIL and Mary
COMP COMP NIL box of
Q-POS Q-POS NIL box the
OBJ OBJ NIL of cookies
Figure 4: 5-tuples: John and Mary ate the box of cookies
Sager?s unpublished work. A relation between two
words is transparent if: the functor fails to character-
ize the selectional properties of the phrase (or sub-
graph in a Dependency Analysis), but its argument
does. For example, relations between conjunctions
(e.g., and, or, but) and their conjuncts are transparent
CONJ relations. Thus although and links together
John and Mary, it is these dependents that deter-
mine that the resulting phrase is noun-like (an NP
in phrase structure terminology) and sentient (and
thus can occur as the subject of verbs like ate). An-
other common example of transparent relations are
the relations connecting certain nouns and the prepo-
sitional objects under them, e.g., the box of cookies
is edible, because cookies are edible even though
boxes are not. These features are marked in the
NOMLEX-PLUS dictionary (Meyers et al, 2004b).
In Figure 4, we represent transparent relations, by
prefixing the LOGIC1 label with asterisks.
The above description most accurately describes
English GLARF. However, Chinese GLARF has
most of the same properties, the main exception be-
ing that PDTB arguments are not currently marked.
For Japanese, we have only a preliminary represen-
tation of LOGIC2 relations and they are not derived
from PropBank/NomBank/PDTB.
2.1 Scoring the LOGIC1 Structure
For purposes of scoring, we chose to focus on
LOGIC1 relations, our proposed high-performance
level of semantics. We scored with respect to: the
LOGIC1 relational label, the identity of the functor
and the argument, and whether the relation is trans-
parent or not. If the system output differs in any of
these respects, the relation is marked wrong. The
following sections will briefly describe each system
and present an evaluation of its results.
The answer keys for each language were created
by native speakers editing system output, as repre-
sented similarly to the examples in this paper, al-
though part of speech is included for added clar-
ity. In addition, as we attempted to evaluate logi-
cal relation (or dependency) accuracy independent
of sentence splitting. We obtained sentence divi-
sions from data providers and treebank annotation
for all the Japanese and most of the English data, but
used automatic sentence divisions for the English
BLOG data. For the Chinese, we omitted several
sentences from our evaluation set due to incorrect
sentence splits. The English and Japanese answer
keys were annotated by single native speakers ex-
pert in GLARF. The Chinese data was annotated by
several native speakers and may have been subject
to some interannotator agreement difficulties, which
we intend to resolve in future work. Currently, cor-
recting system output is the best way to create an-
swer keys due to certain ambiguities in the frame-
work, some of which we hope to incorporate into fu-
ture scoring procedures. For example, consider the
interpretation of the phrase five acres of land in Eng-
land with respect to PP attachment. The difference
in meaning between attaching the PP in England
to acres or to land is too subtle for these authors?
we have difficulty imagining situations where one
statement would be accurate and the other would
not. This ambiguity is completely predictable be-
cause acres is a transparent noun and similar ambi-
guities hold for all such cases where a transparent
noun takes a complement and is followed by a PP
attachment. We believe that a more complex scor-
ing program could account for most of these cases.
149
Similar complexities arise for coordination and sev-
eral other phenomena.
3 English GLARF
We generate English GLARF output by applying a
procedure that combines:
1. The output of the 2005 version of the Charniak
parser described in (Charniak, 2001), which
label precision and recall scores in the 85%
range. The updated version of the parser seems
to perform closer to 90% on News data and per-
form lower on other genres. That performance
would reflect reports on other versions of the
Charniak parser for which statistics are avail-
able (Foster and van Genabith, 2008).
2. Named entity (NE) tags from the JET NE sys-
tem (Ji and Grishman, 2006), which achieves
F-scores ranging 86%-91% on newswire for
both English and Chinese (depending on
Epoch). The JET system identifies seven
classes of NEs: Person, GPE, Location, Orga-
nization, Facility, Weapon and Vehicle.
3. Machine Readable dictionaries: COMLEX
(Macleod et al, 1998), NOMBANK dictio-
naries (from http://nlp.cs.nyu.edu/
meyers/nombank/) and others.
4. A sequence of hand-written rules (citations
omitted) such that: (1) the first set of rules con-
vert the Penn Treebank into a Feature Structure
representation; and (2) each rule N after the
first rule is applied to an entire Feature Struc-
ture that is the output of rule N ? 1.
For this paper, we evaluated the English output for
several different genres, all of which approximately
track parsing results for that genre. For written
genres, we chose between 40 and 50 sentences.
For speech transcripts, we chose 100 sentences?we
chose this larger number because a lot of so-called
sentences contained text with empty logical de-
scriptions, e.g., single word utterances contain no
relations between pairs of words. Each text comes
from a different genre. For NEWS text, we used 50
sentences from the aligned Japanese-English data
created as part of the JENAAD corpus (Utiyama
Genre Prec Rec F
NEWS 731815 = 89.7%
715
812 = 90.0% 89.9%
BLOG 704844 = 83.4%
704
899 = 78.3% 80.8%
LETT 392434 = 90.3%
392
449 = 87.3% 88.8%
TELE 472604 = 78.1%
472
610 = 77.4% 77.8%
NARR 732959 = 76.3%
732
964 = 75.9% 76.1%
Table 1: English Aggregate Scores
Corpus Prec Rec F Sents
NEWS 90.5% 90.8% 90.6% 50
BLOG 84.1% 79.6% 81.7% 46
LETT 93.9% 89.2% 91.4% 46
TELE 81.4% 83.2% 84.9% 103
NARR 77.1% 78.1% 79.5% 100
Table 2: English Score per Sentence
and Isahara, 2003); the web text (BLOGs) was
taken from some corpora provided by the Linguistic
Data Consortium through the GALE (http:
//projects.ldc.upenn.edu/gale/) pro-
gram; the LETTer genre (a letter from Good Will)
was taken from the ICIC Corpus of Fundraising
Texts (Indiana Center for Intercultural Communi-
cation); Finally, we chose two spoken language
transcripts: a TELEphone conversation from
the Switchboard Corpus (http://www.ldc.
upenn.edu/Catalog/readme_files/
switchboard.readme.html) and one NAR-
Rative from the Charlotte Narrative and Conversa-
tion Collection (http://newsouthvoices.
uncc.edu/cncc.php). In both cases, we
assumed perfect sentence splitting (based on Penn
Treebank annotation). The ICIC, Switchboard
and Charlotte texts that we used are part of the
Open American National Corpus (OANC), in
particular, the SIGANN shared subcorpus of the
OANC (http://nlp.cs.nyu.edu/wiki/
corpuswg/ULA-OANC-1) (Meyers et al, 2007).
Comparable work for English includes: (1) (Gab-
bard et al, 2006), a system which reproduces the
function tags of the Penn Treebank with 89% accu-
racy and empty categories (and their antecedents)
with varying accuracies ranging from 82.2% to
96.3%, excluding null complementizers, as these are
theory-internal and have no value for filling gaps.
(2) Current systems that generate LFG F-structure
150
such as (Wagner et al, 2007) which achieve an F
score of 91.1 on the F-structure PRED relations,
which are similar to our LOGIC1 relations.
4 Chinese GLARF
The Chinese GLARF program takes a Chinese
Treebank-style syntactic parse and the output of a
Chinese PropBanker (Xue, 2008) as input, and at-
tempts to determine the relations between the head
and its dependents within each constituent. It does
this by first exploiting the structural information
and detecting six broad categories of syntactic rela-
tions that hold between the head and its dependents.
These are predication, modification, complementa-
tion, coordination, auxiliary, and flat. Predication
holds at the clause level between the subject and the
predicate, where the predicate is considered to be
the head and the subject is considered to the depen-
dent. Modification can also hold mainly within NPs
and VPs, where the dependents are modifiers of the
NP head or adjuncts to the head verb. Coordination
holds almost for all phrasal categories where each
non-punctuation child within this constituent is ei-
ther conjunction or a conjunct. The head in a co-
ordination structure is underspecified and can be ei-
ther a conjunct or a conjunction depending on the
grammatical framework. Complementation holds
between a head and its complement, with the com-
plement usually being a core argument of the head.
For example, inside a PP, the preposition is the head
and the phrase or clause it takes is the dependent. An
auxiliary structure is one where the auxiliary takes
a VP as its complement. This structure is identi-
fied so that the auxiliary and the verb it modifies can
form a verb group in the GLARF framework. Flat
structures are structures where a constituent has no
meaningful internal structure, which is possible in a
small number of cases. After these six broad cate-
gories of relations are identified, more fine-grained
relation can be detected with additional information.
Figure 5 is a sample 4-tuple for a Chinese translation
of the sentence in figure 3.
For the results reported in Table 3, we used the
Harper and Huang parser described in (Harper and
Huang, Forthcoming) which can achieve F-scores
as high as 85.2%, in combination with informa-
tion about named entities from the output of the
Figure 5: Agency claims, Banana Ketchup is very have
nutrition DE.
JET Named Entity tagger for Chinese (86%-91% F-
measure as per section 3). We used the NE tags to
adjust the parts of speech and the phrasal boundaries
of named entities (we do the same with English).
As shown in Table 3, we tried two versions of the
Harper and Huang parser, one which adds function
tags to the output and one that does not. The Chinese
GLARF system scores significantly (13.9% F-score)
higher given function tagged input, than parser out-
put without function tags. Our current score is about
10 points lower than the parser score. Our initial er-
ror analysis suggests that the most common forms
of errors involve: (1) the processing of long NPs;
(2) segmentation and POS errors; (3) conjunction
scope; and (4) modifier attachment.
5 Japanese GLARF
For Japanese, we process text with the KNP parser
(Kurohashi and Nagao, 1998) and convert the output
into the GLARF framework. The KNP/Kyoto Cor-
pus framework is a Japanese-specific Dependency
framework, very different from the Penn Treebank
framework used for the other systems. Process-
ing in Japanese proceeds as follows: (1) we pro-
cess the Japanese with the Juman segmenter (Kuro-
151
Type Prec Rec F
No Function Tags Version
Aggr 8431374 = 61.4%
843
1352 = 62.4% 61.8%
Aver 62.3% 63.5% 63.6%
Function Tags Version
Aggr 10311415 = 72.9%
1031
1352 = 76.3% 74.5%
Aver 73.0% 75.3% 74.9%
Table 3: 53 Chinese Newswire Sentences: Aggregate and
Average Sentence Scores
hashi et al, 1994) and KNP parser 2.0 (Kurohashi
and Nagao, 1998), which has reported accuracy of
91.32% F score for dependency accuracy, as re-
ported in (Noro et al, 2005). As is standard in
Japanese linguistics, the KNP/Kyoto Corpus (K)
framework uses a dependency analysis that has some
features of a phrase structure analysis. In partic-
ular, the dependency relations are between bun-
setsu, small constituents which include a head word
and some number of modifiers which are typically
function words (particles, auxiliaries, etc.), but can
also be prenominal noun modifiers. Bunsetsu can
also include multiple words in the case of names.
The K framework differentiates types of dependen-
cies into: the normal head-argument variety, coor-
dination (or parallel) and apposition. We convert
the head-argument variety of dependency straight-
forwardly into a phrase consisting of the head and
all the arguments. In a similar way, appositive re-
lations could be represented using an APPOSITIVE
relation (as is currently done with English). In the
case of bunsetsu, the task is to choose a head and
label the other constituents?This is very similar to
our task of labeling and subdividing the flat noun
phrases of the English Penn Treebank. Conjunction
is a little different because the K analysis assumes
that the final conjunct is the functor, rather than a
conjunction. We automatically changed this analy-
sis to be the same as it is for English and Chinese.
When there was no actual conjunction, we created a
theory-internal NULL conjunction. The final stages
include: (1) processing conjunction and apposition,
including recognizing cases that the parser does not
recognize; (2) correcting parts of speech; (3) label-
ing all relations between arguments and heads; (4)
recognizing and labeling special constituent types
Figure 6: It is the state?s duty to protect lives and assets.
Type Prec Rec F
Aggr 764843 = 91.0%
764
840 = 90.6% 90.8%
Aver 90.7% 90.6% 90.6%
Table 4: 40 Japanese Sentences from JENAA Corpus:
Aggregate and Average Sentence Scores
such as Named Entities, double quote constituents
and number phrases (twenty one); (5) handling com-
mon idioms; and (6) processing light verb and cop-
ula constructions.
Figure 6 is a sample 4-tuple for a Japanese
sentence meaning It is the state?s duty to protect
lives and assets. Conjunction is handled as dis-
cussed above, using an invisible NULL conjunction
and transparent (asterisked) logical CONJ relations.
Copulas in all three languages take surface subjects,
which are the LOGIC1 subjects of the PRD argu-
ment of the copula. We have left out glosses for the
particles, which act solely as case markers and help
us identify the grammatical relation.
We scored Japanese GLARF on forty sentences of
the Japanese side of the JENAA data (25 of which
are parallel with the English sentences scored). Like
the English, the F score is very close to the parsing
scores achieved by the parser.
152
6 Concluding Remarks and Future Work
In this paper, we have described three systems
for generating GLARF representations automati-
cally from text, each system combines the out-
put of a parser and possibly some other processor
(segmenter, Named Entity Recognizer, PropBanker,
etc.) and creates a logical representation of the sen-
tence. Dictionaries, word lists, and various other
resources are used, in conjunction with hand writ-
ten rules. In each case, the results are very close to
parsing accuracy. These logical structures are in the
same annotation framework, using the same labeling
scheme and the same analysis for key types of con-
structions. There are several advantages to our ap-
proach over other characterizations of logical struc-
ture: (1) our representation is among the most accu-
rate and reliable; (2) our representation connects all
the words in the sentence; and (3) having the same
representation for multiple languages facilitates run-
ning the same procedures in multiple languages and
creating multilingual applications.
The English system was developed for the News
genre, specifically the Penn Treebank Wall Street
Journal Corpus. We are therefore considering
adding rules to better handle constructions that ap-
pear in other genres, but not news. The experi-
ments describe here should go a long way towards
achieving this goal. We are also considering ex-
periments with parsers tailored to particular genres
and/or parsers that add function tags (Harper et al,
2005). In addition, our current GLARF system uses
internal Propbank/NomBank rules, which have good
precision, but low recall. We expect that we achieve
better results if we incorporate the output of state
of the art SRL systems, although we would have to
conduct experiments as to whether or not we can im-
prove such results with additional rules.
We developed the English system over the course
of eight years or so. In contrast, the Chinese and
Japanese systems are newer and considerably less
time was spent developing them. Thus they cur-
rently do not represent as many regularizations. One
obstacle is that we do not currently use subcate-
gorization dictionaries for either language, while
we have several for English. In particular, these
would be helpful in predicting and filling relative
clause and others gaps. We are considering auto-
matically acquiring simple dictionaries by recording
frequently occurring argument types of verbs over
a larger corpus, e.g., along the lines of (Kawahara
and Kurohashi, 2002). In addition, existing Japanese
dictionaries such as the IPAL (monolingual) dictio-
nary (technology Promotion Agency, 1987) or previ-
ously acquired case information reported in (Kawa-
hara and Kurohashi, 2002).
Finally, we are investigating several avenues for
using this system output for Machine Translation
(MT) including: (1) aiding word alignment for other
MT system (Wang et al, 2007); and (2) aiding the
creation various MT models involving analyzed text,
e.g., (Gildea, 2004; Shen et al, 2008).
Acknowledgments
This work was supported by NSF Grant IIS-
0534700 Structure Alignment-based MT.
References
C. F. Baker, C. J. Fillmore, and J. B. Lowe. 1998. The
Berkeley FrameNet Project. In Coling-ACL98, pages
86?90.
E. Charniak. 2001. Immediate-head parsing for language
models. In ACL 2001, pages 116?123.
N. Chomsky. 1957. Syntactic Structures. Mouton, The
Hague.
M. Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, University
of Pennsylvania.
J. Foster and J. van Genabith. 2008. Parser Evaluation
and the BNC: 4 Parsers and 3 Evaluation Metrics. In
LREC 2008, Marrakech, Morocco.
R. Gabbard, M. Marcus, and S. Kulick. 2006. Fully pars-
ing the penn treebank. In NAACL/HLT, pages 184?
191.
D. Gildea. 2004. Dependencies vs. Constituents for
Tree-Based Alignment. In EMNLP, Barcelona.
J. Hajic?, M. Ciaramita, R. Johansson, D. Kawahara,
M. A. Mart??, L. Ma`rquez, A. Meyers, J. Nivre, S. Pado?,
J. ?Ste?pa?nek, P. Stran?a?k, M. Surdeanu, N. Xue, and
Y. Zhang. 2009. The CoNLL-2009 shared task:
Syntactic and semantic dependencies in multiple lan-
guages. In CoNLL-2009, Boulder, Colorado, USA.
M. Harper and Z. Huang. Forthcoming. Chinese Statis-
tical Parsing. In J. Olive, editor, Global Autonomous
Language Exploitation. Publisher to be Announced.
M. Harper, B. Dorr, J. Hale, B. Roark, I. Shafran,
M. Lease, Y. Liu, M. Snover, L. Yung, A. Krasnyan-
skaya, and R. Stewart. 2005. Parsing and Spoken
153
Structural Event. Technical Report, The John-Hopkins
University, 2005 Summer Research Workshop.
Z. Harris. 1968. Mathematical Structures of Language.
Wiley-Interscience, New York.
J. R. Hobbs and R. Grishman. 1976. The Automatic
Transformational Analysis of English Sentences: An
Implementation. International Journal of Computer
Mathematics, 5:267?283.
H. Ji and R. Grishman. 2006. Analysis and Repair of
Name Tagger Errors. In COLING/ACL 2006, Sydney,
Australia.
K. Parton and K. R. McKeown and R. Coyne and M. Diab
and R. Grishman and D. Hakkani-Tu?r and M. Harper
and H. Ji and W. Y. Ma and A. Meyers and S. Stol-
bach and A. Sun and G. Tu?r and W. Xu and S. Yarman.
2009. Who, What, When, Where, Why? Comparing
Multiple Approaches to the Cross-Lingual 5W Task.
In ACL 2009.
D. Kawahara and S. Kurohashi. 2002. Fertilization
of Case Frame Dictionary for Robust Japanese Case
Analysis. In Proc. of COLING 2002.
S. Kurohashi and M. Nagao. 1998. Building a Japanese
parsed corpus while improving the parsing system. In
Proceedings of The 1st International Conference on
Language Resources & Evaluation, pages 719?724.
S. Kurohashi, T. Nakamura, Y. Matsumoto, and M. Na-
gao. 1994. Improvements of Japanese Morpholog-
ical Analyzer JUMAN. In Proc. of International
Workshop on Sharable Natural Language Resources
(SNLR), pages 22?28.
C. Macleod, R. Grishman, and A. Meyers. 1998. COM-
LEX Syntax. Computers and the Humanities, 31:459?
481.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1994. Building a large annotated corpus of en-
glish: The penn treebank. Computational Linguistics,
19(2):313?330.
A. Meyers, M. Kosaka, S. Sekine, R. Grishman, and
S. Zhao. 2001. Parsing and GLARFing. In Proceed-
ings of RANLP-2001, Tzigov Chark, Bulgaria.
A. Meyers, R. Reeves, C. Macleod, R. Szekely, V. Zielin-
ska, B. Young, and R. Grishman. 2004a. The Nom-
Bank Project: An Interim Report. In NAACL/HLT
2004 Workshop Frontiers in Corpus Annotation,
Boston.
A. Meyers, R. Reeves, Catherine Macleod, Rachel Szeke-
ley, Veronkia Zielinska, and Brian Young. 2004b. The
Cross-Breeding of Dictionaries. In Proceedings of
LREC-2004, Lisbon, Portugal.
A. Meyers, N. Ide, L. Denoyer, and Y. Shinyama. 2007.
The shared corpora working group report. In Pro-
ceedings of The Linguistic Annotation Workshop, ACL
2007, pages 184?190, Prague, Czech Republic.
A. Meyers. 2008. Using treebank, dictionaries and
glarf to improve nombank annotation. In Proceedings
of The Linguistic Annotation Workshop, LREC 2008,
Marrakesh, Morocco.
E. Miltsakaki, A. Joshi, R. Prasad, and B. Webber. 2004.
Annotating discourse connectives and their arguments.
In A. Meyers, editor, NAACL/HLT 2004 Workshop:
Frontiers in Corpus Annotation, pages 9?16, Boston,
Massachusetts, USA, May 2 - May 7. Association for
Computational Linguistics.
T. Noro, C. Koike, T. Hashimoto, T. Tokunaga, and
Hozumi Tanaka. 2005. Evaluation of a Japanese CFG
Derived from a Syntactically Annotated corpus with
Respect to Dependency Measures. In 2005 Workshop
on Treebanks and Linguistic theories, pages 115?126.
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The
Proposition Bank: An annotated corpus of semantic
roles. Computational Linguistics, 31(1):71?106.
J. Pustejovsky, A. Meyers, M. Palmer, and M. Poe-
sio. 2005. Merging PropBank, NomBank, TimeBank,
Penn Discourse Treebank and Coreference. In ACL
2005 Workshop: Frontiers in Corpus Annotation II:
Pie in the Sky.
L. Shen, J. Xu, and R. Weischedel. 2008. A New String-
to-Dependency Machine Translation Algorithm with a
Target Dependency Language Model. In ACL 2008.
Y. Shinyama. 2007. Being Lazy and Preemptive at
Learning toward Information Extraction. Ph.D. the-
sis, NYU.
M. Surdeanu, R. Johansson, A. Meyers, Ll. Ma?rquez,
and J. Nivre. 2008. The CoNLL-2008 Shared Task
on Joint Parsing of Syntactic and Semantic Dependen-
cies. In Proceedings of the CoNLL-2008 Shared Task,
Manchester, GB.
Information technology Promotion Agency. 1987. IPA
Lexicon of the Japanese Language for Computers
IPAL (Basic Verbs). (in Japanese).
M. Utiyama and H. Isahara. 2003. Reliable Measures
for Aligning Japanese-English News Articles and Sen-
tences. In ACL-2003, pages 72?79.
J. Wagner, D. Seddah, J. Foster, and J. van Genabith.
2007. C-Structures and F-Structures for the British
National Corpus. In Proceedings of the Twelfth In-
ternational Lexical Functional Grammar Conference,
Stanford. CSLI Publications.
C. Wang, M. Collins, and P. Koehn. 2007. Chinese syn-
tactic reordering for statistical machine translation. In
EMNLP-CoNLL 2007, pages 737?745.
N. Xue, F. Xia, F. Chiou, and M. Palmer. 2005. The
Penn Chinese TreeBank: Phrase Structure Annotation
of a Large Corpus. Natural Language Engineering,
11:207?238.
N. Xue. 2008. Labeling Chinese Predicates with Seman-
tic roles. Computational Linguistics, 34:225?255.
154
Proceedings of the 2009 Workshop on Language Generation and Summarisation, ACL-IJCNLP 2009, pages 48?55,
Suntec, Singapore, 6 August 2009. c?2009 ACL and AFNLP
A Parse-and-Trim Approach with Information Significance               
for Chinese Sentence Compression 
 
 
 
Wei Xu           Ralph Grishman 
Computer Science Department 
New York University 
New York, NY, 10003, USA 
{xuwei,grishman}@cs.nyu.edu 
 
 
 
  
 
Abstract 
In this paper, we propose an event-based ap-
proach for Chinese sentence compression 
without using any training corpus. We en-
hance the linguistically-motivated heuristics 
by exploiting event word significance and 
event information density. This is shown to 
improve the preservation of important infor-
mation and the tolerance of POS and parsing 
errors, which are more common in Chinese 
than English. The heuristics are only required 
to determine possibly removable constituents 
instead of selecting specific constituents for 
removal, and thus are easier to develop and 
port to other languages and domains. The ex-
perimental results show that around 72% of 
our automatic compressions are grammatically 
and semantically correct, preserving around 
69% of the most important information on av-
erage.  
1 Introduction 
The goal of sentence compression is to shorten 
sentences while preserving their grammaticality 
and important information. It has recently at-
tracted much attention because of its wide range 
of applications, especially in summarization 
(Jing, 2000) and headline generation (which can 
be viewed as summarization with very short 
length requirement). Sentence compression can 
improve extractive summarization in coherence 
and amount of information expressed within a 
fixed length. 
An ideal sentence compression will include 
complex paraphrasing operations, such as word 
deletion, substitution, insertion, and reordering. 
In this paper, we focus on the simpler instantia-
tion of sentence simplification, namely word de-
letion, which has been proved a success in the 
literature (Knight and Marcu, 2002; Dorr et al 
2003; Clarke and Lapata, 2006).  
In this paper, we present our technique for 
Chinese sentence compression without the need 
for a sentence/compression parallel corpus. We 
combine linguistically-motivated heuristics and 
word significance scoring together to trim the 
parse tree, and rank candidate compressions ac-
cording to event information density. In contrast 
to probabilistic methods, the heuristics are more 
likely to produce grammatical and fluent com-
pressed sentences. We reduce the difficulty and 
linguistic skills required for composing heuristics 
by only requiring these heuristics to identify pos-
sibly removable constituents instead of selecting 
specific constituents for removal. The word sig-
nificance helps to preserve informative constitu-
ents and overcome some POS and parsing errors. 
In particular, we seek to assess the event infor-
mation during the compression process, accord-
ing to the previous successes in event-based 
summarization (Li et al 2006) and a new event-
oriented 5W summarization task (Parton et al 
2009). 
The next section presents previous approaches 
to sentence compression. In section 3, we de-
scribe our system with three modules, viz. lin-
guistically-motivated heuristics, word signific-
ance scoring and candidate compression selec-
tion. We also develop a heuristics-only approach 
for comparison. In section 4, we evaluate the 
compressions in terms of grammaticality, infor-
48
mativeness and compression rate. Finally, Sec-
tion 5 concludes this paper and discusses direc-
tions of future work. 
2 Previous Work 
Most previous studies relied on a parallel cor-
pus to learn the correspondences between origi-
nal and compressed sentences. Typically sen-
tences are represented by features derived from 
parsing results, and used to learn the transforma-
tion rules or estimate the parameters in the score 
function of a possible compression. A variety of 
models have been developed, including but not 
limited to the noisy-channel model (Knight and 
Marcu, 2002; Galley and McKeown, 2007), the 
decision-tree model (Knight and Marcu, 2002), 
support vector machines (Nguyen et al 2004) 
and large-margin learning (McDonald, 2006; 
Cohn and Lapata 2007).  
Approaches which do not employ parallel cor-
pora are less popular, even though the parallel 
sentence/compression corpora are not as easy to 
obtain as multilingual corpora for machine trans-
lation. Only a few studies have been done requir-
ing no or minimal training corpora (Dorr et al 
2003; Hori and Furui, 2004; Turner and Char-
niak, 2005). The scarcity of parallel corpora also 
constrains the development in languages other 
than English. To the best of our knowledge, no 
study has been done on Chinese sentence com-
pression.  
An algorithm making limited use of training 
corpora was proposed originally by Hori and Fu-
rui (2004) for spoken text in Japanese, and later 
modified by Clarke and Lapata (2006) for Eng-
lish text. Their model searches for the compres-
sion with highest score according to the signific-
ance of each word, the existence of Subject-
Verb-Object structures and the language model 
probability of the resulting word combination. 
The weight factors to balance the three mea-
surements are experimentally optimized by a 
parallel corpus or estimated by experience.  
Turner and Charniak (2005) present semi-
supervised and unsupervised variants of the noi-
sy channel model. They approximate the rules of 
compression from a non-parallel corpus (e.g. the 
Penn Treebank) based on probabilistic context 
free grammar derivation. 
Our approach is most similar to the Hedge 
Trimmer for English headline generation (Dorr et 
al, 2003), in which linguistically-motivated heu-
ristics are used to trim the parse tree. This me-
thod removes low content components in a preset 
order until the desired length requirement is 
reached. It reduces the risk of deleting subordi-
nate clauses and prepositional phrases by delay-
ing these operations until no other rules can be 
applied. This fixed order of applying rules limits 
the flexibility and capability for preserving in-
formative constituents during deletions. It is like-
ly to fail by producing a grammatical but seman-
tically useless compressed sentence. Another 
major drawback is that it requires considerable 
linguistic skill to produce proper rules in a proper 
order.   
3 Algorithms for Sentence Compression 
Our system takes the output of a Chinese Tree-
bank-style syntactic parser (Huang and Harper, 
2009) as input and performs tree trimming opera-
tions to obtain compression. We propose and 
compare two approaches. One uses only linguis-
tically-motivated heuristics to delete words and 
gets the compression result directly. The other 
one uses heuristics to determine which nodes in 
the parse tree are potentially removable. Then all 
removable nodes are deleted one by one accord-
ing to their significance weights to generate a 
series of candidate compressions. Finally, the 
best compression is selected based on sentence 
length and informativeness criteria.  
3.1 Linguistically-motivated Heuristics 
This module aims to identify the nodes in the 
parse tree which may be removed without severe 
loss in grammaticality and information. Based on 
an analysis of the Penn Treebank corpus and 
human-produced compression, we decided that 
the following parse constituents are potential low 
content units.  
 
Set 0 ? basic: 
? Parenthetical elements  
? Adverbs except negative, some temporal 
and degree adverbs 
? Adjectives except when the modified noun 
consists of only one character 
? DNPs (which are formed by various 
phrasal categories plus ??? and appear as 
modifiers of NP in Chinese) 
? DVPs (which are formed by various 
phrasal categories plus ??? in Chinese, 
and appear as modifiers of VP in Chinese) 
? All nodes in noun coordination phrases 
except the first noun 
 
 
49
Set 1 ? fixed: 
? All children of NP nodes except temporal 
nouns and proper nouns and the last noun 
word 
? All simple clauses (IP) except the first one, 
if the sentence consists of more than one 
IP 
? Prepositional phrases except those that 
may contain location or date information, 
according to a hand-made list of preposi-
tions  
 
Set 2 ? flexible: 
? All nodes in verb coordination phrases ex-
cept the first one. 
? Relative clauses 
? Appositive clauses 
? All prepositional phrases 
? All children of NP nodes except the last 
noun word 
? All simple clauses, if the sentence consists 
of more than one IP (at least one clause is 
required to be preserved in later trimming) 
 
Set 0 lists all the fundamental constituents that 
may be removed and is used in both approaches. 
Set 1 and Set 2 are designed to handle more 
complex constituents for the two approaches re-
spectively.  
The heuristics-only approach exploits Set 0 
and Set 1. It can be viewed as the Chinese ver-
sion of Hedge Trimmer (Dorr et al 2003), but 
differs in the following ways: 
1) Chinese has different language construc-
tions and grammar from English. 
2) We eliminate the strict compression 
length constraint in order to yield more 
natural compressions with varying length. 
3) We do not remove time expressions on 
purpose to benefit further applications, 
such as event extraction.  
 
The heuristics-only approach deletes low con-
tent units mechanically while preserving syntac-
tic correctness, as long as parsing is accurate. 
Our preliminary experiments showed that the 
heuristics in Set 0 and Set 1 can generate a com-
paratively satisfying compression, but is sensi-
tive to part-of-speech and parsing errors, e.g. the 
proper noun ??? (Hyundai)? as motor compa-
ny is tagged as an adjective (shown in Figure 1) 
and thus removed since its literal meaning is ??
?(modern)?. Moreover, the rules in Set 1 reduce 
the sentence length in a gross manner, risking 
serious information or grammaticality loss. For 
example, the first clause may not be a complete 
grammatical sentence, and is not always the most 
important clause in the sentence though that is 
usually the case. We also want to point out that 
the heuristics tend to reduce the sentence length 
and preserve the grammar by removing most of 
the modifiers, even though modifiers may con-
tain a lot of important information. 
To address the above problems of heuristics, 
we exploit word significance to measure the im-
portance of each constituent. Set 2 was created to 
work with Set 0 to identify removable low con-
tent units. The heuristics in this approach are 
used only to detect all possible candidates for 
deletion and thus are more general and easier to 
create than Set 1. For instance, we do not need to 
carefully determine which kinds of prepositional 
phrases are safe or dangerous to delete but in-
stead mark all of them as potentially removable.  
The actual word deletion is performed later by 
a compression generation and selection module, 
taking word significance and compression rate 
into consideration. The heuristics in Set 2 are 
able to cover more risky constituents than Set 1, 
e.g. clauses and parallel structures, since the risk 
will be controlled by the later processes. 
 
 ( (IP  
        (NP  
              (*NP (NR ??))                                South Korean  
              (#*ADJP (JJ ??))                            Hyundai 
              (NP     
                       (#*NN ??)                              motor 
                       (NN ??)))                               company 
        (VP (VC ?)                                             is 
              (NP (#*DNP (NP (NR ???))        Volvo 
                                    (DEG ?))                     ?s 
                     (#*ADJP (JJ ??))                     potential 
                     (NP (NN ??))))                        buyer 
        (PU .))) 
 
Figure 1. Parse tree trimming by heuristics 
(#: nodes trimmed out by Set0 & Set1;  
*: nodes labeled as removable by Set0 & Set2.) 
 
Figure 1 shows an example of applying heuris-
tics to the parse tree of the sentence ?????
?????????????? (The South 
Korean Hyundai Motor Company is a potential 
buyer of Volvo.). The heuristics-only approach 
produces ????????? (The South Korean 
company is a buyer.), which is grammatical but 
semantically meaningless. We will see how word 
significance and information density scoring 
produce a better compression in section 3.3. 
50
3.2 Event-based Word Significance 
Based on our observations, a human-compressed 
sentence primarily describes an event or a set of 
relevant events and contains a large proportion of 
named entities, especially in the news article 
domain. Similar to event-based summarization 
(Li et al 2006), we consider only the event 
terms, namely verbs and nouns, with a prefe-
rence for proper nouns. 
The word significance score Ij(wi) indicates 
how important a word wi is to a document j. It is 
a tf-idf weighting scheme with additional weight 
for proper nouns:  
 
?
?
?
?
?
??
?
?
otherwise
nounproperiswifidftf
nouncommonorverbiswifidftf
wI iiij
iiij
ij
,0
,
,
)( ?
 (1) 
where 
wi : a word in the sentence of document j 
tfij :  term frequency of wi in document j 
idfi : inverse document frequency of wi  
? : additional weight for proper noun. 
 
The nodes in the parse tree are then weighted 
by the word significance for leaves or the sum of 
the children?s weights for internal nodes. The 
weighting depends on the word itself regardless 
of its part-of-speech tags in order to overcome 
some part-of-speech errors. 
3.3 Compression Generation and Selection 
In this module, we first apply a greedy algorithm 
to trim the weighted parse tree to obtain a series 
of candidate compressions. Recall that the heu-
ristics Set 0 and 2 have provided the removabili-
ty judgment for each node in the tree. The parse 
tree trimming algorithm is as follows:  
1) remove one node with the lowest weight 
and get a candidate compressed sentence 
2) update  the weights of all ancestors of 
the removed node  
3) repeat until no node is removable  
 
The selection among candidate compressions is a 
tradeoff between sentence length and amount of 
information. Inspired by headlines in news ar-
ticles, most of which contain a large proportion 
of named entities, we create an information den-
sity measurement D(sk) for sentence sk to select 
the best compression: 
                 
)(
)(
)(
k
Pw
i
k sL
wI
sD i
?
??                     (2) 
where 
P : the set of words whose significance scores 
are larger  than ? in (1) 
I(wi) : the significance score of word wi 
L(sk) : the length of sentence in characters  
 
Table 1 shows the effectiveness of information 
density to select a proper compression with a 
balance between length and meaningfulness. Ta-
ble 1 lists all candidate compressions in sequence 
generated from the parse tree in Figure 1. The 
words in bold are considered in information den-
sity. The underlined compression is picked as 
final output as ?????????????
?? (The South Korean Hyundai company is a 
buyer of Volvo.), which makes more sense than 
the one produced by heuristics-only approach as 
????????? (The South Korean company 
is a buyer.). In our approach, ???(Hyundai)? 
tagged as adjective and ?????(Volvo?s)? as 
a modifier to buyer are preserved successfully. 
 
D(s) Sentence  
0.254 ?????????????????. 
The South Korean Hyundai Motor Company 
is a potential buyer of Volvo. 
0.288 ???????????????. 
The South Korean Hyundai Motor Company is 
a buyer of Volvo. 
0.332 ?????????????. 
The South Korean Hyundai Company is a buy-
er of Volvo. 
0.282 ???????????. 
The South Korean company is a buyer of Vol-
vo. 
0.209 ?????????. 
The company is a buyer of Volvo. 
0.0 ?????. 
The company is a buyer. 
 
Table 1. Compression generation and selection 
for the sentence in Figure 1 
 
The compression with highest information 
density is chosen as system output. To achieve a 
better compression rate and avoids overly con-
densed sentences (i.e. very short sentences with 
only a proper noun), we further constrain the 
compression to a limited but varying length 
range [min_length, max_length] according to the 
length of the original sentence: 
 
?
?
? ???
?
?
otherwiselengthoriginal
lengthoriginaliflengthorig
max_length
lengthoriginalmin_length
,_
_,_
},_min{
???
? (3) 
 
51
where 
orig_length : the length of original sentence in 
characters 
?,? : fixed lengths in characters 
 
In contrast to a fixed limitation of length, this 
varying length simulates human behavior in 
creating compression and avoid the overcom-
pression caused by the density selection schema. 
4 Experiments 
4.1 Experiment Setup 
Our experiments were designed to evaluate the 
quality of automatic compression. The evaluation 
corpus is 79 documents from Chinese newswires, 
and the first sentence of each news article is 
compressed.  
The compression of the first sentences in the 
Chinese news articles is a comparatively chal-
lenging task. Unlike English, Chinese often con-
nects two or more self-complete sentences to-
gether without any indicating word or punctua-
tion; this is extremely frequent for the first sen-
tence of news text. The average length of the first 
sentences in the 79 documents is 61.5 characters, 
compared to 46.8 characters for the sentences in 
the body of these news articles.  
We compare the compressions generated by 
four different methods: 
? Human [H]: A native Chinese speaker is 
asked to generate a headline-like compres-
sion (must be a complete sentence, not a 
fragment, and need not preserve original 
SVO structure) based on the first sentence 
of each news article. Only word deletion 
operations are allowed. 
? Heuristics [R]: The heuristics-only ap-
proach mentioned in section 2.1.  
? Heuristics + Word Significance [W]: The 
approach combines heuristics and word 
significance. The parameter ? in (1) is set 
to be 1, which is an upper bound of word?s 
tf-idf value throughout the corpus. 
? Heuristics + Word Significance + Length 
Constraints [L]: Compression is con-
strained to a limited but varying length, as 
mentioned in section 2.3. The length pa-
rameters ? and ? in (3) are set roughly to 
be 10 and 20 characters based on our ex-
perience. 
 
4.2 Human Evaluation 
Sentence compression is commonly evaluated 
by human judgment. Following the literature 
(Knight and Marcu, 2002; Dorr et al 2003; 
Clarke and Lapata, 2006; Cohn and Lapata 
2007), we asked three native Chinese speakers to 
rate the grammaticality of compressions using 
the 1 to 5 scale. We find that all three non-
linguist human judges tend to take semantic cor-
rectness into consideration when scoring gram-
maticality.  
We also asked these three judges to give a list 
of keywords from the original sentence before 
seeing compressions, which they would preserve 
if asked to create a headline based on the sen-
tence. Instead of a subjective score, the informa-
tiveness is evaluated by measuring the keyword 
coverage of the target compression on a percen-
tage scale. The three judges give different num-
bers of keywords varying from 3.33 to 6.51 on 
average over the 79 sentences. 
The compression rate is the ratio of the num-
ber of Chinese characters in a compressed sen-
tence to that in its original sentence. 
The experimental results in Table 2 show that 
our automatically generated compressions pre-
serve grammaticality, with an average score of 
about 4 out of 5, because of the use of linguisti-
cally-motivated heuristics.  
 
 Compres-
sion Rate 
Grammat-
icality 
(1 ~ 5) 
Informa-
tiveness 
(0~100%) 
Human 38.5% 4.962 90.7% 
Heuristics 54.1% 4.114 64.9% 
Heu+Sig 52.8% 3.854 68.8% 
Heu+Sig+L 34.3% 3.664 56.1% 
  
Table 2. Mean rating from human evaluation on 
first sentence compression 
 
Event-based word significance and informa-
tion density increase the amount of important 
information by 6% with similar sentence length, 
but decreases the average grammaticality score 
by 6.5%. This is because the method using word 
significance sacrifices grammaticality to reduce 
the linguistic complexity of the heuristics. None-
theless, this method does improve grammaticali-
ty for 16 of the 79 compressed sentences, typi-
cally for those with POS or parsing errors.  
The compression rates of the two basic auto-
matic approaches are around 53%, while it is 
38.5% for manual compression. This is partially 
because our heuristics only trim the parse tree 
52
but do not transform the structure of it, while a 
human may change the grammatical structure, 
remove more linking words and even abbreviate 
some words. The length constraint boosts the 
compression rate of our combined approach by 
35% with a loss of 18.5% in informativeness and 
5% in grammaticality.  
 
Grammaticali-
ty 
(1 ~ 5) 
Number of 
Sentence 
Compres-
sion Rate 
Informa-
tiveness 
(0~100%) 
Heuristics > 4.5 45 64.1% 75.9% 
Heuristics >= 4 62 54.5% 70.6% 
Heu+Sig  > 4.5 35 59.8% 81.8% 
Heu+Sig  >= 4 57 56.7% 75.8% 
 
Table 3. Compressions with good grammar 
 
We further investigate the performance of our 
automatic system by considering only relatively 
grammatical compressions, as shown in Table 3. 
The compressions which receive an average 
score of more than 4.5 are comparatively reada-
ble.  The combined approach generates 35 such 
compressions among a total of 79 sentences, pre-
serving 81.8% important information on average, 
which is quite satisfying since human-generated 
compression only achieves 90.7%.  
The infomativeness score of human-generated 
compression also demonstrates the difficulty of 
this task. We compare our automatically generat-
ed event words list with the keywords picked by 
human judges. 61.8% of human-selected key-
words are included in the event words list, thus 
considered when calculating information signi-
ficance. This fact demonstrates some success but 
also potential room for improving keyword se-
lection. 
4.3 Some Examples 
We illustrate several representative samples of 
our system output in Table 4. In the first example, 
all three automatic compressions are acceptable, 
though different in preserving important infor-
mation. [W] and [L] concisely contain the WHO, 
WHAT, WHOM information of the event, while 
[R] further preserves the WHY and WHEN in-
formation.  
In the second example, the heuristics-only ap-
proach produced a decent compression by keep-
ing only the first self-complete sub-sentence. The 
weight of word ???(White House)? is some-
what overwhelming and resulted in dense com-
pressions in [W] and [L], which are too short to 
be good. Besides, [W] and [L] in this example 
show that not all the prepositional phrases, noun 
modifiers etc. can be removed in Chinese with-
out affecting grammaticality, though in most 
cases the removals are safe. This is one of the 
main reasons for grammar errors in the compres-
sion results except POS and parsing errors.  
The third example shows how the combined 
approach overcomes POS errors and how length 
constraints avoid overcompression. In [R], ???
?(Nadal)? is deleted because it is mistakenly 
tagged as an adverb modifying the action ?claim 
the victory and progress through?. Since Nadal is 
tagged as proper noun somewhere else in the 
document, its significance makes it survive the 
compression process. [L] produces a perfect 
compression with proper length, information and 
grammar, just as human-made compression. [W] 
selects a very condensed version of compression 
but loses some information.  
 
1.  
[O] ?????????????,???????????
???????????. 
Because both sides were immovable on the drawing of maritime 
borders, a three-day high-level military meeting between North 
and South Korea broke up in discord today. 
[H]??????????????. 
A high-level military meeting between two Koreas broke up in 
discord today. 
[R]??????,??????????????????. 
Because both sides were immovable, a three-day high-level 
meeting between two Koreas broke up in discord today. 
[L]??????????. 
A high-level meeting between two Koreas broke up in discord. 
[W]??????????. 
A high-level meeting between two Koreas broke up in discord. 
2.  
[O]??????????????,??????????
???;??????????????????????
??,??????. 
The White House today called for nuclear inspectors to be sent 
as soon as possible to monitor North Korea?s closure of its nuc-
lear reactors. The White House made this call after US President 
Bush had telephone conversations with South Korean President 
Roh Moo-hyun. 
[H] ????????????????????. 
The White House today called for inspectors to be sent to moni-
tor North Korea?s closure of its nuclear reactors. 
[R]??????????,??????????. 
 The White House today called for inspectors to be sent to moni-
tor North Korea?s closure of its reactors. 
[L]??????????, ???, ????. 
The White House today called for inspectors to be sent. The 
White House is,  made this call. 
[W]???,????. 
The White House is, made this call. 
3.  
[O]??????????,??????,???,????
?????????????. 
Fourth seed Djokovic withdrew from the game, and allowed 
second seed Nadal , who was leading 3-6 , 6-1 , 4-1 , to claim the 
victory and progress through. 
[H]??????????????. 
Djokovic withdrew from the game, and allowed Nadal to claim 
the victory and progress through. 
53
[R]??????,?????,???,?????????
?????. 
Djokovic withdrew from the game, and allowed second seed, 
who was leading 3-6 , 6-1 , 4-1 , to claim the victory and 
progress through. 
 [L]????????????????. 
Djokovic withdrew from the game, and allowed seed Nadal to 
claim the victory and progress through. 
[W]??????. 
Djokovic withdrew from the game. 
4.  
[O]??? 7 ? 31 ????? 30 ???????????
??????????. 
Chinanews.com , July 31 On the 30th Chen Shui-bian questioned 
that members of the judiciary on the island may have tried to get 
involved in elections for leaders in the Taiwan region.  
[H]???????????????????. 
Chen Shui-bian questioned that members of the judiciary may 
get involved in elections for leaders in the Taiwan region. 
[R]??? 7 ? 31 ????? 30 ???????????
????????. 
Chinanews.com , July 31 On the 30th Chen Shui-bian questioned 
that members on the island may have tried to get involved in 
elections for leaders in the Taiwan region.  
[L]??? 30?????????????????. 
On the 30th Chen Shui-bian questioned that members may have 
tried to get involved in elections for leaders in the Taiwan re-
gion. 
[W]??? 30 ?????????????????. 
On the 30th Chen Shui-bian questioned that members may have 
tried to get involved in elections for leaders in the Taiwan re-
gion. 
5.  
[O]??????????????????,?????,
????????????????. 
Patil is India?s first woman presidential candidate, if she is 
elected, she will become India?s first woman president in history. 
[H] ??????????????????. 
Patil is India?s first woman presidential candidate. 
[R]??????????????. 
Patil is the first candidate in the history of India. 
[L]???????,?????????????. 
Patil is the candidate, she will become president of Indian histo-
ry. 
[W]???????. 
Patil is the candidate. 
 
Table 4. Compression examples including human 
and system results, with reference translation 
 (O: Original sentence) 
 
The fourth sample indicates an interesting lin-
guistic phenomenon. The head of the noun 
phrase ???????(members of the judiciary 
on the island)?, ???(members)? cannot stand 
alone making a fluent and valid sentence, though 
all the compressions are grammatically correct. 
Our human assessors also show a preference of 
[R] to [L, W] in grammaticality evaluation, tak-
ing semantic correctness into consideration as 
well. This is probably a reason that our combined 
approach performs worse than heuristic-only ap-
proach in grammaticality. The combined ap-
proach tends to remove risky constituents, but it 
is hard for word significance to control this risk 
properly in every case. This is another of the 
main reasons for bad compression.  
In the fifth sample, all the automatic compres-
sions are grammatically correct preserving well 
the heads of subject and object, but are semanti-
cally incorrect. This case should be hard to han-
dle by any compression approach. 
5 Conclusions and Future Work 
In this paper, we propose a novel approach to 
combine linguistically-motivated heuristics and 
word significance scoring for Chinese sentence 
compression. We take advantage of heuristics to 
preserve grammaticality and not rely on a paral-
lel corpus. We reduce the complexity involved in  
preparing complicated deterministic rules for 
constituent deletion, requiring people only to 
determine potentially removable constituents. 
Therefore, this approach can be easily extended 
to languages or domains for which parallel com-
pression corpora are scarce. The word signific-
ance scoring is used to control the word deletion 
process, pursuing a balance between sentence 
length and information loss. The exploitation of 
event information improves the mechanical rule-
based approach in preserving event-related 
words and overcomes some POS and parsing 
errors.  
The experimental results prove that this com-
bined approach is competitive with a finely-
tuned heuristics-only approach to grammaticality, 
and includes more important information in the 
compressions of the same length.  
In the future, we plan to apply the compres-
sion to Chinese summarization and headline gen-
eration tasks. A careful study on keyword selec-
tion and word weighting may further improve the 
performance of the current system. We also con-
sider incorporating language models to produce 
fluent and natural compression and reduce se-
mantically invalid cases.   
Another important future direction lies in 
creating a parallel compression corpus in Chi-
nese and exploiting statistical and machine learn-
ing techniques. We also expect that an abstrac-
tive approach involving paraphrasing operations 
besides word deletion will create more natural 
compression than an extractive approach.  
 
Acknowledgments 
This work was supported in part by the Defense 
Advanced Research Projects Agency (DARPA) 
under Contract HR0011-06-C-0023.  Any opi-
nions, findings, conclusions, or recommenda-
54
tions expressed in this material are the authors' 
and do not necessarily reflect those of the U.S. 
Government. 
References  
J. Clarke and M. Lapata, 2006. Models for Sentence 
Compression: A Comparison across Domains, 
Training Requirements and Evaluation Measures. 
In Proceedings of the COLING/ACL 2006, Syd-
ney, Australia, pp. 377-384. 
T. Cohn and M. Lapata. 2007. Large Margin Syn-
chronous generation and its application to sentence 
compression. In the Proceedings of the EMNLP/ 
CoNLL 2007, Pragure, Czech Republic, pp. 73-82. 
B. Dorr, D. Zajic and R. Schwartz. 2003. Hedge 
Trimmer: A Parse-and-Trim Approach to Headline 
Generation. In the Proceedings of the 
NAACL/HLT text summarization workshop, Ed-
monton, Canada, pp. 1-8.  
M. Galley and K. McKeown, 2007. Lexicalized Mar-
kov Grammars for Sentence Compression. In the 
Proceedings of NAACL/HLT 2007, Rochester, 
NY, pp. 180-187. 
C. Hori and S. Furui. 2004. Speech Summarization:  
An Approach through Word Extraction and a Me-
thod for Evaluation. IEICE Transactions on Infor-
mation and Systems, E87-D(1): 15-25. 
Z. Huang and M. Harper, 2009. Self-training PCFG 
Grammars with Latent Annotations Across Lan-
guages. In the proceedings of EMNLP 2009, Sin-
gapore.  
H. Jing. 2000. Sentence Reduction for Automatic 
Text Summarization. In Proceedings of the 6th 
ANLP, Seattle, WA, pp. 310-315.  
K. Knight and D. Marcu, 2002. Summarization 
beyond Sentence Extraction: a Probabilistic Ap-
proach to Sentence Compression. Artificial Intelli-
gence, 139(1): 91-107. 
W. Li, W. Xu, M. Wu, C. Yuan and Q. Lu. 2006. Ex-
tractive Summarization using Inter- and Intra- 
Event Relevance. In the Proceedings of COL-
ING/ACL 2006, Sydney, Australia, pp 369-376. 
R. McDonald. 2006. Discriminative Sentence Com-
pression with Soft Syntactic Constraints. In the 
Proceedings of 11th EACL, Trento, Italy, pp. 297-
304. 
M. L. Nguyen, A. Shimazu, S. Horiguchi, T. B. Ho 
and M. Fukushi. 2004. Probabilistic Sentence Re-
duction using Support Vector Machines. In Pro-
ceedings of the 20th COLING, Geneva, Switzer-
land, pp. 743-749.  
K. McKeown, R. Barzilay, S. Blair-Goldensohn, D. 
Evans, V. Hatzivassiloglou, J. Klavans, A. Nenko-
va, B. Schiffman and S. Sigelman. 2002. The Co-
lumbia Multi-Document Summarizer for DUC 
2002. In the Proceedings of the ACL workshop on 
Document Understanding Conference (DUC) 
workshop, Philadelphia, PA, pp. 1-8.  
K. Parton, K. McKeown, R. Coyne, M. Diab, R. 
Grishman, D. Hakkani-T?r, M. Harper, H. Ji, W. 
Ma, A. Meyers, S. Stolbach, A. Sun, G. Tur, W. 
Xu and S. Yaman. 2009. Who, What, When, 
Where, Why? Comparing Multiple Approaches to 
the Cross-Lingual 5W Task. In the Proceedings of 
ACL-IJCNLP, Singapore. 
J. Turner and E. Charniak. 2005. Supervised and Un-
supervised Learning for Sentence Compression. In 
the Proceedings of 43rd ACL, Ann Arbor, MI, pp. 
290-297. 
 
 
55
Proceedings of the Third Linguistic Annotation Workshop, ACL-IJCNLP 2009, pages 116?120,
Suntec, Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Transducing Logical Relations from Automatic and Manual GLARF
Adam Meyers?, Michiko Kosaka?, Heng Ji?, Nianwen Xue?,
Mary Harper?, Ang Sun?, Wei Xu? and Shasha Liao?
? New York Univ., ?Monmouth Univ., ?Brandeis Univ,, ?City Univ. of New York, ?Johns
Hopkins Human Lang. Tech. Ctr. of Excellence & U. of Maryland, College Park
Abstract
GLARF relations are generated from tree-
bank and parses for English, Chinese and
Japanese. Our evaluation of system out-
put for these input types requires consid-
eration of multiple correct answers.1
1 Introduction
Systems, such as treebank-based parsers (Char-
niak, 2001; Collins, 1999) and semantic role la-
belers (Gildea and Jurafsky, 2002; Xue, 2008), are
trained and tested on hand-annotated data. Evalu-
ation is based on differences between system out-
put and test data. Other systems use these pro-
grams to perform tasks unrelated to the original
annotation. For example, participating systems in
CONLL (Surdeanu et al, 2008; Hajic? et al, 2009),
ACE and GALE tasks merged the results of sev-
eral processors (parsers, named entity recognizers,
etc.) not initially designed for the task at hand.
This paper discusses differences between hand-
annotated data and automatically generated data
with respect to our GLARFers, systems for gen-
erating Grammatical and Logical Representation
Framework (GLARF) for English, Chinese and
Japanese sentences. The paper describes GLARF
(Meyers et al, 2001; Meyers et al, 2009) and
GLARFers and compares GLARF produced from
treebank and parses.
2 GLARF
Figure 1 includes simplified GLARF analyses for
English, Chinese and Japanese sentences. For
each sentence, a GLARFer constructs both a Fea-
ture Structure (FS) representing a constituency
analysis and a set of 31-tuples, each representing
1Support includes: NSF IIS-0534700 & IIS-0534325
Structure Alignment-based MT; DARPA HR0011-06-C-
0023 & HR0011-06-C-0023; CUNY REP & GRTI Program.
This work does not necessarily reflect views of sponsors.
up to three dependency relations between pairs of
words. Due to space limitations, we will focus on
the 6 fields of the 31-tuple represented in Figure 1.
These include: (1) a functor (func); (2) the de-
pending argument (Arg); (3) a surface (Surf) la-
bel based on the position in the parse tree with no
regularizations; (4) a logic1 label (L
?
1) for a re-
lation that reflects grammar-based regularizations
of the surface level. This marks relations for fill-
ing gaps in relative clauses or missing infinitival
subjects, represents passives as paraphrases as ac-
tives, etc. While the general framework supports
many regularizations, the relations actually repre-
sented depends on the implemented grammar, e.g.,
our current grammar of English regularizes across
passives and relative clauses, but our grammars
of Japanese and Chinese do not currently.; (5) a
logic2 label (L2) for Chinese and English, which
represents PropBank, NomBank and Penn Dis-
course Treebank relations; and (6) Asterisks (*)
indicate transparent relations, relations where the
functor inherits semantic properties of certain spe-
cial arguments (*CONJ, *OBJ, *PRD, *COMP).
Figure 1 contains several transparent relations.
The interpretation of the *CONJ relations in the
Japanese example, include not only that the nouns
[zaisan] (assets) and [seimei] (lives) are con-
joined, but also that these two nouns, together
form the object of the Japanese verb [mamoru]
(protect). Thus, for example, semantic selection
patterns should treat these nouns as possible ob-
jects for this verb. Transparent relations may serve
to neutralize some of the problematic cases of at-
tachment ambiguity. For example, in the English
sentence A number of phrases with modifiers are
not ambiguous, there is a transparent *COMP re-
lation between numbers and of and a transpar-
ent *OBJ relation between of and phrases. Thus,
high attachment of the PP with modifiers, would
have the same interpretation as low attachment
since phrases is the underlying head of number of
116
Figure 1: GLARF 5-tuples for 3 languages
phrases. In this same example, the adverb not can
be attached to either the copula are or the pred-
icative adjective, with no discernible difference in
meaning?this factor is indicated by the transparent
designation of the relations where the copula is a
functor. Transparent features also provide us with
a simple way of handling certain function words,
such as the Chinese word De which inherits the
function of its underlying head, connecting a vari-
ety of such modifiers to head nouns (an adjective
in the Chinese example.). For conjunction cases,
the number of underlying relations would multi-
ply, e.g., Mary and John bought and sold stock
would (underlyingly) have four subject relations
derived by pairing each of the underlying subject
nouns Mary and John with each of the underlying
main predicate verbs bought and sold.
3 Automatic vs. Manual Annotation
Apart from accuracy, there are several other ways
that automatic and manual annotation differs. For
Penn-treebank (PTB) parsing, for example, most
parsers (not all) leave out function tags and empty
categories. Consistency is an important goal for
manual annotation for many reasons including: (1)
in the absence of a clear correct answer, consis-
tency helps clarify measures of annotation quality
(inter-annotator agreement scores); and (2) consis-
tent annotation is better training data for machine
learning. Thus, annotation specifications use de-
faults to ensure the consistent handling of spurious
ambiguity. For example, given a sentence like I
bought three acres of land in California, the PP in
California can be attached to either acres or land
with no difference in meaning. While annotation
guidelines may direct a human annotator to prefer,
for example, high attachment, systems output may
have other preferences, e.g., the probability that
land is modified by a PP (headed by in) versus the
probability that acres can be so modified.
Even if the manual annotation for a particular
corpus is consistent when it comes to other factors
such as tokenization or part of speech, developers
of parsers sometimes change these guidelines to
suit their needs. For example, users of the Char-
niak parser (Charniak, 2001) should add the AUX
category to the PTB parts of speech and adjust
their systems to account for the conversion of the
word ain?t into the tokens IS and n?t. Similarly, to-
kenization decisions with respect to hyphens vary
among different versions of the Penn Treebank, as
well as different parsers based on these treebanks.
Thus if a system uses multiple parsers, such differ-
ences must be accounted for. Differences that are
not important for a particular application should
be ignored (e.g., by merging alternative analyses).
For example, in the case of spurious attachment
ambiguity, a system may need to either accept both
as right answers or derive a common representa-
tion for both. Of course, many of the particular
problems that result from spurious ambiguity can
be accounted for in hind sight. Nevertheless, it
is precisely this lack of a controlled environment
which adds elements of spurious ambiguity. Us-
ing new processors or training on new treebanks
can bring new instances of spurious ambiguity.
4 Experiments and Evaluation
We ran GLARFers on both manually created tree-
banks and automatically produced parses for En-
glish, Chinese and Japanese. For each corpus, we
created one or more answer keys by correcting
117
system output. For this paper, we evaluate solely
on the logic1 relations (the second column in fig-
ure 1.) Figure 2 lists our results for all three lan-
guages, based on treebank and parser input.
As in (Meyers et al, 2009), we generated 4-
tuples consisting of the following for each depen-
dency: (A) the logic1 label (SBJ, OBJ, etc.), (B)
its transparency (True or False), (C) The functor (a
single word or a named entity); and (D) the argu-
ment (a single word or a named entity). In the case
of conjunction where there was no lexical con-
junction word, we used either punctuation (com-
mas or semi-colons) or the placeholder *NULL*.
We then corrected these results by hand to produce
the answer key?an answer was correct if all four
members of the tuple were correct and incorrect
otherwise. Table 2 provides the Precision, Recall
and F-scores for our output. The F-T columns
indicates a modified F-score derived by ignoring
the +/-Transparent distinction (resulting changes
in precision, recall and F-score are the same).
For English and Japanese, an expert native
speaking linguist corrected the output. For Chi-
nese, several native speaking computational lin-
guists shared the task. By checking compatibil-
ity of the answer keys with outputs derived from
different sources (parser, treebank), we could de-
tect errors and inconsistencies. We processed the
following corpora. English: 86 sentence article
(wsj 2300) from the Wall Street Journal PTB test
corpus (WSJ); 46 sentence letter from Good Will
(LET), the first 100 sentences of a switchboard
telephone transcript (TEL) and the first 100 sen-
tences of a narrative from the Charlotte Narra-
tive and Conversation (NAR). These samples are
taken from the PTB WSJ Corpus and the SIGANN
shared subcorpus of the OANC. The filenames are:
110CYL067, NapierDianne and sw2014. Chi-
nese: a 20 sentence sample of text from the
Penn Chinese Treebank (CTB) (Xue et al, 2005).
Japanese: 20 sentences from the Kyoto Corpus
(KYO) (Kurohashi and Nagao, 1998)
5 Running the GLARFer Programs
We use Charniak, UMD and KNP parsers (Char-
niak, 2001; Huang and Harper, 2009; Kurohashi
and Nagao, 1998), JET Named Entity tagger (Gr-
ishman et al, 2005; Ji and Grishman, 2006)
and other resources in conjunction with language-
specific GLARFers that incorporate hand-written
rules to convert output of these processors into
a final representation, including logic1 struc-
ture, the focus of this paper. English GLAR-
Fer rules use Comlex (Macleod et al, 1998a)
and the various NomBank lexicons (http://
nlp.cs.nyu.edu/meyers/nombank/) for
lexical lookup. The GLARF rules implemented
vary by language as follows. English: cor-
recting/standardizing phrase boundaries and part
of speech (POS); recognizing multiword expres-
sions; marking subconstituents; labeling rela-
tions; incorporating NEs; regularizing infiniti-
val, passives, relatives, VP deletion, predica-
tive and numerous other constructions. Chi-
nese: correcting/standardizing phrase boundaries
and POS, marking subconstituents, labeling rela-
tions; regularizing copula constructions; incorpo-
rating NEs; recognizing dates and number expres-
sions. Japanese: converting to PTB format; cor-
recting/standardizing phrase boundaries and POS;
labeling relations; processing NEs, double quote
constructions, number phrases, common idioms,
light verbs and copula constructions.
6 Discussion
Naturally, the treebank-based system out-
performed parse-based system. The Charniak
parser for English was trained on the Wall Street
Journal corpus and can achieve about 90% accu-
racy on similar corpora, but lower accuracy on
other genres. Differences between treebank and
parser results for English were higher for LET and
NAR genres than for the TEL because the system
is not currently designed to handle TEL-specific
features like disfluencies. All processors were
trained on or initially designed for news corpora.
Thus corpora out of this domain usually produce
lower results. LET was easier as it consisted
mainly of short simple sentences. In (Meyers et
al., 2009), we evaluated our results on 40 Japanese
sentences from the JENAAD corpus (Utiyama
and Isahara, 2003) and achieved a higher F-score
(90.6%) relative to the Kyoto corpus, as JENAAD
tends to have fewer long complex sentences.
By using our answer key for multiple inputs, we
discovered errors and consequently improved the
quality of the answer keys. However, at times we
were also compelled to fork the answer keys?given
multiple correct answers, we needed to allow dif-
ferent answer keys corresponding to different in-
puts. For English, these items represent approxi-
mately 2% of the answer keys (there were a total
118
Treebank Parser
ID % Prec % Rec F F-T % Prec % Rec F F-T
WSJ 12381491 = 83.0
1238
1471 = 84.2 83.6 87.1
1164
1452 = 80.2
1164
1475 = 78.9 79.5 81.8
LET 419451 = 92.9
419
454 = 92.3 92.6 93.3
390
434 = 89.9
390
454 = 85.9 87.8 87.8
TEL 478627 = 76.2
478
589 = 81.2 78.6 82.2
439
587 = 74.8
439
589 = 74.5 74.7 77.4
NAR 8171013 = 80.7
817
973 =84.0 82.3 84.1
724
957 = 75.7
724
969 = 74.7 75.2 76.1
CTB 351400 = 87.8
351
394 = 89.1 88.4 88.7
352
403 = 87.3
352
438 = 80.4 83.7 83.7
KYO 525575 = 91.3
525
577 = 91.0 91.1 91.1
493
581 = 84.9
493
572 = 86.2 85.5 87.8
Figure 2: Logic1 Scores
Figure 3: Examples of Answer Key Divergences
of 74 4-tuples out of a total of 3487). Figure 3 lists
examples of answer key divergences that we have
found: (1) alternative tokenizations; (2) spurious
differences in attachment and conjunction scope;
and (3) ambiguities specific to our framework.
Examples 1 and 2 reflect different treatments of
hyphenation and contractions in treebank specifi-
cations over time. Parsers trained on different tree-
banks will either keep hyphenated words together
or separate more words at hyphens. The Treebank
treatment of can?t regularizes so that (can need
not be differentiated from ca), whereas the parser
treatment makes maintaining character offsets eas-
ier. In example 3, the Japanese parser recognizes
a single word whereas the treebank divides it into
a prefix plus stem. Example 4 is a case of differ-
ences in character encoding (zero).
Example 5 is a common case of spurious attach-
ment ambiguity for English, where a transparent
noun takes an of PP complement?nouns such as
form, variety and thousands bear the feature trans-
parent in the NOMLEX-PLUS dictionary (a Nom-
Bank dictionary based on NOMLEX (Macleod et
al., 1998b)). The relative clause attaches either
to the noun thousands or people and, therefore,
the subject gap of the relative is filled by either
thousands or people. This ambiguity is spurious
since there is no meaningful distinction between
these two attachments. Example 6 is a case of
attachment ambiguity due to a support construc-
tion (Meyers et al, 2004). The recipient of the
gift will be Goodwill regardless of whether the
PP is attached to give or gift. Thus there is not
much sense in marking one attachment more cor-
rect than the other. Example 7 is a case of conjunc-
tion ambiguity?the context does not make it clear
whether or not the pearls are part of a necklace or
just the beads are. The distinction is of little con-
sequence to the understanding of the narrative.
Example 8 is a case in which our grammar han-
dles a case ambiguously: the prenominal adjective
can be analyzed either as a simple noun plus ad-
jective phrase meaning various businesses or as a
noun plus relative clause meaning businesses that
are varied. Example 9 is a common case in Chi-
nese where the verb/noun distinction, while un-
clear, is not crucial to the meaning of the phrase ?
under either interpretation, 5 billion was exported.
7 Concluding Remarks
We have discussed challenges of automatic an-
notation when transducers of other annotation
schemata are used as input. Models underly-
ing different transducers approximate the origi-
nal annotation in different ways, as do transduc-
ers trained on different corpora. We have found
it necessary to allow for multiple correct answers,
due to such differences, as well as, genuine and
spurious ambiguities. In the future, we intend to
investigate automatic ways of identifying and han-
dling spurious ambiguities which are predictable,
including examples like 5,6 and 7 in figure 3 in-
volving transparent functors.
119
References
E. Charniak. 2001. Immediate-head parsing for lan-
guage models. In ACL 2001, pages 116?123.
M. Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, University
of Pennsylvania.
D. Gildea and D. Jurafsky. 2002. Automatic Label-
ing of Semantic Roles. Computational Linguistics,
28:245?288.
R. Grishman, D. Westbrook, and A. Meyers. 2005.
Nyu?s english ace 2005 system description. In ACE
2005 Evaluation Workshop.
J. Hajic?, M. Ciaramita, R. Johansson, D. Kawahara,
M. A. Mart??, L. Ma`rquez, A. Meyers, J. Nivre,
S. Pado?, J. ?Ste?pa?nek, P. Stran?a?k, M. Surdeanu,
N. Xue, and Y. Zhang. 2009. The CoNLL-2009
shared task: Syntactic and semantic dependencies in
multiple languages. In CoNLL-2009, Boulder, Col-
orado, USA.
Z. Huang and M. Harper. 2009. Self-training PCFG
Grammars with Latent Annotations across Lan-
guages. In EMNLP 2009.
H. Ji and R. Grishman. 2006. Analysis and Repair of
Name Tagger Errors. In COLING/ACL 2006, Syd-
ney, Australia.
S. Kurohashi and M. Nagao. 1998. Building a
Japanese parsed corpus while improving the pars-
ing system. In Proceedings of The 1st International
Conference on Language Resources & Evaluation,
pages 719?724.
C. Macleod, R. Grishman, and A. Meyers. 1998a.
COMLEX Syntax. Computers and the Humanities,
31:459?481.
C. Macleod, R. Grishman, A. Meyers, L. Barrett, and
R. Reeves. 1998b. Nomlex: A lexicon of nominal-
izations. In Proceedings of Euralex98.
A. Meyers, M. Kosaka, S. Sekine, R. Grishman, and
S. Zhao. 2001. Parsing and GLARFing. In Pro-
ceedings of RANLP-2001, Tzigov Chark, Bulgaria.
A. Meyers, R. Reeves, and Catherine Macleod. 2004.
NP-External Arguments: A Study of Argument
Sharing in English. In The ACL 2004 Workshop
on Multiword Expressions: Integrating Processing,
Barcelona, Spain.
A. Meyers, M. Kosaka, N. Xue, H. Ji, A. Sun, S. Liao,
and W. Xu. 2009. Automatic Recognition of Log-
ical Relations for English, Chinese and Japanese in
the GLARF Framework. In SEW-2009 at NAACL-
HLT-2009.
M. Surdeanu, R. Johansson, A. Meyers, L. Ma?rquez,
and J. Nivre. 2008. The CoNLL-2008 Shared Task
on Joint Parsing of Syntactic and Semantic Depen-
dencies. In Proceedings of the CoNLL-2008 Shared
Task, Manchester, GB.
M. Utiyama and H. Isahara. 2003. Reliable Mea-
sures for Aligning Japanese-English News Articles
and Sentences. In ACL-2003, pages 72?79.
N. Xue, F. Xia, F. Chiou, and M. Palmer. 2005. The
Penn Chinese Treebank: Phrase Structure Annota-
tion of a Large Corpus. Natural Language Engi-
neering.
N. Xue. 2008. Labeling Chinese Predicates with Se-
mantic roles. Computational Linguistics, 34:225?
255.
120
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1291?1300,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Exploiting Syntactic and Distributional Information
for Spelling Correction with Web-Scale N-gram Models
Wei Xuc,?Joel Tetreaulta Martin Chodorowb Ralph Grishmanc Le Zhaod
aEducational Testing Service, Princeton, NJ, USA
jtetreault@ets.org
bHunter College of CUNY, New York, NY, USA
martin.chodorow@hunter.cuny.edu
cNew York University, NY, USA
{xuwei,grishman}@cs.nyu.edu
dCarnegie Mellon University, Pittsburgh, PA, USA
lezhao@cs.cmu.edu
Abstract
We propose a novel way of incorporating de-
pendency parse and word co-occurrence in-
formation into a state-of-the-art web-scale n-
gram model for spelling correction. The syn-
tactic and distributional information provides
extra evidence in addition to that provided by a
web-scale n-gram corpus and especially helps
with data sparsity problems. Experimental
results show that introducing syntactic fea-
tures into n-gram based models significantly
reduces errors by up to 12.4% over the current
state-of-the-art. The word co-occurrence in-
formation shows potential but only improves
overall accuracy slightly.
1 Introduction
The function of context-sensitive text correction is
to identify word-choice errors in text (Bergsma et
al., 2009). It can be viewed as a lexical disambigua-
tion task (Lapata and Keller, 2005), where a system
selects from a predefined confusion word set, such
as {affect, effect} or {complement, compliment},
and provides the most appropriate word choice given
the context. Typically, one determines if a word has
been used correctly based on lexical, syntactic and
semantic information from the context of the word.
One of the top performing models of spelling cor-
rection (Bergsma et al, 2010) is based on web-scale
n-gram counts, which reflect both syntax and mean-
ing. However, even with a large-scale n-gram cor-
pus, data sparsity can hurt performance in two ways.
?This work was done when the first author was an intern
for Educational Testing Service.
First, n-gram based methods require exact word and
order matches. If there is a low frequency word in
the context, such as a person?s name, there will be
little, if any, evidence in the n-gram data to sup-
port the usage. Second, if the target confusable word
is rare, there will not be enough n-gram support or
training data to render a confident decision. Because
of the data sparsity problem, language modeling is
not always sufficient to capture the meaning of the
sentence and the correct usage of the word.
Take a sentence from The New York Times
(NYT) for example: ??This fellow?s won a war,? the
dean of the capital?s press corps, David Broder, an-
nounced on ?Meet the Press? after complimenting
the president on the ?great sense of authority and
command? he exhibited in a flight suit.? Unfortu-
nately, neither the phrase ?complementing the pres-
ident? nor ?complimenting the president? exists in
the web-scale Google N-gram corpus (Brants and
Franz, 2006). The n-gram models decide solely
based on the frequency of the bi-grams ?after com-
ple(i)menting? and ?comple(i)menting the?, which
are common usages for both words. The real ques-
tion is whether we are more likely to ?compliment?
or ?complement? a person, the ?president?. Several
clues could help us answer that question. A de-
pendency parser can identify the word ?president?
as the subject of ?compliment? or ?complement?
which also may be the case in some of the train-
ing data. Lexical co-occurrence (Edmonds, 1997)
and semantic word relatedness measurements, such
as Random Indexing (Sahlgren, 2006), could pro-
vide evidence that ?compliment? is more likely to
co-occur with ?president? than ?complement?. Fur-
1291
thermore, some important clues can be quite distant
from the target word, e.g. outside the 9-word context
window Bergsma et al (2010) and Carlson (2007)
used. Consider another sentence in the NYT corpus,
?GM says the addition of OnStar, which includes a
system that automatically notifies an OnStar opera-
tor if the vehicle is involved in a collision, comple-
ments the Vue?s top five-star safety rating for the
driver and front passenger in both front- and side-
impact crash tests.? The dependency parser finds the
object of ?complement? is ?rating?, which is outside
the 9-word window.
We propose enhancing state-of-the-art web-scale
n-gram models for spelling correction with syntac-
tic structures and distributional information. For our
work, we build on a baseline system that combines
n-gram and lexical features (Bergsma et al, 2010).
Specifically, this paper makes the following contri-
butions:
1. We show that the baseline system can be
improved by augmenting it with dependency
parse features.
2. We show that the impact of parse features can
be further augmented when combined with dis-
tributional information, specifically word co-
occurrence information.
In the following section, we describe related
work and how our approach differs from these ap-
proaches. In Sections 3 and 4, we discuss our meth-
ods for using parse features and word co-occurrence
information. In Section 5, we present experimental
results and analysis.
2 Related Work
A variety of approaches have been proposed for
context-sensitive spelling correction ranging from
semantic methods to machine learning classifiers to
large-scale n-gram models.
Some semantics-based systems have been devel-
oped based on an intuitive assumption that the in-
tended word is more likely to be semantically coher-
ent with the context than is a spelling error. Jones
and Martin (1997) made use of the semantic simi-
larity produced by Latent Semantic Analysis. Bu-
danitsky and Hirst (2001) investigated the effective-
ness of predicting words based on different semantic
similarity/distance measures in WordNet. Both sys-
tems report performance that is lower than systems
developed more recently.
A variety of machine-learning methods have been
proposed in spelling correction and preposition and
article error correction fields, such as Bayesian clas-
sifiers (Golding, 1995; Golding and Roth, 1996),
Winnow-based learning (Golding and Roth, 1999),
decision lists (Golding, 1995), transformation-based
learning (Mangu and Brill, 1997), augmented mix-
ture models (Cucerzan and Yarowsky, 2002) and
maximum entropy classifiers (Izumi et al, 2003;
Han et al, 2006; Chodorow et al, 2007; Tetreault
and Chodorow, 2008; Felice and Pulman, 2008).
Despite their differences, these approaches mainly
use contextual features to capture the lexical, seman-
tic and/or syntactic environment of the target word.
The use of distributional similarity measures for
spelling correction has been previously explored in
(Mohammad and Hist, 2006). In our work, distribu-
tional similarity is not the primary contribution but
we show the impact it can have when used in con-
junction with a large scale n-gram model and with
parse features, which allows the system to select
words outside the local window for distributional
similarity. In the prior work, the words for distri-
butional similarity are constrained to the local win-
dow, and positional information of the words is not
encoded.
Recent work (Carlson and Fette, 2007; Gamon
et al, 2008; Bergsma et al, 2009) has demon-
strated that large-scale language modeling is ex-
tremely helpful for contextual spelling correction
and other lexical disambiguation tasks. These sys-
tems make the word choice depending on how fre-
quently each candidate word has been seen in the
given context in web-scale data. As n-gram data has
become more readily available, such as the Google
N-gram Corpus, the likelihood of a word being used
in a certain context can be better estimated.
Bergsma et al (2009; 2010) presented a series
of simple but powerful models which relied heavily
on web-scale n-gram counts. From the Google Web
N-gram Corpus, they retrieve counts of n-grams of
different sizes (2-5) and positions that span the tar-
get word w0 within a window of 9 words. For
example, for the following sentence: ?The system
tried to decide {among, between} the two confus-
1292
able words.?, the method would extract the five 5-
gram patterns, shown below in Figure 2, where w0
can be either word in the confusion set {among, be-
tween} in this particular example. Similarly, there
are four 4-grams, three 3-grams, and two 2-grams,
in total, 14 n-grams for each of the words in the con-
fusion set.
system tried to decide w0
tried to decide w0 the
to decide w0 the two
decide w0 the two confusable
w0 the two confusable words
We briefly describe three of Bergsma et al?s
(2009; 2010) best systems below, which are reported
to achieve state-of-the-art accuracy (NG = n-gram;
LEX = lexical).
1. sumLM: For each candidate word, (Bergsma
et al, 2009) sum the log-counts of all 14 pat-
terns filled with the candidate, and choose the
candidate with the highest total.
2. NG: Bergsma et al (2009) exploit each can-
didate?s 14 log-counts of n-gram patterns as
features in a Support Vector Machine (SVM)
model.
3. NG+LEX: Bergsma et al (2010) augment the
NG model with lexical features (described in
detail in Section 3.1).
Bergsma et al (2009; 2010) restricted their exper-
iments to only five confusion sets where the reported
performance in (Golding and Roth, 1999) was below
90%: {among, between}, {amount, number}, {cite,
sight, site}, {peace, piece} and {raise, rise}. They
reported that the SVM model with NG features out-
performed its unsupervised version, sumLM. How-
ever, the limited confusion word sets they evaluated
may not comprehensively represent the word usage
errors that writers typically make. In this paper, we
test nine additional commonly confused word pairs
to expand the scope of the evaluation. These words
were selected based on their lower frequencies com-
pared to the five pairs in the above work (as shown
later in Table 2).
3 Enhanced N-gram Models with Parse
Features
To our knowledge, only (Elmi and Evans, 1998)
have used parsing for spell correction. They focus
on using a parser as a filter to discriminate between
possible real-world corrections where the part-of-
speech differs. In our work, we show that parse fea-
tures are effective when used directly in the classifi-
cation mode (as opposed to as a final filter) to select
the best correction regardless of whether or not the
part-of-speech of the choices differ.
Statistical parsers have also seen limited use in
the sister tasks of preposition and article error detec-
tion (Hermet et al, 2008; Lee and Knutsson, 2008;
Felice and Pulman, 2009; Tetreault et al, 2010)
and verb sense disambiguation (Dligach and Palmer,
2008). In those instances where parsers have been
used, they have mainly provided shallow analyses
or relations involving specific target words, such as
a preposition or verb. Unlike preposition errors,
spelling errors can occur in any word.
In this paper, we propose a novel way to incor-
porate the parse into spelling correction, applying
the parser to sentences filled by each candidate word
equivalently and extracting salient features. This
overcomes two problem in the existing methods: 1)
the parse trees of the same sentence filled by differ-
ent confusion words can be different. However, in
the test phase, we do not know which word should
be put in the sentences to create parse features for
test examples. Previous studies (Tetreault et al,
2010) failed to discuss this issue. 2) Some existing
work (Whitelaw et al, 2009; Rozovskaya and Roth,
2010) in the text correction field introduced artificial
errors into training data to adapt the system to bet-
ter handle ill-formed text. But this method will en-
counter serious data sparsity problems when facing
rare words.
3.1 Baseline System
We chose one of the leading spelling correction sys-
tems, (Bergsma et al, 2010), as our primary base-
line. As noted earlier, it is an SVM-based system
combining web-scale n-gram counts (NG) and con-
textual words (LEX) as features. To simplify the ex-
planation, throughout the paper, we will only con-
sider the situation with two confusion words. The
1293
problem with more than two words in pre-defined
confusion sets can be solved similarly by using a
one-vs.-all strategy. As we mentioned in Section 2,
NG features include log-counts of 3-to-5-gram pat-
terns for each candidate word with the given context.
LEX features can be broken down into three sub-
categories: 1) bag-of-words (words at all positions
in a 9-word window around the target word), 2) in-
dicators for the words preceding or following the tar-
get word, and 3) indicators for all n-grams and their
positions. For the sentence ?The system tried to de-
cide {among, between} the two confusable words.?,
examples of bag-of-word features would be ?tried?,
?two?, etc., the two positional bigrams would be
?decide? and ?the?, and examples of the n-gram fea-
tures would be right-trigram = ?among the two? and
left-4-gram = ?tried to decide between?.
3.2 Parse Features
The benefit of introducing dependency parse fea-
tures is that 1) parse features capture contextual in-
formation in a larger context window; 2) parse fea-
tures specify which words in the context are salient
to the usage of the target word while purely lexi-
cally based approaches treat all words in the context
equally. We use the Stanford dependency parser (de
Marneffe et al, 2006) to extract six relevant feature
classes.
Parse Features (PAR):
1. relation names (target word as head)
2. complement of the target word
3. combination of 1 and 2
4. relation names (target word as complement)
5. head of the target word
6. combination of 4 and 5
Each of these six classes of PAR features can
contain zero to many values, since the target word
can be involved in none to multiple grammatical
relations and features of different filler words are
merged together. The PAR features, like the LEX
features, are binary. In Table 1, we present the parse
features for an example sentence. The parse fea-
tures here are listed as string values, but are later
converted into binary numbers in the vectors for the
SVM model.
4 Distributional Word Co-occurrence
Though lexical and parse features are complemen-
tary to n-gram models, they are learned from a nor-
mal training corpus and may not have enough cov-
erage due to data sparsity. Take a sentence from the
NYT for example: ?An economist, he began his ca-
reer as a professor ? he is still called ?the professor,?
by friends as a compliment and by foes as an insult ?
and taught at Harvard and Stanford .? If the most in-
dicative word ?friends? does not appear or does not
appear enough times in the local context or depen-
dencies with ?compliment? as compared to ?com-
plement? in the training corpus, then the classifier
may be unable to make the correct selection.
It is impractical and computationally costly to en-
large the training corpus without limit to include
all possible language phenomena. A good compro-
mise is to use word co-occurrence information from
web-scale data. The other option is to make use of
high-order word co-occurrence, which is included in
many semantic word relatedness measures, such as
Latent Semantic Analysis (LSA) (Landauer et al,
1998; Deerwester et al, 1990) or Random Indexing,
both of which can be estimated from a moderate-size
corpus.
Our intuition is to choose the confusion word
which is most relevant to a given context. We define
the salient words in context as a set M=m1, m2, m3,
..., and the relevance between two words as a func-
tion Relevance(w1, w2), which can either be calcu-
lated fromword co-occurrence or Random Indexing.
The score of each candidate word c in the confusion
set given a context with meaningful words M is cal-
culated by the following formula:
Score(c) =
?
m?M
Relevance(c,m)
In this paper, we experiment with first-order word
co-occurrence and Random Indexing as relevance
measures. And we define salient contextual words
as heads or complements in the dependency rela-
tions with the target word. In this way, we use the
parse information to constrain the two distribution
models. Thus the word co-occurrence information
1294
Feature Name PAR Features (compliment) PAR Features (complement)
1. Head Relation Name ccomp appos
2. Head of Relation says collisions
3. Head Combination ccomp says appos collisions
4. Comp Relation Name nsubj dep
5. Comp of Relation addition rating
6. Comp Combination nsub addition dep rating
Table 1: Parse Feature Example for the sentence: ?GM says the addition of OnStar, which includes a system that
automatically notifies an OnStar operator if the vehicle is involved in a collision, complements the Vue?s top five-star
safety rating for the driver and front passenger in both front- and side-impact crash tests.?
considerably overlaps with some values of the PAR
features, but provides extra evidence from web-scale
data rather than a limited amount of training data.
4.1 First-order Word Co-occurrence
The relevance based on first-order word co-
occurrence is calculated from the Google Web 5-
gram Corpus in a fashion similar to how we dealt
with n-gram counts in the previous section. Given
two words, w1 and w2, we consider all 8 possible
patterns that appear in a local context (5-word win-
dow), where we use wildcard (*) to indicate any to-
ken:
w1 w2
w1 * w2
w1 * * w2
w1 * * * w2
w2 w1
w2 * w1
w2 * * w1
w2 * * * w1
The relevance is then calculated by summing the
logarithm of each of the 8 different counts. Finally,
we compare the score of each candidate word and
output the one with higher score.
4.2 Random Indexing
The relevance scores based on Random Indexing
are provided by a tool FRanI (Higgins, 2004) and
a model trained on the Touchstone Applied Science
Associates (TASA) corpus which contains 750k sen-
tences and covers diverse topics (from a diversity of
textbooks up to the college level). Take the sentence
at the beginning of this section for example, where
only the words ?a? and ?friends? are related to the
target word (either ?complement? or ?compliment?)
by either relevance measure. The relevance based
on Random Indexing for (complement, friends) is
0.08, (compliment, friends) is 0.19 and both (com-
pliment, a) and (complement, a) are 0 because ?a?
is in the stop word list. Meanwhile, the relevance
based on first order word co-occurrence for (com-
pliment, friends) is 7.39, (complement, friends) is
5.38, (compliment, a) is 13.25, and (complement, a)
is 13.42. The system with either kind of relevance
outputs ?compliment?.
4.3 System Combination
Since the numeric measurement of word co-
occurrence is not as specific as the PAR features and
less trustworthy, adding word co-occurrence infor-
mation as features into the classifier along with n-
gram counts, lexical and parse features will hurt the
overall performance. It is more practical to combine
the two approaches in the following fashion:
1. When the SVM classifier (using NG, LEX and
PAR features) has high confidence (over a cer-
tain threshold) in the output label, output that
label;
2. Otherwise, output the results of the word
relatedness/co-occurrence-based system.
5 Evaluation
We evaluate the effectiveness of syntactic and dis-
tributional information on spelling correction. The
performance of the system is measured by accu-
racy: the percentage of sentences in the test data
for which the system chooses the correct word. We
compare our results against two baselines: 1) MA-
JOR chooses the most frequent candidate from the
1295
confusion set in the training corpus, and 2) Bergsma
et al?s (2010) best systems, NG+LEX. We include
inflectional variants (?-ing?, ?-ed?, ?-s?, ?-ly?) of
confusion words in the evaluation, such as comple-
menting, complimenting in addition to complement,
compliment, because this better corresponds to the
range of errors that may be encountered in actual
use and thus increases the scope of the system as a
real world application. Also following Bergsma et
al. (2010), we use a linear SVM, more exactly, the
L2-regularized L2-loss dual SVM in LIBLINEAR
(Fan et al, 2008). Unlike Bergsma et al, who used
development data to optimize parameters, we always
use default parameters, since training data is limited
for many of the words we are dealing with.
5.1 Data
Following Bergsma et al (2009; 2010), the test
examples are extracted from The New York Times
(NYT) portion of Gigaword1, but constrained to a
9-month publication time frame from October 2005
to July 2006. Unlike Bergsma et al who use the
same source as training data for the lexical features,
our training data (for both lexical and parse features)
comes from larger and more diverse news sources.
We use the very large database from Sekine?s n-gram
search engine (Sekine, 2008) as training data, which
consists of 1.9B words of newspaper text spanning
89 years from NYT, BBC, WSJ, Xinhua, etc.
We evaluate our systems on 5 confusion sets from
Bergsma et al (2009; 2010) and 9 commonly con-
fused word pairs with moderate frequency in daily
usage (randomly selected from those listed in En-
glish educational resources2). Shown in Table 2,
these 9 sets of words appear much less frequently
than the words selected by Bergsma et al, even
given the fact that we are using a considerably large
training corpus.
For each confusable word pair, sentences that
contain either of the words are extracted to form
training and test data. The word that appears in the
original sentences of the news article is treated as
the gold standard. For frequently occurring confu-
sion word sets used by Bergsma et al, we extract
up to 10k examples for testing, and up to 100k ex-
1Available from the LDC as LDC2003T05
2Such as an English learning blog post at
http://elisaenglish.pixnet.net/blog/post/1335194
Word Confusion Set # in Training Corpus
adverse / averse 13.5k / 1.8k
advice / advise 62.k / 12.9k
allusion / illusion 1.0k / 5.4k
complement / compliment 6.8k / 3.1k
confidant / confident 2.4k / 63.6k
desert / dessert 24.7k / 3.7k
discreet / discrete 0.7k / 2.4k
elicit / illicit 1.9k / 10.0k
stationary / stationery 2.5k/2.3k
wander / wonder 3.3k / 39.5k
Table 2: Training Data Sizes for Common ESL Confused
Words
amples for training. For the 9 less frequent confu-
sion word sets, we extract all the unique examples
for training and testing from the above sources. The
spelling correction system is evaluated by measur-
ing its accuracy in comparison to the gold standard
in test data. The error rate is the complement of ac-
curacy.
Following Carlson et al (2007) and Bergsma
et al (2009; 2010), we obtain the n-gram counts
from the GoogleWeb 1T 5-gram Corpus (Brants and
Franz, 2006).
5.2 Experimental Results
We present the results for each set separately be-
cause each set may behave very differently, depend-
ing upon its frequency, part-of-speech, number of
senses and other differences between the words in
each confusion set. The overall accuracy across con-
fusion sets is also presented to show the effective-
ness of different approaches. The results are tested
for statistical significance using McNemar?s test of
correlated proportions. The performance differences
are marked as significant when p < 0.05.
5.2.1 Effectiveness of Parse Features
We exploit the n-gram counts (NG), lexical fea-
tures (LEX) of Bergsma et al (2010) and our own
parse features (PAR) in linear SVM models.
The first comparison is between the supervised
learning systems with LEX and LEX+PAR. As
shown in Table 3, by exploiting our unique parse
features, for the total 14 confusion sets, the accuracy
increases on 12 sets and decreases on 2 sets. Over-
all, the spelling correction accuracy improves an ab-
1296
solute 1.35% for our 9 confusion sets and 0.60% for
Bergsma et al?s 5 confusion sets.
The second comparison is to see how parse fea-
tures interact with n-gram count features in a su-
pervised classifier. The best system from (Bergsma
et al, 2010) is listed in the table as ?NG+LEX?.
As shown in Table 3, the parse features proved to
be beneficial when augmenting this baseline, except
for the decrease in accuracy on adverse, averse by
only 2 cases out of 368, and among, between by
2 cases out of 10227. For all other confusion sets,
parse features decrease the error rate by as much as
2.74% (absolute) and as much as 38.5% (relative).
Improvements are statistically significant on all con-
fusion sets together, although for each separate set,
improvements are significant on only 5 sets, in part
due to an insufficient number of test cases.
The reason that parse features are occasionally not
helpful is because they sometimes include an un-
common word in dependencies, which happens to
appear once with the wrong word but not with the
correct word in the training data; or they sometimes
include too common words, which bias the classifier
in favor of the more frequent word in the confusion
set. We also noticed that lexical features are not al-
ways helpful when added to n-gram count features,
even for in-domain applications (i.e., with training
data and test data coming from the same domain or
corpus), as marked by underlines. However, lexical
and parse features together show more significant
and constant improvement over n-gram count-based
models, as marked by ?.
Of the six systems, every system that uses parse
features gets the example correct in Section 1, ?com-
plementing the president?; LEX by itself also gets
the example correct, but NG and NG+LEX fail.
In summary, our system NG+LEX+PAR outper-
forms the state-of-the-art system NG+LEX. It re-
duces the error rate by 12.4% across our 9 confusion
sets and by 8.4% across Bergsma et al?s 5 confusion
sets. Both improvements are significant (p < 0.05)
by the McNemar test. In addition, while NG+LEX
is not always better than NG, NG+LEX+PAR is con-
sistently better than NG.
5.2.2 Impact of Word Co-occurrence
The LIBLINEAR tool does not provide probabil-
ity estimates for SVM models but Logistic Regres-
sion can. In this set of experiments, we train a Logis-
tic Regression model with NG+LEX+PAR features
and empirically set the confidence threshold at 0.6,
as described in Section 4, based on the performance
on two word pairs. In the combined system, when
the Logistic Regression model estimates a probabil-
ity higher than the threshold we output its results,
otherwise we output the result of the system based
on word co-occurrence.
Surprisingly, although Random Indexing takes
into account more information than first-order word
co-occurrence, it lowered overall performance sub-
stantially. Thus in Table 4, we only present results
of using first-order word co-occurrence rather than
Random Indexing. For all 12 confusion sets, distri-
butional word co-occurrence information improves
9 sets and hurts 5 sets. Overall, it reduces the er-
ror rate slightly by 0.2% for our 9 sets and 1.5% for
Bergsma et al?s sets.
We believe there are two reasons why Ran-
dom Indexing fared worse than first-order word
co-occurrence: 1) Random Indexing considers co-
occurrence on a document level, while our first-
order word co-occurrence is limited to a 5-word win-
dow context. The latter is more suitable to context-
sensitive spelling correction. 2) The model for Ran-
dom Indexing is trained on a relatively small size
corpus compared to the web-scale data we used to
get n-gram count features for the classifier and thus
is not able to introduce much new evidence besides
the information carried by NG+LEX+PAR features.
Reason 2) also suggests why first-order co-
occurrence helps on some occasions while not on
other occasions. Its impact is limited because the
word co-occurrence information overlaps with some
of the PAR feature values as mentioned earlier. It
improves some cases because it provides some new
evidence from web-scale data to the system based on
NG+LEX+PAR features. It introduces new errors
because it simply favors the word that co-occurred
more often regardless of other factors. Its impact is
also limited because it is only considered when clas-
sifiers with NG+LEX+PAR features are not confi-
dent.
1297
CONFUSION SET # TEST MAJOR LEX LEX+PAR NG NG+LEX NG+LEX+PAR (&)
9 commonly cited ESL confusion pairs
adverse / averse 368 85.87 97.01 96.74 91.03 97.55 97.01 (+22.2%) ?
allusion / illusion 535 76.64 91.22 91.40 91.40 92.52 93.08 (-7.5%) ?
complement / compliment 860 51.51 83.84 85.12 88.49 88.37 89.53 (-10.0%)
confidant / confident 2416 94.41 97.97 98.30 98.51 99.05 99.09 (-4.3%) ?
desert / dessert 2357 70.81 90.71 91.56 87.31 93.68 94.57 (-14.1%) ?*
discreet / discrete 219 79.45 84.48 85.84 85.84 90.41 91.32 (-9.5%) ?
elicit / illicit 563 53.46 82.77 95.56 97.51 97.51 98.22 (-28.6%)
stationary / stationery 182 62.64 87.36 92.31* 93.96 92.86 95.60 (-38.5%)
wander / wonder 6506 86.37 96.42 97.42* 97.56 98.23 98.48 (-13.9%) ?*
Total 13972 81.08 93.94 95.29* 94.82 96.56 96.99 (-12.4%) ?*
5 Original Bergsma pairs
# among / between 10227 57.46 91.89 91.86 88.34 93.60 93.58 (+3.1%) ?
# amount / number 7398 76.44 92.34 93.16* 93.03 93.42 94.08 (-10.1%) ?*
# cite / site 10185 95.71 99.42 99.53 99.16 99.52 99.63 (-22.4%)?
# peace / piece 7330 56.81 95.01 97.01* 95.55 96.74 97.46 (-22.2%)? *
# raise / rise 9464 55.98 96.12 96.64* 94.45 96.68 97.05 (-11.5%) ?
Total 44604 68.92 95.09 95.69* 94.07 96.09 96.42 (-8.4%) ?
Table 3: Spelling correction precision (%), impact of adding parse features
SVM trained on 1G words of news text, tested on 9-months of NYT data.
*: Improvement of (NG+)LEX+PAR vs. (NG+)LEX is statistically significant.
?: Improvement of NG+LEX+PAR vs. NG is statistically significant.
&: Relative increase or decrease of error rate compared to ?NG+LEX?
#: As in Bergsma et al (2009; 2010) no morphological variants of the words are used in evaluation
CONFUSION SET # TEST MAJOR CLASSIFIER COMBINED SYSTEM (&)
9 commonly cited ESL confusion pairs
adverse / averse 368 85.87 97.55 96.74 (+33.3%)
allusion / illusion 535 76.64 92.34 92.34 (- 0.0%)
complement / compliment 860 51.51 89.88 90.81 (-9.2%)
confidant / confident 2416 94.41 99.13 99.05 (+9.5%)
desert / dessert 2357 70.81 93.98 94.23 (-3.7%)
discreet / discrete 219 79.45 90.41 91.78 (-14.3%)
elicit / illicit 563 53.46 98.40 98.76 (-22.2%)
stationary / stationery 182 62.64 93.41 93.96 (-9.1%)
wander / wonder 6506 86.37 98.49 98.36 (+9.2%)
5 Original Bergsma pairs
# among / between 10227 57.46 92.73 92.73 (-0.1%)
# amount / number 7398 76.44 93.44 93.76 (-4.74%)
# cite / site 10185 95.71 99.49 99.47 (+3.8%)
# peace / piece 7330 56.81 96.19 96.38 (-5.0%)
# raise / rise 9464 55.98 96.66 96.59 (+2.2%)
Table 4: Spelling correction accuracy (%), impact of combining word co-occurrence
CLASSIFIER: Logistic Regression trained on 1G words of news text, tested on 9-months NYT data.
COMBINED SYSTEM: CLASSIFER plus system based on first-order word co-occurrence.
&: Relative increase or decrease in error rate compared to CLASSIFIER
#: As in Bergsma et al (2009; 2010), no morphological variants of the words are used in evaluation
1298
6 Conclusions
We propose a novel approach that uses parse
features and lexical features together to improve
the performance of web-scale n-gram models for
spelling correction. This method is especially adap-
tive when less training data are available, which is
the case for confusable words that are not very fre-
quently used. We also investigate the effectiveness
of incorporating web-scale word co-occurrence and
corpus-based semantic word relatedness (Random
Indexing).
For future work, we will investigate using seman-
tic information (e.g. WordNet) to extend n-gram
models. It will be interesting to see if the usage of
the word ?compliment? in ?complimenting the pres-
ident? can be estimated by considering similar us-
ages in the corpus, such as ?complimenting the stu-
dent? or by creating an n-gram database of synset
patterns. We will investigate extending, to other ap-
plications, this general methodology combining dis-
tributional, semantic and syntactic information with
language models.
Acknowledgments
We wish to thank Michael Flor of Educational
Testing Service for his TrendStream tool, which
provides fast access and easy manipulation of the
Google N-gram Corpus. We also thank Derrick Hig-
gins of Educational Testing Service for his Random
Indexing support. We also thank Satoshi Sekine of
New York University, Matthew Snover of City Uni-
versity of New York, and Jing Jiang of Singapore
Management University for their advice.
References
Shane Bergsma, Dekang Lin, and Randy Goebel. 2009.
Web-scale n-gram models for lexical disambiguation.
In IJCAI.
Shane Bergsma, Emily Pitler, and Dekang Lin. 2010.
Creating robust supervised classifiers via web-scale n-
gram data. In ACL.
Thorsten Brants and Alex Franz. 2006. Web 1T 5-gram
version 1. Available at http://www.ldc.upenn.edu.
Alexander Budanitsky and Graeme Hirst. 2001. Seman-
tic distance in wordnet: An experimental, application-
oriented evaluation of five measures. In ACL Work-
shop on WordNet and Other Lexical Resources.
Andrew Carlson and Ian Fette. 2007. Memory-based
context sensitive spelling correction at web scale. In
ICMLA.
Martin Chodorow, Joel Tetreault, and Na-Rae Han. 2007.
Detection of grammatical errors involving preposi-
tions. In Proceedings of the Fourth ACL-SIGSEM
Workshop on Prepositions, pages 25?30.
Silviu Cucerzan and David Yarowsky. 2002. Aug-
mented mixture models for lexical disambigua-tion. In
EMNLP.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed de-
pendency parses from phrase structure parses. In Pro-
ceedings of LREC, Genoa, Italy.
Scott Deerwester, Susan Dumais, George Furmas,
Thomas Landauer, and Richar Harshman. 1990. In-
dexing by latent semantic analysis. The American So-
ciety for Information Science.
Dmitriy Dligach and Martha Palmer. 2008. Novel se-
mantic features for verb sense disambiguation. In
ACL.
Philip Edmonds. 1997. Choosing the word most typical
in context using a lexical co-occurrence network. In
EACL.
Mohammed Ali Elmi and Martha Evans. 1998. Spelling
correction using context. In COLING.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui
Wang, and Chih-Jen Lin. 2008. Liblinear: A library
for large linear classification. Machine Learning Re-
search, 9(1871-1874).
Rachele De Felice and Stephen G. Pulman. 2008.
A classifier-based approach to preposition and deter-
miner error correction in L2 English. In Proceedings
of COLING, Manchester, UK.
Rachele De Felice and Stephen G. Pulman. 2009. Auto-
matic detection of preposition errors in learner writing.
CALICO Journal, 26(3).
Michael Gamon, Jianfeng Gao, Chris Brockett, Alex
Klementiev, William B. Dolan, Dmitriy Belenko, and
Lucy Vanderwende. 2008. Using contextual speller
techniques and language modeling for ESL error cor-
rection. In Proceedings of the International Joint Con-
ference on Natural Language Processing (IJCNLP),
pages 449?456, Hyderabad, India.
Andrew Golding and Dan Roth. 1996. Applying Win-
now to context-sensitive spelling correction. In Pro-
ceedings of the International Conference on Machine
Learning (ICML), pages 182?190.
Andrew Golding and Dan Roth. 1999. A winnow-based
approach to context-sensitive spelling correction. Ma-
chine Learning, 34(1-3):107?130.
Andrew Golding. 1995. A Bayesian hybrid method for
context sensitive spelling correction. In Proceedings
1299
of the Third Workshop on Very Large Corpora (WVLC-
3), pages 39?53.
Na-Rae Han, Martin Chodorow, and Claudia Leacock.
2006. Detecting errors in English article usage by
non-native speakers. Natural Language Engineering,
12(2):115?129.
Matthieu Hermet, Alain De?silets, and Stan Szpakowicz.
2008. Using the web as a linguistic resource to au-
tomatically correct lexico-syntactic errors. In Pro-
ceedings of the Sixth International Conference on Lan-
guage Resources and Evaluation (LREC), pages 390?
396, Marrekech, Morocco.
Emi Izumi, Kiyotaka Uchimoto, Toyomi Saiga, Thepchai
Supnithi, and Hitoshi Isahara. 2003. Automatic er-
ror detection in the Japanese learners? English spoken
data. In Companion Volume to the Proceedings of the
41st Annual Meeting of the Association for Computa-
tional Linguistics (ACL), pages 145?148.
Michael Jones and James Martin. 1997. Contextual
spelling correction using latent semantic analysis. In
ANLC.
Thomas Landauer, Darrell Laham, and Peter Foltz. 1998.
Learning human-like knowledge by singular value de-
composition: A progress report. Advances in Neural
Information Processing Systems, 10:45?51.
Mirella Lapata and Frank Keller. 2005. Web-based mod-
els for natural language processing. ACM Transac-
tions on Speech and Language Processing, 21:1?31.
John Lee and Ola Knutsson. 2008. The role of pp attach-
ment in preposition generation. In CICLING.
Lidia Mangu and Eric Brill. 1997. Automatic rule acqui-
sition for spelling correction. In ICML.
Saif Mohammad and Graeme Hist. 2006. Distributional
measures of concept distance: A task-oriented evalua-
tion. In EMNLP.
Alla Rozovskaya and Dan Roth. 2010. Training
paradigms for correcting errors in grammar and usage.
In ACL.
Magnus Sahlgren. 2006. The Word-Space Model: us-
ing distributional analysis to represent syntagmatic
and paradigmatic relations between words in high-
dimensional vector spaces. Ph.D. thesis.
Joel Tetreault and Martin Chodorow. 2008. The ups and
downs of prepostion error detection in esl writing. In
COLING.
Joel Tetreault, Jennifer Foster, and Martin Chodorow.
2010. Using parse features for preposition selection
and error detection. In ACL.
Casey Whitelaw, Ben Hutchinson, Grace Y. Chung, and
Gerard Ellis. 2009. Using the web for language inde-
pendent spellchecking and autocorrection. In ACL.
1300
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 665?670,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Filling Knowledge Base Gaps for Distant Supervision
of Relation Extraction
Wei Xu+ Raphael Hoffmann? Le Zhao#,* Ralph Grishman+
+New York University, New York, NY, USA
{xuwei, grishman}@cs.nyu.edu
?University of Washington, Seattle, WA, USA
raphaelh@cs.washington.edu
#Google Inc., Mountain View, CA, USA
lezhao@google.com
Abstract
Distant supervision has attracted recent in-
terest for training information extraction
systems because it does not require any
human annotation but rather employs ex-
isting knowledge bases to heuristically la-
bel a training corpus. However, previous
work has failed to address the problem
of false negative training examples misla-
beled due to the incompleteness of knowl-
edge bases. To tackle this problem, we
propose a simple yet novel framework that
combines a passage retrieval model using
coarse features into a state-of-the-art rela-
tion extractor using multi-instance learn-
ing with fine features. We adapt the in-
formation retrieval technique of pseudo-
relevance feedback to expand knowledge
bases, assuming entity pairs in top-ranked
passages are more likely to express a rela-
tion. Our proposed technique significantly
improves the quality of distantly super-
vised relation extraction, boosting recall
from 47.7% to 61.2% with a consistently
high level of precision of around 93% in
the experiments.
1 Introduction
A recent approach for training information ex-
traction systems is distant supervision, which ex-
ploits existing knowledge bases instead of anno-
tated texts as the source of supervision (Craven
and Kumlien, 1999; Mintz et al, 2009; Nguyen
and Moschitti, 2011). To combat the noisy train-
ing data produced by heuristic labeling in distant
supervision, researchers (Bunescu and Mooney,
2007; Riedel et al, 2010; Hoffmann et al, 2011;
Surdeanu et al, 2012) exploited multi-instance
*This work was done while Le Zhao was at Carnegie
Mellon University.
learning models. Only a few studies have directly
examined the influence of the quality of the train-
ing data and attempted to enhance it (Sun et al,
2011; Wang et al, 2011; Takamatsu et al, 2012).
However, their methods are handicapped by the
built-in assumption that a sentence does not ex-
press a relation unless it mentions two entities
which participate in the relation in the knowledge
base, leading to false negatives.
aligned 
mentions 
true 
 mentions 5.5% 2.7% 1.7% false  negatives false  
positives 
Figure 1: Noisy training data in distant supervi-
sion
In reality, knowledge bases are often incom-
plete, giving rise to numerous false negatives in
the training data. We sampled 1834 sentences that
contain two entities in the New York Times 2006
corpus and manually evaluated whether they ex-
press any of a set of 50 common Freebase1 rela-
tions. As shown in Figure 1, of the 133 (7.3%)
sentences that truly express one of these relations,
only 32 (1.7%) are covered by Freebase, leaving
101 (5.5%) false negatives. Even for one of the
most complete relations in Freebase, Employee-of
(with more than 100,000 entity pairs), 6 out of 27
sentences with the pattern ?PERSON executive of
ORGANIZATION? contain a fact that is not in-
cluded in Freebase and are thus mislabeled as neg-
ative. These mislabelings dilute the discriminative
capability of useful features and confuse the mod-
els. In this paper, we will show how reducing this
source of noise can significantly improve the per-
formance of distant supervision. In fact, our sys-
tem corrects the relation labels of the above 6 sen-
tences before training the relation extractor.
1http://www.freebase.com
665
 
D ocuments  Knowledge 
Base  
Relation 
Extractor  
Passage  
Retriever  
? 
? 
? 
Pseudo - relevant 
Relation Instances  
? 
? 
? 
Figure 2: Overall system architecture: The system
(1) matches relation instances to sentences and (2)
learns a passage retrieval model to (3) provide rel-
evance feedback on sentences; Relevant sentences
(4) yield new relation instances which are added
to the knowledge base; Finally, instances are again
(5) matched to sentences to (6) create training data
for relation extraction.
Encouraged by the recent success of simple
methods for coreference resolution (Raghunathan
et al, 2010) and inspired by pseudo-relevance
feedback (Xu and Croft, 1996; Lavrenko and
Croft, 2001; Matveeva et al, 2006; Cao et al,
2008) in the field of information retrieval, which
expands or reformulates query terms based on
the highest ranked documents of an initial query,
we propose to increase the quality and quantity
of training data generated by distant supervision
for information extraction task using pseudo feed-
back. As shown in Figure 2, we expand an orig-
inal knowledge base with possibly missing rela-
tion instances with information from the highest
ranked sentences returned by a passage retrieval
model (Xu et al, 2011) trained on the same data.
We use coarse features for our passage retrieval
model to aggressively expand the knowledge base
for maximum recall; at the same time, we exploit
a multi-instance learning model with fine features
for relation extraction to handle the newly intro-
duced false positives and maintain high precision.
Similar to iterative bootstrapping tech-
niques (Yangarber, 2001), this mechanism uses
the outputs of the first trained model to expand
training data for the second model, but unlike
bootstrapping it does not require iteration and
avoids the problem of semantic drift. We further
note that iterative bootstrapping over a single
distant supervision system is difficult, because
state-of-the-art systems (Surdeanu et al, 2012;
Hoffmann et al, 2011; Riedel et al, 2010; Mintz
et al, 2009), detect only few false negatives in the
training data due to their high-precision low-recall
features, which were originally proposed by Mintz
et al (2009). We present a reliable and novel way
to address these issues and achieve significant
improvement over the MULTIR system (Hoff-
mann et al, 2011), increasing recall from 47.7%
to 61.2% at comparable precision. The key to this
success is the combination of two different views
as in co-training (Blum and Mitchell, 1998):
an information extraction technique with fine
features for high precision and an information
retrieval technique with coarse features for high
recall. Our work is developed in parallel with
Min et al (2013), who take a very different
approach by adding additional latent variables to
a multi-instance multi-label model (Surdeanu et
al., 2012) to solve this same problem.
2 System Details
In this section, we first introduce some formal no-
tations then describe in detail each component of
the proposed system in Figure 2.
2.1 Definitions
A relation instance is an expression r(e1, e2)
where r is a binary relation, and e1 and e2 are
two entities having such a relation, for example
CEO-of(Tim Cook, Apple). The knowledge-based
distant supervised learning problem takes as input
(1) ?, a training corpus, (2) E, a set of entities
mentioned in that corpus, (3) R, a set of relation
names, and (4) ?, a set of ground facts of relations
in R. To generate our training data, we further as-
sume (5) T , a set of entity types, as well as type
signature r(E1, E2) for relations.
We define the positive data set POS(r) to be
the set of sentences in which any related pair
of entities of relation r (according to the knowl-
edge base) is mentioned. The negative data set
RAW (r) is the rest of the training data, which
contain two entities of the required types in the
knowledge base, e.g. one person and one or-
ganization for the CEO-of relation in Freebase.
Another negative data set with more conservative
sense NEG(r) is defined as the set of sentences
which contain the primary entity e1 (e.g. person
in any CEO-of relation in the knowledge base) and
any secondary entity e2 of required type (e.g. or-
ganization for the CEO-of relation) but the relation
does not hold for this pair of entities in the knowl-
edge base.
666
2.2 Distantly Supervised Passage Retrieval
We extend the learning-to-rank techniques (Liu,
2011) to distant supervision setting (Xu et al,
2011) to create a robust passage retrieval system.
While relation extraction systems exploit rich and
complex features that are necessary to extract the
exact relation (Mintz et al, 2009; Riedel et al,
2010; Hoffmann et al, 2011), passage retrieval
components use coarse features in order to provide
different and complementary feedback to informa-
tion extraction models.
We exploit two types of lexical features: Bag-
Of-Words and Word-Position. The two types of
simple binary features are shown in the following
example:
Sentence: Apple founder Steve Jobs died.
Target (Primary) entity: Steve Jobs
Bag-Of-Word features: ?apple? ?founder? ?died? ?.?
Word-Position features: ?apple:-2? ?founder:-1?
?died:+1? ?.:+2?
For each relation r, we assume each sentence
has a binary relevance label to form distantly su-
pervised training data: sentences in POS(r) are
relevant and sentences in NEG(r) are irrelevant.
As a pointwise learning-to-rank approach (Nallap-
ati, 2004), the probabilities of relevance estimated
by SVMs (Platt and others, 1999) are used for
ranking all the sentences in the original training
corpus for each relation respectively. We use Lib-
SVM 2 (Chang and Lin, 2011) in our implementa-
tion.
2.3 Psuedo-relevance Relation Feedback
In the field of information retrieval, pseudo-
relevance feedback assumes that the top-ranked
documents from an initial retrieval are likely rel-
evant, and extracts relevant terms to expand the
original query (Xu and Croft, 1996; Lavrenko and
Croft, 2001; Cao et al, 2008). Analogously, our
assumption is that entity pairs that appear in more
relevant and more sentences are more likely to
express the relation, and can be used to expand
knowledge base and reduce false negative noise in
the training data for information extraction. We
identify the most likely relevant entity pairs as fol-
lows:
2http://www.csie.ntu.edu.tw/?cjlin/
libsvm
initialize ?? ?? ?
for each relation type r ? R do
learn a passage (sentence) retrieval model L(r)
using coarse features and POS(r)?NEG(r)
as training data
score the sentences in the RAW (r) by L(r)
score the entity pairs according to the scores
of sentences they are involved in
select the top ranked pairs of entities, then add
the relation r to their label in ??
end for
We select the entity pairs whose average score
of the sentences they are involved in is greater
than p, where p is a parameter tuned on develop-
ment data.3 The relation extraction model is then
trained using (?, E,R,??) with a more complete
database than the original knowledge base ?.
2.4 Distantly Supervised Relation Extraction
We use a state-of-the-art open-source system,
MULTIR (Hoffmann et al, 2011), as the rela-
tion extraction component. MULTIR is based
on multi-instance learning, which assumes that
at least one sentence of those matching a given
entity-pair contains the relation of interest (Riedel
et al, 2010) in the given knowledge base to tol-
erate false positive noise in the training data and
superior than previous models (Riedel et al, 2010;
Mintz et al, 2009) by allowing overlapping rela-
tions. MULTIR uses features which are based on
Mintz et al (2009) and consist of conjunctions of
named entity tags, syntactic dependency paths be-
tween arguments, and lexical information.
3 Experiments
For evaluating extraction accuracy, we follow the
experimental setup of Hoffmann et al (2011), and
use their implementation of MULTIR4 with 50
training iterations as our baseline. Our complete
system, which we call IRMIE, combines our pas-
sage retrieval component with MULTIR. We use
the same datasets as in Hoffmann et al (2011) and
Riedel et al (2010), which include 3-years of New
York Times articles aligned with Freebase. The
sentential extraction evaluation is performed on
a small amount of manually annotated sentences,
sampled from the union of matched sentences and
3We found p = 0.5 to work well in practice.
4http://homes.cs.washington.edu/
?raphaelh/mr/
667
Test Data Set Original Test Set Corrected Test Set
P? R? F? ?F? P? R? F? ?F?
MULTIR 80.0 44.6 62.3 92.7 47.7 70.2
IRMIE 84.6 56.1 70.3 +8.0 92.6 61.2 76.9 +6.7
MULTIRLEX 91.8 43.0 67.4 79.6 57.0 68.3
IRMIELEX 89.2 52.5 70.9 +3.5 78.0 69.2 73.6 +5.3
Table 1: Overall sentential extraction performance evaluated on the original test set of Hoffmann et
al. (2011) and our corrected test set: Our proposed relevance feedback technique yields a substantial
increase in recall.
system predictions. We define Se as the sentences
where some system extracted a relation and SF
as the sentences that match the arguments of a
fact in ?. The sentential precision and recall is
computed on a randomly sampled set of sentences
from Se?SF , in which each sentence is manually
labeled whether it expresses any relation in R.
Figure 3 shows the precision/recall curves for
MULTIR with and without pseudo-relevance feed-
back computed on the test dataset of 1000 sen-
tence used by Hoffmann et al (2011). With the
pseudo-relevance feedback from passage retrieval,
IRMIE achieves significantly higher recall at a
consistently high level of precision. At the highest
recall point, IRMIE reaches 78.5% precision and
59.2% recall, for an F1 score of 68.9%.
Because the two types of lexical features used in
our passage retrieval models are not used in MUL-
TIR, we created another baseline MULTIRLEX
by adding these features into MULTIR in order
to rule out the improvement from additional infor-
mation. Note that the sentences are sampled from
the union of Freebase matches and sentences from
which some systems in Hoffmann et al (2011) ex-
tracted a relation. It underestimates the improve-
ments of the newly developed systems in this pa-
per. We therefore also created a new test set of
1000 sentences by sampling from the union of
Freebase matches and sentences where MULTIR-
LEX or IRMIELEX extracted a relation. Table 1
shows the overall precision and recall computed
against these two test datasets, with and without
adding lexical features into multi-instance learn-
ing models. The performance improvement by us-
ing pseudo-feedback is significant (p < 0.05) in
McNemar?s test for both datasets.
4 Conclusion and Perspectives
This paper proposes a novel approach to address
an overlooked problem in distant supervision: the
knowledge base is often incomplete causing nu-
Recall
Precision
0.0 0.1 0.2 0.3 0.4 0.5 0.6
0.5
0.6
0.7
0.8
0.9
1.0
IRMIE
MULTIR
Figure 3: Sentential extraction: precision/recall
curves using exact same training and test data,
features and system settings as in Hoffmann et
al. (2011).
merous false negatives in the training data. It
greatly improves a state-of-the-art multi-instance
learning model by correcting the most likely false
negatives in the training data based on the ranking
of a passage retrieval model.
In the future, we would like to more tightly inte-
grate a coarser featured estimator of sentential rel-
evance and a finer featured relation extractor, such
that a single joint-model can be learned.
Acknowledgments
Supported in part by NSF grant IIS-1018317,
the Air Force Research Laboratory (AFRL)
under prime contract number FA8750-09-C-
0181 and the Intelligence Advanced Research
Projects Activity (IARPA) via Department of In-
terior National Business Center contract number
D11PC20154. The U.S. Government is authorized
to reproduce and distribute reprints for Govern-
mental purposes notwithstanding any copyright
annotation thereon. Disclaimer: The views and
conclusions contained herein are those of the au-
thors and should not be interpreted as necessarily
representing the official policies or endorsements,
either expressed or implied, of AFRL, IARPA,
DoI/NBC, or the U.S. Government.
668
References
Avrim Blum and Tom M. Mitchell. 1998. Combin-
ing labeled and unlabeled sata with co-training. In
Proceedings of the 11th Annual Conference on Com-
putational Learning Theory (COLT), pages 92?100.
Razvan C. Bunescu and Raymond J. Mooney. 2007.
Learning to extract relations from the web using
minimal supervision. In Proceedings of the 45th An-
nual Meeting of the Association for Computational
Linguistics (ACL).
Guihong Cao, Jian-Yun Nie, Jianfeng Gao, and
Stephen Robertson. 2008. Selecting good expan-
sion terms for pseudo-relevance feedback. In Pro-
ceedigns of the 31st Annual International ACM SI-
GIR Conference on Research and Development in
Information Retrieval (SIGIR), pages 243?250.
Chih-Chung Chang and Chih-Jen Lin. 2011. Lib-
svm: A library for support vector machines. ACM
Transactions on Intelligent Systems and Technology,
2(3):27.
Mark Craven and Johan Kumlien. 1999. Constructing
biological knowledge bases by extracting informa-
tion from text sources. In Proceedings of the Sev-
enth International Conference on Intelligent Systems
for Molecular Biology (ISMB), pages 77?86.
Raphael Hoffmann, Congle Zhang, Xiao Ling,
Luke S. Zettlemoyer, and Daniel S. Weld. 2011.
Knowledge-based weak supervision for information
extraction of overlapping relations. In Proceedings
of the 49th Annual Meeting of the Association for
Computational Linguistics (ACL), pages 541?550.
Victor Lavrenko and W. Bruce Croft. 2001.
Relevance-based language models. In Proceedings
of the 24th Annual International ACM SIGIR Con-
ference on Research and Development in Informa-
tion Retrieval (SIGIR), pages 120?127.
Tie-Yan Liu. 2011. Learning to Rank for Information
Retrieval. Springer-Verlag Berlin Heidelberg.
Irina Matveeva, Chris Burges, Timo Burkard, Andy
Laucius, and Leon Wong. 2006. High accuracy re-
trieval with multiple nested ranker. In Proceedings
of the 29th Annual International ACM SIGIR Con-
ference on Research and Development in Informa-
tion Retrieval (SIGIR), pages 437?444.
Bonan Min, Ralph Grishman, Li Wan, Chang Wang,
and David Gondek. 2013. Distant supervision for
relation extraction with an incomplete knowledge
base. In Proceedings of the Conference of the North
American Chapter of the Association for Computa-
tional Linguistics (NAACL 2013).
Mike Mintz, Steven Bills, Rion Snow, and Daniel Ju-
rafsky. 2009. Distant supervision for relation ex-
traction without labeled data. In Proceedigns of the
47th Annual Meeting of the Association for Compu-
tational Linguistics and the 4th International Joint
Conference on Natural Language Processing (ACL),
pages 1003?1011.
Ramesh Nallapati. 2004. Discriminative models for
information retrieval. In Proceedigns of the 27th An-
nual International ACM SIGIR Conference on Re-
search and Development in Information Retrieval
(SIGIR), pages 64?71.
Truc Vien T Nguyen and Alessandro Moschitti. 2011.
End-to-end relation extraction using distant super-
vision from external semantic repositories. In Pro-
ceedings of the 49th Annual Meeting of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies, pages 277?282.
John Platt et al 1999. Probabilistic outputs for sup-
port vector machines and comparisons to regular-
ized likelihood methods. Advances in Large Margin
Classifiers, 10(3):61?74.
Karthik Raghunathan, Heeyoung Lee, Sudarshan Ran-
garajan, Nathanael Chambers, Mihai Surdeanu, Dan
Jurafsky, and Christopher Manning. 2010. A multi-
pass sieve for coreference resolution. In Proceed-
ings of the 2010 Conference on Empirical Methods
in Natural Language Processing, pages 492?501.
Association for Computational Linguistics.
Sebastian Riedel, Limin Yao, and Andrew McCallum.
2010. Modeling relations and their mentions with-
out labeled text. In Proceedigns of the European
Conference on Machine Learning and Principles
and Practice of Knowledge Discovery in Databases
(ECML/PKDD), pages 148?163.
Ang Sun, Ralph Grishman, Wei Xu, and Bonan Min.
2011. New york university 2011 system for kbp slot
filling. In Text Analysis Conference 2011 Workshop.
Mihai Surdeanu, Julie Tibshirani, Ramesh Nallapati,
and Christopher D. Manning. 2012. Multi-instance
multi-label learning for relation extraction. In Pro-
ceedings of the 50th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL), pages
455?465.
Shingo Takamatsu, Issei Sato, and Hiroshi Nakagawa.
2012. Reducing wrong labels in distant supervi-
sion for relation extraction. In Proceedings of the
50th Annual Meeting of the Association for Compu-
tational Linguistics (ACL), pages 721?729.
Chang Wang, James Fan, Aditya Kalyanpur, and David
Gondek. 2011. Relation extraction with relation
topics. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP), pages 1426?1436.
Jinxi Xu and W. Bruce Croft. 1996. Query expansion
using local and global document analysis. In Hans-
Peter Frei, Donna Harman, Peter Scha?uble, and Ross
Wilkinson, editors, Proceedings of the 19th Annual
International ACM SIGIR Conference on Research
and Development in Information Retrieval (SIGIR),
pages 4?11. ACM.
669
Wei Xu, Ralph Grishman, and Le Zhao. 2011. Passage
retrieval for information extraction using distant su-
pervision. In Proceedings of the International Joint
Conference on Natural Language Processing (IJC-
NLP), pages 1046?1054.
Roman Yangarber. 2001. Scenario customization for
information extraction. Ph.D. thesis, Department of
Computer Science, Graduate School of Arts and Sci-
ence, New York University.
670
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 732?738,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Infusion of Labeled Data into Distant Supervision for Relation Extraction
Maria Pershina
+
Bonan Min
? ?
Wei Xu
#
Ralph Grishman
+
+
New York University, New York, NY
{pershina, grishman}@cs.nyu.edu
?
Raytheon BBN Technologies, Cambridge, MA
bmin@bbn.com
#
University of Pennsylvania, Philadelphia, PA
xwe@cis.upenn.edu
Abstract
Distant supervision usually utilizes only
unlabeled data and existing knowledge
bases to learn relation extraction models.
However, in some cases a small amount
of human labeled data is available. In this
paper, we demonstrate how a state-of-the-
art multi-instance multi-label model can
be modified to make use of these reli-
able sentence-level labels in addition to
the relation-level distant supervision from
a database. Experiments show that our ap-
proach achieves a statistically significant
increase of 13.5% in F-score and 37% in
area under the precision recall curve.
1 Introduction
Relation extraction is the task of tagging semantic
relations between pairs of entities from free text.
Recently, distant supervision has emerged as an
important technique for relation extraction and has
attracted increasing attention because of its effec-
tive use of readily available databases (Mintz et
al., 2009; Bunescu and Mooney, 2007; Snyder and
Barzilay, 2007; Wu and Weld, 2007). It automat-
ically labels its own training data by heuristically
aligning a knowledge base of facts with an unla-
beled corpus. The intuition is that any sentence
which mentions a pair of entities (e
1
and e
2
) that
participate in a relation, r, is likely to express the
fact r(e
1
,e
2
) and thus forms a positive training ex-
ample of r.
One of most crucial problems in distant super-
vision is the inherent errors in the automatically
generated training data (Roth et al, 2013). Ta-
ble 1 illustrates this problem with a toy exam-
ple. Sophisticated multi-instance learning algo-
rithms (Riedel et al, 2010; Hoffmann et al, 2011;
?
Most of the work was done when this author was at
New York University
Surdeanu et al, 2012) have been proposed to ad-
dress the issue by loosening the distant supervision
assumption. These approaches consider all men-
tions of the same pair (e
1
,e
2
) and assume that at-
least-one mention actually expresses the relation.
On top of that, researchers further improved per-
formance by explicitly adding preprocessing steps
(Takamatsu et al, 2012; Xu et al, 2013) or addi-
tional layers inside the model (Ritter et al, 2013;
Min et al, 2013) to reduce the effect of training
noise.
True Positive ... to get information out of captured
al-Qaida leader Abu Zubaydah.
False Positive ...Abu Zubaydah and former Taliban
leader Jalaluddin Haqqani ...
False Negative ...Abu Zubaydah is one of Osama bin
Laden?s senior operational planners...
Table 1: Classic errors in the training data gener-
ated by a toy knowledge base of only one entry
personTitle(Abu Zubaydah, leader).
However, the potential of these previously pro-
posed approaches is limited by the inevitable
gap between the relation-level knowledge and the
instance-level extraction task. In this paper, we
present the first effective approach, Guided DS
(distant supervision), to incorporate labeled data
into distant supervision for extracting relations
from sentences. In contrast to simply taking the
union of the hand-labeled data and the corpus la-
beled by distant supervision as in the previous
work by Zhang et al (2012), we generalize the
labeled data through feature selection and model
this additional information directly in the latent
variable approaches. Aside from previous semi-
supervised work that employs labeled and unla-
beled data (Yarowsky, 2013; Blum and Mitchell,
1998; Collins and Singer, 2011; Nigam, 2001, and
others), this is a learning scheme that combines
unlabeled text and two training sources whose
quantity and quality are radically different (Liang
et al, 2009).
To demonstrate the effectiveness of our pro-
732
Guideline g = {g
i
|i = 1, 2, 3}: Relation r(g)
types of entities, dependency path, span word (optional)
person person, nsubj ?? dobj, married personSpouse
person organization, nsubj ?? prep of , became personMemberOf
organization organization, nsubj ?? prep of , company organizationSubsidiaries
person person, poss?? appos, sister personSiblings
person person, poss?? appos, father personParents
person title,? nn personTitle
organization person, prep of ? appos? organizationTopMembersEmployees
person cause, nsubj ?? prep of personCauseOfDeath
person number,? appos personAge
person date, nsubjpass?? prep on? num personDateOfBirth
Table 2: Some examples from the final set G of extracted guidelines.
posed approach, we extend MIML (Surdeanu et
al., 2012), a state-of-the-art distant supervision
model and show a significant improvement of
13.5% in F-score on the relation extraction bench-
mark TAC-KBP (Ji and Grishman, 2011) dataset.
While prior work employed tens of thousands of
human labeled examples (Zhang et al, 2012) and
only got a 6.5% increase in F-score over a logistic
regression baseline, our approach uses much less
labeled data (about 1/8) but achieves much higher
improvement on performance over stronger base-
lines.
2 The Challenge
Simply taking the union of the hand-labeled data
and the corpus labeled by distant supervision is not
effective since hand-labeled data will be swamped
by a larger amount of distantly labeled data. An
effective approach must recognize that the hand-
labeled data is more reliable than the automatically
labeled data and so must take precedence in cases
of conflict. Conflicts cannot be limited to those
cases where all the features in two examples are
the same; this would almost never occur, because
of the dozens of features used by a typical relation
extractor (Zhou et al, 2005). Instead we propose
to perform feature selection to generalize human
labeled data into training guidelines, and integrate
them into latent variable model.
2.1 Guidelines
The sparse nature of feature space dilutes the dis-
criminative capability of useful features. Given
the small amount of hand-labeled data, it is im-
portant to identify a small set of features that are
general enough while being capable of predicting
quite accurately the type of relation that may hold
between two entities.
We experimentally tested alternative feature
sets by building supervised Maximum Entropy
(MaxEnt) models using the hand-labeled data (Ta-
ble 3), and selected an effective combination of
three features from the full feature set used by Sur-
deanu et al, (2011):
? the semantic types of the two arguments (e.g.
person, organization, location, date, title, ...)
? the sequence of dependency relations along the
path connecting the heads of the two arguments
in the dependency tree.
? a word in the sentence between the two argu-
ments
These three features are strong indicators of the
type of relation between two entities. In some
cases the semantic types of the arguments alone
narrows the possibilities to one or two relation
types. For example, entity types such as person
and title often implies the relation personTitle.
Some lexical items are clear indicators of partic-
ular relations, such as ?brother? and ?sister? for a
sibling relationship
We extract guidelines from hand-labeled data.
Each guideline g={g
i
|i=1,2,3} consists of a pair
of semantic types, a dependency path, and option-
ally a span word and is associated with a partic-
ular relation r(g). We keep only those guidelines
Model Precision Recall F-score
MaxEnt
all
18.6 6.3 9.4
MaxEnt
two
24.13 10.75 14.87
MaxEnt
three
40.27 12.40 18.97
Table 3: Performance of a MaxEnt, trained on
hand-labeled data using all features (Surdeanu et
al., 2011) vs using a subset of two (types of en-
tities, dependency path), or three (adding a span
word) features, and evaluated on the test set.
733
which make the correct prediction for all and at
least k=3 examples in the training corpus (thresh-
old 3 was obtained by running experiments on the
development dataset). Table 2 shows some exam-
ples in the final set G of extracted guidelines.
3 Guided DS
Our goal is to jointly model human-labeled ground
truth and structured data from a knowledge base
in distant supervision. To do this, we extend the
MIML model (Surdeanu et al, 2012) by adding a
new layer as shown in Figure 1.
The input to the model consists of (1) distantly
supervised data, represented as a list of n bags
1
with a vector y
i
of binary gold-standard labels, ei-
ther Positive(P ) or Negative(N) for each rela-
tion r?R; (2) generalized human-labeled ground
truth, represented as a set G of feature conjunc-
tions g={g
i
|i=1,2,3} associated with a unique re-
lation r(g). Given a bag of sentences, x
i
, which
mention an ith entity pair (e
1
, e
2
), our goal is to
correctly predict which relation is mentioned in
each sentence, or NR if none of the relations under
consideration are mentioned. The vector z
i
con-
tains the latent mention-level classifications for the
ith entity pair. We introduce a set of latent vari-
ables h
i
which model human ground truth for each
mention in the ith bag and take precedence over
the current model assignment z
i
.
G
|R|
|xi|
n
zi
hi
yi
xi
9
>
=
>
;
{
relation
level
mention
level
Figure 1: Plate diagram of Guided DS
Let i, j be the index in the bag and the men-
tion level, respectively. We model mention-
level extraction p(z
ij
|x
ij
;w
z
), human relabel-
ing h
ij
(x
ij
, z
ij
) and multi-label aggregation
p(y
r
i
|h
i
;w
y
). We define:
? y
r
i
?{P,N} : r holds for the ith bag or not.
? x
ij
is the feature representation of the jth rela-
tion mention in the ith bag. We use the same set
of features as in Surdeanu et al (2012).
1
A bag is a set of mentions sharing same entity pair.
? z
ij
?R ? NR: a latent variable that denotes the
relation of the jth mention in the ith bag
? h
ij
?R ?NR: a latent variable that denotes the
refined relation of the mention x
ij
We define relabeled relations h
ij
as following:
h
ij
(x
ij
, z
ij
)=
{
r(g), if ?!g?G s.t.g={g
k
}?{x
ij
}
z
ij
, otherwise
Thus, relation r(g) is assigned to h
ij
iff there
exists a unique guideline g ? G, such that the
feature vector x
ij
contains all constituents of g,
i.e. entity types, a dependency path and maybe a
span word, if g has one. We use mention relation
z
ij
inferred by the model only in case no such a
guideline exists or there is more than one match-
ing guideline. We also define:
? w
z
is the weight vector for the multi-class rela-
tion mention-level classifier
2
? w
r
y
is the weight vector for the rth binary top-
level aggregation classifier (from mention labels
to bag-level prediction). We use w
y
to represent
w
1
y
,w
2
y
, . . . ,w
|R|
y
.
Our approach is aimed at improving the mention-
level classifier, while keeping the multi-instance
multi-label framework to allow for joint modeling.
4 Training
We use a hard expectation maximization algorithm
to train the model. Our objective function is to
maximize log-likelihood of the data:
LL(w
y
,w
z
) =
n
?
i=1
log p(y
i
|x
i
,w
y
,w
z
,G)
=
n
?
i=1
log
?
h
i
p(y
i
,h
i
|x
i
,w
y
,w
z
,G)
=
n
?
i=1
log
?
h
i
|h
i
|
?
j=1
p(h
ij
|x
ij
,w
z
,G)
?
r?P
i
?N
i
p(y
r
i
|h
i
,w
r
y
)
where the last equality is due to conditional
independence. Because of the non-convexity
of LL(w
y
,w
z
) we approximate and maximize
the joint log-probability p(y
i
,h
i
|x
i
,w
y
,w
z
,G) for
each entity pair in the database:
log p(y
i
,h
i
|x
i
,w
y
,w
z
,G)
=
|h
i
|
?
j=1
log p(h
ij
|x
ij
,w
z
,G)+
?
r?P
i
?N
i
log p(y
r
i
|h
i
,w
r
y
).
2
All classifiers are implemented using L2-regularized lo-
gistic regression with Stanford CoreNLP package.
734
Iteration 1 2 3 4 5 6 7 8
(a) Corrected relations: 2052 718 648 596 505 545 557 535
(b) Retrieved relations: 10219 860 676 670 621 599 594 592
Total relabelings 12271 1578 1324 1264 1226 1144 1153 1127
Table 4: Number of relabelings for each training iteration of Guided DS: (a) relabelings due to cor-
rected relations, e.g. personChildren? personSiblings (b) relabelings due to retrieved relations, e.g.
notRelated(NR)?personTitle
Algorithm 1 : Guided DS training
1: Phase 1: build set G of guidelines
2: Phase 2: EM training
3: for iteration = 1, . . . , T do
4: for i = 1, . . . , n do
5: for j = 1, . . . , |x
i
| do
6: z
?
ij
= argmax
z
ij
p(z
ij
|x
i
,y
i
,w
z
,w
y
)
7: h
?
ij
=
{
r(g), if ?!g?G :{g
k
}?{x
ij
}
z
ij
?
, otherwise
8: update h
i
with h
?
ij
9: end for
10: end for
11: w
?
z
=argmax
w
?
n
i=1
?
|x
i
|
j=1
log p(h
ij
|x
ij
,w)
12: for r ? R do
13: w
r?
y
=argmax
w
?
1?i?n s.t. r?P
i
?N
i
log p(y
r
i
|h
i
,w)
14: end for
15: end for
16: return w
z
,w
y
The pseudocode is presented as algorithm 1.
The following approximation is used for infer-
ence at step 6:
p(z
ij
|x
i
,y
i
,w
z
,w
y
) ? p(y
i
, z
ij
|x
i
,w
y
,w
z
)
? p(z
ij
|x
ij
,w
z
)p(y
i
|h
?
i
,w
y
)
= p(z
ij
|x
ij
,w
z
)
?
r?P
i
?N
i
p(y
r
i
|h
?
i
,w
r
y
),
where h
?
i
contains previously inferred and
maybe further relabeled mention labels for group
i (steps 5-10), with the exception of component j
whose label is replaced by z
ij
. In the M-step (lines
12-15) we optimize model parameters w
z
,w
y
,
given the current assignment of mention-level la-
bels h
i
.
Experiments show that Guided DS efficiently
learns new model, resulting in a drastically de-
creasing number of needed relabelings for further
iterations (Table 4). At the inference step we first
classify all mentions:
z
?
ij
= argmax
z?R?NR
p(z|x
ij
,w
z
)
Then final relation labels for ith entity tuple are
obtained via the top-level classifiers:
y
r?
i
= argmax
y?{P,N}
p(y|z
?
i
,w
r
y
)
5 Experiments
5.1 Data
We use the KBP (Ji and Grishman, 2011) dataset
3
which is preprocessed by Surdeanu et al (2011)
using the Stanford parser
4
(Klein and Manning,
2003). This dataset is generated by mapping
Wikipedia infoboxes into a large unlabeled corpus
that consists of 1.5M documents from KBP source
corpus and a complete snapshot of Wikipedia.
The KBP 2010 and 2011 data includes 200
query named entities with the relations they are
involved in. We used 40 queries as development
set and the rest 160 queries (3334 entity pairs that
express a relation) as the test set. The official KBP
evaluation is performed by pooling the system re-
sponses and manually reviewing each response,
producing a hand-checked assessment data. We
used KBP 2012 assessment data to generate guide-
lines since queries from different years do not
overlap. It contains about 2500 labeled sentences
of 41 relations, which is less than 0.09% of the
size of the distantly labeled dataset of 2M sen-
tences. The final set G consists of 99 guidelines
(section 2.1).
5.2 Models
We implement Guided DS on top of the MIML
(Surdeanu et al, 2012) code base
5
. Training
MIML on a simple fusion of distantly-labeled
and human-labeled datasets does not improve the
maximum F-score since this hand-labeled data is
swamped by a much larger amount of distant-
supervised data of much lower quality. Upsam-
pling the labeled data did not improve the perfor-
mance either. We experimented with different up-
sampling ratios and report best results using ratio
1:1 in Figure 2.
3
Available from Linguistic Data Consortium (LDC) at
http://projects.ldc.upenn.edu/kbp/data.
4
http://nlp.stanford.edu/software/lex-parser.shtml
5
Available at http://nlp.stanford.edu/software/mimlre.shtml.
735
a)
b) 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
Recall
Prec
ision
Student Version of MATLAB
0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
Recall
Pre
cis
ion
 
 
Guided DS
MIML
Mintz++
MultiR
Student Version of MATLAB
0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.50.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
Recall
Prec
ision
Student Version of MATLAB
0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
Recall
Pr
ec
isio
n
 
 
Guided DS
Semi?MIML
DS+upsampling
MaxEnt
Student Version of MATLAB
Model P R F1 AUC Model P R F1 AUC
MaxEnt 40.27 12.40 18.97 1.97 MultiR 30.64 19.79 24.05 6.4
DS+upsampling 32.26 24.31 27.72 12.00 Mintz++ 25.17 25.87 25.51 10.94
Semi MIML 30.02 26.21 27.98 12.31 MIML 28.06 28.64 28.35 11.74
Guided DS 31.9 32.46 32.19 16.1
Model P R F1 AUC
s
t
a
t
e
-
o
f
-
a
r
t
Model P R F1 AUC
MaxEnt 40.27 12.40 18.97 1.97 MultiR 30.64 19.79 24.05 6.4
DS+upsampling 32.26 24.31 27.72 12.00 Mintz++ 25.17 25.87 25.51 10.94
Semi MIML 30.02 26.21 27.98 12.31 MIML 28.06 28.64 28.35 11.74
Guided DS 31.9 32.46 32.19 16.1
b
a
s
e
l
i
n
e
Model P R F1 AUC
s
t
a
t
e
-
o
f
-
a
r
t
Model P R F1 AUC
MaxEnt 40.27 12.40 18.97 1.97 MultiR 30.64 19.79 24.05 6.4
DS+upsampling 32.26 24.31 27.72 12.00 Mintz++ 25.17 25.87 25.51 10.94
Semi MIML 30.02 26.21 27.98 12.31 MIML 28.06 28.64 28.35 11.74
Guided DS 31.9 32.46 32.19 16.1
b
a
s
e
l
i
n
e
Model P R F1 AUC
s
t
a
t
e
-
o
f
-
a
r
t
Model P R F1 AUC
MaxEnt 40.27 12.40 18.97 1.97 MultiR 30.64 19.79 24.05 6.4
DS+upsampling 32.26 24.31 27.72 12.00 Mintz++ 25.17 25.87 25.51 10.94
Semi MIML 30.02 26.21 27.98 12.31 MIML 28.06 28.64 28.35 11.74
Guided DS 31.9 32.46 32.19 16.1
1
Figure 2: Performance of Guided DS on KBP task compared to a) baselines: MaxEnt, DS+upsampling,
Semi-MIML (Min et al, 2013) b) state-of-art models: Mintz++ (Mintz et al, 2009), MultiR (Hoffmann
et al, 2011), MIML (Surdeanu et al, 2012)
Our baselines: 1) MaxEnt is a supervised maxi-
mum entropy baseline trained on a human-labeled
data; 2) DS+upsampling is an upsampling ex-
periment, where MIML was trained on a mix of
a distantly-labeled and human-labeled data; 3)
Semi-MIML is a recent semi-supervised exten-
sion. We also compare Guided DS with three
state-of-the-art models: 1) MultiR and 2) MIML
are two distant supervision models that support
multi-instance learning and overlapping relations;
3) Mintz++ is a single-instance learning algorithm
for distant supervision. The difference between
Guided DS and all other systems is significant
with p-value less than 0.05 according to a paired
t-test assuming a normal distribution.
5.3 Results
We scored our model against all 41 relations and
thus replicated the actual KBP evaluation. Figure
2 shows that our model consistently outperforms
all six algorithms at almost all recall levels and im-
proves the aximum F -score by more than 13.5%
relative to MIML (from 28.35% to 32.19%) as well
as increases the area under precision-recall curve
by more than 37% (from 11.74 to 16.1). Also,
Guided DS improves the overall recall by more
than 9% absolute (from 30.9% to 39.93%) at a
comparable level of precision (24.35% for MIML
vs 23.64% for Guided DS), while increases the
running time of MIML by only 3%. Thus, our
approach outperforms state-of-the-art model for
relation extraction using much less labeled data
that was used by Zhang et al, (2012) to outper-
form logistic regression baseline. Performance
of Guided DS also compares favorably with best
scored hand-coded systems for a similar task such
as Sun et al, (2011) system for KBP 2011, which
reports an F-score of 25.7%.
6 Conclusions and Future Work
We show that relation extractors trained with dis-
tant supervision can benefit significantly from a
small number of human labeled examples. We
propose a strategy to generate and select guide-
lines so that they are more generalized forms of
labeled instances. We show how to incorporate
these guidelines into an existing state-of-art model
for relation extraction. Our approach significantly
improves performance in practice and thus opens
up many opportunities for further research in RE
where only a very limited amount of labeled train-
ing data is available.
Acknowledgmen s
Supported by the Intelligence Advanced Research
Projects Activity ( IARPA) via Air Force Research
Laboratory (AFRL) contract number FA8650-10-
C-7058. The U.S. Government is authorized to
reproduce and distribute reprints for Governmen-
tal purposes notwithstanding any copyright anno-
tation thereon. The views and conclusions con-
tained herein are those of the authors and should
not be interpreted as necessarily representing the
official policies or endorsements, either expressed
or implied, of IARPA, AFRL, or the U.S. Govern-
ment.
736
References
Avrim Blum and Tom M. Mitchell. 1998. Combin-
ing labeled and unlabeled sata with co-training. In
Proceedings of the 11th Annual Conference on Com-
putational Learning Theory (COLT), pages 92?100.
Razvan C. Bunescu and Raymond J. Mooney. 2007.
Learning to extract relations from the web using
minimal supervision. In Proceedings of the 45th An-
nual Meeting of the Association for Computational
Linguistics (ACL).
Michael Collins and Yorav Singer. 1999. Unsuper-
vised models for named entity classification. Pro-
ceedings of the Conference on Empirical Methods
in Natural Language Processing (EMNLP-VLC). ,
Mark Craven and Johan Kumlien. 1999. Constructing
biological knowledge bases by extracting informa-
tion from text sources. In Proceedings of the Sev-
enth International Conference on Intelligent Systems
for Molecular Biology (ISMB), pages 77?86.
Oren Etzioni, Michele Banko, Stephen Soderland, and
Daniel S. Weld. 2008. Open information extrac-
tion from the web. Communications of the ACM,
51(12):68?74.
Raphael Hoffmann, Congle Zhang, and Daniel S.
Weld. 2010. Learning 5000 relational extractors.
In Proceedings of the 49th Annual Meetings of the
Association for Computational Linguistics (ACL),
pages 286?295.
Raphael Hoffmann, Congle Zhang, Xiao Ling,
Luke S. Zettlemoyer, and Daniel S. Weld. 2011.
Knowledge-based weak supervision for information
extraction of overlapping relations. In Proceedings
of the 49th Annual Meeting of the Association for
Computational Linguistics (ACL), pages 541?550.
Heng Ji and Ralph Grishman. 2011. Knowledge base
population: Successful approaches and challenges.
In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics (ACL),
pages 1148?1158.
Heng Ji, Ralph Grishman, and Hoa Trang Dang. 2011.
Overview of the TAC-2011 knowledge base popula-
tion track. In Text Analysis Conference Workshop.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41th Annual Meetings of the Association for Com-
putational Linguistics (ACL).
Percy Liang, Michael I.Jordan and Dan Klein. 2009.
Learning From Measurements in Exponential Fami-
lies. In Proceedings of the 26th Annual International
Conference on Machine Learning (ICML), pages =
641?648
Bonan Min, Ralph Grishman, Li Wan, Chang Wang,
and David Gondek. 2013. Distant supervision for
relation extraction with an incomplete knowledge
base. In Proceedings of the Conference of the North
American Chapter of the Association for Computa-
tional Linguistics (NAACL).
Mike Mintz, Steven Bills, Rion Snow, and Daniel Ju-
rafsky. 2009. Distant supervision for relation ex-
traction without labeled data. In Proceedigns of the
47th Annual Meeting of the Association for Compu-
tational Linguistics and the 4th International Joint
Conference on Natural Language Processing (ACL),
pages 1003?1011.
Ramesh Nallapati. 2004. Discriminative models for
information retrieval. In Proceedigns of the 27th An-
nual International ACM SIGIR Conference on Re-
search and Development in Information Retrieval
(SIGIR), pages 64?71.
Truc-Vien T. Nguyen and Alessandro Moschitti. 2011.
End-to-end relation extraction using distant super-
vision from external semantic repositories. In Pro-
ceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL), pages
277?282.
Kamal Paul Nigam. 2001. Using Unlabeled Data to
Improve Text Classification. Ph.D. thesis, School of
Computer Science, Carnegie Mellon University.
Sebastian Riedel, Limin Yao, and Andrew McCallum.
2010. Modeling relations and their mentions with-
out labeled text. In Proceedigns of the European
Conference on Machine Learning and Principles
and Practice of Knowledge Discovery in Databases
(ECML/PKDD), pages 148?163.
Alan Ritter, Luke Zettlemoyer, Mausam, and Oren Et-
zioni. 2013. Modeling missing data in distant su-
pervision for information extraction. Transactions
of the Association for Computational Linguistics.
Benjamin Roth, Tassilo Barth, Michael Wiegand, and
Dietrich Klakow 2013. A Survey of Noise Reduc-
tion Methods for Distant Supervision. In Proceed-
ings of Conference on Information and Knowledge
Management (CIKM-AKBC).
Benjamin Snyder and Regina Barzilay 2007.
Database-text alignment via structured multilabel
classification. In Proceedings of IJCAI.
Ang Sun, Ralph Grishman, Wei Xu, and Bonan Min.
2011. New york university 2011 system for kbp slot
filling. In Text Analysis Conference (TAC-KBP).
Mihai Surdeanu, J. Turmo, and A. Ageno. 2006. A
hybrid approach for the acquisition of information
extraction patterns. In Proceedings of the 11th Con-
ference of the European Chapter of the Associate
for Computational Linguistics Workshop on Adap-
tive Text Extraction and Mining (EACL).
Mihai Surdeanu, Sonal Gupta, John Bauer, David Mc-
Closky, Angel X. Chang, Valentin I. Spitkovsky,
and Christopher D.Manning. 2011. Stanford?s
737
Distantly-Supervised Slot-Filling System. In Pro-
ceedings of the Text Analysis Conference (TAC-
KBP).
Mihai Surdeanu, Julie Tibshirani, Ramesh Nallapati,
and Christopher D. Manning. 2012. Multi-instance
multi-label learning for relation extraction. In Pro-
ceedings of the 50th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL), pages
455?465.
Shingo Takamatsu, Issei Sato, and Hiroshi Nakagawa.
2012. Reducing wrong labels in distant supervi-
sion for relation extraction. In Proceedings of the
50th Annual Meeting of the Association for Compu-
tational Linguistics (ACL), pages 721?729.
Fei Wu and Daniel S. Weld. 2007. Autonomously se-
mantifying wikipedia. In Proceedings of the Inter-
national Conference on Information and Knowledge
Management (CIKM), pages 41?50.
Wei Xu, Raphael Hoffmann, Zhao Le, and Ralph Gr-
ishman. 2013. Filling knowledge base gaps for dis-
tant supervision of relation extraction. In Proceed-
ings of the 51th Annual Meeting of the Association
for Computational Linguistics (ACL).
David Yarowsky. 1995. Unsupervised word sense dis-
ambiguation rivaling supervised methods. In Pro-
ceedings of the 33th Annual Meeting of the Associa-
tion for Computational Linguistics (ACL).
Ce Zhang, Feng Niu, Christopher R?e, and Jude Shav-
lik. 2012. Big data versus the crowd: Looking for
relationships in all the right places. In Proceedings
of the 50th Annual Meeting of the Association for
Computational Linguistics, pages 825?834. Associ-
ation for Computational Linguistics.
Guodong Zhou, Jian Su, Jie Zhang, and Min Zhang.
2005. Exploring various knowledge in relation ex-
traction. In Proceedings of the Annual Meeting
of the Association for Computational Linguistics
(ACL).
738
Proceedings of the Workshop on Language in Social Media (LASM 2013), pages 20?29,
Atlanta, Georgia, June 13 2013. c?2013 Association for Computational Linguistics
A Preliminary Study of Tweet Summarization using Information Extraction
Wei Xu, Ralph Grishman, Adam Meyers
Computer Science Department
New York University
New York, NY 10003, USA
{xuwei,grishman,meyers}@cs.nyu.edu
Alan Ritter
Computer Science and Engineering
University of Washington
Seattle, WA 98125, USA
aritter@cs.washington.edu
Abstract
Although the ideal length of summaries dif-
fers greatly from topic to topic on Twitter, pre-
vious work has only generated summaries of
a pre-fixed length. In this paper, we propose
an event-graph based method using informa-
tion extraction techniques that is able to cre-
ate summaries of variable length for different
topics. In particular, we extend the Pagerank-
like ranking algorithm from previous work to
partition event graphs and thereby detect fine-
grained aspects of the event to be summarized.
Our preliminary results show that summaries
created by our method are more concise and
news-worthy than SumBasic according to hu-
man judges. We also provide a brief survey of
datasets and evaluation design used in previ-
ous work to highlight the need of developing a
standard evaluation for automatic tweet sum-
marization task.
1 Introduction
Tweets contain a wide variety of useful information
from many perspectives about important events tak-
ing place in the world. The huge number of mes-
sages, many containing irrelevant and redundant in-
formation, quickly leads to a situation of informa-
tion overload. This motivates the need for automatic
summarization systems which can select a few mes-
sages for presentation to a user which cover the most
important information relating to the event without
redundancy and filter out irrelevant and personal in-
formation that is not of interest beyond the user?s
immediate social network.
Although there is much recent work focusing on
the task of multi-tweet summarization (Becker et al,
2011; Inouye and Kalita, 2011; Zubiaga et al, 2012;
Liu et al, 2011a; Takamura et al, 2011; Harabagiu
and Hickl, 2011; Wei et al, 2012), most previous
work relies only on surface lexical clues, redun-
dancy and social network specific signals (e.g. user
relationship), and little work has considered taking
limited advantage of information extraction tech-
niques (Harabagiu and Hickl, 2011) in generative
models. Because of the noise and redundancy in
social media posts, the performance of off-the-shelf
news-trained natural language process systems is de-
graded while simple term frequency is proven pow-
erful for summarizing tweets (Inouye and Kalita,
2011). A natural and interesting research question
is whether it is beneficial to extract named entities
and events in the tweets as has been shown for clas-
sic multi-document summarization (Li et al, 2006).
Recent progress on building NLP tools for Twitter
(Ritter et al, 2011; Gimpel et al, 2011; Liu et al,
2011b; Ritter et al, 2012; Liu et al, 2012) makes
it possible to investigate an approach to summariz-
ing Twitter events which is based on Information Ex-
traction techniques.
We investigate a graph-based approach which
leverages named entities, event phrases and their
connections across tweets. A similar idea has been
studied by Li et al (2006) to rank the salience
of event concepts in summarizing news articles.
However, the extreme redundancy and simplicity of
tweets allows us to explicitly split the event graph
into subcomponents that cover various aspects of the
initial event to be summarized to create comprehen-
20
Work Dataset (size of each clus-
ter)
System Output Evaluation Metrics
Inouye and
Kalita (2011)
trending topics (approxi-
mately 1500 tweets)
4 tweets ROUGE and human (over-
all quality comparing to
human summary)
Sharifi et al
(2010)
same as above 1 tweet same as above
Rosa et al
(2011)
segmented hashtag top-
ics by LDA and k-means
clustering (average 410
tweets)
1, 5, 10 tweets Precision@k (relevance to
topic)
Harabagiu and
Hickl (2011)
real-word event topics (a
minimum of 2500 tweets)
top tweets until a limit of
250 words was reached
human (coverage and co-
herence)
Liu et al
(2011a)
general topics and hash-
tag topics (average 1.7k
tweets)
same lengths as of the
human summary, vary
for each topic (about 2 or
3 tweets)
ROUGE and human (con-
tent coverage, grammat-
icality, non-redundancy,
referential clarity, focus)
Wei et al
(2012)
segmented hashtag top-
ics according to burstiness
(average 10k tweets)
10 tweets ROUGE, Precison/Recall
(good readability and rich
content)
Takamura et al
(2011)
specific soccer games
(2.8k - 5.2k tweets)
same lengths as the hu-
man summary, vary for
each topic (26 - 41
tweets)
ROUGE (considering
only content words)
Chakrabarti and
Punera (2011)
specific football games
(1.8k tweets)
10 - 70 tweets Precision@k (relevance to
topic)
Table 1: Summary of datasets and evaluation metrics used in several previous work on tweet summarization
sive and non-redundant summaries. Our work is the
first to use a Pagerank-like algorithm for graph parti-
tioning and ranking in the context of summarization,
and the first to generate tweet summaries of variable
length which is particularly important for tweet sum-
marization. Unlike news articles, the amount of in-
formation in a set of topically clustered tweets varies
greatly, from very repetitive to very discrete. For ex-
ample, the tweets about one album release can be
more or less paraphrases, while those about another
album by a popular singer may involve rumors and
release events etc. In the human study conducted by
Inouye and Kalita (2011), annotators strongly prefer
different numbers of tweets in a summary for dif-
ferent topics. However, most of the previous work
produced summaries of a pre-fixed length and has
no evaluation on conciseness. Liu et al (2011a)
and Takamura et al (2011) also noticed the ideal
length of summaries can be very different from topic
to topic, and had to use the length of human refer-
ence summaries to decide the length of system out-
puts, information which is not available in practice.
In contrast, we developed a system that is capable
of detecting fine-grained sub-events and generating
summaries with the proper number of representative
tweets accordingly for different topics.
Our experimental results show that with informa-
tion extraction it is possible to create more mean-
ingful and concise summaries. Tweets that contain
real-world events are usually more informative and
readable. Event-based summarization is especially
beneficial in this situation due to the fact that tweets
are short and self-contained with simple discourse
structure. The boundary of 140 characters makes it
efficient to extract semi-structured events with shal-
low natural language processing techniques and re-
21
Tweets (Date Created) Named Entity Event Phrases Date Mentioned
Nooooo.. Season premiere of Doctor Who is on
Sept 1 world wide and we?ll be at World Con
(8/22/2012)
doctor who,
world con
season, is on,
premiere
sept 1
(9/1/2012)
guess what I DON?T get to do tomorrow!
WATCH DOCTOR WHO (8/31/2012)
doctor who watch tomorrow
(9/1/2012)
As I missed it on Saturday, I?m now catching up
on Doctor Who (9/4/2012)
doctor who missed,
catching up
saturday
(9/1/2012)
Rumour: Nokia could announce two WP8 de-
vices on September 5 http://t.co/yZUwDFLV (via
@mobigyaan)
nokia, wp8 announce september 5
(9/5/2012)
Verizon and Motorola won?t let Nokia have all
the fun ; scheduling September 5th in New York
http://t.co/qbBlYnSl (8/19/2012)
nokia, verizon,
motorola,
new york
scheduling september 5th
(9/5/2012)
Don?t know if it?s excitement or rooting for the
underdog, but I am genuinely excited for Nokia
come Sept 5: http://t.co/UhV5SUMP (8/7/2012)
nokia rooting,
excited
sept 5
(9/5/2012)
Table 2: Event-related information extracted from tweets
duces the complexity of the relationship (or no re-
lationship) between events according to their co-
occurrence, resulting in differences in constructing
event graphs from previous work in news domain
(Li et al, 2006).
2 Issues in Current Research on Tweet
Summarization
The most serious problem in tweet summarization
is that there is no standard dataset, and conse-
quently no standard evaluation methodology. Al-
though there are more than a dozen recent works on
social media summarization, astonishingly, almost
each research group used a different dataset and a
different experiment setup. This is largely attributed
to the difficulty of defining the right granularity of a
topic in Twitter. In Table 1, we summarize the exper-
iment designs of several selective works. Regardless
of the differences, researchers generally agreed on :
? clustering tweets topically and temporally
? generating either a very short summary for a
focused topic or a long summary for large-size
clusters
? difficulty and necessity to generate summaries
of variable length for different topics
Although the need of variable-length summaries
have been raised in previous work, none has pro-
vide a good solution (Liu et al, 2011a; Takamura
et al, 2011; Inouye and Kalita, 2011). In this pa-
per, our focus is study the feasibility of generating
concise summaries of variable length and improv-
ing meaningfulness by using information extraction
techniques. We hope this study can provide new in-
sights on the task and help in developing a standard
evaluation in the future.
3 Approach
We first extract event information including named
entities and event phrases from tweets and construct
event graphs that represent the relationship between
them. We then rank and partition the events using
PageRank-like algorithms, and create summaries of
variable length for different topics.
3.1 Event Extraction from Tweets
As a first step towards summarizing popular events
discussed on Twitter, we need a way to identify
events from Tweets. We utilize several natural lan-
guage processing tools that specially developed for
noisy text to extract text phrases that bear essential
event information, including named entities (Ritter
et al, 2011), event-referring phrases (Ritter et al,
22
2012) and temporal expressions (Mani and Wilson,
2000). Both the named entity and event taggers uti-
lize Conditional Random Fields models (Lafferty,
2001) trained on annotated data, while the temporal
expression resolver uses a mix of hand-crafted and
machine-learned rules. Example event information
extracted from Tweets are presented in Table 2.
The self-contained nature of tweets allows effi-
cient extraction of event information without deep
analysis (e.g. co-reference resolution). On the other
hand, individual tweets are also very terse, often
lacking sufficient context to access the importance
of events. It is crucial to exploit the highly redun-
dancy in Twitter. Closely following previous work
by Ritter et al (2012), we group together sets of
topically and temporally related tweets, which men-
tion the same named entity and a temporal refer-
ence resolved to the same unique calendar date. We
also employ a statistical significance test to measure
strength of association between each named entity
and date, and thereby identify important events dis-
cussed widely among users with a specific focus,
such as the release of a new iPhone as opposed to in-
dividual users discussing everyday events involving
their phones. By discarding frequent but insignifi-
cant events, we can produce more meaningful sum-
maries about popular real-world events.
3.2 Event Graphs
Since tweets have simple discourse and are self-
contained, it is a reasonable assumption that named
entities and event phrases that co-occurred together
in a single tweet are very likely related. Given a col-
lection of tweets, we represent such connections by
a weighted undirected graph :
? Nodes: named entities and event phrases are
represented by nodes and treated indifferently.
? Edges: two nodes are connected by an undi-
rected edge if they co-occurred in k tweets, and
the weight of edge is k.
We find it helpful to merge named entities and
event phrases that have lexical overlap if they are fre-
quent but not the topic of the tweet cluster. For ex-
ample, ?bbc?, ?radio 1?, ?bbc radio 1? are combined
together in a set of tweets about a band. Figure 1
shows a very small toy example of event graph. In
the experiments of this paper, we also exclude the
edges with k < 2 to reduce noise in the data and
calculation cost.
Figure 1: A toy event graph example built from the three
sentences of the event ?Nokia - 9/5/2012? in Table 2
3.3 Event Ranking and Partitioning
Graph-based ranking algorithms are widely used in
automatic summarization to decide salience of con-
cepts or sentences based on global information re-
cursively drawn from the entire graph. We adapt the
PageRank-like algorithm used in TextRank (Mihal-
cea and Tarau, 2004) that takes into account edge
weights when computing the score associated with a
vertex in the graph.
Formally, let G = (V,E) be a undirected graph
with the set of vertices V and set of edges E, whereE is a subset of V ? V . For a given vertex Vi, letAd(Vi) be the set of vertices that adjacent to it. The
weight of the edge between Vi and Vj is denoted aswij , and wij = wji. The score of a vertex Vi is
defined as follows:S(Vi) = (1  d) + d? X
Vj2Ad(Vi)
wij ? S(Vj)P
Vk2Ad(Vj) wjk
where d is a damping factor that is usually set to 0.85
(Brin and Page, 1998), and this is the value we are
also using in our implementation.
23
Starting from arbitrary values assigned to each
node in the graph, the computation iterates until con-
vergence. Note that the final salience score of each
node is not affected by the choice of the initial val-
ues assigned to each node in the graph, but rather the
weights of edges.
In previous work computed scores are then used
directly to select text fractions for summaries (Li et
al., 2006). However, the redundancy and simplic-
ity of tweets allow further exploration into sub-event
detection by graph partitioning. The intuition is that
the correlations between named entities and event
phrases within same sub-events are much stronger
than between sub-events. This phenomena is more
obvious and clear in tweet than in news articles,
where events are more diverse and complicated re-
lated to each other given lengthy context.
As theoretically studied in local partitioning prob-
lem (Andersen et al, 2006), a good partition of the
graph can be obtained by separating high ranked ver-
tices from low ranked vertices, if the nodes in the
graph have ranks that are distinguishable. Utilizing
a similar idea, we show that a simple greedy algo-
rithm is efficient to find important sub-events and
generate useful summaries in our tasks. As shown
in Figure 2 and 3, the high ranked nodes (whose
scores are greater than 1, the average score of all
nodes in the graph) in tweet event graphs show the
divisions within a topic. We search for strongly con-
nected sub-graphs, as gauged by parameter ?, from
the highest ranked node to lower ranked ones.The
proportion of tweets in a set that are related to a
sub-event is then estimated according to the ratio be-
tween the sum of node scores in the sub-graph ver-
sus the entire graph. We select one tweet for each
sub-event that best covers the related nodes with the
highest sum of node scores normalized by length as
summaries. By adding a cutoff (parameter  ) on
proportion of sub-event required to be included into
summaries, we can produce summaries with the ap-
propriate length according to the diversity of infor-
mation in a set of tweets.
In Figure 2, 3 and 4, the named entity which is
also the topic of tweet cluster is omitted since it is
connected with every node in the event graph. The
size of node represents the salience score, while the
shorter, straighter and more vertical the edge is, the
higher its weight. The nodes with rectangle shapes
Algorithm 1 Find important sub-events
Require: Ranked event graph G = (V,E), the
named entity V0 which is the topic of event
cluster, parameters ? and   that can be set
towards user preference over development data
1: Initialize the pool of high ranked nodesV?  {Vi|8Vi 2 V, S(Vi) > 1}   V0 and the
total weight W  PVi2V? S(Vi)
2: while V? 6= ; do
3: Pop the highest ranked node Vm from V?
4: Put Vm to a temporary sub-event e  {Vm}
5: for all Vn in V? do
6: if wmn/w0m > ? and w0n/w0m > ?
then
7: e  e [ {Vn}
8: end if
9: end for
10: We  PVi2e S(Vi)
11: if We/W >   then
12: Successfully find a sub-event e
13: Remove all nodes in e from V?
14: end if
15: end while
are named entities, while round shaped ones are
event phrases. Note that in most cases, sub-events
correspond to connected components in the event
graph of high ranked nodes as in Figure 2 and 3.
However, our simple greedy algorithm also allows
multiple sub-events for a single connected compo-
nent that can not be covered by one tweet in the
summary. For example, in Figure 4, two sub-eventse1 = {sell, delete, start, payment} and e2 =
{facebook, share user data, privacy policy, debut}
are chosen to accommodate the complex event.
4 Experiments
4.1 Data
We gathered tweets over a 4-month period spanning
November 2012 to February 2013 using the Twitter
Streaming API. As described in more details in pre-
vious work on Twitter event extraction by Ritter et
al. (2012), we grouped together all tweets which
mention the same named entity (recognized using
24
Figure 2: Event graph of ?Google - 1/16/2013?, an example of event cluster with multiple focuses
Figure 3: Event graph of ?Instagram - 1/16/2013?, an example of event cluster with a single but complex focus
25
Figure 4: Event graph of ?West Ham - 1/16/2013?, an
example of event cluster with a single focus
a Twitter specific name entity tagger1) and a refer-
ence to the same unique calendar date (resolved us-
ing a temporal expression processor (Mani and Wil-
son, 2000)). Tweets published during the whole pe-
riod are aggregated together to find top events that
happen on each calendar day. We applied the G2
test for statistical significance (Dunning, 1993) to
rank the event clusters, considering the corpus fre-
quency of the named entity, the number of times the
date has been mentioned, and the number of tweets
which mention both together. We randomly picked
the events of one day for human evaluation, that is
the day of January 16, 2013 with 38 events and an
average of 465 tweets per event cluster.
For each cluster, our systems produce two ver-
sions of summaries, one with a fixed number (set
to 3) of tweets and another one with a flexible num-
ber (vary from 1 to 4) of tweets. Both ? and   are
set to 0.1 in our implementation. All parameters are
set experimentally over a small development dataset
consisting of 10 events in Twitter data of September
2012.
1
https://github.com/aritter/twitter_nlp
4.2 Baseline
SumBasic (Vanderwende et al, 2007) is a simple
and effective summarization approach based on term
frequency, which we use as our baseline. It uses
word probabilities with an update function to avoid
redundancy to select sentences or posts in a social
media setting. It is shown to outperform three other
well-known multi-document summarization meth-
ods, namely LexRank (Erkan and Radev, 2004),
TextRank (Mihalcea and Tarau, 2004) and MEAD
(Radev et al, 2004) on tweets in (Inouye and Kalita,
2011), possibly because that the relationship be-
tween tweets is much simpler than between sen-
tences in news articles and can be well captured by
simple frequency methods. The improvement over
the LexRank model on tweets is gained by consid-
ering the number of retweets and influential users is
another side-proof (Wei et al, 2012) of the effective-
ness of frequency.
EventRank?Flexible EventRank?Fixed SumBasic
Annotator 1
0
1
2
3
4
5 compactnesscompletenessoverall
EventRank?Flexible EventRank?Fixed SumBasic
Annotator 2
0
1
2
3
4
5 compactnesscompletenessoverall
Figure 5: human judgments evaluating tweet summariza-
tion systems
26
Event System Summary
- Google ?s home page is a Zamboni game in celebration of Frank Zam-
boni ?s birthday January 16 #GameOn
EventRank
(Flexible)
- Today social , Tomorrow Google ! Facebook Has Publicly Redefined
Itself As A Search Company http://t.co/dAevB2V0 via @sai
Google
1/16/2013
- Orange says has it has forced Google to pay for traffic . The Head of
the Orange said on Wednesday it had ... http://t.co/dOqAHhWi
- Tomorrow?s Google doodle is going to be a Zamboni! I may have to
take a vacation day.
SumBasic - the game on google today reminds me of hockey #tooexcited #saturday
- The fact that I was soooo involved in that google doodle game says
something about this Wednesday #TGIW You should try it!
EventRank
(Flexible)
- So Instagram can sell your pictures to advertisers without u knowing
starting January 16th I?m bout to delete my instagram !
- Instagram debuts new privacy policy , set to share user data with Face-
book beginning January 16
Instagram
1/16/2013
- Instagram will have the rights to sell your photos to Advertisers as of
jan 16
SumBasic - Over for Instagram on January 16th
- Instagram says it now has the right to sell your photos unless you delete
your account by January 16th http://t.co/tsjic6yA
EventRank
(Flexible)
- RT @Bassa_Mufc : Wayne Rooney and Nani will feature in the FA Cup
replay with West Ham on Wednesday - Sir Alex Ferguson
West Ham
1/16/2013
- Wayne Rooney could be back to face West Ham in next Wednesday?s
FA Cup replay at Old Trafford. #BPL
SumBasic - Tomorrow night come on West Ham lol
- Nani?s fit abd WILL play tomorrow against West Ham! Sir Alex con-
firmed :)
Table 3: Event-related information extracted from tweets
4.3 Preliminary Results
We performed a human evaluation in which two an-
notators were asked to rate the system on a five-
point scale (1=very poor, 5=very good) for com-
pleteness and compactness. Completeness refers to
how well the summary cover the important content
in the tweets. Compactness refers to how much
meaningful and non-redundant information is in the
summary. Because the tweets were collected ac-
cording to information extraction results and ranked
by salience, the readability of summaries generated
by different systems are generally very good. The
top 38 events of January 16, 2013 are used as test
set. The aggregate results of the human evaluation
are displayed in Figure 5. Agreement between an-
notators measured using Pearson?s Correlation Co-
efficient is 0.59, 0.62, 0.62 respectively for compact-
ness, completeness and overall judgements.
Results suggest that the models described in this
paper produce more satisfactory results as the base-
line approaches. The improvement of EventRank-
Flexible over SumBasic is significant (two-tailedp < 0.05) for all three metrics according to stu-
dent?s t test. Example summaries of the events in
Figure 2, 3 and 4 are presented respectively in Table
3. The advantages of our method are the follow-
ing: 1) it finds important facts of real-world events
2) it prefers tweets with good readability 3) it in-
cludes the right amount of information with diversity
and without redundancy. For example, our system
picked only one tweet about ?West Ham -1/16/2013?
that convey the same message as the three tweets to-
27
gether of the baseline system. For another example,
among the tweets about Google around 1/16/2013,
users intensively talk about the Google doodle game
with a very wide range of words creatively, giving
word-based methods a hard time to pick up the di-
verse and essential event information that is less fre-
quent.
5 Conclusions and Future Work
We present an initial study of feasibility to gen-
erate compact summaries of variable lengths for
tweet summarization by extending a Pagerank-like
algorithm to partition event graphs. The evalua-
tion shows that information extraction techniques
are helpful to generate news-worthy summaries of
good readability from tweets.
In the future, we are interested in improving the
approach and evaluation, studying automatic met-
rics to evaluate summarization of variable length
and getting involved in developing a standard eval-
uation for tweet summarization tasks. We wonder
whether other graph partitioning algorithms may im-
prove the performance. We also consider extending
this graph-based approach to disambiguate named
entities or resolve event coreference in Twitter data.
Another direction of future work is to extend the
proposed approach to different data, for example,
temporal-aware clustered tweets etc.
Acknowledgments
This research was supported in part by NSF grant
IIS-0803481, ONR grant N00014-08-1-0431, and
DARPA contract FA8750- 09-C-0179, and carried
out at the University of Washington?s Turing Center.
We thank Mausam and Oren Etzioni of University
of Washington, Maria Pershina of New York Univer-
sity for their advice.
References
Reid Andersen, Fan Chung, and Kevin Lang. 2006.
Local graph partitioning using pagerank vectors. In
Foundations of Computer Science, 2006. FOCS?06.
47th Annual IEEE Symposium on, pages 475?486.
IEEE.
Hila Becker, Mor Naaman, and Luis Gravano. 2011. Se-
lecting quality twitter content for events. In Proceed-
ings of the Fifth International AAAI Conference onWe-
blogs and Social Media (ICWSM?11).
Sergey Brin and Lawrence Page. 1998. The anatomy of a
large-scale hypertextual web search engine. Computer
networks and ISDN systems, 30(1):107?117.
Deepayan Chakrabarti and Kunal Punera. 2011. Event
summarization using tweets. In Proceedings of the
Fifth International AAAI Conference on Weblogs and
Social Media, pages 66?73.
Ted Dunning. 1993. Accurate methods for the statistics
of surprise and coincidence. Computational linguis-
tics, 19(1):61?74.
G?nes Erkan and Dragomir R. Radev. 2004. Lexrank:
Graph-based lexical centrality as salience in text sum-
marization. J. Artif. Intell. Res. (JAIR), 22:457?479.
Kevin Gimpel, Nathan Schneider, Brendan O?Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein, Michael
Heilman, Dani Yogatama, Jeffrey Flanigan, and
Noah A. Smith. 2011. Part-of-speech tagging for twit-
ter: Annotation, features, and experiments. In ACL.
Sanda Harabagiu and Andrew Hickl. 2011. Relevance
modeling for microblog summarization. In Fifth In-
ternational AAAI Conference on Weblogs and Social
Media.
David Inouye and Jugal K Kalita. 2011. Comparing twit-
ter summarization algorithms for multiple post sum-
maries. In Privacy, security, risk and trust (passat),
2011 ieee third international conference on and 2011
ieee third international conference on social comput-
ing (socialcom), pages 298?306. IEEE.
John Lafferty. 2001. Conditional random fields: Proba-
bilistic models for segmenting and labeling sequence
data. pages 282?289. Morgan Kaufmann.
Wenjie Li, Wei Xu, Chunfa Yuan, Mingli Wu, and Qin
Lu. 2006. Extractive summarization using inter- and
intra- event relevance. In Proceedings of the 21st In-
ternational Conference on Computational Linguistics
and the 44th annual meeting of the Association for
Computational Linguistics, ACL-44, pages 369?376,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Fei Liu, Yang Liu, and Fuliang Weng. 2011a. Why is
?sxsw? trending? exploring multiple text sources for
twitter topic summarization. ACL HLT 2011, page 66.
Xiaohua Liu, Shaodian Zhang, Furu Wei, and Ming
Zhou. 2011b. Recognizing named entities in tweets.
In ACL.
Xiaohua Liu, Furu Wei, Ming Zhou, et al 2012. Quick-
view: Nlp-based tweet search. In Proceedings of the
ACL 2012 System Demonstrations, pages 13?18. As-
sociation for Computational Linguistics.
Inderjeet Mani and GeorgeWilson. 2000. Robust tempo-
ral processing of news. In Proceedings of the 38th An-
28
nual Meeting on Association for Computational Lin-
guistics, ACL ?00, pages 69?76, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Rada Mihalcea and Paul Tarau. 2004. Textrank: Bring-
ing order into texts. In Proceedings of EMNLP, vol-
ume 4, pages 404?411. Barcelona, Spain.
Dragomir Radev, Timothy Allison, Sasha Blair-
Goldensohn, John Blitzer, Arda Celebi, Stanko
Dimitrov, Elliott Drabek, Ali Hakim, Wai Lam, Danyu
Liu, et al 2004. Mead-a platform for multidocument
multilingual text summarization. In Proceedings of
LREC, volume 2004.
Alan Ritter, Sam Clark, Mausam, and Oren Etzioni.
2011. Named entity recognition in tweets: An experi-
mental study.
Alan Ritter, Mausam, Oren Etzioni, and Sam Clark.
2012. Open domain event extraction from twitter. In
KDD, pages 1104?1112. ACM.
Kevin Dela Rosa, Rushin Shah, Bo Lin, Anatole Gersh-
man, and Robert Frederking. 2011. Topical clustering
of tweets. Proceedings of the ACM SIGIR: SWSM.
Beaux Sharifi, Mark-Anthony Hutton, and Jugal K
Kalita. 2010. Experiments in microblog summariza-
tion. In Proc. of IEEE Second International Confer-
ence on Social Computing.
Hiroya Takamura, Hikaru Yokono, and Manabu Oku-
mura. 2011. Summarizing a document stream. Ad-
vances in Information Retrieval, pages 177?188.
Lucy Vanderwende, Hisami Suzuki, Chris Brockett, and
Ani Nenkova. 2007. Beyond sumbasic: Task-focused
summarization with sentence simplification and lex-
ical expansion. Information Processing & Manage-
ment, 43(6):1606?1618.
Furu Wei, Ming Zhou, and Heung-Yeung Shum. 2012.
Twitter topic summarization by ranking tweets using
social influence and content quality. In COLING.
Arkaitz Zubiaga, Damiano Spina, Enrique Amig?, and
Julio Gonzalo. 2012. Towards real-time summariza-
tion of scheduled events from twitter streams. In Pro-
ceedings of the 23rd ACM conference on Hypertext
and social media, pages 319?320. ACM.
29
Proceedings of the 6th Workshop on Building and Using Comparable Corpora, pages 121?128,
Sofia, Bulgaria, August 8, 2013. c?2013 Association for Computational Linguistics
Gathering and Generating Paraphrases from Twitter
with Application to Normalization
Wei Xu+ Alan Ritter? Ralph Grishman+
+New York University, New York, NY, USA
{xuwei, grishman}@cs.nyu.edu
?University of Washington, Seattle, WA, USA
aritter@cs.washington.edu
Abstract
We present a new and unique para-
phrase resource, which contains meaning-
preserving transformations between infor-
mal user-generated text. Sentential para-
phrases are extracted from a compara-
ble corpus of temporally and topically
related messages on Twitter which of-
ten express semantically identical infor-
mation through distinct surface forms. We
demonstrate the utility of this new re-
source on the task of paraphrasing and
normalizing noisy text, showing improve-
ment over several state-of-the-art para-
phrase and normalization systems 1.
1 Introduction
Social media services provide a massive amount
of valuable information and demand NLP tools
specifically developed to accommodate their noisy
style. So far not much success has been reported
on a key NLP technology on social media data:
paraphrasing. Paraphrases are alternative ways to
express the same meaning in the same language
and commonly employed to improve the perfor-
mance of many other NLP applications (Madnani
and Dorr, 2010). In the case of Twitter, Petrovic? et
al. (2012) showed improvements on first story de-
tection by using paraphrases extracted from Word-
Net.
Learning paraphrases from tweets could be es-
pecially beneficial. First, the high level of in-
formation redundancy in Twitter provides a good
opportunity to collect many different expressions.
Second, tweets contain many kinds of paraphrases
not available elsewhere including typos, abbre-
viations, ungrammatical expressions and slang,
1Our Twitter paraphrase models are available
online at https://github.com/cocoxu/
twitterparaphrase/
which can be particularly valuable for many appli-
cations, such as phrase-based text normalization
(Kaufmann and Kalita, 2010) and correction of
writing mistakes (Gamon et al, 2008), given the
difficulty of acquiring annotated data. Paraphrase
models that are derived from microblog data could
be useful to improve other NLP tasks on noisy
user-generated text and help users to interpret a
large range of up-to-date abbreviations (e.g. dlt ?
Doritos Locos Taco) and native expressions (e.g.
oh my god ? {oh my goodness | oh my gosh | oh
my gawd | oh my jesus}) etc.
This paper presents the first investigation into
automatically collecting a large paraphrase cor-
pus of tweets, which can be used for building
paraphrase systems adapted to Twitter using tech-
niques from statistical machine translation (SMT).
We show experimental results demonstrating the
benefits of an in-domain parallel corpus when
paraphrasing tweets. In addition, our paraphrase
models can be applied to the task of normalizing
noisy text where we show improvements over the
state-of-the-art.
Relevant previous work has extracted sentence-
level paraphrases from news corpora (Dolan et
al., 2004; Barzilay and Lee, 2003; Quirk et al,
2004). Paraphrases gathered from noisy user-
generated text on Twitter have unique character-
istics which make this comparable corpus a valu-
able new resource for mining sentence-level para-
phrases. Twitter also has much less context than
news articles and much more diverse content, thus
posing new challenges to control the noise in min-
ing paraphrases while retaining the desired super-
ficial dissimilarity.
2 Related Work
There are several key strands of related work, in-
cluding previous work on gathering parallel mono-
lingual text from topically clustered news articles,
normalizing noisy Twitter text using word-based
121
models, and applying out-of-domain paraphrase
systems to improve NLP tasks in Twitter.
On the observation of the lack of a large para-
phrase corpus, Chen and Dolan (2011) have re-
sorted to crowdsourcing to collect paraphrases by
asking multiple independent users for descriptions
of the same short video. As we show in ?5, how-
ever, this data is very different from Twitter, so
paraphrase systems trained on in-domain Twitter
paraphrases tend to perform much better.
The task of paraphrasing tweets is also related
to previous work on normalizing noisy Twitter text
(Han and Baldwin, 2011; Han et al, 2012; Liu
et al, 2012). Most previous work on normaliza-
tion has applied word-based models. While there
are challenges in applying Twitter paraphrase sys-
tems to the task of normalization, access to paral-
lel text allows us to make phrase-based transfor-
mations to the input string rather than relying on
word-to-word mappings (for more details see ?4).
Also relevant is recent work on collecting bilin-
gual parallel data from Twitter (Jehl et al, 2012;
Ling et al, 2013). In contrast, we focus on mono-
lingual paraphrases rather than multilingual trans-
lations.
Finally we highlight recent work on apply-
ing out-of-domain paraphrase systems to improve
performance at first story detection in Twitter
(Petrovic? et al, 2012). By building better para-
phrase models adapted to Twitter, it should be pos-
sible to improve performance at such tasks, which
benefit from paraphrasing Tweets.
3 Gathering A Parallel Tweet Corpus
There is a huge amount of redundant information
on Twitter. When significant events take place in
the world, many people go to Twitter to share,
comment and discuss them. Among tweets on
the same topic, many will convey similar mean-
ing using widely divergent expressions. Whereas
researchers have exploited multiple news reports
about the same event for paraphrase acquisition
(Dolan et al, 2004), Twitter contains more vari-
ety in terms of both language forms and types of
events, and requires different treatment due to its
unique characteristics.
As described in ?3.1, our approach first identi-
fies tweets which refer to the same popular event
as those which mention a unique named entity and
date, then aligns tweets within each event to con-
struct a parallel corpus. To generate paraphrases,
we apply a typical phrase-based statistical MT
pipeline, performing word alignment on the paral-
lel data using GIZA++ (Och and Ney, 2003), then
extracting phrase pairs and performing decoding
uses Moses (Koehn et al, 2007).
3.1 Extracting Events from Tweets
As a first step towards extracting paraphrases from
popular events discussed on Twitter, we need a
way to identify Tweets which mention the same
event. To do this we follow previous work by Rit-
ter et al (2012), extracting named entities and
resolving temporal expressions (for example ?to-
morrow? or ?on Wednesday?). Because tweets are
compact and self-contained, those which mention
the same named entity and date are likely to refer-
ence the same event. We also employ a statistical
significance test to measure strength of association
between each named entity and date, and thereby
identify important events discussed widely among
users with a specific focus, such as the release of
a new iPhone as opposed to individual users dis-
cussing everyday events involving their phones.
By gathering tweets based on popular real-world
events, we can efficiently extract pairwise para-
phrases within a small group of closely related
tweets, rather than exploring every pair of tweets
in a large corpus. By discarding frequent but in-
significant events, such as ?I like my iPhone? and
?I like broke my iPhone?, we can reduce noise
and encourage diversity of paraphrases by requir-
ing less lexical overlap. Example events identified
using this procedure are presented in Table 1.
3.2 Extracting Paraphrases Within Events
Twitter users are likely to express the same mean-
ing in relation to an important event, however not
every pair of tweets mentioning the same event
will have the same meaning. People may have
opposite opinions and complicated events such as
presidential elections can have many aspects. To
build a useful monolingual paraphrase corpus, we
need some additional filtering to prevent unrelated
sentence pairs.
If two tweets mention the same event and also
share many words in common, they are very likely
to be paraphrases. We use the Jaccard distance
metric (Jaccard, 1912) to identify pairs of sen-
tences within an event that are similar at the lexical
level. Since tweets are extremely short with little
context and include a broad range of topics, using
only surface similarity is prone to unrelated sen-
122
Entity/Date Example Tweets
Vote for Obama on November
6th!
Obama
11/6/2012
OBAMA is #winning his 2nd
term on November 6th 2012.
November 6th we will re-elect
Obama!!
Bought movie tickets to see
James Bond tomorrow. I?m a
big #007 fan!
James Bond
11/9/2012
Who wants to go with me and
see that new James Bond movie
tomorrow?
I wanna go see James Bond to-
morrow
North Korea Announces De-
cember 29 Launch Date for
Rocket
North Korea
12/29/2012
Pyongyang reschedules launch
to December 29 due to ?techni-
cal deficiency?
North Korea to extend rocket
launch period to December 29
Table 1: Example sentences taken from automat-
ically identified significant events extracted from
Twitter. Because many users express similar in-
formation when mentioning these events, there are
many opportunities for paraphrase.
tence pairs. The average sentence length is only
11.9 words in our Twitter corpus, compared to
18.6 words in newswire (Dolan et al, 2004) which
also contains additional document-level informa-
tion. Even after filtering tweets with both their
event cluster and lexical overlap, some unrelated
sentence pairs remain in the parallel corpus. For
example, names of two separate music venues in
the same city might be mismatched together if they
happen to have concerts on the same night that
people tweeted using a canonical phrasing like ?I
am going to a concert at in Austin tonight?.
4 Paraphrasing Tweets for
Normalization
Paraphrase models built from grammatical text are
not appropriate for the task of normalizing noisy
text. However, the unique characteristics of the
Twitter data allow our paraphrase models to in-
clude both normal and noisy language and conse-
quently translate between them. Our models have
a tendency to normalize because correct spellings
and grammar are most frequently used,2 but there
is still danger of introducing noise. For the pur-
poses of normalization, we therefore biased our
models using a language model built using text
taken from the New York Times which is used to
represent grammatical English.
Previous work on microblog normalization is
mostly limited to word-level adaptation or out-of-
domain annotated data. Our phrase-based mod-
els fill the gap left by previous studies by exploit-
ing a large, automatically curated, in-domain para-
phrase corpus.
Lexical normalization (Han and Baldwin, 2011)
only considers transforming an out-of-vocabulary
(OOV) word to its standard form, i.e. in-
vocabulary (IV) word. Beyond word-to-word con-
versions, our phrase-based model is also able to
handle the following types of errors without re-
quiring any annotated data:
Error type Ill form Standard
form
1-to-many everytime every time
incorrect IVs can?t want
for
can?t wait for
grammar I?m going a
movie
I?m going to
a movie
ambiguities 4 4 / 4th / for /
four
Kaufmann and Kalita (2010) explored machine
translation techniques for the normalization task
using an SMS corpus which was manually anno-
tated with grammatical paraphrases. Microblogs,
however, contain a much broader range of content
than SMS and have no in-domain annotated data
available. In addition, the ability to gather para-
phrases automatically opens up the possibility to
build normalization models from orders of mag-
nitude more data, and also to produce up-to-date
normalization models which capture new abbrevi-
ations and slang as they are invented.
5 Experiments
We evaluate our system and several baselines
at the task of paraphrasing Tweets using pre-
viously developed automatic evaluation metrics
which have been shown to have high correlation
with human judgments (Chen and Dolan, 2011).
2Even though misspellings and grammatical errors are
quite common, there is much more variety and less agree-
ment.
123
In addition, because no previous work has evalu-
ated these metrics in the context of noisy Twitter
data, we perform a human evaluation in which an-
notators are asked to choose which system gen-
erates the best paraphrase. Finally we evaluate
our phrase-based normalization system against a
state-of-the-art word-based normalizer developed
for Twitter (Han et al, 2012).
5.1 Paraphrasing Tweets
5.1.1 Data
Our paraphrase dataset is distilled from a large
corpus of tweets gathered over a one-year period
spanning November 2011 to October 2012 using
the Twitter Streaming API. Following Ritter et
al. (2012), we grouped together all tweets which
mention the same named entity (recognized using
a Twitter specific name entity tagger3) and a ref-
erence to the same unique calendar date (resolved
using a temporal expression processor (Mani and
Wilson, 2000)). Then we applied a statistical sig-
nificance test (the G test) to rank the events, which
considers the corpus frequency of the named en-
tity, the number of times the date has been men-
tioned, and the number of tweets which mention
both together. Altogether we collected more than
3 million tweets from the 50 top events of each day
according to the p-value from the statistical test,
with an average of 229 tweets per event cluster.
Each of these tweets was passed through a Twit-
ter tokenizer4 and a simple sentence splitter, which
also removes emoticons, URLs and most of the
hashtags and usernames. Hashtags and usernames
that were in the middle of sentences and might
be part of the text were kept. Within each event
cluster, redundant and short sentences (less than 3
words) were filtered out, and the remaining sen-
tences were paired together if their Jaccard simi-
larity was no less than 0.5. This resulted in a par-
allel corpus consisting of 4,008,946 sentence pairs
with 800,728 unique sentences.
We then trained paraphrase models by applying
a typical phrase-based statistical MT pipeline on
the parallel data, which uses GIZA++ for word
alignment and Moses for extracting phrase pairs,
training and decoding. We use a language model
trained on the 3 million collected tweets in the de-
coding process. The parameters are tuned over de-
3https://github.com/aritter/twitter_
nlp
4https://github.com/brendano/
tweetmotif
velopment data and the exact configuration are re-
leased together with the phrase table for system
replication.
Sentence alignment in comparable corpora is
more difficult than between direct translations
(Moore, 2002), and Twitter?s noisy style, short
context and broad range of content present ad-
ditional complications. Our automatically con-
structed parallel corpus contains some proportion
of unrelated sentence pairs and therefore does re-
sult in some unreasonable paraphrases. We prune
out unlikely phrase pairs using a technique pro-
posed by Johnson et al (2007) with their recom-
mended setting, which is based on the significance
testing of phrase pair co-occurrence in the parallel
corpus (Moore, 2004). We further prevent unrea-
sonable translations by adding additional entries
to the phrase table to ensure every phrase has an
option to remain unchanged during paraphrasing
and normalization. Without these noise reduction
steps, our system will produce paraphrases with
serious errors (e.g. change a person?s last name)
for 100 out of 200 test tweets in the evaluation in
?5.1.5.
At the same time, it is also important to promote
lexical dissimilarity in the paraphrase task. Fol-
lowing Ritter et. al. (2011) we add a lexical sim-
ilarity penalty to each phrase pair in our system,
in addition to the four basic components (transla-
tion model, distortion model, language model and
word penalty) in SMT.
5.1.2 Evaluation Details
The beauty of lexical similarity penalty is that it
gives control over the degree of paraphrasing by
adjusting its weight versus the other components.
Thus we can plot a BLEU-PINC curve to express
the tradeoff between semantic adequacy and lexi-
cal dissimilarity with the input, where BLUE (Pa-
pineni et al, 2002) and PINC (Chen and Dolan,
2011) are previously proposed automatic evalua-
tion metrics to measure respectively the two crite-
ria of paraphrase quality.
To compute these automatic evaluation met-
rics, we manually prepared a dataset of gold para-
phrases by tracking the trending topics on Twitter5
and gathering groups of paraphrases in November
2012. In total 20 sets of sentences were collected
and each set contains 5 different sentences that ex-
press the same meaning. Each sentence is used
5https://support.twitter.com/articles/
101125-faqs-about-twitter-s-trends
124
Input Output
Hostess is going outta biz hostess is going out of business
REPUBLICAN IMMIGRATION REFORM IS A
THING NOW
gop imigration law is a thing now
Freedom Writers will always be one of my fav
movies
freedom writers will forever be one of my favorite
movies
sources confirm that Phil Jackson has cancelled
all weekend plans and upcoming guest appear-
ances, will meet with LAL front office
source confirms that phil jackson has canceled all
weekend plans , upcomin guest appearances and
will meet with lakers front office
Table 2: Example paraphrases generated by our system on the test data.
once as input while other 4 sentences in the same
set serve as reference translation for automatic
evaluation of semantic adequacy using BLEU.
5.1.3 Baselines
We consider two state-of-the-art paraphrase sys-
tems as baselines, both of which are trained on
parallel corpora of aligned sentences. The first one
is trained on a large-scale corpus gathered by ask-
ing users of Amazon?s Mechanical Turk Service
(Snow et al, 2008) to write a one-sentence de-
scription of a short video clip (Chen and Dolan,
2011). We combined a phrase table and distor-
tion table extracted from this parallel corpus with
the same Twitter language model, applying the
Moses decoder to generate paraphrases. The ad-
ditional noise removal steps described in ?5.1.1
were found helpful for this model during devel-
opment and were therefore applied. The second
baseline uses the Microsoft Research paraphrase
tables that are automatically extracted from news
articles in combination with the Twitter language
model.6
5.1.4 Results
Figure 1 compares our system against both base-
lines, varying the lexical similarity penalty for
each system to generate BLEU-PINC curves.
Our system trained on automatically gathered
in-domain Twitter paraphrases achieves higher
BLEU at equivalent PINC for the entire length of
the curves. Table 2 shows some sample outputs of
our system on real Twitter data.
One novel feature of our approach, compared
to previous work on paraphrasing, is that it cap-
tures many slang terms, acronyms, abbreviations
and misspellings that are otherwise hard to learn.
6No distortion table or noisy removal process is applied
because the parallel corpus is not available.
lll ll l l l l
l l
l
l
l
l
l
l l
l
0 20 40 60
0
5
10
15
20
PINC
BLE
U
l OursVideoMSR
Figure 1: Results from automatic paraphrase eval-
uation. PINC measures n-gram dissimilarity from
the source sentence, whereas BLEU roughly mea-
sures n-gram similarity to the reference para-
phrases.
Several examples are shown in table 3. The rich
semantic redundancy in Twitter helps generate a
large variety of typical paraphrases as well (see an
example in table 4).
5.1.5 Human Evaluation
In addition to automatic evaluation, we also per-
formed a human evaluation in which annotators
were asked to pick which system generated the
best paraphrase. We used the same dataset of
200 tweets gathered for the automatic evaluation
and generated paraphrases using the 3 systems in
Figure 1 with the highest BLEU which achieve a
PINC of at least 40. The human annotators were
then asked to pick which of the 3 systems gener-
ated the best paraphrase using the criteria that it
should be both different from the original and also
125
Input Top-ranked Outputs
amped pumped
lemme kno let me know
bb bigbang, big brother
snl nbcsnl, saturday night live
apply 4 tix apply for tickets, ask for tickets,
applying for tickets
the boys one direction (a band, whose
members are often referred as
?the boys?), they, the boy, the
gys, the lads, my boys, the direc-
tion (can be used to refer to the
band ?one direction?), the onedi-
rection, our boys, our guys
oh my god oh my gosh, omfg, thank the
lord, omg, oh my lord, thank you
god, oh my jesus, oh god
can?t wait cant wait, cant wait, cannot wait,
i cannot wait, so excited, cnt
wait, i have to wait, i can?wait,
ready, so ready, so pumped, seri-
ously can?wait, really can?t wait
Table 3: Example paraphrases of noisy phrases
and slang commonly found on Twitter
Input Top-ranked Outputs
who want
to get a
beer
wants to get a beer, so who wants
to get a beer, who wants to go
get a beer, who wants to get beer,
who want to get a beer, trying to
get a beer, who wants to buy a
beer, who wants to get a drink,
who wants to get a rootbeer, who
trying to get a beer, who wants to
have a beer, who wants to order
a beer, i want to get a beer, who
wants to get me a beer, who else
wants to get a beer, who wants to
win a beer, anyone wants to get
a beer, who wanted to get a beer,
who wants to a beer, someone to
get a beer, who wants to receive a
beer, someone wants to get a beer
Table 4: Example paraphrases of a given sentence
?who want to get a beer?
Ours Video MSR
0
20
40
60
80
100
120
annotator 1annotator 2
Figure 2: Number of paraphrases (200 in total)
preferred by the annotators for each system
capture as much of the original meaning as pos-
sible. The annotators were asked to abstain from
picking one as the best in cases where there were
no changes to the input, or where the resulting
paraphrases totally lost the meaning.
Figure 2 displays the number of times each an-
notator picked each system?s output as the best.
Annotator 2 was somewhat more conservative
than annotator 1, choosing to abstain more fre-
quently and leading to lower overall frequencies,
however in both cases we see a clear advantage
from paraphrasing using in-domain models. As
a measure of inter-rater agreement, we computed
Cohen?s Kappa between the annotators judgment
as to whether the Twitter-trained system?s output
best. The value of Cohen?s Kappa in this case was
0.525.
5.2 Phrase-Based Normalization
Because Twitter contains both normal and noisy
language, with appropriate tuning, our models
have the capability to translate between these two
styles, e.g. paraphrasing into noisy style or nor-
malizing into standard language. Here we demon-
strate its capability to normalize tweets at the
sentence-level.
5.2.1 Baselines
Much effort has been devoted recently for devel-
oping normalization dictionaries for Microblogs.
One of the most competitive dictionaries avail-
able today is HB-dict+GHM-dict+S-dict used by
Han et al (2012), which combines a manually-
constructed Internet slang dictionary , a small
(Gouws et al, 2011) and a large automatically-
126
derived dictionary based on distributional and
string similarity. We evaluate two baselines using
this large dictionary consisting of 41181 words;
following Han et. al. (2012), one is a simple dic-
tionary look up. The other baseline uses the ma-
chinery of statistical machine translation using this
dictionary as a phrase table in combination with
Twitter and NYT language models.
5.2.2 System Details
Our base normalization system is the same as
the paraphrase model described in ?5.1.1, except
that the distortion model is turned off to exclude
reordering. We tuned the system towards cor-
rect spelling and grammar by adding a language
model built from all New York Times articles
written in 2008. We also filtered out the phrase
pairs which map from in-vocabulary to out-of-
vocabulary words. In addition, we integrated the
dictionaries by linear combination to increase the
coverage of phrase-based SMT model (Bisazza et
al., 2011).
5.2.3 Evaluation Details
We adopt the normalization dataset of Han and
Baldwin (2011), which was initially annotated
for the token-level normalization task, and which
we augmented with sentence-level annotations.
It contains 549 English messages sampled from
Twitter API from August to October, 2010.
5.2.4 Results
Normalization results are presented in figure 5.
Using only our phrase table extracted from Twit-
ter events we achieve poorer performance than the
state-of-the-art dictionary baseline, however we
find that by combining the normalization dictio-
nary of Han et. al. (2012) with our automatically
constructed phrase-table we are able to combine
the high coverage of the normalization dictionary
with the ability to perform phrase-level normaliza-
tions (e.g. ?outta? ? ?out of? and examples in
?4) achieving both higher PINC and BLEU than
the systems which rely exclusively on word-level
mappings. Our phrase table also contains many
words that are not covered by the dictionary (e.g.
?pts? ? ?points?, ?noms? ? ?nominations?).
6 Conclusions
We have presented the first approach to gather-
ing parallel monolingual text from Twitter, and
built the first in-domain models for paraphrasing
BLEU PINC
No-Change 60.00 0.0
SMT+TwitterLM 62.54 5.78
SMT+TwitterNYTLM 65.72 9.23
Dictionary 75.07 22.10
Dicionary+TwitterNYTLM 75.12 20.26
SMT+Dictionary+TwitterNYTLM 77.44 25.33
Table 5: Normalization performance
tweets. By paraphrasing using models trained
on in-domain data we showed significant per-
formance improvements over state-of-the-art out-
of-domain paraphrase systems as demonstrated
through automatic and human evaluations. We
showed that because tweets include both normal
and noisy language, paraphrase systems built from
Twitter can be fruitfully applied to the task of nor-
malizing noisy text, covering phrase-based nor-
malizations not handled by previous dictionary-
based normalization systems. We also make our
Twitter-tuned paraphrase models publicly avail-
able. For future work, we consider developing ad-
ditional methods to improve the accuracy of tweet
clustering and paraphrase pair selection.
Acknowledgments
This research was supported in part by NSF grant
IIS-0803481, ONR grant N00014-08-1-0431, and
DARPA contract FA8750- 09-C-0179.
References
Regina Barzilay and Lillian Lee. 2003. Learn-
ing to paraphrase: an unsupervised approach using
multiple-sequence alignment. In Proceedings of the
2003 Conference of the North American Chapter
of the Association for Computational Linguistics on
Human Language Technology - Volume 1, NAACL
?03.
Arianna Bisazza, Nick Ruiz, and Marcello Federico.
2011. Fill-up versus interpolation methods for
phrase-based smt adaptation. In International Work-
shop on Spoken Language Translation (IWSLT), San
Francisco, CA.
David L. Chen and William B. Dolan. 2011. Collect-
ing highly parallel data for paraphrase evaluation. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL-2011),
Portland, OR, June.
Bill Dolan, Chris Quirk, and Chris Brockett. 2004.
Unsupervised construction of large paraphrase cor-
pora: Exploiting massively parallel news sources. In
Proceedings of Coling 2004.
127
Michael Gamon, Jianfeng Gao, Chris Brockett,
Alexander Klementiev, William B. Dolan, Dmitriy
Belenko, and Lucy Vanderwende. 2008. Using con-
textual speller techniques and language modeling for
esl error correction. IJCNLP.
S. Gouws, D. Hovy, and D. Metzler. 2011. Unsu-
pervised mining of lexical variants from noisy text.
In Proceedings of the First workshop on Unsuper-
vised Learning in NLP, pages 82?90. Association
for Computational Linguistics.
Bo Han and Timothy Baldwin. 2011. Lexical normali-
sation of short text messages: Makn sens a# twitter.
In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies, volume 1, pages 368?378.
Bo Han, Paul Cook, and Timothy Baldwin. 2012. Au-
tomatically constructing a normalisation dictionary
for microblogs. In Proceedings of the 2012 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning, pages 421?432, Stroudsburg, PA,
USA.
P. Jaccard. 1912. The distribution of the flora in the
alpine zone. New Phytologist, 11(2):37?50.
Laura Jehl, Felix Hieber, and Stefan Riezler. 2012.
Twitter translation using translation-based cross-
lingual retrieval. In Proceedings of the Seventh
Workshop on Statistical Machine Translation, pages
410?421. Association for Computational Linguis-
tics.
J.H. Johnson, J. Martin, G. Foster, and R. Kuhn. 2007.
Improving translation quality by discarding most of
the phrasetable.
Max Kaufmann and Jugal Kalita. 2010. Syntac-
tic normalization of twitter messages. In Interna-
tional Conference on Natural Language Processing,
Kharagpur, India.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondr?ej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions.
Wang Ling, Guang Xiang, Chris Dyer, Alan Black, and
Isabel Trancoso. 2013. Microblogs as parallel cor-
pora. In Proceedings of the 51st Annual Meeting of
the Association for Computational Linguistics.
Fei Liu, Fuliang Weng, and Xiao Jiang. 2012. A
broadcoverage normalization system for social me-
dia language. In Proceedings of the 50th Annual
Meeting of the Association for Computational Lin-
guistics (ACL 2012), Jeju, Republic of Korea.
Nitin Madnani and Bonnie J. Dorr. 2010. Generating
phrasal and sentential paraphrases: A survey of data-
driven methods. Comput. Linguist.
Inderjeet Mani and George Wilson. 2000. Robust tem-
poral processing of news. In Proceedings of the
38th Annual Meeting on Association for Computa-
tional Linguistics, ACL ?00, pages 69?76, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Robert C. Moore. 2002. Fast and accurate sentence
alignment of bilingual corpora. In Proceedings of
the 5th Conference of the Association for Machine
Translation in the Americas on Machine Transla-
tion: From Research to Real Users, AMTA ?02.
Robert C. Moore. 2004. On log-likelihood-ratios and
the significance of rare events. In Proceedings of the
2004 Conference on Empirical Methods in Natural
Language Processing, pages 333?340.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Comput. Linguist.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting on Association for Computa-
tional Linguistics.
Sas?a Petrovic?, Miles Osborne, and Victor Lavrenko.
2012. Using paraphrases for improving first story
detection in news and twitter.
Chris Quirk, Chris Brockett, and William Dolan.
2004. Monolingual machine translation for para-
phrase generation. In Proceedings of EMNLP 2004.
Alan Ritter, Colin Cherry, and William B. Dolan. 2011.
Data-driven response generation in social media. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing.
Alan Ritter, Mausam, Oren Etzioni, and Sam Clark.
2012. Open domain event extraction from twitter.
In KDD, pages 1104?1112. ACM.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and
Andrew Y. Ng. 2008. Cheap and fast?but is it
good?: evaluating non-expert annotations for natural
language tasks. In Proceedings of the Conference on
Empirical Methods in Natural Language Process-
ing, EMNLP ?08.
128
