Answering Clinical Questions with Role Identification
Yun Niu, Graeme Hirst, Gregory McArthur, and Patricia Rodriguez-Gianolli
Department of Computer Science
University of Toronto
Toronto, Ontario, Canada M5S 3G4
yun,gh,gregm,prg@cs.toronto.edu
Abstract
We describe our work in progress on natu-
ral language analysis in medical question-
answering in the context of a broader med-
ical text-retrieval project. We analyze the
limitations in the medical domain of the
technologies that have been developed for
general question-answering systems, and
describe an alternative approach whose or-
ganizing principle is the identification of
semantic roles in both question and answer
texts that correspond to the fields of PICO
format.
1 Motivation
In every aspect of patient treatment, questions arise
for which a search of the published medical evidence
is appropriate, as it is very likely that the answer has
already been found from the work of other clinicians.
For example:1
Q: In a child with asthma, do increased doses of inhaled cor-
ticosteroids lead to a decrease in growth?
A: Growth was significantly slower in the group receiving
higher dose inhaled steroids (3.6 cm, 95% CI 3.0 to 4.2
with double dose beclometasone v 5.1 cm, 95% CI 4.5
to 5.7 with salmeterol v 4.5 cm, 95% CI 3.8 to 5.2 with
placebo). (Barton, 2002)
Studies have shown that searching in the literature
can help clinicians in answering questions generated
in patient treatment (Gorman et al, 1994; Cimino,
1All the examples in this paper are taken from a collection of
questions that arose over a two-week period in August 2001 in
a clinical teaching unit at the University of Toronto.
1996; Mendonc?a et al, 2001). It has also been
found that if high-quality evidence is available in
this way at the point of care?for example, the pa-
tients? bedside?clinicians will use it in their deci-
sion making, and it frequently results in additional or
changed decisions (Sackett and Straus, 1998; Straus
and Sackett, 1999). But speed is very important.
Investigation of potential end-users has shown that
physicians need access to the information within 30
seconds, and that if the search takes longer, it is likely
to be abandoned (Takeshita et al, 2002).
The practice of using the current best evidence
to help clinicians in making decisions on the
treatment of patients is called Evidence-Based
Medicine (EBM). Finding relevant evidence is a
typical question-answering (QA) problem in the
medical area. There are many QA systems that have
achieved some success in other domains, such as
finding the answer to ?factoid? questions in a corpus
of general news stories, as in the question-answering
track of recent Text Retrieval Conferences (TREC,
2001). However, we have found that there are large
differences between general QA (GQA) and med-
ical QA (MQA) (see section 3 below). This paper
analyzes the challenges of applying QA technology
to answer clinical questions automatically, and then
describes our on-going work on the problem.
2 The EPoCare Project
Our work is part of the EPoCare project (?Evidence
at Point of Care?) at the University of Toronto. The
project aims to provide fast access at the point of
care to the best available medical information. Clin-
icians will be able to query sources that summarize
and appraise the evidence about the diagnosis, treat-
ment, prognosis, etiology, and prevalence of medi-
cal conditions. In order to make the system available
at the point of care, the question-answering system
will be accessible using hand-held computers. The
project is an interdisciplinary collaboration that in-
volves research in several disciplines. Project mem-
bers in Industrial Engineering and Cognitive Psy-
chology are investigating the design of the system
through a user-centered design process, in which re-
quirements are elicited from end users who are also
involved in the evaluation of prototypes. Project
members in Knowledge Management and Natural
Language Processing aim to ensure that the answers
to queries are accurate and complete. And project
members in Health Informatics will test the influence
of the system on clinical decision-making and clini-
cal outcomes.
The system is presently based on keyword queries
and retrieval, as we describe in section 2.2 below.
The goal of the work that we will report in the later
sections of the paper is to allow the system to accept
questions in natural language2 and to better identify
answers in its natural-language data sources. Our
initial emphasis is on the latter.
2.1 System architecture
There are two main components in the system.
The data sources are stored in an XML document
database. The EPoCare server uses this database to
provide answers to queries posed by clinicians.
The architecture of the system is shown in Fig-
ure 1. A clinical query is passed to the front con-
troller to form a database query of keywords. The
query is sent by the retriever to the XML document
database to retrieve relevant documents in the data
sources using keyword matching. The results are
then passed to the query?answer matcher to find the
best answer candidates. Finally, the best answer is
determined and returned to the user.
The current data sources include the reviews of ex-
perimental results for clinical problems that are pub-
lished in Clinical Evidence (CE) (version 7) (Barton,
2002), and Evidence-based On Call (EBOC) (Ball
2Here and throughout the paper, we make the conven-
tional distinction between query and question; the former is a
keyword-basedstring or structure, and the latter is in natural lan-
guage. A query may represent a question, and vice versa.
UMLS
FTI
ToX Engine
keywords
candidate answers
EPoCare Server
expanded
retrieved
documents
Catalog
clinical answers
answers
  keywords
Client Application
Front Controller
clinical query
   Query-Answer
Matcher
Query Processor
(relevant) 
documents
candidate 
    CE
Retriever
EBOC
expansion of keywords
ToX query / answer
Answer Extractor
Figure 1: EPoCare system architecture.
and Phillips, 2001). The texts are stored with XML
mark-up in the database. The XML database is ma-
nipulated by ToX, a repository manager for XML
data (Barbosa et al, 2001). Repositories of dis-
tributed XML documents may be stored in a file sys-
tem, a relational database, or remotely on the Web.
ToX supports document registration, collection man-
agement, storage and indexing choice, and queries
on document content and structure.
2.2 PICO-format queries
At present, the system accepts queries in a format
known in Evidence-Based Medicine as PICO format
(Sackett et al, 2000). In this format, a clinical ques-
tion is represented by a set of four fields that corre-
spond to the basic elements of the question:
P: a description of the patient (or the problem);
I: an intervention;
C: a comparison or control intervention (may be omitted);
O: the clinical outcome.
For example, the sample question in section 1 can be
represented in a simple PICO format as follows:
P: asthma
I: inhaled corticosteroids
C: ?
O: growth
A more-complete PICO representation of the same
question is this:
P: child with asthma
I: increased doses of inhaled corticosteroids
C: ?
O: decrease in growth
This representation contains more information; but
neither of the two expresses the complete semantics
of the natural-language question. Thus, the PICO
format is limited in its ability to represent the mean-
ing of questions. Especially in the case of yes?no
questions, the point of the question is likely to be un-
clear. However, the PICO format indicates the basic
semantics of the question, and it is commonly used
in question representation in EBM. Thus it was used
as a starting point in the development of the system.
The keyword-based retrieval procedure is com-
posed of three steps:
Retrieving. For each query keyword, XML paths
in which the keyword appears are found.
Filtering. Paths that are not meaningful contexts
for the PICO category of the keyword are filtered out.
For each PICO category in the question, some XML
context is meaningful for it while others not. For ex-
ample, a chapter title is meaningful (and valuable)
context for an instance of patient population in the
keyword matching. But titles of cited references are
not.
Building answers. In the filtered paths, the system
identifies cases in which all the key concepts in the
question have been found, in context, in such a way
that an answer pattern is satisfied. Then it returns
the related segment of text in XML format so that
the user can view it with a browser. A set of answer
patterns were constructed for this matching process;
each answer pattern consists of a set of XML paths
for each of the four PICO categories. To identify a
path as relevant, all four components should find a
match in it.
Clinical question: In a patient with a suspected MI does
thrombolysis decrease the risk of death if it is administered 10
hours after the onset of chest pain?
PICO format:
P: myocardial infarction
I: thrombolysis
C: ?
O: mortality
Keywords: myocardial infarction thrombolysis mortality
Answer: Systematic reviews of RCTs have found that prompt
thrombolytic treatment (within 6 hours and perhaps up to 12
hours and longer after the onset of symptoms) reduces mortality
in people with AMI and ST elevation or bundle branch block
on their presenting ECG.
Fifty six people would need treatment in the acute
phase to prevent one additional death. Strokes, intracranial
haemorrhage, and major bleeds are more common in people
given thrombolysis; with one additional stroke for every
250 people treated and one additional major bleed for every
143 people treated. The reviews have found that intracranial
haemorrhage is more common in people of advanced age and
low body weight, those with hypertension on admission, and
those given tPA rather than another thrombolytic agent.
Figure 2: Example of clinical question, with corre-
sponding EPoCare query and answer from Clinical
Evidence.
While this searching strategy is based on the PICO
format, it is not confined to it. The patterns can be
extended so that additional categories (components)
are included. Thus, it could be applied to questions
that are not expressed in PICO.
Figure 2 shows an example of a clinical question
with the corresponding EPoCare query and the seg-
ment of text that was retrieved from Clinical Evi-
dence in response. The segment that was retrieved is
clearly relevant to the question, but it has too much
irrelevant data.
3 QA in medicine: The problem
We will now discuss medical question-answering,
with the goal of refining the current EPoCare system
by accepting natural-language questions and better
identifying answers in the data sources.
In this section, we examine the difference between
general and medical QA from the perspective of the
three main research problems of QA: question pro-
cessing, question?answer matching, and answer ex-
traction. For each problem, we describe features that
current QA technology is not appropriate for, and
features that are not addressed by existing technol-
ogy.
3.1 Question processing
For a question to be answered correctly, a QA sys-
tem first has to understand what the question is ask-
ing about. This is an important task of question pro-
cessing. Most current QA systems address it by iden-
tifying the type of answer sought. As GQA systems
focus on wh- questions, many of which have named
entities (NEs) as their answer, they usually classify
answers according to different types of NE, such as
product, organization, person, and so on. This clas-
sification is not appropriate in the medical domain,
in which questions often ask about the treatment for
a disease, outcome of a treatment, possible disease,
and so on. As a result, the method of identifying an
answer type must be different in MQA from GQA.
Even for the same answer type, there may be a
different understanding. For example, when ques-
tions ask for the time that an event happens. In GQA
systems, they are usually answered by an absolute
date, e.g., 15 May 1932. However, in the medical
area, when questions are usually answered by rel-
ative time, e.g., two hours after the onset of chest
pain. Sometimes the answers are not even a time; in-
stead, they are a clinical condition, e.g., in response
to When should antibiotics be applied?
Some problems of MQA are not addressed at all
by current QA technologies:
Question focus. Sometimes, the answer type
is not enough to determine what a question is
about. Other information contained in the question
is needed to understand its goal. This information is
defined as the focus of the question (Moldovan and
Harabagiu, 2000). Although different systems use
different names for the idea of question focus, it is re-
garded to be very important in question processing.
However, there is still no special technique to tackle
this problem.
Yes?no questions. As mentioned, most current
QA systems focus on wh- questions; yes?no ques-
tions are still left untouched. However, we have
found that they are very common in our collection
of clinical questions that arose in patient treatment.
Efficient processing of yes?no questions is an impor-
tant task in MQA.
3.2 Question?answer matching
The matching of question and answer is the pro-
cess that most GQA systems put great effort into.
Different methods are applied according to different
views of the problem. The approaches can be clas-
sified into two categories: knowledge-intensive and
data-intensive. Knowledge-intensive approaches try
to find the correct match between a question and
the answer by using effective natural language pro-
cessing techniques that combine linguistic and real-
world knowledge. Typical systems include those of
Pas?ca and Harabagiu (2001) and Hovy, Hermjakob,
and Lin (2001). Data-intensive approaches explore
information embedded in the data sources to extract
the evidence that supports a good answer. They can
be further divided into information extraction?based
(Soubbotin, 2001), redundancy-based (Clarke et al,
2001; Dumais et al, 2002), and statistical QA (It-
tycheriah et al, 2001). Many systems contain ele-
ments of both approaches.
Although there have been many technologies de-
veloped for matching the answer with the question,
they are not applicable to the medical area directly
for the following reasons.
Knowledge taxonomy. WordNet is the main
knowledge base that most current GQA systems use
in analyzing relationships among words when cal-
culating the similarity of a question and a candidate
answer. However, as a general-purpose knowledge
base, it is not possible for WordNet to cover all the
concepts in any particular domain, such as medicine.
A domain-specific knowledge base is needed. For
example, it may be important to know that meto-
prolol is an instance of ?-blocker in order to locate
the correct answer. A good complement to WordNet
is the Unified Medical Language System (UMLS)
(Lindberg et al, 1993), developed by the National
Library of Medicine. UMLS contains three knowl-
edge sources: the Metathesaurus, the Semantic Net-
work, and the Specialist Lexicon. The Metathe-
saurus represents biomedical knowledge by orga-
nizing concepts according to their relationships and
meanings. It will be very helpful in tasks such as
query expansion and answer-type identification in
MQA.
Named entity identification. As the types of
NE in the medical area are different, the method of
identifying them must be changed accordingly. For
example, an MQA system must be able to distin-
guish medication from diseases. Medical terminol-
ogy plays an important role in NE identification, as
before a concept can be classified, the correspond-
ing terminology has to be recognized to make sure
that the correct concept is found. In the medical do-
main, different phrases can be used to refer to the
same medical concept. For example, a drug may be
referred to by its abbreviation, its common name, or
its formal name (ASA, Aspirin, acetylsalicylic acid).
Also, different medical concepts may have the same
abbreviation, which will lead to ambiguities in con-
cept understanding.
Data source. A medical data source is often or-
ganized in accordance with a hierarchy of medical
concepts. For example, Clinical Evidence (Barton,
2002) groups clinical data according to disease cat-
egories. The positive aspect of such well-organized
data is that once the candidate answers are found, it
is very likely that they include the correct answer.
However, it is unlikely that the answer for a ques-
tion will appear redundantly in many different places
in the data source. This is different from GQA sys-
tems, which usually require a relatively large num-
ber of redundant answer candidates to support good
performance by the system.
In current GQA systems, a correct answer to a
question is often independent of its context. This is
not the case in the medical data, in which the con-
text containing a candidate answer may be important
to the question?answer matching. The context usu-
ally explains a conclusion, provides more evidence,
or even presents contrary evidence. A correct answer
may be missed or the incorrect answer may be ex-
tracted if the context is not considered in the match-
ing process.
Complicated constraints. Clinical questions of-
ten contain a very specific description of the patient
conditions, as shown in the following examples:
Q: Should ?-blocker (metoprolol) be used to continue treat-
ment for a male with hypertension and coronary artery
disease even though he has Type 2 diabetes mellitus?
Q: Do patients surviving an AMI and experiencing transient
or ongoing congestive heart failure (CHF) have reduced
mortality and morbidity when treated with an ACE in-
hibitor (ex. Ramipril)?
The detailed description of the patient acts as a
constraint in matching with candidate answers.
As the complexity of questions increases, more-
sophisticated techniques are needed to find a
matching answer.
3.3 Answer extraction
An MQA system should be able to answer clin-
ical questions in the course of patient treatment.
Hence the format of the answer is important, and
this will affect the answer extraction process. For
the three types of questions?wh- questions, yes?no
questions, and no-answer3 questions?the EPoCare
study of user requirements shows that both a short
answer and a long answer should be prepared. The
short answer provides accurate and concise informa-
tion to the physicians so that they can make the deci-
sion quickly. For yes?no questions, the answer can
be just yes or no. If the system cannot find an an-
swer for a question, it should indicate this explic-
itly as its short answer. But sometimes clinicians
want to read a long answer that may contain expla-
nation of the evidence or other results of related ex-
periments. For the no-answer questions, physicians
may expect to read at least some relevant informa-
tion. It is thus important to determine what relevant
information should be included in the answer extrac-
tion.
3.4 Evaluation metrics
Evaluation of QA systems in the medical area is dif-
ferent from current evaluation methods for general
QA systems. The Text Retrieval Conference uses
the Mean Reciprocal Rank (MRR) as an evaluation
metric. In this method, a system may return an or-
dered list of up to five different candidate answers
to a question, and the score received is 1=n, where
n is the position in the list of the correct answer (if it
appears at all); for example, if the correct answer is
fourth in the list, the system receives a score of 0.25
for that test item. This metric cannot be applied here,
since returning a list of alternative candidate answers
to a question, each of which must then be further ver-
ified, is not acceptable for a clinical question that is
posed on site.
Different answer formats should be evaluated sep-
arately. The short answer has to be concise. So what
3A no-answer question is one for which an answer cannot be
found. It is not a yes?no question for which the answer happens
to be no.
a concise answer is must be defined (at least for the
wh- questions). A long answer needs to provide de-
tailed information that explains the short answer. For
no-answer questions, relevant information (if there is
some) should be returned. For these two types of an-
swers, it has to be clear (1) what information can be
viewed as ?detail? or ?relevant?; (2) what the differ-
ence between the two is; and (3) how much informa-
tion should be included.
Partial answers should be considered in the eval-
uation. If part of the correct answer is included in
the system output, it should be evaluated according
to the importance of the correct information. A par-
tial answer that contains more crucial information
should obtain a higher score. Similarly, if an answer
helps make a wrong decision, it should be punished
in the evaluation.
4 Locating answers by role identification
From the discussion in the previous section, we can
see that MQA poses new challenges for QA research
that require new approaches. We have found that the
use of roles and role identification is effective, and
we take them as an organizing principle for MQA
that goes beyond the use of named entities in GQA.
This section will explain the principle. In this ap-
proach, the four roles represented by PICO will first
be located in both the natural-language question and
the candidate answer texts obtained by the retrieval
phase. For example, PICO roles would be identified
in these candidate answers as shown by the labelled
bracketing.
One RCT found [no evidence that (low molecular weight
heparin)I is superior to (aspirin alone)C]O for the treatment
of (acute ischaemic stroke in people with atrial fibrillation)P.
(Thrombolysis)I (reduces the risk of dependency, but in-
creases the risk of death)O.
We found (no evidence of benefit)O from (surgical evacua-
tion of cerebral or cerebellar haematomas)I.
In the matching process, the roles in the question will
be compared with the corresponding roles in the an-
swer candidates to determine whether a candidate is
a correct answer.
4.1 Why roles?
In GQA systems, as mentioned, in the question?
answer matching process, usually the answer candi-
dates are first checked to see if they contain the ex-
pected answer type, in order to rule out irrelevant
candidates. This is shown to be efficient, as indi-
cated by Harabagiu et al (2001): systems that did
not include NE recognizers performed poorly in the
TREC evaluations. The effectiveness of this method
depends on successfully recognizing NEs in the an-
swer candidates. However, for questions that cannot
be answered by named entities, the QA task is more
complex, as it will be more difficult to recognize the
corresponding answer type in the answer candidates.
The same problem occurs in MQA. The important
information in medical text usually corresponds to
the basic PICO fields. For example, therapy-related
text describes the relationshipsamong four elements:
the status of the patient, the therapy, the compari-
son therapy, and the clinical outcome. Descriptions
of the diagnosis process often consist of the patient
status, the test method, and the outcome. These el-
ements are the key concepts of understanding medi-
cal text. They act as different roles, which together
construct the meaning of the text. While some of the
roles correspond to NEs, others do not. For exam-
ple, in answering a therapy-related question, the pa-
tient status and the therapy can often be treated as
NEs, but the clinical outcome often cannot be. In
a description of diagnosis, the test process often is
not represented by an NE. While medical NEs can be
expected to be recognized by applying terminology
techniques with the support of UMLS, the recogni-
tion of non-NE roles in the answer candidates, on the
other hand, becomes the main challenge.
Thus, it is not sufficient to use information-
extraction techniques, as in some GQA systems
(Pas?ca and Harabagiu, 2001; Soubbotin, 2001), in
which patterns are matched against the text to fill in
the roles in the template. In such systems, the cover-
age of the pattern set is quite limited; it is very time-
consuming to manually construct a large set of suit-
able patterns, especially for complicated phrasings;
and the patterns are very specific: specific words or
phrases are usually required to occur at a fixed lo-
cation in each pattern, making it applicable only to
expressions phrased in exactly the same way. While
we will need to look for some specific words, we
need much greater flexibility than is afforded by sim-
ple pattern-matching to identify the PICO roles in
the text. This can be done by analyzing the different
roles and their relationships.
4.2 Understanding the data
To apply a role-based method in MQA, we need to
deal with the following problems:
1. Identifying the roles in text.
2. Determining the textual boundary of each role.
3. Analyzing the relationships among different roles.
4. Determining which combinations of roles are most likely
to contain correct answers.
Our work currently focuses on therapy-related
questions. We manually analyzed 170 sentences
from the Cardiovascular Disorders section of Clini-
cal Evidence to obtain a better understandingof these
problems. Among the sentences, 141 contained at
least one role that we are interested in. For therapy-
related questions, we found that often if an outcome
role appeared in a sentence, then the sentence pro-
vided some interesting information related to clinical
evidence. But clinical outcome is the most difficult
non-NE role to locate.
4.2.1 Identifying clinical outcomes
In our analysis, we found that the lexical iden-
tifiers of clinical outcome belong to three part-of-
speech categories: noun, verb, and adjective. For ex-
ample:
Thrombolysis reduces the risk of dependency, but increases
the risk of death.
Lubeluzole has also been noted to have adverse outcome, es-
pecially at higher doses.
Some words that identify outcomes are listed below:
Nouns: death, benefit, dependency, effect, evidence, out-
come.
Verbs: improve, reduce, prevent, produce, increase.
Adjectives: responsible, negative, adverse, slower.
Clinical outcomes must be carefully distinguished
in the text from the outcomes of clinical trials them-
selves. We refer to the latter as results in the follow-
ing. A result might or might not include a clinical
outcome. They often involve a comparison of the
effects of two (or more) interventions on a disease.
Sometimes a result will state that an outcome did not
occur:
One RCT found evidence that hormone treatment plus radio-
therapy versus radiotherapy alone improved survival in lo-
cally advanced breast cancer.
In the systematic review of calcium channel antagonists, in-
direct and limited comparisons of intravenous versus oral
administration found no significant difference in adverse
events.
We found no evidence of benefit from surgical evacuation of
cerebral or cerebellar haematomas.
The identifiers of results form another group:
Result: evidence, difference, comparison, superior to, ver-
sus.
4.2.2 Determining the textual boundary of
clinical outcomes
In determining the textual boundary of an out-
come, the four groups of words are treated sepa-
rately. Our finding is that for the noun identifiers, the
noun phrase that contains the nouns will be an out-
come. For the verb identifiers, the verb and its ob-
ject together constitutean outcome. For the adjective
identifiers, usually the adjective itself is an outcome.
If several identifiers occur in one sentence, the out-
come is all the text indicated by one or more of the
identifiers.
Determining the textual boundary of the results of
clinical trials is more complicated. If a result is a
comparison of two or more interventions, it will con-
tain the interventions, words that indicate a compar-
ison relationship, and often the aspects that are com-
pared. In the first of the previous group of exam-
ples, the elements of the results are evidence, hor-
mone treatment plus radiotherapy versus radiother-
apy, and improved survival. However, if the inter-
ventions can be identified as NEs, it will not be too
difficult to determine the boundary.
We tested these simple rules manually on 50 sen-
tences from Clinical Evidence on the topic of acute
otitis media. Out of 54 outcomes (including both
clinical outcomes and clinical trial results), 45 were
identified correctly, and 40 correct textual bound-
aries were found.
4.2.3 Relationships among roles
We have also found that roles are helpful in un-
derstanding the relationshipsbetween sentences. For
example, if a sentence contains only the interven-
tion role and the following sentence contains only the
problem and outcome, then it is very likely that the
combination of the two sentences represents a com-
plete idea and the roles themselves are related. We
believe that as the work continues, more interesting
relations will be found.
5 Conclusion
We have described our work in progress on adding
natural language analysis to querry-answering the
EPoCare project. Although this work is at a rela-
tively early stage, we have analyzed the limitations
of GQA technologies in MQA and are developing
techniques whose organizing principle is the identifi-
cation of the semantic roles in both question and an-
swer texts, with our initial emphasis being on the lat-
ter.
Acknowledgements
The EPoCare project is supported by grants from
Bell University Laboratories at the University
of Toronto. This research is also supported by a
grant from the Natural Sciences and Engineering
Research Council of Canada. We are grateful to
Sharon Straus, MD, and other members of the
project for discussion and assistance.
References
Christopher M. Ball and Robert S. Phillips. 2001.
Evidence-based On-Call: Acute Medicine. Churchill
Livingstone, Edinburgh.
Denilson Barbosa, Attila Barta, Alberto Mendel-
zon, George Mihaila, Flavio Rizzolo, and Patricia
Rodriguez-Gianolli. 2001. ToX ? The Toronto XML
Engine. In Proceedings of the International Workshop
on Information Integration on the Web, Rio de Janeiro.
Stuart Barton. 2002. Clinical Evidence. BMJ Publishing
Group, London.
James J. Cimino. 1996. Linking patient information
systems to bibliographic resources. Methods of Infor-
mation in Medicine, 35(2):122?126.
Charles L. A. Clarke, Gordon V. Cormack, and Thomas R.
Lynam. 2001. Exploiting redundancy in question
answering. In Proceedings of the 24th International
Conference on Research and Development in Infor-
mation Retrieval (SIGIR-2001), pages 358?365.
Susan Dumais, Michele Banko, Eric Brill, Jimmy Lin,
and Andrew Ng. 2002. Web question answering: is
more always better? In Proceedings of the 25th Inter-
national Conference on Research and Development in
Information Retrieval (SIGIR-2002), pages 291?298.
Paul N. Gorman, Joan Ash, and L. Wykoff. 1994. Can
primary care physicians? questions be answered using
the medical journal literature? Bulletin of Medical
Library Association, 82(2): 140?146.
Sanda M. Harabagiu, et al 2001. The role of lexico-
semantic feedback in open-domain textual question?
answering. In Proceedings of the 39th Meeting of
the Association for Computational Linguistics (ACL-
2001), pages 274?281.
Eduard Hovy, Ulf Hermjakob, and Chin-Yew Lin. 2001.
The use of external knowledge in factoid QA. In
(TREC, 2001), pages 644?652.
Abraham Ittycheriah, Martin Franz, and Salim Roukos.
2001. IBM?s statistical question answering system ?
TREC-10. In (TREC, 2001), pages 258?264.
Donald A. B. Lindberg, Betsy L. Humphreys, and
Alexa T. McCray. 1993. The Unified Medical Lan-
guage System. Methods of Information in Medicine,
32(4):281?291.
Eneida A. Mendonc?a, James J. Cimino, Stephen B.
Johnson, and Yoon-Ho Seol. 2001. Accessing het-
erogeneous sources of evidence to answer clinical
questions. Journal of Biomedical Informatics, 34:
85?98.
Dan Moldovan and Sanda M. Harabagiu. 2000. The
structure and performance of an open-domain question
answering system. In Proceedings of the 38th Meeting
of the Association for Computational Linguistics,
(ACL-2000), pages 563?570.
Marius A. Pas?ca and Sanda M. Harabagiu. 2001. High
performance question/answering. In Proceedings of
the 24th International Conference on Research and
Development in Information Retrieval (SIGIR-2001),
pages 366?374.
David L. Sackett and Sharon E. Straus. 1998. Finding
and applying evidence during clinical rounds: the
?evidence cart?. Journal of the American Medical
Association, 280(15):1336?1338.
David L. Sackett, Sharon E. Straus, W. Scott Richardson,
William Rosenberg, and R. Brian Haynes. 2000.
Evidence-Based Medicine: How to Practice and
Teach EBM. Churchill Livingstone, Edinburgh.
Martin M. Soubbotin. 2001. Patterns of potential answer
expressions as clues to the right answers. In (TREC,
2001), pages 293?302.
Sharon E. Straus and David L. Sackett. 1999. Bringing
evidence to the point of care. Journal of the American
Medical Association, 281:1171?1172.
Harumi Takeshita, Dianne Davis, and Sharon E. Straus.
2002. Clinical evidence at the point of care in acute
medicine: a handheld usability case study. In Proceed-
ings of the Human Factors and Ergonomics Society
46th Annual Meeting, pages 1409?1413, Baltimore.
TREC. 2001. Proceedings of the Tenth Text Retrieval
Conference, Gaithersburg, MD, November 13?16. Na-
tional Institute of Standards and Technology.
Analysis of Semantic Classes in Medical Text for Question Answering
Yun Niu and Graeme Hirst
Department of Computer Science
University of Toronto
Toronto, Ontario M5S 3G4
Canada
yun@cs.toronto.edu, gh@cs.toronto.edu
Abstract
To answer questions from clinical-evidence texts,
we identify occurrences of the semantic classes ?
disease, medication, patient outcome ? that are
candidate elements of the answer, and the relations
among them. Additionally, we determine whether
an outcome is positive or negative.
1 Motivation
The published medical literature is an important
source to help clinicians make decisions in patient
treatment (Sackett and Straus, 1998; Straus and
Sackett, 1999). Clinicians often need to consult lit-
erature on the latest information in patient care, such
as side effects of a medication, symptoms of a dis-
ease, or time constraints in the use of a medication.
For example:1
Q: In a patient with a suspected MI does throm-
bolysis decrease the risk of death if it is ad-
ministered 10 hours after the onset of chest
pain?
The answer to the question can be found in Clini-
cal Evidence (CE) (Barton, 2002), a regularly up-
dated publication that reviews and consolidates ex-
perimental results for clinical problems:
A: Systematic reviews of RCTs have found
that prompt thrombolytic treatment (within 6
hours and perhaps up to 12 hours and longer
after the onset of symptoms) reduces mor-
tality in people with AMI and ST elevation
or bundle branch block on their presenting
ECG.
The goal of the EpoCare project (?Evidence at
Point of Care?) at the University of Toronto is to
develop methods for answering questions automati-
cally with CE as the source text. (We do not look at
1All the examples in this paper are taken from a collection
of questions that arose over a two-week period in August 2001
in a clinical teaching unit at the University of Toronto.
primary medical research text.) Currently, the sys-
tem accepts keyword queries in PICO format (Sack-
ett et al, 2000). In this format, a clinical question is
represented by a set of four fields that correspond to
the basic elements of the question:
P: a description of the patient (or the problem);
I: an intervention;
C: a comparison or control intervention (may
be omitted);
O: the clinical outcome.
For example, the question shown above can be rep-
resented in PICO format as follows:
P: myocardial infarction
I: thrombolysis
C: ?
O: mortality
Our work in the project is to extend the keyword
retrieval to a system that can answer questions ex-
pressed in natural language.
In our earlier work (Niu et al, 2003), we showed
that current technologies for factoid question an-
swering (QA) are not adequate for clinical ques-
tions, whose answers must often be obtained by
synthesizing relevant context. To adapt to this new
characteristic of QA in the medical domain, we ex-
ploit semantic classes and relations between them in
medical text. Semantic classes are important for our
task because the information contained in them is
often a good candidate for answering clinical ques-
tions. In the example above, PICO elements cor-
respond to three semantic classes: DISEASE (med-
ical problem of the patient), INTERVENTION (med-
ication applied to the disease) and the CLINICAL
OUTCOME. They together constitute a SCENARIO
of treatment. Similarly, a diagnosis scenario often
includes SYMPTOMS, TESTING PROCEDURE, and
HYPOTHESIZED DISEASES. To understand the se-
mantics of medical text and find answers to clinical
questions, we need to know how these classes relate
to each other in a specific scenario. For example, is
this medication a special type of another one; is this
medication applied to this disease? These are the
kind of relations that we are interested in. In this
work, we use a cue-word?based approach to iden-
tify semantic classes in the treatment scenario and
analyze the relations between them. We also apply
an automatic classification process to determine the
polarity of an outcome, as it is important in answer-
ing clinical questions.
2 Identifying Semantic Classes in Medical
Text
2.1 Diseases and Medications
The identification of named entities (NEs) in the
biomedical area, such as PROTEINS and CELLS, has
been extensively explored; e.g., Lee et al (2003),
Shen et al (2003). However, we are not aware of
any satisfactory solution that focuses on the recog-
nition of semantic classes such as MEDICATION and
DISEASE. To straightforwardly identify DISEASE
and MEDICATION in the text, we use the knowledge
base Unified Medical Language System (UMLS)
(Lindberg et al, 1993) and the software MetaMap
(Aronson, 2001).
UMLS contains three knowledge sources: the
Metathesaurus, the Semantic Network, and the Spe-
cialist Lexicon. Given an input sentence, MetaMap
separates it into phrases, identifies the medical con-
cepts embedded in the phrases, and assigns proper
semantic categories to them according to the knowl-
edge in UMLS. For example, for the phrase imme-
diate systemic anticoagulants, MetaMap identifies
immediate as a TEMPORAL CONCEPT, systemic as
a FUNCTIONAL CONCEPT, and anticoagulants as a
PHARMACOLOGIC SUBSTANCE. More than one se-
mantic category in UMLS may correspond to MED-
ICATION or DISEASE. For example, either a PHAR-
MACOLOGIC SUBSTANCE or a THERAPEUTIC OR
PREVENTIVE PROCEDURE can be a MEDICATION;
either a DISEASE OR SYNDROME or a PATHOLOGIC
FUNCTION can be a DISEASE.
We use some training text to find the mapping
between UMLS categories and the two semantic
classes in the treatment scenario. The training
text was tagged for us by a clinician to mark DIS-
EASE and MEDICATION. It was also processed by
MetaMap. After that, the annotated text was com-
pared with the output of MetaMap to find the corre-
sponding UMLS categories. Medical text contain-
ing these categories can then be identified as either
MEDICATION or DISEASE. In the example above,
anticoagulants will be taken as a MEDICATION. The
problem of identification of medical terminology is
still a big challenge in this area. MetaMap does not
provide a full solution to it. For cases in which the
output of MetaMap is not consistent with the judg-
ment of the clinician who annotated our text, our
decisions rely on the latter.
2.2 Clinical Outcome
The task of identifying clinical outcomes is more
complicated. Outcomes are often not just noun
phrases; instead, they usually are expressed in com-
plex syntactic structures. The following are some
examples:
(1) Thrombolysis reduces the risk of depen-
dency, but increases the risk of death.
(2) The median proportion of symptom free days
improved more with salmeterol than with
placebo.
In our analysis of the text, we found another type
of outcome which is also very important: the out-
come of clinical trials:
(3) Several small comparative RCTs [random-
ized clinical trials] have found sodium cro-
moglicate to be less effective than inhaled
corticosteroids in improving symptoms and
lung function.
(4) In the systematic review of calcium chan-
nel antagonists, indirect and limited compar-
isons of intravenous versus oral administra-
tion found no significant difference in ad-
verse events.
We treat these as a special type of clinical outcome.
For convenience, we refer to them as ?results? in the
following description when necessary. A ?result?
might contain a clinical outcome within it, as results
often involve a comparison of the effects of two (or
more) interventions on a disease.
In medical text, the appearance of some words is
found often to be a signal of the occurrence of an
outcome, and usually several words signal the oc-
currence of one single outcome. The combination
approach that we applied for identifying outcomes
is based on this observation. Our approach does
not extract the whole outcome at once. Instead, it
tries to identify the different parts of an outcome
that may be scattered in the sentence, and then com-
bines them to form the complete outcome.
2.2.1 Related work
Rule-based methods and machine-learning ap-
proaches have been used for similar problems.
Gildea and Jurafsky (2002) used a supervised learn-
ing method to learn both the identifier of the seman-
tic roles defined in FrameNet such as theme, target,
goal, and the boundaries of the roles (Baker et al,
2003). A set of features were learned from a large
training set, and then applied to the unseen data to
detect the roles. The performance of the system was
quite good. However, it requires a large training
set for related roles, which is not available in many
tasks, including tasks in the medical area.
Rule-based methods are explored in information
extraction (IE) to identify roles to fill slots in some
pre-defined templates (Catala` et al, 2003). The
rules are represented by a set of patterns, and tem-
plate role identification is usually conducted by pat-
tern matching. Slots indicating roles are embed-
ded in these patterns. Text that satisfies the con-
straints of a pattern will be identified, and the con-
tents corresponding to the slots are extracted. This
approach has been proved to be effective in many IE
tasks. However, pattern construction is very time-
consuming, especially for complicated phrasings.
In order to select the roles and only the roles, their
expression has to be customized specifically in pat-
terns. This results in increasing difficulties in pat-
tern construction, and reduces the coverage of the
patterns.
2.2.2 A combination approach
Different pieces of an outcome are identified by var-
ious cue words. Each occurrence of a cue word sug-
gests a portion of the expression of the outcome.
Detecting all of them will increase the chance of
obtaining the complete outcome. Also, different oc-
currences of cue words provide more evidence of
the existence of an outcome.
The first step of the combination approach is to
collect the cue words. Two sections of CE (stroke
management, asthma in children) were analyzed for
detection of outcome. The text was annotated by a
clinician in the EpoCare project. About two-thirds
of each section (267 sentences in total) was taken as
the analysis examples for collecting the cue words,
and the rest (156 sentences) as the test set. Some
words we found in the analysis are the following:
Nouns: death, benefit, dependency, outcome, evi-
dence, harm, difference.
Verbs: improve, reduce, prevent, produce, in-
crease.
Adjectives: beneficial, harmful, negative, adverse,
superior.
After the cue words are identified, the next ques-
tion is what portion of text each cue word suggests
as the outcome, which determines the boundary of
the outcome. The text was pre-processed by the
Apple Pie parser (Sekine, 1997) to obtain the part-
of-speech and phrase information. We found that
for the noun cues, the noun phrase that contains the
noun will be part of the outcome. For the verb cue
words, the verb and its object together constitute
one portion of the outcome. For the adjective cue
words, often the corresponding adjective phrase or
the noun phrase belongs to the outcome. Cue words
for the results of clinical trials are processed in a
slightly different way. For example, for difference
and superior, any immediately following preposi-
tional phrase is also included in the results of the
trial.
Our approach does not rely on specific patterns,
it is more flexible than pattern-matching techniques
in IE systems, and it does not need a large training
set. A limitation of this approach is that some con-
nections between different portions of an outcome
may be missing.
2.2.3 Evaluation and analysis of results
We evaluated the cue word method of detecting the
outcome on the remaining one-third of the sections
of CE. (The test set is rather small because of the
difficulty in obtaining the annotations.) The out-
come detection task was broken into two sub-tasks,
each evaluated separately: to identify the outcome
itself and to determine its textual boundary. The re-
sult of identification is shown in Table 1. Eighty-one
sentences in the test set contain either an outcome or
result, which is 52% of all the test sentences. This
was taken as the baseline of the evaluation: taking
all sentences in the test set as positive (i.e., contain-
ing an outcome or result). By contrast, the accuracy
of the combination approach is 83%.
There are two main reasons why some outcomes
were not identified. One is that some outcomes do
not have any cue word:
(5) Gastrointestinal symptoms and headaches
have been reported with both montelukast
and zafirlukast.
The other reason is that although some outcomes
contained words that might be regarded as cue
words, we did not include them in our set; for ex-
ample, fewer and higher. Adjectives were found to
have the most irregular usages. It is normal for them
to modify both medications and outcomes, as shown
in the following examples:
(6) . . . children receiving higher dose inhaled
corticosteroids . . .
Table 1: Results of identifying outcomes in CE
False False
Method Correct Positives Negatives Precision% Recall% Accuracy%
Baseline 81 75 0 52 (81/156) 100 52
Combination approach 67 14 14 83 (67/81) 83 82
Table 2: Results of boundary detection of correctly
identified outcomes in CE. A: Identified fragments;
B: true boundary.
Type of Overlap Number Percentage
Exact match 26 39
A entirely within B 19 28
B entirely within A 13 19
Each partially
within the other 8 12
No match 1 1
(7) . . . mean morning PEFR was 4% higher in
the salmeterol group.
Other adjectives such as less, more, lower, shorter,
longer, and different have similar problems. If they
are taken as identifiers of outcomes then some false
positives are very likely to be generated. However,
if they are excluded, some true outcomes will be
missed. There were 14 samples of false positives.
The main cause was sentences containing cue words
that did not have any useful information:
(8) We found that the balance between bene-
fits and harms has not been clearly estab-
lished for the evacuation of supratentorial
haematomas.
(9) The third systematic review did not evaluate
these adverse outcomes.
Table 2 shows the result of boundary detection
for those outcomes that were correctly identified.
The true boundary is the boundary of an outcome
that was annotated manually. The no match case
means that there is a true outcome in the sentence
but the program missed the correct portions of text
and marked some other portions as the outcome.
The program identified 39% of the boundaries ex-
actly the same as the true boundaries. In 19% of the
samples, the true boundaries were entirely within
the identified fragments. The spurious text in them
(the text that was not in the true boundary) was
found to be small in many cases, both in terms of
number of words and in terms of the importance of
the content. The average number of words correctly
identified was 7 for each outcome and the number
of spurious words was 3.4. The most frequent con-
tent in the spurious text was the medication applied
to obtain the outcome. In the following examples,
text in ?hi? is the outcome (result) identified auto-
matically, and text in ?fg? is spurious.
(10) The RCTs found hno significant adverse ef-
fects fassociated with salmeterolgi.
(11) The second RCT . . . also found hno sig-
nificant difference in mortality at 12 weeks
fwith lubeluzole versus placebogi . . .
Again, adjectives are most problematic. Even
when a true adjective identifier is found, the bound-
ary of the outcome is hard to determine by an un-
supervised approach because of the variations in
the expression. In the following examples, the true
boundaries of outcomes are indicated by ?[ ]?, ad-
jectives are highlighted.
(12) Nebulised . . . , but [hserious adverse
effectsi are rare].
(13) Small RCTs . . . found that [. . . was
heffectivei, with . . . ].
The correctness of the output of the parser also
had an important impact on the performance, as
shown in the following example:
(14) RCTs found no evidence that lubeluzole
improved clinical outcomes in people with
acute ischaemic stroke.
(S . . . (NPL (DT that) (JJ lubeluzole) (JJ im-
proved) (JJ clinical) (NNS outcomes)) . . . )
In this parse, the verb improve was incorrectly as-
signed to be an adjective in a noun phrase. Thus im-
prove as a verb cue word was missed in identifying
the outcome. However, another cue word outcomes
was matched, so the whole noun phrase of outcomes
was identified as the outcome. On the one hand,
the example shows that the wrong parsing output
directly affects the identification process. On the
other hand, it also shows that missing one cue word
in identifying the outcome can be corrected by the
occurrence of other cue words in the combination
approach.
3 Analysis of Relations
Recognition of individual semantic classes is not
enough for text understanding; we also need to
know how different entities in the same semantic
class are connected, as well as what relations hold
between different classes. Currently, all these rela-
tions are considered at the sentence level.
3.1 Relations within the same semantic class
Relations between different medications are the fo-
cus of this sub-section, as a sentence often men-
tioned more than one medication. Relations be-
tween diseases can be analyzed in a similar way, al-
though they occur much less often than medications.
Text from CE was analyzed manually to understand
what relations are often involved and how they are
represented. The text for the analysis is the same
as in the class-identification task discussed above.
As with classes themselves, it was found that these
relations can be identified by a group of cue words
or symbols. For example, the word plus refers to
the COMBINATION of two or more medications, the
word or, as well as a comma, often suggests the AL-
TERNATIVE relation, and the word versus (or v) usu-
ally implies a COMPARISON relation, as shown in
the following examples:
(15) The combination of aspirin plus streptoki-
nase significantly increased mortality at 3
months.
(16) RCTs found no evidence that calcium chan-
nel antagonists, lubeluzole, aminobutyric
acid (GABA) agonists, glycine antagonists,
or N-methyl-D-aspartate (NMDA) antago-
nists improve clinical outcomes in people
with acute ischaemic stroke.
(17) One systematic review found no short or
long term improvement in acute ischaemic
stroke with immediate systemic anticoagu-
lants (unfractionated heparin, low molecu-
lar weight heparin, heparinoids, or specific
thrombin inhibitors) versus usual care with-
out systemic anticoagulants.
It is worth noting that in CE, the experimental con-
ditions are often explained in the description of the
outcomes, for example:
(18) . . . receiving higher dose inhaled corticos-
teroids (3.6cm, 95% CI 3.0 to 4.2 with dou-
ble dose beclometasone v 5.1cm, 95% CI 4.5
to 5.7 with salmeterol v 4.5cm, 95% CI 3.8
to 5.2 with placebo).
(19) It found that . . . oral theophylline . . . ver-
sus placebo increased the mean number of
symptom free days (63% with theophylline v
42% with placebo; P=0.02).
(20) Studies of . . . inhaled steroid (see salme-
terol v high dose inhaled corticosteroids un-
der adult asthma).
These descriptions are usually in parentheses. They
are often phrases and even just fragments of strings
that are not represented in a manner that is uniform
with the other parts of the sentence. Their behavior
is more difficult to capture and therefore the rela-
tions among the concepts in these descriptions are
more difficult to identify. Because they usually are
examples and data, omission of them will not af-
fect the understanding of the whole sentence in most
cases.
Six common relations and their cue words were
found in the text which are shown in Table 3. Cue
words and symbols between medical concepts were
first collected from the training text. Then the re-
lations they signal were analyzed. Some cue words
are ambiguous, for example, or, and, and with. Or
could also suggest a comparison relation although
most of the time it means alternative, and could rep-
resent an alternative relation, and with could be a
specification relation. It is interesting to find that
and in the text when it connects two medications
often suggests an alternative relation rather than a
combination relation (e.g., the second and in exam-
ple 5). Also, compared with versus, plus, etc., and
and with are weak cues as most of their appearances
in the text do not suggest a relation between two
medications.
On the basis of this analysis, an automatic re-
lation analysis process was applied to the test set,
which was the same as in outcome identification.
The test process was divided into two parts: one
took parenthetical descriptions into account (case 1)
and the other one did not (case 2). In the evaluation,
for sentences that contain at least two medications,
?correct? means that the relation that holds between
the medications is correctly identified. We do not
evaluate the relation between any two medications
in a sentence; instead, we only considered two med-
ications that are related to each other by a cue word
or symbol (including those connected by cue words
Table 3: Cue words/symbols for relations between
medications
Relation(s) Cue Words/Symbols
comparison superior to, more than, versus, or,
compare with, between . . . and . . .
alternative or, ?,?, and
combination plus, add to, addition of . . . to . . . ,
combined use of, and, with, ?(?
specification with, ?(?
substitute substitute, substituted for
preference rather than
Table 4: Results of relation analysis
Correct Wrong Missing False Positive
Case 1 49 7 10 9
Case 2 48 7 3 6
other than the set collected from the training text).
The results of the two cases are shown in Table 4.
Most errors are because of the weak indicators
with and and. As in the outcome identification task,
both the training and test sets are rather small, as no
standard annotated text is available.
Some of the surface relationships in Table 3 re-
flect deeper relationships of the semantic classes.
For example, COMPARISON, ALTERNATIVE, and
PREFERENCE imply that the two (or more) medi-
cations have some common effects on the disease(s)
they are applied to. The SPECIFICATION relation, on
the other hand, suggests a hierarchical relation be-
tween the first medication and the following ones, in
which the first medication is a higher-level concept
and the following medications are at a lower level.
For example, in example 17 above, systemic anti-
coagulants is a higher-level concept, unfractionated
heparin, low molecular weight heparin, etc., are ex-
amples of it that lie at a lower level.
3.2 Relations between different semantic
classes
In a specific domain such as medicine, some default
relations often hold between semantic classes. For
example, a CAUSE?EFFECT relation is strongly em-
bedded in the three semantic classes appearing in
a sentence of the form: ?medication . . . disease
. . . outcome?, even if not in this exact order. This
default relation helps the relation analysis because
in most cases we do not need to depend on the text
between the classes to understand the whole sen-
tence. For instance, the CAUSE?EFFECT relation
is very likely to express the idea that applying the
intervention on the disease will have the outcome.
This is another reason that semantic classes are im-
portant, especially in a specific domain.
4 The polarity of outcomes
Most clinical outcomes and the results of clinical
trials are either positive or negative:
(21) Positive: Thrombolysis reduced the risk of
death or dependency at the end of the stud-
ies.
(22) Negative: In the systematic review, throm-
bolysis increased fatal intracranial haemor-
rhage compared with placebo.
Polarity information is useful for several reasons.
First of all, it can filter out positive outcomes if the
question is about the negative aspects of a medica-
tion. Secondly, negative outcomes may be crucial
even if the question does not explicitly ask about
them. Finally, from the number of positive or neg-
ative descriptions of the outcome of a medication
applying to a disease, clinicians can form a general
idea about how ?good? the medication is. As a first
step in understanding opposing relations between
scenarios in medical text, the polarity of outcomes
was determined by an automatic classification pro-
cess.
We use support vector machines (SVMs) to dis-
tinguish positive outcomes from negative ones.
SVMs have been shown to be efficient in text clas-
sification tasks (Joachims, 1998). Given a training
sample, the SVM finds a hyperplane with the max-
imal margin of separation between the two classes.
The classification is then just to determine which
side of the hyperplane the test sample lies in. We
used the SVMlight package (Joachims, 2002) in our
experiment.
4.1 Training and test examples
The training and test sets were built by collecting
sentences from different sections in CE; 772 sen-
tences were used, 500 for training (300 positive, 200
negative), and 272 for testing (95 positive, 177 neg-
ative). All examples were labeled manually.
4.2 Evaluation
The classification used four different sets of fea-
tures. The first feature set includes every unigram
that appears at least three times in the whole train-
ing set. To improve the performance by attenuating
the sparse data problem, in the second feature set,
all names of diseases were replaced by the same tag
disease. This was done by pre-processing the text
using MetaMap to identify all diseases in both the
training and the test examples. Then the identified
diseases were replaced by the disease tag automat-
ically. As medications often are not mentioned in
outcomes, they were not generalized in this manner.
The third feature set represents changes described
in outcomes. Our observation is that outcomes of-
ten involve the change in a clinical value. For ex-
ample, after a medication was applied to a disease,
something was increased (enhanced, more, . . . ) or
decreased (reduced, less, . . . ). Thus the polarity
of an outcome is often determined by how change
happens: if a bad thing (e.g., mortality) is reduced
then it is a positive outcome; if the bad thing is in-
creased, then the outcome is negative. We try to
capture this observation by adding context features
to the feature set. The way they were added is sim-
ilar to incorporating the negation effect described
by Pang et al (2002). But instead of just finding a
?negation word? (not, isn?t, didn?t, etc.), we need to
find two groups of words: those indicating more and
those indicating less. In the training text, we found
9 words in the first group and 7 words in the second
group. When pre-processing text for classification,
following the method of Pang et al, we attached the
tag MORE to all words between the more-words
and the following punctuation mark, and the tag
LESS to the words after the less-words.
The fourth feature set is the combination of the
effects of feature set two and three. In representing
each sentence by a feature vector, we tested both
presence (feature appears or not) and frequency
(count the number of occurrences of the feature in
the sentence).
The accuracy of the classification is shown in Ta-
ble 5. The baseline is to assign a random class (here
we use negative, as they are more frequent in the test
set) to all test samples.
The presence of features performs better than fre-
quency of features in general. Using a more gen-
eral category instead of specific diseases has a pos-
itive effect on the presence-based classification. We
speculate that the effect of this generalization will
be bigger if a larger test set were used. Pang et al
(2002) did not compare the result of using and not
using the negation context effect, so it is not clear
how much it improved their result. In our task, it
is clear that the MORE/ LESS feature has a signif-
icant effect on the performance, especially for the
frequency features.
Table 5: Results of outcome polarity classification
Presence Frequency
Features (%) (%)
Baseline 65.07 65.07
Original unigrams 88.97 87.87
Unigrams with disease 90.07 88.24
Unigrams with
MORE/ LESS tag 91.54 91.91
Unigrams with disease
and MORE/ LESS tag 92.65 92.28
5 Conclusion
We have described our work in medical text anal-
ysis by identifying semantic classes and the rela-
tions between them. Our work suggests that seman-
tic classes in medical scenarios play an important
role in understanding medical text. The scenario
view may be extended to a framework that acts as
a guideline for further semantic analysis.
Semantic classes and their relations have di-
rect applications in medical question answering and
query refinement in information retrieval. In ques-
tion answering, the question and answer candidates
will contain some semantic classes. After identify-
ing them on both sides, the question can be com-
pared with the answer to find whether there is a
match. In information retrieval, relations between
semantic classes can be added to the index. If the
query posed by the user is too general, the system
will ask the user to refine the query by adding more
concepts and even relations so that it will be more
pertinent according to the content of the source. For
example, a user may search for a document describ-
ing the comparison of aspirin and placebo. Instead
of just using aspirin and placebo as the query terms,
the user can specify the comparison relation as well
in the query.
We will continue working on the second level of
the semantic analysis, to explore the relations on
the scenario level. A complete scenario contains all
three semantic classes. One scenario may be the ex-
planation or justification of the previous scenario(s),
or contradictory to the previous scenario(s). De-
tecting these relationships will be of great help for
understanding-based tasks, such as context-related
question answering, topic-related summarization,
etc. As different scenarios might not be adjacent to
each other in the texts, classical rhetorical analysis
cannot provide a complete solution for this problem.
Acknowledgements
The EpoCare project is supported by grants from
Bell University Laboratories at the University of
Toronto. Our work is also supported by a grant
from the Natural Sciences and Engineering Re-
search Council of Canada and an Ontario Graduate
Scholarship. We are grateful to Sharon Straus, MD,
and other members of the EpoCare project for dis-
cussion and assistance.
References
Alan R. Aronson. 2001. Effective mapping of
biomedical text to the UMLS metathesaurus: The
MetaMap program. In Proceedings of Ameri-
can Medical Informatics Association Symposium,
pages 17?21.
Collin F. Baker, Charles J. Fillmore, and Beau
Cronin. 2003. The structure of the Framenet
database. International Journal of Lexicography,
16(3):281?296.
Stuart Barton. 2002. Clinical Evidence. BMJ Pub-
lishing Group, London.
Neus Catala`, Nu?ria Castell, and Mario Mart?in.
2003. A portable method for acquiring infor-
mation extraction patterns without annotated cor-
pora. Natural Language Engineering, 9(2):151?
179.
Daniel Gildea and Daniel Jurafsky. 2002. Auto-
matic labeling of semantic roles. Computational
Linguistics, 28(3):245?288.
Thorsten Joachims. 1998. Text categorization with
support vector machines: Learning with many
relevant features. In Proceedings of the European
Conference on Machine Learning (ECML), pages
137?142.
Thorsten Joachims. 2002. SVMlight homepage. In
http://svmlight.joachims.org/.
Ki-Joong Lee, Young-Sook Hwang, and Hae-Chang
Rim. 2003. Two-phase biomedical NE recog-
nition based on SVMs. In Proceedings of 41st
annual meeting of the Association for Compu-
tational Linguistics, Workshop on Natural Lan-
guage Processing in Biomedicine, pages 33?40.
Donald A. B. Lindberg, Betsy L. Humphreys, and
Alexa. T. McCray. 1993. The Unified Medical
Language System. Methods of Information in
Medicine, 32(4):281?291.
Yun Niu, Graeme Hirst, Gregory McArthur, and
Patricia Rodriguez-Gianolli. 2003. Answering
clincal questions with role identification. In Pro-
ceedings of 41st annual meeting of the Associ-
ation for Computational Linguistics, Workshop
on Natural Language Processing in Biomedicine,
pages 73?80.
Bo Pang, Lillian Le, and Shivakumar
Vaithyanathan. 2002. Thumbs up? Senti-
ment classification using machine learning
techniques. In Proceedings of 2002 Conference
on Empirical Methods in Natural Language
Processing (EMNLP), pages 79?86.
David L. Sackett and Sharon E. Straus. 1998. Find-
ing and applying evidence during clinical rounds:
The ?evidence cart?. Journal of the American
Medical Association, 280(15):1336?1338.
David L. Sackett, Sharon E. Straus, W. Scott
Richardson, William Rosenberg, and R. Brian
Haynes. 2000. Evidence-Based Medicine: How
to Practice and Teach EBM. Harcourt Publishers
Limited, Edinburgh.
Satoshi Sekine. 1997. Apple pie parser homepage.
In http://nlp.cs.nyu.edu/app/.
Dan Shen, Jie Zhang, Guodong Zhou, Jian Su, and
Chew-Lim Tan. 2003. Effective adaptation of
hidden Markov model?based named entity rec-
ognizer for biomedical domain. In Proceedings
of 41st annual meeting of the Association for
Computational Linguistics, Workshop on Natural
Language Processing in Biomedicine, pages 49?
56.
Sharon E. Straus and David L. Sackett. 1999.
Bringing evidence to the point of care. Journal
of the American Medical Association, 281:1171?
1172.
Extracting Causal Knowledge from a Medical Database
Using Graphical Patterns
Christopher S.G. Khoo, Syin Chan and Yun Niu
Centre for Advanced Information Systems, School of Computer Engineering
Blk N4, Rm2A-32, Nanyang Avenue
Nanyang Technological University
Singapore 639798
assgkhoo@ntu.edu.sg; asschan@ntu.edu.sg; niuy n@hotmail.com
Abstract
This paper reports the first part of a project
that aims to develop a knowledge extrac-
tion and knowledge discovery system that
extracts causal knowledge from textual da-
tabases. In this initial study, we develop a
method to identify and extract cause-effect
information that is explicitly expressed in
medical abstracts in the Medline database.
A set of graphical patterns were constructed
that indicate the presence of a causal rela-
tion in sentences, and which part of the
sentence represents the cause and which
part represents the effect. The patterns are
matched with the syntactic parse trees of
sentences, and the parts of the parse tree
that match with the slots in the patterns are
extracted as the cause or the effect.
1 Introduction
Vast amounts of textual documents and data-
bases are now accessible on the Internet and the
World Wide Web. However, it is very difficult
to retrieve useful information from this huge
disorganized storehouse. Programs that can
identify and extract useful information, and re-
late and integrate information from multiple
sources are increasingly needed. The World
Wide Web presents tremendous opportunities
for developing knowledge extraction and knowl-
edge discovery programs that automatically ex-
tract and acquire knowledge about a domain by
integrating information from multiple sources.
New knowledge can be discovered by relating
disparate pieces of information and by infer-
encing from the extracted knowledge.
This paper reports the first phase of a project
to develop a knowledge extraction and knowl-
edge discovery system that focuses on causal
knowledge. A system is being developed to
identify and extract cause-effect information
from the Medline database ? a database of ab-
stracts of medical journal articles and conference
papers. In this initial study, we focus on cause-
effect information that is explicitly expressed
(i.e. indicated using some linguistic marker) in
sentences. We have selected four medical areas
for this study ? heart disease, AIDS, depression
an  schizophrenia.
The medical domain was selected for two
reasons:
1. The causal relation is particular important in
medicine, which is concerned with devel-
oping treatments and drugs that can effect a
cure for some disease
2. Because of the importance of the causal re-
lation in medicine, the relation is more likely
to be explicitly indicated using linguistic
means (i.e. using words such as result, ef-
fect, cause, etc.).
2 Previous Studies
The goal of information extraction research is to
develop systems that can identify the passage(s)
in a document that contains information that is
relevant to a prescribed task, extract the infor-
mation and relate the pieces of information by
filling a structured template or a database record
(Cardie, 1997; Cowie & Lehnert, 1996; Gai-
zauskas & Wilks, 1998).
Information extraction research has been
influenced tremendously by the series of Mes-
sage Understanding Conferences (MUC-5,
MUC-6, MUC-7), organized by the U.S. Ad-
vanced Research Projects Agency (ARPA)
(http://www.muc.saic.com/proceedings/proceedi
ngs_index.html). Participants of the conferences
develop systems to perform common informa-
tion extraction tasks, defined by the conference
organizers.
For each task, a template is specified that
indicates the slots to be filled in and the type of
information to be extracted to fill each slot. The
set of slots defines the various entities, aspects
and roles relevant to a prescribed task or topic of
interest. Information that has been extracted can
be used for populating a database of facts about
entities or events, for automatic summarization,
for information mining, and for acquiring
knowledge to use in a knowledge-based system.
Information extraction systems have been devel-
oped for a wide range of tasks. However, few of
them have focused on extracting cause-effect
information from texts.
Previous studies that have attempted to ex-
tract cause-effect information from text have
mostly used knowledge-based inferences to infer
the causal relations. Selfridge, Daniell & Sim-
mons (1985) and Joskowsicz, Ksiezyk &
Grishman (1989) developed prototype computer
programs that extracted causal knowledge from
short explanatory messages entered into the
knowledge acquisition component of an expert
system. When there was an ambiguity whether a
causal relation was expressed in the text, the
systems used a domain model to check whether
such a causal relation between the events was
possible.
Kontos & Sidiropoulou (1991) and Kaplan
& Berry-Rogghe (1991) used linguistic patterns
to identify causal relations in scientific texts, but
the grammar, lexicon, and patterns for identify-
ing causal relations were hand-coded and devel-
oped just to handle the sample texts used in the
studies. Knowledge-based inferences were also
used. The authors pointed out that substantial
domain knowledge was needed for the system to
identify causal relations in the sample texts ac-
curately.
More recently, Garcia (1997) developed a
computer program to extract cause-effect infor-
mation from French technical texts without us-
ing domain knowledge. He focused on causative
verbs and reported a precision rate of 85%.
Khoo, Kornfilt, Oddy & Myaeng (1998) devel-
oped an automatic method for extracting cause-
effect information from Wall Street Journal texts
using linguistic clues and pattern matching.
Their system was able to extract about 68% of
the causal relations with an error rate of about
36%.
The emphasis of the current study is on ex-
tracting cause-effect information that is explic-
itly expressed in the text without knowledge-
based inferencing. It is hoped that this will result
in a method that is more easily portable to other
subject areas and document collections. We also
make use of a parser (Conexor?s FDG parser) to
construct syntactic parse trees for the sentences.
Graphical extraction patterns are constructed to
extract information from the parse trees. As a
result, a much smaller number of patterns need
be constructed. Khoo et al (1998) who used
only part-of-speech tagging and phrase bracket-
ing, but not full parsing, had to construct a large
number of extraction patterns.
3 Initial Analysis of the Medical Texts
200 abstracts were downloaded from the Med-
line database for use as our training sample of
texts. They are from four medical areas: depres-
sion, schizophrenia, heart disease and AIDs
(fifty abstracts from each area). The texts were
analysed to identify:
1. the different roles and attributes that are in-
volved in a causal situation. Causeand effect
are, of course, the main roles, but other roles
also exist including enabling conditions, size
of the effect, and size of the cause (e.g. dos-
age).
2. the various linguistic markers used by the
writers to explicitly signal the presence of a
causal relation, e.g. as a result, affect, re-
duce, etc.
3.1 Cause-effect template
The various roles and attributes of causal situ-
tions identified in the medical abstracts are
s ructured in the form of a template. There are
three levels in our cause-effect template, Level 1
giving the high-level roles and Level 3 giving
the most specific sub-roles. The first two levels
are given in Table 1. A more detailed description
i  provided in Khoo, Chan & Niu (1999).
The information extraction system devel-
oped in this initial study attempts to fill only the
m in slots of cause, effect and modality, without
attempting to divide the main slots into subslots.
Table 1. The cause-effect template
Level 1 Level 2
Object
State/EventCause
Size
Object
State/EventEffect
Size
Polarity (e.g. ?Increase?, ?Decrease?,
etc.)
Object
State/Event
Size
Duration
Condition
Degree of necessity
Modality (e.g. ?True?, ?False?,
?Probable?, ?Possible?, etc.)
Research method
Sample size
Significance level
Information source
Evidence
Location
Type of causal relation
Table 2. Common causal expressions for
depression & schizophrenia
Expression No. of
Occurrences
 causative verb  69
 effect (of) ?(on)  51
 associate with  35
 treatment of  31
 have effect on  28
 treat with  26
 treatment with  22
 effective (for)  14
 related to  10
Table 3. Common causal expressions for
AIDs & heart disease
 Expression  No. of
Occurrences
 causative verb  119
 have effect on  30
 effect (of)?(on)  25
 due to  20
 associate with  19
 treat with  15
 causative noun (including
nominalized verbs)
 12
 effective for  10
3.2 Causal expressions in medical texts
Causal relations are expressed in text in various
ways. Two common ways are by using causal
links and causative verbs. Causal links are words
used to link clauses or phrases, indicating a
causal relation between them. Altenburg (1984)
provided a comprehensive typology of causal
links. He classified them into four main types:
the adverbial link (e.g. hence, therefore), the
prepositional link (e.g. because of, on account
of), subordination (e.g. because, as, since, for,
so) and the clause-integrated line (e.g. that?s
why, the result was). Causative verbs are transi-
tive action verbs that express a causal relation
between the subject and object or prepositional
phrase of the verb. For example, the transitive
verb break can be paraphrased as to cause to
break, and the transitive verb kill can be para-
phrased as to cause to die.
We analyzed the 200 training abstracts to
identify the linguistic markers (such as causal
links and causative verbs) used to indicate causal
relations explicitly. The most common linguistic
expressions of cause-effect found in the Depres-
sion and Schizophrenia bstracts (occurring at
least 10 times in 100 abstracts) are listed in Ta-
ble 2. The common expressions found in the
AIDs and Heart Disease abstracts (with at least
10 occurrences) are listed in Table 3. The ex-
pressions listed in the two tables cover about
70% of the explicit causal expressions found in
the sample abstracts. Six expressions appear in
both tables, indicating a substantial overlap in
the two groups of medical areas. The most fr-
quent way of expressing cause and effect is by
using causative verbs.
4 Automatic Extraction of Cause-
Effect Information
The information extraction process used in this
study makes use of pattern matching. This is
similar to methods employed by other research-
ers for information extraction. Whereas most
studies focus on particular types of events or
topics, we are focusing on a particular type of
relation. Furthermore, the patterns used in this
study are graphical patterns that are matched
with syntactic parse trees of sentences. The pat-
terns represent different words and sentence
structures that indicate the presence of a causal
relation and which parts of the sentence repr-
sent which roles in the causal situation. Any part
of the sentence that matches a particular pattern
is considered to describe a causal situation, and
the words in the sentence that match slots in the
pattern are extracted and used to fill the appro-
priate slots in the cause-effect template.
4.1 Parser
The sentences are parsed using Conexor?s Func-
tional Dependency Grammar of English (FDG)
parser (http://www.conexor.fi), which generates
a representation of the syntactic structure of the
sentence (i.e. the parse tree). For the example
sentence
Paclitaxel was well tolerated and resulted in a
significant clinical response in this patient.
a graphical representation of the parser output is
given in Fig. 1. For easier processing, the syn-
tactic structure is converted to the linear con-
ceptual graph formalism (Sowa, 1984) given in
Fig. 2.
A conceptual graph is a graph with the
nodes representing concepts and the directed
arcs representing relations between concepts.
Although the conceptual graph formalism was
developed primarily for semantic representation,
we use it to represent the syntactic structure of
sentences. In the linear conceptual graph nota-
tion, concept labels are given within square
brackets and relations between concepts are
Fig. 1. Syntactic structure of a sentence
given within parentheses. Arrows indicate the
direction of the relations.
4.2 Construction of causality patterns
We developed a set of graphical patterns that
specifies the various ways a causal relation can
be explicitly expressed in a sentence. We call
them causality patterns. The initial set of pat-
terns was constructed based on the training set
of 200 abstracts mentioned earlier. Each abstract
was analysed by two of the authors to identify
the sentences containing causal relations, and the
p rts of the sentences representing the cause and
the effect. For each sentence containing a causal
relation, the words (causality identifiers) that
were used to signal the causal relation were also
iden ified. These are mostly causal links and
causative verbs described earlier.
Example sentence
Paclitaxel was well tolerated and resulted in a
significant clinical response in this patient.
Syntactic structure in linear conceptual
gr ph format
[tolerate]-
   (vch)->[be]->(subj)->[paclitaxel]
   (man)->[well]
   (cc)->[and]
   (cc)->[result]-
(loc)->[in]->(pcomp)->[response]-
(det)->[a]
(attr)->[clinical]->(attr)
->[significant],
(phr)->[in]->(pcomp)->[patient]
->(det)->[this],,.
Example causality pattern
[*]-
&(v-ch)->(subj)->[T:cause.object]
(cc|cnd)->[result]+-
(loc)+->[in]+->(pcomp)
->[T:effect.event]
(phr)->[in]->(pcomp)
->[T:effect.object],,.
Cause-effect template
Cause: paclitaxel
Effect: a significant clinical response in this
patient
Fig. 2. Sentence structure and causality
pattern in conceptual graph format
main
root
tolerate
be
v-ch
well and
man cc result
cc
in in
loc phr
response
pcomp
patient
pcomp
clinical
attr
a
det
this
det
significant
attr
We constructed the causality patterns for
each causality identifier, to express the different
sentence constructions that the causality identi-
fier can be involved in, and to indicate which
parts of the sentence represent the cause and the
effect. For each causality identifier, at least 20
sentences containing the identifier were ana-
lysed. If the training sample abstracts did not
have 20 sentences containing the identifier, ad-
ditional sentences were downloaded from the
Medline database. After the patterns were con-
structed, they were applied to a new set of 20
sentences from Medline containing the identi-
fier. Measures of precision and recall were cal-
culated. Each set of patterns are thus associated
with a precision and a recall figure as a rough
indication of how good the set of patterns is.
The causality patterns are represented in lin-
ear conceptual graph format with some exten-
sions. The symbols used in the patterns are as
follows:
1. Concept nodes take the following form:
[concept_label] or [concept_label:
role_indicator]. Concept_label can be:
? a character string in lower case, represent-
ing a stemmed word
? a character string in uppercase, refering to a
class of synonymous words that can occupy
that place in a sentence
? ?*?, a wildcard character that can match
any word
? ?T?, a wildcard character that can match
with any sub-tree.
Role_indicator refers to a slot in the cause-
effect template, and can take the form:
? role_label which is the name of a slot in the
cause-effect template
? role_label = ?value?, where value is a
character string that should be entered in
the slot in the cause-effect template (if
?value? is not specified, the part of the
sentence that matches the conc pt_label is
entered in the slot).
2. Relation nodes take the following form:
(set_of_relations). Set_of_relations can be:
? a relation_label, which is a character string
representing a syntactic relation (these are
the relation tags used by Conexor?s FDG
parser)
? relation_label | set of relations (?|? indi-
cates a logical ?or?)
3. &subpattern_label refers to a set of sub-
graphs.
Each node can also be followed by a ?+?
indicating that the node is mandatory. If the
mandatory nodes are not found in the sentence,
then the pattern is rejected and no information is
extracted from the sentence. All other nodes are
optional. An example of a causality pattern is
given in Fig. 2.
4.3 Pattern matching
The information extraction process involves
matching the causality patterns with the parse
trees of the sentences. The parse trees and the
ca sality patterns are both represented in the
linear conceptual graph notation. The pattern
matching for each sentence follows the follow-
ing procedure:
1. the causality identifiers that match with
keywords in the sentence are identified,
2. the causality patterns associated with each
matching causality identifier are shortlisted,
3. for each shortlisted pattern, a matching pro-
cess is carried out on the sentence.
The matching process involves a kind of
spreading activation in both the causality pattern
graph and the sentence graph, starting from the
node representing the causality identifier. If a
pattern node matches a sentence node, the
matching node in the pattern and the sentence
are activated. This activation spreads outwards,
with the causality identifier node as the center.
When a pattern node does not match a sentence
node, then the spreading activation stops for that
branch of the pattern graph. Procedures are at-
tached to the nodes to check whether there is a
match and to extract words to fill in the slots in
the cause-effect template. The pattern matching
program has been implemented in Java (JDK
1.2.1). An example of a sentence, matching pat-
tern and filled template is given in Fig. 2.
5 Evaluation
A total of 68 patterns were constructed for the
35 causality identifiers that occurred at least
twice in the training abstracts. The patterns were
appli d to two sets of new abstracts downloaded
from Medline: 100 new abstracts from the origi-
nal four medical areas (25 abstracts from each
area), and 30 abstracts from two new domains
(15 each) ? digestive system diseases and respi-
ratory tract diseases. Each test abstract was
analyzed by at least 2 of the authors to identify
?medically relevant? cause and effect. A fair
number of causal relations in the abstracts are
trivial and not medically relevant, and it was felt
that it would not be useful for the information
extraction system to extract these trivial causal
relations.
Of the causal relations manually identified
in the abstracts, about 7% are implicit (i.e. have
to be inferred using knowledge-based inferenc-
ing) or occur across sentences. Since the focus
of the study is on explicitly expressed cause and
effect within a sentence, only these are included
in the evaluation. The evaluation results are pre-
sented in Table 4. Recall is the percentage of the
slots filled by the human analysts that are cor-
rectly filled by the computer program. Precision
is the percentage of slots filled by the computer
program that are correct (i.e. the text entered in
the slot is the same as that entered by the human
analysts). If the text entered by the computer
program is partially correct, it is scored as 0.5
(i.e. half correct). The F-measure given in Table
4 is a combination of recall and precision
equally weighted, and is calculated using the
formula (MUC-7):
2*precision*recall / (precision + recall)
Table 4. Extraction results
Slot Recall
Preci-
sion
F-
Measure
Results for 100 abstracts from the
original 4 medical areas
Causality
Identifier
.759 .768 .763
Cause .462 .565 .508
Effect .549 .611 .578
Modality .410 .811 .545
Results for 30 abstracts from 2 new
medical areas
Causality
Identifier
.618 .759 .681
Cause .415 .619 .497
Effect .441 .610 .512
Modality .542 .765 .634
For the 4 medical areas used for building the
extraction patterns, the F-measure for the cause
and effect slots are 0.508 and 0.578 respectively.
If implicit causal relations are included in the
evaluation, the recall measures for cause and
effect are 0.405 and 0.481 respectively, yielding
an F-measure of 0.47 for cause and 0.54 for ef-
fect. The results are not very good, but not very
bad either for an information extraction task.
For the 2 new medical areas, we can see in
Table 4 that the precision is about the same as
for the original 4 medical areas, indicating that
the current extraction patterns work equally well
in th  new areas. The lower recall indicates that
n w causality identifiers and extraction patterns
need to be constructed.
The sources of errors were analyzed for the
set of 100 test abstracts and are summarized in
Table 5. Most of the spurious extractions (in-
formation extracted by the program as cause or
effect but not identified by human analysts) were
actually causal relations that were not medically
relevant. As mentioned earlier, the manual iden-
tification of causal relations focused on medi-
cally relevant causal relations. In the cases
wher  the program did not correctly extract
cause and effect information identified by the
analysts, half were due to incorrect parser out-
put, and in 20% of the cases, causality patterns
have not been constructed for the causality iden-
tifier found in the sentence.
We also analyzed the instances of implicit
causal relations in sentences, and found that
many of them can be identified using some
amount of semantic analysis. Some of them in-
volve words like when, after and with that indi-
cate a time sequence, for example:
? The results indicate that changes to 8-OH-
DPAT and clonidine-induced responses oc-
cur quicker with the combination treatment
than with either reboxetine or sertraline
treatments alone.
? There are also no reports of serious adverse
events when lithium is added to a monoam-
ine oxidase inhibitor.
? Four days after flupenthixol administration,
the patient developed orolingual dyskinetic
movements involving mainly tongue biting
and protrusion.
Table 5. Sources of Extraction Errors
A. Spurious errors (the program identified
cause or effect not identified by the hu-
man judges)
A1.The relations extraced are not relevant to medi-
cine or disease. (84.1%)
A2.Nominalized or adjectivized verbs are identified
as causative verbs by the program because of
parser error. (2.9%)
A3.Some words and sentence constructions that are
used to indicate cause-effect can be used to indi-
cate other kinds of relations as well. (13.0%)
B. Missing slots (cause or effect not ex-
tracted by program), incorrect text ex-
tracted, and partially correct extraction
B1.Complex sentence structures that are not in-
cluded in the pattern. (18.8%)
B2.The parser gave the wrong syntactic structure of
a sentence. (49.2%)
B3.Unexpected sentence structure resulting in the
program extracting information that is actually
not a cause or effect. (1.5%)
B4.Patterns for the causality identifier have not been
constructed. (19.6%)
B5.Sub-tree error. The program extracts the relevant
sub-tree (of the parse tree) to fill in the cause or
effect slot. However, because of the sentence
construction, the sub-tree includes both the cause
and effect resulting in too much text being ex-
tracted. (9.5%)
B6.Errors caused by pronouns that refer to a phrase
or clause within the same sentence. (1.3%)
In these cases, a treatment or drug is associated
with a treatment response or physiological event.
If noun phrases and clauses in sentences can be
classified accurately into treatments and treat-
ment responses (perhaps by using Medline?s
Medical Subject Headings), then such implicit
causal relations can be identified automatically.
Another group of words involved in implicit
causal relations are words like receive, get and
take, that indicate that the patient received a
drug or treatment, for example:
? The nine subjects who received p24-VLP
and zidovudine had an augmentation and/or
broadening of their CTL response compared
with baseline (p = 0.004).
Such causal relations can also be identified by
semantic analysis and classifying noun phrases
and clauses into treatments and treatment r-
sponses.
6. Conclusion
We have described a method for performing
automatic extraction of cause-effect information
from textual documents. We use Conexor?s FDG
parser to construct a syntactic parse tree for each
target sentence. The parse tree is matched with a
set of graphical causality patterns that indicate
the presence of a causal relation. When a match
is found, various attributes of the causal relation
(e.g. the cause, the effect, and the modality) can
then be extracted and entered in a cause-effect
template.
The accuracy of our extraction system is not
yet satisfactory, with an accuracy of about 0.51
(F-measure) for extracting the cause and 0.58
for extracting the effect that are explicitly ex-
pressed. If both implicit and explicit causal rela-
tions are included, the accuracy is 0.41 for cause
and 0.48 for effect. We were heartened to find
that when the extraction patterns were applied to
2 new medical areas, the extraction precision
was the same as for the original 4 medical areas.
Future work includes:
1. Constructing patterns to identify causal re-
lations across sentences
2. Expanding the study to more medical areas
3. Incorporating semantic analysis to extract
implicit cause-effect information
4. Incorporating discourse processing, includ-
ing anaphor and co-reference resolution
5. Developing a method for constructing ex-
traction patterns automatically
6. Investigating whether the cause-effect in-
formation extracted can be chained together
to synthesize new knowledge.
Two aspects of discourse processing is being
studied: co-reference resolution and hypothesis
confirmation. Co-reference resolution is impor-
t nt for two reasons. The first is the obvious rea-
son that to extract complete cause-effect infor-
mation, pronouns and references have to be
r solved and replaced with the information that
they refer to. The second reason is that quite of-
ten a causal relation between two events is ex-
pressed more than once in a medical abstract,
each time providing new information about the
causal situation. The extraction system thus
needs to be able to recognize that the different
causal expressions refer to the same causal
situation, and merge the information extracted
from the different sentences.
The second aspect of discourse processing
being investigated is what we refer to as hy-
pothesis confirmation. Sometimes, a causal rela-
tion is hypothesized by the author at the begin-
ning of the abstract. This hypothesis may be
confirmed or disconfirmed by another sentence
later in the abstract. The information extraction
system thus has to be able to link the initial hy-
pothetical cause-effect expression with the con-
firmation or disconfirmation expression later in
the abstract.
Finally, we hope eventually to develop a
system that not only extracts cause-effect infor-
mation from medical abstracts accurately, but
also synthesizes new knowledge by chaining the
extracted causal relations. In a series of studies,
Swanson (1986) has demonstrated that logical
connections between the published literature of
two medical research areas can provide new and
useful hypotheses. Suppose an article reports
that A causes B, and another article reports that
B causes C, then there is an implicit logical link
between A and C (i.e. A causes C). This relation
would not become explicit unless work is done
to extract it. Thus, new discoveries can be made
by analysing published literature automatically
(Finn, 1998; Swanson & Smalheiser, 1997).
References
Altenberg, B. (1984). Causal linking in spoken and
written English. Studia Linguistica, 38(1), 20-69.
Cardie, C. (1997). Empirical methods in information
extraction. AI Magazine, 18(4), 65-79.
Cowie, J., & Lehnert, W. (1996). Information extrac-
tion. Communications of the ACM, 39(1), 80-91.
Finn, R. (1998). Program Uncovers Hidden Connec-
tions in the Literature. Th  Scientist, 12( 0), 12-13.
Gaizauskas, R., & Wilks, Y. (1998). Information
extraction beyond document retrieval. Journ l of
Documentation, 54(1), 70-105.
Garcia, D. (1997). COATIS, an NLP system to locate
expressions of actions connected by causality links.
In Knowledge Acquisition, Modeling and Ma-
agement, 10th European Workshop, EKAW ?97
Proceedings (pp. 347-352). Berlin: Springer-
Verlag.
Joskowsicz, L., Ksiezyk, T., & Grishman, R. (1989).
Deep domain models for discourse analysis. In The
Annual AI Systems in Government Conference (pp.
195-200). Silver Spring, MD: IEEE Computer So-
ciety.
Kaplan, R. M., & Berry-Rogghe, G. (1991). Knowl-
edge-based acquisition of causal relationships in
text. Knowledge Acquisition, 3(3), 317-337.
Khoo, C., Chan, S., Niu, Y., & Ang, A. (1999). A
method for extracting causal knowledge from tex-
tual databases. Singapore Journal of Library &
Information Management, 28, 48-63.
Khoo, C.S.G., Kornfilt, J., Oddy, R.N., & Myaeng,
S.H. (1998). Automatic extraction of cause-effect
information from newspaper text without knowl-
edge-based inferencing. Literary and Linguistic
Computing, 13(4), 177-186.
Kontos, J., & Sidiropoulou, M. (1991). On the acqui-
sition of causal knowledge from scientific texts
with attribute grammars. Expert Systems for Infor-
mation Management, 4(1), 31-48.
MUC-5. (1993). Fifth Message Understanding Co-
fer nce (MUC-5). San Francisco: Morgan Kauf-
mann.
MUC-6. (1995). Sixth Message Understanding Con-
ference (MUC-6). San Francisco: Morgan Kauf-
mann.
MUC-7. (2000).  Message Understanding Confer-
e ce proceedings (MUC-7) [Online]. Available:
http://www.muc.saic.com/proceedings/muc_7_toc.
html.
Selfri ge, M., Daniell, J., & Simmons, D. (1985).
Learning causal models by understanding real-
world natural language explanations. In The Sec-
ond Conference on Artificial Intelligence Applica-
tions: The Engineering of Knowledge-Based Sys-
tems (pp. 378-383). Silver Spring, MD: IEEE
Computer Society.
Sowa, J.F. (1984). Conceptual structures: Informa-
 processing in man and machine. Reading,
MA: Addison-Wesley,.
Swanson, D.R. (1986). Fish oil, Raynaud?s Syn-
drome, and undiscovered public knowledge. Per-
spectives in Biology and Medicine, 30(1), 7-18.
Swanson, D.R., & Smalheiser, N.R. (1997). An inter-
active system for finding complementary litera-
tures: A stimulus to scientific discovery. Artificial
Intelligence, 91, 183-203.
