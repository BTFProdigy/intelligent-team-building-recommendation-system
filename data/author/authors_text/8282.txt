Wordform- and class-based prediction of the components
of German nominal compounds in an AAC system
Marco Baroni Johannes Matiasek
Austrian Research Institute for
Artificial Intelligence
Schottengasse 3,
A-1010 Vienna, Austria
{marco,john}@oefai.at
Harald Trost
Department of Medical Cybernetics and
Artificial Intelligence, University of Vienna
Freyung 6/2
A-1010 Vienna, Austria
harald@ai.univie.ac.at
Abstract
In word prediction systems for augmentative and al-
ternative communication (AAC), productive word-
formation processes such as compounding pose a
serious problem. We present a model that predicts
German nominal compounds by splitting them into
their modifier and head components, instead of try-
ing to predict them as a whole. The model is im-
proved further by the use of class-based modifier-
head bigrams constructed using semantic classes
automatically extracted from a corpus. The eval-
uation shows that the split compound model with
class bigrams leads to an improvement in keystroke
savings of more than 15% over a no split compound
baseline model. We also present preliminary results
obtained with a word prediction model integrating
compound and simple word prediction.
1 Introduction
N-gram language modeling techniques have been
successfully embedded in a number of natural lan-
guage processing applications, including word pre-
dictors for augmentative and alternative communi-
cation (AAC). N-gram based techniques rely cru-
cially on the assumption that the large majority of
words to be predicted have also occurred in the cor-
pus used to train the models.
Productive word-formation by compounding in
languages such as German, Dutch, the Scandina-
vian languages and Greek, where compounds are
commonly written as single orthographic words, is
problematic for this assumption.
Productive compounding implies that a sizeable
number of new words will constantly be added to
the language. Such words cannot, in principle, be
contained in any already existing training corpus,
no matter how large. Moreover, the training cor-
pus itself is likely to contain a sizeable number of
newly formed compounds that, as such, will have
an extremely low frequency, causing data sparse-
ness problems.
New compounds, however, differ from other
types of new/rare words in that, while they are rare,
they can typically be decomposed into more com-
mon smaller units (the words that were put together
to form them). For example, in the corpus we an-
alyzed, Abend ?evening? and Sitzung ?session?, the
two components of the German compound Abend-
sitzung ?evening session?, are much more frequent
words than the latter. Thus, a natural way to handle
productively formed compounds is to treat them not
as primitive units, but as the concatenation of their
components.
A model of this sort will be able to predict newly
formed compounds that never occurred in the train-
ing corpus, as long as they can be analyzed as the
concatenation of constituents that did occur in the
training corpus. Moreover, a model of this sort
avoids the specific type of data sparseness problems
caused by newly formed compounds in the training
corpus, since it collects statistics based on their (typ-
ically more frequent) components.
Building upon previous work (Spies, 1995;
Carter et al, 1996; Fetter, 1998; Larson et al,
2000), Baroni et al (2002) reported encouraging
results obtained with a model in which two-element
nominal German compounds are predicted by treat-
ing them as the concatenation of a modifier (left el-
ement) and a head (right element).
Here, we report of further improvements to this
model that we obtained by adding a class-based bi-
gram term to head prediction. As far as we know,
this it the first time that semantic classes auto-
matically extracted from the training corpus have
been used to enhance compound prediction, inde-
pendently of the domain of application of the pre-
diction model.
Moreover, we present the results of preliminary
experiments we conducted in the integration of
compound predictions and simple word predictions
within the AAC word prediction task.
The remainder of this paper is organized as fol-
lows. In section 2, we describe the AAC word pre-
diction task. In section 3, we describe the basic
properties of German compounds. In section 4, we
present our split compound prediction model, focus-
ing on the new class-based head prediction compo-
nent. In section 5, we report the results of simu-
lations run with the enhanced compound prediction
model. In section 6, we report about our prelimi-
nary experiments with the integration of compound
and simple word prediction. Finally, in section 7,
we summarize the main results we obtained and in-
dicate directions for further work.
2 Word prediction for AAC
Word prediction systems based on n-gram statistics
are an important component of AAC devices, i.e.,
software and possibly hardware typing aids for dis-
abled users (Copestake, 1997; Carlberger, 1998).
Word predictors provide the user with a predic-
tion window, i.e. a menu that, at any time, lists the
most likely next word candidates, given the input
that the user has typed until the current character.
If the word that the user intends to type next is in
the prediction window, the user can select it from
there. Otherwise, the user will keep typing letters,
until the target word appears in the prediction win-
dow (or until she finishes typing the word).
The (percentage) keystroke savings rate (ksr) is a
standard measure used in AAC research to evaluate
word predictors. The ksr can be thought of as the
percentage of keystrokes that a ?perfect? user would
save by employing the relevant word predictor to
type the test set, over the total number of keystrokes
that are needed to type the test set without using the
word predictor.
Usually, the ksr is defined by
ksr = (1? ki + ks
kn
) ? 100 (1)
where: ki is the number of input characters actually
typed, ks is the number of keystrokes needed to se-
lect among the predictions presented by the model
and kn is the number of keystrokes that would be
needed if the whole text was typed without any
prediction aid. Typically, the user will need one
keystroke to select among the predictions , and thus
we assume that ks equals 1.1
1In the split compound model, the user needs one keystroke
to select the modifier and one keystroke to select the head.
The ksr is influenced not only by the quality of
the prediction model but also by the size of the pre-
diction window. In our simulations, we use a 7 word
prediction window.
Ksr is not a function of perplexity, but it is gener-
ally true that there is an inverse correlation between
ksr and perplexity (Carlberger, 1998).
3 Compounding in German
Compounding is an extremely common and produc-
tive mean to form words in German.
In an analysis of the APA newswire corpus (a
corpus of over 28 million words), we found that al-
most half (47%) of the word types were compounds.
However, the compounds accounted for a small por-
tion of the overall token count (7%). This suggests
that, as expected, many of them are productively
formed hapax legomena or very rare words (83%
of the compounds had a corpus frequency of 5 or
lower).
By far the most common type of German com-
pound is the N+N type, i.e., a sequence of two
nouns (62% of the compounds in our corpus have
this shape). Thus, we decided to limit ourselves, for
now, to handling compounds of this shape.
In German, nominal compounds, including the
N+N type, are right-headed, i.e., the rightmost ele-
ment of the compound determines its basic semantic
and morphosyntactic properties.
Thus, the context of a compound is often more
informative about its right element (the head) than
about its left element (the modifier).
In modifier context, nouns are sometimes fol-
lowed by a linking suffix (Krott, 2001; Dressler et
al., 2001), or they take other special inflectional
shapes.
As a consequence of the presence of linking suf-
fixes and related patterns, the forms that nouns take
in modifier position are sometimes specific to this
position only, i.e., they are bound forms that do not
occur as independent words.
We did not parse special modifier forms in or-
der to reconstruct their independent nominal forms.
Thus, we treat all inflected modifier forms, includ-
ing bound forms, as unanalyzed primitive nominal
wordforms.
4 The split compound prediction model
In Baroni et al (2002), we present and evaluate a
split compound model in which N+N compounds
are predicted by treating them as the sequence of a
modifier and a head.
Modifiers are predicted on the basis of weighed
probabilities deriving from the following three
terms: the unigram and bigram training corpus fre-
quency of nominal wordforms as modifiers or in-
dependent words, and the training corpus type fre-
quency of nominal wordforms as modifiers:2
Pmod(w) = ?1P (w) + ?2P (w|c) + ?3Pismod(w) (2)
The type frequency of nouns as modifiers is de-
termined by the number of distinct compounds in
which a noun form occurs as modifier.
Heads are predicted on the basis of weighted
probabilities deriving from three terms analogous to
the ones used for modifiers: the unigram and bigram
frequency of nouns as heads or independent words,
and the type frequency of nouns as heads:
Phead(w) = ?1P (w) +?2P (w|c) +?3Pishead(w) (3)
The type frequency of nouns as heads is de-
termined by the number of distinct compounds in
which a noun form occurs as head.
Given that compound heads determine the syn-
tactic properties of compounds, bigrams for head
prediction are collected by considering not the im-
mediate left context of heads (i.e., their modifiers),
but the word preceding the compound (e.g., die
Abendsitzung is counted as an instance of the bi-
gram die Sitzung).
For reasons of size and efficiency, single uni- and
bigram count lists are used for predicting modifiers
and heads.3 For the same reasons, and to minimize
the chances of over-fitting to the training corpus, all
n-gram/frequency tables are trimmed by removing
elements that occur only once in the training corpus.
We currently use a simple interpolation model, in
which all terms are assigned equal weight.
4.1 Improving head prediction
While we obtained encouraging results with it (Ba-
roni et al, 2002), we feel that a particularly unsat-
isfactory aspect of the model described in the previ-
ous section is that information on the modifier is not
2Here and below, c stands for the last word in the left con-
text of w; w is the suffix of the word to be predicted minus
the (possibly empty) prefix typed by the user up to the current
point.
3This has a distorting effect on the bigram counts (words
occurring before compounds are counted twice, once as the left
context of the modifier and once as the left context of the head).
However, preliminary experiments indicated that the empirical
effect of this distortion is minimal.
exploited when trying to predict the head of a com-
pound. Intuitively, knowing what the modifier is
should help us in guessing the head of a compound.
However, constructing a plausible head-prediction
term based on modifier-head dependencies is not
straightforward.
The word-form-based compound-bigram fre-
quency of a head, i.e., the number of times a specific
head occurs after a specific modifier, is not a very
useful measure: Counting how often a modifier-
head pair occurs in the training corpus is equiv-
alent to collecting statistics on unanalyzed com-
pounds, and it will not help us to generalize beyond
the compounds encountered in the training corpus.
Moreover, if a specific modifier-head bigram is fre-
quent, i.e., the corresponding compound is a fre-
quent word, it is probably better to treat the whole
compound as an unanalyzed lexical unit anyway.
POS-based head-modifier bigrams are not going
to be of any help either, since we are considering
only N+N compounds, and thus we would collect a
single POS bigram (N N) with probability 1.4
We decided instead to try to exploit a
semantically-driven route. It seems plausible
that modifiers that are semantically related will
tend to co-occur with heads that are, in turn,
semantically related. Consider for example the
relationship between the class of fruits and the
class of sweets in English compounds. It is easy
to think of compounds in which a member of
the class of fruits (bananas, cherries, apricots...)
modifies a member of the class of sweets (pies,
cakes, muffins...). Thus, if you have to predict the
head of a compound given a fruit modifier, it would
be reasonable, all else being equal, to guess some
kind of sweet.
4.1.1 Class-based modifier-head bigrams
While semantically-driven prediction makes sense
in principle, clustering nouns into semantic classes
is certainly not a trivial job, and, if a large input lex-
icon must be partitioned, it is not a task that could
be accomplished by a human expert. Drawing inspi-
ration from Brown et al (1990), we constructed in-
stead semantic classes using a clustering algorithm
extracting them from a corpus, on the basis of the
average mutual information (MI) between pairs of
words (Rosenfeld, 1996).5
4Even if the model handled other compound types, very few
POS combinations are attested within compounds.
5We are aware of the fact that other measures of lexical as-
sociation have been proposed (Evert and Krenn, 2001, and
MI values were computed using Adam Berger?s
trigger toolkit (Berger, 1997).6 The same training
corpus of about 25.5M words (and with N+N com-
pounds split) that we describe below was used to
collect MI values for noun pairs. All modifiers and
heads of N+N compounds and all corpus words that
were parsed as nouns by the Xerox morphological
analyzer (Karttunen et al, 1997) were counted as
nouns for this purpose.
MI was computed only for pairs that co-occurred
at least three times in the corpus (thus, only a subset
of the input nouns appears in the output list). Valid
co-occurrences were bound by a maximal distance
between elements of 500 words, and a minimal dis-
tance of 2 words (to avoid lexicalized phrases, such
as proper names or phrasal loanwords).
Having obtained a list of pairs from the toolkit,
the next step was to cluster them into classes, by
grouping together nouns with a high MI. For space
reasons, we do not discuss our clustering algorithm
in detail here (we motivate and analyze the algo-
rithm in a paper currently in preparation).
In short, the algorithm starts by building classes
out of nouns that occur with very few other nouns in
the MI pair list, and thus their assignment to classes
is relatively unambiguous, and it then adds progres-
sively more ambiguous nouns (ambiguous in the
sense that they occur in a progressively larger num-
ber of MI pairs, and thus it becomes harder to deter-
mine with which other nouns they should be clus-
tered). Each input word is assigned to a single class
(thus, we do not try to capture polysemy). More-
over, not all words in the input are clustered (see
step 5 below).7
Schematically, the algorithm works as follows
(the input vocabulary of step 1 is simply a list of
all the words that occur at least once in the MI pair
references quoted there) and are sometimes claimed to be more
reliable than MI, and we are planning to run our clustering al-
gorithm using alternative measures.
6The trigger toolkit returns directional MI values (i.e., sepa-
rate MI values for the pairs N1 N2 and N2 N1). Since we were
not interested in directional information, we merged pairs con-
taining identical nouns by summing their MI. We realize that
this is not mathematically equivalent to computing symmetric
MI values, but it is a practical approximation that allowed us to
use the trigger toolkit for our purposes.
7We also experimented with an iterative version of the al-
gorithm that tried to cluster all words, through multiple passes.
The classes generated by the non-iterative procedure described
in the text, however, gave better results, when integrated in the
head prediction task, than those generated with the iterative ver-
sion.
list):
? step 1: Rank words in input vocabulary on the
basis of how often they occur in the MI pair list
(from least to most frequent);
? step 2: Shift top word from ranked list and de-
termine with the members of which existing
class it has the highest average mutual infor-
mation;
? step 3: If highest value found in step 2 is 0,
assign current word to new class; else, assign
it to class corresponding to highest value;
? step 4: If ranked list is not empty, go back to
step 2;
? step 5: Discard all classes that have only one
member.
This is a heuristic clustering procedure and there
is no guarantee that it will construct classes that
maximize MI. A cursory inspection of the output
list indicates that most classes constructed by our
algorithm are intuitively reasonable, while there are
also, undoubtedly, classes that contain heteroge-
neous elements, and missed generalizations. Table
1 reports a list of ten randomly selected classes that
were constructed using this procedure.
Alleinstehende, Singles, Alben, Platten, Platte, Sound,
Hits, Hit, Live, Songs, Single, Album, Pop, Studio,
Rock, Fans, Band
Atrophie, Hartung, Neurologe
Magische, Magie
Bilgen, Tivoli, Baur, Scharrer, Streiter, Winkel, Pfeffer,
Schmid, M
Effizienz, Transparenz
Harm, Radar, Jets, Flugzeugen, Typs, Abwehr, Raketen,
Maschinen, Angriffen, Flugzeuge, Kampf
Relegation, Birmingham, Stephen
Partnerschafts, Partnerschaft, Kooperation,
Bereichen, Aktivita?ten
Importeure, Zo?lle
Labyrinths, Labyrinth
Table 1: Randomly selected noun classes
The algorithm generated 3744 classes, containing
a total of 14059 nouns (about one third of the nouns
in the training corpus).
Class-based modifier-head bigrams were then
collected by labeling all the modifiers and heads in
the training corpus with their semantic classes, and
counting how often each combination of modifier
and head class occurred.
Like the other tables, class-based bigrams were
trimmed by removing elements with a frequency of
1.
4.1.2 The class-based head prediction model
We compute the class-based probability of a com-
pound head given its modifier in the following way:
Pclass(h|m) = P (Cl(h)|Cl(m))P (h|Cl(h)) (4)
where
P (Cl(h)|Cl(m)) =
count(Cl(m), Cl(h))
count(Cl(m))
(5)
and P (h|Cl(h)) = 1
|Cl(h)|
(6)
The latter term assigns equal probability to all
members of a class, but lower probability to mem-
bers of larger classes.
Class-based probability is added to the
wordform-based terms of equation 3 obtaining
the following formula to compute head probability:
Phead(w) = (7)
?1P (w) +?2P (w|c) +?3Pishead(w) +?4Pclass(w|m)
5 Evaluation
The new split compound model and a baseline
model with no compound processing were eval-
uated in a series of simulations, using the APA
newswire articles from January to September 1999
(containing 25,466,500 words) as the training cor-
pus, and all the 90,643 compounds found in the
Frankfurter Rundschau newspaper articles from
June 29 to July 12 of 1992 (in bigram context) as
the testing targets.8
In order to train and test the split compound
model, all words in both sets were run though the
morphological analyzer, and all N+N compounds
were split into their modifier and head surface
forms.
We first ran simulations in which compound
heads were predicted using each of the terms in
equation 7 separately. The results are reported in
table 2.
As an independent predictor, the class-based term
performs slightly worse than wordform-based bi-
gram prediction.
We then simulated head and compound predic-
tion using the head prediction model of equation 7.
8In other experiments, including those reported in Baroni
et al (2002), we tested on another section of the APA corpus
from the same year. Not surprisingly, ksr?s in the experiments
with the APA corpus were overall higher, and the difference
between the split compound and baseline models was less dra-
matic (because many compounds in the test set were already in
the training corpus).
model P (w) P (w|c) Pishead Pclass(w|m)
head ksr 42.2 30.0 47.1 29.4
Table 2: Predicting heads with single term models
The results of this simulation are reported in table
3, together with the results of a simulation in which
class-based prediction was not used, and the re-
sults obtained with the baseline no-split-compound
model.
Model split split no split
w/ classes no classes
head ksr 51.2 48.8 N/A
compound ksr 50.1 48.8 34.9
Table 3: Predicting heads and compounds
When used in conjunction with the other terms,
class bigrams lead to an improvement in head pre-
diction of more than 2% over the split compound
model without class-based prediction. This trans-
lates into an improvement of 1.3% in the prediction
of whole compounds. Overall, the split compound
model with class bigrams leads to an improvement
of more than 15% over the baseline model.
The results of these experiments confirm the use-
fulness of the split compound model, and they
also show that the addition of class-based predic-
tion improves the performance of the model, even
if this improvement is not dramatic. Clearly, fu-
ture research should concentrate on whether alterna-
tive measures of association, clustering techniques
and/or integration strategies can make class-based
prediction more effective.
6 Preliminary experiments in integration
In a working word prediction system, compounds
are obviously not the only type of words that the
user needs to type. Thus, the predictions provided
by the compound model must be integrated with
predictions of simple words. In this section, we re-
port preliminary results we obtained with a model
limited to the integration of N+N compound predic-
tion with simple noun prediction.
In our approach to compound/simple prediction
integration, candidate modifiers are presented to-
gether and in competition with simple word so-
lutions as soon as the user starts typing a new
word. The user can distinguish modifiers from sim-
ple words in the prediction window because the for-
mer are suffixed with a special symbol (for exam-
ple an underscore). If the user selects a modifier,
the head prediction model is activated, and the user
can start typing the prefix of the desired compound
head, while the system suggests completions based
on the head prediction model.
For example, if the user has just typed Abe,
the prediction window could contain, among other
things, the candidates Abend and Abend . If the
user selects the latter, possible head completions for
a compound having Abend as its modifier are pre-
sented.
Modifier candidates are proposed on the basis of
Pmod(w) computed as in equation 2 above. Simple
noun candidates are proposed on the basis of their
unigram and bigram probabilities (interpolated with
equal weights).
We experimented with two versions of the inte-
grated model.
In one, modifier and simple noun candidates are
ranked directly on the basis of their probabilities.
This risks to lead to over-prediction of modifier can-
didates (recall that, from the point of view of token
frequency, compounds are much rarer than simple
words; the prediction window should not be clut-
tered by too many modifier candidates when, most
of the time, users will want to type simple words).
Thus, we constructed a second version of the in-
tegrated model in which Pmod(w) is multiplied by a
penalty term. This term discounts the probability of
modifier candidates built from nominal wordforms
that occur more frequently in the training corpus as
independent nouns than as modifiers (forms that are
equally or more frequent in modifier position are not
affected by the penalty).
The same training corpus and procedures de-
scribed in section 5 above were used to train the two
versions of the integrated model, and the baseline
model that does not use compound prediction.
These models were tested by treating all the
nouns in the test corpus as prediction targets. The
integrated test set contained 90,643 N+N tokens and
395,731 more nouns. The results of the simulations
are reported in table 4.
Model integrated integrated simple pred
no penalty w/ penalty only
compound ksr 47.6 45.9 34.9
simple n ksr 40.5 42.5 45.6
combined ksr 42.5 43.5 42.6
Table 4: Integrated prediction
Because of the simple noun predictions getting in
the way, the integrated models perform compound
prediction worse than the non-integrated split com-
pound model of table 3. However the integrated
models still perform compound prediction consid-
erably better than the baseline model.
The integrated model with modifier penalties per-
forms worse than the model without penalties when
predicting compounds. This is expected, since the
modifier penalties make this model more conserva-
tive in proposing modifier candidates.
However, the model with penalties outperforms
the model without penalties in simple noun predic-
tion. Given that in our test set (and, we expect, in
most German texts) simple noun tokens greatly out-
number compound tokens, this results in an overall
better performance of the model with penalties.
The integrated model with penalties achieves
an overall ksr that is about 1% higher than that
achieved by the baseline model.
Thus, these preliminary experiments indicate that
an approach to integrating compound and simple
word predictions along the lines sketched at the be-
ginning of this section, and in particular the version
of the model in which modifier predictions are pe-
nalized, is feasible. However, the model is clearly in
need of further refinement, given that the improve-
ment over the baseline model is currently minimal.
7 Conclusion
The main result concerning German compound pre-
diction that was reported in this paper pertains to the
introduction of class-based modifier-head bigrams
to enhance head prediction.
We presented a procedure to cluster nominal
wordforms into semantic classes and to extract
class-based modifier-head bigrams, and then a
model to calculate the class-based probability of
candidate heads using these bigrams.
While we evaluated our system in the context
of the AAC word prediction task, we believe that
the class-based prediction model we proposed could
be extended to any other domain in which n-gram-
based compound prediction must be performed.
The addition of class-based head prediction to the
split compound model of Baroni et al (2002) leads
to an improvement in head prediction (from a ksr of
48.8% to a ksr of 51.2%). This translates into an
improvement of 1.3% in whole compound predic-
tion (from 48.8% to 50.1%). Overall, the split com-
pound model with class bigrams led to an improve-
ment of more than 15% over a no split compound
baseline model.
This result was presented in the context of the
AAC word prediction task, but we believe that the
class-based prediction model we proposed could be
extended to any other domain in which n-gram-
based compound prediction must be performed.
While the results we report are encouraging,
the improvement obtained with the addition of the
class-based model is hardly dramatic. It is clear that
further work in this area is required.
In particular, we plan to experiment with different
measures of association to determine the degree of
relatedness of words, and with alternative clustering
techniques.
Moreover, we hope to improve the overall perfor-
mance of the compound predictor by resorting to a
better interpolation strategy than the uniform weight
assignment model we are currently using.
We also reported results obtained with a prelim-
inary model in which split compound prediction is
integrated with simple noun prediction. This model
outperforms the baseline model without compound
prediction, but only of about 1% ksr. Clearly, fur-
ther work in this area is also necessary. In partic-
ular, as suggested by a reviewer, we will try to ex-
ploit morpho-syntactic differences between simple
nouns and modifiers to help distinguishing between
the two types.
Acknowledgements
We would like to thank an anonymous reviewer for
helpful comments and the Austria Presse Agentur
for kindly making the APA corpus available to us.
This work was supported by the European Union
in the framework of the IST programme, project
FASTY (IST-2000-25420). Financial support for
?OFAI is provided by the Austrian Federal Ministry
of Education, Science and Culture.
References
M. Baroni, J. Matiasek, and H. Trost, ?Predict-
ing the Components of German Nominal Com-
pounds?, to appear in Proc. ECAI 2002.
A. Berger: Trigger Toolkit, publicly available soft-
ware, 1997.
http://www-2.cs.cmu.edu/ aberger/software.html
P. Brown, V. Della Pietra, P. DeSouza, J. Lai, and
R. Mercer, ?Class-based n-gram models of nat-
ural language?, Computational Linguistics 18(4),
pp.467-479, 1990.
J. Carlberger, Design and Implementation of a Prob-
abilistic Word Prediction Program, Royal Insti-
tute of Technology (KTH), 1998.
D. Carter, J. Kaja, L. Neumeyer, M. Rayner, F.
Weng, and M. Wire`n, ?Handling Compounds in
a Swedish Speech-Understanding System?, Proc.
ICSLP-96.
A Copestake, ?Augmented and alternative NLP
techniques for augmentative and alternative com-
munication?, Proceedings of the ACL workshop
on Natural Language Processing for Communi-
cation Aids, 1997.
W. Dressler, G. Libben, J. Stark, C. Pons, and G.
Jarema, ?The processing of interfixed German
compounds?, Yearbook of Morphology 1999, pp.
185-220, 2001.
S. Evert and B. Krenn, ?Methods for the Qual-
itative Evaluation of Lexical Association Mea-
sures?, Proceedings of the 39th Annual Meeting
of the Association for Computational Linguistics,
Toulouse, France, 2001.
P. Fetter, Detection and Transcription of OOV
Words, Verbmobil Report 231, 1998.
L. Karttunen, K. Gal, and A. Kempe, Xerox
Finite-State Tool, Xerox Research Centre Europe,
Grenoble, 1997.
A. Krott, Analogy in Morphology, Max Planck In-
stitute for Psycholinguistics, Nijmegen, 2001.
M. Larson, D. Willett, J. Kohler, and G. Rigoll,
?Compound splitting and lexical unit recombi-
nation for improved performance of a speech
recognition system for German parliamentary
speeches?, Proceedings of the 6th Interna-
tional Conference of Spoken Language Pro-
cessing (ICSLP-2000), October 16-20., Peking,
China, 2000.
R. Rosenfeld, ?A Maximum Entropy Approach to
Adaptive Statistical Language Modeling?, Com-
puter Speech and Language 10, 187?228, 1996.
M. Spies, ?A Language Model for Compound
Words?, Proc. Eurospeech ?95, pp.1767-1779,
1995.
Robust Interpretation of User Requests for Text Retrieval in a Multimodal
Environment
Alexandra Klein and Estela Puig-Waldmu?ller
Austrian Research Institute for
Artificial Intelligence
Schottengasse 3
A-1010 Vienna, Austria
falex, stellag@oefai.at
Harald Trost
Department of Medical Cybernetics and
Artificial Intelligence, University of Vienna
Freyung 6/2
A-1010 Vienna, Austria
harald@ai.univie.ac.at
Abstract
We describe a parser for robust and flexible inter-
pretation of user utterances in a multi-modal sys-
tem for web search in newspaper databases. Users
can speak or type, and they can navigate and follow
links using mouse clicks. Spoken or written queries
may combine search expressions with browser com-
mands and search space restrictions. In interpreting
input queries, the system has to be fault-tolerant to
account for spontanous speech phenomena as well
as typing or speech recognition errors which often
distort the meaning of the utterance and are difficult
to detect and correct. Our parser integrates shallow
parsing techniques with knowledge-based text re-
trieval to allow for robust processing and coordina-
tion of input modes. Parsing relies on a two-layered
approach: typical meta-expressions like those con-
cerning search, newspaper types and dates are iden-
tified and excluded from the search string to be sent
to the search engine. The search terms which are
left after preprocessing are then grouped according
to co-occurrence statistics which have been derived
from a newspaper corpus. These co-occurrence
statistics concern typical noun phrases as they ap-
pear in newspaper texts.
1 Introduction
In this paper we describe a parser for robust and
flexible interpretation of user utterances in a web-
based multi-modal text retrieving system. The
parser forms part of a system for web search in Aus-
trian newspaper databases. In this system, users can
formulate queries or navigation commands using ut-
terances in both spontaneous spoken or written lan-
guage, and they can navigate and follow links using
mouse clicks. Users are completely free in formu-
lating their utterances and in the use and combina-
tion of the input modes. Typed and spoken utter-
ances may contain combinations of query expres-
sions, browser commands and search space restric-
tions. Users may search for texts with a specific
date, in a specific newspaper or in a specific sec-
tion of a newspaper. They may give complex con-
text descriptions of the texts and they may refer to
previously found texts. A dialogue manager stores
actions and results from previous states and supplies
information in order to construct fully specified for-
mal queries from underspecified user requests.
In order to allow for this freedom in user be-
haviour, flexible processing modules are needed.
For every utterance, the parser and the dialogue
manager must come up with an adequate interpre-
tation. At the same time, in interpreting the in-
put, they have to be robust and fault-tolerant. They
have to cope with typical phenomena of sponta-
neous speech like hesitation, correction and repe-
tition. There may be typographical errors in written
input or ? even more difficult to deal with ? speech
recognition errors from the spoken queries. Such
errors often distort the meaning of the utterance and
are difficult to detect and correct.
In our interpretation component, shallow pars-
ing techniques and knowledge-based text retrieval
methods are combined to allow for robust process-
ing and coordination of input modes. We employ
a two-layered approach. The first layer serves to
separate structure from content, i.e., parts of utter-
ances referring to browser commands and search re-
strictions (temporal expressions, newspaper types or
sections) are analyzed with a combination of key-
word spotting and pattern recognition. The under-
lying assumption is that users will restrict them-
selves to a rather small vocabulary and a limited
range of expressions in expressing this sort of in-
formation (this assumption is also confirmed by our
Wizard-of-Oz experiments). During this process,
stop words (function words and other words typi-
cally not contributing to the content of the query)
are also removed. The remaining words ? which are
assumed to describe the search content ? are then
grouped according to co-occurrence statistics which
have been derived from a newspaper corpus. While
text retrieval with the help of linguistic process-
ing has become rather common, multimodal inter-
action with textual databases on the web is a fairly
recent application of Natural Language Process-
ing. Experience from text retrieval shows that most
information is expressed in adjective-noun, noun-
preposition-noun, and noun-verb groups (Grefen-
stette, 1992). In our specific domain, the third type
can be neglected, because verbs typically denote the
action ? mostly search ? which is already extracted
in the first layer. Thus, co-occurrence statistics con-
sist of typical noun phrases as they appear in news-
paper texts.
2 Empirical evidence and user
experiments
In order to assess user behaviour, we carried
out Wizard-of-Oz experiments (Fraser and Gilbert,
1991). Speech recognition and text retrieval were
simulated. In different sessions the users interacted
with a number of versions of the system: single in-
put mode versions and versions with combinations
of input modes. Their performance in terms of num-
ber of interactions as well as task completion time
was measured, and their comments regarding the in-
terface and the (simulated) system were collected
in a questionnaire. Users were grouped according
to previous experience with search engines and the
web in general. Our results show that both, be-
ginners and advanced users, preferred multimodal
interaction over single input modes, and beginners
in particular were able to speed up task completion
times significantly with the help of a combination of
spoken and written input with mouse clicks (Klein
et al, 2001).
From these experiments, we also obtained a cor-
pus of written and spoken utterances which were
considered in the further design of the system. The
queries which were posed by the users in spoken
language were recorded. The recorded utterances
were later read to a speech recognition system. This
gave us an impression of the number and type of er-
rors to be expected in dealing with queries in spon-
taneous speech.
3 NL Text or Speech Input: Language
Analysis
Users can access articles with spoken or typed ut-
terances. Web queries may relate to the way some
particular piece of infomation is presented and what
this information refers to. They may also express
browser commands or a combination of browser and
query commands while referring either to structure
(Search for Noll in the previous newspaper) or to
content (Search for Noll in the sports? section).1
Within our application web queries may relate to
the way some particular piece of information is pre-
sented (e.g. the browser?s history about the accessed
pages), and what this information refers to (e.g. the
section a search string belongs to). To successfully
interpret such an utterance, one needs to analyze its
structure to find out which of these command modes
the utterance can be assigned to. This is done in a
two-step process. First, each word is looked up in a
lexicon and assigned a semantic category. Second,
certain rules are applied to strings of these seman-
tic categories. As a result, commands and search
restrictions are recognized and the rest of the utter-
ance is passed to search expression interpretation.
3.1 Keyword Spotting and Semantic
Classification
We will now describe in more detail how the user?s
input is parsed within the Natural Language In-
terface, and structured into either search patterns
? consisting of search strings, sections, dates and
timeranges, that are understood by the search en-
gine of the newspaper ? or commands for the Java
browser. Structure is analyzed by a flexible bottom-
up parser using a rule-based mechanism with simple
syntactic patterns.
In the user?s query input, each word of the utterance
is looked up in a lexicon and - if found - assigned a
corresponding semantic category. This lexicon con-
tains a small list of semantic categories, that we con-
sider important for the interpretation of an utterance
in the domain of searching articles and browsing.
The lexicon assigns semantic classes for closed cat-
egories that are:
 nouns denoting search, newspaper, section, links
like ?Suche? (search) or ?Artikel? (article).
 nouns expressing a specific section like
?Wirtschaft? (economy).
 nouns expressing a specific page like ?Homepage?.
 temporal expressions and temporal prepositions
like ?Monat? (month) and ?vor? (ago).
1We will use the italic font for language expressions and the
typewriter font for meta-language expressions.
 expressions indicating something new like in a
?neue? (new) search.
 adjectives and adverbs indicating direction in time
or space, like in ?letzte? (previous) search or in
?letzte? (last) week.
 cardinal and ordinal numbers used in conjunction
with temporal expressions and link expressions,
like in ?zwei? (two) years ago or when opening the
?ersten? (first) link.
 adverbs and connectives indicating constraints on
search mode, like ?nur? (only) and ?nicht? (not).
 prepositions indicating whether the request was to
browse or to search, cf ?zum Sport? (to the sports?
section) versus ?im Sport? (within the sports? sec-
tion).
 stop words.
All words found within the lexicon are replaced
by their corresponding semantic classes, search ex-
pressions are marked as such, and stop words are
deleted.
We distinguish between semantic atoms and se-
mantic classes: atoms by itself do not have a mean-
ing that can be used for searching or browsing com-
mands. They have to be joined following a given
set of rules to form a semantic class. To yield
such a class, rules are applied in ? mostly ? one to
three steps. However, rules are not always neces-
sary, a word may also be mapped onto a semantic
class right away. Our lexicon has about 30 semantic
atoms, from which about 40 semantic classes can be
formed. Certain patterns of semantic classes which
we obtain through lexical look-up can be assigned
new meanings via rules. So, by composing the indi-
vidual meanings, another more abstract meaning is
defined. This compositional approach to interpreta-
tion is supported by the layered approach. The re-
sult of this process is a list of chunks2, where stress
is laid on the content words. The advantage of con-
centrating on chunks is ? especially within German,
a language with a relatively free word order ? that
the order in which chunks occur is much more flex-
ible than the order of words within chunks. This
approach might be too shallow for a deeper seman-
tic analysis, but is sufficient for our needs. So, e.g.
2According to Abney (1991) a chunk is defined in terms of
major heads where a major head is any content word that does
not appear between a function word f and the content word
f selects, OR a pronoun selected by a preposition. [...] The
typical chunk consists of a single content word surrounded by
a constellation of function words, matching a fixed template.
?letzte? (last) plus a time expression would together
yield the new meaning date -1w. To overcome
ambiguities and avoid potential rule conflicts, rules
spanning larger chunks have a higher priority and
are thus preferred, such that ?zuru?ck zum Sport?
(back to sports) would win over ?zuru?ck? (back).
If no rules can be applied to a semantic class, it will
be ignored in the final interpretation.
Summing up the process of the structure analysis,
the partial analyses are stored, a sequence of partial
analyses from the set of rules is chosen, and then
combined to yield larger structures.
3.2 Search String Filter: Extraction of
Adjective-Noun Pairs
In the next step, the content of the query must be an-
alyzed in more detail. In this chapter we will explain
how content analysis is done in our application.
From a corpus of Austrian newspaper texts,
adjective-noun- and adjective-proper-name pairs
were extracted and counted. These pairs were stored
and consulted in query interpretation. Since the
texts are tagged manually, the lists of adjectives and
nouns/proper names contain a considerable number
of errors. Therefore it is necessary to use large
amounts of text; it may even be useful to eventu-
ally introduce a threshold so that only adjective-
noun/proper-name pairs which appear more than
once or a certain number of times are considered.
This of course can not prevent systematic tagging
errors.
A robust stemming algorithm maps all adjective-
noun/proper-name pairs to an approximate ?stem?,
thus eliminating flectional forms which result in
morphological variation which is typical for the
German language. For the purpose of creating a
repository of co-occurrence pairs, we do not care
about proper stemming. Rather, it is our aim to map
various inflectional forms onto one base form.
Spelling variations, numbers etc. are smoothed
as far as it is possible in automatic processing. For
example, ordinal numbers which are labelled as ad-
jectives are reduced to a placeholder for numbers.
Whenever a word is encountered in processing
which can be considered an adjective, it is kept.
Whenever the following word may be a noun or a
proper name, it is checked whether the adjective-
noun/proper-name combination is contained in the
repository of adjective-noun/proper-name combina-
tions which has previously been extracted from a
corpus. If the adjective-noun/proper-name combi-
nation is found, it is passed on to the search engine
as a query. Whenever the combination has not oc-
curred in the corpus, only the noun or proper name
is considered a key word.
Again, inflectional variations as well as different
spellings etc. are mapped onto base forms as far
as possible. The same stemming algorithm is used
which was employed in creating the repository of
adjective-noun/proper name pairs. The robust (and
rough) stemming and categorization algorithms pro-
duce a certain amount of mistakes in the lists of
pairs as well as in the mapping process, but tak-
ing into account larger text corpora evens out these
problems as more text is processed.
Our approach distinguishes noun phrases which
have a record of co-occurrence from noun phrases
which may be spontaneous expressions or modifica-
tions or even errors created by users. For example,
the phrase ?europa?ische Staaten? (European coun-
tries) would be retained while ?beteiligte Staaten?
(participating countries) would be reduced to the
noun. Some adjectives used in search expressions
serve to qualify the global search expression rather
than the noun or proper name in quesion. For ex-
ample, a search for yesterday?s speech would only
yield articles from the day after a speech, not about
the speech in general.
4 Action History: Integration of the
knowledge sources
Multimodal dialogue requires a unified interpreta-
tion of the involved knowledge sources, all input
modes have to be considered. The information
transmitted needs to be interpreted within discourse
context including previous user actions, possibly
with data coming from other input modes.
After the analysis of the user utterance has been
performed in the pattern-matching and the search-
word-extraction modules, the computed meaning of
the utterance has to been interpreted in the context
of the discourse sitiuation. This concerns mostly
the history of previous queries. Here, it is impor-
tant to consult previous queries in all possible input
modes (spoken, typed, mouse clicks). Therefore, a
record of the action history is kept and consulted.
All typed, written and spoken actions are assigned
an entry in the action history where the main param-
eters and their values are collected.
With this contextual information, the meaning of
the user?s utterance as the sum of the results of the
component analyses is computed in the global dis-
course context. Underspecified queries can be in-
terpreted in the discourse contexts, and parameters
are filled. Thus, the results are combined into one
unambiguous command line.
A powerful interaction control is necessary in or-
der to recognize the user?s intent by comparing it to
what the system knows about the addressed entities
and their relation to each other as well as to the data
which are accessible at the specific moment in the
interaction. The interface language between the lan-
guage analysis module and the controller consists of
a fixed set of parameters, which are assigned appro-
priate values:
 DIRECTION
the direction for browsing (forward, backward)
 SECTION
the section in the newspaper (politics, sports, ...)
 SEARCHSTRING
the string which has to be searched by the newspa-
per search engine
 DATE
the date when the article to be searched has ap-
peared (also intervals)
 ZEITUNG (NEWSPAPER)
the newspaper which is supposed to be searched
 OPENLINK
the link in a document which should be followed in
the browser
 OPENURL
the URL which is supposed to be opened by the
browser
The outcome ? or left-hand side ? of a rule-based
simplification can be divided into three command
types:
 Simple Search Command, New Search Com-
mand: E.g. ?Suche nach Camilleri im Kulturres-
sort? (Search for Camilleri in the cultural section)
or ?Neue Suche beginnen mit Krimis? (Start a new
search on thrillers).
 Complex Search Command: Search using the Ac-
tion history. E.g. ?Suche nach Christie im letzten
Ressort? (Search for Christie in the previous sec-
tion).
 Simple History Browsing Commands: Normal
Browsing using the Accessed Page History. E.g.
?Zur na?chsten Seite gehen? (Go to the next page).
 Complex History Browsing Commands: Browse
using the Action history. E.g. ?Geh zum let-
zten Ressort? (Go to the last ressort) or ?Zuru?ck
zur Suche mit Montalbano gehen? (Go back to the
search containing Montalbano).
 WWW Browsing: E.g. ?Geh zum heutigen Sport-
bereich? (Go to today?s sport section), ?den Stan-
dard lesen? (read the Standard) or ?Geh zur Home-
page? (Go home).
 Opening Link Command: E.g.?den ersten Artikel
o?ffnen? (Open the first article)
The action history browsing command refers to
the timeline and the point of reference of a brows-
ing but also of a search command. For instance,
take an utterance, where someone wants to search
for a topic but within a context that was defined in
the previous search. For our application, we would
first have to locate the user?s point of reference and
then execute her search command. If there is no
given reference, we assume by default that a new
time point is created in our time line.
One such command could look like this: the
utterance ?Ich suche etwas u?ber Highsmith im
letzten Ressort? (I am looking for something
about Highsmith within the previous section) would
be mapped to: DIRECTION 0, SECTION x
(where x is the section of the action with in-
dex -1), SEARCHSTRING Highsmith, TIME
nil, ZEITUNG nil. We are not moving in the
timeline, instead we are adding a new search action,
thus the direction is zero. Anyway, the controller
has to look up the action history to fill the value of
the section. The values of all empty parameters will
be filled with the values of the last actions, so in our
example, these parameters have not been explicitly
filled and remain empty (nil).
5 Result: Translating into Http Request or
Browser Command
After the command has been processed by the con-
trol module, it is either executed by the Java browser
or translated into a GET method through an Http
request to the newspaper?s archive database. The
resulting articles are displayed in the Java browser,
another search can be started by the user.
6 Conclusion
We have presented an interpretation component for
natural language user input in a web-based multi-
modal text retrieval system. By applying well-
known and simple methods from shallow parsing
and knowledge-based text retrieval and integrating
them in a novel way we have succeeded in creating
a robust, flexible and efficient parser for our appli-
cation.
An important feature is the distinction between
those parts of utterances relating to structure and
those relating to content. This is achieved by tak-
ing advantage of the fact that only a limited vocab-
ulary and set of expressions are used for the former.
This allows us to employ simple rule-based tech-
niques for their interpretation. The identification of
the content on the other hand is done with the help
of a co-occurrence repository, at the moment con-
sisting of adjective-noun/proper name pairs. In the
future we will have to investigate whether search re-
sults can be improved by inserting other combina-
tions, like noun-preposition-noun triples.
Acknowledgements
This work was supported by the Austrian Science
Fund (FWF) under project number P-13704. Finan-
cial support for ?OFAI is provided by the Austrian
Federal Ministry of Education, Science and Culture.
References
Steven Abney. 1991. Parsing by chunks. In Robert
Berwick, Steven Abney, and Carol Tenny, editors,
Principle-Based Parsing, Tu?bingen (Germany).
Kluwer Academic Publishers.
Norman M. Fraser and G. Nigel Gilbert. 1991.
Simulating speech systems. Computer Speech
and Language, 5(1):81?99.
Gregory Grefenstette. 1992. Use of Syntactic Con-
text to Produce Term Association Lists for Text
Retrieval. In N.J. Belkin, P. Ingwersen, and A.M.
Pejtersen, editors, Proceedings of the 15th An-
nual International ACM SIGIR Conference on
Research and Development in Information Re-
trieval, pages 89?97, Copenhagen: Denmark.
ACM Press.
Alexandra Klein, Ingrid Schwank, Michel
Ge?ne?reux, and Harald Trost. 2001. Evaluating
Multimodal Input Modes in a Wizard-of-Oz
Study for the Domain of Web Search. In Ann
Blandford, Jean Vanderdonckt, and Phil Gray,
editors, People and Computer XV ? Interaction
without Frontiers: Joint Proceedings of HCI
2001 and IHM 2001, pages 475?483. Springer:
London, September.
Unsupervised discovery of morphologically related words based on
orthographic and semantic similarity
Marco Baroni
?OFAI
Schottengasse 3
A-1010 Vienna, Austria
marco@oefai.at
Johannes Matiasek
?OFAI
Schottengasse 3
A-1010 Vienna, Austria
john@oefai.at
Harald Trost
IMKAI
Freyung 6
A-1010 Vienna, Austria
harald@ai.univie.ac.at
Abstract
We present an algorithm that takes an
unannotated corpus as its input, and re-
turns a ranked list of probable morpho-
logically related pairs as its output. The
algorithm tries to discover morphologi-
cally related pairs by looking for pairs
that are both orthographically and seman-
tically similar, where orthographic simi-
larity is measured in terms of minimum
edit distance, and semantic similarity is
measured in terms of mutual information.
The procedure does not rely on a mor-
pheme concatenation model, nor on dis-
tributional properties of word substrings
(such as affix frequency). Experiments
with German and English input give en-
couraging results, both in terms of pre-
cision (proportion of good pairs found at
various cutoff points of the ranked list),
and in terms of a qualitative analysis of
the types of morphological patterns dis-
covered by the algorithm.
1 Introduction
In recent years, there has been much interest in com-
putational models that learn aspects of the morphol-
ogy of a natural language from raw or structured
data. Such models are of great practical interest as
tools for descriptive linguistic analysis and for mini-
mizing the expert resources needed to develop mor-
phological analyzers and stemmers. From a theo-
retical point of view, morphological learning algo-
rithms can help answer questions related to human
language acquisition.
In this study, we present a system that, given a
corpus of raw text from a language, returns a ranked
list of probable morphologically related word pairs.
For example, when run with the Brown corpus as
its input, our system returned a list with pairs such
as pencil/pencils and structured/unstructured at the
top.
Our algorithm is completely knowledge-free, in
the sense that it processes raw corpus data, and it
does not require any form of a priori information
about the language it is applied to. The algorithm
performs unsupervised learning, in the sense that it
does not require a correctly-coded standard to (iter-
atively) compare its output against.
The algorithm is based on the simple idea that a
combination of formal and semantic cues should be
exploited to identify morphologically related pairs.
In particular, we use minimum edit distance to mea-
sure orthographic similarity,1 and mutual informa-
tion to measure semantic similarity. The algo-
rithm does not rely on the notion of affix, and it
does not depend on global distributional properties
of substrings (such as affix frequency). Thus, at
least in principle, the algorithm is well-suited to
discover pairs that are related by rare and/or non-
concatenative morphological processes.
The algorithm returns a list of related pairs, but
it does not attempt to extract the patterns that relate
the pairs. As such, it can be used as a tool to pre-
1Given phonetically transcribed input, our model would
compute phonetic similarity instead of orthographic similarity.
                     July 2002, pp. 48-57.  Association for Computational Linguistics.
        ACL Special Interest Group in Computational Phonology (SIGPHON), Philadelphia,
       Morphological and Phonological Learning: Proceedings of the 6th Workshop of the
process corpus data for an analysis to be performed
by a human morphologist, or as the first step of a
fully automated morphological learning program, to
be followed, for example, by a rule induction pro-
cedure that extracts correspondence patterns from
paired forms. See the last section of this paper for
further discussion of possible applications.
We tested our model with German and English
input. Our results indicate that the algorithm is
able to identify a number of pairs related by a va-
riety of derivational and inflectional processes with
a remarkably high precision rate. The algorithm is
also discovering morphological relationships (such
as German plural formation with umlaut) that would
probably be harder to discover using affix-based ap-
proaches.
The remainder of the paper is organized as fol-
lows: In section 2, we shortly review related work.
In section 3, we present our model. In section 4, we
discuss the results of experiments with German and
English input. Finally, in section 5 we summarize
our main results, we sketch possible directions that
our current work could take, and we discuss some
potential uses for the output of our algorithm.
2 Related work
For space reason, we discuss here only three ap-
proaches that are closely related to ours. See, for
example, Goldsmith (2001) for a very different (pos-
sibly complementary) approach, and for a review of
other relevant work.
2.1 Jacquemin (1997)
Jacquemin (1997) presents a model that automati-
cally extracts morphologically related forms from a
list of English two-word medical terms and a corpus
from the medical domain.
The algorithm looks for correspondences between
two-word terms and orthographically similar pairs
of words that are adjacent in the corpus. For exam-
ple, the list contains the term artificial ventilation,
and the corpus contains the phrase artificially ven-
tilated. Jacquemin?s algorithm thus postulates the
(paired) morphological analyses artificial ventilat-
ion and artificial-ly ventilat-ed.
Similar words, for the purposes of this pairing
procedure, are simply words that share a common
left substring (with constraints that we do not dis-
cuss here).
Jacquemin?s procedure then builds upon these
early steps by clustering together sets that follow the
same patterns, and using these larger classes to look
for spurious analyses. Finally, the algorithm tries to
cluster classes that are related by similar, rather than
identical, suffixation patterns. Again, we will not
describe here how this is accomplished.
Our basic idea is related to that of Jacquemin, but
we propose an approach that is more general both
in terms of orthography and in terms of semantics.
In terms of orthography, we do not require that two
strings share the left (or right) substring in order to
constitute a candidate pair. Thus, we are not limited
to affixal morphological patterns. Moreover, our al-
gorithm extracts semantic information directly from
the input corpus, and thus it does not require a pre-
compiled list of semantically related pairs.
2.2 Schone and Jurafsky (2000)
Schone and Jurafsky (2000) present a knowledge-
free unsupervised model in which orthography-
based distributional cues are combined with seman-
tic information automatically extracted from word
co-occurrence patterns in the input corpus.
They first look for potential suffixes by search-
ing for frequent word-final substrings. Then, they
look for potentially morphologically related pairs,
i.e., pairs that end in potential suffixes and share the
left substring preceding those suffixes. Finally, they
look, among those pairs, for those whose semantic
vectors (computed using latent semantic analysis)
are significantly correlated. In short, the idea behind
the semantic component of their model is that words
that tend to co-occur with the same set of words,
within a certain window of text, are likely to be se-
mantically correlated words.
While we follow Schone and Jurafsky?s idea of
combining orthographic and semantic cues, our al-
gorithm differs from them in both respects. From the
point of view of orthography, we rely on the com-
parison between individual word pairs, without re-
quiring that the two pairs share a frequent affix, and
indeed without requiring that they share an affix at
all.
From the point of view of semantics, we compute
scores based on mutual information instead of latent
semantic analysis. Thus, we only look at the co-
occurrence patterns of target words, rather than at
the similarity of their contexts.
Future research should try to assess to what extent
these two approaches produce significantly different
results, and/or to what extent they are complemen-
tary.
2.3 Yarowsky and Wicentowski (2000)
Yarowsky and Wicentowski (2000) propose an algo-
rithm that extracts morphological rules relating roots
and inflected forms of verbs (but the algorithm can
be extended to other morphological relations).
Their algorithm performs unsupervised, but not
completely knowledge-free, learning. It requires
a table of canonical suffixes for the relevant parts
of speech of the target language, a list of the con-
tent word roots with their POS (and some informa-
tion about the possible POS/inflectional features of
other words), a list of the consonants and vowels of
the language, information about some characteristic
syntactic patterns and, if available, a list of function
words.
The algorithm uses a combination of different
probabilistic models to find pairs that are likely to be
morphologically related. One model matches root
+ inflected form pairs that have a similar frequency
profile. Another model matches root + inflected
form pairs that tend to co-occur with the same sub-
jects and objects (identified using simple regular ex-
pressions). Yet another model looks for words that
are orthographically similar, in terms of a minimum
edit distance score that penalizes consonant changes
more than vowel changes. Finally, the rules relating
stems and inflected forms that the algorithm extracts
from the pairs it finds in an iteration are used as a
fourth probabilistic model in the subsequent itera-
tions.
Yarowsky and Wicentowski show that the al-
gorithm is extremely accurate in identifying En-
glish root + past tense form pairs, including those
pairs that are related by non-affixal patterns (e.g.,
think/thought.)
The main issue with this model is, of course, that
it cannot be applied to a new target language with-
out having some a priori knowledge about some of
its linguistic properties. Thus, the algorithm can-
not be applied in cases in which the grammar of
the target language has not been properly described
yet, or when the relevant information is not available
for other reasons. Moreover, even when such infor-
mation is in principle available, trying to determine
to what extent morphology could be learned with-
out relying on any other knowledge source remains
an interesting theoretical pursuit, and one whose an-
swer could shed some light on the problem of human
language acquisition.
3 The current approach: Morphological
relatedness as a function of orthographic
and semantic similarity
The basic intuition behind the model presented here
is extremely simple: Morphologically related words
tend to be both orthographically and semantically
similar. Obviously, there are many words that are or-
thographically similar, but are not morphologically
related; for example, blue and glue. At the same
time, many semantically related words are not mor-
phologically related (for example, blue and green).
However, if two words have a similar shape and a
related meaning (e.g., green and greenish), they are
very likely to be also morphologically related.
In order to make this idea concrete, we use min-
imum edit distance to identify words that are ortho-
graphically similar, and mutual information between
words to identify semantically related words.
3.1 Outline of the procedure
Given an unannotated input corpus, the algorithm
(after some elementary tokenization) extracts a list
of candidate content words. This is simply a list
of all the alphabetic space- or punctuation-delimited
strings in the corpus that have a corpus frequency
below .01% of the total token count.2
Preliminary experiments indicated that our proce-
dure does not perform as well without this trimming.
Notice in any case that function words tend to be of
little morphological interest, as they display highly
lexicalized, often suppletive morphological patterns.
The word list extracted as described above and the
input corpus are used to compute two lists of word
pairs: An orthographic similarity list, in which the
2In future versions of the algorithm, we plan to make this
high frequency threshold dependent on the size of the input cor-
pus.
pairs are scored on the basis of their minimum edit
distance, and a semantic similarity list, based on mu-
tual information. Because of minimum thresholds
that are enforced during the computation of the two
measures, neither list contains all the pairs that can
in principle be constructed from the input list.
Before computing the combined score, we get rid
of the pairs that do not occur in both lists (the ra-
tionale being that we do not want to guess the mor-
phological status of a pair on the sole basis of ortho-
graphic or semantic evidence).
We then compute a weighted sum of the ortho-
graphic and semantic similarity scores of each re-
maining pair. In the experiments reported below, the
weights are chosen so that the maximum weighted
scores for the two measures are in the same order
of magnitude (we prefer to align maxima rather than
means because both lists are trimmed at the bottom,
making means and other measures of central ten-
dency less meaningful).
The pairs are finally ranked on the basis of the
resulting combined scores.
In the next subsections, we describe how the or-
thographic and semantic similarity lists are con-
structed, and some properties of the measures we
adopted.
3.2 Scoring the orthographic similarity of
word pairs
Like Yarowsky and Wicentowski, we use mini-
mum edit distance to measure orthographic simi-
larity. The minimum edit distance between two
strings is the minimum number of editing oper-
ations (insertion, deletion, substitution) needed to
transform one string into the other (see section 5.6
of Jurafsky and Martin (2000) and the references
quoted there).
Unlike Yarowsky and Wicentowski, we do not at-
tempt to define a phonologically sensible edit dis-
tance scoring function, as this would require making
assumptions about how the phonology of the target
language maps onto its orthography, thus falling out-
side the domain of knowledge-free induction. In-
stead, we assign a cost of 1 to all editing operations,
independently of the nature of the source and target
segments. Thus, in our system, the pairs dog/Dog,
man/men, bat/mat and day/dry are all assigned a
minimum edit distance of 1.3
Rather than computing absolute minimum edit
distance, we normalize this measure by dividing
it by the length of the longest string (this corre-
sponds to the intuition that, say, two substitutions
are less significant if we are comparing two eight-
letter words than if we are comparing two three-
letter words). Moreover, since we want to rank pairs
on the basis of orthographic similarity, rather than
dissimilarity, we compute (1 - normalized minimum
edit distance), obtaining a measure that ranges from
1 for identical forms to 0 for forms that do not share
any character.
This measure is computed for all pairs of words in
the potential content word list. However, for reasons
of size, only pairs that have a score of .5 or higher
(i.e., where the two members share at least half of
their characters) are recorded in the output list.
Notice that orthographic similarity does not favor
concatenative affixal morphology over other types
of morphological processes. For example, the pairs
woman/women and park/parks both have an ortho-
graphic similarity score of .8.
Moreover, orthographic similarity depends only
on the two words being compared, and not on global
distributional properties of these words and their
substrings. Thus, words related by a rare morpho-
logical pattern can have the same score as words
related by a very frequent pattern, as long as the
minimum edit distance is the same. For example,
both nucleus/nuclei and bench/benches have an or-
thographic similarity score of .714, despite the fact
that the latter pair reflects a much more common plu-
ralization pattern.
Of course, this emancipation from edge-anchored
concatenation and global distributional salience also
implies that orthographic similarity will assign high
3Following a suggestion by two reviewers, we are currently
experimenting with an iterative version of our algorithm, along
the lines of the one described by Yarowsky and Wicentowski.
We start with the cost matrix described in the text, but we re-
estimate the editing costs on the basis of the empirical character-
to-character (or character-to-zero/zero-to-character) probabili-
ties observed in the output of the previous run of the algorithm.
Surprisingly, the revised version of the algorithm leads to (mod-
erately) worse results than the single-run version described in
this paper. Further experimentation with edit cost re-estimation
is needed, in order to understand which aspects of our iterative
procedure make it worse than the single-run model, and how it
could be improved.
scores to many pairs that are not morphologically
related ? for example, the pair friends/trends also has
an orthographic similarity score of .714.
Furthermore, since in most languages the range of
possible word lengths is narrow, orthographic simi-
larity as a ranking measure tends to suffer of a ?mas-
sive tying? problem. For example, when pairs from
the German corpus described below are ranked on
the sole basis of orthographic similarity, the result-
ing list is headed by a block of 19,597 pairs that all
have the same score. These are all pairs where one
word has 9 characters, the other 9 or 8 characters,
and the two differ in only one character.4
For the above reasons, it is crucial that ortho-
graphic similarity is combined with an independent
measure that allows us to distinguish between simi-
larity due to morphological relatedness vs. similar-
ity due to chance or other reasons.
3.3 Scoring the semantic similarity of word
pairs
Measuring the semantic similarity of words on the
basis of raw corpus data is obviously a much harder
task than measuring the orthographic similarity of
words.
Mutual information (first introduced to compu-
tational linguistics by Church and Hanks (1989)) is
one of many measures that seems to be roughly
correlated to the degree of semantic relatedness be-
tween words. The mutual information between two
words A and B is given by:
I(A,B) = log
Pr(A,B)
Pr(A)Pr(B)
(1)
Intuitively, the larger the deviation between the
empirical frequency of co-occurrence of two words
and the expected frequency of co-occurrence if they
were independent, the more likely it is that the oc-
currence of one of the two words is not independent
from the occurrence of the other.
Brown et ali (1990) observed that when mutual
information is computed in a bi-directional fashion,
and by counting co-occurrences of words within a
4Most of the pairs in this block ? 78% ? are actually morpho-
logically related. However, given that all pairs contain words
of length 9 and 8/9 that differ in one character only, they are
bound to reflect only a very small subset of the morphological
processes present in German.
relatively large window, but excluding ?close? co-
occurrences (which would tend to capture colloca-
tions and lexicalized phrases), the measure identifies
semantically related pairs.
It is particularly interesting for our purposes that
most of the examples of English word clusters con-
structed on the basis of this interpretation of mutual
information by Brown and colleagues (reported in
their table 6) include morphologically related words.
A similar pattern emerges among the examples of
German words clustered in a similar manner by
Baroni et ali (2002). Rosenfeld (1996) reports that
morphologically related pairs are common among
words with a high (average) mutual information.
We computed mutual information by considering,
for each pair, only co-occurrences within a maxi-
mal window of 500 words and outside a minimal
window of 3 words. Given that mutual informa-
tion is notoriously unreliable at low frequencies (see,
for example, Manning and Schu?tze (1999), section
5.4), we only collected mutual information scores
for pairs that co-occurred at least three times (within
the relevant window) in the input corpus. Obvi-
ously, occurrences across article boundaries were
not counted. Notice however that the version of the
Brown corpus we used does not mark article bound-
aries. Thus, in this case the whole corpus was treated
as a single article.
Our ?semantic? similarity measure is based on the
notion that related words will tend to often occur in
the nears of each other. This differs from the (more
general) approach of Schone and Jurafsky (2000),
who look for words that tend to occur in the same
context. It remains an open question whether the
two approaches produce complementary or redun-
dant results.5
Taken by itself, mutual information is a worse
predictor of morphological relatedness than mini-
mum edit distance. For example, among the top one
hundred pairs ranked by mutual information in each
language, only one German pair and five English
pairs are morphologically motivated. This poor per-
formance is not too surprising, given that there are
5We are currently experimenting with a measure based on
semantic context similarity (determined on the basis of class-
based left-to-right and right-to-left bigrams), but the current im-
plementation of this requires ad hoc corpus-specific settings to
produce interesting results with both our test corpora.
plenty of words that often co-occur together without
being morphologically related. Consider for exam-
ple (from our English list) the pairs index/operand
and orthodontist/teeth.
4 Empirical evaluation
4.1 Materials
We tested our procedure on the German APA corpus,
a corpus of newswire containing over twenty-eight
million word tokens, and on the English Brown cor-
pus (Kuc?era and Francis, 1967), a balanced corpus
containing less than one million two hundred thou-
sand word tokens. Of course, the most important dif-
ference between these two corpora is that they rep-
resent different languages. However, observe also
that they have very different sizes, and that they are
different in terms of the types of texts constituting
them.
Besides the high frequency trimming procedure
described above, for both languages we removed
from the potential content word lists those words
that were not recognized by the XEROX morpholog-
ical analyzer for the relevant language. The reason
for this is that, as we describe below, we use this tool
to build the reference sets for evaluation purposes.
Thus, morphologically related pairs composed of
words not recognized by the analyzer would unfairly
lower the precision of our algorithm.
Moreover, after some preliminary experimenta-
tion, we also decided to remove words longer than 9
characters from the German list (this corresponds to
trimming words whose length is one standard devi-
ation or more above the average token length). This
actually lowers the performance of our system, but
makes the results easier to analyze ? otherwise, the
top of the German list would be cluttered by a high
number of rather uninteresting morphological pairs
formed by inflected forms from the paradigm of
very long nominal compounds (such as Wirtschafts-
forschungsinstitut ?institute for economic research?).
Unlike high frequency trimming, the two opera-
tions we just described are meant to facilitate empir-
ical evaluation, and they do not constitute necessary
steps of the core algorithm.
4.2 Precision
In order to evaluate the precision obtained by our
procedure, we constructed a list of all the pairs that,
according to the analysis provided by the XEROX
analyzer for the relevant language, are morpholog-
ically related (i.e., share one of their stems).6 We
refer to the lists constructed in the way we just de-
scribed as reference sets.
The XEROX tools we used do not provide deriva-
tional analysis for English, and a limited form of
derivational analysis for German. Our algorithm,
however, finds both inflectionally and derivationally
related pairs. Thus, basing our evaluation on a com-
parison with the XEROX parses leads to an underes-
timation of the precision of the algorithm. We found
that this problem is particularly evident in English,
since English, unlike German, has a rather poor in-
flectional morphology, and thus the discrepancies
between our output and the analyzer parses in terms
of derivational morphology have a more visible im-
pact on the results of the comparison. For example,
the English analyzer does not treat pairs related by
the adverbial suffix -ly or by the prefix un- as mor-
phologically related, whereas our algorithm found
pairs such as soft/softly and load/unload.
In order to obtain a more fair assessment of the
algorithm, we went manually through the first 2,000
English pairs found by our algorithm but not parsed
as related by the analyzer, looking for items to be
added to the reference set. We were extremely
conservative, and we added to the reference set
only those pairs that are related by a transparent
and synchronically productive morphological pat-
tern. When in doubt, we did not correct the analyzer-
based analysis. Thus, for example, we did not count
pairs such as machine/machinery, variables/varies
or electric/electronic as related.
We did not perform any manual post-processing
on the German reference set.
Tables 1 and 2 report percentage precision (i.e.,
the percentage of pairs that are in the reference set
over the total number of ranked pairs up to the rele-
vant threshold) at various cutoff points, for German
and English respectively.
6The XEROX morphological analyzers are state-of-the-art,
knowledge-driven morphological analysis tools (see for exam-
ple Karttunen et ali (1997)).
# of pairs precision
500 97%
1000 96%
1500 96%
2000 94%
3000 81%
4000 65%
5000 53%
5279 50%
Table 1: German precision at various cutoff points
(5279 = total number of pairs)
# of pairs precision
500 98%
1000 95%
1500 91%
2000 83%
3000 72%
4000 58%
5000 48%
8902 29%
Table 2: English precision at various cutoff points
(8902 = total number of pairs)
For both languages we notice a remarkably high
precision rate (> 90%) up to the 1500-pair cutoff
point.
After that, there is a sharper drop in the English
precision, whereas the decline in German is more
gradual. This is perhaps due in part to the problems
with the English reference set we discussed above,
but notice also that English has an overall poorer
morphological system and that the English corpus is
considerably smaller than the German one. Indeed,
our reference set for German contains more than ten
times the forms in the English reference set.
Notice anyway that, for both languages, the preci-
sion rate is still around 50% at the 5000-pair cutoff.7
7Yarowsky and Wicentowski (2000) report an accuracy of
over 99% for their best model and a test set of 3888 pairs. Our
precision rate at a comparable cutoff point is much lower (58%
at the 4000-pair cutoff). However, Yarowksy and Wicentowski
restricted the possible matchings to pairs in which one member
is an inflected verb form, and the other member is a potential
verbal root, whereas in our experiments any word in the corpus
(as long as it was below a certain frequency threshold, and it was
recognized by the XEROX analyzer) could be matched with any
other word in the corpus. Thus, on the one hand, Yarowsky and
Wicentowski forced the algorithm to produce a matching for a
certain set of words (their set of inflected forms), whereas our
algorithm was not subject to an analogous constraint. On the
other hand, though, our algorithm had to explore a much larger
possible matching space, and it could (and did) make a high
number of mistakes on pairs (such as, e.g., sorry and worry) that
Of course, what counts as a ?good? precision rate
depends on what we want to do with the output of
our procedure. We show below that even a very
naive morphological rule extraction algorithm can
extract sensible rules by taking whole output lists as
its input, since, although the number of false pos-
itives is high, they are mostly related by patterns
that are not attested as frequently in the list as the
patterns relating true morphological pairs. In other
words, true morphological pairs tend to be related
by patterns that are distributionally more robust than
those displayed by false positives. Thus, rule ex-
tractors and other procedures processing the output
of our algorithm can probably tolerate a high false
positive rate if they take frequency and other distri-
butional properties of patterns into account.
Notice that we discussed only precision, and not
recall. This is because we believe that the goal of a
morphological discovery procedure is not to find the
exhaustive list of all morphologically related forms
in a language (indeed, because of morphological
productivity, such list is infinite), but rather to dis-
cover all the possible (synchronically active and/or
common) morphological processes present in a lan-
guage. It is much harder to measure how good our
algorithm performed in this respect, but the qualita-
tive analysis we present in the next subsection indi-
cates that, at least, the algorithm discovers a varied
and interesting set of morphological processes.
4.3 Morphological patterns discovered by the
algorithm
The precision tables confirm that the algorithm
found a good number of morphologically related
pairs. However, if it turned out that all of these
pairs were examples of the same morphological pat-
tern (say, nominal plural formation in -s), the al-
gorithm would not be of much use. Moreover, we
stated at the beginning that, since our algorithm does
not assume an edge-based stem+affix concatenation
model of morphology, it should be well suited to dis-
cover relations that cannot be characterized in these
Yarowksy and Wicentowski?s algorithm did not have to con-
sider. Schone and Jurafsky (2000) report a maximum precision
of 92%. It is hard to compare this with our results, since they
use a more sophisticated scoring method (based on paradigms
rather than pairs) and a different type of gold standard. More-
over, they do not specify what was the size of the input they
used for evaluation.
terms (e.g., pairs related by circumfixation, stem
changes, etc.). It is interesting to check whether the
algorithm was indeed able to find relations of this
sort.
Thus, we performed a qualitative analysis of the
output of the algorithm, trying to understand what
kind of morphological processes were captured by
it.
In order to look for morphological processes in
the algorithm output, we wrote a program that ex-
tracts ?correspondence rules? in the following sim-
ple way: For each pair, the program looks for the
longest shared (case-insensitive) left- and right-edge
substrings (i.e., for a stem + suffix parse and for a
prefix + stem parse). The program then chooses the
parse with the longest stem (assuming that one of the
two parses has a non-zero stem), and extracts the rel-
evant edge-bound correspondence rule. If there is a
tie, the stem + suffix parse is preferred. The program
then ranks the correspondence rules on the basis of
their frequency of occurrence in the original output
list.8
We want to stress that we are adopting this proce-
dure as a method to explore the results, and we are
by no means proposing it as a serious rule induction
algorithm. One of the most obvious drawbacks of
the current rule extraction procedure is that it is only
able to extract linear, concatenative, edge-bound suf-
fixation and prefixation patterns, and thus it misses
or fails to correctly generalize some of the most in-
teresting patterns in the output. Indeed, looking at
the patterns missed by the algorithm (as we do in
part below) is as instructive as looking at the rules it
found.
Tables 3 and 4 report the top five suffixation and
prefixation patterns found by the rule extractor by
taking the entire German and English output lists as
its input.
These tables show that our morphological pair
scoring procedure found many instances of various
common morphological patterns. With the excep-
tion of the German ?prefixation? rule ers?drit (ac-
tually relating the roots of the ordinals ?first? and
?second?), and of the compounding pattern  ? ?Ol
(?Oil?), all the rules in these lists correspond to re-
alistic affixation patterns. Not surprisingly, in both
8Ranking by cumulative score yields analogous results.
rule example fq
?s Jelzin?Jelzins 921
?n lautete?lauteten 670
?en digital?digitalen 225
?e rot?rote 201
?es Papst?Papstes 113
?ge stiegen?gestiegen 9
? ?Ol Embargo? ?Olembargo 6
?vor Mittag?Vormittag 5
aus?ein ausfuhren?einfuhren 4
ers?drit Erstens?Drittens 4
Table 3: The most common German suffixation and
prefixation patterns
rule example fq
?s allotment?allotments 860
?ed accomplish?accomplished 98
ed?ing established?establishing 87
?ing experiment?experimenting 85
?d conjugate?conjugated 58
?un structured?unstructured 17
?re organization?reorganization 12
?in organic?inorganic 7
?non specifically?nonspecifically 6
?dis satisfied?dissatisfied 5
Table 4: The most common English suffixation and
prefixation patterns
languages many of the most frequent rules (such as,
e.g., ?s) are poly-functional, corresponding to a
number of different morphological relations within
and across categories.
The results reported in these tables confirm that
the algorithm is capturing common affixation pro-
cesses, but they are based on patterns that are so
frequent that even a very naive procedure could un-
cover them9
More interesting observations emerge from fur-
ther inspection of the ranked rule files. For exam-
ple, among the 70 most frequent German suffixation
rules extracted by the procedure, we encounter those
in table 5.10
The patterns in this table show that our algorithm
is capturing the non-concatenative plural formation
9For example, as shown by a reviewer, a procedure that pairs
words that share the same first five letters, and extracts the di-
verging substrings following the common prefix from each pair.
10In order to find the set of rules presented in table 5 using the
naive algorithm described in the previous footnote, we would
have to consider the 2672 most frequent rules. Most of these
2672 rules, of course, do not correspond to true morphological
patterns ? thus, the interesting rules would be buried in noise.
rule example fq
ag?a?ge Anschlag?Anschla?ge 10
ang?a?nge Ru?ckgang?Ru?ckga?nge 6
all?a?lle ?Uberfall? ?Uberfa?lle 6
ug?u?ge Tiefflug?Tiefflu?ge 5
and?a?nde Vorstand?Vorsta?nde 5
uch?u?che Einbruch?Einbru?che 3
auf?a?ufe Verkauf?Verka?ufe 3
ag?a?gen Vertrag?Vertra?gen 3
Table 5: Some German rules involving stem vowel
changes found by the rule extractor
process involving fronting of the stem vowel plus
addition of a suffix (-e/-en). A smarter rule extractor
should be able to generalize from patterns like these
to a smaller number of more general rules capturing
the discontinuous change. Other umlaut-based pat-
terns that do not involve concomitant suffixation ?
such as in Mutter/Mu?tter ? were also found by our
core algorithm, but they were wrongly parsed as in-
volving prefixes (e.g., Mu?Mu?) by the rule extrac-
tor.
Finally, it is very interesting to look at those pairs
that are morphologically related according to the
XEROX analyzer, and that were discovered by our
algorithm, but where the rule extractor could not
posit a rule, since they do not share a substring at
either edge. These are listed, for German, in table 6.
Alter a?lteren fordern gefordert
Arzt ?Arzte forderten gefordert
Arztes ?Arzte fo?rdern gefo?rdert
Fesseln gefesselt genannt nannte
Folter gefoltert genannten nannte
Putsch geputscht geprallt prallte
Spende gespendet gesetzt setzte
Spenden gespendet gestu?rzt stu?rzte
Streik gestreikt
Table 6: Morphologically related German pairs that
do not share an edge found by the basic algorithm
We notice in this table, besides three further in-
stances of non-affixal morphology, a majority of
pairs involving circumfixation of one of the mem-
bers.
While a more in-depth qualitative analysis of our
results should be conducted, the examples we dis-
cussed here confirm that our algorithm is able to cap-
ture a number of different morphological patterns,
including some that do not fit into a strictly concate-
native edge-bound stem+affix model.
5 Conclusion and Future Directions
We presented an algorithm that, by taking a raw cor-
pus as its input, produces a ranked list of morpho-
logically related pairs at its output. The algorithm
finds morphologically related pairs by looking at the
degree of orthographic similarity (measured by min-
imum edit distance) and semantic similarity (mea-
sured by mutual information) between words from
the input corpus.
Experiments with German and English inputs
gave encouraging results, both in terms of precision,
and in terms of the nature of the morphological pat-
terns found within the output set.
In work in progress, we are exploring various pos-
sible improvements to our basic algorithm, includ-
ing iterative re-estimation of edit costs, addition of a
context-similarity-based measure, and extension of
the output set by morphological transitivity, i.e. the
idea that if word a is related to word b, and word b
is related to word c, then word a and word c should
also form a morphological pair.
Moreover, we plan to explore ways to relax the re-
quirement that all pairs must have a certain degree of
semantic similarity to be treated as morphologically
related (there is evidence that humans treat certain
kinds of semantically opaque forms as morpholog-
ically complex ? see Baroni (2000) and the refer-
ences quoted there). This will probably involve tak-
ing distributional properties of word substrings into
account.
From the point of view of the evaluation
of the algorithm, we should design an as-
sessment scheme that would make our exper-
imental results more directly comparable to
those of Yarowsky and Wicentowski (2000),
Schone and Jurafsky (2000) and others. Moreover,
a more in depth qualitative analysis of the results
should concentrate on identifying specific classes of
morphological processes that our algorithm can or
cannot identify correctly.
We envisage a number of possible uses for the
ranked list that constitutes the output of our model.
First, the model could provide the input for a
more sophisticated rule extractor, along the lines of
those proposed by Albright and Hayes (1999) and
Neuvel (2002). Such models extract morphologi-
cal generalizations in terms of correspondence pat-
terns between whole words, rather than in terms of
affixation rules, and are thus well suited to iden-
tify patterns involving non-concatenative morphol-
ogy and/or morphophonological changes. A list
of related words constitutes a more suitable input
for them than a list of words segmented into mor-
phemes.
Rules extracted in this way would have a number
of practical uses ? for example, they could be used
to construct stemmers for information retrieval ap-
plications, or they could be integrated into morpho-
logical analyzers.
Our procedure could also be used to re-
place the first step of algorithms, such as those
of Goldsmith (2001) and Snover and Brent (2001),
where heuristic methods are employed to generate
morphological hypotheses, and then an information-
theoretically/probabilistically motivated measure is
used to evaluate or improve such hypotheses. More
in general, our algorithm can help reduce the size
of the search space that all morphological discovery
procedures must explore.
Last but not least, the ranked output of (an im-
proved version of) our algorithm can be of use to
the linguist analyzing the morphology of a language,
who can treat it as a way to pre-process her/his
data, while still relying on her/his analytical skills
to extract the relevant morphological generalizations
from the ranked pairs.
Acknowledgements
We would like to thank Adam Albright, Bruce
Hayes and the anonymous reviewers for helpful
comments, and the Austria Presse Agentur for
kindly making the APA corpus available to us. This
work was supported by the European Union in the
framework of the IST programme, project FASTY
(IST-2000-25420). Financial support for ?OFAI is
provided by the Austrian Federal Ministry of Edu-
cation, Science and Culture.
References
A. Albright and B. Hayes. 1999. An automated learner
for phonology and morphology. UCLA manuscript.
M. Baroni. 2000. Distributional cues in morpheme
discovery: A computational model and empirical ev-
idence. Ph.D. dissertation, UCLA.
M. Baroni, J. Matiasek and H. Trost. 2002. Wordform-
and class-based prediction of the components of Ger-
man nominal compounds in an AAC system. To ap-
pear in Proceedings of COLING 2002.
P. Brown, P. Della Pietra, P. DeSouza, J. Lai, and R. Mer-
cer. 1990. Class-based n-gram models of natural lan-
guage. Computational Linguistics, 18:467-479.
K. Church and P. Hanks. 1989. Word association norms,
mutual information, and lexicography. Proceedings of
ACL 27, 76?83.
J. Goldsmith. 2001. Unsupervised learning of the mor-
phology of a natural language. Computational Lin-
guistics, 27:153-198.
C. Jacquemin. 1997. Guessing morphology from terms
and corpora. Proceedings of SIGIR 97, 156?265.
D. Jurafsky and J. Martin. 2000. Speech and Language
Processing. Prentice-Hall, Upper Saddle River, NJ.
L. Karttunen, K. Gaa?l, and A. Kempe. 1997. Xe-
rox Finite-State Tool Xerox Research Centre Europe,
Grenoble.
H. Kuc?era and N. Francis. 1967. Computational analysis
of present-day American English. Brown University
Press, Providence, RI.
C. Manning and H. Schu?tze. 1999. Foundations of sta-
tistical natural language processing. MIT Press, Cam-
bridge, MASS.
S. Neuvel. 2002. Whole word morphologizer. Expand-
ing the word-based lexicon: A non-stochastic compu-
tational approach. Brain and Language, in press.
R. Rosenfeld. 1996. A maximum entropy approach
to adaptive statistical language modeling. Computer
Speech and Language, 10:187?228.
P. Schone and D. Jurafsky. 2000. Knowldedge-free in-
duction of morphology using latent semantic analysis.
Proceedings of the Conference on Computational Nat-
ural Language Learning.
M. Snover and M. Brent. 2001. A Bayesian model for
morpheme and paradigm identification. Proceedings
of ACL 39, 482-490.
D. Yarowksy and R. Wicentowski. 2000. Minimally su-
pervised morphological analysis by multimodal align-
ment. Proceedings of ACL 38, 207?216.
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 1?10,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Revealing the Structure of Medical Dictations
with Conditional Random Fields
Jeremy Jancsary and Johannes Matiasek
Austrian Research Institute for Artificial Intelligence
A-1010 Vienna, Freyung 6/6
firstname.lastname@ofai.at
Harald Trost
Department of Medical Cybernetics and Artificial Intelligence
of the Center for Brain Research, Medical University Vienna, Austria
harald.trost@meduniwien.ac.at
Abstract
Automatic processing of medical dictations
poses a significant challenge. We approach
the problem by introducing a statistical frame-
work capable of identifying types and bound-
aries of sections, lists and other structures
occurring in a dictation, thereby gaining ex-
plicit knowledge about the function of such
elements. Training data is created semi-
automatically by aligning a parallel corpus
of corrected medical reports and correspond-
ing transcripts generated via automatic speech
recognition. We highlight the properties of
our statistical framework, which is based on
conditional random fields (CRFs) and im-
plemented as an efficient, publicly available
toolkit. Finally, we show that our approach
is effective both under ideal conditions and
for real-life dictation involving speech recog-
nition errors and speech-related phenomena
such as hesitation and repetitions.
1 Introduction
It is quite common to dictate reports and leave the
typing to typists ? especially for the medical domain,
where every consultation or treatment has to be doc-
umented. Automatic Speech Recognition (ASR) can
support professional typists in their work by provid-
ing a transcript of what has been dictated. However,
manual corrections are still needed. In particular,
speech recognition errors have to be corrected. Fur-
thermore, speaker errors, such as hesitations or rep-
etitions, and instructions to the transcriptionist have
to be removed. Finally, and most notably, proper
structuring and formatting of the report has to be
performed. For the medical domain, fairly clear
guidelines exist with regard to what has to be dic-
tated, and how it should be arranged. Thus, missing
headings may have to be inserted, sentences must be
grouped into paragraphs in a meaningful way, enu-
meration lists may have to be introduced, and so on.
The goal of the work presented here was to ease
the job of the typist by formatting the dictation ac-
cording to its structure and the formatting guide-
lines. The prerequisite for this task is the identifi-
cation of the various structural elements in the dic-
tation which will be be described in this paper.
complaint dehydration weakness and diarrhea
full stop Mr. Will Shawn is a 81-year-old
cold Asian gentleman who came in with fever
and Persian diaper was sent to the emergency
department by his primary care physician due
him being dehydrated period . . . neck physical
exam general alert and oriented times three
known acute distress vital signs are stable
. . . diagnosis is one chronic diarrhea with
hydration he also has hypokalemia neck number
thromboctopenia probably duty liver cirrhosis
. . . a plan was discussed with patient in
detail will transfer him to a nurse and
facility for further care . . . end of dictation
Fig. 1: Raw output of speech recognition
Figure 1 shows a fragment of a typical report as
recognized by ASR, exemplifying some of the prob-
lems we have to deal with:
? Punctuation and enumeration markers may be
dictated or not, thus sentence boundaries and
numbered items often have to be inferred;
? the same holds for (sub)section headings;
? finally, recognition errors complicate the task.
1
CHIEF COMPLAINT
Dehydration, weakness and diarrhea.
HISTORY OF PRESENT ILLNESS
Mr. Wilson is a 81-year-old Caucasian
gentleman who came in here with fever and
persistent diarrhea. He was sent to the
emergency department by his primary care
physician due to him being dehydrated.
. . .
PHYSICAL EXAMINATION
GENERAL: He is alert and oriented times
three, not in acute distress.
VITAL SIGNS: Stable.
. . .
DIAGNOSIS
1. Chronic diarrhea with dehydration. He
also has hypokalemia.
2. Thromboctopenia, probably due to liver
cirrhosis.
. . .
PLAN AND DISCUSSION
The plan was discussed with the patient
in detail. Will transfer him to a nursing
facility for further care.
. . .
Fig. 2: A typical medical report
When properly edited and formatted, the same
dictation appears significantly more comprehensi-
ble, as can be seen in figure 2. In order to arrive
at this result it is necessary to identify the inherent
structure of the dictation, i.e. the various hierarchi-
cally nested segments. We will recast the segmenta-
tion problem as a multi-tiered tagging problem and
show that indeed a good deal of the structure of med-
ical dictations can be revealed.
The main contributions of our paper are as fol-
lows: First, we introduce a generic approach that can
be integrated seamlessly with existing ASR solu-
tions and provides structured output for medical dic-
tations. Second, we provide a freely available toolkit
for factorial conditional random fields (CRFs) that
forms the basis of aforementioned approach and is
also applicable to numerous other problems (see sec-
tion 6).
2 Related Work
The structure recognition problem dealt with here
is closely related to the field of linear text segmen-
tation with the goal to partition text into coherent
blocks, but on a single level. Thus, our task general-
izes linear text segmentation to multiple levels.
A meanwhile classic approach towards domain-
independent linear text segmentation, C99, is pre-
sented in Choi (2000). C99 is the baseline which
many current algorithms are compared to. Choi?s al-
gorithm surpasses previous work by Hearst (1997),
who proposed the Texttiling algorithm. The best re-
sults published to date are ? to the best of our knowl-
edge ? those of Lamprier et al (2008).
The automatic detection of (sub)section topics
plays an important role in our work, since changes
of topic indicate a section boundary and appropri-
ate headings can be derived from the section type.
Topic detection is usually performed using methods
similar to those of text classification (see Sebastiani
(2002) for a survey).
Matsuov (2003) presents a dynamic programming
algorithm capable of segmenting medical reports
into sections and assigning topics to them. Thus, the
aims of his work are similar to ours. However, he is
not concerned with the more fine-grained elements,
and also uses a different machinery.
When dealing with tagging problems, statistical
frameworks such as HMMs (Rabiner, 1989) or, re-
cently, CRFs (Lafferty et al, 2001) are most com-
monly applied. Whereas HMMs are generative
models, CRFs are discriminative models that can in-
corporate rich features. However, other approaches
to text segmentation have also been pursued. E.g.,
McDonald et al (2005) present a model based on
multilabel classification, allowing for natural han-
dling of overlapping or non-contiguous segments.
Finally, the work of Ye and Viola (2004) bears
similarities to ours. They apply CRFs to the pars-
ing of hierarchical lists and outlines in handwritten
notes, and thus have the same goal of finding deep
structure using the same probabilistic framework.
3 Problem Representation
For representing our segmentation problem we use a
trick that is well-known from chunking and named
entity recognition, and recast the problem as a tag-
ging problem in the so-called BIO1 notation. Since
we want to assign a type to every segment, OUTSIDE
labels are not needed. However, we perform seg-
1BEGIN - INSIDE - OUTSIDE
2
...t1
t2
t3
t4
timestep
... ... ......
...
...
...
...
...
t5
t6
tokens level 1 level 2 level 3 ...< < <
...
B-T3 B-T4B-T1
I-T3 I-T4I-T1
I-T3 I-T4B-T2
I-T3 I-T4I-T2
B-T3 I-T4B-T2
I-T3 I-T4I-T2
Fig. 3: Multi-level segmentation as tagging problem
mentation on multiple levels, therefore multiple la-
bel chains are required. Furthermore, we also want
to assign types to certain segments, thus the labels
need an encoding for the type of segment they rep-
resent. Figure 3 illustrates this representation: B-Ti
denotes the beginning of a segment of type Ti, while
I-Ti indicates that the segment of type Ti continues.
By adding label chains, it is possible to group the
segments of the previous chain into coarser units.
Tree-like structures of unlimited depth can be ex-
pressed this way2. The gray lines in figure 3 denote
dependencies between nodes. Node labels also de-
pend on the input token sequence in an arbitrarily
wide context window.
4 Data Preparation
The raw data available to us consists of two paral-
lel corpora of 2007 reports from the area of medi-
cal consultations, dictated by physicians. The first
corpus, CRCG, consists of the raw output of ASR
(figure 1), the other one, CCOR, contains the corre-
sponding corrected and formatted reports (figure 2).
In order to arrive at an annotated corpus in a for-
2Note, that since we omit a redundant top-level chain, this
structure technically is a hedge rather than a tree.
mat suitable for the tagging problem, we first have
to analyze the report structure and define appropri-
ate labels for each segmentation level. Then, every
token has to be annotated with the appropriate begin
or inside labels. A report has 625 tokens on average,
so the manual annotation of roughly 1.25 million to-
kens seemed not to be feasible. Thus we decided
to produce the annotations programmatically and re-
strict manual work to corrections.
4.1 Analysis of report structure
When inspecting reports in CCOR, a human reader
can easily identify the various elements a report con-
sists of, such as headings ? written in bold on a sepa-
rate line ? introducing sections, subheadings ? writ-
ten in bold followed by a colon ? introducing sub-
sections, and enumerations starting with indented
numbers followed by a period. Going down further,
there are paragraphs divided into sentences. Using
these structuring elements, a hierarchic data struc-
ture comprising all report elements can be induced.
Sections and subsections are typed according to
their heading. There exist clear recommendations
on structuring medical reports, such as E2184-02
(ASTM International, 2002). However, actual med-
ical reports still vary greatly with regard to their
structure. Using the aforementioned standard, we
assigned the (sub)headings that actually appeared in
the data to the closest type, introducing new types
only when absolutely necessary. Finally we arrived
at a structure model with three label chains:
? Sentence level, with 4 labels: Heading,
Subheading, Sentence, Enummarker
? Subsection level, with 45 labels: Paragraph,
Enumelement, None and 42 subsection types
(e.g. VitalSigns, Cardiovascular ...)
? Section level, with 23 section types (e.g.
ReasonForEncounter, Findings, Plan ...)
4.2 Corpus annotation
Since the reports in CCOR are manually edited they
are reliable to parse. We employed a broad-coverage
dictionary (handling also multi-word terms) and a
domain-specific grammar for parsing and layout in-
formation. A regular heading grammar was used for
mapping (sub)headings to the defined (sub)section
labels (for details see Jancsary (2008)). The output
3
CCOR OP CRCG
. . . . . . . . . . . . . . .
B ? Head CHIEF del
Head COMPLAINT sub complaint B ? Head
B ? Sent Dehydration sub dehydration B ? Sent
Sent , del
Sent weakness sub weakness Sent
Sent and sub and Sent
Sent diarrhea sub diarrhea Sent
Sent . sub fullstop Sent
B ? Sent Mr. sub Mr. B ? Sent
Sent Wilson sub Will Sent
ins Shawn Sent
Sent is sub is Sent
Sent a sub a Sent
Sent 81-year-old sub 81-year-old Sent
Sent Caucasian sub cold Sent
Sent ins Asian Sent
Sent gentleman sub gentleman Sent
Sent who sub who Sent
Sent came sub came Sent
Sent in del
Sent here sub here Sent
Sent with sub with Sent
Sent fever sub fever Sent
Sent and sub and Sent
Sent persistent sub Persian Sent
Sent diarrhea sub diaper Sent
Sent . del
. . . . . . . . . . . . . . .
Fig. 4: Mapping labels via alignment
of the parser is a hedge data structure from which
the annotation labels can be derived easily.
However, our goal is to develop a model for rec-
ognizing the report structure from the dictation, thus
we have to map the newly created annotation of re-
ports in CCOR onto the corresponding reports in
CRCG. The basic idea here is to align the tokens
of CCOR with the tokens in CRCG and to copy the
annotations (cf. figure 43). There are some peculiar-
ities we have to take care of during alignment:
1. non-dictated items in CCOR (e.g. punctuation,
headings)
2. dictated words that do not occur inCCOR (meta
instructions, repetitions)
3. non-identical but corresponding items (recog-
nition errors, reformulations)
Since it is particularly necessary to correctly align
items of the third group, standard string-edit dis-
tance based methods (Levenshtein, 1966) need to be
augmented. Therefore we use a more sophisticated
3This approach can easily be generalized to multiple label
chains.
cost function. It assigns tokens that are similar (ei-
ther from a semantic or phonetic point of view) a low
cost for substitution, whereas dissimilar tokens re-
ceive a prohibitively expensive score. Costs for dele-
tion and insertion are assigned inversely. Seman-
tic similarity is computed using Wordnet (Fellbaum,
1998) and UMLS (Lindberg et al, 1993). For pho-
netic matching, the Metaphone algorithm (Philips,
1990) was used (for details see Huber et al (2006)).
4.3 Feature Generation
The annotation discussed above is the first step to-
wards building a training corpus for a CRF-based
approach. What remains to be done is to provide ob-
servations for each time step of the observed entity,
i.e. for each token of a report; these are expected to
give hints with regard to the annotation labels that
are to be assigned to the time step. The observa-
tions, associated with one or more annotation labels,
are usually called features in the machine learning
literature. During CRF training, the parameters of
these features are determined such that they indicate
the significance of the observations for a certain la-
bel or label combination; this is the basis for later
tagging of unseen reports.
We use the following features for each time step
of the reports in CCOR and CRCG:
? Lexical features covering the local context of
? 2 tokens (e.g., patient@0, the@-1, is@1)
? Syntactic features indicating the possible syn-
tactic categories of the tokens (e.g., NN@0,
JJ@0, DT@-1 and be+VBZ+aux@1)
? Bag-of-word (BOW) features intend to cap-
ture the topic of a text segment in a wider
context of ? 10 tokens, without encoding any
order. Tokens are lemmatized and replaced
by their UMLS concept IDs, if available, and
weighed by TF. Thus, different words describ-
ing the same concept are considered equal.
? Semantic type features as above, but using
UMLS semantic types instead of concept IDs
provide a coarser level of description.
? Relative position features: The report is di-
vided into eight parts corresponding to eight bi-
nary features; only the feature corresponding to
the part of the current time step is set.
4
5 Structure Recognition with CRFs
Conditional random fields (Lafferty et al, 2001) are
conditional models in the exponential family. They
can be considered a generalization of multinomial
logistic regression to output with non-trivial internal
structure, such as sequences, trees or other graphical
models. We loosely follow the general notation of
Sutton and McCallum (2007) in our presentation.
Assuming an undirected graphical model G over
an observed entity x and a set of discrete, inter-
dependent random variables4 y, a conditional ran-
dom field describes the conditional distribution:
p(y|x;?) =
1
Z(x)
?
c?G
?c(yc,x;?c) (1)
The normalization term Z(x) sums over all possible
joint outcomes of y, i.e.,
Z(x) =
?
y?
p(y?|x;?) (2)
and ensures the probabilistic interpretation of
p(y|x). The graphical model G describes interde-
pendencies between the variables y; we can then
model p(y|x) via factors ?c(?) that are defined over
cliques c ? G. The factors ?c(?) are computed from
sufficient statistics {fck(?)} of the distribution (cor-
responding to the features mentioned in the previous
section) and depend on possibly overlapping sets of
parameters ?c ? ? which together form the param-
eters ? of the conditional distribution:
?c(yc,x;?c) = exp
?
?
|?c|?
k=1
?ckfck(x,yc)
?
? (3)
In practice, for efficiency reasons, independence as-
sumptions have to be made about variables y ? y,
so G is restricted to small cliques (say, (|c| ? 3).
Thus, the sufficient statistics only depend on a lim-
ited number of variables yc ? y; they can, however,
access the whole observed entity x. This is in con-
trast to generative approaches which model a joint
distribution p(x,y) and therefore have to extend the
independence assumptions to elements x ? x.
4In our case, the discrete outcomes of the random variables
y correspond to the annotation labels described in the previous
section.
The factor-specific parameters ?c of a CRF are
typically tied for certain cliques, according to the
problem structure (i.e., ?c1 = ?c2 for two cliques
c1, c2 with tied parameters). E.g., parameters are
usually tied across time if G is a sequence. The
factors can then be partitioned into a set of clique
templates C = {C1, C2, . . . CP }, where each clique
template Cp is a set of factors with tied parameters
?p and corresponding sufficient statistics {fpk(?)}.
The CRF can thus be rewritten as:
p(y|x) =
1
Z(x)
?
Cp?C
?
?c?Cp
?c(yc,x;?p) (4)
Furthermore, in practice, the sufficient statistics
{fpk(?)} are computed from a subset xc ? x that
is relevant to a factor ?c(?). In a sequence labelling
task, tokens x ? x that are in temporal proximity to
an output variable y ? y are typically most useful.
Nevertheless, in our notation, we will let factors de-
pend on the whole observed entity x to denote that
all of x can be accessed if necessary.
For our structure recognition task, the graphical
model G exhibits the structure shown in figure 3,
i.e., there are multiple connected chains of variables
with factors defined over single-node cliques and
two-node cliques within and between chains; the pa-
rameters of factors are tied across time. This corre-
sponds to the factorial CRF structure described in
Sutton and McCallum (2005). Structure recognition
using conditional random fields then involves two
separate steps: parameter estimation, or training, is
concerned with selecting the parameters of a CRF
such that they fit the given training data. Prediction,
or testing, determines the best label assignment for
unknown examples.
5.1 Parameter estimation
Given IID training dataD = {x(i),y(i)}Ni=1, param-
eter estimation determines:
?? = argmax
??
(
N?
i
p(y(i)|x(i);??)
)
(5)
i.e., those parameters that maximize the conditional
probability of the CRF given the training data.
In the following, we will not explicitly sum over
N
i=1; as Sutton and McCallum (2007) note, the train-
ing instances x(i),y(i) can be considered discon-
nected components of a single undirected model G.
5
We thus assume G and its factors ?c(?) to extend
over all training instances. Unfortunately, (5) cannot
be solved analytically. Typically, one performs max-
imum likelihood estimation (MLE) by maximizing
the conditional log-likelihood numerically:
`(?) =
?
Cp?C
?
?c?Cp
|?p|?
k=1
?pkfpk(x,yc)? logZ(x)
(6)
Currently, limited-memory gradient-based methods
such as LBFGS (Nocedal, 1980) are most com-
monly employed for that purpose5. These require
the partial derivatives of (6), which are given by:
?`
??pk
=
?
?c?Cp
fpk(x,yc)?
?
y?c
fpk(x,y
?
c)p(y
?
c|x)
(7)
and expose the intuitive form of a difference be-
tween the expectation of a sufficient statistic accord-
ing to the empiric distribution and the expectation
according to the model distribution. The latter term
requires marginal probabilities for each clique c, de-
noted by p(y?c|x). Inference on the graphical model
G (see sec 5.2) is needed to compute these.
Depending on the structure ofG, inference can be
very expensive. In order to speed up parameter es-
timation, which requires inference to be performed
for every training example and for every iteration
of the gradient-based method, alternatives to MLE
have been proposed that do not require inference.
We show here a factor-based variant of pseudolike-
lihood as proposed by Sanner et al (2007):
`p(?) =
?
Cp?C
?
?c?Cp
log p(yc|x,MB(?c)) (8)
where the factors are conditioned on the Markov
blanket, denoted by MB6. The gradient of (8) can
be computed similar to (7), except that the marginals
pc(y?c|x) are also conditioned on the Markov blan-
ket, i.e., pc(y?c|x,MB(?c)). Due to its dependence
on the Markov blanket of factors, pseudolikelihood
5Recently, stochastic gradient descent methods such as On-
line LBFGS (Schraudolph et al, 2007) have been shown to per-
form competitively.
6Here, the Markov blanket of a factor ?c denotes the set of
variables occurring in factors that share variables with ?c, non-
inclusive of the variables of ?c
cannot be applied to prediction, but only to param-
eter estimation, where the ?true? assignment of a
blanket is known.
5.1.1 Regularization
We employ a Gaussian prior for training of CRFs
in order to avoid overfitting. Hence, if f(?) is the
original objective function (e.g., log-likelihood or
log-pseudolikelihood), we optimize a penalized ver-
sion f ?(?) instead, such that:
f ?(?) = f(?)?
|?|?
k=1
?2k
2?2
and
?f ?
??k
=
?f
??k
?
?k
?2
.
The tuning parameter ?2 determines the strength of
the penalty; lower values lead to less overfitting.
Gaussian priors are a common choice for parame-
ter estimation of log-linear models (cf. Sutton and
McCallum (2007)).
5.2 Inference
Inference on a graphical model G is needed to ef-
ficiently compute the normalization term Z(x) and
marginals pc(y?c|x) for MLE, cf. equation (6).
Using belief propagation (Yedidia et al, 2003),
more precisely its sum-product variant, we can com-
pute the beliefs for all cliques c ? G. In a tree-
shaped graphical model G, these beliefs correspond
exactly to the marginal probabilities pc(y?c|x). How-
ever, if the graph contains cycles, so-called loopy
belief propagation must be performed. The mes-
sage updates are then re-iterated according to some
schedule until the messages converge. We use a TRP
schedule as described by Wainwright et al (2002).
The resulting beliefs are then only approximations
to the true marginals. Moreover, loopy belief propa-
gation is not guaranteed to terminate in general ? we
investigate this phenomenon in section 6.5.
With regard to the normalization term Z(x),
as equation (2) shows, naive computation requires
summing over all assignments of y. This is too ex-
pensive to be practical. Fortunately, belief propaga-
tion produces an alternative factorization of p(y|x);
i.e., the conditional distribution defining the CRF
can be expressed in terms of the marginals gained
during sum-product belief propagation. This repre-
sentation does not require any additional normaliza-
tion, so Z(x) need not be computed.
6
5.3 Prediction
Once the parameters ? have been estimated from
training data, a CRF can be used to predict the la-
bels of unknown examples. The goal is to find:
y? = argmax
y?
(
p(y?|x;?)
)
(9)
i.e., the assignment of y that maximizes the condi-
tional probability of the CRF. Again, naive computa-
tion of (9) is intractable. However, the max-product
variant of loopy belief propagation can be applied to
approximately find the MAP assignment of y (max-
product can be seen as a generalization of the well-
known Viterbi algorithm to graphical models).
For structure recognition in medical reports, we
employ a post-processing step after label prediction
with the CRFmodel. As in Jancsary (2008), this step
enforces the constraints of the BIO notation and ap-
plies some trivial non-local heuristics that guarantee
a consistent global view of the resulting structure.
6 Experiments and Results
For evaluation, we generally performed 3-fold cross-
validation for all performance measures. We cre-
ated training data from the reports in CCOR so as
to simulate a scenario under ideal conditions, i.e.,
perfect speech recognition and proper dictation of
punctuation and headings, without hesitation or rep-
etitions. In contrast, the data from CRCG reflects
real-life conditions, with a wide variety of speech
recognition error rates and speakers frequently hes-
itating, repeating themselves and omitting punctua-
tion and/or headings.
Depending on the experiment, two different sub-
sets of the two corpora were considered:
? C{COR,RCG}-ALL: All 2007 reports were used,
resulting in 1338 training examples and 669
testing examples at each CV-iteration.
? C{COR,RCG}-BEST : The corpus was restricted
to those 1002 reports that yielded the lowest
word error rate during alignment (see section
4.2). Each CV-iteration hence amounts to 668
training examples and 334 testing examples.
From the crossvalidation runs, a 95%-confidence
interval for each measure was estimated as follows:
Y? ? t(?/2,N?1)
s
?
N
= Y? ? t(0.025,2)
s
?
3
(10)
 0
 10
 20
 30
 40
 50
 60
 70
 80
 90
 100
 0  100  200  300  400  500  600  700  800
re
lat
iv
e l
os
s /
 a
cc
ur
ac
y 
(%
)
number of iterations
Loss on training set
Accuracy on validation set
Fig. 5: Accuracy vs. loss function on CRCG-ALL
where Y? is the sample mean, s is the sample stan-
dard deviation, N is the sample size (3), ? is the de-
sired significance level (0.05) and t(?/2,N?1) is the
upper critical value of the t-distribution with N ? 1
degrees of freedom. The confidence intervals are in-
dicated in the ? column of tables 1, 2 and 3.
For CRF training, we minimized the penalized,
negative log-pseudolikelihood using LBFGS with
m = 3. The variance of the Gaussian prior was set
to ?2 = 1000. All supported features were used for
univariate factors, while the bivariate factors within
chains and between chains were restricted to bias
weights. For testing, loopy belief propagation with
a TRP schedule was used in order to determine the
maximum a posteriori (MAP) assignment. We use
VieCRF, our own implementation of factorial CRFs,
which is freely available at the author?s homepage7.
6.1 Analysis of training progress
In order to determine the number of required train-
ing iterations, an experiment was performed that
compares the progress of the Accuracy measure on
a validation set to the progress of the loss function
on a training set. The data was randomly split into
a training set (2/3 of the instances) and a validation
set. Accuracy on the validation set was computed
using the intermediate CRF parameters ?t every 5
iterations of LBFGS. The resulting plot (figure 5)
demonstrates that the progress of the loss function
corresponds well to that of the Accuracy measure,
7http://www.ofai.at/?jeremy.jancsary/
7
Estimated Accuracies
Acc. ?
Average 97.24% 0.33
Chain 0 99.64% 0.04
Chain 1 95.48% 0.55
Chain 2 96.61% 0.68
Joint 92.51% 0.97
(a) CCOR-ALL
Estimated Accuracies
Acc. ?
Average 86.36% 0.80
Chain 0 91.74% 0.16
Chain 1 85.90% 1.25
Chain 2 81.45% 2.14
Joint 69.19% 1.93
(b) CRCG-ALL
Table 1: Accuracy on the full corpus
Estimated Accuracies
Acc. ?
Average 96.48% 0.82
Chain 0 99.55% 0.08
Chain 1 94.64% 0.23
Chain 2 95.25% 2.16
Joint 90.65% 2.15
(a) CCOR-BEST
Estimated Accuracies
Acc. ?
Average 87.73% 2.07
Chain 0 93.77% 0.68
Chain 1 87.59% 1.79
Chain 2 81.81% 3.79
Joint 70.91% 4.50
(b) CRCG-BEST
Table 2: Accuracy on a high-quality subset
thus an ?early stopping? approach might be tempt-
ing to cut down on training times. However, during
earlier stages of training, the CRF parameters seem
to be strongly biased towards high-frequency labels,
so other measures such as macro-averaged F1 might
suffer from early stopping. Hence, we decided to
allow up to 800 iterations of LBFGS.
6.2 Accuracy of structure prediction
Table 1 shows estimated accuracies for CCOR-ALL
and CRCG-ALL. Overall, high accuracy (> 97%)
can be achieved on CCOR-ALL, showing that the ap-
proach works very well under ideal conditions. Per-
formance is still fair on the noisy data (CRCG-ALL;
Accuracy > 86%). It should be noted that the la-
bels are unequally distributed, especially in chain 0
(there are very few BEGIN labels). Thus, the base-
line is substantially high for this chain, and other
measures may be better suited for evaluating seg-
mentation quality (cf. section 6.4).
6.3 On the effect of noisy training data
Measuring the effect of the imprecise reference an-
notation of CRCG is difficult without a correspond-
ing, manually created golden standard. However, to
get a feeling for the impact of the noise induced
by speech recognition errors and sloppy dictation
Estimated WD
WD ?
Chain 0 0.007 0.000
Chain 1 0.050 0.007
Chain 2 0.015 0.001
(a) CCOR-ALL
Estimated WD
WD ?
Chain 0 0.193 0.008
Chain 1 0.149 0.005
Chain 2 0.118 0.013
(b) CRCG-ALL
Table 3: Per-chain WindowDiff on the full corpus
on the quality of the semi-automatically generated
annotation, we conducted an experiment with sub-
sets CCOR-BEST and CRCG-BEST . The results are
shown in table 2. Comparing these results to ta-
ble 1, one can see that overall accuracy decreased
for CCOR-BEST , whereas we see an increase for
CRCG-BEST . This effect can be attributed to two
different phenomena:
? In CCOR-BEST , no quality gains in the anno-
tation could be expected. The smaller number
of training examples therefore results in lower
accuracy.
? Fewer speech recognition errors and more con-
sistent dictation in CRCG-BEST allow for bet-
ter alignment and thus a better reference anno-
tation. This increases the actual prediction per-
formance and, furthermore, reduces the num-
ber of label predictions that are erroneously
counted as a misprediction.
Thus, it is to be expected that manual correction of
the automatically created annotation results in sig-
nificant performance gains. Preliminary annotation
experiments have shown that this is indeed the case.
6.4 Segmentation quality
Accuracy is not the best measure to assess segmen-
tation quality, therefore we also conducted experi-
ments using the WindowDiff measure as proposed
by Pevzner and Hearst (2002). WindowDiff re-
turns 0 in case of a perfect segmentation; 1 is the
worst possible score. However, it only takes into
account segment boundaries and disregards segment
types. Table 3 shows the WindowDiff scores for
CCOR-ALL and CRCG-ALL. Overall, the scores are
quite good and are consistently below 0.2. Further-
more, CRCG-ALL scores do not suffer as badly from
inaccurate reference annotation, since ?near misses?
are penalized less strongly.
8
Converged (%) Iterations (?)
CCOR-ALL 0.999 15.4
CRCG-ALL 0.911 66.5
CCOR-BEST 0.999 14.2
CRCG-BEST 0.971 37.5
Table 4: Convergence behaviour of loopy BP
6.5 Convergence of loopy belief propagation
In section 5.2, we mentioned that loopy BP is not
guaranteed to converge in a finite number of itera-
tions. Since we optimize pseudolikelihood for pa-
rameter estimation, we are not affected by this limi-
tation in the training phase. However, we use loopy
BP with a TRP schedule during testing, so we must
expect to encounter non-convergence for some ex-
amples. Theoretical results on this topic are dis-
cussed by Heskes (2004). We give here an empir-
ical observation of convergence behaviour of loopy
BP in our setting; the maximum number of itera-
tions of the TRP schedule was restricted to 1,000.
Table 4 shows the percentage of examples converg-
ing within this limit and the average number of iter-
ations required by the converging examples, broken
down by the different corpora. From these results,
we conclude that there is a connection between the
quality of the annotation and the convergence be-
haviour of loopy BP. In practice, even though loopy
BP didn?t converge for some examples, the solutions
after 1,000 iterations where satisfactory.
7 Conclusion and Outlook
We have presented a framework which allows for
identification of structure in report dictations, such
as sentence boundaries, paragraphs, enumerations,
(sub)sections, and various other structural elements;
even if no explicit clues are dictated. Furthermore,
meaningful types are automatically assigned to sub-
sections and sections, allowing ? for instance ? to
automatically assign headings, if none were dic-
tated.
For the preparation of training data a mechanism
has been presented that exploits the potential of par-
allel corpora for automatic annotation of data. Us-
ing manually edited formatted reports and the cor-
responding raw output of ASR, reference annotation
can be generated that is suitable for learning to iden-
tify structure in ASR output.
For the structure recognition task, a CRF frame-
work has been employed and multiple experiments
have been performed, confirming the practicability
of the approach presented here.
One result deserving further investigation is the
effect of noisy annotation. We have shown that
segmentation results improve when fewer errors are
present in the automatically generated annotation.
Thus, manual correction of the reference annotation
will yield further improvements.
Finally, the framework presented in this paper
opens up exciting possibilities for future work.
In particular, we aim at automatically transform-
ing report dictations into properly formatted and
rephrased reports that conform to the requirements
of the relevant domain. Such tasks are greatly facili-
tated by the explicit knowledge gained during struc-
ture recognition.
Acknowledgments
The work presented here has been carried out in
the context of the Austrian KNet competence net-
work COAST. We gratefully acknowledge funding
by the Austrian Federal Ministry of Economics and
Labour, and ZIT Zentrum fuer Innovation und Tech-
nologie, Vienna. The Austrian Research Institute
for Artificial Intelligence is supported by the Aus-
trian Federal Ministry for Transport, Innovation, and
Technology and by the Austrian Federal Ministry for
Science and Research.
Furthermore, we would like to thank our anony-
mous reviewers for many insightful comments that
helped us improve this paper.
References
ASTM International. 2002. ASTM E2184-02: Standard
specification for healthcare document formats.
Freddy Choi. 2000. Advances in domain independent
linear text segmentation. In Proceedings of the first
conference on North American chapter of the Associa-
tion for Computation Linguistics, pages 26?33.
C. Fellbaum. 1998. WordNet: an electronic lexical
database. MIT Press, Cambridge, MA.
Marti A. Hearst. 1997. Texttiling: Segmenting text into
multi-paragraph subtopic passages. Computational
Linguistics, 23(1):36?47.
9
Tom Heskes. 2004. On the uniqueness of loopy
belief propagation fixed points. Neural Comput.,
16(11):2379?2413.
Martin Huber, Jeremy Jancsary, Alexandra Klein, Jo-
hannes Matiasek, and Harald Trost. 2006. Mismatch
interpretation by semantics-driven alignment. In Pro-
ceedings of KONVENS ?06.
Jeremy M. Jancsary. 2008. Recognizing structure in re-
port transcripts. Master?s thesis, Vienna University of
Technology.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional Random Fields: Probabilistic mod-
els for segmenting and labeling sequence data. In Pro-
ceedings of the Eighteenth International Conference
on Machine Learning (ICML).
S. Lamprier, T. Amghar, B. Levrat, and F. Saubion.
2008. Toward a more global and coherent segmen-
tation of texts. Applied Artificial Intelligence, 23:208?
234, March.
Vladimir I. Levenshtein. 1966. Binary codes capable of
correcting deletions, insertions and reversals. Soviet
Physics Doklady, 10(8):707?710.
D. A. B. Lindberg, B. L. Humphreys, and A. T. McCray.
1993. The Unified Medical Language System. Meth-
ods of Information in Medicine, 32:281?291.
Evgeny Matsuov. 2003. Statistical methods for text
segmentation and topic detection. Master?s the-
sis, Rheinisch-Westfa?lische Technische Hochschule
Aachen.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005. Flexible text segmentation with structured
multilabel classification. In Proceedings of Human
Language Technology Conference and Conference on
Empirical Methods in Natural Language Processing
(HLT/EMNLP), pages 987?994.
Jorge Nocedal. 1980. Updating Quasi-Newton matri-
ces with limited storage. Mathematics of Computa-
tion, 35:773?782.
Lev Pevzner and Marti Hearst. 2002. A critique and
improvement of an evaluation metric for text segmen-
tation. Computational Linguistics, 28(1), March.
Lawrence Philips. 1990. Hanging on the metaphone.
Computer Language, 7(12).
L. R. Rabiner. 1989. A tutorial on hidden Markov mod-
els and selected applications in speech recognition.
Proceedings of the IEEE, 77:257?286, February.
Scott Sanner, Thore Graepel, Ralf Herbrich, and Tom
Minka. 2007. Learning CRFs with hierarchical fea-
tures: An application to go. International Conference
on Machine Learning (ICML) workshop.
Nicol N. Schraudolph, Jin Yu, and Simon Gu?nter. 2007.
A stochastic Quasi-Newton Method for online convex
optimization. In Proceedings of 11th International
Conference on Artificial Intelligence and Statistics.
Fabrizio Sebastiani. 2002. Machine learning in auto-
mated text categorization. ACM Computing Surveys,
34(1):1?47.
Charles Sutton and Andrew McCallum. 2005. Composi-
tion of Conditional Random Fields for transfer learn-
ing. In Proceedings of Human Language Technologies
/ Empirical Methods in Natural Language Processing
(HLT/EMNLP).
Charles Sutton and Andrew McCallum. 2007. An intro-
duction to Conditional Random Fields for relational
learning. In Lise Getoor and Ben Taskar, editors,
Introduction to Statistical Relational Learning. MIT
Press.
Martin Wainwright, Tommi Jaakkola, and Alan S. Will-
sky. 2002. Tree-based reparameterization framework
for analysis of sum-product and related algorithms.
IEEE Transactions on Information Theory, 49(5).
Ming Ye and Paul Viola. 2004. Learning to parse hi-
erarchical lists and outlines using Conditional Ran-
dom Fields. In Proceedings of the Ninth International
Workshop on Frontiers in Handwriting Recognition
(IWFHR?04), pages 154?159. IEEE Computer Soci-
ety.
Jonathan S. Yedidia, William T. Freeman, and YairWeiss,
2003. Understanding Belief Propagation and its Gen-
eralizations, Exploring Artificial Intelligence in the
New Millennium, chapter 8, pages 236?239. Science
& Technology Books, January.
10
Proceedings of EACL 2009 Workshop on Semantic Representation of Spoken Language - SRSL 2009, pages 19?25,
Athens, Greece, 30 March 2009. c?2009 Association for Computational Linguistics
Identifying Segment Topics in Medical Dictations
Johannes Matiasek, Jeremy Jancsary
Alexandra Klein
Austrian Research Institute for
Artificial Intelligence
Freyung 6, Wien, Austria
firstname.lastname@ofai.at
Harald Trost
Department of Medical Cybernetics
and Artificial Intelligence
of the Center for Brain Research,
Medical University Vienna, Austria
harald.trost@meduniwien.ac.at
Abstract
In this paper, we describe the use of lexi-
cal and semantic features for topic classi-
fication in dictated medical reports. First,
we employ SVM classification to assign
whole reports to coarse work-type cate-
gories. Afterwards, text segments and
their topic are identified in the output
of automatic speech recognition. This
is done by assigning work-type-specific
topic labels to each word based on fea-
tures extracted from a sliding context win-
dow, again using SVM classification uti-
lizing semantic features. Classifier stack-
ing is then used for a posteriori error cor-
rection, yielding a further improvement in
classification accuracy.
1 Introduction
The use of automatic speech recognition (ASR) is
quite common in the medical domain, where for
every consultation or medical treatment a written
report has to be produced. Usually, these reports
are dictated and transcribed afterwards. The use of
ASR can, thereby, significantly reduce the typing
efforts, but, as can be seen in figure 1, quite some
work is left.
complaint dehydration weakness and diarrhea full
stop Mr. Will Shawn is a 81-year-old cold Asian
gentleman who came in with fever and Persian
diaper was sent to the emergency department by his
primary care physician due him being dehydrated
period . . . neck physical exam general alert and
oriented times three known acute distress vital
signs are stable . . . diagnosis is one chronic
diarrhea with hydration he also has hypokalemia
neck number thromboctopenia probably duty liver
cirrhosis . . . a plan was discussed with patient in
detail will transfer him to a nurse and facility
for further care . . . end of dictation
Figure 1: Raw output of speech recognition
When properly edited and formatted, the same
dictation appears significantly more comprehensi-
ble, as can be seen in figure 2.
CHIEF COMPLAINT
Dehydration, weakness and diarrhea.
HISTORY OF PRESENT ILLNESS
Mr. Wilson is a 81-year-old Caucasian gentleman
who came in here with fever and persistent
diarrhea. He was sent to the emergency department
by his primary care physician due to him being
dehydrated.
. . .
PHYSICAL EXAMINATION
GENERAL: He is alert and oriented times three,
not in acute distress.
VITAL SIGNS: Stable.
. . .
DIAGNOSIS
1. Chronic diarrhea with dehydration. He also
has hypokalemia.
2. Thromboctopenia, probably due to liver
cirrhosis.
. . .
PLAN AND DISCUSSION
The plan was discussed with the patient in detail.
Will transfer him to a nursing facility for
further care.
. . .
Figure 2: A typical medical report
Besides the usual problem with recognition er-
rors, section headers are often not dictated or hard
to recognize as such. One task that has to be per-
formed in order to arrive at the structured report
shown in figure 2 is therefore to identify topical
sections in the text and to classify them accord-
ingly.
In the following, we first describe the problem
setup, the steps needed for data preparation, and
the division of the classification task into subprob-
lems. We then describe the experiments performed
and their results.
In the outlook we hint at ways to integrate this
approach with another, multilevel, segmentation
framework.
2 Data Description and Problem Setup
Available corpus data consists of raw recognition
results and manually formatted and corrected re-
ports of medical dictations. 11462 reports were
19
available in both forms, 51382 reports only as cor-
rected transcripts. When analysing the data, it
became clear that the structure of segment topics
varied strongly across different work-types. Thus
we decided to pursue a two-step approach: firstly
classify reports according to their work-type and,
secondly, train and apply work-type specific clas-
sification models for segment topic classification.
2.1 Classification framework
For all classification tasks discussed here, we em-
ployed support-vector machines (SVM, Vapnik
(1995)) as the statistical framework, though in dif-
ferent incarnations and setups. SVMs have proven
to be an effective means for text categorization
(Joachims, 1998) as they are capable to robustly
deal with high-dimensional, sparse feature spaces.
Depending on the task, we experimented with dif-
ferent feature weighting schemes and SVM kernel
functions as will be described in section 3.
2.2 Features used for classification
The usual approach in text categorization is to use
bag-of-word features, i.e. the words occuring in a
document are collected disregarding the order of
their appearance. In the domain of medical dic-
tation, however, often abbreviations or different
medical terms may be used to refer to the same se-
mantic concept. In addition, medical terms often
are multi-word expressions, e.g., ?coronary heart
disease?. Therefore, a better approach for feature
mapping is needed to arrive at features at an ap-
propriate generalization level:
? Tokenization is performed using a large
finite-state lexicon including multi-word
medical concepts extracted from the UMLS
medical metathesaurus (Lindberg et al,
1993). Thus, multi-word terms remain intact.
In addition, numeric quantities in special
(spoken or written) formats or together with a
dimension are mapped to semantic types (e.g.
?blood pressure? or ?physical quantity?), also
using a finite-state transducer.
? The tokens are lemmatized and, if possi-
ble, replaced by the UMLS semantic con-
cept identifier(s) they map to. Thus,
?CHD?, ?coronary disease? and ?coronary
heart disease? all map to the same concept
?C0010068?.
? In addition, also the UMLS semantic type, if
available, is used as a feature, so, in the ex-
ample above, ?B2.2.1.2.1? (Disease or Syn-
drome) is added.
? Since topics in a medical report roughly fol-
low an order, for the segment topic identifica-
tion task also the relative position of a word
in the report (ranging from -1 to +1) is used.
We also explored different weighting schemes:
? binary: only the presence of a feature is in-
dicated.
? term frequency: the number of occurences
of a feature in the segment to be classified is
used as weight.
? TFIDF: a measure popular from information
retrieval, where tfidfi,j of term ti in docu-
ment dj ? D is usually defined as
cti,j
?
i cti,j
. log
|D|
|{dj : ti ? dj}|
An example of how this feature extraction pro-
cess works is given below:
token(s) feature(s) comment
...
an stop word
78 year old QH OLD pattern-based type
female C0085287 UMLS concept
A2.9.2 UMLS semtype
intubated intubate lemmatized (no concept)
with stop word
lung cancer C0242379 UMLS concept
C0684249 UMLS concept
B2.2.1.2.1.2 UMLS semtype
...
2.3 Data Annotation
For the first classification task, i.e. work-type clas-
sification, no further annotation is necessary, ev-
ery report in our data corpus had a label indicating
the work-type. For the segment topic classification
task, however, every token of the report had to be
assigned a topic label.
2.3.1 Analysis of Corrected Transcripts
For the experiments described here, we con-
centrated on the ?Consultations? work-type, for
which clear structuring recommendations, such
as E2184-02 (ASTM International, 2002), exist.
However, in practice the structure of medical re-
ports shows high variation and deviations from
the guidelines, making it harder to come up with
20
an appropriate set of class labels. Therefore, us-
ing the aforementioned standard, we assigned the
headings that actually appeared in the data to the
closest type, introducing new types only when ab-
solutely necessary. Thus we arrived at 23 heading
classes. Every (possibly multi-word) token was
then labeled with the heading class of the last sec-
tion heading occurring before it in the text using a
simple parser.
2.3.2 Aligment and Label Transfer
When inspecting manually corrected reports (cf.
fig. 2), one can easily identify a heading and clas-
sify the topic of the text below it accordingly.
However, our goal is to develop a model for iden-
tifying and classifying segments in the dictation,
thus we have to map the annotation of corrected
reports onto the corresponding ASR output. The
basic idea here is to align the tokens of the cor-
rected report with the tokens in ASR output and to
copy the annotations (cf. figure 3). There are some
problems we have to take care of during align-
ment:
1. non-dictated items in the corrected test (e.g.
punctuation, headings)
2. dictated words that do not occur in the cor-
rected text (meta instructions, repetitions)
3. non-identical but corresponding items
(recognition errors, reformulations)
For this alignment task, a standard string-edit
distance based method is not sufficient. There-
fore, we augment it with a more sophisticated cost
function. It assigns tokens that are similar (ei-
ther from a semantic or from a phonetic point of
view) a low cost for substitution, whereas dissimi-
lar tokens receive a prohibitively expensive score.
Costs for deletion and insertion are assigned in-
versely. Semantic similarity is computed using
Wordnet (Fellbaum, 1998) and UMLS. For pho-
netic matching, the Metaphone algorithm (Philips,
1990) was used (for details see Huber et al (2006)
and Jancsary et al (2007)).
3 Experiments
3.1 Work-Type Categorization
In total we had 62844 written medical reports
with assigned work-type information from differ-
ent hospitals, 7 work-types are distinguished. We
randomly selected approximately a quarter of the
corrected report OP ASR output
. . . . . . . . . . . . . . .
ChiefCompl CHIEF del
ChiefCompl COMPLAINT sub complaint ChiefCompl
ChiefCompl Dehydration sub dehydration ChiefCompl
ChiefCompl , del
ChiefCompl weakness sub weakness ChiefCompl
ChiefCompl and sub and ChiefCompl
ChiefCompl diarrhea sub diarrhea ChiefCompl
ChiefCompl . sub fullstop ChiefCompl
HistoryOfP Mr. sub Mr. HistoryOfP
HistoryOfP Wilson sub Will HistoryOfP
ins Shawn HistoryOfP
HistoryOfP is sub is HistoryOfP
HistoryOfP a sub a HistoryOfP
HistoryOfP 81-year-old sub 81-year-old HistoryOfP
HistoryOfP Caucasian sub cold HistoryOfP
HistoryOfP ins Asian HistoryOfP
HistoryOfP gentleman sub gentleman HistoryOfP
HistoryOfP who sub who HistoryOfP
HistoryOfP came sub came HistoryOfP
HistoryOfP in del
HistoryOfP here sub here HistoryOfP
HistoryOfP with sub with HistoryOfP
HistoryOfP fever sub fever HistoryOfP
HistoryOfP and sub and HistoryOfP
HistoryOfP persistent sub Persian HistoryOfP
HistoryOfP diarrhea sub diaper HistoryOfP
HistoryOfP . del
. . . . . . . . . . . . . . .
Figure 3: Mapping labels via alignment
reports as the training set, the rest was used for
testing. The distribution of the data can be seen in
table 1.
Trainingset Testset Work-Type
649 4.1 1966 4.2 CACardiology
7965 51.0 24151 51.1 CL ClinicalReports
1867 11.9 5590 11.8 CNConsultations
1120 7.2 3319 7.0 DS DischargeSummaries
335 2.1 878 1.8 ER EmergencyMedicine
2185 14.0 6789 14.4 HP HistoryAndPhysicals
1496 9.6 4534 9.6 OROperativeReports
15617 47227 Total
Table 1: Distribution of Work-types
As features for categorization, we used a bag-
of-words approach, but instead of the surface form
of every token of a report, we used its semantic
features as described in section 2.2. As a catego-
rization engine, we used LIBSVM (Chang&Lin,
2001) with an RBF kernel. The features where
weighted with TFIDF. In order to compensate for
different document length, each feature vector was
normalized to unit length. After some param-
eter tuning iterations, the SVM model performs
really well with a microaveraged F11 value of
0.9437. This indicates high overall accuracy, and
the macroaveraged F1 value of 0.9341 shows, that
also lower frequency categories are predicted quite
reliably. The detailed results are shown in table 2.
Thus the first step in the cascaded model, i.e.
the selection of the work-type specific segment
1F1 = 2?precision?recallprecision+recall
21
predicted rec. prec. F1
true CA CL CN DS ER HP OR
CA 1966 1882 53 5 6 0 9 11 0.9573 0.9787 0.9679
CL 24151 25 23675 217 13 18 155 48 0.9803 0.9529 0.9664
CN 5590 1 447 4695 7 17 413 10 0.8399 0.8814 0.8601
DS 3319 1 37 8 3241 2 27 3 0.9765 0.9818 0.9792
ER 878 0 90 7 10 754 13 4 0.8588 0.9425 0.8987
HP 6789 4 512 393 22 7 5838 13 0.8599 0.9040 0.8814
OR 4534 10 31 2 2 2 3 4484 0.9890 0.9805 0.9847
microaveraged 0.9437
macroaveraged 0.9341
Table 2: Work-Type categorization results
topic model, yields reliable performance.
3.2 Segment Topic Classification
In contrast to work-type categorization, where
whole reports need to be categorized, the identifi-
cation of segment topics requires a different setup.
Since not only the topic labels are to be deter-
mined, but also segment boundaries are unknown
in the classification task, each token constitutes
an example under this setting. Segments are then
contiguous text regions with the same topic label.
It is clearly not enough to consider only features
of the token to be classified, thus we include also
contextual and positional features.
3.2.1 Feature and Kernel Selection
In particular, we employ a sliding window ap-
proach, i.e. for each data set not only the token to
be classified, but also the 10 preceding and the 10
following tokens are considered (at the beginning
or towards the end of a report, context is reduced
appropriately). This window defines the text frag-
ment to be used for classifying the center token,
and features are collected from this window again
as described in section 2.2. Additionaly, the rela-
tive position (ranging from -1 to +1) of the center
token is used as a feature.
The rationale behind this setup is that
1. usually topics in medical reports follow an or-
dering, thus relative position may help.
2. holding features also from adjacent segments
might also be helpful since topic succession
also follows typical patterns.
3. a sufficiently sized context might also smooth
label assignment and prevent label oscilla-
tion, since the classification features for ad-
jacent words overlap to a great deal.
A second choice to be made was the selection
of the kernel best suited for this particular classifi-
cation problem. In order to get an impression, we
made a preliminary mini-experiment with just 5
reports each for training (4341 datasets) and test-
ing (3382 datasets), the results of which are re-
ported in table 3.
Accuracy
Feature Weight linear RBF
TFIDF 0.4977 0.3131
TFIDF normalized 0.5544 0.6199
Binary 0.6417 0.6562
Table 3: Preliminary Kernel Comparison
While these results are of course not significant,
two things could be learned from the preliminary
experiment:
1. linear kernels may have similar or even better
performance,
2. training times with LIBSVM with a large
number of examples may soon get infeasible
(we were not able to repeat this experiment
with 50 reports due to excessive runtime).
Since LibSVM solves linear and nonlinear
SVMs in the same way, LibSVM is not particu-
larly efficient for linear SVMs. Therefore we de-
cided to switch to Liblinear (Fan et al, 2008), a
linear classifier optimized for handling data with
millions of instances and features2.
2Indeed, training a model from 669 reports (463994 ex-
amples) could be done in less then 5 minutes!
22
predicted class label (#)
# True Label Total F1 . . . 3 4 . . . 14 . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . .
3 Diagnosis 40871 0.603 . . . 24391 2864 . . . 8691 . . .
4 DiagAndPlan 21762 0.365 . . . 5479 6477 . . . 7950 . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . .
14 Plan 31729 0.598 . . . 5714 3419 . . . 21034 . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . .
Table 4: Confusion matrix (part of)
3.2.2 Segment Topic Classification Results
Experiments were performed on a randomly se-
lected subset of reports from the ?Consultations?
work-type (1338) that were available both in cor-
rected form and in raw ASR output form. An-
notations were constructed for the corrected tran-
scripts, as described in section 2.3, transfer of la-
bels to the ASR output was performed as shown in
section 2.3.2.
Both data sets were split into training and test
sets of equal size (669 reports each), experiments
with different feature weighting schemes have
been performed on both corrected data and ASR
output. The overall results are shown in table 5.
corrected reports ASR output
micro- macro- micro- macro-
Feature weights avg.F1 avg.F1 avg.F1 avg.F1
TFIDF 0.7553 0.5178 0.7136 0.4440
TFIDF norm. 0.7632 0.3470 0.7268 0.3131
Binary 0.7693 0.4636 0.7413 0.3953
Table 5: Segment topic classification results
Consistently, macroaveraged F1 values are
much lower than their microaveraged counterparts
indicating that low-frequency topic labels are pre-
dicted with less accuracy.
Also, segment classification works better with
corrected reports than with raw ASR output. The
reason for that behaviour is
1. ASR data are more noisy due to recognition
errors, and
2. while in corrected reports appropriate section
headers are available (not as header, but the
words) this is not necessarily the case in ASR
output (also the wording of dictated headers
and written headers may be different).
A general note on the used topic labels must
also be made: Due to the nature of our data it
was inevitable to use topic labels that overlap in
some cases. The most prominent example here is
?Diagnosis?, ?Plan?, and ?Diagnosis and Plan?.
The third label clearly subsumes the other two, but
in the data available the physicians often decided
to dictate diagnoses and the respective treatment
in an alternating way, associating each diagnosis
with the appropriate plan. This made it necessary
to include all three labels, with obvious effects that
could easily seen when inspecting the confusion
matrix, a part of which is shown in table 4.
When looking at the misclassifications in these
3 categories it can easily be seen, that they are pre-
dominantly due to overlapping categories.
Another source of problems in the data is the
skewed distribution of segment types in the re-
ports. Sections labelled with one of the four la-
bel categories that weren?t predicted at all (Chief-
Complaints, Course, Procedure, and Time, cf. ta-
ble 6) occur in less than 2% of the reports or are
infrequent and extremely short. This fact had, of
course, undesirable effects on the macroavered F1
scores. Additional difficulties that are similar to
the overlap problem discussed above are strong
thematic similarities between some section types
(e.g., Findings and Diagnosis, or ReasonForEn-
counter andHistoryOfPresentIllness) that result in
a very similar vocabulary used.
Given these difficulties due to the data, the re-
sults are encouraging. There is, however, still
plenty of room left for improvement.
3.3 Improving Topic Classification
Liblinear does not only provide class label pre-
dictions, it is also possible to obtain class proba-
bilities. The usual way then to predict the label
is to choose the one with the highest probability.
When analysing the errors made by the segment
topic classification task described above, it turned
out that often the correct label was ranked second
or third (cf. table 6). Thus, the idea of just taking
23
correct prediction in
Label count best best 2 best 3
Allergies 3456 29.72 71.64 85.21
ChiefComplai 697
Course 30
Diagnosis 43565 64.69 83.29 91.37
DiagAndPlan 19409 35.24 70.45 86.81
DiagnosticSt 35554 82.47 91.34 93.05
Findings 791 0.38 1.26
Habits 2735 7.31 32.69 41.76
HistoryOfPre 122735 92.26 97.55 98.20
Medication 14553 85.87 93.38 95.22
Neurologic 5226 54.08 86.93 89.19
PastHistory 43775 71.13 86.26 88.82
PastSurgical 5752 49.32 78.88 84.47
PhysicalExam 86031 93.56 97.01 97.57
Plan 36476 62.57 84.63 94.65
Practitioner 1262 55.07 76.78 82.73
Procedures 109
ReasonForEnc 15819 25.42 42.35 43.47
ReviewOfSyst 29316 79.81 89.90 91.87
Time 58
Total 467349 76.93 88.65 92.00
Table 6: Ranked predictions
the highest ranked class label could be possibly
improved by a more informed choice.
While the segment topic classifier already takes
contextual features into account, it has still no in-
formation on the classification results of the neigh-
boring text segments. However, there are con-
straints on the length of text segments, thus, e.g.
a text segment of length 1 with a different topic la-
bel than the surrounding text is highly implausible.
Furthermore, there are also regularities in the suc-
cession of topic labels, which can be captured by
the monostratal local classification only indirectly
? if at all.
A look at figure 4 exemplifies how a bet-
ter informed choice of the label could result in
higher prediction accuracy. The segment labelled
?PastHistory? correctly ends 4 tokens earlier than
predicted, and, additionally, this label erroneously
is predicted again for the phrase ?progressive
weight loss?. The correct label, however, has still
a rather high probability in the predicted label
distribution. By means of stacking an additional
classier onto the first one we hope to be able to
correct some of the locally made errors a posteri-
ori.
The setup for the error correction classifier
we experimented with was as follows (it was
performed only for the segment topic classi-
fier trained on ASR output with binary feature
weights):
1. The training set of the classifier was clas-
Label probabilities (%)
True Label Predicted ... 10 11 12 ... 17 18
. . .
= PastHistory [11] age PastHistory 0 95 0 0 0
= PastHistory [11] 63 PastHistory 0 95 0 0 0
= PastHistory [11] and PastHistory 0 95 0 0 1
= PastHistory [11] his PastHistory 0 95 0 0 1
= PastHistory [11] father PastHistory 0 88 0 0 9
= PastHistory [11] died PastHistory 0 90 0 0 8
= PastHistory [11] from PastHistory 0 84 0 0 14
= PastHistory [11] myocardial infa PastHistory 0 81 0 0 17
= PastHistory [11] at PastHistory 0 77 0 0 20
= PastHistory [11] age PastHistory 0 78 0 1 19
= PastHistory [11] 57 PastHistory 0 78 0 1 19
= PastHistory [11] period PastHistory 0 78 0 1 19
- ReviewOfSyst[18] review PastHistory 0 76 0 1 20
- ReviewOfSyst[18] of PastHistory 0 76 0 1 21
- ReviewOfSyst[18] systems PastHistory 0 78 0 0 19
- ReviewOfSyst[18] he PastHistory 1 57 0 1 37
= ReviewOfSyst[18] has ReviewOfSyst 1 32 0 1 58
= ReviewOfSyst[18] had ReviewOfSyst 1 32 0 1 58
- ReviewOfSyst[18] progressive PastHistory 1 49 0 1 42
- ReviewOfSyst[18] weight loss PastHistory 1 60 0 1 32
= ReviewOfSyst[18] period ReviewOfSyst 1 31 0 0 62
= ReviewOfSyst[18] his ReviewOfSyst 1 13 0 1 81
= ReviewOfSyst[18] appetite ReviewOfSyst 1 13 0 1 81
. . .
Figure 4: predicted label probabilites
sified, and the predicted label probabilities
were collected as features.
2. Again, a sliding window (with different
sizes) was used for feature construction. Fea-
tures were set up for each label at each win-
dow position and the respective predicted la-
bel probability was used as its value.
3. A linear classifier was trained on these fea-
tures of the training set
4. This classifier was applied to the results of
classifying the test set with the original seg-
ment topic classifier.
Three different window sizes were used on the
corrected reports, only one window was applied
on ASR output (cf. table 7). As can be seen, each
corrected reports ASR output
micro- macro- micro- macro-
context window avg.F1 avg.F1 avg.F1 avg.F1
No correction 0.7693 0.4636 0.7413 0.3953
[?3, +3] 0.7782 0.4773 - -
[?6, +0] 0.7798 0.4754 - -
[?3, +4] 0.7788 0.4769 0.7520 0.4055
Table 7: A posteriori correction results
context variant improved on both microaveraged
and macroaveraged F1 in a range of 0,9 to 1.4 per-
cent points. Thus, stacked error correction indeed
is possible and able to improve classification re-
sults.
24
4 Conclusion and Outlook
We have presented a 3 step approach to seg-
ment topic identification in dictations of medi-
cal reports. In the first step, a categorization of
work-type is performed on the whole report us-
ing SVM classification employing semantic fea-
tures. The categorization model yields good per-
formance (over 94% accuracy) and is a prerequi-
site for subsequent application of work-type spe-
cific segment classification models.
For segment topic detection, every word was as-
signed a class label based on contextual features
in a sliding window approach. Here also semantic
features were used as a means for feature gener-
alisation. In various experiments, linear models
using binary feature weights had the best perfor-
mance. A posteriori error correction via classifier
stacking additionally improved the results.
When comparing our results to the results of
Jancsary et al (2008), who pursue a multi-level
segmentation aproach using conditional random
fields optimizing over the whole report, the locally
obtained SVM results cannot compete fully. On
label chain 2, which is equivalent to segment top-
ics as investigated here, Jancsary et al (2008) re-
port an estimated accuracy of 81.45 ? 2.14 % on
ASR output (after some postprocessing), whereas
our results, even with a posteriori error correction,
are at least 4 percent points behind. This is prob-
ably due to the fact that the multi-level annotation
employed in Jancsary et al (2008) contains addi-
tional information useful for the learning task, and
constraints between the levels improve segmenta-
tion behavior at the segment boundaries. Never-
theless, our approach has the merit of employing a
framework that can be trained in a fraction of the
time needed for CRF training, and classification
works locally.
An investigation on how to combine these two
complementary approaches is planned for the fu-
ture. The idea here is to use the probability distri-
butions on labels returned by our approach as (ad-
ditional) features in the CRF model. It might be
possible to leave out some other features currently
employed in return, thereby reducing model com-
plexity. The benefit we hope to get by doing so are
shorter training time for CRF training, and, since,
contrary to CRFs, SVMs are a large margin classi-
fication method, hopefully the CRF model can be
improved by the present approach.
Acknowledgments
The work presented here has been carried out in
the context of the Austrian KNet competence net-
work COAST. We gratefully acknowledge fund-
ing by the Austrian Federal Ministry of Economics
and Labour, and ZIT Zentrum fuer Innovation und
Technologie, Vienna. The Austrian Research In-
stitute for Artificial Intelligence is supported by
the Austrian Federal Ministry for Transport, Inno-
vation, and Technology and by the Austrian Fed-
eral Ministry for Science and Research.
References
ASTM International. 2002. ASTM E2184-02: Stan-
dard specification for healthcare document formats.
C.-C. Chang and C.-J. Lin. 2001. LIBSVM: a library
for support vector machines. Software available at
http://www.csie.ntu.edu.tw/ cjlin/libsvm
R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and
C.-J. Lin. 2008. LIBLINEAR: A library for large
linear classification. Journal of Machine Learning
Research, 9(2008):1871?1874.
C. Fellbaum. 1998. WordNet: an electronic lexical
database. MIT Press, Cambridge, MA.
M. Huber, J. Jancsary, A. Klein, J. Matiasek, H. Trost.
2006. Mismatch interpretation by semantics-driven
alignment. Proceedings of Konvens 2006.
J. Jancsary, A. Klein, J. Matiasek, H. Trost. 2007.
Semantics-based Automatic Literal Reconstruction
Of Dictations. In Alcantara M. and Declerck
T.(eds.), Semantic Representation of Spoken Lan-
guage 2007 (SRSL7) Universidad de Salamanca,
Spain, pp. 67-74.
J. Jancsary, J. Matiasek, H. Trost. 2008. Reveal-
ing the Structure of Medical Dictations with Con-
ditional Random Fields. Proceedings of the 2008
Conference on Empirical Methods in Natural Lan-
guage Processing. Association for Computational
Linguistics, pp. 1?10.
T. Joachims. 1998. Text Categorization with Sup-
port Vector Machines: Learning with Many Rele-
vant Features. Proceedings of the European Confer-
ence on Machine Learning. Springer, pp. 137?142.
D.A.B. Lindberg, B.L. Humphreys, A.T. McCray.
1993. The Unified Medical Language System.
Methods of Information in Medicine, (32):281-291.
Lawrence Philips. 1990. Hanging on the metaphone.
Computer Language, 7(12).
V.N. Vapnik 1995. The Nature of Statistical Learning
Theory. Springer.
25
Proceedings of the NAACL HLT 2010 Second Louhi Workshop on Text and Data Mining of Health Documents, pages 22?28,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Using Domain Knowledge about Medications to Correct Recognition Errors
in Medical Report Creation
Stephanie Schreitter
Alexandra Klein
Johannes Matiasek
Austrian Research Institute
for Artificial Intelligence (OFAI)
Freyung 6/6
1010 Vienna, Austria
firstname.lastname@ofai.at
Harald Trost
Section for Artificial Intelligence
Center for Med. Statistics, Informatics,
and Intelligent Systems
Medical University of Vienna
Freyung 6/2
1010 Vienna, Austria
harald.trost@meduniwien.ac.at
Abstract
We present an approach to analysing auto-
matic speech recognition (ASR) hypotheses
for dictated medical reports based on back-
ground knowledge. Our application area is
prescriptions of medications, which are a fre-
quent source of misrecognitions: In a sam-
ple report corpus, we found that about 40%
of the active substances or trade names and
dosages were recognized incorrectly. In about
25% of these errors, the correct string of words
was contained in the word graph. We have
built a knowledge base of medications based
on information contained in the Unified Med-
ical Language System (UMLS), consisting
of trade names, active substances, strengths
and dosages. From this, we generate a va-
riety of linguistic realizations for prescrip-
tions. Whenever an inconsistency in a pre-
scription is encountered on the best path of
the word graph, the system searches for alter-
native paths which contain valid linguistic re-
alizations of prescriptions consistent with the
knowledge base. If such a path exists, a new
concept edge with a better score is added to
the word graph, resulting in a higher plausi-
bility for this reading. The concept edge can
be used for rescoring the word graph to obtain
a new best path. A preliminary evaluation led
to encouraging results: in nearly half of the
cases where the word graph contained the cor-
rect variant, the correction was successful.
1 Introduction
Automatic speech recognition (ASR) is widely used
in the domain of medical reporting. Users appreciate
the fact that the records can be accessed immediately
after their creation and that speech recognition pro-
vides a hands-free input mode, which is important as
physicians often simultaneously handle documents
such as notes and X-rays (Alapetite et al, 2009).
A drawback of using ASR is the fact that speech-
recognition errors have to be corrected manually by
medical experts before the resulting texts can be
used for electronic patient records, quality control
and billing purposes. This manual post-processing is
time-consuming, which slows down hospital work-
flows.
A number of recognition errors could be avoided
by incorporating explicit domain knowledge. We
consider prescriptions of medications a good start-
ing point as they are common and frequent in the
various medical fields. Furthermore, they contain
trade names and dosages, i.e. proper names and dig-
its, which are frequently misrecognized by ASR in
all domains.
For our approach, we have extracted and adapted
information about medications from the Unified
Medical Language System (UMLS) (Lindberg et al,
1993). This data contains information about trade
names, active substances, strengths and dosages and
can easily be modified, e.g. when new medications
are released.
In the first step, we assessed the potential for im-
provement by analyzing a sample corpus of medical
reports. It turned out that in 4383 dictated reports
which were processed by a speech-recognition sys-
tem, the word-error rate for medications was about
40%, which is slightly higher than the the average
word-error rate of the reports. Examining a sample
22
of word graphs for the reports, we realized that in
about 30% of these errors, the correct string of words
was contained in the word graph, but not ranked as
the best path.
In the following sections, we will first give
an overview of previous approaches to detecting
speech-recognition errors and semantic rescoring of
word-graph hypotheses. Then, we will describe
how we have adapted information about medications
from the UMLS to enhance the word graph with
concept nodes representing domain-specific infor-
mation. Finally, we will illustrate the potential for
improving the speech-recognition result by means
of an evaluation of word graphs for medical reports
which were processed by our system.
2 Extraction of Medication Information,
Error Handling and Semantic Rescoring
(Gold et al, 2008) gives an overview on extract-
ing structured medication information from clinical
narratives. Extracted medication information may
serve as a base for quality control, pharmaceutical
research and the automatic creation of Electronic
Health Records (EHR) from clinical narratives. The
i2b2 Shared Task 2009 focussed on medication ex-
traction, e.g. (Patrick and Li, 2009; Halgrim et al,
2010). These approaches work on written narrative
texts from clinical settings, which may have been
typed by physicians, transcribed by medical tran-
scriptionists or recognized by ASR and corrected by
medical transcriptionists.
In contrast, our approach takes as input word
graphs produced by an ASR system from dictated
texts and aims at minimizing the post-processing re-
quired by human experts.
Speech-recognition systems turn acoustic input
into word graphs, which are directed acyclic graphs
representing the recognized spoken forms and their
confidence scores (Oerder and Ney, 1993). In most
speech-recognition systems, meaning is implicitly
represented in the language model (LM), indicat-
ing the plausibility of sequences of words in terms
of n-grams. It has often been stated that the intro-
duction of an explicit representation of the utterance
meaning will improve recognition results. Naturally,
this works best in limited domains: the larger an
application domain, the more difficult it is to build
an optimal knowledge representation for all possi-
ble user utterances. Limited domains seem to be
more rewarding with regard to coverage and perfor-
mance. Consequently, combining speech recogni-
tion and speech understanding has so far mostly re-
sulted in applications in the field of dialogue systems
where knowledge about the domain is represented in
terms of the underlying database, e.g. (Seneff and
Polifroni, 2000).
Several approaches have investigated the poten-
tial of improving the mapping between the user ut-
terance and the underlying database by constructing
a representation of the utterance meaning. Mean-
ing analysis is either a separate post-processing step
or an integral part of the recognition process. In
some approaches, the recognition result is analyzed
with regards to content to support the dialogue man-
ager in dealing with inconsistencies (Macherey et
al., 2003). As far as dictated input is concerned,
which is not controlled by a dialogue manager, (Voll,
2006) developed a post-ASR error-detection mech-
anism for radiology reports. The hybrid approach
uses statistical as well as rule-based methods. The
knowledge source UMLS is employed for measur-
ing the semantic distance between concepts and for
assessing the coherence of the recognition result.
In other approaches, the analysis of meaning
is integrated into the recognition process. Se-
mantic confidence measurement annotates recogni-
tion hypotheses with additional information about
their assumed plausibility based on semantic scores
(Zhang and Rudnicky, 2001; Sarikaya et al, 2003).
(Gurevych and Porzel, 2003; Gurevych et al, 2003)
present a rescoring approach where the hypothe-
ses in the word graph are reordered according to
semantic information. Usually, conceptual parsers
are employed which construct a parse tree of con-
cepts representing the input text for mapping be-
tween the recognition result and the underlying rep-
resentation. Semantic language modeling (Wanget
al., 2004; Buehler et al, 2005) enhances the lan-
guage model to incorporate sequences of concepts
which are considered coherent and typical for a spe-
cific context. In these approaches, the representa-
tions of the underlying knowledge are created spe-
cially for the applications or are derived from a text
corpus.
In our approach, we aim at developing a prototype
23
for integrating available knowledge sources into the
analysis of the word graph during the recognition
process. We have decided not to integrate the com-
ponent directly into the ASR system but to introduce
a separate post-processing step for the recognition of
information about medications with the word graphs
as interface. This makes it easier to update the med-
ication knowledge base, e.g. if new medications are
released. Furthermore, it is not necessary to retrain
the ASR system language model for each new ver-
sion of the medication knowledge base.
3 Knowledge Base and Text Corpus
For our approach, we prepared a knowledge base
concerning medications and dosages, and we used
a corpus of medical reports, dictated by physicians
in hospitals. The ASR result and a manual transcrip-
tion is available for each report. For a subset of the
corpus, word graphs could be obtained. By aligning
the recognition result with the manual transcriptions,
error regions can be extracted.
3.1 Knowledge Base
As it is our aim to find correct dosages of med-
ications in the word graph, we built a domain-
specific knowledge base which contains medica-
tions and strengths as they occur in prescriptions.
In our sample of medical reports, about 1/3 of the
medications occurred as active ingredients while the
rest were trade names. Therefore, both had to be
covered in our knowledge base which is based on
RxNorm (Liu et al, 2005). RxNorm is a standard-
ized nomenclature for clinical drugs and drug de-
livery devices and part of UMLS, ensuring a broad
coverage of trade names and active ingredients. Of
several available versions of RxNorm, the semantic
branded drug form is the most suitable one for our
purposes as it contains pharmaceutical ingredients,
strengths, and trade names. For example, the trade
name Synthroid R? is listed as follows:
Thyroxine 0.025 MG Oral Tablet [Synthroid R?]
Thyroxine is the active ingredient with the dosage
value 0.025 and the dosage unit milligrams. The
dosage unit form is oral tablet.
We used a RxNorm version with 1,508 active sub-
stances and 7,688 trade names (11,263 trade names
counting the different dosages). The active ingre-
dients in RxNorm are associated with Anatomical
Therapeutic Chemical (ATC) Codes.
3.2 Sample Corpus
The corpus is a random sample of 924 clinical re-
ports which were dictated by physicians from var-
ious specialties and hospitals. The dications were
processed by an ASR system and transcribed by hu-
man experts. Word graphs marked with the best path
(indicating the highest acoustic and language-model
scores) represent the recognition result. Tradenames
are part of the recognition lexicon, but they are fre-
quently misrecognized.
Of the 9196 medications (i.e. trade names and
active substances) in RxNorm, only 330 (3.6%) ap-
peared in the sample corpus.
We searched the corpus for recognition errors
concerning trade names, active ingredients and their
dosages by comparing the manual transcriptions to
the best paths in the word graphs, and a list of the
mismatches (i.e. recognition errors) and their fre-
quencies was compiled. It turned out that 39.3% of
all trade names and active ingredients were recog-
nized incorrectly. The average ASR word-error rate
of the reports was 38.1%. Aproximately 1-2% of the
trade names were not covered by RxNorm.
4 Approach
Our approach consists of a generation mechanism
which anticipates possible spoken forms for the
content of the knowledge base. The word graphs
are searched for trade names or active substances
and, subsequently, matching dosages. New concept
edges are inserted if valid prescriptions are found in
the word graph.
4.1 Detecting Medications in the Word Graph
The (multi-edge) word graphs are scanned, and the
words associated with each edge are compared to the
medications in the knowledge base. Figure 1 shows
a word graph consisting of hypotheses generated by
ASR, which is the input to our system. The dashed
edges indicate the best path, while dotted lines are
hypotheses which are not on the best path.
24
Figure 1: Sample word graph fragment
In case a match, i.e. a trade name or an active sub-
stance, is found, all edges succeeding the medica-
tion edge are searched for dosage values and dosage
units. So far, we only examine the context to the
right-hand side; in the data, we did not encounter
any medications where the dosage occurred before
the trade name or active substance. The following
kinds of fillers between the trade name or active sub-
stance and the dosage are allowed: ?to? and ?of?
as well as non-utterances such as hesitation, noise
and silence; in the corpus, we did not encounter any
other fillers.
4.2 Generation of Spoken Forms and Mapping
The medication found in the word graph is looked up
in RxNorm, and all possible spoken forms of valid
dosage values and dosage units for this medication
are generated. Spoken forms for the medication
names consist of the trade names and the active sub-
stances. Variation in the pronunciation of the trade
names or active substances is handled by the ASR
recognition lexicon. For generating spoken forms of
the dosage values, finite-state tools were used. For
dosage units, we wrote a small grammar. Looking
at two examples, the medication Synthroid R? and
Colace R? (the latter appears in the word graphs in
Figure 2 and Figure 1), the spoken forms shown in
Table 1 are generated. Each box contains the al-
ternative spoken variants. Synthroid R? contains the
active substance Thyroxine and Colace R? contains
the active substance Docusate; users may either re-
fer to the trade name or the active substance, so both
possibilities are generated for each medication and
dosage. RxNorm does not contain the dosage unit
?mcg? (micrograms), which occurred in the reports.
Therefore, microgram dosage values were converted
to milligrams. Since both ?miligram(s)? and ?mi-
crogram(s)? may occur for Synthroid R?, dosage val-
ues for both dosage units are generated. Although
strictly, ?twenty five? and ?twenty-five? are identical
spoken forms, both versions may appear in the word
graph and thus are provided by our system.
Sometimes, a medication may contain several ac-
tive substances, e.g. Hyzaar R?, a medication against
high blood pressure:
Hydrochlorothiazide 12.5 MG / Losartan 50 MG
Oral Tablet [Hyzaar]
25
trade name/ dosage value dosage unit
active
substance
?Synthroid? ?zero point zero two five? ?milligram?
?Thyroxine? ?zero point O two five? ?milligrams?
?O point zero two five?
?O point O two five?
?point zero two five?
?point O two five?
?twenty five? ?microgram?
?twenty-five? ?micrograms?
?two five?
?Colace? ?one hundred? ?miligram?
?Docusate? ?a hundred? ?miligrams?
?hundred?
Table 1: Generated spoken forms found in the word graph
In these cases, the generation of possible spoken
forms also includes different permutations of sub-
stances, as well as a spoken forms containing the
dosage unit either only at the end or after each value
if the dosage unit is identical.
4.3 Inserting Concept Edges
The sequences of words which constitute the word
graph are compared to the spoken forms generated
for the RxNorm knowledge base. The active sub-
stances or trade names serve as a starting point: in
case a trade name is found in the word graph, the
spoken forms for dosages of all active substances are
generated in all permutations. If an active substance
is found in the word graph, only the spoken forms
for the substance dosage are searched in the word
graph.
A new concept edge is inserted into the word
graph for each path matching one of the generated
spoken forms of the medications data base. The in-
serted concept edges span from the first matching
node to the last matching node on the path. Fig-
ure 2 shows the word graph from Figure 1 with an in-
serted concept edge (in bold). For each inserted con-
cept edge, new concept-edge attributes are assigned
containing the IDs of the original edges as children,
their added scores plus an additional concept score
and the sequence of words. Since no large-scale ex-
periments have yet been carried out, so far the con-
cept score which is added to the individual scores of
the children is an arbitrary number which improves
the score of the medication subpath in constrast to
paths which do not contain valid medication infor-
mation. If several competing medication paths are
found, a concept edge is inserted for each path, and
the concept edges can be ranked according to their
acoustic and language-model scores.
5 Evaluation
In the first step, we examined a report sample in or-
der to determine if there are cases where a valid pre-
scription is recognised although the physician did
not mention a prescription. We did not encounter
this phenomenon in our report corpus.
We then applied our method to a sample of 924
word graphs. In this sample,
? 481 valid dosages could be found, although
? only 325 of these were on the best path.
With our approach, for the 156 prescriptions
(32%) which were not on the best path, alternatives
could be reconstructed from the word graph. Based
on the inserted concept edges, the best path can be
rescored.
In order to measure recall, i.e. how many of all
existing prescriptions in the reports can be detected
with our knowledge base, we manually checked
a sample of 132 reports (containing manual tran-
scriptions and ASR results). In this sample, 85 er-
rors concerning medications and/or prescriptions oc-
curred. For 19 of the 85 errors, the correct result was
contained in the word graph. For 8 errors, it could
be reconstructed. So about 9% of the errors concern-
ing medications can be corrected in our sample. For
the cases where the prescription could not be recon-
structed although it was contained in the word graph,
an analysis of the errors is shown in Table 2.
Since new medications are constantly being re-
leased, and trade names change frequently, mis-
matches may be due to the fact that our version of
RxNorm was from a more recent point in time than
the report corpus. We assume that under real-world
conditions, both RxNorm and the medications pre-
scribed by physicians reflect the current situation.
Some problems concerning medication names
and dosage units were caused by missing spoken
forms containing abbreviations, e.g. of dosage units
(mg vs. mg/ml) or names (Lantus vs. Lantus in-
sulin). Here, the coverage needs to be improved.
26
Figure 2: Sample word graph fragment with inserted concept node (left)
Table 2: Error types found in manual evaluation
type of error # example
Word Graph RxNorm
differences in medication names 3 Cardizem CD 120 mg Cardizem 120 mg
between the knowledge base and the word graph
differences in dosage values 4 Tapazole 60 mg Tapazole 10 mg
between the knowledge base and the word graph
differences in dosage units 4 Epogen 20000 units Epogen 20000 ml
between the knowledge base and the word graph
There are also cases where two medications appear
in the word graph, and both had the valid prescrip-
tion strength, therefore the system was not able to
determine the correct medication.
6 Conclusion
In this paper, we present an attempt to reduce
the number of speech-recognition errors concern-
ing prescriptions of medications based on a domain-
specific knowledge base. Our approach uses word
graphs as input and creates new versions of the word
graph with inserted concept edges if more plausi-
ble prescriptions are found. The concept edges can
be used for rescoring the best path. An evaluation
showed that 32% of prescriptions found in the word
graphs were not on the best path but could be re-
constructed. The manual evaluation of 132 reports
shows that our method covers 42% of the prescrip-
tions which are actually spoken during the dictation.
At present, we have only investigated the reduc-
tion of medication misrecognitions in our evalua-
tion. In a larger evaluation, we will determine the ac-
tual impact of our method on the word-error rate of
medical reports. Furthermore, we are working on in-
tegrating additional available knowledge sources so
that the plausibility of prescriptions can also be as-
27
sessed from a broader medical point of view, e.g. in
case two subsequent prescriptions are encountered
in the word graph which are incompatible due to
drug interactions. As a next step, the system can
be extended to compare the prescriptions with the
patient record, e.g. if a patient has medication al-
lergies. So far, our simple solution integrating only
available, constantly updated knowledge about med-
ications has already turned out to be a good starting
point for rescoring word graphs based on domain
knowledge.
Acknowledgments
The work presented here has been carried out in
the context of the Austrian KNet competence net-
work COAST. We gratefully acknowledge funding
by the Austrian Federal Ministry of Economics and
Labour, and ZIT Zentrum fuer Innovation und Tech-
nologie, Vienna. The Austrian Research Institute
for Artificial Intelligence is supported by the Aus-
trian Federal Ministry for Transport, Innovation, and
Technology and by the Austrian Federal Ministry
for Science and Research. The authors would like
to thank the anonymous reviewers for their helpful
comments.
References
A. Alapetite, A., H.B. Andersen, H.B. and M. Hertzumb.
Acceptance of speech recognition by physicians: A
survey of expectations, experiences, and social in-
fluence. International Journal of Human-Computer
Studies 67(1) (2009) 36?49
D. Bu?hler, W. Minker and A. Elciyanti. Us-
ing language modelling to integrate speech
recognition with a flat semantic analysis. In:
6th SIGdial Workshop on Discourse and Di-
alogue, Lisbon, Portugal (September 2005)
http://www.sigdial.org/workshops/workshop6/proceed-
ings/pdf/86-paper.pdf.
S. Gold, N. Elhadad, X. Zhu, J.J. Cimino, G. Hripcsak.
Extracting Structured Medication Event Information
from Discharge Summaries. In: Proceedings of the
AMIA 2008 Symposium.
I. Gurevych and R. Porzel. Using knowledge-based
scores for identifying best speech recognition hy-
pothesis. In: Proceedings of ISCA Tutorial and
Research Workshop on Error Handling in Spoken
Dialogue Systems, Chateau-d?Oex-Vaud, Switzer-
land (2003) 77?81 http://proffs.tk.informatik.tu-
darmstadt.de/TK/abstracts.php3?lang=en&bibtex=1&-
paperID=431.
R. Porzel, I. Gurevych and C. Mu?ller. Ontology-based
contextual coherence scoring. Technical report, Euro-
pean Media Laboratory, Heidelberg, Germany (2003)
http://citeseer.ist.psu.edu/649012.html.
S.R. Halgrim, F. Xia, I. Solti, E. Cadag and O. Uzuner.
Statistical Extraction of Medication Information from
Clinical Records. In: Proc. of AMIA Summit on Trans-
lational Bioinformatics, San Francisco, CA, March
10-12, 2010.
D.A. Lindberg, B.L. Humphreys and A.T. McCray. The
unified medical language system. Methods of In-
formation in Medicine 32(4) (August 1993) 281?291
http://www.nlm.nih.gov/research/umls/.
S. Liu, W. Ma, R. Moore, V. Ganesan and S. Nelson.
Rxnorm: Prescription for electronic drug information
exchange. IT Professional 7(5) (September/October
2005) 17?23
K. Macherey, O. Bender and H. Ney. Multi-level er-
ror handling for tree based dialogue course man-
agement. In: Proceedings of ISCA Tutorial and
Research Workshop on Error Handling in Spo-
ken Dialogue Systems, Chateau-d?Oex-Vaud, Switzer-
land (2003) 123?128, http://www-i6.informatik.rwth-
aachen.de/?bender/papers/isca tutorial 2003.pdf.
M. Oerder and H. Ney. Word graphs: An efficient inter-
face between continuous speech recognition and lan-
guage understanding. In: Proc. IEEE ICASSP?93. Vol-
ume 2. 119?122.
J. Patrick and M. Li. A Cascade Approach to Extracting
Medication Events. In: Proc. Australasian Language
Technology Workshop (ALTA) 2009.
R. Sarikaya, Y. Gao and M. Picheny. Word level confi-
dence measurement using semantic features. In: Proc.
of IEEE ICASSP2003. Volume 1. (April 2003) 604?
607.
S. Seneff and J. Polifroni. Dialogue Management in the
MERCURY Flight Reservation System. In: Satel-
lite Dialogue Workshop, ANLP-NAACL, Seattle (April
2000).
K.D. Voll. A Methodology of Error Detection:
Improving Speech Recognition in Radiology.
PhD thesis, Simon Fraser University (2006)
http://ir.lib.sfu.ca/handle/1892/2734.
K. Wang, Y.Y. Wang and A. Acero. Use and acquisition
of semantic language model. In: HLT-NAACL. (2004)
http://www.aclweb.org/anthology-new/N/N04/N04-
3011.pdf.
R. Zhang and A.I. Rudnicky. Word level confi-
dence annotation using combinations of fea-
tures. In: Proceedings of Eurospeech. (2001)
http://www.speech.cs.cmu.edu/Communicator/papers/-
RecoConf2001.pdf.
28
