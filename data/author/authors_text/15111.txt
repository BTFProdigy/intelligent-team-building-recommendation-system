A Language Independent Method for Question Classification
Thamar Solorio1, Manuel Pe?rez-Coutin?o1, Manuel Montes-y-Go?mez1,2
Luis Villasen?or-Pineda1 and Aurelio Lo?pez-Lo?pez1
1Language Technologies Group, Computer Science Department
National Institute of Astrophysics, Optics and Electronics
72840 Tonantzintla, Puebla,
Mexico
2Departamento de Sistemas Informa?ticos y Computacio?n
Universidad Polite?cnica de Valencia
Espan?a
{thamy,mapco,mmontesg,villasen,allopez}@inaoep.mx
Abstract
Previous works on question classification are
based on complex natural language process-
ing techniques: named entity extractors,
parsers, chunkers, etc. While these ap-
proaches have proven to be effective they
have the disadvantage of being targeted to a
particular language. We present here a sim-
ple approach that exploits lexical features
and the Internet to train a classifier, namely
a Support Vector Machine. The main fea-
ture of this method is that it can be applied
to different languages without requiring ma-
jor modifications. Experimental results of
this method on English, Italian and Span-
ish show that this approach can be a prac-
tical tool for question answering systems,
reaching a classification accuracy as high as
88.92%.
1 Introduction
Open-domain Question Answering (QA) sys-
tems are concerned with the problem of trying
to answer questions from users posed in nat-
ural language. What makes these systems a
very complex and interesting research area is
that the answers they retrieve must be concise,
as opposed to traditional search engines that in
response to a user query retrieve a list of docu-
ments believed to contain the answer. More-
over, current evaluation environments of QA
systems, such as TREC QA track (Voorhees,
2001) and CLEF (Peters et al, 2003), restrict
the size of the answers to a maximum of 50
bytes. Given the complexity involved in this
problem, traditional approaches to QA take a
divide-and-conquer strategy, where the prob-
lem is divided into several less complex subtasks
that combined lead to the resolution of the ques-
tions. An important subtask of a QA system
is question analysis, since it can provide useful
clues for identifying potential answers in a large
collection of texts. For instance, Question Clas-
sification is concerned with assigning semantic
classes to questions. This semantic classifica-
tion can be used to reduce the search space of
possible answers, i.e. if we can determine that
the question Who is the Italian Prime Minis-
ter? belongs to the semantic category PER-
SON, then we only need to look for instances of
type PERSON as possible answers. Clearly, the
advantage of such classification relies on hav-
ing the ability of extracting from the documents
such instances. In other words, a good question
classification module may be useless if we lack
an accurate named entity extractor for the doc-
ument collection.
Results of the error analysis of an open-
domain QA system showed that 36.4% of the
errors were generated by the question classifi-
cation module (Moldovan et al, 2003). Thus
it is not surprising that an increasing interest
has arisen aimed at developing accurate ques-
tion classifiers (Zhang and Lee, 2003; Li and
Roth, 2002; Suzuki et al, 2003). However, most
of these approaches are targeted to the English
language. Besides, the machine learning algo-
rithms used are trained on features extracted by
natural language processing tools that are lan-
guage dependent, and for some languages these
tools are not available. This implies that if we
want to reproduce the results of these methods
in a different language we need first to solve the
problem of making available the appropriate an-
alyzers in the given language.
We present here a flexible method for ques-
tion classification. We claim that the method
is language-independent since no complex nat-
ural language processing tools are needed; we
use plain lexical features that can be extracted
automatically from the questions. A machine
learning algorithm that has proven to perform
well over high dimensional data, is trained on
prefixes of words and on additional attribute in-
formation gathered automatically from the In-
ternet. The method was evaluated experimen-
tally, achieving high accuracy on questions in
three different languages: English, Italian and
Spanish.
The next section briefly summarizes some of
the previous approaches for question classifica-
tion. Section 3 presents the learning scenario
of this work, together with a brief introduction
to Support Vector Machines (SVM). Section 4
shows our experimental results and we conclude
with a discussion of this work and ideas for fu-
ture research in Section 5.
2 Related Work
Most approaches to question classification are
based on handcrafted rules (Voorhees, 2001).
It is not until recently that machine learn-
ing techniques are being used to tackle the
problem of question classification. In (Zhang
and Lee, 2003) they present a new method
for question classification using Support Vector
Machines. They compared accuracy of SVM
against Nearest Neighbors, Naive Bayes, De-
cision Trees and Sparse Network of Winnows
(SNoW), with SVM producing the best results.
In their work, Zhang and Sun Lee improve accu-
racy by introducing a tree kernel function that
allows to represent the syntactic structure of
questions. Their experimental results show that
SVM using this tree kernel function achieves an
accuracy of 90%, however, a parser is needed in
order to acquire the syntactic information.
Li and Roth reported a hierarchical approach
for question classification based on the SNoW
learning architecture (Li and Roth, 2002). This
hierarchical classifier discriminates among 5
coarse classes, which are then refined into 50
more specific classes. The learners are trained
using lexical and syntactic features such as pos
tags, chunks and head chunks together with two
semantic features: named entities and seman-
tically related words. They reported question
classification accuracy of 98.80% for a coarse
classification, using 5,500 instances for training.
A different approach, used for Japanese ques-
tion classification, is that of Suzuki et al
(Suzuki et al, 2003). They used SVM whith
a new kernel function, called Hierarchical Di-
rected Acyclic Graph, which allows the use of
structured data. They experimented with 68
question types and compared performance of us-
ing bag-of-words against using more elaborated
combinations of attributes, namely named en-
tities and semantic information. Their best re-
sults, an accuracy of 94.8% at the first level of
the hierarchy, were obtained when using SVM
trained on bag-of-words together with named
entities and semantic information.
The idea of using the Internet in a QA sys-
tem is not new. What is new, however, is that
we are using the Internet to obtain values for
features in our question classification process,
as opposed to previous approaches where the
redundancy of information available on the In-
ternet has been used in the answer extraction
process (Brill et al, 2002; Lin et al, 2002; Katz
et al, 2003).
3 Learning Question Classifiers
Question classification is very similar to text
classification. One thing they have in common
is that in both cases we need to assign a class,
from a finite set of possible classes, to a natural
language text. Another similarity is attribute
information; what has been used as attributes
for text classification can also be extracted and
used in question classification. Finally, in both
cases we have high dimensional attributes: if we
want to use the bag-of-words approach, we will
face the problem of having very large attribute
sets.
An important difference is that question clas-
sification introduces the problem of dealing with
short sentences, compared with text documents,
and thus we have less information available on
each question instance. This is the reason why
question classification approaches are trying to
use other information (e.g. chunks and named
entities) besides the words within the questions.
However, the main disadvantage of relying on
semantic analyzers, named entity taggers and
the like, is that for some languages these tools
are not yet well developed. Plus, most of them
are very sensitive to changes in the domain of
the corpus; and even if these tools are accu-
rate, in some cases acquiring one for a partic-
ular language may be a difficult task. This is
our prime motivation for searching for differ-
ent, more easier to gather, information to solve
the question classification problem. Our learn-
ing scenario considers as attribute information
prefixes of words in combination with attributes
whose values are obtained from the Internet.
These Internet based attributes are targeted to
extract evidence of the possible semantic class
of the question.
The next subsection will explain how the In-
ternet is used to extract attributes for our ques-
tion classification problem. In subsection 3.2
we present a brief description of Support Vec-
tor Machines, the learning algorithm used on
our experiments.
3.1 Using Internet
As Kilgarriff and Grefenstette wrote, the In-
ternet is a fabulous linguists? playground (Kil-
garriff and Grefenstette, 2003). It has become
the greatest information source available world-
wide, and although English is the dominant
language represented on the Internet it is very
likely that one can find information in almost
any desired language. Considering this, and the
fact that the texts are written in natural lan-
guage, we believe that new methods that take
advantage of this large corpus must be devised.
In this work we propose using the Internet in or-
der to acquire information that can be used as
attributes in our classification problem. This at-
tribute information can be extracted automat-
ically from the web and the goal is to provide
an estimate about the possible semantic class of
the question.
The procedure for gathering this information
from the web is as follows: we use a set of heuris-
tics to extract from the question a word w, or
set of words, that will complement the queries
submitted for the search. We then go to a search
engine, in this case Google, and submit queries
using the word w in combination with all the
possible semantic classes for our purpose. For
instance, for the question Who is the President
of the French Republic? we extract the word
President using our heuristics, and run 5 queries
in the search engine, one for each possible class.
These queries take the following form:
? ?President is a person?
? ?President is a place?
? ?President is a date?
? ?President is a measure?
? ?President is an organization?
We count the number of results returned by
Google for each query and normalize them by
their sum. The resultant numbers are the val-
ues for the attributes used by the learning algo-
rithm. As can be seen, it is a very straightfor-
ward approach, but as the experimental results
will show, this information gathered from the
Internet is quite useful. In Table 1 we present
the figures obtained from Google for the ques-
tion presented above, column Results show the
number of hits returned by the search engine
and in column Normalized we present the num-
ber of hits normalized by the total of all results
returned for the different queries.
An additional advantage of using the Internet
is that by approximating the values of attributes
in this way, we take into account words or en-
tities belonging to more than one class (poly-
semy).
Now that we have introduced the use of the
Internet in this work, we continue describing the
set of heuristics that we use in order to perform
the web search.
3.1.1 Heuristics
We begin by eliminating from the questions
all words that appear in our stop list. This
stop list contains the usual items: articles,
prepositions and conjunctions plus all the
interrogative adverbs and all lexical forms
of the verb ?to be?. The remaining words
are sent to the search engine in combina-
tion with the possible semantic classes, as
described above. If no results are returned
for any of the semantic classes we then start
eliminating words from right to left until the
search engine returns results for at least one
of the semantic categories. As an example
consider the question posed previously: Who
is the President of the French Republic? we
eliminate the words from the stop list and
then formulate queries for the remaining
words. These queries are of the following form:
?President French Republic is a si? where s ?
{Person,Organization, P lace,Date,Measure}.
The search engine did not return any results
for this query, so we start eliminating words
from right to left. The query is now like this:
?President French is a si? and given that
again we have no results returned we finally
formulate the last possible query: ?President is
a si? which returns results for all the semantic
classes except for Date.
Being heuristics, we are aware that in some
cases they do not work well. Nevertheless,
for the vast majority of the cases they pre-
sented surprisingly good results, in the three
Query Results Normalized
?President is a person? 259 0.8662
?President is a place? 9 0.0301
?President is an organization? 11 0.0368
?President is a measure? 20 0.0669
?President is a date? 0 0
Table 1: Example of using the Internet to extract features for question classification
Class Number of Instances
Person 91
Organization 41
Measure 103
Date 64
Object 12
Other 54
Place 85
Table 2: Distribution of semantic classes for the
DISEQuA corpus
languages, as shown in the experimental eval-
uation.
3.2 Support Vector Machines
Given that Support Vector Machines have
proven to perform well over high dimensionality
data they have been successfully used in many
natural language related applications, such as
text classification (Joachims, 1999; Joachims,
2002; Tong and Koller, 2001) and named entity
recognition (Mitsumori et al, 2004; Solorio and
Lo?pez, 2004). This technique uses geometrical
properties in order to compute the hyperplane
that best separates a set of training examples
(Stitson et al, 1996). When the input space
is not linearly separable SVM can map, by us-
ing a kernel function, the original input space
to a high-dimensional feature space where the
optimal separable hyperplane can be easily cal-
culated. This is a very powerful feature, be-
cause it allows SVM to overcome the limitations
of linear boundaries. They also can avoid the
over-fitting problems of neural networks as they
are based on the structural risk minimization
principle. The foundations of these machines
were developed by Vapnik, for more informa-
tion about this algorithm we refer the reader to
(Vapnik, 1995; Scho?lkopf and Smola, 2002).
4 Experimental Evaluation
4.1 Data sets
The data set used in this work consists of the
questions provided in the DISEQuA Corpus
(Magnini et al, 2003). Such corpus was made
up of simple, mostly short, straightforward and
factual queries that sound naturally sponta-
neous, and arisen from a real desire to know
something about a particular event or situation.
The DISEQuA Corpus contains 450 questions,
each one formulated in four languages: Dutch,
English, Italian and Spanish. The questions
are classified into seven categories: Person, Or-
ganization, Measure, Date, Object, Other and
Place. The experiments performed in this work
used the English, Italian and Spanish versions
of these questions.
4.2 Experiments
In the experiments performed in this work we
used the evaluation technique 10-fold cross-
validation which consists of randomly dividing
the data into 10 equally-sized subgroups and
performing 10 different experiments. We sep-
arated nine groups together with their original
classes as the training set, the remaining group
was considered the test set. Each experiment
consists of ten runs of the procedure described
above, and the overall average are the results
reported here.
In our experiments we used the WEKA imple-
mentation of SVM (Witten and Frank, 1999).
In this setting multi-class problems are solved
using pairwise classification. The optimization
algorithm used for training the support vec-
tor classifier is an implementation of Platt?s se-
quential minimal optimization algorithm (Platt,
1999). The kernel function used for mapping
the input space was a polynomial of exponent
one.
The most common approach to question clas-
sification is bag-of-words, so we decided to com-
pare results of using bag-of-words against using
just prefixes of the words in the questions. In
Language Words Prefix-5 Prefix-4 Internet
ENGLISH 81.77% 81.32% 80.21% 67.77%
ITALIAN 88.03% 87.59% 88.70% 60.79%
SPANISH 79.90% 81.45% 76.97% 68.86%
Table 3: Experimental results of accuracy when training SVM with words, prefixes and Internet-
based attributes
order to choose an appropriate prefix size we
compute the average length of the words in the
three languages used in this work. For English
the average length of words is 4.62, for Italian
is 4.8 while for Spanish the average length is
4.75. So we decided to experiment with pre-
fixes of size 4 and 5. In Table 3 we can see a
comparison of classification accuracy of train-
ing SVM using all the words in the questions,
using prefixes of size 4 and 5 and using only
the Internet-based attributes. As we can see for
English the best results were obtained when us-
ing words as attributes, although the difference
between using just prefixes and using words is
not so large. For Spanish however, the best re-
sults were achieved when using prefixes of size
5. This can be due to the fact that some of
the interrogative words, that by themselves can
define the semantic class of questions in this
language, such as Cua?ndo (When) and Cua?nto
(How much) could be considered as the same
prefix of size 4 i.e. Cua?n. But if we consider
prefixes of size 5, then these two words will form
two different prefixes: Cua?nd and Cua?nt, thus
reducing the loss of information, as oppose to
using prefixes of size 4. For Italian language
the best results were obtained from using pre-
fixes of size 4. And for the three languages the
Internet-based attributes had rather low accu-
racies, the lowest being for Italian. When we
analyzed the results computed for Italian, us-
ing our Internet-based attributes, we realized
that in many cases we could not get any re-
sults to the queries. One plausible explanation
for this lack of information, is that the num-
ber of Italian documents available on Internet
is much smaller than for English and Spanish.
Estimates reported in (Kilgarriff and Grefen-
stette, 2003) show that for Italian the web size
in words is 1,845,026,000; while for English and
Spanish the web sizes are 76,598,718,000 and
2,658,631,000 respectively. Thus our method
was not able to extract as much information as
for the other two languages.
4.3 Combining Internet-based
Attributes with Lexical Features
Results presented in the previous subsection
show how by using just lexical information we
can train SVM and achieve high accuracies in
the three languages. But our goal is to discover
the usefulness of using Internet in order to ex-
tract attributes for question classification. We
performed other experiments combining the lex-
ical attributes with the Internet information in
order to discover if we can further improve ac-
curacy. Table 4 show experimental results of
this attribute combination and Figure 1 shows
a graphical representation of these results.
It is interesting to note that for English and
Spanish we did gain accuracy when using the
Internet features in all the cases. In contrast,
for Italian classification accuracy was decreased
when incorporating Internet-based attributes to
words and prefixes of size 5. We believe that this
drop in accuracy for Italian may be due to the
weakly supported information extracted from
the Internet, Table 3 shows that SVM trained
only on the coefficients from the Internet per-
formed worse for Italian. It is not surprising
that adding this rather sparse information to
the attributes in the Italian language did not
produce an advantage in the classifiers perfor-
mance.
5 Conclusions
We have presented here experimental results of
a language independent question classification
method. The method is claimed to be lan-
guage independent since the features used as
attributes in the learning task can be extracted
from the questions in a fully automated manner;
we do not use semantic or syntactic informa-
tion because otherwise we will be restricted to
work on languages for which we do have parsers
that can extract this information. We believe
that this method can be successfully applied
to other languages, such as Romanian, French,
Portuguese and Catalan, that share the mor-
phologic characteristics of the three languages
Language Words+Internet Prefix-5 + Internet Prefix-4 + Internet
ENGLISH 82.88% 82.66% 83.55%
ITALIAN 87.34% 86.93% 88.92%
SPANISH 83.43% 84.09% 81.45%
Table 4: Experimental results combining Internet-valued attributes with words and prefixes
Inte
rne
t
Wo
rds
Pre
fix-
5
Pre
fix-
4
Wo
rds
+In
ter
ne
t
Pre
fix-
5+
Inte
rne
t
Pre
fix-
4+
Inte
rne
t
60
65
70
75
80
85
90
Spanish
English
Italian
Figure 1: Graphical comparison of question classification accuracies
tested here.
Comparing our results with those of previous
works we can say that our method is promis-
ing. For instance Zhang and Sun Lee (Zhang
and Lee, 2003) reported an accuracy of 90% for
English questions, while Li and Roth (Li and
Roth, 2002) achieved 98.8% accuracy. How-
ever, they used a training set of 5,500 questions
and a test set of 500 questions, while in our ex-
periments we used for training 405 for each 45
test questions (10-fold-cross-validation). When
Zhang and Sun Lee used only 1,000 questions
for training they achieved an accuracy of 80.2%.
It is well known that machine learning algo-
rithms perform better when a bigger training
set is available, so it is expected that experi-
ments of our method with a larger training set
will provide improved results.
As future work we plan to investigate active
learning with SVM for this problem. Given that
manually labelling questions is a very time con-
suming task, active learning can provide a faster
approach to build accurate question classifiers.
Instead of randomly selecting question instances
to label manually and then provide them to the
learner, the learner can analyze the unlabeled
instances and select for labelling the instances
that seem more relevant to the task.
Another interesting line for future work is ex-
ploring the advantage of using mixed languages
corpora lo learn question classification. The
Romance languages, for instance, such as Ital-
ian, French and Spanish have stems in common.
Then it is feasible that questions for several lan-
guages may help to train a classifier for a differ-
ent language. The advantage of this idea will be
the availability of larger corpora for languages
for which a large enough corpus is not available,
counting in favor of languages that are under-
represented on the Internet. We could circum-
vent this lack of presence on the Internet for
some languages by using information available
on other, more well represented, languages.
6 Acknowledgements
We would like to thank CONACyT for par-
tially supporting this work under grants 166934,
166876 and U39957-Y, and Secretar??a de Estado
de Educacio?n y Universidades de Espan?a.
References
E. Brill, S. Dumais, and M. Banko. 2002. An
analysis of the AskMSR question-answering
system. In 2002 Conference on Empirical
Methods in Natural Language Processing.
T. Joachims. 1999. Transductive inference for
text classification using support vector ma-
chines. In Proceedings of the Sixteenth In-
ternational Conference on Machine Learning
(ICML), pages 200?209. Morgan Kaufmann.
T. Joachims. 2002. Learning to Classify Text
using Support Vector Machines: Methods
Theory and Algorithms, volume 668 of The
Kluwer International Series in Engineering
and Computer Science. Kluwer Academic
Publishers.
B. Katz, J. Lin, D. Loreto, W. Hilde-
brandt, M. Bilotti, S. Felshin, A. Fernandes,
G. Marton, and F. Mora. 2003. Integrat-
ing web-based and corpus-based techniques
for question answering. In Twelfth Text RE-
trieval Conference (TREC 2003), Gaithers-
burg, Maryland, November.
A. Kilgarriff and G. Grefenstette. 2003. Intro-
duction to the special issue on the web as cor-
pus. Computational Linguistics, 29(3):333?
347.
X. Li and D. Roth. 2002. Learning question
classifiers. In COLING?02.
J. Lin, A. Fernandes, B. Katz, G. Marton,
and S. Tellex. 2002. Extracting answers
from the web using knowledge annotation and
knowledge mining techniques. In Eleventh
Text REtrieval Conference (TREC 2002),
Gaithersburg, Maryland, November.
B. Magnini, S. Romagnoli, A. Vallin, J. Her-
rera, A. Pen?as, V. Peinado, F. Verdejo, and
M. de Rijke. 2003. Creating the DISEQuA
corpus: a test set for multilingual question
answering. In Carol Peters, editor, Working
Notes for the CLEF 2003 Workshop, Trond-
heim, Norway, August.
T. Mitsumori, S. Fation, M. Murata, K. Doi,
and H. Doi. 2004. Boundary correction of
protein names adapting heuristic rules. In
Alexander Gelbukh, editor, Fifth Interna-
tional Conference on Intelligent Text Process-
ing and Computational Linguistics, CICLing
2004, volume 2945 of Lecture Notes in Com-
puter Science, pages 172?175. Springer.
D. Moldovan, M. Pas?ca, S. Harabagiu, and
M. Surdeanu. 2003. Performance issues and
error analysis in an open-domain question
answering system. ACM Trans. Inf. Syst.,
21(2):133?154.
C. Peters, M. Braschler, J. Gonzalo, and
M. Kluck, editors. 2003. Advances in
Cross-Language Information Retrieval, Third
Workshop of the Cross-Language Evaluation
Forum, CLEF 2002. Rome, Italy, September
19-20, 2002. Revised Papers, volume 2785 of
Lecture Notes in Computer Science. Springer.
J. Platt. 1999. Fast training of support vec-
tor machines using sequential minimal op-
timization. In Advances in Kernel Meth-
ods ?Support Vector Learning, (B. Scho?lkopf,
C.J.C. Burges, A.J. Smola, eds.), pages 185?
208, Cambridge, Massachusetts. MIT Press.
B. Scho?lkopf and A. J. Smola. 2002. Learning
with Kernels: Support Vector Machines, Reg-
ularization, Optimization and Beyond. MIT
Press.
T. Solorio and A. Lo?pez Lo?pez. 2004. Learn-
ing named entity classifiers using support vec-
tor machines. In Alexander Gelbukh, editor,
Fifth International Conference on Intelligent
Text Processing and Computational Linguis-
tics, CICLing 2004, volume 2945 of Lecture
Notes in Computer Science, pages 158?167.
Springer.
M. O. Stitson, J. A. E. Wetson, A. Gammer-
man, V. Vovk, and V. Vapnik. 1996. Theory
of support vector machines. Technical Report
CSD-TR-96-17, Royal Holloway University of
London, England, December.
J. Suzuki, H. Taira, Y. Sasaki, and E. Maeda.
2003. Question classification using HDAG
kernel. In Workshop on Multilingual Summa-
rization and Question Answering 2003, pages
61?68.
S. Tong and D. Koller. 2001. Support vector
machine active learning with applications to
text classification. Journal of Machine Learn-
ing Research, 2:45?66.
V. Vapnik. 1995. The Nature of Statisti-
cal Learning Theory. Number ISBN 0-387-
94559-8. Springer, N.Y.
E. Voorhees. 2001. Overview of the TREC
2001 question answering track. In Proceed-
ings of the 10th Text REtrieval Conference
(TREC01), NIST, pages 157?165, Gaithers-
burg, MD.
I. H. Witten and E. Frank. 1999. Data Mining,
Practical Machine Learning Tools and Tech-
niques with Java Implementations. The Mor-
gan Kaufmann Series in Data Management
Systems. Morgan Kaufmann.
D. Zhang and W. Sun Lee. 2003. Question
classification using support vector machines.
In Proceedings of the 26th Annual Interna-
tional ACM SIGIR Conference on Research
and Development in Information Retrieval,
pages 26?32, Toronto, Canada. ACM Press.
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1228?1237, Dublin, Ireland, August 23-29 2014.
Cross-Topic Authorship Attribution: Will Out-Of-Topic Data Help?
Upendra Sapkota and Thamar Solorio
The University of Alabama at Birmingham
1300 University Boulevard
Birmingham, AL 35294, USA
{upendra,solorio}@cis.uab.edu
Manuel Montes-y-G
?
omez
Instituto Nacional de Astrof??sica
Optica y Electr?onica
Puebla, Mexico
mmontesg@ccc.inaoep.mx
Steven Bethard
The University of Alabama at Birmingham
1300 University Boulevard
Birmingham, AL 35294, USA
bethard@cis.uab.edu
Paolo Rosso
NLE Lab - PRHLT Research Center
Universitat Polit`ecnica de Val`encia
Valencia, Spain
prosso@dsic.upv.es
Abstract
Most previous research on authorship attribution (AA) assumes that the training and test data
are drawn from same distribution. But in real scenarios, this assumption is too strong. The goal
of this study is to improve the prediction results in cross-topic AA (CTAA), where the training
data comes from one topic but the test data comes from another. Our proposed idea is to build
a predictive model for one topic using documents from all other available topics. In addition
to improving the performance of CTAA, we also make a thorough analysis of the sensitivity to
changes in topic of four most commonly used feature types in AA. We empirically illustrate that
our proposed framework is significantly better than the one trained on a single out-of-domain
topic and is as effective, in some cases, as same-topic setting.
1 Introduction
Authorship Attribution is the problem of identifying who, from a number of given candidate authors,
wrote the given piece of text. The authorship attribution task can be viewed as a multi-class single-label
text classification task where each author indicates a class. However, the purpose of AA is to model each
author?s writing style. AA methods have a wide range of applications, including Forensic Linguistics (spam
filtering (de Vel et al., 2001), verifying the authorship of threatening emails), cybercrimes (identifying
authors of malicious code and defending against pedophiles), and plagiarism detection (Stamatatos, 2011).
The AA methods can be useful in applied areas such as law and journalism where the identification
of the true author of a piece of text (such as a ransom note) may be able to save lives or help prosecute
offenders. One of the outstanding problems in AA studies is the unrealistic assumption that the samples of
both known and unknown authorship are drawn from the same distribution. This assumption considerably
simplifies the AA task but also limits the practical usability of the methods. In practical scenarios usually
the documents under investigation are from a different domain than that of the training documents. We
feel the need to advance the way AA methods are designed so that the bridge between domains will
be minimized to obtain the optimum performance. Therefore, we try to improve the performance of
cross-topic AA (CTAA), one of the dimensions of cross-domain AA (CDAA) where training and test data
come from different topics.
In this paper, we focus on one of the outstanding research questions on AA: Can we reliably predict
the author of a document written in one topic with a predictive model developed using documents from
other topics? We hypothesize that the addition of training data even if it comes from a topic different
than that of the test data improves cross-topic AA performance. To test the hypothesis, we compare
the performance of our proposed model trained on documents from all available out-of-topic data with
two models, one trained on single out-of-topic data and another trained on the same topic (intra-topic)
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1228
data. We also compare the performance of using four widely used features in AA to demonstrate their
discriminative power in intra-topic and cross-topic AA. The contributions of this study are as follows:
? We propose a new method to identify the author of a document on a topic using a predictive model
trained on examples from different topics. The successful results attained indicate that authors
maintain a consistent style across topics.
? This is the first comprehensive study showing empirically which widely used features in AA are
effective for cross-topic AA. We demonstrate that character n-grams are a strong discriminator among
authors in CTAA and that lexical features are less effective in CTAA than they are for intra-topic AA.
? We empirically illustrate that having the same amount of training documents from multiple topics is
significantly better than having documents from a single topic. It shows that topic variety in training
documents improves the performance of CTAA.
? We also demonstrate that across all genres, adding an extra topic to the training data gives a character
n-gram model a greater boost in performance than to a stop-word, a stylistic or a lexical model. This
is true regardless of the topics on which the model is trained.
? Our proposed methodology is simple to implement suggesting that our findings on cross-topic AA
will be generalizable to other classification problems too.
The paper is organized as follows. Section 2 describes two cross-topic datasets while Section 3 describes
the methodology for our experiments. Section 4 describes different features while Section 5 presents
the experimental setup. We present the evaluation and analysis in Sections 6 and 7. In Section 8, we
describe previous studies on cross-topic AA. Finally, Section 9 presents our conclusions and some future
directions.
2 Cross-Topic Datasets
Although several corpora are available for traditional AA, we need datasets containing documents from a
number of authors from different domains (different topics, different genres). We need many topics to be
able to test cross-topic performance, and many genres to ensure that our findings are robust across different
styles of text. Obtaining such corpora is a challenging task since most authorship attribution studies focus
on a single domain. We have found two datasets that meet our criteria, one having both cross-topic and
cross-genre flavor, and the other having only cross-topic flavor. The first corpus contains communication
samples from 21 authors in six genres (Email, Essay, Blog, Chat, Phone Interview, and Discussion) on six
topics (Catholic Church, Gay Marriage, War in Iraq, Legalization of Marijuana, Privacy Rights, and Sex
Discrimination), which we call dataset 1. This dataset was obtained from Goldstein-Stewart et al. (2009).
Using this dataset, it is possible to see how the performance of cross-topic AA changes across different
genres.
Another corpus is composed of texts published in The Guardian daily newspaper written by 13 authors
in one genre on four topics (dataset 2) due Stamatatos et al. (2013). It contains opinion articles (comments)
about World, U.K., Culture, and Politics. Table 1 shows some statistics about the datasets.
Corpus #authors #genres #topics
avg avg avg
#docs/author #sentences/doc #words/doc
Dataset 1 21 6 6 36 31.7 600
Dataset 2 13 1 4 64 53 1034
Table 1: Some statistics about dataset 1 and dataset 2.
In dataset 1, the average document length is almost half the average document length in dataset 2, while
the number of authors is almost twice as that in dataset 2. Also, in dataset 1, there is only one document
written by an author on each topic on each genre. However, there are, on average, 16 documents per author
per topic on each genre in dataset 2. Overall, dataset 1 seems more challenging and resembles more a
realistic scenario of forensic investigations where very few short documents per author might be available.
1229
3 Methodology
To answer our research question and test our hypothesis, we designed three training scenarios. First
of all, to demonstrate the complexity of cross-topic tasks, we compare the performance between two
training conditions: Intra-Topic (IT), and Single Cross-Topic (SCT). Once we show that it is important to
solve this CTAA problem, we design one more training condition based on our proposed idea, Multiple
Cross-Topics (MCT) and compare its performance with the IT and the SCT scenarios.
Intra-Topic (IT) In this scenario, all the documents in both the training and test data belong to the same
topic. Although this is a strong assumption that does not hold true in most of the realistic scenarios, we
examine AA under such conditions in order to be able to compare it with our proposed methods.
Single Cross-Topic (SCT) In this setting, the test data consists of documents from a single topic while
the AA model is trained using documents belonging to another topic different than the topic of the test
data, but from the same genre. For example, in dataset 1, for ?Chat? genre, a model could be trained
on a topic ?Gay Marriage? and tested on the topic ?Legalization of Marijuana?. We experiment on all
combinations of test/train topics, i.e., for each test topic, we train separately on each of the remaining
topics.
Multiple Cross-Topics (MCT) Unlike in SCT and IT scenarios, here for each test topic, we train
on documents from all available topics other than the one used for testing. Our assumption is that
authors somehow maintain their unique writeprints across different topics. Therefore, even though the
additional data comes from a topic different than that of the test data, we expect to see improvements in
the performance of cross-topic AA.
In the SCT scenario, since there is a mismatch between the training and test topic, we expect to obtain
experimental results worst than that of the IT scenario. However, we expect that the performance of
cross-topic AA using our proposed MCT scenario will be better than SCT in all the cases.
4 Features
The choice of features depends greatly on the type of classification problem. Previous research has
explored various types of features that can discriminate among the candidate authors. Stylistic features,
character-level and word-level n-grams are the most frequently and successfully used features (Houvardas
and Stamatatos, 2006; Zheng et al., 2006; Frantzeskou et al., 2007; Abbasi and Chen, 2008; Luyckx
and Daelemans, 2011; Koppel et al., 2011). We consider four of the most widely used features. Our
goal behind exploring four different types of features is to understand which features are the best for
cross-topic AA.
Lexical Features. Bag-of-words is one of the commonly used document representations that uses
single-content words as document features. Authorship attribution approaches using a bag-of-words
representation have been found to be effective (Diederich et al., 2003; Kaster et al., 2005; Zhao and
Zobel, 2005; Coyotl-Morales et al., 2006). We call bag-of-words the lexical features since we exclude
stop-words.
Stop-Words. Stop-words carry no or very little semantic meaning of the texts, however, their use
indicates the presence of certain syntactic structures. Although, these words are excluded in the topic-
based text classification tasks due to lack of any semantic information in them, we believe these features
will be effective in cross-domain AA as hinted by previous work (Goldstein-Stewart et al., 2009). Typically,
words such as articles, prepositions, and conjunctions are considered as stop-words. We use a list of stop
words publicly available for download (www.webconfs.com/stop-words.php).
Stylistic Features. Previous research has shown stylistic features to be effective in AA (Stamatatos,
2006; Bhargava et al., 2013). We use 13 stylistic features: number of sentences, number of tokens per
sentence, number of punctuations per sentence, number of emoticons per document, percentage of words
without vowel, percentage of contractions, percentage of total alphabetic characters, percentage of two
consecutive punctuations, percentage of three consecutive punctuations, percentage of upper case words,
1230
total parenthesis count, percentage of sentence initial words with first letter capitalized, and percentage of
words without vowel.
Character n-grams. An n-gram is a sequence of n-contiguous characters. These features capture both
the thematic as well as stylistic information of the texts, and hence have been proven to be very effective
in previous AA studies (Keselj et al., 2003; Peng et al., 2003; Escalante et al., 2011). Since these features
carry stylistic choices of the authors, we believe they will be stable across domains.
5 Experimental Settings
Following the training scenarios discussed previously in Section 3, we performed a set of experiments.
We used 643 predefined stop-words. We considered as lexical features all words that were not stop words,
and were among the 3,500 most frequent words occurring at least twice in the training data. We used
3,500 most frequent character 3-grams occurring at least six times in the training data.
Since dataset 1 is already balanced across authors, we used all the documents from this dataset. However,
dataset 2 was originally imbalanced, therefore we chose at most ten documents per author to avoid a highly
skewed distribution. In order to create a corpus like in the realistic scenarios of forensic investigations
such as tweets, SMS, and emails, we chunked each selected document by sentence boundaries into five
new short documents. This shortening of the documents increases the complexity of the task but enhances
the practical applicability of our methods. We use these chunked versions for evaluating our proposed
method. Splitting the documents in this way has been used in the past to deal with the lack of more
documents per author(Luyckx and Daelemans, 2011; Koppel and Winter, 2014).
We obtained the performance measures using support vector machines (SVMs) implemented in Weka
(Witten and Frank, 2005) with default parameters. We considered using SVMs because preliminary results
showed this algorithm outperformed other reasonable alternatives. We used prediction accuracy as the
performance measure to evaluate different training scenarios. Rather than just comparing the accuracies,
we make most of the decisions based on statistical significance computed using two-tailed t-tests with
95% confidence interval.
All the experiments for cross-topic settings are carried out by controlling the genre. In the IT scenario,
we computed the accuracy on each test topic using stratified 10-fold cross-validation. In the SCT scenario,
for each test topic, prediction accuracy was computed by training separately on each remaining topic and
averaging performances. We computed the accuracy on each test topic in the MCT scenario by withholding
one topic as test topic and training on all other topics. For each training scenario, we computed one single
score for each genre by averaging the accuracies across all test topics belonging to that genre.
6 Experimental Results and Evaluation
In this section, we report results and analysis on different experiments we carried out. We will start
by showing empirically the challenge of cross-topic AA. Then, we will show results of our proposed
approach.
6.1 Is Cross-Topic AA More Difficult than Intra-Topic AA?
Genre
Lexical Features Stop-words Stylistic Features Character n-grams
IT SCT IT-SCT IT SCT IT-SCT IT SCT IT-SCT IT SCT IT-SCT
Chat 25.71 13.11 96.11
?
19.21 16.54 16.14
?
41.90 27.49 34.39
?
39.21 27.56 42.27
?
Essay 26.58 5.92 348.99
?
16.80 11.77 42.74
?
15.66 14.56 7.02 30.90 13.28 132.68
?
Email 19.80 6.22 218.33
?
16.43 12.67 29.68
?
25.29 24.4 3.52 24.94 14.52 71.76
?
Phone Interview 37.62 10.29 265.6
?
33.49 18.00 86.06
?
33.02 16.16 51.06
?
56.99 25.46 123.84
?
Blog 22.18 6.32 250.95
?
15.37 11.25 36.62
?
13.16 11.31 14.06
?
25.38 12.03 110.97
?
Discussion 23.37 11.64 100.77
?
23.37 16.31 43.29
?
30.99 15.8 49.02
?
40.69 25.28 60.96
?
Table 2: Comparison of AA performance on IT and SCT scenarios on dataset 1. For each feature type, the
IT and SCT columns indicate the accuracy (%) while the IT-SCT column is the relative gain of IT over
SCT. For each genre, bold figures represent the best accuracy. Statistical significance is indicated by
?
in
positive direction and by
[
in negative direction.
1231
First of all, we want to understand if the cross-topic problem is more difficult than the intra-topic
problem of AA. We compared the performance of the IT and the SCT scenarios using four types of
features on various genres of dataset 1 as shown in Table 2. We clearly observed that for each genre, and
for each feature type, the performance of the IT scenario is better than the SCT scenario and the difference
is statistically significant. The only exceptions are ?Email? and ?Essay? genres for stylistic features. This
is a strong indication that irrespective of the type of domain as well as the features considered, cross-topic
AA is much more difficult than intra-topic AA.
6.2 Does Our Proposed Method Improve CTAA Performance?
We target to answer: Can we reliably predict the author of a document written in one topic with a predictive
model developed using documents from multiple other topics? We carry out various experiments and
compare the performance of our proposed MCT scenario with that of IT and SCT scenarios separately.
Although, comparing MCT with only SCT would be enough to answer our research question and test our
hypothesis, we are also interested in gaining more insights about cross-topic AA and understanding how
it compares to IT, the simplest case of AA.
Genre IT SCT MCT MCT-IT MCT-SCT
Chat 25.71 13.11 33.02 28.43
?
151.87
?
Essay 26.58 5.92 12.64 -52.45
[
113.51
?
Email 19.80 6.22 11.87 -40.05
[
90.84
?
Phone Interview 37.62 10.29 20.95 -44.31
[
103.6
?
Blog 22.18 6.32 13.15 -40.71 108.07
?
Discussion 23.37 11.64 25.26 8.09 117.01
?
(a) Lexical Features
Genre IT SCT MCT MCT-IT MCT-SCT
Chat 19.21 16.54 33.49 74.34
?
102.48
?
Essay 16.80 11.77 22.06 31.31
?
97.08
?
Email 16.43 12.67 24.97 51.98
?
116.06
?
Phone Interview 33.49 18,00 38.89 16.12 115.67
?
Blog 15.37 11.25 20.43 32.92 81.6
?
Discussion 23.37 16.31 32.59 39.45
?
99.82
?
(b) Stop-words
Genre IT SCT MCT MCT-IT MCT-SCT
Chat 41.90 27.49 37.62 -10.21 36.85
?
Essay 15.66 14.56 23.36 49.17
?
60.44
?
Email 25.29 24.4 33.12 30.96
?
35.74
?
Phone Interview 33.02 16.16 23.49 -28.86 45.36
?
Blog 13.16 11.31 15.67 26.29
?
38.55
?
Discussion 30.99 15.8 24.33 -21.49 53.99
?
(c) Stylistic Features
Genre IT SCT MCT MCT-IT MCT-SCT
Chat 39.21 27.56 57.46 46.54
?
108.49
?
Essay 30.9 13.28 36.66 18.64 176.05
?
Email 24.94 14.52 36.53 46.47
?
151.58
?
Phone Interview 56.99 25.46 56.35 -1.12 121.33
?
Blog 25.38 12.03 33.41 31.64 177.72
?
Discussion 40.69 25.28 49.91 22.66
?
97.43
?
(d) Character n-grams
Table 3: Performance of lexical, stop-words, stylistic, and character n?gram features on dataset 1. The
SCT, IT and MCT columns indicate the accuracy (%) while the MCT-SCT and MCT-IT columns present
the relative gain of MCT over the other scenario. Statistical significance is indicated by
?
in positive
direction and by
[
in negative direction.
MCT-SCT columns on Table 3 illustrate the statistical significance of MCT over SCT in a positive
direction for all the genres. Using any type of feature in any genre, it is possible to significantly improve
the performance of CTAA by training a machine learning algorithm using documents from all available
out-of-domain topics. This serves as evidence to confirm our hypothesis and answer our research question
that documents written in one topic can be reliably predicted with a model developed using documents
from multiple other topics. This indicates that authors maintain a consistent writing style across topics.
In the MCT-IT column in Table 3(a), we can seen that the IT is significantly better than the MCT in
three genres, while the MCT is better than the IT in only one. This is because lexical features directly
capture the choices of authors in a certain thematic area, and hence they yield a good performance in the
intra-topic setting. However, we observed contrasting and interesting patterns using stop-words, stylistic
features, and character n-grams (MCT-IT column of Tables 3(b), 3(c), and 3(d)). MCT was better than IT,
and the difference was significantly better, in 10 genres, while IT performance was significantly better
than MCT in none of the genres. This is a very interesting finding as we observed that the cross-topic AA
problem can be solved as effectively as the intra-topic AA problem using these features and a variety of
topics.
Also using dataset 2, we found that for each type of feature, MCT is better than SCT, and the difference
is statistically significant as shown in Table 4. This is another supporting evidence to our hypothesis. The
small gain of IT over MCT suggests that our proposed approach is competitive even with the IT scenario.
1232
Feature Type IT SCT MCT MCT-IT MCT-SCT
Lexical Features 63.98 21.46 38.62 -39.64
[
79.96
?
Stop-words 45.01 31.66 41.21 -8.44 30.16
?
Stylistic Features 32.85 27.46 32.17 -2.07 17.15
?
Character n-grams 75.08 45.87 64.54 -14.04
[
40.7
?
Table 4: Performance of four types of features on three different training scenarios on dataset 2. For each
feature type, the SCT, IT and MCT columns indicate the accuracy (%) while the MCT-SCT and MCT-IT
columns present the relative gain of MCT over the other scenario. Statistical significance is indicated by
?
in positive direction and by
[
in negative direction.
6.3 Sensitivity of Features to Changes in Topic
We also want to demonstrate the behavior of four different feature types to changes in topic. We want to
test if lexical features favor intra-topic AA and character n-grams favor cross-topic AA. Unlike lexical
features, character n-grams carry stylistic choices of authors, and hence are expected to be robust across
topics. In Table 2, for each genre, the relative gain of IT over SCT using lexical features is highest
compared to that of stop-words, stylistic features, and character n-grams, thereby indicating that lexical
features are more effective for ITAA than for CTAA. It is also apparent in Table 2 that the gain of
characters n-grams is always better than that of stop-words and stylistic features. While looking at the
performance on the SCT scenario using four features, it is observed that character n-grams give the best
performance, while stop-words and stylistic features give the second best performance, which leaves
lexical features at the bottom. This is because the first three features are topic-independent and hence
were able to better discriminate among authors in cross-topic scenarios than lexical features. However,
overall, character n-grams have the highest discriminative power in both IT and SCT, which confirms
findings of earlier research (Stamatatos, 2013).
In Table 3, character n-grams, when compared to lexical features, stop-words, and stylistic features,
yield the highest average relative gain on MCT over the SCT scenario (138.77%, vs 114.15% for lexical
features, 97.41% for stop-words, 46.55% for stylistic features). Also, comparing the prediction accuracies
of all four features separately in SCT, IT, and MCT scenarios, it is observed that character n-grams score
best in most of the genres on each training scenario. This confirms that character n-grams have higher
discriminative power in cross-topic AA than stop-words, stylistic features and lexical features.
For cross-topic AA, we observed that the accuracy across the board is not high. It is because the CTAA
task is harder than other single domain classification tasks since the topics of the test data are fully disjoint
with the topics of the training data. On top of that, the shorter document length makes it more challenging.
The current system might not be production quality, but our findings will enable better models in the
future that hopefully will be accurate enough to solve CTAA problems more effectively.
6.4 Cross-Topic AA with Varying Number of Training Topics
For traditional AA, it has been shown that around 10,000 word-tokens per author suffice as a ?reliable
minimum for an authorial set? (Burrows, 2007). In our study, we have as few as 600 word-tokens per
author, much less than the minimum size requirement stated by previous research. In this section, we look
at how performance improves with increase in amount of training data by adding additional topics.
To explore this, we experimented by training on documents from all possible combinations of topics. In
dataset 1, there are a total of six topics. Therefore, for each test topic, we experiment separately using
one, two, three, four, and five topics for training. When measuring performance on k training topics, we
gather all possible combinations of training on k of the five topics and then average the performance
across all these combinations. For example, if we use two topics for training, then for each test topic, there
are
(
5
2
)
= 10 possible training combinations that we then average to get a final score. We illustrate the
results in Figure 1 for four genres using four types of features. Irrespective of the genres, topics, and types
of features used, CTAA performance improves gradually with addition of more data. In most genres, this
improvement seems to be almost linear with the number of topics trained on, suggesting that gathering
more out-of-topic data should continue to improve the performance. We also observed that the character
1233
(a) Genre = Discussion (b) Genre = Phone Interview
(c) Genre = Essay (d) Genre = Blog
Figure 1: Effect of training on varying number of topics in CTAA using lexical, stop-words, stylistic, and
character n-gram features on dataset 1.
n-grams are the most effective author discriminator in cross-topic AA.
We performed a deeper analysis of the effect of individual topics, which is shown in Table 5. We took
an initial topic as training data and then paired it with each of the other topics as additional training data
and measured the average performance gain from the addition of the second topic. It is shown that across
all genres, adding a second topic to the training data gives a character n-gram model greatest boost in
performance than to a stop word or a stylistic or a lexical model. This is true regardless of the topics on
which the model is trained. We do not observe negative transfer as in transfer learning (Pan and Yang,
2010) because in cross topic AA authors maintain styles across topics.
Initial Topic
Genre = Chat Genre = Email
Lexical Stop-words Stylistic Character n-grams Lexical Stop-words Stylistic Character n-grams
Sex Discrimination 5.85 5.57 1.67 10.33 2.24 7.29 8.86 9.72
Legalization of Marijuana 7.86 7.76 1.57 12.19 2.91 3.32 5.21 7.39
Catholic Church 6.24 8.76 6.24 14.33 2.41 4.48 3.59 5.22
Privacy Rights 5.9 4.66 1.9 14.05 2.97 6.45 4.6 10.06
War in Iraq 8.1 7.95 3.48 15.57 3.96 7.58 2.99 7.79
Gay Marriage 7.19 5.85 7.19 10.29 2.57 4.31 1.98 6.82
Table 5: Average performance gain from adding an additional topic as training data across different initial
topics on dataset 1. Each value is the average accuracy gain after adding the second topic.
7 Is it Just ?More Data? that is Helping or is ?Diversity? Relevant?
The quantity of training data was not controlled in the experiments presented in Section 6, therefore,
we performed some additional experiments where we did control for this. In Table 6, we present the
comparison of SCT and MCT scenarios using the same amount of training data to understand whether
the performance improvement in the MCT scenario is due to diversity or due to the fact of adding more
data. We use dataset 1 to make this comparison. For the SCT scenario, for each test topic, we averaged
1234
performance over three random samplings, where in each sampling we randomly selected four documents
per author in each training topic. For the MCT scenario, for each test topic, we averaged performance
Lexical Features Stop-words Stylistic Features Character n-grams
Genre SCT MCT MCT-SCT SCT MCT MCT-SCT SCT MCT MCT-SCT SCT MCT MCT-SCT
Chat 12.24 13.94 13.89
?
14.37 16.35 13.78
?
26.52 28.52 7.54
?
24.39 25.17 3.2
?
Essay 9.11 11.3 24.04
?
12.43 14.12 13.6
?
21.35 22.93 7.4
?
18.37 19.58 6.59
?
Discussion 9.65 10.52 9.02
?
12.93 13.7 5.96
?
19.57 20.85 6.54
?
19.84 21.48 8.27
?
Email 8.84 9.98 12.9
?
12.48 13.91 11.46
?
20.89 21.92 4.93
?
17.91 20.76 15.91
?
Phone Interview 8.94 10.84 21.25
?
14.65 17.67 20.61
?
19.73 20.94 6.13
?
18.84 26.35 39.86
?
Blog 8.45 9.66 14.32
?
12.78 14.05 9.94
?
18.53 19.62 5.88
?
17.58 19.95 13.48
?
Table 6: Comparison of MCT and SCT scenarios on controlled training data using four types of features
on dataset 1. For each feature type, the SCT and MCT columns indicate the accuracy (%) while the
MCT-SCT columns present the relative gain of MCT over the SCT. Statistical significance is indicated by
?
in positive direction and by
[
in negative direction.
over three random samplings, where in each sampling we randomly selected four training topics. For
each selection of four training topics, we averaged performance over three random samplings where in
each sampling we randomly selected one document per author in each training topic. Thus, we ended up
with the same number of documents for training both models. Even with the same amount of training
data, training on documents from different topics is better than training on documents from a single topic,
with statistically significant performance gains ranging from 3.2% to 39.86% as shown in Table 6. This
demonstrates that data from a diverse set of topics will still give a boost in performance and is always
significantly better than using data from the same topic.
8 Related Work
The majority of the work in authorship attribution deals with single-domain datasets. However, there
have been a handful of studies that add some cross-topic flavor in the AA task (Mikros and Argiri, 2007;
Goldstein-Stewart et al., 2009; Schein et al., 2010; Stamatatos, 2013). Mikros et al. (2007) concluded that
many stylometric variables are actually discriminating topic rather than author and their use in AA should
be done carefully. However, the study was performed on a single corpus containing only two authors
in two topics that raises questions on reliability of their conclusions. Stamatatos (2013) illustrated the
effectiveness of character n-grams in cross-topic AA. It was also shown in that study that avoiding rare
features is effective in both intra-topic and cross-topic AA. However, all these conclusions came from
training an SVM classifier in only one fixed topic. In contrast, in our paper, we draw our conclusions from
all possible training/testing combinations rather than fixing in advance the training topic.
Goldstein-Stewart et al. (2009) also carried out some cross-topic experiments by concatenating the texts
of an author from different genres. This experimental setting results in a corpus where each test document
contains a mix of genres, which is not representative of real world AA problems. Still, to provide some
comparisons to the work of Goldstein-Stewart et al. (2009), we concatenated all the texts in dataset 1
produced by an individual on a single topic, across all genres to produce one document per author on each
topic. We compare our results with those reported in the paper under same training/testing conditions. We
withheld one topic and trained on documents from the other five topics.
Test Topic Lexical Stop-words Stylistic Character n-grams Stop-words + Character n-grams Previous Work
Sex Discrimination 66.67 76.19 33.33 95.24 95.24 95
Catholic Church 76.19 95.24 38.10 95.24 100 95
Gay Marriage 80.95 80.95 42.86 90.48 90.48 95
Legalization of Marijuana 52.38 66.67 33.33 95.24 100 100
Privacy Rights 42.86 52.38 28.57 95.24 90.48 100
War in Iraq 57.14 71.43 38.10 100 100 81
Average 62.7 73.81 35.72 95.24 96.03 94.33
Table 7: Comparing performance of our work with previous work in the same training/testing setting. The
results in the last column were obtained from Goldstein-Stewart et al.(2009). For each test topic, the bold
figure represents the best performance.
1235
The last column of Table 7 presents the results obtained by using the combination of stop-words and 88
Linguistic Inquiry and Word Count (LIWC) features as reported in Goldstein-Stewart et al. (2009). We
observed that the combination of character n-grams and stop-words, on average, performs better than
those reported in the paper. On this fixed training/testing scenario, we see better accuracies, as high as
100%, across the board. This is because, in this experiment, each training sample on average was ? 25
times longer than the training sample in our chunked versions. This illustrates that authorship attribution
of short documents, as in our chunked versions, is a challenging task, but we believe it resembles a more
realistic scenario of forensic investigations.
9 Conclusions and Future Work
In this research, we presented the first comprehensive study with rigorous analysis on cross-topic AA.
Although previous work had hinted some of our findings, it was based on very limited experiments (using
only one fixed topic for training). We investigated CTAA using all possible combinations of topics to draw
more robust and stable conclusions. We first illustrated the difficulty of cross-topic AA by comparing its
performance with intra-topic AA using different types of features. We demonstrated that a framework
trained on documents belonging to thematic areas different than that of the documents under investigation
statistically improves the performance of cross-topic AA. This improves the ability of the model to find the
authors of documents belonging to a new topic not present during the training of the model. By controlling
the training data, we demonstrated that training on diverse topics is better than training on a single topic
confirming that MCT not only benefits from more data but also from a thematic variety. We also showed a
statistical analysis that lexical features are closer to the thematic area and hence were an effective author
discriminator in intra-topic attribution. Similarly, character n-grams prove to be a very powerful feature
especially in a condition where training and test documents come from different thematic areas. Although
intra-topic AA is easier than cross-topic AA, our proposed model for CTAA achieves performance close
or in some cases, better than that of an intra-topic AA model. Another interesting conclusion of our study
is that addition of more training data from any topic, no matter how distant or close it is with the topic of
documents under investigation, improves the performance of CTAA for all types of features. We believe
that our contribution to cross-topic AA will be generalizable to other classification problems too.
In the future, we plan to explore the cross-genre problem of AA that is critical for tasks like linking
user accounts across emails, blogs, and other social media. Our proposed CTAA approach can be directly
applied to the cross-genre problem but we may discover different feature behavior in this scenario. We
also plan to explore domain adaptation and transfer learning techniques to solve CDAA problems.
Acknowledgements
This research was partially supported by ONR grant N00014-12-1-0217, NSF award 1254108, and NSF
award 1350360. It was also supported in part by the CONACYT grant 134186 and the WIQ-EI IRSES
project (grant no. 269180) within the FP 7 Marie Curie.
References
A. Abbasi and H. Chen. 2008. Writeprints: A stylometric approach to identity-level identification and similarity
detection in cyberspace. ACM Trans. Inf. Syst., 26(2):7:1?7:29, April.
M. Bhargava, P. Mehndiratta, and K. Asawa. 2013. Stylometric analysis for authorship attribution on twitter. In
Big Data Analytics, volume 8302 of Lecture Notes in Computer Science, pages 37?47. Springer International
Publishing.
J. Burrows. 2007. All the way through: Testing for authorship in different frequency strata. Literary & Linguistic
Computing, 22:27 ? 47.
R. Mar??a Coyotl-Morales, L. Villase?nor Pineda, M. Montes-y G?omez, and P. Rosso. 2006. Authorship attribution
using word sequences. In Proceedings of the 11th Iberoamerican Conference on Progress in Pattern Recogni-
tion, Image Analysis and Applications, CIARP?06, pages 844?853, Berlin, Heidelberg. Springer-Verlag.
1236
O. de Vel, A. Anderson, M. Corney, and G. Mohay. 2001. Multi-topic e-mail authorship attribution forensics.
In Proceedings of the Workshop on Data Mining for Security Applications, 8th ACM Conference on Computer
Security.
J. Diederich, J. Kindermann, E. Leopold, and G. Paass. 2003. Authorship attribution with support vector machines.
Applied Intelligence, 19:109?123, May.
H. J. Escalante, T. Solorio, and M. Montes-y G?omez. 2011. Local histograms of character n-grams for author-
ship attribution. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:
Human Language Technologies, pages 288?298, Portland, Oregon, USA, June. Association for Computational
Linguistics.
G. Frantzeskou, E. Stamatatos, S. Gritzalis, and C. E. Chaski. 2007. Identifying authorship by byte-level n-grams:
The source code author profile (SCAP) method. Journal of Digital Evidence, 6(1).
J. Goldstein-Stewart, R. Winder, and R. Evans Sabin. 2009. Person identification from text and speech genre
samples. In Proceedings of the 12th Conference of the European Chapter of the Association for Computational
Linguistics, EACL ?09, pages 336?344, Stroudsburg, PA, USA. Association for Computational Linguistics.
J. Houvardas and E. Stamatatos. 2006. N-gram feature selection for authorship identification. In J. Euzenat and
J. Domingue, editors, AIMSA 2006, volume 4183 of LNAI, pages 77?86.
A. Kaster, S. Siersdorfer, and G. Weikum. 2005. Combining text and linguistic document representations for
authorship attribution. In SIGIR Workshop: Stylistic Analysis of Text for Information Access, pages 27?35.
V. Keselj, F. Peng, N. Cercone, and C. Thomas. 2003. N-gram based author profiles for authorship attribution. In
Proceedings of the Pacific Association for Computational Linguistics, pages 255?264.
M. Koppel and Y. Winter. 2014. Determining if two documents are written by the same author. Journal of the
Association for Information Science and Technology, 65(1):178?187.
M. Koppel, J. Schler, and S. Argamon. 2011. Authorship attribution in the wild. Language Resources and
Evaluation, 45:83?94.
K. Luyckx and W. Daelemans. 2011. The effect of author set size and data size in authorship attribution. Literary
and Linguistic Computing, 26(1):35?55.
G. K. Mikros and E. K. Argiri. 2007. Investigating topic influence in authorship attribution. In Proceedings of
the International Workshop on Plagiarism Analysis, Authorship Identification, and Near-Duplicate Detection,
pages 29?35.
S. Jialin Pan and Q. Yang. 2010. A survey on transfer learning. IEEE Trans. on Knowl. and Data Eng.,
22(10):1345?1359, October.
F. Peng, D. Schuurmans, V. Keselj, and S. Wang. 2003. Language independent authorship attribution using char-
acter level language models. In 10th Conference of the European Chapter of the Association for Computational
Linguistics, EACL, pages 267?274.
A. I. Schein, J. F. Caver, R. J. Honaker, and C. H. Martell. 2010. Author attribution evaluation with novel topic
cross-validation. In The 2010 International Conference on Knowledge Discovery and Information Retrieval,
Valencia, Spain, October.
E. Stamatatos. 2006. Authorship attribution based on feature set subspacing ensembles. International Journal on
Artificial Intelligence tools, 15(5):823?838.
E. Stamatatos. 2011. Plagiarism detection using stopword n-grams. Journal of the American Society for Informa-
tion Science and Technology, 62(12):2512?2527.
E. Stamatatos. 2013. On the robustness of authorship attribution based on character n-gram features. Journal of
Law & Policy, 21(2):421 ? 439.
I. H. Witten and E. Frank. 2005. Data Mining: Practical Machine Learning Tools and Techniques. Morgan
Kauffmann, 2nd edition.
Y. Zhao and J. Zobel. 2005. Effective and scalable authorship attribution using function words. In Proceedings of
2nd Asian Information Retrieval Symposium, volume 3689 of LNCS, pages 174?189, Jeju Island, Korea.
R. Zheng, J. Li, H. Chen, and Z. Huang. 2006. A framework for authorship identification of online messages:
Writing-style features and classification techniques. J. Am. Soc. Inf. Sci. Technol., 57(3):378?393, February.
1237
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 288?298,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Local Histograms of Character N -grams for Authorship Attribution
Hugo Jair Escalante
Graduate Program in Systems Eng.
Universidad Auto?noma de Nuevo Leo?n,
San Nicola?s de los Garza, NL, 66450, Me?xico
hugo.jair@gmail.com
Thamar Solorio
Dept. of Computer and Information Sciences
University of Alabama at Birmingham,
Birmingham, AL, 35294, USA
solorio@cis.uab.edu
Manuel Montes-y-Go?mez
Computer Science Department, INAOE,
Tonantzintla, Puebla, 72840, Me?xico
Department of Computer and Information Sciences,
University of Alabama at Birmingham,
Birmingham, AL, 35294, USA
mmontesg@cis.uab.edu
Abstract
This paper proposes the use of local his-
tograms (LH) over character n-grams for au-
thorship attribution (AA). LHs are enriched
histogram representations that preserve se-
quential information in documents; they have
been successfully used for text categorization
and document visualization using word his-
tograms. In this work we explore the suitabil-
ity of LHs over n-grams at the character-level
for AA. We show that LHs are particularly
helpful for AA, because they provide useful
information for uncovering, to some extent,
the writing style of authors. We report experi-
mental results in AA data sets that confirm that
LHs over character n-grams are more help-
ful for AA than the usual global histograms,
yielding results far superior to state of the art
approaches. We found that LHs are even more
advantageous in challenging conditions, such
as having imbalanced and small training sets.
Our results motivate further research on the
use of LHs for modeling the writing style of
authors for related tasks, such as authorship
verification and plagiarism detection.
1 Introduction
Authorship attribution (AA) is the task of deciding
whom, from a set of candidates, is the author of a
given document (Houvardas and Stamatatos, 2006;
Luyckx and Daelemans, 2010; Stamatatos, 2009b).
There is a broad field of application for AA meth-
ods, including spam filtering (de Vel et al, 2001),
fraud detection, computer forensics (Lambers and
Veenman, 2009), cyber bullying (Pillay and Solorio,
2010) and plagiarism detection (Stamatatos, 2009a).
Therefore, the development of automated AA tech-
niques has received much attention recently (Sta-
matatos, 2009b). The AA problem can be natu-
rally posed as one of single-label multiclass clas-
sification, with as many classes as candidate au-
thors. However, unlike usual text categorization
tasks, where the core problem is modeling the the-
matic content of documents (Sebastiani, 2002), the
goal in AA is modeling authors? writing style (Sta-
matatos, 2009b). Hence, document representations
that reveal information about writing style are re-
quired to achieve good accuracy in AA.
Word and character based representations have
been used in AA with some success so far (Houvar-
das and Stamatatos, 2006; Luyckx and Daelemans,
2010; Plakias and Stamatatos, 2008b). Such rep-
resentations can capture style information through
word or character usage, but they lack sequential in-
formation, which can reveal further stylistic infor-
mation. In this paper, we study the use of richer
document representations for the AA task. In partic-
ular, we consider local histograms over n-grams at
the character-level obtained via the locally-weighted
bag of words (LOWBOW) framework (Lebanon et
al., 2007).
Under LOWBOW, a document is represented by a
set of local histograms, computed across the whole
document but smoothed by kernels centered on dif-
ferent document locations. In this way, document
288
representations preserve both word/character usage
and sequential information (i.e., information about
the positions in which words or characters occur),
which can be more helpful for modeling the writ-
ing style of authors. We report experimental re-
sults in an AA data set used in previous studies un-
der several conditions (Houvardas and Stamatatos,
2006; Plakias and Stamatatos, 2008b; Plakias and
Stamatatos, 2008a). Results confirm that local his-
tograms of character n-grams are more helpful for
AA than the usual global histograms of words or
character n-grams (Luyckx and Daelemans, 2010);
our results are superior to those reported in re-
lated works. We also show that local histograms
over character n-grams are more helpful than lo-
cal histograms over words, as originally proposed
by (Lebanon et al, 2007). Further, we performed
experiments with imbalanced and small training
sets (i.e., under a realistic AA setting) using the
aforementioned representations. We found that the
LOWBOW-based representation resulted even more
advantageous in these challenging conditions. The
contributions of this work are as follows:
? We show that the LOWBOW framework can be
helpful for AA, giving evidence that sequential in-
formation encoded in local histograms is useful for
modeling the writing style of authors.
? We propose the use of local histograms over
character-level n-grams for AA. We show that
character-level representations, which have proved
to be very effective for AA (Luyckx and Daelemans,
2010), can be further improved by adopting a local
histogram formulation. Also, we empirically show
that local histograms at the character-level are more
helpful than local histograms at the word-level for
AA.
? We study several kernels for a support vector ma-
chine AA classifier under the local histograms for-
mulation. Our study confirms that the diffusion ker-
nel (Lafferty and Lebanon, 2005) is the most ef-
fective among those we tried, although competitive
performance can be obtained with simpler kernels.
? We report experimental results that are superior to
state of the art approaches (Plakias and Stamatatos,
2008b; Plakias and Stamatatos, 2008a), with im-
provements ranging from 2%?6% in balanced data
sets and from 14%? 30% in imbalanced data sets.
2 Related Work
AA can be faced as a multiclass classifica-
tion task with as many classes as candidate au-
thors. Standard classification methods have been
applied to this problem, including support vec-
tor machine (SVM) classifiers (Houvardas and Sta-
matatos, 2006) and variants thereon (Plakias and
Stamatatos, 2008b; Plakias and Stamatatos, 2008a),
neural networks (Tearle et al, 2008), Bayesian clas-
sifiers (Coyotl-Morales et al, 2006), decision tree
methods (Koppel et al, 2009) and similarity based
techniques (Keselj et al, 2003; Lambers and Veen-
man, 2009; Stamatatos, 2009b; Koppel et al, 2009).
In this work, we chose an SVM classifier as it has
reported acceptable performance in AA and because
it will allow us to directly compare results with pre-
vious work that has used this same classifier.
A broad diversity of features has been used to rep-
resent documents in AA (Stamatatos, 2009b). How-
ever, as in text categorization (Sebastiani, 2002),
word-based and character-based features are among
the most widely used features (Stamatatos, 2009b;
Luyckx and Daelemans, 2010). With respect to
word-based features, word histograms (i.e., the bag-
of-words paradigm) are the most frequently used
representations in AA (Zhao and Zobel, 2005;
Argamon and Levitan, 2005; Stamatatos, 2009b).
Some researchers have gone a step further and
have attempted to capture sequential information
by using n-grams at the word-level (Peng et al,
2004) or by discovering maximal frequent word se-
quences (Coyotl-Morales et al, 2006). Unfortu-
nately, because of computational limitations, the lat-
ter methods cannot discover enough sequential in-
formation from documents (e.g., word n-grams are
often restricted to n ? {1, 2, 3}, while full se-
quential information would be obtained with n ?
{1 . . . D} where D is the maximum number of
words in a document).
With respect to character-based features, n-grams
at the character level have been widely used in AA
as well (Plakias and Stamatatos, 2008b; Peng et
al., 2003; Luyckx and Daelemans, 2010). Peng et
al. (2003) propose the use of language models at the
n-gram character-level for AA, whereas Keselj et
al. (2003) build author profiles based on a selection
of frequent n-grams for each author. Stamatatos and
co-workers have studied the impact of feature se-
lection, with character n-grams, in AA (Houvardas
and Stamatatos, 2006; Stamatatos, 2006a), ensem-
ble learning with character n-grams (Stamatatos,
2006b) and novel classification techniques based
289
on characters at the n-gram level (Plakias and Sta-
matatos, 2008a).
Acceptable performance in AA has been reported
with character n-gram representations. However,
as with word-based features, character n-grams are
unable to incorporate sequential information from
documents in their original form (in terms of the
positions in which the terms appear across a doc-
ument). We believe that sequential clues can be
helpful for AA because different authors are ex-
pected to use different character n-grams or words
in different parts of the document. Accordingly,
in this work we adopt the popular character-based
and word-based representations, but we enrich them
in a way that they incorporate sequential informa-
tion via the LOWBOW framework. Hence, the pro-
posed features preserve sequential information be-
sides capturing character and word usage informa-
tion. Our hypothesis is that the combination of se-
quential and frequency information can be particu-
larly helpful for AA.
The LOWBOW framework has been mainly used
for document visualization (Lebanon et al, 2007;
Mao et al, 2007), where researchers have used in-
formation derived from local histograms for dis-
playing a 2D representation of document?s con-
tent. More recently, Chasanis et al (2009) used
the LOWBOW framework for segmenting movies
into chapters and scenes. LOWBOW representa-
tions have also been applied to discourse segmen-
tation (AMIDA, 2007) and have been suggested for
text summarization (Das and Martins, 2007). How-
ever, to the best of our knowledge the use of the
LOWBOW framework for AA has not been studied
elsewhere. Actually, the only two references using
this framework for text categorization are (Lebanon
et al, 2007; AMIDA, 2007). The latter can be due to
the fact that local histograms provide little gain over
usual global histograms for thematic classification
tasks. In this paper we show that LOWBOW rep-
resentations provide important improvements over
global histograms for AA; in particular, local his-
tograms at the character-level achieve the highest
performance in our experiments.
3 Background
This section describes preliminary information on
document representations and pattern classification
with SVMs.
3.1 Bag of words representations
In the bag of words (BOW) representation, docu-
ments are represented by histograms over the vo-
cabulary1 that was used to generate a collection of
documents; that is, a document i is represented as:
di = [xi,1, . . . , xi,|V |] (1)
where V is the vocabulary and |V | is the number of
elements in V , di,j = xi,j is a weight that denotes
the contribution of term j to the representation of
document i; usually xi,j is related to the occurrence
(binary weighting) or the weighted frequency of oc-
currence (e.g., the tf-idf weighting scheme) of the
term j in document i.
3.2 Locally-weighted bag-of-words
representation
Instead of using the BOW framework directly, we
adopted the LOWBOW framework for document
representation (Lebanon et al, 2007). The underly-
ing idea in LOWBOW is to compute several local
histograms per document, where these histograms
are smoothed by a kernel function, see Figure 1.
The parameters of the kernel specify the position of
the kernel in the document (i.e., where the local his-
togram is centered) and its scale (i.e., to what extent
it is smoothed). In this way the sequential informa-
tion in the document is preserved together with term
usage statistics.
Let Wi = {wi,1, . . . , wi,Ni}, denote the terms
(in order of appearance) in document i where Ni
is the number of terms that appear in document i
and wi,j ? V is the term appearing at position
j; let vi = {vi,1, . . . , vi,Ni} be the set of indexes
in the vocabulary V of the terms appearing in Wi,
such that vi,j is the index in V of the term wi,j ;
let t = [t1, . . . , tNi ] be a set of (equally spaced)
scalars that determine intervals, with 0 ? tj ? 1 and?Ni
j=1 tj = 1, such that each tj can be associated to
a position in Wi. Given a kernel smoothing function
Ks?,? : [0, 1] ? R with location parameter ? and
scale parameter ?, where ?kj=1 Ks?,?(tj) = 1 and
1In the following we will refer to arbitrary vocabularies,
which can be formed with terms from either words or character
n-grams.
290
Figure 1: Diagram of the process for obtaining local
histograms. Terms (wi) appearing in different posi-
tions (1, . . . , N ) of the document are weighted according
to the locations (?1, . . . , ?k) of the smoothing function
K?,?(x). Then, the term position weighting is combined
with term frequency weighting for obtaining local his-
tograms over the terms in the vocabulary (1, . . . , |V |).
? ? [0, 1]. The LOWBOW framework computes a
local histogram for each position ?j ? {?1, . . . , ?k}
as follows:
dlji,{vi,1,...,vi,Ni} = di,{vi,1,...,vi,Ni} ?K
s
?j ,?(t) (2)
where dli,vj :vj 6?vi = const, a small constant value,
and di,j is defined as above. Hence, a set dl{1,...,k}i
of k local histograms are computed for each doc-
ument i. Each histogram dlji carries information
about the distribution of terms at a certain position
?j of the document, where ? determines how the
nearby terms to ?j influence the local histogram
j. Thus, sequential information of the document is
considered throughout these local histograms. Note
that when ? is small, most of the sequential informa-
tion is preserved, as local histograms are calculated
at very local scales; whereas when ? ? 1, local his-
tograms resemble the traditional BOW representa-
tion.
Under LOWBOW documents can be represented
in two forms (Lebanon et al, 2007): as a single his-
togram dLi = const ?
?k
j=1 dlji (hereafter LOW-
BOW histograms) or by the set of local histograms
itself dl{1,...,k}i . We performed experiments with
both forms of representation and considered words
and n-grams at the character-level as terms (c.f. Sec-
tion 5). Regarding the smoothing function, we con-
sidered the re-normalized Gaussian pdf restricted to
[0, 1]:
Ks?,?(x) =
?
?
?
N (x;?,?)
?( 1??? )??(??? ) if x ? [0, 1]
0 otherwise
(3)
where ?(x) is the cumulative distribution function
for a Gaussian with mean 0 and standard deviation 1,
evaluated at x, see (Lebanon et al, 2007) for further
details.
3.3 Support vector machines
Support vector machines (SVMs) are pattern classi-
fication methods that aim to find an optimal sepa-
rating hyperplane between examples from two dif-
ferent classes (Shawe-Taylor and Cristianini, 2004).
Let {xi, yi}N be pairs of training patterns-outputs,
where xi ? Rd and y ? {?1, 1}, with d the di-
mensionality of the problem. SVMs aim at learn-
ing a mapping from training instances to outputs.
This is done by considering a linear function of the
form: f(x) = Wx + b, where parameters W and b
are learned from training data. The particular linear
function considered by SVMs is as follows:
f(x) =
?
i
?iyiK(xi, x)? b (4)
that is, a linear function over (a subset of) training
examples, where ?i is the weight associated with
training example i (those for which ?i > 0 are the so
called support vectors) and yi is the label associated
with training example i, K(xi, xj) is a kernel2 func-
tion that aims at mapping the input vectors, (xi, xj),
into the so called feature space, and b is a bias
term. Intuitively, K(xi, xj) evaluates how similar
instances xi and xj are, thus the particular choice of
kernel is problem dependent. The parameters in ex-
pression (4), namely ?{1,...,N} and b, are learned by
using exact optimization techniques (Shawe-Taylor
and Cristianini, 2004).
2One should not confuse the kernel smoothing function,
Ks?,?(x), defined in Equation (3) with the Mercer kernel in
Equation (4), as the former acts as a smoothing function and
the latter acts as a similarity function.
291
4 Authorship Attribution with LOWBOW
Representations
For AA we represent the training documents of
each author using the framework described in Sec-
tion 3.2, thus each document of each candidate au-
thor is either a LOWBOW histogram or a bag of lo-
cal histograms (BOLH). Recall that LOWBOW his-
tograms are an un-weighted sum of local histograms
and hence can be considered a summary of term us-
age and sequential information; whereas the BOLH
can be seen as term occurrence frequencies across
different locations of the document.
For both types of representations we consider an
SVM classifier under the one-vs-all formulation for
facing the AA problem. We consider SVM as base
classifier because this method has proved to be very
effective in a large number of applications, including
AA (Houvardas and Stamatatos, 2006; Plakias and
Stamatatos, 2008b; Plakias and Stamatatos, 2008a);
further, since SVMs are kernel-based methods, they
allow us to use local histograms for AA by consid-
ering kernels that work over sets of histograms.
We build a multiclass SVM classifier by con-
sidering the pairs of patterns-outputs associated to
documents-authors. Where each pattern can be ei-
ther a LOWBOW histogram or the set of local his-
tograms associated with the corresponding docu-
ment, and the output associated to each pattern is
a categorical random variable (outputs) that asso-
ciates the representation of each document to its cor-
responding author y1,...,N ? {1, . . . , C}, with C
the number of candidate authors. For building the
multiclass classifier we adopted the one-vs-all for-
mulation, where C binary classifiers are built and
where each classifier fi discriminates among exam-
ples from class i (positive examples) and the rest
j : j ? {1, . . . , C}, j 6= i; despite being one of the
simplest formulations, this approach has shown to
obtain comparable and even superior performance to
that obtained by more complex formulations (Rifkin
and Klautau, 2004).
For AA using LOWBOW histograms, we con-
sider a linear kernel since it has been success-
fully applied to a wide variety of problems (Shawe-
Taylor and Cristianini, 2004), including AA (Hou-
vardas and Stamatatos, 2006; Plakias and Sta-
matatos, 2008b). However, standard kernels can-
not work for input spaces where each instance is de-
scribed by a set of vectors. Therefore, usual kernels
are not applicable for AA using BOLH. Instead, we
rely on particular kernels defined for sets of vectors
rather than for a single vector. Specifically, we con-
sider kernels of the form (Rubner et al, 2001; Grau-
man, 2006):
K(P,Q) = exp (? D(P,Q)
2
?
) (5)
where D(P,Q) is the sum of the distances between
the elements of the bag of local histograms asso-
ciated to author P and the elements of the bag of
histograms associated with author Q; ? is the scale
parameter of K. Let P = {p1, . . . , pk} and Q =
{q1, . . . , qk} be the elements of the bags of local
histograms for instances P and Q, respectively, Ta-
ble 1 presents the distance measures we consider for
AA using local histograms.
Kernel Distance
Diffusion D(P,Q) = ?kl=1 arccos
(??pl ?
?ql?
)
EMD D(P,Q) = EMD(P,Q)
Eucidean D(P,Q) =
??k
l=1(pl ? ql).2
?2 D(P,Q) =
??k
l=1
(pl?ql)2
(pl+ql)
Table 1: Distance functions used to calculate the kernel
defined in Equation (5).
Diffusion, Euclidean, and ?2 kernels compare lo-
cal histograms one to one, which means that the lo-
cal histograms calculated at the same locations are
compared to each other. We believe that for AA
this is advantageous as it is expected that an author
uses similar terms at similar locations of the docu-
ment. The Earth mover?s distance (EMD), on the
other hand, is an estimate of the optimal cost in tak-
ing local histograms from Q to local histograms in
P (Rubner et al, 2001); that is, this measure com-
putes the optimal matching distance between local
histograms from different authors that are not neces-
sarily computed at similar locations.
5 Experiments and Results
For our experiments we considered the data set used
in (Plakias and Stamatatos, 2008b; Plakias and Sta-
matatos, 2008a). This corpus is a subset of the
RCV1 collection (Lewis et al, 2004) and comprises
292
documents authored by 10 authors. All of the docu-
ments belong to the same topic. Since this data set
has predefined training and testing partitions, our re-
sults are comparable to those obtained by other re-
searchers. There are 50 documents per author for
training and 50 documents per author for testing.
We performed experiments with LOWBOW3 rep-
resentations at word and character-level. For the ex-
periments with words, we took the top 2,500 most
common words used across the training documents
and obtained LOWBOW representations. We used
this setting in agreement with previous work on
AA (Houvardas and Stamatatos, 2006). For our
character n-gram experiments, we obtained LOW-
BOW representations for character 3-grams (only
n-grams of size n = 3 were used) considering
the 2, 500 most common n-grams. Again, this set-
ting was adopted in agreement with previous work
on AA with character n-grams (Houvardas and
Stamatatos, 2006; Plakias and Stamatatos, 2008b;
Plakias and Stamatatos, 2008a; Luyckx and Daele-
mans, 2010). All our experiments use the SVM im-
plementation provided by Canu et al (2005).
5.1 Experimental settings
In order to compare our methods to related works
we adopted the following experimental setting. We
perform experiments using all of the training doc-
uments per author, that is, a balanced corpus (we
call this setting BC). Next we evaluate the perfor-
mance of classifiers over reduced training sets. We
tried balanced reduced data sets with: 1, 3, 5 and
10 documents per author (we call this configura-
tion RBC). Also, we experimented with reduced-
imbalanced data sets using the same imbalance rates
reported in (Plakias and Stamatatos, 2008b; Plakias
and Stamatatos, 2008a): we tried settings 2 ? 10,
5? 10, and 10? 20, where, for example, setting 2-
10 means that we use at least 2 and at most 10 doc-
uments per author (we call this setting IRBC). BC
setting represents the AA problem under ideal con-
ditions, whereas settings RBC and IRBC aim at em-
ulating a more realistic scenario, where limited sam-
ple documents are available and the whole data set is
highly imbalanced (Plakias and Stamatatos, 2008b).
3We used LOWBOW code of G. Lebanon and Y. Mao avail-
able from http://www.cc.gatech.edu/?ymao8/lowbow.htm
5.2 Experimental results in balanced data
We first compare the performance of the LOWBOW
histogram representation to that of the traditional
BOW representation. Table 2 shows the accuracy
(i.e., percentage of documents in the test set that
were associated to its correct author) for the BOW
and LOWBOW histogram representations when us-
ing words and character n-grams information. For
LOWBOW histograms, we report results with three
different configurations for ?. As in (Lebanon et al,
2007), we consider uniformly distributed locations
and we varied the number of locations that were in-
cluded in each setting. We denote with k the number
of local histograms. In preliminary experiments we
tried several other values for k, although we found
that representative results can be obtained with the
values we considered here.
Method Parameters Words Characters
BOW - 78.2% 75.0%
LOWBOW k = 2;? = 0.2 75.8% 72.0%
LOWBOW k = 5;? = 0.2 77.4% 75.2%
LOWBOW k = 20;? = 0.2 77.4% 75.0%
Table 2: Authorship attribution accuracy for the BOW
representation and LOWBOW histograms. Column 2
shows the parameters we used for the LOWBOW his-
tograms; columns 3 and 4 show results using words and
character n-grams, respectively.
From Table 2 we can see that the BOW repre-
sentation is very effective, outperforming most of
the LOWBOW histogram configurations. Despite a
small difference in performance, BOW is advanta-
geous over LOWBOW histograms because it is sim-
pler to compute and it does not rely on parameter
selection. Recall that the LOWBOW histogram rep-
resentations are obtained by the combination of sev-
eral local histograms calculated at different locations
of the document, hence, it seems that the raw sum of
local histograms results in a loss of useful informa-
tion for representing documents. The worse perfor-
mance was obtained when k = 2 local histograms
are considered (see row 3 in Table 2). This re-
sult is somewhat expected since the larger the num-
ber of local histograms, the more LOWBOW his-
tograms approach the BOW formulation (Lebanon
et al, 2007).
We now describe the AA performance obtained
when using the BOLH formulation; these results
293
are shown in Table 3. Most of the results from
this table are superior to those reported in Table 2,
showing that bags of local histograms are a better
way to exploit the LOWBOW framework for AA.
As expected, different kernels yield different results.
However, the diffusion kernel outperformed most of
the results obtained with other kernels; confirming
the results obtained by other researchers (Lebanon
et al, 2007; Lafferty and Lebanon, 2005).
Kernel Euc. Diffusion EMD ?2
Words
Setting-1 78.6% 81.0% 75.0% 75.4%
Setting-2 77.6% 82.0% 76.8% 77.2%
Setting-3 79.2% 80.8% 77.0% 79.0%
Characters
Setting-1 83.4% 82.8% 84.4% 83.8%
Setting-2 83.4% 84.2% 82.2% 84.6%
Setting-3 83.6% 86.4% 81.0% 85.2%
Table 3: Authorship attribution accuracy when using bags
of local histograms and different kernels for word-based
and character-based representations. The BC data set is
used. Settings 1, 2 and 3 correspond to k = 2, 5 and 20,
respectively.
On average, the worse kernel was that based on
the earth mover?s distance (EMD), suggesting that
the comparison of local histograms at different loca-
tions is not a fruitful approach (recall that this is the
only kernel that compares local histograms at differ-
ent locations). This result evidences that authors use
similar word/character distributions at similar loca-
tions when writing different documents.
The best performance across settings and kernels
was obtained with the diffusion kernel (in bold, col-
umn 3, row 9) (86.4%); that result is 8% higher
than that obtained with the BOW representation and
9% better than the best configuration of LOWBOW
histograms, see Table 2. Furthermore, that result
is more than 5% higher than the best reported re-
sult in related work (80.8% as reported in (Plakias
and Stamatatos, 2008b)). Therefore, the consid-
ered local histogram representations over character
n-grams have proved to be very effective for AA.
One should note that, in general, better per-
formance was obtained when using character-level
rather than word-level information. This confirms
the results already reported by other researchers
that have used character-level and word-level infor-
mation for AA (Houvardas and Stamatatos, 2006;
Plakias and Stamatatos, 2008b; Plakias and Sta-
matatos, 2008a; Peng et al, 2003). We believe this
can be attributed to the fact that character n-grams
provide a representation for the document at a finer
granularity, which can be better exploited with local
histogram representations. Note that by considering
3-grams, words of length up to three are incorpo-
rated, and usually these words are function words
(e.g., the, it, as, etc.), which are known to be in-
dicative of writing style. Also, n-gram information
is more dense in documents than word-level infor-
mation. Hence, the local histograms are less sparse
when using character-level information, which re-
sults in better AA performance.
True author
AC AS BL DL JM JG MM MD RS TN
88 2 0 0 0 0 0 0 0 0
10 98 0 0 0 0 0 0 0 0
0 0 68 0 40 0 0 0 0 0
0 0 0 80 0 0 0 0 0 4
0 0 12 2 42 0 0 2 0 0
0 0 0 0 0 100 0 0 0 2
2 0 2 0 0 0 100 0 0 0
0 0 18 0 18 0 0 98 0 0
0 0 0 2 0 0 0 0 100 4
0 0 0 16 0 0 0 0 0 90
Table 4: Confusion matrix (in terms of percentages) for
the best result in the BC corpus (i.e., last row, column 3
in Table 3). Columns show the true author for test docu-
ments and rows show the authors predicted by the SVM.
Table 4 shows the confusion matrix for the setting
that reached the best results (i.e., column 3, last row
in Table 3). From this table we can see that 8 out
of the 10 authors were recognized with an accuracy
higher or equal to 80%. For these authors sequential
information seems to be particularly helpful. How-
ever, low recognition performance was obtained for
authors BL (B. K. Lim) and JM (J. MacArtney).
The SVM with BOW representation of character n-
grams achieved recognition rates of 40% and 50%
for BL and JM respectively. Thus, we can state that
sequential information was indeed helpful for mod-
eling BL writing style (improvement of 28%), al-
though it is an author that resulted very difficult to
model. On the other hand, local histograms were not
very useful for identifying documents written by JM
(made it worse by ?8%). The largest improvement
(38%) of local histograms over the BOW formula-
tion was obtained for author TN (T. Nissen). This
294
result gives evidence that TN uses a similar distri-
bution of words in similar locations across the doc-
uments he writes. These results are interesting, al-
though we would like to perform a careful analysis
of results in order to determine for what type of au-
thors it would be beneficial to use local histograms,
and what type of authors are better modeled with a
standard BOW approach.
5.3 Experimental results in imbalanced data
In this section we report results with RBC and
IRBC data sets, which aim to evaluate the perfor-
mance of our methods in a realistic setting. For
these experiments we compare the performance of
the BOW, LOWBOW histogram and BOLH repre-
sentations; for the latter, we considered the best set-
ting as reported in Table 3 (i.e., an SVM with dif-
fusion kernel and k = 20). Tables 5 and 6 show
the AA performances when using word and charac-
ter information, respectively.
We first analyze the results in the RBC data set
(recall that for this data set we consider 1, 3, 5, 10,
and 50, randomly selected documents per author).
From Tables 5 and 6 we can see that BOW and
LOWBOW histogram representations obtained sim-
ilar performance to each other across the different
training set sizes, which agree with results in Table 2
for the BC data sets. The best performance across
the different configurations of the RBC data set was
obtained with the BOLH formulation (row 6 in Ta-
bles 5 and 6). The improvements of local histograms
over the BOW formulation vary across different set-
tings and when using information at word-level and
character-level. When using words (columns 2-6
in Table 5) the differences in performance are of
15.6%, 6.2%, 6.8%, 2.9%, 3.8% when using 1, 3, 5,
10 and 50 documents per author, respectively. Thus,
it is evident that local histograms are more beneficial
when less documents are considered. Here, the lack
of information is compensated by the availability of
several histograms per author.
When using character n-grams (columns 2-6 in
Table 6) the corresponding differences in perfor-
mance are of 5.4%, 6.4%, 6.4%, 6% and 11.4%,
when using 1, 3, 5, 10, and 50 documents per au-
thor, respectively. In this case, the larger improve-
ment was obtained when 50 documents per author
are available; nevertheless, one should note that re-
sults using character-level information are, in gen-
eral, significantly better than those obtained with
word-level information; hence, improvements are
expected to be smaller.
When we compare the results of the BOLH for-
mulation with the best reported results elsewhere
(c.f. last row 6 in Tables 5 and 6) (Plakias and Sta-
matatos, 2008b), we found that the improvements
range from 14% to 30.2% when using character n-
grams and from 1.2% to 26% when using words.
The differences in performance are larger when less
information is used (e.g., when 5 documents are
used for training) and we believe the differences
would be even larger if results for 1 and 3 documents
were available. These are very positive results; for
example, we can obtain almost 71% of accuracy, us-
ing local histograms of character n-grams when a
single document is available per author (recall that
we have used all of the test samples for evaluating
the performance of our methods).
We now analyze the performance of the different
methods when using the IRBC data set (columns 7-
9 in Tables 5 and 6). The same pattern as before can
be observed in experimental results for these data
sets as well: BOW and LOWBOW histograms ob-
tained comparable performance to each other and
the BOLH formulation performed the best. The
BOLH formulation outperforms state of the art ap-
proaches by a considerable margin that ranges from
10% to 27%. Again, better results were obtained
when using character n-grams for the local his-
tograms. With respect to RBC data sets, the BOLH
at the character-level resulted very robust to the re-
duction of training set size and the highly imbal-
anced data.
Summarizing, the results obtained in RBC and
IRBC data sets show that the use of local histograms
is advantageous under challenging conditions. An
SVM under the BOLH representation is less sen-
sitive to the number of training examples available
and to the imbalance of data than an SVM using
the BOW representation. Our hypothesis for this
behavior is that local histograms can be thought of
as expanding training instances, because for each
training instance in the BOW formulation we have
k?training instances under BOLH. The benefits of
such expansion become more notorious as the num-
ber of available documents per author decreases.
295
WORDS
Data set Balanced Imbalanced
Setting 1-doc 3-docs 5-docs 10-docs 50-docs 2-10 5-10 10-20
BOW 36.8% 57.1% 62.4% 69.9% 78.2% 62.3% 67.2% 71.2%
LOWBOW 37.9% 55.6% 60.5% 69.3% 77.4% 61.1% 67.4% 71.5%
Diffusion kernel 52.4% 63.3% 69.2% 72.8% 82.0% 66.6% 70.7% 74.1%
Reference - - 53.4% 67.8% 80.8% 49.2% 59.8% 63.0%
Table 5: AA accuracy in RBC (columns 2-6) and IRBC (columns 7-9) data sets when using words as terms. We report
results for the BOW, LOWBOW histogram and BOLH representations. For reference (last row), we also include the
best result reported in (Plakias and Stamatatos, 2008b), when available, for each configuration.
CHARACTER N-GRAMS
Data set Balanced Imbalanced
Setting 1-doc 3-docs 5-docs 10-docs 50-docs 2-10 5-10 10-20
BOW 65.3% 71.9% 74.2% 76.2% 75.0% 70.1% 73.4% 73.1%
LOWBOW 61.9% 71.6% 74.5% 73.8% 75.0% 70.8% 72.8% 72.1%
Diffusion kernel 70.7% 78.3% 80.6% 82.2% 86.4% 77.8% 80.5% 82.2%
Reference - - 50.4% 67.8% 76.6% 49.2% 59.8% 63.0%
Table 6: AA accuracy in the RBC and IRBC data sets when using character n-grams as terms.
6 Conclusions
We have described the use of local histograms (LH)
over character n-grams for AA. LHs are enriched
histogram representations that preserve sequential
information in documents (in terms of the positions
of terms in documents); we explored the suitabil-
ity of LHs over n-grams at the character-level for
AA. We showed evidence supporting our hypothe-
sis that LHs are very helpful for AA; we believe that
this is due to the fact that LOWBOW representations
can uncover, to some extent, the writing preferences
of authors. Our experimental results showed that
LHs outperform traditional bag-of-words formula-
tions and state of the art techniques in balanced,
imbalanced, and reduced data sets. The improve-
ments were larger in reduced and imbalanced data
sets, which is a very positive result as in real AA
applications one often faces highly imbalanced and
small sample issues. Our results are promising and
motivate further research on the use and extension
of the LOWBOW framework for related tasks (e.g.
authorship verification and plagiarism detection).
As future work we would like to explore the use
of LOWBOW representations for profile-based AA
and related tasks. Also, we would like to develop
model selection strategies for learning what combi-
nation of hyperparameters works better for modeling
each author.
Acknowledgments
We thank E. Stamatatos for making his data set
available. Also, we are grateful for the thought-
ful comments of L. A. Barro?n and those of the
anonymous reviewers. This work was partially sup-
ported by CONACYT under project grants 61335,
and CB-2009-134186, and by UAB faculty develop-
ment grant 3110841.
References
AMIDA. 2007. Augmented multi-party interaction
with distance access. Available from http://www.
amidaproject.org/, AMIDA Report.
S. Argamon and S. Levitan. 2005. Measuring the useful-
ness of function words for authorship attribution. In
Proceedings of the Joint Conference of the Association
for Computers and the Humanities and the Association
for Literary and Linguistic Computing, Victoria, BC,
Canada.
S. Canu, Y. Grandvalet, V. Guigue, and A. Rakotoma-
monjy. 2005. SVM and kernel methods Matlab tool-
box. Perception Systmes et Information, INSA de
Rouen, Rouen, France.
V. Chasanis, A. Kalogeratos, and A. Likas. 2009. Movie
segmentation into scenes and chapters using locally
weighted bag of visual words. In Proceedings of the
ACM International Conference on Image and Video
Retrieval, pages 35:1?35:7, Santorini, Fira, Greece.
ACM Press.
R. M. Coyotl-Morales, L. Villasen?or-Pineda, M. Montes-
y-Go?mez, and P. Rosso. 2006. Authorship attribu-
tion using word sequences. In Proceedings of 11th
296
Iberoamerican Congress on Pattern Recognition, vol-
ume 4225 of LNCS, pages 844?852, Cancun, Mexico.
Springer.
D. Das and A. Martins. 2007. A survey on au-
tomatic text summarization. Available from:
http://www.cs.cmu.edu/?nasmith/LS2/
das-martins.07.pdf, Literature Survey for the
Language and Statistics II course at Carnegie Mellon
University.
O. de Vel, A. Anderson, M. Corney, and G. Mohay. 2001.
Multitopic email authorship attribution forensics. In
Proceedings of the ACM Conference on Computer Se-
curity - Workshop on Data Mining for Security Appli-
cations, Philadelphia, PA, USA.
K. Grauman. 2006. Matching Sets of Features for Ef-
ficient Retrieval and Recognition. Ph.D. thesis, Mas-
sachusetts Institute of Technology.
J. Houvardas and E. Stamatatos. 2006. N-gram fea-
ture selection for author identification. In Proceedings
of the 12th International Conference on Artificial In-
telligence: Methodology, Systems, and Applications,
volume 4183 of LNCS, pages 77?86, Varna, Bulgaria.
Springer.
V. Keselj, F. Peng, N. Cercone, and C. Thomas. 2003. N-
gram-based author profiles for authorship attribution.
In Proceedings of the Pacific Association for Compu-
tational Linguistics, pages 255?264, Halifax, Canada.
M. Koppel, J. Schler, and S. Argamon. 2009. Computa-
tional methods in authorship attribution. Journal of the
American Society for Information Science and Tech-
nology, 60:9?26.
J. Lafferty and G. Lebanon. 2005. Diffusion kernels
on statistical manifolds. Journal of Machine Learning
Research, 6:129?163.
M. Lambers and C. J. Veenman. 2009. Forensic author-
ship attribution using compression distances to pro-
totypes. In Computational Forensics, Lecture Notes
in Computer Science, Volume 5718. ISBN 978-3-642-
03520-3. Springer Berlin Heidelberg, 2009, p. 13, vol-
ume 5718 of LNCS, pages 13?24. Springer.
G. Lebanon, Y. Mao, and J. Dillon. 2007. The locally
weighted bag of words framework for document rep-
resentation. Journal of Machine Learning Research,
8:2405?2441.
D. Lewis, T. Yang, and F. Rose. 2004. RCV1: A new
benchmark collection for text categorization research.
Journal of Machine Learning Research, 5:361?397.
K. Luyckx and W. Daelemans. 2010. The effect of au-
thor set size and data size in authorship attribution.
Literary and Linguistic Computing, pages 1?21, Au-
gust.
Y. Mao, J. Dillon, and G. Lebanon. 2007. Sequential
document visualization. IEEE Transactions on Visu-
alization and Computer Graphics, 13(6):1208?1215.
F. Peng, D. Shuurmans, V. Keselj, and S. Wang. 2003.
Language independent authorship attribution using
character level language models. In Proceedings of the
10th conference of the European chapter of the Associ-
ation for Computational Linguistics, volume 1, pages
267?274, Budapest, Hungary.
F. Peng, D. Shuurmans, and S. Wang. 2004. Augmenting
naive Bayes classifiers with statistical language mod-
els. Information Retrieval Journal, 7(1):317?345.
S. R. Pillay and T. Solorio. 2010. Authorship attribution
of web forum posts. In Proceedings of the eCrime Re-
searchers Summit (eCrime), 2010, pages 1?7, Dallas,
TX, USA. IEEE.
S. Plakias and E. Stamatatos. 2008a. Author identifi-
cation using a tensor space representation. In Pro-
ceedings of the 18th European Conference on Artifi-
cial Intelligence, volume 178, pages 833?834, Patras,
Greece. IOS Press.
S. Plakias and E. Stamatatos. 2008b. Tensor space mod-
els for authorship attribution. In Proceedings of the 5th
Hellenic Conference on Artificial Intelligence: Theo-
ries, Models and Applications, volume 5138 of LNCS,
pages 239?249, Syros, Greece. Springer.
R. Rifkin and A. Klautau. 2004. In defense of one-vs-all
classification. Journal of Machine Learning Research,
5:101?141.
Y. Rubner, C. Tomasi, J. Leonidas, and J. Guibas. 2001.
The earth mover?s distance as a metric for image re-
trieval. International Journal of Computer Vision,
40(2):99?121.
F. Sebastiani. 2002. Machine learning in automated text
categorization. ACM Computing Surveys, 34(1):1?47.
J. Shawe-Taylor and N. Cristianini. 2004. Kernel Meth-
ods for Pattern Analysis. Cambridge University Press.
E. Stamatatos. 2006a. Authorship attribution based on
feature set subspacing ensembles. International Jour-
nal on Artificial Intelligence Tools, 15(5):823?838.
E. Stamatatos. 2006b. Ensemble-based author identifi-
cation using character n-grams. In Proceedings of the
3rd International Workshop on Text-based Information
Retrieval, pages 41?46, Riva del Garda, Italy.
E. Stamatatos. 2009a. Intrinsic plagiarism detec-
tion using character n-gram profiles. In Proceed-
ings of the 3rd International Workshop on Uncovering
Plagiarism, Authorship, and Social Software Misuse,
PAN?09, pages 38?46, Donostia-San Sebastian, Spain.
E. Stamatatos. 2009b. A survey of modern authorship
attribution methods. Journal of the American Society
for Information Science and Technology, 60(3):538?
556.
M. Tearle, K. Taylor, and H. Demuth. 2008. An
algorithm for automated authorship attribution using
neural networks. Literary and Linguist Computing,
23(4):425?442.
297
Y. Zhao and J. Zobel. 2005. Effective and scalable au-
thorship attribution using function words. In Proceed-
ings of 2nd Asian Information Retrieval Symposium,
volume 3689 of LNCS, pages 174?189, Jeju Island,
Korea. Springer.
298
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
and the Shared Task, pages 229?233, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational Linguistics
INAOE_UPV-CORE: Extracting Word Associations from  
Document Corpora to estimate Semantic Textual Similarity 
 
 
Fernando S?nchez-Vega 
Manuel Montes-y-G?mez 
Luis Villase?or-Pineda 
Paolo Rosso 
Laboratorio de Tecnolog?as del Lenguaje, 
Instituto Nacional de Astrof?sica, ?ptica y 
Electr?nica (INAOE), Mexico. 
Natural Language Engineering Lab., ELiRF, 
Universitat Polit?cnica de Val?ncia, Spain 
prosso@dsic.upv.es 
{fer.callotl,mmontesg,villasen} 
@inaoep.mx 
 
 
Abstract 
This paper presents three methods to evaluate 
the Semantic Textual Similarity (STS). The 
first two methods do not require labeled train-
ing data; instead, they automatically extract 
semantic knowledge in the form of word asso-
ciations from a given reference corpus. Two 
kinds of word associations are considered: co-
occurrence statistics and the similarity of 
word contexts. The third method was done in 
collaboration with groups from the Universi-
ties of Paris 13, Matanzas and Alicante. It 
uses several word similarity measures as fea-
tures in order to construct an accurate predic-
tion model for the STS. 
1 Introduction 
Even with the current progress of the natural lan-
guage processing, evaluating the semantic text 
similarity is an extremely challenging task. Due to 
the existence of multiple semantic relations among 
words, the measuring of text similarity is a multi-
factorial and highly complex task (Turney, 2006). 
Despite the difficulty of this task, it remains as 
one of the most attractive research topics for the 
NLP community. This is because the evaluation of 
text similarity is commonly used as an internal 
module in many different tasks, such as, informa-
tion retrieval, question answering, document sum-
marization, etc. (Resnik, 1999). Moreover, most of 
these tasks require determining the ?semantic? 
similarity of texts showing stylistic differences or 
using polysemicwords (Hliaoutakis et al, 2006). 
The most popular approach to evaluate the se-
mantic similarity of words and texts consists in 
using the semantic knowledge expressed in ontolo-
gies (Resnik, 1999); commonly, WorldNet is used 
for this purpose (Fellbaum, 2005). Unfortunately, 
despite the great effort that has been the creation of 
WordNet, it is still far to cover all existing words 
and senses (Curran, 2003).Therefore, the semantic 
similarity methods that use this resource tend to 
reduce their applicability to a restricted domain 
and to a specific language. 
We recognize the necessity of having and using 
manually-constructed semantic-knowledge sources 
in order to get precise assessments of the semantic 
similarity of texts, but, in turn, we also consider 
that it is possible to obtain good estimations of 
these similarities using less-expensive, and perhaps 
broader, information sources. In particular our 
proposal is to automatically extract the semantic 
knowledge from large amounts of raw data sam-
ples i.e. document corpora without labels. 
In this paper we describe two different strategies 
to compute the semantic similarity of words from a 
reference corpus. The first strategy uses word co-
occurrence statistics. It determines that two words 
are associated (in meaning) if they tend to be used 
together, in the same documents or contexts. The 
second strategy measures the similarity of words 
by taking into consideration second order word co-
occurrences. It defines two words as associated if 
they are used in similar contexts (i.e., if they co-
occur with similar words). The following section 
describes the implementation of these two strate-
gies for our participation at the STS-SEM 2013 
task, as well as their combination with the meas-
ures designed by the groups from the Universities 
of Matanzas, Alicante and Paris 13. 
229
2 Participation in STS-SEM2013 
The Semantic Textual Similarity (STS) task con-
sists of estimated the value of semantic similarity 
between two texts,?1 and ?2 for now on. 
As we mentioned previously, our participation in 
the STS task of SEM 2013 considered two differ-
ent approaches that aimed to take advantage of the 
language knowledge latent in a given reference 
corpus. By applying simple statistics we obtained a 
semantic similarity measure between words, and 
then we used this semantic word similarity (SWS) 
to get a sentence level similarity estimation. We 
explored two alternatives for measuring the seman-
tic similarity of words, the first one, called 
???????? , uses the co-occurrence of words in a 
limited context1,and the second, ?????????? , com-
pares the contexts of the words using the vector 
model and cosine similarity to achieve this com-
parison. It is important to point out that using the 
vector space model directly, without any spatial 
transformation as those used by other approaches2, 
we could get greater control in the selection of the 
features used for the extraction of knowledge from 
the corpus. It is also worth mentioning that we 
applied a stemming procedure to the sentences to 
be compared as well as to all documents from the 
reference corpus. We represented the texts ?1 and 
?2 by bags of tokens, which means that our ap-
proaches did not take into account the word order.  
Following we present our baseline method, then, 
we introduce the two proposed methods as well as 
a method done in collaboration with other groups. 
The idea of this shared-method is to enhance the 
estimation of the semantic textual similarity by 
combining different and diverse strategies for 
computing word similarities. 
2.1 STS-baseline method 
Given texts?1 and ?2, their textual similarity is 
given by: 
 
??? ? ???????? = ???(??? ?1 ,?2 , ???(?2 ,?1)) 
 
where 
                                                          
1 In the experiments we considered a window (context) formed 
of 15 surrounding words. 
2Such as Latent Semantic Analysis (LSA) (Turney, 2005). 
??? ?? ,??  =
1
|??|
 1(??  ?  ?? )
?????
 
 
This measure is based on a direct matching of to-
kens. It simply counts the number of tokens from 
one text ??  that also exist in the other text ?? . Be-
cause STS is a symmetrical attribute, unlike Tex-
tual Entailment (Agirre et al, 2012), we designed 
it as a symmetric measure. We assumed that the 
relationship between both texts is at least equal to 
their smaller asymmetric similarity. 
2.2 The proposed STS methods 
These methods incorporate semantic knowledge 
extracted from a reference corpus. They aim to 
take advantage of the latent semantic knowledge 
from a large document collection. Because the 
extracted knowledge from the reference corpus is 
at word level, these methods for STS use the same 
basic ?word matching? strategy for comparing the 
sentences like the baseline method. Nevertheless, 
they allow a soft matching between words by in-
corporating information about their semantic simi-
larity. 
The following formula shows the proposed 
modification to the SIM function in order to incor-
porate information of the semantic word similarity 
(SWS). This modification allowed us not only to 
match words with exactly the same stem but also 
to link different but semantically related words. 
 
??? ?? ,??  =  ???  ???(?? , ??)
?????
 
?????
 
 
We propose two different strategies to compute 
the semantic word similarity (SWS), ????????  and 
????????? . The following subsections describe in 
detail these two strategies. 
2.2.1 STS based on word co-occurrence 
???????? uses a reference corpus to get a numeri-
cal approximation of the semantic similarity be-
tween two terms ??and ??  (when these terms have 
not the same stem). As shown in the following 
formula, ????????  takes values between 0 and 1; 
0 indicates that it does not exist any text sample in 
the corpus that contains both terms, whereas, 1 
indicates that they always occur together. 
230
 ????????  ?? , ?? =  
?? = ??                1               
?????
#(?? , ?? )
???(#(??), #(?? ))
  
 
where# ?? , ??  is the number of times that ??  and 
??  co-occur and # ??  and # ??  are the number of 
times that terms ??  and ??  occur in the reference 
corpus respectively. 
2.2.2 STS based on context similarity 
?????????? is based on the idea that two terms are 
semantically closer if they tend to be used in simi-
lar contexts. This measure uses the well-known 
vector space model and cosine similarity to com-
pare the terms? contexts. In a first step, we created 
a context vector for each term, which captures all 
the terms that appear around it in the whole refer-
ence corpus. Then, we computed the semantic 
similarity of two terms by the following formula. 
 
??????????  ?? , ?? =  
?? = ??                1               
????? ?????? ?  ? ,?  ?  
  
 
where the cosine similarity, SIMCOS, is calcu-
lated on the vectors ?  ?and ?  ? corresponding to the 
vector space model representation of terms ??  and 
?? , as indicated in the following equation: 
 
??????(?  ? ,?  ? ) =
 ??? ? ????  ? |?|
|?  ?| ? |?  ? |
 
 
It is important to point out that SIMCOS is cal-
culated on a ?predefined? vocabulary of interest; 
the appropriate selection of this vocabulary helps 
to get a better representation of terms, and, conse-
quently, a more accurate estimation of their seman-
tic similarities. 
2.3 STS based on a combination of measures 
In addition to our main methods we also developed 
a method that combines our SWS measures with 
measures proposed by other two research groups, 
namely: 
 
? LIPN (Laboratoire d'Informatique de Paris-
Nord, Universit? Paris 13, France). 
? UMCC_DLSI (Universidad de Matanzas Cami-
lo Cienfuegos, Cuba, in conjuction with the 
Departamento de Lenguajes y Sistemas In-
form?ticos, Universidad de Alicante, Spain). 
 
The main motivation for this collaboration was to 
investigate the relevance of using diverse strategies 
for computing word similarities and the effective-
ness of their combination for estimating the seman-
tic similarity of texts. 
The proposed method used a set of measures 
provided by each one of the groups. These meas-
ures were employed as features to obtained a pre-
diction model for the STS. Table 1 summarizes the 
used measures. For the generation and fitting of the 
model we used three approaches: linear regression, 
a Gaussian process and a multilayer neural net-
work. 
 
Description Team #  
Mean 
Rank 
Best 
Rank 
Based on IR measures LIPN 2 2.0 1 
Based on distance on WordNet LIPN 2 8.5 2 
STS-Context 
INAOE-
UPV 
1 4.0 4 
Complexity of the sentences 
INAOE-
UPV 
34 27.8 5 
STS-Occur 
INAOE-
UPV 
1 7.0 7 
Based on the alignment of 
particulars POS. 
UMCC_ 
DLSI 
12 40.9 18 
n-gram overlap LIPN 1 20.0 20 
Based on Edit distance 
UMCC_ 
DLSI 
4 42.6 27 
Syntactic dependencies overlap LIPN 1 29.0 29 
Levenshtein?s distance LIPN 1 42.0 42 
Named entity overlap LIPN 1 57.0 57 
Table 1. General description of the features used by the shared me-
thod. The second column indicates the source team for each group of 
features; the third column indicates the number of used features from 
each group; the last two columns show the information gain rank of 
each group of features over the training set.  
3 Implementation considerations  
The extraction of knowledge for the computation 
of the SWS was performed over the Reuters-21578 
collection. This collection was selected because it 
is a well-known corpus and also because it in-
cludes documents covering a wide range of topics. 
Due to time and space restrictions we could not 
consider all the vocabulary from the reference cor-
pus; the vocabulary selection was conducted by 
taking the best 20,000 words according to the tran-
231
sition point method (Pinto et al, 2006). This me-
thod selects the terms associated to the main topics 
of the corpus, which presumably contain more 
information for estimating the semantic similarity 
of words. We also preserved the vocabulary from 
the evaluation samples, provided they also occur in 
the reference corpus. The size of the vocabulary 
used in the experiments and the size of the corpus 
and test set vocabularies are shown in Table 2. 
 
Experiment?s  
Vocabulary 
Selected 
Vocabulary 
Ref. Corpus 
Vocabulary 
Evaluation 
Vocabulary 
 
26724 20000 31213 11491 
Table 2. Number of different stems from each of the 
considered vocabularies 
4 Evaluation and Results 
The methods proposed by our group do not require 
to be trained, i.e., they do not require tagged data, 
only a reference corpus, therefore, it was possible 
to evaluate them on the whole training set available 
this year. Table 3 shows their results on this set. 
 
Method Correlation 
STS-Baseline 0.455 
STS-Occur 0.500 
STS-Contex 0.511 
Table 3. Correlation values of the proposed methods and 
our baseline method with human judgments. 
 
Results in Table 3 show that the use of the co-
occurrence information improves the correlation 
with human judgments. It also shows that the use 
of context information further improves the results. 
One surprising finding was the competitive per-
formance of our baseline method; it is considerably 
better than the previous year?s baseline result 
(0.31). 
In order to evaluate the method done in collabo-
ration with LIPN and UMCC_DLSI, we carried 
out several experiments using the features provided 
by each group independently and in conjunction 
with the others. The experiments were performed 
over the whole training set by means of two-fold 
cross-validation. The individual and global results 
are shown in Table 4. 
As shown in Table 4, the result corresponding to 
the combination of all features clearly outper-
formed the results obtained by using each team?s 
features independently. Moreover, the best combi-
nation of features, containing selected features 
from the three teams, obtained a correlation value 
very close to last year's winner result. 
 
Featured by  Group Perdition Model Correlation 
LIPN Gaussian Process 0.587 
LIPN Lineal Regression 0.701 
LIPN Multilayer-NN 0.756 
UMCC_DLSI Gaussian Process 0.388 
UMCC_DLSI Lineal Regression 0.388 
UMCC_DLSI Multilayer-NN 0.382 
INAOE-UPV Gaussian Process 0.670 
INAOE-UPV Lineal Regression 0.674 
INAOE-UPV Multilayer-NN 0.550 
ALL Gaussian Process 0.770 
ALL Lineal Regression 0.777 
ALL Multilayer-NN 0.633 
SELECTED-SET Multilayer-NN 0.808 
LAST YEAR?S 
WINNER 
Simple 
log-linear regression 
0.823 
Table 4. Results obtained by the different subsets of 
features, from the different participating groups. 
 
4.1 Officials Runs 
For the official runs (refer to Table 5) we submit-
ted the results corresponding to the ????????  and 
??????????  methods. We also submitted a result 
from the method done in collaboration with LIPN 
and UMCC_DLSI. Due to time restrictions we 
were not able to submit the results from our best 
configuration; we submitted the results for the 
linear regression model using all the features 
(second best result from Table 4).Table 5 shows 
the results in the four evaluation sub-collections; 
Headlines comes from news headlines, OnWN  
and FNWN contain pair senses definitions from 
WordNet and other resources, finally, SMT are  
translations from automatic machine translations 
and from the reference human translations. 
As shown in Table 5, the performances of the 
two proposed methods by our group were very 
close. We hypothesize that this result could be 
caused by the use of a larger vocabulary for the 
computation of co-occurrence statistics than for the 
calculation of the context similarities. We had to 
use a smaller vocabulary for the later because its 
higher computational cost. 
Finally, Table 5 also shows that the method 
done in collaboration with the other groups ob-
232
tained our best results, confirming that using more 
information about the semantic similarity of words 
allows improving the estimation of the semantic 
similarity of texts. The advantage of this approach 
over the two proposed methods was especially 
clear on the OnWN and FNWN datasets, which 
were created upon WordNet information. Some-
how this result was predictable since several meas-
ures from this ?share-method? use WordNet 
information to compute the semantic similarity of 
words. However, this pattern was not the same for 
the other two (WordNet unrelated) datasets. In 
these other two collections, the average perfor-
mance of our two proposed methods, without using 
any expensive and manually constructed resource, 
improved by 4% the results from the share-method. 
 
Method Headlines OnWN FNWN SMT MEAN 
STS-Occur 0.639 0.324 0.271 0.349 0.433 
STS-Contex 0.639 0.326 0.266 0.345 0.431 
Collaboration 0.646 0.629 0.409 0.304 0.508 
Table 4. Correlation values from our official runs over the 
four sub-datasets.  
5 Conclusions 
The main conclusion of this experiment is that it is 
possible to extract useful knowledge from raw 
corpora for evaluating the semantic similarity of 
texts.  Other important conclusion is that the com-
bination of methods (or word semantic similarity 
measures) helps improving the accuracy of STS.  
As future work we plan to carry out a detailed 
analysis of the used measures, with the aim of de-
termining their complementariness and a better 
way for combining them. We also plan to evaluate 
the impact of the size and vocabulary richness of 
the reference corpus on the accuracy of the pro-
posed STS methods. 
Acknowledgments 
This work was done under partial support of 
CONACyT project Grants: 134186, and Scholar-
ship 224483. This work is the result of the collabo-
ration in the framework of the WIQEI IRSES 
project (Grant No. 269180) within the FP 7 Marie 
Curie. The work of the last author was in the 
framework the DIANA-APPLICATIONS-Finding 
Hidden Knowledge in Texts: Applications 
(TIN2012-38603-C02-01) project, and the 
VLC/CAMPUS Microcluster on Multimodal Inte-
raction in Intelligent Systems. We also thank the 
teams from the Universities of Paris 13, Matanzas 
and Alicante for their willingness to collaborate 
with us in this evalaution exercise. 
References  
AngelosHliaoutakis, GiannisVarelas, EpimeneidisVout-
sakis, Euripides G. M. Petrakis, EvangelosMilios, 
2006, Information Retrieval by Semantic Similarity, 
Intern. Journal on Semantic Web and Information 
Systems: Special Issue of Multimedia Semantics 
(IJSWIS), 3(3): 55?73. 
Carmen Banea, Samer Hassan, Michael Mohler and 
RadaMihalcea, 2012, UNT: A Supervised Synergistic 
Approach to Semantic Text Similarity, SEM 2012: 
The First Joint Conference on Lexical and Computa-
tional Semantics, Proceedings of the Sixth Interna-
tional Workshop on Semantic Evaluation (SemEval 
2012), Montreal, Vol. 2: 635-642. 
Christiane Fellbaum,2005, WordNet and wordnets, 
Encyclopedia of Language and Linguistics, Second 
Ed., Oxford, Elsevier: 665-670. 
David Pinto, Hector Jim?nez H. and Paolo Rosso. Clus-
tering abstracts of scientific texts using the Transi-
tion Point technique, Proc. 7th Int. Conf. on Comput. 
Linguistics and Intelligent Text Processing, CICL-
ing-2006, Springer-Verlag, LNCS(3878): 536-546. 
EnekoAgirre, Daniel Cer, Mona Diab and Aitor Gonza-
lez-Agirre, SemEval-2012 Task 6: A Pilot on Seman-
tic Textual Similarity. SEM 2012: The First Joint 
Conference on Lexical and Computational Seman-
tics, Proceedings of the Sixth International Workshop 
on Semantic Evaluation (SemEval2012), Montreal, 
Vol. 2: 386-393. 
James Richard Curran, 2003, Doctoral Thesis: From 
Distributional to Semantic Similarity, Institute for 
Communicating and Collaborative Systems, School 
of Informatics, University of Edinburgh. 
Peter D. Turney, 2005, Measuring semantic similarity 
by latent relational analysis, IJCAI'05 Proceedings of 
the 19th international joint conference on Artificial 
intelligence, Edinburgh, Scotland: 1136-1141 
Peter D. Turney, 2006, Similarity of Semantic Relations, 
Computational Linguistics, Vol. 32, No. 3: 379-416. 
Philip Resnik, 1999, Semantic Similarity in a Taxono-
my: An Information-Based Measure and its Applica-
tion to Problems of Ambiguity in Natural Language, 
Journal of Artificial Intelligence Research, Vol. 11: 
95-130. 
233
Proceedings of the 4th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 46?54,
Atlanta, Georgia, 14 June 2013. c?2013 Association for Computational Linguistics
Sexual predator detection in chats with chained classifiers
Hugo Jair Escalante
LabTL, INAOE
Luis Enrique Erro No. 1,
72840, Puebla, Mexico
hugojair@inaoep.mx
Esau? Villatoro-Tello?
Universidad Auto?noma Metropolitana
Unidad Cuajimalpa
Mexico City, Mexico
villatoroe@inaoep.mx
Antonio Jua?rez
LabTL, INAOE
Luis Enrique Erro No. 1,
72840, Puebla, Mexico
antjug@inaoep.mx
Luis Villasen?or
LabTL, INAOE
72840, Puebla, Mexico
villasen@inaoep.mx
Manuel Montes-y-Go?mez
LabTL, INAOE
72840, Puebla, Mexico
mmontesg@inaoep.mx
Abstract
This paper describes a novel approach for sex-
ual predator detection in chat conversations
based on sequences of classifiers. The pro-
posed approach divides documents into three
parts, which, we hypothesize, correspond to
the different stages that a predator employs
when approaching a child. Local classifiers
are trained for each part of the documents and
their outputs are combined by a chain strat-
egy: predictions of a local classifier are used
as extra inputs for the next local classifier.
Additionally, we propose a ring-based strat-
egy, in which the chaining process is iterated
several times, with the goal of further improv-
ing the performance of our method. We re-
port experimental results on the corpus used
in the first international competition on sex-
ual predator identification (PAN?12). Experi-
mental results show that the proposed method
outperforms a standard (global) classification
technique for the different settings we con-
sider; besides the proposed method compares
favorably with most methods evaluated in the
PAN?12 competition.
1 Introduction
Advances in communications? technologies have
made possible to any person in the world to com-
municate with any other in different ways (e.g.,
text, voice, and video) regardless of their geograph-
ical locations, as long as they have access to in-
ternet. This undoubtedly represents an important
and highly needed benefit to society. Unfortunately,
this benefit also has brought some collateral issues
?Esau? Villatoro is also external member of LabTL at
INAOE.
that affect the security of internet users, as nowa-
days we are vulnerable to many threats, including:
cyber-bullying, spam, fraud, and sexual harassment,
among others.
A particularly important concern has to do with
the protection of children that have access to inter-
net (Wolak et al, 2006). Children are vulnerable
to attacks from paedophiles, which ?groom? them.
That is, adults who meet underage victims online,
engage in sexually explicit text or video chat with
them, and eventually convince the children to meet
them in person. In fact, one out of every seven
children receives an unwanted sexual solicitation
online (Wolak et al, 2006). Hence, the detection
of cyber-sexual-offenders is a critical security issue
that challenges the field of information technologies.
This paper introduces an effective approach for
sexual predator detection (also called sexual preda-
tor identification) in chat conversations based on
chains of classifiers. The proposed approach di-
vides documents into three parts, with the hypoth-
esis that different parts correspond to the differ-
ent stages that predators adopt when approaching
a child (Michalopoulos and Mavridis, 2011). Lo-
cal classifiers are trained for each part of the doc-
uments and their outputs are combined by a chain-
ing strategy. In the chain-based approach the pre-
dictions of a local classifier are used as extra inputs
for the next local classifier. This strategy is inspired
from chain-based classifiers developed for the task
of multi-label classification (Read et al, 2011). A
ring-based approach is proposed, in which the gener-
ation of chains of classifiers is iterated several times.
We report experimental results in the corpus used in
the first international competition on sexual preda-
tor identification (PAN-2012) (Inches and Crestani,
46
2012). Experimental results show that chain-based
classifiers outperform standard classification meth-
ods for the different settings we considered. Further-
more, the proposed method compares favorably with
alternative methods developed for the same task.
2 Sexual predator detection
We focus on the detection of sexual predators in chat
rooms, among the many cyber-menaces targeting
children. This is indeed a critical problem because
most sexually-abused children have agreed volun-
tarily to met with their abuser (Wolak et al, 2006).
Therefore, anticipatively detecting when a person at-
tempts to approach a children, with malicious inten-
tions, could reduce the number of abused children.
Traditionally, a term that is used to describe mali-
cious actions with a potential aim of sexual exploita-
tion or emotional connection with a child is referred
as ?Child Grooming? or ?Grooming Attack? (Ku-
cukyilmaz et al, 2008). Defined in (Harms, 2007)
as: ?a communication process by which a perpetra-
tor applies affinity seeking strategies, while simulta-
neously engaging in sexual desensitization and in-
formation acquisition about targeted victims in or-
der to develop relationships that result in need ful-
fillment? (e.g. physical sexual molestation).
The usual approach1 to catch sexual predators is
through police officers or volunteers, whom behave
as fake children in chat rooms and provoke sexual
offenders to approach them. Unfortunately, online
sexual predators always outnumber the law enforce-
ment officers and volunteers. Therefore, tools that
can automatically detect sexual predators in chat
conversations (or at least serve as support tool for
officers) are highly needed.
A few attempts to automate processes related to
the sexual predator detection task have been pro-
posed already (Pendar, 2007; Michalopoulos and
Mavridis, 2011; RahmanMiah et al, 2011; Inches
and Crestani, 2012; Villatoro-Tello et al, 2012;
Bogdanova et al, 2013). The problem of detect-
ing conversations that potentially include a sex-
ual predator approaching a victim has been ap-
proached, for example, by (RahmanMiah et al,
2011; Villatoro-Tello et al, 2012; Bogdanova et al,
1Adopted for example by the Perverted Justice organization,
http://www.perverted-justice.com/
2013). RahmanMiah et al discriminated among
child-exploitation, adult-adult and general-chatting
conversations using a text categorization approach
and psychometric information (RahmanMiah et al,
2011). Recently, Bogdanova et al approached
the same problem, the authors concluded that stan-
dard text-mining features are useful to distinguish
general-chatting from child-exploitation conversa-
tions, but not for discriminating between child-
exploitation and adult-adult conversations (Bog-
danova et al, 2013). In the latter problem, fea-
tures that model behavior and emotion resulted par-
ticularly helpful. N. Pendar approached the prob-
lem of distinguishing predators from victims within
chat conversations previously confirmed as contain-
ing a grooming attack (Pendar, 2007). The author
collapsed all of the interventions from each partici-
pant into a document and approached the problem as
a standard text categorization task with two classes
(victim vs. predator).
A more fine grained approximation to the problem
was studied by (Michalopoulos and Mavridis, 2011).
The authors developed a probabilistic method that
classifies chat interventions into one of three classes:
1) Gaining Access: indicate predators intention to
gain access to the victim; 2) Deceptive Relationship:
indicate the deceptive relationship that the preda-
tor tries to establish with the minor, and are pre-
liminary to a sexual exploitation attack; and 3) Sex-
ual Affair: clearly indicate predator?s intention for
a sexual affair with the victim. These categories
correspond to the different stages that a sexual of-
fender adopt when approaching a child. As (Pen-
dar, 2007), (Michalopoulos and Mavridis, 2011) ap-
proached this problem as one of text categorization
(equating interventions to short-documents). They
removed stop words and applied a spelling correc-
tion strategy, their best results were obtained with a
Na??ve Bayes classifier, reaching performance close
to 96%. Thus giving evidence that the three cate-
gories can be recognized reasonably well. Which
in turn gives evidence that modeling the three stages
could be beneficial for recognizing sexual predators;
for example, when it is not known whether a con-
versation contains or not a grooming attack. This
is the underlying hypothesis behind the proposed
method. We aim to use local classifiers, special-
ized in the different stages a predator approaches a
47
child. Then, we combine the outputs of local classi-
fiers with the goal of improving the performance on
sexual predator detection in conversations including
both: grooming attacks and well-intentioned conver-
sations.
Because of the relevance of the problem, and of
the interest of several research groups from NLP,
it was organized in 2012 the first competition of
sexual predator identification (Inches and Crestani,
2012). The problem approached in the competition
was that of identifying sexual predators from con-
versations containing both: grooming attacks and
well-intentioned conversations. The organizers pro-
vided a large corpus divided into development and
evaluation data. Development (training) data were
provided to participants for building their sexual-
predator detection system. In a second stage, eval-
uation (testing) data were provided to participants,
whom had to apply their system to that data and sub-
mit their results. Organizers evaluated participants
using their predictions on evaluation data (labels for
the evaluation data were not provided to participants
during the competition).
Several research groups participated in that com-
petition, see (Inches and Crestani, 2012). Some
participants developed tailored features for detect-
ing sexual predators (see e.g., (Eriksson and Karl-
gren, 2012)), whereas other researchers focused on
the development of effective classifiers (Parapar et
al., 2012). The winning approach implemented a
two stage formulation (Villatoro-Tello et al, 2012):
in a first step suspicious conversations where iden-
tified using a two class classifier. Suspicious con-
versations are those that potentially include a sexual
predator (i.e., a similar approach to (RahmanMiah
et al, 2011)). In a second stage, sexual predators
were distinguished from victims in the suspicious
conversations identified in the first stage (a similar
approach to that of (Pendar, 2007)). For both stages
a standard classifier and a bag-of-words representa-
tion was used.
The methods proposed in this paper were eval-
uated in the corpus used in the first interna-
tional competition on sexual predator detection,
PAN?12 (Inches and Crestani, 2012). As explained
in the following sections, the proposed method uses
standard representation and classification methods,
therefore, the proposed methods can be improved if
we use tailored features or learning techniques for
sexual predator detection.
3 Chain-based classifiers for SPD
Chain-based classifiers were first proposed to deal
with multi-label classification (Read et al, 2011).
The goal was to incorporate dependencies among
different labels, which are disregarded by most
multi-label classification methods. The underlying
idea was to increase the input space of classifiers
with the outputs provided by classifiers trained for
other labels. The authors showed important im-
provements over traditional methods.
In this paper, we use chain-based classifiers to in-
corporate dependencies among local classifiers asso-
ciated to different segments of a chat conversation.
The goal is building an effective predator-detection
model made of a set of local models specialized at
classifying certain segments of the conversation. In-
tuitively, we would like to have a local model asso-
ciated to each of the stages in which a sexual preda-
tor approaches a child: gaining access, deceptive
relationship and sexual affair (Michalopoulos and
Mavridis, 2011). We associate a segment of the con-
versation to each of the three stages. The raw ap-
proach proposed in this work consists of dividing
the conversation into three segments of equal length.
The first, second and third segments of each conver-
sation are associated to the first, second and third
stages, respectively. Although, this approach is too
simple, our goal was to determine whether having
local classifiers combined via a chaining strategy
could improve the performance on sexual predator
detection.
We hypothesize that as the vocabulary used in dif-
ferent segments of the conversation is different, spe-
cialized models can result in better performance for
classifying these local segments. Since local classi-
fiers can only capture local information, it is desir-
able to somehow connect these classifiers in order to
make predictions taking into account the whole con-
versation. One way to make local classifiers depen-
dent is thought the chain-based methodology, where
the outputs of one local classifier are feed as inputs
for the next local classifier; the final prediction for
the whole conversation can be obtained in several
ways as described below.
48
The proposed approach is described in Figure 1.
Since our goal is to detect sexual predators from
chat conversations directly, we model each user
(well-intentioned user, victim or sexual predator)
by their set of interventions. Thus, we generate a
single conversation for each user using their inter-
ventions, keeping the order in which such interven-
tions happened. The approached problem is to clas-
sify these conversations into sexual-predator or any-
other-type-of-user. In the following we call sim-
ply conversations to the generated per-user conver-
sations.
Chat conversations are divided into three
(equally-spaced) parts. Next, one local-classifier
is trained for each part of the document according
to a predefined order2, where two out of the three
classifiers (second and third) are not independent.
Let p1, p2, and p3 denote the segments of text
that will be used for generating the first, second
and third classifiers. The triplet {p1, p2, p3} can
be any of the six permutations of 3 segments, this
tripled determines the order in which classifiers
will be built. Once that a particular order has been
defined, a first local-classifier, f1, is trained using
the part p1 from all of the training documents
(p1 ? {first, second, third}). Next, a second
local-classifier, f2, is trained by using the part p2
from all of the training documents. f2 is built
by using both attributes extracted from part p2 of
conversations and the outputs of the first classifier
over the training documents. Thus, classifier f2
depends on classifier f1, through the outputs of the
latter model. A third local-classifier, f3, is trained
using attributes extracted from part p3 from all
conversations, the input space for training f3 is
augmented with the predictions of classifiers f2 and
f1 over the training documents. Hence, the third
classifier depends on the outputs of the first and
second classifiers.
Once trained, the chain of local-classifiers can be
used to make predictions for the whole conversation
in different ways. When a test conversation needs to
be classified it is also split into 3 parts. Part p1 is
feeded to classifier f1, which generates a prediction
for f1. Next, part p2 from the test document, to-
2We hypothesize that building a chain of classifiers using
different orders results in different performances, we evaluate
this aspect in Section 4.
Figure 1: General diagram of the chain-based approach.
gether with the prediction for p1 as generated by f1
are feeded to classifier f2. Likewise, the outputs of
f2 and f1, together with part p3 from the document
are used as inputs for classifier f3. Clearly, since we
have predictions for the test document at the three
stages of the chain (from f1,2,3) we can make a pre-
diction at any stage. The prediction from classifier
f3 is called chain-prediction as it is the outcome of
the dependent local-classifiers.
Additionally to local and chain-prediction, we
propose a ring-like structure for chain-based classi-
fiers in which the outputs of the third local-classifier
are used again as inputs for another local model,
where the order can be different to that used in the
previous iteration. This process is iterated for a num-
ber of times, where we can make predictions at every
link (local-classifier) of the ring. In addition, after
a number of iterations we can make predictions by
combining the outputs (like in an ensemble) gener-
ated by all of the classifiers considered in the ring
up to that iteration. The underlying idea is to ex-
plore the performance of the chain as more local-
models, that can use short and long term dependen-
cies with other classifiers, are incorporated. Our hy-
pothesis is that after incorporating a certain number
of local-dependent-models, the predictions for the
whole conversations will be steady and will improve
the performance of the straight chain approach.
Algorithm 1 describes the proposed ring-based
classifier. E denotes the set of extra inputs that have
to be added to individual classifiers, which are the
cumulative outputs of individual classifiers. P is a
set of predefined permutations from which different
orders can be took from, where Pi is the ith per-
mutation. We denote with atts (pi, E) to the pro-
49
cess of extracting attributes from documents? part
pi and merging them with attributes stored in E .
atts generates the representation that a classifier
can use. train [f(X)] denotes the process of train-
ing classifier f using inputs X . Mc stores the mod-
els trained through the ring process.
Algorithm 1 Ring-based classifier.
Require: g : # iterations; P : set of permutations;
E = {}
i = 0; c = 1;
while i ? g do
i++;
{p1, p2, p3} ? Pi;
for j = 1? 3 do
X ? atts [pj , E ]
f?j ? train [fj(X)];
Mc ? f?j ;
E ? E ? f?j (pj , E);
c++;
end for
end while
return Mc : trained classifiers (ring-based approach);
When a test conversation needs to be labeled, the
set of classifiers in M are applied to it using the
same order in the parts that was used when generat-
ing the models. Each time a model is applied to the
test instance, the prediction of such model is used
to increase the input space that is to be used for the
next model. We call the prediction given by the last
model Mg, ring-prediction. One should note that,
as before, we can have predictions for the test con-
versation from every model Mi. Besides, we can
accumulate the predictions for the whole set of mod-
els M1,...,g. Another alternative is to combine the
predictions of the three individual classifiers in each
iteration of the ring (every execution of the for-loop
in Algorithm 1); this can be done, e.g., by weight
averaging. In the next section we report the perfor-
mance obtained by all these configurations.
4 Experiments and results
For the evaluation of the proposed approach we
considered the data set used in the first interna-
tional competition on sexual predator identification3
(PAN-2012) (Inches and Crestani, 2012). Table 1
3http://pan.webis.de/
presents some features from the considered data set.
The data set contains both chat conversations includ-
ing sexual predators approaching minors and (au-
thentic) conversations between users (which can or
cannot be related to a sexual topic). The data set pro-
vided by the organizers contained too much noisy in-
formation that could harm the performance of classi-
fication methods (e.g., conversations with only one
participant, conversations of a few characters long,
etc.). Therefore, we applied a preprocessing that
aimed to both remove noisy conversations and re-
ducing the data set for scalability purposes. The fil-
tering preprocessing consisted of eliminating: con-
versations with only one participant, conversations
with less than 6 interventions per each participant,
conversations that had long sequences of unrecog-
nized characters (images, apparently). The char-
acteristics of the data set after filtering are shown
within parentheses in Table 1. It can be seen that
the size of the data set was reduced considerably,
although a few sexual predators were removed, we
believe the information available from them was in-
sufficient to recognize them.
Table 1: Features of the data set considered for experi-
mentation (Inches and Crestani, 2012). We show the fea-
tures of the raw data and in parentheses the corresponding
features after applying the proposed preprocessing.
Feature Development Evaluation
# Convers. 66, 928 (6, 588) 155, 129 (15, 330)
# Users 97, 690 (11, 038) 218, 702 (25, 120)
# Sexual Pr. 148(136) 254 (222)
Conversations were represented using their bag-
of-words. We evaluated the performance of dif-
ferent representations and found that better results
were obtained with a Boolean weighting scheme.
No stop-word removal nor stemming was applied,
in fact, punctuation marks were conserved. We pro-
ceeded this way because we think in chat conver-
sations every character conveys useful information
to characterize users, victims and sexual predators.
This is because of the highly unstructured and in-
formal language used in chat conversations, as dis-
cussed in related works (Kucukyilmaz et al, 2008;
RahmanMiah et al, 2011; Rosa and Ellen, 2009).
For indexing conversations we used the TMG
toolbox (Zeimpekis and Gallopoulos, 2006). The re-
50
sultant vocabulary was of 56, 964 terms. For build-
ing classifiers we used a neural network as imple-
mented in the CLOP toolbox (Saffari and Guyon,
2006). Our choice is based on results from a prelim-
inary study.
4.1 Performance of local classifiers
We first evaluate the performance of global and local
classifiers separately. A global classifier is that gen-
erated using the content of the whole conversation, it
resembles the formulation from (Pendar, 2007). Lo-
cal classifiers were generated for each of the seg-
ments. Table 2 shows the performance of the global
and local models. We report the average (of 5 runs)
of precision, recall and F1 measure for the positive
class (sexual predators).
Table 2: Performance of global (row 2) and local classi-
fiers (rows 3-6).
Setting Precision Recall F1 Measure
Global 95.14% 49.91% 65.42%
Segment 1 96.16% 59.20% 73.23%
Segment 2 96.25% 48.82% 64.72%
Segment 3 93.43% 51.87% 66.68%
It can be seen from Table 2 that the performance
of the global model and that obtained for segments
2 and 3 are comparable to each other in terms of
the three measures we considered. Interestingly,
the best performance was obtained when the only
the first segment of the conversation was used for
classification. The difference is considerable, about
11.93% of relative improvement. This is a first con-
tribution of our work: using the first segment of a
conversation can improve the performance obtained
by a global classifier. Since the first segment of con-
versations (barely) corresponds to the gaining ac-
cess stage, the result provides evidence that sexual
predators can be detected by the way they start ap-
proaching to their victims. That is, the way a well-
intentioned person starts a conversation is somewhat
different to that of sexual predators approaching a
child. Also, it is likely that this makes a difference
because for segments 2 and 3, conversations con-
taining grooming attacks and well-intentioned con-
versations can be very similar (well-intentioned con-
versations can deal sexual thematic as well).
4.2 Chain-based classifiers
In this section we report the performance obtained
by different settings of chain based classifiers. We
first report the performance of the chain-prediction
strategy, see Section 3. Figure 2 shows the precision,
recall and F1 measure, obtained by the chain-based
classifier for the different permutations of the 3 seg-
ments (i.e., all possible orders for the segments). For
each order, we report the initial performance (that
obtained with the segment in the first order) and the
chain-prediction, that is the prediction provided by
the last classifier in the chain.
Figure 2: F1 measure by the initial and chain-based clas-
sifier for different orders.
From Figure 2 it can be observed that the chain-
prediction outperformed the initial classifier for
most of the orders in terms of F1 measure. For or-
ders starting with segment 1 (1-2-3 and 1-3-2) chain-
based classifiers worsen the initial performance.
This is due to the high performance of local clas-
sifier for segment 1 (see Table 2), which cannot be
improved with successive local classifiers. However,
the best performance overall was obtained by the
chain-based classifier with the order 2-3-1. The rela-
tive improvement of this configuration for the chain-
based method over the global classifier (the one us-
ing the whole conversations) was of 18.52%. One
should note that the second-best performance was
obtained with the order 3-2-1. Hence, putting the
most effective classifier (that for segment 1) at the
end seems to have a positive influence in the chain-
based classifier. We have shown evidence that chain-
based classifiers outperform both the global classi-
fier and any of the local methods. Also, the order of
classifiers is crucial for obtaining acceptable results
with the chain technique: using the best classifier
in the last position yields better performance; and,
putting the best classifier at the beginning would
lead the chain to worsen initial performance.
51
4.3 Ring-based classifiers
In this section we report experimental results on sex-
ual predator detection obtained with the ring-based
strategy. Recall a ring-based classifier can be seen as
a chain that is replicated several times with different
orders, so we can have predictions for each of the
local classifiers at each node of the ring/chain. Be-
sides, we can obtain periodical/cumulative predic-
tions from the chain and predictions derived from
combining predictions from a subset of local classi-
fiers in the chain. We explore the performance of all
of these strategies in the rest of this section.
We implement ring-based classifiers by succes-
sively applying chain-based classifiers with differ-
ent orders. We consider the following alternatives
for detecting predators with ring-based classifiers:
? Local. We make predictions with local classifiers
each time a local classifier is added to the ring (no
dependencies are considered). We report the av-
erage performance (segments avg.) and the maxi-
mum performance (segments max.) obtained by lo-
cal classifiers in each of the orders tried.
? Chain-prediction. We make predictions with
chain-based classifiers each time a local classifier
is added to the ring. We report the average perfor-
mance (chain-prediction avg.) and the maximum
performance (chain-prediction max.) obtained by
chain-based classifiers per each of the orders tried.
? Ensemble of chain-based classifiers. We combine
the outputs of the three chain-based classifiers built
for each order; this method is referred to as LC-
Ensemble.
? Cumulative ensemble. We combine the outputs
(via averaging) of all the chain-based classifiers that
have been built each time an order is added to the
ring; we call this method Cumulative-Ensemble.
Besides reporting results for these approaches we
also report the performance obtained by the global
classifier (Whole conversations), see Table 2.
We iterated the ring-based classifier for a fixed
number of orders. We tried 24 orders, repeating the
following process two times: we tried the permu-
tations of the 3 segments in lexicographical order,
followed by the same permutations on inverted lex-
icographical order. So a total of 24 different orders
were evaluated. Figure 3 shows the results obtained
by the different settings we consider for a typical run
of our approach.
Several findings can be drawn from Figure 3.
With exception of the average of local classifiers
(segments avg.), all of the methods outperformed
consistently the global classifier (whole conversa-
tions). Thus confirming the competitive perfor-
mance of local classifiers and that of chain-based
variants. The best local classifier from each or-
der (segments max.) achieved competitive perfor-
mance, although it was outperformed by the average
of chain-based classifiers (chain-prediction avg.).
Since local classifiers are independent, no tendency
on their performance can be observed as more orders
are tried. On the contrary, the performance chain-
based methods (as evidenced by the avg. and max
of chain-predictions) improves for the first 8-9 or-
ders and then remains steady. In fact, the best (per-
order) chain-prediction (chain-prediction max.) ob-
tained performance comparable to that obtained by
ensemble methods. One should note, however, that
in the chain-prediction max. formulation we report
the best performance from each order tried, which
might correspond to different segments in the differ-
ent orders. Therefore, it is not clear how the select
the specific order to use and the specific segment of
the chain that will be used for making predictions,
when putting in practice the method for a sexual-
predator detection system. Notwithstanding, stable
average predictions can be obtained when more than
6-8 orders are used (chain-prediction avg.), still the
performance of this approach is lower than that of
ensembles.
Clearly, the best performance was obtained
with the ensemble methods: chain-ensemble and
cumulative-ensemble. Both approaches obtained
similar performance, although the chain-ensemble
slightly outperformed cumulative-ensemble. The
chain-ensemble considers dependencies within each
order and not across orders, thus its performance af-
ter trying the 6 permutations of 3 segments did not
vary significantly. This is advantageous as only 6
orders have to be evaluated to obtain competitive
performance. Unfortunately, as with single chain-
classifiers it may be unclear how to select the par-
ticular order to use to implement a sexual-predator
detection system.
On the other hand, the cumulative-ensemble ob-
52
Figure 3: Performance of the different variants of ring-based classifiers for sexual predator detection.
tained stable performance after ? 12 orders were
considered. Recall this method incorporates depen-
dencies among the different orders tried. Although
it requires the evaluation of more orders than the
chain-ensemble to converge, this method is advanta-
geous for a real application: after a certain number
of orders it achieves steady performance, and since
it averages the outputs of all of the chain-classifiers
evaluated up to a certain iteration, its performance
does not rely on selecting a particular configuration.
In consequence, we claim the cumulative-ensemble
offers the best tradeoff between performance, stabil-
ity and model selection.
4.4 Comparison with related works
Table 3 shows a comparison of the configuration
cumulative-ensemble against the top-ranked partic-
ipants in the PAN?12 competition. We show the
performance of the top-5 participants as described
in (Inches and Crestani, 2012), additionally we re-
port the average performance obtained by the meth-
ods of the 16 participating teams. We report, F1
and F0.5 measures, and the rank for each participant.
We report F0.5 measure because that was the leading
evaluation measure for the PAN?12 competition.
From Table 3 it can be observed that the proposed
method is indeed very competitive. The results ob-
tained by our method outperformed significantly the
average performance (row 7) obtained by all of the
participants in all of the considered measures. In
terms of F1 measure our method would be ranked in
the fourth position, while in terms of the F0.5 mea-
sure our method would be ranked third.
Table 3: Comparison of the proposed method with related
works evaluated in the PAN?12 competition (Inches and
Crestani, 2012).
Participant F1 F0.5 Rk.
(Villatoro-Tello et al, 2012) 87.34 93.46 1
(Inches and Crestani, 2012) 83.18 91.68 2
(Parapar et al, 2012) 78.16 86.91 3
(Morris and Hirst, 2012) 74.58 86.52 4
(Eriksson and Karlgren, 2012) 87.48 86.38 5
(Inches and Crestani, 2012) 49.10 51.06 -
Our method 78.98 89.14 -
5 Conclusions
We introduced a novel approach to sexual-predator
detection in which documents are divided into 3
segments, which, we hypothesize, could correspond
to the different stages in that a sexual predator ap-
proaches a child. Local classifiers are built for each
of the segments, and the predictions of local classi-
fiers are combined through a strategy inspired from
chain-based classifiers. We report results on the
corpus used in the PAN?12 competition, the pro-
posed method outperforms a global approach. Re-
sults are competitive with related works evaluated in
PAN?12. Future work includes applying the chain-
based classifiers under the two-stage approach from
Villatoro et al (Villatoro-Tello et al, 2012).
Acknowledgments
This project was supported by CONACYT under
project grant 134186. The authors thank INAOE,
UAM-C and SNI for their support.
53
References
D. Bogdanova, P. Rosso, and T. Solorio. 2013. Explor-
ing high-level features for detecting cyberpedophilia.
In Special issue on on Computational Approaches
to Subjectivity, Sentiment and Social Media Analysis
(WASSA 2012), Computer Speech and Language (ac-
cepted).
G. Eriksson and J. Karlgren. 2012. Features for mod-
elling characteristics of conversations. In P. Forner,
J. Karlgren, and C. Womser-Hacker, editors, Working
notes of the CLEF 2012 Evaluation Labs and Work-
shop, Rome, Italy. CLEF.
C. Harms. 2007. Grooming: An operational definition
and coding scheme. Sex Offender Law Report, 8(1):1?
6.
G. Inches and F. Crestani. 2012. Overview of the inter-
national sexual predator identification competition at
PAN-2012. In P. Forner, J. Karlgren, and C. Womser-
Hacker, editors, Working notes of the CLEF 2012
Evaluation Labs and Workshop, Rome, Italy. CLEF.
T. Kucukyilmaz, B. Cambazoglu, C. Aykanat, and F. Can.
2008. Chat mining: predicting user and message at-
tributes in computer-mediated communication. In In-
formation Processing and Management, 44(4):1448?
1466.
D. Michalopoulos and I. Mavridis. 2011. Utilizing doc-
ument classification for grooming attack recognition.
In Proceedings of the IEEE Symposium on Computers
and Communications, pages 864?869.
C. Morris and G. Hirst. 2012. Identifying sexual preda-
tors by svm classification with lexical and behavioral
features. In P. Forner, J. Karlgren, and C. Womser-
Hacker, editors, Working notes of the CLEF 2012
Evaluation Labs and Workshop, Rome, Italy. CLEF.
J. Parapar, D. E. Losada, and A. Barreiro. 2012. A
learning-based approach for the identification of sex-
ual predators in chat logs. In P. Forner, J. Karlgren,
and C. Womser-Hacker, editors, Working notes of the
CLEF 2012 Evaluation Labs and Workshop, Rome,
Italy. CLEF.
N. Pendar. 2007. Toward spotting the pedophile telling
victim from predator in text chats. In Proceedings of
the IEEE International Conference on Semantic Com-
puting, pages 235?241, Irvine California USA.
M. W. RahmanMiah, J. Yearwood, and S. Kulkarni.
2011. Detection of child exploiting chats from a mixed
chat dataset as text classification task. In Proceed-
ings of the Australian Language Technology Associ-
ation Workshop, pages 157?165.
J. Read, B. Pfahringer, G. Holmes, and E. Frank. 2011.
Classifier chains for multi-label classification. Ma-
chine Learning Journal, 85(3):333?359.
K. D. Rosa and J. Ellen. 2009. Text classification
methodologies applied to micro-text in military chat.
In Proceedings of the eight IEEE International Con-
ference on Machine Learning and Applications, pages
710?714.
A. Saffari and I Guyon. 2006. Quick start guide for
CLOP. Technical report, Graz-UT and CLOPINET,
May.
E. Villatoro-Tello, A. Jua?rez-Gonza?lez, H. J. Escalante,
M. Montes-Y-Go?mez, and L. Villasen?or-Pineda. 2012.
A two-step approach for effective detection of mis-
behaving users in chats. In P. Forner, J. Karlgren,
and C. Womser-Hacker, editors, Working notes of the
CLEF 2012 Evaluation Labs and Workshop, Rome,
Italy. CLEF.
J. Wolak, K. Mitchell, and D. Finkelhor. 2006. On-
line victimization of youth: Five years later. Bulleting
07-06-025, National Center for Missing and Exploited
Children, Alexandia, Alexandria, VA.
D. Zeimpekis and E. Gallopoulos, 2006. Grouping Mul-
tidimensional Data: Recent Advances in Clustering,
chapter TMG: A MATLAB toolbox for generating
term-document matrices from text collections, pages
187?210. Springer.
54
Proceedings of the 2013 Workshop on Biomedical Natural Language Processing (BioNLP 2013), pages 89?97,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Exploring word class n-grams to measure
language development in children
Gabriela Ram??rez de la Rosa and Thamar Solorio
University of Alabama at Birmingham
Birmingham, AL 35294, USA
gabyrr,solorio@cis.uab.edu
Manuel Montes-y-Go?mez
INAOE
Sta. Maria Tonantzintla, Puebla, Mexico
mmontesg@ccc.inaoep.mx
Yang Liu
The University of Texas at Dallas
Richardson, TX 75080, USA
yangl@hlt.utdallas.edu
Aquiles Iglesias
Temple University
Philadelphia, PA 19140, USA
iglesias@temple.edu
Lisa Bedore and Elizabeth Pen?a
The University of Texas at Austin
Austin, TX 78712, USA
lbedore,lizp@mail.utexas.edu
Abstract
We present a set of new measures designed
to reveal latent information of language
use in children at the lexico-syntactic
level. We used these metrics to analyze
linguistic patterns in spontaneous narra-
tives from children developing typically
and children identified as having a lan-
guage impairment. We observed signif-
icant differences in the z-scores of both
populations for most of the metrics. These
findings suggest we can use these metrics
to aid in the task of language assessment
in children.
1 Introduction
The analysis of spontaneous language samples is
an important task across a variety of fields. For in-
stance, in language assessment this task can help
to extract information regarding language profi-
ciency (e.g. is the child typically developing or
language impaired). In second language acqui-
sition, language samples can help determine if
a child?s proficiency is similar to that of native
speakers.
In recent years, we have started seeing a grow-
ing interest in the exploration of NLP techniques
for the analysis of language samples in the clinical
setting. For example, Sahakian and Snyder (2012)
propose a set of linguistic measures for age pre-
diction in children that combines three traditional
measures from language assessment with a set of
five data-driven measures from language samples
of 7 children. A common theme in this emerg-
ing line of research is the study of the syntax in
those language samples. For instance, to annotate
data to be used in the study of language develop-
ment (Sagae et al, 2005), or to build models to
map utterances to their meaning, similar to what
children do during the language acquisition stage
(Kwiatkowski et al, 2012). In addition, language
samples are also used for neurological assessment,
as for example in (Roark et al, 2007; Roark et
al., 2011) where they explored features such as
Yngve and Frazier scores, together with features
derived from automated parse trees to model syn-
tactic complexity and surprisal. Similar features
are used in the classification of language samples
to discriminate between children developing typ-
ically and children suffering from autism or lan-
guage impairment (Prud?hommeaux et al, 2011).
In a similar line of research, machine learning and
features inspired by NLP have been explored for
the prediction of language status in bilingual chil-
dren (Gabani et al, 2009; Solorio et al, 2011).
More recent work has looked at the feasibility of
scoring coherence in story narratives (Hassanali et
al., 2012a) and also on the inclusion of coherence
89
as an additional feature to boost prediction accu-
racy of language status (Hassanali et al, 2012b).
The contribution of our work consists on new
metrics based on n-grams of Part of Speech (POS)
tags for assessing language development in chil-
dren that combine information at the lexical and
syntactic levels. These metrics are designed to
capture the lexical variability of specific syntac-
tic constructions and thus could help to describe
the level of language maturity in children. For in-
stance, given two lists of examples of the use of
determiner + noun: ?the dog, the frog, the tree?
and ?this dog, a frog, these trees? we want to be
able to say that the second one has more lexical
variability than the first one for that grammatical
pattern.
Our approach to compute these new metrics
does not require any special treatment on the tran-
scripts or special purpose parsers beyond a POS
tagger. On the contrary, we provide a set of mea-
sures that in addition to being easy to interpret by
practitioners, are also easy to compute.
2 Background and Motivation
To establish language proficiency, clinical re-
searchers and practitioners rely on a variety of
measures, such as number of different words,
type-token ratio, distribution of part-of-speech
tags, and mean length of sentences and words per
minute (Lu, 2012; Yoon and Bhat, 2012; Chen and
Zechner, 2011; Yang, 2011; Miller et al, 2006), to
name a few. Most of these metrics can be cate-
gorized as low-level metrics since they only con-
sider rates of different characteristics at the lexi-
cal level. These measures are helpful in the so-
lution of several problems, for example, building
automatic scoring models to evaluate non-native
speech (Chen and Zechner, 2011). They can also
be used as predictors of the rate of growth of En-
glish acquisition in specific populations, for in-
stance, in typically developing (TD) and language
impaired (LI) bilingual children (Rojas and Igle-
sias, 2012; Gutie?rrez-Clellen et al, 2012). Among
the most widely used metrics are mean length of
utterance (MLU), a measure of syntactic complex-
ity (Bedore et al, 2010), and measures of lexi-
cal productivity, such as the number of different
words (NDW) and the child?s ratio of functional
words to content words (F/C) (Sahakian and Sny-
der, 2012).
MLU, NDW, F/C and some other low-level
measures have demonstrated to be valuable in the
assessment of language ability considering that
practitioners often only need to focus on produc-
tivity, diversity of vocabulary, and sentence or-
ganization. Although useful, these metrics only
provide superficial measures of the children?s lan-
guage skills that fail to capture detailed lexico-
syntactic information. For example, in addition to
knowing that a child is able to use specific verb
forms in the right context, such as, third person
singular present tense or regular past tense, knowl-
edge about what are the most common patterns
used by a child, or how many different lexical
forms for noun + verb are present in the child?s
speech is needed because answering these ques-
tions provides more detailed information about the
status of grammatical development. To fill in this
need, we propose a set of measures that aim to cap-
ture language proficiency as a function of lexical
variability in syntactic patterns. We analyze the
information provided by our proposed metrics on
a set of spontaneous story retells and evaluate em-
pirically their potential use in language status pre-
diction.
3 Proposed measures
To present the different metrics we propose in this
study we begin with the definition of the following
concepts:
A syntactic pattern p is an n-gram of part-of-
speech tags denoted as p = ?t1 t2 ... tn?, where
ti indicates the part-of-speech tag corresponding
to the word at position i. For simplicity we use
tpi to indicate the tag at position i from pattern p.
Two examples of syntactic patterns of length two
are ?DT NN? and ?DT JJ? 1.
A lexical form f is an n-gram of words. It is de-
fined as f = ?w1 w2 ... wn?, where wi is the word
at position i. Similarly to the previous definition,
we use wfi to indicate the word at position i in a
lexical form f .
A lexical form f corresponds to a syntactic
pattern p if and only if |f | is equal to |p| and
?ktag(w
f
k ) = t
p
k, where tag() is a function that re-
turns the part-of-speech of its argument. The set of
lexical forms in a given transcript corresponding to
a syntactic pattern p is denoted by LF p. Two ex-
amples of lexical forms from the syntactic pattern
?DT NN? are ?the cat? and ?the frog?.
1We use the Penn Treebank POS tagset
90
DT the (62), a (17), all (8), no(2), that (1)
NN frog (16), boy(7), dog (6), boat (4), name (3), place (2), house (2), water (2), rabbit (2), noise (2), stick (1), tree
(1), bye(1), floor (1), um (1), baby (1), forest (1), room (1), foot (1), rock (1), squirrel (1), back (1), rabb (1),
card (1), one (1), present (1), dress (1), box (1), family (1)
VBD saw (7), dropped (4), said (4), started (4), looked (3), kicked (3), called (3), found (2), took (2), got (2), jumped
(2), heard (2), thought (1), turned (1), fell (1), waked (1), stood (1), wa (1), touched (1), told (1), scared (1), tur
(1), haded (1), opened (1), shh (1)
DT NN the frog (3), the dog (2), the place (2), the water (2), the boat (2), a noise (2), the forest (1), the rock (1), a tree
(1), a present (1), a um (1), the card (1), the box (1), the rabb (1), the floor (1), the back (1), no one (1)
DT VBD all started (2), all heard (1)
Table 1: Example of 5 syntactic patterns with their lists of lexical forms and the number of repetitions
of each of them. This information corresponds to an excerpt of an example transcript. DT is the part-of-
speech tag for determiner, NN for noun, and VBD for verb in past tense.
The bag-of-words associated to a syntactic pat-
tern p is denoted as W p. This set is composed
of all the words from the lexical forms that corre-
spond to the syntactic pattern p. It is formally de-
fined as follows: W p = {w|w ? f, f ? LF p}.
For example, the bag-of-words of the syntactic
pattern ?DT NN? with lexical forms ?the cat? and
?the frog? is {the, cat, frog}.
Table 1 shows five syntactic patterns of a tran-
script?s fragment. For each syntactic pattern in the
transcript we show the list of its lexical forms and
their frequency. We will use this example in the
description of the measures in the following sub-
sections.
3.1 Number of different lexical forms
(NDLF)
Analogous to the number of different words
(NDW), where words in the transcript are consid-
ered atomic units, we propose a metric where the
atomic units are lexical forms. Then, we measure
the number of different lexical forms used for each
syntactic pattern in the transcript. Formally, given
a syntactic pattern p and its set of lexical forms
LF p, the number of different lexical forms is com-
puted as follows:
NDLF(p) = |LF p| (1)
This measure gives information about the num-
ber of different ways the child can combine words
in order to construct a fragment of a speech that
corresponds to a specific grammatical pattern. Re-
search in language assessment has shown that
when children are in the early acquisition stages
of certain grammatical constructions they will use
the patterns as ?fixed expressions?. As children
master these constructions they are able to use
these grammatical devices in different contexts,
but also with different surface forms. Thereby, we
could use this measure to discriminate the syntac-
tic patterns the child has better command of from
those that might still be problematic and used in-
frequently or with a limited combination of sur-
face forms. For example, from the information
on Table 1 we see that NDLF(DT NN) = 17, and
NDLF(DT VBD) = 2. This seems to indicate that
the child has a better command of the grammatical
construction determiner + noun (DT NN) and can
thus produce more different lexical forms of this
pattern than determiner + verb (DT + VBD). But
also, we may use this measure to identify rare pat-
terns, that are unlikely to be found in a typically
developing population.
3.2 Lexical forms distribution (LFdist)
Following the idea of lexical forms as atomic
units, NDLF allows to know the different lexical
forms present in the transcripts. But we do not
know the distribution of use of each lexical form
for a specific syntactic pattern. In other words,
NDLF tells us the different surface forms observed
for each syntactic pattern, but it does not measure
the frequency of use of each of these lexical forms,
nor whether each of these forms are used at similar
rates. We propose to use LFdist to provide infor-
mation about the distribution of use for LF p, the
set of lexical forms observed for the syntactic pat-
tern p. We believe that uniform distributions can
be indicative of syntactic structures that the child
has mastered, while uneven distributions can re-
veal structures that the child has only memorized
(i.e. the child uses a fixed and small set of lex-
ical forms). To measure this distribution we use
the entropy of each syntactic pattern. In particu-
lar, given a syntactic pattern p and its set of lexical
forms LF p, the lexical form distribution is com-
puted as follows:
91
LFdist(p) = ?
?
fi?LF p
prob(fi) log prob(fi)
(2)
where
prob(fi) =
count(fi)
?
fk?LF p count(fk)
(3)
and count() is a function that returns the fre-
quency of its argument. Larger values of LFdist
indicate a greater difficulty in the prediction of
the lexical form that is being used under a spe-
cific grammatical pattern. For instance, in the ex-
ample of Table 1, LFdist(DT VBD) = 0.91 and
LFdist(DT NN) = 3.97. This indicates that the
distribution in the use of lexical forms for deter-
miner + noun is more uniform than the use of
lexical forms for determiner + verb, which im-
plies that for determiner + verb there are some
lexical forms that are more frequently used than
others2. Syntactic patterns with small values of
LFdist could flag grammatical constructions the
child does not feel comfortable manipulating and
thus might still be in the acquisition stage of lan-
guage learning.
3.3 Lexical variation (LEX)
Until now we are considering lexical forms as
atomic units. This could lead to overestimating
the real lexical richness in the sample, in particu-
lar for syntactic patterns of length greater than 1.
To illustrate this consider the syntactic pattern p =
?DT NN? and suppose we have the following set
of lexical forms for p = {?the frog?, ?a frog?, ?a
dog?, ?the dog?}. The value for NDLF (p) = 4.
But how many of these eight words are in fact dif-
ferent? That is the type of distinction we want to
make with the next proposed measure: LEX, that
is also an adaptation of type-token ratio (Lu, 2012)
used in the area of communication disorders but
computed over each grammatical pattern. For this
example, we want to be able to find that the lex-
ical variation of ?DT NN? is 0.5 (because there
are only four different words out of eight). For-
mally, given a syntactic pattern p, its set of lexical
forms LF p, and the bag-of-words W p, the lexical
variation is defined as shown in Equation 4.
2We recognize that this is an oversimplification of the en-
tropy measure since the number of outcomes will most likely
be different for each syntactic pattern.
LEX(p) =
|W p|
|LF p| ? n
(4)
Note that |LF p| = NDLF(p), and n is the
length of the syntactic pattern p. In Table 1 the lex-
ical variation of the pattern ?determiner + noun?
(DT+NN) is equal to 0.58 ( 2017?2 ), and for deter-
miner + verb (DT+VBD) is equal to 0.75 ( 32?2 ).
That means 58% of total words used under the pat-
tern ?DT+NN? are different, in comparison with
the 75% for ?DT+VBD?. In general, the closer the
value of LEX is to 1, there is less overlap between
the words in the lexical forms for that pattern.
Our hypothesis behind this measure is that for the
same syntactic pattern TD children may have less
overlap of words than children with LI, e.g. less
overlap indicates the use of a more diverse set of
words.
3.4 Lexical use of syntactic knowledge
(LexSyn)
With LEX we hope to accomplish the character-
ization of lexical richness of syntactic patterns
assuming that each part-of-speech has a similar
number of possible lexical forms. We assume as
well that less overlap in the words used for the
same grammatical pattern represents a more devel-
oped language than that with more overlap. How-
ever the definition of LEX overlooks a well known
fact about language: different word classes have
a different range of possibilities as their lexical
forms. Consider open class items, such as nouns
and verbs, where the lexicon is large and keeps
growing. In contrast, closed class items, such as
prepositions and determiners are fixed and have a
very small number of lexical forms. Therefore it
seems unfair to assign equal weight to the overlap
of words for these different classes. To account
for this phenomenon, we propose a new measure
that includes the information about the syntactic
knowledge that the child shows for each part of
speech. That is, we weigh the level of overlap
for specific grammatical constructions according
to the lexicon for the specific word classes in-
volved. Since we limit our analysis to the language
sample at hand, we define the ceiling of the lexi-
cal richness of a specific word class to be the to-
tal number of different surface forms found in the
transcript. In particular, given a syntactic pattern
p = ?t1 t2 ... tn?, with its set of lexical forms
LF p, the lexical use of syntactic knowledge is de-
fined as:
92
LexSyn(p) =
1
n
n?
i=1
|wfi |f ? LF
p|
NDLF(tpi )
(5)
where the numerator is the size of the set of
words in the i-th position in all the lexical forms.
Note that this measure does not make sense for
syntactic patterns of length < 2. Instead, syn-
tactic patterns of length 1 were used to identify
the syntactic knowledge of the child by using the
NDLF of each POS in p. In the example of Ta-
ble 1, LexSyn(DT NN) = 0.59. This value corre-
sponds to the sum of the number of different de-
terminers used in position 1 for LF p divided by
the total number of different determiners that this
child produced in the sample (for this case, the
number of determiners that this child produced is
given by NDLF(DT), that is 5), plus the number
of different nouns used under this syntactic pat-
tern over the total number of nouns produced by
the child (NDLF(NN)=29). The complete calcula-
tion of LexSyn(DT NN) = 12 ?(
3
5+
17
29) = 0.59.
This contrasts with the value of LexSyn for the pat-
tern ?determiner + verb?, LexSyn(DT VBD) =
1
2 ? (
1
5 +
2
25) = 0.14 that seems to indicate that the
child has more experience combining determiners
and nouns than determiners and verbs. Perhaps
this child has had limited exposure to other pat-
terns combining determiner and verb, or this pat-
tern is at a less mature stage in the linguistic reper-
toire of the child.
Children with LI tend to exhibit a less devel-
oped command of syntax than their TD cohorts.
Syntactic patterns with large values of LexSyn
show a high versatility in the use of those syntactic
patterns. However, since the syntactic reference is
taken from the same child, this versatility is rela-
tive only to what is observed in that single tran-
script. For instance, suppose that the total num-
ber of different determiners observed in the child?s
transcript is 1. Then any time the child uses that
determiner in a syntactic pattern, the knowledge of
this class, according to our metric, will be 100%,
which is correct, but this might not be enough to
determine if the syntactic knowledge of the child
for this grammatical class corresponds to age ex-
pectations for a typically developing child. In or-
der to improve the measurement of the lexical use
of syntactic knowledge we propose the measure
LexSynEx, that instead of using the information
of the same child to define the coverage of use for
a specific word class, it uses the information ob-
served for a held out set of transcripts from TD
children. This variation allows the option of mov-
ing the point of reference to a specific cohort, ac-
cording to what is needed.
4 Data set
The data used in this research is part of an ongoing
study of language impairment in Spanish-English
speaking children (Pen?a et al, 2003). From this
study we used a set of 175 children with a mean
age of about 70 months. Language status of these
children was determined via expert judgment by
three bilingual certified speech-language pathol-
ogists. At the end of the data collection period,
the experts reviewed child records in both lan-
guages including language samples, tests proto-
cols, and parent and teacher questionnaire data.
They made independent judgments about chil-
dren?s lexical, morphosyntactic, and narrative per-
formance in each language. Finally, they made an
overall judgment about children?s language abil-
ity using a 6 point scale (severely language im-
paired to above normal impairment). If at least two
examiners rated children?s language ability with
mild, moderate or severe impairment they were as-
signed to the LI group. Percent agreement among
the three examiners was 90%. As a result of this
process, 20 children were identified by the clinical
researchers as having LI, while the remaining 155
were identified as typically developing (TD).
The transcripts were gathered following stan-
dard procedures for collection of spontaneous lan-
guage samples in the field of communication dis-
orders. Using a wordless picture book, the chil-
dren were asked to narrate the story. The two
books used were ?A boy, a dog, and a frog? (Mayer,
1967) and ?Frog, where are you?? (Mayer, 1969).
For each child in the sample, 4 transcripts of story
narratives were collected, 2 in each language. In
this study we use only the transcripts where En-
glish was the target language.
5 Procedure
The purpose of the following analysis is to inves-
tigate the different aspects in the child?s language
that can be revealed by the proposed metrics. All
our measures are based on POS tags. We used the
Charniak parser (Charniak, 2000) to generate the
POS tags of the transcripts. For all the results re-
ported here we removed the utterances from the
interrogators and use all utterances by the chil-
93
dren. From the 155 TD instances, we randomly se-
lected 20, that together with the 20 instances with
LI form the test set. The remaining 135 TD in-
stances were used as the normative population, our
training set.
After the POS tagging process, we extracted the
set of syntactic patterns with length equal to 1, 2, 3
and 4 that appear in at least 80% of the transcripts
in the training set. The 80% threshold was chosen
with the goal of preserving the content that is most
likely to represent the TD population.
6 Analysis of the proposed measures and
implications
Figure 1 shows 5 plots corresponding to each of
our proposed measures. Each graph shows a com-
parison between the average values of the TD and
the LI populations. The x-axis in the graphs rep-
resents all the syntactic patterns gathered from the
training set that appeared on the test data, and the
y-axis represents the difference in the z-score val-
ues of each measure from the test set. The x-axis
is sorted in descending order according to the z-
score differences between values of TD and LI.
The most relevant discovery is that NDFL,
LFdist, LexSyn and LexSynEx show a wider gap
in the z-scores between the TD and LI popula-
tions for most of the syntactic patterns analyzed.
This difference is easy to note visually as most of
the TD patterns tend to have larger values, while
the ones for children with LI have lower scores.
Therefore, it seems our measures are indeed cap-
turing relevant information that characterizes the
language of the TD population.
Analyzing LEX from Figure 1, we see that most
of the LEX values are positive, for both TD and
LI instances, and we cannot observe marked dif-
ferences between them. That might be a con-
sequence of assuming all word classes can have
an equivalent number of different lexical forms.
Once we weigh each POS tag in the pattern by the
word forms the child has used (as in LexSyn and
LexSynEx), noticeable differences across the two
groups emerge. When we include syntactic knowl-
edge of a group of children (as in LexSynEx), those
similarities disappear. This behavior highlights the
need for a combined lexico-syntactic measure that
can describe latent information about language us-
age in children.
For building an intervention plan that helps to
improve child language skills, practitioners could
LFdist
verb (3rd person singular present)
verb (past tense) + personal pronoun
personal pronoun + auxiliary verb + adverb
verb (gerund)
NDLF
there + auxiliary verb
personal pronoun + auxiliary verb + adverb
adjective + noun
verb (3rd person singular present)
LexSyn
verb (past tense) + personal pronoun
personal pronoun + verb (past tense) + personal pronoun
personal pronoun + auxiliary verb + adverb
there + auxiliary verb
LexSynEx
personal pronoun + auxiliary verb + adverb
personal pronoun + verb (past tense) + personal pronoun
verb (past tense) + personal pronoun
there + auxiliary verb
Table 2: List of syntactic patterns with the biggest
difference between LI and TD in 4 measures:
LFdist, NDLF, and LexSyn and LexSynEx.
use the knowledge of specific grammatical con-
structions that need to be emphasized ?those that
seem to be problematic for the LI group. These
structures can be identified by pulling the syntac-
tic patterns with the largest difference in z-scores
from the TD population. Table 2 shows a list of
syntactic patterns with small values for LI and the
largest differences between LI and TD instances
in the test set. As the table indicates, most of the
syntactic patterns have length greater than 1. This
is not surprising since we aimed for developing
measures of higher-order analysis that can com-
plement the level of information provided by com-
monly used metrics in language assessment (as in
the case of MLU, NDW or F/C). The table also
shows that while each measure identifies a differ-
ent subset of syntactic patterns as relevant, some
syntactic patterns emerge in all the metrics. For
instance, personal pronoun + auxiliary verb + ad-
verb and there + auxiliary verb. This repetition
highlights the importance of those grammatical
constructions. But the differences also show that
the metrics complement each other. In general,
the syntactic patterns in the list represent complex
grammatical constructions where children with LI
are showing a less advanced command of language
use.
Table 3 shows some statistics about the lexical
forms present under pronoun + verb (3rd person
singular present) + verb (gerund or present par-
ticiple) (PP VBZ VBG) in all our data set. The last
94
-0.6
-0.4
-0.2
 0
 0.2
 0.4
 0.6
 0.8
 1
 1.2
z-s
cor
e
Syntactic patterns
Difference (TD-LI)
(a) NDLF
-0.6
-0.4
-0.2
 0
 0.2
 0.4
 0.6
 0.8
 1
 1.2
z-s
cor
e
Syntactic patterns
Difference (TD-LI)
(b) LFdist
-0.6
-0.4
-0.2
 0
 0.2
 0.4
 0.6
 0.8
 1
 1.2
z-s
cor
e
Syntactic patterns
Difference (TD-LI)
(c) LEX
-0.6
-0.4
-0.2
 0
 0.2
 0.4
 0.6
 0.8
 1
 1.2
z-s
cor
e
Syntactic patterns
Difference (TD-LI)
(d) LexSyn
-0.6
-0.4
-0.2
 0
 0.2
 0.4
 0.6
 0.8
 1
 1.2
z-s
cor
e
Syntactic patterns
Difference (TD-LI)
(e) LexSynEx
Figure 1: Performance comparison of the proposed measures for the TD and LI groups. Each data point
represents the difference in z-scores between the average values of the TD and LI instances in the test
set.
row in that table presents an example of the lexi-
cal forms used by two children. Note that for the
child with LI, there is only one lexical form: he is
touching. On the other hand, the TD child is using
the grammatical pattern with six different surface
forms. Clinical practitioners can take this infor-
mation and design language tasks that emphasize
the use of ?PP VBZ VBG? constructions.
6.1 Analysis of correlations among measures
To analyze the level of overlap between our mea-
sures we computed correlation coefficients among
them. The results are shown in Table 4.
The results from the correlation analysis are not
that surprising. They show that closely related
measures are highly to moderately correlated. For
instance, LEX and eLEX have a correlation of
TD LI
number of PP 6 5
number of VBZ 3 2
number of VBG 7 4
Example (instances: she is putting he is touching
td-0156 and li-3022) she is going
he is pushing
she is looking
she is carrying
she is playing
Table 3: Statistics of the surface forms for the
grammatical pattern PP VBZ VBG.
0.69, and LexSynEx and LexSyn have a correla-
tion of 0.61. NDLF and LFdist showed a posi-
tive correlation score of 0.81. This high correla-
tion hints to the fact that as the number of lexical
forms increases, so does the gap between their fre-
95
LFdist NDLF LEX eLEX LexSyn LexSynEx
LFdist 1.00
NDLF 0.81 1.00
LEX -0.53 -0.31 1.00
eLEX -0.54 -0.43 0.69 1.00
LexSyn 0.07 0.02 -0.23 -0.10 1.00
LexSynEx -0.02 -0.03 -0.08 -0.03 0.61 1.00
Table 4: Correlation matrix for the proposed metrics.
quency of use. While this may be a common phe-
nomenon of language use, it does not have a neg-
ative effect since the same effect will be observed
in both groups of children and we care to see the
differences in performance between a TD and an
LI population.
For all other pairs of measures, the correlation
scores were in the range of [?0.5, 0.1]. It was in-
teresting to note that LexSyn showed the lowest
correlation with the rest of the measures (between
[?0.11, 0.01]).
Correlation coefficients between our metrics
and MLU, NDW, and F/C were computed sepa-
rately for syntactic patterns of different lengths.
However all the different matrices showed the
same correlation patterns. We found a high cor-
relation between MLU and NDW, but low cor-
relation with all our proposed measures, except
for one case: NDW and LexSyn seemed to be
highly correlated (?-0.7). Interestingly, we noted
that despite the high correlation between MLU and
NDW, MLU and LexSyn showed weak correlation
(?-0.4). Overall, the findings from this analysis
support the use of our metrics as complimentary
measures for child language assessment.
7 Conclusions and future work
We proposed a set of new measures that were de-
veloped to characterize the lexico-syntactic vari-
ability of child language. Each measure aims to
find information that is not captured by traditional
measures used in communication disorders.
Our study is still preliminary in nature and re-
quires an in depth evaluation and analysis with a
larger pool of subjects. However the results pre-
sented are encouraging. The set of experiments
we discussed showed that TD and LI children have
significant differences in performance according
to our metrics and thus these metrics can be used to
enrich models of language trajectories in child lan-
guage acquisition. Another potential use of met-
rics similar to those proposed here is the design of
targeted intervention practices.
The scripts to compute the metrics as described
in this paper are available to the research commu-
nity by contacting the authors. However, the sim-
plicity of the metrics makes it easy for anyone to
implement, and it certainly makes it easy for clin-
ical researchers to interpret.
Our proposed metrics are a contribution to the
set of already known metrics for language assess-
ment. The goal of these new metrics is not to
replace existing ones, but to complement what is
already available with concise information about
higher-order syntactic constructions in the reper-
toire of TD children.
We are interested in evaluating the use of our
metrics in a longitudinal study. We believe they
are a promising framework to represent language
acquisition trajectories.
Acknowledgments
This research was partially funded by NSF under
awards 1018124 and 1017190. The first author
also received partial funding from CONACyT.
References
Lisa M. Bedore, Elizabeth D. Pen?a, Ronald B. Gillam,
and Tsung-Han Ho. 2010. Language sample mea-
sures and language ability in Spanish-English bilin-
gual kindergarteners. Journal of Communication
Disorders, 43:498?510.
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In Proceedings of the 1st North
American chapter of the Association for Computa-
tional Linguistics conference, NAACL 2000, pages
132?139, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Miao Chen and Klaus Zechner. 2011. Computing
and evaluating syntactic complexity features for au-
tomated scoring of spontaneous non-native speech.
In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies - Volume 1, HLT ?11, pages
722?731, Stroudsburg, PA, USA. Association for
Computational Linguistics.
96
Keyur Gabani, Melissa Sherman, Thamar Solorio,
Yang Liu, Lisa M. Bedore, and Elizabeth D. Pen?a.
2009. A corpus-based approach for the prediction
of language impairment in monolingual English and
Spanish-English bilingual children. In Proceedings
of Human Language Technologies: The 2009 An-
nual Conference of the North American Chapter
of the Association for Computational Linguistics,
NAACL ?09, pages 46?55, Stroudsburg, PA, USA.
Association for Computational Linguistics.
V. Gutie?rrez-Clellen, G. Simon-Cereijido, and
M. Sweet. 2012. Predictors of second language
acquisition in Latino children with specific language
impairment. American Journal of Speech Language
Pathology, 21(1):64?77.
Khairun-nisa Hassanali, Yang Liu, and Thamar
Solorio. 2012a. Coherence in child language narra-
tives: A case study of annotation and automatic pre-
diction of coherence. In Proceedings of 3rd Work-
shop on Child, Computer and Interaction (WOCCI
2012).
Khairun-nisa Hassanali, Yang Liu, and Thamar
Solorio. 2012b. Evaluating NLP features for auto-
matic prediction of language impairment using child
speech transcripts. In Interspeech.
Tom Kwiatkowski, Sharon Goldwater, Luke Zettel-
moyer, and Mark Steedman. 2012. A probabilis-
tic model of syntactic and semantic acquisition from
child-directed utterances and their meanings. In
Proceedings of the 13th Conference of the European
Chapter of the Association for Computational Lin-
guistics, pages 234?244, Avignon, France. Associa-
tion for Computational Linguistics.
Xiaofei Lu. 2012. The relationship of lexical richness
to the quality of ESL learners? oral narratives. The
Modern Language Journal, 96(2):190?208.
Mercer Mayer. 1967. A boy, a dog, and a frog. Dial
Press.
Mercer Mayer. 1969. Frog, where are you? Dial
Press.
Jon F. Miller, John Heilmann, Ann Nockerts, Aquiles
Iglesias, Leah Fabiano, and David J. Francis. 2006.
Oral language and reading in bilingual children.
Learning Disabilities Research and Practice, 21:30?
43.
Elizabeth D. Pen?a, Lisa M. Bedore, Ronald B. Gillam,
and Thomas Bohman. 2003. Diagnostic markers
of language impairment in bilingual children. Grant
awarded by the NIDCH, NIH.
Emily T. Prud?hommeaux, Brian Roark, Lois M.
Black, and Jan van Santen. 2011. Classification of
atypical language in autism. In Proceedings of the
2nd Workshop on Cognitive Modeling and Compu-
tational Linguistics, pages 88?96, Portland, Oregon,
USA, June. Association for Computational Linguis-
tics.
Brian Roark, Margaret Mitchell, and Kristy Holling-
shead. 2007. Syntactic complexity measures for
detecting mild cognitive impairment. In Biologi-
cal, translational, and clinical language processing,
pages 1?8, Prague, Czech Republic, June. Associa-
tion for Computational Linguistics.
Brian Roark, Margaret Mitchell, John-Paul Hosom,
Kristy Hollingshead, and Jeffrey Kaye. 2011. Spo-
ken language derived measures for detecting mild
cognitive impairment. IEEE Transcations on Au-
dio, Speech, and Language Processing, 19(7):2081?
2090, September.
Rau?l Rojas and Aquiles Iglesias. 2012. The language
growth of Spanish-speaking English language learn-
ers. Child Development.
Kenji Sagae, Alon Lavie, and Brian MacWhinney.
2005. Automatic measurement of syntactic devel-
opment in child language. In Proceedings of the
43rd Annual Meeting of the Association for Com-
putational Linguistics, ACL ?05, pages 197?204,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Sam Sahakian and Benjamin Snyder. 2012. Automat-
ically learning measures of child language develop-
ment. In ACL, pages 95?99. The Association for
Computational Linguistics.
Thamar Solorio, Melissa Sherman, Y. Liu, Lisa
Bedore, Elizabeth Pen?a, and A. Iglesias. 2011. An-
alyzing language samples of Spanish-English bilin-
gual children for the automated prediction of lan-
guage dominance. Natural Language Engineering,
pages 367?395.
Charles Yang. 2011. A statistical test for grammar.
In Proceedings of the 2nd Workshop on Cognitive
Modeling and Computational Linguistics, pages 30?
38, Portland, Oregon, USA, June. Association for
Computational Linguistics.
Su-Youn Yoon and Suma Bhat. 2012. Assessment of
ESL learners? syntactic competence based on sim-
ilarity measures. In EMNLP-CoNLL, pages 600?
608. Association for Computational Linguistics.
97
