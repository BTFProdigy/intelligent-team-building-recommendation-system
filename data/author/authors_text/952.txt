Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 89?96
Manchester, August 2008
Are Morpho-Syntactic Features More Predictive for
the Resolution of Noun Phrase Coordination Ambiguity
than Lexico-Semantic Similarity Scores?
Ekaterina Buyko and Udo Hahn
Jena University
Language & Information Engineering (JULIE) Lab
Fu?rstengraben 30, 07743 Jena, Germany
ekaterina.buyko|udo.hahn@uni-jena.de
Abstract
Coordinations in noun phrases often pose
the problem that elliptified parts have to
be reconstructed for proper semantic inter-
pretation. Unfortunately, the detection of
coordinated heads and identification of el-
liptified elements notoriously lead to am-
biguous reconstruction alternatives. While
linguistic intuition suggests that semantic
criteria might play an important, if not su-
perior, role in disambiguating resolution
alternatives, our experiments on the re-
annotated WSJ part of the Penn Treebank
indicate that solely morpho-syntactic crite-
ria are more predictive than solely lexico-
semantic ones. We also found that the
combination of both criteria does not yield
any substantial improvement.
1 Introduction
Looking at noun phrases such as
?cat and dog owner?
?novels and travel books?
their proper coordination reading (and asymmetric
distribution of coordinated heads) as
?cat owner? AND ?dog owner?
?novels? AND ?travel books?
seems to be licensed by the striking semantic sim-
ilarity between ?cat? and ?dog?, and ?novels? and
?books?, respectively. If this were a general rule,
then automatic procedures for the resolution of co-
ordination ambiguities had to rely on the a priori
provision of potentially large amounts of semantic
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
background knowledge to make this similarity ex-
plicit. Furthermore, any changes in languages or
domains where such resources were missing (or,
were incomplete) would severely hamper coordi-
nation analysis.
Indeed, previous research has gathered lot of
evidence that conjoined elements tend to be se-
mantically similar. The important role of seman-
tic similarity criteria for properly sorting out con-
juncts was first tested by Resnik (1999). He in-
troduced an information-content-based similarity
measure that uses WORDNET (Fellbaum, 1998) as
a lexico-semantic resource and came up with the
claim that semantic similarity is helpful to achieve
higher coverage in coordination resolution for co-
ordinated noun phrases of the form ?noun1 and
noun2 noun3? than similarity measures based on
morphological information only.
In a similar vein, Hogan (2007b) inspected
WORDNET similarity and relatedness measures
and investigated their role in conjunct identifi-
cation. Her data reveals that several measures
of semantic word similarity can indeed detect
conjunct similarity. For the majority of these
similarity measures, the differences between the
mean similarity of coordinated elements and non-
coordinated ones were statistically significant.
However, it also became evident that these were
only slight differences, and not all coordinated
heads were semantically related as evidenced, e.g.,
by ?work?/?harmony? in ?hard work and harmony?.
The significance tests did also not reveal particu-
larly useful measures for conjunct identification.
Rus et al (2002) in an earlier study presented an
alternative heuristics-based approach to conjunct
identification for coordinations of the form ?noun1
and noun2 noun3?. They exploit, e.g., look-ups
in WORDNET for a compound noun as a con-
89
cept, and for the sibling relation between nouns in
the coordination and report bracketing precision of
87.4% on 525 candidate coordinations. Although
the authors demonstrated that WORDNET was re-
ally helpful in coordination resolution, the eval-
uation was only conducted on compound nouns
extracted from WORDNET?s noun hierachy and,
furthermore, the senses of nouns were manually
tagged in advance for the experiments.
Despite this preference for semantic criteria, one
might still raise the question how far non-semantic
criteria might guide the resolution of noun phrase
coordination ambiguities, e.g., by means of the dis-
tribution of resolution alternatives in a large corpus
or plain lexical or morpho-syntactic criteria. This
idea has already been explored before by various
researchers from different methodological angles
including distribution-based statistical approaches
(e.g., Chantree et al (2005), Nakov and Hearst
(2005)), similarity-based approaches incorporat-
ing orthographical, morpho-syntactic, and syntac-
tic similarity criteria (e.g., Agarwal and Boggess
(1992), Okumura and Muraki (1994)), as well as
a combination of distribution information and syn-
tactic criteria (Hogan, 2007a).
Statistical approaches enumerate all candidate
conjuncts and calculate the respective likelihood
according to a distribution estimated on a cor-
pus. For the coordination ?movie and television
industry? the distributional similarity of ?movie?
and ?industry? and the collocation frequencies of
the pairs [?movie? - ?industry?] and [?television? -
?industry?] would be compared against each other.
However, for such an approach only an F-measure
under 50% was reported (Chantree et al, 2005).
Unsupervised Web-distribution-based algorithms
(Nakov and Hearst, 2005) achieved 80% on the
disambiguation of coordinations of the fixed form
?noun1 and noun2 noun3?. Hogan (2007a) pre-
sented a method for the disambiguation of noun
phrase coordination by modelling two sources of
information, viz. distribution-based similarity be-
tween conjuncts and the dependency between con-
junct heads. This method was incorporated in
Bikel?s parsing model (Bikel, 2004) and achieved
an increase in NP coordination dependency F-
score from 69.9% to 73.8%.
Similarity-based approaches consider those el-
ements of a coordination as conjuncts which are
most ?similar? under syntactic, morphological, or
even semantic aspects. Agarwal and Boggess
(1992) include in their NP coordination analysis
syntactic and some semantic information about
candidate conjuncts and achieve an accuracy boost
up to 82%. Okumura and Muraki (1994) estimate
the similarity of candidate conjuncts by means of
a similarity function which incorporates syntactic,
orthographical, and semantic information about
the conjuncts. The model provides about 75% ac-
curacy.
The resolution of coordination ambiguity can
also be tried at parsing time. Charniak and John-
son (2005), e.g., supply a discriminative reranker
that uses e.g., features to capture syntactic paral-
lelism across conjuncts. The reranker achieves an
F-score of 91%.
Recently, discriminative learning-based ap-
proaches were proposed, which exploit only lex-
ical, morpho-syntactic features and the symmetry
of conjuncts. Shimbo and Hara (2007) incorpo-
rate morpho-syntactic and symmetry features in
a discriminative learning model and end up with
57% F-measure on the GENIA corpus (Ohta et al,
2002). Buyko et al (2007) employ Conditional
Random Fields (Lafferty et al, 2001) and success-
fully tested this technique in the biomedical do-
main for the identification and resolution of ellipti-
fied conjuncts. They evaluate on the GENIA corpus
and report an F-score of 93% for the reconstruc-
tion of the elliptical conjuncts employing lexical
and morpho-syntactic criteria only. At least two
questions remain ? whether the latter approach
can achieve similar results in the newswire lan-
guage domain (and is thus portable), and whether
the incorporation of additional semantic criteria in
this approach might boost the resolution rate, or
not (and is thus possibly more parsimonious). The
latter question is the main problem we deal with in
this paper.
2 Data Sets for the Experiments
2.1 Coordination Annotation in the PENN
TREEBANK
For our experiments, we used the WSJ part of the
PENN TREEBANK (Marcus et al, 1993). Some re-
searchers (e.g., Hogan (2007a)) had recently found
several inconsistencies in its annotation of the
bracketing of coordinations in NPs. These bugs
were shown to pose problems for training and test-
ing of coordination resolution and parsing tools.
Fortunately, a re-annotated version has been pro-
vided by Vadas and Curran (2007), with a focus
90
on the internal structure of NPs. They added addi-
tional bracketing annotation for each noun phrase
in the WSJ section of the PENN TREEBANK as-
suming a right-bracketing structure in NPs. In ad-
dition, they introduced tags, e.g., ?NML? for ex-
plicitly marking any left-branching constituents as
in
(NP (NML (JJ industrial) (CC and) (NN food))
(NNS goods))
where ?industrial? and ?food? are conjuncts. In the
example
(NP (DT some) (NN food) (CC and) (NN house-
hold) (NNS goods))
the structure of the noun phrase is already cor-
rect and should not be annotated further, since
?household goods? is already right-most and is co-
ordinated with ?food?. Still, in the original PENN
TREEBANK annotation, we find annotations of
noun phrases such as
(NP (NN royalty) (CC and) (NP (NN rock)
(NNS stars)))
that remain unchanged after the re-annotation pro-
cess.
2.2 Coordination Corpus
We, first, extracted a set of 3,333 non-nested NP
coordinations involving noun compounds and one
conjunction, with a maximal number of nine nouns
(no prepositional phrases were considered). We
focused on two patterns in the re-annotated WSJ
portion:
(1) Noun phrases containing at least two nouns and
a conjunction as sister nodes as in
(NP (NML (NN movie) (CC and) (NN book))
(NNS pirates))
or in
(NP (DT some) (NN food) (CC and) (NN house-
hold) (NNS goods))
(2) Noun phrases containing at least two noun
phrases and a conjunction as sister nodes (as
they remained unchanged from the original PENN
TREEBANK version). Thereby, the second noun
phrase contains at least two nouns as sister nodes
as in
(NP (NP (NNP France)) (CC and) (NP (NNP
Hong) (NNP Kong)))
We removed from this original set NPs which
could not be reduced to the following pattern:1
1These are typically coordinations of the form ?(W )N1 and
N2?, e.g., ?government sources and lobbyists?, where W is a
sequence of i tokens (i ? 0). 646 coordinations of this type
occurred in the WSJ portion of the PTB.
(W ) N1 and (W ) N2 N3,
where (W ) is a sequence of i tokens with i ? 0 as
in ?street lampsN1 and ficusN2 treesN3?.
The remaining major data set (A) then contained
2,687 NP coordinations. A second data set (B)
was formed, which is a proper subset of A and
contained only those coordination structures that
match the following pattern:
(X) N1 and (W ) N2 N3,
where (X) is defined as a sequence of i tokens
(i ? 0) with all part-of-speech (POS) tags except
nouns and (W ) defined as above; e.g., ?a happy
catN1 and dogN2 ownerN3?. Test set B contains,
in our opinion, a selection of less ?hard? coordi-
nations from the set A, and includes 1,560 items.
All these patterns focus on three forms of con-
junctions, namely ?and?, ?or?, and ?but not?, which
connect two conjuncts (the extension of which
varies in our data from one up to maximally eight
tokens as in ?London?s ?Big Bang? 1986 deregu-
lation and Toronto?s ?Little Bang? the same year?.
The remainders from the conjunctions and the
two conjuncts in a coordinated NP are called
shared elements (e.g., ?owner? and ?a happy? in
the above example). It is evident that the correct
recognition of conjunct boundaries allows for the
proper identification of the shared elements.
Set A contains 1,455 coordinations where N1
and N3 are coordinated (e.g, ?food and household
goods?) and 1,232 coordinations where N1 and N2
are coordinated (e.g., ?cotton and acetate fibers?).
Set B consists of 643 coordinations where N1 and
N3 are coordinated and 917 coordinations where
N1 and N2 are coordinated.
The extracted data sets were converted into an
IO representation of tokens labeled as ?C? for con-
junct, ?CC? for conjunction, and ?S? for the shared
element(s). The noun phrase ?cotton and acetate
fibers?, e.g., is represented as a sequence ?C CC
C S?, while ?food and household goods? is repre-
sented as a sequence ?C CC C C?.
3 Methods
We here compare three different approaches to the
resolution of noun phrase coordination ambiguity,
viz. ones relying solely on morpho-syntactic infor-
mation, solely on lexico-semantic information, and
a cumulative combination of both. As far as se-
mantic information is concerned we make use of
various WORDNET similarity measures.
91
3.1 Baselines
We used three baselines for resolving noun phrase
coordination ambiguities ? one incorporating
only lexico-semantic information, the WordNet
Similarity baseline, and two alternative ones in-
corporating only morpho-syntactic and syntactic
parse information, the Number Agreement and the
Bikel Parser baseline, respectively.
3.1.1 WORDNET Similarity (WN) Baseline
Our lexico-semantic baseline comes with
WORDNET semantic similarity scores of puta-
tively coordinated nouns. For our experiments,
we used the implementation of WORDNET simi-
larity and relatedness measures provided by Ted
Pedersen.2 The following similarity measures
were considered: two measures based on path
lenghts between concepts (path and lch (Leacock
et al, 1998)), three measures based on informa-
tion content, i.e., corpus-based measures of the
specificity of a concept (res (Resnik, 1999), lin
(Lin, 1998), and jcn (Jiang and Conrath, 1997)).
Furthermore, we used two relatedness measures,
namely, lesk (Banerjee and Pedersen, 2003) and
vector (Patwardhan et al, 2003), which score the
similarity of the glosses of both concepts. We
applied these similarity measures to any pair of
putatively coordinated nouns in the noun phrases
from our data sets, A and B. To determine poten-
tial conjuncts we calculate two similarity scores
relative to the structures discussed in Section 2.2:
s1 = sim(N1,N2) and s2 = sim(N1,N3)
Our final score is the maximum over both scores
which is then the semantic indicator for the most
plausible resolution of the coordination.
3.1.2 Number Agreement (NA) Baseline
We compared here the number agreement
between selected nouns (see Resnik (1999)).
Accordingly, N1 and N2 are coordinated, if
number(N1) = number(N2) AND number(N1) 6=
number(N3), while N1 and N3 are coordinated, if
number(N1) = number(N3) AND number(N1) 6=
number(N2).
3.1.3 Post-Processing Heuristics
In the WN and NA baselines, after the detection
of coordinated elements we used simple heuris-
tics to tag the remaining part of the noun phrase.
If N1 and N2 were hypothesized to be coordi-
nated, then all tokens preceding N1 were tagged as
2http://www.d.umn.edu/?tpederse/
shared elements, N3 was tagged as shared element
as well, while all tokens between the conjunction
and N2 were tagged as conjuncts. For example,
in ?a happy dogN1 and catN2 ownerN3? we identify
?dog? and ?cat? as coordinated elements and tag ?a
happy? and ?owner? as shared elements. The final
resolution looks like ?S S C CC C S?. If N1 and N3
were hypothesized to be coordinated, then all other
elements except conjunctions were tagged as parts
of conjuncts, as well.
3.1.4 Bikel Parser (BP) Baseline
We used the well-known Bikel Parser (Bikel,
2004) in its original version and the one used by
Collins (2003). We trained both of them only
with NPs extracted from the re-annotated version
of WSJ (see Section 2) and converted the bracket-
ing output of the parsers to the IO representation
for NP coordinations for further evaluations.
3.2 Chunking of Conjuncts with CRFs
The approach to conjunct identification presented
by Buyko et al (2007) employs Conditional Ran-
dom Fields (CRF) (Lafferty et al, 2001),3 which
assign a label to each token of coordinated NPs
according to its function in the coordination: ?C?
for conjuncts, ?CC? for conjunctions, and ?S? for
shared elements. Since non-nested conjuncts can
be assumed to be in a sequential order, sequen-
tial learning approaches (instead of single position
classification approaches) seem appropriate here.
Buyko et al (2007) report an F-measure of 93%
on conjunct identification in the GENIA corpus.
They use a feature set including lexical (words),
and morpho-syntactic features (POS tags, morpho-
syntactic similarity of putative conjuncts), but ex-
clude any semantic criteria. The morpho-syntactic
similarity features were generated from a rule-
based approach to conjunct identification using the
maximal symmetry of conjuncts as constituted by
their respective POS annotation.
We here intend to apply this approach for resolv-
ing coordination ambiguities involving noun com-
pounds in the newswire language such as ?presi-
dent and chief executive?. This restricts the spec-
trum of considered coordinations in noun phrases
to more complicated cases than those considered
by Buyko et al (2007). We will thus test the vari-
ous resolution models under harder test conditions,
3They employ the linear-chain CRF implementation from
the MALLET toolsuite available at http://mallet.cs.
umass.edu/index.php/Main_Page
92
Feature Class Description
default feature prior probability distribution over the
possible argument labels
lexical word
morpho-
syntactic
the token?s POS tag; output labels of the
morpho-syntactic similarity (?C?,?CC?
and ?S?) (see Buyko et al (2007)); out-
put labels of the number agreement
baseline (?C?, ?CC? and ?S?)
semantic WN output labels of the WORDNET similar-
ity baseline (?C?, ?CC? and ?S?)
contextual conjunctions of all features of neighbor-
ing tokens (two tokens to the left and
one token to the right)
Table 1: Feature Classes Used for Conjunct Iden-
tification
since Buyko et al consider, e.g., adjective coordi-
nations in noun phrases such as ?positive and neg-
ative IL-2 regulator? that are predominant in the
biomedical language domain.
We also propose in this work an extension of the
feature space in terms of lexico-semantic features
(see Table 1), information that originates from sim-
ilarity computations on WORDNET data. Further-
more, we do not use orthographical features of the
original approach as they are well suited only for
the biomedical language domain.
4 Results and Error Analysis
To evaluate the different approaches to conjunct
identification, we used recall and precision scores
since they are well suited for the evaluation of seg-
mentation tasks. Two types of decisions were eval-
uated ? the assignment of ?C? labels denoting con-
juncts in terms of the F-measure, and (given the
tagged conjuncts) the accuracy of the complete co-
ordination resolution. A coordination is resolved
properly only, if all tokens of both conjuncts are
correctly identified.
We carried out a ten-fold cross-validation of all
ML-based methods (Bikel parser (Bikel, 2004) and
CRF-based conjunct identification (Buyko et al,
2007)). For the evaluation of the NA and WN base-
lines, we tested their performance on the complete
data sets, A and B (see Section 2).
As Table 2 depicts the NA baseline achieved an
accuracy of 28.4% on A (36.6% on B), the Bikel
parser reached 77.2% on A (73.4% in B), while
the WN baseline got in its best run (vector mea-
sure) an accuracy of 41.7% on A (49.6% on B).
These results already reveal that parsing almost
dramatically outperforms the coordination resolu-
tion based on the NA similarity by up to 35.5%
points. The results of the WN baseline indicate
that the best similarity measure for conjunct iden-
tification is the vector similarity (Patwardhan et
al., 2003) that scores the similarity between the
glosses of the concepts.
Our error analysis of the WN baseline on the
test set A reveals that its low accuracy has var-
ious reasons. First, about 37% of the coordina-
tions could not be resolved due to the absence of at
least one noun involved in the coordination from
the WORDNET. These coordinations usually in-
clude named entities such as person and organi-
zation names (e.g., ?brothers Francis and Gilbert
Gros?). These coverage gaps have clearly a nega-
tive effect on the resolution results for all WORD-
NET similarity measures.
To find out errors which are specific for the
considered similarity measures, we have chosen
the res measure and inspected the analysis results
on all noun phrases where nouns are covered by
WORDNET. The remaining set of coordinations
contains 1,740 noun phrases. 1,022 coordinations
(59%) of this set were completely resolved by the
WN baseline, while 1,117 coordinations (64% of
the remaining part, 41.5% of the test set A) could
be at least partly resolved. Obviously, the coordi-
nated heads are properly detected by the res mea-
sure but our heuristics for tagging the remaining
modifiers (see Subsection 3.1.3) fail to provide the
correct conjunct boundaries.
623 coordinations (36%) were mis-classified by
the res measure. A closer look at this data re-
veals two types of errors. The first and minor
type is the misleading selection of putatively co-
ordinated heads N1, N2, and N3. We presuppose
in the WN baseline that the heads appear right-
most in the noun phrase, although that is not al-
ways the case as illustrated by the phrase ?North-
ern California earthquake and Hurricane Hugo?.
The res measure detected correctly a higher sim-
ilarity between ?earthquake? (N1) and ?hurricane?
(N2), but ?Hugo? (N3) is a modifier of ?hurricane?.
Although the res measure works fine, the coordi-
nation cannot be properly resolved due to syntac-
tic reasons. In some cases, N2 is wrongly selected
as in ?life and health insurance operation? where
the WN baseline selects ?insurance? as right-most
noun (except the last noun ?operation?) and not
?health?.4
4
?turbineN2 ? is, however, correctly selected in ?steam tur-
bine and gas turbine plants?.
93
Set A Set B
Recall/Precision/F-Score Accuracy Recall/Precision/F-Score Accuracy
NA 32.7 / 75.9 / 45.7 28.4 41.6 / 83.9 / 55.6 36.6
Bikel 85.6 / 85.4 / 85.5 77.2 83.8 / 83.6 / 83.7 73.4
Bikel (Collins) 85.9 / 85.7 / 85.8 77.5 83.6 83.4 / 83.5 72.9
WN jcn 45.6 / 69.3 / 55.0 36.2 54.7 / 72.1 / 62.2 41.2
WN lch 48.7 / 70.8 / 57.7 39.2 57.8 / 74.1 / 65.0 44.4
WN lesk 49.2 / 66.2 / 56.5 38.1 59.3 / 70.3 / 64.3 43.3
WN lin 44.7 / 69.9 / 54.6 35.5 53.5 / 72.6 / 61.6 40.5
WN res 45.9 / 71.8 / 56.0 37.4 55.7 / 75.8 / 64.2 43.8
WN path 48.7 / 70.8 / 57.7 39.2 57.8 / 74.1 / 65.0 44.4
WN vector 51.2 / 68.9 / 58.8 41.7 62.8 / 74.5 / 68.1 49.6
CRF (default), contextual 75.2 / 72.4 / 73.8 60.3 77.9 / 75.1 / 76.5 63.1
+ Lexical, morpho-syntactic 87.1 / 87.2 / 87.1 77.9 88.1/ 88.0 / 88.0 78.8
+ WN (lesk) 87.2 / 87.2 / 87.2 78.0 88.2/ 88.2 / 88.2 79.1
CRF (default), contextual + only WN (lesk) 79.3 / 78.4 / 78.9 64.8 81.2 / 80.6 / 80.9 66.9
CRF (default), contextual + only morpho-
syntactic
86.2 / 86.3 / 86.3 76.6 87.6 / 87.6 / 87.6 78.1
Table 2: F-measure of Conjunct Identification and Accuracy of Coordination Resolution on the WSJ
Section of the PENN TREEBANK Corpus
The second type of error comes as erroneous
classifications of the res measure such as in ?hos-
pitals and blood banks? where ?hospitals? and
?blood? have a higher similarity than ?hospitals?
and ?banks? although they are, in fact, coordi-
nated here. ?hotels and large restaurant chains?,
?records and music publishing?, ?chemicals and
textiles company? are other examples for the obser-
vation that the coordinated elements have a lower
similarity as non-coordinated ones.
We also carried out a ten-fold cross-validation of
the CRF-based approach for the conjunct identifi-
cation. First of all, the CRF-based approach (with
and without WN similarity) achieved the highest
accuracy score ? up to 78.0% on set A, and 79.1%
on B ? compared with all other approaches we
scrutinized on. We also tested the performance of
the original semantics-free approach and the ad-
ditional effects of the WORDNET similarity mea-
sures (see Table 2). Although the integration of se-
mantic information leads to a mild gain compared
with the original approach (up to 0.3% points, with
the lesk measure), the results indicate that no sub-
stantial benefit can be traced to semantic features.
We ran several tests with solely morpho-
syntactic features (as enumerated in Table 1) and
solely WN features, too. They reveal that solely
morpho-syntactic features are up to 11.8% points
more predictive than WN features. The best re-
sults were still achieved using the gloss-oriented
lesk measure (see Table 3).
The inspection of the errors types from the var-
ious runs is not fully conclusive though. After
adding WN features to both sets, we detected some
improvements for conjunct tagging with high WN
similarity. Some conjunct boundaries could be cor-
rected as in ?record and movie producer? where,
in the first run, ?producer? was tagged as a con-
junct and was corrected as being shared by inte-
grating WN features. But we also detected a de-
grading tagging behavior of conjuncts with WN
features where the WN similarity was not helpful
at all as in ?chairman and chief designer? where
?chairman? and ?chief? under the influence of WN
features were judged to be conjuncts. We found
out that the addition of WN features positively in-
fluences the classification of coordinations where
N1 and N2 are coordinated, while it increased er-
rors in the classification of coordinations where N1
and N3 are coordinated.
In addition, we calculated intersections between
the set A error data (unique) of the res WN base-
line (1400 phrases) and the error data of the CRF
approach without WN features (391), and the er-
ror data of the CRF approach with WN features
(385), respectively. These error data sets con-
tain noun phrases where coordinated heads could
not be properly detected. The set of the res WN
baseline and the set of the CRF approach with-
out WN features have an intersection of 230 in-
stances, where 138 instances could not be found
in the WORDNET. That means that for about 161
instances (59%) in the mis-classified data of the
CRF approach the additional WN features would
not be helpful. The intersection remains similar
(226 instances) between the set of the res WN base-
94
Default, Context, WN Recall Accu-
Lexical, Morpho- Sim Precision / racy
Syntactic Feats. F-Score
? 88.1/ 88.0 / 88.0 78.8
? jcn 87.9/ 87.8 / 87.9 78.3
? lch 87.9 / 87.9 / 87.9 78.5
? lesk 88.2/ 88.2 / 88.2 79.1
? lin 88.0 / 88.0 / 88.0 78.8
? res 88.2 / 88.2 / 88.2 79.0
? path 87.9 / 87.9 / 87.9 78.5
? vector 87.8 / 87.7 / 87.7 78.2
Table 3: Conjunct Identification ? Cross-
validation on the WSJ section of the PENN TREE-
BANK Corpus on Test Set B
line and the set of the CRF approach enriched with
WN features. The intersection between the errror
sets of the both CRF approaches includes 352 in-
stances. The integration of the WN features was
not helful for almost the complete error data from
the original CRF approach. We have previously
shown that the res WN baseline features correlate
with the correct label sequence for only 1,117 co-
ordinations (41.5%) of the complete evaluation set
A and the features thus do not seem to be effective
in our approach.
Furthermore, we evaluated the results of the
CRF approach only for the correct detection of
coordinated heads (see above for the res measure
and intersection counts) and disregarded the mod-
ifier classification. The results ? 85.3% on set A
and 85.4% on set B ? reveal that the classification
of modifiers is a major source of classification er-
rors. In both configurations the problematic noun
phrases are the ones with (e.g., adjectival) modi-
fiers. The boundaries of conjuncts are not properly
recognized in such noun phrases, as for example in
?American comic book and television series? where
the correct label sequence is ?S C C CC C S?, since
?American? is the shared modifier of ?book? and
?television?, while ?comic? just modifies ?book?.
As most adjectives appearing at the beginning
of the noun phrase as in ?medical products and
services company? tend to be used as shared modi-
fiers of coordinations in our data, this, erroneously,
leads to false taggings, e.g., ?personal? in ?per-
sonal computer and software design? as a shared
element. To cope adequately with modifiers we
need to integrate more appropriate features such
as collocation frequencies of modifiers and coor-
dinated heads. The detection of a higher collo-
cation frequency of ?personal computer? in com-
parison to ?personal software? (e.g., using the pro-
cedures proposed by Wermter and Hahn (2004))
would help tagging the conjunct boundaries.
5 Conclusions and Future Work
We investigated the problem of noun phrase co-
ordination resolution as a segmentation problem
among conjuncts involved in the coordination.
While resolving coordination ellipsis is often con-
sidered as a semantically constrained problem, we
wanted to assess a less ?costly? solution strategy,
namely relying on ?cheaper? to get syntactic crite-
ria as much as possible, though not sacrificing the
accurary of resolutions.
We, first looked at morpho-syntactic criteria
only and lexico-semantic criteria only, and then at
the combination of both approaches. The evalu-
ation results from a variety of experiments reveal
that the major part of ambiguous coordinations
can be resolved using solely morpho-syntactic fea-
tures. Surprising as it might be, the semantic in-
formation as derived from the WORDNET sim-
ilarity measures does not yield any further sub-
stantial improvement for our approach. This is
somehow counter-intuitive, but our findings, un-
like those from earlier studies which emphasized
the role of semantic criteria, are based on exten-
sive corpus data ? the PENN TREEBANK.
Results from our error analysis will guide future
work to further boost results. Particular empha-
sis will be laid on the integration of named en-
tity recognizers, collocation frequencies and dis-
tributional similarity data as also advocated by
Chantree et al (2005).
The presented sequential labeling-based ap-
proach to coordination resolution was here ap-
plied to the resolution of a special type of ambigu-
ous noun phrases. In general, this approach can
easily be applied to the resolution of other types
of coordinative structures in noun phrases as al-
ready presented in Buyko et al (2007). As far as
other phrasal types (e.g., verbal phrases) are con-
cerned, long-distance coordinations play a much
more prominent role. The token-based labeling ap-
proach may be thus substituted by a chunk-based
approach operating on sentences.
Acknowledgements
This research was partly funded by the German
Ministry of Education and Research within the
STEMNET project (01DS001A-C) and by the EC
within the BOOTSTREP project (FP6-028099).
95
References
Agarwal, R. and L. Boggess. 1992. A simple but
useful approach to conjunct identification. In Pro-
ceedings of the 30th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 15?21.
Newark, DE, USA, 28 June - 2 July 1992.
Banerjee, S. and T. Pedersen. 2003. Extended gloss
overlaps as a measure of semantic relatedness. In IJ-
CAI?03 ? Proceedings of the 18th International Joint
Conference on Artificial Intelligence, pages 805?
810. Acapulco, Mexico, August 9-15, 2003.
Bikel, D. 2004. Intricacies of Collins? parsing model.
Computational Linguistics, 30(4):479?511.
Buyko, E., K. Tomanek, and U. Hahn. 2007. Reso-
lution of coordination ellipses in biological named
entities using Conditional Random Fields. In PAC-
LING 2007 - Proceedings of the 10th Conference of
the Pacific Association for Computational Linguis-
tics, pages 163?171. Melbourne, Australia, Septem-
ber 19-21, 2007.
Chantree, F. A. Kilgarriff, A. de Roeck, and A. Willis.
2005. Disambiguating coordinations using word dis-
tribution information. In RANLP 2005 ? Proceed-
ings of the Intl. Conference on ?Recent Advances
in Natural Language Processing?, pages 144?151.
Borovets, Bulgaria, 21-23 September, 2005.
Charniak, E. and M. Johnson. 2005. Coarse-to-fine
n-best parsing and MaxEnt discriminative reranking.
In ACL?05 ? Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguistics,
pages 173?180. Ann Arbor, MI, 25-30 June 2005.
Collins, M. 2003. Head-driven statistical models for
natural language parsing. Computational Linguis-
tics, 29(4):589?637.
Fellbaum, C., editor. 1998. WORDNET: An Electronic
Lexical Database. MIT Press.
Hogan, D. 2007a. Coordinate noun phrase disam-
biguation in a generative parsing model. In ACL?07
? Proceedings of the 45th Annual Meeting of the As-
sociation of Computational Linguistics, pages 680?
687. Prague, Czech Republic, June 28-29, 2007.
Hogan, D. 2007b. Empirical measurements of lexi-
cal similarity in noun phrase conjuncts. In Proceed-
ings of the 45th Annual Meeting of the Association
of Computational Linguistics. Demo and Poster Ses-
sions, pages 149?152. Prague, Czech Republic, June
28-29, 2007.
Jiang, J. and D. Conrath. 1997. Semantic similar-
ity based on corpus statistics and lexical taxonomy.
In ROCLING-X ? Proceedings of the 1997 Inter-
national Conference on Research in Computational
Linguistics. Taipei, Taiwan, August 22-24, 1997.
Lafferty, J., A. McCallum, and F. Pereira. 2001. Condi-
tional Random Fields: Probabilistic models for seg-
menting and labeling sequence data. In ICML-2001
? Proceedings of the 18th International Conference
on Machine Learning, pages 282?289. Williams
College, MA, USA, June 28 - July 1, 2001.
Leacock, C., M. Chodorow, and G. Miller. 1998.
Using corpus statistics and WORDNET relations
for sense identification. Computational Linguistics,
24(1):147?165.
Lin, D. 1998. An information-theoretic definition of
similarity. In Proceedings of the 15th International
Conference on Machine Learning, pages 296?304.
Madison, WI, USA, July 24-27, 1998.
Marcus, M., B. Santorini, and M.-A. Marcinkiewicz.
1993. Building a large annotated corpus of English:
The PENN TREEBANK. Computational Linguistics,
19(2):313?330.
Nakov, P. and M. Hearst. 2005. Using the Web as an
implicit training set: Application to structural ambi-
guity resolution. In HLT-EMNLP?05 ? Proceedings
of the 5th Human Language Technology Conference
and 2005 Conference on Empirical Methods in Nat-
ural Language Processing, pages 835?842. Vancou-
ver, B.C., Canada, October 6-8, 2005.
Ohta, T., Y. Tateisi, and J.-D. Kim. 2002. The GE-
NIA corpus: An annotated research abstract corpus
in molecular biology domain. In HLT 2002 ? Pro-
ceedings of the 2nd International Conference on Hu-
man Language Technology Research, pages 82?86.
San Diego, CA, USA, March 24-27, 2002.
Okumura, A. and K. Muraki. 1994. Symmetric pattern
matching analysis for English coordinate structures.
In ANLP 1994 ? Proceedings of the 4th Conference
on Applied Natural Language Processing, pages 41?
46. Stuttgart, Germany, 13-15 October 1994.
Patwardhan, S., S. Banerjee, and T. Pedersen. 2003.
Using measures of semantic relatedness for word
sense disambiguation. In CICLing 2003 ? Proceed-
ings 4th Intl. Conference on Computational Linguis-
tics and Intelligent Text Processing, pages 241?257.
Mexico City, Mexico, February 16-22, 2003.
Resnik, P. 1999. Semantic similarity in a taxonomy:
An information-based measure and its application to
problems of ambiguity in natural language. Journal
of Artificial Intelligence Research, 11:95?130.
Rus, V., D. Moldovan, and O. Bolohan. 2002. Bracket-
ing compound nouns for logic form derivation. In
FLAIRS 2002 ? Proceedings of the 15th Interna-
tional Florida Artificial Intelligence Research Soci-
ety Conference, pages 198?202. Pensacola Beach,
FL, USA, May 14-16, 2002.
Shimbo, M. and K. Hara. 2007. A discriminative learn-
ing model for coordinate conjunctions. In EMNLP-
CoNLL 2007 ? Proceedings of the 2007 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning, pages 610?619. Prague,Czech Republic,
June 28-29, 2007.
Vadas, D. and J. Curran. 2007. Adding noun phrase
structure to the PENN TREEBANK. In ACL?07 ? Pro-
ceedings of the 45th Annual Meeting of the Associ-
ation of Computational Linguistics, pages 240?247.
Prague, Czech Republic, June 28-29, 2007.
Wermter, J. and U. Hahn. 2004. Collocation extraction
based on modifiability statistics. In COLING 2004 ?
Proceedings of the 20th International Conference on
Computational Linguistics, pages 980?986. Geneva,
Switzerland, August 23-27, 2004.
96
Proceedings of the Linguistic Annotation Workshop, pages 33?40,
Prague, June 2007. c?2007 Association for Computational Linguistics
An Annotation Type System for a Data-Driven NLP Pipeline
Udo Hahn Ekaterina Buyko Katrin Tomanek
Jena University Language & Information Engineering (JULIE) Lab
Fu?rstengraben 30, 07743 Jena, Germany
{hahn|buyko|tomanek}@coling-uni-jena.de
Scott Piao John McNaught Yoshimasa Tsuruoka Sophia Ananiadou
NaCTeM and School of Computer Science
University of Manchester
{scott.piao|john.mcnaught|yoshimasa.tsuruoka|sophia.ananiadou}@manchester.ac.uk
Abstract
We introduce an annotation type system for
a data-driven NLP core system. The specifi-
cations cover formal document structure and
document meta information, as well as the
linguistic levels of morphology, syntax and
semantics. The type system is embedded in
the framework of the Unstructured Informa-
tion Management Architecture (UIMA).
1 Introduction
With the maturation of language technology, soft-
ware engineering issues such as re-usability, in-
teroperability, or portability are getting more and
more attention. As dozens of stand-alone compo-
nents such as tokenizers, stemmers, lemmatizers,
chunkers, parsers, etc. are made accessible in vari-
ous NLP software libraries and repositories the idea
sounds intriguing to (re-)use them on an ?as is? basis
and thus save expenditure and manpower when one
configures a composite NLP pipeline.
As a consequence, two questions arise. First, how
can we abstract away from the specific code level of
those single modules which serve, by and large, the
same functionality? Second, how can we build NLP
systems by composing them, at the abstract level
of functional specification, from these already ex-
isting component building blocks disregarding con-
crete implementation matters? Yet another burning
issue relates to the increasing availability of multiple
metadata annotations both in corpora and language
processors. If alternative annotation tag sets are cho-
sen for the same functional task a ?data conversion?
problem is created which should be solved at the ab-
stract specification level as well (Ide et al, 2003).
Software engineering methodology points out that
these requirements are best met by properly identi-
fying input/output capabilities of constituent compo-
nents and by specifying a general data model (e.g.,
based on UML (Rumbaugh et al, 1999)) in or-
der to get rid of the low-level implementation (i.e.,
coding) layer. A particularly promising proposal
along this line of thought is the Unstructured Infor-
mation Management Architecture (UIMA) (Ferrucci
and Lally, 2004) originating from IBM research ac-
tivities.1 UIMA is but the latest attempt in a series
of proposals concerned with more generic NLP en-
gines such as ATLAS (Laprun et al, 2002) or GATE
(Cunningham, 2002). These frameworks have in
common a data-driven architecture and a data model
based on annotation graphs as an adaptation of the
TIPSTER architecture (Grishman, 1997). They suf-
fer, however, from a lack of standards for data ex-
change and abstraction mechanisms at the level of
specification languages.
This can be achieved by the definition of a com-
mon annotation scheme. We propose an UIMA
schema which accounts for a significant part of the
complete NLP cycle ? from the collection of doc-
uments and their internal formal structure, via sen-
tence splitting, tokenization, POS tagging, and pars-
ing, up until the semantic layer (still excluding dis-
course) ? and which aims at the implementation-
independent specification of a core NLP system.
1Though designed for any sort of unstructured data (text,
audio and video data), we here focus on special requirements
for the analysis of written documents.
33
2 Related work
Efforts towards the design of annotation schemata
for language resources and their standardization
have a long-standing tradition in the NLP commu-
nity. In the very beginning, this work often fo-
cused exclusively on subdomains of text analysis
such as document structure meta-information, syn-
tactic or semantic analysis. The Text Encoding Ini-
tiative (TEI)2 provided schemata for the exchange
of documents of various genres. The Dublin Core
Metadata Initiative3 established a de facto standard
for the Semantic Web.4 For (computational) lin-
guistics proper, syntactic annotation schemes, such
as the one from the Penn Treebank (Marcus et al,
1993), or semantic annotations, such as the one un-
derlying ACE (Doddington et al, 2004), are increas-
ingly being used in a quasi standard way.
In recent years, however, the NLP community is
trying to combine and merge different kinds of an-
notations for single linguistic layers. XML formats
play a central role here. An XML-based encod-
ing standard for linguistic corpora XCES (Ide et al,
2000) is based on CES (Corpus Encoding Standard)
as part of the EAGLES Guidelines.5 Work on TIGER
(Brants and Hansen, 2002) is an example for the li-
aison of dependency- and constituent-based syntac-
tic annotations. New standardization efforts such as
the Syntactic Annotation Framework (SYNAF) (De-
clerck, 2006) aim to combine different proposals and
create standards for syntactic annotation.
We also encounter a tendency towards multiple
annotations for a single corpus. Major bio-medical
corpora, such as GENIA (Ohta et al, 2002) or
PennBioIE,6 combine several layers of linguistic
information in terms of morpho-syntactic, syntac-
tic and semantic annotations (named entities and
events). In the meantime, the Annotation Compat-
ibility Working Group (Meyers, 2006) began to con-
centrate its activities on the mutual compatibility of
annotation schemata for, e.g., POS tagging, tree-
banking, role labeling, time annotation, etc.
The goal of these initiatives, however, has never
been to design an annotation scheme for a complete
2http://www.tei-c.org
3http://dublincore.org
4http://www.w3.org/2001/sw
5http://www.ilc.cnr.it/EAGLES96/
6http://bioie.ldc.upenn.edu
NLP pipeline as needed, e.g., for information ex-
traction or text mining tasks (Hahn and Wermter,
2006). This lack is mainly due to missing standards
for specifying comprehensive NLP software archi-
tectures. The MEANING format (Pianta et al, 2006)
is designed to integrate different levels of morpho-
syntactic annotations. The HEART OF GOLD mid-
dleware (Scha?fer, 2006) combines multidimensional
mark-up produced by several NLP components. An
XML-based NLP tool suite for analyzing and anno-
tating medical language in an NLP pipeline was also
proposed by (Grover et al, 2002). All these propos-
als share their explicit linkage to a specific NLP tool
suite or NLP system and thus lack a generic annota-
tion framework that can be re-used in other develop-
mental environments.
Buitelaar et al developed in the context of an in-
formation extraction project an XML-based multi-
layered annotation scheme that covers morpho-
syntactic, shallow parsing and semantic annotation
(Buitelaar et al, 2003). Their scheme borrows con-
cepts from object-oriented programming (e.g., ab-
stract types, polymorphism). The object-oriented
perspective already allows the development of a
domain-independent schema and extensions of core
types without affecting the base schema. This
schema is comprehensive indeed and covers a sig-
nificant part of advanced NLP pipelines but it is also
not connected to a generic framework.
It is our intention to come full circle within a
general annotation framework. Accordingly, we
cover a significant part of the NLP pipeline from
document meta information and formal document
structure, morpho-syntactic and syntactic analysis
up to semantic processing. The scheme we propose
is intended to be compatible with on-going work
in standardization efforts from task-specific annota-
tions and to adhere to object-oriented principles.
3 Data-Driven NLP Architecture
As the framework for our specification efforts, we
adopted the Unstructured Information Management
Architecture (UIMA) (Ferrucci and Lally, 2004). It
provides a formal specification layer based on UML,
as well as a run-time environment for the interpreta-
tion and use of these specifications. This dualism is
going to attract more and more researchers as a basis
34
for proper NLP system engineering.
3.1 UIMA-based Tool Suite
UIMA provides a platfrom for the integration
of NLP components (ANALYSIS ENGINES in the
UIMA jargon) and the deployment of complex
NLP pipelines. It is more powerful than other
prominent software systems for language engineer-
ing (e.g., GATE, ATLAS) as far as its pre- and
post-processing facilities are concerned ? so-called
COLLECTION READERS can be developed to handle
any kind of input format (e.g., WWW documents,
conference proceedings), while CONSUMERS, on
other hand, deal with the subsequent manipulation
of the NLP core results (e.g., automatic indexing).
Therefore, UIMA is a particularly suitable architec-
ture for advanced text analysis applications such as
text mining or information extraction.
We currently provide ANALYSIS ENGINES for
sentence splitting, tokenization, POS tagging, shal-
low and full parsing, acronym detection, named
entity recognition, and mapping from named enti-
ties to database term identifiers (the latter is mo-
tivated by our biological application context). As
we mainly deal with documents taken from the bio-
medical domain, our collection readers process doc-
uments from PUBMED,7 the most important liter-
ature resource for researchers in the life sciences.
PUBMED currently provides more than 16 million
bibliographic references to bio-medical articles. The
outcomes of ANALYSIS ENGINES are input for var-
ious CONSUMERS such as semantic search engines
or text mining tools.
3.2 Common Analysis System
UIMA is based on a data-driven architecture. This
means that UIMA components do not exchange or
share code, they rather exchange data only. The
components operate on common data referred to
as COMMON ANALYSIS SYSTEM (CAS)(Go?tz and
Suhre, 2004). The CAS contains the subject of anal-
ysis (document) and provides meta data in the form
of annotations. Analysis engines receive annotations
through a CAS and add new annotations to the CAS.
An annotation in the CAS then associates meta data
with a region the subject of the analysis occupies
7http://www.pubmed.gov
(e.g., the start and end positions in a document).
UIMA defines CAS interfaces for indexing, ac-
cessing and updating the CAS. CASes are modelled
independently from particular programming lan-
guages. However, JCAS, an object-oriented inter-
face to the CAS, was developed for JAVA. CASes are
crucial for the development and deployment of com-
plex NLP pipelines. All components to be integrated
in UIMA are characterized by abstract input/output
specifications, so-called capabilities. These speci-
fications are declared in terms of descriptors. The
components can be integrated by wrappers conform-
ing with the descriptors. For the integration task, we
define in advance what kind of data each component
may manipulate. This is achieved via the UIMA
annotation type system. This type system follows
the object-oriented paradigm. There are only two
kinds of data, viz. types and features. Features spec-
ify slots within a type, which either have primitive
values such as integers or strings, or have references
to instances of types in the CAS. Types, often called
feature structures, are arranged in an inheritance hi-
erarchy.
In the following section, we propose an ANNO-
TATION TYPE SYSTEM designed and implemented
for an UIMA Tool Suite that will become the back-
bone for our text mining applications. We distin-
guish between the design and implementation lev-
els, talking about the ANNOTATION SCHEME and
the TYPE SYSTEM, respectively.
4 Annotation Type System
The ANNOTATION SCHEME we propose currently
consists of five layers: Document Meta, Document
Structure & Style, Morpho-Syntax, Syntax and Se-
mantics. Accordingly, annotation types fall into five
corresponding categories. Document Meta and Doc-
ument Structure & Style contain annotations about
each document?s bibliography, organisation and lay-
out. Morpho-Syntax and Syntax describe the results
of morpho-syntactic and syntactic analysis of texts.
The results of lemmatisation, stemming and decom-
position of words can be represented at this layer, as
well. The annotations from shallow and full parsing
are represented at the Syntax layer. The appropri-
ate types permit the representation of dependency-
and constituency-based parsing results. Semantics
35
uima.tcas.Annotation
+begin: uima.cas.Integer
+end: uima.cas.Integer
Annotation
+componentId: uima.cas.String
+confidence: uima.cas.Double
Descriptor
pubmed.ManualDescriptor
+MeSHList: uma.cas.FSArray = MeSHHeading
+...
AutoDescriptor
+...
Header
+docType: uima.cas.String
+source: uima.cas.String
+docID: uima.cas.String
+language: uima.cas.String
+copyright: uima.cas.String
+authors: uima.cas.FSArray = AuthorInfo
+title: uima.cas.String
+pubTypeList: uima.cas.FSArray = PubType
+...
pubmed.Header
+citationStatus: uima.cas.String {...}
ManualDescriptor
+keywordList: uima.cas.FSArray = Keyword
+...
PubType
+name: uima.cas.Sting
Journal
+ISSN: uima.cas.String
+volume: uima.cas.String
+journalTitle: uima.cas.String
+impactFactor: uima.cas.String
Keyword
+name: uima.cas.String
+source: uima.cas.String
Token
+posTag: uima.cas.FSArray  = POSTag
+lemma: Lemma
+feats: GrammaticalFeats
+stemmedForm: StemmedForm
+depRelList: uima.cas.FSArray = DependencyRelation
+orthogr: uima.cas.FSArray = String
POSTag
+tagsetId: uima.cas.String
+language: uima.cas.String
+value: uima.cas.String
Lemma
+value: String
Acronym
Abbreviation
+expan: String
StemmedForm
+value: String
GrammaticalFeats
+language: uima.cas.String
DiscontinuousAnnotation
+value: FSArray = Annotation
PennPOSTag
NounFeats
+...
...
+...
...
+...
DependencyRelation
+head: Token
+projective: uima.cas.Boolean
+label: uima.cas.String
Relation
DepRelationSet...
Chunk
PhraseChunk
PTBConstituent
+formFuncDisc: uima.cas.String
+gramRole: uima.cas.String
+adv: uima.cas.String
+misc: uima.cas.String
+map: Constituent
+tpc: uima.cas.Boolean
+nullElement: uima.cas.String
+ref: Constituent
Constituent
+parent: Constituent
+head: Token
+cat: uima.cas.String
GENIAConstituent
+syn: uima.cas.String 
...
+...
...
+...
NP ...PP
Entity
+dbEntry: uima.cas.FSArray = DBEntry
+ontologyEntry: uima.cas.FSArray = OntologyEntry
+specificType: uima.cas.String
BioEntity
Cytokine
Organism VariationGene
...
LexiconEntry OntologyEntryDBEntry
ResourceEntry
+source: uima.cas.String
+entryId: uima.cas.String
+version: uima.cas.String
Zone 
Title TextBody Paragraph Figure
+caption: Caption
Section
+title: Title
+depth: uima.cas.Integer
Misc ... PersonOrganization
MUCEntity
...
2
3
4
5 6
1
CAS Core
Figure 1: Multi-Layered UIMA Annotation Scheme in UML Representation. 1: Basic Feature Structure and
Resource Linking. 2: Document Meta Information. 3: Morpho-Syntax. 4: Syntax. 5: Document Structure
& Style. 6: Semantics.
36
currently covers information about named entities,
events and relations between named entities.
4.1 Basic Feature Structure
All types referring to different linguistic lay-
ers derive from the basic type Annotation,
the root type in the scheme (cf. Figure 1-
1). The Annotation type itself derives infor-
mation from the default UIMA annotation type
uima.tcas.Annotation and, thus, inherits the
basic annotation features, viz. begin and end (mark-
ing spans of annotations in the subject of analysis).
Annotation extends this default feature structure
with additional features. The componentId marks
which NLP component actually computed this an-
notation. This attribute allows to manage multiple
annotations of the same type The unique linkage be-
tween an analysis component and an annotation item
is particularly relevant in cases of parallel annota-
tions. The component from which the annotation
originated also assigns a specific confidence score
to its confidence feature. Each type in the scheme is
at least supplied with these four slots inherited from
their common root type.
4.2 Document Meta Information
The Document Meta layer (cf. Figure 1-2) describes
the bibliographical and content information of a doc-
ument. The bibliographical information, often re-
trieved from the header of the analyzed document,
is represented in the type Header. The source
and docID attributes yield a unique identifier for
each document. We then adopted some Dublin Core
elements, e.g., language, title, docType. We dis-
tinguish between domain-independent information
such as language, title, document type and domain-
dependent information as relevant for text mining
in the bio-medical domain. Accordingly, the type
pubmed.Header was especially created for the
representation of PUBMED document information.
A more detailed description of the document?s pub-
lication data is available from types which specialize
PubType such as Journal. The latter contains
standard journal-specific attributes, e.g., ISSN, vol-
ume, journalTitle.
The description of the document?s content of-
ten comes with a list of keywords, informa-
tion assigned to the Descriptor type. We
clearly distinguish between content descriptors man-
ually provided by an author, indexer or cura-
tor, and items automatically generated by text
analysis components after document processing.
While the first kind of information will be stored
in the ManualDescriptor, the second one
will be represented in the AutoDescriptor.
The generation of domain-dependent descriptors is
also possible; currently the scheme contains the
pubmed.ManualDescriptor which allows to
assign attributes such as chemicals and genes.
4.3 Document Structure & Style
The Document Structure & Style layer (cf. Figure 1-
5) contains information about the organization and
layout of the analyzed documents. This layer en-
ables the marking-up of document structures such
as paragraphs, rhetorical zones, figures and tables,
as well as typographical information, such as italics
and special fonts. The focus of modeling this layer is
on the annotation of scientific documents, especially
in the life sciences. We adopted here the SCIXML8
annotation schema, which was especially developed
for marking-up scientific publications. The Zone
type refers to a distinct division of text and is the par-
ent type for various subtypes such as TextBody,
Title etc. While it seems impossible to predict all
of the potential formal text segments, we first looked
at types of text zones frequently occurring in sci-
entific documents. The type Section, e.g., repre-
sents a straightforward and fairly standard division
of scientific texts into introduction, methods and re-
sults sections. The divisions not covered by current
types can be annotated with Misc. The annotation
of tables and figures with corresponding types en-
ables to link text and additional non-textual infor-
mation, an issue which is gaining more and more
attention in the text mining field.
4.4 Morpho-Syntax
The Morpho-Syntax layer (cf. Figure 1-3) represents
the results of morpho-syntactic analysis such as to-
kenization, stemming, POS tagging. The small-
est annotation unit is Token which consists of five
attributes, including its part-of-speech information
8http://www.cl.cam.ac.uk/?aac10/
escience/sciborg.html
37
(posTag), stemmedForm, lemma, grammatical fea-
tures (feats), and orthographical information (or-
thogr).
With respect to already available POS tagsets,
the scheme allows corresponding extensions of
the supertype POSTag to, e.g., PennPOSTag
(for the Penn Tag Set (Marcus et al, 1993)) or
GeniaPOSTag (for the GENIA Tag Set (Ohta et
al., 2002)). The attribute tagsetId serves as a unique
identifier of the corresponding tagset. The value of
the POS tag (e.g., NN, VVD, CC) can be stored in
the attribute value. The potential values for the in-
stantiation of this attribute are always restricted to
the tags of the associated tagset. These constraints
enforce formal control on annotation processes.
As for morphologically normalized lexical items,
the Lemma type stores the canonical form of a lexi-
cal token which can be retrieved from a lexicon once
it is computed by a lemmatizer. The lemma value,
e.g., for the verb ?activates? would be ?activate?. The
StemmedForm represents a base form of a text to-
ken as produced by stemmers (e.g., ?activat-? for the
noun ?activation?).
Due to their excessive use in life science docu-
ments, abbreviations, acronyms and their expanded
forms have to be considered in terms of appropriate
types, as well. Accordingly, Abbreviation and
Acronym are defined, the latter one being a child
type of the first one. The expanded form of a short
one can easily be accessed from the attribute expan.
Grammatical features of tokens are represented
in those types which specialize the supertype
GrammaticalFeats. Its child types, viz.
NounFeats, VerbFeats, AdjectiveFeats,
PronounFeats (omitted from Figure 1-3) cover
the most important word categories. Attributes
of these types obviously reflect the properties
of particular grammatical categories. While
NounFeats comes with gender, case and num-
ber only, PronounFeats must be enhanced with
person. A more complex feature structure is asso-
ciated with VerbFeats which requires attributes
such as tense, person, number, voice and aspect. We
adapted here specifications from the TEI to allow
compatibility with other annotation schemata.
The type LexiconEntry (cf. Figure 1-1) en-
ables a link to the lexicon of choice. By designing
this type we achieve much needed flexibility in link-
ing text snaps (e.g., tokens, simplex forms, multi-
word terms) to external resources. The attributes
entryId and source yield, in combination, a unique
identifier of the current lexicon entry. Resource ver-
sion control is enabled through an attribute version.
Text annotations often mark disrupted text spans,
so-called discontinuous annotations. In coordinated
structures such as ?T and B cell?, the annotator
should mark two named entities, viz. ?T cell? and ?B
cell?, where the first one results from the combina-
tion of the disjoint parts ?T? and ?cell?. In order to
represent such discontinous annotations, we intro-
duced the type DiscontinuousAnnotation
(cf. Figure 1-1) which links through its attribute
value spans of annotations to an annotation unit.
4.5 Syntax
This layer of the scheme provides the types and at-
tributes for the representation of syntactic structures
of sentences (cf. Figure 1-4). The results from shal-
low and full parsing can be stored here.
Shallow parsing (chunking) aims at dividing
the flow of text into phrases (chunks) in a non-
overlapping and non-recursive manner. The type
Chunk accounts for different chunk tag sets by sub-
typing. Currently, the scheme supports Phrase-
Chunks with subtypes such as NP, VP, PP, or ADJP
(Marcus et al, 1993).
The scheme also reflects the most popular full
parsing approaches in NLP, viz. constituent-based
and dependency-based approaches. The results
from constituent-based parsing are represented in
a parse tree and can be stored as single nodes in
the Constituent type. The tree structure can
be reconstructed through links in the attribute par-
ent which stores the id of the parent constituent.
Besides the attribute parent, Constituent holds
the attributes cat which stores the complex syntac-
tic category of the current constituent (e.g., NP, VP),
and head which links to the head word of the con-
stituent. In order to account for multiple annota-
tions in the constituent-based approach, we intro-
duced corresponding constituent types which spe-
cialize Constituent. This parallels our approach
which we advocate for alternatives in POS tagging
and the management of alternative chunking results.
Currently, the scheme supports three differ-
ent constituent types, viz. PTBConstituent,
38
GENIAConstituent (Miyao and Tsujii, 2005)
and PennBIoIEConstituent. The attributes
of the type PTBConstituent cover the com-
plete repertoire of annotation items contained in
the Penn Treebank, such as functional tags for
form/function dicrepancies (formFuncDisc), gram-
matical role (gramRole), adverbials (adv) and mis-
cellaneous tags (misc). The representation of null
elements, topicalized elements and gaps with corre-
sponding references to the lexicalized elements in a
tree is reflected in attributes nullElement, tpc, map
and ref, respectively. GENIAConstituent and
PennBIoIEConstituent inherit from PTB-
Constituent all listed attributes and provide, in
the case of GENIAConstituent , an additional
attribute syn to specify the syntactic idiosyncrasy
(coordination) of constituents.
Dependency parsing results are directly linked to
the token level and are thus referenced in the Token
type. The DependencyRelation type inherits
from the general Relation type and introduces
additional features which are necessary for describ-
ing a syntactic dependency. The attribute label char-
acterizes the type of the analyzed dependency rela-
tion. The attribute head indicates the head of the
dependency relation attributed to the analyzed to-
ken. The attribute projective relates to the property
of the dependency relation whether it is projective
or not. As different dependency relation sets can be
used for parsing, we propose subtyping similar to
the constituency-based parsing approaches. In order
to account for alternative dependency relation sets,
we aggregate all possible annotations in the Token
type as a list (depRelList).
4.6 Semantics
The Semantics layer comprises currently the repre-
sentation of named entities, particularly for the bio-
medical domain. The entity types are hierarchically
organized. The supertype Entity (cf. Figure 1-
6) links annotated (named) entities to the ontologies
and databases through appropriate attributes, viz. on-
tologyEntry and sdbEntry. The attribute specific-
Type specifies the analyzed entity in a more detailed
way (e.g., Organism can be specified through
the species values ?human?, ?mouse?, ?rat?, etc.)
The subtypes are currently being developed in the
bio-medical domain and cover, e.g., genes, pro-
teins, organisms, diseases, variations. This hierar-
chy can easily be extended or supplemented with
entities from other domains. For illustration pur-
poses, we extended it here by MUC (Grishman
and Sundheim, 1996) entity types such as Person,
Organization, etc.
This scheme is still under construction and will
soon also incorporate the representation of relation-
ships between entities and domain-specific events.
The general type Relation will then be extended
with specific conceptual relations such as location,
part-of, etc. The representation of events will be
covered by a type which aggregates pre-defined re-
lations between entities and the event mention. An
event type such as InhibitionEventwould link
the text spans in the sentence ?protein A inhibits
protein B? in attributes agent (?protein A?), patient
(?protein B?), mention (?inhibits?).
5 Conclusion and Future work
In this paper, we introduced an UIMA annotation
type system which covers the core functionality
of morphological, syntactic and semantic analysis
components of a generic NLP system. It also in-
cludes type specifications which relate to the formal
document format and document style. Hence, the
design of this scheme allows the annotation of the
entire cycle of (sentence-level) NLP analysis (dis-
course phenomena still have to be covered).
The annotation scheme consists mostly of core
types which are designed in a domain-independent
way. Nevertheless, it can easily be extended with
types which fit other needs. The current scheme sup-
plies an extension for the bio-medical domain at the
document meta and structure level, as well as on the
semantic level. The morpho-syntactic and syntactic
levels provide types needed for the analysis of the
English language. Changes of attributes or attribute
value sets will lead to adaptations to other natural
languages.
We implemented the scheme as an UIMA type
system. The formal specifications are implemented
using the UIMA run-time environment. This direct
link of formal and implementational issues is a ma-
jor asset using UIMA unmatched by any previous
specification approach. Furthermore, all annotation
results can be converted to the XMI format within
39
the UIMA framework. XMI, the XML Metadata In-
terchange format, is an OMG9 standard for the XML
representation of object graphs.
The scheme also eases the representation of an-
notation results for the same task with alternative
and often competitive components. The identifica-
tion of the component which provided specific an-
notations can be retrieved from the attribute com-
ponentId. Furthermore, the annotation with alterna-
tive and multiple tag sets is supported as well. We
have designed for each tag set a type representing
the corresponding annotation parameters. The inher-
itance trees at almost all annotation layers support
the parallelism in annotation process (e.g., tagging
may proceed with different POS tagsets).
The user of the scheme can restrict the potential
values of the types or attributes. The current scheme
makes use of the customization capability for POS
tagsets, for all attributes of constituents and chunks.
This yields additional flexibility in the design and,
once specified, an increased potential for automatic
control for annotations.
The scheme also enables a straightforward con-
nection to external resources such as ontologies,
lexicons, and databases as evidenced by the corre-
sponding subtypes of ResourceEntry (cf. Figure
1-1). These types support the specification of a re-
lation between a concrete text span and the unique
item addressed in any of these resources.
With these considerations in mind, we strive for
the elaboration of a common standard UIMA type
system for NLP engines. The advantages of such a
standard include an easy exchange and integration
of different NLP analysis engines, the facilitation
of sophisticated evaluation studies (where, e.g., al-
ternative components for NLP tasks can be plugged
in and out at the spec level), and the reusability of
single NLP components developed in various labs.
Acknowledgments. This research was funded by the EC?s 6th Framework Programme
(4th call) within the BOOTStrep project under grant FP6-028099.
References
S. Brants and S. Hansen. 2002. Developments in the TIGER
annotation scheme and their realization in the corpus. In
Proc. of the 3rd LREC Conference, pages 1643?1649.
P. Buitelaar, T. Declerck, B. Sacaleanu, ?S. Vintar, D. Raileanu,
and C. Crispi. 2003. A multi-layered, XML-based approach
9http://www.omg.org
to the integration of linguistic and semantic annotations. In
Proc. of EACL 2003 Workshop NLPXML-03.
H. Cunningham. 2002. GATE, a general architecture for text
engineering. Computers and the Humanities, 36:223?254.
T. Declerck. 2006. SYNAF: Towards a standard for syntactic
annotation. In Proc. of the 5th LREC Conference.
G. Doddington, A. Mitchell, M. Przybocki, L. Ramshaw,
S. Strassel, and R. Weischedel. 2004. The Automatic Con-
tent Extraction (ACE) Program. In Proc. of the 4th LREC
Conference, pages 837?840.
D. Ferrucci and A. Lally. 2004. UIMA: an architectural ap-
proach to unstructured information processing in the corpo-
rate research environment. Natural Language Engineering,
10(3-4):327?348.
T. Go?tz and O. Suhre. 2004. Design and implementation of the
UIMA Common Analysis System. IBM Systems Journal,
43(3):476?489.
R. Grishman and B. Sundheim. 1996. Message Understand-
ing Conference ? 6: A brief history. In Proc. of the 16th
COLING, pages 466?471.
R. Grishman. 1997. Tipster architecture design document,
version 2.3. Technical report, Defense Advanced Research
Projects Agency (DARPA), U.S. Departement of Defense.
C. Grover, E. Klein, M. Lapata, and A. Lascarides. 2002.
XML-based NLP tools for analysing and annotating medi-
cal language. In Proc. of the 2nd Workshop NLPXML-2002,
pages 1?8.
U. Hahn and J. Wermter. 2006. Levels of natural language pro-
cessing for text mining. In S. Ananiadou and J. McNaught,
editors, Text Mining for Biology and Biomedicine, pages 13?
41. Artech House.
N. Ide, P. Bonhomme, and L. Romary. 2000. XCES: An XML-
based standard for linguistic corpora. In Proc. of the 2nd
LREC Conference, pages 825?830.
N. Ide, L. Romary, and E. de la Clergerie. 2003. International
standard for a linguistic annotation framework. In Proc. of
the HLT-NAACL 2003 SEALTS Workshop, pages 25?30.
C. Laprun, J. Fiscus, J. Garofolo, and S. Pajot. 2002. A prac-
tical introduction to ATLAS. In Proc. of the 3rd LREC Con-
ference, pages 1928?1932.
M. Marcus, B. Santorini, and M.A. Marcinkiewicz. 1993.
Building a large annotated corpus of English: The PENN
TREEBANK. Computational Linguistics, 19(2):313?330.
A. Meyers. 2006. Annotation compatibility working group re-
port. In Proc. of the COLING-ACL 2006 Workshop FLAC
2006?, pages 38?53.
Y. Miyao and J. Tsujii. 2005. Probabilistic disambiguation
models for wide-coverage HPSG parsing. In Proc. of the
ACL 2005, pages 83 ? 90.
T. Ohta, Y. Tateisi, and J.-D. Kim. 2002. The GENIA corpus:
An annotated research abstract corpus in molecular biology
domain. In Proc. of the 2nd HLT, pages 82?86.
E. Pianta, L. Bentivogli, C. Girardi, and B. Magnini. 2006.
Representing and accessing multilevel linguistic annotation
using the MEANING format. In Proc. of the 5th EACL-2006
Workshop NLPXML-2006, pages 77?80.
J. Rumbaugh, I. Jacobson, and G. Booch. 1999. The Unified
Modeling Language Reference Manual. Addison-Wesley.
U. Scha?fer. 2006. Middleware for creating and combining
multi-dimensional NLP markup. In Proc. of the 5th EACL-
2006 Workshop NLPXML-2006, pages 81?84.
40
Proceedings of the Workshop on BioNLP, pages 37?45,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
How Feasible and Robust is the Automatic Extraction of Gene Regulation
Events ? A Cross-Method Evaluation under Lab and Real-Life Conditions
Udo Hahn1 Katrin Tomanek1 Ekaterina Buyko1 Jung-jae Kim2 Dietrich Rebholz-Schuhmann2
1Jena University Language & Information Engineering (JULIE) Lab
Friedrich-Schiller-Universita?t Jena, Germany
{udo.hahn|katrin.tomanek|ekaterina.buyko}@uni-jena.de
2EMBL-EBI, Wellcome Trust Genome Campus, Hinxton, Cambridge, UK
{kim|rebholz}@ebi.ac.uk
Abstract
We explore a rule system and a machine learn-
ing (ML) approach to automatically harvest
information on gene regulation events (GREs)
from biological documents in two different
evaluation scenarios ? one uses self-supplied
corpora in a clean lab setting, while the other
incorporates a standard reference database of
curated GREs from REGULONDB, real-life
data generated independently from our work.
In the lab condition, we test how feasible
the automatic extraction of GREs really is
and achieve F-scores, under different, not di-
rectly comparable test conditions though, for
the rule and the ML systems which amount
to 34% and 44%, respectively. In the REGU-
LONDB condition, we investigate how robust
both methodologies are by comparing them
with this routinely used database. Here, the
best F-scores for the rule and the ML systems
amount to 34% and 19%, respectively.
1 Introduction
The extraction of binary relations from biomedical
text has caught much attention in the recent years.
Progress on this and other tasks has been monitored
in challenge competitions such as BIOCREATIVE I
and II,1 which dealt with gene/protein names and
and protein-protein interaction.
The BIOCREATIVE challenge and other related
ones have shown at several occasions that partici-
pants continue to use two fundamentally different
1http://biocreative.sourceforge.net/
systems: symbolic pattern-based systems (rule sys-
tems), on the one hand, and feature-based statisti-
cal machine learning (ML) systems, on the other
hand. This has led to some rivalry with regard to the
interpretation of their performance data, the costs
of human efforts still required and their scalability
for the various tasks. While rule systems are of-
ten hand-crafted and fine-tuned to a particular ap-
plication (making a major manual rewrite often nec-
essary when the application area is shifted), ML
systems are trained automatically on manually an-
notated corpora, i.e., without manual intervention,
and thus have the advantage to more easily adapt to
changes in the requested identification tasks. Time
costs (human workload) are thus shifted from rule
design and adaptation to metadata annotation.
Text mining systems as usually delivered by
BioNLP researchers render biologically relevant en-
tities and relations on a limited set of test documents
only. While this might be sufficient for the BioNLP
community, it is certainly insufficient for bioinfor-
maticians and molecular biologists since they re-
quire large-scale data with high coverage and reli-
ability. For our analysis, we have chosen the topic
of gene regulatory events in E. coli, which is a do-
main of very active research and grand challenges.2
Currently the gold standard of the existing body of
knowledge of such events is represented by the fact
database REGULONDB.3 Its content has been man-
2The field of gene regulation is one of the most prominent
topics of research and often mentioned as one of the core fields
of future research in molecular biology (cf, e.g., the Grand
Challenge I-2 described by Collins et al (2003)).
3http://regulondb.ccg.unam.mx/
37
ually gathered from the scientific literature and de-
scribes the curated computational model of mecha-
nisms of transcriptional regulation in E. coli. Having
this gold standard in mind, we face the challenging
task to automatically reproduce this content from the
available literature, to enhance this content with re-
liable additional information and to update this re-
source as part of a regular automatic routine.
Hence, we first explore the feasibility and per-
formance of a rule-based and an ML-based system
against special, independently created corpora that
were generated to enable measurements under clean
experimental lab conditions. This part, due to dif-
ferent experimental settings, is not meant as a com-
parison between both approaches though. We then
move to the even more demanding real-life scenario
where we evaluate and compare these solutions for
the identification of gene regulatory events against
the REGULONDB data resource. This approach tar-
gets the robustness of the proposed text mining so-
lutions from the perspectives of completeness, cor-
rectness and novelty of the generated results.
2 Related Work
Considering relation extraction (RE) in the biomed-
ical domain, there are only few studies which deal
primarily with gene regulation. Yang et al (2008)
focus on the detection of sentences that contain
mentions of transcription factors (proteins regulat-
ing gene expression). They aim at the detection
of new transcription factors, while relations are not
taken into account. In contrast, S?aric? et al (2004)
extract gene regulatory networks and achieve in the
RE task an accuracy of up to 90%. They disregard,
however, ambiguous instances, which may have led
to the low recall around 20%. The Genic Interaction
Extraction Challenge (Ne?dellec, 2005) was orga-
nized to determine the state-of-the-art performance
of systems designed for the detection of gene regula-
tion interactions. The best system achieved a perfor-
mance of about 50% F-score. The results, however,
have to be taken with care as the LLL corpus used in
the challenge is of extremely limited size.
3 Extraction of Gene Regulation Events
Gene regulation is a complex cellular process that
controls the expression of genes. These genes are
then transcribed into their RNA representation and
later translated into proteins, which fulfill various
tasks such as maintaining the cell structure, enabling
the generation of energy and interaction with the en-
vironment.
The analysis of the gene regulatory processes is
ongoing research work in molecular biology and af-
fects a large number of research domains. In par-
ticular the interpretation of gene expression profiles
from microarray analyses could be enhanced using
our understanding of gene regulation events (GREs)
from the literature.
We approach the task of the automatic extraction
of GREs from literature from two different method-
ological angles. On the one hand, we provide a set of
hand-crafted rules ? both for linguistic analysis and
conceptual inference (cf. Section 3.1), the latter be-
ing particularly helpful in unveiling only implicitly
stated biological knowledge. On the other hand, we
supply a machine learning-based system for event
extraction (cf. Section 3.2). No regularities are spec-
ified a priori by a human although, at least in the su-
pervised scenario we have chosen, this approach re-
lies on training data supplied by human (expert) an-
notators who provide sufficiently many instances of
ground truth decisions from which regularities can
automatically be learnt. At the level of system per-
formance, rules tend to foster precision at the cost
of recall and ML systems tend to produce inverse
figures, while there is no conclusive evidence for or
against any of these two approaches.
The extraction of GREs, independent of the ap-
proach one subscribes to, is a complex problem
composed of a series of subtasks. Abstracting away
from lots of clerical and infrastructure services (e.g.,
sentence splitting, tokenization) at the core of any
GRE extraction lie the following basic steps:
? the identification of pairs of gene mentions as
the arguments of a relation ? the well-known
named entity recognition and normalization
task,
? the decision whether the entity pair really con-
stitutes a relation,
? and the identification of the roles of the argu-
ments in the relation which implicitly amounts
to characterize each argument as either agent or
patient.
38
3.1 Rule-based Extraction
The rule-based system extracts GREs from text em-
ploying logical inference. The motivation of using
inference is that the events under scrutiny are often
expressed in text in either a compositional or an in-
complete way. We address this issue by composi-
tionally representing textual semantics and by log-
ically inferring implicit meanings of text over the
compositional representation of textual semantics.
Entity Identification. The system first recognizes
named entities of the types that can be participants of
the target events. We have collected 15,881 E. coli
gene/protein and operon names from REGULONDB
and UNIPROT. Most of the gene/protein names are
associated with UNIPROT identifiers. An operon in
prokaryotes is a DNA sequence with multiple genes
whose expression is controlled by a shared promoter
and which thus express together. We have mapped
the operon names to corresponding gene sets.
Named entity recognition relies on the use of dic-
tionaries. If the system recognizes an operon name,
it then associates the operon with its genes. The
system further recognizes multi-gene object names
(e.g., ?acrAB?), divides them into individual gene
names (e.g., ?acrA?, ?acrB?) and associates the gene
names with the multi-gene object names.
Relation Identification. The system then iden-
tifies syntactic structures of sentences in an in-
put corpus by utilizing the ENJU parser (Sagae et
al., 2007). The ENJU parser generates predicate-
argument structures, and the system converts them
into dependency structures.
The system then analyzes the semantics of the
sentences by matching syntactic-semantic patterns
to the dependency structures. We constructed 1,123
patterns for the event extraction according to the fol-
lowing workflow. We first collected keywords re-
lated to gene regulation, from GENE ONTOLOGY,
INTERPRO, WORDNET, and several papers about
information extraction from biomedical literature
(Hatzivassiloglou and Weng, 2002; Kim and Park,
2004; Huang et al, 2004). Then we collected sub-
categorization frames for each keyword and created
patterns for the frames manually.
Each pattern consists of a syntactic pattern and
a semantic pattern. The syntactic patterns com-
ply with dependency structures. The system tries
to match the syntactic patterns to the dependency
structures of sentences in a bottom-up way, consid-
ering syntactic and semantic restrictions of syntac-
tic patterns. Once a syntactic pattern is successfully
matched to a sub-tree of the available dependency
structure, its corresponding semantic pattern is as-
signed to the sub-tree as one of its semantics. The
semantic patterns are combined according to the de-
pendency structures to form a compositional seman-
tic structure.
The system then performs logical inference over
the semantic structures by using handcrafted infer-
ence rules and extracts target information from the
results of the inference. We have manually created
28 inference rules that reflect the knowledge of the
gene regulation domain. Only relations where the
identified agent is one of those known TFs are kept,
while all others are discarded.
3.2 Generic, ML-based Extraction
Apart from the already mentioned clerical pre-
processing steps, the ML-based extraction of GREs
requires several additional syntactic processing
steps including POS-tagging, chunking, and full
dependency- and constituency-based parsing.4
Entity Identification. To identify gene names in
the documents, we applied GENO, a multi-organism
gene name recognizer and normalizer (Wermter
et al, 2009) which achieved a top-rank perfor-
mance of 86.4% on the gene normalization task
of BIOCREATIVE-II. GENO recognizes gene men-
tions by means of an ML-based named entity tag-
ger trained on publicly available corpora. Subse-
quently, it attempts to map all identified mentions to
organism-specific UNIPROT5 identifiers. Mentions
that cannot be mapped are discarded; only success-
fully mapped mentions are kept. We utilized GENO
in its original version, i.e., without special adjust-
ments to the E. coli organism. However, only those
mentions detected to be genes of E. coli were fed
into the relation extraction component.
4These tasks were performed with the OPENNLP tools
(http://opennlp.sourceforge.net/) and the
MST parser (http://sourceforge.net/projects/
mstparser), both retrained on biomedical corpora.
5http://www.uniprot.de
39
Relation Identification. The ML-based approach
to GRE employs Maximum Entropy models and
constitutes and extension of the system proposed by
Buyko et al (2008) as it also makes use of depen-
dency parse information including dependency tree
level features (Katrenko and Adriaans, 2006) and
shortest dependency path features (Kim et al, 2008).
In short, the feature set consists of:
? word features (covering words before, after and
between both entity mentions);
? entity features (accounting for combinations of
entity types, flags indicating whether mentions
have an overlap, and their mention level);
? chunking and constituency-based parsing fea-
tures (concerned with head words of the
phrases between two entity mentions; this class
of features exploits constituency-based parsing
as well and indicates, e.g., whether mentions
are in the same NP, PP or VP);
? dependency parse features (analyzing both the
dependency levels of the arguments as dis-
cussed by Katrenko and Adriaans (2006) and
dependency path structure between the argu-
ments as described by Kim et al (2008));
? and relational trigger (key)words (accounting
for the connection of trigger words and men-
tions in a full parse tree).
An advantage of ML-based systems is that they
allow for thresholding. To achieve higher recall
values for our system, we may set the confidence
threshold for the negative class (i.e., a pair of en-
tity mentions does not constitute a relation) to values
> 0.5. Clearly, this is at the cost of precision as the
system more readily assigns the positive class.
4 Intrinsic Evaluation of Feasibility
The following two sections aim at evaluating the
rule-based and ML-based GRE extraction systems.
The systems are first ?intrinsically? evaluated, i.e.,
in a cross-validation manner on corpora annotated
with respect to GREs. Second, in a more realistic
scenario, both systems were evaluated against REG-
ULONDB, a database collecting knowledge about
gene regulation in E. coli. This scenario tests which
part of manually accumulated knowledge about gene
regulation in E. coli can automatically be identified
by our systems and at what level of quality.
4.1 Rule-based system
Corpus. For the training and evaluation of the
rule-based system, we annotated 209 MEDLINE ab-
stracts with three types of events: specific events
of gene transcription regulation, general events of
gene expression regulation, and physical events of
binding of transcription factors to gene regulatory
regions. Strictly speaking, only the first type is rele-
vant to REGULONDB. However, biologists often re-
port gene transcription regulation events in the sci-
entific literature as if they are gene expression regu-
lation events, which is a generalization of gene tran-
scription regulation, or the binding event, which it-
self is insufficient evidence for gene transcription
regulation. The two latter types may indicate that
the full-texts contain evidence of the first type.
We asked two curators to annotate the abstracts.
Curator A was trained with example annotations and
interactive discussions. Curator B was trained only
with example annotations and guidelines. For cross-
checking of annotations, we asked them to annotate
an unseen corpus of 97 abstracts and found that Cu-
rator A made 10.8% errors, misjudging three event
additions and, in the other 14 errors, mistaking in
annotating event types, event attributes, and pas-
sage boundaries, while Curator B made 32.4% er-
rors as such. This result indicates that the annotation
of GREs requires intensive and interactive training.
The curators have discussed and agreed on the final
release of the corpora.6
Results. The system has successfully extracted 79
biologically meaningful events among them (21.1%
recall) and incorrectly produced 15 events (84.0%
precision) which constitutes an overall F-score of
33.6%. Among the 79 events, the system has cor-
rectly identified event types of 39 events (49.4% pre-
cision), polarity of 46 events (58.2% precision), and
directness of 51 events (64.6% precision). Note that
the system employed a fully automatic module for
named entity recognition. The event type recogni-
tion is impaired, because it often fails to recognize
6The resultant annotated corpora are available at http://
www.ebi.ac.uk/?kim/eventannotation/.
40
the specific event type of transcription regulation,
but only identifies the general event type of gene ex-
pression regulation due to the lack of identified evi-
dence.
4.2 ML-based system
GeneReg corpus. The GENEREG corpus (Buyko
et al, 2008) constitutes a selection of 314 MED-
LINE abstracts related to gene regulation in E. coli.
These abstracts were randomly drawn from a set of
32,155 selected by MESH term queries from MED-
LINE using keywords such as Escherichia coli, Gene
Expression and Transcription Factors. These 314
abstracts were manually annotated for named enti-
ties involved in gene regulatory processes (such as
transcription factor, including co-factors and regu-
lators, and genes) and pairwise relations between
transcription factors (TFs) and genes, as well as trig-
gers (e.g., clue verbs) essential for the description of
gene regulation relations. As for the relation types,
the GENEREG corpus distinguishes between (a) un-
specified regulation of gene expression, (b) positive,
and (c) negative regulation of gene expression. Out
of the 314 abstracts a set of 65 were randomly se-
lected and annotated by a second annotator to iden-
tify inter-annotator agreement (IAA) values. For the
task of correct identification of the pair of interacting
named entities in gene regulation processes, an IAA
of 78.4% (R), 77.3% (P ), 77.8% (F) was measured ,
while 67% (R), 67.9% (P), 67.4% (F) were achieved
for the identification of interacting pairs plus the 3-
way classification of the interaction relation.
Experimental Setting. The ML-based extraction
system merges all of the above mentioned three
types (unspecific, negative and positive) into one
common type ?relation of gene expression?. So, it
either finds that there is a relation of interest be-
tween a pair of gold entity mentions or not. We
evaluated our system by a 5-fold cross-validation on
the GENEREG corpus. The fold splits were done
on the abstract-level to avoid the otherwise unrealis-
tic scenario where a system is trained on sentences
from an abstract and evaluated on other sentences
but from the same abstract (Pyysalo et al, 2008).
As our focus here is only on the performance of the
GRE extraction component, gold entity mentions as
annotated in the respective corpus were used.
Results. For the experimental settings given
above, the system achieved an F-score of 42% with
a precision of 59% and a recall of 33%. Increasing
the confidence threshold for the negative class in-
creases recall as shown for two different thresholds
in Table 1. As expected this is at the cost of preci-
sion. It shows, that using an extremely high thresh-
old of 0.95 results in a dramatically increased recall
of 73% compared to 33% with the default threshold.
Although at the cost of diminished precision of 32%
compared to originally 59%, the lifted threshold in-
creases the overall F-score (44%) by 2 points.
threshold R P F
default (0.5) 0.33 0.59 0.42
0.80 0.54 0.43 0.48
0.95 0.73 0.32 0.44
Table 1: Different confidence thresholds for the ML-
based system achieved by intrinsic evaluation
5 Extrinsic Evaluation of Robustness
REGULONDB is the primary and largest reference
database providing manually curated knowledge of
the transcriptional regulatory network of E. coli
K12. On K12, approximately for one-third of K12?s
genes, information about their regulation is avail-
able. REGULONDB is updated with content from
recent research papers on this issue. While REG-
ULONDB contains much more, for this paper our
focus was solely on REGULONDB?s information
about gene regulation events in E. coli. In the fol-
lowing, the term REGULONDB refers to this part of
the REGULONDB database. REGULONDB includes
e.g., the following information for each regulation
event: regulatory gene (the ?agent? in such an event,
a transcription factor), the regulated gene (the ?pa-
tient?), the regulatory effect on the regulated gene
(activating, suppression, dual, unknown), and evi-
dence that supports the existence of the regulatory
interaction.
Evaluation against REGULONDB constitutes a
real-life scenario. Thus, the complete extraction sys-
tems were run, including gene name recognition and
normalization as well as relation detection. Hence,
the systems? overall recall values are highly affected
by the gene name identification. REGULONDB is
here taken as a ?true? gold standard and thus as-
41
sumed to be correct and exhaustive with respect to
the GREs contained. As, however, every manu-
ally curated database is likely to be incomplete and
might contain some errors, we supplement our eval-
uation against REGULONDB with a manual analy-
sis of false positives errors caused by our system (cf.
Section 5.4).
5.1 Evaluation Scenario and Experimental
Settings
To evaluate our extraction systems against REG-
ULONDB we first processed a set of input docu-
ments (see below), collected all unique gene reg-
ulation events extracted and compared this set of
events against the full set of known events in REG-
ULONDB. A true positive (TP) hit is obtained, when
an event found automatically corresponds to one in
REGULONDB, i.e., having the same agent and pa-
tient. The type of regulation is not considered. A
false positive (FP) hit is counted, if an event was
found which does not occur in the same way in
REGULONDB, i.e., either patient or agent (or both)
are wrong. False negatives (FN) are those events
covered by REGULONDB but not found by a sys-
tem automatically. From these hit values, standard
precision, recall, and F-score values are calculated.
Of course, the systems? performance largely depend
on the size of the base corpus collection processed.
Thus, for both systems and all three document sets
we got separate performance scores.
Table 2 gives an overview to the document col-
lections used for evaluating the robustness of our
systems: The ?ecoli-tf? variants are documents fil-
tered both with E. coli TF names and with relevance
to E. coli. Abstracts are taken from Medline cita-
tions, while full texts are from a corpus of different
biomedical journals. The third document set, ?regu-
lon ra?, is a set containing abstracts from the REG-
ULONDB references.
name type # documents
ecoli-tf.abstracts abstract 4,347
ecoli-tf.fulltext full texts 1,812
regulon ra abstracts 2,704
Table 2: Document sets for REGULONDB evaluation
5.2 Rule-based-System
Table 3 shows the evaluation results of the rule-
based system against REGULONDB. Though the
system distinguishes the three types of events, we
have considered them all as events of gene tran-
scription regulation for the evaluation. For instance,
the system has extracted 718 unique events with
single-unit participants (i.e., excluding operons), not
considering event types and attributes (e.g., polar-
ity), from the ?ecoli-tf.fulltext? corpus. Among the
events, 347 events are found in Regulon (9.7% re-
call, 48.3% precision). If we only consider the
events that are specifically identified as gene tran-
scription regulation, the system has extracted 379
unique events among which 201 are also found in
Regulon (5.6% recall, 53.0% precision).
participant document set R P F
single-unit ecoli-tf.abstracts 0.09 0.60 0.15
multi-unit ecoli-tf.abstracts 0.24 0.61 0.34
single-unit ecoli-tf.fulltext 0.10 0.48 0.16
multi-unit ecoli-tf.fulltext 0.25 0.49 0.33
single-unit regulon ra 0.07 0.73 0.13
multi-unit regulon ra 0.18 0.70 0.28
Table 3: Results of evaluation against REGULONDB of
rule-based system.
When we split multi-unit participants into individ-
ual genes, the rule-based system shows better per-
formance, as shown in Table 3 with the participant
type ?multi-unit?. This may indicate that the gene
regulatory events of E. coli are often described as
interactions of operons. At best, the system shows
34% F-score with the ?ecoli-tf.abstracts? corpus.
5.3 ML-based System
The ML-based system was designed to recognize
all types of gene regulation events. REGULONDB,
however, contains only the subtype, i.e., regulation
of transcription. Thus, the ML-based system was
evaluated against REGULONDB in two modes: by
default, all events extracted by the systems are con-
sidered; in the ?TF-filtered? mode, only relations
with an agent from the list of all known TFs in E.
coli are considered (as done for the rule-based sys-
tem by default). Thus, comparing to the rule-based
system, only the results obtained in the ?TF-filtered?
mode should be considered.
42
5.3.1 Raw performance scores
The results for the ML-based system are shown in
Table 4. Recall values here range between 7 and
10%, while precision is between 29 and 78% de-
pending on both the document set as well as the
application of the TF filter. The low recall of the
ML-based system is partially due to the fact that the
system does not recognize multi-gene object names
(e.g., ?acrAB?), in this configuration the recall is
similar to the recall of the rule-based system in a
?single-unit modus? (see Table 3).
mode document set R P F
TF-filtered ecoli-tf.abstracts 0.09 0.70 0.16
default ecoli-tf.abstracts 0.09 0.45 0.15
TF-filtered ecoli-relevant.fulltext 0.10 0.54 0.17
default ecoli-relevant.fulltext 0.10 0.29 0.15
TF-filtered regulon ra 0.07 0.78 0.13
default regulon ra 0.07 0.47 0.12
Table 4: Results of evaluation against REGULONDB of
ML-based system
As already shown in the intrinsic evaluation,
application of different confidence thresholds in-
creases the recall of the ML-based system. This was
also done for the evaluation against REGULONDB.
Table 5 shows the impact of increased confidence
thresholds for the negative class on the ?regulon ra?
set for the ?TF-filtered? evaluation mode. Given an
extremely high threshold of 0.95, the recall is in-
creased from 7 to 11% which constitutes a relative
increase of over 60%. Precision obviously drops,
however, the overall F-score has improved from 13
to 19%. These results emphasize that an ML-based
system has an important handle which allows to ad-
just recall according to the application needs.
threshold R P F
default (0.5) 0.07 0.78 0.13
0.8 0.09 0.70 0.16
0.95 0.11 0.63 0.19
Table 5: Different confidence thresholds for the ML-
based system tested on the ?regulon ra? set
5.4 Manual analysis of false positives
REGULONDB was taken as an absolute gold stan-
dard in this evaluation. If a system correctly extracts
an event which is not contained in REGULONDB
for some reason, this constitutes a FP. Moreover, all
kinds of error (e.g., agent and patient mixed up) were
subsumed as FP errors. To analyze the cause and
distribution of FPs in more detail, a manual analysis
of the FP errors was performed and original FP hits
were assigned to one out of four FP error categories:
Cat1: Not a GRE This is really an FP error, as the
extracted relation does not at all constitute a
gene regulation event.
Cat2a: GRE but other than transcription
Unlike REGULONDB which contains only one
subtype of GREs, namely transcriptions, the
ML-based system identifies all kinds of GREs.
Therefore, the ML-based system clearly
identifies events which cannot be contained in
REGULONDB and, therefore, are not really
FPs.
Cat 3: Partially correct transcription event This
category deals with incorrect arguments of
GREs. We distinguish three types of FPs: (a)
the patient and the agent role are interchanged,
(b) the patient is wrong, while the agent is
right, and (c) the agent is wrong, while the
patient is right. In all these three cases, though
errors were committed human curators might
find the partially incorrect information useful
to speed up the curation process.
Cat4: Relation missing in REGULONDB Those
are relations which should be contained in
REGULONDB but are missing for some
reason. The agent is a correctly identified
transcription factor and the sentence contains
a mention of a transcription event. There are
several reasons why this relation was not found
in REGULONDB as we will discuss in the
following.
Table 6 shows the results of the manual FP anal-
ysis of the ML-based system (no TF filter applied)
on the ?ecoli-tf-abstracts? and ?ecoli-tf-fulltexts?.
It shows that the largest source of error is due
to Cat1, i.e., an identified relation is completely
wrong. As fulltext documents are generally more
complex, the relative amount of this kind of errors
is higher here than on abstracts (54.5 % compared
43
category abstracts (%) fulltexts (%)
Cat 1 44.5 54.5
Cat 2 11.2 10.9
Cat 3a 3.8 3.9
Cat 3b 8.5 4.4
Cat 3c 8.2 5.4
Cat 4 23.8 21.0
Table 6: Manual analysis of false positive errors (FP).
Percentages of FPs by category are reported on ?ecoli-tf-
abstracts? and ?ecoli-tf-fulltexts?
to 44.5 %). However, on abstracts and fulltexts, a
bit more than 10 % of the FP are because the sys-
tem found too general GREs which, by definition,
are not contained in REGULONDB (Cat2). Iden-
tified GREs that were partially correct constitute
20.5 % (abstracts) or 13.7 % (fulltexts) of the FP er-
rors (Cat3).
Finally, 23.8% and 21.0% of the FPs for abstracts
and fulltext, respectively, are correct transcription
events but could not be found in REGULONDB
(Cat4). This is due to several reasons. For instance,
identified gene names were incorrectly normalized
so that they could not be found in REGULONDB,
REGULONDB curators have not yet added a relation
or simply overlooked it; relations are correctly iden-
tified as such in the narrow context of a paragraph of
a document but were actually of speculative nature
only (this includes relations whose status is unsure,
often indicated by ?likely? or ?possibly?).
Summarizing, the manual FP analysis shows that
about 50% of all FPs are not completely erroneous.
These numbers must clearly be kept in mind when
interpreting the raw numbers (especially for preci-
sion) reported on in the previous subsection.
5.5 Integration of text mining results
We have integrated the results of the two different
text mining systems and found that both systems are
complementary to each other such that their result
sets do not heavily overlap. For instance, from the
?ecoli-tf.abstract? corpus, the rule-based system ex-
tracts 992 events, while the ML-based system ex-
tracts 705 events. For the integration, we have con-
sidered only the events whose participants are as-
sociated with UNIPROT identifiers. Among the ex-
tracted events, only 285 events are extracted by both
systems. We might speculate that the overlapping
events are more reliable than the rest of the extracted
events. It also leaves 71.3% of the results from
the rule-based system and 59.6% of results from the
ML-based system as unique contributions from each
of the approaches for the integration.
6 Conclusions
We have explored a rule-based and a machine
learning-based approach to the automatic extrac-
tion of gene regulation events. Both approaches
were evaluated under well-defined lab conditions us-
ing self-supplied corpora, and under real-life condi-
tions by comparing our results with REGULONDB,
a well-curated reference data set. While the re-
sults for the first evaluation scenario are state of the
art, performance figures in the real-life scenario are
not so shiny (the best F-scores for the rule-based
and the ML-based system are on the order of 34%
and 19%, respectively). This holds, in particular,
for the comparison with the work of Rodr??guez-
Penagos et al (2007). Still, at least the ML-based
approach is much more general than the very specifi-
cally tuned manual rule set from Rodr??guez-Penagos
et al (2007) and has potential for increases in perfor-
mance. Also, this has been the first extra-mural eval-
uation of automatically generating content for REG-
ULONDB.
Still, the analysis of false positives reveals that
the strict criteria we applied for our evaluation may
appear in another light for human curators. Con-
founded agents and patients (21% on the abstracts,
14% on full texts) and information not contained in
REGULONDB (24% on the abstracts, 21% on full
texts) might be useful from a heuristic perspective to
focus on interesting data during the curation process.
Acknowledgements
This work was funded by the EC within the BOOT-
Strep (FP6-028099) and the CALBC (FP7-231727)
projects. We want to thank Tobias Wagner (Centre
for Molecular Biomedicine, FSU Jena) for perform-
ing the manual FP analysis.
44
References
Ekaterina Buyko, Elena Beisswanger, and Udo Hahn.
2008. Testing different ACE-style feature sets for
the extraction of gene regulation relations from MED-
LINE abstracts. In Proceedings of the 3rd Interna-
tional Symposium on Semantic Mining in Biomedicine
(SMBM 2008), pages 21?28.
Francis Collins, Eric Green, Alan Guttmacher, and Mark
Guyer. 2003. A vision for the future of genomics re-
search. Nature, 422(6934 (24 Feb)):835?847.
Vasileios Hatzivassiloglou and Wubin Weng. 2002.
Learning anchor verbs for biological interaction pat-
terns from published text articles. International Jour-
nal of Medical Informatics, 67:19?32.
Minlie Huang, Xiaoyan Zhu, Donald G. Payan, Kun-
bin Qu, and Ming Li. 2004. Discovering patterns
to extract protein-protein interactions from full texts.
Bioinformatics, 20(18):3604?3612.
Sophia Katrenko and P. Adriaans. 2006. Learning rela-
tions from biomedical corpora using dependency trees.
In KDECB 2006 ? Knowledge Discovery and Emer-
gent Complexity in Bioinformatics, pages 61?80.
Jung-jae Kim and Jong C. Park. 2004. BioIE: retar-
getable information extraction and ontological anno-
tation of biological interactions from the literature.
Journal of Bioinformatics and Computational Biology,
2(3):551?568.
Seon-Ho Kim, Juntae Yoon, and Jihoon Yang. 2008.
Kernel approaches for genic interaction extraction.
Bioinformatics, 24(1):118?126.
Claire Ne?dellec. 2005. Learning language in logic -
genic interaction extraction challenge. In Learning
language in logic - genic interaction extraction LLL?
2005, pages 31?37.
Sampo Pyysalo, Antti Airola, Juho Heimonen, Jari
Bjo?rne, Filip Ginter, and Tapio Salakoski. 2008.
Comparative analysis of five protein-protein interac-
tion corpora. BMC Bioinformatics, 9(3), April.
Carlos Rodr??guez-Penagos, Heladia Salgado, Irma
Mart??nez-Flores, and Julio Collado-Vides. 2007. Au-
tomatic reconstruction of a bacterial regulatory net-
work using natural language processing. BMC Bioin-
formatics, 8(293).
Kenji Sagae, Yusuke Miyao, and Junichi Tsujii. 2007.
HPSG parsing with shallow dependency constraints.
In Annual Meeting of Association for Computational
Linguistics, pages 624?631.
Jasmin S?aric?, Lars J. Jensen, Rossitza Ouzounova, Isabel
Rojas, and Peer Bork. 2004. Extracting regulatory
gene expression networks from pubmed. In ACL ?04:
Proceedings of the 42nd Annual Meeting on Associa-
tion for Computational Linguistics, page 191, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
Joachim Wermter, Katrin Tomanek, and Udo Hahn.
2009. High-performance gene name normalization
with GeNo. Bioinformatics, 25(6):815?821.
Hui Yang, Goran Nenadic, and John Keane. 2008. Iden-
tification of transcription factor contexts in literature
using machine learning approaches. BMC Bioinfor-
matics, 9(Supplement 3: S11).
45
Proceedings of the Workshop on BioNLP: Shared Task, pages 19?27,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Event Extraction from Trimmed Dependency Graphs
Ekaterina Buyko, Erik Faessler, Joachim Wermter and Udo Hahn
Jena University Language & Information Engineering (JULIE) Lab
Friedrich-Schiller-Universita?t Jena
Fu?rstengraben 30, 07743 Jena, Germany
{ekaterina.buyko|erik.faessler|joachim.wermter|udo.hahn}@uni-jena.de
Abstract
We describe the approach to event extrac-
tion which the JULIELab Team from FSU
Jena (Germany) pursued to solve Task 1 in
the ?BioNLP?09 Shared Task on Event Ex-
traction?. We incorporate manually curated
dictionaries and machine learning method-
ologies to sort out associated event triggers
and arguments on trimmed dependency graph
structures. Trimming combines pruning ir-
relevant lexical material from a dependency
graph and decorating particularly relevant lex-
ical material from that graph with more ab-
stract conceptual class information. Given
that methodological framework, the JULIELab
Team scored on 2nd rank among 24 competing
teams, with 45.8% precision, 47.5% recall and
46.7% F1-score on all 3,182 events.
1 Introduction
Semantic forms of text analytics for the life sciences
have long been equivalent with named entity recog-
nition and interpretation, i.e., finding instances of se-
mantic classes such as proteins, diseases, or drugs.
For a couple of years, this focus has been comple-
mented by analytics dealing with relation extraction,
i.e., finding instances of relations which link one or
more (usually two) arguments, the latter being in-
stances of semantic classes, such as the interaction
between two proteins (PPIs).
PPI extraction is a complex task since cascades
of molecular events are involved which are hard to
sort out. Many different approaches have already
been tried ? pattern-based ones (e.g., by Blaschke
et al (1999), Hakenberg et al (2005) or Huang et
al. (2004)), rule-based ones (e.g., by Yakushiji et al
(2001), ?Saric? et al (2004) or Fundel et al (2007)),
and machine learning-based ones (e.g., by Katrenko
and Adriaans (2006), S?tre et al (2007) or Airola et
al. (2008)), yet without conclusive results.
In the following, we present our approach to solve
Task 1 within the ?BioNLP?09 Shared Task on Event
Extraction?.1 Task 1 ?Event detection and charac-
terization? required to determine the intended rela-
tion given a priori supplied protein annotations. Our
approach considers dependency graphs as the cen-
tral data structure on which various trimming oper-
ations are performed involving syntactic simplifica-
tion but also, even more important, semantic enrich-
ment by conceptual overlays. A description of the
component subtasks is provided in Section 2, while
the methodologies intended to solve each subtask
are discussed in Section 3. The system pipeline for
event extraction reflecting the task decomposition is
described in Section 4, while Section 5 provides the
evaluation results for our approach.
2 Event Extraction Task
Event extraction is a complex task that can be sub-
divided into a number of subtasks depending on
whether the focus is on the event itself or on the ar-
guments involved:
Event trigger identification deals with the large
variety of alternative verbalizations of the same
event type, i.e., whether the event is expressed in
1http://www-tsujii.is.s.u-tokyo.ac.jp/
GENIA/SharedTask/
19
a verbal or in a nominalized form (e.g., ?A is ex-
pressed? and ?the expression of A? both refer to the
same event type, viz. expression(A)). Since the
same trigger may stand for more than one event type,
event trigger ambiguity has to be resolved as well.
Event trigger disambiguation selects the correct
event name from the set of alternative event triggers.
Event typing, finally, deals with the semantic
classification of a disambiguated event name and the
assignment to an event type category.2
Argument identification is concerned with find-
ing all necessary participants in an event, i.e., the
arguments of the relation.
Argument typing assigns the correct semantic
category (entity class) to each of the determined par-
ticipants in an event (which can be considered as in-
stances of that class).
Argument ordering assigns each identified par-
ticipant its functional role within the event, mostly
Agent (and Patient/Theme).
The sentence ?Regulation of jun and fos gene ex-
pression in human monocytes by the macrophage
colony-stimulating factor?, e.g., contains mentions
of two Gene Expression events with respective
THEME arguments ?jun? and ?fos?, triggered in the
text by the literal phrase ?gene expression?.
Task 1 of the ?BioNLP?09 Shared Task on Event
Extraction? was defined in such a way as to iden-
tify a proper relation (event) name and link it with
its type, plus one or more associated arguments de-
noting proteins. To focus on relation extraction only
no automatic named entity recognition and interpre-
tation had to be performed (subtask ?argument typ-
ing? from above); instead candidate proteins were
already pre-tagged. The complexity of Task 1 was
raised by the condition that not only proteins were
allowed to be arguments but also were events.
3 Event Extraction Solution
Our event extraction approach is summarized in Fig-
ure 1 and consists of three major streams ? first, the
detection of lexicalized event triggers (cf. Section
3.1), second, the trimming of dependency graphs
which involves pruning irrelevant and semantically
enriching relevant lexical material (cf. Section 3.2),
2In our approach, event trigger disambiguation already im-
plies event typing.
Pre-processing
Argument Identi f icat ion with
Ensemble of Classifiers
Event Detection
Post-processing
     Trimming of
Dependency Graphs
  Typing of Putative
    Event Triggers
Figure 1: General Architecture of the Event Extraction
Solution of the JULIELab Team.
and, third, the identification of arguments for the
event under scrutiny (cf. Section 3.3). Event typ-
ing results from proper event trigger identification
(see Section 3.1.2), which is interlinked with the out-
come of the argument identification. We talk about
putative triggers because we consider, in a greedy
manner, all relevant lexical items (see Section 3.1.1)
as potential event triggers which might represent an
event. Only those event triggers that can eventually
be connected to arguments, finally, represent a true
event. To achieve this goal we preprocessed both the
original training and test data such that we enrich the
original training data with automatically predicted
event triggers in order to generate more negative ex-
amples for a more effective learning of true events.3
3.1 Event Trigger Identification
Looking at the wide variety of potential lexicalized
triggers for an event, their lacking discriminative
power relative to individual event types and their
inherent potential for ambiguity,4 we decided on
a dictionary-based approach whose curation princi-
ples are described in Section 3.1.1. Our disambigua-
tion policy for the ambiguous lexicalized event trig-
3Although the training data contains cross-sentence event
descriptions, our approach to event extraction is restricted to
the sentence level only.
4Most of the triggers are neither specific for molecular event
descriptions, in general, nor for a special event type. ?Induc-
tion?, e.g., occurs 417 times in the training data. In 162 of these
cases it acts as a trigger for Positive regulation, 6 times as a
trigger for Transcription, 8 instances trigger Gene expression,
while 241 occurrences do not trigger an event at all.
20
gers assembled in this suite of dictionaries, one per
event type, is discussed in Section 3.1.2.
3.1.1 Manual Curation of the Dictionaries
We started collecting our dictionaries from the
original GENIA event corpus (Kim et al, 2008a).
The extracted event triggers were then automatically
lemmatized5 and the resulting lemmata were subse-
quently ranked by two students of biology according
to their predictive power to act as a trigger for a par-
ticular event type. This expert assessment led us to
four trigger groups (for each event type these groups
were determined separately):
(1) Triggers are important and discriminative for
a specific event type. This group contains event trig-
gers such as ?upregulate? for Positive regulation.
(2) Triggers are important though not fully dis-
criminative for a particular event type; yet, this defi-
ciency can be overcome by other lexical cues within
the context of the same sentence. This group with in-
context disambiguators contains lexical items such
as ?proteolyse? for Protein catabolism.
(3) Triggers are non-discriminative for an event
type and even cannot be disambiguated by linguistic
cues within the context of the same sentence. This
group contains lexical items such as ?presence? for
Localization and Gene expression.
(4) Triggers are absolutely non-discriminative for
an event. This group holds general lexical triggers
such as ?observe?, ?demonstrate? or ?function?.
The final dictionaries used for the detection of
putative event triggers are a union of the first two
groups. They were further extended by biologists
with additional lexical material of the first group.
The dictionaries thus became event type-specific ?
they contain all morphological forms of the original
lemma, which were automatically generated using
the Specialist NLP Tools (2008 release).
We matched the entries from the final set of dic-
tionaries with the shared task data using the Ling-
pipe Dictionary Chunker.6 After the matching pro-
cess, some cleansing had to be done.7
5We used the lemmatizer from the Specialist NLP Tools
(http://lexsrv3.nlm.nih.gov/SPECIALIST/
index.html, 2008 release).
6http://alias-i.com/lingpipe/
7Event triggers were removed which (1) were found within
sentences without any protein annotations, (2) occurred within
3.1.2 Event Trigger Disambiguation
Preliminary experiments indicated that the dis-
ambiguation of event triggers might be beneficial
for the overall event extraction results since events
tend to be expressed via highly ambiguous triggers.
Therefore, we performed a disambiguation step pre-
ceding the extraction of any argument structures.
It is based on the importance of an event trig-
ger ti for a particular event type T as defined by
Imp(tTi ) := f(t
T
i )
P
i f(tTi )
, where f(tTi ) is the frequency
of the event trigger ti of the selected event type T
in a training corpus divided by the total amount of
all event triggers of the selected event type T in
that training corpus. The frequencies are measured
on stemmed event triggers. For example, Imp for
the trigger stem ?depend? amounts to 0.013 for the
event type Positive regulation, while for the event
type Regulation it yields 0.036 . If a text span con-
tains several event triggers with the same span off-
set, the event trigger with max(Imp) is selected and
other putative triggers are discarded. The trigger
stem ?depend? remains thus only for Regulation.
3.2 Trimming Dependency Graphs
When we consider event (relation) extraction as a se-
mantic interpretation task, plain dependency graphs
as they result from deep syntactic parsing might not
be appropriate to directly extract semantic informa-
tion from. This is due to two reasons - they contain
a lot of apparently irrelevant lexical nodes (from the
semantic perspective of event extraction) and they
also contain much too specific lexical nodes that
might better be grouped and further enriched se-
mantically. Trimming dependency graphs for the
purposes of event extraction, therefore, amounts to
eliminate semantically irrelevant and to semantically
enrich relevant lexical nodes (i.e., overlay with con-
cepts). This way, we influence the final representa-
tion for the machine learners we employ (in terms of
features or kernel-based representations) ? we may
avoid an overfitting of the feature or kernel spaces
with syntactic and lexical data and thus reduce struc-
tural information in a linguistically motivated way.
a longer event trigger, (3) overlapped with a longer trigger of
the same event type, (4) occurred inside an entity mention an-
notation.
21
3.2.1 Syntactic Pruning
Pruning targets auxiliary and modal verbs which
govern the main verb in syntactic structures such as
passives, past or future tense. We delete the aux-
iliars/modals as govenors of the main verbs from
the dependency graph and propagate the semantics-
preserving dependency relations of these nodes di-
rectly to the main verbs. Adhering to the depen-
dency tree format and labeling conventions set up
for the 2006 and 2007 CONLL shared tasks on de-
pendency parsing main verbs are usually connected
with the auxiliar by the VC dependency relation (see
Figure 2). Accordingly, in our example, the verb
?activate? is promoted to the ROOT in the depen-
dency graph and governs all nodes that were origi-
nally governed by the modal ?may?.
Figure 2: Trimming of Dependency Graphs.
3.2.2 Conceptual Decoration
Lexical nodes in the (possibly pruned) depen-
dency graphs deemed to be important for argument
extraction were then enriched with semantic class
annotations, instead of keeping the original lexical
(stem) representation (see Figure 2). The rationale
behind this decision was to generate more powerful
kernel-based or features representations (see Section
3.3.2 and 3.3.1).
The whole process is based on a three-tier task-
specific semantic hierarchy of named entity classes.
The top rank is constituted by the equivalent classes
Transcription factor, Binding site, and Promoter.
The second rank is occupied by MESH terms, and
the third tier assembles the named entity classes
Gene and Protein. Whenever a lexical item is cat-
egorized by one of these categories, the associated
node in the dependency graph is overlaid with that
category applying the ranking in cases of conflicts.
We also enriched the gene name mentions with
their respective Gene Ontology Annotations from
GOA.8 For this purpose, we first categorized GO
terms both from the ?molecular function? and from
the ?biological process? branch with respect to
their matching event type, e.g., Phosphorylation
or Positive regulation. We then mapped all gene
name mentions which occurred in the text to their
UNIPROT identifier using the gene name normalizer
GENO (Wermter et al, 2009). This identifier links a
gene with a set of (curated) GO annotations.
In addition, we inserted semantic information in
terms of the event trigger type and the experimen-
tal methods. As far as experimental methods are
concerned, we extracted all instances of them an-
notated in the GENIA event corpus. One student
of biology sorted the experimental methods relative
to the event categories under scrutiny. For example
?affinity chromatography? was assigned both to the
Gene expression and to the Binding category. For
our purposes, we only included those GO annota-
tions and experimental methods which matched the
event types to be identified in a sentence.
3.3 Argument Identification and Ordering
The argument identification task can be subdivided
into three complexity levels. Level (1) incorpo-
rates five event types (Gene expression, Transcrip-
tion, Protein catabolism, Localization, Phosphory-
lation) which involve a single participant with a
THEME role only. Level (2) is concerned with one
event type (Binding) that provides an n-ary argument
structure where all arguments occupy the THEME(n)
role. Level (3) comprises three event types (Posi-
tive regulation, Negative regulation, or an unspeci-
fied Regulation) that represent a regulatory relation
between the above-mentioned event classes or pro-
teins. These events have usually a binary structure,
with a THEME argument and a CAUSE argument.
For argument extraction, we built sentence-wise
pairs of putative triggers and their putative argu-
ment(s), the latter involving ontological informa-
tion about the event type. For Level (1), we built
pairs only with proteins, while for Level (3) we al-
8http://www.ebi.ac.uk/GOA
22
lowed all events as possible arguments. For Level
(2), Binding events, we generated binary (trigger,
protein) pairs as well as triples (trigger, protein1,
protein2) to adequately represent the binding be-
tween two proteins.9 Pairs of mentions not con-
nected by a dependency path could not be detected.
For the argument extraction we chose two ma-
chine learning-based approaches, feature-based and
a kernel-based one, as described below.10
3.3.1 Feature-based Classifier
We distinguished three groups of features. First,
lexical features (covering lexical items before, af-
ter and between both mentions (of the event trigger
and an argument) as described by Zhou and Zhang
(2007)); second, chunking features (concerned with
head words of the phrases between two mentions as
described by Zhou and Zhang (2007)); third, de-
pendency parse features (considering both the se-
lected dependency levels of the arguments (parents
and least common subsumer) as discussed by Ka-
trenko and Adriaans (2006), as well as a shortest de-
pendency path structure between the arguments as
used by Kim et al (2008b) for walk features).
For the feature-based approach, we chose the
Maximum Entropy (ME) classifier from MALLET.11
3.3.2 Graph Kernel Classifier
The graph kernel uses a converted form of depen-
dency graphs in which each dependency node is rep-
resented by a set of labels associated with that node.
The dependency edges are also represented as nodes
in the new graph such that they are connected to the
nodes adjacent in the dependency graph. Subgraphs
which represent, e.g., the linear order of the words
in the sentence can be added, if required. The entire
graph is represented in terms of an adjacency matrix
which is further processed to contain the summed
weights of paths connecting two nodes of the graph
(see Airola et al (2008) for details).
9We did not account for the binding of more than two pro-
teins as this would have led to a combinatory explosion of pos-
sible classifications.
10In our experiments, we used full conceptual overlaying
(see Section 3.2) for the kernel-based representation and partial
overlaying for the dependency parse features (only gene/protein
annotation was exploited here). Graph representations allow for
many semantic labels to be associated with a node.
11http://mallet.cs.umass.edu/index.php/
Main_Page
Figure 3: Graph Kernel Representation for a Trimmed
Dependency Graph ? (1) original representation, (2)
representation without graph dependency edge nodes
(weights (0.9, 0.3) taken from Airola et al (2008)).
For our experiments, we tried some variants of the
original graph kernel. In the original version each
dependency graph edge is represented as a node.
That means that connections between graph token
nodes are expressed through graph dependency edge
nodes (see Figure 3; (1)). To represent the connec-
tions between original tokens as direct connections
in the graph, we removed the edge nodes and each
token was assigned the edge label (its dependency
label; see Figure 3; (2)). Further variants included
encodings for (1) the shortest dependency path (sp)
between two mentions (argument and trigger)12 (2)
the complete dependency graph (sp-dep), and (3) the
complete dependency graph and linear information
(sp-dep-lin) (the original configuration from Airola
et al (2008)).
For the graph kernel, we chose the LibSVM
(Chang and Lin, 2001) Support Vector Machine as
classifier.
3.4 Postprocessing
The postprocessing step varies for the three different
Levels (see Section 3.3). For every event trigger of
Level (1) (e.g., Gene expression), we generate one
event per relation comprising a trigger and its argu-
ment. For Level (2) (Binding), we create a Binding
event with two arguments only for triples (trigger,
protein1, protein2). For the third Level, we create
for each event trigger and its associated arguments
e = n ? m events, for n CAUSE arguments and m
THEME arguments.
12For Binding we extracted the shortest path between two
protein mentions if we encounter a triple (trigger, protein1,
protein2).
23
4 Pipeline
The event extraction pipeline consists of two ma-
jor parts, a pre-processor and the dedicated event
extractor. As far as pre-processing is concerned,
we imported the sentence splitting, tokenization and
GDep parsing results (Sagae and Tsujii, 2007) as
prepared by the shared task organizers for all data
sets (training, development and test). We processed
this data with the OpenNLP POS tagger and Chun-
ker, both re-trained on the GENIA corpus (Buyko et
al., 2006). Additionally, we enhanced the original
tokenization by one which includes hyphenization
of lexical items such as in ?PMA-dependent?. 13
The data was further processed with the gene nor-
malizer GENO(Wermter et al, 2009) and a num-
ber of regex- and dictionary-based entity taggers
(covering promoters, binding sites, and transcrip-
tion factors). We also enriched gene name men-
tions with their respective Gene Ontology annota-
tions (see Section 3.2.2). The MESH thesaurus (ex-
cept chemical and drugs branch) was mapped on the
data using the Lingpipe Dictionary Chunker.14
After preprocessing, event extraction was started
distinguishing between the event trigger recognition
(cf. Section 3.1), the trimming of the dependency
graphs (cf. Section 3.2), and the argument extrac-
tion proper (cf. Section 3.3).15 We determined in
our experiments on the development data the perfor-
mance of every classifier type and its variants (for
the graph kernel), and of ensembles of the most per-
formant (F-Score) graph kernel variant and an ME
model.16 We present here the argument extraction
configuration used for the official run.17 For the
prediction of Phosphorylation, Localization, Pro-
tein catabolism types we used the graph kernel in
13This tokenization is more advantageous for the detection
of additional event triggers as it allows to generate depen-
dency relations from hyphenated terms. For example, in ?PMA-
dependent?, ?PMA? will be a child of ?dependent? linked by
the AMOD dependency relation, and ?dependent? receives the
original dependency relation of the ?PMA-dependent? token.
14http://alias-i.com/lingpipe/
15For the final configurations of the graph kernel, we opti-
mized the C parameter in the spectrum between 2?3 and 23 on
the final training data for every event type separately.
16In the ensemble configuration we built the union of positive
instances.
17We achieved with this configuration the best performance
on the development set.
its ?sp without dependency-edge-nodes? configura-
tion, while for the prediction of Transcription and
Gene expression events we used an ensemble of the
graph kernel in its ?sp with dependency-edge-nodes?
variant, and an ME model. For the prediction of
Binding we used an ensemble of the graph kernel
(?sp-dep with dependency-edge-nodes?) and an ME
model. For the prediction of regulatory events we
used ME models for each regulatory type.
5 Results
The baseline against which we compared our ap-
proach can be captured in a single rule. We extract
for every pair of a putative trigger and a putative ar-
gument the shortest dependency path between them.
If the shortest dependency path does not contain any
direction change, i.e., the argument is either a direct
child or a direct parent of the trigger, and if the path
does not contain any other intervening event trig-
gers, the argument is taken as the THEME role.
We performed evaluations on the shared task de-
velopment and test set. Our baseline achieved com-
petitive results of 36.0% precision, 34.0% recall,
35.0% F-score on the development set (see Table
1), and 30.4% precision, 35.7% recall, 32,8% F-
score on the test set (see Table 2). In particular
the one-argument events, i.e., Gene expression, Pro-
tein catabolism, Phosphorylation are effectively ex-
tracted with an F-score around 70.0%. More com-
plex events, in particular events of Level (3), i.e.,
(Regulation) were less properly dealt with because
of their strong internal complexity.
Event Class gold recall prec. F-score
Localization 53 75.47 30.30 43.24
Binding 248 33.47 20.80 25.66
Gene expression 356 76.12 75.07 75.59
Transcription 82 68.29 40.58 50.91
Protein catabolism 21 76.19 66.67 71.11
Phosphorylation 47 76.60 72.00 74.23
Regulation 169 14.20 15.09 14.63
Positive regulation 617 15.40 20.83 17.71
Negative regulation 196 11.73 13.22 12.43
TOTAL 1789 36.00 34.02 34.98
Table 1: Baseline results on the shared task development
data. Approximate Span Matching/Approximate Recur-
sive Matching.
24
Event Class gold recall prec. F-score gold recall prec. F-score
Localization 174 42.53 44.85 43.66 174 42.53 44.85 43.66
Binding 347 32.28 37.09 34.51 398 44.22 58.28 50.29
Gene expression 722 61.36 80.55 69.65 722 61.36 80.55 69.65
Transcription 137 39.42 35.06 37.11 137 39.42 35.06 37.11
Protein catabolism 14 71.43 66.67 68.97 14 71.43 66.67 68.97
Phosphorylation 135 65.93 90.82 76.39 135 65.93 90.82 76.39
EVT-TOTAL 1529 51.14 60.90 55.60 1580 53.54 65.89 59.08
Regulation 291 9.62 11.72 10.57 338 9.17 12.97 10.75
Positive regulation 983 10.38 11.33 10.83 1186 14.67 19.33 16.68
Negative regulation 379 14.25 19.22 16.36 416 14.18 21.00 16.93
REG-TOTAL 1653 11.13 12.96 11.98 1940 13.61 18.59 15.71
ALL-TOTAL 3182 30.36 35.72 32.82 3520 31.53 41.05 35.67
Table 2: Baseline results on the shared task test data. Approximate Span Matching/Approximate Recursive Matching
(columns 3-5). Event decomposition, Approximate Span Matching/Approximate Recursive Matching (columns 7-9).
The event extraction approach, in its final config-
uration (see Section 4), achieved a performance of
50.4% recall, 45.8% precision and 48.0% F-score on
the development set (see Table 4), and 45.8% recall,
47.5% precision and 46.7% F-score on the test set
(see Table 3). This approach clearly outperformed
the baseline with an increase of 14 percentage points
on the test data. In particular, the events of Level (2)
and (3) were more properly dealt with than by the
baseline. In the event decomposition mode (argu-
ment detection is evaluated in a decomposed event)
we achieved a performance of 49.4% recall, 56.2%
precision, and 52.6% F-score (see Table 3).
Our experiments on the development set showed
that the combination of the feature-based and the
graph kernel-based approach can boost the results up
to 6 percentage points F-score (for the Binding event
type). It is interesting that the combination for Bind-
ing increased recall without dropping precision. The
original graph kernel approach for Binding events
performs with 38.3% recall, 27.9% precision and
32.3% F-score on the development set. The com-
bined approach comes with a remarkable increase
of 14 percentage points in recall. The combination
could also boost the recall of the Gene expression
and Transcription by 15 percentage points and 5 per-
centage points, respectively, without seriously drop-
ping the precision (4 points for every type). For
the other event types, no improvements were found
when we combined both approaches.
5.1 Error Discussion
One expert biologist analyzed 30 abstracts randomly
extracted from the development error data. We de-
termined seven groups of errrors based on this anal-
ysis. The first group contains examples for which
an event should be determined, but a false argument
was found (e.g., Binding arguments were not prop-
erly sorted, or correct and false arguments were de-
tected for the same trigger) (44 examples). The sec-
ond group comprised examples where no trigger was
found (23 examples). Group (3) stands for cases
where no events were detected although a trigger
was properly identified (14 examples). Group (4)
holds examples detected in sentences which did not
contain any events (12 examples). Group (5) lists bi-
ologically meaningful analyses, actually very close
to the gold annotation, especially for the cascaded
regulatory events (12 examples), while Group (6) in-
corporates examples of a detected event with incor-
rect type (1 example). Group (7) gathers misleading
gold annotations (10 examples).
This assessment clearly indicates that a major
source of errors can be traced to the level of argu-
ment identification, in particular for Binding events.
The second major source has its offspring at the
level of trigger detection (we ignored, for exam-
ple, triggers such as ?in the presence of ?, ?when?,
?normal?). About 10% of the errors are due to a
slight difference between extracted events and gold
events. For example, in the phrase ?role for NF-
kappaB in the regulation of FasL expression? we
25
Event Class gold recall prec. F-score gold recall prec. F-score
Localization 174 43.68 77.55 55.88 174 43.68 77.55 55.88
Binding 347 49.57 35.25 41.20 398 63.57 54.88 58.91
Gene expression 722 64.82 80.27 71.72 722 64.82 80.27 71.72
Transcription 137 35.77 62.03 45.37 137 35.77 62.03 45.37
Protein catabolism 14 78.57 84.62 81.48 14 78.57 84.62 81.48
Phosphorylation 135 76.30 91.15 83.06 135 76.30 91.15 83.06
EVT-TOTAL 1529 57.49 63.97 60.56 1580 60.76 71.27 65.60
Regulation 291 31.27 30.13 30.69 338 35.21 37.54 36.34
Positive regulation 983 34.08 37.18 35.56 1186 40.64 49.33 44.57
Negative regulation 379 40.37 31.16 35.17 416 42.31 39.11 40.65
REG-TOTAL 1653 35.03 34.18 34.60 1940 40.05 44.55 42.18
ALL-TOTAL 3182 45.82 47.52 46.66 3520 49.35 56.20 52.55
Table 3: Offical Event Extraction results on the shared task test data of the JULIELab Team. Approximate
Span Matching/Approximate Recursive Matching (columns 3-5). Event decomposition, Approximate Span Match-
ing/Approximate Recursive Matching (columns 7-9).
could not extract the gold event Regulation of Regu-
lation (Gene expression (FasL)) associated with the
trigger ?role?, but we were able to find the (inside)
event Regulation (Gene expression (FasL)) associ-
ated with the trigger ?regulation?. Interestingly, the
typing of events is not an error source in spite of
the simple disambiguation approach. Still, our dis-
ambiguation strategy is not appropriate for the anal-
ysis of double-annotated triggers such as ?overex-
pression?, ?transfection?, etc., which are annotated
as Gene expression and Positive regulation and are
a major source of errors in Group (2). As Group
(6) is an insignificant source of errors in our ran-
domly selected data, we focused our error analysis
on the especially ambiguous event type Transcrip-
tion. We found from 34 errors that 14 of them were
due to the disambiguation strategy (in particular for
triggers ?(gene) expression? and ?induction?).
6 Conclusion
Our approach to event extraction incorporates man-
ually curated dictionaries and machine learning
methodologies to sort out associated event triggers
and arguments on trimmed dependency graph struc-
tures. Trimming combines pruning irrelevant lexi-
cal material from a dependency graph and decorat-
ing particularly relevant lexical material from that
graph with more abstract conceptual class informa-
tion. Given that methodological framework, the
JULIELab Team scored on 2nd rank among 24 com-
Event Class gold recall prec. F-score
Localization 53 71.70 74.51 73.08
Binding 248 52.42 29.08 37.41
Gene expression 356 75.28 81.46 78.25
Transcription 82 60.98 73.53 66.67
Protein catabolism 21 90.48 79.17 84.44
Phosphorylation 47 82.98 84.78 83.87
Regulation 169 37.87 36.78 37.32
Positive regulation 617 34.36 35.99 35.16
Negative regulation 196 41.33 33.61 37.07
TOTAL 1789 50.36 45.76 47.95
Table 4: Event extraction results on the shared task
development data of the official run of the JULIELab
Team. Approximate Span Matching/Approximate Recur-
sive Matching.
peting teams, with 45.8% precision, 47.5% recall
and 46.7% F1-score on all 3,182 events.
7 Acknowledgments
We wish to thank Rico Landefeld for his technical
support, Tobias Wagner and Rico Pusch for their
constant help and great expertise in biological is-
sues. This research was partially funded within the
BOOTSTREP project under grant FP6-028099 and
the CALBC project under grant FP7-231727.
References
Antti Airola, Sampo Pyysalo, Jari Bjo?rne, Tapio
Pahikkala, Filip Ginter, and Tapio Salakoski. 2008. A
26
graph kernel for protein-protein interaction extraction.
In Proceedings of the Workshop on Current Trends in
Biomedical Natural Language Processing, pages 1?9.
Christian Blaschke, Miguel A. Andrade, Christos Ouzou-
nis, and Alfonso Valencia. 1999. Automatic ex-
traction of biological information from scientific text:
Protein-protein interactions. In ISMB?99 ? Proceed-
ings of the 7th International Conference on Intelligent
Systems for Molecular Biology, pages 60?67.
Ekaterina Buyko, Joachim Wermter, Michael Poprat, and
Udo Hahn. 2006. Automatically adapting an NLP
core engine to the biology domain. In Proceedings
of the Joint BioLINK-Bio-Ontologies Meeting. A Joint
Meeting of the ISMB Special Interest Group on Bio-
Ontologies and the BioLINK Special Interest Group on
Text Data M ining in Association with ISMB, pages
65?68. Fortaleza, Brazil, August 5, 2006.
Chih-Chung Chang and Chih-Jen Lin, 2001. LIB-
SVM: a library for support vector machines. Soft-
ware available at http://www.csie.ntu.edu.
tw/
?
cjlin/libsvm.
Katrin Fundel, Robert Ku?ffner, and Ralf Zimmer.
2007. Relex-relation extraction using dependency
parse trees. Bioinformatics, 23(3):365?371.
Jo?rg Hakenberg, Ulf Leser, Conrad Plake, Harald Kirsch,
and Dietrich Rebholz-Schuhmann. 2005. LLL?05
challenge: Genic interaction extraction - identifica-
tion of language patterns based on alignment and finite
state automata. In Proceedings of the 4th Learning
Language in Logic Workshop (LLL05), pages 38?45.
Minlie Huang, Xiaoyan Zhu, Donald G. Payan, Kun-
bin Qu, and Ming Li. 2004. Discovering patterns
to extract protein-protein interactions from full texts.
Bioinformatics, 20(18):3604?3612.
Sophia Katrenko and Pieter W. Adriaans. 2006. Learn-
ing relations from biomedical corpora using depen-
dency trees. In Karl Tuyls, Ronald L. Westra, Yvan
Saeys, and Ann Nowe?, editors, KDECB 2006 ? Knowl-
edge Discovery and Emergent Complexity in Bioin-
formatics. Revised Selected Papers of the 1st Inter-
national Workshop., volume 4366 of Lecture Notes
in Computer Science, pages 61?80. Ghent, Belgium,
May 10, 2006. Berlin: Springer.
Jin-Dong Kim, Tomoko Ohta, and Jun?ichi Tsujii. 2008a.
Corpus annotation for mining biomedical events from
literature. BMC Bioinformatics, 9(10).
Seon-Ho Kim, Juntae Yoon, and Jihoon Yang. 2008b.
Kernel approaches for genic interaction extraction.
Bioinformatics, 24(1):118?126.
Rune S?tre, Kenji Sagae, and Jun?ichi Tsujii. 2007. Syn-
tactic features for protein-protein interaction extrac-
tion. In Christopher J. O. Baker and Jian Su, editors,
LBM 2007, volume 319, pages 6.1?6.14.
Kenji Sagae and Jun?ichi Tsujii. 2007. Dependency pars-
ing and domain adaptation with LR models and par ser
ensembles. In Proceedings of the CoNLL Shared Task
Session of EMNLP-CoNLL 2007, pages 1044?1050.
Jasmin ?Saric?, Lars J. Jensen, Rossitza Ouzounova, Isabel
Rojas, and Peer Bork. 2004. Extracting regulatory
gene expression networks from pubmed. In ACL ?04:
Proceedings of the 42nd Annual Meeting on Associa-
tion for Computational Linguistics, page 191, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
Joachim Wermter, Katrin Tomanek, and Udo Hahn.
2009. High-performance gene name normalization
with GeNo. Bioinformatics, 25(6):815?821.
Akane Yakushiji, Yuka Tateisi, Yusuke Miyao, and
Jun?ichi Tsujii. 2001. Event extraction from biomed-
ical papers using a full parser. In Russ B. Altman,
A. Keith Dunker, Lawrence Hunter, Kevin Lauderdale,
and Teri E. Klein, editors, PSB 2001 ? Proceedings
of the 6th Pacific Symposium on Biocomputing, pages
408?419. Maui, Hawaii, USA. January 3-7, 2001. Sin-
gapore: World Scientific Publishing.
Guodong Zhou and Min Zhang. 2007. Extracting re-
lation information from text documents by exploring
various types of knowledge. Information Processing
& Management, 43(4):969?982.
27
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 982?992,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Evaluating the Impact of Alternative Dependency Graph Encodings on
Solving Event Extraction Tasks
Ekaterina Buyko and Udo Hahn
Jena University Language & Information Engineering (JULIE) Lab
Friedrich-Schiller-Universita?t Jena
Fu?rstengraben 30, 07743 Jena, Germany
{ekaterina.buyko|udo.hahn}@uni-jena.de
Abstract
In state-of-the-art approaches to information
extraction (IE), dependency graphs constitute
the fundamental data structure for syntactic
structuring and subsequent knowledge elicita-
tion from natural language documents. The
top-performing systems in the BioNLP 2009
Shared Task on Event Extraction all shared the
idea to use dependency structures generated
by a variety of parsers ? either directly or
in some converted manner ? and optionally
modified their output to fit the special needs
of IE. As there are systematic differences be-
tween various dependency representations be-
ing used in this competition, we scrutinize on
different encoding styles for dependency in-
formation and their possible impact on solv-
ing several IE tasks. After assessing more
or less established dependency representations
such as the Stanford and CoNLL-X dependen-
cies, we will then focus on trimming opera-
tions that pave the way to more effective IE.
Our evaluation study covers data from a num-
ber of constituency- and dependency-based
parsers and provides experimental evidence
which dependency representations are partic-
ularly beneficial for the event extraction task.
Based on empirical findings from our study
we were able to achieve the performance of
57.2% F-score on the development data set of
the BioNLP Shared Task 2009.
1 Introduction
Relation and event extraction are among the most
demanding semantics-oriented NLP challenge tasks
(both in the newspaper domain such as for ACE1, as
well as in the biological domain such as for BioCre-
ative2 or the BioNLP Shared Task3), comparable in
terms of analytical complexity with recent efforts di-
rected at opinion mining (e.g., NTCIR-74 or TREC
Blog tracks5) or the recognition of textual entail-
ment.6 The most recent BioNLP 2009 Shared Task
on Event Extraction (Kim et al, 2009) required, for
a sample of 260 MEDLINE abstracts, to determine
all mentioned events ? to be chosen from a given
set of nine event types, including ?Localization?,
?Binding?, ?Gene Expression?, ?Transcription?,
?Protein Catabolism?, ?Phosphorylation?, ?Posi-
tive Regulation?, ?Negative Regulation?, and (un-
specified) ?Regulation? ? and link them appropri-
ately with a priori supplied protein annotations. The
demands on text analytics to deal with the complex-
ity of this Shared Task in terms of relation diversity
and specificity are unmatched by former challenges.
For relation extraction in the biomedical domain
(the focus of our work), a stunning convergence
towards dependency-based syntactic representation
structures is witnessed by the performance results
of the top-performing systems in the BioNLP?09
1http://papers.ldc.upenn.edu/LREC2004/
ACE.pdf
2http://biocreative.sourceforge.net/
3www-tsujii.is.s.u-tokyo.ac.jp/GENIA/
SharedTask/
4http://research.nii.ac.jp/ntcir/
workshop/OnlineProceedings7/pdf/revise/
01-NTCIR-OV-MOAT-SekiY-revised-20081216.
pdf
5http://trec.nist.gov/data/blog08.html
6http://pascallin.ecs.soton.ac.uk/
Challenges/RTE/
982
Shared Task on Event Extraction.7 Regarding the
fact that dependency representations were always
viewed as a vehicle to represent fundamental seman-
tic relationships already at the syntactic level, this
is not a great surprise. Yet, dependency grammar
is not a monolithic, consensually shaped and well-
defined linguistic theory. Accordingly, associated
parsers tend to vary in terms of dependency pairing
or structuring (which pairs of words join in a depen-
dency relation?) and dependency typing (how are
dependency relations for a particular pair labelled?).
Depending on the type of dependency theory or
parser being used, various representations emerge
(Miyao et al, 2007). In this paper, we explore these
different representations of the dependency graphs
and try, first, to pinpoint their effects on solving the
overall event extraction task and, second, to further
enhance the potential of JREX, a high-performance
relation and event extractor developed at the JULIE
Lab (Buyko et al, 2009).
2 Related Work
In the biomedical domain, the focus has largely been
on binary relations, in particular protein-protein
interactions (PPIs). Accordingly, the biomedi-
cal NLP community has developed various PPI-
annotated corpora (e.g., LLL (Ne?dellec, 2005),
AIMED (Bunescu et al, 2005), BIOINFER (Pyysalo
et al, 2007)). PPI extraction does clearly not count
as a solved problem, and a deeper look at its bio-
logical and representational intricacies is certainly
worthwhile. The GENIA event corpus (Kim et al,
2008) and the BioNLP 2009 Shared Task data (Kim
et al, 2009) contain such detailed annotations of
PPIs (amongst others).
The BioNLP Shared Task was a first step towards
the extraction of specific pathways with precise in-
formation about the molecular events involved. In
that task, 42 teams participated and 24 of them sub-
mitted final results. The winner system, TURKU
(Bjo?rne et al, 2009), achieved with 51.95% F-score
the milestone result in that competition followed by
the JULIELab system (Buyko et al, 2009) which
peaked at 46.7% F-score. Only recently, an ex-
tension of the TURKU system, the TOKYO system,
7www-tsujii.is.s.u-tokyo.ac.jp/GENIA/
SharedTask/results/results-master.html
has been realized (Miwa et al, 2010). TOKYO sys-
tem?s event extraction capabilities are based on the
TURKU system, yet TURKU?s manually crafted rule
system for post-processing and the combination of
extracted trigger-argument relations is replaced by
a machine learning approach in which rich features
collected from classification steps for triggers and
arguments are re-combined. TOKYO achieves an
overall F-score of 53.29% on the test data, thus out-
performing TURKU by 1.34 percentage points.
The three now top-performing systems, TOKYO,
TURKU and JULIELab, all rely on dependency
graphs for solving the event extraction tasks. While
the TURKU system exploits the Stanford dependen-
cies from the McClosky-Charniak parser (Charniak
and Johnson, 2005), and the JULIELab system uses
the CoNLL-like dependencies from the GDep parser
(Sagae and Tsujii, 2007),8 the TOKYO system over-
lays the Shared Task data with two parsing represen-
tations, viz. Enju PAS structure (Miyao and Tsujii,
2002) and GDep parser dependencies. Obviously,
one might raise the question as to what extent the
performance of these systems depends on the choice
of the parser and its output representations. Miyao
et al (2008) already assessed the impact of different
parsers for the task of biomedical relation extraction
(PPI). Here we perform a similar study for the task
of event extraction and focus, in particular, on the
impact of various dependency representations such
as Stanford and CoNLL?X dependencies and addi-
tional trimming procedures.
For the experiments on which we report here, we
performed experiments with the JULIELab system.
Our main goal is to investigate into the crucial role
of proper representation structures for dependency
graphs so that the performance gap from Shared
Task results between the best-performing TOKYO
system and the JULIELab system be narrowed.
3 Event Extraction
3.1 Objective
Event extraction is a complex task that can be sub-
divided into a number of subtasks depending on
8The GDep parser has been trained on the GENIA Tree-
bank pre-official version of the version 1.0 converted with the
script available from http://w3.msi.vxu.se/
?
nivre/
research/Penn2Malt.html
983
whether the focus is on the event itself or on the ar-
guments involved:
Event trigger identification deals with the large
variety of alternative verbalizations of the same
event type, e.g., whether the event is expressed in a
verbal or in a nominalized form (?A is expressed?
as well as ?the expression of A? both refer to the
same event type, viz. Expression(A)). Since the
same trigger may stand for more than one event type,
event trigger ambiguity has to be resolved as well.
Event trigger disambiguation selects the correct
event name from the set of alternative event triggers.
Argument identification is concerned with find-
ing all necessary participants in an event, i.e., the
arguments of the relation.
Argument ordering assigns each identified par-
ticipant its functional role within the event, mostly
Agent and Patient.
3.2 JULIELab System
The JULIELab solution can best be characterized as
a single-step learning approach for event detection
as the system does not separate the overall learn-
ing task into independent event trigger and event
argument learning subtasks.9 The JULIELab sys-
tem incorporates manually curated dictionaries and
machine learning (ML) methodologies to sort out
associated event triggers and arguments on depen-
dency graph structures. For argument extraction, the
JULIELab system uses two ML-based approaches,
a feature-based and a kernel-based one. Given
that methodological framework, the JULIELab team
scored on 2nd rank among 24 competing teams, with
45.8% precision, 47.5% recall and 46.7% F-score on
all 3,182 events. After the competition, this system
was updated and achieved 57.6% precision, 45.7%
recall and 51.0% F-score (Buyko et al, 2010) using
modified dependency representations from the MST
parser (McDonald et al, 2005). In this study, we
perform event extraction experiments with various
dependency representations that allow us to measure
their effects on the event extraction task and to in-
crease the overall JULIELab system performance in
terms of F-score.
9The JULIELab system considers all relevant lexical items as
potential event triggers which might represent an event. Only
those event triggers that can eventually be connected to argu-
ments, finally, represent a true event.
4 Dependency Graph Representations
In this section, we focus on representation formats
of dependency graphs. In Section 4.1, we introduce
fundamental notions underlying dependency pars-
ing and consider established representation formats
for dependency structures as generated by various
parsers. In Section 4.2, we account for selected trim-
ming operations for dependency graphs to ease IE.
4.1 Dependency Structures: Representation
Issues
Dependency parsing, in the past years, has increas-
ingly been recognized as an alternative to long-
prevailing constituency-based parsing approaches,
particularly in semantically-oriented application
scenarios such as information extraction. Yet even
under purely methodologically premises, it has
gained wide-spread attention as witnessed by recent
activities performed as part of the ?CoNLL Shared
Tasks on Multilingual Dependency Parsing? (Buch-
holz and Marsi, 2006).
In a nutshell, in dependency graphs of sentences,
nodes represent single words and edges account for
head-modifier relations between single words. De-
spite this common understanding, concrete syntactic
representations often differ markedly from one de-
pendency theory/parser to the other. The differences
fall into two main categories: dependency pairing or
structuring (which pairs of words join in a depen-
dency relation?) and dependency typing (how are
dependency relations for a particular pair labelled?).
The CoNLL?X dependencies, for example, are
defined by 54 relation types,10 while the Stanford
scheme (de Marneffe et al, 2006) incorporates 48
types (so called grammatical relations or Stanford
dependencies). The Link Grammar Parser (Sleator
and Temperley, 1991) employs a particularly fine-
grained repertoire of dependency relations adding
up to 106 types, whereas the well-known MINIPAR
parser (Lin, 1998) relies on 59 types. Differences in
dependency structure are at least as common as dif-
ferences in dependency relation typing (see below).
10Computed by using the conversion script on WSJ
data (accessible via http://nlp.cs.lth.se/
pennconverter/; see also Johansson and Nugues (2007)
for additional information). From the GENIA corpus, using this
script, we could only extract 29 CoNLL dependency relations.
984
Figure 1: Example of CoNLL 2008 dependencies, as used in most of the native dependency parsers.
Figure 2: Stanford dependencies, basic conversion from Penn Treebank.
In general, dependency graphs can be generated
by syntactic parsers in two ways. First, native de-
pendency parsers output CoNLL?X or Stanford de-
pendencies dependent on which representation for-
mat they have been trained on.11 Second, in a deriva-
tive dependency mode, the output of constituency-
based parsers, e.g., phrase structure representations,
is subsequently converted either into CoNLL?X or
Stanford dependencies using Treebank conversion
scripts (see below). In the following, we provide
a short description of these two established depen-
dency graph representations:
? CoNLL?X dependencies (CD). This depen-
dency tree format was used in the CoNLL?X
Shared Tasks on multi-lingual dependency
parsing (see Figure 1). It has been adopted
by most native dependency parsers and was
originally obtained from Penn Treebank (PTB)
trees using constituent-to-dependency conver-
sion (Johansson and Nugues, 2007). It differs
slightly in the number and types of dependen-
cies being used from various CoNLL rounds
(e.g., CoNLL?08 provided a dependency type
for representing appositions).12
? Stanford dependencies (SD). This format was
proposed by de Marneffe et al (2006) for
11We disregard in this study other dependency representa-
tions such as MINIPAR and LINK GRAMMAR representations.
12For the differences between CoNLL?07 and CoNLL?08 rep-
resentations, cf. http://nlp.cs.lth.se/software/
treebank_converter/
semantics-sensitive applications using depen-
dency representations, and can be obtained us-
ing the Stanford tools13 from PTB trees. The
Stanford format is widely used in the biomed-
ical domain (e.g., by Miyao et al (2008) or
Clegg and Shepherd (2005)).
There are systematic differences between
CoNLL?X and Stanford dependencies, e.g., as far
as the representation of passive constructions, the
position of auxiliary and modal verbs, or coordi-
nation representation is concerned. In particular,
the representation of the passive construction and
the role of the auxiliary verb therein may have
considerable effects for semantics-sensitive tasks.
While in SD the subject of the passive construction
is represented by a special nsubj dependency
label, in CD we find the same subject label as for
active constructions SUB(J). On CoNLL?08 data,
the logical subject is marked by the LGS depen-
dency edge that connects the passive-indicating
preposition ?by? with the logical subject of the
sentence.
The representation of active constructions are
similar in CD and SD though besides the role of
auxiliary and modal verbs. In the Stanford de-
pendency representation scheme, rather than taking
auxiliaries to be the heads in passive or tense con-
structions, main verbs are assigned this grammatical
function (see Figure 2). The CoNLL?X represen-
13Available from nlp.stanford.edu/software/
lex-parser.shtml
985
Figure 3: Noun phrase representation in
CoNLL?X dependency trees.
Figure 4: Trimming procedure noun phrase on
CoNLL?X dependency trees.
tation scheme is completely different in that auxil-
iaries ? much in common with standard dependency
theory ? are chosen to occupy the role of the gov-
ernor (see Figure 1). From the perspective of rela-
tion extraction, however, the Stanford scheme is cer-
tainly closer to the desired predicate-argument struc-
ture representations than the CoNLL scheme.
4.2 Dependency Graph Modifications in Detail
Linguistic intuition suggests that the closer a depen-
dency representation is to the format of the targeted
semantic representation, the more likely will it sup-
port the semantic application. This idea is directly
reflected in the Stanford dependencies which narrow
the distance between nodes in the dependency graph
by collapsing procedures (the so-called collapsed
mode of phrase structure conversion). An example
of collapsing is the conversion of ?expression nmod????
in pmod???? cells? to ?expression prep in????? cells?. An ex-
tension of collapsing is the re-structuring of coor-
dinations with sharing the dependency relations of
conjuncts (the so-called ccprocessed mode of phrase
structure conversion).
According to the Stanford scheme, Buyko et al
(2009) proposed collapsing scenarios on CoNLL?X
dependency graphs. Their so-called trimming op-
erations treat three syntactic phenomena, viz. coor-
dinations (coords), auxiliaries/modals (auxiliaries),
and prepositions (preps). For coordinations, they
propagate the dependency relation of the first con-
junct to all the other conjuncts within the coordi-
nation. For auxiliaries/modals, they prune the aux-
iliaries/modals as governors from the dependency
graph and propagate the dependency relations of
these nodes to the main verbs. Finally, for preposi-
tions, they collapse a pair of typed dependencies into
a single typed dependency (as illustrated above).
For the following experiments, we extended the
trimming procedures and propose the re-structuring
of noun phrases with action adjectives to make the
dependency representation even more compact for
semantic interpretation. The original dependency
representation of the noun phrase selects the right-
most noun as the head of the NP and thus all re-
maining elements are its dependents (see Figure 3).
For the noun phrases containing action adjectives
(mostly verb derivations) this representation does
not reflect the true semantic relations between the
elements. For example, in ?IL-10 mediated expres-
sion? it is ?IL-10? that mediates the expression.
Therefore, we re-structure the dependency graph by
changing the head of ?IL-10? from ?expression?
to ?mediated?. Our re-coding heuristics selects,
first, all the noun phrases containing action adjec-
tives ending with ?-ed?, ?-ing?, ?-ible? suffixes and
with words such as ?dependent?, ?specific?, ?like?.
In the second step, we re-structure the noun phrase
by encoding the adjective as the head of all the nouns
preceding this adjective in the noun phrase under
scrutiny (see Figure 4).
5 Experiments and Results
In this section, we describe the experiments and
results related to event extraction tasks based on
alternative dependency graph representations. For
our experiments, we selected the following top-
performing parsers ? the first three phrase structure
based and thus the origin of derivative dependency
structures, the last three fully dependency based for
making native dependency structures available:
? C+J, Charniak and Johnson?s reranking parser
(Charniak and Johnson, 2005), with the WSJ-
trained parsing model.
? M+C, Charniak and Johnson?s reranking parser
(Charniak and Johnson, 2005), with the self-
trained biomedical parsing model from Mc-
Closky (2010).
986
? Bikel, Bikel?s parser (Bikel, 2004) with the
WSJ-trained parsing model.
? GDep (Sagae and Tsujii, 2007), a native depen-
dency parser.
? MST (McDonald et al, 2005), another native
dependency parser.
? MALT (Nivre et al, 2007), yet another native
dependency parser.
The native dependency parsers were re-trained on
the GENIA Treebank (Tateisi et al, 2005) conver-
sions.14 These conversions,15 i.e., Stanford basic,
CoNLL?07 and CoNLL?08 were produced with the
currently available conversion scripts. For the Stan-
ford dependency conversion, we used the Stanford
parser tool,16 for CoNLL?07 and CoNLL?08 we used
the treebank-to-CoNLL conversion scripts17 avail-
able from the CoNLL?X Shared Task organizers.
The phrase structure based parsers were applied
with already available models, i.e., the Bikel and
C+J parsers as trained on the WSJ corpus, and
M+C as trained on the GENIA Treebank corpus.
For our experiments, we converted the prediction
results of the phrase structure based parsers into
five dependency graph representations, viz. Stanford
basic, Stanford collapsed, Stanford ccprocessed,
CoNLL?07 and CoNLL?08, using the same scripts
as for the conversion of the GENIA Treebank.
The JULIELab event extraction system was re-
trained on the Shared Task data enriched with differ-
ent outputs of syntactic parsers as described above.
The results for the event extraction task are repre-
sented in Table 1. Due to the space limitation of
this paper we provide the summarized results of im-
portant event extraction sub-tasks only, i.e., results
for basic events (Gene Expression, Transcription,
Localization, Protein Catabolism) are summarized
14For the training of dependency parsers, we used from the
available Stanford conversion variants only Stanford basic. The
collapsed and ccprocessed variants do not provide dependency
trees and are not recommended for training native dependency
parsers.
15We used the GENIA Treebank version 1.0, available from
www-tsujii.is.s.u-tokyo.ac.jp
16http://nlp.stanford.edu/software/
lex-parser.shtml
17http://nlp.cs.lth.se/software/treebank_
converter/
under SVT-TOTAL; regulatory events are summa-
rized under REG-TOTAL; the overall extraction re-
sults are listed in ALL-TOTAL (see Table 1).
Obviously, the event extraction system trained on
various dependency representations indeed produces
truly different results. The differences in terms of F-
score come up to 2.4 percentage points for the SVT-
TOTAL events (cf. the MALT parser, difference
between SD basic (75.6% F-score) and CoNLL?07
(78.0% F-score)), up to 3.6 points for REG-TOTAL
(cf. the M+C parser, difference between SD ccpro-
cessed (40.9% F-score) and CoNLL?07 (44.5% F-
score)) and up to 2.5 points for ALL-TOTAL (cf.
the M+C parser, difference between SD ccprocessed
(52.8% F-score) and CoNLL?07 (55.3% F-score)).
The top three event extraction results on the de-
velopment data based on different syntactic parsers
results are achieved with M+C parser ? CoNLL?07
representation (55.3% F-score), MST parser ?
CoNLL?08 representation (54.6% F-score) and
MALT parser ? CoNLL?08 representation (53.8%
F-score) (see Table 1, ALL-TOTAL). Surprisingly,
both the CoNLL?08 and CoNLL?07 formats clearly
outperform Stanford representations on all event ex-
traction tasks. Stanford dependencies seem to be
useful here only in the basic mode. The collapsed
and ccprocessed modes produce even worse results
for the event extraction tasks.
Our second experiment focused on trimming op-
erations on CoNLL?X dependency graphs. Here
we performed event extraction after the trimming of
the dependency trees as described in Section 4.2 in
different modes: coords ? re-structuring coordina-
tions; preps ? collapsing of prepositions; auxiliaries
? propagating dependency relations of auxiliars and
modals to main verbs; noun phrase ? re-structuring
noun phrases containing action adjectives. Our sec-
ond experiment showed that the extraction of se-
lected events can profit in particular from the trim-
ming procedures coords and auxiliaries, but there is
no evidence for a general trimming configuration for
the overall event extraction task.
In Table 2 we summarize the best configurations
we found for the events in focus. It is quite evi-
dent that the CoNLL?08 and CoNLL?07 dependen-
cies modified for auxiliaries and coordinations are
the best configurations for four events (out of nine).
For three events no modifications are necessary and
987
Parser SD basic SD collapsed SD ccprocessed CoNLL?07 CoNLL?08
SVT-TOTAL
R P F R P F R P F R P F R P F
Bikel 70.5 75.5 72.9 70.7 74.5 72.5 71.6 73.5 72.5 69.4 75.9 72.5 69.7 75.7 72.6
C+J 73.0 77.4 75.1 73.2 77.3 75.2 72.8 77.2 75.0 73.5 78.3 75.8 73.0 77.9 75.4
M+C 76.4 78.0 77.2 76.4 77.6 77.0 76.4 77.2 76.8 76.4 79.0 77.7 76.6 79.3 77.9
GDEP 77.1 77.5 77.3 N/A N/A N/A N/A N/A N/A 72.5 80.2 76.1 72.6 77.2 74.8
MALT 73.1 78.2 75.6 N/A N/A N/A N/A N/A N/A 75.9 80.3 78.0 73.7 78.2 75.9
MST 76.4 78.5 77.4 N/A N/A N/A N/A N/A N/A 74.8 78.4 76.6 76.7 80.8 78.7
REG-TOTAL
R P F R P F R P F R P F R P F
Bikel 35.3 40.6 37.8 33.8 40.3 36.8 34.3 39.6 36.8 33.9 39.2 36.3 34.0 41.0 37.2
C+J 36.2 41.8 38.8 37.3 41.8 39.4 36.5 41.9 39.0 38.1 43.9 40.8 37.4 44.0 40.4
M+C 39.4 45.5 42.3 38.8 45.3 41.8 38.5 43.7 40.9 41.9 47.4 44.5 40.1 47.9 43.7
GDEP 39.6 42.8 41.6 N/A N/A N/A N/A N/A N/A 38.4 43.7 40.9 39.8 44.4 42.0
MALT 38.8 44.3 41.4 N/A N/A N/A N/A N/A N/A 39.0 44.3 41.5 39.2 46.4 42.5
MST 39.5 43.6 41.4 N/A N/A N/A N/A N/A N/A 39.6 45.6 42.4 40.6 45.8 43.0
ALL-TOTAL
R P F R P F R P F R P F R P F
Bikel 47.4 51.5 49.4 46.3 50.8 48.5 46.9 50.2 48.5 44.8 50.7 47.6 44.7 51.8 48.0
C+J 49.3 53.8 51.5 49.6 52.8 51.2 49.0 53.0 50.9 50.3 54.4 52.3 49.5 54.3 51.8
M+C 52.3 56.4 54.3 51.8 55.7 53.7 51.3 54.3 52.8 53.2 57.5 55.3 52.2 58.2 55.0
GDEP 52.7 54.5 53.6 N/A N/A N/A N/A N/A N/A 50.6 55.2 52.8 51.3 55.0 53.1
MALT 50.4 54.7 52.4 N/A N/A N/A N/A N/A N/A 51.5 56.0 53.7 51.2 56.8 53.8
MST 52.3 54.8 53.5 N/A N/A N/A N/A N/A N/A 51.7 56.4 53.9 52.4 56.9 54.6
Table 1: Results on the Shared Task development data for Event Extraction Task. Approximate Span Match-
ing/Approximate Recursive Matching.
Event Class Best Parser Best Configuration R P F
Gene Expression MST CoNLL?08, auxiliaries, coords 79.5 81.8 80.6
Transcription MALT CoNLL?07, auxiliaries, coords 67.1 75.3 71.0
Protein Catabolism MST CoNLL?08, preps 85.7 100 92.3
Phosphorylation MALT CoNLL?08 80.9 88.4 84.4
Localization MST CoNLL?08, auxiliaries 81.1 87.8 84.3
Binding MST CoNLL?07, auxiliaries, coords, noun phrase 51.2 51.0 51.1
Regulation MALT CoNLL?07, auxiliaries, coords 30.8 49.5 38.0
Positive Regulation M+C CoNLL?07 43.0 49.9 46.1
Negative Regulation M+C CoNLL?07 49.5 45.3 47.3
Table 2: Best Configurations for Dependency Representations for Event Extraction Task on the development data.
Binding R P F
CoNLL?07 47.3 46.8 47.0
CoNLL?07 auxiliaries, coords 46.8 48.1 47.4
CoNLL?07 auxiliaries, coords, noun phrase 51.2 51.0 51.1
Table 3: Effects of trimming of CoNLL dependencies on the Shared Task development data for Binding events. Ap-
proximate Span Matching/Approximate Recursive Matching. The data was processed by the MST parser.
988
JULIELab JULIELab TOKYO System
(M+C, CoNLL?08) Final Configuration
Event Class gold R P F R P F R P F
Gene Expression 356 79.2 80.3 79.8 79.5 81.8 80.6 78.7 79.5 79.1
Transcription 82 59.8 72.0 65.3 67.1 75.3 71.0 65.9 71.1 68.4
Protein Catabolism 21 76.2 88.9 82.0 85.7 100 92.3 95.2 90.9 93.0
Phosphorylation 47 83.0 81.2 82.1 80.9 88.4 84.4 85.1 69.0 76.2
Localization 53 77.4 74.6 75.9 81.1 87.8 84.3 71.7 82.6 76.8
SVT-TOTAL 559 76.4 79.0 77.7 78.2 82.6 80.3 77.3 77.9 77.6
Binding 248 45.6 45.9 45.8 51.2 51.0 51.1 50.8 47.6 49.1
EVT-TOTAL 807 66.9 68.7 67.8 69.9 72.5 71.2 69.1 68.1 68.6
Regulation 169 32.5 46.2 38.2 30.8 49.5 38.0 36.7 46.6 41.1
Positive regulation 617 42.3 49.0 45.4 43.0 49.9 46.1 43.9 51.9 47.6
Negative regulation 196 48.5 44.0 46.1 49.5 45.3 47.3 38.8 43.9 41.2
REG-TOTAL 982 41.9 47.4 44.5 42.2 48.7 45.2 41.7 49.4 45.2
ALL-TOTAL 1789 53.2 57.5 55.3 54.7 60.0 57.2 54.1 58.7 56.3
Table 4: Results on the Shared Task development data. Approximate Span Matching/Approximate Recursive Match-
ing.
JULIELab JULIELab TOKYO system
(Buyko et al, 2010) Final Configuration
Event Class gold R P F R P F R P F
Gene Expression 722 66.3 79.6 72.4 67.0 77.2 71.8 68.7 79.9 73.9
Transcription 137 33.6 61.3 43.4 35.0 60.8 44.4 54.0 60.7 57.1
Protein Catabolism 14 71.4 90.9 80.0 71.4 90.9 80.0 42.9 75.0 54.6
Phosphorylation 135 80.0 85.0 82.4 80.7 84.5 82.6 84.4 69.5 76.3
Localization 174 47.7 93.3 63.1 45.4 90.8 60.5 47.1 86.3 61.0
SVT-TOTAL 1182 61.4 80.3 69.6 61.8 78.2 69.0 65.3 76.4 70.4
Binding 347 47.3 52.4 49.7 47.3 52.2 49.6 52.2 53.1 52.6
EVT-TOTAL 1529 58.2 73.1 64.8 58.5 71.7 64.4 62.3 70.5 66.2
Regulation 291 24.7 40.5 30.7 26.8 38.2 31.5 28.9 39.8 33.5
Positive Regulation 983 35.8 45.4 40.0 34.8 45.8 39.5 38.0 48.3 42.6
Negative Regulation 379 37.2 39.7 38.4 37.5 40.9 39.1 35.9 47.2 40.8
REG-TOTAL 1653 34.2 43.2 38.2 34.0 43.3 38.0 35.9 46.7 40.6
ALL-TOTAL 3182 45.7 57.6 51.0 45.8 57.2 50.9 48.6 59.0 53.3
Table 5: Results on the Shared Task test data. Approximate Span Matching/Approximate Recursive Matching.
989
only one event profits from trimming of prepositions
(Protein Catabolism). Only the Binding event prof-
its significantly from noun phrase modifications (see
Table 3). The increase in F-score for trimming pro-
cedures is 4.1 percentage points for Binding events.
In our final experiment we connected the best con-
figurations for each of the BioNLP?09 events as pre-
sented in Table 2. The overall event extraction re-
sults of this final configuration are presented in Ta-
bles 4 and 5. We achieved an increase of 1.9 per-
centage points F-score in the overall event extrac-
tion compared to the best-performing single parser
configuration (M+C, CoNLL?07) (see Table 4, ALL-
TOTAL). The reported results on the development
data outperform the results of the TOKYO system by
2.6 percentage points F-score for all basic events in-
cluding Binding events (see Table 4, EVT-TOTAL)
and by 0.9 percentage points in the overall event ex-
traction task (see Table 4, ALL-TOTAL).
On the test data we achieved an F-score similar
to the current JULIELab system trained on modified
CoNLL?07 dependencies from the MST parser (see
Table 5, ALL-TOTAL).18 The results on the offi-
cial test data reveal that the performance differences
between various parsers may play a much smaller
role than the proper choice of dependency represen-
tations.19 Our empirical findings that the best per-
formance results could only be achieved by event-
specific dependency graph configurations reveal that
the syntactic representations of different semantic
events vary considerably at the level of dependency
graph complexity and that the automatic prediction
of such syntactic structures can vary from one de-
pendency parser to the other.
6 Discussion
The evaluation results from Table 1 show that an in-
creased F-score is basically due to a better perfor-
mance in terms of precision. For example, the M+C
evaluation results in the Stanford basic mode pro-
vide an increased precision by 2 percentage points
compared to the Stanford ccprocessed mode. There-
fore, we focus here on the analysis of false positives
18The current JULIELab system uses event-specific trimming
procedures on CoNLL?07 dependencies determined on the de-
velopment data set (see Buyko et al (2010)).
19Trimmed CoNLL dependencies are used in both system
configurations.
that the JULIELab system extracts in various modes.
For the first analysis we took the outputs of the
systems based on the M+C parsing results. We
scrutinized on the Stanford basic and ccprocessed
false positives (fps) and we compared the occur-
rences of dependency labels in two data sets, namely
the intersection of false positives from both sys-
tem modes (set A) and the false positives produced
only by the system with a worse performance (set
B, ccprocessed mode). About 70% of all fps are
contained in set A. Our analysis revealed that some
dependency labels have a higher occurrence in set
B, e.g., nsubjpass, prep on, prep with,
prep in, prep for, prep as. Some depen-
dency labels occur only in set B such as agent,
prep unlike, prep upon. It seems that col-
lapsing some prepositions, such as ?with?, ?in?,
?for?, ?as?, ?on?, ?unlike?, ?upon?, does not have
a positive effect on the extraction of argument struc-
tures. In a second step, we compared the Stan-
ford basic and CoNLL?07 false positive sets. The
fps of both systems have an intersection of about
70%. We also compared the intersection of fps
between two outputs (set A) and the set of addi-
tional fps of the system with worse results (Stan-
ford basic mode, set B). The dependency labels such
as abbrev, dep, nsubj, nsubjpass have
a higher occurrence in set B than in set A. This anal-
ysis renders evidence that the distinction of nsubj
and nsubjpass does not seem to have been prop-
erly learned for event extraction.
For the second analysis round we took the out-
puts of the MST parsing results. As in the previ-
ous experiments, we compared false positives from
two mode outputs, here the CoNLL?07 mode and
the CoNLL?07 modified for auxiliaries and coor-
dinations mode. The fps have an intersection of
75%. The dependency labels such as VC, SUBJ,
COORD, and IOBJ occur more frequently in the ad-
ditional false positives from the CoNLL?07 mode
than in the intersection of false positives from both
system outputs. Obviously, the trimming of auxil-
iary and coordination structures has a direct positive
effect on the argument extraction reducing false pos-
itive numbers especially with corresponding depen-
dency labels in shortest dependency paths.
Our analysis of false positives shows that the dis-
tinction between active and passive subject labels,
990
abbreviation labels, as well as collapsing preposi-
tions in the Stanford dependencies, could not have
been properly learned, which consequently leads to
an increased rate of false positives. The trimming
of auxiliary structures and the subsequent coordina-
tion collapsing on CoNLL?07 dependencies has in-
deed event-specific positive effects on the event ex-
traction.
The main focus of this work has been on the eval-
uation of effects of different dependency graph rep-
resentations on the IE task achievement (here the
task of event extraction). But we also targeted the
task-oriented evaluation of top-performing syntactic
parsers. The results of this work indicate that the
GENIA-trained parsers, i.e., M+C parser, the MST,
MALT and GDep, are a reasonable basis for achiev-
ing state-of-the art performance in biomedical event
extraction.
But the choice of the most suitable parser should
also take into account its performance in terms of
parsing time. Cer et al (2010) and Miyao et al
(2008) showed in their experiments that native de-
pendency parsers are faster than constituency-based
parsers. When it comes to scaling event extraction
to huge biomedical document collections, such as
MEDLINE, the selection of a parser is mainly in-
fluenced by its run-time performance. MST, MALT
and GDep parsers or the M+C parser with reduced
reranking (Cer et al, 2010) would thus be an appro-
priate choice for large-scale event extraction under
these constraints.20
7 Conclusion
In this paper, we investigated the role different de-
pendency representations may have on the accom-
plishment of the event extraction task as exemplified
by biological events. Different representation for-
mats (mainly, Stanford vs. CoNLL) were then ex-
perimentally compared employing different parsers
(Bikel, Charniak+Johnson, GDep, MST, MALT),
both constituency based (for the derivative depen-
dency mode) as well as dependency based (for
the native dependency mode), considering different
training scenarios (newspaper vs. biology domain).
From our experiments we draw the conclusion
20For large-scale experiments an evaluation of the M+C with
reduced reranking should be provided.
that the dependency graph representation has a cru-
cial impact on the level of achievement of IE task
requirements. Surprisingly, the CoNLL?X depen-
dencies outperform the Stanford dependencies for
four from six parsers. With additionally trimmed
CoNLL?X dependencies we could achieve an F-
score of 50.9% on the official test data and an F-
score of 57.2% on the official development data of
the BioNLP Shared Task on Event Extraction (see
Table 5, ALL-TOTAL).
Acknowledgements
This research was partially funded by the BOOT-
STREP project under grant FP6-028099 within the
6th Framework Programme (EC), by the CALBC
project under grant FP7-231727 within the 7th
Framework Programme (EC), and by the JENAGE
project under grant 0315581D from the German
Ministry of Education and Research (BMBF) as part
of the GERONTOSYS funding initiative. We also
want to thank Kenji Sagae (Institute for Creative
Technologies, University of Southern California) for
kindly providing the models of the GDEP parser.
References
Daniel M. Bikel. 2004. Intricacies of Collins? parsing
model. Computational Linguistics, 30(4):479?511.
Jari Bjo?rne, Juho Heimonen, Filip Ginter, Antti Airola,
Tapio Pahikkala, and Tapio Salakoski. 2009. Extract-
ing complex biological events with rich graph-based
feature sets. In Proceedings BioNLP 2009. Compan-
ion Volume: Shared Task on Event Extraction, pages
10?18. Boulder, Colorado, USA, June 4-5, 2009.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-
X Shared Task on multilingual dependency parsing.
In CoNLL-X ? Proceedings of the 10th Conference
on Computational Natural Language Learning, pages
149?164, New York City, N.Y., June 8-9, 2006.
Razvan Bunescu, Ruifang Ge, Rohit J. Kate, Edward M.
Marcotte, Raymond J. Mooney, Arun K. Ramani, and
Yuk Wah Wong. 2005. Comparative experiments
on learning information extractors for proteins and
their interactions. Artificial Intelligence in Medicine,
33(2):139?155.
Ekaterina Buyko, Erik Faessler, Joachim Wermter, and
Udo Hahn. 2009. Event extraction from trimmed
dependency graphs. In Proceedings BioNLP 2009.
Companion Volume: Shared Task on Event Extrac-
991
tion, pages 19?27. Boulder, Colorado, USA, June 4-5,
2009.
Ekaterina Buyko, Erik Faessler, Joachim Wermter, and
Udo Hahn. 2010. Syntactic simplification and se-
mantic enrichment - Trimming dependency graphs for
event extraction. Computational Intelligence, 26(4).
Daniel Cer, Marie-Catherine de Marneffe, Dan Jurafsky,
and Chris Manning. 2010. Parsing to Stanford De-
pendencies: Trade-offs between speed and accuracy.
In LREC?2010 ? Proceedings of the 7th International
Conference on Language Resources and Evaluation,
pages 1628?1632. Valletta, Malta, May 19-21, 2010.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and MaxEnt discriminative rerank-
ing. In ACL?05 ? Proceedings of the 43rd Annual
Meeting of the Association for Computational Linguis-
tics, pages 173?180. Ann Arbor, MI, USA, 25-30,
June, 2005.
Andrew B. Clegg and Adrian J. Shepherd. 2005. Evalu-
ating and integrating Treebank parsers on a biomedical
corpus. In Proceedings of the ACL 2005 Workshop on
Software, pages 14?33. Ann Arbor, MI, USA, June 30,
2005.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
LREC?2006 ? Proceedings of the 5th International
Conference on Language Resources and Evaluation,
pages 449?454. Genoa, Italy, 24-26 May 2006.
Richard Johansson and Pierre Nugues. 2007. Extended
constituent-to-dependency conversion for English. In
NODALIDA 2007 ? Proceedings of the 16th Nordic
Conference of Computational Linguistics, pages 105?
112. Tartu, Estonia, May 24-25, 2007.
Jin-Dong Kim, Tomoko Ohta, and Jun?ichi Tsujii. 2008.
Corpus annotation for mining biomedical events from
literature. BMC Bioinformatics, 9(10).
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Jun?ichi Tsujii. 2009. Overview of
BioNLP?09 Shared Task on Event Extraction. In Pro-
ceedings BioNLP 2009. Companion Volume: Shared
Task on Event Extraction, pages 1?9. Boulder, Col-
orado, USA, June 4-5, 2009.
Dekang Lin. 1998. Dependency-based evaluation of
MINIPAR. In Proceedings of the LREC?98 Workshop
on the Evaluation of Parsing Systems, pages 48?56.
Granada, Spain, 28-30 May 1998.
David McClosky. 2010. Any Domain Parsing: Auto-
matic Domain Adaptation for Natural Language Pars-
ing. Ph.D. thesis, Department of Computer Science,
Brown University.
Ryan T. McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic. 2005. Non-projective dependency pars-
ing using spanning tree algorithms. In HLT/EMNLP
2005 ? Proceedings of the Human Language Tech-
nology Conference and the Conference on Empirical
Methods in Natural Language Processing, pages 523?
530. Vancouver, B.C., Canada, October 6-8, 2005.
Makoto Miwa, Rune S?tre, Jin-Dong Kim, and Jun?ichi
Tsujii. 2010. Event extraction with complex event
classification using rich features. Journal of Bioinfor-
matics and Computational Biology, 8:131?146.
Yusuke Miyao and Jun?ichi Tsujii. 2002. Maximum en-
tropy estimation for feature forests. In HLT 2002 ?
Proceedings of the 2nd International Conference on
Human Language Technology Research, pages 292?
297. San Diego, CA, USA, March 24-27, 2002.
Yusuke Miyao, Kenji Sagae, and Jun?ichi Tsujii. 2007.
Towards framework-independent evaluation of deep
linguistic parsers. In Proceedings of the GEAF 2007
Workshop, CSLI Studies in Computational Linguistics
Online, page 21 pages.
Yusuke Miyao, Rune S?tre, Kenji Sagae, Takuya Mat-
suzaki, and Jun?ichi Tsujii. 2008. Task-oriented eval-
uation of syntactic parsers and their representations. In
ACL 2008 ? Proceedings of the 46th Annual Meeting
of the Association for Computational Linguistics: Hu-
man Language Technologies, pages 46?54. Columbus,
Ohio, USA, June 15-20, 2008.
Claire Ne?dellec. 2005. Learning Language in Logic:
Genic interaction extraction challenge. In Proceedings
LLL-2005 ? 4th Learning Language in Logic Work-
shop, pages 31?37. Bonn, Germany, August 7, 2005.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2007.
MALTPARSER: A language-independent system for
data-driven dependency parsing. Natural Language
Engineering, 13(2):95?135.
Sampo Pyysalo, Filip Ginter, Juho Heimonen, Jari
Bjo?rne, Jorma Boberg, Jouni Jarvinen, and Tapio
Salakoski. 2007. BIOINFER: A corpus for informa-
tion extraction in the biomedical domain. BMC Bioin-
formatics, 8(50).
Kenji Sagae and Jun?ichi Tsujii. 2007. Dependency pars-
ing and domain adaptation with LR models and parser
ensembles. In EMNLP-CoNLL 2007 ? Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing and the Conference on Computa-
tional Natural Language Learning, pages 1044?1050,
Prague, Czech Republic, June 28-30, 2007.
Daniel Sleator and Davy Temperley. 1991. Parsing En-
glish with a link grammar. Technical report, Depart-
ment of Computer Science, CMU.
Yuka Tateisi, Akane Yakushiji, and Jun?ichi Tsujii. 2005.
Syntax annotation for the GENIA corpus. In IJC-
NLP 2005 ? Proceedings of the 2nd International Joint
Conference on Natural Language Processing, pages
222?227. Jeju Island, Korea, October 11-13, 2005.
992
